@inproceedings{antunes_exploring_2024,
	location = {Cham},
	title = {Exploring Domain Adaptation with Depth-Based 3D Object Detection in {CARLA} Simulator},
	isbn = {978-3-031-58676-7},
	doi = {10.1007/978-3-031-58676-7_9},
	abstract = {Data collection and scene understanding have become crucial tasks in the development of intelligent vehicles, particularly in the context of autonomous driving. Deep Learning ({DL}) and transformer-based architectures have emerged as the preferred methods for object detection and segmentation tasks. However, {DL}-based methods often require extensive training with diverse data, posing challenges in terms of data availability and labeling. To address this problem, techniques such as transfer learning and data augmentation have been adopted. Simulators like {CARLA} have gained popularity in the autonomous driving domain, enabling the evaluation of architectures in realistic environments before real-world deployment. Synthetic data generated by simulators offers several advantages, including cost-effectiveness, access to diverse scenarios, and the ability to generate accurate ground truth annotations. In this paper, we focus our investigation on evaluating the performance and domain adaptation capabilities of a 3D object detection pipeline based on depth estimation using a stereo camera in the {CARLA} simulator. Our main objective is to analyze the results of the depth estimation stage using two different approaches: {CoEx} and {SDN}. The different experiments will be performed on real and synthetic scenarios from the {KITTI} and {SHIFT} datasets.},
	pages = {105--117},
	booktitle = {Robot 2023: Sixth Iberian Robotics Conference},
	publisher = {Springer Nature Switzerland},
	author = {Antunes, Miguel and Bergasa, Luis M. and Montiel-Marín, Santiago and Sánchez-García, Fabio and Pardo-Decimavilla, Pablo and Revenga, Pedro},
	editor = {Marques, Lino and Santos, Cristina and Lima, José Luís and Tardioli, Danilo and Ferre, Manuel},
	date = {2024},
	langid = {english},
	keywords = {3D object detection, {CARLA}, Deep Learning, {KITTI}, Transfer Learning},
}

@online{autonomousvision_kitti-360_2024,
	title = {{KITTI}-360 Annotation Tool},
	rights = {{MIT}},
	url = {https://github.com/autonomousvision/kitti360LabelTool},
	author = {autonomousvision},
	urldate = {2024-11-11},
	date = {2024-10-14},
}

@article{bavirisetti_simulated_2023,
	title = {Simulated {RGB} and {LiDAR} Image based Training of Object Detection Models in the Context of Autonomous Driving},
	rights = {Copyright (c) 2023 Norsk {IKT}-konferanse for forskning og utdanning},
	issn = {1892-0721},
	url = {https://www.ntnu.no/ojs/index.php/nikt/article/view/5665},
	abstract = {The topic of object detection, which involves giving cars the ability to perceive their environment has drawn greater attention. For better performance, object detection algorithms often need huge datasets, which are frequently manually labeled. This procedure is expensive and time-consuming. Instead, a simulated environment due to which one has complete control over all parameters and allows for automated image annotation. Carla, an open-source project created exclusively for the study of autonomous driving, is one such simulator. This study examines if object detection models that can recognize actual traffic items can be trained using automatically annotated simulator data from Carla. The findings of the experiments demonstrate that optimizing a trained model using Carla’s data, along with some real data, is encouraging. The Yolov5 model, trained using pre-trained Carla weights, exhibited improvements across all performance metrics compared to one trained exclusively on 2000 Kitti images. While it didn’t reach the performance level of the 6000-image Kitti model, the enhancements were indeed substantial. The {mAP}0.5:0.95 score saw an approximate 10\% boost, with the most significant improvement occurring in the Pedestrian class. Furthermore, it is demonstrated that a substantial performance boost can be achieved by training a base model with Carla data and fine-tuning it with a smaller portion of the Kitti dataset. Moreover, the potential utility of Carla {LiDAR} images in reducing the volume of real images required while maintaining respectable model performance becomes evident. Our code is available at: https://tinyurl.com/3fdjd9xb.},
	number = {1},
	journaltitle = {Norsk {IKT}-konferanse for forskning og utdanning},
	author = {Bavirisetti, Durga Prasad and Brobak, Eskild and Espen, Peder and Kiss, Gabriel and Lindseth, Frank},
	urldate = {2024-11-11},
	date = {2023-11-30},
	langid = {english},
	note = {Number: 1},
	keywords = {Computer vision},
}

@misc{brekke_multimodal_2019,
	title = {Multimodal 3D Object Detection from Simulated Pretraining},
	url = {http://arxiv.org/abs/1905.07754},
	doi = {10.48550/arXiv.1905.07754},
	abstract = {The need for simulated data in autonomous driving applications has become increasingly important, both for validation of pretrained models and for training new models. In order for these models to generalize to real-world applications, it is critical that the underlying dataset contains a variety of driving scenarios and that simulated sensor readings closely mimics real-world sensors. We present the Carla Automated Dataset Extraction Tool ({CADET}), a novel tool for generating training data from the {CARLA} simulator to be used in autonomous driving research. The tool is able to export high-quality, synchronized {LIDAR} and camera data with object annotations, and offers configuration to accurately reflect a real-life sensor array. Furthermore, we use this tool to generate a dataset consisting of 10 000 samples and use this dataset in order to train the 3D object detection network {AVOD}-{FPN}, with finetuning on the {KITTI} dataset in order to evaluate the potential for effective pretraining. We also present two novel {LIDAR} feature map configurations in Bird's Eye View for use with {AVOD}-{FPN} that can be easily modified. These configurations are tested on the {KITTI} and {CADET} datasets in order to evaluate their performance as well as the usability of the simulated dataset for pretraining. Although insufficient to fully replace the use of real world data, and generally not able to exceed the performance of systems fully trained on real data, our results indicate that simulated data can considerably reduce the amount of training on real data required to achieve satisfactory levels of accuracy.},
	number = {{arXiv}:1905.07754},
	publisher = {{arXiv}},
	author = {Brekke, Åsmund and Vatsendvik, Fredrik and Lindseth, Frank},
	urldate = {2024-11-11},
	date = {2019-05-19},
	eprinttype = {arxiv},
	eprint = {1905.07754},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{cabon_virtual_2020,
	title = {Virtual {KITTI} 2},
	url = {http://arxiv.org/abs/2001.10773},
	doi = {10.48550/arXiv.2001.10773},
	abstract = {This paper introduces an updated version of the well-known Virtual {KITTI} dataset which consists of 5 sequence clones from the {KITTI} tracking benchmark. In addition, the dataset provides different variants of these sequences such as modified weather conditions (e.g. fog, rain) or modified camera configurations (e.g. rotated by 15 degrees). For each sequence, we provide multiple sets of images containing {RGB}, depth, class segmentation, instance segmentation, flow, and scene flow data. Camera parameters and poses as well as vehicle locations are available as well. In order to showcase some of the dataset's capabilities, we ran multiple relevant experiments using state-of-the-art algorithms from the field of autonomous driving. The dataset is available for download at https://europe.naverlabs.com/Research/Computer-Vision/Proxy-Virtual-Worlds.},
	number = {{arXiv}:2001.10773},
	publisher = {{arXiv}},
	author = {Cabon, Yohann and Murray, Naila and Humenberger, Martin},
	urldate = {2024-11-19},
	date = {2020-01-29},
	eprinttype = {arxiv},
	eprint = {2001.10773},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{caesar_nuscenes_2020,
	title = {{nuScenes}: A multimodal dataset for autonomous driving},
	url = {http://arxiv.org/abs/1903.11027},
	doi = {10.48550/arXiv.1903.11027},
	shorttitle = {{nuScenes}},
	abstract = {Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present {nuTonomy} scenes ({nuScenes}), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. {nuScenes} comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering {KITTI} dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.},
	number = {{arXiv}:1903.11027},
	publisher = {{arXiv}},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	urldate = {2024-02-28},
	date = {2020-05-05},
	eprinttype = {arxiv},
	eprint = {1903.11027 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
}

@misc{deng_voxel_2021,
	title = {Voxel R-{CNN}: Towards High Performance Voxel-based 3D Object Detection},
	url = {http://arxiv.org/abs/2012.15712},
	doi = {10.48550/arXiv.2012.15712},
	shorttitle = {Voxel R-{CNN}},
	abstract = {Recent advances on 3D object detection heavily rely on how the 3D data are represented, {\textbackslash}emph\{i.e.\}, voxel-based or point-based representation. Many existing high performance 3D detectors are point-based because this structure can better retain precise point positions. Nevertheless, point-level features lead to high computation overheads due to unordered storage. In contrast, the voxel-based structure is better suited for feature extraction but often yields lower accuracy because the input data are divided into grids. In this paper, we take a slightly different viewpoint -- we find that precise positioning of raw points is not essential for high performance 3D object detection and that the coarse voxel granularity can also offer sufficient detection accuracy. Bearing this view in mind, we devise a simple but effective voxel-based framework, named Voxel R-{CNN}. By taking full advantage of voxel features in a two stage approach, our method achieves comparable detection accuracy with state-of-the-art point-based models, but at a fraction of the computation cost. Voxel R-{CNN} consists of a 3D backbone network, a 2D bird-eye-view ({BEV}) Region Proposal Network and a detect head. A voxel {RoI} pooling is devised to extract {RoI} features directly from voxel features for further refinement. Extensive experiments are conducted on the widely used {KITTI} Dataset and the more recent Waymo Open Dataset. Our results show that compared to existing voxel-based methods, Voxel R-{CNN} delivers a higher detection accuracy while maintaining a real-time frame processing rate, {\textbackslash}emph\{i.e\}., at a speed of 25 {FPS} on an {NVIDIA} {RTX} 2080 Ti {GPU}. The code is available at {\textbackslash}url\{https://github.com/djiajunustc/Voxel-R-{CNN}\}.},
	number = {{arXiv}:2012.15712},
	publisher = {{arXiv}},
	author = {Deng, Jiajun and Shi, Shaoshuai and Li, Peiwei and Zhou, Wengang and Zhang, Yanyong and Li, Houqiang},
	urldate = {2022-10-31},
	date = {2021-02-05},
	eprinttype = {arxiv},
	eprint = {2012.15712 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{deschaud_kitti-carla_2021,
	title = {{KITTI}-{CARLA}: a {KITTI}-like dataset generated by {CARLA} Simulator},
	url = {http://arxiv.org/abs/2109.00892},
	doi = {10.48550/arXiv.2109.00892},
	shorttitle = {{KITTI}-{CARLA}},
	abstract = {{KITTI}-{CARLA} is a dataset built from the {CARLA} v0.9.10 simulator using a vehicle with sensors identical to the {KITTI} dataset. The vehicle thus has a Velodyne {HDL}64 {LiDAR} positioned in the middle of the roof and two color cameras similar to Point Grey Flea 2. The positions of the {LiDAR} and cameras are the same as the setup used in {KITTI}. The objective of this dataset is to test approaches of semantic segmentation {LiDAR} and/or images, odometry {LiDAR} and/or image in synthetic data and to compare with the results obtained on real data like {KITTI}. This dataset thus makes it possible to improve transfer learning methods from a synthetic dataset to a real dataset. We created 7 sequences with 5000 frames in each sequence in the 7 maps of {CARLA} providing different environments (city, suburban area, mountain, rural area, highway...). The dataset is available at: http://npm3d.fr},
	number = {{arXiv}:2109.00892},
	publisher = {{arXiv}},
	author = {Deschaud, Jean-Emmanuel},
	urldate = {2024-11-11},
	date = {2021-08-17},
	eprinttype = {arxiv},
	eprint = {2109.00892},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{deschaud_paris-carla-3d_2021,
	title = {Paris-{CARLA}-3D: A Real and Synthetic Outdoor Point Cloud Dataset for Challenging Tasks in 3D Mapping},
	volume = {13},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/22/4713},
	doi = {10.3390/rs13224713},
	shorttitle = {Paris-{CARLA}-3D},
	abstract = {Paris-{CARLA}-3D is a dataset of several dense colored point clouds of outdoor environments built by a mobile {LiDAR} and camera system. The data are composed of two sets with synthetic data from the open source {CARLA} simulator (700 million points) and real data acquired in the city of Paris (60 million points), hence the name Paris-{CARLA}-3D. One of the advantages of this dataset is to have simulated the same {LiDAR} and camera platform in the open source {CARLA} simulator as the one used to produce the real data. In addition, manual annotation of the classes using the semantic tags of {CARLA} was performed on the real data, allowing the testing of transfer methods from the synthetic to the real data. The objective of this dataset is to provide a challenging dataset to evaluate and improve methods on difficult vision tasks for the 3D mapping of outdoor environments: semantic segmentation, instance segmentation, and scene completion. For each task, we describe the evaluation protocol as well as the experiments carried out to establish a baseline.},
	pages = {4713},
	number = {22},
	journaltitle = {Remote Sensing},
	author = {Deschaud, Jean-Emmanuel and Duque, David and Richa, Jean Pierre and Velasco-Forero, Santiago and Marcotegui, Beatriz and Goulette, François},
	urldate = {2024-02-28},
	date = {2021-01},
	langid = {english},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D mapping, {LiDAR}, dataset, laser scanning, mobile mapping, outdoor, point cloud, scene completion, semantic, synthetic},
}

@misc{dosovitskiy_carla_2017,
	title = {{CARLA}: An Open Urban Driving Simulator},
	url = {http://arxiv.org/abs/1711.03938},
	doi = {10.48550/arXiv.1711.03938},
	shorttitle = {{CARLA}},
	abstract = {We introduce {CARLA}, an open-source simulator for autonomous driving research. {CARLA} has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, {CARLA} provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use {CARLA} to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by {CARLA}, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
	number = {{arXiv}:1711.03938},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	urldate = {2024-11-12},
	date = {2017-11-10},
	eprinttype = {arxiv},
	eprint = {1711.03938},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{dworak_performance_2019,
	title = {Performance of {LiDAR} object detection deep learning architectures based on artificially generated point cloud data from {CARLA} simulator},
	url = {https://ieeexplore.ieee.org/document/8864642},
	doi = {10.1109/MMAR.2019.8864642},
	abstract = {Training deep neural network algorithms for {LiDAR} based object detection for autonomous cars requires huge amount of labeled data. Both data collection and labeling requires a lot of effort, money and time. Therefore, the use of simulation software for virtual data generation environments is gaining wide interest from both researchers and engineers. The big question remains how well artificially generated data resembles the data gathered by real sensors and how the differences affects the final algorithms performance. The article is trying to make a quantitative answer to the above question. Selected state-of-the-art algorithms for {LiDAR} point cloud object detection were trained on both real and artificially generated data sets. Their performance on different test sets were evaluated. The main focus was to determinate how well artificially trained networks perform on real data and if combined train sets can achieve better results overall.},
	eventtitle = {2019 24th International Conference on Methods and Models in Automation and Robotics ({MMAR})},
	pages = {600--605},
	author = {Dworak, Daniel and Ciepiela, Filip and Derbisz, Jakub and Izzat, Izzat and Komorkiewicz, Mateusz and Wójcik, Mateusz},
	urldate = {2024-11-11},
	date = {2019-08},
	keywords = {{CARLA}, Databases, Feature extraction, {KITTI}, Laser radar, {LiDAR}, Object detection, Sensors, Three-dimensional displays, Training, artificial data, automotive, deep learning, object detection, point cloud, simulator},
}

@software{epic_games_unreal_2019,
	title = {Unreal Engine},
	url = {https://www.unrealengine.com},
	version = {4.22.1},
	author = {{Epic Games}},
	date = {2019-04-25},
}

@misc{gaidon_virtual_2016,
	title = {Virtual Worlds as Proxy for Multi-Object Tracking Analysis},
	url = {http://arxiv.org/abs/1605.06457},
	doi = {10.48550/arXiv.1605.06457},
	abstract = {Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called Virtual {KITTI} (see http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds), automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.},
	number = {{arXiv}:1605.06457},
	publisher = {{arXiv}},
	author = {Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},
	urldate = {2024-11-19},
	date = {2016-05-20},
	eprinttype = {arxiv},
	eprint = {1605.06457},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{geiger_are_2012,
	title = {Are we ready for autonomous driving? The {KITTI} vision benchmark suite},
	doi = {10.1109/CVPR.2012.6248074},
	shorttitle = {Are we ready for autonomous driving?},
	abstract = {Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/{SLAM} and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti.},
	eventtitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {3354--3361},
	booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Geiger, A. and Lenz, P. and Urtasun, R.},
	date = {2012-06},
	note = {{ISSN}: 1063-6919},
	keywords = {3D object detection, Benchmark testing, Cameras, {KITTI} vision benchmark suite, Measurement, Middlebury perform, Optical imaging, Optical sensors, {SLAM}, {SLAM} (robots), Velodyne laser scanner, Visualization, autonomous driving, high resolution video cameras, image sequences, object detection, optical flow image pairs, robot vision, stereo image processing, stereo visual odometry sequences, video signal processing, visual recognition systems},
}

@misc{huch_quantifying_2023,
	title = {Quantifying the {LiDAR} Sim-to-Real Domain Shift: A Detailed Investigation Using Object Detectors and Analyzing Point Clouds at Target-Level},
	url = {http://arxiv.org/abs/2303.01899},
	doi = {10.48550/arXiv.2303.01899},
	shorttitle = {Quantifying the {LiDAR} Sim-to-Real Domain Shift},
	abstract = {{LiDAR} object detection algorithms based on neural networks for autonomous driving require large amounts of data for training, validation, and testing. As real-world data collection and labeling are time-consuming and expensive, simulation-based synthetic data generation is a viable alternative. However, using simulated data for the training of neural networks leads to a domain shift of training and testing data due to differences in scenes, scenarios, and distributions. In this work, we quantify the sim-to-real domain shift by means of {LiDAR} object detectors trained with a new scenario-identical real-world and simulated dataset. In addition, we answer the questions of how well the simulated data resembles the real-world data and how well object detectors trained on simulated data perform on real-world data. Further, we analyze point clouds at the target-level by comparing real-world and simulated point clouds within the 3D bounding boxes of the targets. Our experiments show that a significant sim-to-real domain shift exists even for our scenario-identical datasets. This domain shift amounts to an average precision reduction of around 14 \% for object detectors trained with simulated data. Additional experiments reveal that this domain shift can be lowered by introducing a simple noise model in simulation. We further show that a simple downsampling method to model real-world physics does not influence the performance of the object detectors.},
	number = {{arXiv}:2303.01899},
	publisher = {{arXiv}},
	author = {Huch, Sebastian and Scalerandi, Luca and Rivera, Esteban and Lienkamp, Markus},
	urldate = {2024-11-11},
	date = {2023-03-03},
	eprinttype = {arxiv},
	eprint = {2303.01899},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{iglesias_analysis_2024,
	location = {Rome, Italy},
	title = {Analysis of Point Cloud Domain Gap Effects for 3D Object Detection Evaluation:},
	isbn = {978-989-758-679-8},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012357200003660},
	doi = {10.5220/0012357200003660},
	shorttitle = {Analysis of Point Cloud Domain Gap Effects for 3D Object Detection Evaluation},
	eventtitle = {19th International Conference on Computer Vision Theory and Applications},
	pages = {278--285},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Iglesias, Aitor and García, Mikel and Aranjuelo, Nerea and Arganda-Carreras, Ignacio and Nieto, Marcos},
	urldate = {2024-11-11},
	date = {2024},
	langid = {english},
}

@misc{ku_joint_2018,
	title = {Joint 3D Proposal Generation and Object Detection from View Aggregation},
	url = {http://arxiv.org/abs/1712.02294},
	doi = {10.48550/arXiv.1712.02294},
	abstract = {We present {AVOD}, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses {LIDAR} point clouds and {RGB} images to generate features that are shared by two subnetworks: a region proposal network ({RPN}) and a second stage detector network. The proposed {RPN} uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the {KITTI} 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
	number = {{arXiv}:1712.02294},
	publisher = {{arXiv}},
	author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
	urldate = {2024-11-12},
	date = {2018-07-12},
	eprinttype = {arxiv},
	eprint = {1712.02294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mao_one_2021,
	title = {One Million Scenes for Autonomous Driving: {ONCE} Dataset},
	url = {http://arxiv.org/abs/2106.11037},
	doi = {10.48550/arXiv.2106.11037},
	shorttitle = {One Million Scenes for Autonomous Driving},
	abstract = {Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the {ONCE} (One {millioN} {sCenEs}) dataset for 3D object detection in the autonomous driving scenario. The {ONCE} dataset consists of 1 million {LiDAR} scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (e.g. {nuScenes} and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the {ONCE} dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at https://once-for-auto-driving.github.io/index.html.},
	number = {{arXiv}:2106.11037},
	publisher = {{arXiv}},
	author = {Mao, Jiageng and Niu, Minzhe and Jiang, Chenhan and Liang, Hanxue and Chen, Jingheng and Liang, Xiaodan and Li, Yamin and Ye, Chaoqiang and Zhang, Wei and Li, Zhenguo and Yu, Jie and Xu, Hang and Xu, Chunjing},
	urldate = {2024-11-11},
	date = {2021-10-25},
	eprinttype = {arxiv},
	eprint = {2106.11037},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@incollection{marcus_gan-based_2023,
	title = {{GAN}-Based {LiDAR} Intensity Simulation},
	volume = {1875},
	url = {http://arxiv.org/abs/2311.15415},
	abstract = {Realistic vehicle sensor simulation is an important element in developing autonomous driving. As physics-based implementations of visual sensors like {LiDAR} are complex in practice, data-based approaches promise solutions. Using pairs of camera images and {LiDAR} scans from real test drives, {GANs} can be trained to translate between them. For this process, we contribute two additions. First, we exploit the camera images, acquiring segmentation data and dense depth maps as additional input for training. Second, we test the performance of the {LiDAR} simulation by testing how well an object detection network generalizes between real and synthetic point clouds to enable evaluation without ground truth point clouds. Combining both, we simulate {LiDAR} point clouds and demonstrate their realism.},
	pages = {419--433},
	author = {Marcus, Richard and Gabel, Felix and Knoop, Niklas and Stamminger, Marc},
	urldate = {2024-02-28},
	date = {2023},
	doi = {10.1007/978-3-031-39059-3_28},
	eprinttype = {arxiv},
	eprint = {2311.15415 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@software{nozarian_fnozariancarla-kitti_2024,
	title = {fnozarian/{CARLA}-{KITTI}},
	url = {https://github.com/fnozarian/CARLA-KITTI},
	abstract = {{CARLA}-{KITTI} generates synthetic data from the {CARLA} simulator for {KITTI} 2D/3D Object Detection task.},
	author = {Nozarian, Farzad},
	urldate = {2024-11-11},
	date = {2024-09-24},
	keywords = {autonomous-driving, carla-simulator, kitti-dataset, lidar, object-detection, point-cloud, sim2real},
}

@online{nvidia_drive_2024,
	title = {{DRIVE} Sim},
	url = {https://developer.nvidia.com/drive/simulation},
	abstract = {{NVIDIA} {DRIVE} Sim - Powered by Omniverse is an end-to-end simulation platform, architected from the ground up to run large-scale, physically accurate multi-sensor simulation. It's open, scalable, modular and supports {AV} development and validation from concept to deployment, improving developer productivity and accelerating time to market.},
	titleaddon = {{NVIDIA} Developer},
	author = {{NVIDIA}},
	urldate = {2024-11-12},
	date = {2024-12-11},
	langid = {american},
}

@inproceedings{patel_simulation-based_2024,
	location = {Angers, France},
	title = {Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a {LiDAR} Point Cloud Dataset in a {SOTIF}-related Use Case:},
	isbn = {978-989-758-703-0},
	url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0012707300003702},
	doi = {10.5220/0012707300003702},
	shorttitle = {Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a {LiDAR} Point Cloud Dataset in a {SOTIF}-related Use Case},
	eventtitle = {10th International Conference on Vehicle Technology and Intelligent Transport Systems},
	pages = {415--426},
	publisher = {{SCITEPRESS} - Science and Technology Publications},
	author = {Patel, Milin and Jung, Rolf},
	urldate = {2024-11-11},
	date = {2024},
	langid = {english},
}

@misc{sanchez_parisluco3d_2024,
	title = {{ParisLuco}3D: A high-quality target dataset for domain generalization of {LiDAR} perception},
	url = {http://arxiv.org/abs/2310.16542},
	doi = {10.48550/arXiv.2310.16542},
	shorttitle = {{ParisLuco}3D},
	abstract = {{LiDAR} is an essential sensor for autonomous driving by collecting precise geometric information regarding a scene. \%Exploiting this information for perception is interesting as the amount of available data increases. As the performance of various {LiDAR} perception tasks has improved, generalizations to new environments and sensors has emerged to test these optimized models in real-world conditions. This paper provides a novel dataset, {ParisLuco}3D, specifically designed for cross-domain evaluation to make it easier to evaluate the performance utilizing various source datasets. Alongside the dataset, online benchmarks for {LiDAR} semantic segmentation, {LiDAR} object detection, and {LiDAR} tracking are provided to ensure a fair comparison across methods. The {ParisLuco}3D dataset, evaluation scripts, and links to benchmarks can be found at the following website:https://npm3d.fr/parisluco3d},
	number = {{arXiv}:2310.16542},
	publisher = {{arXiv}},
	author = {Sanchez, Jules and Soum-Fontez, Louis and Deschaud, Jean-Emmanuel and Goulette, Francois},
	urldate = {2024-11-12},
	date = {2024-06-03},
	eprinttype = {arxiv},
	eprint = {2310.16542},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{sekkat_amodalsynthdrive_2024,
	title = {{AmodalSynthDrive}: A Synthetic Amodal Perception Dataset for Autonomous Driving},
	url = {http://arxiv.org/abs/2309.06547},
	doi = {10.48550/arXiv.2309.06547},
	shorttitle = {{AmodalSynthDrive}},
	abstract = {Unlike humans, who can effortlessly estimate the entirety of objects even when partially occluded, modern computer vision algorithms still find this aspect extremely challenging. Leveraging this amodal perception for autonomous driving remains largely untapped due to the lack of suitable datasets. The curation of these datasets is primarily hindered by significant annotation costs and mitigating annotator subjectivity in accurately labeling occluded regions. To address these limitations, we introduce {AmodalSynthDrive}, a synthetic multi-task multi-modal amodal perception dataset. The dataset provides multi-view camera images, 3D bounding boxes, {LiDAR} data, and odometry for 150 driving sequences with over 1M object annotations in diverse traffic, weather, and lighting conditions. {AmodalSynthDrive} supports multiple amodal scene understanding tasks including the introduced amodal depth estimation for enhanced spatial understanding. We evaluate several baselines for each of these tasks to illustrate the challenges and set up public benchmarking servers. The dataset is available at http://amodalsynthdrive.cs.uni-freiburg.de.},
	number = {{arXiv}:2309.06547},
	publisher = {{arXiv}},
	author = {Sekkat, Ahmed Rida and Mohan, Rohit and Sawade, Oliver and Matthes, Elmar and Valada, Abhinav},
	urldate = {2024-11-12},
	date = {2024-03-11},
	eprinttype = {arxiv},
	eprint = {2309.06547},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{shah_airsim_2017,
	title = {{AirSim}: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles},
	url = {http://arxiv.org/abs/1705.05065},
	doi = {10.48550/arXiv.1705.05065},
	shorttitle = {{AirSim}},
	abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop ({HITL}) simulations with support for popular protocols (e.g. {MavLink}). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
	number = {{arXiv}:1705.05065},
	publisher = {{arXiv}},
	author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
	urldate = {2024-11-12},
	date = {2017-07-18},
	eprinttype = {arxiv},
	eprint = {1705.05065},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Systems and Control},
}

@inproceedings{shi_pointrcnn_2019,
	location = {Long Beach, {CA}, {USA}},
	title = {{PointRCNN}: 3D Object Proposal Generation and Detection From Point Cloud},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954080/},
	doi = {10.1109/CVPR.2019.00086},
	shorttitle = {{PointRCNN}},
	abstract = {In this paper, we propose {PointRCNN} for 3D object detection from raw point cloud. The whole framework is composed of two stages: stage-1 for the bottom-up 3D proposal generation and stage-2 for reﬁning proposals in the canonical coordinates to obtain the ﬁnal detection results. Instead of generating proposals from {RGB} image or projecting point cloud to bird’s view or voxels as previous methods do, our stage-1 sub-network directly generates a small number of high-quality 3D proposals from point cloud in a bottom-up manner via segmenting the point cloud of the whole scene into foreground points and background. The stage-2 sub-network transforms the pooled points of each proposal to canonical coordinates to learn better local spatial features, which is combined with global semantic features of each point learned in stage-1 for accurate box reﬁnement and conﬁdence prediction. Extensive experiments on the 3D detection benchmark of {KITTI} dataset show that our proposed architecture outperforms state-of-the-art methods with remarkable margins by using only point cloud as input. The code is available at https://github.com/sshaoshuai/{PointRCNN}.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {770--779},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2019-06},
	langid = {english},
}

@inproceedings{shi_pv-rcnn_2020,
	location = {Seattle, {WA}, {USA}},
	title = {{PV}-{RCNN}: Point-Voxel Feature Set Abstraction for 3D Object Detection},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157234/},
	doi = {10.1109/CVPR42600.2020.01054},
	shorttitle = {{PV}-{RCNN}},
	abstract = {We present a novel and high-performance 3D object detection framework, named {PointVoxel}-{RCNN} ({PV}-{RCNN}), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network ({CNN}) and {PointNet}-based set abstraction to learn more discriminative point cloud features. It takes advantages of efﬁcient learning and high-quality proposals of the 3D voxel {CNN} and the ﬂexible receptive ﬁelds of the {PointNet}-based networks. Speciﬁcally, the proposed framework summarizes the 3D scene with a 3D voxel {CNN} into a small set of keypoints via a novel voxel set abstraction module to save follow-up computations and also to encode representative scene features. Given the highquality 3D proposals generated by the voxel {CNN}, the {RoIgrid} pooling is proposed to abstract proposal-speciﬁc features from the keypoints to the {RoI}-grid points via keypoint set abstraction. Compared with conventional pooling operations, the {RoI}-grid feature points encode much richer context information for accurately estimating object conﬁdences and locations. Extensive experiments on both the {KITTI} dataset and the Waymo Open dataset show that our proposed {PV}-{RCNN} surpasses state-of-the-art 3D detection methods with remarkable margins.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {10526--10535},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2020-06},
	langid = {english},
}

@misc{shi_pv-rcnn_2022,
	title = {{PV}-{RCNN}++: Point-Voxel Feature Set Abstraction With Local Vector Representation for 3D Object Detection},
	url = {http://arxiv.org/abs/2102.00463},
	doi = {10.48550/arXiv.2102.00463},
	shorttitle = {{PV}-{RCNN}++},
	abstract = {3D object detection is receiving increasing attention from both industry and academia thanks to its wide applications in various fields. In this paper, we propose Point-Voxel Region-based Convolution Neural Networks ({PV}-{RCNNs}) for 3D object detection on point clouds. First, we propose a novel 3D detector, {PV}-{RCNN}, which boosts the 3D detection performance by deeply integrating the feature learning of both point-based set abstraction and voxel-based sparse convolution through two novel steps, i.e., the voxel-to-keypoint scene encoding and the keypoint-to-grid {RoI} feature abstraction. Second, we propose an advanced framework, {PV}-{RCNN}++, for more efficient and accurate 3D object detection. It consists of two major improvements: sectorized proposal-centric sampling for efficiently producing more representative keypoints, and {VectorPool} aggregation for better aggregating local point features with much less resource consumption. With these two strategies, our {PV}-{RCNN}++ is about \$3{\textbackslash}times\$ faster than {PV}-{RCNN}, while also achieving better performance. The experiments demonstrate that our proposed {PV}-{RCNN}++ framework achieves state-of-the-art 3D detection performance on the large-scale and highly-competitive Waymo Open Dataset with 10 {FPS} inference speed on the detection range of 150m * 150m.},
	number = {{arXiv}:2102.00463},
	publisher = {{arXiv}},
	author = {Shi, Shaoshuai and Jiang, Li and Deng, Jiajun and Wang, Zhe and Guo, Chaoxu and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
	urldate = {2024-11-12},
	date = {2022-11-07},
	eprinttype = {arxiv},
	eprint = {2102.00463},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{sun_scalability_2020,
	title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html},
	shorttitle = {Scalability in Perception for Autonomous Driving},
	eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {2446--2454},
	author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
	urldate = {2024-11-11},
	date = {2020},
}

@misc{team_openpcdet_2020,
	title = {{OpenPCDet}: An Open-source Toolbox for 3D Object Detection from Point Clouds},
	url = {https://github.com/open-mmlab/OpenPCDet},
	author = {Team, {OpenPCDet} Development},
	date = {2020},
}

@article{tian_acf-net_2024,
	title = {{ACF}-Net: Asymmetric Cascade Fusion for 3D Detection With {LiDAR} Point Clouds and Images},
	volume = {9},
	issn = {2379-8904},
	url = {https://ieeexplore.ieee.org/document/10363115/?arnumber=10363115},
	doi = {10.1109/TIV.2023.3341223},
	shorttitle = {{ACF}-Net},
	abstract = {The recognition and utilization of complementary information arising from modality-intrinsic properties play crucial roles in multimodal 3D detection. However, most of the current approaches for fusion-based 3D detection follow symmetrical fusion paradigms and adopt early fusion, middle fusion as well as late fusion styles, which ignore the unequal status of data with different modalities. In this paper, according to the timing of fusion, we adopt an asymmetric cascade fusion network to exploit both the structural information from point clouds and the complementary semantic information from images. A multi-stage cascade design of 3D object detection is proposed to iteratively refine predictions and several late image features (comprised of detection clues, segmentation clues, and deep features from encoders) are incorporated into different stages of the {LiDAR} branch to maintain the integrity of image features and enable deep multimodal interactions. Besides, to mitigate the effects of the down-sampling of voxelized features and possible mismatching of multimodal data, we propose proxy-based cross-modality sampling to utilize the high-density point clouds coordinates and develop an image degeneration process to simulate the noise in cross-modality matching for robust training. Extensive experiments are conducted on {KITTI} and Waymo Open Dataset, which validate the effectiveness of the proposed method.},
	pages = {3360--3371},
	number = {2},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Tian, Yonglin and Zhang, Xianjing and Wang, Xiao and Xu, Jintao and Wang, Jiangong and Ai, Rui and Gu, Weihao and Ding, Weiping},
	urldate = {2024-11-12},
	date = {2024-02},
	note = {Conference Name: {IEEE} Transactions on Intelligent Vehicles},
	keywords = {3D detection, Feature extraction, Fuses, Laser radar, Object detection, Point cloud compression, Three-dimensional displays, Timing, asymmetric fusion, autonomous driving, cascade fusion, multimodal fusion},
}

@misc{tobin_domain_2017,
	title = {Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World},
	url = {http://arxiv.org/abs/1703.06907},
	abstract = {Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We ﬁnd that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the ﬁrst successful transfer of a deep neural network trained only on simulated {RGB} images (without pre-training on real images) to the real world for the purpose of robotic control.},
	number = {{arXiv}:1703.06907},
	publisher = {{arXiv}},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	urldate = {2024-11-11},
	date = {2017-03-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.06907 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{triess_realism_2022,
	title = {A Realism Metric for Generated {LiDAR} Point Clouds},
	url = {http://arxiv.org/abs/2208.14958},
	doi = {10.48550/arXiv.2208.14958},
	abstract = {A considerable amount of research is concerned with the generation of realistic sensor data. {LiDAR} point clouds are generated by complex simulations or learned generative models. The generated data is usually exploited to enable or improve downstream perception algorithms. Two major questions arise from these procedures: First, how to evaluate the realism of the generated data? Second, does more realistic data also lead to better perception performance? This paper addresses both questions and presents a novel metric to quantify the realism of {LiDAR} point clouds. Relevant features are learned from real-world and synthetic point clouds by training on a proxy classification task. In a series of experiments, we demonstrate the application of our metric to determine the realism of generated {LiDAR} data and compare the realism estimation of our metric to the performance of a segmentation model. We confirm that our metric provides an indication for the downstream segmentation performance.},
	number = {{arXiv}:2208.14958},
	publisher = {{arXiv}},
	author = {Triess, Larissa T. and Rist, Christoph B. and Peter, David and Zöllner, J. Marius},
	urldate = {2024-11-12},
	date = {2022-08-31},
	eprinttype = {arxiv},
	eprint = {2208.14958},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{velodyne_velodyne_2009,
	title = {Velodyne {HDL}-64E Data Sheet},
	url = {https://hypertech.co.il/wp-content/uploads/2015/12/HDL-64E-Data-Sheet.pdf},
	author = {Velodyne},
	urldate = {2025-01-10},
	date = {2009},
}

@misc{wang_train_2020,
	title = {Train in Germany, Test in The {USA}: Making 3D Object Detectors Generalize},
	url = {http://arxiv.org/abs/2005.08139},
	doi = {10.48550/arXiv.2005.08139},
	shorttitle = {Train in Germany, Test in The {USA}},
	abstract = {In the domain of autonomous driving, deep learning has substantially improved the 3D object detection accuracy for {LiDAR} and stereo camera data alike. While deep networks are great at generalization, they are also notorious to over-fit to all kinds of spurious artifacts, such as brightness, car sizes and models, that may appear consistently throughout the data. In fact, most datasets for autonomous driving are collected within a narrow subset of cities within one country, typically under similar weather conditions. In this paper we consider the task of adapting 3D object detectors from one dataset to another. We observe that naively, this appears to be a very challenging task, resulting in drastic drops in accuracy levels. We provide extensive experiments to investigate the true adaptation challenges and arrive at a surprising conclusion: the primary adaptation hurdle to overcome are differences in car sizes across geographic areas. A simple correction based on the average car size yields a strong correction of the adaptation gap. Our proposed method is simple and easily incorporated into most 3D object detection frameworks. It provides a first baseline for 3D object detection adaptation across countries, and gives hope that the underlying problem may be more within grasp than one may have hoped to believe. Our code is available at https://github.com/cxy1997/3D\_adapt\_auto\_driving.},
	number = {{arXiv}:2005.08139},
	publisher = {{arXiv}},
	author = {Wang, Yan and Chen, Xiangyu and You, Yurong and Erran, Li and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q. and Chao, Wei-Lun},
	urldate = {2024-11-12},
	date = {2020-05-17},
	eprinttype = {arxiv},
	eprint = {2005.08139},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{wilson_argoverse_2023,
	title = {Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting},
	url = {http://arxiv.org/abs/2301.00493},
	doi = {10.48550/arXiv.2301.00493},
	shorttitle = {Argoverse 2},
	abstract = {We introduce Argoverse 2 ({AV}2) - a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-{DOF} map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions between the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for "scored actors" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own {HD} Map with 3D lane and crosswalk geometry - sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the {CC} {BY}-{NC}-{SA} 4.0 license.},
	number = {{arXiv}:2301.00493},
	publisher = {{arXiv}},
	author = {Wilson, Benjamin and Qi, William and Agarwal, Tanmay and Lambert, John and Singh, Jagjeet and Khandelwal, Siddhesh and Pan, Bowen and Kumar, Ratnesh and Hartnett, Andrew and Pontes, Jhony Kaesemodel and Ramanan, Deva and Carr, Peter and Hays, James},
	urldate = {2024-11-11},
	date = {2023-01-02},
	eprinttype = {arxiv},
	eprint = {2301.00493},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{wu_transformation-equivariant_2022,
	title = {Transformation-Equivariant 3D Object Detection for Autonomous Driving},
	url = {http://arxiv.org/abs/2211.11962},
	doi = {10.48550/arXiv.2211.11962},
	abstract = {3D object detection received increasing attention in autonomous driving recently. Objects in 3D scenes are distributed with diverse orientations. Ordinary detectors do not explicitly model the variations of rotation and reflection transformations. Consequently, large networks and extensive data augmentation are required for robust detection. Recent equivariant networks explicitly model the transformation variations by applying shared networks on multiple transformed point clouds, showing great potential in object geometry modeling. However, it is difficult to apply such networks to 3D object detection in autonomous driving due to its large computation cost and slow reasoning speed. In this work, we present {TED}, an efficient Transformation-Equivariant 3D Detector to overcome the computation cost and speed issues. {TED} first applies a sparse convolution backbone to extract multi-channel transformation-equivariant voxel features; and then aligns and aggregates these equivariant features into lightweight and compact representations for high-performance 3D object detection. On the highly competitive {KITTI} 3D car detection leaderboard, {TED} ranked 1st among all submissions with competitive efficiency.},
	number = {{arXiv}:2211.11962},
	publisher = {{arXiv}},
	author = {Wu, Hai and Wen, Chenglu and Li, Wei and Li, Xin and Yang, Ruigang and Wang, Cheng},
	urldate = {2024-11-12},
	date = {2022-12-01},
	eprinttype = {arxiv},
	eprint = {2211.11962},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{yan_second_2018,
	title = {{SECOND}: Sparsely Embedded Convolutional Detection},
	volume = {18},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/18/10/3337},
	doi = {10.3390/s18103337},
	shorttitle = {{SECOND}},
	abstract = {{LiDAR}-based or {RGB}-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud {LiDAR} data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the {KITTI} 3D object detection benchmarks while maintaining a fast inference speed.},
	pages = {3337},
	number = {10},
	journaltitle = {Sensors},
	author = {Yan, Yan and Mao, Yuxing and Li, Bo},
	urldate = {2024-11-12},
	date = {2018-10},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D object detection, {LIDAR}, autonomous driving, convolutional neural networks},
}

@misc{yang_st3d_2021,
	title = {{ST}3D++: Denoised Self-training for Unsupervised Domain Adaptation on 3D Object Detection},
	url = {http://arxiv.org/abs/2108.06682},
	doi = {10.48550/arXiv.2108.06682},
	shorttitle = {{ST}3D++},
	abstract = {In this paper, we present a self-training method, named {ST}3D++, with a holistic pseudo label denoising pipeline for unsupervised domain adaptation on 3D object detection. {ST}3D++ aims at reducing noise in pseudo label generation as well as alleviating the negative impacts of noisy pseudo labels on model training. First, {ST}3D++ pre-trains the 3D object detector on the labeled source domain with random object scaling ({ROS}) which is designed to reduce target domain pseudo label noise arising from object scale bias of the source domain. Then, the detector is progressively improved through alternating between generating pseudo labels and training the object detector with pseudo-labeled target domain data. Here, we equip the pseudo label generation process with a hybrid quality-aware triplet memory to improve the quality and stability of generated pseudo labels. Meanwhile, in the model training stage, we propose a source data assisted training strategy and a curriculum data augmentation policy to effectively rectify noisy gradient directions and avoid model over-fitting to noisy pseudo labeled data. These specific designs enable the detector to be trained on meticulously refined pseudo labeled target data with denoised training signals, and thus effectively facilitate adapting an object detector to a target domain without requiring annotations. Finally, our method is assessed on four 3D benchmark datasets (i.e., Waymo, {KITTI}, Lyft, and {nuScenes}) for three common categories (i.e., car, pedestrian and bicycle). {ST}3D++ achieves state-of-the-art performance on all evaluated settings, outperforming the corresponding baseline by a large margin (e.g., 9.6\% \${\textbackslash}sim\$ 38.16\% on Waymo \${\textbackslash}rightarrow\$ {KITTI} in terms of {AP}\$\_\{{\textbackslash}text\{3D\}\}\$), and even surpasses the fully supervised oracle results on the {KITTI} 3D object detection benchmark with target prior. Code will be available.},
	number = {{arXiv}:2108.06682},
	publisher = {{arXiv}},
	author = {Yang, Jihan and Shi, Shaoshuai and Wang, Zhe and Li, Hongsheng and Qi, Xiaojuan},
	urldate = {2024-11-14},
	date = {2021-08-15},
	eprinttype = {arxiv},
	eprint = {2108.06682},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{zhang_uni3d_2023,
	title = {Uni3D: A Unified Baseline for Multi-dataset 3D Object Detection},
	url = {http://arxiv.org/abs/2303.06880},
	doi = {10.48550/arXiv.2303.06880},
	shorttitle = {Uni3D},
	abstract = {Current 3D object detection models follow a single dataset-specific training and testing paradigm, which often faces a serious detection accuracy drop when they are directly deployed in another dataset. In this paper, we study the task of training a unified 3D detector from multiple datasets. We observe that this appears to be a challenging task, which is mainly due to that these datasets present substantial data-level differences and taxonomy-level variations caused by different {LiDAR} types and data acquisition standards. Inspired by such observation, we present a Uni3D which leverages a simple data-level correction operation and a designed semantic-level coupling-and-recoupling module to alleviate the unavoidable data-level and taxonomy-level differences, respectively. Our method is simple and easily combined with many 3D object detection baselines such as {PV}-{RCNN} and Voxel-{RCNN}, enabling them to effectively learn from multiple off-the-shelf 3D datasets to obtain more discriminative and generalizable representations. Experiments are conducted on many dataset consolidation settings including Waymo-{nuScenes}, {nuScenes}-{KITTI}, Waymo-{KITTI}, and Waymo-{nuScenes}-{KITTI} consolidations. Their results demonstrate that Uni3D exceeds a series of individual detectors trained on a single dataset, with a 1.04x parameter increase over a selected baseline detector. We expect this work will inspire the research of 3D generalization since it will push the limits of perceptual performance.},
	number = {{arXiv}:2303.06880},
	publisher = {{arXiv}},
	author = {Zhang, Bo and Yuan, Jiakang and Shi, Botian and Chen, Tao and Li, Yikang and Qiao, Yu},
	urldate = {2024-11-21},
	date = {2023-04-28},
	eprinttype = {arxiv},
	eprint = {2303.06880},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

