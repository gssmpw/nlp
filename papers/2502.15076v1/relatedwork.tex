\section{Related Work}
To build a competitive system for this, it is necessary to look at both real world datasets and existing simulation approaches for object detection and domain adaption.
\subsection{Datasets}
The traditional way to perform object detection tasks for autonomous driving is using real sensor data from test drives. The respective algorithms are based on visual sensors like \li, radar, or camera images. Neural networks have emerged as the standard solution in recent research but require ground truth labels for training. 
There are semi-automatic approaches~\cite{autonomousvision_kitti-360_2024}, but considerable human effort is still necessary for large-scale datasets.  

%\subsubsection{KITTI}
For our approach, the primary focus was 3D object detection, which commonly relies on \li point clouds. 
The KITTI dataset~\cite{geiger_are_2012} offers an established benchmark for comparing the performance of object detection systems.
As such, it is the perfect candidate to evaluate how well objects can be detected when training on synthetic data.
The test vehicle for the KITTI dataset is equipped with a stereo camera setup and a Velodyne \li sensor~\cite{velodyne_velodyne_2009}. %(now bought by ouster no official spec sheet available but random pdfs exist: %\url{https://hypertech.co.il/wp-content/uploads/2015/12/HDL-64E-Data-Sheet.pdf}, , \url{https://www.researchgate.net/profile/Joerg_Fricke/post/How_the_LiDARs_photodetector_distinguishes_lasers_returns/attachment/5fa947b8543da600017dcf9b/AS\%3A955957442002980\%401604929423693/download/HDL-64E_S3_UsersManual.pdf}) \li sensor. It consists of 64 laser diodes, each collecting about 2000 points in a single rotation with a frame rate of 10 Hz.

%\subsubsection{Waymo}
Another popular dataset is Waymo~\cite{sun_scalability_2020}, which especially stands out for the number and diversity of captured scenarios. It uses a rather complex sensor setup with five cameras and \li sensors. 
%Therefore, it %is not part of our core analysis of the domain shifts between real and synthetic data but 
%we use it to get insights about how object detectors trained on real data generalize to differences in the point clouds and object labels.
There are further datasets with 3D object labels like Argoverse2~\cite{wilson_argoverse_2023}, nuScenes~\cite{caesar_nuscenes_2020} or ~\cite{mao_one_2021}, but in general it is not trivial to compare results and have compatible formats between these. 
Our work mainly takes KITTI as an example of how well a synthetic dataset can mimic a real one.%could have served a similar purpose, but the metrics for Waymo are more comparable to KITTI and the main \li sensor has similar specifications to the Velodyne HDL64 in the KITTI dataset.


\subsection{Object Detection}
Given these labeled datasets, the task of object detection also includes predicting the dimensions and rotation of the vehicle, even when there are only partial observations of the respective objects. 
\subsubsection{Approaches}
To isolate the impacts of the point clouds, we limited our experiments to architectures that use only \li point clouds as input.
Aside from the distance, these sensors also measure \emph{intensity} values based on the strength of the reflected signal.
We call the phenomenon that some rays do not return a signal as \emph{raydrop}.
Due to material properties some high frequency details can become visible, in particular retro-reflective surfaces as can be found on number plates.
This offers additional features for accurately detecting cars in point clouds.

Voxel-based methods such as SECOND~\cite{yan_second_2018} or Voxel-R-CNN~\cite{deng_voxel_2021} have proven to robustly extract features from point clouds for object detection and thus may lead to good generalization between point clouds generated by different sensors.
%An alternative are point-based methods, e.g. PointRCNN~\cite{shi_pointrcnn_2019} also show strong performance, especially when also making use of the 3D voxel CNN~\cite{shi_pv-rcnn_2020,shi_pv-rcnn_2022}.
%Many of these methods are especially relevant for comparisons with previous work that employs training on synthetic data as it is difficult to reproduce the specific data simulations.
Furthermore, we are mostly interested in the relative change of detection quality when incorporating synthetic data and less in the behavior of different detection architectures, so we base all experiments on Voxel-R-CNN.
We use the implementation provided by OpenPCDet\cite{team_openpcdet_2020}, a framework that supports different object detection models and datasets. 
This complements the modular nature of our pipeline and allows us to use additional models and datasets for future experiments.
%State of the art:  \cite{tian_acf-net_2024,wu_transformation-equivariant_2022}

\subsubsection{Domain Adaptation}
Domain adaption regarding object detection in driving scenarios is covered quite well. On the one hand, there is work that directly examines the domain shift between synthetic and real datasets~\cite{triess_realism_2022,huch_quantifying_2023}, where we rather focus on the aspect of data collection, i.e., how typical road scenes are structured or the composition of datasets.
This is more in line with methods that show good performance for optimizing the domain gap and robustness between different real datasets~\cite{wang_train_2020,yang_st3d_2021,zhang_uni3d_2023}.
However, our goal is to be able to directly generate data that can be used for generalization purposes, independent of the network architecture, so that we can gather more insight on what actually impedes the object detections.
One such issue has already been identified by the works mentioned above, namely the difference in vehicle dimensions between different datasets.
We will revisit this topic in Section~\ref{bb}.
Finally, the core idea behind our randomization concepts are closely related to the findings of Tobin et al.: ”With enough variability in
the simulator, the real world may appear to the model as just
another variation.”~\cite{tobin_domain_2017}. 

\subsection{Simulation}
For synthetic data generation, the open source driving simulator CARLA~\cite{dosovitskiy_carla_2017} has seen widespread adoption for training neural networks on car perception tasks. CARLA is built on the Unreal Engine~\cite{epic_games_unreal_2019}. Features include its scalable architecture with one server and multiple clients controlling different parts of the simulation, a flexible Python API for writing custom clients, and a basic sensor suite for autonomous driving applications. Other tools like NVIDIA DriveSim~\cite{nvidia_drive_2024} are not easily accessible or, in the case of AirSim~\cite{shah_airsim_2017}, do not offer as much support for driving simulation integrations.

There are several approaches regarding object detection in synthetic environments~\cite{bavirisetti_simulated_2023,deschaud_paris-carla-3d_2021,patel_simulation-based_2024,sanchez_parisluco3d_2024,sekkat_amodalsynthdrive_2024}, but direct use is often difficult as 3D data often is not available and there are no benchmarks on real data.
%\cite{antunes_exploring_2024} predicted depth based object detection carla/real


%Predecessor of Paris-CARLA based on CARLA 0.9.10: jedeschaud kitticarlasimulator: KITTI-CARLA: Python scripts to generate the KITTI-CARLA dataset
While we make use of some concepts of these approaches, we base our code on a simple standalone implementation for generating data in the KITTI format from CARLA~\cite{nozarian_fnozariancarla-kitti_2024}. %\cite{deschaud_kitti-carla_2021}
Overall, the specific combination of 3D bounding boxes and evaluation on real datasets was often not the main focus, which our work is supposed to address.

\subsubsection{Synthetic 3D Object Detection on Real Data}
There are also some approaches that have already gathered insights for using synthetically trained networks on real data~\cite{iglesias_analysis_2024}, e.g., that fine-tuning results in significantly better performance on the target domain compared to training on both datasets simultaneously~\cite{dworak_performance_2019}.
Other works like CADET~\cite{brekke_multimodal_2019} (based on AVOD~\cite{ku_joint_2018}) use the same KITTI benchmarks we target, so we can compare our results.
The same is true for IntensitySim~\cite{marcus_gan-based_2023}, which uses data from VKITTI2~\cite{gaidon_virtual_2016,cabon_virtual_2020}, a synthetic remodeling of KITTI instead of CARLA.
It consists of selected sequences, summing up to about 2k frames.
In addition to sampling point clouds from the depth maps, it also utilizes a GAN trained on real data to simulate raydrop.
Our approach will employ further methods to increase the realism of the synthetic data.