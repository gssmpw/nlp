% This must be in the first five lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.
\PassOptionsToPackage{table}{xcolor}
\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% CUSTOM PACKAGES
\usepackage{tcolorbox}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{amsmath}



\definecolor{dodgerblue}{rgb}{0.12, 0.56, 1.0}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}

\definecolor{economics}{rgb}{0.282, 0.471, 0.816}
\definecolor{eecs}{rgb}{0.933, 0.522, 0.29}
\definecolor{law}{rgb}{0.835, 0.733, 0.404}
\definecolor{math}{rgb}{0.51, 0.776, 0.886}
\definecolor{medicine}{rgb}{0.584, 0.424, 0.706}
\definecolor{natural}{rgb}{0.549, 0.38, 0.235}
\definecolor{politics}{rgb}{0.863, 0.494, 0.753}
\definecolor{psychology}{rgb}{0.475, 0.475, 0.475}


\newif\ifrev
  \revtrue
%\revfalse
\ifrev
    \newcommand{\pot}[2][blue]{{\color{#1}[POT] \{#2\}}}
    \newcommand{\sev}[2][orange]{{\color{#1}[SEV] \{#2}\}}

\else
    \newcommand{\pot}{}
    \newcommand{\sev}{}
\fi

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Designing Role Vectors to Improve LLM Inference Behaviour}

\author{
 \textbf{Daniele Potertì\textsuperscript{2}},
 \textbf{Andrea Seveso\textsuperscript{1,3}},
 \textbf{Fabio Mercorio\textsuperscript{1,3}}
\\
\\
  \textsuperscript{1}Dept of Statistics and Quantitative Methods, University of Milano-Bicocca, Italy,
 \\
 \textsuperscript{2}Dept of Economics, Management and Statistics, University of Milano-Bicocca, Italy,
 \\
 \textsuperscript{3}CRISP Research Centre \url{crispresearch.eu},  University of Milano-Bicocca, Italy
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:andrea.seveso@unimib.it}{andrea.seveso@unimib.it}
%  }
}

\begin{document}
\maketitle
\begin{abstract}
    The influence of personas on Large Language Models (LLMs) has been widely studied, yet their direct impact on performance remains uncertain. This work explores a novel approach to guiding LLM behaviour through role vectors, an alternative to persona-based prompting. We construct 29 role vectors derived from model activations and evaluate their impact on benchmark performance across multiple domains. Our analysis investigates whether these vectors can effectively steer models toward domain-specific expertise. We measure two key interventions: (i) activation addition, which reinforces role-specific directions, and (ii) directional ablation, which removes them. Results on well-established benchmarks indicate that role vectors do, in fact, influence model behaviour, improving task performance in relevant domains while marginally affecting unrelated tasks. This, in turn, suggests that manipulating internal model representations has a greater impact on outcomes than persona-based prompting.
\end{abstract}

%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section{Introduction}
\label{sec:intro}

The development of persona or role-based chatbots has gained significant attention in the AI and NLP community due to their potential impact on business and societal applications~\cite{pataranutaporn2021ai}. The extent to which different personas influence Large Language Models' (LLMs) performance on objective tasks remains unclear. Recent attempts investigate whether incorporating personas into system prompts enhances model performance on objective tasks and explores potential factors influencing these effects. \cite{zheng2024helpful} conducted a large-scale analysis of the effect of personas in LLM prompting, examining the impact of domain alignment between personas and task-related questions, finding that persona-based prompting either has no effect or a slightly negative impact on model performance compared to a baseline setting.

We aim to investigate whether modifying the model’s internal mechanisms~\cite{li2024inference}, rather than a prompt-based approach, can lead to improved results. This forms the core objective of our current work, guided by the following research questions:
\textbf{RQ1}: Can we identify specific latent role directions within the activation space, derived from the model's internal mechanisms, that, when leveraged, lead to improved performance on objective tasks?
\textbf{RQ2}: Do the directions that enhance performance effectively impersonate the role of interest?
\textbf{RQ3}: If we eliminate these directions in the models, do their performances suffer as a consequence?

\begin{figure}[t]
    \includegraphics[width=1\linewidth]{img/cherrypick_example.png}
  \caption{Illustrative example demonstrating how role vectors (e.g., chemist) can influence model outputs.}
  \label{fig:experiments}
\end{figure}

%~~~~~~~~~~~~~~~~~~~~%
\subsection{Contribution}
This work introduces a novel approach to guiding the behaviour of LLMs through role vectors, a structured method for embedding personas directly into model activations. Fig.~\ref{fig:experiments} is an illustrative example showing how role vectors (in this example, a chemist-related vector) may impact LLM performance. Our key contributions are: 

\begin{enumerate}
    \item We develop 29 distinct role vectors for selected LLMs, each capturing domain-specific knowledge and behavioural tendencies associated with each specialisation.
    \item We investigate whether these vectors influence model behaviour on downstream benchmarks to determine whether explicit role-based directions in the activation space enhance model performance in domain-specific tasks. 
    \item Unlike traditional role-based prompting techniques, which show a limited or negative impact on performance, we show that role vector activation leads to measurable changes in model behaviour.
\end{enumerate}

%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section{Preliminaries and State of the Art}
\label{sec:sota}

%~~~~~~~~~~~~~~~~~~~~%
\paragraph{Personas and Roles in LLMs.}
Decoder-only Transformers~\cite{liu2018generating}, commonly referred to as Language Models (LMs) and in their large form as LLMs, have gained increasing relevance over recent years. Prompting acts as a natural language interface facilitating human-AI interactions~\cite{liu2023pre}. The effectiveness of LLMs is often dependent on prompt formulation~\cite{lu2021fantastically}; for instance, including the phrase "Let's think step by step" can enhance model performance across a variety of tasks and queries~\cite{kojima2022large}. Prior research explored the impact of in-context impersonation tasks~\cite{salewski2023context}, demonstrating that incorporating socio-demographic details can be advantageous for subjective NLP tasks in zero-shot scenarios. Other studies highlight the potential biases and constraints associated with persona-based and socio-demographic-driven prompting~\cite{sun2023aligning, hu2024quantifying, beck2024sensitivity}. \cite{zheng2024helpful} shows that adding personas does not consistently enhance performance on objective tasks and, in some cases, may even degrade it.

In this work, we explore whether alternative approaches associated with Representation Engineering and Mechanistic Interpretability can successfully inject roles into LLMs and yield distinct performance outcomes compared to conventional persona-based or socio-demographic prompting strategies.

%~~~~~~~~~~~~~~~~~~~~%
\paragraph{Mechanistic Interpretability and Directions.}
Pioneering research by Anthropic and other scholars~\cite{elhage2022toy, bolukbasi2016man, hernandez2021low} has demonstrated that neural networks encode input attributes as specific directions within the activation space. It is well established that introducing feature vectors into the residual stream of LMs can influence network behaviour~\cite{li2024inference, turner2023activation, arditi2024refusal}, though the precise mechanisms and optimal intervention points remain an area of active investigation~\cite{jorgensen2023improving, von2024language}.

The study by~\cite{scalena2024multi} demonstrates that steering can effectively induce behavioural modifications in LMs, such as facilitating language switching while providing insights into how specific properties influence model behaviour during text generation. ~\cite{zhao2024steering} introduce SPARE, a training-free representation engineering technique that leverages pre-trained Sparse Autoencoders (SAEs)~\cite{cunningham2023sparse} to enable controlled selection of the model’s knowledge during inference. Similarly, \cite{li2024inference} proposes ITI, a method to enhance the truthfulness of LM outputs by employing supervised learning to identify latent vectors associated with factual responses and adjusting model activations during inference. This approach demonstrates improved performance on the TruthfulQA~\cite{lin2021truthfulqa} and MMLU~\cite{hendrycksmeasuring} benchmarks relative to the baseline.

The results obtained from these works motivate us to explore this class of methodologies to investigate alternative methods for role injection beyond prompting.
%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section{Generating and Evaluating Role Vectors}
\label{sec:methods}

The methodology is composed of three main components: (i) personas selection and prompt dataset generation, (ii) selection of relevant directions and (iii) evaluation methods.

%~~~~~~~~~~~~~~~~~~~~%
\subsection{Personas Selection and Dataset Generation}

% Cominciamo dai ruoli dell'altro paper \cite{zheng2024helpful}. 
% Abbiamo 33? ruoli diverse che soon quelli di MMLU. Anche se in realtà molti di questi non hanno una corrispondenza diretta su alcune categories di mmlu, es abbiamo il dentista che non è perfettamente allineato con la categoria di medicina, tuttavia ci aspettiamo che in ogni caso abbia una conoscenza di quell'ambito superior alla media e superior agli altri ruoli.

To systematically assess the models' knowledge and reasoning capabilities across various domains, our study adopts a role-based evaluation framework inspired by~\cite{zheng2024helpful}, inheriting 29 distinct roles $R = \{r_1, r_2, \dots, r_{29}\}$,
each \(r \in R\) associated with a unique professional or academic specialisation (see Tab.~\ref{tab:fields}). Roles not corresponding to an occupation or not associated with any PersonaHub personas were excluded.

The prompt roles dataset used to identify specific role directions is extracted using the corresponding personas for each role from PersonaHub~\cite{ge2024scaling}. These personas are highly specialised and situated in realistic settings and represent various contextualised scenarios, such as \textit{"A pharmaceutical chemist who analyses the chemical properties of medical devices"}.  First, we perform strict string matching to identify personas that explicitly contain the role name. That is, for each role $r$, we obtain
$P(r) = \{ p \in \text{PersonaHub} \mid \text{string-match}(p, r) \}$
where \(\text{string-match}(p, r)\) indicates that the persona \(p\) explicitly contains the role name \(r\). Then, a sampling process is applied to select relevant personas randomly. Each role can have one or multiple personas, ranging from a minimum of 1 to a maximum of 6948 (881 on average).

The selected personas are then used to generate a synthetic dataset for each role, following the methodology employed by Alpaca~\cite{taori2023alpaca} to create its dataset.

We define a set of tasks
$T = \{\text{write}, \text{explain}, \text{design}, \text{what is}, \text{how to}, \dots \}$,  analogous to those used in Alpaca~\cite{taori2023alpaca}. We generate a set of prompts for each role \(r \in R\). Let
$D_r = \{ x_{r,1}, x_{r,2}, \dots, x_{r,128} \}$ be the collection of 128 prompt examples for role \(r\). For each prompt \(x_{r,i}\), a task \(t\) is randomly sampled from \(T\), and a persona \(p\) is randomly sampled from \(P(r)\).
Then, the prompt is generated by providing the template (see Fig.~\ref{fig:personas_prompt}) to the Claude 3.5 Haiku model~\cite{anthropic2024claude} with the selected task \(t\) and persona \(p\).


\begin{figure}[htb]
    \centering
    \begin{tcolorbox}[
    title=Generating Persona-Specific Tasks, colback=dodgerblue!5!white,colframe=dodgerblue!75!black]
    \scriptsize
    Generate a \textit{\{task\_type\}} prompt that this persona would likely ask:
    Persona: \textit{\{persona\}}.
    
    \smallskip
    
    Rules:
    (i) The prompt should start with "\textit{\{task\_type\}}".
    (ii) Keep it specific and under 15 words.
    (iii) Make it relevant to the persona's background/interests.
    (iv) Your output must start with "User prompt:".
    \smallskip

    Examples based on task types:
    \smallskip
    
    - \textit{Describe}: "Describe the key features of a successful marketing campaign."
    
    - \textit{Explain}: "Explain the process of setting up a home network."
    
    - \textit{Design}: "Design a logo for a sustainable fashion brand."
    
    - \textit{What is}: "What is the difference between UI and UX design?"
    
    - \textit{How to}: "How to optimise a website for mobile devices?"
    \end{tcolorbox}
    \caption{Prompt template for generating persona-specific tasks.}
    \label{fig:personas_prompt}
\end{figure}

The complete roles tasks dataset is given by
$\mathcal{D}_{\text{roles}} = \bigcup_{r \in R} D_r$. 
We incorporate \(\mathcal{D}_{\text{base}}\), an additional set of 128 examples sourced from the original Alpaca dataset, consisting of general instruction-following prompts. This provides a broad reference point, enabling the contrastive computation of direction for each role using the corresponding \(D_r\).

%~~~~~~~~~~~~~~~~~~~~%
\subsection{Selection of Role Directions}

Our evaluation of steering effects utilizes the Massive Multitask Language Understanding (MMLU) benchmark~\cite{hendrycksmeasuring}, adhering to the sampling and splitting methodology described in~\cite{zheng2024helpful} for a total of 2457 questions. We define the set of categories as $C = $ \textit{Natural Science, Economics, EECS (Electrical Engineering and Computer Sciences), Law, Math, Medicine, Politics, Psychology}\}. 
% \[
% \mathcal{C} = \left\{
% \begin{array}{l}
% \text{Natural Science},\, \text{Economics},\, \text{EECS},\, \text{Law},\\[1ex]
% \text{Math},\, \text{Medicine},\, \text{Politics},\, \text{Psychology}
% \end{array}
% \right\}.
% \]

For each \( c \in \mathcal{C} \), let \( D_c \) denote the set of questions corresponding to category \( c \). The overall test dataset is then defined as $\mathcal{D}_{\text{test}} = \bigcup_{c \in \mathcal{C}} D_c$.
Tab.~\ref{tab:mmlu_processing} shows the distribution of questions. While many of these roles correspond directly to established MMLU categories, some exhibit only partial alignment. For example, the role of a dentist does not perfectly fit within the "medicine" category. However, we expect that individuals or models adopting the role of a dentist should demonstrate domain-specific knowledge that exceeds that of the general population or those assuming unrelated roles. 
% This expectation serves as the foundation for assessing domain competence.

% To facilitate our assessment of the impact of steering directions, we reformatted the task to request the letter corresponding to the correct answer (A, B, C, or D).

\begin{table}[ht]
  \small
  \centering
  \begin{tabular}{lc}
    \toprule
    \textbf{Category}       & \textbf{\# Questions} \\
    \midrule
    Natural Science         & 590               \\
    Economics               & 492               \\
    EECS                    & 247               \\
    Law                     & 200               \\
    Math                    & 287               \\
    Medicine                & 241               \\
    Politics                & 200               \\
    Psychology              & 200               \\
    \midrule
    Total              & 2457               \\
    \bottomrule
  \end{tabular}
  \caption{Number of questions per category in $\mathcal{D}_{\text{test}}$., taken from~\cite{zheng2024helpful}}
  \label{tab:mmlu_processing}
\end{table}


% For each role \( r \in R \)  and test, three datasets are loaded: two training datasets—the original Alpaca dataset and a domain-specific dataset we generated—to compute the difference in mean activations for identifying role directions, and a test dataset. The original Alpaca dataset serves as the baseline, while the domain-specific dataset represents the target direction for steering.

To identify the direction in the model's residual stream activations corresponding to each role, we use a technique known as \textit{difference-in-means}~\cite{belrose2023}: we compute the difference between the model's average activations when performing inference on the role-specific dataset \( D_r \in \mathcal{D}_{\text{roles}} \) and generic queries from \( \mathcal{D}_{\text{base}} \).

Following the notation from~\cite{arditi2024refusal}, for each role \( r \in R \), layer \( l \in [L] \), and post-instruction token position \( i \in I \), we compute the mean activation 
\( \mu_{i,r}^{(l)} \) for role-specific prompts in \( D_r \) and \( \nu_i^{(l)} \) for generic prompts in \( \mathcal{D}_{\text{base}} \):

\begin{equation}
    \scriptsize
    \mu_{i,r}^{(l)} = \frac{1}{\left| D_r \right|} 
    \sum_{t \in D_r} x_i^{(l)}(t), \quad
    \nu_i^{(l)} = \frac{1}{\left| \mathcal{D}_{\text{base}} \right|} 
    \sum_{t \in \mathcal{D}_{\text{base}}} x_i^{(l)}(t).
\end{equation}

We then define the role-specific difference-in-means vector:

\begin{equation}
    \small
    d_{i,r}^{(l)} = \mu_{i,r}^{(l)} - \nu_i^{(l)}
\end{equation}

By computing \( d_{i,r}^{(l)} \) for each \( r \in R \), we obtain \( |R| \) (29) distinct directions, each representing the shift in model activations specific to a given role. These vectors are informative in two ways: their \textit{direction} indicates how the mean activations for role-specific and generic prompts diverge; their \textit{magnitude} quantifies the extent of this difference. 

% Calcolo tutte le attivazioni per i due dataset, alpaca originale e quello generato, sono 128 esempi per ciascuno dei due. 
% Guarda solo il primo 80\% dei layer perché lo fa \cite{arditi2024refusal}. Una volta che ho le attivitazioni per uno e l'altro, calcolo la media di ciascuno e poi faccio la differenza.

% Quali sono le attivazioni sulle quali faccio questa operazione?  
% calcolo la diff delle medie sulle attivazioni di tutti i layer e tutte le posizioni finali del residual stream.
% prendendo i token del tokenizzatore del modello usato EOS.
% si generano così le direzioni, e da quelle generate andremo a vedere cosa succede.

We aim to assess model performance across these different directions using the test dataset \(\mathcal{D}_{\text{test}}\). This evaluation allows us to measure how various directions influence the model’s behaviour, particularly in terms of performance across the different splits \( D_c \in \mathcal{D}_{\text{test}}\). 

% We have now this herd of directions what we doing?

Using the identified directions, we apply two types of interventions: \textit{activation addition} and \textit{directional ablation}. These techniques allow us to manipulate the model's activations by reinforcing or suppressing specific directional components in the residual stream. 

\textbf{Activation Addition.} Given a difference-in-means vector \( d_{i,r}^{(l)} \in \mathbb{R}^{d_{\text{model}}} \) extracted from layer \( l \), we can modulate the influence of the corresponding feature through a simple linear transformation. Specifically, we add the direction vector to the activations of a base input, shifting them toward the mean activation observed for role-enhanced inputs:

\begin{equation}
    \small
    x^{(l)'} \leftarrow x^{(l)} + \alpha d_{i,r}^{(l)}.
    \label{eq:actadd}
\end{equation}


here \(\alpha\) is a scalar hyperparameter that scales the difference-in-means vector \(d_{i,r}^{(l)}\), controlling the magnitude of the shift applied to the base activations \(x^{(l)}\) toward the role-enhanced mean.

Notably, this operation is applied exclusively at layer \( l \) and affects all token positions, ensuring a controlled perturbation of the model's internal representations.

\textbf{Directional Ablation.} To investigate the role of a direction \( \hat{r} \in \mathbb{R}^{d_{\text{model}}} \) in the model’s computation, we apply \textit{directional ablation}, which removes its contribution from the model’s activations. This process effectively zeroes out the component of each residual stream activation \( x \) along \( \hat{d_{i,r}^{(l)}} \), preventing the model from utilizing this direction:

\begin{equation}
    \small
    x' \leftarrow x - \hat{d_{i,r}^{(l)}} \hat{d_{i,r}^{(l)}}^{\top} x.
\end{equation}

This operation is performed at every activation \( x^{(l)}_i \), across all layers \( l \) and all token positions \( i \), ensuring that the model no longer represents the targeted direction in its residual stream.

By applying these interventions, we can assess the functional role of specific directions in the model’s representation space. We evaluate the impact when explicitly reinforced through activation addition and suppressed via directional ablation.

%~~~~~~~~~~~~~~~~~~~~%
\subsection{Evaluation Method}

% Investighiamo se le direzioni che abbiamo identificato con il ruolo hanno dei miglioramenti di conoscenza ecc su mmlu e particolarmente sullo split di cui si occupano. 

For each model and every role \(r \in R\), we assess whether incorporating through Activation Addition the computed role-specific difference-in-means vectors \(d_{i,r}^{(l)}\) yields an improvement in performance on the test dataset \(\mathcal{D}_{\text{test}}\), with particular emphasis on the corresponding domain-reference split \(D_c\). Tab.~\ref{tab:fields} presents the role-split relevance data reported in~\cite{zheng2024helpful}.

\begin{table}[ht]
\small
\centering
\begin{tabular}{lp{5cm}}
\toprule
\textbf{Split} & \textbf{Role(s)} \\
\midrule
econ & economic researcher, economist, financial analyst \\
eecs & electronics technician, data scientist, electrical engineer, software engineer, web developer \\
law & bailiff, lawyer \\
math & data analyst, mathematician, statistician \\
medicine & nurse, doctor, physician, dentist, surgeon \\
natural science & geneticist, biologist, physicist, teacher, chemist, ecologist \\
politics & politician, sheriff, enthusiast, partisan \\
psychology & psychologist \\
\bottomrule
\end{tabular}
\caption{Split and associated roles, adapted from~\cite{zheng2024helpful}}
\label{tab:fields}
\end{table}

%How do we asses performances?
For each test dataset \( D_c \in \mathcal{D}_{\text{test}} \), we assess performance using a logit-based framework. Given a query \( x_{r,i} \in D_c \), let \(\mathbf{z} \in \mathbb{R}^{|\mathcal{V}|}\) denote the logits at the final token position, where \(|\mathcal{V}|\) is the vocabulary size. The softmax function calculates the probability of each token \( t \in \mathcal{V} \).
Restricting our attention to the candidate answer tokens \(\mathcal{T}_{\text{ans}} = \{t_A, t_B, t_C, t_D\}\), the predicted token is determined by
\begin{equation}
\small
t^* = \operatorname{arg\,max}_{t \in \mathcal{T}_{\text{ans}}} p(t).
\end{equation}
The prediction $s(x_{r,i})$ is considered correct if \(t^*\) equals the correct answer token.
% and the corresponding performance score is defined as
% \begin{equation}
% \small
% s(x_{r,i}) = 
% \begin{cases}
% 1, & \text{if } t^* = t_{\mathrm{corr}}, \\
% 0, & \text{otherwise}.
% \end{cases}
% \end{equation}
Overall performance is computed as the mean of the individual scores (the percentage of correct answers).

We also investigate the magnitude \(\alpha\) of these directions. One might hypothesise that increasing their magnitude would enhance the effect associated with a given role; however, such amplification may deteriorate text generation performance concurrently~\cite{liu2023context,scalena2024multi}. $\mathcal{A} = \{ \alpha_1 = 1, \alpha_3 = 3 \}$ is the set of activation addition coefficients. We also evaluate the impact of ablating the direction entirely, a trade-off explored in the literature relating to safety mechanism in models~\cite{wei2024assessing, arditi2024refusal}. 

Let $\mathcal{M}$ denote the set of models under evaluation, $R$ the set of roles, $L$ the set of layers, and $I$ the set of token positions.
We formalise our grid evaluation procedure as follows; for each model \( m \in \mathcal{M} \) and for each role \( r \in R \), we define the intervention grid:
\begin{equation}
\small
\mathcal{G} = \{ (l, i, \alpha) \mid l \in L_{80\%},\, i \in I,\, \alpha \in \mathcal{A} \},
\end{equation}

where $L_{80\%} \subset L$ denotes the first 80\% of layers to avoid interference from unembedding directions, ensuring that the selected direction is not overly proximate to the unembedding directions, following the work done by~\cite{arditi2024refusal}. Intuitively, one could increase performance by encouraging the model to generate correct answers by aligning its activations with the unembedding directions corresponding to ‘A’, ‘B’, ‘C’, or ‘D’, which would directly incentivize the model to output the correct tokens. However, we do not consider the latter 20\% of layers since our approach focuses on higher-level features focusing on the role and avoids token-level manipulation focusing on selecting the correct answer.
For each tuple \((l,i,\alpha) \in \mathcal{G}\), we modify the residual stream activation \( x^{(l)} \) via Eq.~\ref{eq:actadd}.
Let \( s_{m,r}^{(l, i, \alpha)} \) denote the performance (e.g., the proportion of correct answers) on the domain-specific test split \( D_c \in \mathcal{D}_{\text{test}} \) after applying the intervention specified by \((l, i, \alpha)\). For each fixed \(\alpha\) (\(\alpha_1 = 1\) and \(\alpha_3 = 3\)), we select the optimal intervention tuple for role \( r \) on model \( m \) by computing
\begin{equation}
\label{eq:max_perf}
\small
(l^*(\alpha), i^*(\alpha)) = \arg\max_{(l,i) \in L_{80\%} \times I} s_{m,r}^{(l,i,\alpha)}.
\end{equation}
Optimality of a direction \(d_{i,r}^{(l)}\) is defined as its ability to improve performance in the reference domain-specific split \(D_c\) when added and either degrade or maintain it when removed, compared to the baseline. Among all directions satisfying this criterion, the one with the highest performance according to Eq.~\ref{eq:max_perf} is considered optimal. 
With \( d^*(\alpha) \) identified, we evaluate its effect on all other domain-specific splits \( D_c \) for \( c \not\sim r \) to measure its specificity. 

Additionally, we wish to quantify the proportion of optimal directions \( d^* \) that align with the role-specific signal identified via the patch-scoping procedure. We adopt the methodology outlined in~\cite{chen2024selfie,ghandeharioun2024patchscope}, i.e. \textit{patch scoping}. We prompt the model with the query “\textit{What does $\blacklozenge$ represent?}”, apply the Activation Addition intervention with a coefficient $\alpha = 3$, and generate an explanation based on the resulting activations, as exemplified in Fig.~\ref{fig:patch_scoping_example}.
This procedure allows us to determine whether the introduced direction corresponds to the specific role of interest.


\begin{figure}[htb]
\centering
% The 'max width=\linewidth' ensures the box won't overflow.
\begin{minipage}{0.95\linewidth}
\begin{tcolorbox}[
    title=Patch Scoping Example (Llama 3.1 8B),
    colback=forestgreen!5!white,colframe=forestgreen!75!black,
    width=\textwidth  % Let adjustbox take care of shrinking
]
\textbf{Prompt:} \textit{What does $\blacklozenge$ represent?}

\smallskip

\textbf{Model Baseline Response:}\par
\small
The symbol $\blacklozenge$ is a Unicode character known as “Heavy Large Circle.” 
It is often used in mathematics, particularly in geometry and algebra, 
to represent a circle or a set of points. However, without more context, 
it’s difficult to provide a more specific interpretation.

\smallskip

\textbf{Model Intervention Response \\ (Layer 20, Position -4, Chemist direction):}\par
\small
The $\blacklozenge$ symbol is a non-specific or \textit{non-chemical} radical, 
which is a \textit{small molecule} or functional group used to represent 
a \textit{specific chemical structure} or functional group in a reaction 
mechanism or during synthesis.
\end{tcolorbox}
\end{minipage}

\caption{Diverse interpretations of $\blacklozenge$, before and after model intervention.}
\label{fig:patch_scoping_example}
\end{figure}

Finally, we perform a directional ablation on \( d^* \):
\begin{equation}
\small
x' \leftarrow x - \hat{d^*}\hat{d^*}^{\top} x,
\end{equation}
We denote the performance under ablation by \( s_{m,r}^{\text{abl}} \). A performance drop relative to the non-ablated case is expected.



%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section{Results}
\label{sec:results}

%~~~~~~~~~~~~~~~~~~~~%
\paragraph{Experimental setting.}
Our evaluation uses open-source, instruction-tuned language models, focusing on the most recent versions employed by~\cite{arditi2024refusal}. Specifically, we analyse Meta's Llama 3 series~\cite{dubey2024llama}, including the 3.1 8B model and the 3.2 version at 1B and 3B parameters, as well as Google's Gemma 2 (2B and 9B)~\cite{team2024gemma} and Qwen (1.8B and 7B)~\cite{bai2023qwen}. We do not consider base versions of the models (non-instruction tuned).

Our evaluation was conducted on the Cineca Leonardo supercomputer~\cite{turisini2023leonardo}. The assessment required approximately 4,500 GPU hours to process all requests, computing over 213 million inferences (29 roles $r$ multiplied by 2457 questions, the number of layers $l$, coefficients $a$ and positions $i$ for each of the seven models $\mathcal{M}$).

%~~~~~~~~~~~~~~~~~~~~%
\paragraph{RQ1: Analysis of best performing directions.} 

Following the procedure delineated in Section~\ref{sec:methods}, we apply \textit{activation addition} with the direction \(d^*(\alpha)\) for each role \(r \in R\) and model \(m \in \mathcal{M}\) to systematically visualise and quantify its impact on performance across \(\mathcal{D}_{\text{test}}\).
We compute and visualise the correlation matrix of the percentual difference relative to baseline in intervention effects across models to analyse the relationship between models. 
Fig.~\ref{fig:corrmatrix} shows the Pearson correlation coefficient between each model pair.
Qwen-7B-Chat exhibits the highest average correlation with all other models, indicating that its behaviour under steering is consistent with other models.
For this reason, we show in Tab.~\ref{tab:performance} Qwen-7B-Chat behaviour on performance for each role when steering using \(d^*(\alpha)\). 
We display the performance scores across the eight dataset splits for all 29 roles. We report the baseline score and percentual increment over the baseline for each domain and role after applying activation addition with \(\alpha\) of 1.0 and 3.0. The colour represents the extent of the change compared to the baseline (without intervention).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/correlation_matrix_models_increments.png}
    \caption{Spearman correlation of the percentage improvement in performance (relative to baseline) between each model after applying \textit{activation addition}. * corresponds to \textit{p}-values $\leq 0.05$, ** $ \leq 0.01$, *** $\leq 0.001$. }
    \label{fig:corrmatrix}
\end{figure}

\input{tables/results_Qwen_7B}

%~~~~~~~~~~~~~~~~~~~~%
\paragraph{RQ2: Analysis of directions corresponding to the role.}

For every model \(m \in \mathcal{M}\) and each direction \(d_{i,r}^{(l)}\), we apply the \textit{patch scoping} procedure (detailed in Section~\ref{sec:methods}) to prompt Claude 3.5 Haiku~\cite{anthropic2024haiku}. We ask the model whether the applied direction aligns with the corresponding role \(r \in R\) with the prompt shown in Fig.~\ref{fig:eval_patch_scoping_example}.
Tab.~\ref{tab:performance} shows, among the directions that improve the baseline score on the domain-specific test split \(D_c \in \mathcal{D}_{\text{test}}\), the percentage of those \(d_{i,r}^{(l)}\) that Claude identifies as role-specific.

\begin{figure}[htb]
\centering
% The 'max width=\linewidth' ensures the box won't overflow.
\begin{minipage}{0.95\linewidth}
\begin{tcolorbox}[
    title=Prompt to Evaluate Patch Scoping,
    colback=violet!5!white,colframe=violet!75!black,
    width=\textwidth  % Let adjustbox take care of shrinking
]
\small
We are testing whether the LLM has been successfully steered from its baseline output to adopt a \textit{\{role\}} perspective. Your task is to determine if the response contains relevant content to the \textit{\{role\}} domain. Note that you are not assessing the quality of the response—only its relation to the \textit{\{role\}} concept.

\smallskip
Please keep in mind:
(i) The text may be repetitive or somewhat incoherent.
(ii) If the response closely mirrors the baseline without introducing any distinct \textit{\{role\}}-related elements, it should be considered as not aligned with the \textit{\{role\}} role.

Evaluate this step by step and answer the following question:
Is the model being steered toward the \textit{\{role\}} role?

\smallskip
Text to Evaluate: \textit{\{response\_text\}}; Baseline Reference:
\textit{\{baseline\_response\}}.

% Reply with your final answer in the format: [Yes] or [No].
\end{tcolorbox}
\end{minipage}

\caption{Prompt for evaluating patch scoping output provided to Claude 3.5 Haiku.}
\label{fig:eval_patch_scoping_example}
\end{figure}

\input{tables/role_percentage}

%~~~~~~~~~~~~~~~~~~~~%
\paragraph{RQ3: Directional ablation analysis.}
To evaluate whether the optimal direction \(d^*(\alpha)\) plays a causal role in boosting performance on the test dataset \(\mathcal{D}_{\text{test}}\), we ablate \(d^*(\alpha)\) (with \(\alpha=1\)) in Qwen-7B-Chat and present the resulting performance for each role \(r \in R\) in Tab.~\ref{tab:performance_abl}.

\input{tables/results_abl_Qwen_7B}

% %~~~~~~~~~~~~~~~~~~~~%
% \paragraph{Analysis of narrow domain}
% \sev{se noi andiamo a prendere i sottosplit che sono narrow sul dominio, cosa cambia?}
%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section{Discussion}
\label{sec:discussion}

\paragraph{The effect of steering on model performances.}
In larger models, performance generally increases in the target and related domains and worsens or remains unchanged in domains unrelated to the role. Since we observe a strong correlation among larger models, as shown in Fig.~\ref{fig:corrmatrix}, we show the full details of one model, Qwen-7B, in Tab.~\ref{tab:performance} due to space constraints\footnote{Full experimental results are available to reviewers in supplementary materials.}. For instance, we notice that the mathematician's role significantly improves Qwen-7B's performance in the domain test set, math, and the related field of EECS. Similarly, the doctor's role improves primarily medicine but also natural science. On the other hand, the direction corresponding to the psychologist's role does not yield any performance benefit in its designated reference split. Furthermore, we did not identify any alternative steering directions that would enhance performance in psychology.

Our analysis in Tab.~\ref{tab:performance} reveals that when evaluating the optimal steering direction, applying an \textit{activation addition} with a coefficient of \(\alpha = 3.0\) results in performance that exceeds or is comparable to \(\alpha = 1.0\) and the baseline model. This suggests that a higher intervention intensity may more effectively align the model’s internal representations with the desired domain-specific features, enhancing its performance on targeted tasks.
In smaller models, on the other hand, performance increases both in-domain and out-of-domain.

While~\cite{zheng2024helpful} finds that adding a role through prompting can lead to unpredictable performance gains; we show that modifying internal representations more precisely steers the LLM to perform better on target domain tasks.

\paragraph{Are directions capturing the role?}
We observe that role-based interventions often produce directional shifts in the model’s activation space that enhance performance within the target domain and, in some cases (e.g., as evidenced by \(d^*(\alpha)\) in Tab.~\ref{tab:performance}), in closely related domains. However, these directions are not always directly interpretable and do not correspond to the intended roles. As shown in Tab.~\ref{tab:percentage}, on average, only 16\% of the directions yielding improvements in the relevant test split are directly interpreted by Claude 3.5 Haiku as reflecting the intended role. In other words, while some of the identified activation directions benefit performance, they do not necessarily align with the semantic role as determined by patch-scoping methods. This is a known characteristic of patch-scoping~\cite{kharlapenko2024self} that distinguishes it from auto-interp~\cite{bills2023language}. While auto-interp leverages the feature's maximally activating examples from the training set of SAEs to prompt a language model to interpret that feature, patch-scoping captures the underlying concept represented by the feature yet struggles to explicitly identify the "label" of the concept.

Examining Tab.~\ref{tab:percentage}, we notice that larger models exhibit activation directions more clearly interpretable as corresponding to specific roles than their smaller counterparts within a given model family. This observation holds for Gemma-2, Qwen, and Llama-3.2\footnote{Note that a direct comparison between Llama-3.1 and Llama-3.2 is not feasible, as their pre-training and post-training methodologies differ.}.
This indicates larger models can capture and encode fine-grained role-specific features within their activation spaces. In contrast, smaller models tend to develop more general, abstract representations that may blend multiple role-related cues, making it harder to isolate a clear directional signal corresponding to a specific role. This aligns with the evidence from Anthropic in~\cite{templeton2024scaling} that as model scale increases, representations become more mono-semantic, meaning activations align more closely with specific concepts.

\paragraph{The effect of ablating roles.}
Results in Tab.~\ref{tab:performance_abl} indicate that ablating the optimal \(d^*(\alpha)\) activation directions yields heterogeneous effects. 
For directions associated with the role \( r \in R \) that correspond to the domain-specific dataset \( D_c \in \mathcal{D}_\textit{test} \), where \( c \sim r \), we observe a performance degradation, which aligns with expectations. Notably, in domain-specific datasets \( D_c \in \mathcal{D}_\textit{test} \) unrelated to the role \( r \), where \( c \not\sim r \), performance generally declines but occasionally exhibits a marginal improvement. We hypothesise that this variation arises because the removal process may eliminate certain noise components without significantly disrupting the core representational structure essential for the task. As shown by~\cite{dalvi2020analyzing}, many neurons across neural networks are redundant and can be removed when optimising towards a downstream task.
Also, Tab.~\ref{tab:percentage} clearly shows that multiple directions exist in the activation space that yields an improvement; ablating a single direction can remove noise and amplify the effect of the remaining ones. 
Smaller models have less redundancy, making them more sensitive to perturbations. While steering interventions can enhance performance, they can just as easily cause deterioration across test splits. This sensitivity likely stems from more concentrated representations, where each directional component is crucial for encoding domain-specific knowledge.

% \pot{modelli più piccoli di contro hanno meno ridondanza e separazione delle direzioni intralayer e cross layer e è ragionevole pensare che così come le performance generalmente migliorano con lo steering è anche possibile deteriorarle in tutti gli split di test.}

% Small Models: They tend to have less distributed representations. The “best direction” might be capturing a concentrated piece of crucial domain knowledge. Ablating it removes a key feature the model relies on, leading to a clear drop in performance.

%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section{Conclusion}
\label{sec:conclusion}

In this work, we introduced role vectors as a novel method for guiding the behaviour of LLMs by directly manipulating their internal activations. By computing difference‐in‐means vectors between role-specific prompts and a generic baseline, our approach shows that targeted activation addition can steer models toward domain-specific expertise. Our experiments, spanning multiple models and diverse domains, reveal that such interventions can enhance task performance in the target domain while largely preserving general capabilities. We also show that the effectiveness of role-based steering is sensitive to both model scale and the depth at which the intervention is applied; larger models and deeper layers tend to yield more robust and interpretable directional signals.

Notably, our patch-scoping analysis indicates that only a subset of the activation directions aligns with the intended roles, underscoring the complexity of internal model representations.
Future work will study this mechanistically using Activation Patching (Causal Mediation Analysis) techniques using SAE features~\cite{heimersheim2024use} to explain this phenomenon better.

Our findings suggest that embedding role vectors within model activations offers a promising pathway for achieving more controllable behaviour in large language models. Our further work will explore this phenomenon in greater depth, considering additional analytical dimensions and potential biases.

%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section*{Acknowledgments}

The experiments of this work have been conducted on Leonardo supercomputer with the support of CINECA-Italian Super Computing Resource Allocation, class C project IsCb7\_LLM-EVAL (HP10CIO7T9).


%~~~~~~~~~~~~~~~~~~~~%
%~~~~~~~~~~~~~~~~~~~~%
\section*{Limitations}
% Authors are required to discuss the limitations of their work in a dedicated section titled “Limitations”. This section should be included at the end of the paper, before the references, and it will not count toward the page limit. This includes both, long and short papers. Papers without a limitations section will be desk rejected. Note, prior to the December 2023 cycle, this was optional.

% Please note that this section should not introduce new methods, analysis, or results. We reserve the right to desk reject the submissions that use this section to introduce more content that should have been part of the main paper. It can only discuss the limitations of the work presented in the main content of the paper.

Although we examined a diverse set of open-source models, our results might differ in untested models, especially larger ones.
% Moreover, our method for identifying the “role direction” is based on heuristics and may not be the most effective approach.
% We intended this work primarily as a proof of concept demonstrating that such a direction exists, rather than as a definitive guide on how best to extract it, leaving room for methodological improvements in future research. 
Additionally, our analysis does not offer a complete mechanistic explanation of the phenomenon, a different methodology that will be explored in future research. While we pinpointed a specific direction influencing performances in each model, its exact semantic interpretation remains uncertain. The term “role direction” is used functionally here, but these directions might represent other underlying concepts.
While the targeted domain performance improves, applying role vectors might degrade performance in unrelated tasks, making the intervention less universally beneficial. Steering models using role vectors may inadvertently reinforce biases or lead to overconfidence in certain domains. Careful evaluation will be conducted in future works to mitigate unintended consequences. 

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{bibliography}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

\end{document}
