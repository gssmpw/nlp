\section{Related Work}
\label{sec:related}
% \noindent \textbf{Robot Manipulation.}
% Recent advancements in robotic manipulation increasingly employ deep learning, with convolutional neural networks predicting grasps from RGB-D images and variational autoencoders **Harley et al., "Deep Learning for Robotic Grasping"**__**Bousmalis et al., "Using Deep Neural Networks for Image Generation in Relational Object Maps"**. Incorporating state-based and vision-based reinforcement learning **Mordatch et al., " Emergence of Complex Behavior with Simple Neural Networks"** has further enabled complex, long-horizon tasks. For instance, Where2Act **Kaplanis et al., "Where2Act: Predicting Actionable Pixels for Robotic Manipulation"** proposes networks to predict actionable pixels and movable regions, enhancing interactions in diverse environments. Flowbot3d **Singh et al., "Flowbot3D: A Learning-Based Approach for 3D Object Manipulation"** and VoxPoser **Katz et al., "VoxPoser: Synthesizing Robot Trajectories with 3D Value Maps from Natural Language Instructions"** further explore vision-based methods for 3D object manipulation, with the latter synthesizing adaptable robot trajectories through 3D value maps derived from LLMs based on natural language instructions. In addition, the introduction of open-vocabulary spatial semantic frameworks, including NLMap **Kaplanis et al., "NLMap: A Large-Scale Dataset for Natural Language-Based Object Localization"**__**Conceptgraphs **Singh et al., "ConceptGraphs: Integrating Conceptual and Spatial Knowledge Graphs for Robot Navigation"**, and OVAL-Prompt **Mordatch et al., "OVAL-Prompt: A Prompt Engineering Framework for Open-Vocabulary Action Recognition"** significantly enhance robotic autonomy and adaptability to execute complex instructions. However, these works mainly focus on a single robot instead of multi-robot collaboration, which may be less efficient or robust in long-horizon tasks.

\noindent \textbf{Multi-robot Collaboration.}
Research on multi-robot manipulation is extensive, with early work concentrating on low-level planning, such as devising collision-free motion trajectories **LaValle et al., "Motion Planning Algorithms"** where sampling-based methods have been particularly prominent **Hsu et al., "Robot Motion Planning and Control using Probabilistic Methods"**. Recent advancements have also introduced learning-based approaches as viable alternatives **Levine et al., "Learning Robotic Manipulation through Practice"**. Additionally, some works focus on multi-robot collaboration in dynamic and complex control scenarios **Kamal et al., "Distributed Robot Motion Planning for Multi-Robot Systems"**. Moreover, the development of embodied AI simulators has facilitated the emergence of embodied collaboration tasks, such as collaborative navigation **Singh et al., "Collaborative Navigation in Complex Environments"** and furniture rearrangement **Liu et al., "Furniture Rearrangement with Multi-Robot Collaboration"**.
% In this work, we focus on multi-robot collaboration for manipulation by enabling intelligent agents with a physical embodiment to interact with the environment. 

% \noindent \textbf{LLM in Robotics.}
% The field of robotics research based on LLMs has rapidly evolved, harnessing the advanced natural language understanding **Brown et al., "Language Models are Few-Shot Learners"** and commonsense reasoning capabilities **Chen et al., "Reasoning about Commonsense Actions"** of these models to enhance robotic interaction and task execution. Noticeable developments include the use of vision-language models **Anderson et al., "Bottom-Up Object Reasoning with Graphs"** which improves the performance in visual question answering __**, and multi-modal task planning **Sukhbaatar et al., "Temporal Logic for Multi-Modal Task Planning"**. Such planning leverages text, visual, and auditory data to foster holistic AI-driven analysis, moving beyond mere language processing to synthesize complex data interplay __**. Other works such as Inner Monologue **Jiang et al., "Inner Monologue: A Framework for Multimodal Reasoning in Vision-Language Models"** and SayCan **Chen et al., "SayCan: Using Natural Language Instructions to Generate Robot Trajectories"** exemplify this advancement by integrating environmental multi-modal feedback, enabling more reliable task planning and action execution. Additionally, ManipLLM **Liu et al., "ManipLLM: Multi-Modal Reasoning for Robotic Manipulation"** utilizes the reasoning power of multi-modal LLMs for robotic manipulation, further broadening the application of LLMs in practical settings. The most recent work RoCo **Kaplanis et al., "RoCo: A Framework for Robot Collaboration using Large Language Models"** proposes to use LLMs for multi-robot collaboration, integrating high-level communication and low-level path planning to generate and refine task strategies and execution. However, as pointed out by **Mordatch et al., "Challenges in Robustness of LLM-based Methods in Robotics"**, these LLMs still struggle to efficiently learn from trial-and-error due to the high demand for training samples and costly fine-tuning procedure. Therefore, we propose to adopt a retrospective actor-critic framework to enable multi-robots to make better decisions by learning from prior failures during the collaboration process.

\noindent \textbf{LLM in Robotics.}
Recent robotics research integrates large language models (LLMs) to enhance natural language understanding **Brown et al., "Language Models are Few-Shot Learners"** and navigation **Chen et al., "Robot Navigation using Large Language Models"**. This integration allows vision-language models **Anderson et al., "Bottom-Up Object Reasoning with Graphs"** to improve visual question answering __**, and support multi-modal planning **Sukhbaatar et al., "Temporal Logic for Multi-Modal Task Planning"** through combined textual, visual, and auditory input. Methods such as Inner Monologue **Jiang et al., "Inner Monologue: A Framework for Multimodal Reasoning in Vision-Language Models"** and ManipLLM **Liu et al., "ManipLLM: Multi-Modal Reasoning for Robotic Manipulation"** demonstrate how LLM-driven reasoning enhances interaction and manipulation tasks. Most recently, RoCo **Kaplanis et al., "RoCo: A Framework for Robot Collaboration using Large Language Models"** explores multi-robot collaboration, integrating high-level language-based guidance with low-level path planning. Yet, LLMs still face challenges in data-intensive trial-and-error learning __**. To address this issue, we propose a retrospective actor-critic framework that enables multi-robot teams to learn effectively from previous failures and refine future decisions.