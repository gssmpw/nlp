\section{Related Work}
\label{sec:related}
% \noindent \textbf{Robot Manipulation.}
% Recent advancements in robotic manipulation increasingly employ deep learning, with convolutional neural networks predicting grasps from RGB-D images and variational autoencoders~\cite{kingma2013auto} for predicting 6-DOF grasp poses from point clouds~\cite{mahler2017dex,mousavian2019}. Incorporating state-based and vision-based reinforcement learning~\cite{andrychowicz2020learning} has further enabled complex, long-horizon tasks. For instance, Where2Act \cite{mo2021where2act} proposes networks to predict actionable pixels and movable regions, enhancing interactions in diverse environments. Flowbot3d \cite{eisner2022flowbot3d} and VoxPoser \cite{huang2023voxposer} further explore vision-based methods for 3D object manipulation, with the latter synthesizing adaptable robot trajectories through 3D value maps derived from LLMs based on natural language instructions. In addition, the introduction of open-vocabulary spatial semantic frameworks, including NLMap \cite{chen2023open}, Conceptgraphs~\cite{gu2024conceptgraphs}, and OVAL-Prompt~\cite{tong2024oval}, significantly enhance robotic autonomy and adaptability to execute complex instructions. However, these works mainly focus on a single robot instead of multi-robot collaboration, which may be less efficient or robust in long-horizon tasks.

\noindent \textbf{Multi-robot Collaboration.}
Research on multi-robot manipulation is extensive, with early work concentrating on low-level planning, such as devising collision-free motion trajectories \cite{xie2023language}, where sampling-based methods have been particularly prominent \cite{karaman2011sampling}. Recent advancements have also introduced learning-based approaches as viable alternatives \cite{zhang2022multi}. Additionally, some works focus on multi-robot collaboration in dynamic and complex control scenarios \cite{zeng2022socratic}. Moreover, the development of embodied AI simulators has facilitated the emergence of embodied collaboration tasks, such as collaborative navigation \cite{liu2022multi} and furniture rearrangement \cite{jain2020cordial}. 
% In this work, we focus on multi-robot collaboration for manipulation by enabling intelligent agents with a physical embodiment to interact with the environment. 

% \noindent \textbf{LLM in Robotics.}
% The field of robotics research based on LLMs has rapidly evolved, harnessing the advanced natural language understanding \cite{khurana2023natural} and commonsense reasoning capabilities \cite{ravi2023vlc} of these models to enhance robotic interaction and task execution. Noticeable developments include the use of vision-language models \cite{zhang2021vinvl} which improves the performance in visual question answering \cite{lin2023medical}, and multi-modal task planning \cite{shao2023prompting}. Such planning leverages text, visual, and auditory data to foster holistic AI-driven analysis, moving beyond mere language processing to synthesize complex data interplay \cite{zitkovich2023rt}. Other works such as Inner Monologue \cite{huang2022inner} and SayCan \cite{ahn2022can} exemplify this advancement by integrating environmental multi-modal feedback, enabling more reliable task planning and action execution. Additionally, ManipLLM \cite{li2024manipllm} utilizes the reasoning power of multi-modal LLMs for robotic manipulation, further broadening the application of LLMs in practical settings. The most recent work RoCo~\cite{mandi2024roco} proposes to use LLMs for multi-robot collaboration, integrating high-level communication and low-level path planning to generate and refine task strategies and execution. However, as pointed out by~\cite{shinn2024reflexion}, these LLMs still struggle to efficiently learn from trial-and-error due to the high demand for training samples and costly fine-tuning procedure. Therefore, we propose to adopt a retrospective actor-critic framework to enable multi-robots to make better decisions by learning from prior failures during the collaboration process.

\noindent \textbf{LLM in Robotics.}
Recent robotics research integrates large language models (LLMs) to enhance natural language understanding~\cite{khurana2023natural} and navigation~\cite{wen2025zero,wen2024secure}. This integration allows vision-language models~\cite{zhang2021vinvl} to improve visual question answering~\cite{lin2023medical} and support multi-modal planning~\cite{shao2023prompting} through combined textual, visual, and auditory input. Methods such as Inner Monologue~\cite{huang2022inner} and ManipLLM~\cite{li2024manipllm} demonstrate how LLM-driven reasoning enhances interaction and manipulation tasks. Most recently, RoCo~\cite{mandi2024roco} explores multi-robot collaboration, integrating high-level language-based guidance with low-level path planning. Yet, LLMs still face challenges in data-intensive trial-and-error learning~\cite{shinn2024reflexion}. To address this issue, we propose a retrospective actor-critic framework that enables multi-robot teams to learn effectively from previous failures and refine future decisions.