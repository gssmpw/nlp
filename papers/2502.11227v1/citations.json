[
  {
    "index": 0,
    "papers": [
      {
        "key": "kingma2013auto",
        "author": "Kingma, Diederik P",
        "title": "Auto-encoding variational bayes"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "mahler2017dex",
        "author": "Mahler, Jeffrey and Liang, Jacky and Niyaz, Sherdil and Laskey, Michael and Doan, Richard and Liu, Xinyu and Ojea, Juan Aparicio and Goldberg, Ken",
        "title": "Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics"
      },
      {
        "key": "mousavian2019",
        "author": "Mousavian, Arsalan and Eppner, Clemens and Fox, Dieter",
        "title": "6-dof graspnet: Variational grasp generation for object manipulation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "andrychowicz2020learning",
        "author": "Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others",
        "title": "Learning dexterous in-hand manipulation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "mo2021where2act",
        "author": "Mo, Kaichun and Guibas, Leonidas J and Mukadam, Mustafa and Gupta, Abhinav and Tulsiani, Shubham",
        "title": "Where2act: From pixels to actions for articulated 3d objects"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "eisner2022flowbot3d",
        "author": "Eisner, Ben and Zhang, Harry and Held, David",
        "title": "Flowbot3d: Learning 3d articulation flow to manipulate articulated objects"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "huang2023voxposer",
        "author": "Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li",
        "title": "Voxposer: Composable 3d value maps for robotic manipulation with language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2023open",
        "author": "Chen, Boyuan and Xia, Fei and Ichter, Brian and Rao, Kanishka and Gopalakrishnan, Keerthana and Ryoo, Michael S and Stone, Austin and Kappler, Daniel",
        "title": "Open-vocabulary queryable scene representations for real world planning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "gu2024conceptgraphs",
        "author": "Gu, Qiao and Kuwajerwala, Ali and Morin, Sacha and Jatavallabhula, Krishna Murthy and Sen, Bipasha and Agarwal, Aditya and Rivera, Corban and Paul, William and Ellis, Kirsty and Chellappa, Rama and others",
        "title": "Conceptgraphs: Open-vocabulary 3d scene graphs for perception and planning"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "tong2024oval",
        "author": "Tong, Edmond and Opipari, Anthony and Lewis, Stanley Robert and Zeng, Zhen and Jenkins, Odest Chadwicke",
        "title": "OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xie2023language",
        "author": "Xie, Amber and Lee, Youngwoon and Abbeel, Pieter and James, Stephen",
        "title": "Language-conditioned path planning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "karaman2011sampling",
        "author": "Karaman, Sertac and Frazzoli, Emilio",
        "title": "Sampling-based algorithms for optimal motion planning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2022multi",
        "author": "Zhang, Han and Chen, Jingkai and Li, Jiaoyang and Williams, Brian and Koenig, Sven",
        "title": "Multi-agent path finding for precedence-constrained goal sequences"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zeng2022socratic",
        "author": "Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and others",
        "title": "Socratic models: Composing zero-shot multimodal reasoning with language"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2022multi",
        "author": "Liu, Xinzhu and Guo, Di and Liu, Huaping and Sun, Fuchun",
        "title": "Multi-agent embodied visual semantic navigation with scene prior knowledge"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "jain2020cordial",
        "author": "Jain, Unnat and Weihs, Luca and Kolve, Eric and Farhadi, Ali and Lazebnik, Svetlana and Kembhavi, Aniruddha and Schwing, Alexander",
        "title": "A cordial sync: Going beyond marginal policies for multi-agent embodied tasks"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "khurana2023natural",
        "author": "Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh, Sukhdev",
        "title": "Natural language processing: state of the art, current trends and challenges"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ravi2023vlc",
        "author": "Ravi, Sahithya and Chinchure, Aditya and Sigal, Leonid and Liao, Renjie and Shwartz, Vered",
        "title": "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhang2021vinvl",
        "author": "Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng",
        "title": "Vinvl: Revisiting visual representations in vision-language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "lin2023medical",
        "author": "Lin, Zhihong and Zhang, Donghao and Tao, Qingyi and Shi, Danli and Haffari, Gholamreza and Wu, Qi and He, Mingguang and Ge, Zongyuan",
        "title": "Medical visual question answering: A survey"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "shao2023prompting",
        "author": "Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun",
        "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "zitkovich2023rt",
        "author": "Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan and others",
        "title": "RT-2: Vision-language-action models transfer web knowledge to robotic control"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "huang2022inner",
        "author": "Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others",
        "title": "Inner monologue: Embodied reasoning through planning with language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "ahn2022can",
        "author": "Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Fu, Chuyuan and Gopalakrishnan, Keerthana and Hausman, Karol and others",
        "title": "Do as I can, not as I say: Grounding language in robotic affordances"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "li2024manipllm",
        "author": "Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao",
        "title": "Manipllm: Embodied multimodal large language model for object-centric robotic manipulation"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "mandi2024roco",
        "author": "Mandi, Zhao and Jain, Shreeya and Song, Shuran",
        "title": "Roco: Dialectic multi-robot collaboration with large language models"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "shinn2024reflexion",
        "author": "Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu",
        "title": "Reflexion: Language agents with verbal reinforcement learning"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "khurana2023natural",
        "author": "Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh, Sukhdev",
        "title": "Natural language processing: state of the art, current trends and challenges"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "wen2025zero",
        "author": "Wen, Congcong and Huang, Yisiyuan and Huang, Hao and Huang, Yanjia and Yuan, Shuaihang and Hao, Yu and Lin, Hui and Liu, Yu-Shen and Fang, Yi",
        "title": "Zero-shot object navigation with vision-language models reasoning"
      },
      {
        "key": "wen2024secure",
        "author": "Wen, Congcong and Liang, Jiazhao and Yuan, Shuaihang and Huang, Hao and Fang, Yi",
        "title": "How Secure Are Large Language Models (LLMs) for Navigation in Urban Environments?"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "zhang2021vinvl",
        "author": "Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng",
        "title": "Vinvl: Revisiting visual representations in vision-language models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "lin2023medical",
        "author": "Lin, Zhihong and Zhang, Donghao and Tao, Qingyi and Shi, Danli and Haffari, Gholamreza and Wu, Qi and He, Mingguang and Ge, Zongyuan",
        "title": "Medical visual question answering: A survey"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "shao2023prompting",
        "author": "Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun",
        "title": "Prompting large language models with answer heuristics for knowledge-based visual question answering"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "huang2022inner",
        "author": "Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others",
        "title": "Inner monologue: Embodied reasoning through planning with language models"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "li2024manipllm",
        "author": "Li, Xiaoqi and Zhang, Mingxu and Geng, Yiran and Geng, Haoran and Long, Yuxing and Shen, Yan and Zhang, Renrui and Liu, Jiaming and Dong, Hao",
        "title": "Manipllm: Embodied multimodal large language model for object-centric robotic manipulation"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "mandi2024roco",
        "author": "Mandi, Zhao and Jain, Shreeya and Song, Shuran",
        "title": "Roco: Dialectic multi-robot collaboration with large language models"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "shinn2024reflexion",
        "author": "Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu",
        "title": "Reflexion: Language agents with verbal reinforcement learning"
      }
    ]
  }
]