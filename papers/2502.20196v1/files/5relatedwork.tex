\section{Related Work}
\subsection{LLM Factuality}
LLMs have demonstrated exceptional capabilities in memorizing and utilizing factual knowledge through their strong parametric memories, enabling applications in knowledge-intensive domains. This potential has driven significant research into deploying LLMs for e-commerce applications, including product recommendation systems \cite{xu2024leveraginglargelanguagemodels,10.1145/3580305.3599519,10.1145/3616855.3635853}, search ranking \cite{10.1007/978-3-031-56060-6_24,rathee2025guidingretrievalusingllmbased}, and attribute extraction \cite{10.1007/978-3-031-78090-5_4, Baumann2024UsingLF,zou-etal-2024-eiven}. Recent domain-specific adaptations like EcomGPT \cite{10.1609/aaai.v38i17.29820} and eCeLLM \cite{10.5555/3692070.3693702} employ instruction tuning to align general-purpose LLMs with e-commerce scenarios. However, these approaches remain constrained by training data, which focus on narrow operational competencies (e.g., single-task attribute recognition) rather than comprehensive understanding.

\subsection{E-commerce Datasets}
Prior e-commerce datasets predominantly concentrate on isolated or narrowly related tasks. For instance, Amazon-M2 \cite{10.5555/3666122.3666473} focuses on session-based recommendation systems, Amazon-ESCI \cite{reddy2022shoppingqueriesdatasetlargescale} specializes in query-product matching, while EComInstruct \cite{10.1609/aaai.v38i17.29820} targets shopping concept understanding. These datasets exhibit limited coverage of the multifaceted skill requirements inherent to real-world e-commerce applications due to their constrained task diversity.

Shopping MMLU \cite{NEURIPS2024_2049d75d}, concurrently with our work, presents a multi-dimensional benchmark constructed from Amazon data. However, it exclusively focuses on the English-language domain. To address the absence of comprehensive evaluation resources for Chinese e-commerce ecosystems, we propose ChineseEcomQA - a benchmark systematically assessing diverse e-commerce capabilities in Chinese contexts.

\begin{figure}[t]
    \centering
    %\vspace{-2mm}
\includegraphics[width=1\columnwidth]{pics/rank_range.pdf}
    \vspace{-3mm}
    \caption{
     The rankings of different LLMs on ChineseSimpleQA and ChineseEcomQA.
    }
    \label{fig:rank_range}
    \vspace{-3mm}
\end{figure}

\vspace{-1mm}