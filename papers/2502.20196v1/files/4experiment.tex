
\section{Experiments}
\begin{table*}[ht]
    \centering
    \begin{tabular}{c|c|cccccccccc}
       % \toprule
        \toprule
         & \textbf{Accuracy} & \multicolumn{10}{c}{\textbf{Accuracy on 10 sub-concepts}} \\
        \cmidrule(lr){2-12}
         \textbf{Models} & Avg. &IC & IDC & CC & BC & AC & SC & ITC & RVC & RLC & PC \\
        \cmidrule(lr){1-12}
%        \multicolumn{12}{c}{\textit{Closed-Source Large Language Models}} \\
        \rowcolor{lightgray} \multicolumn{12}{c}{{\bf \textit{Closed-Source Large Language Models}}} \\
        \cmidrule(lr){1-12}
        GLM-4-Plus & 69.2 & 57.3 & 54.1 & 76.3 & 77.6 & 69.5 & 59.5 & 72.2 & 83.1 & 68.5 & 74.0  \\
        Qwen2.5-max & 68.5 & 62.2 & 62.2 & 71.1 & 77.6 & 63.0 & 58.5 & 57.8 & 88.7 & 63.9 & 80.4 \\
        Yi-Large & 67.6 & 56.6 & 59.5 & 71.1 & 81.8 & 62.0 & 58.5 & 70.0 & 70.4 & 68.5 & 78.0 \\
        o1-preview & 66.8 & 69.2 & 63.1 & 78.4 & 80.0 & 67.0 & 52.0 & 43.3 & 83.1 & 61.3 & 71.1 \\
        Baichuan4-Turbo & 66.4 & 57.3 & 56.8 & 82.0 & 72.4 & 61.0 & 59.5 & 66.7 & 78.9 & 55.4 & 74.6 \\
        GPT-4o & 65.6 & 68.2 & 52.3 & 74.7 & 72.4 & 64.5 & 56.5 & 50.0 & 80.3 & 57.7 & 79.8 \\
        Doubao-1.5-pro-32k & 64.0 & 69.6 & 64.0 & 62.9 & 74.1 & 56.5 & 64.5 & 48.9 & 69.0 & 62.6 & 68.2 \\
        Claude-3.5-Sonnet & 63.8 & 70.6 & 56.8 & 73.2 & 64.1 & 63.0 & 31.5 & 62.2 & 81.7 & 65.2 & 69.4  \\
        Gemini-1.5-pro & 61.1 & 59.8 & 49.6 & 67.0 & 70.0 & 56.0 & 43.5 & 55.6 & 81.7 & 54.1 & 73.4 \\
        o1-mini & 55.4 & 59.1 & 41.4 & 53.1 & 37.1 & 59.0 & 53.0 & 58.9 & 64.8 & 63.6 & 64.2  \\
        Gemini-1.5-flash & 54.5 & 62.9 & 35.1 & 57.2 & 46.5 & 52.5 & 53.0 & 36.7 & 74.7 & 54.4 & 71.7 \\
        \cmidrule(lr){1-12}
        %\multicolumn{12}{c}{\textit{Open-Source Large Language Models}} \\
        \rowcolor{lightgray} \multicolumn{12}{c}{{\bf \textit{Open-Source Large Language Models}}} \\
        \cmidrule(lr){1-12}
        %\cmidrule(lr){1-12}
        DeepSeek-R1 & 74.0 & 62.9  & 72.1 & 72.1 & 84.7  & 70.5 & 55.5 & 67.8 & 85.9 & 76.1 & 92.5 \\
        DeepSeek-V3 & 72.2 & 67.5 & 64.9 & 74.2 & 80.6 & 69.0 & 62.0 & 72.2 & 77.5 & 68.2 & 86.1 \\
        DeepSeek-V2.5 & 67.4 & 66.4  & 58.6  & 73.7 & 76.5  & 64.0 & 60.0 & 75.6 & 83.1 & 54.1 & 61.8 \\
        DeepSeek-67B & 58.4 & 61.2  & 47.7  & 70.6 & 62.9  & 47.0 & 52.5 & 60.0 & 59.2 & 55.7 & 67.1 \\
        DeepSeek-7B & 47.5 & 38.5  & 41.1  & 59.3 & 45.9  & 40.0 & 49.0 & 54.4 & 47.9 & 54.4 & 44.7 \\
        \cmidrule(lr){1-12}
        DeepSeek-R1-Distill-Qwen-32B & 57.1 & 63.6 & 46.0 & 62.4 & 47.6 & 36.0 & 43.0 & 61.1 & 78.9 & 61.6 & 70.6 \\
        DeepSeek-R1-Distill-Qwen-14B & 50.6 & 64.7 & 43.2 & 62.9 & 38.8 & 27.5 & 41.0 & 60.0 & 67.6 & 59.0 & 40.9 \\
        DeepSeek-R1-Distill-Qwen-7B & 38.9 & 48.6 & 18.0 & 53.6 & 16.5 & 25.0 & 33.5 & 36.7 & 52.1 & 48.2 & 57.2 \\
        DeepSeek-R1-Distill-Qwen-1.5B & 26.2 & 35.7 & 2.7 & 46.9 & 6.5 & 8.5 & 23.5 & 18.9 & 40.9 & 40.0 & 38.2 \\
        \cmidrule(lr){1-12}
        Qwen2.5-72B & 62.7 & 57.3  & 46.0  & 66.0  & 64.7  & 55.5  & 58.0  & 67.8  & 76.1  & 56.7  & 78.6 \\
        Qwen2.5-32B & 60.9 & 62.2  & 42.3  & 58.8  & 50.6  & 61.5  & 52.5  & 66.7  & 74.7  & 62.3  & 77.5 \\
        Qwen2.5-14B & 55.3 & 57.0  & 40.5  & 54.6  & 48.8  & 59.0  & 49.0  & 40.0  & 66.2  & 59.3  & 78.6  \\
        Qwen2.5-7B & 47.1 & 45.8  & 24.3  & 51.6  & 37.6  & 44.5  & 54.0  & 31.1  & 64.8  & 48.5  & 68.8 \\
        Qwen2.5-3B & 41.7 & 52.1  & 14.4  & 41.8  & 34.1  & 42.5  & 34.0  & 30.0  & 60.6  & 51.1  & 56.7 \\
        \cmidrule(lr){1-12}
        LLaMA3.1-70B & 54.6 & 59.1 & 35.7 & 58.8 & 39.4 & 58.0 & 37.5 & 73.3 & 74.7 & 53.8 & 56.1 \\
        LLaMA3.1-8B & 42.4 & 40.6 & 11.7 & 61.3 & 17.1 & 42.0 & 38.5 & 42.2 & 66.2 & 44.6 & 60.0 \\
        \bottomrule
        \bottomrule
    \end{tabular}
    \vspace{5mm}
    \caption{Results of different models on ChineseEcomQA. For
sub-concepts, IC, IDC, CC, BC, AC, SC, ITC, RVC, RLC and PC represent “Industry Categorization”, “Industry Concept”, “Category Concept”, “Brand Concept”, “Attribute Concept”, “Spoken Concept”, “Intent Concept”, “Review Concept”, “Relevance Concept” and “Personalized Concept” respectively.}
    \label{tab:main_result}
    \vspace{-3mm}
\end{table*}

\subsection{Baseline Models}
We evaluate 11 closed-source LLMs (i.e., o1-preview, Qwen2.5-max~\cite{yang2024qwen2}, Doubao-1.5-pro-32k, GLM-4-Plus, GPT-4o~\cite{OpenAI2023GPT4}, Gemini-1.5-pro~\cite{team2024gemini}, Gemini-1.5-flash~\cite{team2024gemini}, Claude-3.5-Sonnet, Yi-Large, Baichuan4-Turbo, o1-mini), and 16 open-source LLMs (i.e., Qwen2.5 series~\cite{yang2024qwen2}, LLaMA3.1 series~\cite{dubey2024llama}, DeepSeek series~\cite{bi2024deepseek,liu2024deepseek,guo2025deepseek}).

\subsection{Main Results}

As shown in Table \ref{tab:main_result}, we provide the performance results of different LLMs on our ChineseEcomQA. From Table \ref{tab:main_result} and Figure \ref{fig:model_radar}, we have the following insightful observations: 

\begin{figure*}[t]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=1\linewidth]{pics/model_radar.pdf}
    % \vspace{-4mm}
    \caption{
    Detailed results on some selected models across ten sub-concept tasks.
    }
    \label{fig:model_radar}
    % \vspace{-4mm}
    % \captionsetup{belowskip=-20pt}
\end{figure*}

\begin{itemize}[leftmargin=*]
\item Deepseek-R1 \cite{guo2025deepseek} and Deepseek-V3 \cite{liu2024deepseek} are currently the best models, demonstrating the promising potential of powerful foundation LLMs (reasoning LLMs) in the e-commerce field.

\item The conclusion of Scaling Law holds true. There is significant overlap between concepts in e-commerce and general knowledge. For some complex e-commerce concepts, the performance difference between larger and smaller LLMs is more pronounced.

\item LLMs developed by Chinese companies generally show better performance, especially on advanced e-commerce concept. While O1-preview perform better on basic concepts, it faces difficulties with more advanced ones.

\item Certain types of e-commerce concepts, such as relevance concept, still pose significant challenges for LLMs. Large models, with their strong general capabilities, can partially generalize to e-commerce tasks, whereas smaller models struggle significantly in this domain. These characteristics underscore the necessity of developing models specifically tailored for the e-commerce field.

\item Deepseek-R1-Distill-Qwen series performs worse than the original Qwen series, indicating that there are still many challenges in the reasoning ability of open domains.

\item The performance gap between open-source models and closed-source models is close. Open source models represented by Deepseek bring the two to a similar level.

\end{itemize}

\subsection{Further Analysis}
\subsubsection{Analysis of Calibration}
The confidence level reflects the model's self-evaluation of its responses, indicating whether language models truly "know what they know." \cite{Steyvers_2025} A perfectly calibrated model should exhibit confidence levels that precisely align with its prediction accuracy \cite{li-etal-2022-calibration}. Following SimpleQA \cite{wei2024measuring}, we prompt the LLM to generate confidence scores (ranging 0 to 100) alongside its answers to quantify prediction certainty (see prompt details in Appendix \ref{sec:appendix C}). We subsequently aggregate model accuracy across different confidence intervals. In this section, we evaluate the calibration capabilities of various models on category concept and brand concept, with results visualized in Figure \ref{fig:calibration}.

The results demonstrate the correlation between the stated confidence of the model, and how accurate the model actually was. Notably, o1-preview exhibits the best alignment performance, followed by o1-mini. Within the Qwen2.5 series, the calibration hierarchy emerges as Qwen2.5-MAX > Qwen2.5-72B > Qwen2.5-14B > Qwen2.5-7B > Qwen2.5-3B, suggesting that larger model scales correlate with improved calibration. However, most models consistently fall below the perfect alignment line, indicating a prevalent tendency towards overconfidence in predictions. This highlights significant room for improving large language model calibration to mitigate overconfident generation of erroneous responses.

\begin{figure*}[tb]
%\vskip 0.15in
\begin{center}
\begin{minipage}[b]{0.49\textwidth} % 左侧
    \centering
\includegraphics[width=\textwidth]{pics/itemcate_qa_Calibration.pdf}\\
    (a) Category concept
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\textwidth} % 右侧
    \centering    \includegraphics[width=\textwidth]{pics/brand_qa_Calibration.pdf}\\
    (b) Brand concept
\end{minipage}
\end{center}
\centering
\vspace{-3mm}
\caption{Capabilities of various models on category concept and brand concept}
\label{fig:calibration}
%\vskip -0.1in
\end{figure*}

\subsubsection{Analysis on the Effect of RAG}
\begin{figure}[t]
    \centering
    %\vspace{-2mm}
\includegraphics[width=1\linewidth]{pics/rag.pdf}
    \vspace{-3mm}
    \caption{
    Performance comparison of models with and without RAG.
    }
    \label{fig:rag}
    \vspace{-2mm}
\end{figure}

In this subsection, we explore the effectiveness of the Retrieval-Augmented Generation (RAG) strategy in enhancing the domain knowledge of
large language models (LLMs) on the ChineseEcomQA dataset. Specifically, we reproduce a RAG system referring to the settings of ChineseSimpleQA~\cite{he2024chinese} on category concept and brand concept. In Figure \ref{fig:rag} and Table \ref{tab:rag_accuracy}, all models improve significantly with RAG. We can summarize three detailed conclusions: 

\begin{itemize}[leftmargin=*]
\item For small LLMs, introducing RAG information can significantly increase the absolute value of evaluation metrics. For example, Qwen2.5-14B has achieved a 27.9\% improvement.

\item For large LLMs, RAG can also achieve significant relative improvements. For example, Deepseek V3 \cite{liu2024deepseek}'s average relative improvement reached 10.44\% (accuracy from 77.4 to 85.5).

\item Under the RAG setting, the performance between models still follows the scaling law, but the gap is rapidly narrowed. For example, the difference in accuracy between Deepseek-V3 \cite{liu2024deepseek} and Qwen2.5-72B has narrowed from 12.1\% to 4\%.

\end{itemize}

In conclusion, the discussions above suggest that RAG serves as an
effective method for enhancing the e-commerce knowledge of LLMs.

\subsubsection{Analysis of Reasoning LLMs}
Reasoning LLMs have attracted a lot of attention in recent times. In the main results, Deepseek-R1 achieved the best results, fully demonstrating the potential of reasoning LLMs in open domains. However, on the Qwen series models distilled from Deepseek-R1, the accuracy was significantly lower than expected. Because open-source reasoning LLMs reveal their thinking process, we have the ability to further investigate the reasons for its errors. Inspired by ~\cite{liu2025oatzero}, we categorize the thinking process of reasoning models into the following four types:
\begin{itemize}[leftmargin=*]
\item Type A: Reasoning LLMs repeatedly confirm the correct answer through self-reflections.
\item Type B: Reasoning LLMs initially makes a mistake but corrects it through self-reflection.
\item Type C: Reasoning LLMs introduce knowledge errors through self-reflections, resulting in potentially correct answers being modified into an incorrect ones.
\item Type D: Reasoning LLMs undergo repeated self-reflections. Although it ultimately produced an answer, it does not obtain a highly certain and confident answer through reflection.
\end{itemize}

We used the judge LLMs (GPT-4o) to classify the thinking types of different models on category and brand concept tasks. The specific results can be seen in Table \ref{tab:reason_llm_analysis}. Analyzing the dominant reasoning types, we found the following conclusions: 

\begin{itemize}[leftmargin=*]
\item According to column Type A, after arriving at the correct answer, reasoning LLMs will verify this answer through multiple rounds of reflections.

\item According to column Type B, reasoning LLMs, regardless of their size, have acquired the ability to correct their own erroneous thinking. In the context of e-commerce concept, the underlying reasoning paths are less complex than in areas like mathematics or programming, leading to less frequent self-correction. The results also indicate that the error-correction processes do not lead to a substantial enhancement in their knowledge capacity~\cite{guo2025deepseek}.

\item According to column Type C, smaller LLMs are more likely to introduce factual errors during their thinking process, which can lead to incorrect answers. This is one of the important reasons why smaller reasoning LLMs perform worse than the original Qwen series models.
\end{itemize}

\begin{table*}[t]
    % \fontsize{8}{11}\selectfont
    \caption{The proportion of different thinking types in the reasoning LLMs.}
        \vspace{-2mm} 
    \begin{tabular}{ccccc}
        \toprule
        \toprule
        Model & Behavior A & Behavior B & Behavior C & Behavior D \\
        \cmidrule(lr){1-5}
        DeepSeek-R1-Distill-Qwen-7B &  23.97 & 2.17 & 68.02 & 5.85 \\
        DeepSeek-R1-Distill-Qwen-14B &  40.27 & 3.75 & 47.05 & 8.94 \\
        DeepSeek-R1-Distill-Qwen-32B &  39.57 & 2.14 & 52.01 & 6.29 \\
        Deepseek-R1 & 62.80 & 2.80 & 26.49 & 7.92 \\
        \bottomrule
        \bottomrule
    \end{tabular}
    % \vspace{0cm}
        \label{tab:reason_llm_analysis}
        %\vspace{-1mm}
\end{table*}

Overall, types A and B are the ability of reasoning LLMs obtained through scaling up test-time computation. Types C and D are superficial self-reflections that lead to incorrect final answers. Deepseek-R1 demonstrates better generalization ability based on a powerful base model. In contrast, the DeepSeek-R1-Distill-Qwen series, distilled in some specific fields~\cite{guo2025deepseek}, appears to struggle with superficial self-reflections. The accumulation of factual errors during the intermediate reasoning steps increases the overall error rate. For smaller reasoning LLMs, reasoning ability in open domains cannot be directly generalized through mathematical logic ability, and we need to find better methods to improve their performance.

\subsubsection{Comparison Between ChineseSimpleQA and 
ChineseEcomQA}
To demonstrate the distinctions between ChineseSimpleQA \cite{he2024chinesesimpleqachinesefactuality} and ChineseEcomQA, we compare the ranking differences of various models across these two benchmarks. As illustrated in Figure \ref{fig:rank_range}, significant performance discrepancies emerge among various models. Notably, the o1-preview model ranks first on ChineseSimpleQA but drops to 4th position on ChineseEcomQA. Conversely, GLM-4-Plus ascends from 3rd to 1st place between the two benchmarks. These ranking variations reveal that most Chinese community-developed models (e.g., Qwen-Max, GLM-4-Plus, Yi-Large) exhibit superior performance on Chinese e-commerce domain adaptation when operating within identical linguistic contexts. Furthermore, the distinct ranking distributions across models indicate that ChineseEcomQA exhibits discriminative power complementary to ChineseSimpleQA, enabling comprehensive evaluation of LLMs' domain-specific capability in Chinese e-commerce scenarios.
