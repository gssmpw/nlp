\section{Introduction}
Recent years have witnessed rapid advancements in large language models (LLMs) and their widespread adoption in different fields such as e-commerce. In the e-commerce domain, accurate comprehension of fundamental concepts is a crucial prerequisite for successful application of LLMs~\cite{Yu2024}. However, existing LLMs often suffer from generating lengthy responses that contain factual inaccuracies (hallucination)~\cite{huang2025survey}, making it challenging to systematically assess their factual abilities~\cite{he2024chinese}. Therefore, it is necessary to build a scalable e-commerce knowledge benchmark.

\begin{figure*}[t]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=0.9\linewidth]{pics/task_overview.pdf}
    % \vspace{-4mm}
    \caption{Overview of fundamental e-commerce concepts. From basic concept to advanced concept, we categorize into 10 sub-concepts.
    }
    \label{fig:overview}
    % \vspace{-4mm}
    % \captionsetup{belowskip=-20pt}
\end{figure*}

Constructing scalable evaluation benchmarks faces two specific challenges: (1) Heterogeneity and Diverse
Nature: The e-commerce domain encompasses a wide range of task formats, with definitions varying significantly across scenarios ~\cite{jin2024shopping}. For example, understanding user queries includes query error correction, query tagging, and other processes. (2)  Distinguish between Generality and Specificity: While e-commerce knowledge intersects with general world knowledge, it requires a high degree of specialized expertise. Addressing practical e-commerce problems necessitates the integration of domain-specific knowledge with general knowledge. Based on key features of existing factuality benchmarks (such as high quality and static) , we argue that a scalable e-commerce benchmark should possess three essential characteristics:

\begin{itemize}[leftmargin=*]
\item \textbf{Focus on Fundamental Concept}: We focus on fundamental concepts that enable unified generative evaluation.

\item \textbf{E-commerce Generality}: The concepts assessed by the benchmark must be common across the e-commerce industry, avoiding platform-specific implementations or task-specific formulations.

\item \textbf{E-commerce Expertise}: Real-world e-commerce problems often require a foundation of specialized e-commerce knowledge, complemented by the application of general comprehension and reasoning skills.

\end{itemize}

Existing works like ShoppingMMLU~\cite{jin2024shopping} proposed diversified e-commerce instruction evaluation. Their common idea is to abstract multiple e-commerce tasks into several main task skills. However, they lack the granularity to dissect and analyze specific, foundational e-commerce concepts. Besides, they lack a comprehensive process to ensure both the e-commerce generality and e-commerce expertise. To address this gap, we propose a highly scalable dataset construction process that combines the strengths of LLM validation, Retrieval-Augmented Generation (RAG) validation, and rigorous manual annotation. This hybrid process ensures the coverage of three core characteristics outlined above. Fundamental concepts are designed to be applicable across a diverse array of e-commerce tasks, thus addressing the challenge of heterogeneity and diversity. Additionally, by carefully balancing generality and specificity, ChineseEcomQA~\footnote{~\url{https://github.com/OpenStellarTeam/ChineseEcomQA}} effectively differentiates between broad e-commerce concepts, allowing for precise validation of domain capabilities. The resulting benchmark, ChineseEcomQA, encompasses 20 major industries and 10 core concept dimensions, comprising 1,800 carefully curated question-answer pair. Through extensive evaluations of mainstream LLMs using ChineseEcomQA, we reveal several key insights:
\begin{itemize}[leftmargin=*]

\item \textit{Leading Models}: Deepseek-R1 and Deepseek-V3 are currently the best models, demonstrating the promising potential of powerful foundation LLMs (and reasoning LLMs) in the e-commerce field.

\item \textit{Significant Challenges}: ChineseEcomQA poses considerable challenges, with many state-of-the-art models achieving below 60\% accuracy on specific sub-concepts.

\item \textit{Scaling Laws}: E-commerce concepts follow scaling law, where larger models demonstrate superior capability in advanced concepts.

\item \textit{Calibration}: Larger models show better calibration in confidence estimation.

\item \textit{Reasoning LLMs}: Deepseek-R1-Distill-Qwen series performs worse than the original Qwen series and struggles to identify and correct its own factual errors, indicating that there are still many challenges in the reasoning ability of open domains.

\item \textit{RAG matters}: When introducing the RAG strategy into existing LLMs, models of various sizes have shown significant performance improvements, narrowing the gap among models.

\end{itemize}

The main contributions to our work are threefold:

\begin{itemize}[leftmargin=*]

\item We define and categorize core e-commerce concepts that are scalable to different e-commerce tasks. A standardized QA evaluation benchmark has been constructed around e-commerce core concepts.

\item We propose a highly scalable domain-specific benchmark construction process, ensuring that ChineseEcomQA adheres three core characteristics (Focus on Fundamental Concept, E-commerce Generality, E-commerce Expertise).

\item We conduct thorough experiments on existing LLMs, providing valuable insights into their strengths and weaknesses, and offering directions for future research in the e-commerce field.

\end{itemize}

% 比较的表格考虑放到附录
% \begin{table*}
% % \fontsize{8}{11}\selectfont
%     \centering
%     \begin{fiveparttable}
%     \begin{tabular}{lllll}
%         \toprule
%         \toprule
%         \textbf{Benchmark} & \textbf{Concept Understanding} & \textbf{Decompose Basic Concepts} & \textbf{Data Source} & \textbf{Metrics} \\ 
%         EComInstruct-Test (EcomGPT) & YES      & NO   & Real World  & Accuracy \\
%         \bottomrule
%         \bottomrule
%     \end{tabular}
%     \caption{Comparisons between our ChineseEcomQA and other benchmarks.}
%     \end{fiveparttable}
%     \label{tab:comparison}
% \end{table*}
