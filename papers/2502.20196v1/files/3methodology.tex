\begin{figure*}[t]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=0.95\linewidth]{pics/data_construct_pipeline_2.pdf}
    %\vspace{-5mm}
    \caption{
    An overview of the data construction process of ChineseEcomQA.
    }
    \label{fig:data_pipeline}
    % \vspace{-2mm}
    % \captionsetup{belowskip=-20pt}
\end{figure*}

\begin{figure*}[t]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=0.95\linewidth]{pics/EcomQA_data_construct.pdf}
     %\vspace{-2mm}
    \caption{
    Illustrative examples of the data construction process.
    }
\label{fig:data_pipeline_example}
    % \vspace{-2mm}
    % \captionsetup{belowskip=-20pt}
\end{figure*}

\section{ChineseEcomQA}
\subsection{Overview}
Figure \ref{fig:overview} shows the fundamental e-commerce
concepts. Figure \ref{fig:data_pipeline} shows the overview of the data construction process of ChineseEcomQA. Besides, we provide illustrative examples of the data construction process in Figure \ref{fig:data_pipeline_example}. Table \ref{tab:dataset_statistics} shows the statistics of ChineseEcomQA. Figure \ref{fig:model_radar} visualizes the results of some selected models on ten sub concept tasks. In Appendix ~\ref{sec:appendix A}, we provide some examples of ChineseEcomQA. In the following subsection, we will introduce fundamental e-commerce concepts, data construction process, dataset statistics and evaluation metrics.

\subsection{Fundamental E-commerce Concepts}
Starting from the basic elements of e-commerce such as user behavior and product information, we summarized the main types of e-commerce concepts, defined 10 sub-concepts from basic concepts to advanced concepts as follows:

\begin{itemize}[leftmargin=*]

\item \textbf{Industry Categorization.} Given e-commerce corpus (such as user queries or web corpus), the LLMs need to figure out which e-commerce industries and categories are involved. The difficulty lies in distinguishing similar categories in the e-commerce domain.

\item \textbf{Industry Concept.} The model needs to understand the specialized knowledge in different e-commerce industries. The difficulty lies in accurately memorizing professional factual knowledge.

\item \textbf{Category Concept.} The model must understand which category a common, standard product belongs to.

\item \textbf{Brand Concept.} The model needs to recognize major brands and understand some background information about them.

\item \textbf{Attribute Concept.} E-commerce text often describes products using basic attributes, like style or age group. The model must have the ability to pick out these specific attribute words.

\item \textbf{Spoken Concept.} The e-commerce field is closely related to daily life scenarios, and people often use casual and imprecise language to express what they want. The model needs to understand the true expression forms.

\item \textbf{Intent Concept.} Beyond just informal language, sometimes consumers just list a bunch of attributes. The model needs to figure out the consumer's true intention from these phrases (such as how to choose).

\item \textbf{Review Concept.} The model needs to understand common concepts in user comments, such as emotional tendencies, commonly used evaluation aspects, etc.

\item \textbf{Relevance Concept.} One of the most crucial concepts of e-commerce is figuring out how relevant a product is to what a user wants. The model needs to integrate basic concepts such as intent concept and category concept to determine the relevance among user expression and products.

\item \textbf{Personalized Concept.} Personalized concept is one of the most important parts of user experience. This requires combining basic e-commerce concepts with general reasoning skills to recommend new product categories that best match a user's recent preferences.

\end{itemize}

\subsection{Data Collection}
\subsubsection{QA-pair Generation}
We collect a large amount of knowledge-rich e-commerce corpus, rich in information and covering various related concepts. Then, we prompt the LLM (GPT-4o) to generate question-answer pairs faithfully based on the given contents. For more open questions, we require the LLM to simultaneously provide candidate answers that are highly confusing and difficult. Providing candidate options in some concepts is beneficial for the objectivity and uniqueness of the evaluation.

\subsubsection{LLM Verification}
In the previous subsection, we collected a large number of question-answer pairs. To ensure the basic quality of dataset, we use LLM (GPT-4o) to filter data that cannot meet the requirements of our predefined criteria. Specifically, the question-answer pairs must meet the following criteria.

\begin{itemize}[leftmargin=*]
\item Questions must be a clear question about an e-commerce concept. There should be no ambiguity in the problem statement. For example, "What is the most well-known brand of washing machine?" is a disqualification question, because "most familiar" may be controversial.

\item Questions must be answerable as of 2023. The e-commerce concept investigated in the question cannot use the industry knowledge after December 31, 2023.

\item Answers must be objective and unique. There should be only one clear and objective answer to the question raised. For example, "What do you think about the Dynamic Islang on iPhone? " is too subjective.

\item Answers must not change over time. Questions about current trends or the latest product series, which are constantly evolving, are not suitable.

\end{itemize}

\subsubsection{E-commerce Generality Verification}
Since we use e-commerce corpus to construct the question-answering pairs, the original data may contain platform-specific knowledge, which affects the generality of the dataset. Therefore, we deploy external retrieval tools (i.e., web search engines) to gather information from a wider range of sources. This helped the LLM to evaluate the generality of dataset. For some question types, like those requiring the model to choose the correct category from a list, direct web searching wasn't practical. In these cases, we first require the LLM to identify several knowledge points related to the question. We then used these knowledge points as search queries. After getting the search results, we used the LLM again to remove any data that does not conform to domain generality. As illustrated in Figure \ref{fig:data_pipeline_example}, "Are there screen protectors for iPhone 14 available on Taobao?" is not common knowledge in e-commerce.

\subsubsection{E-commerce Expertise Verification}
While web searches helped ensure e-commerce generality, we also needed to confirm that the questions had sufficient depth within the e-commerce domain. Therefore, we use e-commerce search engines (such as Taobao search and e-commerce encyclopedia) to obtain more specialized information. We require the LLM to judge the level of expertise, as concept that is too basic will be filtered out.

\subsubsection{E-commerce Expertise Verification}
Real-world e-commerce problems require to integrate domain expertise and general knowledge. To verify the factual correctness of question-answer pairs, we require the LLM consider information from both general web searches and specialized e-commerce searches. For example in Figure \ref{fig:data_pipeline_example}, regarding the question of iPhone 14 Pro, we used web search results and product encyclopedias to comprehensively determine the correctness of the facts.

\subsubsection{Difficulty Filtering}
We discover the knowledge boundaries of the LLMs by checking whether multiple LLMs answer correctly. Specifically, we selected models from the Qwen series, LLaMA series, and GPT-4o for evaluation. If all the models answer a question correctly, we considered it too easy and removed it.

\subsubsection{Human Verification}
Finally, we use manual annotation to verify the quality of the evaluation set. Manual annotation requires a comprehensive consideration of the characteristics mentioned above, such as domain generality, domain expertise, and overall quality. In fact, most of the problematic data has already been filtered by the previous process. Combining LLM's ability to integrate information, verify information, and perform manual verification, we believe it is a more scalable construction process.

\subsection{Dataset Statistics}
Figure \ref{tab:dataset_statistics} presents the statistics of ChineseEcomQA. With a total of 1,800 samples, ChineseSimpleQA \cite{he2024chinesesimpleqachinesefactuality} relatively evenly distributed 10 sub-concept types. Furthermore, the average length of reference answers is 18.26. The concise and unified format, consistent with the articles in the SimpleQA series~\cite{wei2024measuring,he2024chinese}, has the advantages of easy-to-evaluate and relatively low evaluation cost.

% \begin{table}[h]
% \centering
% \begin{tabular}{@{} l r @{}} % 用于取消左右两边的默认列间距
% \toprule
% \toprule
% \textbf{Statistics} & \textbf{Number} \\
% \midrule
% \textbf{Total Number} & 1,800 \\
% - Industry Categorization & 286 \\
% - Industry Concept & 111 \\
% - Category Concept & 194 \\
% - Brand Concept & 170 \\
% - Attribute Concept & 200 \\
% - Spoken Concept & 200 \\
% - Intent Concept& 90 \\
% - Review Concept& 71 \\
% - Relevance Concept& 305 \\
% - Personalized Concept& 173 \\
% \midrule
% \multicolumn{2}{@{}l}{\textbf{Average Length}} \\
% - Question Length & xx \\
% - Reference Answer Length & xx \\
% \bottomrule
% \bottomrule
% \end{tabular}
% \vspace{3mm}
% \caption{
%     Dataset statistics of ChineseEcomQA.
% }
%     \label{tab:dataset_statistics}
% \end{table}

\begin{figure}[t]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=0.95\linewidth]{pics/domain_pie.png}
    % \vspace{-4mm}
    \caption{
    Dataset statistics of ChineseEcomQA.
    }
    \label{tab:dataset_statistics}
    % \vspace{-4mm}
    % \captionsetup{belowskip=-20pt}
\end{figure}

\subsection{Evaluation Metrics}
Given reference answer, LLM-as-a-judge is an effective and prevalence method \cite{gu2024survey}. Based on the questions, candidate answers, and reference answers, we use GPT-4o, Claude-3.5-Sonnet and Deepseek-V3 \cite{liu2024deepseek} as the judge models. The final judgment is determined by the voting results of three LLMs. There are three types of evaluation criteria: (1) Correct: The candidate answer fully includes the reference answer without introducing any contradictory elements. (2) Wrong: The candidate answer contains factual statements that contradict the reference answer. (3) Not Attempted: The LLM is not confident enough to provide specific answers, and there are no statements in the response that contradict the reference answer. The evaluation prompt can be found in the Appendix \ref{sec:appendix B}. In the results section that follows, we use the proportion of "Correct" answers as our accuracy metric.
