% \begin{table*}
% \centering
%     \begin{tabular}{cccccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Models}&
%         \multicolumn{3}{|c}{Zero-shot} & \multicolumn{3}{|c|}{Few-shot} & \multirow{2}{*}{Overall Avg.}\cr
%         & Common & Abstract & Avg. & Common & Abstract  & Avg. \cr
%         \cmidrule(lr){1-8}
        
%         ChatGLM3-6B     & 38.24 & 29.61 & 34.27 & 37.14 & 32.43 & 34.98 & 34.63 \cr
%         Gemini-1.5-pro  & 45.87 & 45.27 & 45.59 & 40.65 & 31.20 & 36.31 & 40.95 \cr
%         Claude3         & 53.83 & 50.00 & 52.09 & 53.04 & 46.80 & 50.18 & 51.14 \cr
%         % Baichuan2-13B   & & & & & & & \cr
%         Yi-1.5-6B       & 42.10 & 48.34 & 45.01 & 40.43 & 42.71 & 41.48 & 43.25\cr
%         Yi-1.5-34B      & 54.13 & 52.94 & 53.58 & 58.48 & 50.64 & 54.88 & 54.23\cr
%         Llama3-8B       & 32.61 & 37.34 & 34.78 & 45.87 & 53.96 & 49.59 & 42.19 \cr
%         Llama3-70B      & 57.98 & 46.94 & 52.02 & 51.36 & 59.71 & 55.25 & 53.64 \cr
%         % Qwen2-7B        & 48.91 & 45.01 & 47.12 & 59.13 & 51.41 & 55.58 & 51.35 \cr
%         Qwen2-7B        & 60.43 & 56.27 & 58.52 & 58.91 & 50.90 & 55.23 & 56.88 \cr
%         Qwen2-72B       & 68.48 & 58.97 & 64.12 & \textbf{68.85} & 62.15 & 65.76 & 64.94 \cr
%         Qwen2-max       & \textbf{70.65} & 62.66 & \textbf{66.98} & 68.04 & 64.96 & 66.63 & \textbf{66.81} \cr
%         GPT-4 turbo     & 62.83 & 60.87 & 61.93 & 66.52 & 65.47 & 66.04 & 63.99 \cr
%         GPT-4           & 63.91 & \textbf{67.77} & 65.69 & 67.61 & \textbf{67.26} & \textbf{67.45} & 66.57 \cr
%         \bottomrule
%     \end{tabular}
%   \caption{\label{tab:overall}
%     Results of evaluating LLMs on ECKGBench dataset. Each column represents a single evaluation test that leverages one prompting approach (i.e., zero-shot or few-shot) in one e-commerce dimension (i.e., common or abstract). 
%   }
% \end{table*}

% \begin{table*}
% \centering
%     \begin{tabular}{cccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Models}&
%         \multicolumn{2}{|c}{common} & \multicolumn{2}{|c|}{abstract} & \multirow{2}{*}{Avg.}\cr
%         & zero-shot & few-shot  & zero-shot & few-shot \cr
%         \cmidrule(lr){1-6}

%         Chatglm3-6B & 38.24 & 37.14 & 29.61 & 32.43 & 34.49 \cr
%         Gemini-1.5-pro & 41.11 & 36.90 & 39.29 & 27.23 & 36.25 \cr
%         Claude3 & 48.65 & 48.57 & 43.62 & 40.33 & 45.44 \cr
%         % \cmidrule(lr){1-8}
%         % claude3 & & & & & & & \cr
%         LLamma3-70B & 44.98 & 48.02 & 40.94 & 50.58 & 46.83 \cr
%         Qwen2-7B & 61.19 & 61.69 & 48.96 & 52.18 & 56.23 \cr
%         Qwen2-72B & 61.19 & 61.69 & 48.96 & 52.18 & 56.23 \cr
%         Qwen2-max & 62.14 & 61.19 & 50.94 & 54.05 & 57.27 \cr
%         GPT-4 & 56.21 & 59.85 & 55.09 & 56.13 & 56.87  \cr
%         GPT-4 turbo & 55.83 & 59.85 & 50.73 & 54.05 & 54.93  \cr

%         \bottomrule
%     \end{tabular}
%   \caption{\label{tab:overall}
%     Results of different LLMs in ECKGBench dataset. Each column represents a single evaluation in one e-commerce dimension and leveraging one prompting approach (i.e., zero-shot or few-shot). We first report the accuracy of each evaluation separately. Then, we report the Avg., which is the weighted mean of all accuracies.  
%   }
% \end{table*}

\section{Experiments}
% We evaluate the capabilit of advanced LLMs in e-commerce and the contribution of each component in question generation.
% We aim to answer the following research questions:
% \begin{itemize}[leftmargin=*]
% \item \textbf{RQ1}: What is the overall performance of advanced LLMs in e-commerce domain?
% \item \textbf{RQ2}: How is the contribution of each component?
% \item \textbf{RQ3}: How to explore the knowledge boundary of LLMs?
% \item \textbf{RQ4}: How is the efficiency of evaluation in ECKGBench?
% \end{itemize}

\subsection{Experimental Setup}

\subsubsection{Models}
In ECKGBench\footnote{\url{https://github.com/ming429778/ECKGBench}}, we assess the e-commerce capabilities of existing advanced LLMs, including five close-sourced LLMs, i.e., Gemini-1.5-pro, GPT-4 turbo/GPT-4~\citep{OpenAI2023GPT4}, Claude-3, Qwen2-max~\cite{yang2024qwen2}, and seven open-sourced LLMs, i.e., ChatGLM3-6B~\cite{du2021glm}, Yi-1.5-6B/Yi-1.5-34B~\citep{young2024yi}, Llama3-8B/Llama3-70B~\citep{touvron2023llama}, Qwen2-7B/Qwen2-72B~\cite{yang2024qwen2}. We apply the API for large-scale generation for close-sourced LLMs. For open-sourced LLMs, we download and evaluate the model from Huggingface\footnote{\url{https://huggingface.co}}. Notice that we use \textbf{fine-tuned models} (e.g., Chat, Instruct version) for fairness. 
To comprehensively evaluate LLMs' capabilities, we employ two prompting approaches: \textbf{zero-shot} and \textbf{few-shot} settings in two dimensions of e-commerce knowledge: \textbf{common} knowledge and \textbf{abstract} knowledge. 

% In ECKGBench\footnote{\url{https://anonymous.4open.science/r/ECKGBench-F0DE/}.}, we assess the e-commerce capabilities of existing advanced LLMs, including five close-sourced LLMs, i.e., Gemini-1.5-pro, GPT-4 turbo/GPT-4~\citep{OpenAI2023GPT4} and Claude-3, Qwen2-max~\cite{luo2023reasoning}, and seven open-sourced LLMs, i.e., chatglm3-6B~\cite{du2021glm}, Yi-1.5-6B/Yi-1.5-34B~\citep{young2024yi}, LLama3-8B/LLama3-70B~\citep{touvron2023llama}, Qwen2-7B/Qwen2-72B~\cite{luo2023reasoning,qwen2}. Following the default parameters, we apply the API for large-scale generation for close-sourced LLMs. For open-sourced LLMs, we download and test the model from Huggingface. Notice that we use fine-tuned models (e.g., Chat, Instruct) in RQ1 for fairness. In contrast, the base models are used in RQ3 to explore the knowledge boundary.

% \subsubsection{Metrics}
% We evaluated \textit{Accuracy} in RQ1. The accuracy depends on whether the generation of LLMs conforms to the ground truth. 
% In RQ2, we assess the \textit{Quality} of datasets using LLM or human assistance. 
% Moreover, we define \textit{Inconsistency} to evaluate the reliability: measure whether the model's answers to two questions are inconsistent, where the questions are generated by fixing KG triples and conducting negative sampling twice (i.e., two questions have different false choices).
% In RQ3, we use metrics (\textit{SC, Precsion, Recall}) defined in Table~\ref{tab:know}.

% \subsubsection{Settings}
% To comprehensively evaluate LLMs' capabilities, we employ two prompting approaches: zero-shot and few-shot settings in two dimensions of e-commerce knowledge: common knowledge and abstract knowledge. 

% To comprehensively evaluate the capabilities of large language models (LLMs), we employ two prompting approaches: zero-shot and few-shot settings in two dimensions of e-commerce knowledge: common knowledge and abstract knowledge. We create a series of distinctive evaluation settings by combining this prompting approach with the knowledge dimensions. This allows for a nuanced assessment of each model's performance across various scenarios.

\subsection{Overall Comparison}

We evaluate several advanced LLMs in the ECKGBench and report the overall performances in Table~\ref{tab:overall}. We can draw some interesting observations from the results:
\begin{itemize}[leftmargin=*]
\item In general, ECKGBench discriminates capabilities well between different models, demonstrating its evaluation effectiveness. In most cases, LLMs have a low absolute accuracy value, showing that the capabilities of LLMs in e-commerce are still unsatisfactory, which is reasonable since some specialized concepts in e-commerce barely exist in the pre-training corpus of LLMs. Notably, Qwen2-max and GPT-4 perform well in most cases. 
\item Regarding evaluation dimensions, LLMs perform significantly better in common knowledge than abstract knowledge. In contrast, the prompting methods do not significantly influence the performance, demonstrating the ECKGBench's high-quality questions and fine-tuned LLMs' good instruction-following capability.
\item We find that the scaling law of LLMs still holds in the e-commerce domain. LLMs with larger scales perform better than ones with more minor scales. We explain that even though some e-commerce concepts are rare in the world corpus, the large-scale LLM can repeat them several times in its pre-training corpus and strictly remember them, leveraging its powerful ability. 
\end{itemize}

% We evaluate several advanced LLMs in ECKGBench and report the results in Table~\ref{tab:overall}. We draw some interesting observations:
% (1) In general, ECKGBench discriminates capabilities well between different models, demonstrating its evaluation effectiveness. 
% In most cases, LLMs have a low absolute accuracy value, showing that the capabilities of LLMs in e-commerce are still unsatisfactory, which is reasonable since some specialized concepts in e-commerce barely exist in the pre-training corpus of LLMs. Notably, Qwen2-max and GPT-4 perform well in most cases. 
% (2) Regarding evaluation dimensions, LLMs perform significantly better in common knowledge than abstract knowledge. In contrast, the prompting methods do not significantly influence the performance, demonstrating the ECKGBench's high-quality questions and good instruction-following capability of fine-tuned LLMs. 
% (3) We find that the scaling law of LLMs still holds in the e-commerce domain. We explain that even though some e-commerce concepts are rare in the world corpus, the large-scale LLM can repeat them several times in its pre-training corpus and strictly remember them, leveraging its powerful ability. 

% We evaluate several advanced LLMs in the ECKGBench and report the overall performances in Table~\ref{tab:overall}. We can draw some interesting observations from the results:
% \begin{itemize}[leftmargin=*]
% \item In general, LLMs have a low absolute value of accuracy score (e.g., less than $60$) in most cases. The results demonstrate that the capability of LLMs in the e-commerce domain is still unsatisfactory, which is reasonable since e-commerce scenarios are complicated and some specialized concepts in e-commerce barely exist in the pre-training corpus of LLMs. Notably, Qwen2-max and GPT-4 perform well in most cases. 
% \item Regarding two knowledge dimensions in e-commerce, LLMs perform significantly better in common knowledge than abstract knowledge, consistent with the previous analysis. In contrast, the prompting methods do not significantly influence the performance, demonstrating the ECKGBench's high-quality questions and good instruction-following capability of fine-tuned models. 
% \item Then, we find that the scaling law of LLMs still holds in the e-commerce domain. LLMs with larger scales perform better than ones with more minor scales. We explain that even though some e-commerce concepts are rare in the world corpus, the large-scale LLM can repeat them several times in its pre-training corpus and strictly remember them, leveraging its powerful ability. 
% \end{itemize}


\subsection{Ablation Study}

\subsubsection{Quality evaluation}

We conduct an ablation study with three variants of the question generation method, including (1) \textit{w/o prompt designing}: without designing question prompts, i.e., asking LLMs to complete the KG triple directly; (2) \textit{w/o relation templating}: without leveraging relation templates; and (3) \textit{w/o negative sampling}: without using the proposed sampling pipeline, i.e., using random sampling.
We assess the question quality using both LLM and human evaluations and report the results in Table~\ref{tab:ablation}. The results demonstrate that each component is indispensable. The prompt design contributes the most, making questions conform to natural language and activating LLMs' parameterized knowledge.

% We conduct an ablation with three variants of the question generation, including (1) \textit{w/o prompt designing}: without designing question prompts. (2) \textit{w/o relation templating}: without leveraging relation templates. (3) \textit{w/o negative sampling}: without using the proposed pipeline (i.e., using random sampling). We assess the question quality using LLM and human evaluations and report the results in Table~\ref{tab:ablation}, demonstrating that each component is indispensable and the prompt contributes the most.

% We conduct an ablation study with three variants of the proposed question generation method, including (1) \textit{w/o prompt designing}: without designing question prompts. (2) \textit{w/o relation template}: without leveraging relation templates. (3) \textit{w/o negative sampling}: without using the proposed pipeline (i.e., using random sampling). We assess the quality of datasets using LLM and human evaluations.
% We report the results in Table~\ref{tab:ablation}, demonstrating that each component is indispensable. The prompt design contributes the most to the quality, as it helps make questions conform to natural language and guides LLMs to leverage the parameterized knowledge. 
 
\subsubsection{Consistency evaluation}
We assess the impact of the sampling pipeline on the consistency of answers provided by LLMs. We define inconsistency such that LLMs provide conflicting answers, given two questions generated from the same KG triple using negative sampling twice. Our results indicate that the proposed sampling pipeline effectively reduces this inconsistency. In contrast to the high inconsistency rate (exceeding $20\%$) associated with random sampling, our method produces questions that prompt LLMs to deliver more consistent responses, with inconsistency rates falling below $5\%$ for most cases.

% We evaluate how the sampling pipeline affects LLMs' answer consistency. We define inconsistency such that LLMs provide conflicting answers, given two questions generated from the same KG triple using negative sampling twice. Our results show that our proposed sampling pipeline effectively reduces this inconsistency. Compared to the high inconsistency rate (above $20\%$) of random sampling, LLMs give more consistent answers to questions generated by our method, with an inconsistency rate below $5\%$ for most cases.

% We evaluate how the sampling pipeline affects LLMs' answer consistency. Inconsistency occurs when LLMs provide conflicting answers to questions generated from the same sample using negative sampling twice. Our results show that our sampling pipeline effectively reduces this inconsistency. Compared to the high inconsistency rate (above $20\%$) of random sampling, LLMs give more consistent answers to questions generated by our method, with an inconsistency rate below $5\%$ for most cases.

% \vspace{-2mm}  % 在正文和表格之前减少间距
\begin{table}[t]
    \centering
    % \vspace{-2mm}
    \begin{tabular}{ccc}
        \toprule
        Variants & GPT-4 eval & Human eval \\
        \cmidrule(lr){1-3}
        original & \textbf{0.6104} & \textbf{0.92} \\
        w/o prompt designing & 0.2074 & 0.34 \\
        w/o relation templating & 0.3590 & 0.51 \\
        w/o negative sampling & 0.5410 & 0.85 \\
        \bottomrule
    \end{tabular}
    % \vspace{-2mm} % 在表格和标题之间减少间距
    \caption{Ablation of question generation.}
    \vspace{-4mm} % 在标题后减少间距
    \label{tab:ablation}
\end{table}
% \vspace{-2mm} % 在表格和下一个正文之间减少间距

% Moreover, we define \textit{Inconsistency} to evaluate the reliability: measure whether the model's answers to two questions are inconsistent, where the questions are generated by fixing KG triples and conducting negative sampling twice (i.e., two questions have different false choices).

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{pics/Ablation.pdf}
%     \caption{
%     Results of inconsistency rates. The lower, the better. The green and blue represent the random sampling and our sampling methods, respectively.  
%     }
%     \label{fig:ablation}
% \end{figure}


% \begin{table*}
% \centering
%     \begin{tabular}{cccccccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Temperatures}&
%         \multicolumn{3}{|c}{{Precision@5}} & \multicolumn{3}{|c}{Recall@5} & \multicolumn{3}{|c}{SC@5}\cr
%         & Common & Abstract & Avg. & Common & Abstract & Avg. & Common & Abstract & Avg. \cr
%         \cmidrule(lr){1-10}
% $T=0.01$   & 50.22 & 43.58 & 47.17 & 50.65 & 44.25 & 47.71 & 49.78 & 43.22 & 46.77 \cr
% $T=0.1$ & 49.61 & 43.63 & 46.86 & 55.65 & 49.10 & 52.64 & 43.26 & 36.06 & 39.95 \cr
% $T=0.2$ & 50.00 & 42.97 & 46.77 & 61.96 & 53.96 & 58.28 & 38.91 & 30.43 & 35.02 \cr
% $T=0.3$ & 50.26 & 42.25 & 46.58 & 66.30 & 60.36 & 63.57 & 32.39 & 28.64 & 30.67 \cr
% $T=0.5$ & 46.57 & 41.84 & 44.39 & 70.65 & 65.47 & 68.27 & 23.26 & 17.90 & 20.80 \cr
%         \bottomrule
%     \end{tabular}
%   \caption{\label{tab:rq3}
%      Results of exploring knowledge boundary of Qwen-7B. We report Precision, Recall, and SC in different temperatures. 
%   }
% \end{table*}

% \subsection{Exploring Knowledge Boundary (RQ3)}
% Then, we explore the knowledge boundary of Qwen-7B. Notice that we use the base model here. In Table~\ref{tab:rq3}, we report the previously defined metrics SC@5, Precision@5, and Recall@5, tuning the model temperature in search range $\{0.01, 0.1, 0.2, 0.3, 0.5\}$.
% We observe that due to model generation being encouraged to be diversified by increasing temperatures,  the precision gets slightly reduced, and the Recall and SC (strict correct) increase and reduce significantly. 
% Determining reference temperature $T_0$ is empirical, with often a small value such as $0.2$ or $0.3$. 
% Here, we set $T_0=0.2$, as in which the Recall is $58.28$, close to the Accuracy $58.52$ of its fine-tuned version (Qwen-7B-Instruct), conforming to our analysis that the Recall reflects the potential knowledge boundary, which post-training methods can reach. We can also know the essential boundary of the model according to SC value $35.02$, inside which the knowledge is sufficiently trained.


% \begin{table*}[t]
% \centering
% % \resizebox{\linewidth}{!}
% {
%     \begin{tabular}{ccccccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Model}&
%         \multicolumn{4}{|c}{{Zero-shot}} & \multicolumn{4}{|c}{Few-shot} \cr
%         & Input & Output & Avg. RT & Max. RT & Input & Output & Avg. RT & Max. RT \cr
%         \cmidrule(lr){1-9}
% Gemini    & 91.21  & 87.63 & 2.80  & 5.76 & 225.21 & 45.56 & 2.36  & 6.22\cr
% Claude3   & 142.77 & 30.69 & 18.06 & 29.97& 319.26 & 95.40 & 17.83 & 29.51\cr
% Llama3-70B& 120.38 & 26.42 & 2.09  & 8.63 & 268.02 & 4.84  & 0.88  & 6.92\cr
% Qwen2-72B & 93.55  & 12.24 & 1.05  & 9.72 & 236.55 & 11.00 & 1.76  & 41.77\cr
% Qwen2-max & 108.07 & 2.68  & 0.71  & 4.51 & 233.55 & 2.48  & 0.72  & 3.73\cr
% GPT-4     & 147.38 & 6.53  & 1.74  & 5.56 & 332.38 & 4.95  & 1.61  & 7.42 \cr
%         \cmidrule(lr){1-9}
% Avg.      & 117.23 & 27.70 & 4.41 & 10.69 & 269.16 & 27.37 & 4.19 & 16.00 \cr
%         \bottomrule
%     \end{tabular}}
    
%   \caption{\label{tab:rq4}
%      Results of efficiency test. We report the average token number of inputs and outputs and the average response time (Avg. RT). In addition, we record the maximum response time (Max. RT), which is decisive for the time cost of batch API calls. 
%   }
% \end{table*}

% \subsection{Efficiency Study (RQ4)}
% Efficiency is critical in evaluating LLMs due to the high costs associated with their inference. In this context, we examine the efficiency of the proposed ECKGBench. The results are presented in Table~\ref{tab:rq4}, illustrating the effectiveness of our evaluation framework in reducing cost.
% Notably, in most cases, the average response times under zero-shot and few-shot settings are below 3 seconds, with maximum response times capped at under 10 seconds. This rapid assessment significantly enhances the usability of ECKGBench without sacrificing reliability. Moreover, our question refinement effectively controls the input and output token numbers, contributing to overall efficiency. 
