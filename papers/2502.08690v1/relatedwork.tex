\section{Related Works}
\label{related_works}
\subsection{Efficient diffusion model}
As diffusion generative models scale, T2I synthesis has achieved impressive results in generating high-fidelity, text-aligned images. However, this advancement comes with significant computational and memory overhead. Previous research has primarily focused on optimizing the efficiency of the denoising module through methods such as knowledge distillation~\cite{li2024snapfusion, song2024multi, castells2024edgefusion, zhao2025mobilediffusion, kim2025bk}, pruning model weights~\cite{fang2023structural, ganjdanesh2024not, wang2024patch, castells2024ld, lee2024dit}, and quantization of weights to lower precision bits~\cite{li2023q, he2024ptqd, li2024svdqunat, wang2024quest, ryu2025dgq}. While these approaches effectively reduce the computational and memory costs of the pipeline, they overlook the substantial memory burden imposed by the text encoder, which remains largely underexplored. To address this gap, we propose a targeted pruning strategy for the text encoder of a T2I diffusion model, which allows more memory efficient T2I synthesis with comparable performance.

\subsection{Blockwise pruning for LLMs}
Although LLMs show promising performance in various tasks, their size often limits practical deployment. To address this problem, model compression techniques have been developed, with pruning emerging as a promising solution due to its ability to reduce the parameter count with minimal retraining. Notably, blockwise pruning of transformers~\cite{men2024shortgpt, yang2024laco, zhang2024finercut} effectively reduces parameters while preserving performance.

ShortGPT~\cite{men2024shortgpt} proposed a method to prune blocks to the desired sparsity by removing them one by one and assigning a Block Influence (BI) score based on changes in cosine similarity and pruning those with lower BI first. LaCo~\cite{yang2024laco} introduced a strategy to reduce the size of LLMs by merging the weights of adjacent transformer blocks, effectively compressing the model. FinerCut~\cite{zhang2024finercut} refined this approach by pruning sub-blocks, composed of Multi-Head Attention (MHA) and Feed-Forward Network (FFN) layers with normalization, in a fine-grained manner. It sequentially pruned sub-blocks while partially considering interactions between blocks, %thereby 
reducing the performance gap with the dense model.

\begin{figure*}[!t]
  \centering
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=1\linewidth]{figures/overall_framework_a.pdf}
    \caption{Illustration of Skip phase.}
    \label{fig:overall_framework_skip}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\linewidth}
    \includegraphics[width=1\linewidth]{figures/overall_framework_b.pdf}
    \caption{Illustration of Re-use phase.}
    \label{fig:overall_framework_reuse}
  \end{subfigure}
  \vspace{-1em}
  \caption{
The visualization of overall framework of Skrr. \textbf{(a)} shows the \textit{Skip} phase, which repeatedly assesses each sub-block by determining the output discrepancy (Disc.) between the dense and skipped models using a calibration dataset (Calib. data). To account for block interactions, it keeps the top $k$ options with the smallest discrepancies and uses beam search for refined selection. \textbf{(b)} presents the \textit{Re-use} phase, evaluating if recycling remaining block instead of skipped sub-blocks results in a smaller output discrepancy. If so, hidden states are fed back into the chosen layers. This two-phase approach efficiently reduces model size with minimal T2I performance loss.}
  \label{fig:overall_framework}
\vspace{-1em}
\end{figure*}

Despite these advances, text encoder compression in diffusion models remains underexplored. To address this discrepancy, we propose Skrr, a blockwise text encoder pruning approach tailored for T2I diffusion models. We evaluated Skrr against existing LLM-based pruning techniques, demonstrating its effectiveness in reducing the size of the model while maintaining the T2I performance of its dense model.