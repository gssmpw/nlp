@article{yang2024laco,
  title={Laco: Large language model pruning via layer collapse},
  author={Yang, Yifei and Cao, Zouying and Zhao, Hai},
  journal={arXiv preprint arXiv:2402.11187},
  year={2024}
}

@inproceedings{chen2025pixart,
  title={PIXART-Sigma: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation},
  author={Chen, Junsong and Ge, Chongjian and Xie, Enze and Wu, Yue and Yao, Lewei and Ren, Xiaozhe and Wang, Zhongdao and Luo, Ping and Lu, Huchuan and Li, Zhenguo},
  booktitle={ECCV},
  pages={74--91},
  year={2024},
  organization={Springer}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={ICML},
  year={2024},
  organization={PMLR}
}

@article{zhang2024finercut,
  title={FinerCut: Finer-grained Interpretable Layer Pruning for Large Language Models},
  author={Zhang, Yang and Li, Yawei and Wang, Xinpeng and Shen, Qianli and Plank, Barbara and Bischl, Bernd and Rezaei, Mina and Kawaguchi, Kenji},
  journal={arXiv preprint arXiv:2405.18218},
  year={2024}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={NeurIPS},
  volume={30},
  year={2017}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{fu2023dreamsim,
  title={Dreamsim: Learning new dimensions of human visual similarity using synthetic data},
  author={Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},
  journal={arXiv preprint arXiv:2306.09344},
  year={2023}
}

@article{ghosh2024geneval,
  title={Geneval: An object-focused framework for evaluating text-to-image alignment},
  author={Ghosh, Dhruba and Hajishirzi, Hannaneh and Schmidt, Ludwig},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={CVPR},
  pages={2818--2826},
  year={2016}
}

@inproceedings{changpinyo2021conceptual,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={CVPR},
  pages={3558--3568},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{li2024snapfusion,
  title={Snapfusion: Text-to-image diffusion model on mobile devices within two seconds},
  author={Li, Yanyu and Wang, Huan and Jin, Qing and Hu, Ju and Chemerys, Pavlo and Fu, Yun and Wang, Yanzhi and Tulyakov, Sergey and Ren, Jian},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@article{song2024multi,
  title={Multi-student Diffusion Distillation for Better One-step Generators},
  author={Song, Yanke and Lorraine, Jonathan and Nie, Weili and Kreis, Karsten and Lucas, James},
  journal={arXiv preprint arXiv:2410.23274},
  year={2024}
}

@article{castells2024edgefusion,
  title={EdgeFusion: On-Device Text-to-Image Generation},
  author={Castells, Thibault and Song, Hyoung-Kyu and Piao, Tairen and Choi, Shinkook and Kim, Bo-Kyeong and Yim, Hanyoung and Lee, Changgwun and Kim, Jae Gon and Kim, Tae-Ho},
  journal={arXiv preprint arXiv:2404.11925},
  year={2024}
}

@inproceedings{zhao2025mobilediffusion,
  title={Mobilediffusion: Instant text-to-image generation on mobile devices},
  author={Zhao, Yang and Xu, Yanwu and Xiao, Zhisheng and Jia, Haolin and Hou, Tingbo},
  booktitle={ECCV},
  pages={225--242},
  year={2024},
  organization={Springer}
}

@inproceedings{kim2025bk,
  title={Bk-sdm: A lightweight, fast, and cheap version of stable diffusion},
  author={Kim, Bo-Kyeong and Song, Hyoung-Kyu and Castells, Thibault and Choi, Shinkook},
  booktitle={ECCV},
  pages={381--399},
  year={2024},
  organization={Springer}
}

@article{ganjdanesh2024not,
  title={Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image Diffusion Models},
  author={Ganjdanesh, Alireza and Shirkavand, Reza and Gao, Shangqian and Huang, Heng},
  journal={arXiv preprint arXiv:2406.12042},
  year={2024}
}

@article{wang2024patch,
  title={Patch diffusion: Faster and more data-efficient training of diffusion models},
  author={Wang, Zhendong and Jiang, Yifan and Zheng, Huangjie and Wang, Peihao and He, Pengcheng and Wang, Zhangyang and Chen, Weizhu and Zhou, Mingyuan and others},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@inproceedings{li2023q,
  title={Q-diffusion: Quantizing diffusion models},
  author={Li, Xiuyu and Liu, Yijiang and Lian, Long and Yang, Huanrui and Dong, Zhen and Kang, Daniel and Zhang, Shanghang and Keutzer, Kurt},
  booktitle={CVPR},
  pages={17535--17545},
  year={2023}
}

@article{he2024ptqd,
  title={Ptqd: Accurate post-training quantization for diffusion models},
  author={He, Yefei and Liu, Luping and Liu, Jing and Wu, Weijia and Zhou, Hong and Zhuang, Bohan},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@inproceedings{castells2024ld,
  title={LD-Pruner: Efficient Pruning of Latent Diffusion Models using Task-Agnostic Insights},
  author={Castells, Thibault and Song, Hyoung-Kyu and Kim, Bo-Kyeong and Choi, Shinkook},
  booktitle={CVPR},
  pages={821--830},
  year={2024}
}

@article{ryu2025dgq,
  title={DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models},
  author={Ryu, Hyogon and Park, NaHyeon and Shim, Hyunjung},
  journal={arXiv preprint arXiv:2501.04304},
  year={2025}
}

@inproceedings{lee2024dit,
  title={DiT-Pruner: Pruning Diffusion Transformer Models for Text-to-Image Synthesis Using Human Preference Scores},
  author={Lee, Youngwan and Lee, Yong-Ju and Hwang, Sung Ju},
  booktitle={ECCVW},
  pages={1--9},
  year={2024}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  pages={10684--10695},
  year={2022}
}

@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}

@inproceedings{brooks2023instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle={CVPR},
  pages={18392--18402},
  year={2023}
}

@inproceedings{kawar2023imagic,
  title={Imagic: Text-based real image editing with diffusion models},
  author={Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},
  booktitle={CVPR},
  pages={6007--6017},
  year={2023}
}

@inproceedings{cao2023masactrl,
  title={Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing},
  author={Cao, Mingdeng and Wang, Xintao and Qi, Zhongang and Shan, Ying and Qie, Xiaohu and Zheng, Yinqiang},
  booktitle={CVPR},
  pages={22560--22570},
  year={2023}
}

@article{seo2023ditto,
  title={Ditto-nerf: Diffusion-based iterative text to omni-directional 3d model},
  author={Seo, Hoigi and Kim, Hayeon and Kim, Gwanghyun and Chun, Se Young},
  journal={arXiv preprint arXiv:2304.02827},
  year={2023}
}

@article{poole2022dreamfusion,
  title={Dreamfusion: Text-to-3d using 2d diffusion},
  author={Poole, Ben and Jain, Ajay and Barron, Jonathan T and Mildenhall, Ben},
  journal={arXiv preprint arXiv:2209.14988},
  year={2022}
}

@inproceedings{liu2023zero,
  title={Zero-1-to-3: Zero-shot one image to 3d object},
  author={Liu, Ruoshi and Wu, Rundi and Van Hoorick, Basile and Tokmakov, Pavel and Zakharov, Sergey and Vondrick, Carl},
  booktitle={CVPR},
  pages={9298--9309},
  year={2023}
}

@article{wang2024prolificdreamer,
  title={Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation},
  author={Wang, Zhengyi and Lu, Cheng and Wang, Yikai and Bao, Fan and Li, Chongxuan and Su, Hang and Zhu, Jun},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@article{polyak2024movie,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{liu2402sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={URL https://arxiv. org/abs/2402.17177},
  year={2024}
}

@inproceedings{jiao2020tinybert,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  booktitle={EMNLP},
  pages={4163--4174},
  year={2020}
}

@article{huang2023towards,
  title={Towards efficient pre-trained language model via feature correlation distillation},
  author={Huang, Kun and Guo, Xin and Wang, Meng},
  journal={NeurIPS},
  volume={36},
  pages={16114--16128},
  year={2023}
}

@inproceedings{hsieh2023distilling,
  title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alex and Krishna, Ranjay and Lee, Chen-Yu and Pfister, Tomas},
  booktitle={ACL},
  pages={8003--8017},
  year={2023}
}

@article{ko2024distillm,
  title={Distillm: Towards streamlined distillation for large language models},
  author={Ko, Jongwoo and Kim, Sungnyun and Chen, Tianyi and Yun, Se-Young},
  journal={arXiv preprint arXiv:2402.03898},
  year={2024}
}

@inproceedings{xiao2023smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  booktitle={ICML},
  pages={38087--38099},
  year={2023},
  organization={PMLR}
}

@article{lin2024awq,
  title={AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  journal={MLSys},
  volume={6},
  pages={87--100},
  year={2024}
}

@article{ashkboos2024quarot,
  title={Quarot: Outlier-free 4-bit inference in rotated llms},
  author={Ashkboos, Saleh and Mohtashami, Amirkeivan and Croci, Maximilian L and Li, Bo and Cameron, Pashmina and Jaggi, Martin and Alistarh, Dan and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2404.00456},
  year={2024}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@inproceedings{song2024sleb,
  title={SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks},
  author={Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon},
  booktitle={ICML},
year={2024}
}

@article{ashkboos2024slicegpt,
  title={Slicegpt: Compress large language models by deleting rows and columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Nascimento, Marcelo Gennari do and Hoefler, Torsten and Hensman, James},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}

@inproceedings{van2023llm,
  title={The LLM Surgeon},
  author={van der Ouderaa, Tycho FA and Nagel, Markus and Van Baalen, Mart and Blankevoort, Tijmen},
  booktitle={ICLR},
year={2023}
}

@article{gromov2024unreasonable,
  title={The unreasonable ineffectiveness of the deeper layers},
  author={Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and Glorioso, Paolo and Roberts, Daniel A},
  journal={arXiv preprint arXiv:2403.17887},
  year={2024}
}

@inproceedings{ho2021classifier,
  title={Classifier-Free Diffusion Guidance},
  author={Ho, Jonathan and Salimans, Tim},
  booktitle={NeurIPSW},
  year={2021}
}

@inproceedings{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={ICLR},
  year={2016}
}

@misc{cerebras2023slimpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = June,
  year = 2023
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={NeurIPS},
  year={2017}
}

@article{freitag2017beam,
  title={Beam Search Strategies for Neural Machine Translation},
  author={Freitag, Markus and Al-Onaizan, Yaser},
  journal={ACL},
  pages={56},
  year={2017}
}

@article{karras2024guiding,
  title={Guiding a Diffusion Model with a Bad Version of Itself},
  author={Karras, Tero and Aittala, Miika and Kynk{\"a}{\"a}nniemi, Tuomas and Lehtinen, Jaakko and Aila, Timo and Laine, Samuli},
  journal={arXiv preprint arXiv:2406.02507},
  year={2024}
}

@inproceedings{ahn2025self,
  title={Self-rectifying diffusion sampling with perturbed-attention guidance},
  author={Ahn, Donghoon and Cho, Hyoungwon and Min, Jaewon and Jang, Wooseok and Kim, Jungwoo and Kim, SeonHwa and Park, Hyun Hee and Jin, Kyong Hwan and Kim, Seungryong},
  booktitle={ECCV},
  pages={1--17},
  year={2024},
  organization={Springer}
}

@article{sherstinsky2020fundamentals,
  title={Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network},
  author={Sherstinsky, Alex},
  journal={Physica D: Nonlinear Phenomena},
  volume={404},
  pages={132306},
  year={2020},
  publisher={Elsevier}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{fang2023structural,
  title={Structural pruning for diffusion models},
  author={Gongfan Fang and Xinyin Ma and Xinchao Wang},
  booktitle={NeurIPS},
  year={2023},
}

@article{li2024svdqunat,
  title={Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models},
  author={Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song},
  journal={arXiv preprint arXiv:2411.05007},
  year={2024}
}

@article{wang2024quest,
  title={Quest: Low-bit diffusion model quantization via efficient selective finetuning},
  author={Wang, Haoxuan and Shang, Yuzhang and Yuan, Zhihang and Wu, Junyi and Yan, Junchi and Yan, Yan},
  journal={arXiv preprint arXiv:2402.03666},
  year={2024}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={ICCV},
  pages={4195--4205},
  year={2023}
}

@inproceedings{cohen2019empirical,
  title={Empirical analysis of beam search performance degradation in neural sequence models},
  author={Cohen, Eldan and Beck, Christopher},
  booktitle={ICML},
  pages={1290--1299},
  year={2019},
  organization={PMLR}
}

@inproceedings{he2023empirical,
  title={Empirical Analysis of Beam Search Curse and Search Errors with Model Errors in Neural Machine Translation},
  author={He, Jianfei and Sun, Shichao and Jia, Xiaohua and Li, Wenjie},
  booktitle={EAMT},
  pages={91--101},
  year={2023}
}