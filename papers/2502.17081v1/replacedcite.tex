\section{Related Work}
%-------------------------------------------------------------------------------

% \color{red}
% \subsubsection*{Vertical Federated Learning}

% Vertical Federated Learning (VFL) is ..... In general, there are two types of VFL, i.e., Aggregate VFL (AggVFL) and Split VFL (SplitVFL) ____. AggVFL is..... (representative work on AggVFL). SplitVFL is ..... (SplitVFL).

% In this work, we focus on AggVFL .....

\color{black}
Vertical Federated Learning (VFL) is a distributed machine learning paradigm that enables collaboration between multiple parties without sharing sensitive data. In general, there are two types of VFL, i.e., Aggregate VFL (AggVFL) and Split VFL (SplitVFL) ____. AggVFL employs a non-trainable global module (e.g., Sigmoid activation) to aggregate intermediate results through secure computation. Representative work on AggVFL includes SecureBoost ____ for federated gradient boosting trees and SFTL ____ for  cryptographic defense. On the other hand, SplitVFL utilizes trainable global modules aligned with vertical split neural networks ____. This type of VFL has been studied in various works, including privacy-preserving split learning ____ and communication-efficient CELU-VFL ____. In this work, we focus on AggVFL due to its superior compatibility with applications requiring strict data isolation. 

\subsubsection*{Theoretically-Guaranteed Machine Unlearning}

Machine unlearning has attracted considerable attention in recent years. However, most existing research on machine unlearning lacks theoretical guarantees about its efficacy. There are currently two main approaches that provide formal guarantees of complete forgetting. The first approach, exemplified by SISA____, focuses on exact unlearning. These methods____ partition both the model and the training set, performing retraining only on the sub-model relevant to the unlearning request, thereby ensuring complete unlearning. The second approach draws from differential privacy concepts____ and proves that the model post-unlearning is statistically indistinguishable from a retrained model, demonstrating the effect of approximate unlearning, also called \textit{certified unlearning}____.

While the first approach offers exact unlearning, it is inefficient in scenarios where unlearning requests are scatted in many sub-models____. %As such, it is unsuitable for our use case. 
Hence, our proposed framework aligns with the second approach, achieving certified approximate unlearning while ensuring efficiency.

\subsubsection*{Federated Unlearning}

Federated unlearning (FU) can be categorized based on the type of unlearning target____, including client removal, feature removal, and sample removal, among others. Most existing unlearning methods address specific unlearning requests and are not easily interoperable____. However, in real-world applications, the targets for forgetting vary widely. This calls for a unified approach that can handle multiple forgetting requests simultaneously.

Existing research on FU primarily focuses on Horizontal FU (HFU), with few papers addressing Vertical FU (VFU). Deng \textit{et al}____ first introduce a VFU method for Logistic Regression (LR) models and client removal. Zhang \textit{et al}____ propose VFU approach for Gradient Boosted Decision Trees, which can unlearn samples by recalculating split points, or unlearn features by changing the splitting feature. Wang \textit{et al}____ introduce a model-agnostic VFU method that accelerates retraining, but it only addresses client removal. %Unlike models that are retrained from scratch, this does not guarantee complete unlearning. 
Gu \textit{et al}____ propose a VFU method to forget labels through gradient ascent techniques.

%\subsection{Asynchronous Federated Unlearning}

Recent research on FU has acknowledged the significance of asynchronous unlearning, but all on HFU tasks. Su \textit{et al}____ divide the HFL system into multiple small modules based on slicing, enabling asynchronous forgetting across different modules. This method assumes unlearning requests arrive during the system training phase. Gu \textit{et al}____ assume that all clients possess identical models, so that after a client performs a unlearning operation, the updated model can be broadcast to all clients, thus achieving global unlearning. %Both of these studies are based on the concept of HFU, where partial client forgetting does not require coordination with other clients.

VFU presents unique challenges in asynchronous unlearning. In VFU, each client holds only a partial model, and forgetting operations require global coordination among clients. This introduces distinct difficulties for asynchronous forgetting, as the model segments are distributed across clients.

Table \ref{table:relatedwork} provides a comparative summary of our approach and existing VFU methods, emphasizing the distinct contributions of our research.

\begin{table}[t]
\centering
\caption{Vertical Federated Unlearning Research Summary}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Paper} & \textbf{Unlearning Target} & \textbf{Asynchronous} & \textbf{Certified} & \textbf{Model-Agnostic} \\
% \midrule
% \textbf{VFU} & & & & \\
\midrule
____ & Client & & & \\
____ & Feature\&Sample & & & \\
____ & Client & & & \checkmark\\
____ & Label & & & \checkmark\\
 \textbf{Our Work}& Multiple& \checkmark & \checkmark & \checkmark\\
% \midrule
% \textbf{HFU} & & & & \\
% \midrule
% ____ & Sample  & \checkmark & & \checkmark\\
% ____ & Feature & \checkmark & & \checkmark\\
\bottomrule
\end{tabular}
%
}
\label{table:relatedwork}
\end{table}


%-------------------------------------------------------------------------------