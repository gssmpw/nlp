




\begin{table}[htb]
\caption{Zero-shot task accuracy comparison on LLaMA 3.1 8B, OPT 6.7B, and Vicuna 1.5 7B. The best performances are marked in \textbf{bold}, and the runner-up is marked with \underline{underline}. We have applied 20\% sparsity (\textit{i.e.}, pruned seven blocks).} \label{tab:model_generality}
\vskip 0.1in
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lccc@{}}
\toprule
Method & LLaMA 3.1 8B & OPT 6.7B & Vicuna 1.5 7B \\ \midrule
Dense & 74.90 & 62.51 & 70.49 \\ \midrule
FLAP & 50.96 & 46.68 & 51.45\\
SliceGPT & 50.28 & 55.45 & 59.11 \\
SLEB & {\ul 57.24} & {\ul 56.55} & 58.68 \\
Shortened LLaMA & 55.77 & 54.58 & {\ul 59.78} \\ \midrule
PuDDing (Ours) & \textbf{61.93} & \textbf{58.37} & \textbf{60.01} \\ \bottomrule
\end{tabular}%
}
\vskip -0.1in
\end{table}




