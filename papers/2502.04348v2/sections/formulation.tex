\section{Problem Description}\label{sec:formulation}
Inspired by the observations in \cref{sec:observation}, we now formalize the problem of prompt-based depth pruning.

In a nutshell, given some pretrained LLM and a prompt, the goal of the prompt-based depth pruning is to designate which transformer blocks should be removed from the model to generate the most accurate response to the prompt.


More concretely, let $\mathbf{x}$ be the prompt given to the model, and let $\mathbf{W} = (W_1,\ldots,W_d)$ be the weight parameters of a pretrained LLM consisting of $d$ transformer blocks, with $W_i$ indicating the weights of the $i$th block. The prediction quality of this language model is measured by the expected loss between the model output and the ground-truth, \textit{i.e.},
\begin{align}
L(\mathbf{W}) := \mathbb{E}[\ell((\mathbf{x},\mathbf{y});\mathbf{W})],\label{eq:risk}
\end{align}
where $\ell((\cdot,\cdot);\mathbf{W})$ is some loss function which also encapsulates the generative procedure of language model with parameter $\mathbf{W}$ (\textit{e.g.}, perplexity). In static depth pruning, the goal is to find which blocks to prune from the given LLM. More formally, define \textbf{omission set} as a(n unordered) set of transformer block indices
\begin{align}
\mathbf{b} = \{b_1,b_2,\ldots,b_k\} \subseteq \{1,2,\ldots,d\},
\end{align}
which designates which blocks will be omitted from the target LLM. Then, let $\mathbf{W}^{\setminus \mathbf{b}}$ be a sequence of $d-k$ weights, with $b_i$th block eliminated from the $\mathbf{W}$. Then, the static depth pruning aims to solve the minimization
\begin{align}
\min_{\mathbf{b}:|\mathbf{b}| \ge k} L(\mathbf{W}^{\setminus \mathbf{b}}) \label{eq:static_depth_pruning},
\end{align}
given the depth constraint $k$ designated by the operational constraints, such as the desired latency or the peak memory.

\textbf{Prompt-based Depth Pruning.} The problem of prompt-based depth pruning can be described as optimizing the omission set as a function $\hat{\mathbf{b}}( \mathbf{x})$, \textit{i.e.}, solving
\begin{align}
\min_{\hat{\mathbf{b}}(\cdot)} \:\mathbb{E}\big[ \ell((\mathbf{x},\mathbf{y}); \mathbf{W}^{\setminus \hat{\mathbf{b}}(\mathbf{x})})\big] \label{eq:prompt_based_depth_pruning},\\
\mathrm{subject\:to}\quad \mathbf{Pr}\big[|\hat{\mathbf{b}}(\mathbf{x})| \ge k\big] = 1.\nonumber
\end{align}
Note that we are constraining the omission set to have the cardinality greater than $k$ for all $\mathbf{x}$. In other words, the pruned model should always have $d-k$ or less blocks. This is because we mainly consider the peak memory constraint, \textit{i.e.}, the RAM cannot hold more than $d-k$ blocks. Otherwise, one can consider a slightly modified version of the problem \eqref{eq:prompt_based_depth_pruning} with a probabilistic constraint.