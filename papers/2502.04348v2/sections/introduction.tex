\section{Introduction}

%\setlength{\intextsep}{10pt}

Recent advances in large language models (LLMs) have achieved remarkable success in a wide range of natural language processing tasks \cite{brown2020language,touvron2023llama,dubey2024llama}. However, significant computational requirements of LLMs pose challenges in resource-constrained environments, limiting their practicality. For example, LLaMA-3.3-70B needs 140GB of RAM to be loaded in bf16, which is often too big for memory-constrained local devices. Thus, reducing the model size is essential to make LLMs feasible for on-device applications.

Depth pruning is a versatile model compression technique that is particularly effective for on-device scenarios \cite{songsleb,kim2024shortened}. Such methods simply remove several transformer blocks (which we call ``omission set'') from the pretrained model, based on some measures of block importance computed using a small amount of calibration samples. As everything is identical except for the number of blocks, the pruned model is suitable to be deployed on any hardware without tailored supports on low-precision (\textit{e.g.}, integer cores) or fine-grained sparsity (\textit{e.g.}, 2:4 sparsity). Furthermore, as there is no extensive training involved, depth pruning can be easily done in a device-by-device manner for deployment on various devices.

\input{figures/1_intro}

A key limitation of typical depth pruning algorithms is that their pruning decision is \textbf{\textit{static}}, \textit{i.e.}, the same omission set is removed regardless of the query given to the model. While this choice allows one to save storage (\textit{e.g.}, flash drives) by discarding the pruned parameters at the local device, it sacrifices the ability to adapt to various downstream tasks. Indeed, our empirical observations show that pruning some transformer blocks in an LLM may incur significant accuracy degradation on certain tasks, while being highly unnecessary for other tasks (see \cref{sec:observation}).


Can we make dynamic depth pruning decisions to improve the performance on various tasks? This question has not been well studied yet, especially in the context of on-device inference. A recent line of work develops effective \textbf{\textit{dynamic token routing}} mechanisms to save training/inference computation by processing each token with a limited number of transformer blocks \cite{raposo2024mixture,wangd}. However, such methods require all parameters to be loaded on high-speed memories (\textit{e.g.}, on-GPU memory); thus, the methods are appropriate for large-scale server clusters, not for on-device inference with memory constraints.

\textbf{Contribution.} To overcome the limitations, we develop a new \textbf{\textit{prompt-based depth pruning}} approach (\cref{sec:formulation}): In the pre-fill stage, based on the prompt given from the user, a limited number of transformer blocks are selected and loaded to the on-device RAM from the storage drive. This approach does not require a large memory to hold all parameters or highly repeated per-token routing, and thus can effectively accelerate inference on low-memory devices.

A na\"{i}ve way to achieve this goal might be to conduct conventional static depth pruning at each inference, using the given prompt as calibration samples. However, this approach incurs a large latency in running static pruning algorithms in every inference. Furthermore, such a method is likely to fail making a good pruning decision due to the shortage of calibration data, especially in single-batch inference cases common in on-device scenarios.

To this end, we propose a \textit{training-based} method for the prompt-based depth pruning of large langauge models (\cref{sec:method}). Our method, coined \underline{P}rompt-ro\underline{u}ted \underline{D}ynamic \underline{D}epth Prun\underline{ing} (PuDDing), works in two steps. 
\begin{enumerate}[leftmargin=*,topsep=0pt,parsep=0pt,itemsep=1.5pt]
\item \textit{Candidate omission set generation}. We construct a small yet diverse and performant family of omission sets. This is done by drawing multiple splits of calibration data from various task dataset, and then finding an omission set which achieves low loss on each split; here, we use a newly developed task-centric loss instead of perplexity.
\item \textit{Router training.} We train a lightweight router which predicts the appropriate omission set from the given prompt. This is done by generating a training dataset consisting of prompt-loss pairs for each omission set, and training the model to predict the loss from the prompt; routing can be done by choosing the minimum-loss option.
\end{enumerate}

Empirically, we find that the proposed PuDDing enjoys a clear advantage over static depth pruning algorithms, achieving more than 4\%p accuracy increase on zero-shot commonsense reasoning tasks (\cref{sec:experiments}). At the same time, as the algorithm uses the router only once per each prompt, PuDDing enjoys over 1.2$\times$ generation speedup over the dense model, similar to the static depth pruning algorithms.

Our key contributions can be summarized as follows:
\begin{itemize}[leftmargin=*,topsep=0pt,parsep=0pt,itemsep=1.5pt]
\item Our observations reveal that optimal depth pruning decisions may be highly depend on the task given at hand, underscoring the need for task-dependent depth pruning.
\item We consider the task of prompt-based depth pruning for the first time (to our knowledge), and propose a training-based strategy as a solution.
\item Comparing with static depth pruning algorithms, our algorithm achieves a much higher zero-shot accuracies on various tasks, while being competitive in terms of the computational efficiency.
\end{itemize}
