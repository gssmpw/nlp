
\section{Experiment}\label{sec:experiments}

We now empirically validate the performance of the proposed algorithm, PuDDing, on zero-shot tasks against the static depth and width pruning baselines.


\input{tables/maintable_357}
\input{tables/model_generality}







\subsection{Experimental Setup}

\textbf{Models.} We evaluate the proposed method on compressing three popular open-weight language models. As the main model, we use the LLaMA-3.1 model with 8B parameters \citep{dubey2024llama}. In addition, we evaluate on two language models: Vicuna 1.5 with 7B \citep{vicuna2023}, and OPT with 6.7B parameters \citep{zhang2022opt}. We use these models for two reasons. First, the models have an appropriate scale for on-device deployments. Second, all three models consist of 32 transformer blocks, and thus can be compared with the same sparsity criterion.

\textbf{Baselines.} We mainly compare against four recent static depth and width pruning baselines with open source.
\begin{itemize}[leftmargin=*,itemsep=1.5pt,topsep=0pt,parsep=0pt]
\item SLEB \citep{songsleb}: A depth pruning baseline that iteratively selects the omission set based on perplexity.
\item Shortend LLaMA \citep{kim2024shortened}: A depth pruning algorithm which selects the omission set one-shot; here, we compare with the version that uses perplexity as the loss criterion and does not apply LoRA.
\item FLAP \citep{an2024flap}: A retraining-free width pruning algorithm based on structural fluctuation metric.
\item SliceGPT \citep{ashkboosslicegpt}: Another width pruning algorithm based on principal component analysis.
\end{itemize}
In addition, we also compare with ``SLEB per prompt,'' which is simply SLEB which is conducted by using each given prompt as the calibration data. As this option does not work well in general, and requires a long inference time, we only compare on a limited number of scenarios.


\textbf{Dataset: Evaluation.} We evaluate on the test splits of six zero-shot commonsense reasoning tasks: ARC-Challenge and ARC-Easy \citep{clark2018arc}, HellaSwag \citep{zellers2019hellaswag}, PIQA \citep{bisk2020piqa}, WinoGrande \citep{sakaguchi2021winogrande}, and BoolQ \citep{clark2019boolq}.

\textbf{Dataset: Calibration for Baselines.} For the baseline algorithms, we have used the calibration data designated in the original papers. For SLEB, FLAP, and SliceGPT, we have used the WikiText-2 \citep{merity2022pointer}. For Shortened LLaMA, we have used the BookCorpus \citep{zhu2015aligning}.

\textbf{Training.} To generate the candidate omission set for our algorithm, we have used 128 randomly drawn samples from the training splits of five zero-shot commonsense reasoning tasks: ARC-Challenge, ARC-Easy, HellaSwag, PIQA, and WinoGrande. That is, we use total $10$ omission sets (as we use two different losses). For training the router, we have used the full training splits. BoolQ dataset has been left out in order to evaluate the generalization to unseen sets. The router has been trained with AdamW with learning rate $10^{-5}$, weight decay $0.01$, and batch size $32$ for 10 epochs, with 500 warm-up steps. Also, for WinoGrande dataset, we use a tailored data pre-processing procedure; we describe this in detail in \cref{app:winogrande}.

\textbf{Hardware.} We have mainly used NVIDIA RTX 6000 Ada for evaluation and training. In addition, we have used cloud instances of NVIDIA A100 for evaluation. %and Intel Gaudi 2 HPUs for evaluation.

\input{tables/loratable}
\input{tables/outdomain_tab}

\subsection{Main Experiment}

\cref{tab:main_tab} provides a comparison of zero-shot accuracies of the model compression methods, on LLaMA-3.1 8B model. From the table, we observe that PuDDing achieves the highest average accuracy on all sparsity levels tested. Especially when $7$ blocks have been pruned (over 20\% sparsity), the improvement over the best baselines is almost 3\%p.

An interesting observation is the poor performance of ``SLEB per prompt,'' which measures which block to remove on the fly, by using the given prompt as a calibration dataset. In fact, the performance is worse than the vanilla SLEB. We hypothesize that this is because a single prompt usually does not contain enough information to work as a good calibration data. Our training-based strategy circumvents such difficulty by training a router from the data.

Regarding the out-of-distribution generalization, we observe that PuDDing also works well on unseen dataset (BoolQ). PuDDing outperforms all other baselines except for FLAP, which works extraordinarily well on this specific dataset.

In \cref{tab:model_generality}, we provide comparisons on other language models: Vicuna and OPT. We confirm that our algorithm works better than other baselines under this setup as well. 

\subsection{LoRA Fine-tuning}

Next, we compare the performance where we assume that we can recover the accuracies using LoRA \citep{hulora}. For PuDDing, we generate LoRA updates for each omission set (thus total $10$ for these experiments). This requires additional storage space for storing $10$ separate copies of LoRA weights for each omission set. However, this increase only incurs $\sim$2.5\% increase in the total storage space.
For training LoRA weights, we have followed the setup and hyperparameters used for LoRA training in shortened LLaMA \citep{kim2024shortened}; we have used Alpaca dataset \citep{taori2023alpaca} for training, as in the paper.

\cref{tab:lora} provides LoRA fine-tuned results of depth pruning algorithms on zero-shot commonsense reasoning tasks, for LLaMA-3.1-8B pruned to 20\% sparsity. We observe that PuDDing continues to achieve the best performance among all options evaluated. That is, the advantages of prompt-adaptivity also exists after fine-tuning.

\subsection{More Complicated Tasks}

In \cref{tab:complicated}, we compare the performance of various depth pruning algorithms on more complicated tasks, including OpenBookQA \citep{mihaylov2018openbookqa}, MathQA \citep{amini2019mathqa}, and MMLU \citep{hendrycksmmlu}; for MMLU, we take an average on five key tasks: High-school math, physics, medicine, machine learning, and philosophy. From the results, we observe that PuDDing continues to perform better than the baselines, even though these tasks have not been observed during the training of the router.
