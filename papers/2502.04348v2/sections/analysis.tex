\section{Analysis}
We now provide further analyses on PuDDing. In particular, we provide the following analyses: Wall clock speedup (\cref{ssec:speedup}), and visualization of omission sets for tasks (\cref{ssec:blockanalysis}). In \cref{app:ablation}, we conduct ablation studies.



\input{tables/speed}

\input{tables/model_loading}

\input{figures/7_heatmap}

\subsection{Wall-clock Speedup}\label{ssec:speedup}

We now provide wall-clock analyses and estimates on the latency and throughput of the PuDDing-compressed models.



\textbf{Inference.} \cref{tab:speed} presents the average wall-clock inference time comparison between the dense and PuDDing-pruned version of the LLaMA 3.1 8B, evaluated on NVIDIA A100 and RTX 6000 Ada. For PuDDing, we have pruned seven layers (21.88\% sparsity). We observe that PuDDing provides a consistent 1.19-1.23$\times$ speedup during the pre-fill stage, and 1.22-1.25$\times$ speedup including the generation stage. The total routing time takes up to 4-8ms, which can be deemed negligible comparing with the overall latency.



\textbf{Parameter loading.} \cref{tab:commestimate} presents the estimated time required for loading the model parameters of LLaMA-3.1 8B (16GB in FP32) from the storage to the GPU. PuDDing can save around 52ms on PCIe and 6ms on NVLink, which is nonnegligibly large comparing with the computational scale of running these models. However, a pitfall is that, for repeated inference, PuDDing may require loading additional weights to account for different prompts. This additional cost can be minimized by loading only the previously unloaded blocks from the storage; in fact, many blocks overlap, as we will demonstrate in \cref{ssec:blockanalysis}.


\subsection{Pruned Block vs. Task}\label{ssec:blockanalysis}

\cref{fig:heatmap} depicts the distribution of the pruned transformer blocks in LLaMA-3.1-8B model, given the prompts from different tasks. Again, we consider the case where we drop seven transformer blocks for each prompt.

From the figure, we make two intriguing observations: First, several blocks are considered almost universally unnecessary. In particular, the blocks 20, 26, 27 are removed with over 80\% probability in all tasks. Similarly, there are certain block which are almost never pruned, \textit{e.g.}, blocks 1--3 and 5--8. Second, regarding some blocks, the importance of the block highly varies over task. For instance, transformer block 4 is pruned with over 80\% for ARC-Easy and ARC-Challenge. On the other hand, for PIQA and WinoGrande, the pruning rate is less than 40\%; in these tasks, the blocks 9 and 10 are likelier to be less important.

We note that similar patterns can be observed for OPT and Vicuna; see \cref{app:visualization} for visualizations on these models.

