\section{Pre-processing for the WinoGrande Dataset}\label{app:winogrande}


The WinoGrande dataset, originally consisting of fill-in-the-blank sentences, was initially computed using the \textit{sentence-level likelihood (sl)} as follows:
\begin{align}
\mathrm{sl}(\mathbf{z};\mathbf{W}) = - \frac{1}{T} \sum_{i=1}^{T} \log p_i(z_i|z_{<i};\mathbf{W}).\label{eq:sl}
\end{align}
By reformulating the dataset into a Question-Answer format and evaluating the \textit{task likelihood (tl) }score for the answer part using \cref{eq:tl}, performance improved significantly, from 61.09\% to 64.16\%. 


\section{Ablation Studies}\label{app:ablation}

We have conducted various ablation studies on the proposed algorithm, PuDDing.  
Below, we provide a summary of our key findings, with corresponding pointers to the relevant section.
\begin{itemize}[leftmargin=*,topsep=0pt,parsep=0pt,itemsep=1.5pt]
\item \textit{Number of candidate omission sets} (\cref{app:ssec_numomission}): We have varied the number of candidate omission sets inside the set $\mathcal{B}$, and find that having $10$ classes is sufficient for handling zero-shot tasks; the gain from adding omission sets quickly saturates.
\item \textit{Proposed task likelihood score} (\cref{app:ssec_tasklikelihood}): We compare the performance of the task likelihood-based routing and the perplexity-based routing under both static and dynamic setups. We find that the using the task likelihood score leads to a clear advantage in both scenarios.
\item \textit{MSE loss for training} (\cref{app:ssec_loss}): We have used the mean-squared error (MSE) loss to train the router using the soft labels. Our experiments show that this leads to a slightly better performance than using the classification loss, namely the cross-entropy loss.
\end{itemize}


\subsection{Number of Candidate Omission Sets}\label{app:ssec_numomission}

\input{tables/omission_set_num} 

\cref{tab:omission_set_num} shows the accuracy of zero-shot task with varying omission set sizes, comparing the impact of using 5, 10, and 30 omission sets across common-sense reasoning tasks. 

Using only 5 omission sets results in a lower accuracy, with an average of 61.19\%, as it shows insufficient performance for optimal results. 30-set configuration, despite some task-specific advantages, does not lead to consistently higher performance. In contrast, the 10-set configuration provides an improvement across multiple tasks, with a highest average accuracy of 61.93\%, indicating that this size offers a better balance between performance and model efficiency. 

\subsection{Effectiveness of the Proposed Task Likelihood Score}\label{app:ssec_tasklikelihood}

\input{tables/ablation_static_dynamic}
Both dynamic and static pruning methods were set up on LLaMA-3.1 8B, with a sparsity of 20\%, corresponding to the pruning of seven blocks.

Static pruning experiments in \cref{tab:ablation_static} show the experimental validation of task-adaptive pruning and our proposed \textit{tl} scoring method. First, we compare two pruning strategies using batch-PPL: one that applies a fixed omission set based on a Wikitext-calibrated batch (57.24\% accuracy) and another that dynamically selects omission sets per task (59.32\% accuracy). The improvement confirms the claim that different tasks require different layer sets. 
Next, keeping the task-wise adaptive setting constant, we replace batch-PPL with our \textit{task likelihood (tl) } loss for omission set selection. This further improves accuracy from 59.32\% to 61.02\%, demonstrating that our method is more effective at identifying layers that impact quality of task-specific inference. 


By comparing Batch-PPL Router (58.19\%) with PuDDing (Task Likelihood Loss) (61.93\%) in \cref{tab:ablation_dynamic}, we observe that our \textit{tl} metric results in omission sets that generalize better across tasks, contributing to an additional performance gain. When comparing these \cref{tab:ablation_dynamic} results with \cref{tab:ablation_static}, Batch-PPL static pruning (57.24\%) improves with router training (58.19\%), and our method (60.12\%) also benefits from router training, increasing to 61.93\%. This validates that even within the same task, different prompts favor slightly different omission sets, and adapting omission sets dynamically through router training is essential for optimal pruning.





\subsection{Using MSE Loss for Training}\label{app:ssec_loss}

\input{tables/ablation_loss}

In \cref{tab:ablation_loss}, we present the result of zero-shot task accuracies in the different router training settings. For the label, a one-hot vector signifies that the router learns only from the highest confidence value within the omission block set derived from the training dataset. In contrast, the log-likelihood label allows the router to incorporate all confidence values during training. Our findings show that training with log-likelihood label leads to improved average accuracy (from 60.16\% to 61.93\%). Hence, we observe that the mean-squared error (MSE) loss function outperforms cross entropy (CE). As a result, in this case for routers to train for finding optimal omission sets by given prompts, richer information in the label (i.e., un-chosen labels are not assigned to zero values), and employing MSE loss enhances better performance.

\clearpage

\section{Additional Visualizations}\label{app:visualization}

\cref{fig:app_heatmap} illustrates the dynamic block selection process in various tasks, highlighting that this process has also been analyzed with different models to highlight how the block selection strategy varies not only varying tasks but also depending on the specific architectures of the models.


\input{figures/app_heatmap}