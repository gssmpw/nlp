\input{tables/compare_table}

\section{Related Work}

In this section, we provide an in-depth comparison of the proposed framework against existing depth and width sparsity frameworks. See \cref{tab:compare_table} for a concise summary.

\subsection{Static Depth Pruning}

Static depth pruning methods select and remove unnecessary blocks from a pretrained LLM using various proxy metrics to measure the importance of the blocks. ShortGPT \citep{men2024shortgpt} measures the block importance using the expected cosine similarity between the input and output activations of the block; a block that does not change the direction of the activation is deemed unnecessary. Shortened-LLaMA \citep{kim2024shortened} directly measures the perplexity drop after removing each transformer block, and SLEB \citep{songsleb} combines this idea with an iterative pruning.

Several recent works also focus on layer-level depth pruning, instead of removing an entire transformer block. In particular, \citet{siddiqui24}, \citet{he24} discover that pruning out self-attention layers have a much less significant impact than removing the feed-forward layers.


Unlike these works, this paper aims to perform dynamic depth pruning using the prompts for the downstream tasks; to account for this difference, we design and use new likelihood-based metrics to measure the block importance.

\subsection{Dynamic Token Routing}

Inspired by the success of mixture-of-experts \citep{jacobs1991adaptive,fedus2022switch}, several recent works have developed mechanisms to route tokens through only a fraction of all transformer blocks. Mixture-of-Depth \citep{raposo2024mixture} adopts the depth sparsity during the training phase with a jointly trained router, to reduce the training cost of LLMs. Here, the trained router can also be used at inference. D-LLM \citep{wangd} trains a router that can be applied on pretrained LLMs to reduce their inference cost.

Our approach differs from both of these works in the sense that it needs only a limited number of transformer blocks active for a single input query (or prompt); the routing is conducted once per input prompt, not per token.

\subsection{Side Note: Width Pruning Techniques}

Width pruning, \textit{i.e.}, pruning out columns and rows of the weight matrices in a structured manner, has also been popularly used for LLM compression. Recent examples include FLAP \citep{an2024flap} and SliceGPT \citep{ashkboosslicegpt} for the category of static width pruning, and D\'{e}j\`{a} Vu \cite{liu2023deja} for the dynamic width pruning.

As these methods tend to alter the size of the weight matrices, which has been highly customized for hardware considerations, they often lead to further challenges in deployment \citep{songsleb,kim2024shortened}. In this work, we thus mainly focus on comparison with depth pruning, but also compare with width pruning in terms of the accuracy.




