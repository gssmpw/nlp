\input{figures/4_pipeline}

\section{Method}\label{sec:method}
We now formally describe the proposed PuDDing (Prompt-routed Dynamic Depth Pruning)---an algorithm to train a router $\hat{\mathbf{b}}(\cdot)$ for the prompt-based depth pruning.

In a nutshell, PuDDing operates in two steps:
\begin{enumerate}[leftmargin=*,topsep=0pt,parsep=0pt,itemsep=1.5pt]
\item Generating candidate omission sets using the prompt-answer dataset collected from various tasks (\cref{ssec:method1})
\item Training a router to predict the best option among the candidate omission sets (\cref{ssec:method2})
\end{enumerate}

During the inference phase, the given prompt is fed to the router, which predicts which omission set (among the candidates) one should use for the given prompt. Then, the model parameters are loaded from the storage to the high-speed memory to constitute the depth-pruned LLM (see \cref{model pipeline}).

We note that this classification-based approach is in contrast with the approach of dynamic token routing \citep{wangd}, where one makes yes/no decisions for omitting each block in a sequential manner; this change is to make the router training easier and generalizable.


\subsection{Candidate Omission Set Generation}\label{ssec:method1}

The first step is to generate a candidate pool of omission sets. That is, we generate a family of omission sets
\begin{align}
\mathcal{B} = \{\mathbf{b}_1,\ldots,\mathbf{b}_m\},
\end{align}
which will be used as the codomain of the router $\hat{\mathbf{b}}(\cdot)$, which will simply be an $m$-class classifier.

Desirable properties of the candidate set $\mathcal{B}$ are as follows:
\begin{itemize}[leftmargin=*,topsep=0pt,parsep=0pt,itemsep=1.5pt]
\item Coverage: For any realistic prompt-answer pair $(\mathbf{x},\mathbf{y})$ from a wide range of tasks, the set $\mathcal{B}$ should contain at least one $\mathbf{b}_i$ with a small loss $\ell(\mathbf{y},f(\mathbf{x};\mathbf{W}^{\setminus \mathbf{b}_i}))$.
\item Cardinality: The number of omission sets $m$ should be sufficiently small, so that one can train a nice predictor for $\mathcal{B}$ with a limited number of samples.
\end{itemize}

To obtain these properties, we adopt the following strategy: First, we collect $t$ calibration datasets $D_1,\ldots,D_t$ on a diverse set of downstream tasks. Then, on each calibration dataset, we select the omission set that minimizes some loss criterion, \textit{i.e.}, solve
\begin{align}
\mathbf{b}_i = \argmin_{\mathbf{b}} \mathbb{E}_{D_i}\big[\ell(\mathbf{y};f(\mathbf{x};\mathbf{W}^{\setminus \mathbf{b}})\big].
\end{align}
Here, the minimization is done in a greedy manner, similar to \citet{songsleb}. We apply $l$ different loss criteria on each calibration dataset to get $m = t \times l$ omission sets.

\textbf{Losses.} As the loss function, we use new task-focused variants of the perplexity loss, which we call the \textit{\textbf{task likelihood losses}}. The perplexity measures the fluency of the generated sentences by measuring the average log-likelihood losses over the whole sequence. That is, for a sample sentence $\mathbf{z} = (z_1,z_2,\ldots,z_T)$, the perplexity is
\begin{align}
\mathrm{ppl}(\mathbf{z};\mathbf{W}) = \exp\left(-\frac{1}{T}\sum_{i=1}^{T} \log p_i(z_i|z_{<i};\mathbf{W})\right), \label{eq:perp}
\end{align}
where $p_i(\cdot|\cdot;\mathbf{W})$ denotes the conditional generative probability of the target language model with parameters $\mathbf{W}$, at the $i$th token. We modify this loss to measure the likelihood only the sequence that matters for on-task performance. That is, if the given datum $\mathbf{z}$ can be broken down into the prompt and answer pair:
\begin{align}
\mathbf{z} = (\mathbf{x},\mathbf{y}) = (\underbrace{z_1,\ldots,z_S}_{=\mathbf{x}},\underbrace{z_{S+1},\ldots,z_{T}}_{=\mathbf{y}}),
\end{align}
then we can define the \textit{task likelihood (tl)} loss as:
\begin{align}
\mathrm{tl}(\mathbf{z};\mathbf{W}) = -\frac{1}{T-S}\sum_{i=S+1}^{T} \log p_i(z_i|z_{<i};\mathbf{W}).\label{eq:tl}
\end{align}
In addition, we also consider the \textit{task likelihood difference} (tld) loss, which is defined as follows: In many tasks, the answer choices are limited (\textit{e.g.} ``true'' or ``false''). In such cases, we can also use the likelihood difference of the correct and wrong answers, i.e., 
\begin{align}
\mathrm{tld}(\mathbf{z};\mathbf{W}) = \mathrm{tl}((\mathbf{x},\mathbf{y});\mathbf{W}) - \mathrm{tl}((\mathbf{x},\mathbf{y}^{\mathrm{wrong}});\mathbf{W}),\label{eq:tld}
\end{align}
where $\mathbf{y}^{\mathrm{wrong}}$ denotes the wrong version of the answer. We use both $\mathrm{tl}(\cdot)$ and $\mathrm{tld}(\cdot)$ as our loss criteria.

We note that the task likelihood losses \cref{eq:tl,eq:tld} is different from the perplexity (\cref{eq:perp}), in the sense that we do not exponentiate the values. We use this version as it empirically works better than the exponentiated one.


\subsection{Router Training}\label{ssec:method2}

After generating the candidate omission set $\mathcal{B}$, we train a router that maps the given prompt to the best omission set. Roughly, this is done by first constructing a soft-labeled dataset with task-specific datasets and then training a BERT-based router on the constructed dataset \citep{devlin19}

\textbf{Dataset Construction.} To construct the training dataset, we first collect various prompt-answer pairs from the task datasets, similarly to the calibration datasets in \cref{ssec:method1}. Then, for each sample, we compute the task likelihood losses on all omission sets, and store them as a label vector. That is, each datum inside the dataset takes the form $(\mathbf{x}_i,\mathbf{s}_i)$, where $\mathbf{x}_i$ is the prompt and the $\mathbf{s}_i$ is a length-$m$ vector with
\begin{align}
\mathbf{s}_i = \big(\mathrm{tl}((\mathbf{x}_i,\mathbf{y}_i);\mathbf{W}^{\setminus \mathbf{b}_1}),\ldots, \mathrm{tl}((\mathbf{x}_i,\mathbf{y}_i);\mathbf{W}^{\setminus \mathbf{b}_m})\big).
\end{align}
Note that we no longer need to store the correct answers $\mathbf{y}_i$.

\textbf{Router training.} We train a router to accurately predict the label vector $\mathbf{s}$ given the input prompt $\mathbf{x}$, for all samples in this dataset. That is, we train a function $\hat{\mathbf{s}} = f(\mathbf{x})$ such that $\hat{\mathbf{s}} \approx \mathbf{s}$ holds. We use the MSE loss
\begin{align}
\mathrm{MSE}(\mathbf{s},\hat{\mathbf{s}}) = \big\|\mathbf{s} - \hat{\mathbf{s}}\big\|^2_2
\end{align}
to train the router.
At inference, we will select the omission set with the minimum-achieving index of the predicted $\hat{\mathbf{s}}$.

\textbf{Router architecture.} We use a lightweight transformer-based encoder as our router. More specifically, we insert a single linear layer on pretrained BERT-base \citep{devlin19}, and jointly fine-tune during the training. While this router has more parameters ($\sim$110M) than typical routers that are used for dynamic token routing---such as D-LLM \citep{wangd} which uses 2-layer MLP---the computational cost is bearable as we route only once per prompt. In our experiments, the routing cost typically takes up around $2-4\%$ of the total pre-fill cost.
