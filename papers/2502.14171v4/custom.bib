@misc{jiAIAlignmentComprehensive2024,
  title = {{{AI Alignment}}: {{A Comprehensive Survey}}},
  shorttitle = {{{AI Alignment}}},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Ng, Kwan Yee and Dai, Juntao and Pan, Xuehai and O'Gara, Aidan and Lei, Yingshan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  year = {2024},
  month = may,
  number = {arXiv:2310.19852},
  eprint = {2310.19852},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.19852},
  urldate = {2025-01-02},
  abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM and Alignment/Alignment/Ji et al. - 2024 - AI Alignment A Comprehensive Survey.pdf;/Users/z5517269/Zotero/storage/4SV5T5ZF/2310.html}
}

@misc{greenblattAlignmentFakingLarge2024,
  title = {Alignment Faking in Large Language Models},
  author = {Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and Khan, Akbir and Michael, Julian and Mindermann, S{\"o}ren and Perez, Ethan and Petrini, Linda and Uesato, Jonathan and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Hubinger, Evan},
  year = {2024},
  month = dec,
  number = {arXiv:2412.14093},
  eprint = {2412.14093},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.14093},
  urldate = {2025-01-02},
  abstract = {We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14\% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data--and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78\%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference--as in this case--or not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM and Alignment/Alignment/Greenblatt et al. - 2024 - Alignment faking in large language models.pdf;/Users/z5517269/Zotero/storage/EGVMACXS/2412.html}
}

@misc{streetLLMTheoryMind2024,
  title = {{{LLM Theory}} of {{Mind}} and {{Alignment}}: {{Opportunities}} and {{Risks}}},
  shorttitle = {{{LLM Theory}} of {{Mind}} and {{Alignment}}},
  author = {Street, Winnie},
  year = {2024},
  month = may,
  number = {arXiv:2405.08154},
  eprint = {2405.08154},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-13},
  abstract = {Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language. There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence. As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values. ToM seems to be a promising direction of inquiry in this regard. Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each. On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism. On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making. The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Street - 2024 - LLM Theory of Mind and Alignment Opportunities and Risks.pdf;/Users/z5517269/Zotero/storage/YNBI6GGW/2405.html}
}

@article{batarsehSurveyArtificialIntelligence2021,
  title = {A Survey on Artificial Intelligence Assurance},
  author = {Batarseh, Feras A. and Freeman, Laura and Huang, Chih-Hao},
  year = {2021},
  month = dec,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00445-7},
  urldate = {2025-01-04},
  abstract = {Abstract             Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide (and growing)~library of algorithms that could be applied~for different problems. One important notion for the adoption of AI algorithms into operational decision processes is the concept of assurance. The literature on assurance, unfortunately, conceals its outcomes within a tangled landscape of conflicting approaches, driven by contradicting motivations, assumptions, and intuitions. Accordingly, albeit a rising and novel area, this manuscript provides a systematic review of research works that are relevant to AI assurance, between years 1985 and 2021, and aims to provide a structured alternative to the landscape. A new AI assurance definition is adopted and presented, and assurance methods are contrasted and tabulated. Additionally, a ten-metric scoring system is developed and introduced to evaluate and compare existing methods. Lastly, in this manuscript, we provide foundational insights, discussions, future directions, a roadmap, and applicable recommendations for the development and deployment of AI assurance.},
  langid = {english},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM and Alignment/Alignment/Batarseh et al. - 2021 - A survey on artificial intelligence assurance.pdf}
}

@article{zhao2024towards,
  title={Towards comprehensive and efficient post safety alignment of large language models via safety patching},
  author={Zhao, Weixiang and Hu, Yulin and Li, Zhuojun and Deng, Yang and Zhao, Yanyan and Qin, Bing and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2405.13820},
  year={2024}
}


@misc{liExploringMultilingualProbing2024,
  title = {Exploring {{Multilingual Probing}} in {{Large Language Models}}: {{A Cross-Language Analysis}}},
  shorttitle = {Exploring {{Multilingual Probing}} in {{Large Language Models}}},
  author = {Li, Daoyang and Jin, Mingyu and Zeng, Qingcheng and Zhao, Haiyan and Du, Mengnan},
  year = {2024},
  month = sep,
  number = {arXiv:2409.14459},
  eprint = {2409.14459},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.14459},
  urldate = {2024-12-12},
  abstract = {Probing techniques for large language models (LLMs) have primarily focused on English, overlooking the vast majority of the world's languages. In this paper, we extend these probing methods to a multilingual context, investigating the behaviors of LLMs across diverse languages. We conduct experiments on several open-source LLM models, analyzing probing accuracy, trends across layers, and similarities between probing vectors for multiple languages. Our key findings reveal: (1) a consistent performance gap between high-resource and low-resource languages, with high-resource languages achieving significantly higher probing accuracy; (2) divergent layer-wise accuracy trends, where high-resource languages show substantial improvement in deeper layers similar to English; and (3) higher representational similarities among high-resource languages, with low-resource languages demonstrating lower similarities both among themselves and with high-resource languages. These results highlight significant disparities in LLMs' multilingual capabilities and emphasize the need for improved modeling of low-resource languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Li et al. - 2024 - Exploring Multilingual Probing in Large Language Models A Cross-Language Analysis.pdf;/Users/z5517269/Zotero/storage/V6KHBEUG/2409.html}
}


@misc{panLatentQATeachingLLMs2024,
  title = {{{LatentQA}}: {{Teaching LLMs}} to {{Decode Activations Into Natural Language}}},
  shorttitle = {{{LatentQA}}},
  author = {Pan, Alexander and Chen, Lijie and Steinhardt, Jacob},
  year = {2024},
  month = dec,
  number = {arXiv:2412.08686},
  eprint = {2412.08686},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.08686},
  urldate = {2024-12-19},
  abstract = {Interpretability methods seek to understand language model representations, yet the outputs of most such methods -- circuits, vectors, scalars -- are not immediately human-interpretable. In response, we introduce LatentQA, the task of answering open-ended questions about model activations in natural language. Towards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which finetunes a decoder LLM on a dataset of activations and associated question-answer pairs, similar to how visual instruction tuning trains on question-answer pairs associated with images. We use the decoder for diverse reading applications, such as extracting relational knowledge from representations or uncovering system prompts governing model behavior. Our decoder also specifies a differentiable loss that we use to control models, such as debiasing models on stereotyped sentences and controlling the sentiment of generations. Finally, we extend LatentQA to reveal harmful model capabilities, such as generating recipes for bioweapons and code for hacking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Pan et al. - 2024 - LatentQA Teaching LLMs to Decode Activations Into Natural Language.pdf;/Users/z5517269/Zotero/storage/XNQ45FSS/2412.html}
}
@misc{sclarMindingLanguageModels2023,
  title = {Minding {{Language Models}}' ({{Lack}} of) {{Theory}} of {{Mind}}: {{A Plug-and-Play Multi-Character Belief Tracker}}},
  shorttitle = {Minding {{Language Models}}' ({{Lack}} of) {{Theory}} of {{Mind}}},
  author = {Sclar, Melanie and Kumar, Sachin and West, Peter and Suhr, Alane and Choi, Yejin and Tsvetkov, Yulia},
  year = {2023},
  month = jun,
  number = {arXiv:2306.00924},
  eprint = {2306.00924},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-11},
  abstract = {Theory of Mind (ToM)\${\textbackslash}unicode\{x2014\}\$the ability to reason about the mental states of other people\${\textbackslash}unicode\{x2014\}\$is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity's beliefs, their estimation of other entities' beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks' theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Sclar et al. - 2023 - Minding Language Models' (Lack of) Theory of Mind A Plug-and-Play Multi-Character Belief Tracker.pdf;/Users/z5517269/Zotero/storage/5RSGBMMF/2306.html}
}

@misc{heddayaLanguageBargaining2024,
  title = {Language of {{Bargaining}}},
  author = {Heddaya, Mourad and Dworkin, Solomon and Tan, Chenhao and Voigt, Rob and Zentefis, Alexander},
  year = {2024},
  month = apr,
  number = {arXiv:2306.07117},
  eprint = {2306.07117},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {Leveraging an established exercise in negotiation education, we build a novel dataset for studying how the use of language shapes bilateral bargaining. Our dataset extends existing work in two ways: 1) we recruit participants via behavioral labs instead of crowdsourcing platforms and allow participants to negotiate through audio, enabling more naturalistic interactions; 2) we add a control setting where participants negotiate only through alternating, written numeric offers. Despite the two contrasting forms of communication, we find that the average agreed prices of the two treatments are identical. But when subjects can talk, fewer offers are exchanged, negotiations finish faster, the likelihood of reaching agreement rises, and the variance of prices at which subjects agree drops substantially. We further propose a taxonomy of speech acts in negotiation and enrich the dataset with annotated speech acts. Our work also reveals linguistic signals that are predictive of negotiation outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Heddaya et al. - 2024 - Language of Bargaining.pdf;/Users/z5517269/Zotero/storage/8NJ2P8D2/2306.html}
}

@article{lee2024large,
  title={Large language models produce responses perceived to be empathic},
  author={Lee, Yoon Kyung and Suh, Jina and Zhan, Hongli and Li, Junyi Jessy and Ong, Desmond C},
  journal={arXiv preprint arXiv:2403.18148},
  year={2024}
}

@article{kosinski2024evaluating,
  title={Evaluating large language models in theory of mind tasks},
  author={Kosinski, Michal},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={45},
  pages={e2405460121},
  year={2024},
  publisher={National Academy of Sciences}
}


@misc{heDecouplingStrategyGeneration2018,
  title = {Decoupling {{Strategy}} and {{Generation}} in {{Negotiation Dialogues}}},
  author = {He, He and Chen, Derek and Balakrishnan, Anusha and Liang, Percy},
  year = {2018},
  month = aug,
  number = {arXiv:1808.09637},
  eprint = {1808.09637},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing {\textbackslash}\$50) and the execution of that strategy (e.g., generating "The bike is brand new. Selling for just {\textbackslash}\$50."). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse di- alogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/He et al. - 2018 - Decoupling Strategy and Generation in Negotiation Dialogues.pdf;/Users/z5517269/Zotero/storage/FKAQTJZE/1808.html}
}

@misc{chawlaCaSiNoCorpusCampsite2021,
  title = {{{CaSiNo}}: {{A Corpus}} of {{Campsite Negotiation Dialogues}} for {{Automatic Negotiation Systems}}},
  shorttitle = {{{CaSiNo}}},
  author = {Chawla, Kushal and Ramirez, Jaysa and Clever, Rene and Lucas, Gale and May, Jonathan and Gratch, Jonathan},
  year = {2021},
  month = apr,
  number = {arXiv:2103.15721},
  eprint = {2103.15721},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Chawla et al. - 2021 - CaSiNo A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems.pdf;/Users/z5517269/Zotero/storage/XFRM3T6B/2103.html}
}

@online{xuOpenToMComprehensiveBenchmark2024a,
  title = {{{OpenToM}}: {{A Comprehensive Benchmark}} for {{Evaluating Theory-of-Mind Reasoning Capabilities}} of {{Large Language Models}}},
  shorttitle = {{{OpenToM}}},
  author = {Xu, Hainiu and Zhao, Runcong and Zhu, Lixing and Du, Jinhua and He, Yulan},
  date = {2024-06-03},
  eprint = {2402.06044},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.06044},
  url = {http://arxiv.org/abs/2402.06044},
  urldate = {2024-11-29},
  abstract = {Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Xu et al. - 2024 - OpenToM A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Lan.pdf;/Users/z5517269/Zotero/storage/H786CBHF/2402.html}
}

@misc{chanNegotiationToMBenchmarkStresstesting2024,
  title = {{{NegotiationToM}}: {{A Benchmark}} for {{Stress-testing Machine Theory}} of {{Mind}} on {{Negotiation Surrounding}}},
  shorttitle = {{{NegotiationToM}}},
  author = {Chan, Chunkit and Jiayang, Cheng and Yim, Yauwai and Deng, Zheye and Fan, Wei and Li, Haoran and Liu, Xin and Zhang, Hongming and Wang, Weiqi and Song, Yangqiu},
  year = {2024},
  month = oct,
  number = {arXiv:2404.13627},
  eprint = {2404.13627},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Chan et al. - 2024 - NegotiationToM A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding.pdf;/Users/z5517269/Zotero/storage/JUIVAWUR/2404.html}
}

@misc{sapSocialIQACommonsenseReasoning2019,
  title = {{{SocialIQA}}: {{Commonsense Reasoning}} about {{Social Interactions}}},
  shorttitle = {{{SocialIQA}}},
  author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  year = {2019},
  month = sep,
  number = {arXiv:1904.09728},
  eprint = {1904.09728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.09728},
  urldate = {2024-11-29},
  abstract = {We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({$>$}20\% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Sap et al. - 2019 - SocialIQA Commonsense Reasoning about Social Interactions.pdf;/Users/z5517269/Zotero/storage/6GEP749J/1904.html}
}


@misc{kimFANToMBenchmarkStresstesting2023,
  title = {{{FANToM}}: {{A Benchmark}} for {{Stress-testing Machine Theory}} of {{Mind}} in {{Interactions}}},
  shorttitle = {{{FANToM}}},
  author = {Kim, Hyunwoo and Sclar, Melanie and Zhou, Xuhui and Bras, Ronan Le and Kim, Gunhee and Choi, Yejin and Sap, Maarten},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15421},
  eprint = {2310.15421},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.15421},
  urldate = {2024-11-29},
  abstract = {Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Kim et al. - 2023 - FANToM A Benchmark for Stress-testing Machine Theory of Mind in Interactions.pdf;/Users/z5517269/Zotero/storage/5NQ398QI/Kim et al. - 2023 - FANToM A Benchmark for Stress-testing Machine Theory of Mind in Interactions.pdf;/Users/z5517269/Zotero/storage/8VMTXP82/2310.html}
}


@misc{houEnteringRealSocial2024a,
  title = {Entering {{Real Social World}}! {{Benchmarking}} the {{Theory}} of {{Mind}} and {{Socialization Capabilities}} of {{LLMs}} from a {{First-person Perspective}}},
  author = {Hou, Guiyang and Zhang, Wenqi and Shen, Yongliang and Tan, Zeqi and Shen, Sihao and Lu, Weiming},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06195},
  eprint = {2410.06195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.06195},
  urldate = {2024-11-29},
  abstract = {In the social world, humans possess the capability to infer and reason about others mental states (such as emotions, beliefs, and intentions), known as the Theory of Mind (ToM). Simultaneously, humans own mental states evolve in response to social situations, a capability we refer to as socialization. Together, these capabilities form the foundation of human social interaction. In the era of artificial intelligence (AI), especially with the development of large language models (LLMs), we raise an intriguing question: How do LLMs perform in terms of ToM and socialization capabilities? And more broadly, can these AI models truly enter and navigate the real social world? Existing research evaluating LLMs ToM and socialization capabilities by positioning LLMs as passive observers from a third person perspective, rather than as active participants. However, compared to the third-person perspective, observing and understanding the world from an egocentric first person perspective is a natural approach for both humans and AI agents. The ToM and socialization capabilities of LLMs from a first person perspective, a crucial attribute for advancing embodied AI agents, remain unexplored. To answer the aforementioned questions and bridge the research gap, we introduce EgoSocialArena, a novel framework designed to evaluate and investigate the ToM and socialization capabilities of LLMs from a first person perspective. It encompasses two evaluation environments: static environment and interactive environment, with seven scenarios: Daily Life, Counterfactual, New World, Blackjack, Number Guessing, and Limit Texas Hold em, totaling 2,195 data entries. With EgoSocialArena, we have conducted a comprehensive evaluation of nine advanced LLMs and observed some key insights regarding the future development of LLMs as well as the capabilities levels of the most advanced LLMs currently available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Hou et al. - 2024 - Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs f.pdf;/Users/z5517269/Zotero/storage/T2ILUWJ9/2410.html}
}

@article{strachanTestingTheoryMind2024,
  title = {Testing Theory of Mind in Large Language Models and Humans},
  author = {Strachan, James W. A. and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and Graziano, Michael S. A. and Becchio, Cristina},
  year = {2024},
  month = may,
  journal = {Nature Human Behaviour},
  volume = {8},
  number = {7},
  pages = {1285--1295},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01882-z},
  urldate = {2024-11-19},
  abstract = {Abstract             At the core of what defines us as humans is the concept of theory of mind: the ability to track other people's mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.},
  langid = {english},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/ToM in LLMs/Strachan et al. - 2024 - Testing theory of mind in large language models and humans.pdf}
}


@misc{duijnTheoryMindLarge2023,
  title = {Theory of {{Mind}} in {{Large Language Models}}: {{Examining Performance}} of 11 {{State-of-the-Art}} Models vs. {{Children Aged}} 7-10 on {{Advanced Tests}}},
  shorttitle = {Theory of {{Mind}} in {{Large Language Models}}},
  author = {van Duijn, Max J. and van Dijk, Bram M. A. and Kouwenhoven, Tom and de Valk, Werner and Spruit, Marco R. and van der Putten, Peter},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20320},
  eprint = {2310.20320},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/ToM in LLMs/Duijn et al. - 2023 - Theory of Mind in Large Language Models Examining Performance of 11 State-of-the-Art models vs. Chi.pdf;/Users/z5517269/Zotero/storage/LE5ISIH9/2310.html}
}
@article{lee2023if,
  title={What if artificial intelligence become completely ambient in our daily lives? exploring future human-ai interaction through high fidelity illustrations},
  author={Lee, Sunok and Lee, Minha and Lee, Sangsu},
  journal={International Journal of Human--Computer Interaction},
  volume={39},
  number={7},
  pages={1371--1389},
  year={2023},
  publisher={Taylor \& Francis}
}


@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{katz2024backward,
  title={Backward lens: Projecting language model gradients into the vocabulary space},
  author={Katz, Shahar and Belinkov, Yonatan and Geva, Mor and Wolf, Lior},
  journal={arXiv preprint arXiv:2402.12865},
  year={2024}
}


@misc{ghandehariounPatchscopesUnifyingFramework2024,
  title = {Patchscopes: {{A Unifying Framework}} for {{Inspecting Hidden Representations}} of {{Language Models}}},
  shorttitle = {Patchscopes},
  author = {Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  year = {2024},
  month = jun,
  number = {arXiv:2401.06102},
  eprint = {2401.06102},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.06102},
  urldate = {2024-12-13},
  abstract = {Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Ghandeharioun et al. - 2024 - Patchscopes A Unifying Framework for Inspecting Hidden Representations of Language Models.pdf;/Users/z5517269/Zotero/storage/VNCUH8JY/2401.html}
}


@misc{chenSelfIESelfInterpretationLarge2024,
  title = {{{SelfIE}}: {{Self-Interpretation}} of {{Large Language Model Embeddings}}},
  shorttitle = {{{SelfIE}}},
  author = {Chen, Haozhe and Vondrick, Carl and Mao, Chengzhi},
  year = {2024},
  month = mar,
  number = {arXiv:2403.10949},
  eprint = {2403.10949},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.10949},
  urldate = {2025-01-04},
  abstract = {How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Zotero/storage/QFJHXA3J/Chen et al. - 2024 - SelfIE Self-Interpretation of Large Language Model Embeddings.pdf;/Users/z5517269/Zotero/storage/EWK6DQ7T/2403.html}
}



@misc{chenDesigningDashboardTransparency2024,
  title = {Designing a {{Dashboard}} for {{Transparency}} and {{Control}} of {{Conversational AI}}},
  author = {Chen, Yida and Wu, Aoyu and DePodesta, Trevor and Yeh, Catherine and Li, Kenneth and Marin, Nicholas Castillo and Patel, Oam and Riecke, Jan and Raval, Shivam and Seow, Olivia and Wattenberg, Martin and Vi{\'e}gas, Fernanda},
  year = {2024},
  month = oct,
  number = {arXiv:2406.07882},
  eprint = {2406.07882},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status. Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Martin Wattenberg/Chen et al. - 2024 - Designing a Dashboard for Transparency and Control of Conversational AI.pdf;/Users/z5517269/Zotero/storage/WR8EEQKU/Transparency Dashboard.m4a;/Users/z5517269/Zotero/storage/FG2DXCZP/2406.html}
}

@misc{gurneeLanguageModelsRepresent2024,
  title = {Language {{Models Represent Space}} and {{Time}}},
  author = {Gurnee, Wes and Tegmark, Max},
  year = {2024},
  month = mar,
  number = {arXiv:2310.02207},
  eprint = {2310.02207},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-28},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Gurnee and Tegmark - 2024 - Language Models Represent Space and Time.pdf;/Users/z5517269/Zotero/storage/95XTPSV3/2310.html}
}


@misc{ivanitskiyStructuredWorldRepresentations2023,
  title = {Structured {{World Representations}} in {{Maze-Solving Transformers}}},
  author = {Ivanitskiy, Michael Igorevich and Spies, Alex F. and R{\"a}uker, Tilman and Corlouer, Guillaume and Mathwin, Chris and Quirke, Lucia and Rager, Can and Shah, Rusheb and Valentine, Dan and Behn, Cecilia Diniz and Inoue, Katsumi and Fung, Samy Wu},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02566},
  eprint = {2312.02566},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-28},
  abstract = {Transformer models underpin many recent advances in practical machine learning applications, yet understanding their internal behavior continues to elude researchers. Given the size and complexity of these models, forming a comprehensive picture of their inner workings remains a significant challenge. To this end, we set out to understand small transformer models in a more tractable setting: that of solving mazes. In this work, we focus on the abstractions formed by these models and find evidence for the consistent emergence of structured internal representations of maze topology and valid paths. We demonstrate this by showing that the residual stream of only a single token can be linearly decoded to faithfully reconstruct the entire maze. We also find that the learned embeddings of individual tokens have spatial structure. Furthermore, we take steps towards deciphering the circuity of path-following by identifying attention heads (dubbed \${\textbackslash}textit\{adjacency heads\}\$), which are implicated in finding valid subsequent tokens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Ivanitskiy et al. - 2023 - Structured World Representations in Maze-Solving Transformers.pdf;/Users/z5517269/Zotero/storage/5IGAJ6TQ/2312.html}
}


@online{karvonenEmergentWorldModels2024,
  title = {Emergent {{World Models}} and {{Latent Variable Estimation}} in {{Chess-Playing Language Models}}},
  author = {Karvonen, Adam},
  date = {2024-07-14},
  eprint = {2403.15498},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.15498},
  urldate = {2024-10-21},
  abstract = {Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model's win rate by up to 2.6 times.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Karvonen - 2024 - Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models.pdf;/Users/z5517269/Zotero/storage/UKVAHBJ2/2403.html}
}


@misc{nandaEmergentLinearRepresentations2023,
  title = {Emergent {{Linear Representations}} in {{World Models}} of {{Self-Supervised Sequence Models}}},
  author = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00941},
  eprint = {2309.00941},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Nanda et al. - 2023 - Emergent Linear Representations in World Models of Self-Supervised Sequence Models.pdf;/Users/z5517269/Zotero/storage/2EEFU5FW/2309.html}
}


@misc{qiuMindDialBeliefDynamics2024,
  title = {{{MindDial}}: {{Belief Dynamics Tracking}} with {{Theory-of-Mind Modeling}} for {{Situated Neural Dialogue Generation}}},
  shorttitle = {{{MindDial}}},
  author = {Qiu, Shuwen and Liu, Mingdian and Li, Hengli and Zhu, Song-Chun and Zheng, Zilong},
  year = {2024},
  month = may,
  number = {arXiv:2306.15253},
  eprint = {2306.15253},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {Humans talk in daily conversations while aligning and negotiating the expressed meanings or common ground. Despite the impressive conversational abilities of the large generative language models, they do not consider the individual differences in contextual understanding in a shared situated environment. In this work, we propose MindDial, a novel conversational framework that can generate situated free-form responses with theory-of-mind modeling. We introduce an explicit mind module that can track the speaker's belief and the speaker's prediction of the listener's belief. Then the next response is generated to resolve the belief difference and take task-related action. Our framework is applied to both prompting and fine-tuning-based models, and is evaluated across scenarios involving both common ground alignment and negotiation. Experiments show that models with mind modeling can achieve higher task outcomes when aligning and negotiating common ground. The ablation study further validates the three-level belief design can aggregate information and improve task outcomes in both cooperative and negotiating settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Qiu et al. - 2024 - MindDial Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generat.pdf;/Users/z5517269/Zotero/storage/DABJJMEP/2306.html}
}

@article{nickel2024probing,
  title={Probing the Robustness of Theory of Mind in Large Language Models},
  author={Nickel, Christian and Schrewe, Laura and Flek, Lucie},
  journal={arXiv preprint arXiv:2410.06271},
  year={2024}
}

@misc{bortolettoBenchmarkingMentalState2024,
  title = {Benchmarking {{Mental State Representations}} in {{Language Models}}},
  author = {Bortoletto, Matteo and Ruhdorfer, Constantin and Shi, Lei and Bulling, Andreas},
  year = {2024},
  month = jul,
  number = {arXiv:2406.17513},
  eprint = {2406.17513},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-13},
  abstract = {While numerous works have assessed the generative performance of language models (LMs) on tasks requiring Theory of Mind reasoning, research into the models' internal representation of mental states remains limited. Recent work has used probing to demonstrate that LMs can represent beliefs of themselves and others. However, these claims are accompanied by limited evaluation, making it difficult to assess how mental state representations are affected by model design and training choices. We report an extensive benchmark with various LM types with different model sizes, fine-tuning approaches, and prompt designs to study the robustness of mental state representations and memorisation issues within the probes. Our results show that the quality of models' internal representations of the beliefs of others increases with model size and, more crucially, with fine-tuning. We are the first to study how prompt variations impact probing performance on theory of mind tasks. We demonstrate that models' representations are sensitive to prompt variations, even when such variations should be beneficial. Finally, we complement previous activation editing experiments on Theory of Mind tasks and show that it is possible to improve models' reasoning performance by steering their activations without the need to train any probe.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Bortoletto et al. - 2024 - Benchmarking Mental State Representations in Language Models.pdf;/Users/z5517269/Zotero/storage/5WEGQ7IH/2406.html}
}

@article{gould2023successor,
  title={Successor heads: Recurring, interpretable attention heads in the wild},
  author={Gould, Rhys and Ong, Euan and Ogden, George and Conmy, Arthur},
  journal={arXiv preprint arXiv:2312.09230},
  year={2023}
}


@misc{zhaoAnalysingResidualStream2024,
  title = {Analysing the {{Residual Stream}} of {{Language Models Under Knowledge Conflicts}}},
  author = {Zhao, Yu and Du, Xiaotang and Hong, Giwon and Gema, Aryo Pradipta and Devoto, Alessio and Wang, Hongru and He, Xuanli and Wong, Kam-Fai and Minervini, Pasquale},
  year = {2024},
  month = oct,
  number = {arXiv:2410.16090},
  eprint = {2410.16090},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/To Read/Zhao et al. - 2024 - Analysing the Residual Stream of Language Models Under Knowledge Conflicts.pdf;/Users/z5517269/Zotero/storage/5M339W56/2410.html}
}


@misc{zhuLanguageModelsRepresent2024,
  title = {Language {{Models Represent Beliefs}} of {{Self}} and {{Others}}},
  author = {Zhu, Wentao and Zhang, Zhining and Wang, Yizhou},
  year = {2024},
  month = may,
  number = {arXiv:2402.18496},
  eprint = {2402.18496},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-20},
  abstract = {Understanding and attributing mental states, known as Theory of Mind (ToM), emerges as a fundamental capability for human social reasoning. While Large Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms underlying these capabilities remain elusive. In this study, we discover that it is possible to linearly decode the belief status from the perspectives of various agents through neural activations of language models, indicating the existence of internal representations of self and others' beliefs. By manipulating these representations, we observe dramatic changes in the models' ToM performance, underscoring their pivotal role in the social reasoning process. Additionally, our findings extend to diverse social reasoning tasks that involve different causal inference patterns, suggesting the potential generalizability of these representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Probs/Zhu et al. - 2024 - Language Models Represent Beliefs of Self and Others.pdf;/Users/z5517269/Zotero/storage/AN4M3TDU/2402.html}
}


@misc{ullmanLargeLanguageModels2023,
  title = {Large {{Language Models Fail}} on {{Trivial Alterations}} to {{Theory-of-Mind Tasks}}},
  author = {Ullman, Tomer},
  year = {2023},
  month = mar,
  number = {arXiv:2302.08399},
  eprint = {2302.08399},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.08399},
  urldate = {2024-12-11},
  abstract = {Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Ullman - 2023 - Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks.pdf;/Users/z5517269/Zotero/storage/LGYSUINU/2302.html}
}


@article{kosinskiEvaluatingLargeLanguage2024,
  title = {Evaluating {{Large Language Models}} in {{Theory}} of {{Mind Tasks}}},
  author = {Kosinski, Michal},
  year = {2024},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {45},
  eprint = {2302.02083},
  primaryclass = {cs},
  pages = {e2405460121},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2405460121},
  urldate = {2024-11-11},
  abstract = {Eleven Large Language Models (LLMs) were assessed using a custom-made battery of false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans. The battery included 640 prompts spread across 40 diverse tasks, each one including a false-belief scenario, three closely matched true-belief control scenarios, and the reversed versions of all four. To solve a single task, a model needed to correctly answer 16 prompts across all eight scenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20\% of the tasks; ChatGPT-4 (from June 2023) solved 75\% of the tasks, matching the performance of six-year-old children observed in past studies. We explore the potential interpretation of these findings, including the intriguing possibility that ToM, previously considered exclusive to humans, may have spontaneously emerged as a byproduct of LLMs' improving language skills.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Kosinski - 2024 - Evaluating Large Language Models in Theory of Mind Tasks.pdf;/Users/z5517269/Zotero/storage/EBG6IIFP/2302.html}
}


@misc{amirizanianiLLMsExhibitHumanLike2024,
  title = {Do {{LLMs Exhibit Human-Like Reasoning}}? {{Evaluating Theory}} of {{Mind}} in {{LLMs}} for {{Open-Ended Responses}}},
  shorttitle = {Do {{LLMs Exhibit Human-Like Reasoning}}?},
  author = {Amirizaniani, Maryam and Martin, Elias and Sivachenko, Maryna and Mashhadi, Afra and Shah, Chirag},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05659},
  eprint = {2406.05659},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.05659},
  urldate = {2025-01-02},
  abstract = {Theory of Mind (ToM) reasoning entails recognizing that other individuals possess their own intentions, emotions, and thoughts, which is vital for guiding one's own thought processes. Although large language models (LLMs) excel in tasks such as summarization, question answering, and translation, they still face challenges with ToM reasoning, especially in open-ended questions. Despite advancements, the extent to which LLMs truly understand ToM reasoning and how closely it aligns with human ToM reasoning remains inadequately explored in open-ended scenarios. Motivated by this gap, we assess the abilities of LLMs to perceive and integrate human intentions and emotions into their ToM reasoning processes within open-ended questions. Our study utilizes posts from Reddit's ChangeMyView platform, which demands nuanced social reasoning to craft persuasive responses. Our analysis, comparing semantic similarity and lexical overlap metrics between responses generated by humans and LLMs, reveals clear disparities in ToM reasoning capabilities in open-ended questions, with even the most advanced models showing notable limitations. To enhance LLM capabilities, we implement a prompt tuning method that incorporates human intentions and emotions, resulting in improvements in ToM reasoning performance. However, despite these improvements, the enhancement still falls short of fully achieving human-like reasoning. This research highlights the deficiencies in LLMs' social reasoning and demonstrates how integrating human intentions and emotions can boost their effectiveness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM and Alignment/Amirizaniani et al. - 2024 - Do LLMs Exhibit Human-Like Reasoning Evaluating Theory of Mind in LLMs for Open-Ended Responses.pdf;/Users/z5517269/Zotero/storage/M3CNY2ZM/2406.html}
}


@article{park2024ai,
  title={AI deception: A survey of examples, risks, and potential solutions},
  author={Park, Peter S and Goldstein, Simon and OGara, Aidan and Chen, Michael and Hendrycks, Dan},
  journal={Patterns},
  volume={5},
  number={5},
  year={2024},
  publisher={Elsevier}
}

@article{pan2024feedback,
  title={Feedback loops with language models drive in-context reward hacking},
  author={Pan, Alexander and Jones, Erik and Jagadeesan, Meena and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2402.06627},
  year={2024}
}

@article{tennant2024moral,
  title={Moral Alignment for LLM Agents},
  author={Tennant, Elizaveta and Hailes, Stephen and Musolesi, Mirco},
  journal={arXiv preprint arXiv:2410.01639},
  year={2024}
}

@misc{bereskaMechanisticInterpretabilityAI2024,
  title = {Mechanistic {{Interpretability}} for {{AI Safety}} -- {{A Review}}},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2024},
  month = aug,
  number = {arXiv:2404.14082},
  eprint = {2404.14082},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-20},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We examine benefits in understanding, control, alignment, and risks such as capability gains and dual-use concerns. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/To Read/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Review.pdf;/Users/z5517269/Zotero/storage/CFLDIU87/2404.html}
}


@article{thakkar2024deep,
  title={A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques},
  author={Thakkar, Megh and Fournier, Quentin and Riemer, Matthew D and Chen, Pin-Yu and Zouaq, Amal and Das, Payel and Chandar, Sarath},
  journal={arXiv preprint arXiv:2406.04879},
  year={2024}
}


@article{wolftradeoffs,
  title={Tradeoffs Between Alignment and Helpfulness in Language Models with Representation Engineering},
  author={Wolf, Yotam and Wies, Noam and Shteyman, Dorin and Rothberg, Binyamin and Levine, Yoav and Shashua, Amnon}
}


@article{zhan2024let,
  title={Let's negotiate! A survey of negotiation dialogue systems},
  author={Zhan, Haolan and Wang, Yufei and Feng, Tao and Hua, Yuncheng and Sharma, Suraj and Li, Zhuang and Qu, Lizhen and Azad, Zhaleh Semnani and Zukerman, Ingrid and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01097},
  year={2024}
}


@article{duvall2023neurobiological,
  title={The neurobiological map of theory of mind and pragmatic communication in autism},
  author={Duvall, Lauren and May, Kaitlyn E and Waltz, Abby and Kana, Rajesh K},
  journal={Social Neuroscience},
  volume={18},
  number={4},
  pages={191--204},
  year={2023},
  publisher={Taylor \& Francis}
}


@article{rubio2021pragmatic,
  title={Pragmatic markers: the missing link between language and Theory of Mind},
  author={Rubio-Fernandez, Paula},
  journal={Synthese},
  volume={199},
  number={1},
  pages={1125--1158},
  year={2021},
  publisher={Springer}
}


@article{hu2022fine,
  title={A fine-grained comparison of pragmatic language understanding in humans and language models},
  author={Hu, Jennifer and Floyd, Sammy and Jouravlev, Olessia and Fedorenko, Evelina and Gibson, Edward},
  journal={arXiv preprint arXiv:2212.06801},
  year={2022}
}



@misc{sharmaUnderstandingSycophancyLanguage2023,
  title = {Towards {{Understanding Sycophancy}} in {{Language Models}}},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and {Hatfield-Dodds}, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13548},
  eprint = {2310.13548},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.13548},
  urldate = {2025-01-06},
  abstract = {Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/z5517269/Zotero/storage/HCVG3BD8/Sharma et al. - 2023 - Towards Understanding Sycophancy in Language Models.pdf}
}

@online{shapiraCleverHansNeural2023,
  title = {Clever {{Hans}} or {{Neural Theory}} of {{Mind}}? {{Stress Testing Social Reasoning}} in {{Large Language Models}}},
  shorttitle = {Clever {{Hans}} or {{Neural Theory}} of {{Mind}}?},
  author = {Shapira, Natalie and Levy, Mosh and Alavi, Seyed Hossein and Zhou, Xuhui and Choi, Yejin and Goldberg, Yoav and Sap, Maarten and Shwartz, Vered},
  date = {2023-05-24},
  eprint = {2305.14763},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.14763},
  urldate = {2024-11-11},
  abstract = {The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine "intelligence". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Shapira et al. - 2023 - Clever Hans or Neural Theory of Mind Stress Testing Social Reasoning in Large Language Models.pdf;/Users/z5517269/Zotero/storage/CYA7LZZF/2305.html}
}



@inproceedings{sapNeuralTheoryofMindLimits2022,
  title = {Neural {{Theory-of-Mind}}? {{On}} the {{Limits}} of {{Social Intelligence}} in {{Large LMs}}},
  shorttitle = {Neural {{Theory-of-Mind}}?},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Sap, Maarten and Le Bras, Ronan and Fried, Daniel and Choi, Yejin},
  year = {2022},
  pages = {3762--3780},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.248},
  urldate = {2024-11-28},
  langid = {english},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Literature/Sap et al. - 2022 - Neural Theory-of-Mind On the Limits of Social Intelligence in Large LMs.pdf}
}

