@article{batarsehSurveyArtificialIntelligence2021,
  title = {A Survey on Artificial Intelligence Assurance},
  author = {Batarseh, Feras A. and Freeman, Laura and Huang, Chih-Hao},
  year = {2021},
  month = dec,
  journal = {Journal of Big Data},
  volume = {8},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-021-00445-7},
  urldate = {2025-01-04},
  abstract = {Abstract             Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide (and growing)~library of algorithms that could be applied~for different problems. One important notion for the adoption of AI algorithms into operational decision processes is the concept of assurance. The literature on assurance, unfortunately, conceals its outcomes within a tangled landscape of conflicting approaches, driven by contradicting motivations, assumptions, and intuitions. Accordingly, albeit a rising and novel area, this manuscript provides a systematic review of research works that are relevant to AI assurance, between years 1985 and 2021, and aims to provide a structured alternative to the landscape. A new AI assurance definition is adopted and presented, and assurance methods are contrasted and tabulated. Additionally, a ten-metric scoring system is developed and introduced to evaluate and compare existing methods. Lastly, in this manuscript, we provide foundational insights, discussions, future directions, a roadmap, and applicable recommendations for the development and deployment of AI assurance.},
  langid = {english},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM and Alignment/Alignment/Batarseh et al. - 2021 - A survey on artificial intelligence assurance.pdf}
}

@misc{chanNegotiationToMBenchmarkStresstesting2024,
  title = {{{NegotiationToM}}: {{A Benchmark}} for {{Stress-testing Machine Theory}} of {{Mind}} on {{Negotiation Surrounding}}},
  shorttitle = {{{NegotiationToM}}},
  author = {Chan, Chunkit and Jiayang, Cheng and Yim, Yauwai and Deng, Zheye and Fan, Wei and Li, Haoran and Liu, Xin and Zhang, Hongming and Wang, Weiqi and Song, Yangqiu},
  year = {2024},
  month = oct,
  number = {arXiv:2404.13627},
  eprint = {2404.13627},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {Large Language Models (LLMs) have sparked substantial interest and debate concerning their potential emergence of Theory of Mind (ToM) ability. Theory of mind evaluations currently focuses on testing models using machine-generated data or game settings prone to shortcuts and spurious correlations, which lacks evaluation of machine ToM ability in real-world human interaction scenarios. This poses a pressing demand to develop new real-world scenario benchmarks. We introduce NegotiationToM, a new benchmark designed to stress-test machine ToM in real-world negotiation surrounding covered multi-dimensional mental states (i.e., desires, beliefs, and intentions). Our benchmark builds upon the Belief-Desire-Intention (BDI) agent modeling theory and conducts the necessary empirical experiments to evaluate large language models. Our findings demonstrate that NegotiationToM is challenging for state-of-the-art LLMs, as they consistently perform significantly worse than humans, even when employing the chain-of-thought (CoT) method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Chan et al. - 2024 - NegotiationToM A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding.pdf;/Users/z5517269/Zotero/storage/JUIVAWUR/2404.html}
}

@misc{chawlaCaSiNoCorpusCampsite2021,
  title = {{{CaSiNo}}: {{A Corpus}} of {{Campsite Negotiation Dialogues}} for {{Automatic Negotiation Systems}}},
  shorttitle = {{{CaSiNo}}},
  author = {Chawla, Kushal and Ramirez, Jaysa and Clever, Rene and Lucas, Gale and May, Jonathan and Gratch, Jonathan},
  year = {2021},
  month = apr,
  number = {arXiv:2103.15721},
  eprint = {2103.15721},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Chawla et al. - 2021 - CaSiNo A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems.pdf;/Users/z5517269/Zotero/storage/XFRM3T6B/2103.html}
}

@misc{chenDesigningDashboardTransparency2024,
  title = {Designing a {{Dashboard}} for {{Transparency}} and {{Control}} of {{Conversational AI}}},
  author = {Chen, Yida and Wu, Aoyu and DePodesta, Trevor and Yeh, Catherine and Li, Kenneth and Marin, Nicholas Castillo and Patel, Oam and Riecke, Jan and Raval, Shivam and Seow, Olivia and Wattenberg, Martin and Vi{\'e}gas, Fernanda},
  year = {2024},
  month = oct,
  number = {arXiv:2406.07882},
  eprint = {2406.07882},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status. Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Martin Wattenberg/Chen et al. - 2024 - Designing a Dashboard for Transparency and Control of Conversational AI.pdf;/Users/z5517269/Zotero/storage/WR8EEQKU/Transparency Dashboard.m4a;/Users/z5517269/Zotero/storage/FG2DXCZP/2406.html}
}

@misc{chenSelfIESelfInterpretationLarge2024,
  title = {{{SelfIE}}: {{Self-Interpretation}} of {{Large Language Model Embeddings}}},
  shorttitle = {{{SelfIE}}},
  author = {Chen, Haozhe and Vondrick, Carl and Mao, Chengzhi},
  year = {2024},
  month = mar,
  number = {arXiv:2403.10949},
  eprint = {2403.10949},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.10949},
  urldate = {2025-01-04},
  abstract = {How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond to inquiries about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge in LLM without supervision targets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Zotero/storage/QFJHXA3J/Chen et al. - 2024 - SelfIE Self-Interpretation of Large Language Model Embeddings.pdf;/Users/z5517269/Zotero/storage/EWK6DQ7T/2403.html}
}

@misc{duijnTheoryMindLarge2023,
  title = {Theory of {{Mind}} in {{Large Language Models}}: {{Examining Performance}} of 11 {{State-of-the-Art}} Models vs. {{Children Aged}} 7-10 on {{Advanced Tests}}},
  shorttitle = {Theory of {{Mind}} in {{Large Language Models}}},
  author = {van Duijn, Max J. and van Dijk, Bram M. A. and Kouwenhoven, Tom and de Valk, Werner and Spruit, Marco R. and van der Putten, Peter},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20320},
  eprint = {2310.20320},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/ToM in LLMs/Duijn et al. - 2023 - Theory of Mind in Large Language Models Examining Performance of 11 State-of-the-Art models vs. Chi.pdf;/Users/z5517269/Zotero/storage/LE5ISIH9/2310.html}
}

@misc{ghandehariounPatchscopesUnifyingFramework2024,
  title = {Patchscopes: {{A Unifying Framework}} for {{Inspecting Hidden Representations}} of {{Language Models}}},
  shorttitle = {Patchscopes},
  author = {Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  year = {2024},
  month = jun,
  number = {arXiv:2401.06102},
  eprint = {2401.06102},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.06102},
  urldate = {2024-12-13},
  abstract = {Understanding the internal representations of large language models (LLMs) can help explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that many prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and multihop reasoning error correction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Ghandeharioun et al. - 2024 - Patchscopes A Unifying Framework for Inspecting Hidden Representations of Language Models.pdf;/Users/z5517269/Zotero/storage/VNCUH8JY/2401.html}
}

@article{gould2023successor,
  title={Successor heads: Recurring, interpretable attention heads in the wild},
  author={Gould, Rhys and Ong, Euan and Ogden, George and Conmy, Arthur},
  journal={arXiv preprint arXiv:2312.09230},
  year={2023}
}

@misc{gurneeLanguageModelsRepresent2024,
  title = {Language {{Models Represent Space}} and {{Time}}},
  author = {Gurnee, Wes and Tegmark, Max},
  year = {2024},
  month = mar,
  number = {arXiv:2310.02207},
  eprint = {2310.02207},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-28},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Gurnee and Tegmark - 2024 - Language Models Represent Space and Time.pdf;/Users/z5517269/Zotero/storage/95XTPSV3/2310.html}
}

@misc{heDecouplingStrategyGeneration2018,
  title = {Decoupling {{Strategy}} and {{Generation}} in {{Negotiation Dialogues}}},
  author = {He, He and Chen, Derek and Balakrishnan, Anusha and Liang, Percy},
  year = {2018},
  month = aug,
  number = {arXiv:1808.09637},
  eprint = {1808.09637},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-18},
  abstract = {We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing {\textbackslash}\$50) and the execution of that strategy (e.g., generating "The bike is brand new. Selling for just {\textbackslash}\$50."). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse di- alogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/He et al. - 2018 - Decoupling Strategy and Generation in Negotiation Dialogues.pdf;/Users/z5517269/Zotero/storage/FKAQTJZE/1808.html}
}

@misc{heddayaLanguageBargaining2024,
  title = {Language of {{Bargaining}}},
  author = {Heddaya, Mourad and Dworkin, Solomon and Tan, Chenhao and Voigt, Rob and Zentefis, Alexander},
  year = {2024},
  month = apr,
  number = {arXiv:2306.07117},
  eprint = {2306.07117},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-19},
  abstract = {Leveraging an established exercise in negotiation education, we build a novel dataset for studying how the use of language shapes bilateral bargaining. Our dataset extends existing work in two ways: 1) we recruit participants via behavioral labs instead of crowdsourcing platforms and allow participants to negotiate through audio, enabling more naturalistic interactions; 2) we add a control setting where participants negotiate only through alternating, written numeric offers. Despite the two contrasting forms of communication, we find that the average agreed prices of the two treatments are identical. But when subjects can talk, fewer offers are exchanged, negotiations finish faster, the likelihood of reaching agreement rises, and the variance of prices at which subjects agree drops substantially. We further propose a taxonomy of speech acts in negotiation and enrich the dataset with annotated speech acts. Our work also reveals linguistic signals that are predictive of negotiation outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Heddaya et al. - 2024 - Language of Bargaining.pdf;/Users/z5517269/Zotero/storage/8NJ2P8D2/2306.html}
}

@misc{houEnteringRealSocial2024a,
  title = {Entering {{Real Social World}}! {{Benchmarking}} the {{Theory}} of {{Mind}} and {{Socialization Capabilities}} of {{LLMs}} from a {{First-person Perspective}}},
  author = {Hou, Guiyang and Zhang, Wenqi and Shen, Yongliang and Tan, Zeqi and Shen, Sihao and Lu, Weiming},
  year = {2024},
  month = oct,
  number = {arXiv:2410.06195},
  eprint = {2410.06195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.06195},
  urldate = {2024-11-29},
  abstract = {In the social world, humans possess the capability to infer and reason about others mental states (such as emotions, beliefs, and intentions), known as the Theory of Mind (ToM). Simultaneously, humans own mental states evolve in response to social situations, a capability we refer to as socialization. Together, these capabilities form the foundation of human social interaction. In the era of artificial intelligence (AI), especially with the development of large language models (LLMs), we raise an intriguing question: How do LLMs perform in terms of ToM and socialization capabilities? And more broadly, can these AI models truly enter and navigate the real social world? Existing research evaluating LLMs ToM and socialization capabilities by positioning LLMs as passive observers from a third person perspective, rather than as active participants. However, compared to the third-person perspective, observing and understanding the world from an egocentric first person perspective is a natural approach for both humans and AI agents. The ToM and socialization capabilities of LLMs from a first person perspective, a crucial attribute for advancing embodied AI agents, remain unexplored. To answer the aforementioned questions and bridge the research gap, we introduce EgoSocialArena, a novel framework designed to evaluate and investigate the ToM and socialization capabilities of LLMs from a first person perspective. It encompasses two evaluation environments: static environment and interactive environment, with seven scenarios: Daily Life, Counterfactual, New World, Blackjack, Number Guessing, and Limit Texas Hold em, totaling 2,195 data entries. With EgoSocialArena, we have conducted a comprehensive evaluation of nine advanced LLMs and observed some key insights regarding the future development of LLMs as well as the capabilities levels of the most advanced LLMs currently available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Hou et al. - 2024 - Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs f.pdf;/Users/z5517269/Zotero/storage/T2ILUWJ9/2410.html}
}

@misc{ivanitskiyStructuredWorldRepresentations2023,
  title = {Structured {{World Representations}} in {{Maze-Solving Transformers}}},
  author = {Ivanitskiy, Michael Igorevich and Spies, Alex F. and R{\"a}uker, Tilman and Corlouer, Guillaume and Mathwin, Chris and Quirke, Lucia and Rager, Can and Shah, Rusheb and Valentine, Dan and Behn, Cecilia Diniz and Inoue, Katsumi and Fung, Samy Wu},
  year = {2023},
  month = dec,
  number = {arXiv:2312.02566},
  eprint = {2312.02566},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-28},
  abstract = {Transformer models underpin many recent advances in practical machine learning applications, yet understanding their internal behavior continues to elude researchers. Given the size and complexity of these models, forming a comprehensive picture of their inner workings remains a significant challenge. To this end, we set out to understand small transformer models in a more tractable setting: that of solving mazes. In this work, we focus on the abstractions formed by these models and find evidence for the consistent emergence of structured internal representations of maze topology and valid paths. We demonstrate this by showing that the residual stream of only a single token can be linearly decoded to faithfully reconstruct the entire maze. We also find that the learned embeddings of individual tokens have spatial structure. Furthermore, we take steps towards deciphering the circuity of path-following by identifying attention heads (dubbed \${\textbackslash}textit\{adjacency heads\}\$), which are implicated in finding valid subsequent tokens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Ivanitskiy et al. - 2023 - Structured World Representations in Maze-Solving Transformers.pdf;/Users/z5517269/Zotero/storage/5IGAJ6TQ/2312.html}
}

@misc{jiAIAlignmentComprehensive2024,
  title = {{{AI Alignment}}: {{A Comprehensive Survey}}},
  shorttitle = {{{AI Alignment}}},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Ng, Kwan Yee and Dai, Juntao and Pan, Xuehai and O'Gara, Aidan and Lei, Yingshan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  year = {2024},
  month = may,
  number = {arXiv:2310.19852},
  eprint = {2310.19852},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.19852},
  urldate = {2025-01-02},
  abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM and Alignment/Alignment/Ji et al. - 2024 - AI Alignment A Comprehensive Survey.pdf;/Users/z5517269/Zotero/storage/4SV5T5ZF/2310.html}
}

@online{karvonenEmergentWorldModels2024,
  title = {Emergent {{World Models}} and {{Latent Variable Estimation}} in {{Chess-Playing Language Models}}},
  author = {Karvonen, Adam},
  date = {2024-07-14},
  eprint = {2403.15498},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2403.15498},
  urldate = {2024-10-21},
  abstract = {Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Unlike Li et al's prior synthetic dataset approach, our analysis finds that the model also learns to estimate latent variables like player skill to better predict the next character. We derive a player skill vector and add it to the model, improving the model's win rate by up to 2.6 times.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Karvonen - 2024 - Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models.pdf;/Users/z5517269/Zotero/storage/UKVAHBJ2/2403.html}
}

@article{katz2024backward,
  title={Backward lens: Projecting language model gradients into the vocabulary space},
  author={Katz, Shahar and Belinkov, Yonatan and Geva, Mor and Wolf, Lior},
  journal={arXiv preprint arXiv:2402.12865},
  year={2024}
}

@misc{kimFANToMBenchmarkStresstesting2023,
  title = {{{FANToM}}: {{A Benchmark}} for {{Stress-testing Machine Theory}} of {{Mind}} in {{Interactions}}},
  shorttitle = {{{FANToM}}},
  author = {Kim, Hyunwoo and Sclar, Melanie and Zhou, Xuhui and Bras, Ronan Le and Kim, Gunhee and Choi, Yejin and Sap, Maarten},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15421},
  eprint = {2310.15421},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.15421},
  urldate = {2024-11-29},
  abstract = {Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Kim et al. - 2023 - FANToM A Benchmark for Stress-testing Machine Theory of Mind in Interactions.pdf;/Users/z5517269/Zotero/storage/5NQ398QI/Kim et al. - 2023 - FANToM A Benchmark for Stress-testing Machine Theory of Mind in Interactions.pdf;/Users/z5517269/Zotero/storage/8VMTXP82/2310.html}
}

@article{kosinski2024evaluating,
  title={Evaluating large language models in theory of mind tasks},
  author={Kosinski, Michal},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={45},
  pages={e2405460121},
  year={2024},
  publisher={National Academy of Sciences}
}

@article{lee2023if,
  title={What if artificial intelligence become completely ambient in our daily lives? exploring future human-ai interaction through high fidelity illustrations},
  author={Lee, Sunok and Lee, Minha and Lee, Sangsu},
  journal={International Journal of Human--Computer Interaction},
  volume={39},
  number={7},
  pages={1371--1389},
  year={2023},
  publisher={Taylor \& Francis}
}

@misc{liExploringMultilingualProbing2024,
  title = {Exploring {{Multilingual Probing}} in {{Large Language Models}}: {{A Cross-Language Analysis}}},
  shorttitle = {Exploring {{Multilingual Probing}} in {{Large Language Models}}},
  author = {Li, Daoyang and Jin, Mingyu and Zeng, Qingcheng and Zhao, Haiyan and Du, Mengnan},
  year = {2024},
  month = sep,
  number = {arXiv:2409.14459},
  eprint = {2409.14459},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.14459},
  urldate = {2024-12-12},
  abstract = {Probing techniques for large language models (LLMs) have primarily focused on English, overlooking the vast majority of the world's languages. In this paper, we extend these probing methods to a multilingual context, investigating the behaviors of LLMs across diverse languages. We conduct experiments on several open-source LLM models, analyzing probing accuracy, trends across layers, and similarities between probing vectors for multiple languages. Our key findings reveal: (1) a consistent performance gap between high-resource and low-resource languages, with high-resource languages achieving significantly higher probing accuracy; (2) divergent layer-wise accuracy trends, where high-resource languages show substantial improvement in deeper layers similar to English; and (3) higher representational similarities among high-resource languages, with low-resource languages demonstrating lower similarities both among themselves and with high-resource languages. These results highlight significant disparities in LLMs' multilingual capabilities and emphasize the need for improved modeling of low-resource languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Li et al. - 2024 - Exploring Multilingual Probing in Large Language Models A Cross-Language Analysis.pdf;/Users/z5517269/Zotero/storage/V6KHBEUG/2409.html}
}

@misc{nandaEmergentLinearRepresentations2023,
  title = {Emergent {{Linear Representations}} in {{World Models}} of {{Self-Supervised Sequence Models}}},
  author = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00941},
  eprint = {2309.00941},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/Internal World Models/Nanda et al. - 2023 - Emergent Linear Representations in World Models of Self-Supervised Sequence Models.pdf;/Users/z5517269/Zotero/storage/2EEFU5FW/2309.html}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{panLatentQATeachingLLMs2024,
  title = {{{LatentQA}}: {{Teaching LLMs}} to {{Decode Activations Into Natural Language}}},
  shorttitle = {{{LatentQA}}},
  author = {Pan, Alexander and Chen, Lijie and Steinhardt, Jacob},
  year = {2024},
  month = dec,
  number = {arXiv:2412.08686},
  eprint = {2412.08686},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.08686},
  urldate = {2024-12-19},
  abstract = {Interpretability methods seek to understand language model representations, yet the outputs of most such methods -- circuits, vectors, scalars -- are not immediately human-interpretable. In response, we introduce LatentQA, the task of answering open-ended questions about model activations in natural language. Towards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which finetunes a decoder LLM on a dataset of activations and associated question-answer pairs, similar to how visual instruction tuning trains on question-answer pairs associated with images. We use the decoder for diverse reading applications, such as extracting relational knowledge from representations or uncovering system prompts governing model behavior. Our decoder also specifies a differentiable loss that we use to control models, such as debiasing models on stereotyped sentences and controlling the sentiment of generations. Finally, we extend LatentQA to reveal harmful model capabilities, such as generating recipes for bioweapons and code for hacking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/tom_llm/Pan et al. - 2024 - LatentQA Teaching LLMs to Decode Activations Into Natural Language.pdf;/Users/z5517269/Zotero/storage/XNQ45FSS/2412.html}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{sapNeuralTheoryofMindLimits2022,
  title = {Neural {{Theory-of-Mind}}? {{On}} the {{Limits}} of {{Social Intelligence}} in {{Large LMs}}},
  shorttitle = {Neural {{Theory-of-Mind}}?},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Sap, Maarten and Le Bras, Ronan and Fried, Daniel and Choi, Yejin},
  year = {2022},
  pages = {3762--3780},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.248},
  urldate = {2024-11-28},
  langid = {english},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Literature/Sap et al. - 2022 - Neural Theory-of-Mind On the Limits of Social Intelligence in Large LMs.pdf}
}

@misc{sapSocialIQACommonsenseReasoning2019,
  title = {{{SocialIQA}}: {{Commonsense Reasoning}} about {{Social Interactions}}},
  shorttitle = {{{SocialIQA}}},
  author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  year = {2019},
  month = sep,
  number = {arXiv:1904.09728},
  eprint = {1904.09728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1904.09728},
  urldate = {2024-11-29},
  abstract = {We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: "Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?" A: "Make sure no one else could hear"). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance ({$>$}20\% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Sap et al. - 2019 - SocialIQA Commonsense Reasoning about Social Interactions.pdf;/Users/z5517269/Zotero/storage/6GEP749J/1904.html}
}

@online{shapiraCleverHansNeural2023,
  title = {Clever {{Hans}} or {{Neural Theory}} of {{Mind}}? {{Stress Testing Social Reasoning}} in {{Large Language Models}}},
  shorttitle = {Clever {{Hans}} or {{Neural Theory}} of {{Mind}}?},
  author = {Shapira, Natalie and Levy, Mosh and Alavi, Seyed Hossein and Zhou, Xuhui and Choi, Yejin and Goldberg, Yoav and Sap, Maarten and Shwartz, Vered},
  date = {2023-05-24},
  eprint = {2305.14763},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.14763},
  urldate = {2024-11-11},
  abstract = {The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine "intelligence". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Shapira et al. - 2023 - Clever Hans or Neural Theory of Mind Stress Testing Social Reasoning in Large Language Models.pdf;/Users/z5517269/Zotero/storage/CYA7LZZF/2305.html}
}

@article{strachanTestingTheoryMind2024,
  title = {Testing Theory of Mind in Large Language Models and Humans},
  author = {Strachan, James W. A. and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and Graziano, Michael S. A. and Becchio, Cristina},
  year = {2024},
  month = may,
  journal = {Nature Human Behaviour},
  volume = {8},
  number = {7},
  pages = {1285--1295},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01882-z},
  urldate = {2024-11-19},
  abstract = {Abstract             At the core of what defines us as humans is the concept of theory of mind: the ability to track other people's mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.},
  langid = {english},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/ToM in LLMs/Strachan et al. - 2024 - Testing theory of mind in large language models and humans.pdf}
}

@misc{streetLLMTheoryMind2024,
  title = {{{LLM Theory}} of {{Mind}} and {{Alignment}}: {{Opportunities}} and {{Risks}}},
  shorttitle = {{{LLM Theory}} of {{Mind}} and {{Alignment}}},
  author = {Street, Winnie},
  year = {2024},
  month = may,
  number = {arXiv:2405.08154},
  eprint = {2405.08154},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-13},
  abstract = {Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language. There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence. As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values. ToM seems to be a promising direction of inquiry in this regard. Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each. On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism. On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making. The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Street - 2024 - LLM Theory of Mind and Alignment Opportunities and Risks.pdf;/Users/z5517269/Zotero/storage/YNBI6GGW/2405.html}
}

@online{xuOpenToMComprehensiveBenchmark2024a,
  title = {{{OpenToM}}: {{A Comprehensive Benchmark}} for {{Evaluating Theory-of-Mind Reasoning Capabilities}} of {{Large Language Models}}},
  shorttitle = {{{OpenToM}}},
  author = {Xu, Hainiu and Zhao, Runcong and Zhu, Lixing and Du, Jinhua and He, Yulan},
  date = {2024-06-03},
  eprint = {2402.06044},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.06044},
  url = {http://arxiv.org/abs/2402.06044},
  urldate = {2024-11-29},
  abstract = {Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/ToM/Datasets/Xu et al. - 2024 - OpenToM A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Lan.pdf;/Users/z5517269/Zotero/storage/H786CBHF/2402.html}
}

@article{zhan2024let,
  title={Let's negotiate! A survey of negotiation dialogue systems},
  author={Zhan, Haolan and Wang, Yufei and Feng, Tao and Hua, Yuncheng and Sharma, Suraj and Li, Zhuang and Qu, Lizhen and Azad, Zhaleh Semnani and Zukerman, Ingrid and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01097},
  year={2024}
}

@article{zhao2024towards,
  title={Towards comprehensive and efficient post safety alignment of large language models via safety patching},
  author={Zhao, Weixiang and Hu, Yulin and Li, Zhuojun and Deng, Yang and Zhao, Yanyan and Qin, Bing and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2405.13820},
  year={2024}
}

@misc{zhaoAnalysingResidualStream2024,
  title = {Analysing the {{Residual Stream}} of {{Language Models Under Knowledge Conflicts}}},
  author = {Zhao, Yu and Du, Xiaotang and Hong, Giwon and Gema, Aryo Pradipta and Devoto, Alessio and Wang, Hongru and He, Xuanli and Wong, Kam-Fai and Minervini, Pasquale},
  year = {2024},
  month = oct,
  number = {arXiv:2410.16090},
  eprint = {2410.16090},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/z5517269/Library/CloudStorage/OneDrive-UNSW/Zotero/To Read/Zhao et al. - 2024 - Analysing the Residual Stream of Language Models Under Knowledge Conflicts.pdf;/Users/z5517269/Zotero/storage/5M339W56/2410.html}
}

