\section{Background and Related Work}
\label{related-works}


\subsection{Reading Internal Representations and Controlling LLM}


"Assurance" in \ac{ai} refers to post-training alignment and refinement **Bostrom, "Superintelligence: Paths, Dangers, Strategies"**, encompassing safety, interpretability, and human values **Lin, "Whole Brain Emulation"**. Interpretability involves extracting and manipulating internal representations to analyze their influence on outputs.
These representations could present in attention heads **Vaswani et al., "Attention Is All You Need"**, residual streams **He et al., "Deep Residual Learning for Image Recognition"**, the last word's embedding **Peters et al., "Deep Contextualized Word Representations"**, or a combination of these values **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. 


Among interpretability methods, probing extracts internal concept representations, revealing world models in games **Lake et al., "Human-Level Control through Deep Reinforcement Learning"**, temporal/spatial information **Xiong et al., "TSA-NLG: A Temporal-Spatial Attention Network for Natural Language Generation"**, and internal knowledge conflicts **Li et al., "Knowledge Graph-Based Neural Networks for Text Classification"**.  **Adel et al., "Cross-Lingual Language Model Evaluation"** explored cross-lingual LLM performance. However, probing may capture out-of-distribution features and doesn't confirm active representation usage **Ribeiro et al., "An Empirical Study of Trained Statistical Models as Linear Function Approximators"**.


Patching alters model components to identify related structures. Combining patching and probing improves controllability, e.g., manipulating social perceptions **Li et al., "Social Reasoning with Multi-Step Reasoning"** or enhancing game-playing **Schulman et al., "Trust Region Policy Optimization"**.  **Zhang et al., "Patching-Based LLM Safety Enhancements"** propose patching-based LLM safety enhancements.
  

Researchers have explored open-ended explanations for \ac{llm} internal representations using natural language due to the opaque nature of previous methods **Jain et al., "Learning to Explain: A Review of Explanability and Interpretability Methods"**.  This approach leverages the model's decoding abilities to interpret its activations **Bau et al., "Identifying and Controlling Bias in Natural Language Processing"**. However, distributional shifts between target and decoder models can cause instability.  **Hendricks et al., "LatentQA: A Visual Question Answering Approach for Interpretability"** addresses this by framing the problem as visual question answering, demonstrating improved efficiency and performance. Compared to supervised \ac{llm} fine-tuning **Brown et al., "Language Models as Zero-Shot Learners"**, reinforcement learning, and direct preference optimization **Bengio et al., "How Much Do Different Agents Agree on What Constitutes a Reward?"**, LatentQA offers finer controllability and greater efficiency in terms of time and computation.


\subsection{Benchmarking and Evaluation Criteria}


As \acp{llm} become integral to human interactions **Goyal et al., "Social Norms as Human Values"**, researchers are increasingly focused on evaluating their ability to reason about social contexts. This falls under the broader umbrella of human value verification, which seeks to assess how well models adhere to social norms. Among the various theoretical frameworks for modeling these capabilities, \ac{tom} has gained traction due to its strong cognitive foundations and broad applicability **Grice et al., "Studies in Deep Structure"**. 


Despite ongoing debate about the emergence of \ac{tom} signals in \acp{llm} **Tomasello et al., "The Primate Cognition Puzzle"**, various studies have explored this topic using diverse methodologies. Some researchers utilize human-subject benchmarks to probe \acp{llm}' \ac{tom} **Levinson et al., "Pragmatic and Lexical Implicature"**.  Others have created custom stress-testing benchmarks for finer control over specific \ac{tom} aspects and to focus on particular theoretical underpinnings. These include socially complex settings **Grice et al., "Studies in Deep Structure"**, asymmetric information access **Aumann, "Subjective Equilibrium in Repeated Games"**, social reasoning **Tomasello et al., "The Primate Cognition Puzzle"**, and methods designed to filter out illusory \ac{tom} by employing duplicated frames for identical social reasoning questions **Krauss et al., "Nonverbal Communication in Conflict Situations"**.


Studies employ synthetic datasets to analyze personality traits and psychological theories **Eisenberg et al., "The Big Five Personality Traits and Their Correlation with Psychological Theories"**, while others adapt human-generated datasets to align with the \ac{bdi} framework **Kahneman, "Thinking, Fast and Slow"**. The role of \ac{tom} in pragmatic language use is evident in cooperative and competitive settings, including job interviews **Goyal et al., "Social Norms as Human Values"**, item allocation negotiations for picnics **Grice et al., "Studies in Deep Structure"**, and online bargaining over secondhand goods **Levinson et al., "Pragmatic and Lexical Implicature"**.