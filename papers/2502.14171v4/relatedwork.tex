\section{Background and Related Work}
\label{related-works}


\subsection{Reading Internal Representations and Controlling LLM}


"Assurance" in \ac{ai} refers to post-training alignment and refinement \cite{batarsehSurveyArtificialIntelligence2021}, encompassing safety, interpretability, and human values \cite{jiAIAlignmentComprehensive2024}. Interpretability involves extracting and manipulating internal representations to analyze their influence on outputs.
These representations could present in attention heads \cite{gould2023successor}, residual streams \cite{zhaoAnalysingResidualStream2024, nandaEmergentLinearRepresentations2023}, the last word's embedding \cite{ghandehariounPatchscopesUnifyingFramework2024}, or a combination of these values \cite{panLatentQATeachingLLMs2024}. 


Among interpretability methods, probing extracts internal concept representations, revealing world models in games \cite{nandaEmergentLinearRepresentations2023, karvonenEmergentWorldModels2024, ivanitskiyStructuredWorldRepresentations2023}, temporal/spatial information \cite{gurneeLanguageModelsRepresent2024}, and internal knowledge conflicts \cite{zhaoAnalysingResidualStream2024}.  \cite{liExploringMultilingualProbing2024} explored cross-lingual LLM performance. However, probing may capture out-of-distribution features and doesn't confirm active representation usage \cite{nandaEmergentLinearRepresentations2023}.


Patching alters model components to identify related structures. Combining patching and probing improves controllability, e.g., manipulating social perceptions \cite{chenDesigningDashboardTransparency2024} or enhancing game-playing \cite{karvonenEmergentWorldModels2024}. \cite{zhao2024towards} propose patching-based LLM safety enhancements.
  

Researchers have explored open-ended explanations for \ac{llm} internal representations using natural language due to the opaque nature of previous methods \cite{ghandehariounPatchscopesUnifyingFramework2024, katz2024backward}.  This approach leverages the model's decoding abilities to interpret its activations \cite{chenSelfIESelfInterpretationLarge2024, ghandehariounPatchscopesUnifyingFramework2024}. However, distributional shifts between target and decoder models can cause instability.  LatentQA \cite{panLatentQATeachingLLMs2024} addresses this by framing the problem as visual question answering, demonstrating improved efficiency and performance. Compared to supervised \ac{llm} fine-tuning \cite{ouyang2022training}, reinforcement learning, and direct preference optimization \cite{rafailov2024direct}, LatentQA offers finer controllability and greater efficiency in terms of time and computation.


\subsection{Benchmarking and Evaluation Criteria}


As \acp{llm} become integral to human interactions \cite{lee2023if}, researchers are increasingly focused on evaluating their ability to reason about social contexts. This falls under the broader umbrella of human value verification, which seeks to assess how well models adhere to social norms. Among the various theoretical frameworks for modeling these capabilities, \ac{tom} has gained traction due to its strong cognitive foundations and broad applicability \cite{kosinski2024evaluating, streetLLMTheoryMind2024}. 


Despite ongoing debate about the emergence of \ac{tom} signals in \acp{llm} \cite{sapNeuralTheoryofMindLimits2022, shapiraCleverHansNeural2023}, various studies have explored this topic using diverse methodologies. Some researchers utilize human-subject benchmarks to probe \acp{llm}' \ac{tom} \cite{duijnTheoryMindLarge2023, strachanTestingTheoryMind2024}.  Others have created custom stress-testing benchmarks for finer control over specific \ac{tom} aspects and to focus on particular theoretical underpinnings. These include socially complex settings \cite{houEnteringRealSocial2024a}, asymmetric information access \cite{kimFANToMBenchmarkStresstesting2023}, social reasoning \cite{sapSocialIQACommonsenseReasoning2019}, and methods designed to filter out illusory \ac{tom} by employing duplicated frames for identical social reasoning questions \cite{chanNegotiationToMBenchmarkStresstesting2024}.


Studies employ synthetic datasets to analyze personality traits and psychological theories \cite{xuOpenToMComprehensiveBenchmark2024a}, while others adapt human-generated datasets to align with the \ac{bdi} framework \cite{chanNegotiationToMBenchmarkStresstesting2024}. The role of \ac{tom} in pragmatic language use is evident in cooperative and competitive settings, including job interviews \cite{zhan2024let}, item allocation negotiations for picnics \cite{chawlaCaSiNoCorpusCampsite2021}, and online bargaining over secondhand goods \cite{heDecouplingStrategyGeneration2018, heddayaLanguageBargaining2024}.