\section{Related Work}
\label{sec:related}

Our work draws upon literature from diverse streams, which we organized into four areas as follows.

\smallskip
\noindent\textbf{Visualizing technological risks.} AI risk assessments typically focus on specific technologies, such as facial recognition \cite{moraes2021smile}, LLMs \cite{weidinger2021ethicalsocialrisks}, and generative AI \cite{barrett2023identifying}. Visualization plays a crucial role in making these technologies' input data, components, and outputs more interpretable, aiding in identifying technological risks like data biases \cite{vis4ml_review2024,inel2023collect}. For example, visualizing dataset disparities can reveal sampling biases \cite{aif360-oct-2018}, and comparing data patterns with historical data can expose historical biases \cite{IBMriskAtlas}. Comparing inputs and outputs can show inconsistencies in decision-making \cite{whatAILearns2023}, and model vulnerabilities \cite{perturber2021}. 

\smallskip
\noindent\textbf{Overlooking human-interaction and systemic risks.} 
A systematic analysis of model cards \cite{mitchell2019model}, an essential tool for documenting AI models, reveals that AI developers often emphasize technological risks related to data and models while neglecting impacts on individuals and the environment \cite{liang2024s}. This gap arises from the difficulty in predicting a wide range of risks from diverse model uses, stakeholder types, and deployment contexts \cite{boyarskaya2020overcoming, buccinca2023aha}. To address this, Weidinger et al.\ (\citeyear{weidinger2023sociotechnical}) propose assessing risks across three sociotechnical layers: capability, human interaction, and systemic impact. The capability layer evaluates risks inherent in technical features like poor model performance, the human interaction layer addresses risks from user interactions like overreliance on AI \cite{boyarskaya2020overcoming}, and the systemic impact layer considers broader societal and environmental consequences, including unequal distribution of technology's benefits and risks \cite{aif360-oct-2018}.

\smallskip
\noindent\textbf{Visualizing risks for tech-savvy individuals.} Eppler and Aeschimann \cite{Eppler2009} noted that risk visualizations mainly target tech-savvy users, utilizing quantitative tools like matrices, bow-tie diagrams, and cognitive maps. AI risk visualizations often depict model inaccuracies with confusion matrices, accuracy curves, or activation maps, and are embedded in interactive systems handling diverse, high-dimensional data for real-time evaluation \cite{shergadwala2022human,vis4ml_review2024, kwon2022rmexplorer,johnson2023does}. Despite these tools, practitioners may misunderstand visualizations from interpretability tools \cite{interpret_ml, lundberg2017unified}, overrelying on them as proof of model readiness \cite{kaur2020interpreting}. 

\smallskip
\noindent\textbf{Overlooking visualizing risks for ordinary individuals.} Effectively communicating AI risks to ordinary individuals, even those with a strong interest in technology, is challenging and requires understanding their beliefs, experiences, and media representations of AI \cite{buccinca2024towards, AINarratives2020}. Communication techniques for this group can be categorized into three areas: \emph{explaining}, \emph{relating}, and \emph{fostering} engagement with risks \cite{visWhatWorks2021}. To \emph{explain} risks, visualizations should enhance comparisons and aid understanding, especially for those with low numeracy skills \cite{visWhatWorks2021}, using techniques like visual groupings and icon arrays. To \emph{relate} risks to existing knowledge, visual metaphors like hazard labels can be effective \cite{data_hazards}, along with personalized risk presentations using individuals' own data \cite{climateChange}. To \emph{foster} engaging exploration, visualizations should employ narrative techniques, interactive features, and aesthetic styles to enhance engagement and understanding, using methods like story structures, gamified exploration, and contrasting colors \cite{narrativeViz, Lavie2004, calculatingEmpires2023}.

\smallskip
\noindent\textbf{Research gap.} Past efforts in visualizing AI risks focused on technological aspects, neglecting human interaction and systemic risks. Additionally, these visualizations were often tailored to experts and tech-savvy users. To address this gap, we developed a new tool that shows broad AI risks and uses visualization techniques to make them accessible to ordinary individuals.