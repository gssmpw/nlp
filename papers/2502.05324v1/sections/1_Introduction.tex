\section{Introduction}

Effectively communicating AI risks to ordinary individuals enables them to make informed decisions, advocate for their rights and interests, and push for better regulations and safer practices \cite{bao2022whose}, while also limiting unrealistic AI perceptions and expectations \cite{nourani2020role, neri2020role}. Such AI risks range from biases in decision-making to effects on jobs and collective freedoms  \cite{mcgregor2021preventing}. 

In order to communicate risks of an AI technology use, the process of its specific risk discovery needs to take place. Current approaches to risk discovery, targeting AI practitioners, include harm description templates \cite{buccinca2023aha}, impact assessment reports \cite{microsoft2022Assessment, stahl2023systematicReview}, interactive risk cards \cite{constantinides2023prompts}, and databases of technical risks \cite{IBMriskAtlas, AIRiskDatabase}. Risk discovery and AI system's impact assessment are also required to comply with regulations and standards like the European Union AI Act (EU AI Act) \cite{EUACT2024} and the NIST AI Risk Management Framework (NIST AI RMF) \cite{nist2023aiRisk}.

However, effectively communicating AI risks to ordinary individuals, even those with a strong interest in technology, remains a significant challenge. Ojewale et al. (\citeyear{ojewale2024towards}) identified only two  such communication approaches among the 390 AI auditing tools they surveyed. This challenge happens because tools like the AI Incident Database \cite{spatialDatabaseView}, which are made for experts, focus too much on technical risks, making it hard for regular people to understand how AI risks affect their lives.

Our study aims to bridge this gap by first crowdsourcing the design requirements and then developing a tool that uses information visualization techniques to fulfil these requirements and communicate AI risks to ordinary individuals interested in technology. We made four contributions:
\begin{enumerate}
    \item We conducted a crowdsourcing formative study with 40 participants to identify requirements for the new risk communication tool. Using facial recognition as a case study, participants generated only 22 unique uses, paired with 18 risks, 9 mitigations, and 8 benefits. From their feedback, we derived six design requirements: \emph{multiple uses (R1)}, \emph{balanced assessment of uses (R2)}, \emph{structured uses (R3)}, \emph{reduced complexity (R4)}, \emph{broad appeal (R5)}, and \emph{engaging exploration (R6)}.  
    \item To meet design requirements \emph{R1-R2} by increasing the number and variety of generated uses, risks, mitigations, and benefits, we used a Large Language Model (LLM) and a Generative Image Model (GIM). We started by using the LLM to devise 138 facial recognition uses across application domains. We then assessed each use for its risks and benefits. For each identified risk, where applicable, we generated mitigations that could be understood by individuals regardless of their technical knowledge. We used textual data from the LLM to generate GIM illustrations for each use and evaluated the whole generated content for correctness.
    \item To clearly communicate the evaluated content to ordinary individuals interested in technology and meet the remaining design requirements \emph{R3-6}, we employed information visualization techniques. These included visual groupings, visual metaphors, narrative patterns, interactions, and aesthetic styling, all of which contributed to the development of our tool -- the Atlas of AI Risks.
    \item We evaluated the Atlas in a crowdsourcing user study with 140 participants who reflect US population in terms of age, sex, and ethnicity, and compared it against the spatial view of the AI Incident Database. We found that the Atlas met all design requirements and participants preferred it both in terms of usability and aesthetics, finding it more helpful in shaping their decision-making process about facial recognition. 
\end{enumerate}