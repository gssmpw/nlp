\section{Evaluating the Tool Mapping Risks of AI Technology Uses}
The goal of the study was to evaluate if our Atlas met the design requirements and how effectively it communicated the broad risks associated with the diverse uses of facial recognition to ordinary individuals. Next, we describe our study's design (i.e., metrics), setup, execution, and results.

\subsection{Metrics}
We created a task for our study that reflects how people usually handle and question AI decisions in real life, such as writing an email to challenge or praise a system\cite{contestableCars2023}. Our participants might have done similar things before, like giving feedback to a company about its smart devices or checking their town's AI city register, where local governments list technologies used in their cities \cite{cityRegister}. Specifically, we instructed participants to ``Write a brief email to the AI policymakers and ask them to stop and approve some uses of facial recognition. For each use, argue by explaining its risks or benefits''. This task also links the risks in the Atlas to three higher-order decision-making skills: problem-solving (identifying reasonable actions), critical thinking (evaluating and synthesizing information on risks, mitigation, and benefits), and reasoning (constructing logical arguments to support actions).

We outlined six questions to evaluate whether Atlas met our design criteria (R1-6) and how effectively it supported individuals in completing the task, asking, ``How successful was the tool in...    
\begin{enumerate}
    \addtolength{\leftskip}{5pt}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item [Q1] ...communicating multiple uses (R1)?
    \item [Q2] ...providing a balanced assessment of uses (R2)?
    \item [Q3] ...structuring uses (R3)?
    \item [Q4] ...reducing complexity (R4)?
    \item [Q5] ...achieving a broad appeal (R5)?
    \item [Q6] ...engaging individuals in exploration? (R6)?
\end{enumerate}

We then defined four quantitative and four qualitative metrics to answer these questions. 

\smallskip
\noindent\textbf{Quantitative metrics.}
The first quantitative metric assessed the extent to which the tool provided a balanced assessment of uses. It was measured as the percentage of participants who agreed with the statement: \emph{``The tool helped me to understand both the risks and benefits of facial recognition''}. Three metrics measured how successful the tool was in engaging users in exploration. 

The first metric was the about the \emph{usability} of the tool and it was measured through the System Usability Scale~\cite{brooke1996sus}. The second metric was about the \emph{visual aesthetics}, understood as classic aesthetics (for clarity and order), expressive  aesthetics (for originality), and pleasurable interaction (for user enjoyment), three factors crucial for making technology use exploration intuitive and engaging for ordinary individuals. We measured them through the Perceived Visual Aesthetics scale \cite{Lavie2004}. The third metric was about the \emph{exploration time} required for a participant to complete the task.

\smallskip
\noindent \textbf{Qualitative metrics.}
Qualitative metrics were captured through four open-ended questions. The first assessed the effectiveness in communicating multiple uses, by asking: \emph{``How successful was the tool in helping you learn about a variety of uses?''} The second measured the usefulness of structuring uses, by asking \emph{``How useful were for you the categories of uses shown in the tool?''}. The third measured effectiveness in reducing complexity by asking: \emph{``How successful was the tool in simplifying the information?''} The fourth assessed the \emph{relevance} of presented information to ordinary individuals: \emph{``How successful was the tool in identifying uses, risks, benefits, and mitigations relevant to you?''}. 

To assess the impact of knowledge on responses, participants self-evaluated whether they were more skilled or knowledgeable than the average person in the task, technology, facial recognition, and AI.
    
\subsection{Setup}
The study consisted of seven steps (Supplementary Materials, Appendix C). In the first step, participants were introduced to the first reflective judgment task and were asked to write the first email to the regulators without using any tools. In the second step, participants completed the self-assessment control questions, alongside the first randomly assigned attention-check. Moving to the third step, participants interacted with either our Atlas (treatment) or the baseline (control), analyzed the technology's uses in the visualization and completed the second reflective judgment task. After interaction with the treatment or control, participants evaluated its usability and completed the second attention-check sentence in the fourth step. In the fifth step, participants assessed the visualization's visual aesthetics. \hspace{1cm} This step also included the third attention check. In the sixth step, participants explained if the tool helped them to learn about multiple uses of facial recognition, if the proposed categories of uses were useful, and if the content presented in the visualization was relevant to them. Finally, in the last, seventh step participants provided recommendations for future development of the visualization.
\smallskip

\noindent\textbf{Baseline.} 
The control group's visualization mimicked state-of-the-art AI risk visualization -- a dashboard (Supplementary Materials, Appendix D, Figure \ref{fig:baseline}). It displayed technology uses grouped by similarity on the left, pop-ups for the uses in the center, and a dropdown menu with a legend on the right. Like the Atlas, it displayed various uses (R1), categorized them (R2), and allowed exploration (R6). However, it differed in balancing risks and benefits (R2), reducing complexity (R4), and appealing to audience (R5). 

\subsection{Execution}
\label{subsec:execution}

\noindent\textbf{Participants.}
We recruited participants from Prolific~\cite{prolific}, aiming for a sufficiently large number in both the treatment and baseline groups to achieve statistical significance. The study focused on individuals interested in technology, who are representative of the general U.S. population, for two key reasons. First, recruiting English native speakers ensured comprehension of the study materials, enhancing results reliability. Second, exposing US participants to a risk-based approach to AI regulation is relevant, as both the US and EU adopt similar frameworks, with crosswalk documents showing their compatibility. Recruited study participants were compensated approx. \$12 (USD) per hour. 

\smallskip
\noindent\textbf{Procedure.}
We developed a web-based survey that included a reflective judgment task and administered it on Prolific. The survey comprised seven pages, each corresponding to a setup step plus a final confirmation screen. In step three, participants were randomly presented with a link to access either the treatment or the control in a new browser window.

To ensure response quality, we implemented four survey design features. First, we disabled pasting text from external sources and prohibited editing previous responses to encourage original answers and maintain survey flow. Second, we set word ranges for open-ended questions (50-250 words~\cite{liang2024s}) and validated them in real-time to prevent survey fatigue. Third, we used a click tracker to verify participants accessed the treatment or baseline link. Fourth, we randomly administered one of three attention checks. To be included, participants had to correctly respond to at least two attention checks and click the link.

\smallskip
\noindent\textbf{Analysis.} We performed both quantitative and qualitative analyses. 
For the quantitative analysis, we measured four metrics: the share of participants who found the visualization helpful for understanding both the risks and benefits; the average SUS usability score; the average value of perceived aesthetics across classic aesthetics, expressive aesthetics, and pleasurable interaction; and the average time to complete the task. For the qualitative analysis, we thematically analyzed the open-ended questions \cite{miles1994qualitative} to identify repeating themes within design requirements and key factors affecting decision-making.

\begin{figure}[t!]
  \centering
  \includegraphics[width=1\columnwidth]{figures/results.pdf}
  \caption{The Atlas outperformed baseline across all quantitative metrics. It offered a more balanced assessment of uses, scored higher in usability, visual aesthetics, and encouraged longer exploration time.}
  \label{fig:results}
\end{figure}

\subsection{Results}
We received a total of 140 responses, split equally into treatment and control (Supplementary Materials, Appendix E, Table \ref{tbl:participants_demographics}). Participants reflected the US population in terms of age, sex, and ethnicity.

\subsubsection{Quantitative results.}
\textbf{The Atlas provided a more balanced assessment of uses}, with 53\% of participants finding it offered comprehensive perspectives on risks, benefits, and mitigations, compared to 32\% of those who engaged with the baseline (Figure \ref{fig:results}Q2).

\vspace{2pt}
\noindent\textbf{Participants found the Atlas more usable than the baseline} with an average SUS score of 50, while the Atlas scored 68 (Figure \ref{fig:results}Q6). These higher scores were consistent across all levels of technological knowledge (Supplementary Materials, Appendix F, Figure \ref{fig:sus}). They also spent more time exploring the Atlas, indicating higher engagement.

\vspace{2pt}
\noindent\textbf{Participants found the Atlas more aesthetically pleasing than the baseline} and did so across all three dimensions of aesthetics (Figure \ref{fig:results}Q6). In \emph{classic aesthetics}, it provided a better-ordered structure. In \emph{expressive aesthetics}, it reflected more innovative way for presenting risks. In \emph{pleasurable interaction}, it was regarded as moderately successful, indicating room for improvement.

\subsubsection{Qualitative results.}
Participants (referred to as CP) found that the tool supported task completion by reducing the complexity of information about facial recognition ($n$=20), providing numerous examples of uses ($n$ = 18), listing their pros and cons ($n$ = 17), and effectively presenting them visually in the tool ($n$ = 17 mentions). As summarized by CP22, it \emph{``gave both quick tidbits, but also in depth explanations''}, while \emph{``making transitioning from one use to another feel smooth and understandable''} (CP26). The tool did not support participants who had strong pre-existing opinions about facial recognition ($n$ = 20) and needed more details to deliberate about each use ($n$ = 19). For example, CP9 felt they \emph{``didn't see much relating to environment''}, and CP6 wished for a feature where \emph{``AI can explain a picked dot like I'm 5''}.