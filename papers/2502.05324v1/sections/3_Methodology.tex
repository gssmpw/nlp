\begin{figure*}[t!]
  \centering
  \includegraphics[width=\textwidth]{figures/methodology.pdf}
  \caption{Proposing a tool for mapping risks of AI technology uses for ordinary individuals involved four steps. First, we conducted a crowdsourcing formative study to identify six design requirements for visualizing them. Next, using facial recognition as a case study, we asked participants of the formative study to generate its uses (including benefits, risks and mitigations) and evaluated them for correctness. Due to the low number of identified uses, we generated new uses by prompting the LLM and also evaluated them for correctness. Then, with the dataset in hand, we used information visualization techniques to build an interactive tool. We evaluated it against design requirements and its support for a decision-making task. Finally, to demonstrate how it generalizes, we visualized over 300 uses sourced from the AI Incident Database \cite{mcgregor2021preventing}.}
  \label{fig:methods}
\end{figure*}

\section{Proposing a Tool for Mapping Risks of AI Technology Uses for Ordinary Individuals}

\subsection{Identifying Design Requirements}
\label{sec:formative_study}

The formative study aimed to generate AI technology uses, identify effective design techniques for understanding their trade-offs, and establish design requirements for the tool. We selected facial recognition technology as a case study because it is well-documented \cite{facialRecognitionReview}, it is relevant for both identifying humans \cite{moraes2021smile} and animals \cite{Roberts2023_animals}, and it has sparked numerous public debates \cite{crawford2019halt}.

We recruited 40 participants interested in technology residing in the US through Prolific \cite{prolific}. These participants reflected the US population demographics \cite{census_ethnicity_2020, census_age_gender_2022} in terms of sex (21 males and 19 females) and ethnicity (24 White, 5 Black, 2 Asian, 4 Mixed or Other, and 1 Native American), with ages ranging from 19 to 63 years old. They were also digitally literate, exposed to visual media, and skilled in communication for providing feedback.

The study consisted of four parts. In the first part, we asked participants to rate on a scale from 1 to 5 (ranging from very low to very high) their knowledge in facial recognition, AI, and technology in general. In the second part, we asked them to write emails to regulators, requesting either a ban or further adoption of specific uses of facial recognition. They were also asked to explain their reasons by enumerating the risks, benefits, and steps to minimize the risks. In the third part, we presented participants with a list of 30 facial recognition uses \cite{facialRecognitionReview, Roberts2023_animals}, asking them how to best present their risks in an interactive tool. In the final part, participants were given three design techniques -- an \emph{explorative} dashboard, a \emph{narrative} infographic, and a \emph{simulative} tool with dropdowns -- and asked to choose the best one and describe its pros and cons. 

The study was approximately 30-45 minutes long and participants were paid on average about \$12 (USD) per hour. We then conducted inductive thematic analysis \cite{miles1994qualitative}, examining the uses mentioned in participant's emails and their recommendations for the tool. Participants self-reported slightly above-average knowledge in technology in general ($\mu$ = 3.8), followed by average knowledge in AI ($\mu$ = 3.2) and facial recognition ($\mu$ = 2.9). Their recommendations (quotes are marked with FP) resulted in the following six design requirements for the tool:

\vspace{1.25pt}
\noindent\textbf{(R1) Multiple uses.}
The tool should help participants to learn about a variety of uses instead of a limited number of them, as stated by FP30: \emph{``it should prompt me to consider what this technology can do''}. 

\vspace{1.25pt}
\noindent\textbf{(R2) Balanced assessment of uses.}
The tool should present each use with its risks, benefits, and mitigation strategies. This can be achieved by providing  \emph{``concrete examples''} (FP13) and \emph{`distinguishing between personal and societal risks and benefits''} (FP22).

\vspace{1.25pt}
\noindent\textbf{(R3) Structured uses.}
The tool should categorize uses for better understanding, as FP14 stated, \emph{``to help me get a clearer picture of the fields that use facial recognition''}.

\vspace{1.25pt}
\noindent\textbf{(R4) Reduced complexity.} 
The tool should present data on uses, risks, and mitigations, but its sheer volume can overwhelm users with limited technical backgrounds. To minimize this effect, the visualization should \emph{``offer different depth levels''} (FP5) and \emph{``break down the information into pieces to make it easier to come up with an opinion''} (FP21).

\vspace{1.25pt}
\noindent\textbf{(R5) Broad appeal.} 
The tool should make the uses, risks, benefits, and mitigation strategies accessible and relevant to individuals interested in technology, regardless of their technical background. As stated by FP20, \emph{``examples should relate to issues and concerns that people commonly have about AI''}. The uses should be visualized in \emph{``a less complicated way, not like designs for a technical audience''} (FP40).

\vspace{1.25pt}
\noindent\textbf{(R6) Engaging exploration.} 
The tool should engage users with \emph{``many interactive elements to allow for deeper exploration''} (FP5) or a \emph{``guided tour''} (FP28).

Over half of participants ($n$ = 22) preferred the narrative technique for the tool because it closely matched R4 by simplifying complex data into understandable snippets, and R6 by engaging users with a coherent flow of information, which helps maintain the viewer’s interest and attention. However, participants raised potential concerns that it could limit user flexibility to explore the data independently and introduce bias if it overfocuses on risks or benefits.

\subsection{Meeting the First and Second Design Requirement}
We outline the design decisions we made, informed by previous research and expert feedback, to ensure the tool effectively presents \textbf{(R1) Multiple uses} and \textbf{(R2) Balanced assessments of uses}.

\subsubsection{Generating uses without any tool.}
To generate many uses of facial recognition and identify their associated risks, mitigations, and benefits, we adopted the EU AI Act's five-component definition of use \cite{Golpayegani2023Risk} and thematically analysed the participants' emails \cite{miles1994qualitative}.

First, we identified phrases in the emails related to one of eight categories: \emph{purpose} (the AI's end goal, e.g., verifying traveler identity at border controls), \emph{capability} (technological solution behind the AI, e.g., matching faces to criminal databases), \emph{AI subject} (those impacted by the AI, e.g., travelers),  \emph{AI user} (the entity managing the AI, e.g., border control agency), \emph{domain} (the specific sector where the AI is applied, e.g., border control management), \emph{risk} (e.g., infringing on the right to privacy), \emph{mitigation} (e.g., implementing an opt-out option), and \emph{benefit} (e.g., improving security measures).
Second, we organized the phrases by category, grouping similar ones together. We summarize all the generated uses, risks, mitigations, and benefits in Supplementary Materials, Appendix A.

\subsubsection{Evaluating uses generated without any tool.}
To evaluate the correctness and variety of generated uses, we introduced four quantitative metrics. The first metric assessed the number of correct uses by implementation potential and risk level per the EU AI Act. We defined correct use as technically feasible, considering its applicability and usability, and categorized its implementation potential into existing (currently in use), upcoming (in development or early prototype stage), and unlikely (lacking applicability and usability). The second, third, and fourth metrics assessend the number of correct risks, mitigations, and benefits, defined as those that are realistic and likely to occur or succeed when implemented. We categorized these into three types—technical capability, human interaction, and systemic impact—based on the existing taxonomy of socio-technical evaluations \cite{weidinger2023sociotechnical}.

We independently assessed each generated use to first determine its correctness and implementation potential and then agreed on the final assessment. To assess the risk level of use, we recruited three AI compliance experts from our company who classified uses as unacceptable risk, high-risk, or low-risk, with justifications for each label.

Participants jointly identified 22 correct uses: 21 existing and 1 upcoming (Supplementary Materials, Appendix A). The top three uses mentioned were unlocking devices ($n$ = 8), identifying suspects for crime prevention ($n$ = 5), and apprehending individuals on the run ($n$ = 4). Four uses were considered unacceptable (e.g., tracking citizens in public spaces for law enforcement), 15 high-risk, 3 low-risk, and 1 varied between high-risk and unacceptable depending on context. Participants identified 18 correct risks, 8 correct benefits, and 9 correct mitigations, mostly focusing on systemic impacts ($n$ = 11, $n$ = 6, and $n$ = 4, respectively).

\subsubsection{Generating uses with the LLM.}
Formative study participants primarily generated high-risk uses, overemphasized risks compared to benefits and mitigations, and barely addressed risks related to technical capability and human interaction, likely due to limited knowledge of AI development. These are, however, the most common risks resulting in incidents \cite{mcgregor2021preventing}. To ensure the tool meets the two design requirements, we needed to increase the variety of generated uses, risks, mitigations, and benefits. We achieved this by using four prompts for LLM-assisted AI impact assessment — \emph{ExploreGen} \cite{herdel2024exploregen}, \emph{RiskGen}, \emph{BenefitGen}, and \emph{MitigationGen} \cite{constantinides2024_risks_benefits, AIDesign2024} — and engineering one prompt for a generative image model — \emph{IllustrationGen}. We report the prompts' content in Supplementary Materials, Appendix B.

\subsubsection{Evaluating uses generated with the LLM.}
Similarly to the previous evaluation of uses generated without any tool, we introduced five quantitative metrics to evaluate the correctness and variety of generated uses. In addition to the already used four metrics -- the number of correct risks, mitigations, and benefits by type -- we evaluated each illustration for correctness of use depiction, defined as recognizable, relatable, and free of visual stereotypes.

Each metric was evaluated using a two-step process involving two authors and external experts. For assessing correctness, implementation potential, and depiction, we first independently evaluated each generated use, then discussed and agreed on the final assessment with the research team. For risk level, we familiarized ourselves with the EU AI Act \cite{EUACT2024} and randomly sampled 18 uses. We then recruited three AI compliance experts from our company to annotate these uses with risk labels and provide justifications. Afterwards, we independently assessed the remaining 120 uses for risk level, reaching a final consensus through discussions with the research team. For the number of correct risks, mitigations, and benefits by type, we recruited 8 raters: two authors and six industry researchers and developers from our company, all experienced in responsible AI. We randomly assigned 46 uses to the raters through an interactive survey, ensuring each use was evaluated by three raters. The raters reviewed the descriptions of the uses, along with three lists of risks, benefits, and mitigations. They marked whether they agreed with each list and indicated which items should be removed if they disagreed. We then compared the correctness scores for each list.

All 138 generated uses to be correct, with 91 (66\%) already existing, 39 (28\%) being upcoming, and 8 (6\%) being unlikely. The agreement between the authors and experts and the LLM's classification was 91\%. Disagreements were about 12 uses related to environmental sustainability, agriculture, farming, and climate change mitigation, which LLM marked as existing, and the experts as unlikely. 10 uses (7\%) were identified as unacceptable, 66 (48\%) as high risk, and 62 (45\%) as limited or low risk. The agreement between the authors and experts and the LLM's classification was 90\%. Disagreements were about 14 uses, for which experts found insufficient information to derive risk label, while the LLM classified them as low risk.

By comparing the correctness scores, we found that 93\% of the risks, 95\% of the mitigations, and 82\% of the benefits were correct. The average agreements across each set of three annotators labeling the same parts of the data, as measured using the intraclass correlation coefficient, were 25\% for the risks (fair), 23\% for the mitigations (fair), and 47\% for the benefits (moderate agreement). As for the illustrations of uses, 126 (91\%) were correct, while 12 (9\%) needed re-generation due to references to national symbols ($n$ = 2), incorrect cultural depictions ($n$ = 5), and insensitive gender role representations ($n$ = 5).

\subsection{Meeting the Remaining Four Design Requirements}
\label{sec:design}

\begin{figure*}[t!]
  \centering
\includegraphics[width=\textwidth]{figures/story_flow.jpg}
  \caption{The interface of the Atlas of AI Risks meets six design requirements: mapping many uses of technology (R1), presenting a balanced assessment of their risks and benefits (R2) categorizing them for better understanding (R3), reducing their complexity (R4), making them relevant to ordinary individuals (R5), and making their exploration engaging (R6).}
  \label{fig:atlas}
\end{figure*}

\begin{figure}[t!]
\includegraphics[width=1\columnwidth]{figures/dashboard.pdf}
  \caption{The final dashboard for use exploration. It includes an impact assessment card available in two versions—a brief tooltip (a) and a detailed profile (b) listing risks, benefits, and mitigations—as well as interactions for use browsing, onboarding, and exploration tracking (c-g).}
  \label{fig:dashboard}
\end{figure}

We describe the design decisions, informed by previous research on effective communication with the public, made to ensure the tool meets the remaining design requirements.

\smallskip
\noindent\textbf{(R3) Structured uses.} 
We use a map atlas metaphor, with each use as a dot in a two-dimensional space. This visually emphasizes the need to assess risks per use and helps people understand the overall probability of risks of facial recognition \cite{visWhatWorks2021}. We then adopted three methods for varying analytical skills: spatial distribution based on semantic similarity (for a continuous view), functionality to split uses by risk (for group comparison), and new data dimensions for color-coding uses (for a discrete view). First, to spatially distribute the uses (Figure \ref{fig:atlas}a), we created sentence-level BERT (SBERT) embeddings for each use description using the \textit{paraphrase-distilroberta-base-v2} model. We used SBERT's standard settings, and showed the resulting embeddings using a JavaScript-based t-distributed stochastic neighbor embedding algorithm. Second, to facilitate comparisons between the high-risk and low-risk uses, we split their dots vertically (Figure \ref{fig:atlas}b) and added the interaction to group the uses back together \cite{visWhatWorks2021}. Third, to color-code the data, we used the existing AI Harm Taxonomy for the AI Incident Database \cite{harmTaxonomy}. We manually label each use case based on its area of application, the types of affected subjects and supervising users, as well as its impacts on critical infrastructure, children, entertainment, and the public sector (Figure \ref{fig:atlas}c).

\smallskip
\noindent\textbf{(R4) Reduced complexity.} 
We adopted a frame-based Martini Glass narrative structure \cite{narrativeViz} and a progressive disclosure design \cite{progressiveDisclosure} to reveal information gradually.

First, we unfold the complexities of assessing risks through five distinct story sections (Figure \ref{fig:atlas}, R4). The first section explains the technological features of facial recognition and introduces its 138 uses, each represented by a dot. The second section highlights the dots representing daily uses of facial recognition. The third section shows how uses can vary in risk, illustrated through an animated transition that categorizes the dots into unacceptable, high, or low risk groups. The fourth section explains the common characteristics of each risk group, emphasizing through color-coding how, despite regulations, all uses can still pose harm. The final section introduces a dashboard that encourages users to explore the uses and to find ways to mitigate risks.

Second, we paired each use with an impact assessment card that comes in two versions: a brief tooltip accessible with mouseover (Figure \ref{fig:dashboard}a), and a detailed profile accessible with a click (Figure \ref{fig:dashboard}b). The tooltip includes an illustration of the use, its short description, and a tag indicating its level of risk according to the EU AI Act. The profile includes four sections, starting with a use summary box that contains an illustration, a long description, and the overall risk level. The subsequent three sections detail the benefits, risks, and mitigations, with benefits and risks grouped by technical capability, human interactions, and social impact, and checkboxes indicating who is affected.

\vspace{12pt}
\noindent\textbf{(R5) Broad appeal.} 
We used different colors to separate the uses into two groups: daily (like unlocking smartphones) and non-daily (like tracking illegal poaching). Nearly half of the dots represented daily uses, showing how facial recognition technology is part of everyday life and making the data more relatable (Figure \ref{fig:atlas}, R5). Additionally, we carefully phrased the mitigations in the impact assessment card to be understandable regardless of (non)technical background.

\smallskip
\noindent\textbf{(R6) Engaging exploration.} 
The final dashboard includes interactions for atlas browsing, onboarding, and exploration tracking (Figure \ref{fig:dashboard}). Users can browse the atlas using the search box for keyword matching (Figure \ref{fig:dashboard}c) and the filter box for color-coding dots based on ten categories (Figure \ref{fig:dashboard}d). The onboarding guide covers the interface, tooltips, profiles, search, and filtering options (Figure \ref{fig:dashboard}e). For exploration tracking, micro-interactions include a counter that changes color as more uses are explored (Figure \ref{fig:dashboard}f) and animations for already explored uses (Figure \ref{fig:dashboard}g). 