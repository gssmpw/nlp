In this work, we propose Control Variates Evaluation to reduce human annotation costs while maintaining unbiasedness. Our method demonstrates significant savings in human annotations across benchmarks like Chatbot Arena and MT Bench, aligning well with theoretical predictions. This provides a scalable and cost-effective alternative to full human evaluation without compromising reliability.

We only study the most canonical evaluation of head-to-head win rate between two LLMs, and it is an interesting future direction to explore more nuanced human evaluation metrics and complex evaluation settings, including multi-model ranking and fine-grained assessments.  Other future work can focus on improving synthetic feedback through adaptive selection or ensembling multiple evaluators. 





