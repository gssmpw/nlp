\subsection{LLM Evaluation}
We consider the problem of evaluating LLMs performance through head-to-head comparisons, via human preference judgments. Given a set of prompt $\mathcal{X}$,  we compare two LLMs $\ell^1$ and $\ell^2$ by estimating the win rate of $\ell^1$ over $\ell^2$ on $\mathcal{X}$. 

Formally, we independently sample a prompt $x \in \mathcal{X}$, and sample two responses $y^1 \sim \ell^1(\cdot \mid x)$ and $y^2 \sim \ell^2(\cdot \mid x)$ from $\ell^1$ and $\ell^2$ respectively. We then ask human annotators to choose the better response with label $z\prn*{y^1 \succ y^2}$, where $$z\prn*{y^1 \succ y^2} = \begin{cases} 1 & \text{if } y^1 \text{ is preferred over } y^2, \\ 0 & \text{if } y^2 \text{ is preferred over } y^1, \\ 0.5 & \text{if tie}. \end{cases}$$
We will use the shorthand $z$ sometimes in the rest of the text when the context is clear. The win rate of $\ell^1$ over $\ell^2$ on the prompt $x$ is defined as 
\begin{align*} &p\prn*{\ell^1 \succ \ell^2} := \E_{x,y^1,y^2}\brk*{z\prn*{y^1 \succ y^2}},
\end{align*}
i.e., the averaged human preference over the prompt set, and $\E_{x,y^1,y^2}[\cdot] := \E_{x \sim \mathrm{Uniform}(\mathcal{X})} \left[ \E_{y^1 \sim \ell^1(\cdot \mid x), y^2 \sim \ell^2(\cdot \mid x)}\brk*{\cdot} \right]$.
To estimate $p\prn*{\ell^1 \succ \ell^2}$ empirically, we collect an evaluation dataset $\mathcal{D^{\mathsf{eval}}} = \{(x_i, y_i^1, y_i^2)\}_{i=1}^n$, estimate human preference $z_i = z(y^1_i \succ y^2_i)$ with $z_i^{\mathsf{em}}$ and output the empirical average $\widehat p^{\mathsf{em}}\prn*{\ell^1 \succ \ell^2} = \frac{1}{n} \sum_{i=1}^n z_i^{\mathsf{em}}$ as the estimate of the win rate. Our goal is to minimize the number of human annotations involved in the process while keeping $\widehat p^{\mathsf{em}}$ close to $p$. \loose

\subsection{Human and Synthetic Evaluation}
\label{sec:eval_baseline}

\emph{Human Evaluation} annotates every sample $(x_i, y_i^1, y_i^2)$ in $\mathcal{D^{\mathsf{eval}}}$ with human, i.e. let $z_i^{\mathsf{em}} := z_i$. This makes the evaluation unbiased. However, leveraging human annotator is extremely expensive, but without enough amount of samples $n$, the empirical mean $\widehat p^{\mathsf{em}}(\ell^1 \succ \ell^2)$ can be very noisy due to high variance from a small sample size.

On the other hand, \emph{Synthetic Evaluation} generates preference estimates $\hat z(y_i^1\succ y_i^2)$ using a reward model or LLM (e.g., GPT-4) \citep{zheng2023judging} on every sample. Although it completely obviates the need for human annotations, the evaluation is biased and can lead to inaccurate win rate prediction. 

\subsection{Other Notations}
For two one-dimensional random variables $x$ and $y$, we use $\cov[x,y]$, $\corr[x,y]$ to denote the covariance and correlation coefficient between $x$ and $y$, respectively. We use $\var[x]$ to denote the variance of $x$. Let $\{x_i\}_{i=1}^n$, $\{y_i\}_{i=1}^n$ be samples of $x$ and $y$, respectively, we abuse the notation and use $\var\Mp{\{x_i\}_{i=1}^n}$  for the empirical variance of $\{x_i\}_{i=1}^n$, and $\cov\Mp{\{x_i\}_{i=1}^n, \{y_i\}_{i=1}^n }$, $\corr\Mp{\{x_i\}_{i=1}^n, \{y_i\}_{i=1}^n }$ for the empirical covariance and correlation coefficient between $\{x_i\}_{i=1}^n$ and $\{y_i\}_{i=1}^n$ respectively. 
