To evaluate the performance of control variates in practice, we conduct experiments on real-world datasets to mainly answer the following questions:
\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item How does Control Variates Evaluation compare to Human Evaluation and Synthetic Evaluation (c.f. \Cref{sec:eval_baseline})?
    \item How does the finetuning process of the synthetic evaluator affect the human annotation saving?
\end{enumerate}

\begin{figure*}[ht]
    \centering
    \subfloat[Skywork-8B]{
    \centering
    \includegraphics[width=0.4\linewidth]{chatbot-arena_skywork_pretrained_var.png}
    }
    \subfloat[Skywork-8B (ft)]{
    \centering
    \includegraphics[width=0.4\linewidth]{chatbot-arena_skywork_finetuned_var.png}
    }
    \caption{Averaged mean-square error versus number of human annotations for Skywork-8B (pretrained and finetuned) on Chatbot Arena. The $x$-coordinate of curves ``Human'' and ``Control Variates'' correspond to the number of human annotations \citep{zheng2023judging}. The curve ``Human (shifted)'' is derived by horizontally scaling the Human Evaluation curve by $(1-s)$, in which $s$ is the averaged human annotation saving ratio in \Cref{tab:result_save}. The averaged mean-square error of Control Variates Evaluation converges to near 0, indicating that it has negligible bias. The human annotation saving ratio aligns perfectly with the actual variance relationship between Human Evaluation and Control Variates Evaluation. }
    \label{fig:bootstrap}
\end{figure*}

\begin{figure}[ht]
    \centering
    \subfloat[Chatbot Arena]{ 
    \centering
    \includegraphics[width=0.47\linewidth]{chatbot-arena_pretrain_finetune_v2_small.png}
    }
    \hfill
    \subfloat[MT Bench]{ 
    \centering
    \includegraphics[width=0.47\linewidth]{mt-bench_pretrain_finetune_v2_small.png}
    }
    \caption{Averaged human annotation saving ratio before and after fine-tuning for GRM-2B and Skywork-8B on Chatbot Arena and MT-Bench. Under all setups, we observe at least 5\% increase in the saving ratio.}
    \label{fig:pretrain_finetune}
\end{figure}

\subsection{Setup}
\label{sec:exp_setup}
\paragraph{Synthetic evaluators.} Towards a comprehensive analysis, we experiment with synthetic evaluators across various model types and sizes, including GRM-Gemma-2B-sftreg (\textbf{GRM-2B}) \citep{yang2024regularizing}, ArmoRM-Llama3-8B (\textbf{ArmoRM-8B}) \citep{ArmoRM}, 
Skywork-Reward-Llama-3.1-8B-v0.2  (\textbf{Skywork-8B}) \citep{liu2024skywork} as well as \textbf{GPT-4} \citep{achiam2023gpt}.

\paragraph{Finetuning procedure.}
The testing of Control Variates with finetuning (Line \ref{line:finetune} of \Cref{alg:cv}) is done in a cross-validation manner.
Suppose there are $K$ LLMs generating responses in the evaluation dataset. Our finetuning procedure trains $K$ reward models, each by leaving out the data for a specific LLM. That is, for each LLM $k$, we finetune the reward model on the head-to-head comparisons over the remaining $K-1$ LLMs. 
This finetuned reward model is then evaluated on the head-to-head comparisons involving LLM $k$ against the other $K-1$ models.
When comparing Control Variates Evaluation with finetuning and Synthetic Evaluation, we apply the same cross-validation procedure to Synthetic Evaluation for a fair comparison.

We tested Control Variates Evaluation with finetuning on GRM-2B and Skywork-8B models, which will be referred to as \textbf{GRM-2B (ft)} and \textbf{Skywork-8B (ft)} respectively.

\paragraph{Benchmark.} We choose LLM evaluation datasets with abundant and trustworthy human annotations. The datasets we considered are:
\begin{itemize}
    \item \emph{ChatBot Arena} \citep{zheng2023judging} contains 33k human-annotated preferences. The responses are generated by 20 models, i.e.,  190 LLM pairs in total. There are 121 pairs that have more than 100 annotations.
    \item \emph{MT Bench} \citep{zheng2023judging} contains about 3.36k human-annotated preferences. The responses are generated by 6 models, i.e., 15 LLM pairs in total. There are 14 pairs that have more than 100 annotations.
\end{itemize}

\subsection{Control Variates Evaluation v.s. Human Evaluation}
\label{sec:exp_cv_human} 
As suggested in \Cref{sec:method_motiv}, the human annotation saving ratio is a practical metric to measure the performance of Control Variates Evaluation. Therefore, we will first present the human annotation saving ratio on different evaluators and benchmarks. After that, we demonstrate that this theoretical measure matches perfectly with the actual variance reduction effect. 

\paragraph{Human annotation saving ratio on different benchmarks and synthetic evaluators.}
For each synthetic evaluator and benchmark, we test the human annotation saving ratio on every LLM pair that have at least 100 human annotations. In order to clearly present the result, we take the mean of the ratios across different LLM pairs to get the average human annotation saving ratio of that evaluator on the benchmark. The result is presented in \Cref{tab:result_save}. We defer the human annotation saving ratio on each LLM pair in \Cref{sec:app_saving}.

\begin{table}[t]
    \centering
    \caption{Averaged human annotation saving ratio across different synthetic evaluators on Chatbot Arena and MT Bench. The averaged human annotation saving ratio is the mean of human annotation saving ratios on LLM pairs with at least 100 human annotations. }
    \vskip 0.15in
    \begin{tabular}{ccc}
    \toprule
       & Chatbot Arena & MT Bench\\
    \hline
     GRM-2B & 10.6\% & 5.7\% \\
    GRM-2B (ft) & 17.1\% & 10.9\% \\
     Skywork-8B & 8.3\% & 7.5\% \\
     Skywork-8B (ft)& \textbf{24.8}\% & \textbf{12.6}\% \\
     ArmoRM-8B & 12.2\% & 9.6\% \\
     GPT-4 & 12.2\% & 11.9\% \\
     \bottomrule
    \end{tabular}
    \label{tab:result_save}
\end{table}




For off-the-shelf evaluators, GPT-4 achieves high saving ratio on both benchmarks. However, an 8B reward model like ArmoRM-8B has comparable performance. Using the finetuning option of Control Variates Evaluation,  Skywork-8B (ft) surpasses the performance of GPT-4 on both benchmarks. With finetune, a small model (GRM-2B (ft)) can also match or outperform GPT-4 in averaged human annotation saving. This means that we can save from 10\% to 20\% human annotations using an easy-to-deploy reward model at nearly no cost.

\paragraph{Theory matches practice.}
We empirically justify that the theoretical human annotation saving ratio aligns well with the practical variance reduction ratio. Besides, we verify the claim in \citep[Chapter 8.9]{mcbook} that \Cref{eq:alpha_est} leads to negligible bias.
\loose

First, we measure the estimated mean square error of Human Evaluation and Control Variates Evaluation w.r.t number of human samples for each fixed LLM pair via bootstrapping.  
That is, we repeatedly run the evaluation method 1000 times with a fixed number of human annotations, collect the output win rate estimates, and compute the mean-square error, where the ground truth win-rate is the averaged human preference on all data of that LLM pair. For Human and Control Variates Evaluation, we run bootstrapping using different numbers of human annotations on different LLM pairs and plot a curve respectively with labels ``Human'' and ``Control Variates'' (c.f. \Cref{fig:var_full}), in which the $y$-axis is the averaged mean square error of the evaluation on different LLM pairs, and the $x$-axis is the number of human annotations. 

Theoretically, the mean-square error can be decomposed into the square of evaluation bias and the variance. Therefore, the mean-square error curve still effectively reflects the variance reduction tendency as the number of human annotations increases, and when the number approaches infinity, we can extract the bias of the evaluation through the limit of mean square error.

Then, we shift the $x$-axis of the Human Evaluation as follows. Suppose $s$ is the averaged human annotation saving ratio we tested in \Cref{tab:result_save}, and $(x,y)$ is a point on the curve of Human Evaluation. Then we shift point $(x,y)$ to $(x (1-s), y)$. After shifting all points of the Human Evaluation, we get a new curve, referred to as \emph{Human (shifted)}. According to \Cref{prop:ctrl_var} (3), the ratio of the number of human annotations in Human Evaluation and Control Variates Evaluation should be $1:(1-s)$ so that they have the same variance. So ideally, the shifted curve of Human Evaluation should coincide with the curve of Control Variates Evaluation. We present the bootstrap curves for Skywork-8B with and without the finetuning procedure on Chatbot Arena in \Cref{fig:bootstrap}. The other results are listed in \Cref{fig:var_full} of Appendix. 

On all figures,
the averaged mean-square error of Control Variates Evaluation converges to near 0, indicating negligible evaluation bias. Furthermore, the shifted curve of Control Variates Evaluation overlaps with that of human evaluation. Therefore, the human annotation saving ratio predicts the actual variance reduction of our algorithm almost perfectly, even if the control variates coefficient $\alpha$ is estimated. This means that we can simply compute the human annotation saving ratio from the correlation coefficient, and then we know whether the synthetic evaluator will bring us the desired variance reduction effect when it is to be used in Control Variates Evaluation.

\subsection{Control Variates Evaluation v.s. Synthetic Evaluation}
\label{sec:bootstrap}
In this section, we compare the error in predicting the win rate between the Control Variates Evaluation and Synthetic Evaluation. The error metric is the mean square error with respect to the ground truth win-rate, which we approximate with the averaged human annotations on all samples of each head-to-head comparison. For Control Variates Evaluation, we use the averaged mean-square error from the previous section. For Synthetic Evaluation, we average the synthetic annotations on all samples of a fixed LLM pair as the predicted win rate and then calculate the mean square error. We also include the averaged mean-square error of human for convenience of comparison. 

\Cref{fig:main} (right) presents the result of finetuned Skywork-8B, and \Cref{fig:bootstrap_err} presents that of GPT-4, both on Chatbot Arena. Other results are deferred to \Cref{fig:bias_full}. Although GPT-4 is claimed to be an accurate evaluator \citep{zheng2023judging}, it still has a significantly high error compared to Control Variates and Human Evaluation.
Similarly, even if we finetune a reward model like Skywork-8B (ft), it also suffers from high error if used in Synthetic Evaluation alone. However, these evaluators can be incorporated into Control Variates Evaluation to achieve much lower evaluation error. \loose

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{chatbot-arena_gpt4_pretrained_err.png}
    \caption{Average mean square error versus number of human annotations for GPT-4 evaluator on Chatbot Arena \citep{zheng2023judging}. Note that even GPT-4 has high bias if used alone for Synthetic Evaluation.}
    \label{fig:bootstrap_err}
\end{figure}

\subsection{How does Finetuning Improve Control Variates Evaluation?}
\label{sec:exp_finetune}
We visualize the averaged human annotation saving ratio before and after finetuning for GRM-2B and Skywork-8B on Chatbot-Arena and MT-Bench in \Cref{fig:pretrain_finetune}. For all experiments, the finetuning procedure provides at least 5\% more saving ratio. Specifically, for Skywork-8B on Chatbot Arena, the saving ratio nearly triples. 

On the other hand, finetuning indeed introduces additional computation requirement. Regarding whether to finetune the evaluator or not, there are two major considerations. The first one is the human annotation saving ratio on the pretrained evaluator. If it is not satisfactory, finetuning can introduce more significant savings if a finetune dataset is available. The other consideration is the number of future tasks, as this is a trade-off between future savings in human annotation cost and the current additional cost of finetuning computation. If there are many future models to evaluate, then finetuning is beneficial because the savings generalize to unseen models.  

\subsection{Control Variates Evaluation for LLM-as-a-judge}\label{sec:exp_llm_as_judge}
Control Variates Evaluation can be similarly applied in the LLM-as-a-judge setting. The difference is that the human annotator is replaced with a strong LLM evaluator, and a smaller, cheaper model plays the role of the synthetic evaluator, to save the cost of querying the expensive model. 

We set GPT-4 as the strong evaluator and test the averaged human annotation saving ratio in the scenario of LLM-as-a-judge, as shown in \Cref{tab:ai_save}. A 2B reward model like GRM-2B can achieve over 20\% saving of GPT-4 annotation on Chatbot Arena and nearly 15\% saving on MT Bench. This can save the cost in LLM-as-a-judge.
\begin{table}[t]
    \centering
    \caption{Averaged strong evaluator's sample saving in LLM-as-a-judge using control variates evaluation. The strong evaluator is GPT-4.}
    
    \begin{tabular}{cccc}
    \toprule
    Weak Evaluator & Chatbot Arena & MT Bench \\
    \hline
     GRM 2B sftreg & 22.8\% & 14.6\% \\
     Skywork 8B & 13.5\% & 15.1\% \\
     ArmoRM 8B & 16.0\% & 18.8\% \\
     \bottomrule
    \end{tabular}
    \label{tab:ai_save}
\end{table}