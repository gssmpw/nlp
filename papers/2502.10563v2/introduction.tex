\begin{figure*}[ht]
    \centering
    \subfloat{
        \centering 
        \includegraphics[width=0.57\linewidth]{workflow.pdf}
    }
    \hfill
    \subfloat{
    \centering
    \includegraphics[width=0.4\linewidth]{chatbot-arena_skywork_finetuned_err.png}
    }
    \caption{(Left) Illustration of Control Variates Evaluation, which makes use of a possibly inaccurate synthetic evaluator to reduce the variance of evaluation, reducing the need of human annotations while preserving unbiasedness.  (Right) Averaged mean square error v.s. number of human annotations for Human Evaluation, Synthetic Evaluation and Control Variates Evaluation using the finetuned Skywork-8B evaluator on Chatbot Arena. The Synthetic Evaluation has high bias, while the bias of Human and Control Variates Evaluations are negligible. Control Variates Evaluation reduces the variance of Human Evaluation. \loose}
    \label{fig:main}
\end{figure*}

Accurately evaluating the performance of large language models (LLMs) is crucial before large-scale deployment. Human judgment remains the gold standard for this evaluation, as it captures nuanced qualities such as coherence, harmlessness, and readability \citep{bai2022training}, while also ensuring alignment with human values \citep{ouyang2022training}. A widely accepted performance metric is the \emph{win rate}, assessed by humans against a reference model \citep{chiang2024chatbot}. However, this approach demands substantial time and financial resources due to human involvement. When conducted with active system users, it may also diminish user experience, see \cref{fig:OpenAI}.

\begin{figure*}[tbhp]
    \centering
    \includegraphics[width=0.8\textwidth]{chatgpt_edit.png}
    \caption{OpenAI's prompting users for feedback; excessive requests may negatively impact user experience.}
    \label{fig:OpenAI}
\end{figure*}

In order to mitigate these challenges, recent works have explored cost-efficient alternatives, most notably the use of synthetic feedback generated by other LLMs, a concept often referred to as ``LLM-as-a-judge" \citep{zheng2023judging,dubois2024length}, to compute the head-to-head win rate. This approach leverages the computational efficiency of LLMs to evaluate other models, reducing the need for extensive human involvement.  Despite its promise, synthetic feedback often introduces biases since LLM can not perfectly reflect human preference, undermining the evaluation reliability \citep{zheng2024cheating}. As a result, a critical need remains for evaluation methods that reduce the cost of human annotation while maintaining the reliability and generalizability. \loose

Besides replacing the evaluator, recently there has been a growing interest in accelerating LLM evaluation \citep{ye-etal-2023-predictable,polo2024tinybenchmarks,zhou2024speeding} with smaller datasets. However, previous methods only focused on reducing the number of prompts in a specific benchmark with predefined answers (e.g., math problems). Thus it is unclear if these methods generalize or apply to other tasks. For example, in math benchmark it is easy to find some problems that are ``representative'' of the whole benchmark, but in general the prompts are more diverse and less structured, and sometimes they are generated on the fly, such as when a user interacts with a language model via APIs. \loose

Towards reliable and cost-efficient LLM evaluation, in this work we propose to leverage LLM generated synthetic feedback to reduce the number of human annotations, in the standard head-to-head win rate setting \citep{chiang2024chatbot}. Specifically, we propose \emph{Control Variates Evaluation} (\Cref{fig:main} left), an unbiased LLM evaluation method based on the classical control variates technique  \citep{lavenberg1981perspective} that combines human annotations and synthetic feedback. Note that there are previous works \citep{chaganty2018price, boyeau2024autoeval} that apply control variates to machine learning evaluation, but they study settings like single-response natural language evaluation or BT modelling \citep{bradley1952rank}. Therefore, the performance of control variates in head-to-head win rate estimation still requires thorough investigation.

In our work, we theoretically show that Control Variates Evaluation enjoys a lower variance, and thus it requires fewer human annotations to achieve the same level of accuracy on the win rate estimation. 
Empirically, Control Variates Evaluation enjoys significant human annotation saving for various types of synthetic evaluators, from a small reward model with 2B parameters to LLMs such as GPT-4. In addition, we can further reduce human annotations by finetuning the synthetic evaluators on existing human annotations for other LLMs. Note that the cost of control variates is minimal as it only requires some additional synthetic feedbacks, which can be generated at a low cost. Somehow surprisingly, the synthetic evaluators that contribute to such achievement are inaccurate themselves and have high prediction bias (c.f. \Cref{fig:main} right). 
 \loose

Besides the advantage of reducing the number of human annotations, Control Variates Evaluation also has a predictable saving, one that can be estimated from the data and one which depends on how strongly the synthetic feedback correlates with human judgments. This is in contrast to the all existing methods that do not provide predictions  on the potential saving. Based on the theoretical guarantee, we propose \emph{human annotation saving ratio} as a metric to evaluate our method, which can be computed through a few human annotations without actually running the evaluation. We demonstrate through experiments that this metric perfectly reflects the practical variance reduction effect in Control Variates Evaluation. \loose

In summary, our contribution is three folds:
\begin{enumerate}[ topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item We introduce Control Variates Evaluation to reduce the number of human annotations in head-to-head win rate estimation with zero bias, resulting in a reliable, cost-efficient and task-agnostic LLM evaluation method.
    \item We demonstrate the viability of improving human annotation saving through fine-tuning.
    \item We propose the human annotation saving ratio as the data-dependent metric to predict the saving in human data when using the Control Variates Evaluation.
\end{enumerate}
We believe our work is a first step towards principled efficient LLM evaluation and can be combined with various existing and future works.  
Our code is available at \url{https://github.com/Zanette-Labs/control_variates_evaluation}.