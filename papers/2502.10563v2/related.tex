\subsection{LLM Evaluation: Metric, Benchmark and Systems}
The earliest attempt for LLM evaluation includes rule based metrics such as BLEU \citep{papineni2002bleu} and ROUGE \citep{lin2004rouge}, which only measures the similarity between the model generation and the reference text. Going beyond rule-based metrics, LLM evaluation has been proposed, with earlier works using LLM to compute similarity \citep{Zhang2020BERTScore,yuan2021bartscore}. Recently, LLM-as-a-judge has been proposed to evaluate LLMs \citep{zheng2023judging,dubois2024length}, by querying powerful LLMs to generate preference of generations between different models, with the hope that the powerful LLMs can serve as a proxy for human evaluation. Towards real human evaluation, very few public systems exist due to their high cost and time-consuming nature, with the large-scale community collective effort Chatbot Arena \citep{chiang2024chatbot} being the most notable one.

\subsection{Speeding Up LLM Evaluation}
Recently there has been a surge of research on speeding up LLM evaluation, with the goal of reducing the cost and time of evaluating LLMs. One approach is to use heuristics to minimize the number of prompts or tasks to evaluate, with the hope that the selected subset can represent the whole distribution of the prompts or tasks \citep{ye-etal-2023-predictable,perlitz2023efficient,polo2024tinybenchmarks}. 

The other approach is to leverage active learning or bandit algorithms to select a subset of the prompts: \citep{polo2024efficient,zhou2024speeding,li2024active}. However, these methods are still limited by the requirement to operate within a specific benchmark with prefined answers, and thus can not be applied to human evaluation, the focus of our work. In addition to the essential benefit that human evaluation can provide, note that it is more challenging because it is task-agonistic and typically has less structure than any specific benchmark.

\subsection{Control Variates, Application, and related techniques}

Control variates is a well-known variance reduction technique in Monte Carlo sampling \citep{mcbook}, with applications to finance \citep{broadie1998risk,hesterberg1998control,kemna1990pricing,glasserman2004monte}. In recent years, it has also been applied to various areas of machine learning, such as variational inference \citep{geffner2018using}, bandits \citep{verma2021stochastic}, optimization \citep{yuan2024mars}, computer graphics \citep{rousselle2016image, muller2020neural}. 
In particular \citep{chaganty2018price} uses control variates to evaluate natural language metrics, but it is restricted to single response evaluation. In our work, we extend control variates evaluation to pairwise LLM comparison.

Prediction-Powered Inference (PPI, and PPI++) \citep{angelopoulos2023prediction, angelopoulos2023ppi++, boyeau2024autoeval} is a related technique which uses variance reduction  to improve the MLE objective.  \cite{boyeau2024autoeval} applies PPI++ to estimate practical metrics in machine learning, such as accuracy, correlation and BT model \citep{bradley1952rank} in pairwise model comparisons. It differs from our work which conducts an in-depth study of control-variates to accelerate head-to-head win rate estimation.