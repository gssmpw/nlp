\begin{algorithm}[tb]
   \caption{Control Variates Evaluation}
   \label{alg:cv}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Evaluation dataset $\cD^{\mathsf{eval}} = \Bp{(x_i,y_i^1, y_i^2)}_{i=1}^n$, \\
   human annotation budget $k$, \\
   \STATE {\bfseries Optional Input:} Finetune dataset $\cD^{\mathsf{finetune}} = \Bp{(x_j,y_j^1, y_j^2)}_{j=1}^m$ with human annotations $\Bp{z_j}_{j=1}^m$.
   \STATE (Optional) Finetune the synthetic evaluator on $\cD_{\mathsf{finetune}}$.   
   \alglinelabel{line:finetune}
    \STATE Get synthetic evaluations $\hat z_1, \hat z_2, \cdots, \hat z_n$ on $\cD^{\mathsf{eval}}$.
    \alglinelabel{line:syn_z}
    \STATE Sample $k$ data from $\cD^{\mathsf{eval}}$ and get human annotations $z_{i_1}, z_{i_2}, \cdots, z_{i_k}$.
    \alglinelabel{line:human_z}
    \STATE Estimate $\mu_{\hat z} = \frac{1}{n} \sum_{i=1}^n \hat z_i$.
    \alglinelabel{line:est_mu}
    \STATE Estimate $\alpha$ using $\Bp{z_{i_j}}_{j=1}^k$ and $\Bp{\hat z_{i_j}}_{j=1}^k$ by \Cref{eq:alpha_est}
    \alglinelabel{line:est_alpha}
    \STATE Output the estimated win rate 
    \[\frac{1}{k} \sum_{j=1}^k z_{i_j} - \alpha\prn*{\frac{1}{k}\sum_{j=1}^k \hat z_{i_j} - \mu_{\hat z} }.
    \]
    \alglinelabel{line:output}
\end{algorithmic}
\end{algorithm}

In this section, we introduce \emph{Control Variates Evaluation}, which combines human and synthetic annotations to realize a variance-reduced unbiased evaluation method, based on control variates \citep{lavenberg1981perspective}. We first recap the classical control variates method in the context of LLM evaluation, and then formally describe how to adapt the control variates method to make it applicable in practice. Finally, we briefly discuss its application in the LLM-as-a-judge setting \citep{zheng2023judging}.

\subsection{Control Variates}
\label{sec:method_motiv}
Given a sample $(x,y^1, y^2)$ with human preference $z$ and synthetic preference $\hat z$, we treat $z$ as the random variable for which we want to estimate its mean. Using $\hat z$ as the control variate, the classical control variates  approach \citep{lavenberg1981perspective} constructs a new estimated preference:
\begin{align}
    & z^{\mathsf{em}} := z^{\mathsf{cv}; \alpha} = z - \alpha (\hat z - \mu_{\hat z}), 
    \label{eq:cv}
\end{align}
where 
$
\mu_{\hat z} = \E_{x, y^1, y^2}\brk*{\hat z\prn*{y^1 \succ y^2}}
$
is the \textbf{synthetic win rate}, and 
$\alpha \in \R$ is the \textbf{control variates coefficient} used to control the variance of $z^{\mathsf{cv}; \alpha}$. Intuitively, $\mu_{\hat z}$ cancels out the bias incurred by the control variate $\hat z$, keeping the estimate unbiased. In addition, assuming that $\hat \mu_z$ is known, we can guarantee variance reduction compared to human evaluation, as stated by
\begin{proposition}[Control Variates Properties \citep{lavenberg1981perspective} ]
\label{prop:ctrl_var}

Suppose the expectations, variances, covariances and correlation coefficients, unless otherwise stated, are taken under the distribution $x \sim \mathrm{Uniform}(\mathcal{X})$, $y^1 \sim \ell^1(\cdot \mid x)$, $y^2 \sim \ell^2(\cdot \mid x)$. Then the control variates estimate $z^{\mathsf{cv}; \alpha}$ enjoys the following properties
\begin{enumerate}[label=(\arabic*)]
    \item (Unbiasedness) For any $\alpha \in\R$, we have \\
    $\E[z^{\mathsf{cv}; \alpha}] = p(\ell^1\succ \ell^2)$.
    \item (Variance Reduction) Let $\rho = \corr[z, \hat z]$ be the correlation coefficient between human and synthetic preference. Then we have 
    \begin{align*}
    \min_{\alpha \in \R}\var[z^{\mathsf{cv}; \alpha}] = \prn*{1-\rho^2} \var [z].
    \end{align*}
    The minimum is achieved if and only if $\alpha$ equals 
    \begin{align*}
    \alpha^* = \frac{\cov[z, \hat z]}{\var[\hat z]}.
    \end{align*}
    \item  (Human Annotation Saving)  Given an evaluation dataset $\mathcal{D_{\mathsf{eval}}} = \{(x_i, y_i^1, y_i^2)\}_{i=1}^n$, in which $\{x_i\}_{i=1}^n$ are sampled i.i.d. from $X$, $y_i^1 \sim \ell^1(\cdot \mid x_i)$, $y_i^2 \sim \ell^2(\cdot \mid x_i)$ ($i\in [n]$). Let $\{i_j\}_{j=1}^m$ be independently sampled from $[n]$. Then when $m = (1-\rho^2) n$, we have 
    \begin{align*}
    \var\Mp{\frac{1}{m}\sum_{j=1}^m z^{\mathsf{cv}; \alpha^*}_{i_j}} = \var\Mp{\frac{1}{n}\sum_{k=1}^n z_{k}}
    \end{align*}
    Here the variance on the right hand side is taken by the randomness of sampling $\{(x_i, y_i^1, y_i^2)\}_{i=1}^n$. The variance on the left hand side is taken by the randomness of sampling $\{(x_i, y_i^1, y_i^2)\}_{i=1}^n$ as well as that of sampling $\{i_j\}_{j=1}^m$.
\end{enumerate}
\end{proposition}
We provide the proof in \Cref{sec:proof} for completeness.

\paragraph{Human annotation saving ratio.} \Cref{prop:ctrl_var} immediately suggests that the control variates method can \emph{reduce the percentage} of human annotations by $\rho^2$ while maintaining the same variance as that of Human Evaluation, with negligible cost of querying the synthetic evaluator. Therefore, $\rho^2$ is an important metrics to measure the performance of control variates method. We formally define it below.

\begin{definition}[Human annotation saving ratio]
The \emph{human annotation saving ratio} of a synthetic evaluator w.r.t. LLMs $\ell^1,\ell^2$ and prompt set $\cX$ is defined as 
\begin{align*}
    \rho^2 = \Sp{\corr\limits_{x,y^1,y^2}[z(y^1\succ y^2), \hat z(y^1\succ y^2)]}^2.
\end{align*}
Here $z(y^1\succ y^2)$ is the human preference, and $\hat z(y^1\succ y^2)$ is the synthetic prefence. The correlation coefficient is computed under the distribution $x \sim \mathrm{Uniform}(\mathcal{X}), y^1 \sim \ell^1(\cdot \mid x), y^2 \sim \ell^2(\cdot \mid x)$. 

\end{definition}


 Nonetheless, to apply control variates approach in the context of LLM evaluation, we still face the following challenges: 1) How to estimate the synthetic win rate $\mu_{\hat z}$? 2) How to compute the correlation coefficient $\alpha$ in practice to achieve the lowest variance? 3) How to improve the correlation coefficient if the off-the-shelf automatic evaluator does not give a satisfactory human annotation saving ratio?
In the following, we discuss how to construct the control variates for LLM evaluation.

\subsection{Control Variates Evaluation}
\Cref{alg:cv} describes the full procedure of control variates evaluation. Same as other evaluation methods, control variates evaluation requires an evaluation dataset $\cD^{\mathsf{eval}} = \Bp{(x_i,y_i^1, y_i^2)}_{i=1}^n$.
The Control Variates Evaluation consists of the following steps: \loose
\paragraph{Synthetic annotation gathering (Line \ref{line:syn_z}).}
We generate synthetic preferences $\hat z_i \in [0,1]$ from an automatic annotator for all samples in the evaluation dataset. Synthetic preferences can be generated in various ways depending on the type of automatic annotator. For an LLM annotator like GPT-4, we query the model to directly generate the preference in natural language. If the automatic annotator is a reward model, we can query the rewards $r_i^1$ and $r_i^2$ from the two responses $y_i^1$ and $y_i^2$ respectively, and then compute the synthetic preference as the Bradley-Terry score of the two rewards \citep{bradley1952rank}, i.e.,  
\[
    \hat z_i = \frac{1}{1 + \exp(r_i^2-r_i^1)}.
\]
\paragraph{Human annotation sampling (Line \ref{line:human_z}).}
We query the human annotator and obtain human preference $z \in \crl{0,0.5,1}$. Instead of annotating all the samples like in Human Evaluation, we only annotate $k$ samples randomly drawn from the evaluation dataset, in which $k$ is the number of human annotations we want to use. Increasing $k$ lowers the variance of the estimation but raises the cost of evaluation. 

\paragraph{Synthetic win rate estimation (Line \ref{line:est_mu}).} Since $\mu_{\hat z}$ is unknown in practice, we estimate it by averaging the synthetic evaluator's preferences on the whole evaluation dataset. In other words,
$
\mu_{\hat z} := \frac{1}{n} \sum_{i=1}^n \hat z_{i}.
$

\paragraph{Control variates coefficient computation (Line \ref{line:est_alpha}).} 
Although \Cref{prop:ctrl_var}(2) already shows the optimal $\alpha$, the covariance between human and synthetic annotations as well as the variance of synthetic annotations needs to be estimated via sampling. Since human annotations are involved in the computation, we reuse the human annotations $\Bp{z_{i_j}}_{j=1}^k$:
\begin{align}
\alpha := \frac{\cov\Mp{\Bp{z_{i_j}}_{j=1}^k, \Bp{\hat z_{i_j}}_{j=1}^k}}{\var\Mp{\Bp{\hat z_{i_j}}_{j=1}^k}}.
\label{eq:alpha_est}
\end{align}

It is standard practice in control variates to estimate $\alpha$ with \Cref{eq:alpha_est} \citep[Chapter 8.9]{mcbook}.  Although it introduces some correlation between $\alpha$ and the final estimator, and thus the estimated win rate in \Cref{alg:cv} is technically biased, the incurred bias is usually negligible, and it is standard practice to ignore such bias \citep[Chapter 8.9]{mcbook}. We also validate this practice through experiments in \Cref{sec:exp_cv_human}. 
\loose

\paragraph{Win rate estimation (Line \ref{line:output}).}
After we obtain estimations of the synthetic win rate $\mu_{\hat z}$, and the control variates coefficient $\alpha$, we can apply \Cref{eq:cv} to get the variance-reduced preference estimates $\Bp{z^{\mathsf{cv}; \alpha}_{i_j}}_{j=1}^k$ for the samples we collected with human annotations. Then we output the win rate estimate by taking the average over the preference estimates:
\begin{align}
    \hat p^{\mathsf{em}}(\ell^1\succ \ell^2) & = \frac{1}{k} \sum_{j=1}^k z^{\mathsf{cv}; \alpha}_{i_j} \notag \\
    & = \frac{1}{k} \sum_{j=1}^k z_{i_j} - \alpha\Sp{\frac{1}{k}\sum_{j=1}^k \hat z_{i_j} - \mu_{\hat z} }.
    \label{eq:output}
\end{align}

\paragraph{(Optional) Synthetic evaluator finetuning (Line \ref{line:finetune}).} 
On many popular LLM evaluation benchmarks such as Chatbot Arena and MT Bench \citep{zheng2023judging}, there are abundant off-the-shelf human annotations for pre-generated language model responses. Now suppose we have a new LLM and we want to compare it with the existing ones in the benchmark. Can we make use of these existing human annotations to help reduce the human annotations needed in Control Variates Evaluation? 

Recall that the human annotation saving ratio is $\rho^2$, the square of correlation coefficient between human and synthetic annotations. One natural idea is to raise the correlation coefficient by finetuning the synthetic evaluator with existing human annotations, to save future human annotations. 

Formally, suppose that we have a finetune dataset $\cD^{\mathsf{finetune}} = \Bp{(x_j,y_j^1, y_j^2}_{j=1}^m$ with precollected human annotations $\Bp{z_j}_{j=1}^m$. We discard the ties and assume $z_j\in \{0,1\}$ for all $1\leq j\leq m$. 
In case that the synthetic evaluator is a reward model, we finetune the evaluator on $\cD^{\mathsf{finetune}}$ to maximize the Bradley-Terry score \citep{bradley1952rank} on the chosen response: 
\begin{align*}
    \mathrm{BT}\prn*{r_j^1, r_j^2,z_j} = \frac{z_j}{1+\exp(r_j^2-r_j^1)} + \frac{1-z_j }{1+\exp(r_j^1-r_j^2)}.
\end{align*}
After finetuning, we can expect an increase in the correlation coefficient $\rho$ and thus also the human annotation saving ratio when we want to evaluate the win rate between a new LLM pair on the same benchmark. Note that the \emph{dataset used for finetuning the synthetic annotator contains responses generated by LLMs that are different from the LLMs that we wish to evaluate}, i.e., the responses in evaluation dataset are out of distribution w.r.t. the finetune dataset. Nevertheless, we show in the experiment section (c.f. \Cref{sec:exp_finetune}) that the finetuned model still generalizes well in terms of the correlation coefficient to the human annotations.

\paragraph{Summary.}
We offer several remarks:
\begin{itemize}
\item Our construction of control variates is \emph{task-agnostic}, i.e, we do not leverage any specific structure or knowledge of the prompt set $\cX$. 
\item The method is \emph{hyperparameter-free} as parameters for control variates like the synthetic win rate $\mu_{\hat z}$ and control variates coefficient $\alpha$ are estimated directly from data. (If fine-tuning is used, one still needs to choose fine-tuning hyper-parameters over a validation dataset) 
\item The performance of Control Variates Evaluation is \emph{predictable}. By sampling a \emph{small} subset of evaluation data, collecting human and synthetic annotations, and computing the human annotation saving ratio, the reduction in human annotations can be accurately estimated without fully performing the evaluation.
In the experiment (cf. \Cref{sec:exp_cv_human}), we show that the saving ratio of human annotations correctly predicts the observed saving.
\end{itemize}