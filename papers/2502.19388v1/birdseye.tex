\section{A Bird's Eye View}
\label{sec:birdseye}

In this section, we explain what makes our method unique by providing a compressed overview of its characteristic features.
We also discuss its limitations and point out a subtle technical challenge.


\subsection{Highlights and Comparison to Other Approaches}

Since we are not the first to address the problem of proving bounds on quantities expressible as $\wpwlpSymb$'s, we now highlight five distinctive features of our approach --- their combination is unique in the literature.
A more in-depth review of related work is deferred to \Cref{sec:relwork}.

Of the following five items, the first two are specific to the way we handle integrals via Riemann sums, whereas the latter three tend to apply to $\wpSymb$-based approaches in general.


\paragraph{An Alternative to Moment-Based Analyses}

A major thread of existing work, e.g.,~\cite{DBLP:conf/sas/ChakarovS14,DBLP:journals/pacmpl/AgrawalC018,DBLP:journals/pacmpl/MoosbruggerSBK22}, circumvents explicit integration altogether.
They achieve this by focusing on classes of programs and properties where, simply speaking, the analysis can soundly replace the distributions in the program by their \emph{mean}~\cite[Appendix A, p.~31]{DBLP:journals/corr/abs-1709-04037} or some higher moment~\cite{DBLP:journals/pacmpl/MoosbruggerSBK22}.
For example, $\wp{\UNIFASSIGN{\pVar}}{\pVar \pVarb}$ is equal to\footnote{In general, replacing $\UNIFASSIGN{\pVar}$ by $\ASSIGN{\pVar}{\tfrac 1 2}$ is sound whenever the post-expectation is linear in $\pVar$. While our approach is compatible with such optimizations, we shall not discuss them any further.} $\wp{\ASSIGN{\pVar}{\tfrac 1 2}}{\pVar \pVarb} = \tfrac 1 2 y$, where $\tfrac 1 2$ is the mean of $\UNIFOVER{0}{1}$.
However, such mean-based analyses do not always suffice.
Indeed, reconsidering the example from \Cref{sec:intro}, the inequality $\wp{\progPiInner}{\pVarcount} \eleq \pVarcount + 0.85$, proved correct with our approach, becomes false if we substitute $\UNIFASSIGN{\pVar}$ and $\UNIFASSIGN{\pVarb}$ in $\progPiInner$ by $\ASSIGN{\pVar}{\tfrac 1 2}$ and $\ASSIGN{\pVarb}{\tfrac 1 2}$, respectively.
In fact, the so-obtained program $\progPiInner'$ satisfies $\wp{\progPiInner'}{\pVarcount} = \pVarcount + 1$ as the point $(\tfrac 1 2, \tfrac 1 2)$ \emph{is certainly inside} the quarter unit circle.


\paragraph{General Conditional Branching}

We make relatively mild assumptions regarding the shape of $\KWIF$- and $\KWWHILE$-guards in the programs --- in practice, general Boolean combinations of the polynomial (in)equalities over all program variables are supported, as in \Cref{fig:intro}.
This is  different from, e.g.,~\cite{DBLP:journals/pacmpl/MoosbruggerSBK22} which restricts to finitely-valued variables in guards or~\cite{DBLP:conf/sas/ChakarovS14,DBLP:conf/popl/ChatterjeeFNH16} which restrict to linear guards.
We achieve this high level of generality thanks to the simplicity of the Riemann approximation which treats integration over the resulting indicator functions in a uniform manner.


\paragraph{Moving Backwards: Verification and Parametric Models}

We conduct a \emph{backward analysis}: 
Given a random variable $\ex$ over final program states (called \emph{post-expectation} in this paper), we approximate the (conditional) weakest pre-expectation $\wp{\prog}{\ex}$, which is \emph{a function of the initial state}.
This is dual to works like~\cite{DBLP:conf/cav/GehrMV16,DBLP:conf/pldi/BeutnerOZ22,wang2024static} that conduct a \emph{forward analysis} to solve a classic Bayesian inference problem, namely to compute a program's posterior density --- \emph{a function of the final state} --- for a given prior distribution.
Let us contrast these two paradigms in greater detail.

Backward analysis, as done in this paper, allows verifying program properties that hold for \emph{all initial states}.
For instance, our framework supports questions such as, \enquote{\emph{Is the posterior mean of $x$ at most twice the initial value of $y$?}}, expressed as $\cwpSymb[\prog](x) \le 2y$.
The backward approach is thus well-suited for tasks such as \emph{verification} --- ensuring a program behaves as intended \emph{for all inputs} --- and \emph{parametric analysis} --- examining how expected behavior changes when program parameters are altered (parameters can be modeled as uninitialized program variables).

Forward analyses, on the other hand, are typically motivated by probabilistic programming languages (PPLs) explicitly designed for automating Bayesian inference, see e.g., \cite{DBLP:journals/corr/abs-1809-10756,dippl,DBLP:journals/jmlr/BinghamCJOPKSSH19,carpenter2017stan}. 
We remark that there are some fundamental discrepancies between these PPLs and the one studied in this paper.
For example, the former usually offer extensive support for general continuous distributions, but do not always support general loops.
Moreover, Bayesian inference problems often require \emph{soft conditioning}, which our approach only encodes as syntactic sugar (see Remark 2).
Finally, some applications of Bayesian inference involve loading and processing datasets with thousands of observations.
In principle, such data could be hardcoded into our programs, but the result would likely be too large for current SMT-based analysis techniques (as indicated by our experiments in \Cref{sec:empirical_eval}).

In summary, our backward approach is particularly suited for proving properties of complex stochastic processes described as probabilistic programs, especially those involving unbounded loops
 However, it is less effective for data-intensive statistical analysis.


\paragraph{A Zoo of Quantities}

The majority of existing methods for (semi-)automated exact analysis of PP with loops and continuous distributions focus on one of these tasks:
(i) bound the posterior distribution~\cite{DBLP:conf/cav/GehrMV16,DBLP:conf/pldi/BeutnerOZ22,wang2024static},
(ii) prove various flavors of (non-)termination, e.g.,~\cite{DBLP:conf/cav/ChakarovS13,DBLP:conf/cav/ChatterjeeFG16,DBLP:journals/pacmpl/AgrawalC018}, and
(iii) bound assertion violation probabilities~\cite{DBLP:conf/cav/ChakarovS13,DBLP:conf/pldi/WangS0CG21}.
An exception is~\cite{DBLP:conf/pldi/Wang0GCQS19} which tackles \emph{cost analysis}.
%
In contrast, in the $\wpSymb$-based framework, we can express and reason about a remarkably large variety of quantities, including the following (assume that $\prog$ does not contain conditioning for simplicity):
%
\begin{itemize}
    \item Termination probabilities: $\wp{\prog}{1}$
    \item The probability of terminating in a predicate\footnote{Recall that $\iv{\guard}$ is the indicator function of the predicate $\guard$.} $\guard$: $\wp{\prog}{\iv{\guard}}$
    \item The expected value of variable $\pVar$ on termination: $\wp{\prog}{\pVar}$
    \item Higher moments of $\pVar$ after termination: $\wp{\prog}{\pVar^k}$ for $k \geq 1$
    \item Expected distance between variables $\pVar$ and $\pVarb$ after termination: $\wp{\prog}{|\pVar-\pVarb|}$
\end{itemize}
%
\iftoggle{arxiv}{%
Note that this includes reasoning about \emph{unbounded} random variables.
For example, a program variable $\pVar$ may become arbitrarily large during program execution.
}{}

\begin{example}
    Let us illustrate the above quantities.
    Consider the following program $\prog$:
    %
    \begin{align*}
        \ITE{\pVar = 1}{ \PCHOICE{\ASSIGN{\pVarb}{0}}{\nicefrac{1}{2}}{\ASSIGN{\pVarb}{2}}}{\PCHOICE{\ASSIGN{\pVarb}{0}}{\nicefrac{4}{5}}{\ASSIGN{\pVarb}{3}}}
    \end{align*}
    %
    \begin{itemize}
    	\item $\wp{\prog}{1}=1$, i.e., $\prog$ terminates with probability $1$ for each initial state.
    	%
    	\item $\wp{\prog}{\iv{\pVarb = 0}} \eeq \nicefrac{1}{2} \cdot \iv{\pVar = 1} + \nicefrac{4}{5} \cdot \iv{\pVar \neq 1}$, i.e., if initially $\pVar=1$, then the probability to terminate in $\pVarb = 0$ is $\nicefrac{1}{2}$. For all other initial states, this probability is $\nicefrac{4}{5}$.
    	%
    	\item  $\wp{C}{\pVarb} = 1\cdot \iv{\pVar = 1} + \nicefrac{3}{5} \cdot \iv{\pVar \neq 1}$, i.e., the expected final value of $\pVarb$ is either $1$ or $\nicefrac{3}{5}$, depending on whether $\pVar = 1$ holds initially or not.
    	%
    	\item $ \wp{C}{\pVarb^2}  = 2 \cdot \iv{\pVar = 1} + \nicefrac{9}{5} \cdot \iv{\pVar \neq 1}$, i.e., the second moment of $\pVarb$ on termination is either $2$ or $\nicefrac{9}{5}$, depending on whether $\pVar = 1$ holds initially or not.
    	%
    	\item $\wp{\prog}{| \pVar - \pVarb |} = \iv{\pVar = 1} \cdot (\nicefrac{1}{2} \cdot |\pVar| + \nicefrac{1}{2} \cdot | \pVar - 2| ) + \iv{\pVar \neq 1} \cdot (\nicefrac{4}{5} \cdot |\pVar| + \nicefrac{1}{5} \cdot | \pVar - 3|)$, which is the expected difference between the final values of $\pVar$ and $\pVarb$ in terms of their initial values. 
    \end{itemize}
    %
    The above weakest pre-expectations can be determined by applying the rules in \Cref{tab:original} (\cpageref{tab:original}).
\end{example}


\paragraph{Conditioning, Everywhere}

Following~\cite{DBLP:journals/toplas/OlmedoGJKKM18}, we allow conditioning in the form of (hard) $\KWOBSERVE$-statements at arbitrary places in the program --- even inside loops.
All of the above quantities can be generalized sensibly to the conditional setting.
We can go further:
For instance, the probability to never violate an $\KWOBSERVE$, even if the program does not terminate, is given by $\wlp{\prog}{1}$.


\subsection{Limitations and Assumptions}

As mentioned, to analyze loops, we assume \emph{user-provided invariants}.
Moreover, we restrict to native support for continuous \emph{uniform} distributions and discrete coin flips.
Further, while we support two-sided bounds for loop-free programs, the kind of invariants we study in this paper can only prove \emph{upper} bounds on $\wpSymb$ and $\cwpSymb$, and \emph{lower} bounds on $\wlpSymb$.
We conjecture that our approach extends to invariant-based methods for the other directions~\cite{DBLP:journals/pacmpl/HarkKGK20} as well, but leave the details for future work.
To keep the presentation simple, we only discuss invariant verification for \emph{non-nested loops}.
The case of general loop structures, including sequential and/or nested, can be dealt with as in~\cite{DBLP:journals/pacmpl/BatzBKW24}.
We assume non-negative real-valued program variables and do not support data structures.
Soft-conditioning is only supported indirectly.
See \Cref{rem:generalDist,rem:nonNeg,rem:softConditioning} for more details.


\subsection{From Lebesgue Integrals to Riemann Sums and Back: A Technical Challenge}
\label{sec:challenges}

\newcommand{\progdirichlet}{D}

The Lebesgue integral is \emph{the} standard integral in probability theory.
There is good reason for this:
Lebesgue integrals can be defined for a broader class of functions than, say, Riemann integrals, and have favorable mathematical properties like Monotone Convergence Theorems.

Therefore, to define the semantics of PPs, most works (e.g., \cite{DBLP:conf/setss/SzymczakK19,dahlqvist2020semantics}) have resorted to Lebesgue integrals, and we do not question that this is the way to go.
To illustrate the usefulness of the Lebesgue integral as opposed to the Riemann integral\footnote{The Riemann integral is the limit of both the lower and upper Riemann sums as $\N \to \infty$, provided the two limits coincide.}, consider the following program $\progdirichlet$: 
%
\begin{align*}
    & \SEQ{\ASSIGN{\pVar}{0}}{\ASSIGN{\pVarb}{1}} \, \SEMICOLON \\
	& \WHILE{\pVarb\cdot\pVarc \pNEQ \pVar} 
	{\ASSIGN{\pVarb}{\pVarb+1} \,\SEMICOLON\, \ASSIGN{\pVar}{0} \,\SEMICOLON \,
	 \WHILE{\pVarb \cdot\pVarc \pNEQ \pVar \pAND \pVar < \pVarb}{\ASSIGN{\pVar}{\pVar+1}} \, \} }
\end{align*}
%
This program \enquote{searches} for non-negative integers $\pVar$ and $\pVarb$ such that $\pVarc = \tfrac \pVar \pVarb$ and stops once it finds such $\pVar$ and $\pVarb$.
It thus terminates if and only if $\pVarc$ is a \emph{rational number} in $\uIval$ initially (recall that we allow actual \emph{real}-valued variables).
In symbols, we have $\wp{\progdirichlet}{1} = \iv{\pVarc \in \rats \land 0 \leq \pVarc \leq 1}$ --- this is the well-known \emph{Dirichlet function} restricted to the unit interval.
Now, consider the program $\SEQ{\UNIFASSIGN{\pVarc}}{\progdirichlet}$.
Its termination probability is zero, the Lebesgue integral of the Dirichlet function over the interval $\uIval$.
Intuitively, this is because almost all real numbers are irrational, i.e., the rational numbers have Lebesgue measure zero.
However, the Dirichlet function is \emph{not Riemann-integrable} --- it is \enquote{too discontinuous}.
As a consequence, one cannot easily define $\wpSymb$ --- neither in general nor in this specific example --- relying on Riemann integrals.

In light of the above example, the following question arises naturally:
%
\begin{myhighlight}
    \emph{How sensible is an analysis of loops based on Riemann sums given the fact that $\wpSymb$'s of loops are \underline{not Riemann-inte}g\underline{rable} in general, not even for programs using only polynomial arithmetic and constant post-expectations?}
\end{myhighlight}
%
This question has at least two dimensions.

(1) Regarding \emph{soundness}, we prove that our Riemann $\wpSymb$'s are \emph{always} sound under- or over-approximations, i.e., the following inequalities hold for all $\N \geq 1$ in a very general setting:
\[  
\forall~\text{initial states $\st$}\colon\quad
    \lwp{\N}{\prog}{\ex}(\st)
    \lleq
    \wp{\prog}{\ex}(\st)
    \lleq
    \uwp{\N}{\prog}{\ex}(\st)        
\]
and similarly for $\wlpSymb$.
This works because the \emph{lower Riemann integral} --- the limit of the lower Riemann sums as the discretization becomes fines --- is a lower bound on the Lebesgue integral, and similarly for the upper Riemann integral.
This is always true, even if the lower and upper integral are different, i.e., if the function at hand is not Riemann-integrable. 

(2) Will the Riemann approximation always converge (in some sense) to the exact $\wpSymb$ as the discretization becomes finer?
We prove that, under mild assumptions such as polynomial arithmetic in the program, the answer is \emph{yes} for loop-free programs.
As one of our theoretical main results for loopy programs, we show that (\Cref{thm:pointwiseConv})
\[
    \sup_{n \geq 1} \lwp{n}{\unfold{\prog}{n}}{\ex}
    \eeq
    \wp{\prog}{\ex}
    \qquad
    \text{but}
    \qquad
    \inf_{n \geq 1} \uwp{n}{\unfold{\prog}{n}}{\ex}
    ~\overset{\text{in general}}{\neq}~
    \wp{\prog}{\ex}
    ~,
    \tag{$\ddagger$}\label{eq:wow}
\]
where $\unfold{\prog}{n}$ arises from $\prog$ by unfolding all loops up to depth $n$ by taking a simultaneous limit of the unfolding depth and the fineness of the lower Riemann sum approximation (the $n$ in $\lwpSymb{n}$).
Via equation \eqref{eq:wow} we \emph{recover} the exact $\wpSymb$ --- defined in terms of \emph{Lebesgue} integrals --- as a limit of our approximate $\wpSymb$'s based on lower Riemann sums for a general class of probabilistic loops.
