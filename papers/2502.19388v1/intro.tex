\section{Introduction}
\label{sec:intro}

\newcommand*{\diffeo}{% 
    \mathrel{\vcenter{\offinterlineskip
            \hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}}}}
        
\newcommand{\progPiInner}{\textcolor{ourdarkblue}{\prog_{inner}}}
\newcommand{\progPi}{\prog}

Probabilistic programs (PP) are usual programs with the additional ability to
%
\begin{enumerate}
    \item\label{ability:branch} \emph{branch} their flow of control based on the outcomes of coin flips,
    \item\label{ability:sample} \emph{sample} numbers from predefined continuous and/or discrete probability distributions, and
    \item\label{ability:condition} \emph{condition} their current state on observations using dedicated $\KWOBSERVE$-instructions.
\end{enumerate}
%
Programs with one or more of these (partly overlapping) abilities have been extensively studied in different contexts and throughout various communities:
\emph{Randomized algorithms} typically rely on ability \eqref{ability:branch} to speed up computations on average~\cite{DBLP:journals/dam/Karp91} or for breaking symmetries in distributed computing~\cite{DBLP:journals/iandc/ItaiR90}.
\emph{Sampling} and \emph{conditioning} (abilities \eqref{ability:sample} and \eqref{ability:condition}) are extensively employed in connection with Bayes' rule in machine learning, AI, and cognitive science to statistically infer information from observed data~\cite{dippl,DBLP:conf/icse/GordonHNR14,DBLP:journals/corr/abs-1809-10756}.
\emph{Continuous distributions} are pivotal for these applications.

Many applications require \emph{expressive} PP with the usual constructs from traditional programs --- including unbounded $\KWWHILE$-loops.
The latter pose a major challenge:
For instance, reasoning about probabilistic termination is provably harder than reasoning about classical termination~\cite{DBLP:conf/lics/KaminskiK17}.


\paragraph{Guaranteed Bounds on Expected Outcomes via Weakest Pre-Expectations}

In recent years, there has been considerable interest in analyzing expressive PP in a mathematically rigorous manner --- with \emph{hard} rather than statistical guarantees --- see e.g., \cite{DBLP:conf/cav/ChakarovS13,DBLP:conf/cav/GehrMV16,DBLP:journals/pacmpl/AgrawalC018,DBLP:journals/pacmpl/MoosbruggerSBK22,DBLP:conf/pldi/GehrSV20} as representative examples.
Such stringent analyses are motivated by the fact that more conventional statistical techniques can be far off even for simple programs~\cite{DBLP:conf/pldi/BeutnerOZ22}, and may be inappropriate for safety-critical applications.

The \emph{weakest pre-expectation ($\wpSymb$) calculus} is a prominent means to specify and establish a broad spectrum of PP properties in a rigorous manner~\cite{DBLP:conf/stoc/Kozen83,DBLP:series/mcs/McIverM05,DBLP:phd/dnb/Kaminski19}.
Generalizing Dijkstra's classical weakest pre-\emph{conditions}, the central objects are so-called \emph{expectations} $\ex$ --- random variables over a program's state space mapping program states to numbers --- which take over the role of predicates from classical program verification.
More precisely, given a probabilistic program $\prog$, an expectation $\ex$, and an initial program state $\pState$, we have
%
\[
	\wp{\prog}{\ex}(\pState)
	\quad=\quad 
	\substack{\text{\normalsize\emph{expected value} of $\ex$ w.r.t.\ the distribution of \emph{final} states} \\ \text{\normalsize reached after executing $\prog$ on \emph{initial} state $\pState$} \\
	\text{\normalsize and aborting all executions violating an observation}~.}
\]
%
Hence, $\wp{\prog}{\ex}$ is a map from \emph{initial} program states to \emph{expected final} values of $\ex$.
The weakest \emph{liberal} pre-expectation $\wlp{\prog}{\ex}$ adds to the above quantity the probability of $\prog$'s divergence\footnote{Provided $\ex$ is upper-bounded by $1$.
See \Cref{sec:programs:wp} for details.}.
Both $\wpSymb$ and $\wlpSymb$ can be defined by induction on the structure of $\prog$.
Notice that neither of these calculi yield \emph{conditional} expected values as one might expect in the presence of conditioning.
One can, however, define \emph{conditional weakest pre-expectations}~\cite{DBLP:journals/toplas/OlmedoGJKKM18,DBLP:conf/aaai/NoriHRS14} as\footnote{Notice that this quantity is undefined if $\wlp{\prog}{1}(\pState)=0$ as is natural when conditioning on a probability-$0$-event.} 
%
\[
	\cwp{\prog}{\ex}(\pState)
	%
	\eeq
	\frac{\wp{\prog}{\ex}(\pState)}
	{\wlp{\prog}{1}(\pState)}
	%
	\eeq
	\frac{\text{\normalsize{expected value} of $\ex$ \ldots}}
	{\text{probability of not violating an observation}}~,
\]
%
which indeed yields the sought-after conditional expected values.
We refer to quantities such as $\wp{\prog}{\ex}(\pState)$, $\wlp{\prog}{\ex}(\pState)$, and $\cwp{\prog}{\ex}(\pState)$ as \emph{(conditional) expected outcomes} of probabilistic programs.
Reasoning about expected outcomes of loops is typically tackled by means of \emph{quantitative loop invariants}, which are, naturally, often hard to find.
However, in the presence of continuous sampling, even \emph{verifying} a \emph{given} candidate loop invariant poses severe challenges as reasoning about the required expected values involves possibly complex \emph{integrals}.
The aim of this paper is to lay the foundations for automated techniques tackling these challenges.


\paragraph{Problem Statement}

We study \emph{imperative} PP with all three abilities \eqref{ability:branch}, \eqref{ability:sample}, and \eqref{ability:condition}.
Our goal is to
%
\begin{myhighlight}
    \emph{semi-automatically verify bounds on (conditional) expected outcomes of programs featuring \underline{continuous uni}f\underline{orm sam}p\underline{lin}g, unbounded \underline{$\KWWHILE$-loo}p\underline{s} with user-provided q\underline{uantitative invariants}, and \underline{conditionin}g.}    
\end{myhighlight}
%
\enquote{Semi-automatic} means that we focus on verification of \emph{user-provided} quantitative loop invariants in the sense of~\cite{DBLP:series/mcs/McIverM05}.
Unlike fully automatic approaches we do not \emph{synthesize} such invariants automatically, which is a promising direction for future work but outside the scope of this paper.


\paragraph{Approach}

Our key idea is simple, yet powerful:
%
\begin{myhighlight}
    \emph{We replace the \underline{inte}g\underline{rals} occurring in the definition of the programs' exact weakest pre-expectations by simpler \underline{sound a}pp\underline{roximations}, namely lower and upper \underline{Riemann sums}}.
\end{myhighlight}
%
This results in a family of approximate \emph{lower} and \emph{upper Riemann $\wpSymb$ transformers}, denoted by $\lwpSymb{\N}$ and $\uwpSymb{\N}$, where $\N$ is the number of sub-intervals used to discretize the domain of integration. 
Crucially, these Riemann expectation transformers ultimately give rise to automated SMT-based techniques for verifying quantitative loop invariants.
Let us now consider an introductory example.


\paragraph{Illustrative Example: A Monte Carlo Approximator.}

The program shown in \Cref{fig:intro} draws $\pVarm$ $(x,y)$-samples uniformly at random from the unit square.
Each time a sample lands in the quarter unit circle, the variable $\pVarcount$ is incremented.
The program hence approximates the transcendental number $\tfrac \pi 4 \approx 0.785$ (the area of the quarter unit circle) in a Monte Carlo manner.

We will first focus on the \textcolor{ourdarkblue}{blue} fragment $\progPiInner$ of the program from \Cref{fig:intro}.
$\progPiInner$ contains two uniform continuous sampling instructions.
Suppose we aim to compute the expected value of $\pVarcount$ after executing $\progPiInner$.
%Using $\wpSymb$, this requires computing $\wp{\prog}{\pVarcount}$ for an arbitrary initial state $\pSt$.
Applying the rules for determining $\wpSymb$'s~\cite{DBLP:conf/setss/SzymczakK19}, we obtain the following double Lebesgue integral, where $\iv{\ldots}$ denotes the indicator function of the enclosed predicate:
%
\begin{align*}
    \wp{\progPiInner}{\pVarcount}
    \eeq
    \underbrace{\pVarcount + \int_0^1 \int_0^1 \iv{\pVar^2 + \pVarb^2 \leq 1} \,d\lebmes(x), d\lebmes(y)
    \eeq
    \pVarcount + \frac{\pi}{4}}_{\text{expected final value of $\pVarcount$ in terms of its initial value}}
    ~.
    \tag{$\dagger$}\label{eq:doubleIntegral}
\end{align*}
%
This reflects the fact that, in \emph{expectation}, $\progPiInner$ increments $\pVarcount$ by $\tfrac \pi 4$.

Symbolic integration-based tools such as PSI~\cite{DBLP:conf/cav/GehrMV16} can solve the above integral directly.
However, we observe two significant issues:
(i) there are  integrals that cannot be evaluated symbolically in closed form,
and (ii) even if all guards and assignments in the program are polynomial and distributions are restricted to uniform distributions in $[0,1]$, the resulting integrals may evaluate to transcendental numbers, making automation notoriously difficult.

As outlined, instead of solving integrals exactly, our strategy is to soundly \emph{under-} or \emph{over-approximate} them by lower or upper Riemann sums.
In our example, the resulting upper sum
%
\begin{align*} 
    \frac{1}{\N^2} \sum_{i=0}^{N-1} \sum_{j=0}^{N-1} ~\sup_{\xi \in [\frac{i}{\N}, \frac{i+1}{\N}]} ~ \sup_{\zeta \in [\frac{j}{\N}, \frac{j+1}{\N}]} ~ \iv{\xi^2 + \zeta^2 \le 1} ~,
\end{align*}
%
where $\N \geq 1$, is a guaranteed upper bound on the double integral in \eqref{eq:doubleIntegral}.
For instance, with $\N = 16$, we can verify that the upper sum is at most $0.85$, and hence that $\wp{\progPiInner}{\pVarcount}(\pState) \leq \pState(\pVarcount) + 0.85$ for all initial states $\pState$.
Crucially, to prove upper bounds on upper Riemann sums, we do \emph{not} have to evaluate any suprema explicitly because for all $A \subseteq \reals$ and $b \in \reals$, 
%
\[
    \sup A \leq b \qquad\text{iff}\qquad \text{$\forall a \in A$}\colon  a \leq b~.
\]
%
In practice, we can thus drop the $\sup$'s in upper Riemann sums (and, dually, the $\inf$'s in lower sums) by introducing $\foForall$-quantifiers, which is highly beneficial in the context of SMT-based automation.

We stress that even though we discretize the integrals' domains uniformly, our approach is \emph{not} the same as replacing the continuous $\UNIF_{\uIval}$ distributions by discrete uniform distributions of $N$ point-masses.
Indeed, the latter would neither yield under- nor over-approximations in general, which is, however, essential for invariant verification. 

\begin{figure}[t]
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{align*}
            & \ASSIGN{\pVari}{1} \,\SEMICOLON\, \ASSIGN{\pVarcount}{0} \,\SEMICOLON\, \\
            &\WHILENOBODY{\pVari \leq \pVarm} \\
            %
            &\qquad \textcolor{ourdarkblue}{\SEQ{\UNIFASSIGN{\pVar}}{\UNIFASSIGN{\pVarb}}\,\SEMICOLON} \\
            %
            &\qquad \textcolor{ourdarkblue}{\ITE{\pVar^2 + \pVarb^2 \leq 1}{\ASSIGN{\pVarcount}{\pVarcount + 1}}{\SKIP}\,\SEMICOLON} \\
            %
            &\qquad \ASSIGN{\pVari}{\pVari + 1} \quad \}
        \end{align*}
    \end{minipage}
    \hspace{5mm}
    \begin{minipage}{0.3\textwidth}
        \centering
        \input{monte_carlo_pi_illustration}
    \end{minipage}    
    \caption{
        Left: Monte-Carlo approximator $\progPi$ for $\pi$ using ability \eqref{ability:sample}.
        Right: 100 random samples.
    }
    \label{fig:intro}
\end{figure}


\paragraph{Loops and Invariants}

Now, consider the entire program $\progPi$ in \Cref{fig:intro}.
We have $\wp{\progPi}{\pVarcount} = \tfrac \pi 4 \cdot \pVarm$ because, intuitively, the percentage of samples landing in the quarter circle equals its area.
Our techniques enable to soundly bound $\wp{\progPi}{\pVarcount}$ in a semi-automated manner \emph{for arbitrary initial values of~$\pVarm$}.
This is done, as is standard in deductive verification, using quantitative loop invariants, which we detail in \Cref{sec:invariants_and_unrolling}.
Suffices to say here that, given a suitable invariant $\exI$, our techniques enable to verify its validity automatically.
This in turn allows us to conclude that:
%
\[
	\forall\,\text{initial states $\pState$}\colon \quad 
	\wp{\progPi}{\pVarcount}(\pState) \lleq 0.85 \cdot \pState(\pVarm)~.
\]
%
See \Cref{ex:invariant_monte_carlo} (\cpageref{ex:invariant_monte_carlo}) and \Cref{ex:caesar_loop} (\cpageref{ex:caesar_loop}) for details. 


\paragraph{Contributions}

In summary, this paper introduces the novel \emph{lower} and \emph{upper Riemann weakest pre-expectation transformers} and demonstrates their applicability to the semi-automated verification of PP with \emph{continuous uniform sampling} and general \emph{$\KWWHILE$-loops}.
Put more concretely:
%
\begin{description}
    \item[Verification of Invariants] 
    We show that externally provided loop invariant candidates can be verified using the Riemann $\wpSymb$ transformers in a way that is suitable for SMT-based automation.
    Such invariants, once verified, imply (one-sided) bounds on $\wpSymb$, $\wlpSymb$, and $\cwpSymb$.
    %
    \item[Convergence and Complexity]
    On the more theoretical side, our approximate Riemann $\wpSymb$'s are shown to \emph{converge} to the exact $\wpSymb$'s under mild assumptions as the discretization becomes finer.
    As a consequence, we obtain $\coRE$-completeness\footnote{This means that the refutation of such bounds is semi-decidable.} results for the problems of verifying upper bounds on $\wpSymb$ and lower bounds on $\wlpSymb$ for fairly general loopy programs.
    %
    \item[Implementation and Case Studies]
    We incorporate our method in the PP verification infrastructure \toolcaesar~\cite{DBLP:journals/pacmpl/SchroerBKKM23} by encoding Riemann sums in its \emph{intermediate verification language}.
    This enables transferring principles from classical SMT-based program verification \cite{DBLP:conf/vmcai/0001SS16}, such as custom first-order theories for reasoning about exponentials, to the verification of PPs involving continuous sampling. 
    We provide various case studies and an empirical evaluation.
\end{description}


\paragraph{Paper Structure}

\Cref{sec:birdseye} provides a bird's eye view on the distinctive features and technicalities of our method.
\Cref{sec:prelims} introduces basic fixed point theory and \Cref{sec:programs} summarizes the existing definitions of $\wpSymb$, $\wlpSymb$, and $\cwpSymb$.
In \Cref{sec:approxwp}, we introduce our Riemann $\wpSymb$ transformers and prove them sound.
\Cref{sec:invariants_and_unrolling} is devoted to loop invariants.
The convergence results are presented in \Cref{sec:convergence}.
Towards automation, \Cref{sec:syntax} introduces a concrete, effective syntax for expectations.
\Cref{sec:case_studies} details the implementation in \toolcaesar and presents case studies.
Additional related work is surveyed in \Cref{sec:relwork}.
We conclude in \Cref{sec:conclusion}.
\iftoggle{arxiv}{Omitted proofs can be found in \Cref{app:proofs}.}{An extended version of this article containing the ommitted proofs and additional material is available~\cite{arxiv}.}

