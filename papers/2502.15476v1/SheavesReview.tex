%\documentclass[12pt,titlepage]{article}
%\documentclass{article}
\documentclass[english,12pt,oneside]{amsproc}
%\documentclass[a4paper, 1wpt]{amsart}
\usepackage[english]{babel}
\usepackage{a4wide}
\usepackage{amsthm}
%\usepackage[T2A]{fontenc}
%\usepackage[cp1251]{inputenc}
\usepackage{graphics}
%\usepackage{natbib}
\usepackage{biblatex}
\addbibresource{SheavesReview.bib}
\usepackage{amsfonts, amssymb, amscd, amsmath}
\usepackage{latexsym}
\usepackage[matrix,arrow,curve]{xy}
\usepackage{mathabx}
\usepackage{color}
\usepackage{pbox}
\usepackage{tikz}
\usepackage{stmaryrd}
%\usetikzlibrary{matrix,decorations.pathreplacing,positioning}
\usepackage{hyperref}
%\usepackage{scalerel}
\usepackage{tikz,tikz-cd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[normalem]{ulem}
\usepackage{array}
\usepackage{mathrsfs}

%\usepackage{geometry}
%\usepackage{amsaddr}

\usetikzlibrary{arrows,arrows.meta, decorations.pathreplacing, calc,positioning,trees,arrows,chains,shapes.geometric,%
    decorations.pathmorphing,shapes, matrix, shapes.symbols}

\tikzcdset{arrow style=tikz, diagrams={>=stealth}}%{>={Straight Barb[scale=0.5]}}

\tikzset{
  altstackar/.style={decorate, decoration={show path construction,
    lineto code={
      \path (\tikzinputsegmentfirst); \pgfgetlastxy{\xstart}{\ystart}
      \path (\tikzinputsegmentlast); \pgfgetlastxy{\xend}{\yend}
      \path ($(0,0)!1.5pt!(\ystart-\yend,\xend-\xstart)$); \pgfgetlastxy{\xperp}{\yperp}
      \foreach \n[evaluate=\n as \k using .5*#1-\n+.5] in {1,...,#1}{
        \ifodd\n{\draw[->, shorten <=2pt, shift={($\k*(\xperp,\yperp)$)}](\xstart,\ystart)--(\xend,\yend);}
        \else{\draw[<-, shorten >=2pt, shift={($\k*(\xperp,\yperp)$)}](\xstart,\ystart)--(\xend,\yend);}\fi
      }
    }
  }}, altstackar/.default={1}
}


%\oddsidemargin=1cm
%%\topmargin=0pt
%\textwidth=14cm %\allowdisplaybreaks[1]

%\oddsidemargin=30pt
%\topmargin=0pt \textwidth=15cm \allowdisplaybreaks[1]


\DeclareMathOperator{\Cone}{Cone}
\DeclareMathOperator{\pt}{pt}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Coker}{Coker}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\const}{const}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Ext}{Ext}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Cy}{Cy}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\OpSets}{OpSets}
\DeclareMathOperator{\ConvHull}{\mathbf{ConvHull}}
\DeclareMathOperator{\Ob}{Ob}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\Cells}{Cells}
\DeclareMathOperator{\crep}{crep}
\DeclareMathOperator{\relel}{RltdElem}
\DeclareMathOperator{\src}{src}
\DeclareMathOperator{\trg}{trg}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\Iso}{Iso}

\DeclareMathOperator{\ChCpx}{ChCpx}
\DeclareMathOperator{\CochCpx}{CochCpx}
\DeclareMathOperator{\AChCpx}{AugChCpx}
\DeclareMathOperator{\Dif}{Dif}


\newcommand{\low}[2]{{_{\lceil}#1_{\rceil #2}}}
\newcommand{\simc}{\!\!\sim}

%Categorical stuff
\newcommand{\ST}[1]{\mathbf{#1}}

\newcommand{\cat}{\ST{cat}}
\newcommand{\Sets}{\ST{Sets}}
\newcommand{\Top}{\ST{Top}}
\newcommand{\Pos}{\ST{Pos}}
\newcommand{\PrePos}{\ST{PrePos}}
\newcommand{\AlexTop}{\ST{AlexTop}}
\newcommand{\Diag}{\ST{Diag}}
\newcommand{\Vect}{\ST{Vect}}
\newcommand{\Abel}{\ST{Abel}}
\newcommand{\Mod}{\ST{Mod}}
\newcommand{\Cochain}{\ST{Cochain}}
\newcommand{\PreShvs}{\ST{PreShvs}}
\newcommand{\Shvs}{\ST{Shvs}}
\newcommand{\FinSets}{\ST{FinSets}}
\newcommand{\FinVect}{\ST{FinVect}}
\newcommand{\FinMod}{\ST{FinMod}}
\newcommand{\FinAbel}{\ST{FinAbel}}
\newcommand{\Topoi}{\ST{Topoi}}
\newcommand{\AbCat}{\ST{AbCat}}
\newcommand{\ConDiag}{\ST{ConDiag}}
\newcommand{\Repr}{\ST{Repr}}
\newcommand{\Type}{\ST{Type}}
\newcommand{\Lat}{\ST{Lat}}

%\newcommand{}{\ST{}}

%\newcommand{\Qq}{\mathfrak{F}}

\newcommand{\ko}{\Bbbk}
\newcommand{\Zo}{\mathbb{Z}}
\newcommand{\Ro}{\mathbb{R}}
\newcommand{\Co}{\mathbb{C}}
\newcommand{\Qo}{\mathbb{Q}}

\newcommand{\Rg}{\mathbb{R}_{\geqslant 0}}
\newcommand{\Zg}{\mathbb{Z}_{\geqslant 0}}
\newcommand{\RP}{\mathbb{R}P}

\newcommand{\ca}[1]{\mathcal{#1}}
\newcommand{\inc}[2]{[#1\colon #2]}
\newcommand{\face}{\trianglelefteqslant}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Der}{\mathscr{D}}

\newcommand{\Vv}{\mathbf{V}}
\newcommand{\Ww}{\mathbf{W}}
\newcommand{\Cc}{\mathbf{C}}

\newcommand{\Fu}{\mathfrak{F}}
\newcommand{\dd}{\partial}
\newcommand{\Hr}{\tilde{H}}
\newcommand{\br}{\tilde{\beta}}
\newcommand{\mult}{\mu}

% for comments
\newcommand{\tom}[1]{\textcolor{blue}{(Tom: {#1})}}

\newcounter{stmcounter}[section]
%\newcounter{thcounter}
\newcounter{problcounter}
\newcounter{quecounter}

\numberwithin{equation}{section}

%\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thestmcounter}{\thesection.\arabic{stmcounter}}
%\renewcommand{\thedefcounter}{\thesection.\arabic{defcounter}}

\theoremstyle{plain}
\newtheorem{thm}[stmcounter]{Theorem}
\newtheorem{cor}[stmcounter]{Corollary}
\newtheorem{thmNo}{Theorem}
\newtheorem{prop}[stmcounter]{Proposition}
\newtheorem{lem}[stmcounter]{Lemma}
\newtheorem{req}[stmcounter]{Requirement}

\newtheorem{conj}[stmcounter]{Conjecture}
\newtheorem{probl}[problcounter]{Problem}
\newtheorem{clai}[stmcounter]{Claim}

\newtheorem{que}[quecounter]{Question}

\theoremstyle{definition}
\newtheorem{defin}[stmcounter]{Definition}
\newtheorem{difin}[stmcounter]{Informal definition}

\theoremstyle{remark}
\newtheorem{ex}[stmcounter]{Example}
\newtheorem{rem}[stmcounter]{Remark}
\newtheorem{con}[stmcounter]{Construction}
\newtheorem{exer}[stmcounter]{Exercise}

\newtheorem{ans}{Answer}

\begin{document}

\begin{center}
    \rule{\linewidth}{1.5pt} \\[10pt]
    {\Large\textbf{Sheaf theory: from deep geometry to deep learning}} \\[2pt]
    \rule{\linewidth}{1.5pt}
\end{center}


\title[Sheaf theory: from deep geometry to deep learning]{}

%\author{Anton Ayzenberg}
%\address{Noeon Research}
%\email{anton@noeon.ai}
%
%\author{Thomas Gebhart}
%\address{University of Minnesota}
%\email{gebharttom8@gmail.com}
%
%\author{German Magai}
%\address{Noeon Research}
%\email{german@noeon.ai}
%
%\author{Grigory Solomadin}
%\address{University of Strasbourg}
%\email{grigorysolomadin@gmail.com}


\author{
\begin{center}
\vspace{1cm}
    \begin{tabular}{ccc}
        \textbf{Anton Ayzenberg} & \hspace{2cm} & \textbf{German Magai} \\
        Noeon Research, Japan & \hspace{2cm} & Noeon Research, Japan \\
        anton@noeon.ai & \hspace{2cm} & german@noeon.ai\\
        \\
        \textbf{Thomas Gebhart} & \hspace{2cm} & \textbf{Grigory Solomadin} \\
        University of Minnesota, USA & \hspace{2cm} & University of Strasbourg, France \\
        gebharttom8@gmail.com & \hspace{2cm} & grigorysolomadin@gmail.com \\
    \end{tabular}
\end{center}
\vspace{1cm}
}


%\affil[1]{Noeon Research, Japan}
%\affil[2]{University of Minnesota, USA}
%\affil[3]{University of Strasbourg, France}

%\date{\today}
\thanks{This work is supported by Noeon Research}

\subjclass[2020]{Primary: 01A65, 68T07, 06A06, 06A11, 13P20, 54B40, 55-08, 55N30, 05C50, Secondary: 68T09, 68T30, 35R02, 90C35, 91D30, 94C15, 93D50, 06A07, 13D02, 14F06, 18F20, 55U15, 55N25, 55N31, 80M35, 05C22, 05C60, 05C65, 68R12, 18C50}

\keywords{sheaf theory, sheaf cohomology, sheaf Laplacian, graph Laplacian, heat diffusion, sheaf learning, sheaf neural network, graph neural network, data representation, geometric deep learning, CW-complex, simplicial complex, poset, finite topology, cell poset, Morse inequalities, connection sheaf, relations representation, cohomology computations algorithm}

\begin{abstract}
%?? to be rewritten
This paper provides an overview of the applications of sheaf theory in deep learning, data science, and computer science in general. The primary text of this work serves as a friendly introduction to applied and computational sheaf theory accessible to those with modest mathematical familiarity. We describe intuitions and motivations underlying sheaf theory shared by both theoretical researchers and practitioners, bridging classical mathematical theory and its more recent implementations within signal processing and deep learning. We observe that most notions commonly considered specific to cellular sheaves translate to sheaves on arbitrary posets, providing an interesting avenue for further generalization of these methods in applications, and we present a new algorithm to compute sheaf cohomology on arbitrary finite posets in response. By integrating classical theory with recent applications, this work reveals certain blind spots in current machine learning practices. We conclude with a list of problems related to sheaf-theoretic applications that we find mathematically insightful and practically instructive to solve. To ensure the exposition of sheaf theory is self-contained, a rigorous mathematical introduction is provided in appendices which moves from an introduction of diagrams and sheaves to the definition of derived functors, higher order cohomology, sheaf Laplacians, sheaf diffusion, and interconnections of these subjects therein.
\end{abstract}

\maketitle
%\begin{center}
%  \textbf{Abstract}
%\end{center}
%This paper provides an overview of the applications of sheaf theory in deep learning, data science, and computer science in general. The primary text of this work serves as a friendly introduction to applied and computational sheaf theory accessible to those with modest mathematical familiarity. We describe intuitions and motivations underlying sheaf theory shared by both theoretical researchers and practitioners, bridging classical mathematical theory and its more recent implementations of sheaf theory within signal processing and deep learning. We observe that most notions commonly considered specific to cellular sheaves translate to sheaves on arbitrary posets, providing an interesting avenue for further generalization of these methods in applications, and we present a new algorithm to compute sheaf cohomology on arbitrary finite posets in response. By integrating classical theory with recent applications, this work reveals certain blind spots in current machine learning practices. We conclude with a list of problems related to sheaf-theoretic applications that we find mathematically insightful and practically instructive to solve. To ensure the exposition of sheaf theory is self-contained, a rigorous mathematical introduction is provided in an appendix which moves from an introduction of diagrams and sheaves to the definition of derived functors, higher order cohomology, sheaf Laplacians, sheaf diffusion, and interconnections of these subjects therein.


\newpage

\tableofcontents
%\newpage

\section{Introduction}\label{secIntro}

Scientific development and the enrichment of human knowledge are predicated on the existence of rigorous mathematical foundations from which new insights may be constructed. This foundation not only systematizes navigation in a particular field by fixing terminology and sharpening intuitions but also links practices from across fields, leading to novel research directions and the emergence of new techniques. Applied mathematics, when practiced correctly, facilitates this multiplicative growth within and across scientific domains. 

Sheaf theory is a highly technical branch of mathematics. The foundation of this discipline was laid by Jean Leray when he was a prisoner of war camp in Austria, and later published in~\cite{Leray}. Sheaf theory was conceptualized by Alexander Gro\-then\-dieck in 1957: his celebrated Tohoku paper~\cite{grothendieck1957tohoku} provided a general categorical perspective on many phenomena known in algebraic geometry, algebraic topology, and commutative algebra. Geometrical structures are ubiquitous in both theoretical mathematics and computer science. It may seem at first glance that geometrical structures studied in mathematics such as topologies, manifolds, varieties, and schemes differ significantly from those studied in applied mathematics: graphs, metric spaces, point clouds, probability distributions. However, it seems that the power of sheaf theory is universal. Not only is this theory formally applicable to finite data structures, but it also solves the very same problem as the one addressed in the classical papers: to put geometrical intuitions on a firm ground by using algebraical tools.

In recent years, there has been a fascinating fusion of methods of sheaf theory with the methods of spectral graph theory, random walks, and classical graph algorithms, leading to the notions of sheaf Laplacian and heat diffusion on a sheaf~\cite{hansen2019toward}. The methods and practices used to design new neural networks architectures have enriched sheaf theory with the ideas of sheaf learning~\cite{hansen2019learning}, message passing on sheaves~\cite{bodnar2023topological}, and sheaf attention~\cite{barbero2022sheaf1}. Sheaf neural networks~\cite{hansensheaf} were proposed as a novel architecture to overcome some problems inherent to classical graph neural architectures. In the past decades, sheaf theory found applications in a variety of other areas of computer science and applied mathematics away from deep learning (DL). The theory provides a powerful tool for the design of algorithms, such as normalization by evaluation~\cite{NormByEvalCoproducts2001} and pattern matching~\cite{Srinivas1993,Conghaile2022CohomologyIC}; it provides an expressive language to describe signal propagation~\cite{robinson2014topological}, obstructions to consistency in databases and models' behavior~\cite{AbramskyContextuality,Abramsky2022presh}, ambiguities in natural languages~\cite{SheafLanguageAmbig}, causal nets~\cite{Robinson2016SheafAC,Rosiak}, scheduling in production chains~\cite{Macfarlane04072014}, structural engineering~\cite{CooperbandThesis}, and many more.

Schematically, the path from the formal definition of a sheaf to the construction of sheaf neural architectures is shown on the left vertical trunk in Fig.~\ref{figPipeline}. The hierarchy goes from more abstract theories to more concrete, but there is no way to strictly define notions towards the bottom of the diagram without defining their top-level predecessors. The same scheme roughly resembles the chronological order in which the mentioned ideas have found applications. The application and contextualization of sheaf theory within data science may also be traced chronologically. Curry's thesis~\cite{Curry} perfectly covers the cohomological part of the scheme with a focus on cellular structures, Hansen's thesis equips cellular sheaves with a spectral theory~\cite{hansen2020laplacians}, Gebhart's thesis discusses sheaf-theoretic inductive biases in the context of deep learning theory~\cite{gebhart2023sheaf}, and Bodnar's thesis~\cite{bodnar2023topological} covers the passage from the mathematical theory of cellular sheaves to the design of neural networks architectures. However, to our knowledge, no existing work contains a detailed description of the entire pipeline from category theory to application in sufficient generality. 

\begin{figure}
  \centering
  \includegraphics[scale=0.24]{pics/scheme.pdf}
  \caption{A way from geometry to neural networks}\label{figPipeline}
\end{figure}

In this review we center partially ordered sets (posets) as the fundamental data structure supporting a sheaf. All items shown on Fig.~\ref{figPipeline} exist and make perfect sense for general posets, not just for cell complexes as is assumed in much of the existing applied literature. In fact, most applications of sheaves in deep learning deal with graphs, which form a highly restricted subclass of cell complexes and hence can be studied with cellular sheaves. However, a number of applications have been recently proposed which seek to extend sheaves to hypergraphs~\cite{duta2024sheaf,nguyen2024sheaf} which, while combinatorial in structure, are not formally cell complexes. Hypergraph sheaf constructions are redesigned each time from scratch by analogy with graphs, but thus far lack a precise mathematical foundation. In a number of applied fields, such as formal concept analysis, knowledge representation, causal nets, and complex systems, general posets play important role. We expect that potential applications of sheaf theory are not bound merely to cellular structures, and this review considers such generalizations accordingly. 

%This paper provides an overview of the history of sheaves' applications based on the mathematical explanation of the whole pipeline~\ref{figPipeline} formulated in the generality of posets. 
We pursue several goals which together motivate the paper's structure.
\begin{enumerate}
  \item We give an overview of basic notions and positions of sheaf theory. This exposition is given in Section~\ref{secIntuitions} and targeted at a general audience.
  \item In Sections~\ref{secReview} and~\ref{secReviewShvsML} we survey the literature applying sheaf theory in deep learning and computer science in general. This survey may be of interest to researchers working in these areas. Section~\ref{secReviewShvsML} is devoted to sheaves applications in modern deep learning and design of neural networks. Section~\ref{secReview} surveys other ideas of sheaves' applications, most of them preceded the applications in deep learning. 
  \item A number of open research problems is gathered in Section~\ref{secProblems}. We address them to a broad community of researchers, including both homology fans and ML enjoyers.
  \item The rigorous and (mostly) self-contained exposition of the mathematical theory is placed in the Appendix sections~\ref{secMathStructures}--\ref{secMathSpaceRestoredFromShvs}. This exposition serves two goals. First, it formalizes some known practices by providing precise claims. Most of the claims are known but scattered in the mathematical literature; we gather the claims, the proofs, and references in a single place. Second, we introduce several new notions and prove a number of new results, mainly in Appendix~\ref{secMathCohomology}. 
\end{enumerate}

See the details on the overall contribution of this paper in subsection~\ref{subsecContribution}. The narration generally mirrors Fig.~\ref{figPipeline}, moving from more abstract topics to more specific topics; from earlier applications to later. Readers interested primarily in applications of sheaves within deep learning are free to start with subsection~\ref{subsecSheafLearning} having the necessary definitions and proceed to Section~\ref{secReviewShvsML}.

The preparation of this paper was greatly influenced by discussions with our colleagues and collaborators: Grigory Kondyrev, Nikita Repeev, Gregory Gelfond, Daniel Rogozin, and Andrei Krutikov. We thank Mikhail Mironov, Nadezhda Khoroshavkina, and Artem Malko for their help in writing some parts of this review. We also thank Gregory Kirgizov for bringing our attention to several extremely important papers and Andrey Filchenkov for providing valuable feedback, which helped improve the presentation.

\subsection{Fundamental notions and ideas}\label{subsecFundNotions}

As a first glimpse into sheaf theory, we provide a brief explanation of the basic terms used in the area, in particular those appearing on the scheme~\ref{figPipeline}. This description aims to introduce a common terminological basis used in sheaf-related papers, either theoretical or applied. More details and examples are provided in Section~\ref{secIntuitions}. The formal claims and proofs are gathered in the Appendices. 

By a \textbf{geometrical structure}, we mean a directed graph (digraph), i.e. a set $S$ together with a collection of ordered pairs of elements $e=(s_1,s_2)$, denoted by $e\colon s_1\to s_2$. Some further restrictions or additional structures are usually imposed on a digraph $S$. In this review we restrict ourselves to directed acyclic graphs of posets, see subsection~\ref{subsecGeometryIsPoset} for the definition and the explanation of this choice. It should be noted however that most theoretical notions described in our paper are well defined in a more general case when $S$ is a finite category.

In order to define sheaves over a structure $S$, one needs to specify a \textbf{target category} $\Vv$. We will be mainly concerned with specific categories: (1) the category $\Sets$ whose objects are sets, and morphisms are maps between sets; (2) the category $\Ro\Vect$, whose objects are real vector spaces, and morphisms are $\Ro$-linear maps. We look at such categories as theories where certain types of calculations can be performedOne can interpret such categories as theories where certain types of calculations can be performed. For example, in the category $\Ro\Vect$ we have systems of linear equations, while equations in the category $\Sets$ are constraint satisfaction problems. 

A sheaf on a geometrical structure $S$ valued in a target category $\Vv$ is a rule $D$, which associates, with any element $s\in S$ an object $D(s)$ of $\Vv$, and with any arrow $e\colon s_1\to s_2$ a morphism $D(e)\colon D(s_1)\to D(s_2)$ in $\Vv$ (i.e. a map of sets in $\Sets$ or a linear map in $\Ro\Vect$). It is assumed that compositionality restrictions (see~\eqref{eqCompositionalityMainPart}) hold for $D$, so that $D$ is a \textbf{functor} from $S$ to $\Vv$. An assignment of a sheaf to a structure $S$ may be understood as turning $S$ into a medium capable of transmitting signals of the type specified by a category $\Vv$.

A given geometrical structure $S$ supports infinitely many sheaves. The whole collection $\Shvs(S,\Vv)$, viewed as a category, faithfully remembers $S$ for meaningful choices of $\Vv$, see Appendix~\ref{secMathSpaceRestoredFromShvs}. However, this collection is too huge to process in practice. Usually, in each particular application, researchers manually choose a sheaf (or a feasible collection of sheaves) which is believed to faithfully resemble properties of $S$. 

Picking a specific sheaf on $S$ allows one to borrow computational techniques available in the category $\Vv$ to describe the properties of $S$. Given a $\Vv$-valued sheaf $D$ on $S$, we can define the \textbf{coherence equations} for $D$, see~\eqref{eqCoherentStates}. The set of solutions $\Gamma(S;D)\in \Vv$ to these equations is called \textbf{the set of global sections}, or the space of global sections if the target category is $\Ro\Vect$. The elements of $\Gamma(S;D)$ can be interpreted as consensus, or equilibrium, states of the medium represented by a sheaf $D$.

Given a sheaf $D$ on $S$, a \textbf{sheaf diffusion} on $D$ is a dynamical system which brings an arbitrary state of the medium to an equilibrium state. For $\Ro\Vect$-valued sheaves $D$ on graphs, the standard way to formally define sheaf diffusion is by applying a gradient descent to a certain quadratic function, called the \textbf{Dirichlet energy} of a sheaf. This functions measures how far is the given state from being an equilibrium state. The symmetric operator corresponding to this quadratic function is called \textbf{sheaf Laplacian}. The convergence rate of sheaf diffusion is determined by the spectrum of the sheaf Laplacian, see subsection~\ref{subsecLaplaciansMainPart}. Spectral characteristics of Laplacians reflect quantitative (i.e. metric) properties of $S$. A variation of a Laplacian, the \textbf{normalized Laplacian}, is often used in practice.

For an $\Ro\Vect$-valued sheaf $D$ on $S$, the space $\Gamma(S;D)$ of global sections enumerates consensus states of the medium $D$. This space belongs to a potentially infinite sequence of vector spaces $H^0(S;D)=\Gamma(S;D)$, $H^1(S;D)$, $H^2(S;D)$, etc., called \textbf{sheaf cohomology}. These spaces describe, respectively, consensus states, relations between consensus states, relations between relations between consensus states, and so on. The sheaf Betti numbers, $\beta_j(S;D)=\dim H^j(S;D)$ reflect qualitative (i.e. topological) properties of $S$.

In order to compute either sheaf Laplacian or cohomology of sheaves, one needs to use the device from homological algebra called the \textbf{cochain complex} $(C^*,d)$, i.e. a sequence of linear maps $C^0\stackrel{d_0}{\rightarrow} C^1\stackrel{d_1}{\rightarrow} C^2 \stackrel{d_2}{\rightarrow}\cdots$, satisfying $d_j\circ d_{j-1}=0$, see Definition~\ref{definCochainCpx}. If the cochain complex is chosen in a topologically correct way, then the sheaf Laplacian is defined by the operator $d_0^*d_0$, and cohomology $H^j(S;D)$ is isomorphic to $\Ker d_j/\im d_{j-1}$. \textbf{Discrete Hodge theory} claims that $\Ker(d_j^*d_j+d_{j-1}d_{j-1}^*)$ is isomorphic to $H^j(S;D)$. The operator $d_j^*d_j+d_{j-1}d_{j-1}^*$ appearing in this expression is called \textbf{higher order Laplacian}. Its spectral characteristics and the corresponding diffusion processes reflect both metric and topological properties of $S$ at the same time.

The story above provides the basic vocabulary of the mathematical side of sheaf theory. Recent advances in deep learning enriched applied sheaf theory with a number of important notions and ideas.

Given a sheaf $D$ on $S$, a single step of sheaf diffusion, may be used as a single layer or a building block in the construction of \textbf{sheaf-type neural networks}. Given a task formulated over a certain geometrical structure, one seeks a sheaf which can provide informative inductive bias for solving the task. One can either construct such a sheaf over the geometrical structure manually, use a predefined sheaf (if one exists), or perform \textbf{sheaf learning} to infer the sheaf structure from task data during training. To learn a sheaf, one often fixes a finite-dimensional family of potential candidate sheaves along with a suitable loss function which is minimized through gradient descent to arrive at a particular sheaf within this family, see subsection~\ref{subsecSheafLearning}. 

Sheaf-type neural networks are particularly effective for tasks on \textbf{heterophilic graphs} and are more resilient to \textbf{oversmoothing} compared to predecessor graph neural networks.

%Sheaf-type neural networks have been shown to overcome two big problems of the preceding graph neural architectures: \textbf{oversmoothing}, and poor performance on \textbf{heterophilic graphs}. Sheaf-type neural networks were shown useful and effective for tasks on \textbf{heterophilic graphs}. %Sheaf-type neural networks do not have a problem of oversmoothing, which was present in the preceding graph neural networks.

\subsection{Overall contribution of the paper}\label{subsecContribution}

We provide an informal and application-oriented exposition of sheaf theory in Section~\ref{secIntuitions}. This part is targeted at a general audience: it explains the basic philosophy behind the subject, its core concepts, what can and what cannot be done with sheaves. 

In order to make the exposition self-contained, we standardize basic terminology and provide a mathematical background for each part of Fig.~\ref{figPipeline}. This is done in the Appendices. This mathematical part is necessary to accurately transition between the various levels of generality at which sheaf theory may be expressed and applied. %We prove new results showing that all meaningful applications of cellular sheaves can be rightfully extended to sheaves over arbitrary partially ordered sets (posets). In particular, in Appendix~\ref{secMathCohomology} we propose a new algorithm of sheaf cohomology computation with proven minimal complexity. 

Sections~\ref{secReview} and~\ref{secReviewShvsML} act as a review of the papers on the applications of sheaf theory within computer science and deep learning. Many fascinating applications of sheaf theory in theoretical computer science and natural sciences have been proposed in the literature; Section~\ref{secReview} surveys this class of papers. We believe that some of these papers attracted less attention in applied sheaf community than they actually deserve; sheaf-theoretical ideas emerged in a variety of areas of computer science. We provide brief exposition of these areas, and how sheaves work in each particular case. 

More recent and data-driven applications of sheaves in deep learning are described in Section~\ref{secReviewShvsML}. Most applications of this sort are based, explicitly or implicitly, on the prior works in applied topology. This is the main reason why this section is placed after the description of non-DL applications. In this section we survey the papers introducing various sheaf-type neural networks as well as the papers that apply sheaf-type neural networks for solving practical problems. We concentrate on the verbal explanations of the novelty of such papers and reduce the number of formulas to a minimum: the precise details on implementation of architectures can be found in the original papers.

The content of this review motivates a number of open problems related to the application of sheaf theory which is the focus of Section~\ref{secProblems}. Some of these problems have emerged from our attempts to formalize commonly used practices, while others are natural questions which arose from mathematical and industrial practice. We address them towards a broad community of mathematicians, computer scientists, and specialists in geometric and topological deep learning. 

It is highly likely that the list of applications and ideas presented in our survey is incomplete. We would be happy to know if some important papers or ideas are missing, especially if they already address or solve the stated problems. 

As already mentioned above, most modern applications of sheaves are restricted to the class of cellular sheaves: sheaves defined on posets of cells of regular CW-complexes. In this case there is a canonical\footnote{up to change of cells' orientations} choice for a cochain complex, called the cellular cochain complex, that is suitable for computations of laplacians and cohomology. The mathematical contributions of this view were motivated, in part, by the question ``Does the pipeline of Fig.~\ref{figPipeline} make sense for arbitrary finite posets instead of CW-complexes?''. We answer this question in the affirmative and prove a number of results, mainly within algorithmic homological algebra. These results are present in Appendix~\ref{secMathCohomology}. The main theoretical contributions of the paper are described below.

We introduce the following notions.
\begin{itemize}
  \item Morse cell poset, Definition~\ref{definMorseHomologyCellPoset}.
  \item Cochain complex which honestly computes cohomology, Definition~\ref{definHonestlyComputes}.
  \item One-shot cohomology computation, Definition~\ref{definOneShotComplex}.
\end{itemize}
Every cell poset, in particular a poset of cells of a regular CW-complex, is a Morse cell poset. However, there exist geometrically meaningful examples of Morse cell posets which are not cell posets. See Example ~\ref{exMorseExample} for an example. We note the similarity of the two classical constructions:
\begin{enumerate}
  \item Cellular cochain complex defined over any cell poset (subsection~\ref{subsecMathCohomologyCellular}).
  \item Roos cochain complex (also called the standard simplicial resolution, or bar-construction) is defined over arbitrary posets (Subsection~\ref{subsecMathCohomologySimplicial}).
\end{enumerate} 
Both complexes honestly compute cohomology, but the visible advantage of cellular cochain complexes is that they provide one-shot computation.
\begin{enumerate}
  \item A poset $S$ supports a one-shot cohomology computation if and only if $S$ is a Morse cell poset, see Theorem~\ref{thmOneShotTheorem}.
  \item For any poset $S$ we prove a lower bound on complexity of sheaf cohomology computation (Theorem~\ref{thmMorseBound}) and prove that this bound is exact (Theorem~\ref{thmExactBound}) by providing the construction of a minimal cochain complex (Algorithm~\ref{algIncMatrixMain}). 
\end{enumerate}
The latter construction allows for the computation of sheaf cohomology over general finite posets in a way that is more optimal than with the Roos complex. In the case of cell posets (or Morse cell posets), the minimal cochain complex coincides with the cellular cochain complex, and the provided algorithm reduces to an algorithm which outputs incidence numbers between adjacent cells. In general, this algorithm defines an analogue to incidence numbers for non-cellular posets.

Finally, in Appendix~\ref{secMathLaplacians}, we develop a theory of sheaf Laplacians and diffusion in the generality of finite posets. It is shown that several known non-cellular laplacians, such as the hypergraph laplacian, fit in the constructed formalism.


\section{An intuitive introduction to sheaves}\label{secIntuitions}

\subsection{Geometrical structures}\label{subsecGeometryIsPoset}

\begin{defin}\label{definPoset}
A \textbf{partially ordered set (poset)} is a set $S$ together with a binary relation $\leq$, called the (non-strict) partial order, which satisfies
\begin{enumerate}
  \item $s\leq s$ for any $s\in S$;
  \item $s_1\leq s_2$ and $s_2\leq s_3$ imply $s_1\leq s_3$;
  \item $s_1\leq s_2$ and $s_2\leq s_1$ imply $s_1=s_2$.
\end{enumerate}  
\end{defin}

If $s_1\leq s_2$ and $s_1\neq s_2$ we simply write $s_1<s_2$.

We focus on posets as our primary geometric basis. There are several reasons for this choice.
\begin{itemize}
  \item Common geometrical data structures can be universally encoded as posets, see Examples~\ref{exGraphToPoset}--\ref{exGridToPoset} below.
  \item The majority of applied works covered in this review can be described as sheaf-like objects on posets.
  \item Sheaf theory in mathematics is originally formulated for topological spaces~\cite{BredonSheaves}. A finite topological space can be encoded as a finite poset and vice versa. See Proposition~\ref{propPosTop} for the precise equivalence.
\end{itemize}

%???? will do it
%\tom{include definition of poset here, especially $<$ operator} 

\begin{ex}\label{exGraphToPoset}
A simple undirected \emph{graph} $G=(V,E)$ is treated as a poset $\Cells(G)=(V\sqcup E,\leq)$ where the partial order is given by inclusion: if $v\in V$, $e\in E$, then $v<e$ if $v$ is a vertex\footnote{A symbol $v\face e$ is used in modern applied papers in this context.} of $e$. See Figure~\ref{figPosets}.
\end{ex}

Simple graphs are restricted to two levels in their posetal order and are therefore limited in their ability to represent more complex, higher-order relationships that may exist within a space. To address this limitation, various discrete geometric structures have been suggested as a means to encode higher-order relationships in data, assuming that the data is supported on topological domains. These, too, may be represented as posets.

\begin{ex}\label{exHypergraphToPoset}
A \emph{hypergraph} $H$ on a vertex set $V$ is a collection of subsets of $V$ (which is commonly assumed to contain all singletons $\{v\}$ for $v\in V$). Then we have a partially ordered set $S(H)=(H,\subset)$ with the order given by inclusion. See Figure~\ref{figPosets}.
\end{ex}

\begin{ex}\label{exBinRelToPoset}
Another way to transform a hypergraph $H$ into a poset is similar to Example~\ref{exGraphToPoset}: we can assume all hyperedges incomparable and set $\bar{S}(H)=V\sqcup H$, with relation $v<h$ if a vertex $v$ belongs to a hyperedge $h$. More generally, one can take any \emph{binary relation} between two sets $R\subseteq A\times B$. This relation defines a strict order on $A\sqcup B$, if we assume that all elements of $A$ are pairwise incomparable and all elements of $B$ are pairwise incomparable. %\emph{Combinatorial complexes}~\footnote{As mathematicians, we find the naming of the term unfortunate.}, a construction introduced in topological deep learning~\cite[Ch.4]{TDLbeyond}, belongs to this example. It may not seem so formally, but all message passing techniques in CCNNs conceptually reduce to binary relations.
\end{ex}

\begin{ex}\label{exSimpCpxToPoset}
A \emph{simplicial complex} $\ca{K}$ on a finite vertex set $V=\{1,\ldots,m\}$ is a collection of subsets of $V$, which is closed under inclusion, i.e. $\sigma\in\ca{K}$ and $\tau\subset \sigma$ implies $\tau\in\ca{K}$. The elements of $\ca{K}$ are called simplices. The collection $\Cells(\ca{K})$ of non-empty simplices of $\ca{K}$ is partially ordered by inclusion. Any simplicial complex $\ca{K}$ defines a poset $\Cells(\ca{K})$. See Figure~\ref{figPosets}.
\end{ex}

\begin{ex}\label{ex2pts2edges}
There exist natural examples of ``point-free'' posets, which do not originate from partial order in a Boolean cube. For example, consider a finite set $\{1,2,a,b\}$ and impose a partial order by setting $1<a$, $1<b$, $2<a$, $2<b$.
\end{ex}

\begin{ex}\label{exCellCpxToPoset}
Consider a \emph{regular CW-complex} $\ca{X}$, see Definition~\ref{difinCWhausdorff}. The collection $\Cells(\ca{X})$ of its cells is partially ordered by inclusion. See Figure~\ref{figPosets}.
\end{ex}

\begin{ex}\label{exGridToPoset}
A pixel grid $P$ can be considered as a 2-dimensional CW-complex with square cells (\emph{quadrillage}). This gives rise to the poset $\Cells(P)$. This poset (and the corresponding Alexandrov topology) is the main object of study in digital topology~\cite{HermanBook,KovalevskyBook}
\end{ex}

Different geometric structures are well suited to represent different types of relationships. In particular, hypergraphs are well-suited to represent social networks ~\cite{amato2017influence}, where hyperedges encode groups of people. Hypergraphs have also been used to model the structure of thematic corpora in document analysis applications ~\cite{lee2024hints}. Simplicial complexes are used to represent coauthorship graphs ~\cite{ebli2020simplicial}, molecular graphs in computational chemistry and biology ~\cite{lan2023simplicial}, and in encoding 3D shapes as meshes ~\cite{lee2019implementation}. 

\begin{figure}
  \centering
  \includegraphics[scale=0.24]{pics/posetsExamples.pdf}
  %\includegraphics[scale=0.12]{pics/GeomStructures.jpg}
  \caption{Common geometrical structures encoded as posets}\label{figPosets}
\end{figure}

\begin{con}\label{conHasseDiag}
It is convenient to represent finite posets graphically by means of Hasse diagrams. The \emph{Hasse diagram} $G(S)$ of a poset $S$ is a finite directed graph $(S,E)$ with $(s_1,s_2)\in E$ if and only if $s_1<s_2$ and there is no element $s'\in S$ such that $s_1<s'<s_2$. In other words, we represent a relation $s_1<s_2$ by an edge from $s_1$ to $s_2$ if there are no other intermediate elements between $s_1$ and $s_2$. The original order relation is recovered\footnote{In the finite case, of course.} as the transitive closure of $G(S)$: we have $s_1\leq s_2$ in $S$ if and only if there is a directed path from $s_1$ to $s_2$ in $G(S)$. 

The Hasse diagram is the particular case of transitive reduction of a graph known in computer science. Since transitive reductions are known to be the most memory-optimal data structures to represent a directed graph~\cite{TransitiveReduction}, Hasse diagrams are the most optimal structures to store posets. All figures in the paper show not the posets themselves (not all relations), but the corresponding Hasse diagrams.
\end{con}

\begin{rem}\label{remManyCellPosets}
There exists a subclass of particularly well-behaved posets, called \emph{cell posets}, see Definition~\ref{definCellPoset}. It serves as a finite data structure, which captures the topological intuitions about compact CW-complexes. The posets introduced in Examples~\ref{exGraphToPoset},\ref{exSimpCpxToPoset},\ref{exCellCpxToPoset},\ref{exGridToPoset} are cell posets, following Proposition~\ref{propCellPosetToCpx}. Examples~\ref{exHypergraphToPoset} and~\ref{exBinRelToPoset} are generally not cell posets. %Practically, cell posets are helpful when it comes to effective computation of higher-order cohomology, see Theorem~\ref{thmOneShotTheorem}. In other cases, there seem to be no reason to prefer cell posets over other posets.
\end{rem}


\subsection{Exploration of geometrical structure}\label{subsecGeometryExploration}

\begin{ex}\label{exBat}
A bat navigates an environment by means of echolocation. How is it even possible to understand something about the global geometry of an environment by staying at a single point? There is an obvious answer. An environment is capable to conduct signals: there are PDE's telling how a signal at a single point propagates to infinitely close points. At a more fundamental level sheaves, as informally introduced in subsection~\ref{subsecFundNotions}, are involved. A sheaf\footnote{In the continuous mathematical model where a bat flies in a manifold $X\subseteq\Ro^3$, we deal with the sheaf of differential functions $\ca{O}^\infty(X)$.} is needed to mathematically model two claims: (1) every point of the environment is capable to store some information, or ``the state of matters'' in this point, (2) if points are close, there is a specified relation connecting their states.

A somewhat similar but a way more classical example is~\cite{HearDrum}, hearing the shape of a drum. Essentially, we speak about eigenvalues of Laplace--Beltrami operator on Riemannian manifold $X$. But on the more fundamental level we need a drum to store local information and conduct signals. This is why we are working not with the drum $X$ itself, but with a sheaf on $X$.  %\tom{the above two examples don't really make sense to a reader who doesn't know what a sheaf is. we might consider either moving this example until after sheaves are defined, or describing the aspects of a sheaf that are relevant to these examples, instead of using the word ``sheaf'' itself}
\end{ex}

\begin{figure}
  \centering
  \includegraphics[scale=0.24]{pics/posetSheaf.pdf}
  \caption{Poset is just a pure structure. Sheaf assigns some information container to each node and specifies how information flows between containers.}\label{figPosetSheaf}
\end{figure}

It is difficult to perform exact computations about continual spaces and differential sheaves so in practice we are interested in discretized problems. In the previous subsection we assumed that geometry is encoded in a (finite) poset. The point highlighted by the previous two examples stays valid for finite posets: in order to say something about a poset, we should first impose an additional structure on it, i.e. a sheaf, and then run some calculations over this structure.

\begin{con}\label{conInformalSheaf}
The information stored in a poset $S$ has purely syntactic nature: each claim $s_1<s_2$ means precisely ``there is a relation between $s_1$ and $s_2$''. In order to be able to say something more meaningful about the geometry of $S$, we impose some sort of semantics on $S$, and work with this semantics. This means that some mathematical entity $D(s)$ should be specified for every point $s\in S$, and, whenever $s_1<s_2$ in $S$, some specific relation $D(s_1<s_2)$ between the $D(s_1)$ and $D(s_2)$ is defined. Informally this means the following.
\begin{enumerate}
  \item The data type $D(s)$ prescribed to $s\in S$ serves as a local information container about possible ``states'' at the point $s$.
  \item The relation $D(s_1<s_2)\colon D(s_1)\to D(s_2)$ describes information flow or interaction between related points of $S$.
\end{enumerate}
See Figure~\ref{figPosetSheaf} as a general illustration. The compositionality is required from the information flow: whenever $s_1<s_2<s_3$, we should be able to compose the information flow $D(s_1<s_2)$ from $D(s_1)$ to $D(s_2)$ with the information flow $D(s_2<s_3)$ from $D(s_2)$ to $D(s_3)$ and get the information flow $D(s_1<s_3)$ from $D(s_1)$ to $D(s_3)$.

The most general framework formalizing these intuitions is \emph{category theory}~\cite{MacLane}. With a category $\Vv$ chosen, the data containers $D(s)$ are just objects of $\Vv$, and $D(s_1<s_2)\colon D(s_1)\to D(s_2)$ are morphisms of $\Vv$. Every poset $(S,\leq)$ may be treated as a syntactic category $\cat(S)$, whose objects are the elements of $S$, and morphisms are the relations $a\leq b$. The compositionality requirement states that $D$ should be a functor from $\cat(S)$ to $\Vv$, i.e. a diagram on $S$. In the case of posets, a sheaf is synonymous with a diagram: diagrams on $S$ correspond to sheaves on the related Alexandrov topology on $S$ (Proposition~\ref{propDiagSheaf}). Henceforth, the words `sheaf' and `diagram' are used interchangeably in the main part of the text.
\end{con}

\begin{defin}\label{definSheafDiagramMainText}
A $\Vv$-valued \emph{diagram}, or \emph{sheaf}, $D$ on a poset $S$ is a functor from $\cat(S)$ to $\Vv$. In more detail, a sheaf consists of
\begin{enumerate}
  \item an assignment, to each element $s\in S$ of an object $D(s)\in \Vv$, called the stalk of $D$ at $s$;
  \item an assignment, to each inequality $s<t$, of a morphism $D(s<t)\colon D(s)\to D(t)$, called the structure maps or restriction maps~\footnote{In applied papers these maps are called restriction maps, however this may create a confusion with the term used in the definition of a presheaf on a topology, see Remark~\ref{remPresheafInformally}.} of the sheaf $D$. Structure maps should satisfy the \emph{compositionality equations}: for any $s_0<s_1<s_2$, the equality\footnote{Here and below we use the notation for the composition borrowed from theoretical programming: $f;g=g\circ f$.} holds
      \begin{equation}\label{eqCompositionalityMainPart}
      D(s_0<s_1);D(s_1<s_2)=D(s_0<s_2).
      \end{equation}
\end{enumerate}
\end{defin}

For example, if $\Vv=\Sets$ is the category of finite sets, then each stalk is a set, and structure maps are mappings between sets. If $\Vv=\Ro\Vect$ is the category of real vector spaces, then each stalk $D(s)$ is just a vector space, and structure maps are linear maps of vector spaces. In finite-dimensional case each stalk corresponds to $\Ro^{d_s}$ and structure maps $D(s<t)$ are matrices of size $d_t\times d_s$.

\begin{ex}
If $G=(V,E)$ is a graph, and $S=\Cells(G)$ is the corresponding poset, defined in Example~\ref{exGraphToPoset}, then assigning a $\Ro\Vect$-valued sheaf $D$ on $S$ means that every vertex $v\in V$ is assigned a vector space $D(v)$, every edge $e\in E$ is assigned a vector space $D(e)$, and, whenever $v\in e$ (also denoted $v\face e$), there is a linear map $f_{v,e}\colon D(v)\to D(e)$.
\end{ex}

%???? Example with the graph was requested by German. Will do (or do not)

\begin{rem}
Imposing a $\Vv$-valued sheaf on $S$ makes it possible to borrow computational abilities of the target category $\Vv$ and use them to mine geometrical features of $S$. Sheaves algebraize geometry.
\end{rem}

The remark above is informal but motivates subsequent discussion providing evidence towards its veracity. %However, we need certain theoretical guarantee, that consideration of all possible sheaves on $S$ does not forget the actual geometrical structure of $S$. There is such a guarantee: the poset $S$, up to isomorphism, can be reconstructed from the category $\Shvs(S;\Vv)$ of all $\Vv$-valued sheaves on $S$, --- at least for meaningful choices of $\Vv$. See Propositions~\ref{propTopoiFullyFaithful} for $\Vv=\FinSets$ and~\ref{propRecoverXfromAbCat} for $\Vv=\ko\FinVect$ respectively.

\subsection{Global sections and coherent states}\label{subsecGlobalSectionsAndCoherence}

The choice of target category $\Vv$ has significant computational ramifications. The compositionality requirements imply a notion of equality in $\Vv$ and an ability to solve the system of equations defined by the underlying poset structure. This is covered in more detail in Construction~\ref{conOtherCategoriesInvLim}.

\begin{rem}\label{remConcrete}
In most applications, the target category is assumed \emph{concrete}~\cite[p.26]{MacLane}, meaning its objects are treated as sets and morphisms as structure-preserving maps between sets. In this case, the notion of choosing an element $x\in X$ of an object $X\in\ST{C}$ is a well-defined operation.
\end{rem}

The categories $\Sets$ and $\Ro\Vect$ are concrete. The category $\cat(S)$ of a poset is not concrete. To make it concrete would mean to define some good $\Sets$-valued sheaf on $S$. In the following, $\Vv$ denotes a concrete category.

\begin{defin}\label{definGlobalSectionsConcrete}
Consider a sheaf $D$ on a poset $S$. A \emph{global section} of $D$ is a choice, for each $s\in S$, of an element $x_s\in D(s)$, such that whenever $s_1<s_2$ in $S$, the following equality holds true
\begin{equation}\label{eqCoherentStates}
D(s_1<s_2)\colon x_{s_1}\mapsto x_{s_2}.
\end{equation}
The set of all global sections is denoted by $\Gamma(S;D)$. The set $\Gamma(S;D)$ is a subset of the Cartesian product $\prod_{s\in S}D(s)$. For each $t\in S$, there is a map $\Gamma(S;D)\to D(t)$ which picks the $t$-th component of a global section: $(x_s\mid s\in S)\mapsto x_t$. %\tom{$\Gamma$ was used earlier as notation for a graph. Not sure if you're intending to overload that notation or not. AA: seems resolved}
\end{defin}

\begin{figure}
  \centering
  \includegraphics[scale=0.24]{pics/globalSections.pdf}
  \caption{Example of non-coherent states of a given sheaf, and an example of coherent collection of states, i.e. global section.}\label{figGlobalSections}
\end{figure}

\begin{rem}\label{remCoherentStates}
The collection of elements $(x_s\mid s\in S)\in\prod_{s\in S}D(s)$ satisfying equations~\eqref{eqCoherentStates} may be interpreted as the choice of local states $x_s\in D(s)$ which are coherent with respect to information transfer rules provided by $D$, see Fig.~\ref{figGlobalSections}
\end{rem}

\begin{rem}\label{remReduceEquationsToGenerators}
In the system of equations defined by an arbitrary poset $S$~\eqref{eqCoherentStates}, we must only consider the equations corresponding to edges of the Hasse diagram $G(S)$ to determine global sections: the set of solutions will not change. Indeed, if $D(s_1<s_2)x_{s_1}=x_{s_2}$ and $D(s_2<s_3)x_{s_2}=x_{s_3}$ hold true, then the equality
\[
D(s_1<s_3)x_{s_1}=D(s_2<s_3)\circ D(s_1<s_2)x_{s_1}=x_{s_3},
\]
follows from the compositionality assumption~\eqref{eqCompositionalityEquations}. This remark is a straightforward way to reduce the number of equations defining a global section. A less straightforward way to simplify calculations even further is to utilize the local topological structure of $S$, e.g. if $S$ is a cell poset, see Remark~\ref{remCompatibilityReducesToCells}.
\end{rem}

\begin{rem}\label{remLocalSection}
The term ``global section'' suggests, there should be a notion of a ``local section''. It exists indeed, and it is quite natural: the coherent relations may be defined and satisfied only on a chosen subset $U$ of $S$. A collection $(x_s\mid s\in U)$ is called coherent on $U$ if it satisfies equations~\eqref{eqCoherentStates} restricted to $U$.

Notice that, whenever $x_s$ is chosen, equations~\eqref{eqCoherentStates} allow to compute $x_t=D(s<t)x_s$ for any $t>s$. For this reason, the subset $U$ in this construction is usually assumed to satisfy the assumption that whenever $s\in U$ and $t>s$, then $t\in U$. Such subsets are precisely the open subsets of the Alexandrov topology $X_S$ corresponding to $S$, see Construction~\ref{conDiagToSheaf}. In general, sheaves are defined on a topology --- with each open set $U\subset S$ we associate the set $\ca{F}(U)=\Gamma(U;S|_U)$ of sections coherent on $U$, see Construction~\ref{conDiagToSheaf}.
\end{rem}

\begin{con}\label{conOtherCategoriesInvLim}
Consider the category $\Ro\Vect$. All equations~\eqref{eqCoherentStates} are linear, so that $\Gamma(S;D)$ is a vector subspace in the direct sum $\bigoplus_{s\in S}D(s)$. This means $\Gamma(S;D)$ lives in the same category $\Ro\Vect$, it is not just a set, but a vector space itself. 

Instead of making such remark for any category $\Vv$ under consideration, a universal categorical construction of inverse limit is used for the formal definition of ``the global sections object'' in arbitrary category, 
%since it works in any reasonable category. The global sections $\Gamma(S;D)\in\Vv$ are defined as the inverse limit $\lim\limits_{\leftarrow S}D$ of the $S$-shaped diagram in the category $\Vv$,
see Constructions~\ref{conGlobalSections} and~\ref{conInverseLimit} and Remark~\ref{remConcreteLimSets}. The ability to take (finite) inverse limits in a category reduces to two basic operations: formation of finite direct products %(notice that the product $\prod_{s\in S}D(s)$ appears in the intermediate step of the definition of $\Gamma(S;D)$),
and formation of finite equalizers, i.e. the ability to solve equations. Both opportunities are present in the common categories, such as $\Sets$ or $\ko\Vect$, see Remark~\ref{remLimsToProductsEqualizers} for more details. From this perspective, all the subsequent constructions such as Laplacians and sheaf diffusion can be seen as numerical approaches to find the equalizer.
\end{con}

%???? will hopefully clarify it by returning some stuff from QA 
%\tom{I think the above two paragraphs need more motivation. I don't think the casual reader will know what a limit object or equalizer is. Consider either introducing these definitions earlier or providing an approachable description of these notions along with the constructions.}

For the purpose of this brief exposition we restrict to the target category $\Vv=\Ro\Vect$ of real vector spaces.

\begin{con}\label{conConstantSheaf}
Consider a vector space $V\in\Ro\Vect$. The easiest and most natural way to define a sheaf on $S$ is to set all its stalks equal to the same object $V$, and all structure maps to be the identity maps $\id_V$. This sheaf is denoted $\bar{V}$ and called \emph{the constant sheaf}. Such sheaves are frequently used in algebraic topology.
\end{con}

\begin{ex}\label{exConsensusAlongApath}
Consider a path graph $G_p$ and its corresponding poset $S_p=\Cells(G_p)$ see Fig.~\ref{figConstSheaves}, left. The constant sheaf $\overline{\Ro^1}$ is shown schematically on the same figure. In this example the vector space of global sections is the diagonal subspace
\[
\Gamma(S_p;\overline{\Ro^1})=\left\{(x,x,\ldots)\in\prod\nolimits_{s\in S_p}\overline{\Ro^1}(s)\right\}\cong \Ro^1.
\]
Indeed, the states are coherent if and only if they coincide on the endpoints of each arrow of the Hasse diagram, since all structure maps of $\overline{\Ro^1}$ are just the identity maps.
\end{ex}

\begin{figure}
  \centering
  \includegraphics[scale=0.23]{pics/constantSheaves.pdf}
  \caption{Constant sheaves over a path graph, and over a cycle graph, and their global sections.}\label{figConstSheaves}
\end{figure}

\begin{ex}\label{exConsensusAlongAcircle}
Consider a similar example with a cycle graph $\Cy_n$ on $n$ vertices, the corresponding poset $S_n=\Cells(\Cy_n)$ and, the constant sheaf $\overline{\Ro^1}$ as before, see Fig.~\ref{figConstSheaves}, right. Again, we have $\Gamma(S_n;\overline{\Ro^1})\cong \Ro^1$.
\end{ex}

\begin{ex}\label{exConsensusGraphCase}
In general, if $G$ is a graph and $\overline{\Ro^1}$ is a constant sheaf on $\Cells(G)$, then we have
\begin{equation}\label{eqNumberOfConComponents}
\Gamma(\Cells(G);\overline{\Ro^1})\cong\Ro^{c(G)},
\end{equation}
where $c(G)$ is the number of connected components of $G$. Indeed, the coherence relations~\eqref{eqCoherentStates} imply the equality of states over graph elements (vertices and edges) from the same connected component. Distinct connected components may have distinct coherent states.

More generally $\dim\Gamma(S;\overline{\Ro^1})$ equals the number of connected components of the geometric realization $|S|$ of $S$, see Definition~\ref{definGeomRealPoset} and Example~\ref{exConstantSheaf}.
\end{ex}

\begin{figure}
  \centering
  \includegraphics[scale=0.25]{pics/moebius.pdf}
  \caption{M\"{o}bius sheaf on a cycle graph.}\label{figMoebius}
\end{figure}

\begin{ex}\label{exConsensusAlongMoebius}
Consider the same cycle graph $\Cy_n$ as in Example~\ref{exConsensusAlongAcircle}, but this time with the different sheaf $D_\mu$. All stalks of $D_\mu$ are $\Ro^1$ and all maps but one are identity maps. The exceptional map is the multiplication by $-1$, see Fig.~\ref{figMoebius}. We can call it a Mbius sheaf, as the analogy with the Mbius strip is transparent. In this case, the system of coherence equations~\eqref{eqCoherentStates} only has a trivial zero solution: propagating the equality along the circle, we end up with the equation $c=-c$, which has a unique solution $c=0$ (since $\Ro$ is not a field of characteristic $2$). Therefore $\Gamma(S_n;D_\mu)=0$.
\end{ex}

\subsection{Consensus higher and wider}\label{subsecConsensusGoesWild}

We have seen that the notion of a global section allows one to reason about coherent (or consensual, or compatible) states of some system over a geometrical structure. This connection of possible system state with system structure is encoded by a sheaf. 

However, the collection $\Gamma(S;D)$ of global sections, as a stand-alone object, is not very informative. Examples~\ref{exConsensusAlongApath} and~\ref{exConsensusAlongAcircle} demonstrate that the difference between path-graph and cycle-graph can't be seen by only observing the consensus states of the respective constant sheaves. The two classical ways to squeeze more information from the consensus equations are the following.
\begin{enumerate}
  \item Estimate how ``proofs of consensus'' (equations~\eqref{eqCoherentStates}) relate to each other. We have not only proofs of consensus, but also proofs between proofs, proofs between proofs between proofs, etc. This naturally leads to the notion of higher-order cohomology $H^*(S;D)$ of a sheaf, allowing one to estimate qualitative (i.e. topological) properties of $S$. This cohomological framework could potentially be replaced with higher categories and homotopy theory, but this latter framework is syntactical and algorithmically undecidable problems quickly emerge, such as the word problem. Although complicated, homological algebra is algorithmically feasible.
  \item Use ``finding consensus'' as an algorithmic prior. We can estimate the speed and direction of convergence to consensus using the spectral theory of Laplacians. Or we can use the process of reaching the consensus (or the combination of several such processes) to model unknown dynamical systems on a geometrical structure, leading to the collection of sheaf neural network architectures which have been proposed within the deep learning literature.  
  %Estimate the speed of ``reaching the consensus'', --- the global properties of a computational process that converges to a consensus. This idea leads to heat diffusion and random walks --- and ultimately to the study of spectral characteristics of the Laplacian matrices, and further applications in constructions of neural networks. %This involves some sort of physical modelling, where we initialize random states $x_s^{(0)}\in D(S)$ and then modify them in order to minimize the loss function $L(x)=\sum_{s<t}\|x_t-D(s<t)x_s\|^2$ (or any other quadratic function on $\bigoplus_{s\in S}D(s)$ whose zero subspace coincides with the subspace $\Gamma(S;D)$ of consensus states). The gradient descent flow of this function is called the heat diffusion on a sheaf. It is a system of linear differential equations and its matrix is called the sheaf Laplacian. The speed of convergence of the process (the speed of reaching the consensus) depends on the spectral characteristics of the Laplacian. These characteristics allow to estimate quantitative (i.e. geometrical) properties of $S$.
\end{enumerate}

These two items are covered in more detail in the next subsections.

\subsection{Sheaf cohomology}\label{subsecCohomologyMainPart}

The general idea of cohomology can be understood by analyzing the difference between Examples~\ref{exConsensusAlongApath} (consensus on a segment) and~\ref{exConsensusAlongAcircle} (consensus on a circle).

\begin{ex}\label{exPathVsCircle}
Informally, the difference between a segment and a circle lies in the number of substantially different ways the consensus is propagated. In the path graph, the consensus between any two nodes $s_0$ and $s_n$ propagates along a single path. A value $x\in\Ro^1=\overline{\Ro^1}(s_0)$ can be copied into to the stalk $D(s_n)$ by the consecutive solution of linear equations $x_s=x_t$ with $s<t$ along the unique path $s_0<t_1>s_1<t_2>s_2<\cdots>s_n$. In the cycle case, given a value $x\in\Ro^1=\overline{\Ro^1}(s_0)$, another point $s_n$ receives the signal from two sides. It is the same signal ``Set $x_{s_n}$ equal to $x$'', however, it arrives at $s_n$ by two distinct routes: clockwise and counterclockwise, see Fig.~\ref{figConstSheaves} for reference.

Algebraically, this intuition can be quantified by the fact that, in the cycle, there are redundant linear equations among~\eqref{eqCoherentStates}--the coherence equations have dependencies. These dependencies may be formalized with higher-order cohomology. We have
\[
H^1(S_p;\overline{\Ro^1})=0,\mbox{ for a path, and } H^1(S_c;\overline{\Ro^1})\cong\Ro^1\mbox{ for a cycle.}
\]
In other words, higher cohomology of the constant sheaf can distinguish a path graph from a cycle graph.
\end{ex}

Relations between relations are sometimes called \emph{syzygies}. To have a notion of a syzygy, one needs the target category $\Vv$ to be an abelian category. Abelian categories integrate nicely into the toolbox of linear and homological algebra. Sufficiently well-behaved abelian categories such as the category $\ko\Vect$ of vector spaces over a fixed ground field $\ko$, or algebras, or sheaves, support the machinery of derived functors, introduced in mathematics by Cartan and Eilenberg~\cite{careil1956homalg}.

\begin{defin}\label{definCohomologyMainPart}
Let $S$ be a poset, $D$ a sheaf on $S$ valued in a sufficiently nice abelian category $\Vv$, and $j\geq 0$ a nonnegative integer. Then \emph{the cohomology} $H^j(S;D)\in\Vv$ is the \emph{$j$-th right derived functor} $R^j\Gamma(S;D)$ of the functor of global sections.
\end{defin}

In most deep learning-related papers, a different definition of cohomology can be found: the one based on cellular cochain complex. It is only applicable to sheaves over CW-complexes, but in this case it coincides with the above definition (see Theorem~\ref{thmCWcohomologyIsAGcohomology}). The posetal perspective employed thus far is more general than the CW-complex case, so we provide the above fundamental definition of cohomology and move all associated details to Appendix~\ref{secMathCohomology}.  

\begin{con}\label{conCochainMainPart}
Despite its fundamental theoretical importance, Definition~\ref{definCohomologyMainPart} is inconvenient from computational perspective. In order to actually compute sheaf cohomology, one needs to associate, with a sheaf $D$, a cochain complex $(C^*(S;D);d)$ (see Definition~\ref{definCochainCpx}):
\begin{equation}\label{eqCochainSheafMainPart}
0\stackrel{d}{\rightarrow} C^0(S;D)\stackrel{d_0}{\rightarrow} C^1(S;D)\stackrel{d_1}{\rightarrow} C^2(S;D)\stackrel{d_2}{\rightarrow} \cdots,
\end{equation}
with a theoretical guarantee that its cohomology
\[
H^j(C^*(S;D);d)=\Ker d_j\colon C^j(S;D)\to C^{j+1}(S;D)/\im d_{j-1} C^{j-1}(S;D)\to C^j(S;D)
\]
is isomorphic to the sheaf cohomology $H^j(S;D)$ as given by Definition~\ref{definCohomologyMainPart}. If this is the case, we say that the cochain complex $C^*(S;\cdot)$ \emph{honestly computes cohomology} of sheaves on $S$, see Definition~\ref{definHonestlyComputes}. 

There are infinitely many cochain complexes which honestly compute cohomology, the examples can be found in subsections~\ref{subsecMathCohomologySimplicial} and~\ref{subsecMathCohomologyCellular}. Luckily, there is a convenient criterion which checks if a given cochain complex honestly computes sheaf cohomology, given by the famous Grothendieck's universal delta-functor theorem~\cite{grothendieck1957tohoku}. We reformulate it in a less technically loaded manner as Theorem~\ref{thmHonestCohomology}.
\end{con}

Cohomology $H^j(S;D)$ in degrees $j>0$ is typically not found in deep learning applications outside of the computation of persistent homology. The $0$-th cohomology $H^0(S;D)$ coincides with the global sections $\Gamma(S;D)$ of a sheaf, and this is typically the cohomological information employed by most applications. The cochain complex~\eqref{eqCochainSheafMainPart} is important for the actual computation of global sections, since any complex honestly computing cohomology satisfies
\[
\Gamma(S;D)=H^0(S;D)\cong H^0(C^*(S;D),d)=\Ker d_0\colon C^0(S;D)\to C^1(S;D).
\]
Although only the components of degrees $0$ and $1$ are involved in the above formula, the choice of the suitable cochain complex affects the construction of objects like the sheaf Laplacian and consequently influences signal processing applications based on this spectral information.
%This the notion of cochain complex essentially important for all the subsequent story.

The construction of a cochain complex that honestly computes cohomology constitutes a nontrivial procedure and heavily depends on the type of geometrical structure $S$.

\begin{ex}\label{exCellPosetsSupportCanonicalCCpx}
Cell posets $\ca{X}$ (see Remark~\ref{remManyCellPosets}) are considered the most convenient class of discrete geometrical structures, because, among all cochain complexes honestly computing cohomology of sheaves on $\ca{X}$, there exists the distinguished one --- \emph{the cellular cochain complex} $C_{CW}^*(\ca{X};D)$ (Definition~\ref{definCWcochainComplex}). Cell complexes are used, among other applications, in the definitions of the cellular sheaf Laplacian, commonly used in deep learning applications. See Construction~\ref{conCellComplexDiffusion} for further discussion.

A similar construction, called a Morse cochain complex, is suggested in subsection~\ref{subsecMathOneShot}, which generalizes cellular cochain complexes to more general posets. The construction, as its name implies, is motivated by Morse theory.
%Cellular cochain complex is unique, at least up to changing orientations of cells, see Definition~\ref{definCellOrientation}.
%The cellular cochain complex is one-shot in the sense that each stalk $D(s)$ contributes to the direct sum $\bigoplus_j C^j_{CW}(\ca{X};D)$ exactly once, see Definition~\ref{definOneShotComplex}. We prove that a sort of a converse statement holds true: if $S$ supports a \emph{one-shot functor} which honestly computes cohomology of sheaves on $S$, then $S$ should be something like a cell poset, see the One-shot Theorem~\ref{thmOneShotTheorem}.
\end{ex}

\begin{ex}\label{exHypergraphsRoosCpx}
Hypergraphs, viewed as two-layered posets like in Example~\ref{exBinRelToPoset}, are not cell posets. A cellular cochain complex does not exist for such structures because incidence numbers cannot be canonically defined for a hypergraph as is natural for graphs. Nevertheless, there is a classical construction of Roos complex, also known as standard simplicial resolution, which honestly computes cohomology (see Theorem~\ref{thmRoosIsAGcohomology}) on any poset, including those derived from hypergraphs. It can be shown that the definition of the hypergraph sheaf Laplacian, appearing in the recent papers on hypergraph sheaf networks, are basically the Laplacians defined over Roos complexes. See Construction~\ref{conHypergraphDiffusion} for further discussion.
\end{ex}

It should be mentioned that Roos complexes are sub-optimal for computing cohomology and for defining Laplacians due to their large size. Other cochain complexes can be used, leading to matrices of smaller size. This is an important issue in practice as well: both sheaf neural networks and more general cohomology computations suffer from large computational complexity. In subsection~\ref{subsecMathMinimalComputations} we prove the lower bound for the complexity of cochain complexes honestly computing cohomology, and propose a new construction of the minimal cochain complex $C^*_{\min}(S;D)$ which is well-defined for any finite poset $S$ and achieves the lower complexity bound. 

\subsection{Laplacians and heat diffusion}\label{subsecLaplaciansMainPart}

The general utility of Laplacians can be understood with a basic example.

\begin{ex}\label{exConsensusAlongVariousCycles}
Consider a poset $S_n$ corresponding to a cycle graph $\Cy_n$ on $n$ vertices, as in Example~\ref{exConsensusAlongAcircle}, and the constant $\Ro^1$-valued sheaf on $S_n$. Its cohomology is given by
\[
(H^0(S_n;\Ro^1)\cong \Ro^1,H^1(S_n;\Ro^1)\cong \Ro^1,0,0,\ldots)
\]
which is independent of the number of vertices composing the cycle. This is not surprising: all graphs $\Cy_n$ are homeomorphic, and have the same qualitative properties. %\tom{This notation for the cycle graph is different than in section 2.3. Is this intentional? Note the one in 2.3 overloads the global section notation, as discussed in a prior comment.} Seems resolved

One distinguishing feature between the graph $\Cy_3$ and the graph $\Cy_{100}$ is the time needed for a signal to spread across each graph. The process of signal propagation is modeled by heat diffusion (Definition~\ref{definHeatDiffusion}). The convergence rate of the process is determined by the smallest positive eigenvalue $\lambda_{\min}$ of the Laplacian matrix of the graph: the bigger $\lambda_{\min}$ the faster the process converges, as explained in Subsection~\ref{subsecMathDiffusion}. It is known~(see e.g.~\cite[ex.1.5]{Chung}) that the (unnormalized) Laplacian of $\Cy_n$ has eigenvalues $2-2\cos\frac{2\pi k}{n}$, for $k=0,1,\ldots,n-1$. Hence we have
\[
\lambda_{\min}(\Cy_n)=2-2\cos\frac{2\pi}{n} \approx \frac{C}{n^2}
\]
which is smaller for $n=100$ than for $n=3$. The heat spreads faster over a cycle of smaller length--an intuitive result.
\end{ex}

\begin{figure}
  \centering
  \includegraphics[scale=0.24]{pics/diffusion.pdf}
  \caption{Heat diffuses in a constant sheaf, resulting in a system of coherent states}\label{figDiffusion}
\end{figure}

Let $D$ be $\Ro\Vect$-valued sheaf on $S$. Laplacians appear as a byproduct of solving the equations on global sections of $D$. Assume that some cochain complex $(C^*(S;D),d)$ (see~\eqref{eqCochainSheafMainPart}) honestly computes cohomology of $D$. Then the computation of $j$-th cohomology reduces to finding the subquotient $\ker d_j/\im d_{j-1}$ for the sequence
\begin{equation}\label{eqShortExactMainPart}
C^{j-1}(S;D)\stackrel{d_{j-1}}{\rightarrow} C^j(S;D)\stackrel{d_j}{\rightarrow} C^{j+1}(S;D).
\end{equation}
If inner products are chosen on the vector spaces $C^i(S;D)$, we can construct the sheaf Laplacian.

\begin{defin}\label{definSheafLaplacianMainPart}
\emph{The sheaf Laplacian} of $j$-th order of sheaf $D$ on a poset $S$ (defined on a suitable cochain complex $(C^*(S;D),d)$) is the operator
\[
\Delta_j=d_j^*d_j+d_{j-1}d_{j-1}^*\colon C^j(S;D)\to C^j(S;D).
\]
The subspace $\ca{H}^j(S;D)=\Ker\Delta_j\subseteq C^j(S;D)$ is called \emph{the subspace of harmonic cochains} of $D$.
\end{defin}

Here $A^*$ denotes the adjoint of $A$, or the transposed matrix $A^\top$ if an orthogonal basis is chosen. The basic linear algebra implies the isomorphism $\ca{H}^j(S;D)\cong H^j(S;D)$, see Corollary~\ref{corHarmonicIsCohomology}. %Consequently, the $j$-th Betti number $\dim H^j(S;D)$ of a sheaf is the number of fundamental solutions to the linear system $\Delta_jx=0$, or equivalently, the multiplicity of the zero eigenvalue of $\Delta_j$.

\begin{rem}\label{remVanillaLaplacian}
When $j=0$, we have $\Delta_0=d_0^*d_0$, since there is no component of degree $-1$ in the cochain complex. In this case we have
\[
\ca{H}^0(S;D)=\Ker\Delta_0=\Ker d_0=H^0(S;D)\cong \Gamma(S;D).
\]
The subspace of global sections is isomorphic to, and coincides with, the subspace of harmonic $0$-cochains, which is the zero eigenspace of $\Delta_0$. In most sources the term ``sheaf Laplacian'' refers to $\Delta_0$. We call $\Delta_0$ \emph{the vanilla sheaf Laplacian}, to distinguish it from higher order sheaf Laplacians $\Delta_j$ with $j>0$ described above.
\end{rem}

%We have seen that the Betti number is the multiplicity of the zero eigenvalue of $\Delta_j$. However the Laplacian $\Delta_j$ is strictly more informative than just the Betti number.

\begin{con}\label{conSheafEnergyMainPart}
The vanilla sheaf Laplacian $\Delta_0$ is a nonnegative self-adjoint operator. It is diagonalizable with all eigenvalues nonnegative (Subsection~\ref{subsecMathHodge}). Henceforth the quadratic function $Q_{\Delta_0}(x)=\langle \Delta_0x,x\rangle$ defined on $C^0(S;D)$ is nonnegative and convex. The function $Q_{\Delta_0}(x)$ is called the (\emph{Dirichlet}) \emph{energy function} of the sheaf $D$. The zero-set of $Q_{\Delta_0}$ coincides with its set of points of local (or global) minima and also coincides with the harmonic subspace $\ca{H}^0(S;D)=\Ker \Delta_0$ isomorphic to the space $\Gamma(S;D)$ of global sections.
\end{con}

\begin{defin}\label{definHeatDiffusion}
The flow of gradient descent, either discrete or continuous, of the energy function $Q_{\Delta_0}$ on the euclidean space $C_0(S;D)$ is the (\emph{$0$-th order}) \emph{heat diffusion equation} on the sheaf $D$.
% (computed on the chosen cochain complex $(C^*(S;D),d)$ and the chosen euclidean structure on a sheaf).
\end{defin}

%\begin{rem}\label{remDiffusionFormula}
%The discrete heat diffusion of order $j$ of a sheaf $D$ on $S$ is a discrete dynamical system (or cascade) on the space of $j$-cochains $C^j(S;D)$ given by
%\[
%x_{k+1}=(1-2\eta \Delta_j)x_k,\qquad k=0,1,2,\ldots,
%\]
%where $\eta$ is the parameter of a gradient descent (a hyperparameter). See details in Definition~\ref{definHeatDiffusion}.
%\end{rem}

In practice, the normalized version of the Laplacian (Construction~\ref{conNormalizationOfLaplacian}) is used in calculating diffusion. Independently of the initial point, the flow converges to a harmonic cochain in $C_0(S;D)$. The convergence rate of heat diffusion is determined by the eigenvalues of $\Delta_0$.

\begin{con}\label{conLambda1asymptotics}
Let $\lambda_{\min}$ denote the minimal positive eigenvalue of $\Delta_0$. For an initial point $x_0$ let $x_{+\infty}$ denote the limiting harmonic cochain of the heat diffusion process starting at $x_0$. Then, for generic initial value $x_0$, the distance $\|x(t)-x_{+\infty}\|$ is asymptotically equivalent to $Ce^{-2\lambda_{\min}\eta t}$ --- in the case of continuous heat diffusion and  $(1-2\eta\lambda_{\min})^k$ --- in the discrete case. See Construction~\ref{conSolutionDiagonalized} for the proof. The remaining positive eigenvalues of the Laplacian $\Delta_0$ measure the lower asymptotic terms of the heat diffusion.
\end{con}

Eigenvalues of sheaf Laplacians provide real numerical characteristics which can be used in applications or as auxiliary information for various machine learning algorithms. These characteristics are interpretable in the sense that they have clear geometrical and physical meaning. A warning about spectral determinability is warranted, however.

\begin{rem}
A Laplacian itself is not an invariant of a sheaf $D$. First of all, to make things well-defined one needs to impose inner product on all the stalks $D(s)$ --- the construction concerns \emph{Euclidean sheaves}, not sheaves in general. This step in the definition is often skipped because the vector spaces in most applications are just the coordinate vectors spaces $\Ro^k$; the basis comes for free and is assumed orthonormal. The second and probably even more important remark is that the Laplacian is defined on a particular cochain complex $(C^*(S;D),d)$. As discussed in Construction~\ref{conCochainMainPart}, there are many cochain complexes which honestly compute cohomology of $D$ (and many more cochain complexes which compute something else!). Which complex to choose to define Laplacians is a modeling problem. If $S=\ca{X}$ is a cell poset, there is a canonical choice of the cellular cochain complex $(C^*_{CW}(S;D);d_{CW})$, as in Example~\ref{exCellPosetsSupportCanonicalCCpx}. The term \emph{sheaf Laplacian} usually refers to the Laplacian defined on this particular cochain complex. The ambiguity in the definition is the choice of cells' orientations: luckily, it is easily proved that Laplacians, up to conjugation, do not depend on the orientations of cells. %So, after all, eigenvalues of Laplacians of cellular sheaves provide invariants of cell posets. The way how cellular cochain complex works in degree 0, i.e. for global sections, is elaborated in Construction~\ref{conCellComplexDiffusion} and used in most applications. 
For hypergraphs, Roos complexes may be chosen to define a Laplacian, as described in Construction~\ref{conHypergraphDiffusion}. 
\end{rem}

\subsection{Sheaf learning}\label{subsecSheafLearning}

Using sheaf theory as a descriptive language to speak algebraically about geometrical objects is prominent in both theoretical mathematics and computer science. In this subsection we show how the language of sheaf theory has recently fused with deep learning. The following specific constructions mirror the specific implementations of sheaf theory that have, thus far, been applied in deep learning applications.

\begin{defin}\label{definSheafDiffusion}
Let $\Delta_0=d_0^*d_0\colon C^0(S;D)\to C^0(S;D)$ be the vanilla sheaf Laplacian of a sheaf $D$ on $S$, and $\eta>0$ a hyperparameter. Then the generator of the discrete heat diffusion, i.e. the map
\[
\sd_D\colon C^0(S;D)\to C^0(S;D),\qquad \sd_D(x)=x-2\eta \Delta_0x
\]
is called \emph{the sheaf diffusion} determined by $D$.
\end{defin}

Heat diffusion on a graph well known in spectral graph theory~\cite{Chung} is the instance of this general definition as the following example shows. 

\begin{ex}
Let $S=\Cells(G)$ be a graph (the poset associated with a graph $G=(V,E)$ as in Example~\ref{exGraphToPoset}), and $\overline{\Ro^1}$ be the constant sheaf on $S$, so that the vector space $C^0(S;\overline{\Ro^1})=\bigoplus_{v\in V}\Ro^1$ is freely spanned by the vectors $\varepsilon_v\in V$. This space can be understood as assigning a single real value, like the temperature, to each vertex of a graph. In this case, the map $\sd_D$, for the unnormalized Laplacian, is defined on the basis vectors by
\[
\sd_D(\varepsilon_v)=\varepsilon_v(1-2\eta \deg v)-\sum\nolimits_{u\sim v}\eta \varepsilon_u,
\]
where $\deg v$ is the degree of the vertex $v$ and the summation is taken over all vertices $u$ adjacent to $v$. The diffusion operates as averaging the temperature of $v$ and all its neighbors.
\end{ex}

\begin{defin}\label{definSheafNN}
A \emph{sheaf diffusion layer} determined by a sheaf $D$ is the linear map $\sd_D\colon C^0(S;D)\to C^0(S;D)$. A \emph{sheaf-type neural network} is an architecture that uses sheaf diffusion layers, probably in combination with other types of layers, in its construction.
\end{defin}

Sheaves $D_i$ (the collection of matrices $D_i(s<t)$) used in the layers are sometimes learned during the training of the neural network.

\begin{ex}\label{exShNNlinear}
The basic example of a sheaf-type neural network is given by a composition $F_{D_1,\ldots,D_l}=\sd_{D_1};\cdots;\sd_{D_l}$ of sheaf diffusion layers determined by a finite sequence of sheaves. This network, as written, is not particularly expressive since the map $F_{D_1,\ldots,D_l}$ is linear. In practice, some non-linear operators (e.g. coordinate-wise sigmoidal or ReLU operations) are inserted between sheaf diffusion layers to make the resulting function class of the network more expressive.
\end{ex}

\begin{rem}\label{remNNdynamical}
Sheaf-type neural networks are related to the mathematical theory of heat diffusion in a similar manner to how transformer networks are related to dynamical properties of Kuramoto models~\cite{TransformersKuramoto} and Vlasov equations~\cite{VlasovTransformers}. In both cases, if all layers of a network are the same, and there are infinitely many infinitesimal layers, the network becomes a mathematically reasonable object: a dynamical system with nice asymptotical properties. It should be noted, however that this perspective implies sheaf-type neural networks are much simpler than transformers because they correspond to linear dynamical systems, while the mathematical models of transformers are non-linear. The applied potential of sheaf networks seem to arise from the fact that the domain $C^0(S;D)$ of their definition has a rich semantical structure determined by the underlying geometry of $S$. 
\end{rem}

\begin{rem}\label{remMessagePassing}
Sheaf diffusion on a graph may be seen as a form of linear \emph{message passing}. In general, message passing updates the state $x_v$ of a vertex by the formula
\begin{equation}
x_v^{(t)} = \phi\left(x_v^{(t-1)}, \biguplus\limits_{u \in N_v}\psi(x_u^{(t-1)}, x_v^{(t-1)}, w_{uv}^{(t-1)}) \right)
\end{equation} 
where $x_v^{(t)} \in \Ro^d$ is the subsequent state of $v$, $N_v$ is the graph neighborhood i.e. the set of nodes adjacent to $v$, $w_{uv}^{(t-1)}$ is a learnable vector associated to an edge $\{u,v\}$, $\psi$ and $\phi$ are fixed piecewise differentiable functions called \emph{message} function, and \emph{update} function respectively, and $\biguplus$ is a permutation invariant aggregation function\footnote{It is common in machine learning to use symbol $\bigoplus$ for such aggregation functions, however this usage contradicts the more classical notation of a direct sum in abelian categories, which plays an important role in the mathematical part of a paper.} of arbitrary number of inputs, e.g. coordinate-wise sum or max or similar. General graph neural networks consist of several layers performing message passing.   

Message passing can be defined in a similar way on arbitrary posets $S$ instead of graphs as long as the neighborhood relation between elements of $S$ is somehow defined from the structure of the poset. For example, if $S$ is a cell poset, then two $k$-dimensional cells $\sigma,\tau$ may be called adjacent if they either cover a common $(k-1)$-cell, or are covered by a common $(k+1)$-cell. In this case, heat diffusion for a higher-order Laplacian given by Definition~\ref{definSheafLaplacianMainPart} becomes an example of linear message passing. Simply speaking, the neighborhood is defined by the 2-hop distance on the Hasse diagram of $S$, and the message passing between cells is performed by summation of matrix products over all such 2-hop routes. General constructions of message passing of this sort can be found in~\cite{TDLbeyond} or~\cite{DemystifyingHOGNN}.
\end{rem}

\subsection{Neural sheaf diffusion}

In practice, sheaf-type neural networks combine the mechanism of sheaf diffusion with other standard engineering practices, such as adding intermediate learnable weight matrices, adding auxiliary channels, and reducing the number of parameters by picking a suitable non-linear submanifold in the manifold of all sheaves.

\begin{con}\label{conNeuralSheafDiffusion}
The \emph{neural sheaf diffusion} (NSD) model~\cite{bodnar2022neural} learns a sheaf $D$ on a graph $G$ with $n$ vertices and all vertex-stalks $D(s)$  isomorphic to $\Ro^d$. Additionally there are $f_1$ input channels and $f_2$ output channels. A single layer of neural sheaf diffusion is given by the function $F\colon C^0(S;G)\otimes \Ro^{f_1}\to C^0(S;G)\otimes \Ro^{f_2}$ defined by
\begin{equation}\label{eqNeuralSheafDiff}
F=\sigma\circ ((\sd_D\circ W_1^{\oplus n})\otimes W_2),
\end{equation} 
where $W_2\colon \Ro^{f_1}\to \Ro^{f_2}$ is a learnable weight matrix mixing the channels, $W_1\colon \Ro^d\to \Ro^d$ is a learnable matrix applied independently in each vertex-stalk, $\sd_D$ is the diffusion operator as in Definition~\ref{definSheafDiffusion}, and $\sigma$ is the coordinate-wise sigmoid, providing nonlinearity. 
\end{con}

\begin{rem}\label{remGCN}
If $D$ is the constant sheaf $\overline{\Ro^1}$, $W_1=\id$, then $W_2$ is the only parameter collection to be learned. In this case formula~\ref{eqNeuralSheafDiff} defines the layer of the classical Graph Convolution Network (GCN) of Welling and Kipf~\cite{kipf2017semisupervised}.
\end{rem}

One can estimate space and time complexity of sheaf-type neural networks, in particular, the complexity of NSD.

\begin{con}
Consider a graph with $m$ edges and $n$ nodes and NSD layer over this graph with stalk dimension $d$, $f_1$ input channels and $f_2$ output channels. Let us introduce $c = f_1 \cdot d$. If we were to learn the matrices $W_1$, $W_2$ and all the restriction maps independently, the number of parameters (i.e. the space complexity) would be $d^2+f_1f_2+2md^2$ as follows easily from Construction~\ref{conNeuralSheafDiffusion}. This complexity depends linearly on the size of the graph. To overcome this issue, the paper~\cite{bodnar2022neural} proposes to learn not every restriction map separately, but a single function $F\colon (\Ro^c\oplus \Ro^c)\to \Ro^{d^2}$ of the form $F(x_v, x_u) = \sigma(V(x_v\oplus x_u))$ with the learnable matrix $V$ of size $dc\times d^2$, where $x_v\oplus x_u$ is the concatenation of the states of vertices computed at runtime. The space complexity of a single NSD layer becomes $d^2+f_1f_2+2cd^2$. 

Notice that GCN, the classical graph neural network described in Remark~\ref{remGCN}, has total number of parameters equal to $c^2$ which is in some cases even bigger than the space complexity of NSD.
\end{con}

One of the main limitations of NSD is computational complexity, by which we mean the time complexity of an inference of a single layer.

\begin{rem}\label{remComputationalComplexity}
For $d\times d$ general matrices serving as restriction maps we have computational complexity $O(n(c^2 + d^3) + m(cd^2 + d^3))$, see~\cite{bodnar2022neural}. Assuming $c\gg d$ this complexity becomes $O(nc^2 + mcd^2)$. It is claimed in~\cite{bodnar2022neural} that in practice it is better to use $1 \leq d \leq 5$, but experiments were conducted only for small graphs; the optimal $d$ for large graphs has to be found. For a GCN with $c$ many channels, the complexity equals $O(nc^2 + mc)$, so we see that NSD has $d^2$ times higher computational complexity than GCN. This could be a serious limiting factor for practical applications of NSD for really large graphs. Some of the modern NSD-based architectures surveyed in subsection~\ref{subsecFurtherArchitectures} were designed to overcome this limitation.
\end{rem}


\section{Review of applications}\label{secReview}

This section reviews the existing applications of sheaf theory in computer science and deep learning. To add order to the wide variety of applications, we introduce an organizing scale. The applications are roughly clustered into six categories starting with the most ``theoretical'' and ending with the most ``practical''. This scale roughly resembles the history of subject, with applications of sheaves within category theory and topology appearing at the beginning and applications of sheaves in the design of neural networks appearing towards the end --- they are moved to a separate section. %For the readers interested in this sort of applications, we recommend to jump directly to subsection~\ref{subsecReviewShvsML}. 

The scale is as follows:
\begin{enumerate}
%\setcounter{enumi}{0}
  \item Categorical level: sheaves as a categorical language to substitute geometry. Subsection~\ref{subsecReviewCatLevel}.
  \item Topological level: sheaves as a framework to study local-to-global properties and interactions. Subsection~\ref{subsecProblTopologyLevel}.
  \item Abelian level: sheaves valued in abelian categories reduce problems to homological algebra. Subsection~\ref{subsecReviewHomologyLevel}.
  \item Real level: sheaves valued in $\Ro$-vector spaces reduce problems to numerical linear algebra and combinatorial Hodge theory. Subsection~\ref{subsecReviewRealLevel}
  \item Manifold level: connection sheaves as a discrete model for vector bundles over smooth manifolds. Subsection~\ref{subsecReviewManifoldLevel}
  \item Data level: sheaves and traces of algorithms operating on sheaves as the source of learning signals, applications in deep learning, and design of neural network architectures. Section~\ref{secReviewShvsML}.
\end{enumerate}
%Subsection~\ref{subsecBeyondML} contains a somewhat arbitrary list of applications of sheaves beyond ML, CS, and pure mathematics. We hope that the reader will find these applications motivating.

%\setcounter{subsection}{-1}

\subsection{Categorical level}\label{subsecReviewCatLevel}

The attempts to use sheaves as a tool to describe behavior of complex systems, either natural or originating in computer science, and translate this behavior into the language of category theory seem to have been made from the very moment that sheaves were invented. The early applications of sheaf theory were based on its tight connection with mathematical logic. We do not give a review of all advances in this direction, and refer to classical sources: the chapter of Fourman and Scott in~\cite[Ch.``Sheaves and Logic'']{ApplOfSheaves} and the book of MacLane and Moerdijk~\cite{MacLaneMoerdijk}. The general utility of sheaves in logic can be roughly explained by the fact that the category $\Shvs(X;\Sets)$ of $\Sets$-valued sheaves on a given space $X$ forms a topos, (see Appendix~\ref{secMathSpaceRestoredFromShvs}), a categorical abstraction corresponding, in a certain sense, to a logical theory. Topos-theoretical foundations of mathematics are more flexible than the classical Zermelo--Frnkel set theory in the sense that they are capable to incorporate ideas of intuitionistic logic. Sheaf theory provides a semantics for intuitionistic logic~\cite[Ch.VI]{MacLaneMoerdijk} and~\cite[Ch.XIV-XV]{troelstra2014constructivism}. Since intuitionistic logic and constructive mathematics are closely related to type theories and programming language theory, sheaves play foundational role in the field.

In modal logic, the space $X$ parameterizes the collection of possible worlds. Sheaves on $X$ (valued in $\Sets$ or other conventional categories) are used as models of the predicate modal logic. Roughly speaking, the topos $\Shvs(X;\Sets)$ is treated as a logic parameterized by possible worlds. In more down-to-earth applications, the topos $\Shvs(X;\Sets)$ is treated as a logic parameterized by points or regions of some understandable topological space $X$. These ideas motivated a number of papers in computer science where sheaves were used to model interacting systems, concurrent processes, or generic ``objects'' which may be spread in physical space, time, or more abstract spaces of possibilities or observations. This category of applications was proposed by Goguen~\cite{GoguenEarly} and developed in subsequent works~\cite{SheafSemConcObjects, ObjAsProcesses, Airchinnigh1997TheGO, HUT-TCS-A23, SheafSemAgents, Malcolm2006, Sendroiu, Malcolm2009} to list just a few. All these papers may be thought of as a part of even more general program on categorical foundations of computer science. See also~\cite{SpivakTemporal} and~\cite[Ch.7]{Spivak} for the modern exposition and treatment of these ideas.

Sheaf semantics of programming languages proved useful in the construction of normalization by evaluation (NbE) algorithms for certain important classes of lambda-calculi~\cite{NormByEval1995,NormByEvalCoproducts2001}. Given a $\lambda$-expression in a specified language, the algorithm seeks to reduce the expression to its normal form, which is needed e.g. to compare two equivalent expression. Instead of doing the reduction syntactically (i.e. performing a long sequence of term rewritings), the normalization by evaluation technique translates the string into a semantical representation, from which the normal form is extracted more easily. Semantics of presheaves over small categories proved useful for a simply typed $\lambda$-calculus~\cite{NormByEval1995}, while sheaves over Grothendieck topologies solve the decision problem for a simply typed $\lambda$-calculus with binary coproducts~\cite{NormByEvalCoproducts2001}. 

\subsection{Topological level}\label{subsecReviewTopLevel}

Most modern papers on applied sheaf theory emphasize that sheaves are about interaction between local and global properties of geometric objects. A sheaf -- understood not as a diagram on a poset, but rather as a diagram on the lattice of open subsets of a topological space -- satisfies the axioms of gluing and locality, see Definition~\ref{definSheaf}. These properties make sheaves a suitable mathematical abstraction to describe situations when compatible local entities can be uniquely assembled into a global entity. Such situations often occur in practice. Goguen~\cite[p.165]{SheafSemConcObjects} even formulated a ``sheaf hypothesis''\footnote{This hypothesis is similar in its epistemic nature to Church thesis in computer science, or the assumption that data are sampled i.i.d. from an unknown distribution which constitutes the basis of statistical learning theory.}: all objects are understood through their observable computational behavior; the observations satisfy the gluing axiom, hence can be axiomatized with sheaves. %This hypothesis is reminiscent to the claim made in subsection~\ref{subsecGeometryExploration}, that the only way to understand a geometrical shape $X$ is to make it into a medium propagating signals, i.e. to define a sheaf on $X$.

An old but quite impressive example of using local-to-global principle in the design of algorithms belongs to Srinivas~\cite{Srinivas1993}. He conceptualized the classical Knuth--Morris--Pratt string matching algorithm in the language of sheaves over Grothendieck topologies, and generalized this algorithm from strings to general data structures satisfying certain category-theoretical assumptions. From the practical perspective, each generalized pattern-matching problem reduces to the problem of subgraph matching (SGM). These ideas were further developed in~\cite{SrinivasSpecWare} and fused into a number of subsequent papers on application of category-theoretical methods in high-level software engineering.

The ideas of algorithm design based on sheaves similar in spirit to those proposed by Srinivas~\cite{Srinivas1993} are reborn in the modern approaches to the pattern matching and structure isomorphism tests in the works of Conghaile~\cite{Conghaile2022CohomologyIC} and Abramsky~\cite{Abramsky2022presh}. These methods take origin in the theory of finite models in logic and its interactions with category theory~\cite{AmbramskyStrPower}. The pattern matching problems $\ca{P}$, such as SGM or constraint satisfaction (CSP), are known to be NP-hard. In practical applications, they are replaced with algorithmically feasible approximations $\ca{P}_k$; the general construction being motivated by Ehrenfeucht--Frass games, also called spoiler-and-duplicator games. The well-known example is the famous Weisfeiler--Leman (WL) graph isomorphism test\footnote{There is no mathematical result that graph isomorphism problem is NP-hard, however it is believed that this problem do not belong to PTime.}, or its generalizations, the $k$-WL tests for $k>1$. We refer the reader to the general modern machine learning-related review~\cite{WLinMLsoFar} and~\cite{WLandLogic} for the result connecting $k$-WL to the first-order logic.

We can gain an informal understanding of the above approximations their relationship to sheaf theory through the following example. Let $\Hom(H,G)$ denote the set of embeddings of a graph $H$ into a graph $G$, while $\Hom_k(H,G)$ consists of collections of embeddings of ``subgraphs of $H$ of size $\leq k$'' in $G$ which are compatible on the intersections. To solve the problem of subgraph matching means to find a coherent collection in $\Hom_k(H,G)$; it can be naturally formulated as finding a global section of a certain presheaf $\ca{H}_k(H,G)$ associated with the problem, see~\cite[Obs.1]{Conghaile2022CohomologyIC}. While finding the global section is still computationally hard, the construction and size of $\ca{H}_k(H,G)$ is polynomial in $|H|\cdot|G|$. This opens a way to improvements of $k$-approximate pattern-matching algorithms, which are still polynomial in time, but are stronger than their standard versions.


\subsection{Abelian level}\label{subsecReviewHomologyLevel}

The modern era of machine learning and statistics is characterized by using arrays of real numbers or the real vector spaces $\Ro\Vect$ as the basic abstractions--as opposed to core computer science where booleans and $\Sets$ play a primary role. Unlike $\Sets$-valued sheaves used in logic, sheaves valued in abelian categories, such as the category $\Ro\Vect$, form an abelian category themselves and support the notion of higher-order cohomology. Sheaves valued in abelian categories can be analyzed by tools from homological and linear algebra. Most methods and intuitions currently used in deep learning applications of sheaves, such as the notions of cellular sheaves and incidence numbers, originate from algebraic topology.

\subsubsection{Applied topology and signal processing}

While applications of algebraic topology may be found throughout the physical sciences at least as far back to the 1930s, the explicit usage of topological concepts within the context of data processing saw a growth in popularity in the early 21st century. Following the popularity of algorithmic techniques like persistent homology~\cite{edelsbrunner2002topological}, the field of applied topology and its data-scientific sub-field topological data analysis (TDA) have emerged as active areas of research within the applied mathematics community.

Early data-driven applications of algebraic topology were motivated by signal processing problems, especially those for which a non-trivial interaction exists between a topological domain and the signals measured on that domain. Problems like signal localization~\cite{de2007coverage}, image reconstruction~\cite{singh2007topological}, shape estimation~\cite{carlsson2004persistence}, or clustering~\cite{carlsson2009topology} benefitted from the global and compositional perspective offered by early TDA methods. While sheaf theory has been used to study and extend many of the theoretical guarantees enjoyed by TDA techniques~\cite{Curry,curry2015topological,kashiwara2018persistent}, the explicit application of sheaf-theoretic methods to data-driven problems only began to appear within the applied topology literature in the 2010s.

This applied sheaf theory program, emerging largely from the work of Curry, Ghrist, and Robinson, further generalized applied topological methods in network and algebraic signal processing through the language of sheaf cohomology. This application of sheaf theory flows primarily from the observation that the assignment of data to points within a space can be generalized from the assignment of single numbers to an assignment of more complex mathematical objects like vectors, groups or lattices to each point. Sheaf cohomology then provides machinery for organizing and observing obstructions to consistent assignment of this data, regardless of its structural complexity as explained in Subsection~\ref{subsecCohomologyMainPart}. 

In~\cite{curry2012euler}, a sheaf-theoretic presentation of Euler calculus is given, along with a number of potential application domains, including target enumeration, sampling, and signal convolution, among others. Such signal processing perspectives were further developed in~\cite{robinson2013nyquist,robinson2013understanding,robinson2015sheaf}, and~\cite{robinson2014topological}, resulting in an encompassing theoretical reinterpretation of signal processing within the language of sheaf theory and algebraic topology. This sheaf-theoretic modeling approach to signal processing has continued to receive modest interest through time, with applications in stratification learning~\cite{brown2021sheaf} and uncertainty quantification~\cite{joslyn2020sheaf} emerging more recently in the literature.

Concurrent to early work in sheaf-theoretic signal processing, Ghrist and Hiraoka~\cite{ghrist2011applications} proposed a reinterpretation of network coding problems through the language of cellular sheaf theory, targeting applications in maximum flow calculations, network extendibility, and network robustness analyses. This network coding application of sheaf theory was later extended in~\cite{ghrist2013topological} and~\cite{krishnan2014flow}, generalizing the max-flow min-cut theorem on networks to sheaves of semimodules over a network and using directed homology to measure realized flow and cut values. This generalization provides a theory for working with multicommodity or multi-source and multi-target flows on network, in addition to expanding the flow data type from real-valued flows to more complex mathematical objects like group- or lattice-valued flows. This network coding application of sheaf theory has persisted into more recent literature on network flows and routing, most notably in the study of delay-tolerant networking~\cite{short2021towards,short2022sheaf}.

Despite early work emerging from the applied topology community showing potential for the application of sheaf theory to data, early work in the area was primarily theoretical, employing sheaves as a model for phrasing desired compositional measurements within an application domain. This early lack of data-driven work was due primarily to the fact that sheaves require the definition of a topological base space upon which data may be associated and measured, and such base spaces are generally difficult to define and measure in many existing signal processing domains. It took the development of \emph{cellular sheaves}, largely accomplished within Curry's Ph.D. thesis~\cite{Curry}, for these sheaf-theoretic concepts to find their way into more realistic data-driven domains. Curry showed that one may define a type of constructible sheaf on a cell complex by endowing that complex with the Alexandrov topology, see details in Appendices~\ref{secMathStructures} and~\ref{secMathSheaves} for a precise definition. Since many signal processing problems are encoded discretely and often in a relational form, the cell complex provides a convenient topological base space for defining sheaves derived from real data thanks to its discretized, combinatorial nature and its ability to encapsulate graph-structured data.

\subsubsection{Utility of cohomology}

While the notion of cohomology comes for free once one defines a sheaf valued in the category of vector spaces, there is a dearth of papers depicting actual utility of cohomology in positive degrees for general sheaves. There are many papers applying (co)homology of constant sheaves and persistence sheaves. In the case of a constant sheaf, the cohomology coincides with the usual (singular) cohomology of a space, see Example~\ref{exConstantSheaf}. Betti numbers, the dimensions of cohomology of such sheaves, measure meaningful topological characteristics of a space: the number of connected components, the number of holes, 2-dimensional cavities, etc. This motivates further constructions of persistence sheaves and Euler calculus, and is explained in any basic book on TDA. The review of all such applications would require writing a separate paper; instead we refer to~\cite{DONUT} for a searchable database of works in applied topology. None of the papers listed in the previous subsection, proposes an actual application of Betti numbers of non-constant (non-persistent) sheaves.

%and lies beyond the scope of the current review. \tom{I'm not sure this is true. There are a number of works which use higher cohomology to study the characteristics of a space. They do not use sheaves because one typically does not need a sheaf to study the (co)homology of the base space itself.}

The main feature of sheaf theory is that individualized vector spaces are associated with various parts of a structure (whether the structure is understood as a purely mathematical entity or an abstraction for a physical mechanism). The parts of various mechanisms and engineering constructions may be modelled by 0-, 1- and 2-dimensional rigid cells of a polyhedral complex embedded in $\Ro^3$; each part has its own vector space representing the degrees of freedom allowed for this part; the way how parts are connected with each other provides linear maps between them. These vector spaces and linear relations naturally patch together into a cellular sheaf or cosheaf. The (co)homology of this sheaf then resemble degrees of freedom of the whole structure. 

Historically, the idea of using sheaves for analyzing degrees of freedom appeared in toric geometry~\cite{HowIsAGraph, GKMZara}, where the ``parallel redrawing problem'' (or deformation problem) was posed for a linear embedding of a graph in $\Ro^n$. The space of all parallel redrawings of a graph, --- those in which edges stay parallel to themselves, --- is isomorphic to the space of global sections of the sheaf which is now called the GKM\footnote{Named after Goresky--Kottwitz--MacPherson theorem known in toric geometry.} sheaf of a graph~\cite{JabeaBaird}. %This whole story was motivated by the famous Goresky--Kottwitz--MacPherson theorem, but its applied potential was not realized that time. 

Cooperband et al.~\cite{CooperbandThesis,HomGraphStat,reciprocalFigures} have shown a great potential of using sheaves and related homological theory in the structural engineering, as an effective language to speak about degrees of freedom. Force cosheaves, linkage sheaves, and position sheaves introduced in~\cite{HomGraphStat} provide an elegant description for many familiar phenomena in graphic statics. The recent paper~\cite{origami2025} applies sheaves to flexions of origami constructions. In~\cite{physicsinformed}, the idea of sheaf-theoretical engineering was applied to study dynamics of molecules viewed as graphs with rigid edges: anisotropic sheaf on a graph is introduced, so that Normal Mode Analysis --- a technique in biophysics --- becomes fully interpreted in terms of the spectrum of the corresponding sheaf Laplacian.

Whenever the underlying geometrical structure with a sheaf carries the action of a monoid $G$, the cohomology becomes $G$-representation. This allows to squeeze more features from the data since $G$-representations are more informative than vector spaces~\footnote{If $G=\Zo_{\geq 0}$ or $\Ro_{\geq 0}$ we get persistent cohomology instead of the ordinary version.}. Utilizing representations of finite groups of symmetries in graphic static was recently proposed in~\cite{equivariantSheaf}. 

An interesting category of applications of higher-order sheaf cohomology exists, which is similar in spirit to the obstruction theory known in algebraic topology (see e.g.~\cite{Adhikari2016}). Here, we make use of the first four nontrivial terms in the long exact sequence~\ref{eqLongExactDerivedCohomology}, i.e.
\[
0\to H^0(S;F)\to H^0(S;D) \to H^0(S;D/F) \to H^1(S;D)
\]
for a subsheaf $F$ of $D$. This sequence lets us interpret the 1-st cohomology group $H^1(S;D)$ as an obstruction to extend a global section of the quotient sheaf $D/F$ to a global section of $D$. Such extension problems appear in seemingly unrelated areas. In the field of topological signal processing, Robinson~\cite[Thm.4.4]{robinson2014topological} used the vanishing of $H^1(S;D)$ as a theoretical guarantee that a signal can be restored from a collection of samples. In the field of applications of finite models to CSP, Conghaile~\cite{Conghaile2022CohomologyIC} applied the vanishing condition of $H^1(S;D)$ to strengthen the classical $k$-WL and $k$-CSP tests. This idea is based on the subsequent work of Abramsky et al.~\cite{ContextParadox}, who used the $1$-st \v{C}ech cohomology group of a sheaf to characterize and measure the contextuality in quantum systems, databases, and CSP-problems. 

GKM-sheaves on non-cellular posets have a number of applications in toric topology. Ayzenberg et al. in~\cite{AyzMasSol,AyzBuchCluster,AyzSor} used computations of cohomology of such sheaves in combination with Morse theory to prove non-existence of asymptotical diagonalization algorithms for matrices of specific sparsity classes.

\subsection{Real level}\label{subsecReviewRealLevel}

Global sections are defined for sheaves valued in all reasonable categories, while the computation of cohomology is specific to abelian categories. Sheaf cohomology is defined for any ground field. The field $\Ro$ of real numbers is specific since it supports the features needed to pose and solve optimization problems, model physical processes, analyze spectral characteristics of objects, and so on. This basic functionality of real numbers and euclidean vector spaces led to the notion of sheaf Laplacians and the application of combinatorial Hodge theory within a sheaf-theoretic context.

Hansen and Ghrist~\cite{hansen2019toward, hansen2020laplacians} developed a spectral theory for cellular sheaves valued in $\Ro\Vect$ and showing that the sheaf Laplacian, a generalization of the Hodge Laplacian for sheaf-structured data, has a number of desirable properties which make it useful as an object for e.g. computing or approximating global sections of cellular sheaves. The sheaf Laplacian also serves as a drop-in replacement for the graph Laplacian in settings where graph-structured data is decorated with more complex assignments of objects to points or inherits functional relationships between points which may be encoded by restriction maps. Towards this end, the sheaf Laplacian has been used to define a distributed optimization technique, as the kernel of this operator provides a globally consistent assignment of sections to a space from a collection of local differencing operations~\cite{hansen2019distributed}. Inversely, given smooth signals assumed to be organized into a cellular sheaf structure, Hansen also showed that one may infer a sheaf Laplacian from these signals through linear optimization over the convex cone of sheaf Laplacians on a graph, mimicking more traditional graphical lasso techniques~\cite{hansen2019learning}. This intersection of TDA and sheaf theory continues to develop, with more recent works investigating persistent sheaf Laplacians~\cite{wei2021persistent}, sheaf-theoretic shape discrimination~\cite{arya2024sheaf}, and community detection~\cite{wolf2023topological}.

Interestingly, the versions of sheaf Laplacians, in particular higher order sheaf Laplacians, exist for non-abelian categories, --- in the situations when higher order cohomology itself is not defined. Ghrist and Riess~\cite{TarskiLapl} introduced the notion of Tarski Laplacian for a cellular sheaf valued in the category of lattices: in this case the convergence of the ``diffusion'' process to the ``harmonic'' cochain is guaranteed by the Tarski's fixed point theorem. The recent work~\cite{CatDiff} introduces the notions of Lawvere Laplacian and categorical diffusion by generalizing from the category of lattices to an arbitrary category enriched over quantales, where a generalization of Tarski's theorem holds. Such categorical diffusion allows one to treat some classical algorithms, such as Dijkstra's algorithm, as a form of sheaf diffusion, --- despite the fact that historically such algorithms are considered irrelevant to sheaf theory due to their non-linear nature. The mixture of sheaf-theoretical (maybe even homological!) methods with tropical mathematics, as it originally appeared in the study of algorithms, seems a promising area of future research.

\subsection{Manifold level}\label{subsecReviewManifoldLevel}

While a theoretical foundation for applied sheaf theory was developing within the applied algebraic topology and TDA literature, another more application-oriented thread of applied sheaf theory began to concurrently emerge from the graph theory literature. This line of work generally exploits generalizations of the graph Laplacian, derived from its original differential-geometric form, to process and manipulate higher-dimensional signals over a graphical base space. Here, a graph is treated as discrete substitute for a smooth manifold; the stalks $D(s)\cong \Ro^d$ are assumed to have the same dimension, and are treated as the ``fibers'' of a vector bundle, e.g. the tangent bundle. The structure maps $D(s_1)\to D(s_2)$ are treated as connection maps between fibers; they model the notion of a geometrical connection on a vector bundle, hence should be invertible; or even lie in a fixed gauge group, e.g. the orthogonal group $O(n)$. In practice, this abstraction provides the ability to integrate more complex signal data types into traditional graph signal processing approaches while permitting the expression of functional constraints within the base space. See Appendix~\ref{secMathConnection} for mathematical details.

%One may remove the requirement that all edge weights (or restriction maps) are strictly orthogonal, resulting in a \emph{matrix-weighted graph} which assigns $(n \times n)$-dimensional weight matrices $W_{uv}$ to each adjacent node pair $u \sim v$. The Laplacian matrix of such matrix-weighted graph is constructed in the same manner as the connection Laplacian, with diagonal blocks $L[u,u] = \sum_{v \sim u} W_{uv}$ and off-diagonal blocks $L[u,v] = -W_{uv}$. If each $W_{uv}$ is positive definite, this type of matrix-weighted graph simplifies to a weighting of the constant sheaf where $W_{uv}$ defines the inner product on edge stalks. More generally, if one defines via decomposition $F_{uv}^*F_{uv} = W_{uv}$, and assigns restriction maps $\ca{F}_{u < e} = F_{uv} = \ca{F}_{v < e}$ for each edge $e = u \sim v$. This is a cellular sheaf with vertex stalks in $\Ro^n$ and edge stalks in $\Ro^{\rk W_{uv}}$ for each $e = u \sim v$. The sheaf Laplacian of this cellular sheaf is equivalent to the matrix-weighted Laplacian.

Singer and Wu~\cite{singer2012vector} employed the graph connection Laplacian in the construction of a more general vector diffusion map technique which accounts for parallel transport of tangent vectors in the distance calculation between points. Given a finite set of points $X=\{x_1,\ldots,x_m\}$ in $\Ro^d$ sampled from an unknown submanifold $M\subset \Ro^d$ of dimension $n$, one may want to mine some geometrical properties of $M$. The procedure proposed in~\cite{singer2012vector} runs as follows. First, a proximity graph $G$ on $\{1,\ldots,m\}$ is built: it models the abstract (i.e. non-embedded) structure of the manifold. The tangent spaces $T_{x_i}M$ are modelled by the linear span of $n$ principal components of the subset $\{x_j\}$, where $j$ lies in the $G$-neighborhood of $i$. As abstract vector spaces all $T_{x_iM}$ have dimension $d$ (we forget their embedding in the ambient $\Ro^d$). The orthogonal matrices $O_{ij}\colon T_{x_iM}\to T_{x_jM}$ can be defined by a natural optimization procedure and constitute a discretized version of parallel transport maps that store information about the intrinsic geometry of $M$. The collection $\{D(i),O_{ij}\}$ may be viewed as a cellular $O(n)-$connection sheaf $D$ on a graph $G$. This abstraction faithfully models the latent manifold $M$.%, see Remark~\ref{remDifGeomConnections} for discussion. 

In particular, the vanilla sheaf Laplacian of an $O(n)$-connection sheaf on a graph may be seen an discretized version of the Laplace--Beltrami operator on a Riemannian manifold. Peach et al.~\cite{peachimplicit} used this idea to generalize Gaussian processes for learning vector-valued signals over latent manifolds. The authors consider pairs $\{(x_i, v_i) \mid i = 1, \dots, n\}$, of training data where $x_i$ are samples from a submanifold $M\subset \Ro^d$ and $v_i$ are sampled from its tangent bundle $TM$, where both $M$ and $TM$ are unknown. One then seeks to infer a Gaussian process which can provide an implicit description of the vector field over $M$ that agrees with the training set and provides a continuous interpolation at out-of-sample test points with controllable smoothness properties. To do this, the authors develop a discrete version of vector-valued Matrn Gaussian process, known for Laplace--Beltrami operators, to a discretized setting of Singer and Wu described above. Vector-valued Gaussian processes allow for the interpolation of vector fields which are consistent with respect to some underlying manifold structure. This has applications in a number of computer graphics and data reconstruction domains like, as is shown empirically by Peach et al., spatiotemporal brain activity reconstruction from EEG data.

The general modeling technique proposed by Singer and Wu has since been extended to improvements on iterative least squares in multi-object matching~\cite{shi2020robust}, matrix-weighted consensus~\cite{trinh2018matrix}, generalizations of random walks and centrality to connection graphs~\cite{kempton2015high, cloninger2024random}, among others~\cite{tuna2016synchronization, huroyan2020solving}. 

It should be mentioned that the original paper~\cite{singer2012vector} does not mention sheaves; however we formulated their results in the general terminology chosen for this review. A number of alternative terminologies can be found in the literature. A similar structure is called an $O(n)$-bundle on a graph by Gao et al.~\cite{gao2021geometry}. On a more theoretical level, similar structures on generic finite posets appear in the work of Barmak and Minian~\cite{Barmak2012GcoloringsOP} where they are called $O(n)$-colorings of posets.

\section{Sheaves and deep learning}\label{secReviewShvsML}

This section describes how sheaf theory has been integrated within deep learning theory and applied to develop new deep learning algorithms.

\subsection{From graphs to sheaves}

Interest in the graph deep learning techniques surged in the mid-2010s following developments in deep learning and the proliferation of graph-structured data from social media and computational biology, resulting in the area of graph deep learning or, more generally, geometric deep learning~\cite{bronstein2021geometric}. The foundational architecture for modern graph deep learning is the graph neural network (GNN), whose layers are determined by a mechanism of message passing on a graph, see Remark~\ref{remMessagePassing}. There exists a variety of GNN architectures implementing the above message passing mechanism, the general review can be found elsewhere~\cite{velivckovic2022message}. We concentrate on the part of this story adjacent to sheaves. 

One of the earliest examples of message passing architectures is the graph convolutional network (GCN)~\cite{kipf2017semisupervised}. It implements the message passing operation as an approximation of a localized spectral filter. In the terminology of this review, GCN is a network based on the sheaf diffusion for the classical graph Laplacian, i.e. the vanilla Laplacian of the constant 1-dimensional sheaf on a graph, see Remark~\ref{remGCN}. 

%While many real-world applications are influenced by homophilous behavior of signal-generating processes with respect to the topology of the base space, this homophily bias, as implemented by the iterative updating of node hidden representations towards the average values of their neighbors', is not always appropriate and sometimes detrimental to task performance. In addition, the assumption that signals are best processed according to a weighted averaging across channels is often overly simplistic: many real-world processes introduce functional interactions between signals within the base space.

In~\cite{hansensheaf} Hansen and Gebhart proposed the idea of Sheaf Neural Network architecture as a generalization of GCN. The architecture implements linear message passing, based on sheaf diffusion on a graph, but this time a sheaf may be arbitrary, not just constant. Arbitrary sheaves allow more freedom than the constant sheaf: nontrivial semantical information on relations between data assigned to adjacent nodes may be encoded in the structure maps of a sheaf. By the manual choice of a problem-related sheaf, the authors showed, through a simple example of a node classification task, the improved performance of sheaf neural networks over GCNs, which fail to perform above random chance in a heterophilous network. This performance discrepancy between Sheaf neural networks and GCN is due to the fact that for properly chosen restriction maps, i.e. when working over non-constant sheaves, the kernel of the laplaican is not the assignment of a constant cochain as in the case with GCNs~\cite{gebhart2022graph}. The idea of learning a sheaf from the data was proposed in~\cite{hansensheaf} but not implemented.

%%%%%%%%%%%%%%%%%%%%%%%%%5

Bodnar et al.~\cite{bodnar2022neural} substantially extended the idea of using sheaves in the design of neural architectures. It was known that GCN architecture (as well as more general GNN architectures proposed until 2022) suffers from oversmoothing and exhibits poor performance on heterophilic graphs. 
%proposed a Neural Sheaf Diffusion model, which is based on the Sheaf Convolutional Network~\cite{hansensheaf}. Again, a sheaf on a simple graph is considered. %A sheaf Laplacian operator measures the aggregated disagreement of opinions at each node. 
Following~\cite{hansensheaf}, the authors defined Sheaf Convolutional Network (SCN) which uses sheaf diffusion in a predefined sheaf, several signal channels, and auxiliary weight matrices (see Construction~\ref{conNeuralSheafDiffusion}) and overcomes the stated problem appearing in existing GNNs\footnote{The paper claims that all GNN architectures suffer from oversmoothing and poor performance on heterophilic data. However, since sheaf diffusion on a graph is a particular case of message passing, SCN becomes a subclass of GNNs, in their modern understanding. This makes the claim ``sheaf networks overcome the problems of GNN'' a logical contradiction, so we avoid stating it in this particular form.}. 

The paper~\cite{bodnar2022neural} justified that SCNs (and sheaf Laplacian) are more expressive than GCNs (and graph Laplacian) on the node classification task, by analyzing their linear separation power. For any connected graph $G$ with $C\geq 3$ classes of nodes GCN cannot linearly separate the classes of $G$ for any initial node features. At the same time SCN with $d \geq C$ and diagonal invertible restriction maps can linearly separate the classes of $G$ for almost any initial node features. They considered various constrains on structure maps in a sheaf, resulting in different types of sheaves: symmetric invertible, non-symmetric invertible, diagonal invertible, and orthogonal. These types of sheaves show different separation capabilities, that is, the node classification task on a graph can be solved by performing diffusion with the specific type of sheaves.

The novelty of the work~\cite{bodnar2022neural} was in the implementation of sheaf learning. The paper proposed the model called Neural Sheaf Diffusion (NSD). The most important difference from SCN is that restriction maps of sheaves depend on the layer, and are considered learnable parameters. The training of a neural sheaf diffusion model can be described as learning a finite set of sheaves from data. Experiments show that Neural Sheaf Diffusion overcomes many limitations of classical graph diffusion equations (and corresponding GNN models) and provides competitive results on node classification tasks, specifically in heterophilic settings.

The combination of cellular sheaf theory with ideas of message passing appeared in Bodnar's thesis~\cite{bodnar2023topological}. Bodnar takes a perspective on topological deep learning based on the two postulates. \textbf{Locality Postulate.} The data is attached to neighbourhoods of a topological space. \textbf{Structure Postulate.} The data has a relational structure, which is induced by the overlaps between the neighbourhoods of the topological space. The author proposes Message Passing Simplicial Networks (MPSNs) and CW Networks operating on simplicial and cell complexes respectively. %, see Definitions~\ref{definSimpComp} and~\ref{difinCWhausdorff}. 
Topological pooling algorithm Deep Graph Mapper is introduced in~\cite{bodnar2023topological}. The algorithm works as some sort of decrease the resolution on a graph in a way that higher-resolution version of a geometrical structure gets compressed into a lower resolution structure with a non-trivial sheaf on top of it. This procedure mimics the existing pooling approaches in classical convolutional neural networks, but may be applied to arbitrary graphs instead of grids. 

%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Development of sheaf-based architectures}\label{subsecFurtherArchitectures}

A number of new sheaf-inspired architectures was proposed after the work~\cite{bodnar2022neural}: they either deal with computational complexity described in Remark~\ref{remComputationalComplexity}, or generalize to non-graphical data structures, or propose some architectural innovations to enhance expressivity of a network.

%Consider a graph with $m$ edges and $n$ nodes and sheaf diffusion model over this graph having stalk dimension $d$ and $f$ many channels. Let us introduce $c = f \cdot d$. For space complexity, the total number of parameters is composed of matrix $W_1^{d\times d}$, $W_2^{f1\times f2}$ with $f_1$ input and $f_2$ output channels, and matrix $V = 2cd^2$ for computing the restriction map with $F(x_v, x_u) = \sigma(V[vec(X_v), vec(X_u)])$, where $vec()$ converts the $d \times f$ matrix into a $df$-sized vector. For GCN we have $c^2$ total number of parameters. 
%
%One of the main limitations of Neural Sheaf Diffusion (NSD) is computational complexity. For $d\times d$ general matrices serving as restriction maps we have computational complexity $O(n(c^2 + d^3 ) + m(cd^2 + d^3 ))$. Assuming $c\gg d$ this complexity becomes $O(nc^2 + mcd^2)$. Work~\cite{bodnar2022neural} claims that in practice it is better to use $1 \leq d \leq 5$, but experiments were conducted only for small graphs; the optimal $d$ for large graphs has to be found. For GCN with $c$ many channels, the complexity is $O(nc^2 + mc)$, so we see that NSD has $d^2$ times higher computational complexity than GCN. This could be a serious limiting factor for practical applications of NSD for really large graphs. Many modern NSD-based architectures are designed to overcome this limitation. 

Barbero et al.~\cite{barbero2022sheaf2} noticed that there were two main approaches to construct a sheaf over a graph described in subsequent works: either by manually constructing a sheaf based on domain knowledge or learn the sheaf end-to-end using gradient-based methods~\cite{bodnar2022neural}. However domain knowledge is often insufficient, and learning a sheaf could be computationally costly. Thus~\cite{barbero2022sheaf2} proposed to construct a sheaf from data in a deterministic way by adapting the method of Singer and Wu~\cite{singer2012vector} described in subsection~\ref{subsecReviewManifoldLevel}. The experiments show that the use of the resulting sheaf before the model-training phase improves the results on node classification tasks. Such an approach seems most well-suited for graphs with a small number of nodes.

%%%%%%%%%%%%%%%%%%%%%%%%%

For two similar but different local neighbourhoods GNN may generate identical node embeddings, thus GNN could fail to distinguish different graphs. Node positional encodings (PE) can prevent such undesirable GNN behavior by informing every node of its global position in the graph, thus breaking the similarity between nodes with similar neighbourhoods. Positional encodings are also required by Graph Transformers. He et al. in~\cite{he2023sheaf} introduced a novel way to construct PE based on eigenvectors of the sheaf Laplacian having the smallest eigenvalues. The authors consider several ways to obtain sheaf and the corresponding PE. The most simple option is to use just a graph Laplacian, generated by the constant sheaf with $d=1$. The other alternatives are to use the precomputed sheaf~\cite{barbero2022sheaf2} or the learned sheaf~\cite{bodnar2022neural}. The authors compared performance of GNNs without PE, with graph Laplacian based PE, and with precomputed sheaf based and learned sheaf based PE for node-level and graph-level tasks. GNNs equipped with precomputed or learned sheaf-based PE showed better results than GNNs without PE or with graph Laplacian based PE.

%%%%%%%%%%%%%%%%%%%%%%%%%

It was noticed by Barbero et al.~\cite{barbero2022sheaf1} that Graph Attention Networks (GATs)~\cite{velivckovic2018graph} suffer from the same two main problems as many other GNNs: oversmoothing and poor performance in heterophilic graphs. To keep the benefits of attention mechanism and fight these two problems at the same time, the authors introduced the Sheaf Attention Network (SheafAN), a generalization of the GAT utilizing sheaves on graphs. The key idea of SheafAN is to take a learned sheaf~\cite{bodnar2022neural} or a precomputed sheaf~\cite{barbero2022sheaf2} and, instead of using the vanilla sheaf Laplacian for sheaf diffusion, the blocks of the Laplacian are multiplied by attention weights. The authors also propose Res-SheafAN layer which is slightly different from SheafAN. The proposed approach shows good performance on heterophilic datasets, in particular it outperforms GAT. It also seems to be effective against oversmoothing, that is, the performance of SheafAN does not decrease as the number of layers increases.

In his thesis~\cite{barberoattention} Barbero studied the ability of sheaves and especially attention-based sheaves to solve the problems of oversmoothing and poor performance on heterophilic graphs. The thesis is built around the results of~\cite{barbero2022sheaf1} described above. The important novel part is an empirical evaluation of the influence of dimensionality (width) of stalks on the performance of the models on node classification task. The higher stalk width provides higher expressive power of the model, while also increasing the risk of overfitting. The experiments show that stalk width $d=4$ is often optimal. The used datasets are relatively small which amplify overfitting, so for larger datasets higher $d$ could be optimal. Another problem with a high stalk width $d$ is the additional memory costs, as the restriction maps grow quadratically with $d$.

%%%%%%%%%%%%%%%%%%%%%%%%%

The construction of local homology sheaf on a simplicial complex is well known in topology, in particular it was applied by McCrory~\cite{McCrory} to generalize Poincare duality to arbitrary simplicial complexes. The stalks and restriction maps of the local homology sheaf store important topological information about local neighborhoods of points in the geometrical structure. This notion appeared to be useful in applied topology~\cite{Robinson2018LocalHO}. The paper of Cesa and Arash~\cite{cesa2023algebraic} proposes using local homology sheaf for a clique complex of a graph to define sheaf diffusion and sheaf neural networks. While the idea of this architecture appears promising in capturing important topological information of the graph, its practical utility haven't shown to be useful so far.

%%%%%%%%%%%%%%%%%%%%%%%%%

In~\cite{bamberger2024bundle} Bamberger et al. introduced Bundle Neural Networks (BuNNs), which are the versions of GNNs operating via ``message diffusion'' instead of message passing. On a poset $S$ corresponding to a simple graph, a specific class of bundles ($O(d)$-connection sheaves) is defined, the so called flat vector bundles. In general, the definition of $O(d)$-connection sheaf $D$ requires to specify an orthogonal matrix $O_{v,e}=D(v<e)\in O(d)$ for each vertex-edge pair, see Appendix~\ref{secMathConnection}. The bundle is called flat if the matrices $O_{v,e}$ are all equal to a single matrix $O_v$ independently on the edge $e$. The monodromy of such bundle (see Remark~\ref{remMonodromy}), is trivial which makes the term consistent with the usual mathematical notation. Learning a flat bundle is easier than a general bundle simply because there are fewer parameters to learn. Still BuNN is reported to keep the advantages of general sheaf neural architectures.

%%%%%%%%%%%%%%%%%%%%%%%%%

Duta et al.~\cite{duta2024sheaf} consider sheaves on hypergraphs~\footnote{They are called cellular sheaves in the paper which is a terminological mistake. Since a hypergraph is not a cellular complex in any sense, the phrase ``cellular sheaf on a hypergraph'' makes as much sense as ``strong non-alcoholic beer''.} i.e. diagrams on 1-dimensional posets defined in Example~\ref{exBinRelToPoset}. Two types of Laplacians, linear and non-linear, are defined for such sheaves. The linear Laplacian is just the vanilla Laplacian as defined in Remark~\ref{remVanillaLaplacian} for the Roos complex, see Construction~\ref{conHypergraphDiffusion} and Remark~\ref{remRelationGrouping}. The linear Laplacian gives rise to sheaf diffusion in the standard manner described in subsection~\ref{subsecLaplaciansMainPart}. The non-linear Laplacian doesn't have a mathematical counterpart in the abelian sheaf theory, and resembles more general message passing mechanism on sheaves. Using these message passing mechanisms, the authors define Sheaf Hypergraph Neural Network (for linear Laplacian) and Sheaf Hypergraph Convolutional Network (for non-linear Laplacian). Their experiments confirm the advantages of using sheaf Laplacians for node classification task on hypergraphs.

The idea of using $O(n)$-connection sheaves on graphs, a natural discrete abstraction for a tangent bundle of a manifold, see subsection~\ref{subsecReviewManifoldLevel}), to define an analogue of convolution on arbitrary manifolds was proposed by Battiloro et al.~\cite{battiloro2024tangent}. Collecting these convolutions into layers combined with pointwise non-linearities gives rise to a continuous Tangent Bundle Neural Networks (TNN) architecture. Following a point cloud discretization procedure similar to that in Singer and Wu~\cite{singer2012vector}, the authors prove that the space-discretized architecture over the cellular sheaf converges to the underlying TNN as the number of sample points increases. Thus, this work reinforces the fact that the sheaf neural network discretizations applied in Barbero et al.~\cite{barbero2022sheaf2} and Bamberger et al.~\cite{bamberger2024bundle} are asymptotically appropriate. The authors also apply their discretized TNN architecture to a number of unique datasets generated by interesting manifold structures like torus denoising, wind field reconstruction and forecasting, and manifold classification. The discretized TNN architecture generally outperforms alternative manifold processing architectures.

Gillespie et al.~\cite{gillespie2024bayesian} also note that learning an informative restriction map structure for a task can risk overparametrization and overspecification to a particular learned sheaf structure during training, especially in scenarios when training data is limited. The authors address this problem by treating the sheaf Laplacian as a latent variable within the sheaf neural network architecture. This distributional perspective leads to the definition of a Bayesian Sheaf Neural Network which models the conditional probability of a classification problem into classes $y$ as $p_{\theta}(y \mid X)$ where $X$ are the input node features and $\theta$ are the parameters of the neural network. In this architecture, the input graph and features are passed into a variational sheaf learning process which approximates the posterior $p_{\theta}(\Fc \mid X, y)$. This results in a distribution over restriction maps that are assumed to be structured as either general linear, special orthogonal, or invertible diagonal transformations. This distribution is then sampled to create the restriction maps for each message passing layer of the underlying sheaf neural network architecture. This sampling procedure forces the parameters of the sheaf neural network to tune to classes of restriction maps instead of particular restriction map observations. This Bayesian approach appears to outperform deterministic learned restriction map structures on small datasets.

A plethora of other alterations to the sheaf neural network architecture have also been proposed within the literature. For example, Zaghen~\cite{zaghen2024nonlinear} inserts an edge-wise nonlinear function $\Phi\colon C^1(S,D)\to C^1(S;D)$ in the definition of the sheaf Laplacian to produce a nonlinear sheaf Laplacian operator $\Delta' = d^*\circ\Phi\circ d$, as in Hansen and Ghrist~\cite{hansen2021opinion}, and introduces the corresponding sheaf neural network architecture. %!!Didn't have time to look through this paper carefully, but it may be useful in the future.
Similarly, Caralt et al.~\cite{caralt2024joint} consider a number of restriction map update operations for sheaf neural networks inspired by opinion dynamics models~\cite{hansen2021opinion}. He et al.~\cite{he2023sheaf} integrate positional encoding within the sheaf neural network architecture to improve distinctiveness during the diffusion operations between neighbors in the network with equivalent restriction maps. Work~\cite{di2025learning} presents a new approach to learning sheaf Laplacians. The authors develop a method to jointly infer both the graph topology and the restriction maps of a sheaf, optimizing them to minimize the total variation of observed data. The main idea of the paper is that distance between signals on different nodes depends only on their cross-correlation and subspace dimensions, and does not depend on the specific structure of local dictionaries. Their approach is more computationally efficient than previous methods based on semidefinite programming, since it uses closed-form solutions for the basic optimization steps. The experiments show that their sheaf-based method achieves better data smoothness compared to conventional approaches.

\subsection{Applications of sheaf-type neural networks}

Recently, the field of application of sheaf theory in combination with deep learning methods in real-world applications has been actively developing. Various features of the subject area and gaps of existing solutions motivate the use of the corresponding advantages and strengths of sheaves.

Atri et al.~\cite{atri2023promoting} solves the problem of multi-document summarization, i.e. aggregation of information from several documents into a short summary. The FABRIC model is proposed, which combines simplicial complex layers to capture non-binary relations, BART as a text encoder and Sheaf Graph Attention to model complex semantic relations. The authors emphasize that for summarization, many documents are encoded by heterophilic graphs; in such conditions, vanilla GNNs have difficulties, but Sheaf Graph Attention is very effective. This ensures that local document relationships will be effectively represented in the global summary, addressing pressing linguistic consistency issues common in existing document summarization systems. FABRIC achieves the best results on the Multinews and CQASumm datasets and shows the usefulness of sheaf theory in handling complex inter-document relationships.

The Sheaf4Rec framework proposed by Purificato et al.~\cite{purificato2023sheaf4rec} applies Sheaf Neural Networks and their corresponding Laplacians to recommended systems. It provides a more efficient representation of complex user-item interactions using Cellular Sheaf than classical GNNs. Sheaf4Rec shows notable performance improvements on the Yahoo and MovieLens datasets.

In networks where nodes model people and edges correspond to relations, the community detection problem reduces to identifying cohesive groups of similar nodes. Wolf et al.~\cite{wolf2023topological} proposes a sheaf-theoretic approach to this problem. Empirical evaluations on the Karate Club benchmark show the robustness of the approach and high modularity scores.

Several papers explore the application of sheaf theory to Personalized Federated Learning (PFL), the training paradigm that delivers unique parameters to clients in a federation based on their local data distribution, enabling parameter sharing across clients. Nguyen et al.~\cite{nguyen2024sheaf} presents the Sheaf Hyper Networks (SHN) approach, which solves the over-smoothing and heterophily problems by integrating Sheaf structure in classical Graph Hyper Networks (GHN) architecture. SHN outperforms baseline methods accuracy by 2.7\%  in non-IID settings. Meanwhile, Liang et al \cite{liang2024fedsheafhn} propose for the Personalized subgraph Federated Learning task the approach FedSheafHN, which constructs a server-side collaboration graph where client embeddings are enriched using Neural Sheaf Diffusion integrated with an attention-based hypernetwork. FedSheafHN achieves high performance, demonstrating fast convergence and effective new clients generalization. Work \cite{issaid2025tackling} propose to use sheaves on graphs for federated multi-task learning (FMTL). FMTL aims to train separate but related models simultaneously across multiple clients (nodes), each potentially focusing on different but related tasks. The clients do not share raw data and can have different model sizes. Sheaves allow to address feature heterogeneity between clients, which is one of the main obstacles for previously used FMTL algorithms.

Huntsman et al.~\cite{huntsman2024prospects} applies sheaf theory in combination with large language models (LLM) to develop an approach for solving the problem of logical consistency assessment in hypertexts in the domains of jurisprudence documents and social networks. Note that it does not use Sheaf Graph Network, but uses the classical formalism of sheaf theory to ``glue'' local consistency judgments assessed by LLMs into a global consistency structure. This application demonstrates the power of sheaves in formalizing local-to-global reasoning problems.

\subsection{Representation Learning and Interpretability}

Sheaf theory lies at the boundary of geometry, algebra, category theory, and topology. Because it is situated on the border of computational tractability and abstract mathematics, sheaf theory has also been used as a lens through which to elicit better understanding for the performance and behavior of many modern deep learning pipelines.

\emph{Representation learning} is a common motif within machine learning which seeks to translate data in a raw format into some information dense latent representation that is better suited for solving down-stream tasks. Representation learning over graphs or network-like structures often requires the translation of elements of the graph, the nodes and edges, into vectorized representations which preserve information about the original graph topology while admitting a more computationally tractable form. This graph representation learning procedure is typically facilitated by the introduction of a self-supervised learning task wherein node representations are used to predict other nodes within their surrounding neighborhoods or along paths passing through that node. While such an approach is sufficient for simple, homogeneous graphs, this homophily-biased approach is ill-suited for heterogeneous graph structures like knowledge graphs wherein disparate nodes may be connected across typed relations whose type conceptually bridges this disparity~\cite{gebhart2023sheaf}.

\emph{Knowledge graph embedding} is a graph-based self-supervised learning approach which seeks to account for this discrepancy between topology and representation by learning node representations which are transformed across learned edge representations. These functional representations which connect adjacent nodes permit the graph embedding to be locally non-constant, allowing heterogeneity to be captured by proper transformations in the embedding space as one moves along the underlying graph's topology. Gebhart et al.~\cite{gebhart2023knowledge} showed that cellular sheaf theory provides a framework within which many pre-existing knowledge graph embedding approaches may be viewed as sheaf learning problems for a particular specification of an underlying sheaf. In other words, the authors show that many popular transductive knowledge graph embedding techniques may be reinterpreted as learning the restriction maps of a sheaf (the edge, or relation, representations) in conjunction with a particular cochain (the node, or entity, representations). In addition, Gebhart et al. show that many of the loss functions employed by knowledge graph embedding techniques may be interpreted as minimizing the inconsistency of the sheaf and cochain observation via implicit minimization using the sheaf Laplacian. Gebhart and Cobb~\cite{gebhart2023extending} later extended this approach to inductive learning, showing that optimal representations for new nodes included in the knowledge graph after the original self-supervised embedding procedure may be inferred by phrasing a harmonic extension problem using the sheaf Laplacian. This harmonic extension treats pre-existing nodes as the fixed, boundary values and the newly-introduced nodes as the interior for which a cochain representation must be inferred which is as close to a global section as possible with respect to the previously-learned relation representations connecting the boundary and the interior nodes. The authors show that this process can be solved exactly via (pseudo)inversion of the sheaf Laplacian, or approximated through iterative methods, again involving the sheaf Laplacian.

Kvinge et al.~\cite{kvinge2021sheaves} propose a sheaf-theoretical approach to understanding machine learning model fit. To do this, they define sheaves on the data by constructing a topology with open sets corresponding to subsets of related points. They build a data sheaf $\ca{D}$ consisting of all possible data value observations and a model presheaf $\ca{M}$ consisting of a specified family of functions, --- those derivable from the model, --- associated with each open set. The authors define a map $\Phi^{\ca{M}}_U: \ca{D}(U) \to \ca{M}(U)$ for each open set $U$ corresponding to a model for a data observation associated with $U$. Measuring the extent to which $\Phi^{\ca{M}}$ fails to satisfy the axioms of a presheaf morphism, in other words, measuring the inconsistency of the data-model association, allows the authors to point to specific subpopulations of a dataset on which a given models ability to fit the data changes, providing a local measure of model behavior and performance which can further inform global performance measures like accuracy.

\subsection{Beyond geometry and ML}\label{subsecBeyondML}

A number of applications of sheaf theory can be found in the literature, which do not fall directly into the categories listed above. The study of contextuality in quantum systems by sheaf-theoretical methods was initiated by Abramsky~\cite{AbramskyContextuality, ContextParadox}. The methods of quantitative analysis of contextuality based on sheaf theory fused into the study of lexical ambiguities in a natural language, in particular in anaphora resolution~\cite{SemanticUnification, SheafLanguageAmbig}.

Robinson~\cite{Robinson2016SheafAC} proposed a homological approach to the analysis of graphical causal models based on sheaf cohomology (on general posets), as defined in subsection~\ref{subsecMathCohomologySimplicial}). The approach is based on the observation that compositionality equations~\eqref{eqCompositionalityMainPart} remind chain rule of probability. The solutions to a graphical model can be treated as global sections of certain sheaf associated with the model. Belief propagation, a particular instance of message passing known in graphical models, is described in the terminology of pushforward sheaves; it happens that reliability of belief propagation can be measured in homological terms. Rosiak~\cite[Ch.9]{Rosiak} describes a related idea; a bayesian network gives rise to a pair (sheaf, cosheaf) on a simplex, and the assignment of joint probabilities to the nodes is treated as a simultaneous global section of both a sheaf and a cosheaf.

A research on using sheaves in networks' science initiated by Robinson, Ghrist, and Hansen had raised interest in engineering applications~\cite{SheafNetworkingNASA}. Macfarlane~\cite{Macfarlane04072014,Macfarlane17022017} introduced the category of ``combinatorial systems'' as an elegant mathematical model for scheduling and resource management in supply and production networks; the notion is potentially applicable to describe more general complex systems, as long as their formal ontology contains quantifiable inputs, outputs and production time. Sheaves over combinatorial systems describe collections of schedules; while higher order \v{C}ech cohomology modules measure discrepancy between local and global schedules.

%Mansourbeigi applied the language of sheaf theory on simplicial complexes to model sensor networks for air traffic~\cite{SheafAirTraffic} and wild fires monitoring~\cite{SheafFires}. The work of Murimi~\cite{Murimi2021OnTD} proposes to combine sheaf theory with ideas used in blockchain: the restriction maps of a sheaf are used to model hashing, while the notion of a stalk resembles the consensual state of a blockchain. The formalism to describe networks of interacting robots is proposed.

The idea of sheaf theory as a language describing how local entities patch into global had found application in modern cognitive science. Philips~\cite{Phillips2018GoingBT} uses sheaves to describe generalization ability: the ability to patch knowledge from a finite set of observations. These ideas could be of interest to specialists in ML and general AI. In deep learning, a different, more statistical, toolbox is applied to measure this notion. Basically, generalization ability of a model is its ability to guess the outputs for inputs which were not used during the training. Such vague definition of generalization ability is independent on a theory describing knowledge representation inside a model\footnote{This reminds of IQ-tests: they neither measure intelligence, nor explain anything about intelligence theoretically. They measure the ability to solve IQ-tests; no more and no less.}. Sheaf theory provides more intrinsic and logical view on generalization ability, which may potentially find its place in theoretical ML and AI alongside with Vapnik--Chervonenkis theory, singular learning theory, and other elegant formalisms.



\section{Proposals and problems}\label{secProblems}

In this section we gather a number of proposals and problems, which in our opinion, may stimulate further work in the area of applied sheaf theory. The solutions may result in both the improvement of practical algorithms, and the further development of theory. It should be noted, however, that none of below problems are problems (or conjectures) in a strict mathematical sense, and their formulations may be imprecise. However, these problems can be placed on the same scale introduced in Section~\ref{secReview}: we believe that these problems may be of certain interest to specialists at each of the corresponding levels.

%\setcounter{subsection}{-1}

\subsection{Categorical level}\label{subsecProblCategoryLevel}

By geometrical structures we mean either of the structures mentioned in Section~\ref{secIntuitions}: undirected graphs, simplicial complexes, CW-complexes, posets or topologies.

\begin{probl}\label{problDescribeGeometry}
For a given finite geometrical structure $X$ provide a general mathematical basis for sheaf learning.
\end{probl}

Informally, sheaf learning, as understood in sheaf neural networks, is a procedure of picking a finite number of sheaves from $\Shvs(S;\Ro\Vect)$ in a way that sufficiently solves some task (whatever it is). A straightforward formalization of this procedure means that $\Shvs(S;\Ro\Vect)$ is treated as a smooth manifold, and gradient optimization is performed on a single component of this manifold (the one with prescribed dimensions of stalks) to optimize a given loss function. Sheaves learned this way stay unrelated to each other; the procedure simply provides a sequence of sheaf diffusion operators stacked together in a network.

In other words, the existing learning procedure is totally agnostic of the fact that the collection $\Shvs(S;\Ro\Vect)$ has a rich structure: it is an abelian category. To our knowledge, the whole area of ``learning over a category'', --- when we want not only to learn some objects, but also learn morphisms between them, --- does not yet exist. One could extend this to infusing the specific structure of the abelian category directly into the learning paradigm. 


\subsection{Topology level}\label{subsecProblTopologyLevel}

The dimension of a (geometric realization of a) poset $S$ equals the largest length $n$ of a chain $s_0<s_1<\cdots<s_n$ in $S$. Henceforth, higher-dimensional topological notions come into play when the underlying structure $S$ is not just a binary relation (as in Examples~\ref{exGraphToPoset} and~\ref{exBinRelToPoset}). The problem here is that the mechanics of sheaf learning described in subsection~\ref{subsecSheafLearning}, while being clear and natural, is only applied in practice to 1-dimensional structures. It may seem at a first glance that high-dimensional structures such as simplicial or cellular complexes are used in topological deep learning~\cite{TDLbeyond}. However, unwinding the constructions, we see that each such structure $S$ is first disassembled into a number of 1-dimensional structures $S_1,\ldots,S_p$, and sheaf learning is performed independently on these structures, not on the original $S$.

The formal definition of a sheaf poses a conceptual challenge if one wants to learn a sheaf. Assume that the structure maps $D(s<t)$ of an $\Ro\Vect$-valued sheaf $D$ are represented by real matrices $D_{st}$ whose entries are to be learned by the gradient descent. By Definition~\ref{definSheafDiagramMainText}, a sheaf should satisfy compositionality~\eqref{eqCompositionalityMainPart} which is a system of nonlinear equations
\begin{equation}\label{eqCompositionalityEquations}
D_{s_1s_2}D_{s_0s_1}=D_{s_0s_2}\mbox{ for each }s_0<s_1<s_2 \mbox{ in }S
\end{equation}
on the matrix entries. How do we ensure that the learned matrices satisfy these equations? In other words, how do we know that a learned sheaf is indeed a sheaf? There are two standard answers, we consider both quite unsatisfactory:

\begin{enumerate}
  \item In practice, sheaf learning is applied to graphs or hypergraphs, which are 1-dimensional, so system~\eqref{eqCompositionalityEquations} is empty. Even if the data were originally represented by higher-order structure, the practical implementation forgets the higher order relations and treats data as a finite collection of binary relations on which sheaf diffusion or message passing is performed.
  \item System~\eqref{eqCompositionalityEquations} is ignored. In this case the learned collection of matrices is not a sheaf but rather a quiver representation. Again, this approach loses the information of commutativity relations which was present in the original diagram, and made all topology essential. The language of quiver representations contains neither global sections nor higher-order cohomology, so one should be careful in applying the terms which are not applicable.
\end{enumerate}

Related to this observation, we highlight the following proposal to take commutativity relations into account.

\begin{probl}\label{problRelations}
Approach the design of ML algorithms in the way that commutativity relations inherent to data are explicitly reflected in the architecture of a neural network.
\end{probl}

\begin{rem}
As a naive solution, the degree of non-commutativity 
\[
\sum_{s_0<s_1<s_2}\|D_{s_1s_2}D_{s_0s_1}-D_{s_0s_2}\|^2
\] 
can be used as an additional regularization term of the loss function when training a sheaf. The precise form of such regularization term may vary. For example, in the case of regular CW-complexes it could be of the form $\sum_{s_0<s_2}\|D_{s_1's_2}D_{s_0s_1'}-D_{s_1''s_2}D_{s_0s_1''}\|^2$, where $\dim s_2=\dim s_0+2$ and $s_1',s_1''$ are the two cells in between $s_0$ and $s_2$; this particular form doesn't use auxiliary ``skip connections'' $D(s_0<s_2)$.

From the mathematical perspective, the construction of such regularization terms is the subject of quiver representations with relations: the set of basic relations may be given in the problem's formal ontology together with the quiver itself. This leads to the consideration of 2-categories instead of 1-categories, so if things are taken seriously, a 2-categorical version of message passing should be developed. There may be even more general relations between relations, and so on, encoded in a structure called $n$-computad~\cite{Makkai} or a polygraph~\cite{BURRONI199343}, which suggests a nontrivial fusion of deep learning with higher categories.
\end{rem}


\subsection{Homology level}\label{subsecProblHomologyLevel}

While most of the theory described in Appendices~\ref{secMathCohomology} and~\ref{secMathLaplacians} is more or less folklore and well known to mathematicians, there is no good working implementation so far. Existing symbolic algebra packages such as Sage~\cite{Sage} and Macaulay2~\cite{Macaulay} do not support sheaf cohomology computations on arbitrary finite posets. Most existing homology-related packages are developed in the framework of topological data analysis, and support neither general geometrical structures, nor general sheaves, nor Laplacian-related numerical linear algebra.

\begin{probl}\label{problPackage}
Develop a computational system with the following functionality.
\begin{enumerate}
  \item Construction of diagrams over arbitrary finite posets (and maybe even finite categories) valued in common computationally feasible categories $\Vv$ such as $\FinSets$, $\ko\FinVect$, $\ko[t]\Mod$. 
  \item The ability to check commutativity of a diagram.
  \item The ability to find some/all global sections of a diagram in an optimal way.
  \item Over abelian categories $\Vv$, the ability to compute cohomology of sheaves and related notions of homological algebra including (minimal) injective resolutions, derived functors, etc.
  \item Over $\Ro\Vect$ the ability to form Laplace operators, compute their eigendecomposition, simulate heat diffusion, and construct sheaf diffusion operators.
\end{enumerate}
\end{probl}

Similar problems were stated in~\cite{Macfarlane04072014,Macfarlane17022017} motivated by the analysis and scheduling of production networks. The problem is motivated not only by practical considerations reviewed in Section~\ref{secReview} but also by the needs originating in theoretical mathematics\footnote{The first author faced the need to compute cohomology of GKM-sheaves in~\cite{AyzSor})}.

\begin{rem}
Although Problem~\ref{problPackage} seems to be pure engineering, we believe that its solution will stimulate further study in a broad area of hardware optimization for scientific computing. If the system is designed to work with arbitrary categories by design, then the required optimization patterns will not be specific to $\Ro\FinVect$ (as in applied linear algebra), or $\FinSets$ (as in SAT- and CSP-solvers), but will have more universal categorical nature.
\end{rem}

\subsection{Real level}\label{subsecProblRealLevel}

In modern ML, there exists a linear representation hypothesis~\cite{park2023LinRepr} which claims that even in nonlinear language models, the high-level semantical concepts are represented by directions or linear functionals in the representation space. The number of meaningful concepts may be bigger than the dimension of the representation space, so that a space $\Ro^l$ is capable of storing more features than the dimensionality $l$. This is confirmed on toy examples~\cite{ToySuperposition} and by the general success of vector embeddings~\cite{Mikolov2013EfficientEO} where the number of words in the vocabulary is usually much bigger than dimensionality of the embedding space. From a topological perspective, the phenomenon is also not surprising: a simplicial complex $\ca{K}$ on $m$ vertices can be embedded in the space $\Ro^m$ (Construction~\ref{conStandardGeomRealization}), but it can also be embedded in the euclidean space of dimension $2\dim\ca{K}+1$ or even less~\cite{Horvatic}. The embedding dimensionality depends on the maximal size of relations between nodes, not their total number.

In Section~\ref{secIntuitions} we discussed that there is nothing magical in the process of heat diffusion on a sheaf $D$ on a cell poset $\ca{X}$: it is just the optimization of a nonnegative quadratic function on the real space $C^0_{CW}(\ca{X};D)$ of cellular cochains (Remark~\ref{remNNdynamical}). What makes this procedure efficient is that $C^0_{CW}(\ca{X};D)$ has a rich semantic structure by design; it comes from its decomposition into the direct sum $\bigoplus_{v\colon V}D(v)$ (see Remark~\ref{remNNdynamical}). Looking at a vector from $C^0_{CW}(\ca{X};D)$, we know which part of it stores the state of each particular node. If the stalks $D(s)$, for $s\in S$, have large dimension, the state vector may be more informative, but the cost of the model becoming computationally inefficient. However, if we don't need precise higher cohomological information about a structure, and only need approximate features of heat diffusion, what if we squeeze the semantic information into a lower-dimensional space instead of keeping stalks orthogonal in the direct sum $\bigoplus_{v\colon V}D(v)$? This question suggests the following framework.

\begin{defin}\label{definRepresentationEvolution}
Let $\ca{X}$ be a cell poset. A working representation $W$ for $\ca{X}$ is a euclidean space $W_{tot}\cong\Ro^d$, and a choice, for each cell $\sigma\in\ca{X}$, of a euclidean subspace $W(\sigma)\subset W_{tot}$ together with the orthogonal projection $p_\sigma\colon W_{tot}\to W(\sigma)$. An evolution process $\Theta$ is a dynamical system on the space $W_{tot}$. We call the pair $(W,\Theta)$ a \emph{condensed sheaf} on $\ca{X}$.
\end{defin}

A similar definition may be given not only for cell posets, but for any meaningful geometrical structure. If a condensed sheaf is chosen well-aligned with the structure of $\ca{X}$, then performing a flow $\Theta$, then projecting to the semantical subspace $W(\sigma)$, we would get the evolution of the state of $\sigma$ which may be a valuable source of learning signal.

\begin{ex}
For a cellular sheaf $D$ on $\ca{X}$, define a working representation $W^{CW}_{tot}=\bigoplus_jC^j(\ca{X};D)$, with $W(\sigma)$ being the corresponding direct summand, and the evolution $\Theta^{CW}$ being the heat diffusion in all degrees at once. This working representation and evolution had proven useful in practice, as confirmed by the papers listed in Section~\ref{secReviewShvsML}.
\end{ex}

In general, there exist subspace arrangements $\{W(\sigma)\subset W_{tot}\mid \sigma\in\ca{X}\}$ which exhibit superposition (non-transversality) similar to the described in~\cite{ToySuperposition}. However, so far there is no working example demonstrating practical viability of condensed sheaves.

\begin{probl}\label{problCondensedSheaves}
Construct a condensed sheaf which has better expressivity compared to cellular sheaves, and the dimension $d=\dim W_{tot}$ of the working representation smaller than the size of $\ca{X}$. Devise methods to construct/learn such condensed sheaves where both operations $\Theta$ and $p_\sigma$ are computationally simple compared to sheaf-type neural networks.
\end{probl}

This problem stands in line with the whole general idea of machine learning as it is understood nowadays: ``the space $\Ro^d$ is capacious enough to store data, gradient descent is enough to process the data''. It may be a good idea to bring this philosophy to geometry.

\subsection{Manifold level}\label{subsecProblManifoldLevel}

The manifold hypothesis is well-known in modern machine learning~\cite{ManHypo} and statistics, although it implicitly originated in 1970's as a part of applied catastrophe theory (and already that time criticized, see~\cite[Sec.``Spurious quantification'']{ApplCatastrophIsShit}).

A data set is a finite subset $X$ of $\Ro^n$. As is common in statistical learning, the data points are assumed to be sampled i.i.d. from some unknown distribution on $\Ro^n$. The manifold hypothesis asserts there exists a submanifold $M$ of $\Ro^n$, with $m=\dim M<n$ such that the distribution is continuously supported on $M$. Such manifold $M$ is called a data manifold, or a latent manifold. The task then becomes to describe $M$, or at least study its mathematical properties, given a finite set $X$ of samples. This research area is called \emph{manifold learning}. It should be noticed however that in machine learning, unlike mathematics, there is no generally accepted definition of a manifold. Some papers treat $M$ as a solution to a system of equations; others consider $M$ to be an immersion of a single chart $\Ro^m$ into $\Ro^n$; in topological data analysis manifolds are replaced by triangulations and filtrations; in statistics probability distributions are the first-class citizens. Usually the definition of a manifold is not even given and in practice one is interested in some vague characteristics of $M$ as there is no practical need to define the space rigorously.

We believe that the problem of manifold learning should be taken more seriously from geometrical perspective. Recall that the classical definition of a (smooth closed) manifold $M$ involves an atlas of open charts $\{U_\alpha\subset\Ro^m\}$, a collection of open subspaces $V_{\alpha\beta}\subseteq U_{\alpha}$, and a collection of smooth invertible gluing maps $g_{\alpha\beta}\colon V_{\alpha\beta}\to V_{\beta\alpha}$ satisfying the cocycle equations $g_{\alpha\beta}=g_{\beta\alpha}^{-1}$ and $g_{\alpha\beta};g_{\beta\gamma};g_{\gamma\alpha}=1$ whenever they make sense. A smooth map (in particular, an embedding) $f\colon M\to \Ro^n$ is encoded by a collection of smooth maps $f_\alpha\colon U_\alpha\to \Ro^n$ which are compatible with respect to gluing maps $f_\alpha=g_{\alpha\beta};f_\beta$.

\begin{probl}\label{problManifoldHypothesis}
Given a finite set of points $X\subset \Ro^n$ and $m<n$, find a manifold $M$, its embedding $f\colon M\to\Ro^n$, and a finite subset $L\subset M$, such that $f(L)=X$ and the data $(M,f)$ have the lowest possible complexity.
\end{probl}

By complexity we mean the complexity of the total description in terms of an atlas and the gluing maps, this is similar to the classical regularization in machine learning. The main difficulty in learning a manifold, as stated in Problem~\ref{problManifoldHypothesis}, is to enforce the cocycle equations, at least approximately, within the output structure. This is similar in nature to Problem~\ref{problRelations}: in order to actually learn a sheaf on a high-dimensional structure one needs to attend to compositionality relations. 

The main tool of smooth geometry is the tangent bundle of a smooth manifold. We expect solution to Problem~\ref{problManifoldHypothesis} should be related to the method of Singer and Wu~\cite{singer2012vector} outlined in subsection~\ref{subsecReviewManifoldLevel}. It will likely make use of connection sheaves, since the latter are an expressive mathematical abstraction for vector bundles.

\begin{rem}\label{remNeuroSymbPlayground}
Problem~\ref{problManifoldHypothesis} does not seem to be solvable in higher dimensions. First of all, manifold learning is usually considered a method of dimensionality reduction, whereas the internal dimension $m$ in Problem~\ref{problManifoldHypothesis} is already given. Second, the curse of dimensionality applies here as well. In high dimensions, even large datasets are too sparse for the problem of manifold reconstruction to make practical sense. Nevertheless, the potential solution of Problem~\ref{problManifoldHypothesis}, in the way it is stated, seems valuable even in low dimensions. The problem natively suggests the fusion of neural and discrete methods. While the cocycle relations can be solved by running gradient descent, the topological structure of an atlas requires discrete optimization methods such as genetical algorithms, and their topological manifestations such as Morse surgery. For this reason we consider Problem~\ref{problManifoldHypothesis} a perfect playground to test neuro-symbolic approaches in machine learning.
\end{rem}
%
%\begin{rem}\label{remBioChemGraphics}
%In small dimensions, Problem~\ref{problManifoldHypothesis} reduces to classical problems of computational geometry. If $(n,m)=(3,1)$, the problem asks to reconstruct the simplest curve through a given set of points, which is a natural question in computational biochemistry. If $(n,m)=(3,2)$, the problem asks to reconstruct a simplest surface through a given set of points, which is natural for computer vision and 3D-graphics. Certainly, there exist beautiful partial solutions to such problems based on connection sheaves, as described in subsection~\ref{subsecReviewManifoldLevel}.
%\end{rem}


\subsection{Data level}\label{subsecProblDataLevel} 
In this subsection we pose two problems, both of which having relatively clear specification.

\textbf{Improvement of algorithms.} As discussed in the introduction and explained in Section~\ref{secReview}, sheaf theory provides an extremely powerful toolbox for the representation of geometrical data in the way that captures geometrical, topological, and human intuitions. Sheaves offer an efficient geometrical data representation for feature extraction. However, most of such applications improve high-level characteristics of neural networks (such as accuracy, precision, generalization ability, etc.) which depend on a large number of factors, many of which have little obvious relationship to the topological properties of the underlying space. It is difficult to quantify the gain of using sheaves in each particular case and, given the computational overhead of many sheaf-related methods, as described in subsection~\ref{subsecFurtherArchitectures}, it is not always clear that the abstraction is worth these costs. 

In the literature, we couldn't find any example, when a computationally heavy precise algorithm achieves higher speed by utilizing concepts of sheaf theory. By heavy precise algorithms we mean empirical algorithms solving NP-hard problems, such as boolean satisfiability problem (SAT), constraint satisfaction problem (CSP), subgraph matching problem (SGM), etc. In all these problems there is no accuracy/precision, and the only characteristic needed to be optimized is the time of performance on a given class of examples. There exist extremely efficient solvers that can empirically solve such problems despite their complexity class. None explicitly utilize sheaves in their internal mechanisms.

\begin{probl}
Find an NP-hard problem for which an application of concepts from sheaf theory beats state-of-the-art solutions in terms of total computation time.
\end{probl}

We believe that solution of this problem will not only be of practical value, but also stimulate new research on interconnections of sheaf and category theory with computer science and complexity theory.

\textbf{Large graphs.} The existing research focuses primarily on the application of sheaf-type neural networks to relatively small graphs. At the same time, many real-world datasets come in the form of large graphs: knowledge graphs, social networks, Wikipedia links' structures, and other large-scale information networks. Applicability of sheaf-type neural networks to problems posed on such graphs remains an open challenge. Understanding the effectiveness and limitations of sheaf-type neural networks in these contexts is crucial for advancing its practical utility.

\begin{probl}
Extend the architecture of sheaf-type neural networks to large graphs, i.e. graphs with more than million nodes.
\end{probl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%       HELL YEAH, FINALLY MATH!!!
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage

\appendix

\input{MathAppendix.tex}

\nocite{*}
\printbibliography

\end{document}
