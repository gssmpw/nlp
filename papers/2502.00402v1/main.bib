@inproceedings{zimmer2024tumtraf,
  title={Tumtraf v2x cooperative perception dataset},
  author={Zimmer, Walter and Wardana, Gerhard Arya and Sritharan, Suren and Zhou, Xingcheng and Song, Rui and Knoll, Alois C},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22668--22677},
  year={2024}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@inproceedings{ghita2024activeanno3d,
  title={ActiveAnno3D-An Active Learning Framework for Multi-Modal 3D Object Detection},
  author={Ghita, Ahmed and Antoniussen, Bj{\o}rk and Zimmer, Walter and Greer, Ross and Cre{\ss}, Christian and M{\o}gelmose, Andreas and Trivedi, Mohan and Knoll, Alois C},
  booktitle={35th IEEE Intelligent Vehicles Symposium (IV) 2024},
  year={2024}
}

@inproceedings{greer2023pedestrian,
  title={Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and Real-World Data Analysis},
  author={Greer, Ross and Desai, Samveed and Rakla, Lulua and Gopalkrishnan, Akshay and Alofi, Afnan and Trivedi, Mohan},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)},
  pages={1--8},
  year={2023},
  organization={IEEE}
}
@article{kulkarni2021create,
  title={Create a large-scale video driving dataset with detailed attributes using Amazon SageMaker Ground Truth},
  author={Kulkarni, N and Rangesh, A and Buck, J and Feltracco, J and Trivedi, M and Deo, N and Greer, R and Sarraf, S and Sathyanarayana, S},
  year={2021}
}
@article{abualsaud2021laneaf,
  title={Laneaf: Robust multi-lane detection with affinity fields},
  author={Abualsaud, Hala and Liu, Sean and Lu, David B and Situ, Kenny and Rangesh, Akshay and Trivedi, Mohan M},
  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={4},
  pages={7477--7484},
  year={2021},
  publisher={IEEE}
}
@inproceedings{philipsen2015traffic,
  title={Traffic light detection: A learning algorithm and evaluations on challenging dataset},
  author={Philipsen, Mark Philip and Jensen, Morten Born{\o} and M{\o}gelmose, Andreas and Moeslund, Thomas B and Trivedi, Mohan M},
  booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems},
  pages={2341--2345},
  year={2015},
  organization={IEEE}
}
@inproceedings{christianos2023planning,
  title={Planning with occluded traffic agents using bi-level variational occlusion models},
  author={Christianos, Filippos and Karkus, Peter and Ivanovic, Boris and Albrecht, Stefano V and Pavone, Marco},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5558--5565},
  year={2023},
  organization={IEEE}
}
@book{fingscheidt2022deep,
  title={Deep neural networks and data for automated driving: Robustness, uncertainty quantification, and insights towards safety},
  author={Fingscheidt, Tim and Gottschalk, Hanno and Houben, Sebastian},
  year={2022},
  publisher={Springer Nature}
}
@inproceedings{greer2023safe,
  title={Safe Control Transitions: Machine Vision Based Observable Readiness Index and Data-Driven Takeover Time Prediction},
  author={Greer, Ross and Deo, Nachiket and Rangesh, Akshay and Trivedi, Mohan and Gunaratne, Pujitha},
  booktitle={27th International Technical Conference on the Enhanced Safety of Vehicles (ESV) National Highway Traffic Safety Administration},
  number={23-0331},
  year={2023}
}
@article{greer2024patterns,
  title={Patterns of vehicle lights: Addressing complexities of camera-based vehicle light datasets and metrics},
  author={Greer, Ross and Gopalkrishnan, Akshay and Keskar, Maitrayee and Trivedi, Mohan M},
  journal={Pattern Recognition Letters},
  volume={178},
  pages={209--215},
  year={2024},
  publisher={Elsevier}
}

@article{cress2024tumtrafevent,
              title={TUMTraf Event: Calibration and Fusion Resulting in a Dataset for Roadside Event-Based and RGB Cameras},
              author={Cre{\ss}, Christian and Zimmer, Walter and Purschke, Nils and Doan, Bach Ngoc and Kirchner, Sven 
                     and Lakshminarasimhan, Venkatnarayanan and Strand, Leah and Knoll, Alois C},
              journal={IEEE Transactions on Intelligent Vehicles},
              doi={10.1109/TIV.2024.3393749},
              year={2024}
          }

@inproceedings{kremer2023digital,
  title={A digital twin for teleoperation of vehicles in urban environments},
  author={Kremer, Philipp and Nourani-Vatani, Navid and Park, Sangyoung},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={12521--12527},
  year={2023},
  organization={IEEE}
}
@article{greer2024and,
  title={The Why, When, and How to Use Active Learning in Large-Data-Driven 3D Object Detection for Safe Autonomous Driving: An Empirical Exploration},
  author={Greer, Ross and Antoniussen, Bj{\o}rk and Andersen, Mathias V and M{\o}gelmose, Andreas and Trivedi, Mohan M},
  journal={arXiv preprint arXiv:2401.16634},
  year={2024}
}
@inproceedings{wang2022ips300+,
  title={Ips300+: a challenging multi-modal data sets for intersection perception system},
  author={Wang, Huanan and Zhang, Xinyu and Li, Zhiwei and Li, Jun and Wang, Kun and Lei, Zhu and Haibing, Ren},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)},
  pages={2539--2545},
  year={2022},
  organization={IEEE}
}

@inproceedings{kataoka2018drive,
  title={Drive video analysis for the detection of traffic near-miss incidents},
  author={Kataoka, Hirokatsu and Suzuki, Teppei and Oikawa, Shoko and Matsui, Yasuhiro and Satoh, Yutaka},
  booktitle={2018 IEEE International Conference on robotics and automation (ICRA)},
  pages={3421--3428},
  year={2018},
  organization={IEEE}
}

 @inproceedings{cress2022tumtrafa9,
              title={A9-dataset: Multi-sensor infrastructure-based dataset for mobility research},
              author={Cre{\ss}, Christian and Zimmer, Walter and Strand, Leah and Fortkord, Maximilian 
                      and Dai, Siyi and Lakshminarasimhan, Venkatnarayanan and Knoll, Alois},
              booktitle={2022 IEEE Intelligent Vehicles Symposium (IV)},
              pages={965--970},
              year={2022},
              organization={IEEE}
        }

@article{minderer2024scaling,
  title={Scaling open-vocabulary object detection},
  author={Minderer, Matthias and Gritsenko, Alexey and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{hekimoglu2023multi,
  title={Multi-task consistency for active learning},
  author={Hekimoglu, Aral and Friedrich, Philipp and Zimmer, Walter and Schmidt, Michael and Marcos-Ramiro, Alvaro and Knoll, Alois},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3415--3424},
  year={2023}
}
@article{greer2024language,
  title={Language-Driven Active Learning for Diverse Open-Set 3D Object Detection},
  author={Greer, Ross and Antoniussen, Bj{\o}rk and M{\o}gelmose, Andreas and Trivedi, Mohan},
  journal={CVPR Workshop on Vision-Language for Autonomous Driving and Robotics},
  year={2024}
}

@article{tesla,
  title={FSD Beta v11: A look at Tesla's Voice Drive Notes},
  author={Cihak, Lennon},
  journal={https://www.notateslaapp.com/tesla-reference/1228/fsd-beta-v11-a-look-at-tesla-s-voice-drive-notes},
  year={2023}
}


@inproceedings{zimmer2023tumtraf,
  title={Tumtraf intersection dataset: All you need for urban 3d camera-lidar roadside perception},
  author={Zimmer, Walter and Cre{\ss}, Christian and Nguyen, Huu Tung and Knoll, Alois C},
  booktitle={2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)},
  pages={1030--1037},
  year={2023},
  organization={IEEE}
}

@inproceedings{meyer2019importance,
  title={The importance of metric learning for robotic vision: Open set recognition and active learning},
  author={Meyer, Benjamin J and Drummond, Tom},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={2924--2931},
  year={2019},
  organization={IEEE}
}
@article{greer2024towards,
  title={Towards explainable, safe autonomous driving with language embeddings for novelty identification and active learning: Framework and experimental analysis with real-world data sets},
  author={Greer, Ross and Trivedi, Mohan},
  journal={arXiv preprint arXiv:2402.07320},
  year={2024}
}


@inproceedings{howard2021don,
  title={Don’t blindly trust your CNN: Towards competency-aware object detection by evaluating novelty in open-ended environments},
  author={Howard, Rhys and Barrett, Sam and Kunze, Lars},
  booktitle={2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={13286--13292},
  year={2021},
  organization={IEEE}
}

@inproceedings{salini2011synthesis,
  title={Synthesis of complex humanoid whole-body behavior: A focus on sequencing and tasks transitions},
  author={Salini, Joseph and Padois, Vincent and Bidaud, Philippe},
  booktitle={2011 IEEE international conference on robotics and automation},
  pages={1283--1290},
  year={2011},
  organization={IEEE}
}

@inproceedings{comparetti2014event,
  title={Event-based device-behavior switching in surgical human-robot interaction},
  author={Comparetti, Mirko Daniele and Beretta, Elisa and Kunze, Mirko and De Momi, Elena and Raczkowsky, J{\"o}rg and Ferrigno, Giancarlo},
  booktitle={2014 IEEE international conference on robotics and automation (ICRA)},
  pages={1877--1882},
  year={2014},
  organization={IEEE}
}

@article{greer2024perception,
  title={Perception Without Vision for Trajectory Prediction: Ego Vehicle Dynamics as Scene Representation for Efficient Active Learning in Autonomous Driving},
  author={Greer, Ross and Trivedi, Mohan},
  journal={arXiv preprint arXiv:2405.09049},
  year={2024}
}

@inproceedings{wu2023uncertainty,
  title={Uncertainty-Guided Active Reinforcement Learning with Bayesian Neural Networks},
  author={Wu, Xinyang and El-Shamouty, Mohamed and Nitsche, Christof and Huber, Marco F},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5751--5757},
  year={2023},
  organization={IEEE}
}
@inproceedings{rangesh2021autonomous,
  title={Autonomous vehicles that alert humans to take-over controls: Modeling with real-world data},
  author={Rangesh, Akshay and Deo, Nachiket and Greer, Ross and Gunaratne, Pujitha and Trivedi, Mohan M},
  booktitle={2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
  pages={231--236},
  year={2021},
  organization={IEEE}
}

@inproceedings{merlo2022dynamic,
  title={Dynamic human-robot role allocation based on human ergonomics risk prediction and robot actions adaptation},
  author={Merlo, Elena and Lamon, Edoardo and Fusaro, Fabio and Lorenzini, Marta and Carfi, Alessandro and Mastrogiovanni, Fulvio and Ajoudani, Arash},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)},
  pages={2825--2831},
  year={2022},
  organization={IEEE}
}

@inproceedings{rosch2022space,
  title={Space, time, and interaction: A taxonomy of corner cases in trajectory datasets for automated driving},
  author={R{\"o}sch, Kevin and Heidecker, Florian and Truetsch, Julian and Kowol, Kamil and Schicktanz, Clemens and Bieshaare, Maarten and Sick, Bernhard and Stiller, Christoph},
  booktitle={2022 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={86--93},
  year={2022},
  organization={IEEE}
}
@inproceedings{ross2015online,
  title={Online novelty-based visual obstacle detection for field robotics},
  author={Ross, Patrick and English, Andrew and Ball, David and Upcroft, Ben and Corke, Peter},
  booktitle={2015 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3935--3940},
  year={2015},
  organization={IEEE}
}

@inproceedings{wang2021joint,
  title={Joint object detection and multi-object tracking with graph neural networks},
  author={Wang, Yongxin and Kitani, Kris and Weng, Xinshuo},
  booktitle={2021 IEEE international conference on robotics and automation (ICRA)},
  pages={13708--13715},
  year={2021},
  organization={IEEE}
}
@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7623--7633},
  year={2023}
}
@inproceedings{yang2023vid2seq,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10714--10726},
  year={2023}
}

@inproceedings{wang2024deepaccident,
  title={Deepaccident: A motion and accident prediction benchmark for v2x autonomous driving},
  author={Wang, Tianqi and Kim, Sukmin and Wenxuan, Ji and Xie, Enze and Ge, Chongjian and Chen, Junsong and Li, Zhenguo and Luo, Ping},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={6},
  pages={5599--5606},
  year={2024}
}

@inproceedings{zimmer20193d,
  title={3d bat: A semi-automatic, web-based 3d annotation toolbox for full-surround, multi-modal data streams},
  author={Zimmer, Walter and Rangesh, Akshay and Trivedi, Mohan},
  booktitle={2019 IEEE Intelligent Vehicles Symposium (IV)},
  pages={1816--1821},
  year={2019},
  organization={IEEE}
}


@online{zimmer2024tumtrafdatasets,
	title = {TUM Traffic Datasets},
    author={Zimmer, Walter and Cre{\ss}, Christian and Zhou, Xingcheng and Strand, Leah and Lakshminarasimhan, Venkatnarayanan and Knoll, Alois},
	url = {https://innovation-mobility.com/tumtraf-dataset},
    note = {\url{https://innovation-mobility.com/tumtraf-dataset}},
	urldate = {2024-07-07},
	langid = {american},
}


@software{zimmer2024devkit,
	title = {{TUM} Traffic Dataset Development Kit},
	rights = {{MIT}},
	url = {https://github.com/tum-traffic-dataset/tum-traffic-dataset-dev-kit},
	abstract = {{TUM} Traffic Dataset Development Kit},
	publisher = {{TUM} Traffic Dataset},
	author={Zimmer, Walter and Cre{\ss}, Christian and Zhou, Xingcheng and Knoll, Alois},
	urldate = {2024-05-20},
	date = {2024-05-20},
	note = {\url{https://github.com/tum-traffic-dataset/tum-traffic-dataset-dev-kit}},
	keywords = {deep-learning, 3d-object-detection, 3d-vision, autonomous-driving, benchmark, camera, dataset, dev-kit, image, lidar, machine-learning, munich, perception, point-cloud, traffic, tum},
}

@article{zimmer2022survey,
  title={A survey of robust 3d object detection methods in point clouds},
  author={Zimmer, Walter and Ercelik, Emec and Zhou, Xingcheng and Ortiz, Xavier Jair Diaz and Knoll, Alois},
  journal={arXiv preprint arXiv:2204.00106},
  year={2022}
}

@article{zimmer2022realdomain,
  title={Real-time and robust 3D object detection within road-side LiDARs using domain adaptation},
  author={Zimmer, Walter and Grabler, Marcus and Knoll, Alois},
  journal={arXiv preprint arXiv:2204.00132},
  year={2022}
}

@inproceedings{zimmer2023infradet3d,
  title={InfraDet3D: Multi-Modal 3D Object Detection based on Roadside Infrastructure Camera and LiDAR Sensors},
  author={Zimmer, Walter and Birkner, Joseph and Brucker, Marcel and Nguyen, Huu Tung and Petrovski, Stefan and Wang, Bohan and Knoll, Alois C.},
  publisher = {IEEE},
  booktitle={2023 IEEE Intelligent Vehicles Symposium (IV)},
  year={2023}
}

@inproceedings{zimmer2023real,
  title={Real-time and robust 3d object detection with roadside lidars},
  author={Zimmer, Walter and Wu, Jialong and Zhou, Xingcheng and Knoll, Alois C},
  booktitle={Proc. of Int. Scientific Conf. on Mobility and Transport: Mobility Innovations for Growing Megacities},
  pages={199--219},
  year={2023},
  organization={Springer}
}

@inproceedings{song2024collaborative,
  title={Collaborative semantic occupancy prediction with hybrid feature fusion in connected automated vehicles},
  author={Song, Rui and Liang, Chenwei and Cao, Hu and Yan, Zhiran and Zimmer, Walter and Gross, Markus and Festag, Andreas and Knoll, Alois},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={17996--18006},
  year={2024}
}

@article{liu2024survey,
  title={A survey on autonomous driving datasets: Statistics, annotation quality, and a future outlook},
  author={Liu, Mingyu and Yurtsever, Ekim and Fossaert, Jonathan and Zhou, Xingcheng and Zimmer, Walter and Cui, Yuning and Zagar, Bare Luka and Knoll, Alois C},
  journal={IEEE Transactions on Intelligent Vehicles},
  year={2024},
  publisher={IEEE}
}

@article{liu2024graphrelate3d,
  title={GraphRelate3D: Context-Dependent 3D Object Detection with Inter-Object Relationship Graphs},
  author={Liu, Mingyu and Yurtsever, Ekim and Brede, Marc and Meng, Jun and Zimmer, Walter and Zhou, Xingcheng and Zagar, Bare Luka and Cui, Yuning and Knoll, Alois},
  journal={arXiv preprint arXiv:2405.06782},
  year={2024}
}

@inproceedings{carta2024roadsense3d,
  title={RoadSense3D: A Framework for Roadside Monocular 3D Object Detection},
  author={Carta, Salvatore and Castrill{\'o}n-Santana, Modesto and Marras, Mirko and Mohamed, Sondos and Podda, Alessandro Sebastian and Saia, Roberto and Sau, Marco and Zimmer, Walter},
  booktitle={Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
  pages={452--459},
  year={2024}
}


@article{zahid2024datadriven,
	title = {A data-driven approach for road accident detection in surveillance videos},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-16193-0},
	doi = {10.1007/s11042-023-16193-0},
	abstract = {The use of machine learning and computer vision techniques for detecting road accidents is a challenging task due to the limited availability of accident data for training. Staging fake accidents with real cars is expensive, and car crashes are rare incidents in roadside {CCTV} footage. Therefore, simulating fake car crashes using computers can be a feasible option. As such, we look at the following question in this paper; how successful can manually generated fake accident data be in terms of enabling a machine learning algorithm to detect real accidents?. In this work, we manually construct fake accident video frames from normal video traffic footage by creating simulated accidents. We do so by following predefined principles that maintain consistency with the scene context of normal frames. In order to detect real accidents in video footage, we fine-tune pre-trained deep convolutional neural networks on the manually generated fake accident frames. We use four pre-trained models i.e., {AlexNet}, {GoogleNet}, {SqueezeNet} and {ResNet}-50 on both normal and abnormal traffic video frames during the learning phase. The experimental results show that the fine-tuned {AlexNet} outperforms other models providing an 80\% percent true positive rate when detecting anomalies (accidents) in real-world surveillance videos of {UCF}-Crime dataset. This demonstrates the validity of our hypothesis that simulated accident data could be valuable for training machine learning algorithms to detect real-world accidents.},
	pages = {17217--17231},
	number = {6},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Zahid, Ariba and Qasim, Tehreem and Bhatti, Naeem and Zia, Muhammad},
	urldate = {2025-01-31},
	date = {2024-02-01},
	langid = {english},
	keywords = {Anomaly detection, Artificial Intelligence, Data preparation, Road accident detection, Transfer learning, Video surveillance},
	file = {Full Text PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/6PMAINZD/Zahid et al. - 2024 - A data-driven approach for road accident detection in surveillance videos.pdf:application/pdf},
}

@misc{xu2022tad,
	title = {{TAD}: A Large-Scale Benchmark for Traffic Accidents Detection from Video Surveillance},
	url = {http://arxiv.org/abs/2209.12386},
	doi = {10.48550/arXiv.2209.12386},
	shorttitle = {{TAD}},
	abstract = {Automatic traffic accidents detection has appealed to the machine vision community due to its implications on the development of autonomous intelligent transportation systems ({ITS}) and importance to traffic safety. Most previous studies on efficient analysis and prediction of traffic accidents, however, have used small-scale datasets with limited coverage, which limits their effect and applicability. Existing datasets in traffic accidents are either small-scale, not from surveillance cameras, not open-sourced, or not built for freeway scenes. Since accidents happened in freeways tend to cause serious damage and are too fast to catch the spot. An open-sourced datasets targeting on freeway traffic accidents collected from surveillance cameras is in great need and of practical importance. In order to help the vision community address these shortcomings, we endeavor to collect video data of real traffic accidents that covered abundant scenes. After integration and annotation by various dimensions, a large-scale traffic accidents dataset named {TAD} is proposed in this work. Various experiments on image classification, object detection, and video classification tasks, using public mainstream vision algorithms or frameworks are conducted in this work to demonstrate performance of different methods. The proposed dataset together with the experimental results are presented as a new benchmark to improve computer vision research, especially in {ITS}.},
	number = {{arXiv}:2209.12386},
	publisher = {{arXiv}},
	author = {Xu, Yajun and Huang, Chuwen and Nan, Yibing and Lian, Shiguo},
	urldate = {2024-11-13},
	date = {2022-09-26},
	eprinttype = {arxiv},
	eprint = {2209.12386},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/HBHNUTRW/Xu et al. - 2022 - TAD A Large-Scale Benchmark for Traffic Accidents Detection from Video Surveillance.pdf:application/pdf},
}


@article{adewopo2024smart,
	title = {Smart City Transportation: Deep Learning Ensemble Approach for Traffic Accident Detection},
	volume = {12},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/10497566},
	doi = {10.1109/ACCESS.2024.3387972},
	shorttitle = {Smart City Transportation},
	abstract = {The dynamic and unpredictable nature of road traffic necessitates effective accident detection methods for enhancing safety and streamlining traffic management in smart cities. This paper offers a comprehensive exploration study of prevailing accident detection techniques, shedding light on the nuances of other state-of-the-art methodologies while providing a detailed overview of distinct traffic accident types like rear-end collisions, T-bone collisions, and frontal impact accidents. Our novel approach introduces the I3D-{CONVLSTM}2D model architecture, a lightweight solution tailored explicitly for accident detection in smart city traffic surveillance systems by integrating {RGB} frames with optical flow information. Empirical analysis of our experimental study underscores the efficacy of our model architecture. The I3D-{CONVLSTM}2D {RGB} + Optical-Flow (trainable) model outperformed its counterparts, achieving an impressive 87\% Mean Average Precision ({MAP}). Our findings further elaborate on the challenges posed by data imbalances, particularly when working with a limited number of datasets, road structures, and traffic scenarios. Ultimately, our research illuminates the path towards a sophisticated vision-based accident detection system primed for real-time integration into edge {IoT} devices within smart urban infrastructures.},
	pages = {59134--59147},
	journaltitle = {{IEEE} Access},
	author = {Adewopo, Victor A. and Elsayed, Nelly},
	urldate = {2025-01-31},
	date = {2024},
	note = {Conference Name: {IEEE} Access},
	keywords = {accident detection, Accidents, action recognition, autonomous transportation, Autonomous vehicles, Cameras, Computer architecture, deep learning, Deep learning, Feature extraction, Road traffic, Smart cities, smart city, Traffic control, Traffic surveillance},
	file = {Full Text PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/U84I9SZ6/Adewopo and Elsayed - 2024 - Smart City Transportation Deep Learning Ensemble Approach for Traffic Accident Detection.pdf:application/pdf},
}

@article{yao2023dota,
	title = {{DoTA}: Unsupervised Detection of Traffic Anomaly in Driving Videos},
	volume = {45},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/9712446},
	doi = {10.1109/TPAMI.2022.3150763},
	shorttitle = {{DoTA}},
	abstract = {Video anomaly detection ({VAD}) has been extensively studied for static cameras but is much more challenging in egocentric driving videos where the scenes are extremely dynamic. This paper proposes an unsupervised method for traffic {VAD} based on future object localization. The idea is to predict future locations of traffic participants over a short horizon, and then monitor the accuracy and consistency of these predictions as evidence of an anomaly. Inconsistent predictions tend to indicate an anomaly has occurred or is about to occur. To evaluate our method, we introduce a new large-scale benchmark dataset called Detection of Traffic Anomaly ({DoTA})containing 4,677 videos with temporal, spatial, and categorical annotations. We also propose a new {VAD} evaluation metric, called spatial-temporal area under curve ({STAUC}), and show that it captures how well a model detects both temporal and spatial locations of anomalies unlike existing metrics that focus only on temporal localization. Experimental results show our method outperforms state-of-the-art methods on {DoTA} in terms of both metrics. We offer rich categorical annotations in {DoTA} to benchmark video action detection and online action detection methods. The {DoTA} dataset has been made available at: https://github.com/{MoonBlvd}/Detection-of-Traffic-Anomaly},
	pages = {444--459},
	number = {1},
	journaltitle = {{IEEE} Trans. on Pattern Analysis and Machine Intelligence},
	author = {Yao, Yu and Wang, Xizi and Xu, Mingze and Pu, Zelin and Wang, Yuchen and Atkins, Ella and Crandall, David J.},
	urldate = {2024-11-14},
	date = {2023-01},
	note = {Conf. Name: {IEEE} Trans. on Pattern Analysis and Machine Intelligence},
	keywords = {Accidents, Annotations, Anomaly detection, Benchmark testing, Cameras, Future object localization, Measurement, Traffic accident detection, Video action recognition, Video anomaly detection, Videos},
}


@inproceedings{dosovitskiy_carla_2017,
	title = {{CARLA}: An Open Urban Driving Simulator},
	url = {https://proceedings.mlr.press/v78/dosovitskiy17a.html},
	shorttitle = {{CARLA}},
	abstract = {We introduce {CARLA}, an open-source simulator for autonomous driving research. {CARLA} has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, {CARLA} provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use {CARLA} to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by {CARLA}, illustrating the platform’s utility for autonomous driving research.},
	eventtitle = {Conference on Robot Learning},
	pages = {1--16},
	booktitle = {Proceedings of the 1st Annual Conference on Robot Learning},
	publisher = {{PMLR}},
	author = {Dosovitskiy, Alexey and Ros, German and Codevilla, Felipe and Lopez, Antonio and Koltun, Vladlen},
	urldate = {2024-07-13},
	date = {2017-10-18},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/7XW28GN4/Dosovitskiy et al. - 2017 - CARLA An Open Urban Driving Simulator.pdf:application/pdf},
}

@software{yolov8_ultralytics,
  author = {Glenn Jocher and Ayush Chaurasia and Jing Qiu},
  title = {Ultralytics YOLOv8},
  version = {8.0.0},
  year = {2023},
  url = {https://github.com/ultralytics/ultralytics},
  orcid = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}


@article{pirdavani_application_2015,
	title = {Application of a Rule-Based Approach in Real-Time Crash Risk Prediction Model Development Using Loop Detector Data},
	volume = {16},
	issn = {1538-9588},
	url = {https://doi.org/10.1080/15389588.2015.1017572},
	doi = {10.1080/15389588.2015.1017572},
	abstract = {Objectives: There is a growing trend in development and application of real-time crash risk prediction models within dynamic safety management systems. These real-time crash risk prediction models are constructed by associating crash data with the real-time traffic surveillance data (e.g., collected by loop detectors). The main objective of this article is to develop a real-time risk model that will potentially be utilized within traffic management systems. This model aims to predict the likelihood of crash occurrence on motorways. Methods: In this study, the potential prediction variables are confined to traffic-related characteristics. Given that the dependent variable (i.e., traffic safety condition) is dichotomous (i.e., “no-crash” or “crash”), a rule-based approach is considered for model development. The performance of rule-based classifiers is further compared with the more conventional techniques like binary logistic regression and decision trees. The crash and traffic data used in this study were collected between June 2009 and December 2011 on a part of the E313 motorway in Belgium between Geel-East and Antwerp-East exits, on the direction toward Antwerp. Results: The results of analysis show that several traffic flow characteristics such as traffic volume, average speed, and standard deviation of speed at the upstream loop detector station and the difference in average speed on upstream and downstream loop detector stations significantly contribute to the crash occurrence prediction. The final chosen classifier is able to predict 70\% of crash occasions accurately, and it correctly predicts 90\% of no-crash instances, indicating a 10\% false alarm rate. Conclusions: The findings of this study can be used to predict the likelihood of crash occurrence on motorways within dynamic safety management systems.},
	pages = {786--791},
	number = {8},
	journaltitle = {Traffic Injury Prevention},
	author = {Pirdavani, Ali and De Pauw, Ellen and Brijs, Tom and Daniels, Stijn and Magis, Maarten and Bellemans, Tom and Wets, Geert},
	urldate = {2025-01-31},
	date = {2015-11-17},
	pmid = {25793926},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15389588.2015.1017572},
	keywords = {dynamic safety management systems, real-time crash risk prediction, rule-based classifiers, traffic surveillance data},
}


@article{yang_freeway_2021,
	title = {Freeway accident detection and classification based on the multi-vehicle trajectory data and deep learning model},
	volume = {130},
	issn = {0968-090X},
	url = {https://www.sciencedirect.com/science/article/pii/S0968090X21003120},
	doi = {10.1016/j.trc.2021.103303},
	abstract = {The freeway accident detection and classification have attracted much attention of researchers in the past decades. With the popularity of Global Navigation Satellite System ({GNSS}) on mobile phones and onboard equipment, increasing amounts of real-time vehicle trajectory data can be obtained more and more easily, which provides a potential way to use the multi-vehicle trajectory data to detect and classify an accident on freeways. The data has the advantages of low cost, high penetration, high real-time performance, and being robust to the outdoor environment. Therefore, this paper proposes a new method for accident detection and classification based on the multi-vehicle trajectory data. Different from the existing methods using {GNSS} positioning data, the proposed method not only uses the position information of the related vehicles but also tries to capture the development tendencies of the trajectories of accident vehicles over a period of time. A Deep Convolutional Neural Network model is developed to recognize an accident from the normal driving of vehicles and also identify the type of the accident, and the six types of traffic accidents are considered in this study. To train and test the proposed model, the simulated trajectory data is generated based on {PC}-Crash, including the normal driving trajectories and the trajectories before, in, and after an accident. The results indicate that the detection accuracy of the proposed method can reach up to 100\%, and the classification accuracy can reach up to 95\%, which both outperform the existing methods using other data. In addition, to ensure the robustness of the detection accuracy, at least 1 s of duration and 5 Hz of frequency for the trajectory data should be adopted in practice. The study will help to accurately and fastly detect an accident, recognize the accident type, and future judge who is liable for the accident.},
	pages = {103303},
	journaltitle = {Transportation Research Part C: Emerging Technologies},
	shortjournal = {Transportation Research Part C: Emerging Technologies},
	author = {Yang, Da and Wu, Yuezhu and Sun, Feng and Chen, Jing and Zhai, Donghai and Fu, Chuanyun},
	urldate = {2025-01-31},
	date = {2021-09-01},
	keywords = {Accident detection and classification, Deep Convolutional Neural Network, Freeway traffic accident, Vehicle trajectory},
}


@article{fang_vision-based_2024,
	title = {Vision-Based Traffic Accident Detection and Anticipation: A Survey},
	volume = {34},
	issn = {1558-2205},
	url = {https://ieeexplore.ieee.org/abstract/document/10227352?casa_token=x2ah_M1pw5YAAAAA:Gt5ODqQktvDbYv5bJC7LFu3iHBFgIxgLeX4ZuQECbCYYJdt4SmsFBiQPCFKbX79Ry7MDWpX8},
	doi = {10.1109/TCSVT.2023.3307655},
	shorttitle = {Vision-Based Traffic Accident Detection and Anticipation},
	abstract = {Traffic accident detection and anticipation is an obstinate road safety problem and painstaking efforts have been devoted. With the rapid growth of video data, Vision-based Traffic Accident Detection and Anticipation (named Vision-{TAD} and Vision-{TAA}) become the last one-mile problem for safe driving and surveillance safety. However, the long-tailed, unbalanced, highly dynamic, complex, and uncertain properties of traffic accidents form the Out-of-Distribution ({OOD}) feature for Vision-{TAD} and Vision-{TAA}. Current {AI} development may focus on these {OOD} but important problems. What has been done for Vision-{TAD} and Vision-{TAA}? What direction we should focus on in the future for this problem? A comprehensive survey is important. We present the first survey on Vision-{TAD} in the deep learning era and the first-ever survey for Vision-{TAA}. The pros and cons of each research prototype are discussed in detail during the investigation. In addition, we also provide a critical review of 31 publicly available benchmarks and related evaluation metrics. Through this survey, we want to spawn new insights and open possible trends for Vision-{TAD} and Vision-{TAA} tasks.},
	pages = {1983--1999},
	number = {4},
	journaltitle = {{IEEE} Transactions on Circuits and Systems for Video Technology},
	author = {Fang, Jianwu and Qiao, Jiahuan and Xue, Jianru and Li, Zhengguo},
	urldate = {2025-01-31},
	date = {2024-04},
	note = {Conference Name: {IEEE} Transactions on Circuits and Systems for Video Technology},
	keywords = {Accidents, Anomaly detection, autoencoder, Benchmark testing, benchmarks, Roads, safe driving, Surveillance, surveillance safety, Surveys, Traffic accident detection and anticipation, Uncertainty},
}



@inproceedings{maaloul_adaptive_2017,
	title = {Adaptive video-based algorithm for accident detection on highways},
	url = {https://ieeexplore.ieee.org/abstract/document/7993382?casa_token=0_OuIy9Y2hkAAAAA:LD6ZOqIxgjrGLuTARIOBccmy0dezii7H06Ap84daHM_Kut9bzgGdBd9X9n7gPlamTYPhCTGO},
	doi = {10.1109/SIES.2017.7993382},
	abstract = {For the past few decades, automatic accident detection, especially using video analysis, has become a very important subject. It is important not only for traffic management but also, for Intelligent Transportation Systems ({ITS}) through its contribution to avoid the escalation of accidents especially on highways. In this paper a novel vision-based road accident detection algorithm on highways and expressways is proposed. This algorithm is based on an adaptive traffic motion flow modeling technique, using Farneback Optical Flow for motions detection and a statistic heuristic method for accident detection. The algorithm was applied on a set of collected videos of traffic and accidents on highways. The results prove the efficiency and practicability of the proposed algorithm using only 240 frames for traffic motion modeling. This method avoids to utilization of a large database while adequate and common accidents videos benchmarks do not exist.},
	eventtitle = {2017 12th {IEEE} International Symposium on Industrial Embedded Systems ({SIES})},
	pages = {1--6},
	booktitle = {2017 12th {IEEE} International Symposium on Industrial Embedded Systems ({SIES})},
	author = {Maaloul, Boutheina and Taleb-Ahmed, Abdelmalik and Niar, Smail and Harb, Naim and Valderrama, Carlos},
	urldate = {2025-01-31},
	date = {2017-06},
	note = {{ISSN}: 2150-3117},
	keywords = {Abnormal behavior detection, Accident detection, Accidents, Computational modeling, computer vision, Farneback optical flow, Feature extraction, Intelligent Transportation Systems, {ITS}, Motion detection, Roads, Tracking, video processing, video surveillance},
	file = {Full Text PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/RP7AG9BW/Maaloul et al. - 2017 - Adaptive video-based algorithm for accident detection on highways.pdf:application/pdf},
}


@software{cvatai_corporation_computer_2023,
	title = {Computer Vision Annotation Tool ({CVAT})},
	rights = {{MIT}},
	url = {https://github.com/cvat-ai/cvat},
	abstract = {Annotate better with {CVAT}, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.},
	version = {2.25.0},
	author = {{CVAT.ai Corporation}},
	urldate = {2025-01-31},
	date = {2023-11},
	note = {original-date: 2018-06-29T14:02:45Z},
}


@inproceedings{cres_a9-dataset_2022,
	title = {A9-Dataset: Multi-Sensor Infrastructure-Based Dataset for Mobility Research},
	url = {https://ieeexplore.ieee.org/document/9827401},
	doi = {10.1109/IV51971.2022.9827401},
	shorttitle = {A9-Dataset},
	abstract = {Data-intensive machine learning based techniques increasingly play a prominent role in the development of future mobility solutions - from driver assistance and automation functions in vehicles, to real-time traffic management systems realized through dedicated infrastructure. The availability of high quality real-world data is often an important prerequisite for the development and reliable deployment of such systems in large scale. Towards this endeavour, we present the A9-Dataset based on roadside sensor infrastructure from the 3 km long Providentia++ test field near Munich in Germany. The dataset includes anonymized and precision-timestamped multi-modal sensor and object data in high resolution, covering a variety of traffic situations. As part of the first set of data, which we describe in this paper, we provide camera and {LiDAR} frames from two overhead gantry bridges on the A9 autobahn with the corresponding objects labeled with 3D bounding boxes. The first set includes in total more than 1000 sensor frames and 14000 traffic objects. The dataset is available for download at https://a9-dataset.com.},
	eventtitle = {2022 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {965--970},
	booktitle = {2022 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Cre{\ss}, Christian and Zimmer, Walter and Strand, Leah and Fortkord, Maximilian and Dai, Siyi and Lakshminarasimhan, Venkatnarayanan and Knoll, Alois},
	urldate = {2024-07-13},
	date = {2022-06},
	keywords = {Cameras, Three-dimensional displays, Laser radar, Point cloud compression, Sensor Fusion, C-{ITS}, Machine learning, Autonomous Driving, Artificial Intelligence, Bridges, Mobility Research, Multimodal sensors},
	file = {IEEE Xplore Abstract Record:/home/walter/snap/zotero-snap/common/Zotero/storage/G5ABENAA/9827401.html:text/html;Submitted Version:/home/walter/snap/zotero-snap/common/Zotero/storage/LRTAGSW7/Creß et al. - 2022 - A9-Dataset Multi-Sensor Infrastructure-Based Data.pdf:application/pdf},
}


@inproceedings{zimmer_tumtraf_2023,
	title = {{TUMTraf} Intersection Dataset: All You Need for Urban 3D Camera-{LiDAR} Roadside Perception},
	url = {https://ieeexplore.ieee.org/document/10422289/references#references},
	doi = {10.1109/ITSC57777.2023.10422289},
	shorttitle = {{TUMTraf} Intersection Dataset},
	abstract = {Intelligent Transportation Systems ({ITS}) allow a drastic expansion of the visibility range and decrease occlusions for autonomous driving. To obtain accurate detections, detailed labeled sensor data for training is required. Unfortunately, high-quality 3D labels of {LiDAR} point clouds from the infrastructure perspective of an intersection are still rare. Therefore, we provide the {TUM} Traffic ({TUMTraf}) Intersection Dataset, which consists of labeled {LiDAR} point clouds and synchronized camera images. Here, we recorded the sensor output from two roadside cameras and {LiDARs} mounted on intersection gantry bridges. The data was labeled in 3D by experienced annotators. Furthermore, we provide calibration data between all sensors, which allow the projection of the 3D labels into the camera images and an accurate data fusion. Our dataset consists of 4.8k images and point clouds with more than 57.4k manually labeled 3D boxes. With ten classes, it has a high diversity of road users in complex driving maneuvers, e.g. left/right turns, overtaking, and U-turns. In experiments, we provided baselines for the perception tasks. Overall, our dataset is a valuable contribution to the scientific community to perform complex 3D camera-{LiDAR} roadside perception tasks. Find data and code at https://innovation-mobility.com/tumtraf-dataset.},
	eventtitle = {2023 {IEEE} 26th International Conference on Intelligent Transportation Systems ({ITSC})},
	pages = {1030--1037},
	booktitle = {2023 {IEEE} 26th International Conference on Intelligent Transportation Systems ({ITSC})},
	author = {Zimmer, Walter and Cre{\ss}, Christian and Nguyen, Huu Tung and Knoll, Alois C.},
	urldate = {2024-07-14},
	date = {2023-09},
	note = {{ISSN}: 2153-0017},
	keywords = {Cameras, Roads, Three-dimensional displays, Laser radar, Dataset, {LiDAR}, Point cloud compression, Task analysis, Intelligent Transportation Systems, Synchronization, Autonomous Driving, 3D Perception, Camera},
	file = {IEEE Xplore Abstract Record:/home/walter/snap/zotero-snap/common/Zotero/storage/QDXT4EWK/references.html:text/html},
}


@misc{zhou_warm-3d_2024,
	title = {{WARM}-3D: A Weakly-Supervised Sim2Real Domain Adaptation Framework for Roadside Monocular 3D Object Detection},
	url = {http://arxiv.org/abs/2407.20818},
	doi = {10.48550/arXiv.2407.20818},
	shorttitle = {{WARM}-3D},
	abstract = {Existing roadside perception systems are limited by the absence of publicly available, large-scale, high-quality 3D datasets. Exploring the use of cost-effective, extensive synthetic datasets offers a viable solution to tackle this challenge and enhance the performance of roadside monocular 3D detection. In this study, we introduce the {TUMTraf} Synthetic Dataset, offering a diverse and substantial collection of high-quality 3D data to augment scarce real-world datasets. Besides, we present {WARM}-3D, a concise yet effective framework to aid the Sim2Real domain transfer for roadside monocular 3D detection. Our method leverages cheap synthetic datasets and 2D labels from an off-the-shelf 2D detector for weak supervision. We show that {WARM}-3D significantly enhances performance, achieving a +12.40\% increase in {mAP} 3D over the baseline with only pseudo-2D supervision. With 2D {GT} as weak labels, {WARM}-3D even reaches performance close to the Oracle baseline. Moreover, {WARM}-3D improves the ability of 3D detectors to unseen sample recognition across various real-world environments, highlighting its potential for practical applications.},
	number = {{arXiv}:2407.20818},
	publisher = {{arXiv}},
	author = {Zhou, Xingcheng and Fu, Deyu and Zimmer, Walter and Liu, Mingyu and Lakshminarasimhan, Venkatnarayanan and Strand, Leah and Knoll, Alois C.},
	urldate = {2024-11-13},
	date = {2024-07-30},
	eprinttype = {arxiv},
	eprint = {2407.20818},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/X5UAKTT9/Zhou et al. - 2024 - WARM-3D A Weakly-Supervised Sim2Real Domain Adaptation Framework for Roadside Monocular 3D Object D.pdf:application/pdf;Snapshot:/home/walter/snap/zotero-snap/common/Zotero/storage/R24JTUSI/2407.html:text/html},
}


@report{van_kempen_autotechagil_2023,
	title = {{AUTOtech}.agil : Architecture and Technologies for Orchestrating Automotive Agility},
	url = {https://publications.rwth-aachen.de/record/971700},
	shorttitle = {{AUTOtech}.agil},
	number = {{RWTH}-2023-09783},
	institution = {Aachener Kolloquium Fahrzeug- und Motorentechnik {GbR}},
	author = {van Kempen, Raphael},
	urldate = {2024-07-14},
	date = {2023},
	langid = {english},
	note = {Conference Name: 32. Aachen Colloquium Sustainable Mobility},
	file = {Snapshot:/home/walter/snap/zotero-snap/common/Zotero/storage/6ULLURQ6/971700.html:text/html},
}


@inproceedings{song_collaborative_2024,
	title = {Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles},
	url = {https://ieeexplore.ieee.org/document/10658094},
	doi = {10.1109/CVPR52733.2024.01704},
	abstract = {Collaborative perception in automated vehicles lever-ages the exchange of information between agents, aiming to elevate perception results. Previous camera-based collabo-rative 3D perception methods typically employ 3D bounding boxes or bird's eye views as representations of the en-vironment. However, these approaches fall short in offering a comprehensive 3D environmental prediction. To bridge this gap, we introduce the first method for collaborative 3D semantic occupancy prediction. Particularly, it improves local 3D semantic occupancy predictions by hybrid fusion of (i) semantic and occupancy task features, and (ii) Compressed orthogonal attention features shared between vehi-cles. Additionally, due to the lack of a collaborative perception dataset designed for semantic occupancy prediction, we augment a current collaborative perception dataset to include 3D collaborative semantic occupancy labels for a more robust evaluation. The experimental findings highlight that: (i) our collaborative semantic occupancy predictions excel above the results from single vehicles by over 30\%, and (ii) models anchored on semantic occupancy outpace state-of-the-art collaborative 3D detection techniques in subsequent perception applications, showcasing enhanced accuracy and enriched semantic-awareness in road environments.},
	eventtitle = {2024 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {17996--18006},
	booktitle = {2024 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Song, Rui and Liang, Chenwei and Cao, Hu and Yan, Zhiran and Zimmer, Walter and Gross, Markus and Festag, Andreas and Knoll, Alois},
	urldate = {2024-12-10},
	date = {2024-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Object detection, Roads, Three-dimensional displays, Visualization, Semantics, Solid modeling, Collaboration, Autonomous Driving, Cooperative Perception, Collaborative Perception, Multi-agent Systems, Robotics, Semantic Occupancy Prediction, Semantic Scene Completion},
	file = {IEEE Xplore Abstract Record:/home/walter/snap/zotero-snap/common/Zotero/storage/K2H8AAKI/10658094.html:text/html;Submitted Version:/home/walter/snap/zotero-snap/common/Zotero/storage/7VQ7DQNB/Song et al. - 2024 - Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicl.pdf:application/pdf},
}


@misc{zimmer_pointcompress3d_2024,
	title = {{PointCompress}3D -- A Point Cloud Compression Framework for Roadside {LiDARs} in Intelligent Transportation Systems},
	url = {http://arxiv.org/abs/2405.01750},
	doi = {10.48550/arXiv.2405.01750},
	abstract = {In the context of Intelligent Transportation Systems ({ITS}), efficient data compression is crucial for managing large-scale point cloud data acquired by roadside {LiDAR} sensors. The demand for efficient storage, streaming, and real-time object detection capabilities for point cloud data is substantial. This work introduces {PointCompress}3D, a novel point cloud compression framework tailored specifically for roadside {LiDARs}. Our framework addresses the challenges of compressing high-resolution point clouds while maintaining accuracy and compatibility with roadside {LiDAR} sensors. We adapt, extend, integrate, and evaluate three cutting-edge compression methods using our real-world-based {TUMTraf} dataset family. We achieve a frame rate of 10 {FPS} while keeping compression sizes below 105 Kb, a reduction of 50 times, and maintaining object detection performance on par with the original data. In extensive experiments and ablation studies, we finally achieved a {PSNR} d2 of 94.46 and a {BPP} of 6.54 on our dataset. Future work includes the deployment on the live system. The code is available on our project website: https://pointcompress3d.github.io.},
	number = {{arXiv}:2405.01750},
	publisher = {{arXiv}},
	author = {Zimmer, Walter and Pranamulia, Ramandika and Zhou, Xingcheng and Liu, Mingyu and Knoll, Alois C.},
	urldate = {2024-07-14},
	date = {2024-05-02},
	eprinttype = {arxiv},
	eprint = {2405.01750 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/5YJZM4Z5/Zimmer et al. - 2024 - PointCompress3D -- A Point Cloud Compression Frame.pdf:application/pdf;arXiv.org Snapshot:/home/walter/snap/zotero-snap/common/Zotero/storage/5VZAY53U/2405.html:text/html},
}


@misc{liu_graphrelate3d_2024,
	title = {{GraphRelate}3D: Context-Dependent 3D Object Detection with Inter-Object Relationship Graphs},
	url = {http://arxiv.org/abs/2405.06782},
	doi = {10.48550/arXiv.2405.06782},
	shorttitle = {{GraphRelate}3D},
	abstract = {Accurate and effective 3D object detection is critical for ensuring the driving safety of autonomous vehicles. Recently, state-of-the-art two-stage 3D object detectors have exhibited promising performance. However, these methods refine proposals individually, ignoring the rich contextual information in the object relationships between the neighbor proposals. In this study, we introduce an object relation module, consisting of a graph generator and a graph neural network ({GNN}), to learn the spatial information from certain patterns to improve 3D object detection. Specifically, we create an inter-object relationship graph based on proposals in a frame via the graph generator to connect each proposal with its neighbor proposals. Afterward, the {GNN} module extracts edge features from the generated graph and iteratively refines proposal features with the captured edge features. Ultimately, we leverage the refined features as input to the detection head to obtain detection results. Our approach improves upon the baseline {PV}-{RCNN} on the {KITTI} validation set for the car class across easy, moderate, and hard difficulty levels by 0.82\%, 0.74\%, and 0.58\%, respectively. Additionally, our method outperforms the baseline by more than 1\% under the moderate and hard levels {BEV} {AP} on the test server.},
	number = {{arXiv}:2405.06782},
	publisher = {{arXiv}},
	author = {Liu, Mingyu and Yurtsever, Ekim and Brede, Marc and Meng, Jun and Zimmer, Walter and Zhou, Xingcheng and Zagar, Bare Luka and Cui, Yuning and Knoll, Alois},
	urldate = {2024-07-29},
	date = {2024-05-10},
	eprinttype = {arxiv},
	eprint = {2405.06782 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/UL5DIIIA/Liu et al. - 2024 - GraphRelate3D Context-Dependent 3D Object Detecti.pdf:application/pdf;arXiv.org Snapshot:/home/walter/snap/zotero-snap/common/Zotero/storage/VKUQY25U/2405.html:text/html},
}



@article{zhou_vision_2024,
	title = {Vision Language Models in Autonomous Driving: A Survey and Outlook},
	issn = {2379-8904},
	url = {https://ieeexplore.ieee.org/abstract/document/10531702},
	doi = {10.1109/TIV.2024.3402136},
	shorttitle = {Vision Language Models in Autonomous Driving},
	abstract = {The applications of Vision-Language Models ({VLMs}) in the field of Autonomous Driving ({AD}) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models ({LLMs}). By integrating language data, the driving systems can be able to deeply understand real-world environments, improving driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream {VLM} tasks and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving dataset thoroughly. At last, we discuss the benefits and challenges of {VLMs} in {AD}, and provide researchers with the current research gaps and future trends. https://github.com/ge25nab/Awesome-{VLM}-{AD}-{ITS}},
	pages = {1--20},
	journaltitle = {{IEEE} Transactions on Intelligent Vehicles},
	author = {Zhou, Xingcheng and Liu, Mingyu and Yurtsever, Ekim and Zagar, Bare Luka and Zimmer, Walter and Cao, Hu and Knoll, Alois C.},
	urldate = {2024-07-26},
	date = {2024},
	note = {Conference Name: {IEEE} Transactions on Intelligent Vehicles},
	keywords = {Autonomous vehicles, Visualization, Computational modeling, Data models, Task analysis, Surveys, Autonomous Driving, Planning, Conditional Data Generation, Decision Making, End-to-End Autonomous Driving, Intelligent Vehicle, Language-guided Navigation, Large Language Model, Vision Language Model},
	file = {IEEE Xplore Abstract Record:/home/walter/snap/zotero-snap/common/Zotero/storage/YUFZSGFJ/10531702.html:text/html;IEEE Xplore Full Text PDF:/home/walter/snap/zotero-snap/common/Zotero/storage/BMUIKY46/Zhou et al. - 2024 - Vision Language Models in Autonomous Driving A Su.pdf:application/pdf},
}



@misc{mohamed_transfer_2024,
	title = {Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection},
	url = {http://arxiv.org/abs/2408.15637},
	doi = {10.48550/arXiv.2408.15637},
	number = {{arXiv}:2408.15637},
	publisher = {{arXiv}},
	author = {Mohamed, Sondos and Zimmer, Walter and Greer, Ross and Ghita, Ahmed Alaaeldin and Castrillon-Santana, Modesto and Trivedi, Mohan and Knoll, Alois and Carta, Salvatore Mario and Marras, Mirko},
	urldate = {2024-11-13},
	date = {2024-08-28},
	eprinttype = {arxiv},
	eprint = {2408.15637},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}
