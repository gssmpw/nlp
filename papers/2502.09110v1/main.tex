% ICCV 2025 Paper Template

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage{iccv}              % To produce the CAMERA-READY version
% \usepackage[review]{iccv}      % To produce the REVIEW version
\usepackage[pagenumbers]{iccv} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{iccvblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=iccvblue]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{*****} % *** Enter the Paper ID here
\def\confName{ICCV}
\def\confYear{2025}

%%%%%%%%% TITLE - PLEASE UPDATE
% \title{Revealing the Unseen:\\
\title{Pulling Back the Curtain:\\
       Unsupervised Adversarial Detection via Contrastive Auxiliary Networks}

%%%%%%%%% AUTHORS - PLEASE UPDATE
\author{Eylon Mizrahi\\
Ben-Gurion University\\% of the Negev\\
Beer-Sheva, 8410501, Israel\\
{\tt\small eylonmiz@post.bgu.ac.il}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Raz Lapid\\
DeepKeep \\
Tel-Aviv, Israel\\
{\tt\small raz.lapid@deepkeep.ai}
\and
Moshe Sipper\\
Ben-Gurion University\\% of the Negev\\
Beer-Sheva, 8410501, Israel\\
{\tt\small sipper@bgu.ac.il}
}

% \usepackage[usenames,dvipsnames]{xcolor}
\usepackage{graphicx} % Required for inserting images
\usepackage{ragged2e}
\usepackage{caption}
\usepackage{multirow}
\usepackage{tcolorbox}
\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{mdframed}
\usepackage{times}
\usepackage{diagbox}
\usepackage{ragged2e}
\usepackage{booktabs}
% \usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,calc,positioning,decorations.pathmorphing}
\usetikzlibrary{shapes.geometric}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\usepackage{algpseudocode}   % if you use the "algorithmic" environment
% or
\usepackage[ruled,vlined]{algorithm2e} % if you prefer the algorithm2e environment

\usepackage{amsmath}         % math environments
\usepackage{amssymb}         % optional for extra math symbols
\usepackage{amsfonts}        % optional for fonts in math mode

\usepackage[numbers,sort&compress]{natbib}
\usepackage{pifont}
% \usepackage{hyperref}
\usepackage{float}
% \usepackage[font={small,it}]{caption}
\usepackage[font={small}]{caption}
\usepackage[export]{adjustbox}

% Define a new caption style 
% \DeclareCaptionFormat{justified}{\begin{justify}#1#2#3\end{justify}} \captionsetup[figure]{format=justified, labelfont=bf}

% \hypersetup{
%     colorlinks=true,
%     linkcolor=purple,
%     filecolor=cyan,      
%     urlcolor=teal,
%     citecolor=blue,
%     }

\usepackage{tcolorbox}
\newenvironment{dialog}{\begin{center}\begin{tcolorbox}[halign=flush center,colframe=olive,width=0.8\textwidth,boxrule=1pt]}{\end{tcolorbox}\end{center}}
%colback=gray!10, colframe=black!100

% Normalized vectors
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\normalize}[1]{\frac{#1}{\norm{#1}}}

% for \autoref
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}
% \renewcommand{\algorithmautorefname}{Algorithm}
\renewcommand{\appendixautorefname}{Appendix}


\begin{document}
\maketitle

\begin{abstract}
Deep learning models are widely employed in safety-critical applications yet remain susceptible to adversarial attacks---imperceptible perturbations that can significantly degrade model performance. Conventional defense mechanisms predominantly focus on either enhancing model robustness or detecting adversarial inputs independently. In this work, we propose an \textbf{U}nsupervised adversarial detection via \textbf{C}ontrastive \textbf{A}uxiliary \textbf{N}etworks (\textbf{U-CAN}) to uncover adversarial behavior within auxiliary feature representations, without the need for adversarial examples. U-CAN is embedded within selected intermediate layers of the target model. These auxiliary networks, comprising projection layers and ArcFace-based linear layers, refine feature representations to more effectively distinguish between benign and adversarial inputs. Comprehensive experiments across multiple datasets (CIFAR-10, Mammals, and a subset of ImageNet) and architectures (ResNet-50, VGG-16, and ViT) demonstrate that our method surpasses existing unsupervised adversarial detection techniques, achieving superior F1 scores against four distinct attack methods. The proposed framework provides a scalable and effective solution for enhancing the security and reliability of deep learning systems.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Deep learning has revolutionized various domains, including safety-critical fields such as autonomous driving, healthcare, and security \citep{grigorescu2020survey,nazir2024deep,pinhasov2024xaibased,esteva2019guide,taherdoost2024beyond,9476037,lapid2024fortify}. However, these systems are increasingly vulnerable to \textit{adversarial attacks}—imperceptible perturbations crafted to degrade model performance. First introduced by \citet{szegedy2013intriguing}, adversarial attacks have since been refined in numerous studies \citep{carlini2017towards,lapid2024open,goodfellow2014explaining,madry2017towards,andriushchenko2020square,croce2020minimally,lapid2022evolutionary,moosavi2016deepfool,su2019one,brown2017adversarial,lapid2023see,lapid2023patch}.

As deep learning advances, adversarial attacks pose a growing threat to system security and reliability, making robust defense mechanisms an urgent research priority.

Defenses against adversarial attacks follow two main strategies: adversarial robustness and adversarial detection. The former strengthens models to maintain accuracy under attack \citep{li2023sok,yeo2021robustness,bai2021recent,cohen2019certified,xie2019feature,anderson2022certified,jain2023efficiently}, while the latter detects and flags adversarial inputs before they impact performance \citep{li2025self,chen2022adversarial,li2024adversarial,lapid2024fortify,feinman2017detecting,deng2021libre,pinhasov2024xaibased}.

This paper proposes a novel adversarial detection method that operates without adversarial examples. By integrating auxiliary networks into the target model, we generate intermediate contrastive features, enhancing the detection performance of adversarial inputs.

Our key contributions are:
\begin{enumerate}
    \item \textbf{Unsupervised adversarial detection}. Our approach requires no labeled adversarial data, making it adaptable to various attack types without prior knowledge or specialized adversarial training.
    
    \item \textbf{No modifications to the target model parameters}. Our method operates through external detection and refinement pipelines, leaving the target model’s structure and weights unchanged. This design ensures enhanced adversarial detection without degrading the original performance or necessitating specialized retraining.
    
    \item \textbf{Compatibility with existing methods}. The proposed framework--U-CAN integrates seamlessly with adversarial detection mechanisms that utilize intermediate model layers, further improving detection efficacy, as demonstrated in \autoref{sec:results}.
\end{enumerate}


\section{Previous Work}
\label{sec:previous_work}

Adversarial attacks challenge the robustness of machine learning models, particularly in critical applications like image classification and object detection \citep{finlayson2019adversarial,guo2019simple,lapid2022evolutionary,alter2024robustness,carlini2017towards,lapid2023patch,lapid2023see,lapid2024open,carlini2017adversarial,vitracktamam2023foiling}.

To counter these threats, adversarial detection methods have been widely explored. Many rely on adversarial examples during training, while others operate without them to improve generalization and reduce computational costs \citep{roth2019odds,li2017adversarial,dathathri2018detecting,ma2018characterizing,carrara2017detecting,metzen2022detecting}. This paper primarily focuses on unsupervised approaches, with a brief overview of supervised methods.

\subsection{Supervised Adversarial Detection}
\label{subsec:previous_supervised}

Supervised methods train on labeled datasets to distinguish adversarial from benign inputs, leveraging explicit attack examples to refine detection.  \citet{carrara2018adversarial} introduce a feature distance–based technique using deep neural networks. By extracting representations from multiple layers, they compute input distances to detect adversarial shifts. An LSTM \citep{graves2012long} processes these distance patterns to classify inputs as adversarial or benign. \citet{lee2018simple} propose a framework that models class-wise feature activations as multivariate Gaussian distributions. Mahalanobis distances measure input deviations, and a logistic regression detector (trained on both benign and adversarial data) aggregates layer-wise distances to enhance detection reliability.

\subsection{Unsupervised Adversarial Detection}
\label{subsec:previous_unsupervised}

Unsupervised methods detect adversarial inputs without pre-training on labeled attack examples, relying on intrinsic data properties or model representations. These approaches often reduce complexity and improve generalization across attack types. \citet{xu2017feature} introduce Feature Squeezing (FS), which applies transformations like bit-depth reduction and spatial smoothing to simplify input features. Detection then is based on prediction discrepancies between original and squeezed inputs. Deep Neural Rejection (DNR) \citep{sotgiu2020deep} enhances networks with auxiliary RBF-SVM classifiers at intermediate layers. An aggregator SVM combines their outputs to reject adversarial inputs with inconsistent layer-wise representations. This method behaves as if the target model would have an additional classification neuron that determines whether to reject the sample or not. \citet{papernot2018deep} propose Deep k-NN (DKNN), which estimates feature-space density using $k$-nearest neighbors across layers. Then, they produce P-values based on those DKNN results with respect to a set of calibration benign images they store. Low p-values indicate low credibility of the input sample, maybe adversarial anomalies, enabling robust detection without attack-specific training.

\subsection{Key Differences from Prior Work}
Our methodology builds on intermediate representations for adversarial detection but introduces key differences:
\begin{enumerate}
    \item \textbf{No dependence on adversarial data.} Unlike supervised methods \citep{lee2018simple,carrara2018adversarial}, which require adversarial examples for training, our approach is fully unsupervised, eliminating computational overhead and bias from adversarial training.
    \item \textbf{Auxiliary networks with ArcFace-based feature refinement.} Rather than relying solely on raw feature statistics \citep{papernot2018deep,carrara2018adversarial,sotgiu2020deep}, we introduce auxiliary networks that refine feature spaces. These networks include: (1) a projection layer that maps features to a lower-dimensional space, and (2) an ArcFace layer that enforces margin-based separation on a hypersphere, enhancing feature discrimination.
    \item \textbf{Layer-wise modularity for fine-grained detection.} Prior works often aggregate final-layer statistics \citep{lee2018simple,sotgiu2020deep}. In contrast, we integrate auxiliary networks across multiple layers ($L_{s}, L_{s+1}, \dots, L_{N}$), capturing diverse feature granularity and improving detection effectiveness without requiring supervised external classifiers such as LSTMs or logistic regression \citep{carrara2018adversarial,lee2018simple}.
\end{enumerate}



\section{Preliminaries}
\label{sec:preliminaries}

aims to embed semantically similar inputs (referred to as \emph{positives}) closer 
together while pushing apart dissimilar inputs (\emph{negatives}). Formally, let 
\(\mathbf{z}_i \in \mathbb{R}^d\) be an \emph{anchor} embedding, 
\(\mathbf{z}_i^+ \in \mathbb{R}^d\) be its corresponding \emph{positive} embedding, 
and \(\{\mathbf{z}_j^-\}_{j=1}^K\) be a set of \(K\) \emph{negative} embeddings. We 
define a similarity function
\[
\mathrm{sim}(\mathbf{z}_p, \mathbf{z}_q),
\]
which can be, for instance, the dot product \(\mathbf{z}_p^\top \mathbf{z}_q\) or 
cosine similarity 
\(\frac{\mathbf{z}_p^\top \mathbf{z}_q}{\|\mathbf{z}_p\|\|\mathbf{z}_q\|}\). Let 
\(\tau > 0\) be a \emph{temperature} hyperparameter controlling the concentration 
of the exponential distribution over similarities. Then, we define the scaled 
similarities:
\[
s_i^+ = \frac{\mathrm{sim}(\mathbf{z}_i,\mathbf{z}_i^+)}{\tau}
\quad \text{and} \quad
s_i^j = \frac{\mathrm{sim}(\mathbf{z}_i,\mathbf{z}_j^-)}{\tau}.
\]
Given a total of \(N\) anchor embeddings in a mini-batch, the InfoNCE loss 
\citep{oord2018representation} is
\[
\mathcal{L}_\text{contrast} 
= -\sum_{i=1}^N \log 
\Biggl( \frac{\exp(s_i^+)}{\exp(s_i^+) + \sum_{j=1}^K \exp(s_i^j)}\Biggr).
\]
This objective encourages robust, \emph{discriminative} representations by enforcing 
high similarity (in scaled form) between each anchor and its positive example while 
reducing similarity to the negative examples. Such an approach naturally aligns with 
our framework’s emphasis on using auxiliary networks to promote robust feature 
separation.

\textbf{ArcFace.} We adopt the ArcFace loss \citep{Deng_2022}, a widely used approach, originally proposed for face recognition but effective in various applications requiring discriminative representations. ArcFace introduces an \textit{additive angular margin penalty} to improve both inter-class separation and intra-class compactness. 

ArcFace projects feature vectors onto a hypersphere by normalizing both input feature vectors and class weight vectors. For an input feature vector $\mathbf{x}$ and class weight vector $\mathbf{W}_i$, cosine similarity is computed as:
\begin{equation} 
\cos(\theta_i) = \frac{\mathbf{W}_i^\top \mathbf{x}}{\lVert\mathbf{W}_i\rVert \lVert\mathbf{x}\rVert}, 
\end{equation} 
where $\theta_i$ represents the angular distance between $\mathbf{x}$ and $\mathbf{W}_i$. To enhance separation, an \textit{angular margin} $m$ is introduced:
\begin{equation} 
\cos(\theta_i + m), 
\end{equation} 
which tightens intra-class clustering and increases inter-class separation. The modified cosine similarity is scaled by $s$ and incorporated into the softmax function:
\begin{equation}
P(y_i|\mathbf{x}) = \frac{\exp(s \cdot \cos(\theta_i + m))}{\sum_{j=1}^{CL} \exp(s \cdot \cos(\theta_j))}, 
\end{equation} 
where $CL$ denotes the number of classes or instances.

Beyond classification, ArcFace’s principles—embedding normalization, angular margin constraints, and hyperspherical separation—are widely applied in embedding-based retrieval, metric learning and clustering. We leverage these properties to enhance adversarial detection by learning discriminative features at each intermediate layer of the target model.


\section{Methodology}
\label{sec:methodology}

We propose \emph{U-CAN}, an unsupervised adversarial detection framework built atop a frozen target network, which extracts intermediate feature maps $\{z_i\}$. As shown in \autoref{fig:algorithm} and \autoref{alg:ucan_shortest}, each $z_k$ is refined by an \emph{Aux.~Block} ($A_k$)—comprising a $1\!\times\!1$ convolution, adaptive average pooling, flattening, and $\ell_{2}$-normalization—followed by an \emph{ArcFace} layer, yielding a well-separated embedding space. Finally, an \emph{Aggregator} ($\mathcal{G}$) combines these refined embeddings into a compact detection vector $\mathbf{v}\in\mathbb{R}^2$, enabling robust adversarial discrimination.

\begin{figure*}[ht]
\centering
\includegraphics[trim={1cm 4cm 5.5cm 0.2cm},clip,scale=0.5]{figures/schemes/new_scheme.pdf}
\caption{Overview of our proposed method. The input \(x\) is passed through a frozen target model, comprising layers \(\{L_1, L_2, \dots, L_N\}\), which produce features \(\{z_1, z_2, \dots, z_N\}\). Each feature \(z_k\) is then processed by an \textit{Aux. Block} ($A_k$), which projects the inputs to a refined space using \(1\times1\) convolution, adaptive average pooling, flattening, and L2-normalization. These refined spaces are represented by unit hyperspheres and trained with the ArcFace loss \citep{Deng_2022} to match the learnable class centers ($W_{CL\times{d'}}$). In these hyperspheres, adversarial samples (black stars) appear as shifts from the well-separated benign (green circles) centers.  
To detect adversaries, an aggregator ($\mathcal{G}$) is applied to a subset of the top \(S\) informative auxiliary networks that best preserve intra-class clustering and inter-class separation. To identify adversaries, the aggregator can effectively analyze these refined representations to produce the adversarial detection vector $\mathbf{v}$.}
\label{fig:algorithm}
\end{figure*}

The auxiliary networks transform intermediate feature representations into a more structured space, where adversarial perturbations become more distinguishable. These refined representations can be integrated into various feature aggregation and anomaly detection methods to enhance robustness against adversarial attacks.

Let $\mathcal{M}$ be the target network. For an input sample $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$, the target network produces a sequence of intermediate feature maps:
\begin{equation}
    \bigl\{ \mathbf{f}_1, \mathbf{f}_2, \dots, \mathbf{f}_N \bigr\},
\end{equation}
where each $\mathbf{f}_k$ corresponds to the output of a specific layer $L_k$ in $\mathcal{M}$ out of $N$ layers. A subset of these feature maps is selected for refinement. Without loss of generality, we assume that $\mathbf{f}_k$ is flattened into a feature vector $\mathbf{z}_k \in \mathbb{R}^{d_k}$.

\subsection{Auxiliary Networks}
Each chosen layer $L_k$ is associated with an auxiliary block ($A_k$), which refines feature representations to amplify Out-of-Distribution (OOD) or adversarial discrepancies. Each auxiliary block consists of projection and ArcFace \citep{Deng_2022} components.

\paragraph{(1) Projection component.} This module projects the spatially flattened feature vector $\mathbf{z}_k \in \mathbb{R}^{d_k \times C_k}$ into a compact representation $\mathbf{p}_k \in \mathbb{R}^{d'}$, where $d' \ll d_k$. The projection consists of a $1\times1$ convolution to reduce channels, followed by adaptive average pooling:
\begin{equation}
    \mathbf{p}_k = \frac{1}{H_k \cdot W_k} \sum_{i=1}^{H_k} \sum_{j=1}^{W_k} \bigl(\mathbf{W}^{(p)} \mathbf{z}_{k,ij} + \mathbf{b}^{(p)}\bigr),
\end{equation}
where $\mathbf{W}^{(p)} \in \mathbb{R}^{{C'}_k \times C_k}$ and $\mathbf{b}^{(p)} \in \mathbb{R}^{{C'}_k}$ are the learnable weights and biases of the $1\times1$ convolution, and $\mathbf{z}_{n,ij} \in \mathbb{R}^{C_k}$ is the feature vector at spatial location $(i,j)$. The global average pooling ensures that the final representation $\mathbf{p}_k$ is spatially invariant.

\paragraph{(2) ArcFace linear layer.} The transformed feature vector $\mathbf{p}_k$ is processed through an ArcFace-based transformation \citep{Deng_2022}:
\begin{equation}
    \tilde{\mathbf{W}}^{(AF)}_k = \normalize{\mathbf{W}^{(AF)}_k}, \quad \tilde{\mathbf{p}}_k = \normalize{\mathbf{p}_k}; \quad CS_k = \tilde{\mathbf{W}}_k \tilde{\mathbf{p}}_k,
\end{equation}
where $\mathbf{W}^{(AF)}_k \in \mathbb{R}^{CL \times d'}$ are the learnable ArcFace weight matrices representing class centers, $CL$ is the number of classes or instances, and $CS_k \in \mathbb{R}^{CL}$ is the cosine similarity score between the normalized projected feature $\tilde{\mathbf{p}}_k$ and the class centers $\tilde{\mathbf{W}}^{(AF)}_k$.

ArcFace \citep{Deng_2022} introduces an angular margin penalty to improve class separation or instance representation by projecting feature vectors onto a normalized hypersphere. This margin enhances inter-class separation while maintaining intra-class compactness.

As a result, vectors on the auxiliary ArcFace hyperspheres $\tilde{\mathbf{p}}_k$ become more sensitive to small changes, such as adversarial perturbations, meaning that minor shifts in the input space result in more pronounced shifts in the refiner embedding space. This property makes it easier to distinguish adversarial examples from benign samples through the target model layers.

\subsection{Training Procedure}
Our framework does not require adversarial samples for training. Given a pretrained $\mathcal{M}$, each auxiliary network $\mathcal{A}_n$ is trained to refine embeddings while keeping $\mathcal{M}$ frozen. During training, the objective is to maximize intra-class similarity (where a 'class' can represent either an actual class label or a pseudo-identity) while enforcing a margin that separates instances from different classes or pseudo-identities. The $k$-th auxiliary ArcFace loss can be formulated as:
\begin{equation}
    \mathcal{L}_k = -\log \frac{s \cdot e^{\cos(\theta^{(k)}_{y_i} + m)}}{s \cdot e^{\cos(\theta^{(k)}_{y_i} + m)} + \sum_{j=1, j \neq y_i}^{N} s \cdot e^{\cos(\theta^{(k)}_j)}},
\end{equation}
where $m$ is the angular margin, $s$ is the hypersphere scale factor, $y_i$ is the target class, and $\theta^{(k)}_{y_i}$ is the angle between the feature vector and the corresponding class center at feature level $k$. Thus, the global loss function is computed as:
\begin{equation}
    \mathcal{L}_{global} = \frac{1}{N} \sum_{i=1}^{N}\mathcal{L}_k.
\end{equation}

\subsection{Layer-Wise Fusion}
Refined feature embeddings $\tilde{\mathbf{p}}_k$ from different layers are fused to improve adversarial detection:
\begin{equation}
    \mathbf{v} = \mathcal{G}(\{\tilde{\mathbf{p}}_k\}) \mid k\in\mathcal{S},
    % \mathbf{v} = \bigoplus_{k \in \mathcal{S}} \tilde{\mathbf{p}}_k,
\end{equation}
where $\mathcal{G}$ is a given feature aggregation algorithm and $\mathcal{S} \leq N$ represents the best \(S\) auxiliary blocks corresponding with the most informative feature representations, contributing to the final adversarial detection score.

\paragraph{Inference.} During inference, a test sample $\mathbf{x}$ is processed through $\mathcal{M}$, producing feature maps $\mathbf{z}_k$. Each selected $\mathbf{z}_k$ is refined by its corresponding auxiliary block, generating $\tilde{\mathbf{p}}_k$. These embeddings are fused and analyzed through $\mathcal{G}$ to determine whether the input is adversarial based on its deviation from the learnable benign feature distribution. The aggregator can use either the normalized representations ($\tilde{\mathbf{p}}_k$) or their logits ($CS_k$) relative to the learnable class centers ($\mathbf{W}^{(AF)}_k$), depending on its specific algorithm.

\begin{algorithm}
\footnotesize
\caption{U-CAN}
\label{alg:ucan_shortest}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{Frozen model $\mathcal{M}$ with layers $L_1,\ldots,L_N$, benign data and off-the-shelf aggregator $\mathcal{G}$.}
\Output{Trained adversarial detector $\mathcal{D}$.}

\BlankLine
\textbf{1.} Initialize Aux.\ Blocks $\{\mathcal{A}_k\}_1^N$.

\BlankLine
\textbf{2.} Stack $\{\mathcal{A}_k\}_1^N$ on top of $\mathcal{M}$ and train them simultaneously using $\mathcal{L}_{global}$.

\BlankLine
\textbf{3.} Compute validation score $CS^{(avg)}_k$ per $\mathcal{A}_k$; select the best $S$ blocks $\{\,\mathcal{A}_k \mid k \in \mathcal{S}\}$.

\vspace{5pt}

\textbf{Inference:} \\ % Larger and bold for emphasis
%\vspace{5pt}
\For{test sample $\mathbf{x}$}{
  Feed $\textbf{x}$ through $\mathcal{M}$. \\  
  Extract embeddings from $\{\,\mathcal{A}_k\}_{k\in\mathcal{S}}$; feed into $\mathcal{G}$.\\
  Flag $\mathbf{x}$ as adversarial or benign.
}

\end{algorithm}


\section{Experimental Setting}\label{sec:experimental_setting}

\subsection{Datasets}
\label{datasets_section}
We conducted our experiments on three datasets: CIFAR-10 \citep{krizhevsky2009learning}, Mammals \citep{asaniczka_2023}, and a subset of ImageNet \citep{russakovsky2015imagenet}. These choices ensure a variety of classes, resolutions, and receptive fields while remaining computationally feasible. CIFAR-10 contains 60,000 images of size 32x32 across 10 classes, Mammals includes 13,751 images of size 256x256 spanning 45 classes, and our ImageNet subset focuses on 6 randomly selected classes (1,050 images per class).

For each dataset, we split the data into training, validation, calibration, and test sets. The training and validation sets were used to train both the baseline classifiers and our auxiliary networks. The calibration set stored reference feature vectors for the DKNN \citep{papernot2018deep} adversarial detection method, and the final test set contained benign examples later transformed into adversarial queries.

\subsection{Models}
\label{models_section}
To accommodate diverse architectural styles, we used three classification models: ResNet-50 \citep{he2016deep}, VGG-16 \citep{simonyan2014very}, and ViT-B-16 \citep{dosovitskiy2020image}. ResNet-50 and VGG-16 are prominent CNNs with different depths and layer structures, whereas ViT-B-16 is based on the transformer paradigm, representing a more recent approach to image classification.

We first trained 9 classifiers (one for each dataset-model combination) for 250 epochs, initializing from ImageNet-pretrained weights \citep{russakovsky2015imagenet} and selecting the best checkpoints by F1 validation scores. We then constructed and trained our auxiliary networks atop these frozen classifiers for up to 1,500 epochs.

During auxiliary-network training, we used a metric capturing inter-class separation and intra-class cohesion. Specifically, we computed the total cosine similarity (TCS) by averaging the difference between positive-class similarities (\autoref{eq10}) and negative-class similarities (\autoref{eq11}) across layers:
\begin{equation}
    \label{eq10}
    CS^{+}_k = \tilde{\mathbf{W}}^{(y_i)}_k \tilde{\mathbf{p}}_k
\end{equation}
\begin{equation}
    \label{eq11}
    CS^{-}_k = \frac{1}{CL-1} \sum_{j=1,j\neq{i}}^{CL}{\tilde{\mathbf{W}}^{(y_j)}_k \tilde{\mathbf{p}}_k}
\end{equation}
\begin{equation}
    \label{eq12}
    CS^{(avg)}_k = \frac{1}{2} (CS^{+}_k - CS^{-}_k)
\end{equation}
\begin{equation}
    TCS = \frac{1}{N} \sum_{i=1}^{N}{CS^{(avg)}_k}
\end{equation}
where $\tilde{\mathbf{W}}^{(y_i)}_k$ denotes the normalized vector for the positive class center, $\tilde{\mathbf{W}}^{(y_{j\neq{i}})}_k$ are the centers for the other (negative) classes, and $\tilde{\mathbf{p}}_k$ is the normalized feature vector of the current sample. We used \texttt{AdamW} with \texttt{ReduceLROnPlateau} and a learning rate of $1\times 10^{-5}$ for CIFAR10, and $1\times 10^{-4}$ for both the ImageNet subset and Mammals. Data augmentations included random zoom, brightness, hue, saturation, rotation, a small random perspective transform, and random box masking.

At this point in our research, we used the default fixed values for the ArcFace \citep{Deng_2022} hyperparameters. Specifically, we kept the default ArcFace hyperparameters and selected a reasonable latent size, $d'$, that was compact yet balanced between the minimum and maximum number of channels in each architecture: 512 for ResNet-50 \citep{he2016deep}, 256 for VGG-16 \citep{simonyan2014very}, and 768 for ViT-B-16 \citep{dosovitskiy2020image}. In addition, for each target model, we chose reasonable blocks for attaching the auxiliary networks for training: 16 for ResNet-50, 13 for VGG-16, and 12 for ViT-B-16.

\subsection{Attacks}
Below, we summarize key attack methods evaluated in our work in the following experiments against adversarial detection methods \citep{papernot2018deep,sotgiu2020deep,xu2017feature}.
\begin{enumerate}
    \item \textbf{FGSM (Fast Gradient Sign Method)} \citep{goodfellow2014explaining} generates adversarial perturbations using the gradient sign of the loss function. It is computationally efficient but often produces suboptimal adversarial examples.
    \item \textbf{PGD (Projected Gradient Descent)} \citep{madry2017towards} iteratively applies FGSM to generate stronger adversarial examples, making it more effective in misleading deep networks.
    \item \textbf{CW (Carlini \& Wagner)} \citep{carlini2017towards} formulates adversarial example generation as an optimization problem, minimizing perturbations while ensuring misclassification.
    \item \textbf{AA (AutoAttack)} \citep{croce2020reliable} is an ensemble of multiple attacks, including Auto-PGD, FAB \citep{croce2020minimally}, and Square Attack \citep{andriushchenko2020square}, designed to identify model vulnerabilities through diverse attack strategies.
\end{enumerate}

For all mentioned attacks, we used two $\epsilon$ values ($16/255$ and $8/255$). Iterative attacks were run for 200 iterations, except for AA \citep{croce2020reliable}, where each iterative attack that is related to it was run for 100 iterations. For the CW attack \citep{carlini2017towards}, we also used $c=0.5$, $\kappa=0$, and a learning rate of $1e-3$.

\subsection{Selecting the subset of auxiliary networks}
To determine the appropriate layer indices of auxiliary networks before conducting each experiment, we calculated $CS^{(avg)}_k$ from \autoref{eq12} on the validation set for each feature level of the auxiliary networks.

We then chose the best \(\mathcal{S}\) layers with the highest $CS^{(avg)}_k$ scores for each model and dataset. \autoref{fig:vis_feats_norm-resnet50} and \autoref{fig:vis_feats_org-resnet50} provide a reduced view comparison of our refined U-CAN features and the original ones for the ResNet-50 backbone. Specifically, for each experiment, we considered all layers starting from a given offset \(s \in \{1, 2, \dots, N\}\), ensuring sufficient separation between classes and effective clustering of similar instances. For each model and dataset, the chosen subset of layers is denoted by \(\{l_k\}^N_s\), where \(\mathcal{N} - s = \mathcal{S}\). The selected layer offset indices are presented in \autoref{tab:layer-offsets}.

\begin{table}[ht!]
\centering
\caption{Chosen $s$ layer offsets for all experiments.}
\label{tab:layer-offsets}
\resizebox{0.3\textwidth}{!}{%
\begin{tabular}{l|l|c}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Layer Offset} \\
\midrule

% ---------- CIFAR-10 -----------
\multirow{3}{*}{CIFAR-10} 
& ResNet-50    & 5 \\
& VGG-16       & 6 \\
& ViT          & 5 \\
\midrule

% ---------- Mammals -----------
\multirow{3}{*}{Mammals}
& ResNet-50    & 7 \\
& VGG-16       & 8 \\
& ViT          & 6 \\
\midrule

% ---------- ImageNet -----------
\multirow{3}{*}{ImageNet}
& ResNet-50    & 3 \\
& VGG-16       & 5 \\
& ViT          & 3 \\
\bottomrule
\end{tabular}%
}
\end{table}

\begin{figure}[ht!]
\centering
% First subfigure (top)
\begin{subfigure}[b]{1\linewidth}
    \centering
    \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm}, 
                     height=7cm, 
                     width=\linewidth]{figures/vis_feats/vis-feats_org-resnet50_imagenet6cls_pgd_r4.pdf}
    \vspace{0.6cm}
    \includegraphics[trim={0.5cm 1.8cm 0.5cm 0.5cm},scale=0.3]{figures/vis_feats/vis_feats_legend.pdf}
    \caption{T-SNE-reduced features \(\{\mathbf{f}_n\}^1_{16}\) from the original ResNet-50 backbone on the ImageNet validation set. 
    The colored points correspond to classes, and the black stars mark layer-wise features of an adversarial example.}
    \label{fig:vis_feats_org-resnet50}
\end{subfigure}
\vspace{1em} % vertical spacing between subfigures

% Second subfigure (bottom)
\begin{subfigure}[b]{1\linewidth}
    \centering
    \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm}, 
                     height=7cm, 
                     width=\linewidth]{figures/vis_feats/vis-feats_norm-resnet50_imagenet6cls_pgd_r4.pdf}
    \vspace{0.6cm}
    \includegraphics[trim={0.5cm 1.8cm 0.5cm 0.5cm},scale=0.3]{figures/vis_feats/vis_feats_legend.pdf}
    \caption{Our contrastive ResNet-50 auxiliary features \(\{\tilde{\mathbf{p}}_n\}^1_{16}\). 
    These refined representations exhibit stronger class separation than the original features, making adversarial perturbations more apparent.}
    \label{fig:vis_feats_norm-resnet50}
\end{subfigure}

% Merged caption for both subfigures
\caption{Layer-wise visualization of (a)~original ResNet-50 features and (b)~contrastive auxiliary features on the ImageNet validation set. 
From top-left to bottom-right, each T-SNE plot shows the layer’s feature clusters (colored points) alongside one adversarial example (black stars). 
The contrastive features exhibit improved separability, aiding the detection of adversarial perturbations.}
\label{fig:vis_feats_merged}
\end{figure}


In this experimental setting, we conducted multiple adversarial detection experiments across the aforementioned architectures and datasets. The following section (\autoref{sec:results}) offers a detailed discussion and analysis of these experiments, highlighting our key insights.

\section{Results}
\label{sec:results}

\paragraph{Experiment 1: Comparative evaluation.}  
In this first experiment, we compared our proposed method to three adversarial detection approaches: DKNN \citep{papernot2018deep}, FS \citep{xu2017feature}, and DNR \citep{sotgiu2020deep}. Our method, referred to as U-CAN+DKNN, stacks the DKNN adversarial detector on top of U-CAN, leveraging its refined features to enhance detection. All compared methods are fully unsupervised, relying solely on benign data. However, they require careful threshold selection to achieve optimal performance. To evaluate these methods, we generated adversarial detection precision-recall (PR) curves for two \(\epsilon\) values (\(8/255\) and \(16/255\)) across all attacks \citep{goodfellow2014explaining,madry2017towards,carlini2017adversarial,croce2020reliable}. The PR graphs illustrate the precision-recall trade-offs for various thresholds determined by fixed percentile steps. \autoref{fig:average_PR} shows the average PR curves for each adversarial detection method, spanning all attacks, models, and datasets.

\begin{figure*}[ht!]
  \centering
  \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm},width=1\textwidth]{figures/PR_graphs/average_PR_eps16.pdf}
  
  \vspace{0.5cm}
  \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm},width=0.8\textwidth]{figures/PR_graphs/legend.pdf}
  
  \vspace{0.5cm}
  \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm},width=1\textwidth]{figures/PR_graphs/average_PR_eps8.pdf}
  
  \caption{Average precision-recall graphs for all methods on all attacks with $\epsilon=16/255$ (top) and $\epsilon=8/255$ (bottom). The thicker point in each graph corresponds with the best F1, and the transparent color is the scaled variance for each graph.}
  \label{fig:average_PR}
\end{figure*}

For each experiment, defined by a specific detection method, attack, model, and dataset, we identified the optimal threshold yielding the highest F1 score. This performance analysis is presented in \autoref{tab:experiment1-results-ep16} and \autoref{tab:experiment1-results-ep8}.

\begin{table*}[ht!]
    \centering
    \caption{Comparative F1-scores of several adversarial detection methods across multiple models and datasets under FGSM, PGD, C\&W, and AA adversarial attacks with $\epsilon=16/255$.}
    \label{tab:experiment1-results-ep16}
    \resizebox{1\textwidth}{!}{%
        \begin{tabular}{l|l|cccc|cccc|cccc|cccc}
            \toprule
            \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Model}} 
            & \multicolumn{4}{c|}{FS} 
            & \multicolumn{4}{c|}{DKNN} 
            & \multicolumn{4}{c|}{DNR} 
            & \multicolumn{4}{c}{\textbf{U-CAN+DKNN (Ours)}} \\
            \cmidrule(lr){3-6}\cmidrule(lr){7-10}\cmidrule(lr){11-14}\cmidrule(lr){15-18}
            & & FGSM & PGD & C\&W & AA 
              & FGSM & PGD & C\&W & AA 
              & FGSM & PGD & C\&W & AA 
              & FGSM & PGD & C\&W & AA \\
            \midrule
            % ---------- CIFAR-10 -----------
            \multirow{3}{*}{CIFAR-10}
            & ResNet-50 
                & 79.1 & 89.7 & 83.7 & 84.3
                & 73.5 & 75.8 & 70.5 & 69.9
                & 66.5 & 66.5 & 67.8 & 67.2
                & 75.1 & 85.5 & 81.2 & 75.3 \\
            & VGG-16
                & 78.8 & 74.9 & 66.4 & 66.5
                & 70.7 & 81.5 & 73.9 & 69.5
                & 69.5 & 75.4 & 75.4 & 77.4
                & 78.6 & 89.3 & 70.9 & 64.5 \\
            & ViT
                & 94.0 & 96.5 & 94.6 & 93.9
                & 88.1 & 89.9 & 86.4 & 86.6
                & 68.0 & 69.0 & 66.6 & 66.5
                & 93.8 & 94.9 & 93.1 & 93.0 \\
            \midrule
            % ---------- Mammals -----------
            \multirow{3}{*}{Mammals}
            & ResNet-50
                & 84.7 & 91.3 & 66.6 & 66.6
                & 73.9 & 89.0 & 75.1 & 74.6
                & 86.3 & 99.0 & 97.3 & 98.0
                & 83.6 & 94.3 & 87.9 & 87.8 \\
            & VGG-16
                & 90.3 & 70.0 & 11.2 & 20.2
                & 78.3 & 78.3 & 76.7 & 72.4
                & 66.9 & 67.6 & 67.4 & 67.4
                & 86.1 & 93.9 & 70.0 & 70.8 \\
            & ViT
                & 82.4 & 98.5 & 86.8 & 84.9
                & 78.1 & 95.8 & 83.0 & 77.6
                & 66.6 & 66.6 & 66.6 & 66.7
                & 81.6 & 97.9 & 88.4 & 86.1 \\
            \midrule
            % ---------- ImageNet -----------
            \multirow{3}{*}{ImageNet}
            & ResNet-50
                & 81.3 & 82.2 & 72.4 & 62.6
                & 77.0 & 92.1 & 84.2 & 79.4
                & 66.7 & 99.4 & 98.8 & 99.4
                & 81.4 & 95.2 & 92.9 & 87.9 \\
            & VGG-16
                & 94.5 & 70.3 & 02.7 & 22.0
                & 81.3 & 92.5 & 86.5 & 83.7
                & 66.7 & 99.1 & 99.1 & 98.6
                & 88.9 & 91.2 & 91.5 & 88.2 \\
            & ViT
                & 87.2 & 99.4 & 98.8 & 98.9
                & 79.7 & 94.5 & 91.9 & 89.3
                & 66.6 & 66.4 & 66.4 & 66.4
                & 78.2 & 97.1 & 96.3 & 94.8 \\
            \midrule
            % ========== AVERAGE ROW ==========
            \multicolumn{2}{c|}{\textbf{Average}} 
              & \textbf{85.8} & 85.9 & 64.8 & 62.5
              & 77.8 & 87.7 & 80.9 & 78.1
              & 69.3 & 78.8 & 78.3 & 78.7
              & 83.0 & \textbf{93.3} & \textbf{85.8} & \textbf{83.2} \\
            \bottomrule
        \end{tabular}%
    }
\end{table*}


\begin{table*}[ht!]
\centering
\caption{Comparative F1-scores with $\epsilon=8/255$.}
\label{tab:experiment1-results-ep8}
\resizebox{1\textwidth}{!}{%
\begin{tabular}{l|l|cccc|cccc|cccc|cccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Model}} 
& \multicolumn{4}{c|}{FS} 
& \multicolumn{4}{c|}{DKNN} 
& \multicolumn{4}{c|}{DNR} 
& \multicolumn{4}{c}{\textbf{U-CAN+DKNN (Ours)}} \\
\cmidrule(lr){3-6}\cmidrule(lr){7-10}\cmidrule(lr){11-14}\cmidrule(lr){15-18}
& & FGSM & PGD & C\&W & AA 
  & FGSM & PGD & C\&W & AA 
  & FGSM & PGD & C\&W & AA 
  & FGSM & PGD & C\&W & AA \\
\midrule

% ---------- CIFAR-10 -----------
\multirow{3}{*}{CIFAR-10}
& ResNet-50 
    & 75.5 & 90.5 & 84.2 & 85.6 
    & 70.9 & 77.8 & 72.1 & 71.1 
    & 66.5 & 66.5 & 67.0 & 67.1 
    & 69.8 & 84.8 & 84.9 & 78.3 \\
& VGG-16
    & 76.4 & 78.7 & 66.5 & 66.5 
    & 73.7 & 81.0 & 73.7 & 72.4 
    & 66.6 & 68.8 & 69.5 & 72.2 
    & 75.1 & 87.8 & 75.9 & 69.4 \\
& ViT
    & 90.3 & 95.2 & 94.3 & 93.9 
    & 81.1 & 88.5 & 86.6 & 84.1 
    & 67.1 & 68.3 & 66.7 & 66.7 
    & 81.1 & 96.2 & 94.0 & 91.4 \\
\midrule

% ---------- Mammals -----------
\multirow{3}{*}{Mammals}
& ResNet-50
    & 85.5 & 94.0 & 66.6 & 66.6 
    & 69.1 & 83.8 & 75.0 & 72.9 
    & 86.3 & 98.8 & 97.3 & 97.3 
    & 81.2 & 91.6 & 83.9 & 86.6 \\
& VGG-16
    & 88.9 & 82.7 & 47.1 & 39.4 
    & 74.3 & 79.8 & 75.1 & 72.3 
    & 67.0 & 70.7 & 68.3 & 67.8 
    & 87.2 & 90.3 & 75.3 & 70.1 \\
& ViT
    & 81.0 & 99.5 & 86.4 & 86.9 
    & 71.1 & 94.1 & 83.1 & 80.2 
    & 66.6 & 66.8 & 66.6 & 66.4 
    & 81.9 & 92.8 & 87.0 & 86.4 \\
\midrule

% ---------- ImageNet -----------
\multirow{3}{*}{ImageNet}
& ResNet-50
    & 82.5 & 94.4 & 77.1 & 72.9 
    & 69.8 & 89.0 & 83.8 & 81.6 
    & 66.4 & 99.4 & 98.9 & 99.4 
    & 74.1 & 93.4 & 93.4 & 88.9 \\
& VGG-16
    & 92.2 & 84.4 & 13.2 & 10.2 
    & 74.6 & 89.5 & 85.6 & 81.9 
    & 66.4 & 98.6 & 98.6 & 98.0 
    & 88.1 & 90.6 & 88.6 & 85.7 \\
& ViT
    & 87.3 & 99.4 & 98.9 & 98.9 
    & 70.9 & 94.1 & 91.6 & 91.8 
    & 67.2 & 66.4 & 66.4 & 66.4 
    & 79.6 & 93.8 & 96.0 & 95.7 \\
\midrule

% ========== AVERAGE ROW ==========
\multicolumn{2}{c|}{\textbf{Average}} 
  & \textbf{84.4}  & 90.9 & 70.5 & 69.0 
  & 72.8  & 86.4 & 80.7 & 78.7 
  & 68.9  & 78.3 & 77.7 & 77.9 
  & 76.9 & \textbf{91.7} & \textbf{86.6} & \textbf{83.6} \\
\bottomrule
\end{tabular}
}
\end{table*}

Our results demonstrate that stacking the DKNN adversarial detector atop our auxiliary networks yields more stable performance, outperforming all other compared methods on average across most attacks, including the original DKNN \citep{papernot2018deep}.

\paragraph{Experiment 2: Enhancing DNR with Our Method.}
In the second experiment, we integrated U-CAN into DNR \citep{sotgiu2020deep} (U-CAN+DNR), which, like DKNN, relies on intermediate-layer features for adversarial detection. We hypothesized that our more clustered features would better suit DNR’s RBF SVMs \citep{cortes1995support}, thereby boosting detection performance.

Results are presented in \autoref{tab:dnr-refined-method}. This experiment was designed to determine whether our method could enhance other adversarial detection techniques that rely on intermediate model features. We also included these results in the PR evaluations \autoref{fig:average_PR}. Integrating our method with DNR yielded higher average F1 scores, confirming the effectiveness of our approach for layer-wise adversarial detection. Notably, both DKNN and DNR showed improved performance when combined with our refined features. Additionally, we show that U-CAN+DNR outperforms U-CAN+DKNN, offering even stronger results.

\begin{table}[ht!]
\centering
\caption{Enhanced DNR performance (F1-scores) under FGSM, PGD, C\&W, and AA attacks 
         with two different attack strengths (\(\epsilon\)).}
\label{tab:dnr-refined-method}
\scalebox{0.8}{%
\begin{tabular}{l l cccc}
\toprule

\multicolumn{6}{c}{\textbf{U-CAN + DNR (Ours)} \quad (\(\epsilon = 16/255\))} \\
\midrule
\textbf{Dataset} & \textbf{Model} & \textbf{FGSM} & \textbf{PGD} & \textbf{C\&W} & \textbf{AA} \\
\midrule
% ---------- CIFAR-10 (16/255) -----------
\multirow{3}{*}{CIFAR-10} 
  & ResNet-50 & 80.0 & 88.5 & 85.4 & 77.4 \\
  & VGG-16    & 77.7 & 90.5 & 74.0 & 80.0 \\
  & ViT       & 89.1 & 97.9 & 95.9 & 96.5 \\
\midrule
% ---------- Mammals (16/255) -----------
\multirow{3}{*}{Mammals}
  & ResNet-50 & 86.6 & 97.6 & 89.5 & 90.5 \\
  & VGG-16    & 89.2 & 94.2 & 79.8 & 75.2 \\
  & ViT       & 86.0 & 99.4 & 93.2 & 94.0 \\
\midrule
% ---------- ImageNet (16/255) -----------
\multirow{3}{*}{ImageNet}
  & ResNet-50 & 80.3 & 96.4 & 93.5 & 91.5 \\
  & VGG-16    & 89.1 & 96.1 & 90.4 & 80.0 \\
  & ViT       & 83.5 & 98.8 & 96.2 & 96.2 \\
\midrule
% ---------- Average row (16/255) ----------
\multicolumn{2}{c}{\textbf{Average}} &
  84.6{\scriptsize\textcolor{Green}{(+1.6)}} &
  95.5{\scriptsize\textcolor{Green}{(+2.2)}} &
  88.6{\scriptsize\textcolor{Green}{(+3.1)}} &
  86.0{\scriptsize\textcolor{Green}{(+2.8)}} \\

% Add an extra line and some spacing here:
\midrule
\addlinespace[20pt]

\toprule
\multicolumn{6}{c}{\textbf{U-CAN + DNR (Ours)} \quad (\(\epsilon = 8/255\))} \\
\midrule
\textbf{Dataset} & \textbf{Model} & \textbf{FGSM} & \textbf{PGD} & \textbf{C\&W} & \textbf{AA} \\
\midrule
% ---------- CIFAR-10 (8/255) -----------
\multirow{3}{*}{CIFAR-10} 
  & ResNet-50 & 69.7 & 85.5 & 84.3 & 77.0 \\
  & VGG-16    & 75.1 & 91.2 & 73.8 & 73.4 \\
  & ViT       & 81.1 & 97.1 & 95.3 & 95.0 \\
\midrule
% ---------- Mammals (8/255) -----------
\multirow{3}{*}{Mammals}
  & ResNet-50 & 80.5 & 96.9 & 89.3 & 89.1 \\
  & VGG-16    & 87.2 & 94.4 & 79.9 & 81.5 \\
  & ViT       & 81.9 & 99.1 & 92.7 & 92.6 \\
\midrule
% ---------- ImageNet (8/255) -----------
\multirow{3}{*}{ImageNet}
  & ResNet-50 & 74.1 & 96.9 & 93.7 & 92.6 \\
  & VGG-16    & 88.1 & 95.6 & 89.9 & 81.5 \\
  & ViT       & 79.6 & 98.9 & 95.9 & 95.4 \\
\midrule
% ---------- Average row (8/255) ----------
\multicolumn{2}{c}{\textbf{Average}} &
  79.7{\scriptsize\textcolor{Green}{(+2.8)}} &
  95.1{\scriptsize\textcolor{Green}{(+3.4)}} &
  88.3{\scriptsize\textcolor{Green}{(+1.7)}} &
  85.8{\scriptsize\textcolor{Green}{(+2.2)}} \\
\bottomrule
\end{tabular}
}% end scalebox
\end{table}
% \begin{table}[ht!]
% \centering
% \caption{Enhanced DNR performance (F1-scores) under FGSM, PGD, C\&W, and AA attacks 
%          with two different attack strengths (\(\epsilon\)).}
% \label{tab:dnr-refined-method}
% \scalebox{0.8}{%
% \begin{tabular}{l l cccc}
% \toprule
% \multicolumn{6}{c}{\textbf{CAN + DNR (Ours)} \quad (\(\epsilon = 8/255\))} \\
% \midrule
% \textbf{Dataset} & \textbf{Model} & \textbf{FGSM} & \textbf{PGD} & \textbf{C\&W} & \textbf{AA} \\
% \midrule
% % ---------- CIFAR-10 (8/255) -----------
% \multirow{3}{*}{CIFAR-10} 
%   & ResNet-50 & 69.7 & 85.5 & 84.3 & 77.0 \\
%   & VGG-16    & 75.1 & 91.2 & 73.8 & 73.4 \\
%   & ViT       & 81.1 & 97.1 & 95.3 & 95.0 \\
% \midrule
% % ---------- Mammals (8/255) -----------
% \multirow{3}{*}{Mammals}
%   & ResNet-50 & 80.5 & 96.9 & 89.3 & 89.1 \\
%   & VGG-16    & 87.2 & 94.4 & 79.9 & 81.5 \\
%   & ViT       & 81.9 & 99.1 & 92.7 & 92.6 \\
% \midrule
% % ---------- ImageNet (8/255) -----------
% \multirow{3}{*}{ImageNet}
%   & ResNet-50 & 74.1 & 96.9 & 93.7 & 92.6 \\
%   & VGG-16    & 88.1 & 95.6 & 89.9 & 81.5 \\
%   & ViT       & 79.6 & 98.9 & 95.9 & 95.4 \\
% \midrule
% % ---------- Average row (8/255) ----------
% \multicolumn{2}{c}{\textbf{Average}} &
%   79.7{\scriptsize\textcolor{Green}{(+2.8)}} &
%   95.1{\scriptsize\textcolor{Green}{(+3.4)}} &
%   88.3{\scriptsize\textcolor{Green}{(+1.7)}} &
%   85.8{\scriptsize\textcolor{Green}{(+2.2)}} \\

% % Add an extra line and some spacing here:
% \midrule
% \addlinespace[20pt]


% \toprule
% \multicolumn{6}{c}{\textbf{CAN + DNR (Ours)} \quad (\(\epsilon = 16/255\))} \\
% \midrule
% \textbf{Dataset} & \textbf{Model} & \textbf{FGSM} & \textbf{PGD} & \textbf{C\&W} & \textbf{AA} \\
% \midrule
% % ---------- CIFAR-10 (16/255) -----------
% \multirow{3}{*}{CIFAR-10} 
%   & ResNet-50 & 80.0 & 88.5 & 85.4 & 77.4 \\
%   & VGG-16    & 77.7 & 90.5 & 74.0 & 80.0 \\
%   & ViT       & 89.1 & 97.9 & 95.9 & 96.5 \\
% \midrule
% % ---------- Mammals (16/255) -----------
% \multirow{3}{*}{Mammals}
%   & ResNet-50 & 86.6 & 97.6 & 89.5 & 90.5 \\
%   & VGG-16    & 89.2 & 94.2 & 79.8 & 75.2 \\
%   & ViT       & 86.0 & 99.4 & 93.2 & 94.0 \\
% \midrule
% % ---------- ImageNet (16/255) -----------
% \multirow{3}{*}{ImageNet}
%   & ResNet-50 & 80.3 & 96.4 & 93.5 & 91.5 \\
%   & VGG-16    & 89.1 & 96.1 & 90.4 & 80.0 \\
%   & ViT       & 83.5 & 98.8 & 96.2 & 96.2 \\
% \midrule
% % ---------- Average row (16/255) ----------
% \multicolumn{2}{c}{\textbf{Average}} &
%   84.6{\scriptsize\textcolor{Green}{(+1.6)}} &
%   95.5{\scriptsize\textcolor{Green}{(+2.2)}} &
%   88.6{\scriptsize\textcolor{Green}{(+3.1)}} &
%   86.0{\scriptsize\textcolor{Green}{(+2.8)}} \\
% \bottomrule
% \end{tabular}
% }% end scalebox
% \end{table}

% RL: Talk more about the refinement of the features, refer to the visualizations
% Our experimental results indicate that the proposed method consistently demonstrates superior stability and efficacy across a range of adversarial attacks. The single exception arises with the FGSM, where FS \citep{xu2017feature} surpasses our approach. We hypothesize that FS is particularly adept at neutralizing the comparatively small, non-iterative perturbations introduced by FGSM. As a one-step adversarial attack, FGSM lacks the iterative refinement characterizing more advanced methods, such as PGD or C\&W. Consequently, it is widely regarded as relatively weak, and its practical importance as a benchmark is diminished. Nonetheless, FS’s effectiveness against FGSM highlights the need for complementary defense strategies that can address even minimally refined attacks and thereby inform the development of more robust adversarial defenses overall. By refining intermediate features, U-CAN also integrates seamlessly with other detection strategies , surpassing the original DKNN and DNR \citep{papernot2018deep,sotgiu2020deep}.
Our experimental results indicate that the proposed method consistently demonstrates superior stability and efficacy across a range of adversarial attacks. The single exception arises with the FGSM, where FS \citep{xu2017feature} outperforms our approach. We believe FS excels at neutralizing FGSM's relatively small perturbations. As a one-step attack, FGSM lacks the iterative refinement of advanced methods like PGD or C\&W, making it relatively weak. In addition, by refining intermediate features, U-CAN also integrates seamlessly with other detection strategies that rely on intermediate representations, surpassing the original DKNN and DNR \citep{papernot2018deep,sotgiu2020deep}.


\section{Conclusions}
\label{sec:conclusions}

We introduced \emph{U-CAN}, an unsupervised adversarial detection framework that combines auxiliary networks with ArcFace-based representations at intermediate layers of a frozen target model. By relying solely on benign data and preserving the original model's structure, U-CAN obviates the need for retraining or adversarial examples. Our experiments on three benchmark datasets and three architectures, demonstrate that mapping intermediate features to hyperspherical embeddings substantially boosts adversarial detection accuracy, even against strong attacks such as PGD and AutoAttack. In particular, integrating these refined embeddings with off-the-shelf layer-wise detectors consistently yields higher F1-scores and greater stability than existing unsupervised baselines. Visualizing the auxiliary features further reveals clear class separations, making small adversarial perturbations more readily detectable. These findings suggest that leveraging intermediate layers through carefully designed auxiliary blocks can significantly enhance adversarial detection in a broad range of safety-critical applications.


\section{Future Work and Limitations}
\label{sec:future_work}
There are several promising directions to refine our proposed method. First, a limitation is that the training procedure is time-consuming despite the relatively few trainable weights. We hypothesize that this is due to the challenge of training all auxiliary networks simultaneously while balancing each other's losses—especially the earlier ones. Future work may explore strategies to accelerate training. Additionally, particularly for datasets with many classes, it would be interesting to investigate whether U-CAN-based adversarial detection can handle attacks targeting the class nearest to the model’s prediction, given that U-CAN may effectively separate even closely related classes. Finally, examining the temporal relationships among refined features could yield further insights for enhancing both adversarial detection and overall model reliability.
In summary, our method lays a solid foundation for adversarial detection but can be strengthened through complementary techniques and a deeper analysis of feature interactions.


\newpage \clearpage
\bibliographystyle{ieeenat_fullname}
\bibliography{refs}
\end{document}
