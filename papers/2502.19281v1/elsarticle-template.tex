\documentclass[preprint,12pt]{elsarticle}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{graphicx}
\newcommand{\etal}{\textit{et al}. }
\usepackage[a4paper, left=2cm, right=2cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{microtype}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\journal{Nuclear Physics B}
\begin{document}
\begin{frontmatter}

\title{Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces}

\author{Jiyuan Wang\textsuperscript{a,b,1},
Weishan Ye\textsuperscript{a,b,1}, Jialin He\textsuperscript{a,b}, Li Zhang\textsuperscript{a,b}, Gan Huang\textsuperscript{a,b},\\ Zhuliang Yu\textsuperscript{c,d}, and Zhen Liang\textsuperscript{a,b,*}} %% Author name

% Author affiliation
\affiliation{organization={The School of Biomedical Engineering},
            addressline={Medical School}, 
            city={Shenzhen University},
            state={Shenzhen},
            country={China}}
\affiliation{organization={The Guangdong Provincial Key Laboratory of Biomedical Measurements and Ultrasound Imaging},
            state={Shenzhen},
            country={China}}
\affiliation{organization={Shien-Ming Wu School of Intelligent Engineering,},
            addressline={South China University of Technology}, 
            state={Guangdong},
            country={China}}
\affiliation{organization={Institute for Super Robotics},
            state={Guangdong},
            country={China}}


% Abstract
\begin{abstract}
With the rapid advancement of deep learning, attention mechanisms have become indispensable in electroencephalography (EEG) signal analysis, significantly enhancing Brain-Computer Interface (BCI) applications. This paper presents a comprehensive review of traditional and Transformer-based attention mechanisms, their embedding strategies, and their applications in EEG-based BCI, with a particular emphasis on multimodal data fusion. By capturing EEG variations across time, frequency, and spatial channels, attention mechanisms improve feature extraction, representation learning, and model robustness. These methods can be broadly categorized into traditional attention mechanisms, which typically integrate with convolutional and recurrent networks, and Transformer-based multi-head self-attention, which excels in capturing long-range dependencies. Beyond single-modality analysis, attention mechanisms also enhance multimodal EEG applications, facilitating effective fusion between EEG and other physiological or sensory data. Finally, we discuss existing challenges and emerging trends in attention-based EEG modeling, highlighting future directions for advancing BCI technology. This review aims to provide valuable insights for researchers seeking to leverage attention mechanisms for improved EEG interpretation and application.
\end{abstract}


\begin{keyword}
EEG, Brain-Computer Interface, Attention Mechanism, Transformer, Multimodal Data Fusion
\end{keyword}

\end{frontmatter}


\section{Introduction}
\label{introduction}
%% Labels are used to cross-reference an item using \ref command.

Research in brain-computer interfaces (BCIs) has long been challenged by the need to process large, complex datasets of brain signals \cite{chen2017brain}. The primary difficulty arises from the diverse and complicated nature of electroencephalography (EEG) signals, which requires efficient and effective strategies for signal analysis and modeling \cite{subha2010eeg}. Attention mechanism-based models have shown exceptional promise in tackling the complexities of EEG signal processing \cite{abibullaev2023deep}. By selectively focusing on critical information within extensive brain signal datasets, Attention models help to minimize irrelevant noise, thereby significantly improving data processing efficiency \cite{de2022attention}. Attention mechanisms not only enhances the effectiveness of BCI research but also introduces greater flexibility and intelligence in the development of models tailored for BCI applications \cite{keutayeva2023exploring}.

Attention mechanisms draw inspiration from biological visual and auditory processes, as well as cognitive processes in psychology \cite{fukui2019attention}. Research has demonstrated that during visual and auditory recognition tasks, humans naturally focus on key elements while suppressing irrelevant information, thereby enhancing the accuracy and efficiency of recognition and decision-making \cite{soydaner2022attention}. Leveraging this principle, attention mechanisms in models, commonly referred to as attention models, are designed to assign flexible weights to different features \cite{lv2022attention}. This enables the model to concentrate on critical information pertinent to the target task while filtering out extraneous data \cite{niu2021review}. Attention models also enhance the understanding of the relationships between input and output data, which in turn improves the model's interpretability \cite{gao2021interpretable}. This enhancement not only maximizes the effective utilization of data but also reduces the impact of data variability caused by individual differences, as the model can learn to prioritize more representative features. In BCI research, these capabilities are particularly valuable, as they help enhance the accuracy of neural decoding, improve the robustness of brain modeling, and enable more adaptive and personalized brain-computer interaction \cite{wang2023mtrt}. Furthermore, attention models are well-suited for multimodal BCI applications, where they facilitate the efficient fusion of features extracted from different modalities \cite{DUAN2024102536}. As a result, the use of attention models in BCI-related research tasks holds significant potential and offers promising avenues for further exploration.


\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{global_picture.pdf}
\end{center}
\caption{The interaction between BCI and attention mechanisms}
\label{fig:global_picture}
\end{figure*}

Attention models have been initially applied extensively in computer vision and natural language processing (NLP) domains, typically integrated as modules within the backbone frameworks of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) \cite{xie2023attention}. In 2014, Mnih \etal \cite{mnih2014recurrent} and Bahdanau \etal \cite{bahdanau2014neural} introduced attention mechanisms in RNNs for image classification and machine translation tasks in NLP, respectively. The introduction of a novel self-attention mechanism by Vaswani \etal \cite{vaswani2017attention} in 2017, through the "Transformer" model architecture for machine translation tasks, further propelled the use of attention models. Since then, Transformer models and their variants have been applied across various tasks \cite{islam2023comprehensive}. For example, Dosovitskiy \etal proposed the Vision Transformer \cite{dosovitskiy2020image}, demonstrating that a pure Transformer architecture could be effective for computer vision tasks without relying on CNN modules. Additionally, Liu \etal introduced the Swin Transformer \cite{liu2021swin}, which utilizes a windowed self-attention mechanism to reduce the computational complexity of Transformer models. Nowadays, attention mechanisms are foundational to modern deep learning, with their flexibility and efficacy continuing to revolutionize artificial intelligence research and applications.

\begin{figure*}
\begin{center}
\includegraphics[width=1\textwidth]{number_of_papers.pdf}
\end{center}
\caption{The number of papers retrieved from Google Scholar for the two keyword combinations.}
\label{fig:papers}
\end{figure*}

Building on significant breakthroughs in computer vision and NLP, attention models have also attracted substantial interest in the BCI field, catalyzing rapid progress in integrating EEG signal processing with attention mechanisms. To assess the growing interest in this area, we conduct a literature search using Google Scholar to track the number of publications since 2019. The search is performed using two keyword combinations: (1) "attention"+"EEG"+"deep learning", and (2) "attention"+"EEG"+"Transformer"
+"deep learning".The searching results are presented in Fig. \ref{fig:papers}, showing the number of papers retrieved from Google Scholar for each of the two keyword combinations. In the BCI domain, attention mechanism modeling generally falls into two categories.  \textbf{(1) Traditional Attention Mechanism-Based Modeling.} It calculates attention weights for various types of information in EEG signals, such as spatial, temporal, and spectral features, prioritizing those most relevant to the task. \textbf{(2) Transformer-Based Multi-Head Self-Attention Modeling.} It employs multiple attention heads to simultaneously focus on different parts of the EEG data, enabling the model to capture both global and local relationships across various dimensions \cite{chen2022exploring}. Furthermore, extending these two modeling strategies to multimodal applications significantly enhances the model's ability to process and integrate information from different modalities \cite{vortmann2022multimodal}. Such integration is particularly critical for developing efficient and accurate BCI systems, as it allows for a more comprehensive understanding of the user's intentions and mental state.

The following sections provide a comprehensive exploration of the applications of attention models in BCIs, focusing on their role in enhancing the understanding of EEG signals and advancing BCI technology. Section 2 introduces the concept of traditional attention mechanisms and categorizes their specific applications in EEG signal modeling. Section 3 details EEG signal modeling methods based on Transformer multi-head self-attention mechanisms. Given the growing popularity of multimodal models, Section 4 discusses the application of attention models in multimodal contexts. Finally, Section 5 summarizes the key points of this work and provides future perspectives on the use of attention mechanisms in EEG signal modeling.

\section{Traditional Attention Mechanisms in EEG}
%% Use \subsection commands to start a subsection.
Traditional attention mechanism-based modeling enhances performance and generalization by efficiently selecting features through adaptive weighting and combining different types of information. Given input data, attention modeling dynamically computes feature weights based on prior knowledge or task-specific requirements.  Depending on how these weights are applied, attention mechanisms can be broadly categorized into soft and hard attention. In the soft attention mechanism, each feature is assigned a weight that is continuously distributed between 0 and 1. These weights are differentiable, which allows them to be optimized through continuous learning within the network model \cite{shen2018reinforced} Compared to hard attention, where features are either entirely selected or ignored, soft attention provides a more refined weighting approach, enabling the model to learn the relative importance of features more effectively. This leads to smoother gradient flow during backpropagation, contributing to more stable and efficient training \cite{lu2023multi}. In contrast, hard attention assigns non-differentiable weights, which cannot be optimized through conventional deep learning techniques \cite{chen2021deep} Due to these limitations, hard attention is challenging to integrate directly with traditional deep learning models. Therefore, this paper will not cover or summarize research work related to hard attention mechanisms.

Building on this foundational understanding of attention mechanisms, it is crucial to explore their implementation in practical scenarios, particularly with EEG data. Attention modules can vary significantly depending on their scope and integration into the model, and analyzing these variations provides a more comprehensive understanding of their impact. To do so, we will focus on two critical aspects of attention module implementation: the specific types of attention modules used and their methods of embedding within broader model architectures.




\subsection{Types of Attention Modules}
In brain modeling tasks, attention mechanisms enhance feature extraction from EEG signals across channel, temporal, and frequency dimensions by assigning weights to highlight the most relevant information, as shown in Fig. \ref{fig:TypeAtt}.

\begin{figure*}[h]
\centering
\includegraphics[width=1\textwidth]{combine.pdf}
\caption{Types of attention modules in traditional attention mechanism-based modeling. The attention weights are defined as $W_c = F_c(f_c(X_c))$ for channel attention, $W_t = F_t(f_t(X_t))$ for temporal attention, and $W_f = F_f(f_f(X_f))$ for frequency attention. Here, $f(\cdot)$ represents a transformation applied by the model to the input, and $F_n(\cdot)$ denotes a normalization function, commonly implemented as the softmax function in practical applications.}
\label{fig:TypeAtt}
\end{figure*}

\textbf{(1) Channel Attention Module.} The channel attention module is designed to assess and adaptively weight the importance of each EEG channel. Given that different brain regions contribute unequally to various tasks, the channel attention module enables the model to prioritize channels with the most relevant information while minimizing the impact of less informative channels, thereby improving task-specific analysis and reducing noise.

For a given multi-channel EEG signal vector $X_c \in \mathbb{R}^{1 \times C}$, where $C$ denotes the number of channels, an attention weight vector $W_c \in \mathbb{R}^{1 \times C}$ is initialized. The model calculates a weighted combination of $X_c$ and $W_c$, followed by the application of the softmax function to produce the output $\hat{X}_c$ from the channel attention module, as shown below:
\begin{equation}
\hat{X}_c= \text{softmax}(X_c \odot W_c) 
\end{equation}
In BCI applications, the channel attention module effectively identifies the most relevant brain regions for specific tasks, such as motor imagery or emotional state classification \cite{wu2023classification, chen2021multiattention, tao2020eeg, liu20213dcann}. By concentrating on channels linked to critical brain functions, this module improves feature extraction, leading to enhanced performance in BCI systems and other brain modeling applications.

\textbf{(2) Temporal Attention Module.} The temporal attention module is designed to effectively capture the dynamic fluctuations in brain activity that occur over time, acknowledging that EEG signals reflect varying brain states depending on the nature of a given task. During task execution, brain activity often exhibits multiple phases of relevance, with some fluctuations directly related to the task while others contribute less meaningful information. By assigning higher attention weights to time periods that are directly correlated with the task at hand, the module enhances the model's ability to extract task-relevant features and reduce the influence of irrelevant temporal variations.

For a given EEG signal vector $X_t \in \mathbb{R}^{1 \times T}$, where $T$ represents the temporal dimension, an attention weight vector $W_t \in \mathbb{R}^{1 \times T}$ is initialized. The model then calculates a weighted combination of $X_t$ and $W_t$, followed by the application of the softmax function to yield the temporal attention output $\hat{X}_t$, as shown below:

\begin{equation}
\hat{X}_t= \text{softmax}(X_t \odot W_t) 
\end{equation}
In BCI applications, the temporal attention module effectively captures relevant patterns of brain activity during cognitive or emotional tasks \cite{zhang2022eeg, jia2020sst}. For example, in motor imagery tasks, it highlights critical moments by identifying peaks in attention at significant time intervals \cite{zhang2020motor, ma2022time}. By concentrating on time segments that are closely linked to the task, the temporal attention module ensures that the model accurately captures the dynamic characteristics of EEG signals, leading to improved performance in analyzing time-series brain activity data and enhancing the robustness of BCI systems.

\textbf{(3) Frequency Attention Module.} The frequency attention module is designed to leverage the significance of frequency components in EEG signals, as frequency-domain analysis reveals crucial insights into neural activities across different scales. Features such as Power Spectral Density (PSD) and Differential Entropy (DE) are valuable indicators of brain activity, and the frequency attention module adaptively highlights these key features by assigning weights to various frequency components within the signal.

For a frequency vector $X_f \in \mathbb{R}^{1 \times F}$, where $F$ represents the frequency dimension, an attention weight vector $W_f \in \mathbb{R}^{1 \times F}$ is initialized. The model computes a weighted combination of $X_f$ and $W_f$, and applies the softmax function to derive the output $\hat{X}_f$ from the frequency attention module, as shown below:

\begin{equation}
\hat{X}_f= \text{softmax}(X_f \odot W_f) 
\end{equation}
In BCI applications, the frequency attention module effectively identifies and enhances the most informative frequency components, such as Alpha, Beta, and Gamma bands, which are known to be critical in tasks like attention monitoring, motor imagery, or emotional state classification \cite{cai2021eeg, xiao20224d}. By adaptively assigning higher weights to these significant frequency bands, the module ensures that the model focuses on the most relevant aspects of the EEG data, ultimately enhancing the feature extraction process. This selective attention to key spectral features contributes to improved classification accuracy and robustness in BCI systems, thereby advancing the reliability of EEG-based analyses.

The aforementioned mentioned attention modules, including channel, temporal, and frequency, can be used individually or in combination based on the specific demands of the task. The selection and integration of these attention modules are determined by the unique characteristics of the problem being addressed. By strategically choosing the appropriate combination of attention mechanisms, researchers can effectively customize EEG signal processing and model design, providing a more adaptable and efficient solution tailored to the task’s requirements.

\subsection{Embedding Methods of Attention Modules}

\begin{figure*}
\centering
\includegraphics[width=0.9\textwidth]{embedding.pdf}
\caption{Embedding Methods of Attention Modules}
\label{fig:embedding}
\end{figure*}

In embedding attention modules, two primary approaches are employed: single attention module embedding and multi-attention module integration. The single attention module approach thoroughly explores the internal dynamics, efficacy, and performance of a particular attention module across diverse application scenarios, providing insights into how it influences model learning and performance \cite{lv2022attention}. This allows researchers to tailor optimization strategies for specific tasks and datasets. In contrast, the multi-attention module approach integrates multiple attention modules, leveraging their complementary strengths to handle complex data more effectively \cite{fukui2019attention}. This integration enhances the model's generalization capabilities and facilitates a deeper understanding of how different attention mechanisms interact and contribute to information extraction, as illustrated in Fig. \ref{fig:embedding}.

\textbf{(1) Single Attention Module Embedding.} In BCI modeling, embedding a single attention mechanism is widely used approach to enhance a model's ability to focus on critical features. The key idea is to emphasize specific information dimensions (channel or temporal or frequency) by leveraging a particular type of attention mechanism, thereby improving feature extraction and recognition efficiency.

For example, embedding a channel attention module specifically enhances the model’s ability to capture important features at the channel level. Du \etal proposed the ATDD-LSTM model, which combined a channel attention module with a long short-term memory (LSTM) network. In this approach, the channel attention module was applied to feature vectors extracted by the LSTM layers, allowing the model to concentrate on channels most relevant to specific emotions while downplaying less relevant ones, which improved the accuracy of emotion recognition \cite{du2020efficient}. Inspired from this, Xu \etal integrated channel attention into a graph convolutional network (GCN), taking into account the spatial relationships between EEG recording electrodes \cite{xu2023dagam}. Beyond channel attention, some studies focus on embedding temporal attention module to emphasize the temporal dynamics of EEG signals. Zhang \etal proposed a convolutional recurrent attention model that used CNNs to encode high-level representations and combined them with recurrent attention mechanisms (including LSTM networks and temporal attention modules). This method calculated attention weights on dynamic temporal features, allowed the model to focus on the most informative time periods, and extracted more valuable temporal information \cite{zhang2019convolutional}. Inspired by the psychological peak-end rule, Kim \etal developed a model that integrated a bidirectional LSTM network with a temporal attention module. It assigned greater weight to emotional states occurring at key moments, capturing the dynamic variability of emotions over time and enhancing the model's interpretability \cite{kim2020eeg}. For frequency-domain features, frequency attention is rarely modeled in isolation. Instead, it is typically integrated with spatial and temporal features to enhance EEG signal representation.

While embedding a single attention module can effectively improve performance in specific tasks, several challenges remain. Choosing the right attention module is crucial. Determining how to integrate it optimally into the model framework for different scenarios is also important. Additionally, understanding how the placement of the attention layer affects model performance requires further exploration. Researchers need to carefully assess both the model architecture and the task requirements to design an optimal embedding strategy.

\textbf{(2) Multi-Attention Module Integration.} Embedding multiple attention modules helps overcome the limitations of single attention module embedding by enabling the model to simultaneously capture various aspects of different feature dimensions. This approach enhances the model’s capacity to learn diverse and informative features, thereby improving its robustness and generalization capabilities. Consequently, transitioning from single to multiple attention embedding is a natural step to better manage the complexity of real-world EEG data.

For example, Tao \etal proposed a deep learning model that incorporated both channel attention and inter-sample attention mechanisms. Since the samples were segmented based on time, the inter-sample attention effectively functioned as temporal attention. This approach enabled the model to effectively prioritize significant information across different channels and temporal segments for feature extraction \cite{tao2020eeg}. Extending beyond channel and temporal information, Jia \etal introduced a spatio-temporal-spectral attention dense network that simultaneously considered temporal, frequency, and spatial features. This model adaptively captured crucial information across brain regions (spatial, i.e., channels), frequency bands (spectral), and time, providing a comprehensive feature extraction framework \cite{jia2020sst}. Xiao \etal extended Jia et al.'s model by proposing a neural network based on 4D attention \cite{xiao20224d}. In this approach, the channel dimension of the input samples is transformed into a two-dimensional feature to preserve the spatial positional information of the EEG signal electrodes, while also incorporating the time and frequency dimensions. It computed spatial attention (addressing the spatial positional relationships between channels) and frequency attention (applied to power spectral density and differential entropy features). These attention weights were then applied to refine the input, resulting in enhanced output features. Jia \etal's and Xiao \etal's models both leveraged temporal, frequency, and spatial characteristics of EEG channels, calculating attention weights across these dimensions to enhance the model’s focus on information pertinent to specific tasks. The main difference between their approaches lies in how they calculated attention weights and structured their models. Jia \etal computed attention separately across the frequency and time dimensions in parallel, and then merged these features for classification. In contrast, Xiao \etal integrated all dimensions into a unified 4D representation before computing attention, providing a different approach to capture feature interdependencies.

Besides of the aforementioned methods, Cai \etal introduced a dynamic attention mechanism that assigned different weights to different frequency sub-bands and channels of EEG signals \cite{cai2021eeg}. This dynamic approach optimized feature representation and was applied within an adaptive decoding framework for complex downstream tasks. A crucial aspect of applying attention mechanisms is defining the shape and dimensions of the input feature matrix. In previous research, attention weight computations have predominantly relied on the strict Euclidean geometric space of the feature matrix. However, given the brain’s complex topological structure, using Euclidean space alone may not accurately capture its underlying properties. Consequently, several studies have sought to align feature matrix definitions more precisely with the brain’s physiological structure by incorporating non-Euclidean space representations within attention mechanisms. For example, Zhang \etal introduced the concept of manifolds, proposing a time-frequency domain feature learning model that integrated both Riemannian manifold and Euclidean space representations. Their work demonstrated the effectiveness of attention mechanisms in synthesizing feature information across different mathematical domains \cite{zhang2020spatio}. Additionally, to better capture the spatial relationships among EEG electrode channels, Jia \etal's GraphSleepNet \cite{jia2020graphsleepnet} and Zhang \etal's hierarchical attention network based on graph structures \cite{zhang2019graph} both utilized GCNs to model the spatial relationships of EEG electrodes. These approaches leveraged attention mechanisms across both time and space, significantly enhancing performance in tasks such as sleep stage classification and movement intention prediction.

These studies collectively highlight the crucial role of attention mechanisms in enhancing the performance of EEG signal processing models, especially in terms of accuracy and efficiency of feature extraction. By calculating attention weights across multiple dimensions, such as channel, temporal, and frequency, and either integrating or applying them independently, these models offer more refined and effective solutions to address the complexities of EEG signal processing.The relevant literature we have reviewed is summarized in Table \ref{tab:table11}.



\begin{table*}[]
\centering
\caption{Embedding methods of attention modules in the literature.}
\label{tab:table11}
\resizebox{1\textwidth}{!}{
\fontsize{9}{12}\selectfont
\begin{tabular}{lccccc}
\hline
Ref                                          & year & embedding style         & backbone & task                                      & dataset                                 \\ \hline
\cite{du2020efficient}           & 2020 & Channel           & LSTM     & emotion recognition                       & DEAP,SEED,CMEED      \\
\cite{xu2023dagam}               & 2023 &  Channel          &GCN       & emotion recognition                       & SEED,SEEDIV          \\
\cite{tao2020eeg}           & 2020 & Channel,Time           & RNN      & emotion recognition                       & DEAP,DREAMER         \\
\cite{zhang2019convolutional} &2019 &Time &CNN,LSTM & subject-independent movement intention recognition & BCI competition IV dataset 2a        \\
\cite{kim2020eeg}   &2020 &temporal  &LSTM,CNN   &emotion recognition  &DEAP \\
\cite{jia2020sst}           & 2020 & Channel,Time,Frequency & CNN      & emotion recognition                       & SEED,SEEDIV          \\
\cite{xiao20224d}           & 2022 & Channel,Time,Frequency & LSTM,CNN & emotion recognition                       & DEAP,SEED,SEEDIV     \\
\cite{cai2021eeg}           & 2021 & Frequency,Channel      & CNN      & AAD                                       & KUL,DTU              \\
\cite{zhang2020spatio}      & 2020 & Time,Frequency         & LSTM,CNN & emotion recognition                       & \makecell[c]{SEED-VIG,SEED,\\ BCI-IV 2A,BCI-IV 2B} \\
\cite{jia2020graphsleepnet} & 2020 & Time,Channel           & GCN      & Sleep stage recognition                   & MASS-SS                                 \\
\cite{zhang2019graph}       & 2019 & Channel,Time           & CNN,GCN  & left/right fist open and close intentions & PhysioNet dataset                       \\ \hline
\end{tabular}
}
\end{table*}

\section{Transformer-based Multi-Head Self-Attention Mechanisms in EEG}

\begin{table*}[]
\centering
\caption{Transformer-based embedding methods in the literature.}
\label{tab:table12}
\resizebox{1\textwidth}{!}{
\begin{tabular}{lccccc}
\hline
Ref                                          & year & embedding style         & backbone & task                                      & dataset                                 \\ \hline
\cite{li2022eeg}           & 2022 & Frequncy,Time           & Transformer      & seizure prediction                       & CHB-MIT,Kaggle                            \\
\cite{qu2020residual}           & 2020 & Time & Transformer      & Sleep stage recognition                     & MASS,Sleep-EDF                             \\
\cite{guo2022transformer}           & 2022 & Channel & Transformer & emotion recognition                       & SEED                       \\
\cite{zheng2022copula}           & 2022 & Time,Channel      & Transformer     & Visual discomfort induces                 & private datasets                                 \\
\cite{song2021transformer}      & 2021 & Time,Channel         & Transformer & Motor imagery                       & BCI-IV 2A,BCI-IV 2B \\
\cite{du2022eeg} & 2022 & Time,Channel           & Transforemr      & Motor imagery                   & PhysioNet                                 \\
\cite{si2023temporal}       & 2023 & Channel,Time           & Transformer  & emotion recognition & DEEP,SEED,THU-EP                       \\ \hline
\end{tabular}}
\end{table*}

The Transformer model, first proposed by Vaswani \etal in 2017 \cite{vaswani2017attention}, introduced a self-attention mechanism that revolutionized machine translation tasks. The Transformer architecture is composed of encoder and decoder modules, which are built by stacking multiple sub-layers, including self-attention, feed-forward neural networks, residual connections, and normalization layers \cite{vaswani2017attention,devlin2018bert}. This composition enables the model to effectively encode input sequences and generate outputs, with the self-attention mechanism at its core enhancing the ability to capture contextual and temporal relationships \cite{kobayashi2020attention}.

Since then, the Transformer and its variants have seen widespread application across fields such as natural language processing, and computer vision \cite{lin2022survey}. The core strength of the Transformer lies in its capacity to capture long-range dependencies and interactions among input features, making it particularly effective for time series modeling and also achieving significant advancements in temporal signal processing task. Table \ref{tab:table12} summarizes a survey of studies that employ the transformer architecture. For example, AST \cite{wu2020adversarial} utilized a generative adversarial encoder-decoder framework to train a sparse Transformer model for time series prediction. It demonstrates that adversarial training can enhance time series prediction by directly shaping the network's output distribution to mitigate error accumulation during one-step-ahead inference. FEDformer \cite{zhou2022fedformer} applied attention mechanisms in the frequency domain using Fourier and wavelet transforms, achieving linear complexity by randomly selecting a fixed-size subset of frequencies. It is noted that FEDformer's success has spurred increased interest in exploring the self-attention mechanism in the frequency domain for time series modeling \cite{wen2022transformers}.

\subsection{Multi-Head Self-Attention Mechanisms}

The self-attention mechanism, also known as "Scaled Dot-Product Attention" \cite{xu2023multimodal}, offers a significant advantage over traditional attention models. It captures contextual relationships effectively, especially in long sequences. This helps overcome challenges like information loss and long-term dependencies. Self-attention analyzes correlations between positions in the input sequence, making it more efficient than CNNs and RNNs for sequence modeling.

In the Transformer model, self-attention is implemented using three matrices: Query ($Q$), Key ($K$), and Value ($V$). These matrices are derived from the input feature matrix $I \in \mathbb{R}^{L \times D}$, where $L$ is the sequence length and $D$ is the feature dimension. The matrices $Q$, $K$, and $V$ are obtained by applying linear transformations to $I$:

\begin{equation}
Q = I \otimes W^Q,
\end{equation}
\begin{equation}
K = I \otimes W^K,
\end{equation}
\begin{equation}
V = I \otimes W^V,
\end{equation}
where $W^Q \in \mathbb{R}^{{D \times D_k}}$, $W^K \in \mathbb{R}^{{D \times D_k}}$, and $W^V \in \mathbb{R}^{{D \times D_v}}$ are trainable weight matrices. The self-attention layer then calculates attention weights and the output:
\begin{equation}
A = \text{softmax}(\frac{Q K^T}{\sqrt{D_k}}).
\end{equation}
\begin{equation}
\text{Attention}(Q,K,V) = A  V.
\end{equation}

To allow multiple self-attention processes to run in parallel, the multi-head attention mechanisms is suggested. For $H$ self-attention heads, the output of the multi-head attention is:
\begin{equation}
\text{MultiHeadAttn}(Q,K,V) = \text{Concat}(head_1,\ldots,head_H)W^O,
\end{equation}
where $W^O$ is a trainable weight matrix. This ensures the output size matches the input. The multi-head approach allows the model to focus on different parts of the sequence simultaneously. This enriches the representation and improves model performance.

\begin{figure*}[h]
	\centering
	  \includegraphics[width=1\textwidth]{./multi-head_self-attention.pdf}
	\caption{Multi-Head Self-Attention Module}
 \label{fig:multi-head}
\end{figure*}

\subsection{Strategies for Applying Transformers}

In practical applications, the complete Transformer architecture is not always necessary for every task. Many models modify specific Transformer components or integrate elements into existing frameworks. Broadly, Transformer applications can be categorized into three main strategies \cite{lin2022survey}.

\textbf{(1) Encoder-Decoder Combination.} This strategy suits sequence-to-sequence tasks, where the input sequence is processed by an encoder and then decoded into a target sequence. This approach addresses long-range dependencies by fully leveraging contextual information. While this method increases model complexity and requires more parameters, longer training times, and larger datasets, it generally yields improved performance.

\textbf{(2) Encoder Only.} Used for non-sequence-to-sequence tasks, this strategy employs only the encoder to convert the input sequence into a specialized representation for subsequent processing. By simplifying the model structure and reducing parameters and training time, it provides a more efficient approach. However, the encoded representation may lack the depth of contextual information needed for complex sequence generation tasks.

\textbf{(3) Decoder Only.} Typically paired with a pre-trained encoder module, this strategy is ideal for generative tasks. The decoder, utilizing self-attention, generates the target sequence based on the representations from upstream tasks. This setup captures comprehensive contextual information, though it may increase time complexity.

In BCI applications, the core benefit of Transformers lies in the self-attention mechanism, essential for capturing temporal correlations and performing effective feature encoding. Consequently, BCI applications frequently implement either the encoder or encoder-decoder strategy, with self-attention as a pivotal component for handling EEG-based tasks.

\subsection{Practical Applications of Transformer Models in EEG Analysis}
The use of Transformer-based self-attention mechanisms shows great potential for enhancing EEG modeling. These mechanisms help capture relevant information from complex, non-stationary EEG signals. We introduce EEG modeling with Transformers in three areas: temporal, spatial, and combined temporal-spatial.

\textbf{(1) Application in Temporal Dimension.}
In the temporal dimension, researchers have successfully leveraged CNN-Transformer models to extract valuable temporal information from EEG signals \cite{xie2022transformer}. Enhancing the self-attention layer has enabled more effective capture of EEG patterns and detection of neural activity changes over time. For example, Li \textit{et al.} \cite{li2022eeg} integrated CNN and Transformer architectures to adapt the self-attention mechanism to EEG-specific features. Unlike traditional approaches, their model segmented EEG signals into 30-second sequence samples and then generated time-frequency spectrograms by Short-Time Fourier Transform. These time-frequency spectrograms undergo preliminary feature extraction through multiple convolutional layers before entering an optimized Transformer module. Transformer module reduces the dimensions of the Query and Key matrices in the self-attention layer, lowering model complexity. Additionally, bias terms \cite{liu2021swin} were introduced to enhance positional information, further improving the model's ability to handle complex temporal details in EEG signals. Similarly, Qu \etal adopted 30-second signal sequences as the model's core units for sleep stage classification \cite{qu2020residual}. They implemented an encoder-decoder structure, beginning with a feature extraction layer combining convolution and pooling layers for initial preprocessing, followed by an encoder with two residual blocks to encode each signal segment. In the decoder, a Transformer-based self-attention mechanism was combined with fully connected layers to capture correlations across temporal sequence samples. The decoder processes these 30-sample subsequences by applying positional encoding to each sample before feeding it through the self-attention layer. The output then moves through two fully connected layers with normalization and dropout, culminating in sleep stage predictions via a softmax activation function. These studies demonstrate that Transformer-based self-attention mechanism can effectively extract dynamic changes in time-series EEG data, improving model efficiency and training speed through parallel computation.

\textbf{(2) Application in Spatial Dimension.}
Inspired by the Vision Transformer (ViT) model for image processing, researchers have developed innovative neural network models tailored for spatial feature analysis in EEG signals. For example, Guo \etal proposed a model called the Deep Convolutional and Transformer Network (DCoT) \cite{guo2022transformer}, focusing on the importance of each EEG channel in emotion recognition and visualizing these features. Based on the extracted differential entropy features, EEG signals were formed in a three-dimensional representation (Time $\times$ Channel $\times$ Frequency), which is similar to RGB representation in image. Inspired from ViT structure, DCoT was designed to analyze correlations between EEG electrodes, with each Transformer token representing a specific EEG electrode channel. The model applied positional encoding to the input tokens (i.e., EEG channels) and introduced an additional token dedicated to classification. After processing the encoded signals through the Transformer encoder module, a fully connected layer generated the final classification results. Furthermore, Zheng \etal proposed a Copula-Based Transformer model (CBT) \cite{zheng2022copula}, designed to learn spatial dependencies between EEG channels while optimizing classification performance. By reducing the size of the attention matrix, CBT lowered dependency on large datasets, improving computational efficiency. The CBT model excelled in an EEG-based visual discomfort assessment task, pioneering the revelation of temporal characteristics in EEG signals linked to visual discomfort. These studies underscore that Transformer-based self-attention mechanisms effectively capture the spatial dynamics of EEG signal sequences, enhancing spatial information processing and optimizing model training efficiency and accuracy.

\textbf{(3) Application in Combined Temporal and Spatial Dimensions.}

Researchers have leveraged the powerful capabilities of the Transformer self-attention mechanism to simultaneously capture both the temporal and spatial dimensions of EEG signals, enabling the extraction of highly discriminative features. For example, Song \etal \cite{song2021transformer} developed an EEG decoding model that primarily relies on the Transformer’s self-attention layers to enhance feature representations in both dimensions. In the spatial transformation component, the self-attention mechanism weighted each channel, emphasizing signals with higher relevance. Meanwhile, the temporal transformation component employed convolutional layers to encode temporal features, with channel compression and sample segmentation steps to reduce computational load. Finally, global average pooling and fully connected layers were combined to classify EEG signals effectively. Similarly, Du \etal proposed an EEG spatio-temporal Transformer network \cite{du2022eeg}, which introduced Temporal Transformer Encoder (TTE) and Spatial Transformer Encoder (STE) modules to independently capture temporal and spatial features. In the TTE, temporal attention mechanisms calculated correlations among sampling points within each sample, extracting temporal features. Since channel correlations are often unique to individuals, the STE calculated spatial attention among channels and applies positional encoding to retain spatial location information. This allows the model to capture inter-channel relationships more accurately, improving its ability to identify individuals based on unique EEG patterns. Additionally, Si \etal \cite{si2023temporal} proposed integrating the Selective Kernel (SK) attention mechanism with the Transformer self-attention mechanism to identify and select the channels most relevant to the current task. This approach enables the model to filter out noise and irrelevant signals, simplifying the processing of complex temporal information. Once the SK attention mechanism isolated key channels, the Transformer encoder module deeply extracted temporal features from these selected channels. This approach's strength lies in its ability to focus on the most informative parts of the data while preserving sensitivity to temporal dependencies, a critical factor for managing complex tasks in BCIs. These studies demonstrate that an incorporation of both temporal and spatial information can significantly enhance the accuracy and efficiency of EEG signal analysis.

\section{Attention Models for Multimodal Applications}
To improve the accuracy of EEG signal recognition, recent studies have gradually introduced multimodal data (such as speech, images, text, etc.) to be jointly trained with EEG signals (as shown in Fig. \ref{fig:multi-model}). By leveraging traditional attention mechanism modeling strategies or Transformer-based multi-head self-attention mechanism modeling strategies, these approaches achieve effective fusion of signals from different modalities, enhancing the accuracy and stability of recognition. As multimodal applications in brain modeling continue to expand, the ability to efficiently utilize information from each modality becomes increasingly critical.

At the current stage, multimodal tasks face two core challenges: (1) effectively fusing multimodal data and (2) facilitating better interaction between different modalities. The key to the first challenge lies in assessing the importance of each modality for a given task and assigning appropriate weights to generate a unified feature representation for downstream processing. The selection of these weights significantly impacts model performance. Traditionally, this process relies on manual tuning, which is both time-consuming and suboptimal. In contrast, automated weight optimization not only improves efficiency but also enhances model performance. This is typically achieved by incorporating attention models that introduce an attention parameter matrix to dynamically adjust weight distribution. Attention modeling can be broadly categorized into traditional attention mechanisms and Transformer-based self-attention strategies. The latter emphasizes capturing complementary and shared information between modalities, ensuring that the information from one modality is reflected in another. For example, in cross-modal generation tasks using diffusion models, cross-attention mechanisms are often employed to facilitate information exchange and allocate importance between modalities.

\begin{figure}[h]
	\centering
	  \includegraphics[width=0.45\textwidth]{multimodal.pdf}
	\caption{Attention Modules for Multimodal Applications}
 \label{fig:multi-model}
\end{figure}

The second challenge focuses on improving interactions between different modalities, ensuring seamless information exchange across data sources. A major difficulty arises from the heterogeneous nature of multimodal data, where differences in feature distributions, temporal alignment, and semantic relevance must be addressed. To tackle this, recent studies have introduced contrastive learning \cite{radford2021learning}, cross-modal attention \cite{rombach2022high}, and graph-based approaches \cite{ding2023mst} to establish meaningful associations between modalities. These methods enhance feature complementarity and reduce modality gaps, ultimately improving model robustness and generalization. Furthermore, explicit modeling of inter-modality dependencies through techniques such as mutual information maximization \cite{cao2024predictive} and self-supervised learning strengthens interactions \cite{wei2023multi}, allowing each modality to contribute more effectively to the overall task performance.

\subsection{Application of Attention Mechanisms in the Fusion of EEG Signals with Multimodal Data}

In multimodal emotion recognition tasks, Liu \etal employed two modality fusion strategies: a basic weighted fusion method and an attention-based fusion method \cite{liu2021comparing}. In the basic weighted fusion approach, the weight coefficients for each modality were manually adjusted, and the weighted sum of data from different modalities was computed to generate a fused output. The fusion process was represented as:
\begin{equation}
     O = \alpha_1O_1+\alpha_2O_2.
\end{equation}
Here, $\alpha_1$ and $\alpha_2$ represented the weight coefficients assigned to different modalities, satisfying $\alpha_1 + \alpha_2 = 1$. The model’s performance was thoroughly evaluated by manually testing various combinations of $\alpha_1$ and $\alpha_2$. In the attention-based fusion method, although a weighted summation formula was still used to obtain the fused feature $O$, in solving for $\alpha_1$ and $\alpha_2$, the authors first initialized an attention layer, which was then optimized through the network. The final attention weights, $\alpha_1$ and $\alpha_2$, were derived from the softmax function applied to the attention layer’s output. This approach allows the model to adaptively adjust the attention weights, dynamically learning an optimal set of weight parameters.

On the other hand, Qiu \etal developed a correlation attention network that effectively integrated eye movement and EEG signals for fusion analysis. The unique aspect of the correlation attention network is that it not only computed attention weights for the two modalities but also adopted the canonical correlation analysis (CCA) method from statistics. This analysis method generated a cross-correlation coefficient matrix $C$ by calculating the cross-correlation between outputs of each recurrent unit in the bidirectional gated recurrent unit (GRU) layer. Utilizing this matrix along with the average features extracted by the bidirectional GRU, the fusion feature matrix $F$ was constructed to determine the attention weights. The computation followed a structured process. The attention score $u_{it}$ at each time step was derived as
\begin{equation}
     u_{it} = tanh(W_{w1}f_{it}+W_{w2}c_{it}+b_w)
\end{equation}
The attention weights for each time point were obtained by normalizing the attention scores using the softmax function $\alpha_{it}$, as
\begin{equation}
     \alpha_{it} = \frac{\exp{(u_{it}^T})}{\sum_{t} \exp{(u_{it}^T})}
\end{equation}
The final attention-weighted output $s_i$ was computed as follows:
\begin{equation}
     s_i = \sum_{t}\alpha_{it}h_{it}
\end{equation}
Here, $f_{it}$ and $c_{it}$ represented elements from the fusion feature matrix $F$ and the cross-correlation coefficient matrix $C$, respectively. $W_{w1}$, $W_{w2}$, and $b_w$ were trainable weight parameters. This approach effectively combined the complementarity of multimodal data with statistical correlation analysis, introducing an innovative attention mechanism for multimodal tasks. This not only enhances the model's efficiency in understanding and utilizing multimodal data features but also demonstrates how traditional attention mechanisms can be extended and improved through statistical analysis methods.

Additionally, Zhang \etal proposed a multimodal neural network model for integrating demographic data and EEG signals \cite{zhang2020eeg}. This model incorporated an attention mechanism to effectively combine the two data types, aiming to uncover complex relationships between EEG signals and demographic factors. Such integration is particularly valuable for applications like depression detection, where demographic information can provide crucial contextual insights. The model first used a one-dimensional CNN to process EEG signals, resulting in a feature matrix $Z \in \mathbb{R}^{C_{out} \times m}$, while demographic data were encoded as a feature vector $S \in \mathbb{R}^d$. The two data types were fused through an attention mechanism, with the calculation as follows:
\begin{equation}
     A = tanh((W_{fe}Z+b_{fe})\oplus(W_{de}S+b_{de})).
\end{equation}
Here, $W_{fe}$ and $b_{fe}$ were trainable weights and biases related to EEG signal features. $W_{de}$ and $b_{de}$ were weights and biases related to demographic features. The symbol $\oplus$ represented the fusion operation between the two modalities. This approach enhances the integration of demographic data and EEG signals by leveraging attention mechanisms, helping to mitigate the impact of individual differences on model performance. By dynamically adjusting the influence of demographic information, the model achieves a more effective and personalized feature representation, improving the accuracy of multimodal analysis.

Furthermore, Choi \etal proposed a multimodal attention network to explore the fusion of facial video and EEG signals \cite{choi2020multimodal}. Unlike traditional methods that rely on a single fusion layer, this network utilized multimodal fusion layers incorporating bilinear and trilinear pooling to extract and integrate deep features. This approach not only enhances the integration of features from both modalities but also improves model performance by dynamically allocating attention weights, enabling more effective cross-modal information exchange.

Recently, Transformer-based multi-head self-attention mechanisms have become a prevalent approach in multimodal data fusion, enabling efficient processing and integration of features from diverse modalities. In the model framework, each token received by the Transformer encoder corresponds to a specific modality, allowing the model to effectively capture and integrate information at the feature level. By leveraging complex inter-modal interactions, the self-attention mechanism uncovers intrinsic relationships and complementary information between modalities, dynamically refining feature representations. A well-known example is the MMASleepNet model \cite{yubo2022mmasleepnet}, which not only incorporated the concept of the Squeeze-and-Excitation (SE) module from SENet \cite{hu2018squeeze} but also integrated the advantages of the Transformer encoder module. This combination enhances the model’s ability to effectively handle multimodal data by dynamically adjusting feature representations across different modalities, thereby improving overall performance.

\subsection{Application of Attention Mechanisms in Information Interaction Between EEG Signals and Multimodal Data}

Compared to self-attention, cross-attention extends its functionality by integrating information from multiple modalities, enabling more precise modeling of inter-modal associations \cite{li2024multimodal}. When the query (Q) matrix and the key (K) and value (V) matrices originate from different modalities, the computation shifts from capturing intra-modal correlations to establishing inter-modal relationships, effectively transforming self-attention into cross-attention \cite{lee2022cross}. Also referred to as cross-modal attention, this mechanism introduces additional input sequences from distinct modalities, enhancing information fusion and improving the overall representation of multimodal data \cite{wang2022scanet}\cite{zhao2024deep}.

In EEG-related research, visual reconstruction from EEG signals is a particularly challenging yet rapidly evolving field. The objective is to decode the information embedded in EEG signals and use it to reconstruct corresponding visual stimuli. This process requires capturing the intricate relationships between EEG and visual modalities, often achieved through cross-attention mechanisms. By computing the relevance between EEG segments and visual data, EEG signals serve as conditioning inputs to guide the generation of visual stimuli. A well-known example is the DreamDiffusion model \cite{bai2023dreamdiffusion}, which employed an EEG encoder trained on large-scale EEG data using a masking strategy to enhance feature extraction. The extracted EEG features then conditioned the Stable Diffusion model to generate images. Within Stable Diffusion, the cross-attention mechanism computes correlations between EEG-derived and image-derived features, enabling coherent information exchange between the two modalities. The mathematical formulation of cross-attention can be expressed as follows:
\begin{equation}
Q=W_{Q}^{(i)}\cdot\varphi_{i}\left(z_{t}\right),K=W_{K}^{(i)}\cdot\tau_{\theta}(y),V=W_{V}^{(i)}\cdot\tau_{\theta}(y).
\end{equation}
Here, $\varphi_i\left(z_t\right) \in \mathbb{R}^{N \times d_e^i}$ is the output of the noise prediction model Unet, and $\tau_{\theta}(y) \in \mathbb{R}^{M \times d_{\tau}}$ is the encoded representation obtained by projecting the EEG features output from the EEG encoder through an additional layer. $W_{V}^{(i)} \in \mathbb{R}^{d \times d_{\epsilon}^{i}}, W_{Q}^{(i)} \in \mathbb{R}^{d \times d_{\tau}}, \mathrm{and} , W_{K}^{(i)} \in \mathbb{R}^{d \times d_{\tau}}$ are learnable parameters. In this setup, the Q matrix is derived from image data, while the K and V matrices are derived from EEG signals. By substituting these Q, K, and V matrices into the self-attention computation formula, the attention score matrix is obtained, leading to the output of the cross-attention mechanism. This process facilitates effective information interaction between EEG and image modalities, enabling more precise multimodal feature integration.

\section{Conclusion and Future Works}
This paper explores the integration of brain-computer interface (BCI) technology with attention models and their applications, highlighting recent advancements in the field. Traditional attention mechanisms, as a classical approach, are widely applied in EEG signal analysis. Transformer-based methods open up new avenues for processing and analyzing EEG signals, providing a more flexible framework. However, the effectiveness of attention mechanisms is often highly dependent on specific data types, limiting model generalization and posing challenges for cross-dataset transfer. These limitations emphasize the need for further empirical research to develop more generalizable and robust model architectures.

\begin{figure*}
\begin{center}
\includegraphics[width=0.7\textwidth]{future_work.pdf}
\end{center}
\caption{Future works of attention modules in BCI.}
\label{fig:future_work}
\end{figure*}

In BCI applications, attention mechanisms not only expand research opportunities at the intersection of artificial intelligence and neuroscience but also enhance the technology’s potential and future prospects across various domains, including clinical applications, disability support, maritime safety, education, emotion recognition, and aerospace systems. Future research could primarily focus on the following aspects.

First, deep learning models, particularly those integrated with attention mechanisms, are expected to be applied in broader domains and tasks, such as cognitive neuroscience, clinical diagnostics, rehabilitation medicine, and affective computing. These applications can facilitate early detection of neurological disorders (e.g., Parkinson’s disease, epilepsy, and stroke recovery), improve assistive BCI systems for individuals with motor impairments, and enhance emotional state recognition for mental health monitoring. In education, BCI-driven attention-tracking and cognitive load assessment could support personalized learning experiences, benefiting students with learning disabilities such as ADHD. In maritime and aerospace industries, EEG-based fatigue monitoring systems could enhance situational awareness and reduce the risk of human errors in high-stress operational environments.

Second, more specialized model structures should be designed based on the characteristics of EEG signals. Currently, most model structures are adapted from fields like computer vision and natural language processing by converting EEG signals into image or text sequence data structures. However, for clinical neurotechnology, specialized architectures could improve real-time seizure prediction and brain-state monitoring for comatose patients. In disability support, adaptive BCI interfaces could enhance communication for individuals with locked-in syndrome or severe speech impairments. In maritime and aerospace neuroergonomics, models tailored to physiological constraints and environmental stressors could optimize pilot and ship crew performance, improving safety in high-risk settings.

Third, models employing self-attention layers based on Transformer architectures often have lower computational efficiency and slower convergence speeds. In future work, considering lightweight and energy-efficient model structures could be a promising direction. In disability assistance, real-time BCI-controlled prosthetics and exoskeletons require rapid response times to ensure seamless user interaction. In clinical applications, mobile and wearable BCI systems demand low-power, high-efficiency architectures for continuous neurophysiological monitoring. Similarly, in maritime and aerospace applications, where computational resources may be limited in remote or high-altitude environments, optimizing Transformer-based models for real-time decision-making could significantly enhance operational efficiency and safety.

In summary, the study of attention mechanisms will further advance the development of BCI technology. By integrating attention mechanisms with advanced technologies such as machine learning and deep learning, we can explore more sophisticated and domain-specific BCI systems. These innovations have significant implications for clinical rehabilitation, disability support, maritime safety, educational neuroscience, emotion-adaptive interfaces, and aerospace neuroergonomics, ultimately enabling safer, more adaptive, and more human-centric interactions between the brain and external systems.

\section{Acknowledgments}
This work was supported in part by the National Natural Science Foundation of China under Grant 62276169, in part by the Medical-Engineering Interdisciplinary Research Foundation of Shenzhen University under Grant 2024YG008, in part by the Shenzhen University-Lingnan University Joint Research Programme, in part by the Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research Institutions under Grant 2023SHIBS0003, in part by the STI 2030-Major Projects 2021ZD0200500, and in part by the Open Research Fund of the State Key Laboratory of Brain-Machine Intelligence, Zhejiang University (Grant No. BMI2400008).


\bibliographystyle{unsrt}
\bibliography{references.bib}

\end{document}

