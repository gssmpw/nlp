\section{Introduction}
\copyrightnotice
\IEEEPARstart{D}{ue} to the rising demand for optimized hardware acceleration of general matrix multiplication (GEMM), the field of hardware design continues to see innovation for ways of better exploiting the inherent parallelism to speed up computation.
However, at a certain point, after technology scaling slows to a halt and the system-level optimizations and known parallelism are exhausted, an accelerator wall exists which limits further progress on the implementation side \cite{fuchs2019accelator}.
A less-explored path for advancement beyond this wall is through reducing the computation at the algebraic level, by computing the same output from a re-arranged compute pattern requiring fewer or cheaper operations to be executed in hardware.

One of the area-dominant computational resources in GEMM and deep learning accelerators can commonly be the multiply-accumulate (MAC) units \multDominant, and an accelerator's throughput can be directly limited by how many MAC units can be afforded in its hardware budget.
As a result, surpassing this performance per MAC limit has been focused on recently with minimal filtering algorithms applied to convolutional neural networks \winoConv and with application of fast inner-product algorithms for speeding up deep learning and GEMM workloads \cite{pogue2024fast}.

The Strassen matrix multiplication algorithm \cite{strassen1969gaussian} can also theoretically be used to reduce the complexity of naive matrix multiplication.
However, its execution on general-purpose central processing units (CPU)s and graphics processing units (GPU)s has been shown to be not suitable for achieving the algorithm's promised theoretical speedups \citeCpu.
\sa even increases execution time on CPUs/GPUs unless the matrix widths/heights are in the range of at least 1024 elements or larger.
This limits the benefits of using the algorithm on these devices for modern workloads that do not decompose to such large matrix multiplications.
\sa contains hidden overheads such as extra data accesses required for reading/computing/storing additional intermediate matrices before/after the matrix multiplication steps.
These extra steps all add to the overall execution time beyond what is expected from a theoretical analysis based on the number of arithmetic operations performed alone.

This then leaves questions surrounding if the promised theoretical complexity reductions can be more efficiently achieved in custom hardware architectures designed specifically for executing \sa.
However, prior work on this topic is limited and it is not immediately clear how to design such architectures or if they can truly lead to real improvements.
In this work, we bridge this gap by presenting and evaluating new systolic array hardware architectures for efficiently exploiting \sa.
The proposed architectures achieve a more efficient implementation of \sa compared to what is possible through execution on CPUs and GPUs by pipelining and performing the extra data movement and addition steps at all levels of recursion in parallel with the matrix multiplications.
The Strassen architectures are functionally equivalent to conventional multisystolic array designs while allowing the theoretical complexity reductions of \sa to be translated directly into hardware resource savings, even for multiplication of small matrices.
Furthermore, the architectures are multisystolic array designs, which is a type of design that can multiply smaller matrices with higher utilization than a single-systolic array design.

Compared to a conventional multisystolic array design, the proposed architecture implemented on FPGA uses 1.3\x fewer DSP units and a similar amount of soft logic resources when instantiated for multiplying matrix sizes down to 24\by24 at 2 levels of Strassen recursion.
We demonstrate how the proposed systolic array architectures are able to increase conventional multiplications/multiplier/clock cycle limits while also allowing the design to scale up in size without increasing the minimum supported matrix sizes.
