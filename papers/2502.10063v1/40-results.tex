\section{Results}


In this section, we evaluate example implementations of the proposed \smm architectures.
In \secn{smm:sec:results-baseline}, we compare the \smm MXU architectures in isolation against our baseline MXU designs in Table \ref{smm:tab:smm-MM}.
In \secn{smm:sec:results-prior-work} we evaluate the \smm MXU architectures in Tables \ref{smm:tab:second}-\ref{smm:tab:last} compared to prior work when integrated into an end-to-end deep learning accelerator system based on the system from our previous work \cite{pogue2024fast}, which has open-source code \cite{ffip-source}.
We first describe the integration of our proposed systolic arrays into the deep learning system in \secn{sec:system}, and in Sections \ref{sec:mu} and \ref{smm:sec:mat-sz}, we define performance metrics used to compare the \smmArch architectures against baseline designs and prior works.

\subsection{System Integration}
\label{sec:system}
We were able to integrate the \smm architectures into a system based on our previous work \cite{pogue2024fast}, which has open-source code \cite{ffip-source}, by swapping the \smmArch MXU architectures from \fg{smm:fig:smm-mxu} into our system design \cite{pogue2024fast} in place of the free-pipeline fast inner-product (FFIP) MXU.

In order to perform GEMM on the proposed MXUs and multiply matrices of arbitrary sizes that can be larger than the MXU dimensions, the full \AB matrices are first divided into \gemmtiles prior to being divided further into smaller \blocks for executing \eq{smm:eq:mm} or \seq.
The \gemmtiles are then fed into the MXU one-by-one.
Each \gemmtile is then considered as the full \AB matrix from \eq{smm:eq:mm} or \seq while being fed into the MXU and gets further divided into smaller \m.A?{ij}/\m.B?{ji} \blocks within the MXU.

Following each \gemmtile multiplication, the partial \gemmtile products are accumulated outside of the MXU to generate each final \gemmtile product.
Prior to each \gemmtile multiplication, a $\bld{B}$ \gemmtile is loaded into the MXU.
It then remains in place as the $\bld{A}$ \gemmtile flows through the MXU producing the \gemmtile product, during which a new \Av vector is fed into the MXU each clock cycle.
Additionally, to hide the latency of loading $\bld{B}$ \gemmtiles, the MXU PEs each contain one extra $b$ buffer to load the next $\bld{B}$ \gemmtile into the MXU as the current \gemmtile is being multiplied.

Each \A, \B, and \C \subblock entering or exiting the top-level MXU for the \smm and baseline MXUs first pass through triangular-shaped register arrays each containing $X$ shift registers of varying depths.
Here, each shift register $SR_k$ has a depth of $k$ and loads one $a_{i,k}$ or $b_{k,j}$ element per clock cycle.
These triangular buffers are explained further in our prior work \cite{pogue2024fast} and they allow the vector elements to enter the MXU in the necessary order as depicted in the element indices in \figs \ref{smm:fig:MM-mxu} and \ref{smm:fig:pes}.

\input{41-arch-mult-efficiency}


\subsection{Comparison to Baseline Designs}
\label{smm:sec:results-baseline}
\input{smm-table-mm}

Table \ref{smm:tab:smm-MM} shows the resource usage and performance comparison between the proposed \smmArch and baseline \mm{/}\mmArch systolic array architectures in isolation (without integration into a deep learning accelerator system).
The \smmArch?1 and \smmArch?2 architectures overall have a similar amount of soft logic resources and the same throughput per clock cycle roof as the \mmArch?1 and \mmArch?2 architectures, respectively, but they require \textbf{1.14-1.31\x fewer DSP units}.
Compared to the multisystolic array \mm?1 and \mm?2 designs, the \smmArch?1 and \smmArch?2 architectures are also functionally equivalent, respectively, other than having a lower clock frequency.
To help mitigate the limitation of having a lower frequency, we added an extra \smmArch?2 design (which had the biggest issue with clock frequency) on the far right of Table \ref{smm:tab:smm-MM} containing additional pipelining registers in the addition logic of each Q Addition Vectors unit from \fig{smm-add-units} (c).
This extra design demonstrates how a trade-off can be optionally made to increase the design's clock frequency at the cost of some extra soft logic resources.

Nonetheless, the lower clock frequencies of the \smm?r designs in Table  \ref{smm:tab:smm-MM} are compensated by the fact that the \smm?r designs achieve more effective operations from the same number DSP units.
Since the reduction in DSP units is greater than the reduction in clock frequency in the \smm?1 design and \smm?2 design with extra registers relative to their \mm?r counterparts, they would be able to achieve a higher overall throughput if scaled up in size to use the same number of DSPs.
This is shown by the Throughput/DSP metric in Table  \ref{smm:tab:smm-MM}, which shows that the \smm?r designs achieve up to \textbf{22\% more throughput per DSP} than their \mm?r counterparts.
Finally, if the frequency-limiting critical path is in external control or other logic outside of the systolic array after integrating it into an end-to-end accelerator system, as was the case in our full-system accelerators from Tables \ref{smm:tab:8}-\ref{smm:tab:ffip}, this limitation of a lower frequency is further mitigated.

\begin{figure}[]
  \hspace*{-0.25cm}
  \centering
  \includegraphics[scale=0.0525]{smm-mult-compute-efficiency.eps}
  \vspace*{-0.6cm}
  \caption{Plotting the multiplier compute efficiency of the architectures in Table \ref{smm:tab:smm-MM} when multiplying different input matrices of size $n$\by$n$.
    As shown, the minimum matrix size that can be efficiently multiplied gets smaller in the order of the \mm?\zero{,} \mm?1{/}\smm?1{,} and \mm?2{/}\smm?2 designs, respectively.}
  \label{smm:fig:perf}
\end{figure}

The throughput per clock cycle roof of the \mm?\zero and \mm?2 baseline designs in Table \ref{smm:tab:smm-MM} are equal and they consume the same number of DSP resources, but the \mm?\zero design requires slightly fewer ALM and register resources.
However, this penalty may be justified in the \mm?2 design when considering that the minimum matrix size (height\by width) that can be multiplied while fully utilizing the MXU is 4$\times$ smaller in the \mm?2 design compared to the \mm?\zero design.
This increases its performance scalability for accelerating modern workloads such as deep learning as discussed in \secn{multi-sys} and \ref{smm:sec:mat-sz}.
This ability of the multisystolic array designs to more efficiently multiply smaller matrices is further illustrated in \fig{smm:fig:perf}.
This same property is true for the \smmArch?2 design, except it achieves this with fewer DSP resources.
This benefit is demonstrated in \secn{smm:sec:results-prior-work}, where this property allowed us to scale up our deep learning accelerator design without compromising utilization to achieve state-of-the-art ResNet throughput.


\subsection{Comparison to Prior Work}
\label{smm:sec:results-prior-work}

Full system-level validation of the experimental accelerator as integrated into the system from our previous work \cite{pogue2024fast} has been done on an Arria 10 SoC Developement Kit \cite{sx-dev-kit} containing the \sx device by measuring throughput in real-time.
However, this device contains fewer soft logic resources than the \gx used in the prior works we compare against, and we generate compilation results for our design on the same \gx device used in prior works for a more fair and consistent comparison.
Throughput values of our designs on the \gx device are then calculated using an accurate throughput estimation model based on our highly deterministic and time-predictable system implementation, which accurately predicts actual throughputs measured on the \sx device available to us.
Tables \ref{smm:tab:second}-\ref{smm:tab:last} show throughputs for ResNet \cite{kaiming2016deep} neural networks.

The works from Liu \ea \cite{liu2022toward} and Fan \ea \cite{fan2022fpga} in Table \ref{smm:tab:8} use a technique to pack two 8-bit multiplications onto each 18\by19-bit multiplier in the DSPs and additional ALMs, and therefore the number of multipliers is calculated as \#DSP\by4 in those works.
The number of multipliers in the works \cite{huang2022fpga}, \cite{kim2023agamotto} from Table \ref{smm:tab-mixed} is calculated as \#DSPs since they are implemented on AMD FPGAs where each DSP instantiates one 18\by27-bit multiplier \cite{amd-dsp}.
In Tables \ref{smm:tab:8} and \ref{smm:tab-mixed}, the number of multipliers in the prior works \cite{an2022opencl}, \cite{dai2024dcp} is equal to \#DSPs\by2, where each DSP in the Altera FPGAs instantiates two 18\by19-bit multipliers \cite{intel-dsp}.
The number of multipliers used in the MXUs from our architectures in Tables \ref{smm:tab:first}-\ref{smm:tab:second} is equal to $8^r$ or $7^r$ times $X$\by$Y$ for the \mmArch and \smmArch MXUs, respectively.
For example, an \mm?\zero 64\by64 MXU (meaning $r = 0$ and $X = Y = 64$) would contain $8^0\times64^2$ multipliers, an \mm?1 32\by32 MXU would contain $8^1\times32^2$ multipliers, and an \smmArch?2 8\by8 MXU would contain $7^2\times8^2$ multipliers.
Due to the FFIP reduction in multipliers as described in our prior work \cite{pogue2024fast}, the number of multipliers for the FFIP architectures in Table \ref{smm:tab:ffip} is equal to $8^r$ or $7^r$ times $X\times Y/2 + X/2$ for the FFIP and FFIP+\smmArch designs, respectively.
Additionally, for our deep learning accelerator implementations in Tables \ref{smm:tab:second}-\ref{smm:tab:ffip}, there are an additional $Y\times4^r$ multipliers located outside the MXU in the Post-GEMM Unit \cite{pogue2024fast} for performing inter-layer quantization rescaling functions.
For our designs requiring more than 3036 multipliers, 3036 are instantiated on 1518 DSPs, and the remainder are instantiated in soft logic resources as the DSP resources are fully utilized.

\label{smm:sec:results-prior-work}
\input{smm-table-arria10}
\input{smm-table-ffip}
\input{smm-table-non-arria10}

Tables \ref{smm:tab:8}-\ref{smm:tab:ffip} show the \smmArch architectures  integrated into the deep learning system from our previous work \cite{pogue2024fast} compared to state-of-the-art accelerators evaluated on the same FPGA family for the same input bitwidths and similar neural network models.
Integrating the \smmArch multisystolic array design into our deep learning accelerator allowed us to increase the multiplier compute efficiency while also scaling up the computational resources and throughput roof without increasing the minimum supported matrix sizes.
This allowed it to significantly surpass the throughput in our prior work \cite{pogue2024fast} and other state-of-the-art prior works evaluated on the same FPGA family as shown in Tables \ref{smm:tab:8}-\ref{smm:tab:ffip}.
If the design is scaled up using a single-systolic array, the minimum supported matrix size increases and compute resources begin to be underutilized for ResNet execution based on the smaller matrix sizes that its workload decomposes to.
This causes the effective throughput to not increase well despite the design having a larger throughput roof.

The \smmArch?1 32\by32 and \sAlg.\sffip<{}?1 32\by32 designs consume noticeably more memory resources than the \smmArch?2 8\by8 and \sAlg.\sffip<{}?2 8\by8 designs.
However, it is worth noting that this is not due to increased memory requirements, but rather is due to the compiler favouring to swap some register resources for memory resources.
This is because the \smmArch?1 32\by32 and \sAlg.\sffip<{}?1 32\by32 designs have a higher register (and overall area) overhead than the \smmArch?2 8\by8 and \sAlg.\sffip<{}?2 8\by8 designs in order to achieve higher throughput roofs.

In Table \ref{smm:tab:8}, the \smmArch architectures achieve the highest throughput and multiplier compute efficiency compared to the prior works.
The \smmArch?1 and \smmArch?2 architectures' multiplier compute efficiencies in Table \ref{smm:tab:8} approach their limits of 1.14 and 1.31 that are derived in \eq{smm:eq:mu-roof}.
This surpasses the limit of 1 of the baseline \mmArch architectures and prior works that is derived in \eq{smm:eq:mm-mu-roof}, validating \smm's ability to increase multiplier compute efficiency and reduce computational complexity as expected from our analysis.

Table \ref{smm:tab:ffip} shows an example of how \smm can be combined with other algebraic techniques to further increase multiplier compute efficiency limits.
FFIP \cite{pogue2024fast} provides a way to reduce the number
of required multiplications by up to a factor of 2, trading half the multiplications for cheap low-bitwidth additions.
Because of this, the limit for the multiplier compute efficiency metric in \eq{smm:eq:mu} for an FFIP architecture becomes 2, and $2\times(8/7)^r$ for a combined FFIP+\smmArch architecture.
In Table \ref{smm:tab:ffip}, we evaluate architectures that combine FFIP+\smmArch by instantiating \smmArch MXUs that use FFIP MXUs at their lowest level of recursion instead of the conventional \mm?\zero MXUs from \fig{smm:fig:MM-mxu}.
This further increases multiplier compute efficiency compared to a standalone \smmArch or standalone FFIP MXU as seen in the achieved multiplier compute efficiencies of the \sAlg.\sffip<{}?{r} architectures listed in Table \ref{smm:tab:ffip}.
