\subsection{Multiplier Compute Efficiency}
\label{sec:mu}
In this subsection, we define an efficiency metric called the multiplier compute efficiency (MCE) in \eq{smm:eq:mu} which we use to compare the \smmArch architectures against baseline designs and prior works.
This is used to quantify how much the algebraic optimizations exploited in an architecture reduce the computational complexity.
Reductions in computational complexity allow an architecture to utilize its multipliers more effectively than conventional designs using no algebraic optimizations.
The multiplier compute efficiency is defined as follows:
\begin{align}  \label{smm:eq:mu}
  \tx{MCE} = \frac{\tx{mults} / \tx{multiplier}} {\tx{clock cycle}}
  &= \frac{(\tx{mults/s})/\tx{\#multipliers}}{f}
  \,.
\end{align}
Here, mults/s above is measured by taking the number of multiplications required to carry out an execution using conventional algebra and dividing it by the measured execution time.
Finally, \#multipliers is the number of instantiated multipliers in the design, and $f$ is the clock frequency that the hardware design is operating at.

Conventional matrix multiplication algorithms such as \eq{smm:eq:mm} have no algebraic optimizations for reducing the computational complexity.
Therefore, the limit/maximum achievable value (also referred to as the roof) of the metric in \eq{smm:eq:mu} is the following when using conventional matrix multiplication in hardware:
\begin{align}  \label{smm:eq:mm-mu-roof}
  \tx{roof}\lb\tx{MCE}_{\tx{\mm?r}}\rb &= 1
  \,.
\end{align}
In contrast, \sa requires $8^r/7^r$ times fewer multiplications than a conventional matrix multiplication algorithm, where $r$ is the number of levels of recursion implemented in \sa.
Therefore, the multiplier compute efficiency can reach the following limit in \smmArch architectures:
\begin{align} \label{smm:eq:mu-roof}
  \tx{roof}\lb\tx{MCE}_{\tx{\smm?r}}\rb &= 
  \lb\frac{8}{7}\rb^{r}
  \,.
\end{align}

As discussed in \secn{smm:sec:background}, \sa reduces the overall number of operations in matrix multiplication.
Furthermore, any additions required before the matrix multiplications in the algorithm are even less of a concern in fixed-point implementations.
This is because the hardware complexity of fixed-point multipliers typically scale quadratically with the input bitwidth compared to linearly for adders and registers \multComplexity, causing the hardware footprint of multipliers to dominate that of adders and registers.

However, one of the impediments of using \sa for fixed-point implementations is that the bitwidths of the multiplication inputs increase by $r$ bits for $r$ levels of Strassen recursion that are implemented, reducing its potential area savings for custom fixed-point hardware designs.
Nonetheless, this impediment for fixed-point designs can be inherently mitigated in FPGA implementations so long as $r$ plus the initial input width is not larger than the maximum input width supported by the FPGA's DSP units.
For example, each DSP in common Intel/Altera FPGAs instantiate two 18\by19-bit multipliers \cite{intel-dsp}, and common input bitwidths for applications such as deep learning are 16 bits or less.
This leaves room for at least 2 or more levels of Strassen recursion to be implemented before surpassing the bitwidth limit supported by the DSPs.

Furthermore, due to the flexible nature of custom hardware design, the \smm?r architectures can be efficiently mapped onto other DSP units in general which support input bitwidths up to $n$ bits by customizing the input datapath bitwidth $w$ and value of $r$ as necessary to ensure that $w+r \le n$.
So long as the accuracy requirements of the application are still met, this will allow the \smm?r designs and their increase in multiplier bitwidth to still be efficiently mapped onto DSP units of any bitwidth in a general way.

\subsection{Supporting Smaller Matrices with the Same Performance}
\label{smm:sec:mat-sz}
Multisystolic array designs such as the \smmArch and baseline \mm?r architectures have the ability to efficiently multiply smaller matrices than a single-systolic array design with the same performance capability.
By executing \eq{smm:eq:mm} or \seq fully in parallel for $r$ levels of recursion, matrix products of size as small as $n\times n$ can be computed up to once every $n/2^r$ clock cycles in an \mm?r or \smmArch multisystolic array design.
Furthermore, these matrix products require $n^3$ multiplications to calculate using conventional algebra.
Therefore, the ratio of an architecture's throughput per clock cycle versus its smallest supported matrix sizes it can multiply, which we refer to as the matrix size efficiency (MSE), is the following:
\begin{align} \label{smm:eq:mat-sz}
  \tx{MSE} = 
  \frac{\tx{mults}/\tx{clock cycle}} {\tx{min. mat. size (h\by w)}}
  \,,
\end{align}
which has the following roof for multisystolic arrays:
\begin{align} \label{smm:eq:mat-sz}
  \tx{roof}\lb\tx{MSE}_{\tx{(S)MM}_r}\rb
  = \frac {n^3/\lb n/2^r\rb} {n\times n} = 2^r
  \,.
\end{align}

In contrast, a single-systolic array design can produce matrix products of size as small as $n\times n$ up to once every $n$ clock cycles, making this ratio the following for a single-systolic array design:
\begin{align}
  \tx{roof}\lb\tx{MSE}_{\tx{MM}}\rb
  = \frac {n^3/ n} {n\times n} = 1
  \,.
\end{align}
This shows that the \smmArch and baseline \mm?r multisystolic array designs can efficiently multiply matrices $2^r$ times smaller than a single-systolic array architecture with the same performance capability.

As discussed in \secn{multi-sys}, this is an important property for increasing a systolic array accelerator's maximum achievable throughput on real-life workloads.
Even if more compute resources are instantiated to scale up the size of the systolic array, the systolic array will begin to be underutilized after its size surpasses the workload's matrix sizes.
This is particularly true in modern workloads such as deep learning acceleration, where the matrix sizes that the workloads break down to can be smaller than the maximum systolic array size that could be instantiated in an accelerator \citeMultiSys.
In \secn{smm:sec:results-prior-work}, we demonstrate how this property allowed us to scale up our deep learning accelerator design without compromising utilization to achieve state-of-the-art ResNet \cite{kaiming2016deep} throughput.
