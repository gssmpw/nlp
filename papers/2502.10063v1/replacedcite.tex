\section{Background and Related Work}
\label{smm:sec:background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conventional Matrix Multiplication}
A conventional matrix multiplication algorithm
computes $\bld{C} = \bld{A} \bld{B}$ for $\bld{A}$ of size $M$\by$K$ and $\bld{B}$ of size $K$\by$N$, where each element $c_{i,j}$ of $\bld{C}$ is calculated as follows:
\begin{align} \label{smm:eq:mmZero}
  c_{i,j} = \sum_{k=1}^{K} a_{i,k} b_{k,j} \,.
\end{align}

Alternatively, \C can also be computed by dividing \A and \B into 4 matrix \blocks, where \C is then computed by carrying out 8 matrix \block multiplications and 4 matrix \block additions between the \A and \B \blocks as follows:
\begin{align}
  \label{smm:eq:mm}
  \begin{bmatrix}
    \bld{C_{11}} & \bld{C_{12}} \\
    \bld{C_{21}} & \bld{C_{22}}
  \end{bmatrix}{=}\begin{bmatrix}
    \bld{A_{11}B_{11}}{+}\bld{A_{12}B_{21}} & \bld{A_{11}B_{12}}{+}\bld{A_{12}B_{22}}\\
    \bld{A_{21}B_{11}}{+}\bld{A_{22}B_{21}} & \bld{A_{21}B_{12}}{+}\bld{A_{22}B_{22}} \end{bmatrix} .
\end{align}
This process can then be carried out recursively again for each matrix \block product by splitting the matrix \blocks again into smaller \blocks and repeating the same process.

\subsection{Strassen Matrix Multiplication}
Strassen's fast matrix multiplication algorithm ____
provides a way to carry out \eq{smm:eq:mm} instead using 7 matrix \block multiplications and 18 matrix \block additions as follows:
\begin{align}
  \label{smm:eq:strass-first}
  \begin{split}
    \bld{T_{1}} &= \bld{A_{11}} + \bld{A_{22}}  \\
    \bld{T_{2}} &= \bld{A_{21}} + \bld{A_{22}}  \\
    \bld{T_{3}} &= \bld{A_{11}}  \\
    \bld{T_{4}} &= \bld{A_{22}}  \\
    \bld{T_{5}} &= \bld{A_{11}} + \bld{A_{12}}  \\
    \bld{T_{6}} &= \bld{A_{21}} - \bld{A_{11}}  \\
    \bld{T_{7}} &= \bld{A_{12}} - \bld{A_{22}}
  \end{split}
  \qquad\qquad
  \begin{split}
    \bld{S_{1}} &= \bld{B_{11}} + \bld{B_{22}}  \\
    \bld{S_{2}} &= \bld{B_{11}}  \\
    \bld{S_{3}} &= \bld{B_{12}} - \bld{B_{22}}  \\
    \bld{S_{4}} &= \bld{B_{21}} - \bld{B_{11}}  \\
    \bld{S_{5}} &= \bld{B_{22}}  \\
    \bld{S_{6}} &= \bld{B_{11}} + \bld{B_{12}}  \\
    \bld{S_{7}} &= \bld{B_{21}} + \bld{B_{22}}
  \end{split}
\end{align}
\begin{align}
  \begin{split}
    \bld{Q_{1}} &= \bld{T_{1}} \cdot \bld{S_{1}} \\
    \bld{Q_{2}} &= \bld{T_{2}} \cdot \bld{S_{2}} \\
    \bld{Q_{3}} &= \bld{T_{3}} \cdot \bld{S_{3}} \\
    \bld{Q_{4}} &= \bld{T_{4}} \cdot \bld{S_{4}} \\
    \bld{Q_{5}} &= \bld{T_{5}} \cdot \bld{S_{5}} \\
    \bld{Q_{6}} &= \bld{T_{6}} \cdot \bld{S_{6}} \\
    \bld{Q_{7}} &= \bld{T_{7}} \cdot \bld{S_{7}} \\
  \end{split}
  \qquad\qquad
  \begin{split}
    \label{smm:eq:strass-last}
    \bld{C_{11}} &= \bld{Q_1} + \bld{Q_4} - \bld{Q_5} + \bld{Q_7} \\
    \bld{C_{12}} &= \bld{Q_3} + \bld{Q_5} \\
    \bld{C_{21}} &= \bld{Q_2} + \bld{Q_4} \\
    \bld{C_{22}} &= \bld{Q_1} - \bld{Q_2} + \bld{Q_3} + \bld{Q_6} \\
  \end{split} \,.
\end{align}
Similarly to \eq{smm:eq:mm}, this algorithm can also be repeated recursively for each matrix \block multiplication, leading to an asymptotic complexity reduction compared to conventional matrix multiplication algorithms such as \eq{smm:eq:mmZero} and \eq{smm:eq:mm}.

\subsubsection{Winograd Form}
The Winograd form of the Strassen algorithm ____ has the same asymptotic complexity but requires 15 matrix \block additions at each level of recursion rather than 18.
However, for fixed-point data types, this form increases the multiplier input datapath bitwidth by up to 2 bits for each recursion level implemented rather than 1 bit, which reduces the implementation benefits.
Due to this, we focus on the original form of the Strassen algorithm from \seq in our work instead.

\subsection{Prior Work on Multisystolic Array Systems}
\label{multi-sys}
Systolic arrays, which we also refer to as matrix multiplication units (MXU)s for convenience, are an effective choice for use in GEMM accelerators as they significantly reduce the required memory traffic and can reach high clock frequencies due to their short and regular interconnects.
Systolic array architectures have been used in state-of-the-art GEMM and deep learning accelerators such as the Tensor Processing Unit (TPU) \citeTPU, among others ____, ____.
However, a systolic array can only be fully utilized when the input matrix sizes at minimum match the dimensions of the systolic array or are larger, and real workloads have limits to the matrix sizes being multiplied.

There is then a limit to how fast the workload can be accelerated on a single-systolic array design.
This is because, even if more compute resources are instantiated to scale up the size of the systolic array, the systolic array will begin to be underutilized after its size surpasses the workload's matrix sizes, and the workload will not be able to execute any faster.
This is particularly true in modern workloads such as deep learning acceleration, where the matrix sizes that the workloads break down to can be smaller than the maximum systolic array size that could be instantiated in an accelerator \citeMultiSys.

To combat this, multiple smaller systolic arrays can be used in parallel, which allows for the total compute power in the systolic array system to increase while the minimum supported matrix sizes remain the same.
Prior works ____, ____ achieve this by implementing variations of \eq{smm:eq:mm} by dividing larger matrices into smaller matrix \blocks, executing the smaller matrix \block multiplications on multiple smaller systolic arrays.
The \block products are then later summed up to form the final larger matrix multiplication product.
In this work, we show how to efficiently implement \seq in hardware to achieve this same goal with less hardware resources.

\subsection{Prior Work on Executing Strassen on CPUs and GPUs}
\sa has been well explored in prior work for execution on general-purpose CPUs and GPUs \citeCpu.
However, its execution on CPUs and GPUs in these prior works is unable to efficiently achieve the algorithm's promised theoretical speedups unless the widths/heights of the matrices being multiplied are in the range of at least 1024 elements or even much larger.

This non-optimal execution of \sa in CPUs and GPUs stems from irregularities introduced in the algorithm such as extra data accesses required for reading/computing/storing additional intermediate matrices before/after the matrix multiplication steps.
These irregularities all add to the overall execution time beyond what would be expected purely from a theoretical analysis of only the number of required arithmetic operations ____, ____.

\subsubsection{Theoretical Complexity Reductions of Strassen's Algorithm}
\label{smm:sec:theoretical-complexity}
In this subsection, we establish what the expected theoretical complexity reductions of Strassen's algorithm are based on number of operations, and how the achieved speedups in prior works on CPU/GPU Strassen implementations fall short of achieving these theoretical complexity reductions.

Letting $M = N = K = n$, the complexity of \sa in number of arithmetic operations is $\mathcal{O}\lb n^{2.8074}\rb$ ____.
Conventional matrix multiplication \eq{smm:eq:mmZero} requires $n^3$ multiplications and $n^2\lb n-1 \rb$ additions for the following number of total operations:
\begin{align}
  \label{smm:eq:mm-ops}
  n^3 + n^2\lb n-1 \rb \,.
\end{align}
In contrast, \sa \eq{smm:eq:strass-first} for 1 recursion level requires $7n^3/8$ multiplications and $7\spc n^2\lb n/2-1 \rb/4 + 18\spc n^3/8$ additions for the following number of total operations:
\begin{align}
  \label{smm:eq:smm-ops}
  7n^3/8 + 7\spc n^2\lb n/2-1 \rb/4 + 18\spc n^3/8 \,.
\end{align}
The Winograd form of Strassen's algorithm ____ for 1 recursion level requires $7n^3/8$ multiplications and $7\spc n^2\lb n/2-1 \rb/4 + 15\spc n^3/8$ additions for the following number of total operations:
\begin{align}
  \label{smm:eq:swmm-ops}
  7n^3/8 + 7\spc n^2\lb n/2-1 \rb/4 + 15\spc n^3/8 \,.
\end{align}
By comparing \eq{smm:eq:mm-ops} to \eq{smm:eq:smm-ops} and \eq{smm:eq:swmm-ops} for different values of $n$ we can then see that \sa requires fewer operations than conventional matrix multiplication for matrix sizes of $n \ge 16$, and $n \ge 13$ for the Winograd form of \sa.

However, \sa on CPUs and GPUs in prior works only starts providing some speedups over traditional matrix multiplication for matrix sizes $n$ of at least 20000 ____, 16384 ____, 896 ____, 5000 ____, 1536 ____, 1006 ____, and 1000 ____ ____.
This limits the applicability of \sa on CPUs and GPUs for modern workloads such as deep learning that do not always decompose to such large matrix multiplications.

As derived above, prior works on CPU/GPU implementations require matrix sizes of at least 896-16384 before having benefits rather than the much lower theoretical threshold of 13 or 16.
In contrast, the custom Strassen hardware architectures presented in this work translate the benefits of \sa into hardware resource savings rather than reductions in execution time.
The proposed designs more closely achieve the theoretical complexity reductions of \sa compared to prior works on CPU/GPU implementations.
This is demonstrated in our results through the fact that the proposed architectures present area savings while achieving the same throughput/clock cycle as traditional designs even when instantiated for multiplying matrices down to size 24\by24.
Additionally, for $r$ Strassen recursion levels implemented, the proposed designs achieve $(8/7)^r$ times reduction in multipliers as expected from \seq compared to conventional designs without significant increase in other hardware components or any increase in throughput/clock cycle.

\subsection{Prior Work on Custom Strassen Hardware Architectures}
\label{background:sec:smm}
While software implementations of \sa on CPUs and GPUs have been well explored in prior work, custom hardware designs for efficiently exploiting the algorithm in hardware remain under-explored.
A systolic array design concept for implementing \sa for one level of recursion on 2\by2 matrices has been proposed in the work by Elfimova \ea ____ without evaluation of an implementation.
Another hardware design for implementing \sa for one level of recursion on 2\by2 matrices has also been proposed in the work by Le√≥n-Vega \ea ____, where the Strassen architecture reduced FPGA DSP usage by up to 12.5\% at the expense of 25-40\% increase in LUT resources to implement the additional adders.

Unlike the only two prior works on custom hardware designs for executing the Strassen algorithm, we propose architectures in this work that allow for \sa to be implemented on matrices larger than 2\by2.
This is essential for minimizing the complexity penalty of the additional adders.
Additionally, the architectures are capable of implementing multiple levels of \s recursion to achieve greater hardware resource savings.
Furthermore, the proposed architectures allow proven traditional systolic arrays to be still used at the core.
Alternatively, they can allow \sa to be used in combination with other hardware designs that can efficiently perform further algebraic optimizations on matrices after the Strassen portion is carried out, such as techniques from our prior work ____.
Finally, the proposed Strassen architectures are multisystolic array designs, meaning they can multiply smaller matrices with higher utilization than single-systolic array designs with the same computational strength.

\subsection{Notation}
The following notation is used throughput the remainder this work for describing different systolic array architectures or their workloads:
\begin{itemize}
\item $r$: The number of recursion levels in \eq{smm:eq:mm} or \seq that are implemented in a hardware architecture.
\item \mm?\zero: A traditional single-systolic array implementing conventional matrix multiplication \eq{smm:eq:mmZero} in hardware.
\item \mm?r: A traditional multisystolic array implementing conventional blocked matrix multiplication \eq{smm:eq:mm} in hardware for $r$ levels of recursion.
\item \smm: The proposed Strassen multisystolic array implementing \seq in hardware for $r$ levels of recursion.
\item MXU: In this work, systolic arrays may also be referred to as matrix multiplication units (MXU)s for convenience.
\item (S)MM$_{(r)}$ $X$\by$Y$: An \mm?\zero, \mm?r, or \smm?r architecture may also be referred to with two numbers $X$\by$Y$ specified beside it.
  Here, $X$ and $Y$ represent the width and height, respectively, in number of MAC units of each \mm?\zero systolic array instantiated at the lowest level of recursion in the architecture.
  For example, an \mm?\zero 64\by64 MXU (meaning $X = Y = 64$) would contain $64^2$ MAC units, an \mm?1 32\by32 MXU (meaning $r = 1$ and $X = Y = 32$) would contain $8^1\times32^2$ MAC units, and an \smmArch?2 8\by8 MXU (meaning $r = 2$ and $X = Y = 8$) would contain $7^2\times8^2$ multipliers.

\item $n$: The width/height of the matrices that are being fed as inputs to a systolic array to be multiplied.
\end{itemize}