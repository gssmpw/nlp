\section{Method}
\label{section:Method}
\subsection{Problem Formulation}
The objective of palletization is to maximize space utilization while ensuring stability. Space utilization refers to the proportion of the total volume occupied by boxes on the pallet relative to the total available space on the pallet. Besides, we define a stable state on the pallet as follows: when the agent places a box, we record the spatial position and orientation of the box at the moment of placement. At each timestep, after placing a new box, we check the difference between the current position and orientation of all boxes on the pallet and their recorded placement positions. If the deviation for any box exceeds a certain threshold, the state is considered unstable.

\textbf{State.} The state $s_t = \{C_t,d_t\}$ is composed of two parts: the first part is the pallet configuration $C_t$, and the second part is the properties of the boxes in the buffer $d_t$.

We represent the pallet's state using voxels, since we need to account for the intrinsic properties of the boxes. More specifically, in addition to the dimensions of the boxes, we also consider their density and rigidity. Therefore, we quantify the pallet configuration with a 2-channel 3D array, stands for the density distribution and rigidity distribution. $d_t$ is a 1D array composed of information from $N$ boxes, where each box's properties include length, width, height, density, and rigidity. Therefore, it is an array of size $5N$, where $N$ represents the buffer capacity.

\textbf{Action.} It should include the following information: which box from the buffer to select, how to rotate the box, and where to place it on the pallet. We assume that after rotation, each box remains parallel to the boundary faces of the pallet, resulting in six possible orientations for each box. Additionally, we consider that new boxes cannot be placed unsupported in mid-air, so the placement is determined by the $(x,y)$ coordinates. We discretize the pallet's length and width into $l_p$ and $w_p$, respectively, giving $l_p \times w_p$ possible placement positions for each box.

As a result, the action $a$ belongs to $ \mathbb{R}^{N \times 6 \times l_p \times w_p} $. For instance, with $N=5$ and $l_p=w_p=25$ \cite{wu2024efficient}, this results in 18,750 possible outcomes for the action. The vast size of this action space greatly increases the complexity of the RL problem, making the optimization process substantially more challenging.

\subsection{Reward Design}
We encourage the planner to improve space utilization, as long as it is stable. Therefore, we use the proportion of the box's volume placed at each step relative to the total space as the reward for each step. The reward function we designed is as follows:
\begin{equation}
    r(s_t) = \mathbb{1}(s_t) \cdot \frac{V_{box}}{V_{max}}
\end{equation}
$V_{box}$ refers to the volume of the box selected at timestep $s_t$, while $V_{max}$ represents the theoretical maximum volume that the pallet can accommodate. $\mathbb{1}(s_t)$ serves as an indicator function that validates the physical stability of the boxes on the pallet at state $s_t$.

In addition, to improve efficiency, we introduced a penalty term. When the number of boxes in the buffer is less than $N$, if the selected index does not correspond to any box, then $r(s_t)= -1$.

Another factor that affects the overall reward is the episode length. There are three reasons for an episode to end: after selecting a box, the action masking model determines that no feasible placement points exist; placing the box results in instability; or all the boxes have been placed on the pallet.

\subsection{Integrating the Action Masking Model into the RL training process}
At each timestep $t$, after the policy outputs an action, the corresponding box is selected from the buffer based on the action's information. The box's properties are then combined with the current pallet configuration to form a multi-channel 3D array, which is fed into the action masking model. The action masking model outputs an action space mask, indicating which points are considered stable and which are unstable at that moment. We refer to the set of stable points as the `feasible set'. The model finds the point closest to the actionâ€™s chosen position within the feasible set (if there are multiple nearest points, one is randomly selected). The box is then placed in the environment at this selected point within the feasible set.

\subsection{Online action masking model learning}
% \yx{I think these contents should be in the related work?}
We choose to use a neural network as the action masking model and train it using an online approach because heuristic methods have numerous limitations that render them inadequate for our task setting.

Heuristic methods inherently face many limitations when addressing normal online 3D BPP. For example, the necessity for multiple hyperparameters in most action masking methods poses a challenge in tuning these parameters to accommodate diverse scenarios, and they fail to account for uncertainties inherent in real-world palletization execution \cite{wu2024efficient}.
When considering the density and rigidity of the boxes, the heuristic methods used in previous works become unusable. In the original scenario, the feasible set identified by the heuristic action masking model was a subset of the true feasible set. However, in this new scenario, they become an intersection, making the heuristic action masking model highly inefficient. This inefficiency is evident in the experimental results presented in Section \ref{section:experiment}. Designing a new heuristic method that incorporates both density and rigidity manually is cumbersome and lacks generalizability. 

Therefore, we need a new paradigm to solve this problem, one that relies on directly acquiring experience from the simulator. We propose the online action space mask learning paradigm, which incorporates supervised learning for the action masking model into the RL training loop, shown in Figure \ref{fig:Framework}. Initially, both the policy network and the action masking model are randomly initialized. During the RL policy rollout, we record data for action masking model training. Those data contain two parts: the configurations of the pallet $C_t$ and the properties of the next to be placed box $b_t$. 
After a certain number of timesteps, we update the action masking model along with the policy network. The update process for the action masking model involves two steps: first, generating an annotation $g_t$ for each recorded data point $(C_t, b_t)$ from the previous rollout phase; second, appending these data points along with their annotations to the dataset, called feasible dataset, and using this feasible dataset to train the action masking model.

We use multiple parallel simulators specifically created to generate annotations. In these simulators, the scene is first reconstructed based on the assigned $(C_t, b_t)$, after which the box is tested at every possible position. It is important to note that some positions can be judged using simple heuristics without requiring the simulator, such as a box placed directly on the pallet surface being inherently stable, or a box with less than 25\% support being inherently unstable.

There are some important details regarding data selection and management. For episodes that end due to instability, the corresponding data is always recorded. For regular rollout data, there is a 0.1 probability of being recorded. This ensures that the action masking model focuses on learning cases where its predictions were less accurate, while still capturing regular data to avoid overfitting. For managing the feasible dataset, we use a deque data structure, meaning that data is managed in a first-in-first-out manner. This allows the action masking model to encounter more diverse data, helping to address the issue of out-of-distribution data.

We demonstrate the effectiveness of our proposed method in the following section. 