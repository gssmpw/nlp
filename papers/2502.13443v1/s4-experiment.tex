\section{EXPERIMENTAL VALIDATIONS}
\label{section:experiment}

\begin{figure}[t]  % [h] 表示将图片放置在当前位置
    \centering
    \includegraphics[width=0.7\linewidth]{images/simulation_visualization.pdf}
    \caption{Visualization of our simulated palletization environment in MuJoCo \cite{todorov2012mujoco}. Although 40 boxes are displayed for illustrative purposes, the robot is programmed to perceive and interact with only N boxes within the buffer area. The arrangement of the boxes is randomized and unknown, shuffled anew for each RL episode.}
    \vspace{-10pt}
    \label{fig:task_setting}
\end{figure}

In this section, we present a thorough quantitative analysis of our proposed approach, and formulate our experiments to answer the following questions:
\begin{enumerate}
    \item Does the action space mask training method mentioned earlier eventually converge and learn a relatively accurate result?
    \item How are the changes in the performance of the policy and the action masking model related?
    \item Does the online action masking model learning method enhance the RL training process, resulting in a superior policy?
\end{enumerate}

\subsection{Simulation Experiments}
\textbf{Setup.} To explore the task planning challenges associated with the palletization problem, we developed a simulated palletization task in MuJoCo \cite{todorov2012mujoco} \cite{robosuite2020}, reflecting a realistic logistic scenario, as depicted in Figure \ref{fig:task_setting}. We adjusted the parameters in the simulation environment that solve for the box deformation under contact forces to give the boxes different levels of rigidity. There are four different types of boxes in the simulation environment: two soft, low-density boxes and two hard, high-density boxes. The size, density, rigidity, and quantity of boxes in the simulation are shown in Table \ref{tab:box_specifications}.

If a hard box is placed on top of a soft box in the simulation environment, the latter will collapse, which is considered an unstable situation. The pallet is specified to have dimensions of 25 $\times$ 25 inches, with a stipulation that the maximum height of stacked boxes cannot exceed 20 inches. For our experiments, we applied a discretization resolution of 1 inch to both the boxes and the pallet. In all the subsequent simulation experiments mentioned, we have chosen a buffer size of $N=5$. And we use PPO \cite{schulman2017proximal} as the RL algorithm for all experiments.

\begin{table}[t]
    \centering
    \caption{Box Specifications in the Simulation Environment.}
    \label{tab:box_specifications}
    \begin{tabular}{cccc}
        \toprule
        Dimension (inches) & Density & Rigidity & Counts\\
        \midrule
        6 $\times$ 6 $\times$ 4 & 500 & 0.5 & 10\\
        6 $\times$ 6 $\times$ 6 & 500 & 0.5 & 10\\
        6 $\times$ 6 $\times$ 6 & 5000 & 3 & 10\\
        8 $\times$ 6 $\times$ 6 & 5000 & 3 & 10\\
        \bottomrule
    \end{tabular}
    \vspace{-9pt}
\end{table}

At each step of the planning process, the task planner receives the current pallet configuration, represented as a density distribution, along with the properties of the next N boxes in the buffer, which include their dimensions and density. The planner’s task is to select one of the N boxes, determine its orientation, and place it on the pallet. Since our focus is on planning rather than manipulation, the actual `pick-rotate-place' actions by the robot are omitted. Instead, the selected box is immediately positioned at the goal pose determined by the planner, to accelerate the learning process.

To better simulate real-world palletization, we added noise to the box placement when generating feasible annotations: positional noise on the xy plane, $\delta_t \sim N(0, 0.05)$, measured in inches, and rotational noise around the z-axis, $\delta_r \sim N(0,5)$, measured in degrees.

\begin{figure*}[t]
\begin{minipage}[t]{0.32\linewidth}
    % \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/val_iou.pdf}
    \vspace{-16pt}
    \caption{IoU score on validation set. After rolling out for a certain number of timesteps, we update the action masking model and calculate the IoU score on the validation set. Results are averaged over 5 random seeds.}
    \label{fig:val_iou}
    % \end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{0.335\linewidth}
% \begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/end_reason.pdf}
    \vspace{-16pt}
    \caption{The frequency change curve for the three types of episode end reasons. During training, we record the frequency of each episode end reason, which is represented by the curve shown. Results are averaged over 5 random seeds.}
    \label{fig:end_reason}
% \end{figure}
\end{minipage}
\hfill
\begin{minipage}[t]{0.32\linewidth}
    \centering
    \includegraphics[width=1\textwidth]{images/space_utilization.pdf}
    \vspace{-16pt}
    \caption{Space utilization. Our method (OL Mask) demonstrates faster convergence and achieves better space utilization compared to the other methods. Results are averaged over 5 random seeds.}
    \label{fig:space_utilization}
\end{minipage}
% \vspace{-12pt}
\end{figure*}


\begin{table*}[t]
    \centering
    \caption{Performance evaluation of different planners under various methods.}
    \label{tab:final performance}
    \setlength{\tabcolsep}{12pt}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccccc}
        \toprule
        ~ &  Nomask & Random Selection & Random Placement & Heuristic mask & \textbf{OL Mask (Ours)} \\
        \midrule
        \textbf{Space utilization}  & 0.18 & 0.53 & 0.41 & 0.26 & \textbf{0.68} \\
        \textbf{Infeasible Rate} & \textbf{0.00} & 0.84 & 0.95 & 0.05 & 0.34 \\
        \textbf{Unstable Rate} & 1.00 & 0.14 & 0.05 & 0.95 & \textbf{0.01} \\
        \textbf{Mean Episode Length} & 12.3 & 31.8 & 25.8 & 15.9 & \textbf{39.4} \\
        \textbf{Mean Episode Reward} & 0.18  & 0.49 & 0.41 & 0.26 & \textbf{0.66} \\
        \textbf{Success Rate} & 0.00 & 0.02 & 0.00 & 0.00 &  \textbf{0.65} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-6pt}
\end{table*}

\textbf{Action space mask training results. }
During the training of the policy, we used a deque with a size of 16,000 as a feasible dataset for training the action masking model.

For each policy rollout step, if the step is unstable and causes the episode to end, that data will definitely be recorded in the dataset. If it is just a normal step, there is a 10\% chance it will be recorded in the dataset. In this way, we can make the action masking model focus more on cases where it failed to make correct judgments, ultimately leading to more accurate performance.

We split the dataset into an 8:2 ratio for the train set and validation set, using the model's IoU score on the validation set to evaluate its performance.
At each policy update, we simultaneously update the action masking model. 
The IoU score is very low at the beginning. As the total number of training epochs increases and the model encounters more new data, the capability of the action masking model gradually improves. The action masking model using the online learning method began to converge after approximately 2 million rollout timesteps, as shown in Figure \ref{fig:val_iou}. Eventually, the IoU score converges to around 98\% on the validation set

Such a high IoU score indicates that the action masking model is already capable of determining which positions are stable or unstable based on the current situation of boxes on the pallet and the information of the next box.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.4\textwidth]{images/end_reason.pdf}
%     \caption{The frequency change curve for the three types of episode end reasons. Results are averaged over 5 random seeds. Each episode can be ended for the following reasons. \textit{Infeasible:} The action masking model indicate that there is no feasible point on the pallet;  \textit{Unstable: }After placing each box, the situation on the pallet becomes unstable, resulting in a collapse. \textit{Success:} All 40 boxes have been placed properly.}
%     \label{fig:end_reason}
% \end{figure}

% \begin{table}[h]
%     \centering
%     \caption{Final frequency for episode end reasons}
%     \label{tab:end_reason}
%     \begin{tabular}{cccc}
%         \hline
%         End Reason & Infeasible & Unstable & Success\\
%         \hline
%         Frequency & 0.34 & 0.01 & 0.65 \\
%         \hline
%     \end{tabular}
% \end{table}

\textbf{Guidance from the action space mask. } To study the relative performance of the action masking model and the policy, we recorded the probability of each of the three end reasons occurring in each episode during training, as shown in Figure \ref{fig:end_reason}. Each episode can be ended for the following reasons. 
\begin{itemize}
    \item \textit{Infeasible:} The action masking model indicates that there is no feasible point on the pallet; \item \textit{Unstable:} After placing each box, the situation on the pallet becomes unstable, resulting in a collapse.
    \item \textit{Success:} All 40 boxes have been placed properly.
\end{itemize}
Initially, because the action masking model is randomly initialized, its judgments on stability are very inaccurate. As a result, in the beginning, every episode ends as unstable. However, as the action masking model continues to be trained, its accuracy gradually improves. At this point, since the policy has not yet learned how to place boxes densely, the frequency of cases where boxes are selected but have no place to be placed increases significantly. Finally, as the policy, guided by the accurate action space mask, continuously attempts different placements, it learns a compact and stable way to place boxes, leading to a significant increase in the success rate. Since the training of the action masking model is supervised learning, it is more likely to achieve better performance more quickly. Therefore, it first converges and then guides the policy to achieve good performance.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.4\textwidth]{images/space_utilization.pdf}
%     \caption{Space utilization. Results are averaged over 5 random seeds. Our method (OL Mask) converges faster and achieves better space utilization compared to other methods.}
%     \label{fig:space_utilization}
% \end{figure}


\begin{figure*}[h]  % [h] 表示将图片放置在当前位置
    \centering
    % \begin{minipage}[b]{0.03\linewidth}
    % \small (a) (b) (c)
    % \end{minipage}
    % \begin{minipage}[t]{0.89\linewidth}
    \includegraphics[width=0.93\textwidth]{images/video_sequence_yixiao.pdf}
    \vspace{-4pt}
    \caption{Real-world experiments with each row as a planning sequence. The boxes with yellow cross tape are considered to have low density and be soft, while other boxes are considered to have high density and be hard.
    1) The first two rows show the planner trained using our method, successfully placing 9 boxes on the pallet while maintaining stability, under two different box arrival sequences. 
    %
    2) The bottom row shows the planner trained with a heuristic mask, which, after stacking 5 boxes, incorrectly placed a high-density, high-rigidity box on top of a low-density, low-rigidity box, which is considered a failure case of unstable. The unstable position is marked with a red cross.}
    \label{fig:frame_sequence}
    \vspace{-12pt}
    % \end{minipage}
\end{figure*}

\textbf{Ablative experiments. }
To verify the importance of both RL planning and online action space mask learning in our method, we selected four baselines.
\begin{itemize}
  \item Nomask: Action masking is not applied during the RL training process.
  \item Heuristic mask: We use the heuristic mask from \cite{zhao2021online}, which considers two criteria: the support ratio and the number of support corners. A placement is considered stable in any of the following three cases: $\text{support ratio}>0.6 \land \text{support corners} = 4$; $\text{support ratio}>0.8 \land \text{support corners}\geq 3$; $\text{support ratio}>0.95$
  \item Random box selection: In this method, an online training action space mask is used, but the selection of boxes is independent of the actions output by the RL algorithm and is instead randomly drawn from the buffer.
  \item Random box placement: Under the premise of using an online training action space mask, the selection of boxes is determined by the policy, but their placement is random.
\end{itemize}

We use the mean episode space utilization to measure the overall quality of the final policy, as shown in Figure \ref{fig:space_utilization}. 
Initially, the space utilization of the three methods using online action masking model learning (`OL mask', `random box selection', `random box placement') is the same as that of the no mask method, due to their action masking models being randomly initialized. At the beginning, the heuristic mask method achieves the highest space utilization. However, as training progresses, the space utilization of the three online action masking methods quickly surpasses that of the other two methods.
Among all the methods, the space utilization for the `nomask' method remains the lowest throughout the training process, while the space utilization for the `nomask' method remains the highest. After 4M timesteps, the three methods using the online learning action space mask perform significantly better than the other two methods. This demonstrates the importance of the action space mask and shows that an accurate action space mask can accelerate convergence and greatly improve the planner's capability. Comparing our method with the `random box' and `random place' methods also highlights the importance of RL in the overall approach, demonstrating that each component of our method is indispensable as part of the whole.

\subsection{Real World Experiments}
We further implement the policy and action masking model learned in simulation onto a real robot arm. The gripper used in our lab environment is relatively large, which requires the boxes to also be larger. Additionally, due to the limited range of motion of the robotic arm, it is not possible to stack 40 boxes as we did in the simulation when working with larger boxes. However, in a real-world logistics warehouse with larger robotic arms, it would be possible to replicate the scenario from the simulation. We also train a new policy and a new action masking model to accommodate the real-world setting.

Since we focus solely on the planning aspect of palletization, we assume that the perception task should be handled earlier in the pipeline. We use AprilTags \cite{wang2016apriltag} to record the size, density, and softness of each box. Near the buffer, we installed an Intel RealSense camera, which scans the AprilTags and compares the data with pre-stored information in the computer to obtain the details of the boxes currently in the buffer. Due to spatial constraints, we set the buffer size to 1. Additionally, since the robotic arm can easily rotate boxes around the z-axis during operation, we only consider rotation around the z-axis.

The pallet we considered has both a length and a width of 24 inches, with a maximum allowable stacking height of 20 inches. We consider a total of 9 boxes, but there are 8 different types of boxes. Finally, our planner successfully places all 9 boxes on the pallet and achieves a stable solution, with a space utilization rate of 70\%.
The compact and stable configuration highlights the effectiveness and reliability of the learned task planner, shown in Figure \ref{fig:real_world_4p}.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{images/real_world_4p.pdf}
    \caption{Final pallet configuration viewed from 4 perspectives. In a stable situation, boxes with high density and rigidity should be placed under heavy and soft ones.}
    \vspace{-6pt}
    \label{fig:real_world_4p}
\end{figure}
