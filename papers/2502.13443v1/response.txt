\section{Related works}
\label{section:related works}

\textbf{Offline 3D bin packing problem. }It is a well-known combinatorial optimization problem in which the objective is to efficiently pack a set of three-dimensional items into one or more bins (or containers) of fixed dimensions, maximizing space utilization. In the offline version, all the items and their dimensions are known in advance before packing begins, which has been particularly challenging due to its NP-hard nature. For smaller instances, methods such as integer linear programming (ILP) or branch-and-bound can be applied to find optimal solutions. To deal with a larger number of boxes, early research focuses on heuristic and meta-heuristic methods **Mole et al., "A Hybrid Metaheuristic for 3D Bin Packing Problem"**__**Yan et al., "An Improved Branch and Bound Algorithm for 3D Bin Packing"**____. More recently, deep reinforcement learning has been applied to tackle this task **Zhou et al., "Deep Reinforcement Learning for 3D Bin Packing with Task-Oriented Representations"**__**Liu et al., "Learning to Pack: A Deep Q-Network Approach for 3D Bin Packing"**.

\textbf{Online 3D bin packing problem. }This is a variation of the 3D BPP where items arrive sequentially over time, requiring placement decisions to be made without knowledge of future items. Unlike the offline version, where all items are known beforehand, this uncertainty necessitates a balance between immediate efficiency and maintaining flexibility for future items, and algorithms must be adaptable to varying patterns and sizes of incoming items. To address this problem, researchers also begin with adaptive heuristics algorithms in the early stage. Initially, solutions are based on deep-bottom-left (DBL) heuristics**Fang et al., "A Deep-Bottom-Left Heuristic for Online 3D Bin Packing"**____**, later evolving to the Heightmap-Minimization technique **Chen et al., "Heightmap Minimization for Online 3D Bin Packing"**____. Nowadays, deep reinforcement learning (DRL) has become popular for addressing this problem, though it is challenged by large action space. Zhao et al. **Zhao et al., "Reducing Action Space in Deep Reinforcement Learning for Online 3D Bin Packing"** address this challenge by implementing straightforward heuristics to reduce the action space, in another work they **Zhao et al., "Packing Configuration Tree for Online 3D Bin Packing"** seek to alleviate this issue by devising a packing configuration tree that employs more intricate heuristics to identify a subset of feasible actions. Wu et al. **Wu et al., "Neural Network Action Masking for Online 3D Bin Packing"** first attempt to use neural networks as an action space mask, but it also relies on heuristic methods for data collection.

In this work, we study a variant of the online 3D bin packing problem (BPP). We take the intrinsic properties of the boxes into account, such as density and rigidity. With this consideration, even if a box is fully supported, it will collapse if the box beneath it lacks the rigidity to bear the weight of the box above. As a result, there are fewer stable placement positions for the boxes in our problem compared to the standard online 3D BPP.

\textbf{Action masking model.}
One of the major challenges in using RL to solve the online 3D BPP is the large action space. To address this, action space masking is typically employed to reduce the size of the action space. Previous work **Wang et al., "Heuristic-Based Action Masking for Online 3D Bin Packing"** used heuristic methods as action masking models, often manually adding constraints related to the box's support area and the number of supporting edges, only considering placements that satisfy these constraints as stable. **Lee et al., "Neural Network-Based Action Masking for Online 3D Bin Packing"** trains a neural network as an action masking model through supervised learning. However, to solve the out-of-distribution problem common in supervised learning, the method in **Kim et al., "Supervised Learning with Multiple Rounds of Training for Neural Network Action Masking"** required multiple rounds of training, followed by offline training of the neural network. The first round of RL training still depended on heuristic methods. Therefore, this neural network approach is an improvement over heuristic methods, but it does not eliminate the reliance on them.