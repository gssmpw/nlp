\section{Additional results on game-agnostic learning}\label{app:game-agnostic}

\subsection{Profile swap regret is not computable from the game agnostic transcript}\label{app:prof-from-transcript}

In this appendix, we show that it is not possible to compute the profile swap regret $\CorrSwap(\bx, \by)$ from the game-agnostic transcript $(\bx, \br)$ corresponding to the game-dependent transcript $(\bx, \by)$. Specifically, we have the following theorem.

\begin{theorem}\label{thm:prof-from-transcript}
There exists a polytope game $G$, and two transcripts $(\bx_1, \by_1)$ and $(\bx_2, \by_2)$ that have the same game-agnostic transcript $(\bx, \br)$, but where $\CorrSwap(\bx_1, \by_1) \neq \CorrSwap(\bx_2, \by_2)$.
\end{theorem}
\begin{proof}
Our polytope game $G$ will be a variant of the polytope game used to separate profile swap regret and polytope swap regret in the proof of Theorem~\ref{thm:comp-prof-swap}. Specifically, we will consider the game where $\learnset = \Delta_{2}^2$ and $\optset = \Delta_{2}^2 \times \Delta_{4}$. We will write elements $y \in \optset$ in the form $y = (y_r, y_s)$ with $y_r \in \Delta_{2}^2$ and $y_s \in \Delta_4$, and write $\Proj_1(y_{r}, y_{s}) = y_r \in \Delta_{2}^{2}$ and $\Proj_2(y_{r}, y_s) = y_{s}\in \Delta_4$. Finally, we define $u_L(x, y) = \langle x, \Proj_{1}(y)\rangle$.

As in the proof of Theorem~\ref{thm:comp-prof-swap}, we will write $\learnvert = \{v_{11}, v_{12}, v_{21}, v_{22}\}$, where $v_{11} = (1, 0, 1, 0)$, $v_{12} = (1, 0, 0, 1)$, $v_{21} = (0, 1, 1, 0)$, and $v_{22} = (0, 1, 0, 1)$. We will likewise write $\optvert = \{w_{ijk}\}_{i, j \in [2], k \in [4]}$, where $w_{ijk} = (v_{ij}, e_k)$. In particular, note that $\Proj_{1}(w_{ijk}) = v_{ij}$.

We now define two transcripts of play. Each transcript of play will be divided into four epochs of $T/4$ rounds during which both players play a fixed action. In the first transcript $(\bx_1, \by_1)$, these four action pairs are $((v_{11}, w_{111}), (v_{12}, w_{121}), (v_{21}, w_{211}), (v_{22}, w_{111}))$. In the second transcript $(\bx_2, \by_2)$, these four action pairs are $((v_{11}, w_{111}), (v_{12}, w_{122}), (v_{21}, w_{213}), (v_{22}, w_{114}))$. Note that since $\bx_{1, t} = \bx_{2, t}$ and $\Proj_{1}(\by_{1, t}) = \Proj_{1}(\by_{2, t})$ for each $t \in [T]$, it follows that both transcripts have the same game-agnostic transcript $(\bx, \br)$. 

Let $\csp_1$ be the CSP corresponding to the first transcript and $\csp_2$ be the CSP corresponding to the second transcript. Note that by following the same derivation as in the proof of Theorem~\ref{thm:comp-prof-swap}, we can write

$$\csp_1 = \frac{1}{4}(v_{12} \otimes (w_{121} + w_{111}) + v_{21} \otimes (w_{211} + w_{111})),$$

\noindent
from which we can conclude that $\CorrSwap(\csp_1) = 0$. 

On the other hand, we claim that the only decomposition of $\csp_2$ into the form

\begin{equation}\label{eq:app-prof-noncomp-1}
\csp_2 = \sum_{v \in \learnvert} \lambda_{v} (v \otimes y_{v}),.
\end{equation}

\noindent
is the decomposition given by its construction as a CSP, namely,

\begin{equation}\label{eq:app-prof-noncomp-2}
\csp_2 = \frac{1}{4}\left(v_{11}\otimes w_{111} + v_{12}\otimes w_{122} + v_{21} \otimes w_{213} + v_{22} \otimes w_{114}\right).
\end{equation}

To see this, for any $k \in [4]$, consider the bilinear function $\rho_k$ defined via $\rho_k(x \otimes y) = \langle \Proj_{2}(y), e_k\rangle x$. For each $k$, there is exactly one $v_k \in \learnvert$ and $w_{k} \in \optvert$ with the property that $v_{k} \otimes w_{k}$ is the only term in \eqref{eq:app-prof-noncomp-2} that does not vanish under $\rho_k$ (e.g., for $k=4$ it is $v_{22} \otimes w_{114}$). But applying $\rho_k$ to \eqref{eq:app-prof-noncomp-1},

$$\rho_k(\csp_2) = \sum_{v \in \learnvert} \lambda_{v}\langle y_v, e_k\rangle v.$$

Since $v_k$ is an extreme point of $\learnset$, this implies that $\langle y_{v}, e_{k}\rangle$ can be non-zero only for $v = v_k$ (for which value it must equal $1$). It follows that the only possible decomposition of $\csp_2$ is that given in \eqref{eq:app-prof-noncomp-2}. But then, since $v_{22} \not\in \BR_{L}(w_{114}) = \{v_{11}\}$, it follows that $\CorrSwap(\csp_2) > 0$ and therefore $\CorrSwap(\csp_1) \neq \CorrSwap(\csp_2)$.

\end{proof}

\subsection{Game-agnostic profile swap regret does not upper bound profile swap regret}\label{app:game-agnostic-profile-bad}

Here we prove Theorem~\ref{thm:game-agnostic-profile-bad}, showing that the game-agnostic profileswap regret $\CorrSwap_{G^{*}}(\bx, \br)$ of a transcript does not necessarily bound its (standard, game-aware) profile swap regret.

\begin{proof}[Proof of Theorem~\ref{thm:game-agnostic-profile-bad}]
We build off of the example in the proof of Theorem~\ref{thm:prof-from-transcript}. Consider specifically the second transcript $(\bx_2, \by_2)$ where $\CorrSwap_{G}(\bx_2, \by_2) > 0$. 

Note that if we define (for any $i, j \in [2]$) $r_{ij} \in \learnset^{*}$ so that $r_{ij}(x) = \langle v_{ij}, x \rangle$, then $\by_{2}$ corresponds to the sequence of rewards $\br_{2} = (r_{11}, r_{12}, r_{21}, r_{11})$ . We now claim that $\CorrSwap_{G^{*}}(\bx_2, \br_2) = 0$. This will follow for similar logic to the original proof that $\CorrSwap_{G}(\bx_1, \by_1) = 0$. Indeed, the CSP $\csp^{*}$ of $(\bx_{2}, \br_{2})$ in $G^{*}$ is given by

$$\phi^{*} = \frac{1}{4}(v_{11} \otimes r_{11} + v_{12} \otimes r_{12} + v_{21} \otimes r_{21} + v_{22} \otimes r_{11})$$

By following the same derivation as in the proof of Theorem~\ref{thm:comp-prof-swap}, we can rewrite this in the form

$$\phi^{*} = \frac{1}{4}(v_{12} \otimes (r_{11} + r_{12}) + v_{21} \otimes (r_{11} + r_{21})).$$

We can check that this decomposition has zero profile swap regret under $G^*$, and so $\CorrSwap_{G^*}(\bx_2, \br_2) = 0$.
\end{proof}