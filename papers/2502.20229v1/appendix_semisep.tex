\section{Orthant-approachability via semi-separation}\label{app:semi-sep}

The goal of this appendix is to prove Theorem~\ref{thm:approach-semisep}; that access to an efficient semi-separation oracle suffices for efficiently performing Blackwell approachability. As mentioned in Section \ref{sec:algorithms}, the proof of this theorem follows very closely from the proof of \cite{daskalakis2024efficient}, who proved this theorem for the special class of approachability problems arising from linear swap-regret minimization, but in a fairly general way that easily extends to the setting of Theorem~\ref{thm:approach-semisep}.

To avoid the unnecessary redundancy of copying the entire proof of \cite{daskalakis2024efficient} with minor changes (but still in an attempt to be relatively self-contained here), we will cite two main ingredients from \cite{daskalakis2024efficient} (describing whatever changes, if any, need to be made to the proof to extend them to our setting) and then describe how to combine them into an efficient algorithm for orthant-approachability.

The first ingredient is a procedure \cite{daskalakis2024efficient} call \emph{Shell Projection}. Shell Projection can be thought of as a ``semi-separation'' variant of the operation of projecting an arbitrary $u$ to the set $\cU$ (a common primitive in many learning algorithms). In Shell Projection, instead of returning a projection of $u$ onto $\cU$ (which is computationally difficult without an actual separation oracle for $\cU$), we instead return a projection of $u$ onto some convex superset $\widetilde{\cU}$ of $\cU$, but with the guarantee that this projection is response-satisfiable.

Before we present this theorem, recall that a semi-separation oracle for a set $\cU$ of bi-affine functions takes any bi-affine function and either returns that $u$ is response-satisfiable or produces a hyperplane separating $u$ from $\cU$. Throughout the rest of this section, we will let $\cU_{RS}$ denote the (non-convex) set of response-satisfiable bi-affine functions $u$ (i.e., all $u$ with the property that there exists some $x \in \learnset$ such that $u(x, y) \leq 0$ for all $y \in \optset$). It is worth mentioning that nothing about the Shell Projection routine below (or the one originally presented in \cite{daskalakis2024efficient}) requires anything specific about the structure of response-satisfiable bi-affine functions -- indeed, the same theorem holds for a general notion of semi-separation of a convex set $\cK$ relative to a (possibly non-convex) superset $\cK_{NC} \supseteq \cK$, where given a point $x$ in the ambient space of $\cK$, the semi-separation oracle either returns that $x$ belongs to $\cK_{NC}$ or returns a separating hyperplane separating $x$ from $\cK$.

\begin{theorem}[Shell Projection]\label{thm:dffps-proj}
    There is an algorithm $\ShellProj$ which takes as input:

    \begin{itemize}
        \item A semi-separation oracle separating a convex set $\cU$ relative to some set $\cU_{RS} \supset \cU$,
        \item A known ball of radius $\rho$ contained within $\cU$,
        \item A superset $\cU'$ of $\cU$ with $\diam(\cU') \leq D$, provided via an efficient separation oracle,
        \item An element $u \in \cU'$, and
        \item A precision $\eps > 0$,
    \end{itemize}

    \noindent
    runs in time $\poly(\dim(\cU), \eps^{-1}, \rho^{-1}, D)$, and returns:
    \begin{itemize}
        \item A convex set $\widetilde{\cU}$ satisfying $\cU \subseteq \widetilde{\cU} \subseteq \cU'$, constructed by intersecting $\cU'$ with at most $\poly(\dim(\cU), \eps^{-1}, \rho^{-1}, D)$ half-spaces, and
        \item A \emph{response-satisfiable} $\tilde{u} \in \widetilde{\cU}$ satisfying

        $$||\tilde{u} - \Proj_{\widetilde{\cU}}(u)|| \leq \eps.$$
    \end{itemize}
\end{theorem}
\begin{proof}
The proof directly follows from the proof of Theorem 4.4 in \citep{daskalakis2024efficient}, with the following changes:

\begin{itemize}
    \item $\Phi(\cP)$, $\Phi_{FP}$, $\tilde{\Phi}$, $\cM$, $\phi$, and $\phi'$ should be replaced by $\cU$, $\cU_{RS}$, $\widetilde{\cU}$, $\cU'$, $u$, and $u'$ respectively. 
    \item Theorem 4.4 uses the fact that if $\cP$ circumscribes and is inscribed in balls of radii $r$ and $R$ respectively, then $\Phi(\cP)$ contains a ball of radius $r/2R$. Here, we simply make the assumption that $\cU$ contains a ball of radius $\rho$ (and so, $\rho^{-1}$ appears in place of $R/r$ in our time complexity).
    \item The dimension of $\Phi(\cP)$ is $d(d+1)$ (where $d = \dim(\cP)$), and a polynomial dependence in $d$ appears in the time complexity. Instead, we directly include a polynomial dependence on $\dim(\cU)$ in our time complexity.
\end{itemize}

At a very high-level, this Shell Projection routine works by first strengthening the semi-separation oracle into a subroutine called Shell Ellipsoid, which can semi-separates \emph{convex sets} $\cK$ from $\cU$ instead of individual points (returning either a hyperplane separating $\cK$ from $\cU$, or a point in $\cK$ belonging to $\cU_{RS}$). They then repeatedly run Shell Ellipsoid on larger and larger balls centered at $u$ until they find one with a point $\tilde{u}$ in $\cU_{RS}$ -- by the guarantees of Shell Ellipsoid, they can then be guaranteed that the point $\tilde{u}$ is close to a projection of $u$ onto some set containing $\cU$.
\end{proof}

The second ingredient we will need from \cite{daskalakis2024efficient} is a slight variant of projected gradient descent they call Shell Gradient Descent (Algorithm 2 in Section 4.3 of \cite{daskalakis2024efficient}). Shell Gradient Descent solves the following variant of standard online linear optimization (that we will call Shell OLO). In Shell OLO, at the beginning of every round $t$, the learner is told (i.e., given efficient oracle access to) a ``shell'' set $\cX_{t}$, from which they must pick their action $x_{t}$ in round $t$. Their goal is to compete with the best fixed action in (some subset of) the intersection of all the sets $\cX_{t}$. \cite{daskalakis2024efficient} prove the following theorem.

\begin{theorem}[Shell Gradient Descent]\label{thm:dffps-grad}
Let $\cX_{1}, \cX_{2}, \dots, \cX_{T}$ be an arbitrary sequence of convex sets satisfying $\cX_{t} \subseteq \cB_{d}(0, D)$, let $\ell_{1}, \ell_{2}, \dots, \ell_{T} \in [-1, 1]^d$ be an arbitrary sequence of adversarial losses, and let $\cX$ be an arbitrary subset of $\bigcap_{t=1}^{T} \cX_{t}$. Then, if we choose $x_1 \in \cX_1$ arbitrarily and choose $x_{t} = \Proj_{\cX_t}(x_{t-1} - \eta_{t-1}\ell_{t-1})$ (for some sequence of step sizes $\eta_t > 0$), this sequence of actions satisfies

$$\max_{x^* \in \cX} \sum_{t=1}^{T} \langle x_t - x^{*}, \ell_t\rangle \leq \frac{D^2}{2\eta_{T}} + \sum_{t=1}^{T}\frac{\eta_{t}}{2} ||\ell_{t}||^2$$
\end{theorem}
\begin{proof}
See Theorem 4.3 of \cite{daskalakis2024efficient}. (Note that this follows nearly immediately from the standard analysis of projected gradient descent).
\end{proof}

We now describe how these pieces fit together to produce an efficient algorithm for orthant-approachability (and thus, allow us to prove Theorem~\ref{thm:approach-semisep}). The starting point is the main idea behind the standard reduction from approachability to online linear optimization (popularized by \cite{abernethy2011blackwell}), which is that if we can choose a sequence of bi-affine functions $u_t \in \cU$ that forces the quantity 

$$\Reg^{\mathrm{dual}}(\bu, \bx, \by) = \max_{u^{*} \in \cU} \sum_{t=1}^{T} u^{*}(x_t, y_t) - \sum_{t=1}^{T} u_{t}(x_t, y_t)$$

\noindent
to grow sublinearly in $T$, and in addition choose our actions $x_t \in \learnset$ so that $u_{t}(x_t, y) \leq 0$ for any $y \in \optset$, it follows that the quantity

$$\AppLoss(\bx, \by) = \max_{u^{*} \in \cU}\sum_{t=1}^{T} u^{*}(x_t, y_t)$$

\noindent
is at most $\Reg^{\mathrm{dual}}(\bu, \bx, \by)$ and is therefore sublinear in $T$. 

Ordinarily, if we are given oracle access to $\cU$, we can guarantee that $\Reg^{\mathrm{dual}}(\bu, \bx, \by) = o(T)$ by simply running any no-external-regret algorithm for online linear optimization where the action set is $\cU$ and the reward function in round $t$ is the linear function sending $u$ to $u(x_t, y_t)$. But without oracle access to $\cU$, it is hard to even guarantee that $u_t$ belongs to $\cU$. The key observation of \cite{daskalakis2024efficient} is that it is not necessary that $u_t$ belong $\cU$, but just that each $u_t$ is response-satisfiable (so that $u_t(x_t, y_t) \leq 0$ holds for some $x_t \in \learnset$ independently of $y_t$) and that the sequence of $u_t$ incurs low regret.

This is where Theorems~\ref{thm:dffps-proj} and \ref{thm:dffps-grad} enter the picture. We will choose our sequence $u_t$ by running Shell Gradient Descent over sets generated by $\ShellProj$. This gives us the regret guarantees of Shell Gradient Descent (Theorem~\ref{thm:dffps-grad}) while also ensuring that each $u_t$ is response-satisfiable (by Theorem~\ref{thm:dffps-proj}).


    

\begin{algorithm}[ht]
    \caption{Orthant-approachability algorithm via semi-separation (analogue of Algorithm 
4 in \cite{daskalakis2024efficient}) %\jon{why is this centered?}
}\label{algo:semisep-approach}
\raggedright
    
    \textbf{Input:} a superset $\cU'$ of $\cU$ contained within $\cB_{d}(D)$, a ball of radius $\rho$ contained within $\cU$, and a semi-separation oracle separating $\cU$ from $\cU_{RS}$. 

    Set step size $\eta := \frac{D}{D_{\cX}D_{\cY}}T^{-1/2}$ and precision $\epsilon := \frac{D}{D_{\cX}D_{\cY}}T^{-1/2}$
    
    Let $u_1$ be any point in $\cU'$ and $x_1$ be any point in $\learnset$

    \For{$t = 1, 2, \dots, T$}{
        Output $x_t \in \learnset$ and receive optimizer action $y_t \in \optset$.

        Set $L_t \in \Rset^{\dim(\cU)}$ so that for any element $u \in \cU$, $\langle u, L_t \rangle = -u(x_t, y_t)$.

        Run $\ShellProj$ on $u_t - \eta L_t$ with superset $\cU'$ and precision $\epsilon$, receiving a shell set $\widetilde{\cU}_{t+1}$ (only used in the regret analysis) and response-satisfiable element $u_{t+1} \in \widetilde{\cU}_{t+1} \cap \cU_{RS}$.

        Compute an $x_{t+1} \in \learnset$ such that $u_{t+1}(x_{t+1}, y) \leq 0$ for all $y \in \optset$ (guaranteed to exist since $u$ is response-satisfiable, and computable efficiently via linear programming).
    }
\end{algorithm}





\begin{theorem}[Approachability via a semi-separation oracle (restatement of Theorem~\ref{thm:approach-semisep})]\label{thm:approach-semisep-restate}
Consider an orthant-approachability instance $(\cX, \cY, \cU)$ where $\cX \subseteq \cB(D_{\cX})$, $\cY \subseteq \cB(D_{\cY})$, and $\cB(u_{0}, \rho) \subseteq \cU \subseteq \cB(D)$ (for some known $u_{0}$ and radius $\rho$). If we have access to efficient separation oracles for $\cX$ and $\cY$, and an efficient semi-separation oracle for the set $\cU$, then Algorithm~\ref{algo:semisep-approach} has the guarantee that

$$\max_{u \in \cU} \frac{1}{T}\sum_{t=1}^{T} u\left(x_t, y_t \right) \leq O(D_{\cX}D_{\cY}D\sqrt{T}),$$

\noindent
and runs in time $\poly(D_{\cX}, D_{\cY}, D, \rho^{-1}, \dim(\cU), T)$.
\end{theorem}
\begin{proof}
This proof closely follows the proof of Theorem 4.5 in \cite{daskalakis2024efficient}.

Note that the sequence $u_t$ generated by Algorithm~\ref{algo:semisep-approach} is almost the same sequence as would be generated by running Shell Gradient Descent on the sequence of shell sets $\widetilde{\cU}_{t}$, with the only caveat that $u_{t+1}$ is distance at most $\eps$ from the true projection of $u_{t} - \eta L_{t}$ onto $\widetilde{\cU}_{t+1}$. The guarantees of Theorem~\ref{thm:dffps-grad} then imply that

\begin{equation}\label{eq:app-semisep-proof-1}
    \max_{u^* \in \cU} \sum_{t=1}^{T} \langle u_t - u^{*}, L_t\rangle \leq \frac{D^2}{2\eta} + \sum_{t=1}^{T}\frac{\eta}{2} ||L_{t}||^2 + \eps\sum_{t=1}^{T} ||L_{t}|| = O(DD_{\cX}D_{\cY}\sqrt{T}), 
\end{equation}

\noindent
where here we have used the fact that $||L_{t}|| \leq D_{\cX}D_{\cY}$ and $\eta = \eps = \frac{D}{D_{\cX}D_{\cY}}T^{-1/2}$. But since $\langle u_t, L_t \rangle = -u_{t}(x_t, y_t)$, the left hand side of \eqref{eq:app-semisep-proof-1} is equal to $\max_{u^{*} \in \cU} \sum_{t=1}^{T} u^{*}(x_t, y_t) - \sum_{t=1}^{T} u_t(x_t, y_t) = \Reg^{\mathrm{dual}}(\bu, \bx, \by)$. Moreover, since each $u_{t}(x_t, y_t) \leq 0$ (by the choice of $x_t$), we have that 

$$\max_{u \in \cU} \frac{1}{T}\sum_{t=1}^{T} u\left(x_t, y_t \right) \leq \Reg^{\mathrm{dual}}(\bu, \bx, \by) \leq O(DD_{\cX}D_{\cY}\sqrt{T}),$$

\noindent
as desired.
\end{proof}