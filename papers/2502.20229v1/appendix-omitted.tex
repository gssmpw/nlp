\section{Omitted proofs}\label{sec:omitted}

\subsection{Proof of Theorem~\ref{thm:nfg-swap-notions}}

\begin{proof}[Proof of Theorem~\ref{thm:nfg-swap-notions}]
When $\learnset = \Delta_{m}$, the set of affine linear transformations that sends $\learnset$ to itself can be expressed the set of $m$-by-$m$ row-stochastic matrices (this follows since every unit vector must map to an element of $\Delta_{m}$). This is a convex set whose extreme points are given by row-stochastic matrices which correspond to swap functions; i.e., 0/1 row-stochastic matrices where each row is a unit vector. Maximizing over all transformations in this set is equivalent to maximizing over all swap functions, so $\LinSwap(\bx, \by) = \Swap(\bx, \by)$.

For polytope swap regret, note that when $\learnset = \Delta_{m}$, there is a unique way to decompose an element in $x \in \learnset$ into an element $\xV \in \Delta(\learnvert)$ (specifically, $x$ itself provides the unique such decomposition). This means that we can simplify the definition of polytope swap regret in this case to $\PolySwap(\bx, \by) =  \max_{\pi: \learnvert \rightarrow \learnvert}(\sum_{t=1}^{T} u_L(\pi(x_t), y_t) - u_L(x_t, y_t))$, which is identical to the definition of $\Swap(\bx, \by)$.

Finally, again, the fact that any action $\xV \in \Delta(\learnvert)$ with $\E_{v \sim \xV}[v] = x$ must in fact satisfy $\xV_i = x_i$ for all $i \in [m]$ means that for normal-form games, $\NormSwap(\bx, \by) = \Swap(\bx, \by)$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:swap-notions-ordering}}

\begin{proof}[Proof of Theorem~\ref{thm:swap-notions-ordering}]
We begin by proving the two inequalities above. To show that $\LinSwap(\bx, \by) \leq \PolySwap(\bx, \by)$, first note that we can relax the set of swap functions $\pi$ we consider in the definition of polytope swap regret to all functions $\pi$ from $\learnvert$ to $\learnset$ (instead of just functions from $\learnvert$ to $\learnvert$). This relaxation does not change the value of polytope swap regret, since for any vertex $v \in \learnvert$, the value of $\pi(v)$ that maximizes polytope swap regret will be achieved at an extreme point of $\learnvert$. But now, for any affine linear function $\psi: \learnset \rightarrow \learnset$, if we consider the swap function $\pi$ with $\pi(v) = \psi(v)$ for all $v \in \learnset$, it is the case that $\overline{\pi}(\xV_t) = \E_{v \sim \xV_t}[\psi(v)] = \psi(\E_{v\sim \xV_{t}}[v]) = \psi(x_t)$ (in particular, since $\psi$ is linear, it commutes with taking expectations). It follows that the polytope swap regret is at least $\sum_{t=1}^{T} u_L(\psi(x_t), y_t) - \sum_{t=1}^{T} u_L(x_t, y_t)$, for any linear function $\psi$ that maps $\learnset$ to itself. Since $\LinSwap(\bx, \by)$ is the maximum of this quantity over all such linear functions $\psi$, it follows that $\LinSwap(\bx, \by) \leq \PolySwap(\bx, \by)$.

To show that $\PolySwap(\bx, \by) \leq \NormSwap(\bxV, \byV)$, note that we can write $\PolySwap(\bx, \by)$ as the minimum of $\NormSwap(\bxV, \byV)$ over all choices of $\bxV$ and $\byV$ with $x_{t} = \E[\bxV_{t}]$ and $y_{t} = \E[\byV_{t}]$. Since the $\bxV$ and $\byV$ in this theorem also satisfy these constraints, it immediately follows that $\PolySwap(\bx, \by) \leq \NormSwap(\bxV, \byV)$.


We now provide examples of transcripts exhibiting the gaps described in the second part of the theorem. An example of a family of transcripts where $\LinSwap(\bx, \by) = 0$ but $\PolySwap(\bx, \by) = \Omega(T)$ can be found in Theorem 7 of \cite{MMSSbayesian}. 

To construct a family of transcripts where $\PolySwap(\bx, \by) = 0$ but $\NormSwap(\bxV, \byV) = \Omega(T)$, consider the polytope game where $\cX = [0, 1]^2$ (and therefore $V(\cX) = \{(0,0), (1, 0), (1, 1), (0, 1)\}$) and $\cY = [-1, 1]$, with $u_{L}((x_1, x_2), y) = y(x_1 - x_2)$. Consider the transcript $(\bx, \by)$ where $x_t = (1/2, 1/2)$ for all $t \in [T]$ but $y_t = -1$ for rounds $t \in [1, T/2]$ and $y_t = 1$ for rounds $t \in [T/2 + 1, T]$.

Note that if we decompose every $x_t$ into the distribution $\frac{1}{2}(0, 0) + \frac{1}{2}(1, 1) \in \Delta(V(\cX))$, there is no swap function $\pi^*: V(\cX) \rightarrow V(\cX)$ that will improve the utility of the learner -- for example, on average when the learner plays the vertex $(0,0)$, they face the average optimizer strategy of $\overline{y} = 0$, and all elements of $V(\cX)$ are best responses to $\overline{y}$. It follows that $\PolySwap(\bx, \by) = 0$. 

On the other hand, consider the vertex game transcript where $\xV_t = \frac{1}{2}(0, 0) + \frac{1}{2}(1, 1)$ for rounds $t \in [1, T/2]$, and $\xV_t = \frac{1}{2}(0, 1) + \frac{1}{2}(1, 0)$ for rounds $t \in [T/2 + 1, T]$ (with $\yV_t = y_t$ for all $t \in [T]$). It is straightforward to check that this transcript satisfies $\E[\xV_t] = x_t$ and $\E[\yV_t] = y_t$. However, on the rounds where the learner plays $(0, 0)$, they face an average optimizer strategy of $\overline{y} = -1$. Against this optimizer strategy, the unique best-response for the learner is the strategy $(0, 1)$, and the learner can increase their total utility by $T/4$ by applying this swap. It follows that $\NormSwap(\bxV, \byV) = \Omega(T)$.
\end{proof}

\subsection{Proof of Theorem~\ref{thm:menu_char}}

\begin{proof}[Proof of Theorem~\ref{thm:menu_char}]
See the proof of Theorem 3.3 in \cite{paretooptimal} (this proof is for a normal-form game $G$, but does not use anything specific about the space of CSPs, and applies essentially as is to our setting).

It is worth noting that one direction of this argument -- that $\cM$ needs to contain some CSP of the form $x \otimes y$ for each $y \in \optset$ follows simply from the fact that the optimizer can always play the action $y_t = y$, in which case some element of this form is guaranteed to exist in $\cM$. It is also possible to prove a slightly weaker version of the converse directly from Blackwell's approachability theorem (which is all we need in this paper): that if $\cM$ contains some CSP of the form $x \otimes y$ for each $y \in \optset$, then some \emph{subset} $\cM'$ of $\cM$ is a valid asymptotic menu (see Lemma 3.4 in \cite{paretooptimal}). Proving the full converse requires showing that we can always extend a menu $\cM'$ to any superset of $\cM'$ (which can be accomplished by offering the optimizer the choice to play a choice of CSPs outside $\cM'$ -- see Lemma 3.5 in \cite{paretooptimal}). 
\end{proof}

\subsection{Proof of Theorem~\ref{thm:comp-prof-swap}}

\begin{proof}[Proof of Theorem~\ref{thm:comp-prof-swap}]
We begin by proving the two inequalities. To show that $\LinSwap(\bx,\by) \leq \CorrSwap(\bx, \by)$, let $\csp$ be the average CSP of the transcript $(\bx, \by)$, and let $\csp = \sum_{k}\lambda_{k}(x_{(k)} \otimes y_{(k)})$ be any decomposition of $\csp$ in the form \eqref{eq:csp-decomp}. Consider also any linear transformation $\psi$ that sends $\learnset$ to $\learnset$. Note that $\sum_{k}\lambda_{k}\Reg(x_{(k)}, y_{(k)}) \geq \sum_{k}\lambda_{k}(u_{L}(\psi(x_{(k)}), y_{(k)}) - u_{L}(x_{(k)}, y_{(k)})) = \frac{1}{T}\sum_{t}(u_{L}(\psi(x_t), y_t) - u_L(x_t, y_t))$, where the last equality follows by the linearity of $\psi$. Taking the minimum of the left-hand side of this inequality (over all decompositions of $\csp$) and the maximum of the right-hand side (over all linear endomorphisms $\psi$), we obtain that $\LinSwap(\bx, \by) \leq \CorrSwap(\bx, \by)$.

To show that $\CorrSwap(\bx, \by) \leq \PolySwap(\bx, \by)$, note that we can rewrite $\PolySwap(\bx, \by)$ in the form:

 \begin{align}
    \PolySwap(\bx, \by) &= \min_{\bxV} \max_{\pi: \learnvert \rightarrow \learnvert} \left(\sum_{t=1}^{T} u_L(\overline{\pi}(\xV_t), y_t) - u_L(x_t, y_t) \right) \notag \\
    &= T \cdot \min_{\bxV} \max_{\pi: \learnvert \rightarrow \learnvert} \left(\sum_{v \in \learnset} \alpha_v \left(u_L(\overline{\pi}(v), y_v) -  u_L(v, y_v)\right) \right) \label{posr_rewrite}
    % &= \min_{\bxV}  \left(\sum_{v \in \learnset} \alpha_v \Reg(v,y_v) \right) \tag{*}\label{posr_rewrite}
    \end{align}

\noindent
where the $\alpha_v$ and $y_v$ in \eqref{posr_rewrite} are obtained by expanding $\xV_t$ out as an explicit convex combination of vertices in $\learnvert$. In particular, every choice of per-round decompositions $\bxV$ in the outer minimum gives rise to a collection of $\alpha_v$ and $y_v$ satisfying $\alpha_v \geq 0$, $\sum_{v} \alpha_v = 1$, and $y_v \in \optset$. Since profile swap regret is the minimum over \emph{all} such decompositions (see \eqref{eq:vertex-csp}), $\CorrSwap(\bx, \by) \leq \PolySwap(\bx, \by)$.

We now prove the second part of the theorem, presenting families of transcripts that exhibit the above gaps. We provide an indirect proof that there exists an example where $\LinSwap(\bx, \by) = 0$ but $\CorrSwap(\bx, \by) = \Omega(T)$, by noticing that Theorem 8 of \cite{MMSSbayesian} provides an example of a transcript $(\bx, \by)$ in a polytope game with $\LinSwap(\bx, \by) = 0$ but that is manipulable. By Theorem~\ref{thm:poly_nonmanip}, it must be the case that $\CorrSwap(\bx, \by) > 0$. It is also possible to verify this example directly, but is computationally somewhat messy.

We now present an example where $\CorrSwap(\bx, \by) = 0$ but $\PolySwap(\bx, \by) = \Omega(T)$. This example is adapted from a counterexample in \cite{rubinstein2024strategizing} used to show that there exist transcripts with high polytope swap regret that were not manipulable. We will set $\learnset = \optset = \Delta_{2}^{2} = \{(a_1, a_2, b_1, b_2) \mid a_1 + a_2 = 1, b_1 + b_2 = 1, a_1, a_2, b_1, b_2 \in \Rset_{\geq 0}\}$ (this can be interpreted as a Bayesian game with two types and two actions for both players). For this set, we can write $\learnvert = \optvert = \{v_{11}, v_{12}, v_{21}, v_{22}\}$, where $v_{11} = (1, 0, 1, 0)$, $v_{12} = (1, 0, 0, 1)$, $v_{21} = (0, 1, 1, 0)$, and $v_{22} = (0, 1, 0, 1)$. We will let the learner's payoff be simply $u_L(x, y) = \langle x, y\rangle$. 

Now, consider the following transcript of play:

\begin{itemize}
    \item For $0 <  t \leq T/4$, $x_{t} = y_{t} = v_{11}$.
    \item For $T/4 < t \leq T/2$, $x_{t} = y_{t} = v_{12}$.
    \item For $T/2 < t \leq 3T/4$, $x_{t} = y_{t} = v_{21}$.
    \item For $3T/4 < t \leq T$, $x_{t} = v_{22}$ and $y_{t} = v_{11}$. \textbf{Note that $x_{t} \neq y_{t}$ for this period.}
\end{itemize}

We claim that $\PolySwap(\bx, \by) = \Omega(T)$ for this transcript of play. Indeed, note that since the learner plays an extreme point of $\learnset$ every round, any decomposition $\bxV$ must satisfy $\xV_{t} = x_t$. Now, the map $\pi$ which sends $v_{22}$ to $v_{11}$ (and fixes all other vertices) improves the learner's utility by $T/4 = \Omega(T)$.

On the other hand, we claim that $\CorrSwap(\bx, \by) = 0$. To see this, note that the CSP corresponding to this transcript can be written in the form

\begin{eqnarray*}
\csp &=& \frac{1}{4}\left(v_{11} \otimes v_{11} + v_{12} \otimes v_{12} + v_{21} \otimes v_{21} + v_{22} \otimes v_{11} \right)\\
&=& \frac{1}{4}\left((v_{11} + v_{22}) \otimes v_{11} + v_{12} \otimes v_{12} + v_{21} \otimes v_{21} \right) \\
&=& \frac{1}{4}\left((v_{12} + v_{21}) \otimes v_{11} + v_{12} \otimes v_{12} + v_{21} \otimes v_{21} \right) \\
&=& \frac{1}{4}\left(v_{12} \otimes (v_{12} + v_{11}) + v_{21} \otimes (v_{21} + v_{11}) \right). \\
\end{eqnarray*}

\noindent
Here, in the third line, we have used the fact that $v_{11} + v_{22} = v_{12} + v_{21}$. But now, note that $v_{12} \in \BR_{L}(v_{12} + v_{11})$, and $v_{21} \in \BR_{L}(v_{21} + v_{11})$. It follows that $\CorrSwap(\csp) = 0$, as desired.
\end{proof}






\subsection{Proof of Theorem~\ref{thm:equilibrium-gap}}

We will need the following two lemmas.

\begin{lemma} Consider any $v \in \cV(\learnset)$ and $\cV(\optset)_{0} \subseteq \cV(\optset)$ such that for all $w_{0} \in \cV(\optset)_{0}$, $(v,w_{0})$ has support zero in any NFCE. Then, if $(v,\hat{w}) \leq (\bar{v},\hat{w})$ for some $\bar{v} \in \cV(\learnset)$ and $\forall \hat{w} \in \cV(\optset) \setminus \cV(\optset)_{0}$, then for all $\hat{w} \in \cV(\optset)$ such that $(v,\hat{w}) < (\bar{v},\hat{w})$, $(v,\hat{w})$ has support zero in any NFCE.  \label{lem:elim_x}
\end{lemma}

\begin{proof}
Consider some $(v,\hat{w})$ pair that satisfies the conditions above, and assume for contradiction that this pair has support $\alpha > 0$ in some NFCE $\phi^{V}$. Conditioned on being recommended vertex $v$, Player $\cX$'s marginal distribution must be over only vertices in $\optvert \setminus \optvert_{0}$, as otherwise a vertex pair $(v,w_{0})$ would have nonzero support on some NFCE. Therefore, the normal-form swap regret of this NFCE is

\begin{align*}
    \NormSwap_{1}(\phi^{V}) & = \max_{\pi^{*}:\learnvert \mapsto \learnvert}\sum_{\forall v_i}\sum_{\forall w_j}\phi^{V}_{v_i,w_j}\left( u_{1}(\phi^{*}(v_i),w_j) - u_{1}(v_i,w_j) \right) \\
    & \geq \max_{\pi^{*}:\learnvert \mapsto \learnvert}\sum_{\forall w_j}\phi^{V}_{v,w_j}\left( u_{1}(\phi^{*}(v),w_j) - u_{1}(v,w_j) \right) \\
    & \geq \sum_{\forall w_j}\phi^{V}_{v,w_j}\left( u_{1}(\bar{v},w_j) - u_{1}(v,w_j) \right) \\
    & = \phi^{V}_{v,\hat{w}}\left( u_{1}(\bar{v},\hat{w}) - u_{1}(v,\hat{w}) \right) + \sum_{\forall w \ne \hat{w}}\phi^{V}_{v,w}\left( u_{1}(\bar{v},w) - u_{1}(v,w) \right) \\
     & \geq \phi^{V}_{v,\hat{w}}\left( u_{1}(\bar{v},\hat{w}) - u_{1}(v,\hat{w}) \right) \tag{As by assumption, for all $w \in \optvert$, either $u_1(\bar{v},w) > u_1(v,w)$ or $\phi^{V}_{v,w} = 0$} \\
       & = \alpha \left( u_{1}(\bar{v},\hat{w}) - u_{1}(v,\hat{w}) \right) > 0 \tag{By our assumption on the support and utility of $(v, \hat{w})$} \\
\end{align*}
Thus, the normal-form-swap regret of our NFCE $\phi^{V}$ is $> 0$, leading to a contradiction. 
\end{proof}


\begin{lemma} Consider any $w \in \cV(\optset)$ and $\cV(\learnset)_{0} \subseteq \cV(\learnset)$ such that for all $v_{0} \in \cV(\learnset)_{0}$, $(v_{0},w)$ has support zero in any NFCE. Then, if $(\hat{v},w) \leq (\hat{v},\bar{w})$ for some $\bar{w} \in \cV(\optset)$ and $\forall \hat{v} \in \cV(\learnset) \setminus \cV(\learnset)_{0}$, then for all $\hat{v} \in \cV(\learnset)$ such that $(\hat{v},w) < (\hat{v},\bar{w})$, $(\hat{v},w)$ has support zero in any NFCE. \label{lem:elim_y}
\end{lemma}

\begin{proof}
    The proof is symmetric to the proof of Lemma~\ref{lem:elim_x}.
\end{proof}

We can now prove Theorem~\ref{thm:equilibrium-gap}.

\begin{proof}[Proof of Theorem~\ref{thm:equilibrium-gap}]
First, note that by Theorem~\ref{thm:comp-prof-swap}, if $\csp = \Proj(\cspV)$, we must have that $\CorrSwap_{X}(\csp) \leq \NormSwap_{X}(\cspV)$ and $\CorrSwap_{Y}(\csp) \leq \NormSwap_{Y}(\cspV)$. Therefore, if $\cspV$ is a normal-form CE, it follows that $\csp$ is a profile CE.

We now construct a polytope game $G$ where $\csp$ is a profile CE, but no normal-form CE $\cspV$ has the property that $\Proj(\cspV) = \csp$.

The game is as follows. The two players share the same action set $\learnset = \optset = [0,1]^2 \times \{1\}$, and so $\learnvert = \optvert = \{[0,0,1],[0,1,1],[1,1,1],[1,0,1]\}$. The utility function $u_X$ of the $\cX$-player is\footnote{Throughout this proof, we regularly flatten elements of $\learnset \otimes \optset$ to elements of $\Rset^{9}$ for convenience of notation. In particular, we use the notation that $v \otimes w = [v_1w_1, v_1w_2, v_1w_3, v_2w_1, \dots, v_3w_3]$. We also represent bilinear functions by elements of $\Rset^{9}$ in a similar way; i.e., we define $u_X$ and $u_Y$ so that $u_X(\csp) = \langle u_x, \csp\rangle$.}

$$u_{X} = [-1, -1, 0.5, 0, 1, 0, -1, -1, -1].$$ 

The utility function of the $\cY$-player is

$$u_{Y} = [-0.5, 1, 1, -0.5, 0.5, 1, 0.5, -1, -0.5]$$

Finally the CSP $\csp$ is 

$$\csp = [0, 0.2, 0.4, 0.2, 0.4, 0.6, 0.4, 0.4, 1].$$

The remainder of the proof is a computation that justifies the correctness of this counterexample.

While the utilities are a direct function of the underlying dimensions of $\learnset$ and $\optset$, we can also compute the utilities of the game defined by $\learnvert$ and $\optvert$ by computing, for each extreme point $v$ and $w$ in the vertex game, $u_{X}(\pi(v \otimes w))$. Doing so, we can write out the utilities of Player $\cX$ and Player $\cY$ in the vertex game:


\begin{blockarray}{ccccc}
[0,0,1] & [0,1,1] & [1,1,1] & [1,0,1]  \\
\begin{block}{(cccc)c}
  -1 & -2 & -3 & -2 & [0,0,1]\\
  -1 &  -1 &  -2 & -2 & [0,1,1]\\
  -0.5 & -1.5 & -3.5 & -2.5 & [1,1,1]\\
  -0.5 & -2.5 & -4.5 & -2.5 & [1,0,1]\\
\end{block}
\end{blockarray}
Utility of Player $\cX$

\begin{blockarray}{ccccc}
[0,0,1] & [0,1,1] & [1,1,1] & [1,0,1]  \\
\begin{block}{(cccc)c}
  -0.5 & -1.5 & -1 & 0 & [0,0,1]\\
  0.5  & 0 &  0 &  0.5 & [0,1,1]\\
  1.5 & 2 &  1.5 & 1 & [1,1,1]\\
  0.5 &  0.5 & 0.5 & 0.5 & [1,0,1]\\
\end{block}
\end{blockarray}
Utility of Player $\cY$

First, we must prove that $\phi$ is a PCE. To prove this, it is sufficient to show that:
\begin{itemize}
    \item $\phi$ is a projection of a vertex CSP $\phi^{V}_{1}$ such that Player $\cX$ gets normal-form-swap regret of $0$ under $\phi^{V}_{1}$, and 
    \item $\phi$ is a projection of a vertex CSP $\phi^{V}_{2}$ such that Player $\cY$ gets normal-form-swap regret of $0$ under $\phi^{V}_{2}$
\end{itemize}

As normal-form swap regret upper-bounds profile swap regret, this proves that both players achieve $0$ PSR under $\phi$, and thus that $\phi$ is a PCE.  

For the first decomposition, consider the following vertex-game CSP $\phi^{V}_{1}$: 

\begin{blockarray}{ccccc}
[0,0,1] & [0,1,1] & [1,1,1] & [1,0,1]  \\
\begin{block}{(cccc)c}
  0.2 & 0 & 0 & 0.2 & [0,0,1]\\
  0  & 0 & 0.2 &  0 & [0,1,1]\\
  0.2 & 0.2 &  0 & 0 & [1,1,1]\\
  0 &  0 & 0 & 0 & [1,0,1]\\
\end{block}
\end{blockarray}

It is easy to see that Player $\cX$ attains NFSR of $0$ under $\phi^{V}_{1}$ by verifying that each of their actions in the vertex game is a best response against the conditional marginal distribution over Player $\cY$'s actions against it. Furthermore, 

\begin{align*}
\Proj(\cspV_{1}) &=
    \sum_{v \in \learnvert, w \in \optvert} \cspV_{1}(v, w) (v \otimes w) \\
    & = 0.2 \cdot ([0,0,1] \otimes [0,0,1]) + 0.2 \cdot ([0,0,1] \otimes [1,0,1]) + 0.2 \cdot ([0,1,1] \otimes [1,1,1]) \\ & + 0.2 \cdot ([1,1,1] \otimes [0,0,1]) + 0.2 \cdot ([1,1,1] \otimes [0,1,1]) \\
    & = 0.2 \cdot ([0,0,0,0,0,0,0,0,1]) + 0.2 \cdot ([0,0,0,0,0,0,1,0,1]) + 0.2 \cdot ([0,0,0,1,1,1,1,1,1]) \\ & + 0.2 \cdot ([0,0,1,0,0,1,0,0,1]) + 0.2 \cdot ([0,1,1,0,1,1,0,1,1]) \\
    =  & [0,0.2,0.4,0.2,0.4,0.6,0.4,0.4,1] = \phi
\end{align*}

For the second decomposition, consider the following vertex-game CSP $\phi^{V}_{2}$: 

\begin{blockarray}{ccccc}
[0,0,1] & [0,1,1] & [1,1,1] & [1,0,1]  \\
\begin{block}{(cccc)c}
  0  & 0  & 0 & 0.2 & [0,0,1]\\
  0  & 0.2& 0 & 0.2 & [0,1,1]\\
  0  & 0.2& 0 & 0 & [1,1,1]\\
  0.2& 0  & 0 & 0 & [1,0,1]\\
\end{block}
\end{blockarray}

It is again easy to see that Player $\cY$ attains NFSR of $0$ under $\phi^{V}_{2}$ by verifying that each of their actions in the vertex game is a best response against the conditional marginal distribution over Player $\cX$'s actions against it. Furthermore, we can verify that it is indeed a decomposition of $\phi$ by again computing its projection:

\begin{align*}
\Proj(\cspV_{2}) &=
    \sum_{v \in \learnvert, w \in \optvert} \cspV_{2}(v, w) (v \otimes w) \\
    & = 0.2 \cdot ([0,0,1] \otimes [1,0,1]) + 0.2 \cdot ([0,1,1] \otimes [0,1,1]) + 0.2 \cdot ([0,1,1] \otimes [1,0,1]) \\ & + 0.2 \cdot ([1,1,1] \otimes [0,1,1]) + 0.2 \cdot ([1,0,1] \otimes [0,0,1]) \\
    & = 0.2 \cdot ([0,0,0,0,0,0,1,0,1]) + 0.2 \cdot ([0,0,0,0,1,1,0,1,1]) + 0.2 \cdot ([0,0,0,1,0,1,1,0,1]) \\ & + 0.2 \cdot ([0,1,1,0,1,1,0,1,1]) + 0.2 \cdot ([0,0,1,0,0,0,0,0,1]) \\
    =  & [0,0.2,0.4,0.2,0.4,0.6,0.4,0.4,1] = \phi
\end{align*}



Therefore, $\phi = [0, 0.2, 0.4, 0.2, 0.4, 0.6, 0.4, 0.4, 1]$ is a PCE.

Next, we will show that there does not exist a NFCE $\sigma$ such that $\pi(\sigma) = \phi$. To prove this, we assume for contradiction that there does exist such an NFCE, and iteratively eliminate move pairs from its possible support until we find a contradiction. To do this, we will crucially make use of Lemmas~\ref{lem:elim_x} and~\ref{lem:elim_y}, which formalize the fact that not only can't a strictly dominated action be present in an NFCE, if there is a weakly dominated action\footnote{Here we use \emph{strictly} and \emph{weakly dominated} in a slightly different way than in when excluding weakly dominated actions from the game; here we say action $a$ strictly dominates $b$ when another $a$ is strictly better than $b$ for all actions of the opponent, and $a$ weakly dominates $b$ when it is weakly better for all actions, and strictly better for at least one.}, the action pairs where it is strictly worse than the dominating action also cannot be in any NFCE. These properties hold even if the domination is only over the support of the NFCE itself. 

It will be clarifying to keep track of this visually: we begin by allowing the possibility of all possible move pairs to be in the support of some NFCE, denoting this by a $?$ denoting the weight placed upon each move pair. As we prove that certain vertex action pairs cannot be in the support of any NFCE, we will replace these $?$s with $0$s. For simplicity, we will also re-label the vertices as $v_{1:4},w_{1:4}$. We begin by noting that since the first index of $\phi$ is $0$, the following entries are all zero:

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$ \\
\begin{block}{(cccc)c}
  ? & ? & ? & ? & $v_{1}$\\
  ? & ? & ? & ? & $v_{2}$\\
  ? & ? & $\mathbf{0}$ & $\mathbf{0}$ & $v_{3}$\\
  ? & ? & $\mathbf{0}$ & $\mathbf{0}$ & $v_{4}$\\
\end{block}
\end{blockarray}

First, note that $v_{1}$ is dominated for Player $\cX$ by $v_{2}$, and strictly dominated in two of the columns. Thus, by Lemma~\ref{lem:elim_x}, $(v_{1},w_{2})$ and $(v_{1},w_{3})$ must have support zero in any NFCE. 

Furthermore, $v_{4}$ is dominated for Player $\cX$ by $v_{3}$, and strictly dominated in two of the columns. Therefore, again by Lemma~\ref{lem:elim_x}, $(v_{4},w_{2})$ and $(v_{4},w_{3})$ must have support zero in any NFCE. Additionally, conditional on Player $\cX$ being recommended $v_{4}$ in a NFCE satisfying our constraints, Player $\cY$ must have support only on $w_{1}$ and $w_{2}$. Therefore by Lemma~\ref{lem:elim_y}, $(v_4,w_2)$ cannot be in the support either.

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$ \\
\begin{block}{(cccc)c}
  ? & $\mathbf{0}$ & $\mathbf{0}$ & ? & $v_{1}$\\
  ? & ? & ? & ? & $v_{2}$\\
  ? & ? & 0 & 0 & $v_{3}$\\
  ? & $\mathbf{0}$ & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}

Now, note that conditional on Player $\cY$ being recommended $w_{3}$, Player $\cX$ must have full support on $v_{2}$. Player $\cY$ would rather play $w_{1}$ in this case, so by Lemma~\ref{lem:elim_y} we can eliminate another vertex pair from the support. 

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$  \\
\begin{block}{(cccc)c}
  ? & 0 & 0 & ? & $v_{1}$\\
  ? & ? & $\mathbf{0}$ & ? & $v_{2}$\\
  ? & ? & 0 & 0 & $v_{3}$\\
  ? & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}

From here, we can label the remaining vertex pairs in order to determine additional constraints: 

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$ \\
\begin{block}{(cccc)c}
  a & 0 & 0 & b & $v_{1}$\\
  c & d & 0 & e & $v_{2}$\\
  f & g & 0 & 0 & $v_{3}$\\
  h & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}

Recall that $\phi[4] = 0.2$, and further that $\phi[4]$ is the sum of the values at the intersection of the leftmost two columns and the middle rwo rows. Therefore, this tells us that $e = 0.2$. Furthermore, $\phi[2] = 0.2$ as well, and this value represents the sum of the four values in the intersection of the middle two columns and the lowest two rows. Thus, $g = 0.2$. Also, $\phi[5] = 0.4$, representing the central four values in the matrix. Thus, $d + g = 0.4$, and therefore $d = 0.2$.

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$  \\
\begin{block}{(cccc)c}
  a & 0 & 0 & b & $v_{1}$\\
  c & $\mathbf{0.2}$ & 0 & $\mathbf{0.2}$ & $v_{2}$\\
  f & $\mathbf{0.2}$ & 0 & 0 & $v_{3}$\\
  h & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}


Also, $\phi[6] = 0.6$, which represents the middle two rows of the matrix. Therefore, $c$ and $f$ must both equal $0$:

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$   \\
\begin{block}{(cccc)c}
  a & 0 & 0 & b & $v_{1}$\\
  $\mathbf{0}$ & 0.2 & 0 & 0.2 & $v_{2}$\\
  $\mathbf{0}$ & 0.2 & 0 & 0 & $v_{3}$\\
  h & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}


And $\phi[3] = 0.4$, which represents the bottom two rows. There is only one variable remaining in these rows we can set, and thus $h = 0.2$:

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$  \\
\begin{block}{(cccc)c}
  a & 0 & 0 & b & $v_{1}$\\
  0 & 0.2 & 0 & 0.2 & $v_{2}$\\
  0 & 0.2 & 0 & 0 & $v_{3}$\\
  $\mathbf{0.2}$ & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}

Next, $\phi[7] = 0.4$, which represents the leftmost two columns. Thus, $b = 0.2$:

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$   \\
\begin{block}{(cccc)c}
  a & 0 & 0 & $\mathbf{0.2}$ & $v_{1}$\\
  0 & 0.2 & 0 & 0.2 & $v_{2}$\\
  0 & 0.2 & 0 & 0 & $v_{3}$\\
  0.2 & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}

Finally, given that these values must all sum to $1$, this leaves us with a unique solution:

\begin{blockarray}{ccccc}
$w_{1}$ & $w_{2}$ & $w_{3}$ & $w_{4}$   \\
\begin{block}{(cccc)c}
  $\mathbf{0}$ & 0 & 0 & 0.2 & $v_{1}$\\
  0 & 0.2 & 0 & 0.2 & $v_{2}$\\
  0 & 0.2 & 0 & 0 & $v_{3}$\\
  0.2 & 0 & 0 & 0 & $v_{4}$\\
\end{block}
\end{blockarray}

However, this is \emph{not} an NFCE. Conditioned on Player $\cX$ playing $v_{3}$, Player $\cX$'s marginal distribution is full support on $w_{2}$. But in this case, Player $\cX$ has normal-form-swap regret to playing $v_{2}$. This leads to a contradiction, and therefore $\phi$ is not the projection of any NFCE.

Furthermore, we verified via a linear program that there does not exist any other $\phi^{*}$ which is the projection of some NFCE, and has the same utility profile as $\phi$. %\nat{maybe awk, should I say more?}
\end{proof}
