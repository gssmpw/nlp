\section{Alternate formulation of profile swap regret}\label{app:alternate-formulation}

In this appendix, we present an alternate formulation of profile swap regret with some advantages for parsing this as a ``swap regret'' notion and comparing it to other notions of swap regret (i.e., the definition below is used in the that profile swap regret is upper bounded by polytope swap regret in Theorem~\ref{thm:comp-prof-swap}). 

We begin by showing that in the original definition of profile swap regret, it suffices to consider decompositions (of the form \eqref{eq:csp-decomp}) where each $x_{(k)}$ is a vertex of $\learnset$.

\begin{lemma}\label{lem:decomp-equiv}
Let $\csp = \sum_{k=1}^{K} \lambda_{k} (x_{(k)} \otimes y_{(k)})$ be an arbitrary decomposition of the CSP $\csp$. Then there exists another decomposition $\csp = \sum_{k=1}^{K'} \lambda'_{k} (x'_{(k)} \otimes y'_{(k)})$ where each $x'_{(k)} \in \learnvert$, all $x'_{(k)}$ are distinct, and

\begin{equation}
\label{eq:decomp_equiv}
\sum_{k=1}^{K'} \lambda'_{k}\Reg(x'_{(k)}, y'_{(k)}) \leq \sum_{k=1}^{K} \lambda_{k}\Reg(x_{(k)}, y_{(k)}).
\end{equation}
\end{lemma}
\begin{proof}
Fix any $k \in [K]$, and decompose $x_{(k)}$ arbitrarily into a convex combination $\sum_{s=1}^{S_k}\gamma_{s, k}v_{s, k}$ of vertices $v_{s, k} \in \learnvert$. Note that since the function $\Reg(x, y)$ is convex in $x$ for any fixed $y$ (it is the maximum of a collection of linear functions), it follows that $\sum_{s=1}^{S_k}\gamma_{s,k}\Reg(v_{s, k}, y_{(k)}) \leq \Reg(x_{(k)}, y_{(k)})$. It follows that the decomposition $(\lambda'_k, x'_{(k)}, y'_{(k)})$ formed by decomposing each $x_{(k)}$ in this way and aggregating the terms with the same vertex $v$ has the desired property. 
\end{proof}

One consequence of Lemma~\ref{lem:decomp-equiv} is that it allows us to write profile swap regret as a ``swap regret'': that is, regret with respect to the set of swap functions $\pi: \learnvert \rightarrow \learnvert$ (swapping vertices of $\learnset$), something not immediately clear from the original definition. In particular, we can equivalently define the profile swap regret $\CorrSwap(\csp)$ of a CSP via the following steps:

\begin{enumerate}
    \item Decompose $\csp$ into a convex combination of strategy profiles of the form

    \begin{equation}\label{eq:vertex-decomp}
    \csp = \sum_{v \in \learnvert} \lambda_{v} (v \otimes y_{v}),
    \end{equation}

    \noindent
    where $\lambda_v \geq 0$, $\sum_{v}\lambda_{v} = 1$ and $y_{v} \in \optset$.

    \item Define $\CorrSwap(\csp)$ to equal

    \begin{equation}\label{eq:vertex-csp}
    \CorrSwap(\csp) = T \cdot \left(\min_{y_{v}, \lambda_{v}} \max_{\pi^{*}:\learnvert \rightarrow \learnvert} \sum_{v \in \learnvert} \lambda_{v} \left(u_L(\pi^{*}(v), y_{v}) - u_L(v, y_{v})\right)\right),
    \end{equation}

    \noindent
    where the outer minimum is over all valid decompositions of the form \eqref{eq:vertex-decomp}. This already has a form similar to the definition of polytope swap regret (where we choose the best decomposition that minimizes our swap regret with respect to all functions mapping $\learnvert$ to $\learnvert$).

    \item Finally, if we desire, we can apply the minimax theorem to switch the order of the minimum and maximum in \eqref{eq:vertex-csp}. Doing so requires convexifying the domain over which we take the maximum, and therefore considering swap functions $\pi^{*}$ which send $\learnvert$ to $\learnset$ (instead of the ``pure'' swap functions which send $\learnvert$ to $\learnvert$).

    \begin{equation}\label{eq:vertex-csp-minimax}
    \CorrSwap(\csp) = T \cdot \left(\max_{\pi^{*}:\learnvert \rightarrow \learnset} \min_{y_{v}, \lambda_{v}}  \sum_{v \in \learnvert} \lambda_{v} \left(u_L(\pi^{*}(v), y_{v}) - u_L(v, y_{v})\right)\right),
    \end{equation}
\end{enumerate}

