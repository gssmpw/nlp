
\section{Game-Agnostic Learning and Polytope Swap Regret}\label{sec:game-agnostic}

In our discussion of online learning thus far, we have assumed that our learning algorithms $\cA$ are tailored to a specific polytope game $G$. By this, we mean that our algorithms operate under the assumption that the learner can see the sequence of actions $y_t \in \optset$ played by the optimizer so far and that the learner can compute the game-specific payoff function $u_L(x, y)$. These assumptions are typical for applications of online learning to games (for example, when such algorithms are used for equilibrium computation in extensive-form games).

However, many algorithms in the field of adversarial online learning are designed so that they require only the counterfactual rewards (alternatively, losses) faced by the learner over time. That is, instead of being told the mixed strategy $y_t$ the optimizer played in round $t$, it is enough for these learning algorithms to have access to the linear function $r_t(x)$ describing what utility the learner would have received if they had played action $x \in \learnset$ that round. Such algorithms can be used by a learner to play any repeated game where the learner has action set $\learnset$, regardless of their specific payoff function $u_L$ or the optimizer's action space $\optset$. In this section, we explore what happens when we restrict ourselves to  game-agnostic learning algorithms -- we will show that profile swap regret is an inherently game-dependent notion, and if we wish to minimize profile swap regret with a game-agnostic learning algorithm, we must actually minimize polytope swap regret.

Formally, a \emph{game-agnostic learning algorithm} $\cA$ is defined in the same way as our earlier (game-dependent) learning algorithms, with the only difference being that in each horizon-dependent learning algorithm, $\cA_{t}^{T}$ now maps a sequence of affine linear \emph{reward functions} $r_1, r_2, \dots, r_{t-1}$ to the action $x_{t} \in \learnset$ the learner will take at time $t$. Each $r_t$ is an affine linear function  sending $\learnset$ to $[-1, 1]$; we will write $\learnset^{*}$ to denote the set of such functions (so, each $r_{t}$ belongs to $\learnset^*$). 

When used to play a specific polytope game $G$, the function $r_t$ is constructed via $r_{t}(x) = u_L(x, y_t)$, where $y_t \in \optset$ is the action of the optimizer at round $t$. In particular, any transcript $(\bx, \by)$ of a polytope game $G$ corresponds to a \emph{game-agnostic transcript} $(\bx, \br)$ via this mapping. The game-agnostic transcript is sufficient to compute many quantities relevant to the learner, including their total utility over the course of the game and several variants of regret, as shown in the following lemma.

\begin{lemma}\label{lem:game-agnostic-regret}
\sloppy{Let $(\bx, \by)$ be the transcript of some repeated polytope game $G$. It is possible to compute external regret $\Reg(\bx, \by)$, linear swap regret $\LinSwap(\bx, \by)$, and polytope swap regret $\PolySwap(\bx, \by)$ from the game-agnostic transcript $(\bx, \br)$ corresponding to $(\bx, \by)$ (without any knowledge of $G$ or $u_L$ aside from the learner's action set $\learnset$).}
\end{lemma}
\begin{proof}
Note that the only way in which the optimizer's actions $y_t$ are used in the definitions of external regret $\Reg(\bx, \by)$, linear swap regret $\LinSwap(\bx, \by)$, and polytope swap regret $\PolySwap(\bx, \by)$ is to compute quantities of the form $u_{L}(\cdot, y_t)$. Since $r_t(x) = u_L(x, y_t)$, we can compute all such quantities given access to $r_t(x)$.
\end{proof}

As a consequence of Lemma~\ref{lem:game-agnostic-regret}, we can define quantities such as $\Reg(\bx, \br)$, $\LinSwap(\bx, \br)$, and $\PolySwap(\bx, \br)$ (defining these swap regrets as functions of the game-agnostic transcript). Similarly, we can define e.g. $\PolySwap(\cA^{T})$ to be the maximum polytope swap regret incurred by the game-agnostic (horizon $T$) algorithm $\cA^{T}$ against the worst-case sequence of reward functions $\br$.

\sloppy{In contrast to the result of Lemma~\ref{lem:game-agnostic-regret}, it is \emph{not possible} to compute the profile swap regret $\CorrSwap(\bx, \by)$ in this way: there exist pairs of transcripts $(\bx_1, \by_1), (\bx_2, \by_2)$ for polytope games with the same action set $\learnset$ that have the same game-agnostic transcript $(\bx, \br)$, but where $\CorrSwap(\bx_1, \by_1) \neq \CorrSwap(\bx_2, \by_2)$. That is, unlike the other variants of swap regret we have discussed, profile swap regret is fundamentally game-dependent (intuitively, this is because the space of possible decompositions of the CSP $\csp$ depends on the optimizer's action space $\optset$). We give an explicit example of this phenomenon in Appendix \ref{app:prof-from-transcript}.}

Instead, for any specific polytope game $G$ and game-agnostic learning algorithm $\cA$ (where $\cA$ and $G$ share the same learner action set $\learnset$), we can define $\CorrSwap_{G}(\cA^{T})$ to be the maximum profile swap regret a learner incurs by using $\cA^{T}$ to select their actions in the repeated game $G$. We will show that if we want to bound the profile swap regret $\CorrSwap_{G}(\cA^{T})$ for all polytope games $G$, we must bound the (game-agnostic) polytope swap regret $\PolySwap(\cA^{T})$. Our main tool is the following lemma, showing that we can always instantiate any game-agnostic transcript as a transcript of an actual polytope game in a way that makes profile swap regret and polytope swap regret agree.

\begin{lemma}\label{lem:agnostic-corr-to-poly}
\sloppy{Let $(\bx, \br)$ be a game-agnostic transcript. There exists a transcript $(\bx, \by)$ of a polytope game $G$ where $(\bx, \br)$ is the game-agnostic transcript corresponding to $(\bx, \by)$ and $\CorrSwap(\bx, \by) = \PolySwap(\bx, \by)$.}
\end{lemma}
\begin{proof}
Let $\bx = (x_1, x_2, x_3, \dots, x_T) \in \learnset^{T}$ and $\br = (r_1, r_2, \dots, r_T) \in (\learnset^{*})^T$. Consider the polytope game $G$ where the action set $\optset$ of the optimizer is $\Delta_{T}$ (distributions over $T$ pure actions), and the learner's utility function is given by $u_{L}(x, y) = \sum_{i=1}^{T} y_i r_i(x)$. Note that if we let $\by = (e_1, e_2, \dots, e_T)$ (where $e_i$ is the $i$th unit vector), then $(\bx, \br)$ is the game-agnostic transcript corresponding to $(\bx, \by)$.

We now argue that $\CorrSwap(\bx, \by) = \PolySwap(\bx, \by)$. The average CSP $\csp$ of the transcript $(\bx, \by)$ is given by

\begin{equation}\label{eq:agnostic-decomp1}
\csp = \frac{1}{T}\left(\sum_{t=1}^{T} x_t \otimes e_t \right).
\end{equation}

\noindent
% By using the definition of $\CorrSwap(\csp)$ given in \eqref{eq:csp-decomp}, it suffices to show that
Now, consider any decomposition of $\csp$ of the form

\begin{equation}\label{eq:agnostic-decomp2}
\csp = \sum_{v \in \learnvert} \lambda_{v} (v \otimes y_{v}),
\end{equation}

\noindent
where $\lambda_{v} \geq 0$, $\sum_{v} \lambda_v = 1$, and $y_{v} \in \optset$. We claim that, for any such  decomposition and any $t \in [T]$, we must have that $\sum_{v} \lambda_{v}y_{v, t} = 1/T$. To see this, consider the outcome when the (bi-affine) linear function $\rho: \learnset \otimes \optset \rightarrow \Rset$ defined via $\rho(x \otimes y) = y_{t}$ is evaluated on the (equal) right hand sides of both \eqref{eq:agnostic-decomp1} and \eqref{eq:agnostic-decomp2}; for \eqref{eq:agnostic-decomp1}, this equals $1/T$, and for \eqref{eq:agnostic-decomp2}, this equals $\sum_{v} \lambda_{v}y_{v, t}$. 

Given this, for each $t$, define $x_{t}^{V} \in \Delta(\learnvert)$ via $x_{t, v}^{V} = T\lambda_{v} y_{v, t}$. Note that since $\sum_{v} x_{t, v}^{V} = T\sum_{v} \lambda_{v}y_{v, t} = 1$, $x_{t}^{V}$ does indeed belong to $\Delta(\learnvert)$. But now, from the definition of polytope swap regret, this specific action decomposition implies that

\begin{eqnarray*}
\PolySwap(\bx, \by) &\leq & \max_{\pi: \learnvert \rightarrow \learnvert} \left(\sum_{t=1}^{T} u_L(\overline{\pi}(\xV_t), y_t) - u_L(x_t, y_t) \right) \\
&=& T \cdot \max_{\pi: \learnvert \rightarrow \learnvert} \left(\sum_{v \in \learnvert} \lambda_{v} u_L(\overline{\pi}(v), y_v) - u_L(v, y_v) \right).
\end{eqnarray*}

Since $\CorrSwap(\bx, \by)$ is equal to the minimum of this final expression over all possible decompositions of the form \eqref{eq:agnostic-decomp2}, this implies that $\PolySwap(\bx, \by) \leq \CorrSwap(\bx, \by)$. Together with the fact that $\CorrSwap(\bx, \by) \leq \PolySwap(\bx, \by)$ (Theorem~\ref{thm:comp-prof-swap}), we have that $\PolySwap(\bx, \by) = \CorrSwap(\bx, \by)$.
\end{proof}


With Lemma~\ref{lem:agnostic-corr-to-poly}, we can prove the main result of this section.

\begin{theorem}\label{thm:agnostic-main}
Let $\cA$ be a game-agnostic learning algorithm. Then for any $T > 0$, $\PolySwap(\cA^{T}) = \max_{G} \CorrSwap_{G}(\cA^{T})$, where this maximum is over all polytope games $G$ with the same learner action set $\learnset$ as $\cA$. 
\end{theorem}

\begin{proof}
For any polytope game $G$ and transcript $(\bx, \by)$ in $G$, Theorem~\ref{thm:comp-prof-swap} tells us that $\CorrSwap(\bx, \by) \leq \PolySwap(\bx, \by)$. It follows that $\CorrSwap_{G}(\cA^{T}) \leq \PolySwap(\cA^{T})$ for any polytope game $G$ (with the same learner action set $\learnset$ as $\cA$). 

Conversely, consider any game-agnostic transcript $(\bx, \br)$ of length $T$ produced by $\cA^{T}$. By Lemma~\ref{lem:agnostic-corr-to-poly}, there exists a polytope game $G$ and a (game-dependent) transcript $(\bx, \by)$ corresponding to $(\bx, \br)$ with the property that $\CorrSwap(\bx, \by) = \PolySwap(\bx, \by)$. But now, note that if the optimizer plays the sequence of actions $\by$ against a learner employing the game-agnostic algorithm $\cA$, the learner will see the sequence of reward functions $\br$ and therefore play the sequence of actions $\bx$. It follows that there exists a polytope game $G$ where $\CorrSwap_{G}(\cA^{T}) \geq \PolySwap(\cA^{T})$. Together with the above result, this implies the theorem statement.
\end{proof}

\begin{remark}
It is interesting to discuss the results of \cite{rubinstein2024strategizing} in light of the above discussion. \cite{rubinstein2024strategizing} answer an open question of \cite{MMSSbayesian} by showing that minimizing \emph{polytope swap regret} is both necessary and sufficient for implying non-manipulability in Bayesian games. However, the original question of \cite{MMSSbayesian} was posed for (a slight variant of) game-agnostic learning algorithms -- indeed, \cite{rubinstein2024strategizing} point out that without the ability to change the Bayesian game (by increasing the number of actions for the optimizer), the conjecture is not true (we use a similar construction to separate profile swap regret and polytope swap regret in Theorem~\ref{thm:comp-prof-swap}). Our Theorem~\ref{thm:agnostic-main} above (along with the characterization of non-manipulability in Theorem~\ref{thm:poly_nonmanip}) can therefore be thought of as a generalization of this result of \cite{rubinstein2024strategizing} to all polytope games. 
\end{remark}

It is tempting to try to define a game-agnostic analogue of profile swap regret by considering the game $G^{*}$ where the optimizer's actions set $\optset$ is the set $\learnset^{*}$ of reward functions, and where $u_L(x, r) = r(x)$ for any $x \in \learnset$, $r \in \learnset^{*}$. We will write $\CorrSwap_{G^{*}}(\bx, \br)$ to be the profile swap regret of the learner in this game, and call this quantity the \emph{game-agnostic profile swap regret} of this transcript. 

Since $\dim(\learnset^{*}) = \dim(\learnset)$, our later constructions for efficient no-profile-swap-regret learning algorithms will also produce efficient game-agnostic learning algorithms that minimize game-agnostic profile swap regret. It would be nice if game-agnostic profile swap regret was an upper bound on (game-specific) profile swap regret, thus giving us a way to efficiently bound polytope swap regret. Unfortunately, this is not the case.

\begin{theorem}\label{thm:game-agnostic-profile-bad}
There exists a polytope game $G$ and a transcript $(\bx, \by)$ in $G$ such that

$$\CorrSwap_{G}(\bx, \by) > \CorrSwap_{G^*}(\bx, \br).$$
\end{theorem}
\begin{proof}
See Appendix \ref{app:game-agnostic-profile-bad} for a proof. We build off the example used to demonstrate profile swap regret is not game-agnostic in Appendix \ref{app:prof-from-transcript}.
\end{proof}

However, there is a weaker sense in which this is true. We say that a polytope game $G$ is \emph{optimizer full-rank} if the map that sends an optimizer action $y \in \optset$ to its corresponding reward function $r \in \learnset^{*}$ is injective -- i.e., we can fully recover the optimizer action from the reward. Note in particular that this requires the optimizer's action set to have dimension at most as large as that of the learner's action set. Under this constraint, game-agnostic profile swap regret does indeed upper bound the profile swap regret of the game.

\begin{theorem}\label{thm:opt-full-rank}
Let $G$ be a polytope game that is optimizer full-rank. Then if $(\bx, \br)$ is the game-agnostic transcript corresponding to the transcript $(\bx, \by)$ of $G$, we have that

$$\CorrSwap_{G}(\bx, \by) \leq \CorrSwap_{G^*}(\bx, \br).$$
\end{theorem}
\begin{proof}
Let $\csp^{*}$ be the ``game-agnostic'' CSP $\csp^{*} = (1/T) \sum_{t} (x_t \otimes r_t) \in \learnset \otimes \learnset^{*}$ of the transcript $(\bx, \br)$ in the game $G^{*}$. Consider the decomposition of $\csp^*$ in the manner of \eqref{eq:csp-decomp}, i.e.,

$$\csp^{*} = \sum_{k=1}^{K} \lambda_{k} (x_{(k)} \otimes r_{(k)}),$$

\noindent
which realizes $\CorrSwap_{G^*}(\bx, \br)$ (so, $\CorrSwap_{G^*}(\bx, \br) = \sum_{k} \lambda_{k} \Reg(x_{(k)}, r_{(k)})$). 

Because $G$ is optimizer full-rank, there exists a linear function $M: \learnset^{*} \rightarrow \optset$ such that $M(r)$ is the unique element in $\optset$ with reward function $r$ (so $M(r_t) = y_t$ for all $t \in [T]$). For each $k \in [K]$, let $y_{(k)} = M(r_{(k)})$. Note that by applying the linear map $x \otimes r \rightarrow x \otimes M(r)$ to the equality

$$\frac{1}{T} \sum_{t} (x_t \otimes r_t) = \sum_{k=1}^{K} \lambda_{k} (x_{(k)} \otimes r_{(k)}),$$

\noindent
we find that the CSP $\csp = (1/T) \sum_{t} (x_t \otimes y_t) = (1/T) \sum_{t} (x_t \otimes M(r_t))$ of the original transcript $(\bx, \by)$ satisfies

$$\csp = \sum_{k=1}^{K} \lambda_{k} (x_{(k)} \otimes y_{(k)}).$$

But from this decomposition, we have that $\CorrSwap_{G}(\bx, \by) \leq \sum_{k=1}^{K}\lambda_{k} \Reg_{G}(x_{(k)}, y_{(k)}) = \sum_{k=1}^{K}\lambda_{k} \Reg_{G^*}(x_{(k)}, r_{(k)}) = \CorrSwap_{G^*}(\bx, \br)$. 
\end{proof}

Can we extend Theorem~\ref{thm:opt-full-rank} to games beyond the class of optimizer-full-rank games (possibly at the cost of increasing the dimension of the optimizer's action set $\optset$)? For example, can we game-agnostically bound profile swap regret in all games where the optimizer has at most $n$ actions, or that are ``almost'' optimizer-full-rank (e.g., there is a low-dimensional subspace of possible actions implementing a given reward)? We leave this as an interesting future direction.