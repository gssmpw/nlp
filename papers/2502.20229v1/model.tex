
\section{Model and Preliminaries}

\paragraph{Notation} Given two convex sets $\cC_1 \subseteq \Rset^{d_1}$ and $\cC_2 \subseteq \Rset^{d_2}$, we define their tensor product $\cC_1 \otimes \cC_2$ to be the subset of $\Rset^{d_1} \otimes \Rset^{d_2} \simeq \Rset^{d_1d_2}$ equal to the convex hull of all vectors of the form $c_1 \otimes c_2 = c_1 c_2^\top$ for $c_1 \in \cC_1$ and $c_2 \in \cC_2$. We write $\ball_{d}(R)$ to denote the $d$-dimensional ball of radius $R$ centered at the origin, and $\ball_{d}(x, R)$ to denote the $d$-dimensional ball of radius $R$ centered at $x$ (sometimes omitting $d$ when it is clear from context).

Unless otherwise specified, all norms $||\cdot||$ refer to the $\ell_2$ norm in the ambient space. Note that any bilinear function $f: \Rset^{d_1} \times \Rset^{d_2} \rightarrow \Rset$ corresponds to a vector $\hat{f}$ such that $f(x, y) = \langle \hat{f}, x \otimes y\rangle$; we define the various norms of $f$ (e.g. $||f||_{1}$, $||f||_{2}$, $||f||_{\infty}$) to equal those of $\hat{f}$. 
% Note that if $||f||_{1} \leq 1$, then $
% |f(x, y)| \leq 1$ for any $x, y$ satisfying $||x||_{\infty} \leq 1$ and $||y||_{\infty} \leq 1$.

Selected proofs are omitted and deferred to Appendix~\ref{sec:omitted}.

\subsection{Polytope Games}

We begin by introducing the notion of an \emph{polytope game}: a two-player game where the action sets of both players are convex polytopes\footnote{Almost all the results should extend to the slightly more general setting of arbitrary bounded convex sets. We focus on the specific setting of polytopes as this captures essentially all the primary settings of interest (e.g. Bayesian games and extensive-form games).} and where the payoffs of both players are provided by bilinear functions over these two sets. Formally, a polytope game $G$ is a game between two players; we call these two players the \emph{learner} and the \emph{optimizer}. The learner selects an action $x$ from the $d_L$-dimensional bounded convex polytope $\learnset \subset \ball_{d_L}(1)$ and the optimizer selects an action $y$ from the bounded convex polytope $\optset \subset \ball_{d_O}(1)$ (note that $\learnset$ and $\optset$ can have different dimensions). After doing so, the learner receives utility $u_L(x, y)$ and the optimizer receives utility $u_O(x, y)$, where $u_L$ and $u_O$ are both bounded bilinear functions satisfying $||u_L||_{\infty}, ||u_O||_{\infty} \leq 1$. These two action sets ($\learnset$ and $\optset$) and payoff functions ($u_L$ and $u_O$) define the game $G$.

We briefly note here that polytope games (for specific choices of $\learnset$ and $\optset$) capture a variety of different strategic settings. For example:

\begin{itemize}
    \item \textbf{Normal-form games}: When $\learnset = \Delta_m$ and $\optset = \Delta_n$, this captures the class of normal-form games where the learner has $m$ available actions and the optimizer has $n$ available actions.
    \item \textbf{Bayesian games}: When $\learnset = (\Delta_{m})^{c_L}$ and $\optset = (\Delta_{n})^{c_O}$, this captures the class of Bayesian games where again the learner and optimizer have $m$ and $n$ available actions respectively, but in addition the learner is one of $c_L$ types and the optimizer is one of $c_O$ types. Here we should interpret the set $(\Delta_{m})^{c_L}$ as representing all functions mapping a learner's type (an element of $[c_L]$) to a learner's mixed action (an element of $\Delta_m$); we can interpret the set $(\Delta_{n})^{c_O}$ similarly. 
    \item \textbf{Extensive-form games}: A two-player extensive form game can be written as a polytope game by letting $\learnset$ and $\optset$ be the sequence-form polytopes for the two players. See e.g. \cite{von1996efficient} for details.
\end{itemize}

Generally, we will be interested in polytope games that are played repeatedly for $T$ rounds (and even more generally, the limiting behavior of such games as $T$ approaches infinity). In a repeated setting, we will let $x_t \in \learnset$ denote the action taken by the learner at round $t$, and let $y_t \in \optset$ denote the action taken by the optimizer at round $t$. Given a transcript of a repeated game where the learner has played the sequence of actions $x_1, x_2, \dots, x_T$ and the optimizer has played the sequence of actions $y_1, y_2, \dots, y_T$, we say that the \emph{correlated strategy profile (CSP)} $\csp$ corresponding to this transcript is given by

\begin{equation}\label{eq:csp-def}
\csp = \frac{1}{T}(x_1 \otimes y_1 + x_2 \otimes y_2 + \dots + x_T \otimes y_T) \in \learnset \otimes \optset.
\end{equation}

\noindent
By construction, the CSP $\csp$ provides sufficient information to evaluate the average value of any bilinear function $f(x_t, y_t)$ over the transcript of play. Under the minor assumption\footnote{This assumption is true for the examples mentioned above, and can always be made to be true by augmenting $\learnset$ and $\optset$ with an additional dummy coordinate fixed to equal $1$.} that $\learnset$ and $\optset$ are contained within proper affine subspaces of $\Rset^{d_L}$ and $\Rset^{d_O}$ respectively, the CSP $\csp$ is also sufficient to evaluate the average of any bi-affine function $f(x_t, y_t)$ over this transcript. We will therefore assume that $\learnset$ and $\optset$ satisfy this assumption throughout the rest of the paper, and write $f(\csp) = \frac{1}{T}\sum_{t}f(x_t, y_t)$ for any such bi-affine function $f$ (e.g, $u_L(\csp)$ is the average learner utility over this transcript of play).

Given a polytope game, we will write $\BR_{L}(x)$ to denote the set of the learner's best responses to the optimizer's action $y$, i.e., $\BR_{L}(y) = \{x \in \learnset \,\mid\, u_L(x, y) = \max_{x^{*} \in \learnset} u_L(x^{*}, y)\}$. We say an action $x \in \learnset$ for the learner is \emph{strictly dominated} if it is not the best response to any action, i.e., there does not exist an $y \in \optset$ such that $x \in \BR_{L}(y)$. We say an action $x \in \learnset$ for the learner is \emph{weakly dominated} if it is \emph{not} strictly dominated but it is impossible for the optimizer to uniquely incentivize $x$, i.e., there does not exist an $y \in \optset$ such that $\BR_{L}(y) = \{x\}$. We say a polytope game $G$ is \emph{non-degenerate} if none of the vertices of $\learnset$ (the learner's extremal actions) are weakly dominated. 

\subsection{Learning Algorithms and Regret}

Thus far, we have made no distinction between the role of the optimizer and the learner. The difference, of course, is that the learner will play this repeated game by running a learning algorithm. 

A \emph{learning algorithm} $\cA$ for the game $G$ is a family $\{\cA^{T}\}_{T \in \mathbb{N}}$ of horizon-dependent learning algorithms. A horizon-dependent learning algorithm $\cA^{T}$ for time horizon $T$ is a collection of $T$ functions $\cA_1^{T}, \cA_2^{T}, \dots, \cA_T^T$, where $\cA_{t}^{T}$ describes the learner's play at time $t$ as a function of the optimizer's play up until round $t-1$, i.e., $\cA_{t}^{T}(y_1, y_2, \dots, y_{t-1}) = x_t$. Note that as written, a learning algorithm is specific to the given game $G$, and a learning algorithm for one game $G$ cannot necessarily easily be applied to a separate game $G'$. The important caveat to this is that a learning algorithm does not depend on the optimizer's utility function $u_O$, and we expect a robust learning algorithm $\cA$ for a game $G$ to perform well against several different opponents with different choices of $u_O$. 

We can measure the performance of a learning algorithm via some version of \emph{regret}. For example, the \emph{external regret} of a learning algorithm is the gap between its utility and the counterfactual utility it would have received if it played the best fixed action in hindsight. Formally, if the learner has played the sequence of actions $\bx = (x_1, x_2, \dots, x_T)$ and the optimizer has played the sequence of actions $\by = (y_1, y_2, \dots, y_T)$, the external regret of the learner is given by

\begin{equation*}
\Reg(\bx, \by) = \max_{x^{*} \in \cX} \sum_{t=1}^{T} u_L(x^{*}, y_t) - \sum_{t=1}^{T} u_L(x_t, y_t).
\end{equation*}

Note that since $u_L$ is bilinear, we can also compute the external regret directly from the CSP $\csp$ corresponding to this transcript of play. In particular, we can alternatively write

\begin{equation*}
\Reg(\csp) = T \cdot \left(\max_{x^{*} \in \cX}u_L(x^* \otimes\mathrm{proj}_{\cY}(\csp)  )  - u_L(\csp) \right),
\end{equation*}
% \mm{Strictly speaking, this is an abuse of notation here I think; in principle, we should write something like $\wt u_L$
% instead of $u$ for the corresponding bilinear function defined over the tensor product.}

\noindent
where $\mathrm{proj}_{\cY}(\csp) = \frac{1}{T}(y_1 + y_2 + \dots + y_T)$~\footnote{This operation is well defined -- we can extract the marginal action of the optimizer from the CSP $\phi$ 
 since it an average of a bi-affine function $g(x_t,y_t) = y_t$ applied to each entry in the transcript.} is the projection of $\csp$ onto the optimizer's action space (i.e., the average action taken by the optimizer over the course of the game).

In this paper we will primarily care about forms of regret stronger than external regret, namely variants of \emph{swap regret}. For the case of normal-form games (when the learner's action set $\learnset = \Delta_m$ is just the simplex over $m$ pure actions), the swap regret of a transcript of play can be defined via

\begin{equation}\label{eq:swap-reg}
\Swap(\bx, \by) = \max_{\pi^{*}:[m]\rightarrow [m]} \sum_{t=1}^{T} u_L(\pi^*(x_t), y_t) - \sum_{t=1}^{T} u_L(x_t, y_t).
\end{equation}

Intuitively, this should be thought of as the gap between the utility received by the learner and the maximum utility the learner could have received if they applied a specific ``swap function'' $\pi^*$ to their sequence of play: a function which transforms each pure strategy $i \in [m]$ of a learner to a new pure strategy $\pi(i) \in [m]$. Implicit in this definition is the fact that although $\pi^*$ is a function on \emph{pure strategies} (elements of $[m]$), we can uniquely extend $\pi^*$ to act on \emph{mixed strategies} (elements $x \in \Delta_m$) via $\pi^*(x)_{i} = \sum_{j \mid \pi(j) = i} x_j$. That is, the weight of action $i$ in $\pi^*(x)$ is equal to the total weight of actions $j$ in $x$ that map to $i$ under $\pi^*$. 

% \begin{equation}\label{eq:swap_pushforward}
% \pi^*(x)_{i} = \sum_{j \mid \pi(j) = i} x_j.
% \end{equation}

% \noindent


When the learner's action set $\learnset$ is an arbitrary polytope (and not a simplex), it is not clear how to perform this extension from swap functions on ``pure strategies'' (vertices of $\learnset$) to swap functions on ``mixed strategies'' (points within $\learnset$). For this reason, it is not clear how to directly generalize the notion of swap regret to polytope games, and a couple different plausible definitions have previously been proposed. We present three of these definitions below, in increasing order of strength\footnote{Meaning that linear swap regret will always be the smallest (and easiest to minimize) and normal-form swap regret will always be the largest (and hardest to minimize).}: \emph{linear swap regret}, \emph{polytope swap regret}, and \emph{normal-form swap regret}.

\paragraph{Linear swap regret}

In linear swap regret, we constrain all of our swap functions to be linear transformations, which can then be applied directly to the mixed strategies played by the learner. Specifically, let $\Psi$ be the set of all affine linear transformations that map the set $\cX$ to itself. The \emph{linear swap regret} of a transcript with learner actions $\bx = (x_1, x_2, \dots, x_T)$ and optimizer actions $\by = (y_1, y_2, \dots, y_T)$ is given by

\begin{equation*}
\LinSwap(\bx, \by) = \max_{\psi \in \Psi} \sum_{t=1}^{T} u_L(\psi(x_t), y_t) - \sum_{t=1}^{T} u_L(x_t, y_t).
\end{equation*}

As was the case with external regret (and swap regret over the simplex), we can write linear swap regret as a function of the CSP $\csp$ corresponding to this transcript of play via

\begin{equation*}
    \LinSwap(\csp) = T \cdot \left(\max_{\psi \in \Psi} u_L(\psi(\csp)) - u_L(\csp)\right),
\end{equation*}

\noindent
where we extend $\psi$ to act on CSPs $\csp$ via $\psi(x \otimes y) = \psi(x) \otimes y$.

\paragraph{Polytope swap regret}

Another way to adapt the definition of swap regret to polytope games is to still keep swap functions that act  (possibly non-linearly) on the set of pure strategies, but choose the decomposition of the learner's mixed actions into pure actions in the way that is optimal for the learner. This is the approach taken by \cite{MMSSbayesian} in the definition of polytope swap regret.

Formally, we define the \emph{polytope swap regret} $\PolySwap(\bx, \by)$ of a transcript with learner actions $\bx = (x_1, x_2, \dots, x_T)$ and optimizer actions $\by = (y_1, y_2, \dots, y_T)$ as follows. 

\begin{enumerate}
    \item First, decompose each of the learner's actions $x_t$ into a convex combination of the extreme points $\learnvert$ of $\learnset$. We will denote this decomposition by $\xV_t \in \Delta(\learnvert)$. Note that there may be many ways to do this decomposition: we are free to choose any of them and will eventually choose the decompositions that \emph{minimize} our eventual regret.

    \item For any swap function $\pi: \learnvert \rightarrow \learnvert$, we will let $\pi(\xV_t) \in \Delta(\learnvert)$ denote the resulting distribution over the extreme points of $\learnset$ after applying $\pi$ (i.e., in the same manner as in the definition of swap regret), and $\overline{\pi}(\xV_t)$ be the element of $\learnset$ formed by taking the average action in $\pi(\xV_t)$. 

    \item Finally, we define

    \begin{equation*}
    \PolySwap(\bx, \by) = \min_{\bxV} \max_{\pi: \learnvert \rightarrow \learnvert} \left(\sum_{t=1}^{T} u_L(\overline{\pi}(\xV_t), y_t) - u_L(x_t, y_t) \right).
    \end{equation*}

    That is, after the learner picks their preferred decomposition of actions into $\xV_t$, the adversary picks the swap function $\pi$ that maximizes the resulting regret from comparing the original sequence of actions $x_t$ to the transformed sequence of actions $\overline{\pi}(\xV_t)$.
\end{enumerate}

% Unlike linear swap regret, polytope swap regret \emph{cannot} be written as a direct function of the CSP $\csp$ corresponding to the transcript of play $(\bx, \by)$.

\paragraph{Normal-form swap regret}

Finally, we can attempt to directly use the original definition of swap regret by ``expanding'' any polytope game to a normal-form game where pure strategies for the optimizer and learner correspond to extreme points of $\learnset$ and $\optset$ (equivalently, forcing the learner to specify their own decomposition of their mixed strategies into pure strategies each round). In particular, we define the \emph{vertex game} corresponding to a polytope game $G$ to be the normal-form game where the learner plays mixtures of actions in $\learnvert$ and the optimizer plays mixtures of actions in $\optvert$ (i.e., with $\learnset' = \Delta(\learnvert)$ and $\optset' = \Delta(\optvert)$). If the learner plays a sequence of vertex game actions $\bxV = (\xV_1, \xV_2, \dots, \xV_T)$ and the optimizer plays a sequence of vertex game actions $\byV = (\yV_1, \yV_2, \dots, \yV_T)$, the \emph{normal-form swap regret} of this transcript of play is given by

\begin{equation}\label{eq:norm-swap-reg}
\NormSwap(\bxV, \byV) = \max_{\pi^{*}:\learnvert\rightarrow \learnvert} \sum_{t=1}^{T} u_L(\pi^*(\xV_t), \yV_t) - \sum_{t=1}^{T} u_L(\xV_t, \yV_t).
\end{equation}

\noindent
Note that this is simply the original definition of swap regret \eqref{eq:swap-reg} as applied to the vertex game.

Technically, this definition of swap regret is of a different flavor from the previous definitions, in that there is no clear way to take a transcript of play $\bx = (x_1, \dots, x_T)$ and $\by = (y_1, \dots, y_T)$ of the original polytope game and evaluate the normal-form swap regret of this transcript. However, note that we can always go the other direction -- given a transcript of play $(\bxV, \byV)$ of the vertex game, we can always construct a corresponding transcript of play $(\bx, \by)$ for the polytope game (e.g., by letting $y_t = \E[\yV_t] \in \optset$) and evaluate the other measures of swap regret on this transcript.


\paragraph{Comparisons between regret definitions}

We conclude this section with a couple of comparisons between regret notions. First, we note that for the case of normal-form games, all three of these notions reduce to the standard notion of swap regret (as we would expect).

\begin{theorem}\label{thm:nfg-swap-notions}
Fix a polytope game $G$ with $\learnset = \Delta_m$. Then for any transcript of play $\bx = (x_1, x_2, \dots, x_T)$ and $\by = (y_1, y_2, \dots, y_T)$ in $G$, we have that

$$\Swap(\bx, \by) = \LinSwap(\bx, \by) = \PolySwap(\bx, \by) = \NormSwap(\bx, \by).$$

\noindent
(Note that for normal-form games, the vertex game is identical to the original game, and therefore the quantity $\NormSwap(\bx, \by)$ is well-defined).
\end{theorem}

The following theorem shows that the above three forms of swap regret are ordered as originally described. Moreover, this ordering is ``strict'' -- minimizing one of these regret notions implies nothing about the larger regret notions. 

\begin{theorem}\label{thm:swap-notions-ordering}
Let $\bx = (x_1, x_2, \dots, x_T)$ and $\by = (y_1, y_2, \dots, y_T)$ be a transcript of play for a given polytope game. Let $\bxV = (\xV_1, \xV_2, \dots, \xV_T)$ and $\byV = (\yV_1, \yV_2, \dots, \yV_T)$ be a transcript of play for the corresponding vertex game such that $x_t = \E[\xV_t]$ and $y_t = \E[\yV_t]$ for each $t \in [T]$. Then:

$$\LinSwap(\bx, \by) \leq \PolySwap(\bx, \by) \leq \NormSwap(\bxV, \byV).$$

Moreover, all these inequalities are asymptotically strict in the following sense: for each two neighboring definitions of swap regret, there exists a family of transcripts for which the larger swap regret grows as $\Omega(T)$ but the smaller swap regret is zero.
\end{theorem}

We will often be interested in the worst-case regret incurred by some algorithm. For a horizon-dependent learning algorithm $\cA^{T}$, we let $\Reg(\cA^{T})$ denote the maximum value of $\Reg(\bx, \by)$ over all transcripts of length $T$ obtainable by playing against $\cA^{T}$. For a general learning algorithm $\cA$, we will let $\Reg(\cA)$ denote the function mapping $T$ to $\Reg(\cA^{T})$ (so e.g., we may have $\Reg(\cA) = O(\sqrt{T})$). We extend this notation to other notions of regret (e.g., $\Swap$, $\LinSwap$, $\PolySwap$, $\NormSwap$) in the obvious way.

\subsection{Manipulability and Menus}\label{sec:manipulability}

Given the above range of possible definitions for swap regret in polytope games, which ones should we target when learning in games? Of course, we can always try to minimize the strongest form of swap regret above (normal-form swap regret), but this might come with trade-offs in the form of worse regret bounds and increased algorithmic complexity. 

Instead, a more principled approach is to understand why we might want to minimize swap regret in normal-form games in the first place, and then pick the variant of swap regret that gives us comparable guarantees for polytope games. In particular, swap-regret minimization has a number of nice game-theoretic consequences in the form of convergence to certain classes of equilibria and robustness of the learner to certain dynamic manipulations of the optimizer. Informally, we can state some of these consequences as follows:

\begin{enumerate}
\item \textbf{(Non-manipulability)} No-swap-regret learning algorithms are ``non-manipulable'', in the sense that if a learner is running a no-swap-regret learning algorithm, the optimizer can do nothing asymptotically better than playing a fixed (possibly mixed) action every round. Moreover, every non-manipulable no-regret learning algorithm must also be no-swap-regret.

\item \textbf{(Minimality)} No-swap-regret learning algorithms form a ``minimal core'' of all no-regret learning algorithms: any CSP $\csp$ that an optimizer can implement against a no-swap-regret learning algorithm can be implemented against any no-regret algorithm.

\item \textbf{(Pareto-optimality)} No-swap-regret learning algorithms are Pareto-optimal: there is no learning algorithm that performs asymptotically better than a no-swap-regret learning algorithm against every possible optimizer. Notably, some classic no-regret learning algorithms like Follow-The-Regularized-Leader are \emph{not} Pareto-optimal in this sense.

\item \textbf{(Correlated equilibria)} When both players in the game run no-swap-regret learning algorithms, the time-averaged CSP converges to the set of correlated equilibria of the game.
\end{enumerate}

We will table the discussion of equilibria until Section \ref{sec:equilibria} -- it is complicated by the fact that it also is not exactly clear what the exact definition of correlated equilibria should be for polytope games. Instead, we will focus on the first three points for now, which all have clear utility-theoretic interpretations that we can extend to general polytope games. To more formally define them, it is useful to introduce the concept of menus.

Given a horizon-dependent learning algorithm $\cA^{T}$, we define the \emph{menu} $\cM(\cA^{T})$ of this algorithm to be the convex hull of all CSPs that an optimizer can implement against $\cA^{T}$. That is, $\cM(\cA^{T}) \subseteq \learnset \otimes \optset$ is the convex hull of all points of the form $\frac{1}{T}\sum_{t=1}^{T} x_t \otimes y_t$ where $x_t = \cA_t^{T}(y_1, y_2, \dots, y_{t-1})$, and $y_1, y_2, \dots, y_T$ is an arbitrary sequence of optimizer actions in $\optset$. For a general learning algorithm $\cA$, we define the \emph{asymptotic menu} $\cM(\cA)$ to be the limit of the sequence of menus $\cM(\cA^{1}), \cM(\cA^{2}), \dots$ in the Hausdorff norm\footnote{Throughout this paper, for simplicity, we will assume that this sequence of convex sets always converges. In general, one can always take some subsequence of this sequence that converges -- see Appendix D of \cite{paretooptimal} for details.}.

There is a simple test (coming from Blackwell approachability) for whether a convex set $\cM$ is the asymptotic menu of some learning algorithm: it suffices that $\cM$ contain some CSP of the form $x \otimes y$ for each $y \in \optset$. 
\begin{theorem}\label{thm:menu_char}
    A convex set $\cM$ is the asymptotic menu of some learning algorithm iff $\cM$ contains some CSP of the form $x \otimes y$ for each $y \in \optset$. 
\end{theorem}

Given a polytope game $G$ (with a specific optimizer payoff $u_O$), we define the \emph{Stackelberg value} $\Stack(G, u_O)$ to be the optimal payoff the optimizer can receive if they commit to playing a fixed strategy $y \in \optset$ and the learner best responds (breaking ties in favor of the optimizer). That is,

\begin{equation*}
\Stack(G, u_O) = \max_{y \in \optset} \max_{x \in \BR_{L}(y)} u_O(x, y).
\end{equation*}

\noindent
We make the dependence on $u_O$ explicit here because we will often want to consider the effect of changing $u_O$ while keeping all other parameters of the game ($\learnset$, $\optset$, and $u_L$) the same -- this captures the strategic problem of facing an optimizer with unknown rewards. 

Against any no-(external)-regret learning algorithm $\cA$, an optimizer can asymptotically achieve $\Stack(G, u_O)$ utility per round by simply playing their optimal Stackelberg action every round. Intuitively, an algorithm of the learner is non-manipulable if the optimizer cannot significantly increase their utility beyond this Stackelberg value by playing a strategy that changes over time. 

It is convenient to phrase this concept of non-manipulability in the language of menus. For any menu $\cM$, define $V_O(\cM, u_O) = \max_{\csp \in \cM} u_O(\csp)$ to be the maximum optimizer utility of any CSP $\csp$ in $\cM$. If $\cM$ is the menu of an algorithm $\cA$, note that this is just the maximum utility an optimizer can achieve by playing against algorithm $\cA$. We therefore say the menu $\cM$ is \emph{non-manipulable} if, for any optimizer payoff $u_O$,

\begin{equation*}
V_O(\cM, u_O) \leq \Stack(G, u_O).
\end{equation*}

The first point can be now rephrased as follows\footnote{The remaining theorems in this section all follow as consequences of results in \cite{deng2019strategizing}, \cite{MMSSbayesian}, and \cite{paretooptimal}. Since they are also special cases of theorems we prove later in this paper for polytope games (Theorems \ref{thm:poly_nonmanip}, \ref{thm:poly_minimal}, and \ref{thm:poly_pareto}), we omit their proofs.}.

\begin{theorem}\label{thm:nfg_nonmanip}
Fix a normal-form game $G$ (i.e., where $\learnset = \Delta_{m}$ and $\optset = \Delta_{n}$). Let $\cA$ be a no-swap-regret algorithm for $G$. Then the asymptotic menu $\cM(\cA)$ is non-manipulable. Conversely, if the asymptotic menu $\cM(\cA)$ of an algorithm $\cA$ is non-manipulable, then the algorithm $\cA$ must additionally be a no-swap-regret algorithm. 
\end{theorem}

We can also provide a version of Theorem~\ref{thm:nfg_nonmanip} quantifying the degree of manipulability of a finite-horizon algorithm. To this end, we define a menu $\cM$ to be \textit{$\alpha$-non-manipulable} if, for any payoff $u_O$, $V_O(\cM, u_O) \leq \Stack(G, u_O) + \alpha$.


% \noindent
% We define a menu $\cM$ to be \textit{$\alpha$-manipulable} if it is not $\alpha$-non-manipulable.  

Interestingly, the relevant swap-theoretic quantity for tightly characterizing the degree of manipulability of an algorithm is not swap regret itself\footnote{To see why, note that the swap regret of an algorithm scales with the learner's utility function $u_L$, but the degree of manipulability is independent of the scale of $u_L$.}, but the \emph{swap regret distance}, which captures the maximum distance between the menu of this algorithm and the no-swap-regret menu. Formally, for any CSP $\csp$, we define $\SwapDist(\csp)$ to equal the maximum $\ell_2$-distance $\mathrm{dist}(\csp, \cM_{NSR})$ from $\csp$ to the no-swap-regret menu $\cM_{NSR}$. Then, for any algorithm $\cA$, we define $\SwapDist(\cA) = \SwapDist(\cM(\cA)) = \max_{\csp \in \cM(\cA)} \SwapDist(\csp)$. 

\begin{theorem}\label{thm:nfg_nonmanip_quant}
Fix a normal-form game $G$ (i.e., where $\learnset = \Delta_{m}$ and $\optset = \Delta_{n}$). If $\cA$ is a learning algorithm for $G$ with the guarantee that $\SwapDist(\cA^{T}) \leq R(T)$, then $\cM(\cA^{T})$ is $\sqrt{mn} \cdot (R(T)/T)$-non-manipulable. Conversely, if $\cM(\cA^{T})$ is $(R(T)/T)$-non-manipulable, then the algorithm $\cA$ must satisfy $\SwapDist(\cA^{T}) \leq R(T)$. 
\end{theorem}


% Moreover, 


Moving onto the second point, we say that a menu $\cM$ is \emph{minimal} if: i. it is the asymptotic menu of some learning algorithm $\cA$, and ii. there is no strict sub-menu $\cM' \subset \cM$ which is the asymptotic menu of a learning algorithm. We can now rephrase the second point as follows. %It is worth mentioning here that there is a simple test (coming from Blackwell approachability) for whether a menu $\cM$ is the asymptotic menu of some learning algorithm: it suffices that $\cM$ contain some CSP of the form $x \otimes y$ for each $x \in \optset$. 

\begin{theorem}\label{thm:nfg_minimal}
Fix a normal-form game $G$ (i.e., where $\learnset = \Delta_{m}$ and $\optset = \Delta_{n}$). All no-swap-regret algorithms $\cA$ for $G$ share the same asymptotic menu $\cM(\cA) = \cM_{NSR}$. Moreover, if $\cA$ is a no-regret algorithm, then $\cM_{NSR} \subseteq \cM(\cA)$.
\end{theorem}

Finally, the result on non-manipulability (Theorem~\ref{thm:nfg_nonmanip}) concerns the utility the \textit{optimizer} can obtain when playing against a no-swap-regret learning algorithm. But we may also wonder when it is in the interest of the learner to play such a learning algorithm. To this end, define 

$$V_L(\cM, u_O) = \max \left\{u_L(\csp) \mid \csp \in \cM, u_O(\csp) = V_O(\cM, u_O)\right\}$$ 

\noindent
to be the utility the learner receives when an optimizer chooses their favorite CSP in $\cM$ (breaking ties in favor of the learner).

\begin{theorem}\label{thm:nfg_pareto}
Fix a normal-form game $G$ (i.e., where $\learnset = \Delta_{m}$ and $\optset = \Delta_{n}$). Let $\cA$ be a learning algorithm for $G$ that incurs $\Omega(T)$ swap regret in the worst-case. Then there exists an optimizer utility $u_O$ such that $V_{L}(\cM_{NSR}, u_O) > V_{L}(\cM(\cA), u_O)$; i.e., there exists an optimizer against whom it is strictly better to play any no-swap-regret algorithm than $\cA$.

% \begin{enumerate}
%     \item There exists an optimizer utility $u_O$ such that $V_L(\cM_{NSR}, u_O) > V_L(\cM(\cA), u_O)$.
%     \item For every $u_O$, $V_L(\cM_{NSR}, u_O) = V_L(\cM(\cA), u_O)$ (i.e., $\cM_{NSR}$ and $\cM(\cA)$ are indistinguishable from the point of view of the learner). 
% \end{enumerate}
\end{theorem}

Theorem \ref{thm:nfg_pareto} can also be thought of as saying that the no-swap-regret menu is \emph{Pareto-optimal}: there is no other learning algorithm which is asymptotically at least as good as swap regret minimization against every single possible optimizer (while strictly better for at least one optimizer). Note that unlike the other two points, this is not a tight characterization of no-swap-regret algorithms -- there exist other Pareto-optimal learning algorithms that incur $\Omega(T)$ swap regret in the worst-case \cite{paretooptimal}. 

% \subsection{Convex Optimization}

% \jon{todo: add preliminaries for convex optimization (e.g. standard oracles)?}