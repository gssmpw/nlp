\section{Additional results on correlated equilibria}\label{app:correlated-equilibria}

\subsection{Mediator protocols correspond to normal-form CE}\label{app:mediator-nf}

In this appendix we make clear the connection between the incentive-compatible signaling schemes a mediator can implement third-party mediator and normal-form correlated equilibria. In particular, we show that there is no need for a mediator to ever send recommendations to the two players that are not extreme points of $\learnvert$ or $\optvert$, and hence that the set of valid mediator outcomes can be exactly characterized by normal-form CE.

Specifically, for any (finite support) distribution $\sigma \in \Delta(\learnset \times \optset)$, we can consider the signaling scheme where a mediator samples a strategy profile $(x, y) \sim \sigma$, privately recommends playing $x$ to the $\cX$-player, and privately recommends playing $y$ to the $\cY$-player. We say this signaling scheme is incentive compatible if neither player has incentive to deviate given their observation. That is, for any $x_{\mathrm{sig}} \in \cX$, there should not exist an $x' \in \cX$ such that:

$$\E_{(x, y) \sim \sigma}[u_X(x', y) \mid x = x_{\mathrm{sig}}] > \E_{(x, y) \sim \sigma}[u_X(x, y) \mid x = x_{\mathrm{sig}}].$$

(i.e., the $\cX$-player cannot increase their utility by playing $x'$ every time they receive the signal $x_{\mathrm{sig}}$). Similarly, for any  $y_{\mathrm{sig}} \in \cY$, there should not exist a $y' \in \cY$ such that:

$$\E_{(x, y) \sim \sigma}[u_Y(x, y') \mid y = y_{\mathrm{sig}}] > \E_{(x, y) \sim \sigma}[u_Y(x, y) \mid y = y_{\mathrm{sig}}].$$

The following lemma shows that we can always convert an incentive-compatible signaling scheme supported on the entire set $\learnset \times \optset$ to one supported on pairs of pure strategies $\learnvert \times \optvert$ with the same utility for both players (in fact, that induce the same CSP).

\begin{lemma}
Let $\sigma \in \Delta(\learnset \times \optset)$ be an incentive compatible signaling scheme. Let $\sigma' \in \Delta(\learnvert \times \optvert)$ be the signaling scheme formed by first sampling a strategy profile $(x, y)$ from $\sigma$, decomposing $x$ and $y$ into convex combinations of pure strategies $x = \sum_{v \in \learnvert} \lambda_{v}v$ and $y = \sum_{w \in \optvert}\mu_{w}w$, and then returning the pure strategy profile $(v, w)$ with probability $\lambda_{v}\mu_{w}$. Then $\sigma'$ is an incentive compatible signaling scheme, and $\E_{(x, y) \sim \sigma}[x \otimes y] = \E_{(v, w) \sim \sigma'}[v \otimes w]$.
\end{lemma}
\begin{proof}

The fact that $\E_{(x, y) \sim \sigma}[x \otimes y] = \E_{(v, w) \sim \sigma'}[v \otimes w]$ follows directly from the construction of $\sigma'$ (in particular, $\sum_{v, w}\lambda_{v}\mu_{w}(v \otimes w) = \left(\sum_{v} \lambda_{v}v\right) \otimes \left(\sum_{w} \mu_{w}w\right) = x \otimes y$). It therefore suffices to show that $\sigma'$ is incentive compatible.

We will do this for the $\cX$-player (it follows for the $\cY$-player by symmetry). Consider any $x \in \learnset$ which is recommended to the $\learnset$-player with positive probability under $\sigma$. Let $y(x) \in \optset$ be the expected action recommended to the $\optset$-player, conditioned on the fact that $x$ is recommended to the $\learnset$-player. Note that since $\sigma$ is incentive compatible, this means that it must be the case that $x \in \BR_{X}(y(x))$ (or the $\learnset$ player could increase their utility by deviating from $x$ to any element of $\BR_{X}(y(x))$).

Now, consider any $v^* \in \learnvert$ such that $\lambda_{v^*} > 0$ in some decomposition of $x$ of the form $x = \sum_{v \in \learnvert} \lambda_{v}v$. We claim that it must also be the case that $v^* \in \BR_{X}(y(x))$. If not, the $\cX$-player could again improve their utility by deviating from $x$ to the strategy formed by replacing the $\lambda_{v^*}v^*$ term in the decomposition $\sum_{v \in \learnvert} \lambda_{v}v$ of $x$ with $\lambda_{v^*}v'$ for some $v' \in \BR_{X}(y(x))$. But now, if we let $y(v^{*}) \in \optset$ to be the expected message in $\sigma'$ received by the $\optset$-player conditioned on the $\learnset$-player receiving $v^*$, $y(v^{*})$ is a convex combination of elements of the form $y(x)$ for $x$ with $\lambda_{v^*} > 0$. It follows that $v^{*} \in \BR_{X}(y(v^{*}))$, and therefore that $\sigma'$ is incentive-compatible.
\end{proof}


\subsection{Every profile CE can be reached by no-profile-swap-regret dynamics}\label{app:all-profile-ce}

Just as normal-form CE exactly characterize the set of outcomes implementable by an incentive-compatible signaling scheme, profile CE exactly characterize the set of outcomes that can occur as a result of no-profile-swap-regret dynamics. One direction of this characterization (that such dynamics result in profile CE) is immediate from the definition of profile CE -- below we show that every profile CE can be realized by such dynamics, completing this characterization.

\begin{lemma}
    Given any profile CE $\phi$, there exist profile swap regret minimizing algorithms for the two players that converge to this CSP when employed against each other.
\end{lemma}


\begin{proof}
    Let $\phi = \sum_{k=1}^{K} \lambda_{k} (x_{(k)} \otimes y_{(k)})$ be a decomposition of the CSP $\phi$ with $K \le d_{\learnset}$ (where $\learnset \subseteq \mathbb{R}^{d_{\learnset}}$) -- Lemma~\ref{lem:decomp-equiv} guarantees such a decomposition. In case any of the $\lambda_k$'s are not rational -- we associate it with a sequence  $\lambda_k^1, \lambda_k^2 \cdots $ of positive rational numbers converging to $\lambda_k$. The two algorithms attempt to follow the schedule described below with any deviation observed triggering a default to a standard profile swap regret minimizing algorithm (such as the algorithm guaranteed by Theorem~\ref{thm:upper-approachability}). The schedule consists of an infinite series of epochs, the $i$-th epoch consists of $\sum_{k=1}^K \lambda^i_k$ rounds -- in each of the first $\lambda^i_1$ rounds in this epoch, the two players play $x_1$ and $y_1$ respectively; in the next $\lambda^i_2$ rounds in this epoch, the two players play $x_2$ and $y_2$ respectively and so on. It it not hard to see that both players following the schedule leads to convergence to the CSP $\phi$, which is a profile CE. On the other hand, any deviation from the schedule by an opposing player results in a reset to a profile swap regret minimizing algorithm, so both player's algorithms retain the worst case profile swap regret minimizing property.
\end{proof}

\subsection{One-sided mediator protocols and profile swap regret}\label{app:one-sided}

We can also think of profile CE as outcomes implementable by ``one-sided mediator'' protocols: in particular, a CSP $\csp$ is a profile CE if each player can imagine a mediator protocol implementing $\csp$ that is incentive compatible for themselves (but note that these mediator protocols may differ across players, and need not be incentive-compatible for other players).

To prove this, it suffices to show that any CSP $\csp$ with low profile swap regret for a specific player (e.g., the $\cX$-player) can be implemented by a mediator protocol that is incentive compatible for that player (and in particular, corresponds to a vertex-game CSP $\cspV$ with the property that $\NormSwap_{\cX}(\cspV) = 0$). We prove this below.

\begin{lemma}\label{lem:one-sided-mediator}
Let $\csp \in \learnset \otimes \optset$ be a CSP with the property that $\CorrSwap_{X}(\csp) = 0$. Then there exists a vertex-game CSP $\cspV \in \Delta(\learnvert \times \optvert)$ with the property that $\NormSwap_{\cX}(\cspV) = 0$ and $\Proj(\cspV) = \csp$.
\end{lemma}
\begin{proof}
Consider any normal-form swap regret minimizing algorithm $\cA$. Consider the menu of $\cA$ 

% Let $\cM_{NSR}^{V} = \{\cspV \in \Delta(\learnvert \times \optvert) \mid \NormSwap(\cspV) = 0\}$. Note that $\cM_{NSR}^{V}$ is a valid asymptotic menu in the vertex game (it is in fact the no-swap-regret menu is this vertex game, and therefore the asymptotic menu of any no-normal-form-swap-regret learning algorithm). In particular, there is a learning algorithm $\cA^{V}$ for the vertex game such that for any $\cspV \in \cM_{NSR}^{V}$, there exists a sequence of optimizer actions that asymptotically implement this CSP.



Let $G$ be the polytope game at hand. Consider a no-normal-form-swap-regret algorithm $\mathcal{A}$ in the vertex game $G^{V}$ (for the $\cX$-player). We can run $\mathcal{A}$ in the original polytope game by arbitrarily decomposing every adversary action $y_t \in \optset$ we see into a distribution $\yV_{t} \in \Delta(\optvert)$ (with $\E[\yV_{t}] = y_t$), obtaining the vertex game response $\xV_{t} \in \Delta(\learnvert)$ to it, and playing $x_t = \E[\xV_{t}]$ in the polytope game.

Note that since profile swap regret is upper bounded by normal-form swap regret (Theorems~\ref{thm:swap-notions-ordering} and \ref{thm:comp-prof-swap}), $\cA$ is also a no-profile-swap-regret algorithm in $G$. By Theorem~\ref{thm:poly_minimal}, this means that the asymptotic menu $\cM(\cA)$ is the no-profile-swap-regret menu $\cM_{NPSR}$, and in particular, any $\csp \in \cM_{NPSR}$ (i.e., any $\csp$ with $\CorrSwap_{X}(\csp) = 0$) can be asymptotically realized by sequence of opponent actions. But then, the corresponding vertex CSP $\cspV$ (realized by the limit of the transcript of play $(\bxV, \byV)$ in the vertex game) must satisfy $\NormSwap_{X}(\cspV) = 0$ (since $\cA$ is also no-normal-form-swap-regret) and $\Proj(\cspV) = \csp$. The conclusion follows.
\end{proof}

One consequence of Lemma~\ref{lem:one-sided-mediator} is that the no-profile-swap-regret menu is the projection of the no-swap-regret menu of the vertex game: that is, $\cM_{NPSR} = \Proj(\cM^{V}_{NSR})$, where $\cM^{V}_{NSR}$ is the set of vertex-game CSPs $\cspV$ with $\NormSwap(\cspV) = 0$. 

%\esh{NF swap regret menus in v-space project to $M_{NPSR}$}


% We remark that even the problem of verifying if a CSP is a profile CE is APX-hard.