\section{Introduction}

The theory of regret minimization is one of our most powerful tools for understanding zero-sum games. Consider, for instance, the statement that two players, choosing their strategies in a repeated zero-sum game by running a no-regret learning algorithm, each asymptotically guarantee that they receive their minimax equilibrium value \citep{FreundSchapire1996}. This simple fact lies at the heart of many theoretical results across computer science and economics, and underlies many of the recent super-human level AI performances in games such as Poker and Go \citep{BrownSandholm2017,MoravcikSBLMBDW2017,SilverHMGSDSAPL2016}. 

Many important games are not zero-sum, but instead \emph{general-sum}, allowing for potential alignment between the incentives of the different players. Regret minimization is also a powerful tool for analyzing general-sum games, albeit with some additional caveats. Minimizing the standard notion of \emph{external regret} -- the gap between one's utility and the utility of the best fixed action in hindsight -- has markedly weaker guarantees than in the zero-sum setting (this only allows convergence to the considerably weaker class of coarse-correlated equilibria, and is vulnerable to certain manipulations by a strategic opponent \citep{deng2019strategizing}). In these cases, one can get stronger game-theoretic guarantees by minimizing \emph{swap regret} -- the gap between one's utility and the counterfactual utility one could have received had they applied the best static \emph{swap function} to all of their actions (e.g., playing rock everywhere they might have previously played scissors). It is a fundamental result in online learning \citep{blum2007external} that there exist efficient swap regret minimization algorithms for playing in general-sum, normal-form games. 

In addition to being general-sum, many games come with some additional structure. For example, in Bayesian games (capturing settings like auctions and markets) players have private information they can use to choose their actions, and in extensive-form games (capturing settings like Poker and bargaining) players take multiple actions in sequence. Both of these classes of games fall under the umbrella of \emph{polytope games}\footnote{These games (or slight variants thereof) also appear in the literature under the names \emph{convex games} \citep{daskalakis2024efficient} and \emph{polyhedral games} \citep{Farina22:NearOptimal}. We choose the term \emph{polytope games} to be consistent with \cite{MMSSbayesian}, whose work we most directly build off of.}: (two-player\footnote{Throughout this paper, we will restrict our attention to games with two players.}) games where each player takes actions in some convex polytope, and where the utilities are provided as bilinear functions of these two action vectors. 

Ideally, we would like to apply the theory of regret minimization by minimizing swap regret in the class of polytope games, thereby taking advantage of the previously-mentioned guarantees. It is here where we hit a stumbling block -- for reasons we will discuss shortly, there is no single definition of swap regret in polytope games, but instead a wealth of different definitions: linear swap regret, polytope swap regret, normal-form swap regret, $\Phi$-regret, low-degree swap regret, etc. \citep{zhang2024efficient, fujii2023bayes, MMSSbayesian, GordonGreenwaldMarks2008}.  Of these notions, the strongest ones which do provide some of the above guarantees (e.g., normal-form swap regret) are conjectured to be intractable to minimize, and the ones for which we have efficient learning algorithms (e.g., linear-swap regret) come with unclear game-theoretic guarantees.

\subsection{Main Results and Techniques}

Our main contribution in this paper is the introduction of a new variant of swap regret in polytope games which we call \emph{profile swap regret}. We argue that profile swap regret is a particularly natural notion of swap regret in polytope games for two primary reasons:

\begin{itemize}
\item First, many of the utility-theoretic properties that swap regret possesses in general-sum normal-form games extend naturally to profile swap regret in polytope games. Take as one example the property of \emph{non-manipulability}. In \cite{deng2019strategizing}, the authors noticed that common no-external-regret algorithms (such as multiplicative weights) have the property that a strategic opponent can manipulate the play of these learning algorithms to achieve asymptotically more utility than they would be able to achieve against a rational follower (i.e., the ``Stackelberg value'' of the game). In contrast, no-swap-regret algorithms cannot be manipulated in this way, and it is now understood that the property of incurring sublinear swap regret exactly characterizes when learning algorithms are non-manipulable in general-sum normal-form games \cite{deng2019strategizing, MMSSbayesian}.

Similarly, we show that the property of incurring sublinear profile swap regret is equivalent to the property of being non-manipulable in polytope games (Theorem \ref{thm:poly_nonmanip}), resolving (one interpretation of) an open question of \cite{MMSSbayesian}. We additionally show that no-profile-swap-regret algorithms have minimal asymptotic menus and are Pareto-optimal, two properties established by \cite{paretooptimal} for no-swap-regret algorithms in normal-form games (Theorems \ref{thm:poly_minimal} and \ref{thm:poly_pareto}).

\item Secondly, by extending a recent technique introduced in \cite{daskalakis2024efficient}, we show that it is possible to design efficient learning algorithms that guarantee sublinear profile swap regret. That is, we show it is possible to construct explicit learning algorithms that run in time polynomial in the size of the game and number of rounds which incur at most $O(\sqrt{T})$ profile swap regret (Theorem~\ref{thm:upper-semi-separation}). Perhaps more meaningfully, these algorithms guarantee that any opponent can gain at most $O(\sqrt{dT})$ utility over their Stackelberg value in a $d$-dimensional polytope game by attempting to manipulate this learning algorithm, and are the first known efficient algorithms that provide this guarantee\footnote{All other algorithms providing this guarantee do so by minimizing stronger notions of swap regret such as normal-form swap regret, which are conjectured to be hard to minimize (both information-theoretically and computationally, see \cite{daskalakis2024lowerboundswapregret}).}. 
\end{itemize}

Before proceeding to discuss these results (and others) in more detail, it is helpful to provide a definition and some intuition for profile swap regret. Consider a game  where one player (the ``learner'', running a learning algorithm) picks actions from a convex polytope $\learnset$, the other player (the ``optimizer'', playing strategically) picks actions from a convex polytope $\optset$, and the utility the learner receives from playing action $x \in \learnset$ against the action $y \in \optset$ is given by $u_L(x, y)$, where $u_L: \learnset \times \optset \rightarrow \Rset$ is some bilinear function of both players' actions. Consider also a hypothetical transcript of a repeated instance of this game, where the learner has played the sequence of actions $\bx = (x_1, x_2, \dots, x_T)$ and the optimizer has played the sequence of actions $\by = (y_1, y_2, \dots, y_T)$. The standard (external) regret of the learner is simply the gap between the utility $\sum_{t} u_L(x_t, y_t)$ that they received and the best possible utility $\sum_{t} u_L(x^{*}, y_t)$ they could have received in hindsight by committing to a fixed action $x^{*} \in \learnset$. 

How should we define the swap regret of the learner on this transcript? If this were a normal-form game where the learner had $m$ actions, we could define swap regret by comparing the learner's utility to the best possible utility they could obtain by applying a ``swap function'' $\pi: [m] \rightarrow [m]$ to their actions, i.e., $\sum_{t} u_L(\pi(x_t), y_t)$. Implicit in this definition is the fact that we can extend the swap function $\pi$ from the set of pure strategies ($[m]$) to the set of mixed strategies (the simplex $\Delta_{m}$). In the case of normal-form games, there is a clear way to do this: decompose each mixed action as a combination of pure strategies and apply the swap function to each of the pure strategies in this decomposition.

In general polytope games, however, there may not be a unique way to decompose mixed strategies of the learner (elements of $\learnset$) into pure strategies (vertices of $\learnset$). For example, if $\learnset = [0, 1]^2$, it is possible to decompose the mixed action $x = (1/2, 1/2)$ as either $\frac{1}{2}(0, 0) + \frac{1}{2}(1, 1)$, or as $\frac{1}{2}(0, 1) + \frac{1}{2}(1, 0)$, and these two decompositions could get sent to very different mixed strategies under an arbitrary swap function mapping pure strategies to pure strategies. This has led to a number of different definitions for swap regret in polytope games, including linear swap regret (where we restrict the swap functions to be given by linear transformations, so that they can act directly on mixed actions), polytope swap regret (where we pick the best possible decomposition for the learner in each round), and normal-form swap regret (where we require the learner to directly play a distribution over pure strategies).

Profile swap regret addresses the issue of non-unique convex decompositions in the following way. Given a transcript $(\bx, \by)$, we construct its corresponding \emph{correlated strategy profile (CSP)} $\csp$, which we define to equal the element $\csp = \frac{1}{T}\sum_{t} x_{t} \otimes y_{t} \in \learnset \otimes \optset$. Note that in normal-form games, the CSP $\csp$ captures the correlated distribution over pairs of pure strategies played by both players -- it plays a somewhat similar role here, allowing us to evaluate the average of any bilinear utility function over the course of play. Now, for any decomposition of $\csp$ into a convex combination $\csp = \sum_{k} \lambda_{k} (x_{(k)} \otimes y_{(k)})$ of product strategy profiles (rank one elements of $\learnset \otimes \optset$), we define the profile swap regret of this decomposition to equal the maximal increase in utility by swapping each $x_{(k)}$ to the best response to $y_{(k)}$. Finally, we define the overall profile swap regret of this transcript to be the minimum profile swap regret of any valid decomposition of this form.

This definition, although perhaps a little peculiar, has a number of interesting properties:

\paragraph{Utility-theoretic properties} One important consequence of the above definition is that profile swap regret can be computed entirely as a function of the CSP $\csp$ (unlike stronger regret notions like polytope swap regret and normal-form swap regret). This allows us to build off the work of \cite{paretooptimal}, who develop techniques for understanding the subset of possible CSPs an adversary can asymptotically induce against a specific learning algorithm (they call this set the \emph{asymptotic menu} of the learning algorithm). By defining profile swap regret in this way, straightforward generalizations of these menu-based techniques to polytope games suffice to establish the  properties of non-manipulability, minimality, and Pareto-optimality for no-profile-swap-regret algorithms.

\paragraph{Efficient no-profile-swap-regret algorithms} Another benefit of of the above definition is that the CSP $\csp$ is a fairly low-dimensional vector, living in a $(\dim(\learnset) \cdot \dim(\optset))$-dimensional vector space (contrast this with the amount of information required to store a distribution over the potentially exponential number of vertices of $\learnset$). This allows us to design learning algorithms that incur at most $O(\sqrt{T})$ profile swap regret by using Blackwell's approachability theorem to force the CSP of the transcript of the game to quickly approach the subset of CSPs with zero profile swap regret. 
    
Interestingly, the computational properties of profile swap regret (and these associated learning algorithms) are somewhat subtle. We prove that it is NP-hard to compute the profile swap regret incurred by a learner during a specific transcript of play $(\bx, \by)$, even in the special case of Bayesian games (Theorem~\ref{thm:hardness}). Ordinarily, this would preclude running the previous approachability-based algorithms efficiently. However, by extending a technique recently introduced in \cite{daskalakis2024efficient} -- optimization via semi-separation oracles -- we show that it is still  possible to implement a variant of the Blackwell approachability algorithm in polynomial time. 

\paragraph{Polytope swap regret and game-agnostic learning} In \cite{MMSSbayesian}, the authors introduced \emph{polytope swap regret} as a measure of swap regret in polytope games that guarantees non-manipulability when minimized, and asked whether any non-manipulable algorithm must necessarily minimize swap regret. Following this, \cite{rubinstein2024strategizing} answered this question affirmatively for the case of Bayesian games. However, this answer has a minor subtlety -- in order to construct Bayesian games where a particular high polytope swap regret algorithm is manipulable, their construction might use games where the optimizer has far more actions available to them than in the games where the learner incurs high regret. \cite{rubinstein2024strategizing} further point out that this subtlety is in some sense unavoidable -- if you fix the number of actions and types of the two players, there are learning algorithms which incur high polytope swap regret but that are not manipulable. 

We study this phenomenon in general polytope games by drawing a distinction between \emph{game-aware learning algorithms} -- algorithms that can see the sequence of actions $\by \in \optset$ the optimizer is directly playing -- and \emph{game-agnostic learning algorithms} -- algorithms that can only see the sequence of induced counterfactual rewards (i.e., the function sending an action $x \in \learnset$ to the utility $u(x, y_t)$ they would have received if they played $x$ in round $t$). While profile swap regret characterizes non-manipulability for game-aware algorithms, we show that  polytope swap regret characterizes non-manipulability for game-agnostic  algorithms (Theorem~\ref{thm:agnostic-main}), thus providing an analogue of the result of \cite{rubinstein2024strategizing} for general polytope games. 

    \paragraph{Implications for equilibrium computation} We finally turn our attention to the question of how to define and compute correlated equilibria (CE) in polytope games. We argue that there are two main motivations for studying correlated equilibria, which are often conflated due to their agreement in the case of normal-form games. The first is the idea that a correlated equilibrium is an outcome that is inducible by a third party mediator providing correlated recommendations to all players. This idea is appealing from a mechanism design point-of-view, as one can imagine directly implementing the correlated equilibrium of our choice in a game by constructing such a mediator (e.g., installing a traffic light). 

    However, in many settings of interest, there is no explicit mediator. A second motivation for studying CE is that a correlated equilibrium represents a possible outcome of repeated strategic play between rational agents. We can still relate this back to our original mediator motivation by saying that an outcome is a correlated equilibrium if every player can individually imagine a ``one-sided'' mediator protocol incentivizing this scheme, where only that player's incentive constraints need to be met (this is in contrast to the ``two-sided'' mediator protocol above, which must work for all players simultaneously).
    
    We show that, in polytope games, this first form of CE corresponds to \emph{normal-form CE} (reached by normal-form swap regret dynamics) whereas the second form of CE corresponds to \emph{profile CE} (reached by profile swap regret dynamics). We further show that there is a gap between these two notions of correlated equilibria in general polytope games, exhibiting a separation which does not appear in the case of normal-form games. Finally, we note that our efficient no-profile-swap-regret algorithms mentioned above allow us to compute a profile CE in general games without being able to optimize over the set of profile CE, echoing the results of \cite{papadimitriou2008computing} for computing correlated equilibria in succinct multiplayer games.

\subsection{Related Work}


% Almost all of these results are constrained to the discrete setting for low dimension. Very recently  \cite{dagan2023external} and \cite{peng2023fast} presented algorithms that achieve full swap regret of $O(T/\log T)$, independent of $d$. This work improves on these results in the regime $d = o(\log(T)/\log\log(T))$. \cite{roth2024forecasting} and \cite{hu2024calibrationerrordecisionmaking} study the problem of designing forecasts such that any downstream agent incurs low swap regret; the problem faced by a single downstream agent (with potentially many actions but where the payoff only depends on a low-dimensional outcome) can be interpreted as a swap-regret minimization problem in a structured game. 

\paragraph{Swap Regret and $\Phi$-regret} Swap regret has long been an object of interest in normal-form games \citep{foster1997calibrated}. Early efficient methods to achieve bounded swap regret were established by~\cite{blum2007external}, who proposed efficient algorithms that guarantee low internal regret. More recently, concurrent work by ~\cite{dagan2023external} and ~\cite{peng2024swap} have shown that it is possible to minimize swap regret in any online learning setting where it is possible to minimize external regret. 

\cite{GordonGreenwaldMarks2008} introduced a generalization of swap regret to convex games called $\Phi$-regret, where one competes with a family $\Phi$ of functions mapping the action set into itself. This generalization captures many other swap regret notions of interest. Restricting $\Phi$ to only contain linear functions, we obtain linear swap regret. Linear swap regret has recently been studied extensively in both Bayesian games \citep{MMSSbayesian, fujii2023bayes, dann2023pseudonorm} and extensive form games \citep{Farina2023:Polynomial, Farina2024:eah, zhang2023mediator}, with \cite{daskalakis2024efficient} providing efficient algorithms for minimizing linear swap regret in general polytope games. Recently \cite{zhang2024efficient} studied $\Phi$-regret minimization for classes of functions $\Phi$ specified by low degree polynomials. Finally, \citep{dagan2023external, peng2024swap, fishelsonfull} study the notion of \emph{full swap regret}, allowing $\Phi$ to be the set of all (non-linear) functions mapping the action set into itself. There are some other variants of $\Phi$-regret minimization studied towards the goal of computing specific variants of correlated equilibria in Bayesian or extensive-form games; we survey those below. 

\paragraph{Strategizing in games} While no-swap regret algorithms were first conceptualized via their connection to correlated equilibrium, a more recent line of work has investigated the strategic properties of these algorithms in their own right.~\cite{braverman2018selling} initiated the study of non-myopic responses to learning algorithms in the context of single buyer auctions, demonstrating that when bidders run standard learning algorithms to choose their bids, they can be fully manipulated by a seller (who can extract the full surplus of the auction, leaving the buyer with zero utility). Since then, there has been a large line of recent work focused on understanding which learning algorithms provide provable game-theoretic guarantees in settings such as auctions \citep{deng2019prior, cai2023selling, lin2024persuading, rubinstein2024strategizing}, principal-agent problems \citep{guruganesh2024contracting, lin2024persuading}, general normal-form games \citep{deng2019strategizing, brown2024learning,  haghtalab2024calibrated, camara2020mechanisms}, and Bayesian games \cite{MMSSbayesian, rubinstein2024strategizing}. 


\paragraph{Correlated equilibria in polytope games} The concept of correlated equilibria originates from~\cite{aumann1974subjectivity} as a generalization of the notion of a Nash equilibrium for players who can correlate their play. Correlated equilibria also have the nice property that unlike Nash equilibria, they are computable in polynomial time \citep{papadimitriou2008computing}, at least in normal-form games. One of the main motivations for designing no-swap-regret learning algorithms is to construct decentralized learning dynamics that provably converge to correlated equilibria at fast rates (e.g, \citep{anagnostides2022near}).

On the other hand, the simplest generalization of correlated equilibria to general polytope games -- that is, to \emph{normal-form correlated equilibria (NFCE)}, formed by each extremal strategy as a pure strategy in the corresponding game -- might blow up the size of the game exponentially, and there is therefore no known efficient algorithm for computing NFCE. Moreover, there is no clear way to ``correctly'' generalize the original definition of \cite{aumann1974subjectivity} to these settings -- \cite{forges1993five} introduces ``five legitimate definitions of correlated equilibrium'' for games with sequential imperfect information. In Bayesian games, \cite{bergemann2016bayes} introduce a notion of Bayes correlated equilibrium, but did not discuss computational aspects; more recently, \cite{fujii2023bayes} studies three refinements of this notion (agent-normal-form CE, communication equilibria, and strategic-form CE), and shows how to compute communication equilibria by minimizing ``untruthful swap regret'' (a variant of linear swap regret). In extensive-form games, \cite{von2008extensive} introduce the concept of \emph{extensive-form correlated equilibria}, which can be computed in polynomial-time in the representation of the game either directly \citep{huang2008computing} or by decentralized learning dynamics \citep{farina2022simple} (minimizing ``trigger swap regret''). 

\paragraph{Blackwell approachability} The main technique we use to design efficient learning algorithms for minimizing profile swap regret is an application of the semi-separation framework of \cite{daskalakis2024efficient} to the general problem of Blackwell approachability \citep{blackwell1956analog}. \cite{abernethy2011blackwell} demonstrated a reduction from Blackwell approachability to  regret-minimization that we use in this application. The orthant-approachability form of Blackwell approachability that we introduce in Section~\ref{sec:algorithms} appears implicitly in many follow-up works that focus on improving the rates of approachability algorithms \citep{perchet2013approachability, Perchet2015, Kwon2021, dann2023pseudonorm, dann2024rate}.

%  In this setting,~\cite{deng2019strategizing} showed that no-swap regret algorithms are \emph{non-manipulable}, and thus optimizing against a no-swap regret algorithm involves optimizing only over the class of static strategies that do not vary over time. 

% ~\cite{MMSSbayesian} introduced the study of non-myopic responses to learning algorithms in \emph{Bayesian} games and Polytope games more generally, identifying Polytope Swap Regret as a sufficient notion for non-manipulability in these games. \cite{rubinstein2024strategizing} prove that Polytope Swap Regret is also \emph{necessary} in general Bayesian games, but in the game-agnostic setting. By contrast, we show that when the structure of the game is fixed, the strictly weaker notion of~\emph{profile} swap regret is both necessary and sufficient for all Polytope games. 

 %In contrast, we show that a significant version of correlated equilibria in more general classes of games (polytope games) are achieved