
\section{Minimizing Profile Swap Distance}\label{sec:algorithms}

We now switch our attention to the problem of designing learning algorithms that minimize profile swap regret over finite time horizons. More specifically, we are interested in the following two questions:

\begin{enumerate}
    \item What is the best bound on profile swap regret  that we can guarantee after $T$ rounds? Can we guarantee that this quantity is sublinear in $T$ and polynomial in the dimensions $d_L$ and $d_O$?
    \item Can we construct \emph{computationally efficient} algorithms with these regret bounds? That is, can we construct learning algorithms that run in per-iteration time that is polynomial in $T$, $d_L$, and $d_O$?
\end{enumerate}

Note that for linear swap regret, we have positive answers to both of these questions. In particular, linear swap regret is known to be tractable to minimize: there is a line of work \citep{GordonGreenwaldMarks2008, Farina2023:Polynomial, Farina2024:eah, dann2023pseudonorm} that provides efficient learning algorithms achieving $\poly(d)\sqrt{T}$ linear swap regret in specific classes of games, with \cite{daskalakis2024efficient} now showing that this is possible as long as you have standard oracle access to the learner's action set $\learnset$. On the other side of the spectrum, the best known algorithms for minimizing normal-form swap regret generally incur regret bounds that are either polynomial in the number of \emph{vertices} of $\learnset$ (which easily can be exponential in dimension for Bayesian games and extensive-form games) or scale as $T/(\log T)^{O(1)}$ (i.e., requiring exponential in $1/\eps$ rounds to guarantee $\eps T$ regret). Recent lower bounds \citep{daskalakis2024lowerboundswapregret} show that these rates are necessary for minimizing normal-form swap regret in extensive-form games, implying a mostly negative answer to the above questions.

\subsection{Information-theoretic regret bounds via Blackwell approachability}\label{sec:it-bounds}

In contrast to normal-form swap regret, we will show that it is possible to design learning algorithms that incur at most $O(\sqrt{T})$ profile swap regret. Our main technique is to frame the problem of minimizing profile swap regret as a specific instance of Blackwell approachability where the goal is to approach the no-profile-swap-regret menu $\cM_{NPSR}$.

We therefore begin by briefly reviewing the standard theory of Blackwell approachability. An instance of Blackwell approachability is specified by a convex action set $\learnset$ for the learner, a convex action set $\optset$ for the adversary, a bilinear \emph{vector-valued} payoff function $v: \learnset \times \optset \rightarrow \Rset^{d}$, and a convex target set $\cS \subseteq \Rset^{d}$. The goal of the learner is to run a learning algorithm $\cA$ (outputting a sequence of values $x_t$ in response to the actions $y_1, y_2, \dots, y_{t-1}$ played by the adversary so far) that guarantees that the average payoff $\frac{1}{T}\sum_{t=1}^{T}v(x_t, y_t)$ is close to the target set $\cS$.

In order for this to be possible at all, the function $v$ must be \emph{response-satisfiable} with respect to the target set $\cS$: that is, for every possible action $y \in \optset$ played by the adversary, there must be a response $x(y) \in \learnset$ for the learner with the property that $v(x(y), y) \in \cS$. If $v$ is not response-satisfiable for some $y$, then if the adversary repeatedly plays this action, it is impossible for the learner to force the average payoff to converge to the set $\cS$. Blackwell's approachability theorem states that response-satisfiability is the only necessary requirement -- if $v$ is response-satisfiable with respect to $\cS$, then it is always possible for the learner to steer the average payoff to approach $\cS$. Below we cite a quantitative, algorithmic version of Blackwell's approachability theorem that appears in \cite{mannor2013approachability}. 


\begin{theorem}[Blackwell's approachability theorem; Proposition 1 of \cite{mannor2013approachability}]\label{thm:blackwell}
Let $v\colon \cX \times \cY \rightarrow \Rset^{d}$ be a vector-valued bilinear payoff function and $\cS$ be a convex subset of $\Rset^{d}$ such that $v$ is response-satisfiable with respect to $\cS$. Then there exists a learning algorithm $\cA$ (choosing $x_{t} \in \cX$ in response to $y_{1}, y_{2}, \dots, y_{t-1} \in \cY$) with the property that:

$$T \cdot \mathrm{dist}\left(\frac{1}{T}\sum_{i=1}^{t} v(x_t, y_t), \cS\right) \leq 2\norm{v}_{\infty}\sqrt{T},$$
where $\norm{v}_{\infty} = \max_{x \in \cX, y \in \cY}\norm{v(x, y)}_{2}$  and $\mathrm{dist}(z, \cS)$ is the minimum ($\ell_2$) distance from $x$ to the point $\cS$. %\mm{Is there really a square here? It doesn't look homogeneous.}

Moreover, this algorithm can be implemented efficiently (in time polynomial in $d$, $\dim(\cX)$, and $\dim(\cY)$) given an efficient separation oracle for the set $\cS$. 
\end{theorem}

There is a clear similarity between the Blackwell approachability problem stated above and the problem of minimizing profile swap regret: whereas in Blackwell approachability, the learner wants to guide the average vector-valued payoff $\frac{1}{T}\sum v(x_t, y_t)$ to converge to the set $\cS$, when minimizing profile swap regret, the learner wants to guide the average CSP $\frac{1}{T}\sum x_t \otimes y_t$ to converge to the no-profile-swap-regret menu $\cM_{NPSR}$. In fact, the function mapping a pair $(x, y)$ of learner and optimizer strategies to the corresponding CSP $x \otimes y$ is itself a vector-valued bilinear payoff function that is response-satisfiable with respect to the no-profile swap regret menu. This allows us to immediately apply Theorem~\ref{thm:blackwell} to get bounds on profile swap regret, which we do below (note that this Theorem will mostly be later subsumed by Theorem~\ref{thm:upper-semi-separation}, which achieves similar bounds but efficiently).

\begin{theorem}\label{thm:upper-approachability}
For any polytope game $G$ (where $\learnset$ and $\optset$ are bounded in unit norm), there exists a learning algorithm $\cA$ that guarantees $\CorrDist(\mathcal{A}^{T}) \leq 2\sqrt{T}$.
\end{theorem}

\begin{proof}
Consider the vector-valued payoff function $v: \learnset \times \optset \rightarrow (\learnset \otimes \optset) \subset \Rset^{d_Ld_O}$ defined via $v(x, y) = x \otimes y$, and let the target set $\cS$ equal the no-profile-swap-regret menu $\cM_{NPSR}$ for the game $G$. Note that $v$ is response-satisfiable for $\cS$, since given any $y \in \optset$, the CSPs in $\BR_{L}(y) \otimes y$ belong to $\cM_{NPSR}$. Note also that $\norm{v}_{\infty} \leq \max_{x \in \cX}\max_{y \in \cY} \norm{x \otimes y} = (\max_{x \in \cX} \norm{x})(\max_{y \in \cY} \norm{y}) \leq 1$, since both $\learnset$ and $\optset$ are bounded in unit norm by assumption.

By Theorem~\ref{thm:blackwell}, there exists a learning algorithm $\cA$ which guarantees that $\mathrm{dist}\left(\frac{1}{T}\sum_{t=1}^{T} v(x_t, y_t), \cS\right) \leq 2\norm{v}_{\infty}\sqrt{T} \leq 2\sqrt{T}$. But $\frac{1}{T}\sum_{t=1}^{T} v(x_t, y_t) = \frac{1}{T}\sum_{t=1}^{T} x_{t} \otimes y_t$ is simply the CSP $\csp$ corresponding to this transcript of play; since $\cS = \cM_{NPSR}$, this distance is exactly the minimum distance from this CSP to the no-profile-swap-regret menu, and is therefore equal to $\CorrDist(\csp)$. It follows that the worst-case profile swap distance $\CorrDist(\mathcal{A}^{T})$ is at most $2\sqrt{T}$. 
\end{proof}

Combining this with our theorem on non-manipulability (Theorem~\ref{thm:poly_nonmanip}), this implies that there exists an $\alpha$-non-manipulable learning algorithm for $\alpha = O(\sqrt{d_Ld_O/T})$.

\subsection{Hardness of computing profile swap regret}

Theorem~\ref{thm:upper-approachability} indicates that it is possible to construct a learning algorithm that achieves good profile swap regret guarantees, but states nothing about the computational complexity of running such an algorithm. The instantiation of Blackwell's approachability in Theorem~\ref{thm:blackwell} is algorithmic, and can be run in polynomial-time as long as one has efficient oracle access to the target set in question. 

In our case, this target set is the no-profile-swap-regret menu $\cM_{NPSR}$. In some cases, we can construct efficient convex oracles for the set $\cM_{NPSR}$; for example, if our polytope game is actually a normal-form game, the menu $\cM_{NPSR}$ reduces to the no-swap-regret polytope $\cM_{NSR}$, which can be written as the explicit intersection of $O(n^2)$ half-spaces when the learner has $n$ actions (i.e., constraints guaranteeing the learner cannot improve their utility by swapping action $i \in [n]$ to action $i' \in [n]$). 

However, we will show that for general polytope games, we cannot hope for any computationally efficient characterization of the no-profile-swap-regret menu (even when we have succinct and efficient descriptions of the sets $\learnset$ and $\optset$). In particular, we will show that even for the setting of Bayesian games, it is impossible to even compute the profile swap distance $\CorrDist(\csp)$ (note that this quantity is simply the distance from $\csp$ to $\cM_{NPSR}$, and would be efficient to compute given standard convex oracles for this set). Our main tool is the following APX-hardness result of computing Stackelberg equilibria in Bayesian games.

\begin{lemma}[Theorem 14 in \cite{MMSSbayesian}]\label{lem:apx-hardness}
It is APX-hard to compute the Stackelberg value for the optimizer in a Bayesian game. That is, there exists a constant $\eps > 0$ such that given a Bayesian game $G$ (with a specific optimizer utility $u_{O}$) and a value $V > 0$ it is NP-hard to distinguish between the cases $\Stack(G, u_O) \leq (1-\eps)V$ and $\Stack(G, u_O) \geq V$. 
\end{lemma}

Lemma~\ref{lem:apx-hardness} almost immediately rules out the possibility of weak optimization oracles for $\cM_{NPSR}$, since by Lemma~\ref{lem:menu_char}, the Stackelberg value $\Stack(G, u_{O})$ is simply the maximum value $u_O$ takes over the menu $\cM_{NPSR}$. Since most weak convex oracles are equivalent (given mild assumptions), this allows us to rule out the existence of any algorithm that can efficiently compute $\CorrDist(\csp)$.

\begin{theorem}\label{thm:hardness}
Define a \emph{profile swap distance oracle} to be an algorithm that takes as input a Bayesian game $G$ (with $\learnset = \Delta_{m}^{c_L}$ and $\optset = \Delta_{n}^{c_O}$), a CSP $\csp \in \learnset \otimes \optset$, and a precision parameter $\delta$, and returns a $\delta$-additive approximation to $\CorrDist(\csp)$. If $P \neq NP$, then there does not exist a profile swap distance oracle that runs in time $\poly(n, m, c_L, c_O, 1/\delta)$.
\end{theorem}
\begin{proof}
Assume to the contrary that such an efficient profile swap distance oracle exists. We first argue that we can use such an algorithm to construct a weak separation oracle for $\cM_{NPSR}$ that also runs in time $\poly(n, m, c_L, c_O, 1/\delta)$.

This follows as a consequence of black-box convex oracle reductions established in \cite{lee2018efficient}. In particular, our profile swap distance oracle is a weak evaluation oracle for the $1$-Lipschitz, convex function $\CorrDist(\csp)$. By Lemma 20 of \cite{lee2018efficient}, it is possible (with polynomially many calls to the original oracle) to extend this to a weak subgradient oracle for the same function. The weak subgradient oracle (when run with precision $\delta$ on input $\csp$) returns a real number $\alpha$ satisfying $|\alpha - \CorrDist(\csp)| \leq \delta$ and a vector $c \in \Rset^{d_Ld_O}$ with the property that $\alpha + c^{T}(\csp' - \csp) \leq \CorrDist(\csp') + \delta$ for all $\csp' \in \learnset \otimes \optset$. 

Now, consider the halfspace given by the inequality $(\alpha - \delta) + c^{T}(\csp' - \csp) \leq 0$. We claim that this is a halfspace that $\delta$-weakly separates $\cM_{NPSR}$ from $\phi$. In particular, every $\csp' \in \cM_{NPSR}$ satisfies $\CorrDist(\csp') = 0$, and thus satisfies this inequality. However, unless $\alpha - \delta \leq 0$, the CSP $\csp' = \csp$ does not satisfy this inequality. But if $\alpha - \delta \leq 0$, then since $|\alpha - \CorrDist(\csp)| \leq \delta$, we must have that $\CorrDist(\csp) \leq 2\delta$. We have therefore either provided a hyperplane separating $\cM_{NPSR}$ from $\phi$, or certified that $\phi$ is distance at most $2\delta$ from $\cM_{NPSR}$, completing the construction of our weak separation oracle.

With a weak separation oracle, we can construct a weak optimization oracle, and approximately optimize any linear function over $\cM_{NPSR}$. But by Lemma~\ref{lem:menu_char}, the optimal value of the linear function $u_{O}$ over $\cM_{NPSR}$ is $\Stack(G, u_O)$. Thus, this oracle would allow us to distinguish between the two cases in Lemma~\ref{lem:apx-hardness} in polynomial time, contradicting the APX-hardness of the result unless $P = NP$.
\end{proof}

\subsection{Efficient learning algorithms via semi-separation}\label{sec:alg-semisep}

Nevertheless, despite the hardness of computing profile swap regret (and thus of running the algorithm of Theorem \ref{thm:upper-approachability}), we will now show that it is still possible to design efficient learning algorithms $\cA$ that obtain profile swap regret guarantees of the form $\poly(d_L, d_O)\sqrt{T}$. 

To describe the idea behind semi-separation, it will be convenient to express the problem of Blackwell approachability in slightly different language. Previously, we described an approachability instance as being parameterized by (in addition to the two action sets of the learner and adversary) a vector-valued bilinear payoff function $v$ and a target set $\cS$, with the idea that the learner is trying to steer the average vector-valued payoff to be close to $\cS$. In learning-theoretic contexts, it is frequently mathematically more convenient to work with another formulation of approachability that we will refer to as \emph{orthant-approachability}. An \emph{orthant-approachability} instance is parameterized by a single \emph{convex set} $\cU$ of \emph{one-dimensional} biaffine payoff functions $u: \learnset \times \optset \rightarrow \Rset$. The goal of the learner is now to minimize the worst-case average value of $u(x_t, y_t)$ over \emph{all} biaffine functions $u \in \cU$; that is, they wish to minimize the approachability loss given by $\AppLoss(\bx, \by) = \max_{u \in \cU} \frac{1}{T}\sum_{t=1}^{T} u(x_t, y_t)$.

In particular, the learner would like to guarantee that $\AppLoss(\bx, \by)$ grows sublinearly in $T$. In order for this to be possible, $\cU$ must contain only \emph{response-satisfiable} functions $u$. In the language of orthant-approachability, this means that for any $y \in \cY$, there exists an $x \in \cX$ such that $u(x, y) \leq 0$ for all $u \in \cU$ (we call this ``orthant''-approachability since this can be thought of approaching the negative orthant for the infinite-dimensional bilinear payoff function whose entries are given by $u(x, y)$ for different $u \in \cU$).

It can be shown that orthant-approachability and our previous formulation of approachability are essentially equivalent (in fact, orthant-approachability is slightly more general, in that it can capture other distance metrics and some infinite-dimensional bilinear payoffs). In particular, we show below (following an argument in \cite{dann2024rate}) that any instance of our previous formulation of approachability can be equivalently expressed as an orthant-approachability problem. 

\begin{lemma}\label{lem:orthant-approach}
Let $v: \learnset \times \optset \rightarrow \Rset^{d}$ be a vector-valued bilinear payoff function that is response-satisfiable with respect to the target set $\cS \subseteq \Rset^{d}$. There exists a convex subset $\cU$ of response-satisfiable biaffine functions $u: \learnset \times \optset \rightarrow \Rset$ such that, for any sequences $x_t \in \learnset$ and $y_t \in \optset$,

$$\max_{u \in \cU} \frac{1}{T}\sum_{t=1}^{T} u(x_t, y_t) = \mathrm{dist}\left(\frac{1}{T}\sum_{i=1}^{t} v(x_t, y_t), \cS\right)$$
\end{lemma}
\begin{proof}
For each unit vector $h \in \Rset^{d}$, consider the biaffine function $u_{h}(x, y) = \langle v(x, y), h\rangle - \sigma_{h}(\cS)$, where $\sigma_{h}(\cS) = \max_{z \in \cS}\langle h, z\rangle$. Let $\cU$ be the convex hull of all such functions $u_{h}$. 

If $\frac{1}{T}\sum_{t=1}^{T} u_{h}(x_t, y_t) \geq R$, then $\left\langle \frac{1}{T}\sum_{t=1}^{T} v(x_t, y_t), h \right\rangle - \sigma_{h}(\cS) \geq R$, and therefore $\mathrm{dist}\left(\frac{1}{T}\sum_{i=1}^{t} v(x_t, y_t), \cS\right)$ is at least $R$. On the other hand, if $\mathrm{dist}\left(\frac{1}{T}\sum_{i=1}^{t} v(x_t, y_t), \cS\right)$ is at least $R$, then there must be a direction $h$ in which $\left\langle \frac{1}{T}\sum_{t=1}^{T} v(x_t, y_t), h \right\rangle - \sigma_{h}(\cS) \geq R$, and therefore we have $\frac{1}{T}\sum_{t=1}^{T} u_{h}(x_t, y_t) \geq R$.
\end{proof}


As with classical Blackwell approachability, there exist efficient learning algorithms for minimizing approachability loss in orthant-approachability -- however, just as the earlier algorithms required separation oracles for the target set $\cS$, the algorithms for orthant-approachability require separation oracles for the set of payoff functions $\cU$. And likewise, just as Theorem~\ref{thm:hardness} precludes the existence of efficient separation oracles for the no-profile-swap-regret menu $\cM_{NPSR}$, it also precludes the existence of separation oracles for the corresponding convex set $\cU$.

\cite{daskalakis2024efficient} bypass a similar obstacle for minimizing linear swap regret via a technique they call \emph{semi-separation}. We introduce a version of this same technique here for orthant-approachability. To motivate this technique, note that if we want to achieve orthant-approachability with respect to a specific (perhaps intractible) set of biaffine functions $\cU$, it suffices to achieve orthant-approachability with respect to any superset $\cU'$ containing $\cU$ (since the maximum over all $u'$ in $\cU'$ will always be at least as large as the maximum over all $u$ in $\cU$). In particular, if we could find a superset $\cU'$ of $\cU$ with more tractable convex oracles, then we could simply run orthant-approachability over that superset.

Unfortunately, it is not too hard to show that the minimality of the menu $\cM_{NPSR}$ implies that no superset $\cU'$ of $\cU$ can contain only response-satisfiable functions. The idea of \cite{daskalakis2024efficient} is to run an approachability algorithm on some tractable superset $\cU'$ anyway, despite $\cU'$ not being an approachable set. The approachability algorithm will either work without issue (in which case it will provide our desired guarantee on the approachability loss), or at some point the algorithm will identify a $u' \in \cU'$ that is not response-satisfiable, and thus not in $\cU$. If we can then construct a separating hyperplane that separates $u'$ from $\cU$, we can use this to refine  our superset $\cU'$. This algorithm -- which takes a non-response-satisfiable $u$, and returns a hyperplane separating $u$ from $\cU$ -- is what we refer to as a semi-separation oracle (unlike a standard separation oracle, it does not work for all $u \not \in \cU$, but only non-response-satisfiable $u$).

% Since we can achieve our approachability guarantees as long as all the functions $u' \in \cU'$ are response-satisfiable, it is natural to take $\cU'$ to be the set of all (bounded) response-satisfiable functions. 

% This choice of $\cU'$ has the advantage that we can, for example, efficiently check whether a specific function $u'$ is response-satisfiable (this amounts to solving an explicit linear program). However, $\cU'$ has one major disadvantage, which is that it is not a convex set -- it is possible for the convex combination of two response-satisfiable functions to not be response-satisfiable, ... .

Formally, a \emph{semi-separation oracle} for a set of response-satisfiable payoff functions $\cU$ is an algorithm which takes an arbitrary bi-affine function $u: \learnset \times \optset \rightarrow \Rset$ and either (i) returns that the function $u$ is response-satisfiable, or (ii) returns a hyperplane separating $u$ from $\cU$. The following theorem states that efficient semi-separation oracles suffice for performing orthant-approachability efficiently. The proof closely follows the proof of \cite{daskalakis2024efficient} for the case of linear swap functions, and is deferred to Appendix \ref{app:semi-sep}.

% \begin{enumerate}
%     \item 
%     \item 
% \end{enumerate}



\begin{theorem}[Approachability with semi-separation oracles]\label{thm:approach-semisep}
Consider an orthant-approachability instance $(\cX, \cY, \cU)$ where $\cX \subseteq \cB(D_{\cX})$, $\cY \subseteq \cB(D_{\cY})$, and $\cB(u_{0}, \rho) \subseteq \cU \subseteq \cB(D)$ (for some known $u_{0}$ and radius $\rho$). If we have access to efficient separation oracles for $\cX$ and $\cY$, and an efficient semi-separation oracle for the set $\cU$, then Algorithm~\ref{algo:semisep-approach} has the guarantee that

$$\max_{u \in \cU} \frac{1}{T}\sum_{t=1}^{T} u\left(x_t, y_t \right) \leq O\left(\frac{D_{\cX}D_{\cY}D}{\sqrt{T}}\right),$$

\noindent
and runs in time $\poly(D_{\cX}, D_{\cY}, D, \rho^{-1}, \dim(\cU), T)$.
\end{theorem}

In the remainder of this section, we will show that we can construct a semi-separation oracle for the approachability problem corresponding to minimizing profile swap regret, thus allowing us to efficiently minimize it via Theorem~\ref{thm:approach-semisep}. In particular, consider the set $\cU$ containing all bi-affine functions $u: \learnset \times \optset \rightarrow \Rset$ of the form $u(x, y) = \langle h, x \otimes y \rangle - b$ that satisfy: i. $\norm{h} \leq 1$, ii. $|b| \leq 1$, and iii. $u(x, y) \leq 0$ for all $x \in \BR_{L}(y)$. Note that this is a convex set of bi-affine functions, is response-satisfiable (via the third constraint), and for any CSP $\csp$ satisfies $\max_{u \in \cU} u(\csp) = \CorrDist(\csp)$ (in particular, it is equivalent to the set constructed in the proof of Lemma~\ref{lem:orthant-approach}). We produce an efficient semi-separation oracle for this choice of $\cU$.

\begin{lemma}\label{lem:semi-sep-exist}
Given efficient separation oracles for the sets $\learnset$ and $\optset$, it is possible to construct an efficient semi-separation oracle for the set $\cU$ defined above.
\end{lemma}
\begin{proof}
Consider an arbitrary bi-affine function $u_{\text{in}}: \learnset \times \optset \rightarrow \Rset$ that our semi-separation oracle receives as input. Compute (e.g., via linear programming) the minimax value $V = \min_{x \in \learnset} \max_{y \in \optset} u_{\text{in}}(x, y)$.

If $V \leq 0$, then this function is response-satisfiable (i.e., there exists an $x^*$ such that $u_{\text{in}}(x^*, y) \leq 0$ for all $y \in \optset$), and we can return that $u$ is response-satisfiable. Otherwise, there exists a $y^* \in \optset$ such that $u_{\text{in}}(x,  y^*) \geq V > 0$ for all $x \in \learnset$, which we can again compute efficiently via a linear program. 

Now, pick an arbitrary $x_{y^*} \in \BR_{L}(y^*)$. Note that, by construction, $u(x_{y^*}, y^*) \leq 0$ for all $u \in \cU$; on the other hand, for our queried $u_{\text{in}}$, we have that $u_{\text{in}}(x_{y^*}, y^*) \geq V > 0$. It follows that the linear constraint $u(x_{y^*}, y^*) \leq 0$ is a linear constraint on bilinear functions $u$ separating $u_{\text{in}}$ from the set $\cU$. 
\end{proof}

From this semi-separation oracle and Theorem~\ref{thm:approach-semisep}, we immediately obtain an efficient algorithm for minimizing profile swap regret. 

\begin{theorem}\label{thm:upper-semi-separation}
Given efficient separation oracles for $\cX$ and $\cY$, there exists a learning algorithm $\cA$ such that $\CorrDist(\mathcal{A}^{T}) = O(\sqrt{T})$ that runs in $\poly(T, d_L, d_O)$ time per iteration.
\end{theorem}
\begin{proof}
Given Lemma~\ref{lem:semi-sep-exist}, it suffices to check the conditions of Theorem~\ref{thm:approach-semisep} for the set $\cU$ defined above. By assumption, our $\learnset$ and $\optset$ are bounded within the unit ball (so we can take $D_{\learnset} = D_{\optset} = 1$). The maximum norm of any element in our set $\cU$ is at most $\sqrt{||h||^{2} + |b|^2} \leq \sqrt{2}$, so we can take $D = \sqrt{2}$. Finally, note that any $u$ with $||h|| \leq 1/2$ and $b \geq 1/2$ will satisfy $u(x, y) \leq 0$ for any $x \in \learnset$ and $y \in \optset$; we can therefore take $u_{0}(x, y) = -3/4$ (i.e., the $u$ specified by $h = 0$ and $b = 3/4$) and $\rho = 1/4$.
\end{proof}

% \jon{todo: add corollaries for Bayesian and extensive-form games?}

