%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025} 
% If accepted, instead use the following line for the camera-ready submission:
 \usepackage[accepted]{icml2025}



% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}



\def \bZ {{\bf Z}}
\def \cF{\mathcal{F}}
\def \P{\mathbb{P}} 
% \def \V {\mathbb{V}}
\def \bfE {\mathbb{E}}
 % \def \cM {\mathcal{M}}
 % \def \cH{\mathcal{H}}
 % \def \cV {\mathcal{V}}
\newcommand{\bcol}{\textcolor{blue}}
\newcommand{\col}[1]{{\color{magenta}  #1}}
\newcommand{\rcol}{\textcolor{red}}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\notindep}{\not \! \perp \!\!\! \perp}
\usepackage{resizegather}
\usepackage{subfig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Learning Counterfactual Outcomes Under Rank Preservation}

\begin{document}

\twocolumn[
\icmltitle{Learning Counterfactual Outcomes Under Rank Preservation}
%Counterfactual Learning under Rank Preservation

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Peng Wu}{btbu}
\icmlauthor{Haoxuan Li}{pku}
\icmlauthor{Chunyuan Zheng}{spku}
\icmlauthor{Yan Zeng}{btbu}

\icmlauthor{Jiawei Chen}{zju}
\icmlauthor{Yang Liu}{abc}
\icmlauthor{Ruocheng Guo}{comp}
\icmlauthor{Kun Zhang}{cmu1,cmu2}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


\icmlaffiliation{btbu}{School of Mathematics and Statistics, Beijing Technology and Business University}
\icmlaffiliation{pku}{Center for Data Science, Peking University} 
\icmlaffiliation{spku}{School of Mathematical Sciences, Peking University} 
\icmlaffiliation{zju}{College of Computer Science and Technology, Zhejiang University}
\icmlaffiliation{abc}{Computer Science and Engineering, University of California, Santa Cruz}
\icmlaffiliation{comp}{ByteDance Research}
\icmlaffiliation{cmu1}{Department of Philosophy, Carnegie
Mellon University}
\icmlaffiliation{cmu2}{Machine Learning Department, Mohamed
bin Zayed University of Artificial Intelligence} 
%\icmlaffiliation{sch}{Department of Philosophy, Carnegie
%Mellon University; Machine Learning Department, Mohamed
%bin Zayed University of Artificial Intelligence}

\icmlcorrespondingauthor{Ruocheng Guo}{ruocheng.guo@bytedance.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%
 % Specifically, 
% even when the structural causal model is unknown.  
% and the exogenous variable is heterogeneous. 
% counterfactual inference 
 
\begin{abstract}
Counterfactual inference aims to estimate the counterfactual outcome at the individual level given knowledge of an observed treatment and the factual outcome, with broad applications in fields such as epidemiology, econometrics, and management science. Previous methods rely on a known structural causal model (SCM) or assume the homogeneity of the exogenous variable and strict monotonicity between the outcome and exogenous variable. 
In this paper, we propose a principled approach for identifying and estimating the counterfactual outcome. 
We first introduce a simple and intuitive rank preservation assumption to identify the counterfactual outcome without relying on a known structural causal model.
Building on this, we propose a novel ideal loss for theoretically unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for its empirical estimation. Our theoretical analysis shows that the rank preservation assumption is not stronger than the homogeneity and strict monotonicity assumptions, and shows that the proposed ideal loss is convex, and the proposed estimator is unbiased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed method. 
% Our theoretical analysis 
\end{abstract}



% =====================================================
\section{Introduction}

% scientific research typically involves three primary tasks: prediction, intervention, and counterfactuals.
% These tasks align with the three levels of causal inference~
Understanding causal relationships is a fundamental goal across various domains, such as epidemiology~\citep{Hernan-Robins2020}, 
econometrics~\citep{Imbens-Rubin2015}, and management science~\citep{kallus2020-policy}. \citet{Pearl-Mackenzie2018} define the three-layer causal hierarchy—association, intervention, and counterfactuals—to distinguish three types of queries with increasing complexity and difficulty~\citep{Bareinboim-etal2022}. 
%and generally, answering higher-level queries from lower-level information is challenging~\citep{Bareinboim-etal2022}.
% Generally, answering higher-level queries from lower-level information is challenging~\cite{Bareinboim-etal2022}. 
Counterfactual inference, the most challenging level, aims to explore the impact of a treatment on an outcome given knowledge about a different observed treatment and the factual outcome. For example, given a patient who has not taken medication before and now suffers from a headache, we want to know whether the headache would have occurred if the patient had taken the medication initially. 
%had the patient  taken the medication initially. 
Answering such counterfactual queries can provide valuable instructions in scenarios such as credit assignment~\citep{Mesnard2021-ICML}, root-causal analysis~\citep{Budhathoki-etal2022}, and fair decision-making~\citep{imai2023principal}. % algorithmic 
%Pearl-etal2016-primer,
% applications like 
  %for an individual given knowledge about what in fact happened to that individual, 
  % offering 
%Typically, given an individual, a treatment assignment, and a treatment outcome,
% the counterfactual question asks what would have happened to that individual, had it been given
% another treatment, everything else being equal. An illustrative and motivating example is the case of clinical time series.

% association and 
Different from interventional queries, which are prospective and estimate the counterfactual outcome in a hypothetical world via only the observations obtained before treatment (as pre-treatment variables), counterfactual inference is retrospective and further incorporates the factual outcome (as a post-treatment variable) in the observed world. This inherent conflict between the hypothetical and the observed world poses a unique challenge and makes the counterfactual outcome generally unidentifiable, even in randomized controlled experiments (RCTs)~\citep{Pearl-etal2016-primer, Ibeling-2020-AAAI, Bareinboim-etal2022, Wu-etal-2024-Harm}. 


% 引入主题，关于反事实推断，Pearl's提出了三步法。但三步法本身需要一定的条件，如已知结构方程模型，U可以被反解出来。但少有文章讨论其背后的识别性假设和估计方法。
For counterfactual inference, \citet{Pearl-etal2016-primer} proposed a three-step procedure (abduction, action, and prediction) to estimate counterfactual outcomes. However, it relies on the availability of structural causal models (SCMs) that fully describe the data-generating process~\citep{Brouwer2022, Xie-etal2023-attribution}. In real-world applications, the ground-truth SCM is likely to be unknown, and estimating it requires additional assumptions to ensure identifiability, such as linearity~\citep{shimizu2006linear} and additive noise~\citep{hoyer2008nonlinear, peters2014causal}. Unfortunately, these assumptions are hard to satisfy in practice and restrict the applicability. 
% limiting their applicability.  
% Unfortunately, these assumptions are difficult to meet in practice,
% the limitations of these assumptions, crucial to Pearl's three-step procedure, are seldom discussed.  \col{hard to satisfied.}
% \col{often ignored and rarely  discussed.}

% \citet{Chao-etal2023}

% 最近，有人开始指出其背后的可识别性假设。但在估计上面，还是pearl's三步法。Xie提出了用quantile regression的方法。它不同于三步法，并不需要识别整个结构方程模型。但是它们忽略了选择偏差，可能导致有偏的估计。
% \cite{Lu-etal2020-attribution, Nasr-Esfahany-2023-attribution, Xie-etal2023-attribution} 
To tackle the above problems, several counterfactual learning approaches have been proposed with respect to different identifiability assumptions. For example, \citet{Lu-etal2020-attribution}, \citet{Nasr-Esfahany-2023-attribution}, and \citet{Xie-etal2023-attribution} established the identifiability of counterfactual outcomes based on homogeneity and strict monotonicity assumptions. The homogeneity assumption posits that the exogenous variable for each individual remains constant across different interventional environments, and the strict monotonicity assumption asserts that the outcome is a strictly monotone function of the exogenous variable given the features. In terms of counterfactual learning, \citet{Lu-etal2020-attribution} and \citet{Nasr-Esfahany-2023-attribution} adopted Pearl's three-step procedure that needs to estimate the SCM initially. In addition, \citet{Xie-etal2023-attribution} proposed using quantile regression to estimate counterfactual outcomes that effectively avoid the estimation of SCM. Nevertheless, it relies on a stringent assumption that the conditional quantile functions for different counterfactual outcomes come from the same model and it requires estimating a different quantile value for each individual, leading to a challenging bi-level optimization problem. % due to its its nonconvexity. 
%Notably, Pearl’s three-step procedure % for  implicitly implies the homogeneity assumption.  
% see Section \ref{sec3-2} for details 

% However, this method relies on the assumption that quantile functions for different counterfactual outcomes originate from the same model and necessitates estimating a distinct quantile value for each individual before estimating the counterfactual outcomes, leading to a challenging bi-level optimization problem.

% Nonetheless, we theoretically reveal the limitations of these methods: the identifiability assumptions are restrictive, and estimation methods are either  biased due to overlooking selection bias or dependent on further assumptions. Both aspects impede their practical applications. See Section \ref{sec4} for a detailed discussion. 

% \citet{Lu-etal2020-attribution} discussed the identifiability assumptions underling Pearl's three-step procedure, \citet{Nasr-Esfahany-2023-attribution} extended it to the case of instrument variables. In addition, \cite{Xie-etal2023-attribution} \citet{Lu-etal2020-attribution}, 

  %that within the corresponding distributions containing the counterfactual and factual outcomes of all individuals with the same features, the counterfactual and factual outcomes of an individual share the same rank.
% 本文从识别和估计两个方法，进一步讨论了反事实推断的问题。
 In this work, we propose a principled counterfactual learning approach with \emph{intuitive identifiability assumptions and theoretically guaranteed estimation methods}. {\bf On one hand}, 
 %for identifiability assumptions, 
 we introduce the simple and intuitive rank preservation assumption, positing that an individual's factual and counterfactual outcomes have the same rank in the corresponding distributions of factual and counterfactual outcomes for all individuals. % containing  with the same features.  
 We establish the identifiability of counterfactual outcomes under the rank preservation assumption and show that it is slightly less restrictive than the homogeneity and monotonicity assumptions used in previous studies. 


{\bf On the other hand}, we further propose a theoretically guaranteed method for unbiased estimation of counterfactual outcomes. The proposed estimation method has several desirable merits. First, unlike Pearl's three-step procedure, it does not necessitate a prior estimation of SCMs and thus relies on fewer assumptions than \citet{Lu-etal2020-attribution} and \citet{Nasr-Esfahany-2023-attribution}. Second, in contrast to the quantile regression method proposed by \citet{Xie-etal2023-attribution}, our approach neither restricts conditional quantile functions for different counterfactual outcomes to originate from the same model, nor does it require estimating a different quantile value for each unit. Third, we improve the previous learning approaches by adopting a convex loss for estimating counterfactual outcomes, which leads to a unique solution. 


%Remarkably, the proposed training loss is convex with a unique solution, whereas the previous quantile regression methods cannot ensure the optimization property. 
% Additionally, our method estimates  counterfactual outcomes by solving a simpler optimization problem. 
% Additionally, our method estimates the counterfactual outcome directly and is computationally efficient, whereas existing methods estimate indirectly and are computationally burdensome. 

     %  We reveal the difficulties in counterfactual inference and highlight its equivalence to estimating individualized treatment effects.  Additionally, uncover
% of this paper 
%\vspace{-1pt} 
In summary, the main contributions are as follows: 
(1) We introduce the intuitive rank preservation assumption to identify the counterfactual outcomes with unknown SCM; (2) We propose a novel ideal loss for unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for the ideal loss. In addition, we provide a comprehensive theoretical analysis for the proposed learning approach; 
 (3) We conduct extensive experiments on both semi-synthetic and real-world datasets to demonstrate the effectiveness of the proposed method.





% ========================================================
% Preliminaries and 
\vspace{-8pt}  
\section{Problem Formulation}
% potential values they may take. 
% (may be unobserved) % disturbances (or noise) due to  factors. 
Throughout, capital letters represent random variables and lowercase letters denote their realizations. 
%  this work
 
{\bf Structural Causal Model} (SCM, \citeauthor{pearl2009causality}, \citeyear{pearl2009causality}). An SCM $\mathcal{M}$ consists of a causal graph $\mathcal{G}$ and a set of structure equation models $\mathcal{F} = \{f_1, ..., f_p \}$.  
The nodes in $\mathcal{G}$ are divided into two categories: (a)  exogenous variables ${\bf U} = (U_1, ..., U_p)$,  which represent the environment during data generation, assumed to be mutually independent;    
(b) endogenous variables $\textbf{V} = \{V_1, ...,  V_p\}$, which denote the relevant features that we need to model in a question of interest. For variable $V_j$, its value is determined by a structure equation $V_j = f_j(PA_j, U_j), ~j=1, ..., p$, where $PA_j$ stands for the set of parents of $V_j$. 
SCM provides a formal language for describing how the variables interact and how the resulting distribution would change in response to certain interventions. Based on SCM, we introduce the counterfactual inference problem in the following. 
%The SCM provides a formal language for describing how the variables interact and how the resulting distribution would changes in response to certain interventions. Based on SCM, we introduce the counterfactual inference problem. 

% This paper concentrates on the counterfactual inference problem below. 
% and we formulate it below. % and introduce  Pearl's Three-Step Procedure for Counterfactual Inference. 
%  In this paper, we focus on the countefactual inference problem and we introduce the classific Pearl's Three-Step Procedure for Counterfactual Inference. 


% 因果三层级
% the generation mechanisms
%  and the $do$-operator 
%{\bf Interventions.} Given a SCM $\cM$ and a set of observed variables $\textbf{X} \subseteq \textbf{V}$, an intervention $do(\textbf{X} = \textbf{x})$   is equivalent to replacing $\mathcal{F}$ 
%   with $\cF_{\textbf{x}} = \{f_j: V_j \not \in \textbf{X} \} \cup \{\textbf{X} = \textbf{x} \}$,  i.e., substituting all functions that determine the values of variables in $\textbf{X}$  with a set of constant functions ${\textbf{X} = \textbf{x} }$. 
%% that is,  removing all functions that determine that values of variables in $\textbf{X}$ and replacing them with the set of constant functions $\{\textbf{X} = \textbf{x} \}$. 
% For an observed variable $Y \in \textbf{V}$ but $Y \not \in \textbf{X}$, let $Y^{\textbf{X=x}}$ be the interventional $Y$ after $do(\textbf{X} = \textbf{x})$, it reflects the potential outcome of $Y$ if we make an intervention or taking an action. 
 %  then invidualized causal effect of interventing from $do(\textbf{X} = \textbf{x}')$ to do(\textbf{X} = \textbf{x}')  $Y^{\textbf{X=x}} - Y^{\textbf{X=x}'} $.  
 
  % also referred as potential $Y$.  

% inference 
 {\bf Counterfactual Inference}  (\citeauthor{pearl2009causality}, \citeyear{pearl2009causality}). Suppose that we have three sets of variables denoted by $X, Y, {\bf E} \subseteq {\bf V}$,  counterfactual inference revolves around the question, ``given evidence  ${\bf E} = {\bf e}$, what would have happened if we had set $X$ to a different value $x'$?".   \citet{Pearl-etal2016-primer} propose using the three-step procedure to answer the problem: (a) {\bf Abduction}: determine the value of ${\bf U}$ according to the evidence ${\bf E} = {\bf e}$; (b) {\bf Action}: modify the model $\mathcal{M}$ by removing the structural equations for $X$ and replacing them with $X = x'$, yielding the modified model $\mathcal{M}_{x'}$; (c) {\bf Prediction}:  Use $\mathcal{M}_{x'}$ and the value of ${\bf U}$ to calculate the counterfactual outcome of $Y$. In this paper, we focus on estimating the counterfactual outcome for each individual.
 %single. 
To illustrate the main ideas, we formulate the common counterfactual inference problem within the context of the backdoor criterion.  
 % performing deterministic counterfactual inference, i.e., 
% where the value of ${\bf U}$ is deterministic for each individual \citep[Section 4]{Pearl-etal2016-primer}. \rc{In fact, we are not following this as we allow $\bf U$ to change with treatment $X$, this implies that an individual can have different $\bf U$ when it has different $X$.} 


{\bf Problem Formulation.}  Let $\mathbf{V} = (Z,  X, Y)$, where $X$ causes $Y$, $Z$ affects both $X$ and $Y$, and the structure equation of $Y$ is given as 
	\begin{equation}  \label{eq1}
   Y = f_Y(X, Z, U_X).  
	 \end{equation} 
% \col{Figure \ref{fig:1}(a) displays a typical causal graph in this setting, where we omit the exogenous variables of $X$ and $Z$ as they will not be used throughout this work.} 
 Let $Y_{x'}$ denotes the potential outcome if we had set $X=x'$.   
 The counterfactual question, ``given evidence $(X = x, Z = z, Y = y)$ of an individual, what would have happened had we set $X = x'$ for this individual", is formally expressed as estimating $y_{x'}$, the realization of $Y_{x'}$ for the individual. Here, we adhere to the deterministic viewpoint of \citet{pearl2009causality} and \citet{Pearl-etal2016-primer}, treating the value of $Y_{x'}$ for each individual as a fixed constant.  
   % outlined by
   % \citet{pearl2009causality} and  
   According to Pearl’s three-step procedure, 
    % for counterfactual inference, 
   given the evidence $(X = x, Z = z, Y = y)$ for an individual, the identifiability of its counterfactual value $y_{x'}$ can be achieved by determining the structural equation $f_Y$ and the value of $U_X$ for this individual. This is the key idea underlying most of the existing methods. 
   
 For clarity, we use $y_{x'}$ to denote the realization of the counterfactual outcome $Y_{x'}$ \emph{for a specific individual} with observed evidence $(X=x, Z=z, Y=y)$.
 % By definition, for this individual, the realization of factual outcome $Y_x$, denoted as $y_x$, equals $y$. 



% =======================================================
% =============================================================== 
\vspace{-6pt} 
\section{Analysis of Existing Methods} \label{sec4}
In this section, we elucidate the challenges of counterfactual inference. Subsequently, we summarize the existing methods and shed light on their limitations.
%This clarification helps further analysis of current approaches. Subsequently, we summarize the existing methods and shed light on their limitations.
% and thereby motivating the proposal of our method. 
  % facilitates further analysis of current methods.
% and reveal the limitations of them, motivating this work.
% and its equivalence to estimating individualized treatment effects. 
% In this section, we elucidate the challenges associated with counterfactual inference. This clarification aids in the subsequent analysis of current approaches. We then provide a summary of existing methods, highlighting their limitations and thereby motivating the proposal of our method

\subsection{Challenges in Counterfactual Inference}  
% To proceed. 
% To clarify, we first elucidate the challenges in counterfactual inference. For simplicity, we consider the case of the backdoor criterion.  

% The challenges lie in the following two aspects. \rc{I feel both aspects say $y_{x'}$is not identifiable in RCT or with interventional expression.}
% outcome  $y_{x'}$
The main challenge lies in that the counterfactual value $y_{x'}$ is generally not identifiable, even in randomized controlled experiments (RCTs). 
% in counterfactual inference  
By definition, $y_{x'}$ is a quantity involving  two ``different worlds" at the same time: the observed world with $(X = x, Z=z, Y = y)$ and the hypothetical world where $X = x'$.  We only observe the factual outcome $Y_x = y$ but never observe the counterfactual outcome $Y_{x'}$, which is the fundamental problem in causal inference~\citep{Holland1986, Morgan-Winship-2015}. 
This inherent conflict prevents us from simplifying the expression of $y_{x'}$ to a do-calculus expression, making it generally unidentifiable, even in RCTs~\citep{Pearl-etal2016-primer}. 
% This issue is specific to counterfactual inference and does not arise in intervention expressions, as statistics of interventional distributions such as $\bfE[Y_{x'}]$ or $\bfE[Y_{x'}|Z=z]$ are defined solely in a single world where the treatment $X$ is set to $x'$ by intervention. 
% Hence, in addition to commonly employed assumptions like conditional exchangeability, overlapping, and consistency, counterfactual inference demands several supplementary assumptions.
 % Consequently, 
Therefore, in addition to the widely used assumptions such as conditional exchangeability, overlapping, and consistency~\citep{Hernan-Robins2020},  counterfactual inference requires extra assumptions to ensure identifiability.  Essentially,  estimating $y_{x'}$ is equivalent to estimating the individual treatment effect $y_{x'} - y_{x}$, while the conditional average treatment effect (CATE) $\bfE[Y_{x'}  - Y_x | Z=z ]$  represents the ATE for a subpopulation with $Z=z$, overlooking the inherent heterogeneity in this subpopulation caused by the noise terms such as $U_X$~\citep{Albert-etal2005, Heckman1997, Djebbari-Smith2008, Ding-etal2019, Lei-Candes2021, Eli-etal2022, Jin-etal2023, Wu-etal-2024-Harm}. 
% Randomization can help identify the treatment effects in an average sense, but cannot identify the treatment effects at the individual level \citep{Imbens-Rubin2015, Lei-Candes2021, Eli-etal2022, Jin-etal2023}.


 % and they are identifiable in RCTs. 
%Such a problem is unique in counterfactual inference and does not occur in intervention expressions, because interventions such as $\bfE[Y_{x'}]$ or $\bfE[Y_{x'}|Z=z]$ are defined only in a single world where $X=x'$, and they are identifiable in RCTs.    

% Second, $\tau$ is a quantity at the individual level. It represents that


%Second,  $y_{x'}$is an individual-level quantity. It signifies  ``for an individual with observed values of $(X=x, Z=z, Y=y)$, what would $Y$ be if $X$ had been set to $x'$ \emph{for the same individual}". However, we only observe the outcome $Y_x = y$ and never observe $Y_{x'}$, which is also known as the fundamental problem in causal inference~\cite{Holland1986, Morgan-Winship-2015}. Therefore, estimating $Y_{x'}$ without additional assumptions \rc{additional to widely used assumptioons such as strong ignorability, overlapping, and consistency etc.?} is an impossible task. 
% In contrast, intervention expressions are often used to define and estimate average treatment effects (ATEs) in a population or subpopulation, rather than at the individual level. 
 
 % the intervention expressions often be used to define and estimate the average causal effects in a population or subpopulation, instead of individual level.  

%  Thus, it is an impossible task to estimate $Y_{x'}$ without imposing further assumptions. 
% For each individual, we need to know both $Y^{X=x}$ and $Y^{X=x'}$ simultaneously. 
 % Since each individual can be only assigned with one treatment, 
% 为什么要给定证据呢？
% 这两个世界有关联；但是不能同时观测到。




% Eli-etal2022, 2022nathan, 
 % , 
% Thus, it involves the joint distribution of potential outcomes $(Y^{X=x}, Y^{X=x'})$, which is correlated by sharing the common noise $E_Y$. 
% (Different from causal effect estimation that does not need to consider the association between $Y^x$ and $Y^{x'}$)
 % Unfortunately, one can never calculate the individual treatment effects,  as only one of $Y^{X=x}$ and $Y^{X=x'}$ is observable in reality, which is the basic problem of causal inference~\citep{Holland1986, Morgan-Winship-2015}.           


%It is helpful to contrast it with causal effect estimation. Conser the case of binary $X$, $X \in \{0, 1\}$.  
%For individual $i$, the individual treatment effect (ITE) is defined as 
%		\[    Y_i^{X_i = 1} - Y_i^{X_i = 0},   \] 
%and the conditional average treatment effect (CATE) is defined as
%	\[  \tau(z) =  \bfE[ Y_i^{X_i = 1} - Y_i^{X_i} | Z_i = z  ], \]
%i.e., the difference in the conditional mean outcomes between treatments given covariate. 
%% For simplification, we drop the subscript $i$ for a generic individual hereafter. 
%Generally, the  CATE is the most fine-grained causal effect that we can identify. 
 
  
 % 第二层级到第三层级的问题，本质上是从群体层面到个体层面，从边缘分布到联合分布。
%Essentially, the problem from intervention to counterfactual, i.e., claiming from the second to the third level of causality, is a question from causal effects to the subpopulation level to causal effects at the individual level, and a question from marginal distribution of potential outcome to joint distributions of potential outcomes.

%
%Let $A$ denote the indicator for binary treatment, with $A=1$ or 0 the treated or control group, $X \in \mathcal{X} \subset \mathbb{R}^p$ denotes the observed pre-treatment covariates,   $Y \in \mathcal{Y} \subset \mathbb{R}$ denotes the long-term outcome of interest, and and $S$ denote the short-term outcome 
%(e.g., intermediate outcomes) that is informative about the long-term outcome $Y$ and measured after the treatment $T$.
%Under the potential outcome framework \citep{Rubin1974, Neyman1990},  let $(S(1), Y(1))$ and $(S(0), Y(0))$ be the potential short-term and long-term outcomes with and without treatment, respectively.    
%We assume that the actual short/long-term  outcome is the potential outcome corresponding to the actual
%treatment, i.e., $S = S(A)$ and $Y = Y(A)$, which implicitly implies the non-interference and consistency assumptions in causal inference~\cite{Imbens-Rubin2015}.

% For a generic individual, we drop the subscript $i$ to simplify the notation.






% Identifiability for Counterfactual Inference
     % Key Assumptions Underlying the



\subsection{Summary of Existing Methods}  \label{sec3-2}
% present the basic ideas of
% We summarize the existing methods for counterfactual inference.  
We summarize the existing methods in terms of identifiability assumptions and estimation strategies.   
% both 

% For clarity,  incorporating counterfactuals
We first present an equivalent expression of Eq. (\ref{eq1}) using  $(Y_x, Y_{x'})$. 
%$=by using the notation of $(Y_x, Y_{x'})$. 
Eq. (\ref{eq1}) be reformulated as the following system 
\begin{equation*}  Y_x = f_Y(x, Z, U_{x}), ~  Y_{x'} = f_Y(x', Z, U_{x'}), \end{equation*} 
where $U_x$ and $U_{x'}$ denote the values of $U_X$ given $X=x$ and $X=x'$, respectively.  
The exogenous variable $U_X$ denotes the background and environment information induced by many unmeasured factors~\citep{Pearl-etal2016-primer}, and thus $U_{x}$ and $U_{x'}$ account for the heterogeneity of $Y_x$ and $Y_{x'}$ in the observed and hypothetical worlds, respectively. 
These two worlds may exhibit different levels of noise due to unmeasured factors~\citep{Heckman1997, Chernozhukov2005, Ding-etal2019}.   
 % The difference between $U_{x}$ and $U_{x}'$ can be caused by the difference in unmeasured background variables of the two different worlds, see Section \ref{sec4-3} for a detailed discussion.  


 
For identification, previous work~\citep{Lu-etal2020-attribution, Xie-etal2023-attribution, Nasr-Esfahany-2023-attribution}  
relies on the key homogeneity and strict monotonicity assumptions.
% two key assumptions, i.e., homogeneity and strict monotonicity.% as follows. % as follows. % (Assumptions \ref{assump1} and \ref{assump2}) below. 
%In what follows, we focus the discussion on the case where the treatment takes on two values $X \in \{0, 1\}$, as it simplifies the discussion and best illustrates the conditions required for identification. However, the definition of the model and its main statistical implications do not rely upon this notational simplification. The Appendix contains generalizations to nonbinary treatments. 
\begin{assumption}[Homogeneity]  \label{assump1}
        $U_{x} = U_{x'}$. 
        % given $Z$, where $U_{x}$ and $U_{x'}$ are the values of $U_X$ under $X=x$ and $X=x'$.   	
 % $U_X \indep X \mid Z$
\end{assumption}

% distribution $\P(U_X |  X=x, Z=z)$ equals to $\P(U_X | Z=z)$ and
% does not vary across $x$.
% $U_X$ are not correlated with $X$ given $Z$, and its 
% For fixed values of
\begin{assumption}[Strict Monotonicity] \label{assump2} 
%given $x$ and $z$, 
For any given $(x,z)$,  
	$Y_x = f_Y(x, z,  U_x)$  is a smooth and strictly monotonic function of $U_x$; or it is a bijective mapping from $U_x$ to $Y_x$. %  for all $(x,z)$
\end{assumption}


Assumption \ref{assump1} implies that the value of $U_X$ for each individual remains unchanged across $x$. Assumption \ref{assump2} implies that $Y_x$ is a strict monotonic function of $U_x$ in the subpopulation of $(X=x, Z=z)$. 
 In Assumption \ref{assump2}, the smoothness and strict monotonicity of $f_Y(x, z,  U_x)$ are akin to a bijective mapping of $Y_x$ and $U_x$ and serve the same purpose,  so we don't distinguish them in detail. 
% In addition,  
%  The identifiability of $y_{x'}$ in  
% \citet{Lu-etal2020-attribution}, \citet{Xie-etal2023-attribution} and \citet{Nasr-Esfahany-2023-attribution} depends on Assumptions \ref{assump1}-\ref{assump2}, 
% % the homogeneity and strict monotonicity assumptions,
% as summarized in Lemma  \ref{lem1}.    
% Assumptions \ref{assump1}-\ref{assump2}. 
% Lemma  \ref{lem1} summarizes the identifiability results.


\begin{lemma} \label{lem1}  
   Under Assumptions \ref{assump1}-\ref{assump2}, $y_{x'}$ is identifiable. 
\end{lemma}

% \citet{Lu-etal2020-attribution}, \citet{Xie-etal2023-attribution} and \citet{Nasr-Esfahany-2023-attribution} employ a slightly different assumption, $U_X \indep (X, Z)$ or $U_X \indep X | Z$, as an alternative to Assumption \ref{assump1}.
% However, in their proof,  they rely on Assumption \ref{assump1} and assert that it is implied by $U_X \indep (X, Z)$ or $U_X \indep X | Z$.  
% substitute for 

% we omit the discussion on it. % stronger assumption.  

 % Table \ref{tab1} compares the identifiability assumptions of existing methods.  

% Table \ref{tab1} compares the identifiability assumptions of existing methods.  The identifiability assumptions in Lemma \ref{lem1} resemble those in \citet{Lu-etal2020-attribution} and \citet{Xie-etal2023-attribution}, but replacing $U_X \indep (X, Z)$ with Assumption \ref{assump1}. In addition, 
% to identify $U_X$ and $f_Y$
 % based on the evidence $(X = x, Z = z, Y = y)$. 
%  The identifiability assumptions in Lemma  \ref{lem1} are also weaker than Theorem 5.3 of \citet{Nasr-Esfahany-2023-attribution} where the authors use some additional regularity conditions like full column rank to solve $U_X$ by the evidence. % $(X = x, Z = z, Y = y)$. 


 % The identifiability assumptions in Lemma \ref{lem1} are less stringent than those in Theorem 5.3 of \citet{Nasr-Esfahany-2023-attribution}, where the authors introduce additional assumptions such as variability for estimating the SCM. 
 
 % For estimation of $y_{x'}$, \citet{Lu-etal2020-attribution}  and \citet{Nasr-Esfahany-2023-attribution} adopt the Pearl’s three-step procedure. 
 % \citet{Xie-etal2023-attribution} employ quantile regression based on the finding that $y_{x'}$ corresponds to the $\tau^*$-th quantile of the distribution $\P(Y| X=x', Z=z)$, where $\tau^*$ is the quantile of $y$ in $\P(Y| X=x, Z=z)$ (See the proof of Lemma \ref{lem1} or Section \ref{sec4-1} for more details).   

For estimation of $y_{x'}$, 
 % For estimating $y_{x'}$,
 following Pearl’s three-step procedure,  
\citet{Lu-etal2020-attribution}  and \citet{Nasr-Esfahany-2023-attribution} initially estimate $f_Y$ and $U_X$ for each individual. However, estimating $f_Y$ and $U_X$ needs to impose extra assumptions, such as linearity~\citep{shimizu2006linear} and additive noise~\cite{peters2014causal}. 
%On the other hand,
In addition, \citet{Xie-etal2023-attribution} demonstrate that $y_{x'}$ corresponds to the $\tau^*$-th quantile of the distribution $\P(Y| X=x', Z=z)$, where $\tau^*$ is the quantile of $y$ in $\P(Y| X=x, Z=z)$ (See the proof of Lemma \ref{lem1} or Section \ref{sec4-1} for more details). Based on it, the authors uses quantile regression to estimate $y_{x'}$, which avoids the problem of estimating $f_Y$ and $U_X$. Nevertheless, this method fits a single model to obtain the conditional quantile functions for both  the counterfactual and factual outcomes. Thus, its validity relies on the underlying assumption that the conditional quantile functions of outcomes for different treatment groups stem from the same model. 
   In addition, it involves estimating a distinct quantile value for each individual before deriving the counterfactual outcomes, posing a challenging  bi-level optimization problem.  




% ===============================================================================
\section{Identification through Rank Perservation}  \label{sec5}
% Counterfactual Inference with 

% \rc{is it possible to make the title of Sec 4 different from Sec 4.1? For example, Sec 4.1 .}
In this section, we introduce the rank preservation assumption for identifying $y_{x'}$.    
% relax the identifiability assumptions \ref{assump1}-\ref{assump2}.  
\emph{From a high-level perspective, identifying $y_{x'}$ essentially involves establishing the relationship between $Y_x$ and $Y_{x'}$ for each individual.} Pearl's three-step procedure achieves this by estimating $f_Y$ and $U_X$, as well as the homogeneity assumption.  
%  based on the observed data. 
% The quantile regression strategy in \citet{Xie-etal2023-attribution} is based on the rank invariance between  $Y_x$ and $Y_{x'}$. 
% , while assuming $U_X$ remains the same between $Y_x$ and $Y_{x'}$


\subsection{Rank Perservation Assumption}   \label{sec4-1}
% \ref{assump1}
Our identifiability assumption is based on Kendall’s rank correlation coefficient defined below. % and we introduce it first. 

%\begin{definition}[Rank and Order Statistics]    Let  $X_1, ..., X_n$  be simple random samples from a population $F$,  where the number of data not exceeding $X_i$, i.e., $R_i=\sum_{j=1}^n \mathbb{I}(X_j \leq X_i)$, is called the rank of $X_i$. If $X_i$ is the $R_i$-th order statistic, then $X_{(R_i)} =X_i$. 
%\end{definition}
% Kendall rank correlation coefficient, , 
\begin{definition}[\citeauthor{Kendall1938}, \citeyear{Kendall1938}]  \label{def1}
Let $(x_1, y_1), ..., (x_n, y_n)$ be a set of observations of two random variables $(X, Y)$,  such that all the values of $x_{i}$ and $y_{i}$ are unique (ties are neglected for simplicity). 
 Any pair of $(x_i, y_i)$ and $(x_j, y_j)$, if $(x_j - x_i)(y_j - y_i) > 0$,  they are said to be concordant;  otherwise they are discordant.  % said to be  if the sort order of 
  The \emph{sample} Kendall rank correlation coefficient  is  defined as 
	\[ \rho_n(X, Y) = \frac{2}{n(n-1)} \sum_{1\leq i < j\leq n} \text{sign}( (x_i - x_j) (y_i - y_j)  ),      \]
% where $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs, and
where $\text{sign}(t) = -1, 0, 1$ for $t < 0$, $t =0$, $t > 0$, respectively. For any two random variables $(X, Y)$, we define $\rho(X, Y) = 1$, if $\rho_n(X, Y) =1$ for all integers $n\geq 2$.

% $n(n-1)/2$ is the total number of pair combinations and $\text{sign}(t) = -1, 0, 1$ for $t < 0$, $t =0$, $t > 0$, respectively. % respectively. 
% = \frac{N_c - N_d}{ n(n-1)/2 } 
% is the total number of pairs. 
%the binomial coefficient for the number of ways to choose two items from $n$ items. 
\end{definition}

% coefficient  
The $\rho_n(X, Y)$ also can be written as $2(N_c - N_d)/ n(n-1)$, where  $N_c$ is the number of concordant pairs, $N_d$ is the number of discordant pairs. It is easy to see that $-1 \leq  \rho_n(X, Y) \leq 1$ and if the agreement between the two rankings is perfect (i.e., perfect concordance),  $\rho_n(X, Y) = 1$. 
% If $X$ and $Y$ are independent, then the expectation of $\rho(X, Y)$ is zero. 
%  and not constant

%  全部数据所有可能的前后数对共有$C_n^2 = n(n-1)/2$对。如果用$N_c$表示同向数对的数目，$N_d$表示反向数对的数目，则$N_d + N_c = n(n-1)/2$，Kendall $\tau$秩相关系数定义为
% $\tau = 1$意味着所有数对协同一致。
% Concordant means change in the same direction.



\begin{assumption}[Rank Preservation]   \label{assump3} 
 $\rho(Y_x, Y_{x'} | Z) = 1$.      %  \mid Z assump3
\end{assumption}

For the individual with observation $(X =x, Z=z, Y= y)$, we denote $(y_x=y, y_{x'})$ as its true values of  ($Y_x, Y_{x'}$).  Assumption \ref{assump3} implies that for this individual,  
 its rankings of $y_{x}$ and $y_{x'}$ are the same in the distributions of $\P(Y_x | Z=z)$ and $\P( Y_{x'} | Z=z )$,  
 respectively. Therefore, we have
       \begin{equation} \label{eq4}
              \P( Y_x \leq y_x  | Z=z)  =  \P(  Y_{x'} \leq y_{x'}  | Z=z ).  
       \end{equation}
Since $y_x = y$ is observed and the distributions $\P(Y_x| Z=z) $ and $\P(Y_{x'}| Z=z)$ can be identified as $\P(Y | X=x, Z=z)$ and $ \P(Y | X=x', Z=z)$, respectively, by the backdoor criterion (i.e., $(Y_x, Y_{x'}) \indep X | Z$). Therefore, we have the following Proposition \ref{prop1} (see Appendix \ref{app-a} for proofs). 

%Next, we show that Assumption \ref{assump3} is a substitution to Assumption \ref{assump1} in terms of identifying $y_{x'}$, and is strictly weaker than Assumption \ref{assump1}. 

 \begin{proposition} \label{prop1}    Under Assumption \ref{assump3}, $y_{x'}$ is identified as %identifiable.  
   the $\tau^*$-th quantile of  
    $\P(Y| X=x', Z=z)$, where $\tau^*$ is the quantile of $y$ in the distribution of $\P(Y | X=x, Z=z)$.
    % that is, $\tau^*$ satisfies  $\P(  Y \leq y  | X=x, Z=z ) = \tau^*$.  % is the solution satisfying
%  	\begin{align*} & E\indep X \mid Z ~\text{and} ~ f(X, Z, \cdot) \text{ is monotonic}    \\
%		&  \implies    \tau(Y_x, Y_{x'}) = 1 \mid Z.  
%	\end{align*} 
 \end{proposition}
 
 
Proposition \ref{prop1} shows that Assumption \ref{assump3} can serve as a substitute for Assumptions \ref{assump1}-\ref{assump2} in identifying $y_{x'}$.  Unlike Assumptions \ref{assump1}-\ref{assump2}, Assumption \ref{assump3} is simple and intuitive, as it directly links $Y_x$ and $Y_{x'}$ for each individual. 
%Compared with   Assumptions \ref{assump1}-\ref{assump2}, 
%Assumption \ref{assump3}  establishes a direct connection between  $Y_x$ and $Y_{x'}$ for each individual.  
% One may wonder about the relationship between Assumption \ref{assump3} and Assumptions \ref{assump1} and \ref{assump2}. We present this relationship in Proposition \ref{prop2} (see Appendix \ref{app-a} for the proofs). 
% In addition, we demonstrate that Assumption \ref{assump3} is not stronger than Assumptions \ref{assump1}-\ref{assump2}, as shown in Proposition \ref{prop2} (see Appendix \ref{app-a} for proofs).
% strictly weaker than Assumptions \ref{assump1}-\ref{assump2}, as shown in Proposition \ref{prop2} (see Appendix \ref{app-a} for proofs).
% Furthermore, 
To clarify the relationship between Assumption \ref{assump3} introduced by this work and Assumptions \ref{assump1}-\ref{assump2} from previous work, we present Proposition \ref{prop2} below.
%(see Appendix \ref{app-a} for proofs).  
\begin{proposition} \label{prop2}
  The proposed Assumption \ref{assump3} is strictly weaker than Assumptions \ref{assump1}-\ref{assump2}.
%   (ii) 
% Under Assumption \ref{assump1}, or more generally, if $U_x$ is a strictly monotone increasing function of $U_{x'}$, Assumption \ref{assump3} is equivalent to Assumption \ref{assump2}. 
\end{proposition}
% It would be better to use an example to explain under what situations $U_x$ can be a monotone increasing function of $U_{x'}$ to justify this assumption. 

% On the hand，之前研究采用的Assumptions \ref{assump1}-\ref{assump2}可以推出我们采用的Assumption \ref{assump3}，通过xx可以证明（see Appendix ? for proofs）。另一方面，我们通过一个例子说明我们的假设严格弱，XXX（具体例子）。

Proposition \ref{prop2} is intuitive, as correlation (Assumption \ref{assump3}) does not necessarily imply identity (Assumption \ref{assump1}). To illustrate, consider a SCM with $X \in \{0, 1\}$,   $Y_1 = Z + U_{1}$, $Y_0 = Z/2 + U_{0}$,  $U_{1} =  U_{0}^3$. In this case, $\rho(Y_0, Y_1 | Z) = 1$, but $U_1 \neq U_0$.
 Nevertheless, Assumption \ref{assump3} is only slightly weaker than Assumptions \ref{assump1}-\ref{assump2} by allowing $U_{x'} \neq U_{x}$. Specifically, we can show that if $U_x$ is a strictly monotone increasing function of $U_{x'}$, Assumption \ref{assump3} is equivalent to Assumption \ref{assump2}, see Appendix \ref{app-a} for proofs.  
%In fact, Assumptions \ref{assump1}-\ref{assump2} is a special case of Assumption \ref{assump3}. Below, we provide an example that violates Assumption \ref{assump1} while Assumption \ref{assump3} still holds.}
% $U_1$ is a strictly monotonic increasing function of $U_0$, 

%\col{Under Assumption \ref{assump1}, or more generally, if $U_x$ is a strictly monotone increasing function of $U_{x'}$, Assumption \ref{assump3} is equivalent to Assumption \ref{assump2}.Proposition \ref{prop2} (see Appendix \ref{app-a} for proofs) indicates that Assumptions \ref{assump3} is equivalent to Assumption \ref{assump2} under more general conditions than those considered in previous work. That is,  Assumption \ref{assump3} is slightly weaker than Assumptions  \ref{assump1}-\ref{assump2} by allowing $U_{x'} \neq U_{x}$.} 
    % \begin{gather*}  \begin{cases}
    %         Y = Z X / 2  + Z / 2 + U_X \\
    % 	%	U_X = X U_{1} + (1- X) U_{0} \\ 
    %       U_{1} = 2 U_{0}   \end{cases} \Longleftrightarrow  \begin{cases}  Y_1 = Z + U_{1}\\
    %              Y_0 = Z/2 + U_{0}. \\ 
    %              U_{1} = 2 U_{0} 
    % \end{cases}     \end{gather*} 
    % \emph{In this case, $\rho(Y_0, Y_1 | Z) = 1$ and clearly, $U_1 \neq U_0$.}  
% Thus, Proposition \ref{prop2} is mainly serve as a 
% In addition, we demonstrate that Assumption \ref{assump3} is not stronger than Assumptions \ref{assump1}-\ref{assump2}, 
% Based on the   
%\bcol{One may argue that }
%not influence by intervening on $X$, that is, $U_X = U_{x} = U_{x'}$ given $Z=z$. [Is it right?]
%[Under what circumstances does intervention $X$ affect $U_X$]?  
% Proposition \ref{prop2} is also intuitive, since correlation (Assumption \ref{assump3}) does not imply independence (Assumption \ref{assump1}).   Below we give an example that violates Assumption \ref{assump1} but Assumption \ref{assump3} holds.   
%Proposition \ref{prop2} is intuitive, as correlation (Assumption \ref{assump3}) does not necessarily imply  identity (Assumption \ref{assump1}). In fact, Assumptions \ref{assump1}-\ref{assump2} is a special case of Assumption \ref{assump3}. Below, we provide an example that violates Assumption \ref{assump1} while Assumption \ref{assump3} still holds.  
    % {\bf Example 1.} \emph{Consider a SCM with $X \in \{0, 1\}$},  
    % \begin{gather*}  \begin{cases}
    %         Y = Z X / 2  + Z / 2 + U_X \\
    % 	%	U_X = X U_{1} + (1- X) U_{0} \\ 
    %       U_{1} = 2 U_{0}   \end{cases} \Longleftrightarrow  \begin{cases}  Y_1 = Z + U_{1}\\
    %              Y_0 = Z/2 + U_{0}. \\ 
    %              U_{1} = 2 U_{0} 
    % \end{cases}     \end{gather*} 
    % \emph{In this case, $\rho(Y_0, Y_1 | Z) = 1$ and clearly, $U_1 \neq U_0$.}  



%  In fact, given the strict monotonicity of $f_Y(x, z, U_X)$ (Assumption \ref{assump2}),  $U_{x}$ determines the relative ranking of $Y_x$ for each specific individual.  Thus, let $g(\cdot)$ be an arbitrary strictly monotone-increasing function.
%  Assumption \ref{assump3} still holds in the following two scenarios (\col{see Proofs in Appendix}). % (i)-(ii) below.   
%  \begin{itemize}
%  	\item {\bf Scenario (i)}. $U_{x'} = g(U_{x})$ given $Z$; 
% 	\item {\bf Scenario (ii)}. $U_{x'}$ and $g(U_{x})$ are identically distributed given $Z$.  %for $\alpha > 0$;  
% %	\item $U_{x} - U$ and $\alpha \cdot U_{x'} - \alpha U$ are identically distributed  
%  \end{itemize}

% Scenario (ii) is weaker than  Scenario (i). In addition, Assumption \ref{assump1} implies that $U_{x} = U_{x'}$ given $Z$, which is a specific case of Scenario (i).  

%Assumption \ref{assump1} $U_X \indep X \mid Z$ implies that $U_X  = U_{x} = U_{x'}$ given $Z$, which is a specical case of  $U_{x} =\alpha \cdot U_{x'} $ for $\alpha > 0$.   \col{Assumption \ref{assump1} implies that the potential outcomes $(Y_x, Y_{x'})$ are not truly multivariate, being jointly degenerate~\citep{Heckman1997}}.

%  Also, $U_{x}$ may be determined by many unobserved factors. Thus, it is desirable to allow $U_{x}$  to change across $x$, reflecting some unobserved, unsystematic variation among different potential outcomes~\citep{Chernozhukov2005}.} 

% and it can be relaxed to Assumption 3 (rank correlation and rank similarity).

% ======================================================
\subsection{Further Relaxation of Strict Monotonicity}

% Although Assumption \ref{assump3} is strictly weaker than the identifiability Assumptions \ref{assump1}-\ref{assump2}, it does not overcome Limitation 2 outlined in Section \ref{sec4-3}. 
% Specifically, $\rho(Y_x, Y_{x'} | Z=z) = 1$ means the absence of ties for $Y_x$ or $Y_{x'}$ given $Z=z$.  Similar to Assumption \ref{assump2}, 
%  it also excludes the scenarios involving discrete or continuous $Y$ with ties, as in such cases, $\rho(Y_x, Y_{x'})$ will always be less than 1 by Definition \ref{def1}.
%  then the maximal value of $\rho(Y_x, Y_{x'})$ is $2/n(n-1)$.  
% of discrete $Y$ or continuous $Y$ with ties, 
% Assumption \ref{assump3} mitigates Assumption \ref{assump1}. 
% Next, we focus on further relaxing  Assumption \ref{assump2}.  
In Definition \ref{def1}, we ignore ties for simplicity. However, 
 when the outcome $Y$ is discrete or continuous variables with tied observations, $\rho(Y_x, Y_{x'})$ will always be less than 1.  % by Definition \ref{def1}. 
To accommodate such cases, 
we introduce a modified version of the Kendall rank correlation coefficient given below. 
\begin{definition}[\citeauthor{Kendall1945}, \citeyear{Kendall1945}]  \label{def2} Let $(x_1, y_1), ..., (x_n, y_n)$ be the observations of two random variables $(X, Y)$, the modified Kendall rank correlation coefficient is define as   
	\begin{gather*} \tilde \rho_n(X, Y) =  \sum_{1\leq i < j\leq n} \frac{\text{sign}( (x_i - x_j) (y_i - y_j)  )}{ \sqrt{n(n-1)/2 - T_x} \cdot \sqrt{n(n-1)/2 - T_y}   }, \end{gather*}
	where $T_x$ is the number of tied pairs in $\{x_1, ..., x_n\}$ and  $T_y$ is the number of tied pairs in $\{y_1, ..., y_n\}$. 
	We define $\tilde \rho(X, Y) = 1$, if $\tilde \rho_n(X, Y) =1$ for all integers $n\geq 2$. 
\end{definition}


%By comparison of Definition \ref{def2} and
Compared with Definition \ref{def1}, one can see that $\tilde \rho(X,Y)$ adjusts $\rho(X, Y)$ by eliminating the ties in the denominator, and $\tilde \rho(X,Y)$ reduces to $\rho(X, Y)$ if there are no ties.   

\begin{assumption}[Rank Preservation] \label{assump4} 
 $\tilde \rho(Y_x, Y_{x'} | Z) = 1$.      %  \mid Z assump3
\end{assumption}

Assumption \ref{assump4} is less restrictive than Assumption \ref{assump3} as it accommodates broader data types of $Y$. To illustrate, consider a dataset with four individuals where the true values of $(Y_x, Y_{x'})$ are $(1, 1), (2, 1.5), (2, 1.5), (3, 2.5)$. In this scenario, $\sum_{1\leq i < j\leq n} \text{sign}( (y_{i,x} - y_{j,x}) (y_{i,x'} - y_{j,x'}) = 5$, $T_{Y_x} =1$, $T_{Y_{x'}}=1$, resulting in $\rho(Y_x, Y_{x'}) = 5/6$ and $\tilde \rho(Y_x, Y_{x'}) = 5/(\sqrt{6-1} \cdot \sqrt{6-1}) = 1$.

% We provide an example to further illustrate it. 
% Assumption \ref{assump4} is weaker than Assumption \ref{assump3} as it allows more general data types of $Y$.  Consider a dataset consisting of four individuals, the true values of $(Y_x, Y_{x'})$ are $(1, 1), (2, 1.5), (2, 1.5), (3, 2.5)$, then $\sum_{1\leq i < j\leq n} \text{sign}( (x_i - x_j) (y_i - y_j) = 5$, $T_{Y_x} =1$, $T_{Y_{x'}}=1$, which leads to $\rho(Y_x, Y_{x'}) = 5/6$ and $\rho_m(Y_x, Y_{x'}) = 5/(\sqrt{6-1} \cdot \sqrt{6-1}) = 1$.  
%Next, we show that Assumption \ref{assump3} is a substitution to Assumption \ref{assump1} in terms of identifying $y_{x'}$, and is strictly weaker than Assumption \ref{assump1}. 
%In addition, 
Assumption \ref{assump4} also guarantees the identifiability of $y_{x'}$.
% as shown in Proposition \ref{prop3}.
%as follows.  
 \begin{proposition} \label{prop3}    Under Assumption \ref{assump4}, the conclusion in Proposition \ref{prop1} also holds. % $y_{x'}$is identifiable.  
 \end{proposition}
% In summary,  substituting Assumption \ref{assump3}  with Assumption \ref{assump4}  not only extends the applicability of our conclusions to all data types of $Y$, but also further relaxes the identifiability assumptions of $y_{x'}$. 


% makes our conclusions applicable to all data types of $Y$,  
 
% In summary, substituting Assumption \ref{assump3} with Assumption \ref{assump4} not only broadens the applicability of our conclusions to all data types of $Y$, but also eases the identifiability assumption fo




% =========================================================
\section{Counterfactual Learning }\label{sec6}
% Novel Algorithm for Inference 

% For counterfactual learning, prior approaches such as those by \citet{Lu-etal2020-attribution} and \citet{Nasr-Esfahany-2023-attribution} involve the initial estimation of $f_Y$ and $U_X$ for each individual. However, this is not a easy task due to the elusive relationship between $Y$ and $(X, Z, U_X)$, and requires additional assumptions to identify $f_Y$ and $U_X$. An alternative approach by \citet{Xie-etal2023-attribution} utilizes quantile regression, effectively circumventing the issues of estimating $f_Y$ and $U_X$. Nonetheless, this method neglects the problem of selection bias and restrict the conditional quantile functions to be the same for different treatment groups.  
% counterfactual learning method 
% that tackles these problems.
%leading to biased estimates.  
% In this section, 
We propose a novel estimation method for counterfactual inference. Suppose that $\{ (x_k, z_k, y_k): k = 1, ...,  N\}$ is a sample consisting of $N$ realizations of random variables $(X, Z, Y)$. For an individual, given its evidence $(X=x, Z=z, Y= y)$, we aim to estimate its counterfactual outcome $y_{x'}$, i.e., the realization of $Y_{x'}$ for this individual. %which s
% In this section, we elucidate the challenges of counterfactual inference. This clarification helps further analysis of current approaches. Subsequently, we summarize the existing methods, shedding light on their limitations and thereby motivating the proposal of our method. 

% Using Rationale and 
\subsection{Rationale and Limitations of Quantile Regression}
%The Rationale Behind Quantile Regression}
  % For ease of exposition and without loss of generality, we let $x_i = 0$ and $x' = 1$,  then $(y_i, y_{i,1})$ are the realizations of $(Y_0, Y_1)$ for individual $i$.     
  For estimating $y_{x'}$, 
  \citet{Xie-etal2023-attribution} formulate it as the following bi-level optimization problem % the counterfactual learning problem
        \begin{align*}
                \tau^* ={}& \arg \min_{\tau} | f_{\tau}(x, z) - y |,  \\ % \quad
                f_{\tau}^* ={}& \arg \min_{f} \frac 1 N \sum_{k=1}^N l_{\tau}( y_k - f(x_k, z_k) ), 
                        \end{align*}
 where $l_{\tau}(\xi) = \tau \xi \cdot \mathbb{I}(\xi \geq 0)+ (\tau - 1)\xi \cdot \mathbb{I}(\xi < 0)$ is the check function~\citep{Koenker1978}, the upper level optimization is to estimate $\tau^*$, the quantile of $y$ in the distribution $\P(Y | X=x, Z =z)$,  and the lower level optimization is to estimate the conditional quantile function $q(x, z; \tau) \triangleq \inf_{y}\{y: \P( Y \leq y | X=x, Z=z) \geq \tau \}$  for a given $\tau$. Then $y_{x'}$ can be estimated using $q(x', z; \tau^*)$. 
 % After obtaining the estimate of $\tau$ and $q_{\tau}(x,z)$,   
% denoted by the $\tau$ corresponding to
 
 
%Before presenting the proposed method, we first introduce the method of . 
     %is an independent and identically distributed sample of 
%,  % we assume
% To reveal the limitations of \citet{Xie-etal2023-attribution}'s method. 
We define two conditional quantile regression functions,
%for $Y_x$ and $Y_{x'}$ as follows,  
        \begin{align*} 
              q_x(z; \tau)  \triangleq{}& \inf_{y}\{y: \P( Y_x \leq y | Z=z) \geq \tau \}, \\ 
              %\quad     
              q_{x'}(z; \tau)   \triangleq{}& \inf_{y}\{y: \P( Y_{x'} \leq y | Z=z) \geq \tau \}. 
        \end{align*}
% be the conditional quantile regression functions for $Y_x$ and $Y_{x'}$, respectively. 
By Eq. (\ref{eq4}), $y_{x'}$ can be expressed as   $q_{x'}(z; \tau^*)$ with $\tau^*$ being  the quantile of $y$ in the distribution of $\P(Y_x | Z=z)$, i.e., $  \P( Y_{x} \leq y  | Z=z ) = \tau^*$. Lemma \ref{prop4} (see Appendix \ref{app-b} for proofs) shows the rationale behind employing the check function as the loss to estimate conditional quantiles.  % following  
\begin{lemma} \label{prop4} 
We have that    \\ 
(i) $q_x(Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y_x - f(Z))]$ for any given $x$; \\ (ii)  $q(X, Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y - f(X, Z))]$.
% (i) $q_x(Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y_x - f(Z))]$ for any given $x$;  
% (ii)  $q(X, Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y - f(X, Z))]$.
\end{lemma}	 
  %(ii) $ \bfE[ l_{\tau}(Y - f(X, Z))] \neq   \bfE[ l_{\tau}(Y_x - f(Z))],$ which implies that $q(x, Z; \tau)$ not equal to $q_x(Z; \tau)$ for $x = 0, 1$. 
% functions.
% This is obviously problematic since selection bias is prevalent in observational data \rc{
% regarding the estimation method proposed by
 There are two major concerns with the estimation method of \citet{Xie-etal2023-attribution}. 
First, it only fits a single quantile regression model for $q(X, Z;\tau)$ to obtain estimates of $q_x(Z; \tau)$ and $q_{x'}(Z; \tau)$. When the two conditional quantile functions $q_x(Z; \tau)$ and $q_{x'}(Z; \tau)$ originate from different models, this method may yield inaccurate estimates.   
Second, it explicitly requires estimating the quantile $\tau^*$ for each individual before estimating the counterfactual outcome $y_{x'}$. % \col{Also, the upper level optimization cannot ensure a convex problem, which may cause instability in the bi-level optimization.} 

% Its validity potentially depends on the conditional quantile functions $q_x(Z; \tau)$ and $q_{x'}(Z; \tau)$ coming from the same model.
% assuming that the conditional quantile functions $q_x(Z; \tau)$ and $q_{x'}(Z; \tau)$ come from the same model. share the same model. 
% This lack of flexibility may hinder the adaptation to diverse function forms for $q_0(Z; \tau)$ and $q_1(Z; \tau)$.
 
 % {First, it neglects selection bias,
 % i.e., the distributions of $Z$ among different $X$ groups may  significantly differ. In other words, this leads to a distribution shift problem as the model is only trained on factual data $P(X=x,Z)$ but tested on counterfactual data $P(X=x',Z)$. Second, it only fits a single quantile regression model for $q(X, Z;\tau)$, assuming that the conditional quantile functions $q_0(Z; \tau)$ and $q_1(Z; \tau)$ share the same model \rc{this claim seems not 100\% correct, as $q_0(Z;\tau)=q(Z,X=0;\tau) \ne q_1(Z;\tau)=q(Z,X=1;\tau)$, but the influence of $X$ on the output of $q$ can be limited as it is one-dimensional (the same as S-learner).}. This lack of flexibility may hinder 
 % the adaptation to diverse function forms for $q_0(Z; \tau)$ and $q_1(Z; \tau)$. 
 %  Third,  
 %  when directly modeling  $q(X, Z;\tau)$,  
 %  if the dimension of feature $Z$ is slightly high, then the signal of $X$ compared to $Z$ is low. Consequently, the role of $X$ may be  underestimated as its influence is partially masked by $Z$.}  
%  This lack of flexibility in 
% adapting to different function forms of $q_0(Z; \tau)$ and $q_1(Z; \tau)$.
% Proposition \ref{prop4}(ii) suggests the inherent bias in the method proposed by \citet{Xie-etal2023-attribution}. This finding is intuitive, considering that the distribution of $\P(X, Y, Z)$ generally differs from $\P(Y_x, X, Z)$ due to the prevalent selection bias in observational data. 

% we propose to estimate 
%  by employing the inverse propensity weighting method
Inspired by \citet{Firpo2007}, a simple improvement is to estimate $q_x(z;\tau)$ and $q_{x'}(z;\tau)$ separately. For example, for estimating $q_x(z;\tau)$, the associated loss function is given as 
 	\begin{align*}
	        R_x(f, \tau) = \frac 1  N \sum_{k=1}^N  \frac{\mathbb{I}(x_k=x) \cdot l_{\tau}(y_k - f(z_k) ) }{\hat p_x(z_k)},
	\end{align*} 
 where $p_x(z)= \P(X=x| Z=z)$ is the propensity score, $\hat p_x(z)$ is its estimate.
 Likewise, we could define $R_{x'}(f, \tau)$ by replacing $x$ with $x'$.   
 Then the estimation procedure for $y_{x'}$ involves four steps: (1) estimating $p_x(z)$; 
 % the propensity score; 
 (2) estimating $q_x(z; \tau)$ by minimizing $R_x(f, \tau)$ for a range of candidate values of $\tau$;  
(3) identifying the $\tau^*$ in the candidate set of $\tau$, that corresponds to the quantile of $y$ in the distribution $\P(Y |X = x, Z=z)$; (4) estimating $y_{x'}$ using $q_{x'}(z; \tau^*)$, where $q_{x'}(z; \tau^*)$ is obtained by minimizing $R_{x'}(f, \tau^*)$.  
% not requiring prior estimation of the SCM and 
Despite this four-step estimation method that allows $q_x(Z;\tau)$ and $q_{x'}(Z; \tau)$ to come from different models, it still needs to estimate a different $\tau^*$ for each individual. % and is cumbersome. 


\subsection{Enhanced Counterfactual Learning Method} 

\begin{table*}[t]
\centering
\caption{$\sqrt{\epsilon_{\text{PEHE}}}$ of individual treatment effect estimation on the simulated Sim-$m$ dataset, where $m$ is the dimension of $Z$.}
%\vspace{-6pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|ll|ll|ll|ll}
\toprule
          & \multicolumn{2}{c|}{Sim-5}                                                                                & \multicolumn{2}{c|}{Sim-10}   
          & \multicolumn{2}{c|}{Sim-20}          
          & \multicolumn{2}{c}{Sim-40}
          
          % \\ \cmidrule{2-9} 
              \\ \midrule
Methods   & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c}{Out-sample} \\
\cmidrule{1-9}
T-learner & 2.95 $\pm$ 0.02                & 2.66 $\pm$ 0.01               &  2.99 $\pm$ 0.01                & 3.17 $\pm$ 0.01                &  3.36 $\pm$ 0.02                        & 3.19 $\pm$ 0.03                        & 5.12 $\pm$ 0.02                        & 4.74 $\pm$ 0.04                       \\
X-learner & 2.94 $\pm$ 0.01                & 2.66 $\pm$ 0.01               & 2.98 $\pm$ 0.02               & 3.19 $\pm$ 0.02               &  3.31 $\pm$ 0.02                        &  3.21 $\pm$ 0.02                       &  5.08 $\pm$ 0.04                       & 4.77 $\pm$ 0.03                        \\
BNN       &  2.91 $\pm$ 0.08               & 2.64 $\pm$ 0.07           & 2.90 $\pm$ 0.11               & 3.08 $\pm$ 0.12   &  3.21 $\pm$ 0.13                        & 3.13 $\pm$ 0.16                        & 4.81 $\pm$ 0.10                         & 4.54 $\pm$ 0.09                        \\
TARNet    & 2.89 $\pm$ 0.07               & 2.64 $\pm$ 0.06            & 2.94 $\pm$ 0.07              & 3.16 $\pm$ 0.08                & 3.18 $\pm$ 0.07                         & 3.11 $\pm$ 0.07                        &  4.82 $\pm$ 0.07                       &  4.56 $\pm$ 0.07                       \\
CFRNet    & 2.88 $\pm$ 0.07                &  2.62 $\pm$ 0.06               & 2.94 $\pm$ 0.07                &  3.15 $\pm$ 0.08                & 3.15 $\pm$ 0.07                         & 3.08 $\pm$ 0.07                        & 4.71 $\pm$ 0.12                         & 4.45 $\pm$ 0.11                        \\
CEVAE    & 2.92 $\pm$ 0.27               & 2.65 $\pm$ 0.21            & 3.04 $\pm$ 0.27              & 3.11 $\pm$ 0.18                & 3.16 $\pm$ 0.17                         & 3.11 $\pm$ 0.17                        &  4.88 $\pm$ 0.23                       &  4.53 $\pm$ 0.20                         \\

DragonNet & 2.90 $\pm$ 0.08                & 2.63 $\pm$ 0.08               &  3.02 $\pm$ 0.07               & 3.25 $\pm$ 0.08                &  3.16 $\pm$ 0.11                        & 3.09 $\pm$ 0.10                        & 4.78 $\pm$ 0.11                        & 4.50 $\pm$ 0.12                        \\
DeRCFR    & 2.88 $\pm$ 0.06                & 2.61 $\pm$ 0.06               &  2.87 $\pm$ 0.05               & 3.07 $\pm$ 0.06                &  3.11 $\pm$ 0.07                        & 3.04 $\pm$ 0.06                        & 4.77 $\pm$ 0.11                         & 4.50 $\pm$ 0.10                        \\
DESCN     & 2.93 $\pm$ 0.11                & 2.66 $\pm$ 0.09               & 3.27 $\pm$ 0.81                & 3.46 $\pm$ 0.79                & 3.12 $\pm$ 0.20                         & 3.06 $\pm$ 0.20                        &  4.91 $\pm$ 0.37                        & 4.59 $\pm$ 0.35                        \\
ESCFR     & 2.87 $\pm$ 0.08               &  2.62 $\pm$ 0.07               & 2.94 $\pm$ 0.08                & 3.15 $\pm$ 0.09               & 3.03 $\pm$ 0.09                         & 3.06 $\pm$ 0.09                       & 4.71 $\pm$ 0.15                         &  4.43 $\pm$ 0.15                       \\
CFQP & 2.91 $\pm$ 0.09	& 2.67 $\pm$ 0.11	& 3.14 $\pm$ 0.30	& 3.40 $\pm$ 0.37 & 3.21 $\pm$ 0.12 &	3.18 $\pm$ 0.11 	& 4.93 $\pm$ 0.14	& 4.55 $\pm$ 0.13 \\
Quantile-Reg     & 2.80 $\pm$ 0.06               &  2.54 $\pm$ 0.05               & 2.78 $\pm$ 0.08                & 3.05 $\pm$ 0.09               & 2.92 $\pm$ 0.07                         & 3.01 $\pm$ 0.08                       & 4.39 $\pm$ 0.13                         &  4.12 $\pm$ 0.10                       \\
Ours      & \textbf{{2.45 $\pm$ 0.17}}                & \textbf{{2.28 $\pm$ 0.23}}               & \textbf{2.25 $\pm$ 0.07}                & \textbf{2.33 $\pm$ 0.07}                & \textbf{2.51 $\pm$ 0.07}                         &  \textbf{2.46 $\pm$ 0.06}                       &  \textbf{{3.74 $\pm$ 0.26}}                       & \textbf{{3.66 $\pm$ 0.21}}                        \\ \bottomrule
\end{tabular}}
\label{tab:sim}
% \vspace{-8pt}
\end{table*}




To address the limitations  mentioned above 
in directly applying quantile regression and improve estimation accuracy, we propose a novel loss that produces an unbiased estimator of $y_{x'}$ for the individual with evidence $(X=x, Z=z, Y=y)$. % function 
% It is noteworthy that each individual corresponds to a different $\tau^*$, and  
% we need to perform the four-step estimation procedure for each individual. 
% the computational burden is huge if we  perform the four-step estimation procedure for each individual, especially with a large sample size. 
% To address this problem, we propose a novel loss that directly achieves unbiased estimators of counterfactual outcomes $Y_{1}$ for all individuals. 
  % Define \rc{should $Y$ also be an input for the function $R_{x'}$? what is the meaning of $t$ here?}
 The proposed ideal loss is constructed as 
 \begin{align*}
     R_{x'}(t| x, z, & y) ={} \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ] \\
     +{}&  \bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t,
 \end{align*}  
 which is a function of $t$ and the expectation operator is taken on the random variable of $(Y_x, Y_{x'})$ given $Z=z$. 
% he expectation operator is solved X, 
The proposed estimation method is based on Theorem \ref{thm-1}. 
% \col{extend to continuous treatment.} 
\begin{theorem}[Validity of the Proposed Ideal Loss] \label{thm-1}
%\col{If the probability density function of $Y$ given $Z$ is continuous}, then t
The loss $R_{x'}(t| x, z, y)$ is convex with respect to $t$ and is minimized uniquely at $t^*$, where $t^*$ is the solution satisfying % of % 
       $$\P(Y_{x'} \leq t^* | Z=z) = \P(Y_x \leq y| Z=z).$$
% i.e., $t^*$ is the solution of $y_{x'}$. 
\end{theorem}

Theorem \ref{thm-1} (see Appendix \ref{app-b} for proofs) implies that given the evidence $(X=x, Z=z, Y=y)$ for an individual, the counterfactual outcome $y_{x'}$
% (a realization of $Y_{x'}$ for this individual) 
satisfies $y_{x'} = \arg \min_{t} R_{x'}(t| x, z, y)$ under Assumption \ref{assump4}. \emph{{\bf Importantly}, the loss $R_{x'}(t| x, z, y)$  neither
estimates the SCM a priori, nor restricts $q_x(z; \tau)$ and $q_{x'}(z; \tau)$ stem from the same model, and it does not need to estimate a different quantile value for each individual explicitly.}

To optimize the ideal loss $R_{x'}(t; x, z, y)$, we first need to estimate it, which presents two significant challenges: (1) $R_{x'}(t| x, z, y)$ involves both $Y_x$ and $Y_{x'}$, but for each unit, we only observe one of them; (2) The terms $ \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ]$ and  $\bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ]$ in  $R_{x'}(t| x, z, y)$ is conditioned on $Z=z$, and when $Z$ is a continuous variable with infinite possible values, it cannot be estimated by simply splitting the data based on $Z$. 
% We leverage the inverse propensity score and kernel smoothing techniques to overcome these two challenges.  
We employ inverse propensity score and kernel smoothing techniques to overcome these two challenges. 
%o address these challenges, we propose a novel surrogate los
% To optimize the ideal loss $R_{x'}(t; x, z, y)$, we need to estimate it first, which presents two significant challenges:  
%  (1) $R_{x'}(t| x, z, y)$ involves both $Y_x$ and $Y_{x'}$, and we only observe one of them for each unit; (2)  $R_{x'}(t| x, z, y)$ is defined conditional on $Z=z$, when $Z$ is continuous variable taking infinite values, it cannot be estimated by simply splitting the data with $Z$.   
% \col{briefly talk about the difficulty of estimating the ideal loss here?}
 Specifically, we propose a kernel-smoothing-based estimator for the ideal loss, which is given as 
  \begin{align*}
        \hat R_{x'}(t| x, &z, y) ={} \frac{ \sum_{k=1}^N K_h(z_k -z) \frac{\mathbb{I}(x_k=x')}{\hat p_{x'}(z_k)} | y_k - t |  }{ \sum_{k=1}^N K_h(z_k -z) }  \\
        +{}& \frac{ \sum_{k=1}^N K_h(z_k -z) \frac{\mathbb{I}(x_k=x)}{\hat p_{x}(z_k)} \cdot \text{sign}(y_k - y) }{  \sum_{k=1}^N K_h(z_k -z) }  \cdot t,
  \end{align*}
 where $h$ is a bandwidth/smoothing parameter, $K_h(u) = K(u/h)/h$, and $K(\cdot)$ is a symmetric kernel function~\citep{Fan-Gijbels1996, Li-Racine-2007} that satisfies $\int K(u)du = 1$ and $\int u K(u)du =1$, such as Epanechnikov kernel $K(u) = 3(1-u^2) \cdot \mathbb{I}(|u|\leq 1)/4$  
 and Gaussian kernel $K(u)= \exp(-u^2/2)/\sqrt{2 \pi}$ for $u\in \mathbb{R}$. Then we can estimate $y_{x'}$ by minimizing $\hat R_{x'}(t; x, z, y)$ directly. 
 % can be estimated 


% and $N h \to \infty$ 
\begin{proposition}[Consistency] \label{prop5} If $h \to 0$ as $N \to \infty$, 
 $\hat p_x(z)$ and $\hat p_{x'}(z)$ are consistent estimates of $p_x(z)$ and $p_{x'}(z)$, and the density function of $Z$ is differentiable, then  
$\hat R_{x'}(t| x, z, y)$ converges to $R_{x'}(t| x, z, y)$ 
 in probability.  
%$ \xrightarrow{\P} R_{x'}(t| x, z, y),$
%where $\xrightarrow{\P}$ means convergence in probability.  
\end{proposition}

% demonstrates the rationality of

Proposition \ref{prop5} indicates that $\hat R_{x'}(t| x, z, y)$ is a consistent 
% n asymptotically unbiased 
estimator of $R_{x'}(t| x, z, y)$, demonstrating the validity of the estimated ideal loss. The loss $\hat R_{x'}(t| x, z, y)$ is applicable only for discrete treatments due to the terms $\mathbb{I}(x_k = x')$ and $\mathbb{I}(x_k = x)$. However, it can be easily extended to continuous treatments, as detailed in Appendix \ref{app-d}. 


% Unfortunately, for counterfactual inference, we need to solve the loss $\hat R_{x'}(t| x_i, z_i, y_i)$ once for each individual $i$, which remains a large computational burden. 
% To further reduce the computational burden, we parametrize $t$ with a neural network. Assume $X\in \{0, 1\}$, then $x_i' = 1-x_i$.    
% Let $t = f(z, y; \phi)$. 

% For all individuals with $x = 0$,  
% $\phi$ is trained by minimizing
%     \[   \frac{1}{N}\sum_{i=1}^N  \hat R_{1}(t; 0, z_i, y_i).   \]
% then $f(z_i, y_i;\hat \phi)$ is the estimator of $y_{i,1}$ for all individuals with $x_i = 0$, which is the realizations of $Y_1$. 

% For all individuals with $x = 1$,  
% $\phi$ is trained by minimizing
%     \[   \frac{1}{N}\sum_{i=1}^N  \hat R_{0}(t; 1, z_i, y_i).   \]
% then $f(z_i, y_i;\hat \phi)$ is the estimator of $y_{i,0}$ for all individuals with $x_i = 1$, which is the realizations of $Y_0$. 


\subsection{Further Theoretical Analysis}
% theoretical 
We further analyze the properties of the proposed method, including the unbiasedness preservation of the ideal loss, the bias of the estimated loss $\hat R_{x'}(t| x, z, y)$ and its impact on the final estimate of the counterfactual outcome $y_{x'}$. 
%even when the propensity scores are inaccurately estimated.} 

%as well as the generalization error bound of the proposed method}. 
% the preservation of the unbiasedness of the ideal loss

% We show the bias and variance of the proposed N-IPS and N-DR estimators, which rely on a standard assumption in kernel-smoothing estimation~\citep{Hardle-2004, li2023nonparametric}.
% The following Theorem \ref{thm-bias} shows the bias of the proposed N-IPS and N-DR estimators.  
% \begin{assumption}[Regularity Conditions for Kernel Smoothing]   \label{assumption_4}
% (a)  $h \to 0$ as $n \to \infty$; (b) $|\cD| h \to \infty$ as $|\cD| \to \infty$; (c) $p(o_{u,i}=1,  \boldsymbol{g}_{u,i} =  \boldsymbol{g} \mid x_{u,i})$ is twice differentiable with respect to $\boldsymbol{g}$. 
% \end{assumption}

First, we present the property of unbiasedness preservation. Let $R_{x'}^{\mathrm{weight}}(t| x, z, y)$ be  the weighted version of $R_{x'}(t| x, z, y)$, defined by   
%of the ideal loss. Let 
\begin{gather*}\bfE \left [ w(X, Z)   | Y_{x'} - t |  \big |  Z=z \right ]+\bfE \left [ w(X, Z)\text{sign}(Y_x - y)   \big |   Z = z\right ] \cdot t, 
\end{gather*} 
where the weight $ w(X, Z)$ is an arbitrary function of $(X, Z)$. The following Theorem \ref{thm5-4} shows that 
% the weighted loss
$R_{x'}^{\mathrm{weight}}(t| x, z, y)$ is also valid for estimating the counterfactual outcome $y_{x'}$.   
\begin{theorem}[Unbiasedness Preservation] \label{thm5-4} 
The loss $R_{x'}^{\mathrm{weight}}(t| x, z, y)$ is convex in % terms of
$t$ and is minimized uniquely at $t^*$ that satisfies 
%$t^*$ satisfies 
       $\P(Y_{x'} \leq t^* | Z=z) = \P(Y_x \leq y| Z=z).$
\end{theorem}

%\bcol{Although $R_{x'}^{\text{weight}}(t| x, z, y)$ and $R_{x'}(t| x, z, y)$ may differ, Theorem \ref{thm5-4} (see Appendix \ref{app-b} for proofs) shows that they share the same unique minimizer, i.e., both are valid for counterfactual estimation.}
From Theorem \ref{thm5-4} (see Appendix \ref{app-b} for proofs), if we set 
  $w(X, Z) = K_h(Z_k -z)/\{ \sum_{k=1}^N K_h(Z_k -z)/N\}$, 
 then the unbiasedness preservation property implicitly indicates that the proposed method is less sensitive to the choice of the kernel function. 
 % and its associated bandwidth. 
 %then the unbiasedness preservation property implies that the proposed method exhibits reduced sensitivity to the choice of the kernel function and its associated bandwidth.
 
%This property of $R_{x'}^{\text{weight}}(t| x, z, y)$ is useful for analyzing the bias of final estimate of $y_{x'}$ induced by the bias of $\hat R_{x'}(t| x, z, y)$, see Theorem \ref{thm5-6} for details.} 
% thereby making both 
% unbiasedness preservation  helpful to

%  
% To quantify the influence of the estimation of propensity scores, we $\hat R_{x'}(t| x, z, y)$. 
% the proposed estimator


Then, we show the bias of the estimated loss $\hat R_{x'}(t| x, z, y)$.
\begin{proposition}[Bias of the Estimated Loss] \label{prop5-5} 
If $h \to 0$ as $N \to \infty$,  $p_x(z)/\hat p_x(z)$ and $p_{x'}(z)/\hat p_{x'}(z)$ are differentiable with respect to $z$, and the density function of $Z$ is differentiable, then the bias of $\hat R_{x'}(t| x, z, y)$, defined by $\bfE[ \hat R_{x'}(t|x,z,y) ] - R_{x'}(t|x,z,y)$, is given as 
    \begin{align*}
        \text{Bias}(&\hat R_{x'})  ={}
     \delta_{p_{x'}}  \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ]  \\
     +{}& \delta_{p_{x}}  \bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t + O(h^2), 
    \end{align*} where $ \delta_{p_{x'}}  = (p_{x'}(z) - \hat p_{x'}(z))/\hat p_{x'}(z)$ and $\delta_{p_{x}}  = (p_{x}(z) - \hat p_{x}(z))/\hat p_{x}(z)$ are estimation errors of propensity scores.      
%where $\mu_2 = \int K(t) t^2 dt$. 
\end{proposition}


%   $\delta_{p_{x'}} $ and $\delta_{p_{x}}$
From Proposition \ref{prop5-5}, the bias of $\hat R_{x'}(t| x, z, y)$ consists of two components. The first is the estimation error of propensity scores. The second component arises from the kernel smoothing technique and is of order $h^2$. 
%It vanishes if the bias is defined as $\bfE[ \hat R_{x'}(t|x,z,y) ] - R_{x'}^\mathrm{weight}(t|x,z,y)$ for appropriate weight by Theorem \ref{thm5-4}.
In addition, when $\hat p_x(z)$ and $\hat p_{x'}(z)$ are consistent estimators of $p_x(z)$ and $p_{x'}(z)$ (a weak condition), the bias converges to zero, and Proposition \ref{prop5-5} simplifies to Proposition \ref{prop5}. 


%\bcol{Note that our goal is to estimate  $y_{x'}$, it is crucial to explore how the estimated  $y_{x'}$, denoted as $\hat y_{x'}$, obtained by minimizing$\hat R_{x'}(t| x, z, y)$, is affected by the bias of $\hat R_{x'}(t| x, z, y)$.} 
Finally, we examine how the estimated $y_{x'}$, denoted as $\hat y_{x'} \triangleq \arg \min_t \hat R_{x'}(t| x, z, y)$, is influenced by the bias in the estimated loss.
%bias of $\hat R_{x'}(t| x, z, y)$.


%  at a rate of  $N^{-1/2}$
\begin{theorem}[Bias of the Estimated Counterfactual Outcome] \label{thm5-6} Under the same conditions as in Proposition \ref{prop5-5}, $\hat y_{x'}$ converges to $\bar y_{x'}$  in probability, where $\bar y_{x'}$ satisfies that 
    \begin{gather*}  \P( Y_{x'} \leq \bar y_{x}|Z=z ) = \frac{ (2+2\delta_{p_{x}}) \P(Y_{x}\leq y| Z=z) + (\delta_{p_{x'}}- \delta_{p_{x}})  }{ 2 + 2\delta_{p_{x'}}  },
    \end{gather*}
 where $\bar y_{x'}$ may not equal to the true value $y_{x'}$ and their difference is the bias.    
\end{theorem}
% \begin{proof}[Proof of Theorem \ref{thm5-6}]
% \end{proof}

% shows that $\hat y_{x'}$ converges to $\bar y_{x'}$ at a rate of  $N^{-1/2}$, which is caused by the approximation of the population mean using the sample mean. Additionally,
%Theorem \ref{thm5-6} presents the bias of $\hat y_{x'}$. 
From Theorem \ref{thm5-6}, one can see that the bias of $\hat y_{x'}$ mainly depends on the estimation error of propensity scores. When $\delta_{p_{x}} = \delta_{p_{x'}}$, the equation in Theorem \ref{thm5-6} reduces to equation (\ref{eq4}), and thus $\bar y_{x'} = y_{x'}$, i.e., no bias. 
% $\bar y_{x'}$ may not equal to the true value $y_{x'}$ and their difference is the bias that       
Typically, both  $\delta_{p_{x}}$ and $\delta_{p_{x'}}$  are small due to the consistency of estimated propensity scores (a weak condition), and thus $\bar y_{x'}$ will close to $ y_{x'}$. 
%Moreover, the right-hand side of Equation (\ref{eq3}) is independent of the bandwidth $h$, indicating that the bias of the estimated counterfactual outcome is ignorable and thus is less influenced by the kernel smoothing technique.} 
% not influenced by the kernel smoothing technique. This arises from the unbiasedness preservation property established in Theorem \ref{thm5-4}.}  





% ======================================================================

\section{Experiments}


\begin{table*}[]
\centering
\caption{$\sqrt{\epsilon_{\text{PEHE}}}$ of individual treatment effect estimation on the simulated Sim-$m$ dataset, where $m$ is the dimension of $Z$.}
% \vspace{-6pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|cc|cc|cc|cc}
\toprule  & \multicolumn{2}{c|}{Sim-80 ($\rho=0.3$)}   & \multicolumn{2}{c|}{Sim-80 ($\rho=0.5$)}  & \multicolumn{2}{c|}{Sim-40 ($\rho=0.3$)}   & \multicolumn{2}{c}{Sim-40 ($\rho=0.5$)}
% \\ \cmidrule{2-9} 
\\ \midrule
Methods   & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c}{Out-sample} \\
\cmidrule{1-9}
TARNet     & 12.63 $\pm$  0.93 & 12.51 $\pm$  0.90 & 12.35 $\pm$  1.24 & 12.68 $\pm$  1.51 & 8.91 $\pm$  0.97 & 8.78 $\pm$  0.74 & 8.76 $\pm$  0.76 & 8.51 $\pm$  0.68 \\
DragonNet  & 12.50 $\pm$  0.75 & 12.36 $\pm$  0.80 & 12.71 $\pm$  1.29 & 13.02 $\pm$  1.54 & 8.83 $\pm$  0.90 & 8.73 $\pm$  0.72 & 8.62 $\pm$  0.70 & 8.39 $\pm$  0.53 \\
ESCFR     & 12.61 $\pm$  1.09 & 12.53 $\pm$  1.09 & 12.56 $\pm$  1.36 & 12.87 $\pm$  1.64 & 8.76 $\pm$  1.03 & 8.65 $\pm$  0.79 & 8.76 $\pm$  0.78 & 8.50 $\pm$  0.48 \\
X\_learner & 12.82 $\pm$  0.91 & 12.68 $\pm$  0.95 & 12.74 $\pm$  1.22 & 12.99 $\pm$  1.43 & 8.97 $\pm$  0.87 & 8.81 $\pm$  0.64 & 8.91 $\pm$  0.75 & 8.61 $\pm$  0.58 \\
Quantile-Reg & 11.59 $\pm$  0.94 & 11.57 $\pm$  0.97 & 11.59 $\pm$  1.26 & 11.91 $\pm$  1.47 & 8.05 $\pm$  0.73 & 8.08 $\pm$  0.75 & 7.74 $\pm$  0.73 & 7.58 $\pm$  0.73 \\
Ours       & \textbf{9.28 $\pm$  0.72}  & \textbf{9.28 $\pm$  0.72}  & \textbf{9.03 $\pm$  1.09}  & \textbf{9.27 $\pm$  0.97}  & \textbf{7.07 $\pm$  0.39} & \textbf{7.05 $\pm$  0.41} & \textbf{7.07 $\pm$  1.23} & \textbf{6.98 $\pm$  1.08} \\
\bottomrule
\end{tabular}}
\vspace{-12pt}
\label{tab:sim_cov}
\end{table*}




\begin{figure*}[t]
\centering
% \subfloat[Reletive error in the user-specific bias scenario]{
\subfloat[Sim-10 (In-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=1\textwidth]{figs/10in_ratio.png}
\end{minipage}}%
% }%
\subfloat[Sim-10 (Out-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=0.93\textwidth]{figs/10out_ratio.png}
\end{minipage}}%
% }%
% \vspace{3pt}
% \subfloat[MNAR]{
\subfloat[Sim-40 (In-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=0.95\textwidth]{figs/40in_ratio.png}
\end{minipage}}%
% }%
% \subfloat[Proposed Method]{
\subfloat[Sim-40 (Out-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=0.95\textwidth]{figs/40out_ratio.png}
\end{minipage}}%
% }%
\centering
\caption{Estimation performance of individual treatment effects under varying heterogeneity degrees.}
\label{fig:ratio}
\vspace{-12pt}
\end{figure*}
% Estimation Performance of Individual Treatment Effects under Varying Heterogeneity Degrees

\begin{figure*}[t]
\centering
% \subfloat[Reletive error in the user-specific bias scenario]{
\subfloat[Sim-10 (In-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=1\textwidth]{figs/10in_kernel.png}
\end{minipage}}%
% }%
\subfloat[Sim-10 (Out-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=0.93\textwidth]{figs/10out_kernel.png}
\end{minipage}}%
% }%
% \vspace{3pt}
% \subfloat[MNAR]{
\subfloat[Sim-40 (In-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=0.93\textwidth]{figs/40in_kernel.png}
\end{minipage}}%
% }%
% \subfloat[Proposed Method]{
\subfloat[Sim-40 (Out-sample)]{
\begin{minipage}[t]{0.245\linewidth}
\centering
\includegraphics[width=0.93\textwidth]{figs/40out_kernel.png}
\end{minipage}}%
% }%
\centering
\vspace{-3pt}
\caption{The estimation performance with different kernels and bandwidths.} % of individual treatment effect
\label{fig:kernel}
\vspace{-6pt}
\end{figure*}



\begin{table*}[t]
\centering
\caption{The experiment results on the \textsc{IHDP} dataset and \textsc{Jobs} dataset. The best result is bolded.}
\vspace{-6pt}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|llll|llll}
\toprule
          & \multicolumn{4}{c|}{\textsc{IHDP}}                                                                                & \multicolumn{4}{c}{\textsc{Jobs}}                                                                                \\ \cmidrule{2-9} 
          & \multicolumn{2}{c}{In-sample}                  & \multicolumn{2}{c|}{Out-sample}                  & \multicolumn{2}{c}{In-sample}                  & \multicolumn{2}{c}{Out-sample}                  \\ \midrule
Methods   & \multicolumn{1}{c}{$\sqrt{\epsilon_{\text{PEHE}}}$} & \multicolumn{1}{c}{$\epsilon_{\text{ATE}}$} & \multicolumn{1}{c}{$\sqrt{\epsilon_{\text{PEHE}}}$} & \multicolumn{1}{c|}{$\epsilon_{\text{ATE}}$} & \multicolumn{1}{c}{$R_{\text{Pol}}$} & \multicolumn{1}{c}{$\epsilon_{\text{ATT}}$} & \multicolumn{1}{c}{$R_{\text{Pol}}$} & \multicolumn{1}{c}{$\epsilon_{\text{ATT}}$} \\
\cmidrule{1-9}
T-learner & 1.49 $\pm$ 0.03                & 0.37 $\pm$ 0.05               & 1.81 $\pm$ 0.04                & 0.49 $\pm$ 0.04                & 0.31 $\pm$ 0.06                         & 0.16 $\pm$ 0.10                        &  0.27 $\pm$ 0.08                        & 0.20 $\pm$ 0.07                        \\
X-learner & 1.50 $\pm$ 0.02                & 0.21 $\pm$ 0.05               & 1.73 $\pm$ 0.03                & 0.36 $\pm$ 0.07                & 0.16 $\pm$ 0.04                         & 0.07 $\pm$ 0.05                        & 0.16 $\pm$ 0.03                         & 0.10 $\pm$ 0.09                        \\
BNN       & 2.09 $\pm$ 0.16                & 1.00 $\pm$ 0.23               & 2.37 $\pm$ 0.15                & 1.18 $\pm$ 0.19                & 0.15 $\pm$ 0.01                         &  0.08 $\pm$ 0.03                        & 0.16 $\pm$ 0.02                         & 0.13 $\pm$ 0.07                        \\
TARNet    & 1.52 $\pm$ 0.07                & 0.22 $\pm$ 0.13               & 1.78 $\pm$ 0.07                & 0.34 $\pm$ 0.18                & 0.17 $\pm$ 0.06                         & 0.06 $\pm$ 0.08                        & 0.18 $\pm$ 0.09                         & 0.10 $\pm$ 0.06                        \\
CFRNet    & 1.46 $\pm$ 0.06                & 0.17 $\pm$ 0.15               & 1.77 $\pm$ 0.06                & 0.32 $\pm$ 0.20                & 0.17 $\pm$ 0.03                         & \textbf{0.05 $\pm$ 0.03}                        & 0.19 $\pm$ 0.07                         & 0.10 $\pm$ 0.04                        \\
CEVAE    & 4.08 $\pm$ 0.88                & 3.67 $\pm$ 1.23               & 4.12 $\pm$ 0.91                & 3.75 $\pm$ 1.23                & 0.18 $\pm$ 0.05                         & 0.09 $\pm$ 0.03                        & 0.22 $\pm$ 0.08                        & 0.10 $\pm$ 0.09                        \\

DragonNet & 1.49 $\pm$ 0.08                & 0.22 $\pm$ 0.14               & 1.80 $\pm$ 0.06                & 0.29 $\pm$ 0.19                & 0.17 $\pm$ 0.06                         & 0.07 $\pm$ 0.07                        & 0.20 $\pm$ 0.08                         &  0.11 $\pm$ 0.09                       \\
DeRCFR    & 1.48 $\pm$ 0.06                & 0.25 $\pm$ 0.14               & 1.69 $\pm$ 0.06                & 0.25 $\pm$ 0.14                & 0.15 $\pm$ 0.02                         & 0.14 $\pm$ 0.04                        & 0.16 $\pm$ 0.04                         & 0.15 $\pm$ 0.11                        \\
DESCN     & 2.08 $\pm$ 0.98                & 0.74 $\pm$ 1.00               & 2.67 $\pm$ 1.45                & 1.04 $\pm$ 1.46                & 0.15 $\pm$ 0.02                         & 0.21 $\pm$ 0.14                        & 0.22 $\pm$ 0.16                         & 0.16 $\pm$ 0.04                        \\
ESCFR     & 1.46 $\pm$ 0.09                & 0.16 $\pm$ 0.16               & 1.73 $\pm$ 0.08                & 0.27 $\pm$ 0.16                & 0.14 $\pm$ 0.02                         & 0.10 $\pm$ 0.03                        & 0.15 $\pm$ 0.02                         & 0.10 $\pm$ 0.08                        \\
Quantile-Reg & 1.43 $\pm$ 0.05	& 0.14 $\pm$ 0.09	& 1.56 $\pm$ 0.03	& 0.18 $\pm$ 0.09 & 0.14 $\pm$ 0.01&	 0.06 $\pm$ 0.01	& 0.15 $\pm$ 0.01	& 0.07 $\pm$ 0.04 \\

CFQP & 1.47 $\pm$ 0.10	& 0.18 $\pm$ 0.17	& 1.48 $\pm$ 0.05	& 0.15 $\pm$ 0.08 & 0.15 $\pm$ 0.02 &	0.23 $\pm$ 0.15	& 0.16 $\pm$ 0.03	& 0.15 $\pm$ 0.07 \\

Ours      & \textbf{1.41 $\pm$ 0.02}                & \textbf{0.11 $\pm$ 0.10}               & \textbf{1.50 $\pm$ 0.06}                & \textbf{0.13 $\pm$ 0.08}                &  \textbf{0.08 $\pm$ 0.04}                         & 0.06 $\pm$ 0.02                        & \textbf{0.11 $\pm$ 0.05}                         & \textbf{0.05 $\pm$ 0.05}                        \\ 

\bottomrule
\end{tabular}}
\label{tab:real}
\vspace{-6pt}
\end{table*}

\subsection{Synthetic Experiment}
{\bf Simulation Process.} We generate the synthetic dataset by the following process. First, we sample the covariate $Z \sim \mathcal{N}\left(0, I_{m}\right)$ and the treatment $X \sim \operatorname{Bern}(\pi(Z))$, where $\operatorname{Bern}(\cdot)$ is the Bernoulli distribution with probability $ \pi(Z) = \mathbb{P}(X = 1 \mid Z) = \sigma(W_x \cdot Z)$, $\sigma(\cdot)$ is the sigmoid function, and $W_x \sim \operatorname{Unif}(-1, 1)^{m}$, $\operatorname{Unif}(\cdot)$ is the uniform distribution. Then, we sample the noise $U_0 \sim \mathcal{N}\left(0, 1\right)$ and $U_1 = \alpha \cdot U_0$ to consider the heterogeneity of the exogenous variables, where $\alpha$ is the hyper-parameter to control the heterogeneity degree. Finally, we simulate $ Y_1 = W_y \cdot Z + U_{1}$ and $Y_0 =  W_y \cdot Z/\alpha + U_{0}$ with $W_y \sim \mathcal{N}(0,I_{m})$. We generate 10,000 samples with 63/27/10 train/validation/test split and vary $m \in \{5, 10, 20, 40\}$ in our synthetic experiment.

{\bf Baselines and Evaluation Metrics.} 
The competing baselines includes: % We compare our method with the following baselines:
T-learner~\citep{kunzel2019metalearners}, X-learner~\citep{kunzel2019metalearners}, BNN~\citep{johansson2016learning}, TARNet~\citep{shalit2017estimating}, CFRNet~\citep{shalit2017estimating}, CEVAE~\citep{louizos2017causal}, DragonNet~\citep{shi2019adapting}, DeRCFR~\citep{wu2022learning}, DESCN~\citep{zhong2022descn}, ESCFR~\citep{wang2023optimal}, CFQP~\citep{Brouwer2022},
and Quantile-Reg~\citep{Xie-etal2023-attribution}. %Following the previous studies~\citep{shalit2017estimating, yao2018representation}, 
We evaluate the individual treatment effect estimation using the \emph{individual level Precision in Estimation
of Heterogeneous Effects} (PEHE): $$\epsilon_{\text{PEHE}} =
{\frac{1}{N} \sum_{i=1}^N [(\hat {Y}_i(1) - \hat {Y}_i(0)) - (Y_i(1) - Y_i(0))]^2},$$
where $\hat {Y}_i(1)$ and $\hat {Y}_i(0)$ are the predicted values for the corresponding true potential outcomes of unit $i$.  
 It is noteworthy that $\epsilon_{\text{PEHE}}$ is tailored for individual-level evaluation and counterfactual estimation, which is different from the common metric~\citep{shalit2017estimating} given by  
     $\frac{1}{N} \sum_{i=1}^N[  (\hat \mu_1(X_i) - \hat \mu_0(X_i)) - ( \mu_1(X_i) -  \mu_0(X_i)  )]^2,$
where  $\mu_1(X_i) -  \mu_0(X_i) := \bfE[ Y(1)|X ]- \bfE[ Y(0)|X ]$ are the true CATE, and $\hat \mu_1(X_i) - \hat \mu_0(X_i)$ is its estimate. 
Both in-sample and out-of-sample performances are reported in our experiments. For more implementation details of the proposed method, please refer to 
 Appendix \ref{app-e}.  
%See Appendix \ref{app-e} for more  implementation details. 
%are the predicted value for the corresponding true potential outcome. Both in-sample and out-of-sample performance are reported in our experiments.


{\bf Performance Analysis.} The results of estimation performance are shown in Table \ref{tab:sim}. Our method stably outperforms all baselines with varying covariate dimensions $m$, demonstrating the effectiveness of the proposed method. In addition, we investigate our method performance with violated assumptions on rank and uncorrelated covariates. Specifically, we modified the data generation process to explore the performance of our method under correlated covariates. Specifically, we sample the covariate $Z \sim \mathcal{N}\left(0, \Sigma_{m}\right)$, where the $\rho_{ij}$ in $\Sigma_m$ is $\max(0.01, \rho^{|i-j|})$. The results are shown in Table \ref{tab:sim_cov}. The results show that our method still outperforms the baseline methods. See Appendix \ref{app-e} for the results with rank assumption violated. Moreover, we further explore the effect of heterogeneity degrees on the performance of the proposed method, as shown in Figure \ref{fig:ratio}, from which one can see that as the heterogeneity degree increases, our method stably outperforms the Quantile-Reg in terms of PEHE. Finally, we examine the effect of different kernels and bandwidths, as shown in Figure \ref{fig:kernel}, our method stably outperforms the Quantile-Reg and ESCFR methods with different kernels and bandwidths.

 % Moreover, as shown in Figure \ref{fig:kernel}, our method stably outperforms the Quantile-Reg and ESCFR methods with different kernels and bandwidths, which further verifies the superiority of our method. 
% further validating the superiority of our approach.
% {\bf In-Depth Analysis.}
%  N 
% N
%  i=1((ˆ yi(1)− ˆ yi(0))−(yi(1)−yi(0)))2 and
%  ϵATE = 1
%  N 
% N
%  i=1|(ˆ yi(1) − ˆ yi(0)) − (yi(1) − yi(0))|, where ˆ yi and yi are predicted and true outcomes.
%\vspace{-6pt}






\subsection{Real-World Experiment}





%  Prepossessing
{\bf Dataset and Preprocessing.} Following previous studies~\citet{shalit2017estimating}, \citet{louizos2017causal}, \citet{yoon2018ganite}, and \citet{yao2018representation}, we conduct experiments on semi-synthetic dataset \textsc{IHDP} and real-world dataset \textsc{Jobs}. The \textsc{IHDP} dataset~\citep{hill2011bayesian} is constructed from the Infant Health and Development Program (IHDP) with 747 individuals and 25 covariates. The \textsc{Jobs} dataset~\citep{lalonde1986evaluating} is based on the National Supported Work program with 3,212 individuals and 17 covariates. We follow \citet{shalit2017estimating} to split the data into training/validation/testing set with ratios 63/27/10 and 56/24/20 with 100 and 10 repeated times on the \textsc{IHDP} and the \textsc{Jobs} datasets, respectively.

% Following the previous studies~\cite{shalit2017estimating, louizos2017causal, yao2018representation},  

%\vspace{-1.95pt}
{\bf Evaluation Metrics.} Following previous studies~\citep{shalit2017estimating, louizos2017causal, yao2018representation}, besides $\epsilon_{\text{PEHE}}$, we also use the absolute error in \emph{Average Treatment Effect} (ATE) for evaluation, which is defined as $\epsilon_{\text{ATE}}=\frac{1}{N}| \sum_{i=1}^N ((\hat {Y}_i(1) - \hat {Y}_i(0)) - (Y_i(1) - Y_i(0)))|.$
%and
We use $\sqrt{\epsilon_{\text{PEHE}}}$ and $\epsilon_{\text{ATE}}$ to evaluate performance on the \textsc{IHDP} dataset. For the \textsc{Jobs} dataset, since one of the potential outcomes is not available, we evaluate the performance using the absolute error in \emph{Average Treatment effect on the Treated} (ATT) as $\epsilon_{\text{ATT}} = |\text{ATT} - \frac{1}{|T|} \sum_{i \in T} (\hat {Y}_i(1) - \hat {Y}_i(0)|$
with $\text{ATT} = |\frac{1}{|T|} \sum_{i \in T} {Y}_i - \frac{1}{|C \cap E|} \sum_{i \in C \cap E} {Y}_i|$. We also use the policy risk ${R}_{\text{Pol}}= 1-(\mathbb{E}[Y(1) \mid \hat {Y}(1) - \hat {Y}(0) >0, X=1] \cdot \mathbb{P}(\hat {Y}(1) - \hat {Y}(0)>0)+\mathbb{E}[Y(0) \mid \hat {Y}(1) - \hat {Y}(0) \leq 0, X=0] \cdot \mathbb{P}(\hat {Y}(1) - \hat {Y}(0) \leq 0))$, where $T, C, E$ are the indexes of  treatment sample set, control sample set, and randomized sample set, respectively. 
% \end{align*}



%\vspace{-1.95pt}
{\bf Performance Comparison.}
The experiment results are shown in Table \ref{tab:real}. Similar to the synthetic experiment, the Quantile-Reg method still achieves the most competitive performance compared to the other baselines. Our method stably outperforms all the baselines on both the semi-synthetic dataset \textsc{IHDP} and the real-world dataset \textsc{Jobs}, especially in the out-sample scenario. This provides the empirical evidence of the effectiveness of our method.
% \newpage % . Our method achieves the most competitive performance compared to the baselines 
% PEHE= 1
%  N · N
%  i=1((ˆ yi(1)−ˆ yi(0))−(yi(1)−yi(0))2
% \vspace{-5.6pt}


% Heterogeneous Treatment Effects
%\vspace{-2pt}
\section{Conclusion}
\vspace{3pt}

This work addresses the fundamental challenge of counterfactual inference in the absence of a known SCM and under heterogeneous endogenous variables. 
% Previous approaches, such as Pearl's three-step procedure and quantile regression, face limitations due to the need for prior SCM estimation or  assumptions like homogeneity and strict monotonicity. 
 % To overcome these challenges,
 We first introduce the rank preservation assumption to identify counterfactual outcomes, showing that it is slightly weaker than the homogeneity and monotonicity assumptions. 
 Then, we propose a novel ideal loss for unbiased learning of counterfactual outcomes and develop a kernel-based estimator for practical implementation. The convexity of the ideal loss and the unbiased nature of the proposed estimator contribute to the robustness and reliability of our method. 
 A potential limitation arises when the propensity score is extremely small in certain data sparsity scenarios, which may cause instability in the estimation method. Further investigation is warranted to address and overcome this challenge.  
 % Further investigation is warranted to address and overcome this challenge. Further investigation is warranted to address and overcome this challenge.  
% A potential limitation is that the rank preservation assumption may still be stringent for binary outcomes. Further investigation into the sensitivity of the proposed method is warranted for binary outcomes. 


% \section*{Software and Data}
% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

%\newpage 
\section*{Impact Statement}
This paper proposed a novel counterfactual learning approach that enhances the previous work in terms of relaxed identifiability assumptions and theoretically guaranteed estimation methods. 
 The proposed approach provides valuable tools and insights for researchers and practitioners across various disciplines, facilitating advancements in understanding causal relationships. It guides decision-making processes in complex, real-world scenarios, particularly in estimating individual treatment effects. Applications include credit assignment, root-causal analysis, algorithmic fair decision-making, policy evaluation and improvement, reinforcement learning, counterfactual explanation, counterfactual harm, classification and detection in medical imaging, among others. 







% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Related Work}
{\bf Conditional Average Treatment Effect (CATE).} 
 CATE also referred to as heterogeneous treatment effect, represents the average treatment effects on subgroups categorized by covariate values, and plays a central role in areas such as precision medicine~\cite{Kosorok+Laber:2019} and policy learning~\cite{Dudik2011-ICML}. Benefiting from recent advances in machine learning, many methods have been proposed for estimating CATE, including matching methods~\cite{rosenbaum1983central,schwab2018perfect,yao2018representation}, tree-based methods~\cite{chipman2010bart,wager2018estimation}, representation learning methods~\cite{johansson2016learning,shalit2017estimating,shi2019adapting,wu2022learning,wang2023optimal}, and generative methods~\cite{louizos2017causal,yoon2018ganite}. Unlike the existing work devoted to estimating CATE at the intervention level for subgroups, our work focuses on counterfactual inference at the more challenging and fine-grained individual level.
 % at the intervention level for subgroups
   %However, these efforts focus on identification and do not address the challenges associated with the estimation.
 % leaving the question of how to estimate counterfactuals potentially intractable. 
 % In recent years, 
 %\vskip -0.15cm
 
 
{\bf Counterfactual Inference.} Counterfactual inference involves the identification and estimation of counterfactual outcomes.
  For identification,  \citet{Shpitser2007-UAI} provided an algorithm  leveraging counterfactual graphs to identify counterfactual queries. In addition, \citet{Correa2021-NIPS} discussed the identifiability of nested counterfactuals within a given causal graph. 
 More relevant to our work, 
  \citet{Lu-etal2020-attribution} and  \citet{Xie-etal2023-attribution} studied the identifiability assumptions in the setting of backdoor criterion under homogeneity and
strict monotonicity assumptions. 
% \cite{Nasr-Esfahany-2023-attribution} extended them in the setting of instrument variables. 
% Instead of aim at identifying and estimating counterfactual outcomes directly, 
Several methods focus on determining its bounds with less stringent assumptions, such as  \citet{Bakle-1994-UAI}, \citet{Tian2000}, \citet{pearl2009causality}, \citet{Pearl-etal2016-primer}, \citet{Finkelstein-2020-UAI}, \citet{Zhang-2022-ICML}, and \citet{Melnychuk2023}.  
%Appendix \ref{app-c} provides additional literature on counterfactual inference.
% These methods include works by 
 %\vskip -0.15cm

 
 For estimation, \citet{Pearl-etal2016-primer}  introduced a three-step procedure for counterfactual inference. Many machine learning methods estimate counterfactual outcomes in this framework, such as  \citet{Lu-etal2020-attribution}, \citet{Mesnard2021-ICML}, \citet{Brouwer2022}, \citet{Shah2022}, \citet{Yan2023}, \citet{Nasr-Esfahany-2023-attribution} and \citet{Chao-etal2023}. 
 % However, this framework relies on the availability of SCM or needs to estimate SCM a priori. 
 Recently, \citet{Xie-etal2023-attribution} employed quantile regression to estimate the counterfactual outcomes,  effectively circumventing the need for SCM estimation. 
 % Nonetheless, this method relies on that the conditional quantile functions for different counterfactual outcomes originate from the same model. It also necessitates estimating a distinct quantile value for each individual, which may cause instability in optimization. 
 In our work, we extend the above methods in both identification and estimation. 


Recently, counterfactual inference methods have been extensively applied across various application scenarios,
   %extensive work has applied counterfactual inference methods to a variety of application scenarios, 
   such as counterfactual fairness~\citep{kusner2017counterfactual, Zuo2023, Anthis2023-NIPS, Kavouras2023-NIPS, Chen2023-NIPS}, policy evaluation and improvement~\citep{Tang2023-NIPS, Saveski2023-NIPS, Chen-etal2023-NIPS}, reinforcement learning~\citep{Lu-etal2020-attribution, Tsirtsis2023-NIPS, Liu2023-NIPS, Shao2023-NIPS, Meulemans-etal-NIPS23, Haugh-ICML23, Zenati-ICML23}, imitaion learning~\citep{Sun2023-NIPS}, counterfactual generation~\citep{Yan2023, Prabhu2023-NIPS, Feder2023-NIPS, Ribeiro-ICML23}, counterfactual explanation~\citep{Kenny2023-NIPS, Raman2023-NIPS, Hamman-ICML23, Wu-eatl-ICML23, Ley-ICML23}, counterfactual harm~\citep{2022counterfactualharm, li2023trustworthy}, physical audiovisual commonsense reasoning~\citep{Lv2023-NIPS}, interpretable time series prediction~\citep{Yan-etal-ICML23},  classification and detection in medical imaging~\citep{Fontanella-ICML23}, data valuation~\citep{Liu-ICML23}, etc. 
   Therefore, developing novel counterfactual inference methods holds significant practical implications. 
    % for real-world applications.  
% dispersed across different disciplines and using slightly different terminology
% We also note that 

% 计量经济学还有一些作品.


\section{Proofs in Sections \ref{sec4} and \ref{sec5}}  \label{app-a}

One can show Lemma \ref{lem1} by a similar argument of the proof of  Theorem 1 in \citet{Xie-etal2023-attribution}. 
For the sake of self-containedness, we provide a novel proof of it.   
%and concise way of proving
 
{\bf Lemma \ref{lem1}} \emph{Under Assumptions \ref{assump1}-\ref{assump2}, $y_{x'}$is identifiable.} 
\begin{proof}[Proof of Lemma \ref{lem1}]  

First, the distributions $\P(Y_x| Z=z) $ and $\P(Y_{x'}| Z=z)$ can be identified as $\P(Y | X=x, Z=z)$ and $ \P(Y | X=x', Z=z)$, respectively, by the backdoor criterion (i.e., $(Y_x, Y_{x'}) \indep X | Z$) of the setting.  
%  $\P(Y_x| Z=z) =  \P(Y | X=x, Z=z)$ and $\P(Y_{x'}| Z=z) =  \P(Y | X=x', Z=z) $, both are identifiable. 

Then, according to the model (\ref{eq1}), we can equivalently write 
	 \[ Y_x = f_Y(x, z, U_{x}), ~  Y_{x'} = f_Y(x', z, U_{x'}), \]
and $Y$ and $U_X$ in model (\ref{eq1}) can be expressed as $Y = \sum_{x\in \mathcal{X}}  \mathbb{I}(X =x) \cdot Y_x$ and $U_X = \sum_{x\in \mathcal{X}}  \mathbb{I}(X =x) \cdot U_{x}$, where $\mathcal{X}$ is the support set of $X$ and $\mathbb{I}(\cdot)$ is an indicator function.   
 Assumption \ref{assump1} implies that $U_X = U_{x} = U_{x'}$ conditional on $Z$, i.e., $Y_x = f_Y(x, z, U_{X}), ~  Y_{x'} = f_Y(x', z, U_{X})$. 
 % $\P(U_X| Z=z, X=x) = \P(U_X| Z=z, X=x')$, i.e., $\P(U_{x} | Z=z, X=x) = \P(U_{x'} | Z=z, X=x')$, which leads to $\P(U_{x} | Z=z) = \P(U_{x'} | Z=z)$ by the     backdoor criterion again.  
% That is, the conditional distribution of $U_{x}$ given $Z=z$ is the same as that of $U_{x'}$, and the quantiles of these two distributions are the same.   
 
 
% \col{implies that $U_X = U_{x} = U_{x'}$} given $Z=z$.  [\col{just implies their distributions are the same.}]
Finally, for the individual with observation $(X =x, Z=z, Y= y)$, we denote $(y_x, y_{x'})$ as the true values of  ($Y_x, Y_{x'}$) for this individual. %Clearly, $y_x = y$.  % by the consistency assumption.   
 For this individual, we can identify the quantile of $y_x$ in the distribution of $\P(Y_x | Z=z) = \P(Y | X=x, Z=z)$, denoted by $\tau^*$. Let $u_{\tau^*}$ be the true value of $U_{X}$ for this individual, it is the $\tau^*$-quantile in the distribution $\P(U_X|Z=z)$, then we have 
 % it is the  $\tau$-quantile in both the distributions $\P(U_{x} | Z=z)$ and $\P(U_{x'} | Z=z)$, then we have 
% Letting $u_{\tau,x}$ be the $\tau$-quantile of $\P(U_{x} | Z=z)$, which is also the $\tau$-quantile of $\P(U_{x'} | Z=z)$,  we have 
 %  which also is the $\tau$-quantile of $\P(U_{x'} | Z=z):= u_{\tau,x'$, and thus $y_x = f_Y(x, z, u_{\tau,x})$ and $y_{x'} = f_Y(x', z, u_{\tau, x} )$. 
      \begin{align*}
       \tau^* ={}& \P( Y_x \leq y_x  | Z=z)     \qquad \qquad \qquad \text{(by the definition of $\tau$)} \\
       	  %   ={}& \P(  f_Y(x, z, U_{x})  \leq y_x | Z=z ) \\
	     ={}& \P(  U_{x} \leq  u_{\tau}   | Z=z )  \qquad \qquad \quad  ~ \text{(by Assumption \ref{assump2})} \\
	     ={}& \P(  U_{x'} \leq  u_{\tau}   | Z=z )  \qquad \qquad ~~~   \text{(by Assumption \ref{assump1})} \\
      %\text{ by $\P(U_{x} | Z=z) = \P(U_{x'} | Z=z)$}\\
	     ={}& \P(  Y_{x'} \leq f_Y(x', z, u_{\tau^*})  | Z=z )  \quad ~ \text{(by Assumption \ref{assump2})} \\  
	     ={}& \P(  Y_{x'} \leq y_{x'}  | Z=z )    \qquad \qquad \quad  \text{(by the definition of $y_{x'}$)},
      \end{align*} 	 
% where the first equality follows from the definition of $\tau$, the second and fourth equalities follow by Assumption \ref{assump2} (i.e., the strict monotonicity of $f_Y(x, z, \cdot)$), the third equality follows by $\P(U_{x} | Z=z) = \P(U_{x'} | Z=z)$, and the fifth equality follows by the definition of $y_{x'}$.  
  which implies that  for this individual,  its rankings of $y_{x}$ and $y_{x'}$ are the same in the distributions of $\P(Y_x | Z=z)$ and $\P( Y_{x'} | Z=z )$, resepcctively.  Thus, $y_{x'}$is identified as the $\tau^*$-quantile of the distribution   $\P( Y_{x'} | Z=z ) = \P(Y| X=x', Z=z)$. 
%   $\P(Y| X=x', Z=z)$.   	 
  
 % Given the evidence $(X =x, Z=z, Y= y_x )$ for this individual, 
 % we can determine the ranking of $y_x$ in the distribution of $\P(Y_x | Z=z) = \P(Y | X=x, Z=z)$ and thereby identify the value of $y_{x'}$  from the distribution $\P( Y_{x'} | Z=z) = \P(Y| X=x', Z=z)$ using the same ranking.  
 


%Let $u_{\tau}$ be the $\tau$-quantile of the distribution $\P(U_X | Z=z)$ for any $\tau \in [0, 1]$,  
%and denote $q_{\tau}(x, z)$ and $q_{\tau}(x', z)$  as the  $\tau$-quantile of the distributions $\P(Y_x| Z=z)$ and $\P(Y_{x'}| Z=z)$, respectively. Then  Assumption \ref{assump2}, i.e., the smoothness and strict monotonicity of $f_Y(x, z, \cdot)$ implies that 
%	\begin{equation}  \label{eq-s1}  \P( Y_x \leq  q_{\tau}(x, z)  \mid Z=z ) =  \P( Y_{x'} \leq  q_{\tau}(x', z)  \mid Z=z ) = \P( U_X \leq u_{\tau} | Z=z )  = \tau.    \end{equation} 

%Note that model (\ref{eq1}) (backdoor criterion) leads to 
% $\P(Y_x| Z=z) =  \P(Y | X=x, Z=z)$ and $\P(Y_{x'}| Z=z) =  \P(Y | X=x', Z=z) $, both are identifiable.  Therefore, 
%  for the individual with   $(X = x, Z = z, Y = y)$, let $\tau$ take the quantile of $y$ in the distribution $\P(Y| X=x, Z=z)$, then $\tau$ is identifiable. According to equation (\ref{eq-s1}),  for the same individual, $\theta $ is identified as the $\tau$-quantile of  
%   $\P(Y| X=x', Z=z)$.   	 
\end{proof}



% =======================================
{\bf Proposition \ref{prop1}}  \emph{   Under Assumption \ref{assump3}, $y_{x'}$ is identified as %identifiable.  
   the $\tau^*$-th quantile of  
    $\P(Y| X=x', Z=z)$, where $\tau^*$ is the quantile of $y$ in the distribution of $\P(Y | X=x, Z=z)$.}   
% \emph{(ii) Under Assumption \ref{assump2},  Assumption \ref{assump1} implies Assumption \ref{assump3}.   }

\begin{proof}[Proof of Proposition \ref{prop1}]   
% For the individual with observation $(X =x, Z=z, Y= y)$, we denote $(y_x, y_{x'})$ as the true values of  ($Y_x, Y_{x'}$) for this individual. Clearly, $y_x = y$.  
 For the individual with observation $(X =x, Z=z, Y= y)$, we denote $(y_x, y_{x'})$ as the true values of  ($Y_x, Y_{x'}$). Assumption \ref{assump3} implies that for this individual,  
 its rankings of $y_{x}$ and $y_{x'}$ are the same in the distributions of $\P(Y_x | Z=z)$ and $\P( Y_{x'} | Z=z )$,  
 respectively. Therefore, 
       \begin{equation} 
              \P( Y_x \leq y_x  | Z=z)  =  \P(  Y_{x'} \leq y_{x'}  | Z=z ).  
       \end{equation}
 

Since $y_x = y$ is observed and the distributions $\P(Y_x| Z=z) $ and $\P(Y_{x'}| Z=z)$ can be identified as $\P(Y | X=x, Z=z)$ and $ \P(Y | X=x', Z=z)$, respectively, by the backdoor criterion (i.e., $(Y_x, Y_{x'}) \indep X | Z$),  we can identify the quantile of $y_x$ in the distribution of $\P(Y | X=x, Z=z)$, denoted by $\tau^*$.  Then
	\[     \P(  Y_{x'} \leq y_{x'}  | Z=z ) = \tau^*,  \]
which yields that $\theta $ is identified as the $\tau^*$-quantile of  
   $\P(Y| X=x', Z=z)$. %where $\tau$ is     the quantile of $y$ in the distribution $\P(Y| X=x, Z=z)$. 	 

\end{proof}



The following Proposition \ref{prop2}$^*$ serves as a complement to Proposition \ref{prop2}. 
% Specifically, we can show that if $U_x$ is a strictly monotone increasing function of $U_{x'}$, Assumption \ref{assump3} is equivalent to Assumption \ref{assump2}, see Appendix \ref{app-a} for proofs.  

{\bf Proposition \ref{prop2}$^*$}  
% \emph{Assumptions \ref{assump1}-\ref{assump2} implies Assumption \ref{assump3}; But not vice versa.}    
Under Assumption \ref{assump1}, or more generally, if $U_x$ is a strictly monotone increasing function of $U_{x'}$, Assumption \ref{assump3} is equivalent to Assumption \ref{assump2}. 


\begin{proof}[Proof of Proposition \ref{prop2}]  According to the model (\ref{eq1}), we can equivalently write 
  	 \[ Y_x = f_Y(x, z, U_{x}), ~  Y_{x'} = f_Y(x', z, U_{x'}). \]
%and $U_X$ in model (\ref{eq1}) can be expressed as $U_X =  \mathbb{I}(X=x)\cdot U_{x} +  \mathbb{I}(X=x')\cdot U_{x'}$. 
Suppose that $U_x$ is a strictly monotone increasing function of $U_{x'}$ (Assumption \ref{assump1}, i.e., $U_x = U_{x'}$, is a special case of it). Under this condition, we next prove sufficiency and necessity, respectively. 

First, we show that Assumption \ref{assump2} implies Assumption \ref{assump3}. If Assumption \ref{assump2} holds, then $Y_x$ is a strictly monotonic function of $U_x$, and $Y_{x'}$ is a strictly monotonic function of $U_{x'}$. Since $U_x$ is a strictly monotone increasing function of $U_{x'}$, then $Y_x$ is a strictly increasing monotonic function of $Y_{x'}$, which leads to Assumption \ref{assump3}.  

Second, we show that  Assumption \ref{assump3} implies Assumption \ref{assump2}.  If Assumption \ref{assump3} holds, then given $Z=z$, 
   $Y_x$ is a strictly increasing function of $Y_{x'}$. 
When $U_x$ is a strictly monotone increasing function of $U_{x'}$ and note that    
    \[ Y_x = f_Y(x, z, U_{X}), ~  Y_{x'} = f_Y(x', z, U_{X}), \]
which implies that $f_Y$ is a strictly monotonic function of $U_X$, i.e.,  Assumption \ref{assump2} holds. 

 This finishes the proof. 
% Assumption \ref{assump1} assumes that $U_X = U_{x} = U_{x'}$.  Thus, 
%  \[ Y_x = f_Y(x, z, U_{X}), ~  Y_{x'} = f_Y(x', z, U_{X}). \]
%  Clearly, $\rho(Y_x, Y_{x'} | Z=z) = 1$ by the strictly monotonicity of $f_Y$ (Assumption \ref{assump2}). 
 % In addition, Example 1 in the main text indicates that Assumption \ref{assump3} does not imply Assumptions \ref{assump1}-\ref{assump2}. }
 
\end{proof}


   
   
   {\bf Proposition \ref{prop3}}  \emph{Under Assumption \ref{assump4}, the conclusion in Proposition \ref{prop1} also holds.}   
% \emph{(ii) Under Assumption \ref{assump2},  Assumption \ref{assump1} implies Assumption \ref{assump3}.   }

\begin{proof}[Proof of Proposition \ref{prop3}]  This can be shown through a proof analogous to that of Proposition \ref{prop1}.  

\end{proof} 

% \begin{proof}[Proof of Proposition \ref{prop2}]  

% 
% \end{proof}



% =========================================
\section{Proofs in Section \ref{sec6}} \label{app-b} 

Recall that $l_{\tau}(\xi) = \tau \xi \cdot \mathbb{I}(\xi \geq 0)+ (\tau - 1)\xi \cdot \mathbb{I}(\xi < 0)$, and 
        \begin{align*}
        q(x, z; \tau) \triangleq{}& \inf_{y}\{y: \P( Y \leq y | X=x, Z=z) \geq \tau \} \\ 
              q_0(z; \tau)  \triangleq{}& \inf_{y}\{y: \P( Y_0 \leq y | Z=z) \geq \tau \} \\
                 q_1(z; \tau)   \triangleq{}& \inf_{y}\{y: \P( Y_1 \leq y | Z=z) \geq \tau \}. 
        \end{align*}
        


   {\bf Lemma \ref{prop4}}  We have that 


\emph{(i) $q_x(Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y_x - f(Z))]$ for $x= 0 , 1$;} 

\emph{(ii)  $q(X, Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y - f(X, Z))]$.}

% \emph{
% (ii) $ \bfE[ l_{\tau}(Y - f(X, Z))] \neq   \bfE[ l_{\tau}(Y_x - f(Z))],$ which implies that $q(x, Z; \tau)$ not equal to $q_x(Z; \tau)$ for $x = 0, 1$. }

\begin{proof}[Proof of Lemma \ref{prop4}] We prove $q_x(Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y_x - f(Z))]$, and   
$q(X, Z; \tau) = \arg\min_{f} \bfE[ l_{\tau}(Y - f(X, Z))]$ can be derived by an exactly similar manner. We write 
\[ \bfE[ l_{\tau}(Y_x - f(Z))] = \bfE [ \bfE \{ l_{\tau}(Y_x - f(Z)) \mid Z\} ].   \]
To obtain the conclusion, note that $l_{\tau}(Y_x - f(Z))$ is always positive, it suffices to show that  
    \begin{equation} \label{eq-s5}
        q_x(z; \tau) = \arg \min_{f} \bfE [ l_{\tau}(Y_x - f(Z)) \mid Z = z ] 
    \end{equation}  
for any given $Z=z$. Next, we focus on analyzing the term $\bfE[ l_{\tau}(Y_x - f(Z)) \mid Z = z ]$. Given $Z=z$, $f(Z)$ is a constant and we denote it by $c$, then 
    \begin{align*}
         & \bfE[ l_{\tau}(Y_x - f(Z)) \mid Z = z ]  \\
       ={}&  \bfE[ l_{\tau}(Y_x - c) \mid Z = z ] \\
       ={}& \bfE\Big [  \tau (Y_x - c) \mathbb{I}(Y_x \geq c) + (\tau - 1) (Y_x - c) \mathbb{I}(Y_x < c)   \mid Z=z \Big ] \\
       ={}&  \tau \int_c^{\infty} (y_x - c) g(y_x|z) dy_x  + (\tau -1) \int_{-\infty}^c (y_x - c) g(y_x|z) dy_x,   
    \end{align*}
where $g(y_x|z)$ denotes the probability density function of $Y_x$ given $Z=z$.    

Since the check function is a convex function, differentiating $\bfE[ l_{\tau}(Y_x - c) \mid Z = z ]$ with respect to $c$ and setting the derivative to zero will yield the solution for the minimum
   \begin{align*}
       & \frac{\partial}{\partial c} \bfE[ l_{\tau}(Y_x - c) \mid Z = z ] \\
      ={}& \tau \int_c^{\infty} \frac{\partial}{\partial c}[(y_x - c) g(y_x|z)] dy_x + (\tau - 1) \int_{-\infty}^c \frac{\partial}{\partial c}[(y_x - c) g(y_x|z)] dy_x \\
      ={}& - \tau \Big (1 - \int_{-\infty}^{c} g(y_x|z)dy_x \Big ) + (1 - \tau) \int_{-\infty}^c g(y_x|z)dy_x.  
   \end{align*}
Then let $ \frac{\partial}{\partial c} \bfE[ l_{\tau}(Y_x - c) \mid Z = z ] = 0$ leads to that 
 \[      \int_{-\infty}^{c} g(y_x|z)dy_x = \tau,  \]
that is, $c = q_x(z; \tau)$. This completes the proof of Proposition \ref{prop4}. %(i). 

\end{proof}





{\bf Theorem \ref{thm-1}.} \emph{If the probability density function of $Y$ given $Z$ is continuous, then the loss  $R_{x'}(t; x, z, y)$ is minimized uniquely at $t^*$, where $t^*$ is the solution satisfying  
    \[   \P(Y_{x'} \leq t^*  |  Z=z ) = \P(Y_x \leq y  |  Z=z ).  \]}

\begin{proof}[Proof of Theorem \ref{thm-1}]
Recall that 
\begin{align*}
     R_{x'}(t| x, z, y)
     % ={}& \bfE \left [ \frac{\mathbb{I}(X=x')}{p_{x'}(z)} | Y - t |  ~ \Big | ~ Z=z \right ]  + \bfE \left [ \frac{\mathbb{I}(X=x)}{p_{x}(z)} \cdot \text{sign}(Y - y)  ~ \Big | ~  Z = z\right ] \cdot t \\
%     ={}&  \bfE \left [ \frac{\mathbb{I}(X=x')}{p_{x'}(z)} | Y_{x'} - t |  ~ \Big | ~ Z=z \right ]  + \bfE \left [ \frac{\mathbb{I}(X=x)}{p_{x}(z)}  \cdot \text{sign}(Y_x - y)  ~ \Big | ~  Z = z\right ] \cdot t \\
%     ={}&    \bfE \left [  | Y_{x'} - t |  ~ \Big | ~ Z=z, X=x' \right ]   + \bfE \left [ \text{sign}(Y_x - y)  ~ \Big | ~  Z = z, X=x\right ] \cdot t \\
   ={}&    \bfE \left [  | Y_{x'} - t |  ~ \Big | ~ Z=z \right ]   + \bfE \left [ \text{sign}(Y_x - y)  ~ \Big | ~  Z = z\right ] \cdot t. 
 \end{align*}
% where the last equality follows from the backdoor criterion $(Y_x, Y_{x'})\indep X \mid Z$.  
Let  $g(y_x|z)$ be the probability density function of $Y_x$ given $Z=z$.  By calculation, 
 \begin{align*}
       \bfE \left [  | Y_{x'} - t |  ~ \Big | ~ Z=z \right ] =   \int_{t}^{\infty} (y_{x'} - t) g(y_{x'}|z) dy_{x'} + \int_{-\infty}^{t} (t - y_{x'})  g(y_{x'}|z) dy_{x'}, 
 \end{align*} 
  \begin{align*}
       \frac{\partial}{\partial t}  \bfE \left [  | Y_{x'} - t |  ~ \Big | ~ Z=z \right ] 
       = -\Big( 1-  \int_{-\infty}^{t} g(y_{x'}|z) dy_{x'} \Big) + \int_{-\infty}^{t} g(y_{x'}|z) dy_{x'} 
       =  2 \P( Y_{x'} \leq t | Z = z) - 1,
  \end{align*}
and 
  \begin{align*}
         \bfE \left [  \text{sign}(Y_x - y)   ~ \Big | ~  Z = z\right ]
      % ={}& \bfE  \left [ 2 \mathbb{I}(Y_x > y) -1  ~ \Big | ~  Z = z, X=x\right ]   \\
      = \bfE  \left [ -2 \mathbb{I}(Y_x \leq y) + 1  ~ \Big | ~  Z = z\right ] 
      = - 2 \P( Y_x \leq y | Z=z) + 1,  
  \end{align*}
we have   
    \begin{align*}
       \frac{\partial}{\partial t} R_{x'}(t| x, z, y) ={}& 2 \P(Y_{x'} \leq t | Z=z) - 1 +  \bfE \left [  \text{sign}(Y - y)   ~ \Big | ~  Z = z\right ] \\
       ={}& 2 \P(Y_{x'} \leq t | Z=z) - 1 - 2 \P(Y_x \leq y| Z=z ) +1 \\
       ={}& 2 \Big \{ \P(Y_{x'} \leq t | z)  -  \P(Y_{x} \leq y | z) \Big  \}.   
      \end{align*}
 Since 
    \begin{align*}
        \frac{\partial^2}{\partial t^2} R_{x'}(t| x, z, y) = 2 \partial \P(Y_{x'} \leq t | z) / \partial t = 2 g(y_{x'}=t|z)  \geq 0,
    \end{align*}
 $R_{x'}(t| x, z, y)$ is a convex function with respect to $t$. 
Letting $\frac{\partial}{\partial t} R_{x'}(t| x, z, y) = 0$ yields that 
     \[  \P(Y_{x'} \leq t | z)  -  \P(Y_{x} \leq y | z) = 0.     \]
That is, $R_{x'}(t| x, z, y)$ attains its minimum at $t = q_{x'}(z; \tau^*)$, where $\tau^*$ is the quantile of $y$ in the distribution $\P(Y_x| Z=z)$. 

\end{proof}


{\bf Proposition \ref{prop5}.} \emph{If $h \to 0$ as $N \to \infty$, $\hat p_x(z)$ and $\hat p_{x'}(z)$ are consistent estimates of $p_x(z)$ and $p_{x'}(z)$, and the density function of $Z$ is differentiable, then 
$$\hat R_{x'}(t; x, z, y) \xrightarrow{\P} R_{x'}(t; x, z, y),$$
where $\xrightarrow{\P}$ means convergence in probability.}

\begin{proof}[Proof of Proposition \ref{prop5}] 
For analyzing the theoretical properties of $\hat R_{x'}(t; x, z, y)$, we rewritten $\hat R_{x'}(t; x, z, y)$ as  
  \begin{align*}
        \hat R_{x'}(t; x, z, y) ={} \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{\hat p_{x'}(Z_k)} | Y_k - t |  }{ \sum_{k=1}^N K_h(Z_k -z) } + \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x)}{\hat p_{x}(Z_k)} \cdot \text{sign}(Y_k - y) }{  \sum_{i=1}^N K_h(Z_k -z) }  \cdot t, 
  \end{align*}
where the capital letters denote random variables and lowercase letters denote their realizations. This is slightly different from that used in the main text. 

When $\hat p_x(z)$ and $\hat p_{x'}(z)$ are consistent estimates of $p_x(z)$ and $p_{x'}(z)$, to show the conclusion, it is sufficient to prove that 
  \begin{align}
      \frac{  \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} | Y_k - t |  }{   \sum_{k=1}^N K_h(Z_k -z) } 
      \xrightarrow{\P}{}&    \bfE \left [ \frac{\mathbb{I}(X=x')}{p_{x'}(z)} | Y - t |  ~ \Big | ~ Z=z \right ] =  \bfE \left [  | Y_{x'} - t |  ~ \Big | ~ Z=z \right ],  \label{eq-s6}  \\
       \frac{  \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x)}{p_{x}(Z_k)} \cdot \text{sign}(Y_k - y) }{  \sum_{i=1}^N K_h(Z_k -z) }    \xrightarrow{\P}{}&   \bfE \left [ \frac{\mathbb{I}(X=x)}{p_{x}(z)} \cdot \text{sign}(Y - y)  ~ \Big | ~  Z = z\right ]= \bfE \left [ \text{sign}(Y_x - y)  ~ \Big | ~  Z = z\right ].  \label{eq-s7}   
  \end{align}
%  Recall that the notation for random variables is denoted by capital letters,
  We prove equation (\ref{eq-s6}) only, as equation (\ref{eq-s7}) can be addressed similarly.  


Note that 
\[      \frac{  \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} | Y_k - t |  }{   \sum_{k=1}^N K_h(Z_k -z) }  =  \frac{ \frac 1 N \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} | Y_k - t |  }{  \frac 1 N \sum_{k=1}^N K_h(Z_k -z) },   \]
we analyze the denominator and numerator on the right side of the equation separately. For the denominator, it is an average of $N$ independent random variables and converges to its expectation 
   $\bfE[ K_{h}(Z_k -z)]$
almost surely. Let  $g(z_k)$ be the probability density function of $Z_k$, and $g^{(1)}(z_k)$ is its first derivative.  
Since
     \begin{align}
         \bfE[ K_{h}(Z_k -z)] ={}& \int \frac{1}{h} K(\frac{z_k - z}{h}) g(z_k) dz_k  \notag \\
         ={}& \int K(u) g(z+hu)  du \qquad \text{(let $z_k=z+hu$)} \notag\\
         ={}& \int K(u)\cdot \{g(z) + g^{(1)}(z) h u + o(h) \} du \qquad \text{(by Taylor Expansion)} \notag \\
         ={}& g(z) \int K(u)du +  g^{(1)}(z) h \int K(u)u du + o(h) \notag\\
         ={}& g(z) + o(h)  \qquad \text{(by the definition of kernel function),}       \label{eq-s8}
     \end{align}
   when $h\to 0$ as $N \to \infty$,     
the denominator converges to $g(z)$ in probability.  

Next, we focus on dealing with the numerator, which also converges to its expectation.  
   \begin{align}
    & \bfE[ K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} | Y_k - t | ] \notag \\
   ={}& \bfE \Big [ K_h(Z_k -z) \bfE  \Big\{ \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} | Y_{k} - t | \Big | Z_k   \Big\}   \Big ] \qquad \text{(by the law of iterated expectations)} \notag \\
   ={}&  \bfE \Big [ K_h(Z_k -z) \bfE  \Big\{ \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} | Y_{x',k} - t | \Big | Z_k   \Big\}   \Big ] \qquad \text{(write $Y_k$ as the form of potential outcome)} \notag \\
   ={}&  \bfE \Big [ K_h(Z_k -z) \bfE  \Big\{ | Y_{x',k} - t | \Big | Z_k   \Big\}   \Big ]   \qquad \text{(by backdoor criterion $Y_{x',k}\indep X_k | Z_k$)}. \label{eq-s9}
   % ={}&  \bfE \Big [ K_h(Z_k -z) \cdot | Y_{x',k} - t | \Big ] \qquad \text{by the law of iterated expectations again} \\
   % ={}& 
   \end{align}
Define $m(Z) = \bfE[ |Y_{x'} -t| \big | Z]$ and $m^{(1)}(Z)$ is its first derivative, then the right side of equation (\ref{eq-s6}) is $m(z)$, and 
    \begin{align}
         \bfE \Big [ &  K_h(Z_k -z) \cdot \bfE  \Big\{ | Y_{x',k} - t | \Big | Z_k   \Big\}   \Big ] 
       ={} \bfE \Big [ K_h(Z_k -z) \cdot m(Z_k) \Big ] \notag\\
       ={}& \int \frac{1}{h} K(\frac{z_k-z}{h}) \cdot m(z_k) \cdot g(z_k) dz_k \notag \\
       ={}& \int K(u) \cdot m(z+hu) \cdot g(z+hu) du \qquad \text{(let $z_k = z+ hu$)}\notag \\
       ={}& \int K(u) \cdot \{m(z) + m^{(1)}(z) h u + o(h)   \} \cdot \{g(z) + g^{(1)}(z) h u + o(h)  \} du \qquad \text{(by Taylor Expansion)} \notag \\
       ={}&  m(z) g(z) + o(h). \label{eq-s10}
    \end{align}
Thus,  when $h\to 0$ as $N \to \infty$,  the numerator converges to $g(z)$ in probability.   

Combining equations (\ref{eq-s8}), (\ref{eq-s9}), and (\ref{eq-s10}) yields the equality (\ref{eq-s6}). This completes the proof. 

\end{proof} 



{\bf Theorem \ref{thm5-4}} (Unbiasedness Preservation). \emph{ 
The loss $R_{x'}^{\mathrm{weight}}(t| x, z, y)$ is convex in terms of $t$ and is minimized uniquely at $t^*$, where  $t^*$ is the solution satisfying 
       $\P(Y_{x'} \leq t^* | Z=z) = \P(Y_x \leq y| Z=z).$}

\begin{proof}[Proof of Theorem \ref{thm5-4}]. 
It is sufficient to show that $R_{x'}^{\mathrm{weight}}(t| x, z, y)$ shares the same minimizer as $R_{x'}(t| x, z, y)$. By the backdoor criterion (i.e., $(Y_x, Y_{x'} \indep X | Z)$), we have that 
  \begin{align*}
        & R_{x'}^{\mathrm{weight}}(t| x, z, y) \\
       ={}& \bfE \left [ w(X, Z)   | Y_{x'} - t |  \big |  Z=z \right ]+\bfE \left [ w(X, Z)\text{sign}(Y_x - y)   \big |   Z = z\right ] \cdot t \\  
        ={}& \bfE[ w(X, Z) \mid Z=z ] \cdot  \bfE \left [   | Y_{x'} - t |  ~ \big | ~ Z=z \right ]  \\
        {}& + \bfE[ w(X, Z) \mid Z=z ] \cdot  \bfE \left [ w(X, Z) \cdot \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t  \\
        ={}&  \bfE[ w(X, Z) \mid Z=z ] \cdot R_{x'}(t| x, z, y).
  \end{align*}
Note that $\bfE[ w(X, Z) \mid Z=z ]$ is not related to $t$.  Consequently, $R_{x'}^{\mathrm{weight}}(t|x, z, y)$ shares the same unique minimizer as $R_{x'}(t|x, z, y)$. Both minimizers satisfy the condition 
       $\P(Y_{x'} \leq t^* | Z=z) = \P(Y_x \leq y| Z=z).$

%independent of 

\end{proof}

\medskip 


{\bf Proposition \ref{prop5-5}} (Bias of the Estimated Loss). 
 \emph{If $h \to 0$ as $N \to \infty$,  $p_x(z)/\hat p_x(z)$ and $p_{x'}(z)/\hat p_{x'}(z)$ are differentiable with respect to $z$, and the density function of $Z$ is differentiable, then the bias of $\hat R_{x'}(t| x, z, y)$, defined by $\bfE[ \hat R_{x'}(t|x,z,y) ] - R_{x'}(t|x,z,y)$, is given as 
    \begin{align*}
        \text{Bias}(&\hat R_{x'})  ={}
     \delta_{p_{x'}}  \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ]  + \delta_{p_{x}}  \bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t + O(h^2), 
    \end{align*} 
where $\delta_{p_{x'}}  = (p_{x'}(z) - \hat p_{x'}(z))/\hat p_{x'}(z)$ and $\delta_{p_{x}}  = (p_{x}(z) - \hat p_{x}(z))/\hat p_{x}(z)$ are estimation errors of propensity scores. }


\begin{proof}[Proof of Proposition \ref{prop5-5}].  
This proof is similar to the proof of Proposition \ref{prop5}. Recall that 
  \begin{align*}
        \hat R_{x'}(t; x, z, y) ={} \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{\hat p_{x'}(Z_k)} | Y_k - t |  }{ \sum_{k=1}^N K_h(Z_k -z) } + \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x)}{\hat p_{x}(Z_k)} \cdot \text{sign}(Y_k - y) }{  \sum_{i=1}^N K_h(Z_k -z) }  \cdot t, 
  \end{align*}
and 
 \[  R_{x'}(t| x, z, y) = \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ]   + \bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t, \]
Given the estimated propensity scores, to show the conclusion, it suffices to prove that 
  \begin{align}
     \bfE \left [  \frac{  \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{\hat p_{x'}(Z_k)} | Y_k - t |  }{   \sum_{k=1}^N K_h(Z_k -z) } \right ]  ={}&  
     \frac{p_{x'}(z)}{\hat p_{x'}(z)} \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ] + O(h^2). \label{eq-s11}  \\
        \bfE \left [  \frac{  \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x)}{p_{x}(Z_k)} \cdot \text{sign}(Y_k - y) }{  \sum_{i=1}^N K_h(Z_k -z) }  \right ]  ={}&   \frac{p_{x}(z)}{\hat p_{x}(z)} \bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t + O(h^2). \label{eq-s12}   
  \end{align}
%  Recall that the notation for random variables is denoted by capital letters,
  We prove the equation (\ref{eq-s11}) only, as remaining equation (\ref{eq-s12}) can be addressed similarly.  
  Let  $g(z_k)$ be the probability density function of $Z_k$ and  $m(Z) = \bfE[ |Y_{x'} -t| \big | Z]$.   
By the proof of  Proposition \ref{prop5}, $N^{-1}\sum_{k=1}^N K_h(Z_k -z) = g(z) + O(h^2)$, and 
   \begin{align*}
   &  \bfE \left [  \frac 1 N \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{\hat p_{x'}(Z_k)} | Y_k - t |  \right ] \\
    & \bfE[ K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{\hat p_{x'}(Z_k)} | Y_k - t | ] \notag \\
   ={}&  \bfE \Big [ K_h(Z_k -z) \cdot \frac{ p_{x'}(Z_k)}{\hat p_{x'}(Z_k)} \bfE  \Big\{ | Y_{x',k} - t | \Big | Z_k   \Big\}   \Big ]    \\
      ={}&  \bfE \Big [ K_h(Z -z) \cdot \frac{ p_{x'}(Z)}{\hat p_{x'}(Z)}\cdot m(Z) \Big ]    \\
       ={}&  \frac{ p_{x'}(z)}{\hat p_{x'}(z)} m(z) g(z) + O(h^2).
    \end{align*}
Thus, equation (\ref{eq-s11}) holds using Slutsky's Theorem.  This completes the proof.
%Thus, when $h\to 0$ as $N \to \infty$,  the numerator converges to $g(z)$ in probability.   
% Combiing equations (\ref{eq-s8}), (\ref{eq-s9}), and (\ref{eqs10}) yields the equality (\ref{eq-s6}). This completes the proof. 
    % \begin{align}
    %      \bfE \Big [ &  K_h(Z_k -z) \cdot \bfE  \Big\{ | Y_{x',k} - t | \Big | Z_k   \Big\}   \Big ] 
    %    ={} \bfE \Big [ K_h(Z_k -z) \cdot m(Z_k) \Big ] \notag\\
    %    ={}& \int \frac{1}{h} K(\frac{z_k-z}{h}) \cdot m(z_k) \cdot g(z_k) dz_k \notag \\
    %    ={}& \int K(u) \cdot m(z+hu) \cdot g(z+hu) du \qquad \text{(let $z_k = z+ hu$)}\notag \\
    %    ={}& \int K(u) \cdot \{m(z) + m^{(1)}(z) h u + o(h)   \} \cdot \{g(z) + g^{(1)}(z) h u + o(h)  \} du \qquad \text{(by Taylor Expansion)} \notag \\
    %    ={}&  m(z) g(z) + o(h). \label{eq-s10}
    % \end{align}
    

\end{proof}

\medskip 


% at a rate of  $N^{-1/2}$
{\bf Theorem \ref{thm5-6}} (Bias of the Estimated Counterfactual Outcome). Under the same conditions as in Proposition \ref{prop5-5}, $\hat y_{x'}$ converges to $\bar y_{x'}$  in probability, where $\bar y_{x'}$ satisfies that 
    \begin{equation} \label{eq3}  \P( Y_{x'} \leq \bar y_{x}|Z=z ) = \frac{ (2+2\delta_{p_{x}}) \P(Y_{x}\leq y| Z=z) + (\delta_{p_{x'}}- \delta_{p_{x}})  }{ 2 + 2\delta_{p_{x'}}  },
    \end{equation}
 where $\bar y_{x'}$ may not equal to the true value $y_{x'}$ and their difference is the bias.   


\begin{proof}[Proof of Theorem \ref{thm5-6}]
By the property of M-estimation~\citep{vdv-1996}, $\hat y_{x'}$ converges to $\bar y_{x'}$  in probability, where $\bar y_{x'}$ is the minimizer of $$\bar R_{x'}(t; x, z, y),$$ 
 where $\bar R_{x'}(t; x, z, y)$ is the probability limit of $\hat R_{x'}(t; x, z, y)$. 
By the proof of Proposition \ref{prop5-5}, 
% the estimation error of propensity scores will lead to a bias of $\hat R_{x'}(t; x, z, y)$  given by 
 \[    \bar R_{x'}(t; x, z, y) =  \frac{p_{x'}(z)}{\hat p_{x'}(z)} \bfE \left [  | Y_{x'} - t |  ~ \big | ~ Z=z \right ]  + \frac{p_{x}(z)}{\hat p_{x}(z)} \bfE \left [ \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t.    \]
By a similar proof of Theorem \ref{thm-1}, as $h \to 0$, $\bar y_{x'} = \arg\min_{t} \bar R_{x'}(t; x, z, y) $ satisfies that 
    \begin{equation*}  \P( Y_{x'} \leq \bar y_{x}|Z=z ) = \frac{ (2+2\delta_{p_{x}}) \P(Y_{x}\leq y| Z=z) + (\delta_{p_{x'}}- \delta_{p_{x}})  }{ 2 + 2\delta_{p_{x'}}  },
    \end{equation*}
\end{proof}
% First,  if we set 
%      \[  w(X, Z) =  K_h(Z -z)\Big / N^{-1} \sum_{k=1}^N K_h(Z-z),    \]
%  then by Theorem \ref{thm5-4},  the following loss 
%   \[ \bfE \left [ w(X, Z) \cdot  | Y_{x'} - t |  ~ \big | ~ Z=z \right ]   + \bfE \left [ w(X, Z) \cdot \text{sign}(Y_x - y)  ~ \big | ~  Z = z\right ] \cdot t \]
% is a valid loss for counterfactual estimation. 
% % we note that 

% If we estimate the above loss with its version of sample mean, 
%  \[  \frac{ \sum_{k=1}^N K_h(Z_k -z)  | Y_{x',k} - t |  }{ \sum_{k=1}^N K_h(Z_k -z) }  + \frac{ \sum_{k=1}^N K_h(Z_k -z)  \cdot \text{sign}(Y_{x,k} - y) }{  \sum_{k=1}^N K_h(Z_k -z) }  \cdot t \]
% then by the property of M-estimation~\citep{vdv-1996}, it will yield a solution that converges to the true value $y_{x}$. 

% Note that $\bfE[\frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)} \mid Z_k ] = 1$ and $\bfE[\frac{\mathbb{I}(X_k=x)}{p_{x}(Z_k)} \mid Z_k ] = 1$, then the following loss 
%  \[  \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{p_{x'}(Z_k)}   | Y_{x',k} - t |  }{ \sum_{k=1}^N K_h(Z_k -z) }  + \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x)}{p_{x}(Z_k)}   \cdot \text{sign}(Y_{x,k} - y) }{  \sum_{k=1}^N K_h(Z_k -z) }  \cdot t \]
% also yield a solution that converges to the true value $y_{x}$. Recall that 
%   \begin{align*}
%         \hat R_{x'}(t; x, z, y) ={} \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x')}{\hat p_{x'}(Z_k)} | Y_k - t |  }{ \sum_{k=1}^N K_h(Z_k -z) } + \frac{ \sum_{k=1}^N K_h(Z_k -z) \frac{\mathbb{I}(X_k=x)}{\hat p_{x}(Z_k)} \cdot \text{sign}(Y_k - y) }{  \sum_{i=1}^N K_h(Z_k -z) }  \cdot t, 
%   \end{align*}
% which equals to the above loss, except that the propensity scores is replaced by theirs estimates.  Therefore, the estimated  $y_{x'}$, denoted as $\hat y_{x'}$, obtained by minimizing$\hat R_{x'}(t| x, z, y)$, is affected by the estimation error of propensity scores, and is not influenced by the kernel smoothing. 






% =========================================
\section{Extension to Continuous Outcome} \label{app-d} 

When the treatment is continuous, we can estimate the ideal loss with the following estimator
 $$ \tilde R_{x'}(t| x, z, y) = \frac{ \sum_{k=1}^N K_h(z_k -z) \frac{K_h(x_k-x') }{p_{x'}(z_k)} | y_k - t |  }{ \sum_{k=1}^N K_h(z_k -z) }  + \frac{ \sum_{k=1}^N K_h(z_k -z) \frac{K_h(x_k - x) }{p_{x}(z_k)} \cdot \text{sign}(y_k - y) }{  \sum_{k=1}^N K_h(z_k -z) }  \cdot t, $$
which is a smoothed version of the estimator 
  \begin{align*}
        \hat R_{x'}(t| x, &z, y) ={} \frac{ \sum_{k=1}^N K_h(z_k -z) \frac{\mathbb{I}(x_k=x')}{p_{x'}(z_k)} | y_k - t |  }{ \sum_{k=1}^N K_h(z_k -z) }  + \frac{ \sum_{k=1}^N K_h(z_k -z) \frac{\mathbb{I}(x_k=x)}{p_{x}(z_k)} \cdot \text{sign}(y_k - y) }{  \sum_{k=1}^N K_h(z_k -z) }  \cdot t,
  \end{align*}
 defined in Section \ref{sec6}. In addition, by a proof similar to that of Proposition \ref{prop5}, we also can show that 
 $\tilde R_{x'}(t; x, z, y) \xrightarrow{\P} R_{x'}(t; x, z, y).$
 % where $h$ is a bandwidth (smoothing) parameter. The new estimator can apply to both discrete and continuous treatments.  

% For the newly defined ideal loss $R_{x'}(t| x, z, y)$ and the estimator  $ \hat R_{x'}(t| x, z, y)$, 





\section{Experiment Details and Additional Experiment Results} \label{app-e}  
We run all experiments on the Google Colab platform. For the representation model, we use the MLP for the base model and tune the layers in $\{1, 2, 3\}$. In addition, we adopt the logistic regression model as the propensity model. We tune the learning rate in $\{0.001, 0.005, 0.01, 0.05, 0.1\}$. For the kernel choice, we select the kernel function between the Gaussian kernel function and the Epanechnikov kernel function, and tune the bandwidth in $\{1, 3, 5, 7, 9\}$.

In addition, we investigate the performance of our method when the rank preservation assumption is violated. Specifically, we simulate $ Y_1 = (W_y + W_{y1})\cdot Z + U_{1}$ and $Y_0 =  W_y \cdot Z/\alpha + U_{0}$ with $W_y \sim \mathcal{N}(0,I_{m})$ and $W_{y1} \sim \mathcal{N}(0,\beta I_{m})$, where $\beta$ is the hyper parameter to control the violation degree. In the added experiments, m is in {10, 40}, which is consistent with the experiments in our original manuscript, and Kendall’s rank correlation coefficient $\rho(Y_0, Y_1)$ is in {0.3, 0.5}. We generate 10 different datasets with different seeds and the experiment results are shown in Table \ref{tab:sim_rank}. The experiment results show that our method still outperforms the baseline methods, even if the rank preservation assumption is violated.
\begin{table*}[]
\centering
\caption{$\sqrt{\epsilon_{\text{PEHE}}}$ of individual treatment effect estimation on the simulated Sim-$m$ dataset, where $m$ is the dimension of $Z$.}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|ll|ll|ll|ll}
\toprule  & \multicolumn{2}{c|}{Sim-10 (Rank=0.3)}   & \multicolumn{2}{c|}{Sim-10 (Rank=0.5)}  & \multicolumn{2}{c|}{Sim-40 (Rank=0.3)}   & \multicolumn{2}{c}{Sim-40 (Rank=0.5)}
% \\ \cmidrule{2-9} 
\\ \midrule
Methods   & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c|}{Out-sample} & \multicolumn{1}{c}{In-sample} & \multicolumn{1}{c}{Out-sample} \\
\cmidrule{1-9}
TARNet     & 6.92 $\pm$  0.79          & 6.96 $\pm$  0.82          & 4.38 $\pm$  0.59          & 4.38 $\pm$  0.60          & 14.91 $\pm$  0.61         & 14.80 $\pm$  0.82         & 9.33 $\pm$  0.36          & 9.33 $\pm$  0.45          \\
DragonNet  & 6.97 $\pm$  0.83          & 7.02 $\pm$  0.88          & 4.43 $\pm$  0.62          & 4.44 $\pm$  0.62          & 15.04 $\pm$  0.54         & 14.94 $\pm$  0.77         & 9.18 $\pm$  0.30          & 9.16 $\pm$  0.50          \\
ESCFR      & 6.91 $\pm$  0.69          & 6.96 $\pm$  0.76          & 4.41 $\pm$  0.65          & 4.41 $\pm$  0.68          & 15.01 $\pm$  0.58         & 14.86 $\pm$  0.82         & 9.25 $\pm$  0.18          & 9.26 $\pm$  0.31          \\
X\_learner & 7.01 $\pm$  0.81          & 7.05 $\pm$  0.85          & 5.29 $\pm$  0.44          & 4.98 $\pm$  0.48          & 15.16 $\pm$  0.64         & 15.05 $\pm$  0.88         & 9.43 $\pm$  0.29          & 9.40 $\pm$  0.42          \\
Quantile-Reg & 6.79 $\pm$  0.79          & 6.63 $\pm$  0.84          & 4.12 $\pm$  0.63          & 4.14 $\pm$  0.65          & 13.12 $\pm$  0.62         & 13.25 $\pm$  0.87         & 8.39 $\pm$  0.30          & 8.43 $\pm$  0.42          \\
Ours       & \textbf{6.00 $\pm$  1.22}* & \textbf{6.01 $\pm$  1.29} & \textbf{3.88 $\pm$  1.25} & \textbf{3.90 $\pm$  1.30} & \textbf{10.76 $\pm$  0.52}* & \textbf{10.84 $\pm$  0.43}* & \textbf{6.75 $\pm$  0.24}* & \textbf{6.80 $\pm$  0.21}* \\
\bottomrule
\end{tabular}}
\label{tab:sim_rank}
\end{table*}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}



