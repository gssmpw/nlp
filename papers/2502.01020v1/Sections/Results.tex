In this section, we answer RQ1 by evaluating the performance of RiskHarvester against RiskBench and RQ2 by assessing whether developers prioritize secret removal from software artifacts ranked by descending security risk scores.

\subsection{Performance of RiskHarvester} \label{RiskHarvesterResult}

\input{Tables/ValueResults}

\textbf{Performance of Finding Database Keywords}: Table~\ref{database-keyword-result} presents RiskHarvester's precision, recall, and F1-score in identifying the database, table, and column names for each database type. The column ``Precision (TP, FP)'' denotes the precision score with the number of true positive and false positive database keywords outputted by RiskHarvester. The column ``Recall (TP, FN)'' denotes the recall score with the number of outputted true positive and false negative database keywords. The column ``F1'' denotes the F1-score (the harmonic mean of precision and recall).

% We observed that RiskHarvester demonstrates overall precision of 95\%, recall (90\%), and F1-score (92\%) in identifying the database keywords. 

We observed that RiskHarvester demonstrated overall precision of 97\%, 96\%, and 92\% in identifying the database, table, and column names, respectively, indicating high precise detection of database keywords. The count of false positives indicates that the tool incorrectly outputted 43 database names, 24 table names, and 106 column names out of 3,304 database keywords. In addition, RiskHarvester demonstrated an overall recall of 94\% and 90\%, indicating a strong ability to identify database and table names, respectively, supported by F1-scores of 95\% and 93\%. However, the recall of identifying column names is 85\%, which is relatively lower than that of database and table names. The count of false negatives indicates that the tool failed to detect 86 database names, 74 table names, and 209 column names. We also observed that among the four database types, the recall of database, table, and column names of SQL Server is low (62\%, 33\%, and 40\%, respectively), though the precision is 100\%. In addition, the recall for table names in MongoDB is relatively low (76\%) compared to MySQL and PostgreSQL. We now discuss our observations on the false positives and false negatives.

\uline{Analysis of False Positives}: Since the SQL drivers use raw SQL queries (V-Pattern 1), we observed that the false positives on table and column names are mostly caused by the dynamically constructed queries (61\% of the false positives). We identified all the string parts flowing in the driver function sink for a SQL query using data flow analysis and reconstructed the query using the source code line and column information (Step 2.1). However, we could not reconstruct the complete query due to the presence of conditional statements and dynamically fetched values from the environment variables or config files. As a result, we identified incorrect table and column names while parsing the SQL query. Similar to dynamic raw SQL query, we observed that 24\% of the total incorrect column names are from dynamically constructed dictionary objects for column names passed in the NoSQL drivers (V-Pattern 2). Additionally, the false positives of database names are mostly triggered by the neighboring lines rule (Step 1.3), comprising 71.5\% of the 43 false positives. We observed that the prefix match of the neighboring key names met the threshold, though the key name is not the correct asset of the corresponding secret containing the database name.


\uline{Analysis of False Negatives}: We observed that the repositories of RiskBench also contain non-Python source codes such as C\# and Java. While we detected secret-asset pairs in non-Python code using regex (Step 1.1), we could not identify table and column names since data flow analysis was only applied to Python code (Step 2.1). For example, the SQL server shows a relatively lower recall (33\% and 40\% for table and column names) since SQL Server table and column names are typically passed to .NET driver functions. In addition, we reconstructed the raw SQL query from the query parts flowing into the driver sinks. However, similar to false positives, we missed table and column names due to improperly reconstructing the original query for having dynamic behavior. Additionally, we observed that 54 (4.2\%) instances of secret-asset pairs in RiskBench do not fall within three neighboring lines. Thus, we failed to detect the database name when the asset identifier containing the database name was not present in the three neighboring lines of the secret (Step 1.3).

\textbf{Performance of Sensitive Data Category Mapping}: We applied lexical and semantic matching to map the identified database keywords to the corresponding sensitive data categories (Step 2.2). We observed a precision of 85\% among the 3,673 database keywords of RiskBench. We manually inspected a random sample of 50 false positive mappings. We noticed that 27 false positives are due to the Jaro-Winkler similarity, which employs prefix bias. For example, ``credit\_limit'' and ``tax\_rate'' keywords are wrongly mapped to ``CREDIT\_CARD\_NUMBER'' and ``TAX\_ID'' data categories, respectively. In addition, we observed that 16 false positives were due to semantic matching. For example, ``transaction\_code'' is mapped to ``CREDIT\_CARD\_NUMBER'' since both terms appear in financial contexts, leading to a semantic link.   

\textbf{Performance of Detecting Placeholder Host}: We observed that the precision of identifying the placeholder host is 96\%. All the 11 false positives are DNS names outputted by the ChatGPT model (Step 3.1.1). For example, ``gg-is-awesome-246.mongodb.net'' is termed a placeholder due to the ``is-awesome'' substring in the DNS name. In addition, our tool shows a recall of 94\% for detecting the placeholder host out of 317 placeholder hosts in RiskBench. Similar to false positives, all the missing placeholder hosts are DNS names.

\subsection{Developer Survey} \label{DeveloperSurvey}

We received 52 responses (10.4\%) out of 500 developers. We now discuss our observations from the responses.

% In the first question, we presented three database secrets and their corresponding severity levels. Secret A was ``Fm)4dj'', while Secret B and C were ``123456''. 

\textbf{Q1: Secret and Severity Information}: We observed that 41 developers (78\% of respondents) wanted to remove Secret A first, terming Secret B and C as placeholder/dummy, supporting our hypothesis. For example, <P23> stated \textit{``A first, then B or C. A appears to have an actual password, whereas B and C are just placeholders.''} In addition, we observed that the severity information did not help the developers. <P13> stated that \textit{``Severity info did not help, needed to look at the secret to determine that B and C are likely fake secret values.''} Additionally, 5 developers were unsure about the order of secret removal due to missing secret contexts, such as the asset information. For example, <P45> stated that \textit{``I have no idea in what order to prioritize. To effectively prioritize, I need to know the context for what these secrets grant access.''} However, 6 developers considered the asset information into account by inspecting the source code that we did not provide.

 % Next, we provided the asset identifier information for each secret where Secret A points to a localhost server and Secret B and C points to public IP addresses.

\textbf{Q2: Additional Asset Information}: We observed that 38 of 41 developers who selected Secret A in Q1 changed their priority after considering the asset identifier information. These developers changed their priority to Secrets B and C even though the secrets looked like placeholders, supporting our hypothesis. <P23> stated that \textit{``Since Secret A coming from localhost, we might not access it directly. But the other two seem on the internet and should be our top priority to address.''} However, 3 developers did not change the priority without providing any reason. Additionally, all the 5 developers who were unsure about which secrets to prioritize in Q1 have used the asset information to make decisions. <P45> stated that \textit{``Based on the added information of ip address of the system secret is used to access, I would deprioritize Secret A compared to the other two, as localhost is more likely to be hardened against outside access.''} Since 6 developers in Q1 already considered the asset information, their priority stayed the same. <P5> stated that \textit{``No, and I detailed in my previous explanation since I already took the IPs into account.''}


% Next, we provided the security risk score for the secret-asset pairs with the value of asset and ease of attack factors. Secret A protects blockchain data (high value) in a localhost server. Secret B protects video metadata (low value) in a public IP address that is not scannable. Secret C protects the database of email addresses and phone numbers (moderate value) in a scannable public IP address, but database port 3306 is not open. 


\textbf{Q3: Additional Security Risk Score Information}: We observed that 86\% (45 out 52) of the respondents changed their priority to Secret C, A, and B based on the descending security risk score, thus supporting our hypothesis. <P9> stated that \textit{``I would make changes to my prioritization (Secret C, A, B). Secret C has high risk score, personal data exposure, and reachable IP make it most critical to address. While Secret A has lower risk (100), high value of blockchain data means cannot be ignored, even though protected by localhost.''} Additionally, developers pointed out that they had not considered the value of asset information before. <P10> stated that ``\textit{I didn't check the Value of Asset. More security is needed for valuable assets.''} However, 4 developers did not change their priority without specifying any reason, and 3 developers wanted more context on the value of asset and ease of attack. 

\textbf{Q4: Feedback on Security Risk Score}: Developers provided feedback on the security score calculation, such as <P3> stating that \textit{``This simple calculation makes sense and is easy to understand.''} Another developer <P11> stated that \textit{``It aligns with some of the ways we do it in my sector (cloud security) at least.''} However, developers also suggested improvements to the security score calculation based on active network analysis. For example, <P40> suggested that \textit{``It would also be important to include deployment information as passive network analysis might not give the real picture.''} In addition, developers suggested accounting for whether the data is encrypted in the value of asset. <P30> stated that \textit{``If it is encrypted, it should be scored less than non-encrypted data.''} Developers also suggested including the attack vectors, such as privileges required and lateral movement, in the ease of attack calculation. <P7> stated that \textit{``attach attack vectors if possible such as privileges required. In general, attack vector score would change the prioritization.''} 


 % Another developer <P33> stated that \textit{``Yes, the additional information helps us to reassess the priority. We can make informed decisions on the order in which we want to tackle this.''}. 
