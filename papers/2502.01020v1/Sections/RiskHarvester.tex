% We utilized the identified value of asset and ease of attack patterns (Section~\ref{AssetBenchPatterns}) and constructed RiskHarvester to calculate the security risk score. We now discuss the six-step process of constructing RiskHarvester.

We calculated the security risk score as the product of value of asset and ease of attack for each secret-asset pair (Equation~\ref{eqriskscore}). We utilized the identified value of asset and ease of attack patterns (Section~\ref{Patterns}) and constructed RiskHarvester to calculate the security risk score. We now discuss the four-step process of constructing RiskHarvester.



\subsection{Step 1: Identifying Secret-Asset Pairs}

Before we identify the value of asset and ease of attack of secret-asset pairs, we used the implementation source code of AssetHarvester~\cite{assetharvester}, an open-source static analysis tool, to detect secret-asset pairs in a repository (Steps 1.1-1.3). AssetHarvester demonstrates precision of (97\%), recall (90\%), and F1-score (94\%) in detecting secret-asset pairs in RiskBench. We extended AssetHarvester as RiskHarvester to calculate the security risk score for each secret-asset pair (Steps 2-4).

\textbf{Step 1.1 Pattern Matching}: In the source code, a secret and the corresponding asset can be present in database connection strings that follow a specific format for different database types. For example, MySQL, PostgreSQL, and MongoDB follow the same connection string format ([scheme://][user:password@]host:port/db). Thus, regular expressions (regex) are formulated to identify the connection strings by manually analyzing the database documentation. In addition, the capturing group~\cite{capturinggroup} feature of the regex is utilized to isolate the secret and the corresponding asset (host, port, and db name) from the connection string. Table~\ref{regex-database-appendix} in the Appendix presents the regexes, which are grouped into three groups based on the connection string format similarity. 

\textbf{Step 1.2 Data Flow Analysis}: A secret and the corresponding asset can be defined separately in variables and passed to database driver functions instead of defined in a connection string. For example, Figure~\ref{fig:vpattern1} shows that the database secret and the corresponding asset are passed to the driver functions (lines 1-3). The secret-asset pair is passed to the driver function as positional or keyword arguments~\cite{positionalarg}. A positional argument is passed based on the position in the argument list without specifying the parameter name, whereas a keyword argument is passed by explicitly specifying the parameter name, such as ``password'' or ``host'', without fixed order in the function. Table~\ref{database-driver-types} presents the list of Python database drivers and ORM frameworks with the supported argument type. Since the argument positions and names for a secret-asset pair are known in the driver functions, data flow analysis~\cite{khedker2017data} is leveraged to identify the secret-asset pair by analyzing the data flow graph (DFG). DFG is a directed graph where the secret-asset pair is the source that flows into the driver function arguments, which act as sinks. CodeQL~\cite{codeql}, an open-source source code analysis framework that provides the data flow graph computed from the repository source code, is used for data flow analysis to identify the secret-asset pair.

\input{Tables/DatabaseDrivers}


\textbf{Step 1.3 Fast-Approximation Heuristics}: The data flow analysis may not always be captured when source code has dynamic behavior, such as extensive use of reflection. In such cases, the secret-asset pair can be identified from the neighboring lines in the source code. Secrets are first extracted using two open-source secret detection tools, TruffleHog~\cite{trufflehog} and Gitleaks~\cite{gitleaks}. Next, an IP address or DNS name is searched in the three neighboring lines of the secret. Since multiple assets can be present in the neighboring lines, the prefixes of both the secret and asset variables are matched to find the correct asset. For example, ``mysql'' is the prefix of MySQL database secret (``mysql-password'') and server (``mysql-host'') variables.


% \begin{figure}
%     \includegraphics[width=\columnwidth, frame]{Figures/neighboring_lines.pdf}
%     \caption{Multiple assets can be present in the neighboring lines of a secret.}
%     \label{fig:neighboring-lines}  
% \end{figure}


\subsection{Step 2: Identifying Value of Asset}
In this section, we described the process of extracting database keywords from the source code (Step 2.1) and mapped these identified keywords to sensitive data categories (Step 2.2).

\textbf{Step 2.1 Extracting Database Keywords}: We extracted the database keywords (database, table, and column names) from database drivers and ORM frameworks. Our study included eight SQL and two NoSQL database drivers and three ORM frameworks for extracting database keywords (Table~\ref{database-driver-types}).

\uline{SQL Database Driver Calls}: We observed that the database name and corresponding table and column names are passed to SQL database driver functions (V-Pattern 1). The database name is passed as a positional or keyword argument based on SQL driver types along with the secret, host, and port in the same driver function, such as in the \texttt{pymysql.connect} function (lines 1-4, Figure~\ref{fig:vpattern1}). Thus, we included the database name argument in the data flow analysis of Step 1.2 and identified the database name along with the host and port.

Additionally, we observed that raw SQL queries are passed in query functions other than the ``connect'' function where the secret-asset pair is passed. Figure~\ref{fig:vpattern1} shows a SELECT SQL query is directly passed in the ``execute'' function (lines 7-8) for retrieving the patient information. However, SQL queries can also be defined in separate files such as .sql and .ddl files, which are mostly used for database migration and executed from the source code. However, CodeQL does not support data flow between source codes of multiple file types. Thus, the flow of raw SQL present in the .sql file can not be captured into the Python database driver's ``execute'' function. As a result, we first identified the SQL file name from the file open functions~\cite{python-open} using data flow analysis. Finally, to parse the table and column names from the raw SQL, we used the \texttt{sql\_metadata}~\cite{sql-metadata} package of Python that provides a parser for retrieving table and column names from raw SQLs.


% \begin{figure}
%     \includegraphics[width=\columnwidth, frame]{Figures/migration_sql.pdf}
%     \caption{Raw SQL is read from an external file and executed.}
%     \label{fig:external-sql}  
% \end{figure}


\uline{NoSQL Database Driver Calls}: We observed that the database name, corresponding table, and column names are passed in the NoSQL database drivers (V-Pattern 2) for non-relational databases. However, unlike SQL database drivers, database and table names are passed as dictionary keys to the connection client and corresponding database instance. Thus, we first located the data flow node in the DFG for the connection client instance (sink) initialized with the secret-asset pair and traced the source that flows into the specific sink to extract the database name. Using the identified database name, we located the data flow node of the corresponding database instance (sink) and repeated the process to identify the table name that flows in the database instance sink. Since the column names are passed as key-value pairs in a dictionary in the driver query function, we first traced the data flow node of the dictionary that flows into the table instance sink. Next, to find the column names, we extracted the keys from the key-value pairs of the dictionary. Finally, the database, table, and column names of non-relational databases are extracted.



\uline{ORM Framework Calls}: From V-Pattern 3, we observed that developers employ ORM frameworks to access relational databases. For ORM framework calls, we found that the database name is passed to the ORM configuration functions as a part of the connection string. Thus, we located the configuration function sink in the DFG and extracted the database name by tracing the flow of the connection string into the sink (similar to Step 1.2). However, ORM abstracts database access through objects instead of raw SQL queries or key-value pairs. The database tables are mapped to model classes, and the columns are mapped to the attributes of the classes. Thus, we first located the ORM class that uses the ORM database instance in the DFG. Then, we identified the class source code from the abstract syntax tree and extracted the attribute names of the class. To extract the attribute names, we used Python's \texttt{py\_models\_parser}~\cite{py-models-parser} package, which can parse the model classes and table definitions. Finally, we separated the table and column names from the corresponding attribute names of the ORM class as database keywords.


\textbf{Step 2.2 Mapping Database Keywords to Sensitive Data Categories}: Since each database keyword can have different sensitivity, we mapped each identified keyword to a data category of 113 categories provided by Google Cloud DLP (Section~\ref{RiskBench}). We observed that the Google Cloud DLP provides API to assign a data category to specific instances of the data. For example, instead of the database keyword ``passport'', the API takes a country-specific passport number as input and outputs the mapped data category. However, in our study, we only have the identified database keywords from the source code for secret-asset pairs (Step 2.1). In addition, we observed that the database keywords will not always match the data category name exactly. For example, the database keyword is ``NID\_NUMBER'', which should be assigned to the ``NATIONAL\_ID\_NUMBER'' category. We now discuss the lexical and semantic string similarity algorithms we used to map each database keyword to a data category.

\uline{Prefix Match}: We observed that database keywords match from the start of a data category. For example, the database keyword is ``FINANCIAL\_ACC'', and the corresponding data category is ``FINANCIAL\_ACCOUNT\_NUMBER''. To measure the similarity between these strings, we utilized the Jaro-Winkler algorithm~\cite{winkler1990string} that emphasizes prefix similarity by assigning higher scores to strings that share common prefixes. The algorithm generates a similarity score between 0 and 1, where 0 indicates entirely dissimilar strings, and 1 indicates identical strings. We set a cut-off score of 0.7. To employ the Jaro-Winkler algorithm, we leveraged the \texttt{jaro\_winkler\_similarity} function of Python's \texttt{jellyfish}~\cite{jellyfish-python} package.

\uline{Substring Match}: We observed that database keywords may not have a longer common prefix with a data category. For example, the database keyword ``NID\_NUMBER'' should match the ``NATIONAL\_ID\_NUMBER'' category, though the middle characters are missing in the keyword. To address the scenario, we used the Gestalt pattern matching algorithm~\cite{black2004ratcliff}, which calculates a similarity by identifying the common substring and recursively comparing characters in the unmatched regions on both sides of the longest common string. Thus, we could match the database keyword with the correct category even if the database keyword is incomplete or has missing segments. Like the Jaro-Winkler algorithm, Gestalt provides a similarity score between 0 and 1, and we set a cut-off score of 0.7. We implemented the algorithm using the \texttt{SequenceMatcher} function of Python's \texttt{difflib}~\cite{sequencematcher} package.


\uline{Semantic Match}: We observed that database keywords differ lexically from the correct data category but have the same meaning. For example, the database keyword ``CELL\_NO'' should map to the ``PHONE\_NO'' category due to the same meaning. Thus, we need to calculate semantic similarity between the strings instead of lexical similarity (Prefix and Substring match). For semantic similarity between words, we leveraged fastText~\cite{bojanowski2017enriching}, a natural language processing (NLP) model for generating word embeddings by capturing semantic information. In addition, we observed that the subwords in the database keyword can be the same as the subwords of the data category but present in different orders. For example, despite the subword's order, the database keyword ``DATE\_OF\_BIRTH'' should match the ``BIRTH\_DATE'' category. We chose fastText over other NLP models, such as Word2Vec~\cite{mikolov2013efficient} and GloVe~\cite{pennington2014glove}, since fastText supports out-of-vocabulary word embeddings and is trained with character n-grams. As a result, fastText can be used to capture the similarity of the words with different subword orders. In our study, we used the pre-trained fastText model \texttt{cc.en.300.bin}, trained on Common Crawl and Wikipedia with 5-character n-grams, a window size of 5, and 10 negatives. We used the \texttt{fasttext}~\cite{fasttext} package of Python to access the model and calculate semantic similarity. We set a cut-off similarity of 0.65.

\uline{Non-English \& Transliterated Word Match}: We observed that non-English or transliterated words are present in the source code as database keywords. A transliterated word is a word from one language written in another language's alphabet by representing the pronunciation. For example, the Chinese word ``\begin{CJK}{UTF8}{gbsn}性别\end{CJK}'' or the corresponding transliterated word ``Xìngbié'' is present in the SQL queries. As a result, we first translated the non-English and transliterated words to English words and then computed the lexical and semantic similarity. In our study, we leveraged the Google Cloud's Translation API~\cite{google-cloud-translation-api} using the \texttt{google-cloud-translate} package~\cite{google-cloud-translate}.    

The cut-off similarity scores are chosen by randomly sampling database keywords and observing the score. We assigned a sensitivity level of ``UNSPECIFIED'' when no data category is matched, such as the database keyword ``test''.

\subsection{Step 3: Identifying Ease of Attack}

In this section, we described the process of identifying ease of attack information (Step 3.1) and assigning ease of attack categories based on the identified information (Step 3.2).

\textbf{Step 3.1 Finding Ease of Attack Information}: We identify different ease of attack information from the host and port part of the asset identifier after each step (Steps 3.1.1-3.1.6).

\uline{Step 3.1.1 Valid DNS Name}: From E-Pattern 1, we observed that developers put a DNS name in the host part of the asset identifier as a database server address. However, the DNS name can be invalid according to the DNS name format set by the Internet Engineering Task Force (IETF)~\cite{ietf} through Request for Comments (RFCs)~\cite{rfc}. For example, each segment between dots in the DNS name can have up to 63 characters and should not start or end with a hyphen. In our study, we utilized the \texttt{domain} function of Python's \texttt{validators}~\cite{validators} package to validate the DNS name format. 

Additionally, developers can put a placeholder DNS name in the source code, such as \texttt{"www.example.com"}. However, detecting placeholder DNS names is challenging because the placeholder DNS names can conform to the DNS name format, and no universal registry exists to identify them. We can apply a rule-based approach by analyzing the common placeholder keywords to detect placeholder DNS names. However, the rule-based approach has limitations, such as a lack of adaptability due to a fixed set of keywords to arbitrary DNS names. Additionally, the rule-based approach cannot interpret the meaning behind names. However, we can apply Large Language Models (LLMs) to detect the placeholder DNS names since LLMs excel in semantic understanding and recognizing contextual clues that differentiate actual DNS names from placeholders~\cite{liu2023summary, yang2024harnessing}. While other Generative pre-trained transformer (GPT) style LLMs exist, we leveraged ChatGPT due to ChatGPT's performance in Zero-shot Learning (ZSL) through Chain-of-Thought (CoT) prompting~\cite{yang2024harnessing, wei2022chain, brown2020language}. The ZSL enables models to address unseen tasks without prior training examples, while CoT prompting guides the models through a structured, step-by-step reasoning process to arrive at more accurate answers. In our study, we employed \texttt{gpt-4o-2024-08-06}~\cite{gpt-model} model of ChatGPT with temperature 0.2 to make the model more deterministic and confident. As shown in Table~\ref{dns-prompt} of the Appendix, we provided one example of a placeholder and one example of actual DNS names with the context source code in the CoT system prompt. In the user prompt, we provided the DNS name to be classified as a placeholder or not, along with two neighboring lines of source code for context. Finally, we identified the valid DNS names from the prompt answer, which we passed on to the next step.


\uline{Step 3.1.2 Resolvable DNS Name}: We observed that all valid DNS names may not resolve to IP addresses due to non-existent domains or misconfigured DNS records (E-Pattern 1). Thus, we checked whether the DNS names from Step 3.1.1 are resolvable by querying the DNS servers. We leveraged \texttt{nslookup}~\cite{nslookup}, a BIND name server software member that obtains the mapping between a domain name and IP address. However, we observed that nslookup can return a Canonical Name (CNAME) record when queried for a DNS name. The DNS system allows aliases using CNAME records to simplify domain management, enabling a single canonical domain to represent multiple aliases. Thus, to identify the IP address from the A (IPv4) or AAAA (IPv6) record for the DNS name, we recursively queried using the canonical domain name. In our study, we used Python's \texttt{nslookup}~\cite{python-nslookup} package.

\uline{Step 3.1.3 Valid IP Address}: From E-Pattern 2, we observed that invalid or placeholder IP addresses are present in the source code. To check the validity of the IP address directly present in the host part or the resolved IP address from the DNS name (Step 3.1.2), we used the \texttt{ip\_address} function of \texttt{validators}~\cite{validators} package of Python.

\uline{Step 3.1.4 Routable IP Address}: Since assets with public IP addresses are easier to access by attackers than non-routable addresses such as localhost or private IP addresses (E-Pattern 2), we checked whether the IP addresses from Step 3.1.3 are routable. We used Python's \texttt{ipaddress}~\cite{ipaddress} package that provides functions for detecting the routable addresses.

\uline{Step 3.1.5 Scannable IP Address}: We observed that not all the public IP addresses identified from the source code are scannable (E-Pattern 3). To detect if the IP addresses from Step 3.1.4 are scannable, we did not use \texttt{ping} command since ping uses Internet Control Message Protocol (ICMP) packets that are typically blocked by servers through firewalls. In addition, ping does not provide information on the active services running on the server. Thus, we used Censys Search API~\cite{durumeric2015search}, which uses TCP and UDP packets in the network scan and maintains a database of publicly available information on the active services of a server. Finally, we filtered the scannable IP addresses and detected corresponding active service ports.

\uline{Step 3.1.6 Port Open}: Developers put the database port number in the asset identifier (E-Pattern 4). Thus, we checked whether the port is open for the scannable IP address using the open ports for the scannable IP address found in Step 3.1.5.

\textbf{Step 3.2 Assigning Ease of Attack Category}: From Step 3.1, we observed that at each step, we find new information regarding the ease of attack of the identified asset. Thus, we need to assign an ease of attack category based on the asset information similar to the value of asset to calculate the security risk score (Step 4). To systematically assign an ease of attack category to an asset, we started with a value of 0 for ease of attack. Next, when we retrieve new information after each step, such as if the DNS name is valid (Step 3.1.1), we increment the value for ease of attack by 1. Similarly, if the valid DNS name is resolvable (Step 3.1.2), we increment the value again by 1. In our study, for ease of attack, we assigned four categories (VERY\_DIFFICULT, DIFFICULT, MODERATE, and EASY). The first and second authors of the paper independently inspected the calculated value for ease of attack and assigned a category based on the asset information. The paper's third author, who has over 15 years of experience in network security, resolved the disagreements related to the assigned categories between the first and second authors. Figure~\ref{fig:ease-of-attack} depicts the final categories assigned for ease of attack on an asset at different steps. For example, the ease of attack for an asset is MODERATE if the IP address is scannable, whereas EASY if the database port is open. Finally, we integrated the ease of attack mappings based on host information into RiskHarvester, eliminating manual effort for tool users.

\begin{figure}[!t]
\centering
    \includegraphics[scale=0.58]{Figures/Ease_of_Attack_Flow.pdf}
    \caption{A flow diagram for assigning ease of attack category for an asset identified in the source code.}
    \label{fig:ease-of-attack}  
\end{figure}


\subsection{Step 4: Calculating Security Risk Score}

We identified the value of asset (Step 2) and ease of attack (Step 3) as ordinal categories. To calculate the security risk score (Equation~\ref{eqriskscore}), we need to perform ordinal scaling~\cite{agresti2012categorical}, which assigns numerical values to the categories while preserving their inherent order. Thus, to assign numerical values, we leveraged Protection Poker~\cite{williams2010protection}, a threat modeling game for security risk quantification. We conducted a Protection Poker session with the first, second, and third authors of the paper. We leveraged the nine values from 1, 2, 3, 5, 8, 13, 20, 40, and 100 used by Protection Poker for estimating the ``value of asset'' and ``ease of attack''. We assigned numerical values to the categories of value of asset and ease of attack after two Protection Poker rounds. For value of asset, the assigned values are HIGH (100), MODERATE (40), LOW (5), and UNSPECIFIED (1). For ease of attack, the assigned values are VERY\_DIFFICULT (1), DIFFICULT (8), MODERATE (40), and EASY (100). This mapping is integrated into RiskHarvester to automatically calculate the security risk score. Finally, we multiplied the value of asset and ease of attack to calculate the security risk score (Equation~\ref{eqriskscore}). For example, if the value of asset is HIGH (100) and the ease of attack is DIFFICULT (8), the security risk score is 800. 



 
