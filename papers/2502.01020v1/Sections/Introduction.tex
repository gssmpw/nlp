In March 2024, GitGuardian reported an increasing trend in the past four years on secrets exposure in GitHub repositories~\cite{gitguardian-secret-sprawl}. In 2023 alone, 12.8 million new secrets were exposed, a four-fold increase compared with 2020. Of the 14.9 million developers who pushed code in GitHub, 1.7 million exposed secrets, such as API keys and database passwords, essential for connecting to external services~\cite{gitguardian-secret-sprawl}. Developers keep hard-coded secrets in application packages and version control systems~\cite{meli2019bad}, leaving sensitive data vulnerable, as seen in Uber's 2022 breach~\cite{uber-breach}, where attackers exploited secrets in a PowerShell script to access Uber's internal tools. 

However, removing all the secrets from software artifacts is not feasible. Rayhanur et al.~\cite{rahman2022secret} found developers ignoring secret detection tool alerts due to false positives, time pressure, and technical challenges. Additionally, the security risks of the secrets are not equal, protecting assets ranging from obsolete databases to sensitive medical data. Thus, secret removal should be prioritized by security risk reduction, which existing secret detection tools do not support. Developers may stop using the tools due to ``alert fatigue''~\cite{alert-fatigue} if the alerts are not efficiently prioritized for secret removal. 

Existing secret detection tools prioritize secret removal based on ``severity'', a rating tied to the secret type~\cite{ggshield-severity}. For example, the same severity rating is assigned to any database secret without considering the protected asset information. However, the value of the asset protected by the secret can vary from a database with mock data to medical data whose breach can incur fines. Similarly, the ease of accessing an asset varies. For example, an asset with a public IP address can be easy for attackers to access. In contrast, an asset with a private IP address or localhost will require attackers to be on the same network or have physical access to the host machine. 

The National Institute of Standards and Technology (NIST) defines the security risk of an entity as a function of the impact of the adverse event and the likelihood of the event by a threat source~\cite{nist-risk}. For a secret, the security risk can be defined as the function of the value of asset and the attacker's ease of accessing the asset protected by the secret. Protection Poker~\cite{williams2010protection}, a threat modeling game, employs the product of relative measures of ``value points'' and ``ease points'' for security risk quantification, such as one requirement being five times easier to attack than another. Similarly, for a secret, the security risk can be defined as the \textit{product of value of asset and ease of attack}. This security risk computation is based upon the hypothesis that attackers are more likely to succeed in attacking assets of high value and that are easier to attack. Table~\ref{protection-poker-relative-estimation} provides an example of security risk computation for three secret-asset pairs. A secret-asset pair consists of a secret, such as a database password, and a protected asset by the secret, such as the database server. Pair 3 is deemed to have the highest security risk because the value of the asset is 40 times more valuable than Pair 1 and 100 times easier to attack than Pair 2. Thus, removing the Pair 3 secret from the source code is of primary importance. We hypothesize that providing a security risk score for each secret can aid developers in prioritizing the secret removal efforts. \textit{The goal of our study is to aid software practitioners in prioritizing secrets removal efforts through our security risk-based tool.}

\input{Tables/RelativeSecurityRisk}

In this research, we studied how we can programmatically calculate the security risk score by identifying the value of asset and ease of attack for each secret-asset pair and provided answers to our research questions:

\textbf{RQ1:} What performance can be achieved in automatically identifying the value of asset and ease of attack for secret-asset pairs in terms of precision, recall, and F1 score? (Section~\ref{RiskHarvesterResult})

\textbf{RQ2:} Do developers prioritize secret removal based on the descending security risk scores? (Section~\ref{DeveloperSurvey})

We constructed RiskHarvester, a risk-based tool to provide security risk scores of database secret-asset pairs based on the value of asset and ease of attack. We calculated the value of asset by identifying the categories of sensitive data, such as personal information present in the database, from the database keywords (database, table, and column names). We utilized data flow analysis, SQL, and Object Relational Mapper (ORM) parsing to detect the database keywords from the source code. To calculate the ease of attack, we used passive network analysis to retrieve the database host information.

To answer RQ1, we constructed RiskBench, a benchmark of 1,791 database secret-asset pairs from 188 GitHub repositories. We manually inspected each secret-asset pair and included the database keywords, corresponding sensitive data categories, and valid host information in RiskBench. We evaluated RiskHarvester against RiskBench in identifying the database keywords, sensitive data categories, and valid hosts for each secret-asset pair. To answer RQ2, we conducted an online developer survey to understand whether developers use the descending security risk scores to prioritize secret removal from software artifacts. We hypothesize that developers will prioritize secret removal ranked by descending risk scores. We provided a summary of our contributions as follows:


\begin{itemize}
    \item We automatically computed relative security risk scores of checked-in secrets to aid developers in prioritizing secret removal, which existing secret detection tools do not support. Additionally, we reported the developer study findings on the effectiveness of the security risk score in the alerts for secret removal prioritization.
    \item We made the implementation of RiskHarvester publicly available~\cite{riskartifacts}. We also provided RiskBench, a dataset of secret-asset pairs to aid researchers and developers, available via a data protection agreement.
\end{itemize}


Our paper is organized as follows: Section~\ref{Methodology} depicts our research methodology. We discuss the RiskHarvester construction and evaluation results against RiskBench in Sections~\ref{RiskHarvester} and~\ref{Results}, respectively, followed by the implications and limitations of our work. We discuss the related work in Section~\ref{RelatedWork} and conclude in Section~\ref{Conclusion}, followed by ethics considerations and open science policy compliance.
