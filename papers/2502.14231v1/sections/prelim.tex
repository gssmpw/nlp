\section{PRELIMINARIES}

\subsection{Minimum-time planning}

Given the vehicle's current position, $\tilde p_{\text{init}}$, and state, $\mathcal{D\,} \tilde p_{\text{init}}$, (e.g., velocity and acceleration), the minimum-time planning problem involves finding the trajectory that reaches the goal position $\tilde p_{\text{goal}}$ in the least time while satisfying all constraints.
We denote $\mathcal{P}(\mathcal{D\,} \tilde p_{\text{init}})$ as the set of dynamically feasible trajectories starting from the initial vehicle state $\mathcal{D\,} \tilde p_{\text{init}}$. 
These are trajectories the underlying controller can track with limited error.
This problem often includes geometric constraints, $\mathcal{F}$, such as passing through prescribed waypoints or avoiding collisions.
Based on this formulation, we can express the general trajectory generation problem as follows:
\begin{equation}
\begin{aligned}
&\underset{p}{\text{minimize}} \; T \;\;\; \text{subject to} \\
&p(T) = \tilde p_{\text{goal}},\; p(0) = \tilde p_{\text{init}},\; \mathcal{D\,} p(0) = \mathcal{D\,} \tilde p_{\text{init}}, \\
&p \in \mathcal{P}(\mathcal{D\,} \tilde p_{\text{init}}),\; p \in \mathcal{F}
\label{eqn:planning_general}
\end{aligned}
\end{equation}
where $p$ is the trajectory, defined as the set of positions over time, and $T$ is the total time taken to execute the trajectory.
$\mathcal{D\,} p$ denotes higher-order position derivatives such as velocity and acceleration. 
$\mathcal{D\,} p(0) = \mathcal{D\,} \tilde p_{\text{init}}$ sets these derivatives' initial values, defining the system's starting state.


\subsection{Minimum-snap trajectory}

Quadrotor planning often employs snap minimization for trajectory optimization~\cite{mellinger2011minimum, richter2016polynomial}.
The differential flatness property of quadrotor dynamics allows describing the quadrotor's state using position, yaw, and their derivatives. 
This method represents the trajectory as a time-parameterized function of position and yaw. 
The coefficients of the trajectory are determined by minimizing the fourth-order position derivative (snap), yaw acceleration together with the total time as follows:

\begin{equation}
\begin{aligned}
&\underset{p}{\text{minimize}} \; \rho T + \sigma\left(p\right) \;\;\; \text{subject to} \\
&p(T) = \tilde p_{\text{goal}},\; p(0) = \tilde p_{\text{init}},\; \mathcal{D\,} p(0) = \mathcal{D\,} \tilde p_{\text{init}}, \\
&p \in \mathcal{P}(\mathcal{D\,} \tilde p_{\text{init}}),\; p \in \mathcal{F}
\label{eqn:naiveminsnap}
\end{aligned}
\end{equation}
where 
\begin{equation}\label{eqn:smoothness}
\sigma(p)=\int_{0}^{T} \mu_r \norm{\frac{d^4 p_{r}}{d^4 t}}^2 + \mu_\psi \Big(\frac{d^2 p_\psi}{d^2 t}\Big)^2 dt
\end{equation}
with $\mu_r$, $\mu_\psi$ and $\rho$ are weighting parameters.
These polynomials map time to position and yaw, \ie, $p(t) = \begin{bmatrix} {{}p_r(t)}^\intercal &  p_\psi(t) \end{bmatrix}^\intercal$ ($p_r(t) \in \mathcal{C}^4, p_\psi(t) \in \mathcal{C}^2$).
$\mathcal{C}^n$ is the differentiability class where its n-th order derivatives exist and are continuous.
To maintain trajectory differentiability, the initial vehicle state includes velocity, acceleration, jerk, and snap of position, along with yaw rate and yaw acceleration:
$\mathcal{D\,} \tilde p_{\text{init}} = \begin{bmatrix} \tilde p_{\text{init},r}^{(1)} \kern2pt \cdots \kern2pt \tilde p_{\text{init},r}^{(4)} \kern2pt \tilde p_{\text{init},\psi}^{(1)} \kern2pt \tilde p_{\text{init},\psi}^{(2)} \end{bmatrix}$.
The minimization of the fourth-order derivative acts as a regularization, resulting in smoother trajectories with gradual state changes. 
% The smoothness enables finding feasible solutions, i.e. $p \in \mathcal{P}(\mathcal{D\,} \tilde p_{\text{init}})$, by either tuning weights between total time and regularization or using simplified velocity or acceleration constraints, instead of considering complex full dynamics.

% This method handles geometric constraints $\mathcal{F}$, such as waypoint passing and collision avoidance, using piecewise polynomial trajectory representation. 
% For waypoint constraints, each polynomial piece connects consecutive points, ensuring the trajectory passes through all prescribed waypoints while maintaining continuity between segments.
% For collision avoidance, it decomposes free space into convex polytopes and finds a sequence towards the goal using a simple graph search. 
% Then, this method keeps each polynomial piece within its designated convex polytope using linear constraints. 
% This approach avoids direct consideration of geometric constraints during optimization, enhancing computational efficiency.

Optimization variables in trajectory generation include coefficients of polynomial pieces and time allocation for each segment $\mathbf{x} = [x_{1} \cdots x_{m}]$, summing to the total trajectory time, $T = {\textstyle\sum \nolimits}_{j=1}^{m}x_{j}$.
When the time allocation is fixed, the problem simplifies to a quadratic programming (QP) problem for determining coefficients and can be solved quickly using convex optimization solvers.
For instance, in the case of waypoint connecting constraints, the problem can be solved analytically using matrix multiplications as follows:
\begin{equation}\label{eqn:minsnap_1}
\begin{gathered}
\chi(\mathbf{x}, \tilde{\mathbf{ p}}, \mathcal{D\,} \tilde p_{\text{init}}) = \underset{p=[p_r, p_\psi]}{\text{argmin}} \;\;\; \sigma\left(p\right) \;\; \text{subject to}\\
p(T) = \tilde p_{\text{goal}},\; p(0) = \tilde p_{\text{init}},\; \mathcal{D\,} p(0) = \mathcal{D\,} \tilde p_{\text{init}},\; p \in \mathcal{F}
\end{gathered}
\end{equation}
where inclusion in $\mathcal{F}$ constrains the trajectory to connect prescribed waypoints $\tilde p^1\, \dots\, \tilde p^{m-1}$:
\begin{equation}
    \mathcal{F} = \{p\;|\;p({\textstyle\sum \nolimits}_{j=1}^{i}x_{j}) = \tilde{p}^i, \; i=1,\;\dots,\;m-1\},
\end{equation}
% and $\tilde{ \mathbf{p}}$ is the set of all end-point constraints, including waypoints and the start and goal positions.
and $\tilde{ \mathbf{p}}$ is the set of all position constraints, including intermediate waypoints and the start and goal positions.
Based on this property, the snap minimization method applies a bi-level optimization approach: QP for coefficients and nonlinear optimization for time allocation. 
The nonlinear optimization determines the time allocation that minimizes the output of the QP:
\begin{equation}\label{eqn:minsnap_2}
    \begin{gathered}
\underset{\mathbf{x} \in \realR^m_{> 0}}{\text{minimize}} \;\;\; \sigma\left(\chi(\mathbf{x}, \tilde{\mathbf{ p}}, \mathcal{D\,} \tilde p_{\text{init}})\right) \\
\text{subject to}\;\; \chi(\mathbf{x}, \tilde{\mathbf{ p}}, \mathcal{D\,} \tilde p_{\text{init}}) \in \mathcal{P}(\mathcal{D\,} \tilde p_{\text{init}})
\end{gathered}
\end{equation}
This bi-level structure decomposes the high-dimensional optimization into two subproblems, improving numerical stability.

\subsection{Learning-based method}

% Recent research has focused on accelerating this process by replacing the computationally expensive non-linear optimization in the outer loop with neural network inference. 
% This approach involves training a neural network to output optimal time allocations based on input conditions and constraints. 
% The trained network can then quickly infer time allocations, significantly reducing computation time while maintaining near-optimal performance. 
% This neural network-based approach enables faster trajectory generation, making it more suitable for real-time planning and replanning scenarios in dynamic environments.
Recent research accelerates trajectory generation by replacing costly nonlinear optimization with neural network inference. 
This approach trains a network to output optimal time allocations based on input conditions and constraints, enabling faster planning suitable for real-time scenarios in dynamic environments. 
In this work, we utilize the planning policy proposed by \cite{ryou2024multi}. 
As illustrated in \cref{fig:policy}, this method trains a sequence-to-sequence neural network model to output time allocations $\mathbf{x} = \pi (\tilde{\mathbf{p}}, \mathcal{D\,} \tilde p_{\text{init}})$ for given waypoint sequences $\tilde{\mathbf{p}}$ and initial conditions $\mathcal{D\,}\tilde{p}_{\text{init}}$.
The training process begins by generating a dataset through solving the nonlinear optimization problem \eqref{eqn:minsnap_2} for randomly sampled waypoint sequences.
The planning policy is then trained to imitate the outputs of this nonlinear optimization. 
Polynomial trajectories are subsequently obtained by solving QP in \eqref{eqn:minsnap_1} using the generated time allocations, which are then used in the tracking controller.

Building upon this pretrained model, the method further enhances the training policy using multi-fidelity reinforcement learning (MFRL). 
It incorporates a Gaussian process classifier trained to predict whether the polynomial trajectories generated from the policy output are trackable by the controller. 
The modeling efficiency is improved by augmenting the training dataset with a simple dynamics model using a multi-fidelity Gaussian process kernel. 
This feasibility prediction, $\mathbb{P} (\cdot|\mathbf{x}, \tilde{\mathbf{p}}, \mathcal{D\,} \tilde p_{\text{init}})$, serves as a reward signal, calculated as the product of relative time reduction and feasibility probability:
\begin{equation}
    r(\mathbf{x}) = \mathbb{P}(\cdot|\mathbf{x}, \tilde{\mathbf{p}}, \mathcal{D\,} \tilde p_{\text{init}})\; \delta_{\text{time}}(\mathbf{x})
\end{equation}
where the relative time reduction $\delta_{time}$ is obtained by comparing the time allocation $\mathbf{x}_{t}$ with the minimium-snap time allocation $\mathbf{x}^{\text{MS}}$: 
\begin{align}
\delta_{\text{time}}(\mathbf{x}) &= 1 - {\sum \nolimits}_{i=1}^m x_{i} / {\sum \nolimits}_{i=1}^m x^{\text{MS}}_{i}.
\end{align}
As illustrated in \cref{fig:policy_res}, the pretrained model generates time allocations similar to those of the optimal minimum snap trajectories. 
The reinforcement learning (MFRL) policy outputs faster time allocations by modeling a feasibility boundary.

\begin{figure}[]
\centering
\begin{subfigure}[b]{0.23\textwidth}
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth,trim=0.cm -0.5cm 0.cm 0.cm,clip]{figures/policy_output_2.png}
    \vspace{-1\baselineskip}
    \caption{}
    \label{fig:policy}
\end{subfigure}
\begin{subfigure}[b]{0.23\textwidth}
    \captionsetup{justification=centering}
    \includegraphics[width=\textwidth,trim=.0cm 0.4cm .0cm .0cm,clip]{figures/policy_training_res.jpg}
    \vspace{-1\baselineskip}
    \caption{}
    \label{fig:policy_res}
\end{subfigure}
\caption{(a) Neural network planning policy consisting of two gated recurrent units, attention, and variational autoencoder. The model outputs time allocations from the sequence of prescribed waypoints. (b) Relative total trajectory time of the planning policy output compared to the minimum snap method. The reinforcement learning (MFRL) model outputs faster time allocations, shifting the output distribution to the left.}
\vspace{-1\baselineskip}
\end{figure}
