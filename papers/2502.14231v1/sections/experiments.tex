\section{EXPERIMENTS}

\begin{figure*}[]
    \centering
    \includegraphics[width=0.80\textwidth,trim=.0cm 0.cm .0cm .0cm,clip]{figures/exp_test_traj.png}
    \caption{Simulation environments and trajectories used for evaluation. 
    Red paths represent the target drone's trajectories, and blue paths represent the defense drone's.
    }
    \label{fig:exp_env}
    \vspace{-1\baselineskip}
\end{figure*}

We evaluate the algorithm in three unique simulated environments, each with 10 different target trajectories, as shown in \cref{fig:exp_env}. 
Environment 1 has dimensions of $10 \si{m} \times 10 \si{m} \times 2 \si{m}$, while environments 2 and 3 each have dimensions of $30 \si{m} \times 15 \si{m} \times 3 \si{m}$. 
The time interval for predictions is set at $dt = 0.1 \si{s}$ for environment 1 and $dt = 0.2 \si{s}$ for environments 2 and 3, reflecting the larger spaces.
The target drone's trajectories are generated using the minimum snap method. 
For each trajectory, we vary the maximum speed of the target drone and the departure time of the defense drone. 
The 10 different maximum speeds range from $0.5 \si{m/s}$ to $5.5 \si{m/s}$, increasing uniformly by $0.5 \si{m/s}$. 
There are also 10 different departure times ranging from $0.5 \si{s}$ to $2.5 \si{s}$. 
Overall, each trajectory is evaluated using 100 different combinations of maximum speed and departure time. 
We add padding of $0.5 \si{m}$ around the obstacles in the occupancy grid, and during the selection of the optimal trajectory in \eqref{eqn:homotopy_1}, we reject trajectories that deviate more than $\delta_\text{max} = 0.5 \si{m}$ from the initial A* path.
All experiments are conducted using the FlightGoggles simulator~\cite{guerra2019flightgoggles} with the INDI trajectory tracking controller~\cite{tal2018accurate}.

% We compare five different setups for prediction models: no prediction, ground truth of the target drone, noisy ground truth (with $\sigma_\text{pred} = 0.01 \si{m}$ and $\sigma_\text{pred} = 0.05 \si{m}$), and a Gaussian Mixture Model (GMM). 
% Initially, we validate the algorithm without any prediction model, where the defense drone continually chases the current position of the target drone. 
% Since this approach does not consider multiple candidate positions, the success rate is typically low. 
% To control for the effects of prediction model uncertainty, we evaluate the algorithm using the ground truth trajectories of the target drone. 
% Subsequently, we introduce noisy ground truth scenarios to compare the resilience of the algorithm under varied levels of accuracy. 
% Here, the performance of the GMM prediction model is observed to lie between the results for noisy ground truth models with $\sigma_\text{pred} = 0.01 \si{m}$ and $\sigma_\text{pred} = 0.05 \si{m}$.
% Finally, the algorithm is tested with the GMM prediction model to demonstrate the full planning pipeline. 
% For both the noisy ground truth and the GMM prediction model, $N_\text{batch} = 20$ positions are sampled at each $N_\text{pred} = 20$ time steps.
% The GMM prediction prediction is derived by marginalizing over $N_\text{obs} = 10$ observed target positions.
% An experiment is marked as successful if the defense drone can approach the target drone within $0.4 \si{m}$ while maintaining a position error under $40 \si{cm}$ and a yaw tracking error less than 45 degrees.

We evaluate the algorithm using five prediction models: no prediction, ground truth, noisy ground truth ($\sigma_\text{pred} = 0.01 \si{m}$ and $\sigma_\text{pred} = 0.05 \si{m}$), and the GMM model. 
Without prediction, the defense drone chases the target's current position, resulting in low success rates. 
Ground truth and noisy ground truth serve to evaluate the algorithm's resilience to different levels of prediction accuracy.
% Ground truth trajectories are used as a baseline to compare against prediction uncertainty. 
% Noisy ground truth scenarios test algorithm resilience under varied accuracy levels. 
The GMM model's performance falls between the noisy ground truth models with $\sigma_\text{pred} = 0.01 \si{m}$ and $\sigma_\text{pred} = 0.05 \si{m}$. 
For noisy ground truth and GMM, we sample $N_\text{batch} = 20$ positions at each $N_\text{pred} = 20$ time steps. 
GMM prediction uses $N_\text{obs} = 10$ observed target positions. 
Success is defined as the defense drone approaching within $0.4 \si{m}$ of the target, maintaining position error under $40 \si{cm}$ and yaw tracking error below 45 degrees.

As shown in \cref{tab:exp_alg_res_sim}, we compare three different planning methods over these prediction models.
The \textit{No Policy} serves as our baseline method, determining time allocation based on the distance between waypoints divided by a maximum velocity of 2.5 m/s, which was optimized for success rate.
% The \textit{No policy} method determines time allocation based on the distance between waypoints divided by a maximum velocity of 2.5 m/s, which has been optimized to maximize the success rate. 
% This serves as our baseline for comparison. 
We also examine a planning policy trained with a minimum snap trajectory (\textit{Pretrained}) and another further refined with reinforcement learning (\textit{MFRL}). 
The main limitation of the \textit{No policy} approach is its inability to generate feasible trajectories. 
By determining time allocations using the trained planning policies, this limitation can be overcome. 
Another common issue occurs when the target drone is too fast for the defense drone to intercept. 
% Figure \ref{fig:ms_mfrl_compare} compares the tracking errors of the pretrained policy and the MFRL policy against each average trajectory time. 
% The x-axis represents the relative trajectory time compared to the unscaled output of the pretrained policy. 
As shown in \cref{fig:policy_res}, the use of reinforcement learning enhances the planning policy's performance, enabling faster maneuvers even with the same tracking error, thus the MFRL policy achieves a higher success rate.
\cref{tab:exp_comp_time} presents the computation times for each procedure. 
The entire process is maintained below 100 ms to ensure suitability for rapid online planning.

% \begin{figure}[]
%     \centering
%     \captionsetup{justification=centering}
%     \includegraphics[width=.48\textwidth,trim=0.15cm 0.15cm 0.15cm .15cm,clip]{figures/plot_data_traj_7_35_t4_v2.pdf}
%     \caption{Trajectory 7 in simulated environment 2.}
%     \label{fig:exp_case_1}
% \end{figure}

\begin{figure}[]
    \centering
    % \captionsetup{justification=centering}
    \includegraphics[width=.46\textwidth,trim=0.15cm 0.15cm 0.15cm .15cm,clip]{figures/plot_data_traj_9_45_t9_v2.png}
    \caption{Optimized defense drone trajectories for each timestep. Trajectory 9 in simulation environment \#3.}
    \label{fig:exp_case_2}
\end{figure}

% \begin{figure*}[]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \captionsetup{justification=centering}
%         \includegraphics[width=\textwidth,trim=0.15cm 0.15cm 0.15cm 0.15cm,clip]{figures/plot_data_traj_7_35_t4_v2.pdf}
%         \caption{}
%         \label{fig:exp_case_1}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \captionsetup{justification=centering}
%         \includegraphics[width=\textwidth,trim=0.15cm 0.15cm 0.15cm .15cm,clip]{figures/plot_data_traj_9_45_t9_v2.pdf}
%         \caption{}
%         \label{fig:exp_case_2}
%     \end{subfigure}
%     \caption{Optimized defense drone trajectories for each timestep. (a) Trajectory 7 in simulated environment 2 (b) Trajectory 9 in simulated environment 3.}
%     \label{fig:exp_case}
% \end{figure*}

\input{tables/res_simple.tex}

\input{tables/comp_time.tex}

% \begin{figure}[]
%     \centering
%     \includegraphics[width=0.48\textwidth,trim=0.2cm 0.5cm 0.2cm 0.2cm,clip]{figures/ms_mfrl_compare.pdf}
%     \caption{Comparison of the pretrained policy and MFRL policy.}
%     \label{fig:ms_mfrl_compare}
% \end{figure}

% Figure \ref{fig:exp_case_1} and \ref{fig:exp_case_2} illustrate the optimized trajectories over time. 
% In both cases, the target drone's positions are predicted using the GMM. 
% In Figure \ref{fig:exp_case_1}, the defense drone initially fails to intercept the target drone and explores alternative paths before successfully finding and capturing it. 
% This demonstrates the algorithm's ability to adapt and recover from initial failures caused by prediction inaccuracy, showcasing its robustness in uncertain environments.
% In Figure \ref{fig:exp_case_2}, the defense drone identifies a feasible trajectory that avoids turning around the left bottom pillar and successfully reaches the target drone.
% The algorithm dynamically updates candidate goal positions based on the target drone's movements, concurrently optimizing trajectories towards these positions and selecting the optimal trajectory.

\cref{fig:exp_case_2} illustrates the optimized trajectories over time, where the defense drone identifies a feasible trajectory that avoids turning around the left bottom pillar and successfully reaches the target drone.
The target drone's positions are predicted using the GMM.
The algorithm dynamically updates candidate goal positions based on the target drone's movements, concurrently optimizing trajectories towards these positions and selecting the optimal trajectory.

\begin{figure}[]
    \centering
    \includegraphics[width=0.4\textwidth,trim=0.15cm 0.15cm 0.15cm 0.15cm,clip]{figures/homotopy_real_2_1.jpg}
    \caption{Real-world flight experiments demonstrating real-time sampling-based online planning for drone interception. 
    Trajectory 1 from simulation environment \#1 is used for the target drone's path. 
    The defense drone intercepts the target drone after 4.9 seconds.}
    \label{fig:exp_real}
    \vspace{-1.3\baselineskip}
\end{figure}

We further evaluate the system in real-world environments shown in \cref{fig:exp_real} using an augmented reality setup.
% In these tests, we operate the target drone in one room and the defense drone in another. 
% The positions of both drones, estimated using motion capture camera, were rendered in the same simulated environment, allowing us to simulate the drone interception scenario without actual collisions. 
The target and defense drones operate in separate rooms, with their motion-captured positions rendered in a shared simulated environment to test interception scenarios safely.
The target drone's positions are predicted using GMM based on motion capture data. 
The planning policy is executed on the Titan Xp GPU in the host computer and communicates with the vehicle's microcontroller to update the trajectory.
We evaluate Trajectories 1 and 2 from the simulated environment 1, in real-world flight experiments. 
In both cases, the planner updates the trajectory at around 10 Hz and successfully intercepts the target drone.
A video of the experiments can be found at~\url{https://youtu.be/dDdshfEAZpg}.
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \captionsetup{justification=centering}
%         \includegraphics[width=\textwidth,trim=0.15cm 0.15cm 0.15cm 0.15cm,clip]{figures/homotopy_real_1.jpg}
%         \caption{}
%     \end{subfigure}
%     \begin{subfigure}[b]{0.48\textwidth}
%         \captionsetup{justification=centering}
%         \includegraphics[width=\textwidth,trim=0.15cm 0.15cm 0.15cm .15cm,clip]{figures/homotopy_real_2_1.jpg}
%         \caption{}
%     \end{subfigure}
%     \caption{Real-world flight experiments demonstrating real-time sampling-based online planning for drone interception. Trajectory 1 from simulated environment 1 is used for the target drone's path. (a) Initial positions of target and defense drones. (b) Defense drone intercepts the target drone after 4.9 seconds.}
%     \label{fig:exp_real}
% \end{figure*}
