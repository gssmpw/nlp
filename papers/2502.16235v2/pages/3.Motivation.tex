% \section{The Rationale of DPTS}
\section{Method Rationale}

\label{sec:motivation}

% \ycj{Rationale?}
% \ycj{Pre-analysis/Motivation and Problem Definition/Motivation and Challenge Analysis/Pilot Study?}
 
% \ycj{Add a short overview statement here.}


{
In this section, we present empirical findings that highlight the key challenges of tree search in LLM and provide the rationale behind our proposed DPTS. 
% First, the inherent sequential nature of tree search complicates parallel execution, leading to irregular node expansions and varying path lengths. Second, excessive exploitation of low-confidence paths wastes computational resources, suggesting that a confidence-based pruning strategy could mitigate this inefficiency. Finally, tree search methods that prioritize breadth often suffer from frequent switching between paths, hindering deep exploitation and resulting in token and expansion redundancy. Addressing these challenges can significantly enhance the efficiency and effectiveness of tree search methods.
}
First, the frequent switch between paths complicates parallel execution and causes shallow thinking, disrupting the model’s ability to engage in efficient deep reasoning (Sec.~\ref{sec:3.1}). 
% sequential nature of tree search complicates parallel execution, leading to irregular node expansions and varying path lengths. Existing tree search algorithms frequently switch between paths, causing shallow thinking and disrupting the model’s ability to engage in deep reasoning, ultimately degrading generation quality. 
Second, excessive exploitation of low-confidence paths results in redundant rollouts and wastes effort on fewer possible candidates (Sec.~\ref{sec:3.2}). 
% consuming unnecessary computational resources. Without an effective pruning strategy,..., reducing overall efficiency and slowing down search convergence.

% \subsection{Unpredictable Growth Behavior}
\subsection{Frequent Switching}
\label{sec:3.1}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.93\linewidth]
    {figs/draw_switch_times.pdf}
    \vspace{-0.1in}
    \caption{Statistics for switch from the best path to the suboptimal (blue), and total switch (green).} 
    \vspace{-0.1in}
    \label{fig:motivation_switch_path}
\end{figure}
% \ycj{Analysis of why vanilla solution doesn't work well?}

% Tree search has become one of the key paradigms for enabling deep reasoning in large language models. However, the inherent sequential nature of tree structures presents significant challenges for GPU parallelism. Regardless of the search algorithm employed, the hierarchical and distributed nature of tree-structured data introduces intrinsic difficulties in memory management—whether using a contiguous memory pool or fragmented memory blocks, the irregularity of tree search remains unavoidable.  % 这一段说的 tree-structured data 难以 parallel

Tree search inherently exhibits retrospective and recursive behaviors, making efficient parallel execution difficult. Even if each node is constrained to generate the same number of tokens, the focus switching between different reasoning trajectories and the diverse path lengths makes it incompatible with the end-to-end parallelism on GPUs. The detailed illustrations for this phenomenon can be found in Appendix~\ref{sec:app:trees}. 

% This behavior demonstrates the fundamental characteristics of tree search that make the efficient parallelization a challenging problem: \textit{diverse path lengths, varying child depths, irregular node jumps, and recursive retrospective exploitation}. 

The focus switching between paths also makes the tree search fail in focused reasoning trajectory~\cite{wang2025thoughts}, which prevents deep thinking and leads to a tendency of shallow exploitation. We quantify the switch times of the reasoning focus on each sample in the Math500 dataset. Figure~\ref{fig:motivation_switch_path} counts the total switch, which is about 35 on average. As well as the switch from the best path to a suboptimal or incorrect one, which is up to 3 times for a single sample. It demonstrates the instability of the tree search algorithm in maintaining a focused reasoning trajectory. 


% 下面这段我放 appendix 了
\jwt{
% Tree search is key for enabling deep reasoning in large language models, but its sequential nature presents challenges for efficient GPU parallelism. Tree-structured data introduces difficulties in memory management, with irregular patterns in node expansions and varying path lengths. This leads to difficulties in maintaining parallelism, as tree search is inherently recursive and retrospective, causing inefficient execution when different paths vary in depth and termination points.

% Figure~\ref{fig:motivation_dfs_trees} visualizes depth-first search (DFS) trees, highlighting the irregular expansion process. Darker nodes represent high-confidence paths, while lighter nodes indicate lower-confidence ones. The figure demonstrates that tree search does not follow a predictable spatial or hierarchical pattern, resulting in diverse path lengths, varying child depths, and irregular node jumps. These behaviors complicate parallel execution and efficient resource utilization.

% To address these challenges, it is crucial to focus on improving the handling of irregular growth in tree structures. By introducing strategies that can better manage the dynamic nature of tree search, we can optimize memory usage and reduce the inherent complexity, enabling more effective parallelization.

}


% These findings highlight a critical shortcoming of BFS: frequent switching between different reasoning paths prevents deep exploitation, leading to significant computational redundancy. This behavior results in excessive token generation, unnecessary expansions on suboptimal paths, and instability in maintaining a coherent line of reasoning—factors that ultimately hinder search efficiency and inference speed.

% 下面这段我放 appendix 了
% \jwt{
%  Tree search focusing on width expansion explores a wide range of paths but suffers from frequent switching between them, preventing deep reasoning and leading to shallow exploitation. This behavior causes two inefficiencies: incomplete reasoning and excessive expansions. These algorithms often generates more tokens and expansions than necessary, exploring many suboptimal paths before finding the best one, which results in significant computational redundancy.

%  Figure~\ref{fig:motivation_bfs_trees} shows how BFS expands in a flat, top-down manner, leading to shallow exploitation. Figure~\ref{fig:motivation_waste_tokens}(left) compares the total tokens generated (blue line) to those required for the best path (yellow line), revealing excessive token redundancy. Figure~\ref{fig:motivation_waste_tokens}(right) highlights unnecessary node expansions, where many explored nodes do not contribute to the final solution. Finally, Figure~\ref{fig:motivation_switch_path} analyzes node-switching frequency, showing that BFS frequently shifts between optimal and suboptimal paths, leading to instability in reasoning.

%  The high frequency of path-switching can be mitigated by introducing mechanisms that help the model maintain focus on the most promising paths.
% }



\subsection{Redundant Exploration}
\label{sec:3.2}


The lack of early termination in existing tree search algorithms leads to excessive exploitation and redundant searching. Observations in Figure~\ref{fig:motivation_low_confidence} show that low-confidence nodes rarely contribute to the best solutions, either terminated with suboptimal results (yellow) or failing to be the first to reach the best path (orange). The average probability of the suboptimal results brought by low confidence is 91.3\%, while the probability of those nodes being the earliest best path is only 6.2\%. It suggests that low-confidence nodes have little potential to reach the best solution, it is even hard to be the first one. It means that most low-confidence nodes have less contribution to the final results but waste computational resources. 
% We reorder the nodes based on the former probability for clearer illustration (the plot with original order is showcased in Appendix~\ref{app:sec:low_reward_original}). 

% One of the fundamental inefficiencies in traditional tree search lies in its inability to terminate early when exploring suboptimal paths. In most cases, a search path is only abandoned when it reaches the termination condition, regardless of whether it is already apparent that the path is unlikely to yield an optimal solution. This behavior can lead to substantial computational redundancy, as a large number of unnecessary expansions and token generations are performed on low-confidence nodes. 


% To quantify the extent of this redundancy, we conducted an observational experiment. In Figure~\ref{fig:motivation_low_confidence}, we analyze the probability that continuing the rollout from a low-confidence node leads to less contribution. Here, we define a node as low confidence if its score is lower than the average confidence of previously visited nodes (refer to Eq. (\ref{eq:theta}))—a deliberately aggressive threshold since such occurrences are relatively frequent (highlighted in yellow). And we also reorder the nodes based this probability for clearer illustration (the plot with original order is showcased in Appendix~\ref{app:sec:low_reward_original}). However, as our observations indicate, the probability that these low-confidence nodes ultimately contribute to the optimal path is extremely low (highlighted in orange). Additionally, in most cases where the optimal path is eventually reached, it is not the first time an optimal solution is discovered (highlighted in blue), meaning that a higher-confidence node had already identified the correct path earlier. 

% Between the yellow and orange regions lies an additional scenario: the probability that continuing a rollout from a low-confidence node either fails to generate an answer at termination or produces an incorrect answer. This category accounts for the majority of cases, further reinforcing the inefficiency of expanding low-confidence paths.

% Based on these observations, it suggest that a node’s prior confidence may serve as a reasonable predictor of whether the search path will ultimately lead to a valid solution. While this does not guarantee a perfect pruning strategy, it indicates that integrating confidence-based heuristics could significantly reduce unnecessary rollouts, improving the overall efficiency of tree search methods.

% 合并到上面的黑色文本里了
\jwt{
% A major inefficiency in traditional tree search is the lack of early termination when exploring low-confidence paths. This results in wasted computational resources, as the algorithm continues expanding these paths even when their confidence scores remain low. Observations show that low-confidence nodes rarely contribute to optimal solutions, suggesting that confidence-based pruning can reduce unnecessary exploitation and improve efficiency.

% Figure~\ref{fig:motivation_dfs_trees} (Tree 2) illustrates the issue, showing that low-confidence nodes (rightmost branches) are expanded despite their low likelihood of contributing to the final answer. Figure~\ref{fig:motivation_low_confidence} further quantifies this inefficiency: yellow regions indicate frequent occurrences of low-confidence nodes, while orange regions show that they rarely lead to optimal solutions. The blue regions reveal that even when the best path is found, a higher-confidence node had typically identified it earlier, confirming the redundancy of expanding low-confidence nodes.

% These findings emphasize the importance of selectively pruning low-confidence paths early in the search process. By incorporating mechanisms that assess the likelihood of a node contributing to the optimal solution, unnecessary expansions can be avoided, leading to a significant reduction in computational overhead.
}

\begin{figure}[t]
    % \centering
    \includegraphics[width=0.85\linewidth]{figs/low_reward_original.pdf}
    \vspace{-0.1in}
    \caption{Probabilities with reordered samples of those have prior confidence below $\theta_{es}(\lambda=1)$ in Eq.~\ref{eq:theta} and do not terminate with the highest reward score (yellow), and paths that are not the earliest best path (orange), which means there is already at least one path that has terminated with the same reward score. }
    \vspace{-0.2in}
    \label{fig:motivation_low_confidence}
\end{figure}

These findings emphasize the importance of maintaining the focus on deep reasoning and pruning low-confidence paths for efficient inference. 

