
\section{Experiments}
\label{sec:exp}
We conduct extensive experiments to evaluate the efficiency of the DPTS framework. We benchmark its performance against various search algorithms across multiple models and datasets to ensure a comprehensive analysis.


\subsection{Settings}
\label{sec:exp_setting}

\paragraph{Models.} We include Qwen-2.5-1.5B-Instruct, Qwen-2.5-7B-Instruct~\cite{yang2024qwen2}, LLaMA-3.1-8B-Instruct, and LLaMA-3.2-3B-Instruct~\cite{touvron2023llama} to cover various model sizes.  

\paragraph{Datasets.} The evaluation datasets include Math500~\cite{hendrycks2021measuring} and GSM8K~\cite{cobbe2021training}, both are widely used for reasoning and mathematical problem-solving tasks. We implement the evaluation by referring the approaches used in Qwen-2.5-Math~\cite{yang2024qwen2}, supplemented in our submission. 

\paragraph{Comparison Methods.} We compare DPTS against three widely used search algorithms: Monte Carlo Tree Search~(MCTS)~\cite{sprueill2023monte}
% balances exploration and exploitation when sampling, while the selected paths rollout till termination
, Best-of-N~\cite{cobbe2021training}
% performs multiple independent rollouts and selects the highest-scoring output
, Beam Search~\cite{Yao_2023_Tree}.
% expands multiple hypotheses in parallel, pruning low-scoring candidates at each step to maintain a fixed-width search~\cite{snell2024scaling}. 
Since efficient tree search algorithms have recently regained attention after the emergence of LLM reasoning, the strong baselines are limited. As a result, we primarily compare DPTS against these typical and well-established search algorithms to demonstrate the effectiveness of our method. An introduction of the comparison methods and other details about experimental settings are provided in Appendix~\ref{app:sec:exp}.

% 实验参数： tree_width:4 tree_depth:16 mini_step:128 % 其实 103 确实实验结果更好不知道为什么 otz
% max token:2048 mcts_max_time:120 

\subsection{Comparisons on Search Algorithms}


% \begin{table}
%     \centering
%     \caption{Comparisons across existing search algorithms on LLM reasoning tasks. }
%     \label{tab:comparisons}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{cllrlr}
%     \toprule
%     \multirow{2}{*}{\textbf{Model}}  & \multirow{2}{*}{\textbf{Algo.}}  & \multicolumn{2}{c}{\textbf{Math500}} & \multicolumn{2}{c}{\textbf{GSM8K}}  \\ 
%     & &  \textbf{Acc.} & \textbf{Time (s)} & \textbf{Acc.} & \textbf{Time (s)}  \\ \midrule
%     \multirow{4}{*}{\begin{tabular}{c} Qwen2.5\\1.5B\end{tabular}} 
%     & MCTS & 56.6 & 117.37 & 75.1 & 73.28 \\ 
%     ~ & Best-of-N & 52.6 & 89.87 & 70.1 & 33.37  \\ 
%     ~ & Beam & 52.4 & 104.58 & 71.5 & 41.27 \\ 
%     ~ & \cellcolor[gray]{0.9}{DPTS} & \cellcolor[gray]{0.9}{59.2} & \cellcolor[gray]{0.9}{\textbf{45.10}} & \cellcolor[gray]{0.9}{75.2} & \cellcolor[gray]{0.9}{\textbf{18.32}}  \\ \midrule
    
%     \multirow{4}{*}{\begin{tabular}{c} Qwen2.5\\7B\end{tabular}}  
%     & MCTS & 75.2 & 121.46 & 89.6 & 79.68 \\ 
%     ~ & Best-of-N & 71.6 & 91.29 & 88.2 & 34.89 \\ 
%     ~ & Beam & 72.4 & 106.89 & 86.7 & 36.49 \\ 
%     ~ & \cellcolor[gray]{0.9}{DPTS} & \cellcolor[gray]{0.9}{76.2} & \cellcolor[gray]{0.9}{\textbf{53.50}} & \cellcolor[gray]{0.9}{89.4} & \cellcolor[gray]{0.9}{\textbf{19.95}} \\ \midrule
    
%     \multirow{4}{*}{\begin{tabular}{c} Llama3\\3B\end{tabular}}  
%     & MCTS & 48.6 & 111.80 & 64.0 & 57.19  \\ 
%     ~ & Best-of-N & 46.4 & 91.34 & 57.1 & 27.27 \\ 
%     ~ & Beam & 45.2 & 104.36 & 58.4 & 28.27 \\ 
%     ~ & \cellcolor[gray]{0.9}{DPTS} & \cellcolor[gray]{0.9}{50.8} & \cellcolor[gray]{0.9}{\textbf{47.75}} & \cellcolor[gray]{0.9}{67.8} & \cellcolor[gray]{0.9}{\textbf{27.74}} \\ \midrule
    
%     \multirow{4}{*}{\begin{tabular}{c} Llama3\\8B\end{tabular}}  
%     & MCTS & 54.2 & 143.36 & 69.5 & 69.74 \\ 
%     ~ & Best-of-N & 49.8 & 122.63 & 67.6 & 33.48  \\ 
%     ~ & Beam & 49.6 & 142.21 & 68.3 & 34.51 \\ 
%     ~ & \cellcolor[gray]{0.9}{DPTS} & \cellcolor[gray]{0.9}{55.4} & \cellcolor[gray]{0.9}{\textbf{37.98}} & \cellcolor[gray]{0.9}{68.2} & \cellcolor[gray]{0.9}{\textbf{17.82}} \\ \bottomrule
%     \end{tabular}
% }
% \end{table}

\begin{table}[ht]
    \centering
    \footnotesize
    \caption{Comparisons across existing search algorithms on LLM reasoning tasks.}
    \vspace{-0.1in}
    \label{tab:comparisons}
    \begin{tabular}{@{}c@{\hskip 4pt} l@{\hskip 6pt} c@{\hskip 6pt} r c@{\hskip 6pt} r}
    % \begin{tabular}{@{}c@{\hskip 4pt} l@{\hskip 6pt} c@{\hskip 6pt} r r@{\hskip 6pt} r@{}}
        \toprule
        \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Algo.}} & \multicolumn{2}{c}{\textbf{Math500}} & \multicolumn{2}{c}{\textbf{GSM8K}} \\
        & & \textbf{Acc.} & \textbf{Time (s)} & \textbf{Acc.} & \textbf{Time (s)} \\
        \midrule
        \multirow{4}{*}{\begin{tabular}{c} Qwen-2.5 \\ 1.5B \end{tabular}} & MCTS & 56.6 & 117.37 & 75.1 & 73.28 \\
        & Best-of-N & 52.6 & 89.87 & 70.1 & 33.37 \\
        & Beam & 52.4 & 104.58 & 71.5 & 41.27 \\
        & \cellcolor[HTML]{D3D3D3} \textbf{DPTS} & \cellcolor[HTML]{D3D3D3} 59.2 & \cellcolor[HTML]{D3D3D3} \textbf{45.10} & \cellcolor[HTML]{D3D3D3} 75.2 & \cellcolor[HTML]{D3D3D3} \textbf{18.32} \\
        \midrule
        \multirow{4}{*}{\begin{tabular}{c} Qwen-2.5 \\ 7B \end{tabular}} & MCTS & 75.2 & 121.46 & 89.6 & 79.68 \\
        & Best-of-N & 71.6 & 91.29 & 88.2 & 34.89 \\
        & Beam & 72.4 & 106.89 & 86.7 & 36.49 \\
        & \cellcolor[HTML]{D3D3D3} \textbf{DPTS} & \cellcolor[HTML]{D3D3D3} 76.2 & \cellcolor[HTML]{D3D3D3} \textbf{53.50} & \cellcolor[HTML]{D3D3D3} 89.4 & \cellcolor[HTML]{D3D3D3} \textbf{19.95} \\
        \midrule
        \multirow{4}{*}{\begin{tabular}{c} Llama-3 \\ 3B \end{tabular}} & MCTS & 48.6 & 111.80 & 64.0 & 57.19 \\
        & Best-of-N & 46.4 & 91.34 & 57.1 & 27.27 \\
        & Beam & 45.2 & 104.36 & 58.4 & 28.27 \\
        & \cellcolor[HTML]{D3D3D3} \textbf{DPTS} & \cellcolor[HTML]{D3D3D3} 50.8 & \cellcolor[HTML]{D3D3D3} \textbf{47.75} & \cellcolor[HTML]{D3D3D3} 67.8 & \cellcolor[HTML]{D3D3D3} \textbf{27.74} \\
        \midrule
        \multirow{4}{*}{\begin{tabular}{c} Llama-3 \\ 8B \end{tabular}} & MCTS & 54.2 & 143.36 & 69.5 & 69.74 \\
        & Best-of-N & 49.8 & 122.63 & 67.6 & 33.48 \\
        & Beam & 49.6 & 142.21 & 68.3 & 34.51 \\
        & \cellcolor[HTML]{D3D3D3} \textbf{DPTS} & \cellcolor[HTML]{D3D3D3} 55.4 & \cellcolor[HTML]{D3D3D3} \textbf{37.98} & \cellcolor[HTML]{D3D3D3} 68.2 & \cellcolor[HTML]{D3D3D3} \textbf{17.82} \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2in}
\end{table}

% \vspace{-20pt}

We conduct a comprehensive comparison across different search algorithms on various models and sizes. We emphasize the search efficiency of our method while maintaining accuracy. 

For efficiency, results in Table~\ref{tab:comparisons} show that DPTS significantly reduces inference time compared to other search methods across various models and datasets, demonstrating superior efficiency. 
On Math500, DPTS achieves the lowest inference time across all models. Particularly, in Qwen-2.5, DPTS reduces the search time from 117.37s (MCTS) to 45.10s in the 1.5B model, achieving nearly a $2.6\times$ speedup, and reduces from 121.46s (MCTS) to 53.50s in 7B model, accelerating nearly $2.2\times$. 
The impact is even more pronounced on the GSM8K, where DPTS achieves a $3.9\times$ speedup from 79.68s to 19.95s in Qwen-2.5-7B. And DPTS even only requires 17.82s for each sample using Llama-3-8B, also $3.9\times$. It forcefully suggests that DPTS effectively mitigates redundant rollouts and optimizes search efficiency. 
% Moreover, while Best-of-N and Beam search show competitive efficiency, they often sacrifice accuracy, whereas DPTS balances both accuracy and computational effectiveness. 
We highlight that, especially on the more challenging tasks, the Early Stop plays a crucial role. Without it, trees often run till timeout on Math500, significantly increasing inference time. In contrast, our approach allows the search tree to terminate earlier within a limited number of expansions, effectively reducing computation time. 
% On simpler datasets, termination primarily occurs when all paths output an end token, making early stop triggers less frequent. However, our method still maintains a clear efficiency advantage overall, demonstrating superior computational efficiency across different levels of task complexity.

For accuracy, DPTS maintains the searching quality and even outperforms the existing algorithms with half or even less of the reasoning time. 
On the Math500 dataset, DPTS achieves the highest accuracy in all experiment cases, surpassing MCTS, Best-of-N, and Beam search. Notably, for Qwen-2.5-1.5B, DPTS improves accuracy from 56.6\% (MCTS) to 59.2\%. 
A similar trend is observed on GSM8K, where DPTS either matches or slightly improves accuracy over MCTS, and surpasses Best-of-N and Beam Search by a wide margin. On Llama-3-3B, DPTS has 67.8\% accuracy, outperforming the previous best MCTS by 3.8\% with only 48.5\% time consumption. 
These results highlight that DPTS maintains or even enhances solution quality while significantly improving inference speed, making it a more robust and efficient search algorithm for complex reasoning tasks. 




\begin{table}
    \centering
    \caption{Ablation study of each component in DPTS framework. ``AP'': adaptive parallelism. ``S'': Searching. ``T'': Transition. ``Best Index'': The average index of terminated path leads to the best solution.  }
    \vspace{-0.1in}
    \label{tab:ablation}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{lccrc}
    \toprule
        \textbf{Algo.} & \textbf{${|P|}$} & \textbf{Acc.}  & \textbf{Time (s)} & \textbf{Best Index} \\ \midrule
         Baseline & 1 & 56.6 & 117.37 & 10.45 \\ \midrule
         % Baseline & 4 & 59.8 & 105.61 & 7.79 \\ %  数据不是很好解释，acc 太高了，虽然推理速度慢
         Baseline & AP & 58.8 & 108.06 & 8.27 \\ 
         + S & AP & 58.2 & 76.81 & 4.66 \\ 
         + T & AP & 57.0 & 32.22 & 2.51 \\ 
         + S + T & AP & 59.2 & 45.10 & 4.17 \\ 
     \bottomrule
    \end{tabular}
    }
    \vspace{-0.2in}
\end{table}


\subsection{Ablation Study}

To analyze the contribution of each component within the DPTS framework, we conduct an ablation study on Qwen-2.5-1.5B with Math500 in Table~\ref{tab:ablation}. In this study, we use the classical MCTS as the baseline and incrementally integrate our proposed techniques to evaluate their impact. 

We begin with the original MCTS (non-parallel, $|P|=1$) as the baseline. It spends the most time per sample and has the largest best index 10.45. 
% , which means that after 10.45 terminated paths, it finds the best solution. 
% P=4 这行数据不是很好，这段先不加了
% Simply increasing $|P|$ does not reduce inference time, due to the severer redundant exploitation when conducting parallelism. This occurs because of two reasons. One is the waiting for all paths to terminate. Without supplementary nodes (such as our Deep Seek: explore → exploit), rollout latency is determined by the longest path, which diminishes efficiency. The other is the larger possibility of being terminated by memory overflow. The excessive computation and KV cache storage on suboptimal rollouts may aggravate the memory occupation. As a result, finalize the search process before it reaches the optimal path due to memory overflow. 
% When the finalization condition is set to a timeout or memory overflow, accuracy may degrade due to excessive computation and KV cache storage on suboptimal rollouts, restricting the model’s ability to store and exploit promising paths. 
We then apply Parallelism Streamline with Adaptive Parallel Generation (AP), and accuracy improves. It shows that the trees are able to grow faster and larger to include a better solution with parallelism. 
% the model allocates available GPU resources more effectively. 

Next, we assign the node status as exploit or explore nodes for each expansion with Search Mechanism, denoted as ``+S | AP'' in Table~\ref{tab:ablation}. 
% By incorporating a depth and breadth balanced strategy, 
The search process becomes significantly more structured and targeted, leading to a boost in efficiency. The time of each sample saves by 31.24s (28.9\%$\downarrow$). It finds the best path within an average of 4.66 terminated paths, much fewer than Baseline AP. 
% demonstrating the impact of the balanced exploitation and exploration. 

Moreover, when only applying the Early Stop strategy in the Transition Mechanism (denoted as ``+T | AP), we obtain fast inference with much less time and paths. However, since we only use the exploitation nodes without exploring the possible branches, the accuracy is relatively low. Therefore, we claim that the Search and Transition Mechanism should be used as a whole: the Search mechanism provides different node statuses for exploitation and exploration, while the Transition mechanism makes them flexibly change and update. 

Finally, we combine our Search and Transition Mechanism (denoted as ``+S+T | AP''), enabling Early Stop and Deep Seek. It shows the best search results in accuracy and efficiency. 
% with only 54.32s per sample, and 4.17 paths to reach the optimal solution on average. 
It demonstrates that DPTS is efficient in quickly identifying optimal solutions and conducting deep reasoning. 

Results show that each component of DPTS contributes significantly to improving inference speed and reasoning accuracy, making it a robust and scalable framework for parallel tree search. 




\subsection{Hyperparameter Analysis}

\begin{table}
    \centering
    \caption{Hyperparameter $\lambda_\mathrm{es}$ and $\lambda_\mathrm{ds}$ in transition thresholds $\theta_\mathrm{es}$ and $\theta_\mathrm{ds}$. ``ES (Early Stop)~\%'' and ``DS (Deep Seek)~\%'' are the ratios of the node type transition between the exploitation and exploration. More results can be found in Appendix~\ref{sec:app:hyperparameter}. }
    \vspace{-0.05in}
    \label{tab:hyperparameter}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
        \textbf{$\lambda_\mathrm{es}$} & \textbf{$\lambda_\mathrm{ds}$} & \textbf{Acc.} & \textbf{Time (s)} & \textbf{ES~(\%)}  & \textbf{DS~(\%)} \\ \midrule
        1.0 & 1.0 & 53.0 & 47.59 & 41.4 & 10.5 \\ 
        % 0.95 & 0.95 & 57.8 & 43.99 & 23.4 & 18.2 \\ 
        0.9 & 0.9  & 58.6 & 43.30 & 15.6 & 20.9 \\
        % 0.85 & 0.85 & 58.4 & 42.60 & 9.67 & 23.7 \\
        0.8 & 0.8 & 58.0 & 46.33 & 8.1  & 23.9 \\
        % 0.7 & 0.8 & & \\
        % 0.7 & 0.7 & 58.4 & 41.58 & 5.3 & 27.5 \\
        0.6 & 0.6 & 57.4 & 44.39 & 6.1 & 32.3   \\
        0.4 & 0.4 & 56.6 & 38.41 & 0 & 0.1 \\
        % 0.2 & 0.2 & 57.0 & 40.71 & 0 & 0.1  \\
        % 0   & 0   & 54.4 & 42.78 & 0 & 0.1  \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-0.12in}
    % \vspace{5pt}
\end{table}

We conduct a hyperparameter study in Table \ref{tab:hyperparameter} on the thresholds $\theta_\mathrm{es}$ and $\theta_\mathrm{ds}$ in the Transition mechanism. 
When $t < t^*$, the threshold $\theta$ follows the mean-based strategy determined by $\lambda$. When $t \geq t^*$, it turns to a max-based one. 
Empirically, we set  $t^* = 5$  to balance the flexibility and efficiency. 

Experimental results demonstrate that our method is robust to $\lambda$. We try different $\lambda$ and report the average ES/DS ratios per sample. 
We highlight that $\lambda_\mathrm{es}$ and $\lambda_\mathrm{ds}$ can be set differently based on the specific task. But DPTS consistently works well when $\lambda\in[0.6, 0.8]$. It demonstrates that the Transition mechanism is effective in mitigating the redundancy issue during search progress. 
% regardless of the transition ratio setting. 
However, it should be noticed that, if $\lambda$ is large, the ES transition may be aggressive, which leads to unsatisfactory results (e.g. $\lambda=1.0$). Meanwhile, if $\lambda$ is too small, it degrades to all exploitation nodes, resulting in low efficiency as well. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figs/singlecol_ee_proportion.pdf}
    \vspace{-0.1in}
    \caption{Proportions of exploit and explore nodes throughout the search process. }
    \label{fig:ee_proportion}
    \vspace{-0.12in}
\end{figure}

\subsection{Visualizations}

To provide an intuitive understanding of the effectiveness of our proposed method, we present visualizations of searching trajectories. % sample tree growth during the search process. 

First, we analyze the dynamic changes in the number of exploitation and exploration nodes throughout the search process in Figure~\ref{fig:ee_proportion}. The Deep Seek transition temporarily increases the proportion of exploit paths, allowing promising nodes to receive deeper reasoning. However, as the threshold $\theta_\mathrm{es}$ increases, exploit nodes are more likely to reach the threshold and stop. As a result, the number of exploit nodes naturally decreases, reinforcing a balance between exploitation and exploration. This dynamic adaptation ensures that DPTS stretches on the most promising branches under the constraint of computational resources. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/visualization_dpts.pdf}
    \vspace{-0.2in}
    \caption{Visualization of DPTS Tree. The green boxes are early stopped nodes based on their prior confidence using our \textit{Early Stop} mechanism, and the purple boxes are the terminated nodes with posterior reward scores. }
    \vspace{-0.12in}
    \label{fig:dpts_tree}
\end{figure}

Second, we show the trees generated by DPTS and analyze the search behavior in Figure~\ref{fig:dpts_tree}. It does not continue exploitation on low-confidence nodes, effectively pruning unpromising branches after shallow exploration. 
Additionally, the trees are capable of stable reasoning focus, with deep exploitation on promising paths. 
Therefore, the generated trees exhibit a relatively narrow width, as DPTS primarily expands nodes that are more relevant to the optimal path and spend less time on unnecessary regions. 
It demonstrates that DPTS 
% efficiently 
% focuses the thinking trajectory, 
ensures high-potential paths receive deeper thinking within a limited time and memory budget. 

