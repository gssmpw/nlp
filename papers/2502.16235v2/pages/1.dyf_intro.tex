The advent of OpenAI-o1~\cite{jaech2024openai}, a reasoning large language model, has sparked significant interest in the academic community. A key factor behind its success is the Chain-of-Thought~(CoT)-based reasoning technique~\cite{Wei_2022_Chain, chu2023survey}, which improves model’s reasoning ability by breaking complex problems into explicit intermediate steps. Building upon this, the Tree of Thoughts~(ToT)~\cite{Yao_2023_Tree} framework has been introduced to further elevate LLMs’ reasoning capacities, and be widely used in multi-step reasoning tasks~\cite{plaat2024reasoning}. ToT restructures reasoning as a tree search process and employs search algorithms, such as Monte Carlo Tree Search~(MCTS)~\cite{chaslot2008monte,xie2024monte}, to construct a tree-like structure that explores various reasoning pathways, leading to more refined and accurate responses~\cite{sprueill2023monte}.
However, current ToT approaches predominantly focus on improving search accuracy while overlooking computational efficiency~\cite{xie2024monte, cheng2024self, zhao2024marco}. We conclude two significant challenges that complicate the acceleration of ToT. 
% This oversight results in two significant challenges. 

\begin{figure}
   \begin{subfigure}{\linewidth}
   \vspace{-0.1in}
    \includegraphics[width=\linewidth]{figs/intro-cropped.pdf}
    \caption{Challenges of implementing parallelism in reasoning tasks.}
    \label{fig:intro}
\end{subfigure}
\hfill
\begin{subfigure}{\linewidth}
    \includegraphics[width=\linewidth]{figs/intro2.pdf}
    \caption{Accuracy and efficiency comparisons across tree search algorithms on various models and datasets.}
    \label{fig:comparison}
\end{subfigure}
\vspace{-0.2in}
\caption{(a) The demonstration of challenges. (b) The experimental results. }
\vspace{-0.25in}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figs/intro-cropped.pdf}
%     \vspace{-17pt}
%     \caption{Illustrations of challenges of implementing parallelism in reasoning tasks.}
%     % \vspace{-0.12in}
%     \label{fig:intro}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{figs/intro2.pdf}
%     \vspace{-20pt}
%     \caption{Accuracy and efficiency comparisons across tree search algorithms.}
%     \label{fig:comparison}
%     \vspace{-0.1in}
% \end{figure}

The first challenge arises from the frequent switching of reasoning focus in traditional sequential tree search, making it difficult to effectively parallelize~\cite{snell2024scaling}. 
% inherent structure of tree search, which is fundamentally sequential and
% fact that traditional sequential tree search methods are difficult to parallelize effectively on GPUs. 
Unlike conventional deep learning inference that is compatible with 
% highly structured and 
end-to-end parallelism, tree search has irregular computational trajectories. As shown in Figure~\ref{fig:intro} Challenge 1, it frequently switches among paths, including retrospect and recursion behaviors which complicates the parallelism (green trajectory). 
% It is incompatible with the existing end-to-end parallelization paradigm. 
% of computational resources. 
For example, if parallelizing nodes 6 and 2 in one generation, the different context lengths and KV cache require additional processing. 
Moreover, frequent switching 
% not only results in varying sequence lengths but 
also tends to yield shallow exploration~\cite{wang2025thoughts}. Due to the limited time and memory budgets, more explored paths mean less exploitation in deep paths. 
% These characteristics not only tend to yield shallow thinking, but complicate synchronization in parallel computation. 

% It makes existing end-to-end parallelization strategies inefficient for tree search scenarios. 

% The second challenge stems from the trade-off between depth-first and breadth-first search strategies. Traditional depth-first tree search methods often waste significant computation on suboptimal paths, as they continue exploring until termination criteria are met. On the other hand, breadth-first search frequently shifts between different reasoning paths, preventing deep exploitation of high-potential solutions. This trade-off becomes even more pronounced in parallel execution. In a parallelized depth-first search, the likelihood of committing to suboptimal paths increases, exacerbating the inefficiency of resource allocation. Conversely, a parallelized breadth-first search expands the tree more widely, amplifying the risk of spreading reasoning too thin without sufficient depth. 

% \jwt{
The second challenge stems from the redundant explorations on suboptimal solutions~\cite{li2024escape, besta2024topologies}. Previous tree search methods fail in identifying the less potential path and terminating it timely~\cite{wan2024dynamic}. Methods like MCTS attempt to balance the exploitation and exploration paths during node selection, but the selected nodes continue to roll out till the termination conditions (time or token limits) even with small prior confidence~\cite{xie2024monte}. For example, dark nodes in Figure~\ref{fig:intro} Challenge 2 have higher confidence. Node 3 with a lower confidence than node 5 will be explored earlier (pink trajectory). 
However, we observe that paths with low prior confidence have less probability of reaching the optimal solution, as illustrated in Section~\ref{sec:3.2}. 
% It suggests that taking the prior confidence as indication to prune low potential path is promising for efficient inference. 
% which inefficiency of tree search algorithms, which may waste computational resources on suboptimal nodes. 
% During reasoning in LLMs, the cost of generating new tree nodes is high, leading to an inherent trade-off between depth exploitation and breadth expansion. 
% When focusing on depth exploitation, tree search methods may be trapped in suboptimal paths, as they continue exploring until termination criteria are met. On the other hand, prioritizing breadth expansion may suffer from shifting between different reasoning paths, preventing deep exploitation of high-potential solutions. 
% MCTS attempts to balance these objectives through its cycle of selection, expansion, simulation, and backpropagation. However, when applied to LLMs, its simulation phase, which is the main bottleneck, reintroduces depth-first search inefficiencies. 
% In parallel execution, these inefficiencies are further exacerbated because of the accumulation across multiple paths, further impairing resource efficiency and limiting the depth of exploitation.
% in a parallelized deep exploitation, the likelihood of committing to suboptimal paths increases, exacerbating resource inefficiency. Conversely, in a parallelized breadth expansion that prioritizes breadth, the search tree is spread too widely, leading to insufficient depth in exploring promising paths.
% }

% To address these challenges, we propose a novel and efficient tree search method, \textbf{Exploit \& Explore Tree Search (EETS)}. This framework dynamically adapts its parallelism to optimize GPU utilization by employing an adaptive parallel expansion strategy. Specifically, during the generation phase, which is typically the main bottleneck in inference latency, EETS explores multiple reasoning paths in parallel, fully leveraging computational resources. Meanwhile, during node selection, our Exploit \& Explore searching algorithm integrates both deep exploitation and broad expansion strategies. High-confidence nodes are assigned to deep exploitation, allowing the model to refine promising pathways, while lower-confidence nodes undergo single-step expansion, preventing breadth-first search from becoming too shallow and reducing the risk of inefficient exploitation caused by suboptimal path selection in highly parallel settings. 


% Moreover, we introduce an Exploit \& Explore bidirectional transition mechanism to further refine the search process. This mechanism consists of two key components: Early Stop (Exploit → Explore), where nodes with low confidence in the exploit phase are dynamically converted into explore nodes, leveraging prior knowledge embedded in node information to prevent excessive time consumption on suboptimal paths; Deep Seek (Explore → Exploit), where High-confidence explore nodes are promptly transitioned into exploit nodes, enabling deeper reasoning in promising directions while maintaining an adaptive balance between depth and breadth.


To address these challenges, we propose a novel and efficient tree search framework, \textbf{DPTS (Dynamic Parallel Tree Search)}. 
This framework implements parallelized tree search and optimizes it by dynamically adjusting reasoning focus during the tree growing, thereby improving computational efficiency. 
% This framework consists of three key components: (streamlined) parallelism mechanism, (balanced) search algorithm, and (dynamic) transition strategy.
It consists of two key components for both the generation and selection phases. 
% The Parallelism Streamline used in generation phase builds up a finer-grained path arrangement. It allows a flexible parallelism with arbitrary nodes. The Dynamic Search and Transition mechanism filters potential candidates with balanced exploitation and exploration during selection phase, which forces the tree search to maintain the focus on more possible solutions and have less redundant rollouts. 
(1)~DPTS implements a Parallelism Streamline tailored for LLM reasoning in the generation phase. It facilitates the rollout for arbitrary paths in parallel, allowing the expanded nodes to be rearranged at each step. 
% The number of parallel paths can be dynamically adjusted based on available memory resources. 
Additionally, we carefully engineer cached data collection and context alignment, paving the way for parallelized inference with varying path length and node selection. 
(2)~Building on this, to prevent excessive exploitation and focus the reasoning on more potential paths,  
we introduce a Search and Transition Mechanism in the selection phase. 
% It manages balanced exploitation-exploration paths, and a bidirectional transition, i.e., \textit{Early Stop} (Exploitation → Exploration), \textit{Deep Seek} (Exploration → Exploitation). 
It dynamically balances the exploitation-exploration paths by the bidirectional transition, i.e., \textit{Early Stop} (Exploitation → Exploration), \textit{Deep Seek} (Exploration → Exploitation), allowing the model to focus on the most promising solutions and mitigate inefficient exploitation on suboptimal solutions. 
% This approach mitigates the risk of inefficient exploitation that arises from suboptimal path choices in highly parallel settings.

% 融合到上面一段了
% Furthermore, we propose a dynamic transition strategy to further refine the search process. The strategy incorporates two key mechanisms to optimize the search process. The first mechanism, \textbf{Early Stop}, dynamically converts low-confidence nodes from the exploitation phase into explore nodes. This conversion leverages prior knowledge embedded in the node information, preventing unnecessary time spent on suboptimal paths. The second mechanism, \textbf{Adaptive Deepening}, ensures that high-confidence explore nodes are promptly returned to exploitation, allowing for deeper reasoning in promising directions. This adjustment maintains a balanced trade-off between depth and breadth, promoting more efficient and targeted exploitation.

% 融到下面 exp 一段了
% In a word, DPTS effectively exploit the most potential solutions by dynamically adjusting the paths in paralleled generation, enabling fine-grained control over node execution to achieve efficient inference. 
% This reduces unnecessary computation and redundant exploitation on suboptimal paths while ensuring that high-confidence nodes receive deeper reasoning, ultimately leading to more efficient and scalable inference. 

% In sum,
% DPTS exploits the most potential solutions and reduces unnecessary exploitation, ensuring that high-confidence nodes receive deeper reasoning. 
% As demonstrated in Figure~\ref{fig:comparison}, experiments shows that DPTS reaches the best solution with fewer expansions, 

Our work provides valuable insights into accelerating the ToT for LLM reasoning, paving the way for future work to solve real-world challenges. 
Our contributions can be concluded as follows:
\begin{itemize}[itemsep=0pt,parsep=2pt,topsep=3pt]
    \item We propose the DPTS framework, which solve the frequent switching and redundant exploration issues in previous tree search methods for LLM reasoning. 
    \item The Parallelism Streamline provides a flexible and efficient generation in node parallel, which bridges the gap between the sequential tree structure and parallelized inference. 
    \item The Search and Transition Mechanism exploits the most potential solutions and reduces unnecessary exploitation, ensuring that high-confidence nodes receive deeper reasoning. 
    \item Experiments shows that DPTS reaches the best solution with less inference time across various models and widely used reasoning datasets, as plotted in Figure~\ref{fig:comparison}. 
\end{itemize}
