\section{Related Work}
\label{app:sec:related_work}

\paragraph{Reasoning with LLMs.}
% Studies on ChatGPT-o1 unveils the "Inference Scaling Law," which suggests that the model's reasoning capabilities can be improved by encoraging it more time for thinking. By adopting a strategy of long-thinking and slow-thinking, ChatGPT-o1 has reached human expert-level proficiency across various domains, such as mathematics, programming, and science. This achievement underscores its outstanding reasoning abilities, highlighting its potential to revolutionize these fields.
The success of ChatGPT~\cite{achiam2023gpt} has driven significant interest in Transformer-based LLMs~\cite{bai2023qwen,touvron2023llama,yang2024qwen2}, initially applied to simple System 1 tasks like translation and summarization~\cite{Brown_2020_Language,Ouyang_2022_Training}. As LLM capabilities grew, research shifted toward enhancing their ability to handle more complex System 2 tasks, such as mathematical reasoning and logic~\cite{Kojima_2022_Large,hao2023reasoning,zhang2024accessing,hosseini2024v}. The introduction of Chain-of-Thought~(CoT)~\cite{Wei_2022_Chain} advanced this domain by breaking down problems into intermediate steps, which proved effective for multi-step reasoning~\cite{Kojima_2022_Large}. Recent research~\cite{zhang2022automatic, bi2024forest} has proposed numerous variations, such as Self-Consistent CoT~\cite{wang2022self}, R-CoT~\cite{deng2024r} to improve its ability,  but its exploration scope remains constrained, limiting its effectiveness.~~\cite{chu2023survey}.
% Despite these advancements, challenges remain in more complex problem-solving because of its linear nature~\cite{chu2023survey, besta2024graph}.

\paragraph{Tree Search for Reasoning.}
To address the limitations of linear reasoning in CoT, the Tree of Thoughts~(ToT)~\cite{Yao_2023_Tree} framework was introduced, which aligns with the inference scaling law, showing that increasing Test-Time Compute can enhance LLM reasoning abilities~\cite{snell2024scaling,wu2024inference}.
The simplest form of tree search, Best-of-N, samples multiple reasoning paths and selects the best one~\cite{ cobbe2021training,lightman2023let,jiao2024learning}. Beam search extends this approach by considering multiple paths in parallel~\cite{Yao_2023_Tree}. Recent work~\cite{hao2023reasoning} has leveraged the exploration capabilities of Monte Carlo Tree Search~(MCTS)~\cite{chaslot2008monte, kocsis2006improved, browne2012survey}. For instance, MCTS-rollout~\cite{wan2024alphazero} introduces backtracking to explore different paths, while Q${^*}$~\cite{wang2024q} uses heuristic functions to guide rollout, and ReST-MCTS${^*}$~\cite{zhang2024rest} uses MCTS to sample traces for self-training. 
However, challenges remain in its computational efficiency, with limited research on accelerating tree search for LLM reasoning.

\paragraph{LLM Inference Acceleration.}
LLMs face efficiency bottlenecks during inference, leading to significant efforts aimed at accelerating inference~\cite{lin2024awq,hong2024flashdecoding++,leviathan2023fast,fu2024break,jing2023deep,cai2024medusa}. However, most of these approaches are designed for linear decoding tasks and are not directly applicable to tree-structured reasoning~\cite{li2024large, zhou2024survey}. 
Recent work~\cite{ning2023skeleton, han2024token,nayab2024concise}, such as Deft ~\cite{yao2024deft} focuses on optimizing common prefixes in tree reasoning using operator-level enhancements, , while others leverage self-consistency to enable early stopping~\cite{li2024escape, wan2024dynamic}. Despite such advancements, building efficient tree search algorithms for reasoning remains an under-explored area. The need for specialized algorithms that can scale with complex tree-structured reasoning, while maintaining efficiency, remains a key open challenge in LLM inference acceleration.
