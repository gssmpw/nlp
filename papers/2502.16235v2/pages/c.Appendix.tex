\newpage

% \appendix  
\section*{Appendix}
\label{sec:appendix}
% \tableofcontents  

\subsection*{Contents}
\begin{description}
    \item [\textbf{A}] \textbf{More Observations of Motivation} .............  \pageref{sec:app:motivation}
        \begin{description}
            \item [A.1] Wasted Tokens and Expansions ......... \pageref{sec:app:wasted}
            \item [A.2] Examples of DFS and BFS Trees ...... \pageref{sec:app:trees}
        \end{description}
    \item [\textbf{B}] \textbf{Formulas and Algorithms} ........................... \pageref{sec:app:formulas_and_algorithms}
        \begin{description}
            \item[B.1] Formulas in Parallelism Streamline ... \pageref{app:sec:parallel_reasoning}
            \item[B.2] Algorithms for Searching and Transition Mechanism ....................................... \pageref{sec:app:algorithms}
        \end{description}
    \item [\textbf{C}] \textbf{Additional Details about Experiment} ...... \pageref{app:sec:exp}
        \begin{description}
            \item [C.1] Comparison Methods ......................... \pageref{app:sec:comparison_methods}
            \item [C.2] Experimental Settings ....................... \pageref{app:sec:exp_setting}
            \item [C.3] Distribution of Best Path Index ......... \pageref{app:sec:best_path_index}
            \item [C.4] Additional Results of $\lambda$ ...................... \pageref{sec:app:hyperparameter}
        \end{description}
    \item [\textbf{D}] \textbf{Related Work} ............................................. \pageref{app:sec:related_work}
\end{description}

\hspace{0pt}

\section{More Observations of Motivation}
\label{sec:app:motivation}
\subsection{Wasted Tokens and Expansions}
\label{sec:app:wasted}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/draw_wasted_tokens.pdf}
    \caption{The proportion of tokens required for the best path relative to the total tokens generated (left), and the proportion of expansions on suboptimal paths relative to the total number of expansions (right). }
    \label{fig:motivation_waste_tokens}
\end{figure}

To better understand the inefficiencies caused by frequent node switching, we conduct a statistical analysis on Qwen-2.5-1.5B with the Math500 dataset and evaluate the redundancy in token generation and node expansion. 

Token redundancy analysis: In Figure~\ref{fig:motivation_waste_tokens}(left), we compare the total number of tokens generated for each sample (blue line) against the number of tokens required for the best path (yellow line). The samples are sorted in descending order primarily by total token count and secondarily by best-path token count. Our analysis shows that the total token count does not exhibit a strict multiplicative relationship with the best-path token count, but in general, the number of tokens required for the best path is significantly lower than the total token count.
% , with an average difference of XX times. 
This suggests that traditional tree search algorithms generate a large number of unnecessary tokens during exploration.

Expansion redundancy analysis: We also examined the number of node expansions during tree growth (Figure~\ref{fig:motivation_waste_tokens}(right)). The blue line represents the total number of expansions for each sample, while the green line represents the number of expansions on suboptimal paths (i.e., nodes that do not contain any part of the optimal solution). While there is no strict multiplicative correlation between these two metrics, the green line closely follows the blue line, indicating that a significant proportion of expansions occur on suboptimal paths. This further supports the observation that traditional tree search algorithms frequently explore unnecessary areas before finding the best solution.



\subsection{Examples of DFS and BFS Trees}
\label{sec:app:trees}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/motivation_dfs_trees.pdf}
    \caption{Growth of two trees with DFS algorithm. }
    \label{fig:motivation_dfs_trees}
\end{figure}

As illustrated in Figure~\ref{fig:motivation_dfs_trees}, which shows two typical depth-first search (DFS) trees, we visualize the node expansion process in layers based on tree depth. 
The darker reddish-brown nodes represent high-confidence nodes, while the lighter nodes indicate lower-confidence ones. The arrows denote parent-child relationships, where dark blue arrows indicate later-generated nodes and light blue arrows represent earlier-generated nodes. 

From the figure, we can clearly observe the reasoning trajectory of tree search: starting from the root node, the search prioritizes the child node with the highest confidence, then recursively expands deeper by selecting the most promising child node at each level. This continues until a termination condition is met, at which point the search backtracks and explores alternative paths from the root node. Due to the nature of this process, different paths vary significantly in their depth and termination points. Moreover, the next explored path does not follow a strict spatial or hierarchical pattern within the tree.

We also observe redundant exploration issues in the right two branches. At tree depths 4/5, the confidence scores of the expanded nodes are noticeably lower compared to previously explored nodes. However, due to the inherent mechanics of depth-first search (DFS), the algorithm continues expanding these nodes until the termination condition is met, even if the intermediate confidence scores remain consistently low. As a result, considerable computation is wasted on redundant expansions and token generations, with little contribution to improving the final output quality.


\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/motivation_bfs_trees.pdf}
    \caption{Growth of two trees with BFS algorithm. }
    \label{fig:motivation_bfs_trees}
\end{figure}

% While breadth-first search (BFS) provides a more structured and evenly distributed exploration pattern, it suffers from frequent node switching, which prevents deep reasoning and leads to a tendency of shallow exploration. Unlike depth-first search (DFS), which aggressively expands a single path before backtracking, BFS systematically explores a wider range of possibilities but often fails to fully develop any single thought process. 

As illustrated in Figure~\ref{fig:motivation_bfs_trees}, BFS results in a flatter, more uniform, top-down expansion structure compared to the trees observed in Figure~\ref{fig:motivation_dfs_trees}. This behavior creates two key inefficiencies: 
(1) Incomplete reasoning before termination: In our experiments on the Math500 dataset, a pure BFS approach resulted in 178 (about 35.6\%)  of reasoning paths terminating without generating an answer (e.g., Tree 1 in Figure~\ref{fig:motivation_bfs_trees}). The algorithm explores many different areas of the tree but often fails to pursue any one path deeply enough to reach a valid conclusion. 
(2) Excessive expansions and token redundancy: Even when BFS eventually finds a correct answer, it tends to consume significantly more expansions and tokens than necessary (e.g., Tree 2 in Figure~\ref{fig:motivation_bfs_trees}). The best path (highlighted in yellow arrows) has a depth of only 4, yet before discovering this optimal solution, BFS explores a large number of additional nodes (light blue arrows), many of which do not contain any part of the optimal path.

\section{Formulas and Algorithms}
\label{sec:app:formulas_and_algorithms}

\subsection{Formulas in Parallelism Streamline}
\label{app:sec:parallel_reasoning}

% \textbf{Adaptive Parallelism Queue}: Parallelism queue $P$ has dynamically adjustable length, which adapts to the available GPU memory during inference. Specifically, the queue size $|P|$ is defined as: $|P| = \frac{O_{{max}} - O_{{init}}}{O_{{peak}} - O_{{init}}}$, where  $O_{{peak}}$  represents the peak memory usage during the last generation, and  $O_{{init}}$  is the memory consumption at model initialization phase. This ensures a reasonable parallel number, allowing an efficient utilization of computational resources. 

% \textbf{Node Data Structure}: In our framework, each node maintains the following key attributes at the top level: a node identifier, a pointer to the parent node, the prior confidence, complete sequence from the root node to the current node ($\mathrm{Seq}^{1 \sim n}$), past key-value cache specific to this node ($\mathrm {KV}^n$). Therefore, the node structure can be written as:
% \begin{equation}
%     Node = [\mathrm{id}, \mathrm {parent}, \mathrm {conf.}, \mathrm {KV}^n, \mathrm {Seq}^{1 \sim n}].
% \end{equation}
% A major challenge in memory optimization arises from the KV cache, which consumes significant GPU memory. To avoid redundant storage, each node only retains its own past KV cache, rather than storing the entire sequence of past KV.  


\paragraph{Data Collection and Preparation.} 
Before executing parallel inference, we should collect the input data that is stored in separate memory locations. Based on the Node Data Structure (refer to Appendix~\ref{app:sec:parallel_reasoning}), which is $[\mathrm{id}, \mathrm {parent}, \mathrm {conf.}, \mathrm {KV}^n, \mathrm {Seq}^{1 \sim n}]$, we need to concatenate the past KV caches and input sequences of different nodes into single large batch matrices. However, as discussed in Sec.~\ref{sec:motivation}, tree search paths exhibit varying path lengths, meaning that both past KV caches and context sequences have different sizes. 
To handle the length disparity and support arbitrary node parallelism, we apply padding for shorter past KV and context sequence: 
% we apply left padding for nodes with shorter past KV caches, and right padding for nodes with shorter input sequences: 
% {\footnotesize
\begin{equation}
\label{eq:kv}
\begin{split}
    & \mathrm{KV}^{1\sim n} = \mathtt{concat}\left(\mathrm{KV}^{r^n}, \cdots, \mathrm{KV}^{a^n_1}, \mathrm{KV}^n\right), \\
    & \mathrm{padding}^n = \mathbf{0}_{\max\left(\forall_{m\in P}|\mathrm{KV}^{1\sim m}| \right) - |\mathrm{KV}^{1\sim n}|},  \\
    & \mathrm{KV}^{1\sim n}_{\mathrm{pad}} =  \mathtt{concat} \left(\mathrm{padding}^{n}, \mathrm{KV}^{1\sim n} \right), \\
    & \mathrm{KV}^{all} =  \mathtt{stack}\left ( [\forall_{n\in P}\ \mathrm{KV}^{1\sim n}_{\mathrm{pad}}]\right), 
\end{split}
\end{equation}
% }
where $r$ and  $a_1, a_2, \dots$ are the root node and $1^{st}, 2^{nd}, \dots$ ancestors of $n$, respectively. $\mathbf{0}_{len}$ is a matrix of zeros with length $len$. Similar to above, the input sequences are also padded and stacked: 
% {\footnotesize
\begin{equation}
\label{eq:seq}
\begin{split}
& \mathrm{padding}^n = padding\_token_{\max(\forall_{n\in P} |\mathrm{Seq}^{1\sim n}|)},  \\
& \mathrm{Seq^{1\sim n}_{pad}} =  \mathtt{concat} \left(  \mathrm{Seq^{1\sim n}}, \mathrm{padding}^n\right), \\ 
& \mathrm{Seq}^{all}=\mathtt{stack}\left( \forall_{n\in P}\mathrm{Seq^{1\sim n}_{pad}} \right), 
\end{split}
\end{equation}
% }
where $padding\_token_{len}$ is a vector of the predefined padding token id with length $len$. 
In this way, the data is fed into the LLM for parallel generation. Additionally, techniques like DEFT~\cite{yao2024deft} are orthogonal to our DPTS and can be integrated to identify and merge shared prefixes, further optimizing inference efficiency.

\paragraph{Generation and Updating.} After the generation phase, we obtain new output sequences and past KV caches. These are then partitioned based on the tree width. Specifically, we segment past KV caches and only store those corresponding to new tokens. Output sequences are completely stored rather than fragmented, due to the negligible memory overhead. For clearer demonstration, the output of $i^\mathrm{th}$ node in $P$ can be written as 
% {\footnotesize 
% \begin{equation}
% \label{eq:n_new}
% \begin{split}
%     &  \mathrm{KV}^{n^{ij}}  = \mathbf{n}.\mathtt{past\_kv}_{[ij, \dots, |\mathrm{KV}^{all}|:]}, \forall j\in[1, \dots, \mathtt{tw}] \\
%     & \mathrm{Seq}^{1\sim n^{ij}} = \mathbf{n}.\mathtt{output}_{[ij]} \\ 
%     & n^{ij}  = [\mathrm{id}, n^i, \mathrm{KV}^{n^{ij}}, \mathrm{Seq}^{1\sim n^{ij}}] \\ 
%     & \mathbf{n^{new}} = \{n^{i1}, \dots, n^{i\mathtt{tw}}\}, 
% \end{split}
% \end{equation}
% }
\begin{center}
\begin{equation}
\label{eq:n_new}
\begin{split}
    & \mathbf{KV}^{n^{ij}} = \mathbf{n}.\mathtt{past\_kv}_{[ij, \dots, |\mathbf{KV}^{\text{all}}|:]}, \forall j \in \left[1, \dots, w\right] \\
    & \mathbf{Seq}^{1 \sim n^{ij}} = \mathbf{n}.\mathtt{output}_{[ij]} \\
    & n^{ij} = \left[\mathrm{id}, n^i, \mathbf{KV}^{n^{ij}}, \mathbf{Seq}^{1 \sim n^{ij}}\right] \\
    & \mathbf{n^{new}} = \left\{ n^{i1}, \dots, n^{iw} \right\}
\end{split}
\end{equation}
\end{center}
where $w$ is the tree width, $\mathbf{n}$ is the generation output in parallel manner. 
If sequences were stored in a fragmented manner, every inference step would require additional collection and concatenation, introducing unnecessary latency.  This approach is a trade-off between inference speed and memory consumption. 
Newly generated nodes are then updated into the candidate node pool $N$, making them available for subsequent selection processes.



\subsection{Algorithms for Searching and Transition Mechanism}
\label{sec:app:algorithms}

In the main text, due to paper length constraints, we only present the overall process in Algorithm~\ref{alg:algorithm_parallel}, which connects the entire framework’s algorithms and formulations, including Parallelism Streamline and the Search and Transition Mechanism.

In the above section, we provided the mathematical formulation of the Parallelism Streamline. In the following, we further supplement the algorithmic details of Search, Transition, and Reward, offering readers a clear and intuitive representation of the algorithmic process. 



\begin{algorithm}[ht]
    \caption{Searching}
\label{alg:searching}
\begin{algorithmic}[1]
\REQUIRE Parallel queue $P$, current parallel queue size $\tau_P$, candidate node pool $N$. 
\ENSURE Updated $P$. 
\STATE $e_1\gets 0$
\FORALL{$n\in P$}
    \IF{$n$.mode$ = \mathtt{EXPLOIT}$}
    \STATE $e_1 \gets e_1  + 1$
    % \ELSE
    % \STATE $e_2 \gets e_2 + 1$
    \ENDIF
\ENDFOR
\IF{$|P|<\tau_P$}
    \STATE $N'\gets$ Descending $N$ based on conf. \\
    % , $\forall n \in N$
    \STATE $\mathbf{u} \gets N'[:\tau_P-|P|]$ 
    \FORALL{$u \in \mathbf{u}$}
        \IF {$ e_1<p|P| $}
        \STATE $u$.mode $\gets \mathtt{EXPLOIT}$
        \STATE $e_1 \gets e_1  + 1$
        \ELSE
        \STATE $u$.mode $\gets \mathtt{EXPLORE}$
        \ENDIF 
    \ENDFOR
    \STATE $P \gets P \cup \mathbf{u}$
\ENDIF
\RETURN $P$
\end{algorithmic}
\end{algorithm}

Firstly, Algorithm~\ref{alg:searching} demonstrates the searching mechanism. Specifically, during initialization or when the number of nodes in the parallel queue $P$ is less than the maximum parallelism $\tau_P$, the algorithm selects the $\tau_P- |P|$ highest-confidence nodes from the candidate node pool $N$ as supplementary nodes (Lines 8-9).

Then, the highest-confidence nodes are designated as exploit nodes (Line 12) until the proportion of exploit nodes reaches the ratio $p$. The remaining selected nodes are assigned as explore nodes. Finally, these newly selected nodes are merged into $P$ to prepare for the next cycle of parallel expansion. 

\begin{algorithm}[ht]
    \caption{Transition}
\label{alg:transition}
\begin{algorithmic}[1]
\REQUIRE Parallel queue $P$, transition thresholds $\theta_\mathrm{es}$ and $\theta_\mathrm{ds}$. 
\ENSURE Updated $P$. 
\FORALL{$n \in P$}
    \STATE $n^*=\max_\mathrm{conf.}$($n$.children)
    \IF{$n$.mode $=$ $\mathtt{EXPLOIT}$ \AND $c_{n^*} >\theta_\mathrm{es}$ \OR $n$.mode $=$ $\mathtt{EXPLORE}$ \AND $c_{n^*} >\theta_\mathrm{ds}$}
        \STATE $P \gets P\cup \{n^*\}$
        \STATE $n^*$.mode $\gets \mathtt{EXPLOIT}$ 
    % \ELSE
    \ENDIF
    \STATE $P \gets P \setminus n$
\ENDFOR
\RETURN $P$
\end{algorithmic}
\end{algorithm}

Next is the Transition Algorithm~\ref{alg:transition}. After each expansion, we iterate through all nodes in the parallel queue $P$ and identify the best child node  $n^*$ for each node.

Then, based on the category of node $n$, we compare $n^*$ with the corresponding threshold  $\theta$. If the early stop condition is not met or the deep seek condition is met, we add  $n^*$ as a new exploit node into $P$. Otherwise, the node will no longer be expanded and will be evicted from $P$.

\begin{algorithm}[ht]
    \caption{Reward}
\label{app:algo:reward}
\begin{algorithmic}[1]
\REQUIRE Parallel queue $P$, candidate node pool $N$. 
\ENSURE Updated $N$. 
\FORALL{$n \in P$} 
    % \PARALLEL
        \STATE $n$.children $\gets \text{Eq. (\ref{eq:n_new})}$ 
        \FORALL{$m \in n$.children}
            \IF{$\mathtt{is\_terminate}(m)$}
                \STATE $m$.reward $\gets \mathtt{reward}(m)$
            \ELSE
                \STATE $N \gets N \cup \{m\}$
            \ENDIF
        \ENDFOR
    \ENDFOR
\RETURN $N$
\end{algorithmic}
\end{algorithm}

After completing an expansion, we check whether each node’s path meets the termination conditions, such as reaching the maximum token limit or generating an end token. 
If a path satisfies the termination condition, exploration of that path stops, and a reward is computed as the final path score. We demonstrate this process in Algorithm~\ref{app:algo:reward}, which is identical to previous tree search algorithms and is not discussed in detail. However, for the sake of algorithmic completeness, we explicitly include it here.

\section{Additional Details about Experiment}
\label{app:sec:exp}

\subsection{Comparison Methods}
\label{app:sec:comparison_methods}

We compare DPTS against three widely used search algorithms: (1) Monte Carlo Tree Search~(MCTS)~\cite{sprueill2023monte} balances exploration and exploitation when sampling, while the selected paths rollout till termination, (2) Best-of-N~\cite{cobbe2021training} performs multiple independent rollouts and selects the highest-scoring output, (3) Beam Search~\cite{Yao_2023_Tree} expands multiple hypotheses in parallel, pruning low-scoring candidates at each step to maintain a fixed-width search~\cite{snell2024scaling}. 
Since efficient tree search algorithms have recently regained attention after the emergence of LLM reasoning, the strong baselines are limited. As a result, we primarily compare DPTS against these typical and well-established search algorithms to demonstrate the effectiveness of our method. 

 
\subsection{Experimental Settings}
\label{app:sec:exp_setting}

% 实验参数： tree_width:4 tree_depth:16 mini_step:128 % 其实 103 确实实验结果更好不知道为什么 otz
% max token:2048 mcts_max_time:120

The experimental settings are as follows: we set the tree width to 4, tree depth to 16, mini step to 100, and the maximum token limit to 2048. The MCTS time limit is 120 seconds, and the threshold parameter is empirically set to $t^*=5$. All models were downloaded from Hugging Face.

For evaluation, we implemented a custom codebase, which is included in the supplementary materials. Inference automatically terminates if it exceeds the timeout limit or encounters a memory overflow. Within these constraints, the search tree can expand and roll out indefinitely, ensuring comprehensive exploration during inference. 

\subsection{Distribution of Best Path Index}
\label{app:sec:best_path_index}

We use a histogram to visualize the earliest (blue bar) and shortest (green bar) best path index. Through an ablation study, we examine how the best solution in the search path evolves as our method is progressively introduced.

In the baseline method, the best path typically appears around the 8th terminated path. Incorporating the parallelism streamline does not directly affect the accuracy of the search path. However, after adding the searching and transition mechanism, DPTS finds the best solution region more quickly and reaches the best path earlier.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/best_path_distribution.pdf}
    \caption{The distribution of the earliest (blue bar) and shortest (green bar) best path index. }
    \label{fig:best_path_index}
\end{figure}


\subsection{Additional Results of $\lambda$}
\label{sec:app:hyperparameter}

We conducted a more detailed experiment on $\lambda$, with results presented in Table~\ref{app:tab:hyperparameter} and Figure~\ref{app:fig:lambda_curve}. It is evident that when $\lambda$ is set within a reasonable range (e.g., $[0.7,0.9]$), both accuracy and inference time exhibit optimal performance. In this range, the proportion of DS\% (deep seek transitions) is higher than ES\% (early stop transitions), indicating that more high-confidence nodes are being timely converted into exploit nodes. At the same time, a small number of paths are reassigned as low-score paths during inference and subsequently terminated.

However, when $\lambda$ is too large (e.g., close to $1$), the proportion of es increases aggressively. This suggests that many exploit nodes being expanded have scores within the range of $[0.9,1.0]$, and setting the threshold in this range may cause some correct paths to be prematurely stopped. Conversely, when $\lambda$ is too small (e.g., $< 0.4$), both ES and DS proportions drop to nearly zero. This occurs because most node scores exceed the threshold, causing nearly all paths to expand under exploitation mode. Moreover, an overly small early stop threshold causes no paths to terminate, effectively degrading the search into an exploitation-only strategy. 

Therefore, selecting a suitable $\lambda$ is important. A larger  $\lambda$ imposes stricter exploitation conditions, leading to more paths being stopped and fewer paths being converted to deep seeking. Conversely, a smaller $\lambda$ results in looser conditions, allowing more exploiting paths to continue rolling out until they reach a termination condition, while more high-confidence paths transition into deep seeking.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/draw_lambda_curve.pdf}
    \caption{Illustrations of hyperparameter analysis. }
    \label{app:fig:lambda_curve}
\end{figure}

\begin{table}
    \centering
    \caption{Hyperparameter $\lambda_\mathrm{es}$ and $\lambda_\mathrm{ds}$ in transition thresholds $\theta_\mathrm{es}$ and $\theta_\mathrm{ds}$. ``ES (Early Stop)~\%'' and ``DS (Deep Seek)~\%'' are the ratios of the node type transition between the exploitation and exploration.  }
    \label{app:tab:hyperparameter}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
        \textbf{$\lambda_\mathrm{es}$} & \textbf{$\lambda_\mathrm{ds}$} & \textbf{Acc.} & \textbf{Time (s)} & \textbf{ES~(\%)}  & \textbf{DS~(\%)} \\ \midrule
        1.0 & 1.0 & 53.0 & 47.59 & 41.4 & 10.5 \\ 
        0.95 & 0.95 & 57.8 & 43.99 & 23.4 & 18.2 \\ 
        0.9 & 0.9  & 58.6 & 43.30 & 15.6 & 20.9 \\
        0.85 & 0.85 & 58.4 & 42.60 & 9.67 & 23.7 \\
        0.8 & 0.8 & 58.0 & 46.33 & 8.1  & 23.9 \\
        % 0.7 & 0.8 & & \\
        0.7 & 0.7 & 58.4 & 41.58 & 5.3 & 27.5 \\
        0.6 & 0.6 & 57.4 & 44.39 & 6.1 & 32.3   \\
        0.4 & 0.4 & 56.6 & 38.41 & 0 & 0.1 \\
        0.2 & 0.2 & 57.0 & 40.71 & 0 & 0.1  \\
        0   & 0   & 54.4 & 42.78 & 0 & 0.1  \\
        \bottomrule
    \end{tabular}
    }
    \vspace{5pt}
\end{table}

\input{pages/2.Related_Work}

% \newpage

% \section{Critical Code}
% \begin{figure*}[t]
%     \centering
%     \begin{lstlisting}[caption=Code]

% print("hello world")
% print("hello world")

%     \end{lstlisting}
%     \label{fig:lst_code}
% \end{figure*}

% \minitoc  % 仅显示附录的目录

