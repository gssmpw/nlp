\section{Proposed Method}
\label{sec:method}
To address the aforementioned challenges, we propose an innovative framework that allows for efficient reasoning, termed Dynamic Parallel Tree Search (DPTS). 
% The proposed parallel framework serves as the foundation of DPTS. Furthermore, the search and the transition mechanism enables efficient and adaptive exploitation. 
In the generation phase, the Parallelism Streamline in Sec.~\ref{sec:method_parallel} supports fine-grained and flexible paralleled expansion for arbitrary paths. 
In the selection phase, the Search and Transition Mechanism in Sec.~\ref{sec:search_and_transition} enables less redundant exploration by identifying the highly potential solutions to focus reasoning. 

% \begin{algorithm}[ht]
% \caption{Algorithmic process DPTS}
% \label{alg:algorithm_parallel}
% \begin{algorithmic}[1]
% \REQUIRE LLM generation function $llm(x)$, PRM reward function $prm(x)$, Query $q$, Candidate Node Pool $N = \varnothing$, Parallel Queue $P = \varnothing$, Exploit Node Proportion $p$, Tree Width $w$. \\
% \ENSURE End node with best path reward $n^*$.

% {\color{ForestGreen}{// Step 1: Initialize the root node}} \\
% \STATE $r \gets \text{generate\_node}(q, \mathrm{None})$
% \STATE $N \gets N \cup \{r\}$

% \WHILE{\text{within computational budget}} 

%     % {\color{ForestGreen}{// Step 2: Adjust Parallel Queue}} \\
%     \STATE $ P_\text{size} \gets \text{Eq. (\ref{eq:queue_size})}$  \\
    
%     {\color{ForestGreen}{// Step 3: Perform searching}} \\
%     \STATE $P \gets \text{Search}(P, P_\text{size}, N)$ (Algorithm~\ref{alg:searching})
%     \STATE $\theta_{\mathrm{es}}, \theta_{\mathrm{ds}} \gets \text{Eq. (\ref{eq:theta})}$ \\
    
%     {\color{ForestGreen}{// Step 4: Parallelism by Eq. (\ref{eq:kv}) and (\ref{eq:seq})}} \\
%     \STATE $\mathbf{n} \gets \text{generate\_node}(
%     \mathrm{Seq}^{all}, \mathrm{KV}^{all})$ \\ 
%     {\color{ForestGreen}{// Step 5: Update new nodes by Eq.~(\ref{eq:n_new})}} \\
%     % \STATE $\mathrm{KV}^{\text{all}}, \mathrm{Seq}^{\text{all}} \gets \text{Eq. (\ref{eq:kv}), Eq. (\ref{eq:seq})}$
%     % \STATE $\mathbf{n} \gets \mathtt{generate\_node}(\mathrm{Seq}^{\text{all}}, \mathrm{KV}^{\text{all}})$
%     {\color{ForestGreen}{// Step 6: Terminate and reward}} \\
%     \STATE $N\gets \text{Reward}(P, N)$ (\text{Algorithm~\ref{app:algo:reward}}) \\
%     {\color{ForestGreen}{// Step 7: Perform transition}} \\
    
%     \STATE $P \gets \text{Transition}(P, \theta_{\mathrm{es}}, \theta_{\mathrm{ds}})$ (\text{Algorithm~\ref{alg:transition}})
% \ENDWHILE

% \RETURN $\max_{\text{reward}}(\forall n \in N)$
% \end{algorithmic}
% \end{algorithm}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/overview.pdf}
    \vspace{-0.35in}
    \caption{Overview of the proposed DPTS framework. The right part demonstrates the Parallelism Streamline, while the left and middle illustrate the proposed Search and Transition Mechanism. }
    \label{fig:overview}
    \vspace{-0.1in}
\end{figure*}

\subsection{Parallelism Streamline}
\label{sec:method_parallel}

% As the foundation of our framework, we fully parallelize the generation process to maximize GPU utilization. Thus, we first introduce the parallel execution architecture, as illustrated in the right side of Figure~\ref{fig:overview}. It consists of three phases: Adaptive Parallelism Queue, which has dynamically adjustable length determined by the available GPU memory; Data Collection and Preparation for parallel inference; Generation and Updating for efficient storage. Additional details can be found in Appendix~\ref{app:sec:parallel_reasoning} due to the limited length.  
% The core component of this architecture are the parallel queue $P$, which holds the currently selected optimal nodes; candidate node pool $N$, which collects all the unexpanded nodes; the data structure of each node. 

% 

% The selection process will be detailed in later sections on searching and transition mechanisms.


% \begin{algorithm}[ht]
% \caption{Process of EETS}
% \label{alg:algorithm_parallel}
% % \resizebox{\linewidth}{!}{
% \begin{algorithmic}[1]
% \REQUIRE Query $q$, candidate node pool $N=\emptyset$, parallel queue $P=\emptyset$, exploit node proportion $p$, tree width $\mathtt{tw}$. \\ 
% \ENSURE  Node with best path $n^*$. \\ 
% % \FOR {$s \gets S$}
%     \STATE $t_0 = \mathtt{time()}$
    
%     {\color{ForestGreen}{// Initializing root node $r$}} 
%     \STATE $r \gets \mathtt{generate\_node}(q, \mathrm{None})$
%     \STATE $N \gets N \cup \{r\}$
    
%     \WHILE{$\mathtt{time()}-t_0<t_\mathrm{{max}}$} 
%     \STATE $\mathtt{queue\_size} \gets \frac{1-O_{init}}{O_{peak} - O_{init}}$  
%     % {\color{ForestGreen}{// Adaptive parallelism number}} 
    
%     % \IF{$|P|<\mathtt{queue\_size}$}
%     % \STATE $N'\gets$ Descending $N$ based on conf. \\
%     % % , $\forall n \in N$
%     % {\color{ForestGreen}{// EE Searching}} \\
%     % \STATE $P \gets P \cup N'[:\mathtt{queue\_size}-|P|]$
%     % \STATE $P'\gets$ Descending $P$ based on conf.
%     % % , $\forall n \in P$
%     % \STATE $n$.mode $\gets \mathtt{EXPLORE}$, $\forall n \in P'[:p|P|]$
%     % \STATE $n$.mode $\gets \mathtt{EXPLOIT}$, $\forall n \in P'[p|P|:]$
%     % \ENDIF
%     {\color{ForestGreen}{// E\&E Searching}} \\
%     \STATE $P\gets$ Algorithm\ref{alg:searching}$(P$, $\mathtt{queue\_size}, N)$ \\
%     % 更新 theta
%     \STATE $\theta_{early\_stop}, \theta_{deep\_seek} \gets $ Eq. (\ref{eq:theta})
    
%     {\color{ForestGreen}{// E\&E Parallelism}} \\
%     % data preparation & generation
%     \STATE $\mathrm{KV}^{all}, \mathrm{Seq}^{all} \gets$ Eq. (\ref{eq:kv}), Eq. (\ref{eq:seq})
%     \STATE $\mathbf{n} \gets \mathtt{generate\_node}(\mathrm{Seq}^{all}, \mathrm{KV}^{all})$
    
%     % updating new node
%     \FORALL{$n \in P$}
%         \STATE $n$.children $\gets$ Eq. (\ref{eq:n_new}) 
%         \FORALL{$m\in n$.children}
%             \IF{$\mathtt{is\_terminate}(m)$}
%                 \STATE $m$.reward$\gets\mathtt{reward}(m)$
%             \ELSE
%                 \STATE $N\gets N\cup \{m\}$ \\ 
%             \ENDIF
%         \ENDFOR
%         % \STATE $a^i \gets P'[i]$
%         % \FOR{$j \gets 1$ to $\mathtt{tw}$}
%         % \STATE $\mathrm{KV}^{n^{ij}}\gets \mathbf{n}.\mathtt{past\_kv}_{[ij, \dots, |\mathrm{KV}^{all}|:]}$
%         % \STATE $\mathrm{Seq}^{1\sim n^{ij}}\gets \mathbf{n}.\mathtt{output}_{[ij]}$
%         % \STATE $n^{ij}\gets[id, a, \mathrm{KV}^{n^{ij}}, \mathrm{Seq}^{1\sim n^{ij}}]$
%         % \ENDFOR
%         % \STATE $a^i$.children$\gets[n^{i1}, \dots, n^{i\mathtt{tw}}]$
%     \ENDFOR
%     % updating N and transition
    
%     {\color{ForestGreen}{// E\&E Transition}} \\
%     \STATE $P \gets$Algorithm\ref{alg:transition}$(P, \theta_{early\_stop}, \theta_{deep\_seek})$
%     % \FORALL{$n \in P$}
%     % \STATE $n^*=\max_\mathrm{conf.}$($n$.children)
%     % \IF{$n$.mode $=$ $\mathtt{EXPLORE}$ \AND $n^*$.conf. $<\theta_{early\_stop}$ \OR $n$.mode $=$ $\mathtt{EXPLOIT}$ \AND $n^*$.conf. $<\theta_{deep\_seek}$}
%     %     % \IF{}
%     %     \STATE $P \gets P \setminus n$
%     %     \ELSE
%     %     \STATE $P \gets P\cup \{n^*\}$
%     %     \STATE $n^*$.mode $\gets \mathtt{EXPLORE}$ 
%     % \ENDIF
%     % \ENDFOR
%     % \FORALL{$n \in P$}
%     % \IF{$\mathtt{is\_terminate}(n)$}
%     % \STATE $P\gets P \setminus n$
%     % \STATE $n$.reward$\gets\mathtt{reward}(n)$
%     % \ENDIF
%     % \ENDFOR
% \ENDWHILE
% \RETURN $\max_\mathrm{reward}(\forall n \in N)$
% \end{algorithmic}
% \end{algorithm}

% \jwt{
As illustrated in Figure~\ref{fig:overview}, We fully parallelize the tree search process in our framework with three main components: \textbf{Tree Structure Building}, \textbf{KV Cache Handling}, and \textbf{Adaptive Parallel Generation}. Each component is designed to optimize memory usage and parallel execution during the reasoning process. 



\subsubsection{Tree Structure Building}
The tree search framework relies on a tree structure where each node represents a reasoning state.  Specifically, the node data structure includes the following elements:
\begin{itemize}[itemsep=0pt,parsep=0pt,left=0pt,topsep=0pt]
    \item \textbf{Node ID}: A unique identifier for each node.
    \item \textbf{Parent Node}: A reference to the parent node, establishing the hierarchical structure of the tree.
    \item \textbf{Prior Confidence}: The confidence of the node, based on prior knowledge and model predictions.
    % \item \textbf{Posterior Reward}:
    \item \textbf{Key-Value Cache} (\(\text{KV}^n\)): The key-value cache specific to this node, storing intermediate results during the reasoning process.
    \item \textbf{Token Sequence} (\(\text{Seq}^{1 \sim n}\)): The complete token sequence from the root node to the current node, representing the reasoning path taken so far.
\end{itemize}
The key challenge lies in managing the KV cache~\cite{floridi2020gpt}. Instead of storing the entire sequences, each node only retains its own KV cache. This significantly reduces memory usage, particularly when dealing with a large number of nodes in the tree. By keeping each node's cache isolated, we avoid redundant memory usage while ensuring that each node has necessary information to continue reasoning process.

\subsubsection{KV Cache Handling}

The KV cache for each node is stored separately, and during parallel execution, these caches need to be collected and concatenated for efficient parallelism. The key challenge is that tree search paths have varying lengths, which means that both the KV caches and the input sequences for different nodes will vary in size and be hard to parallel. 

To address this, we use a simple but straightforward padding technique to ensure that all sequences have consistent lengths before being processed. Specifically, for nodes with shorter KV caches, we apply left padding with zeros. Similarly, input sequences are padded with a predefined padding token to match the longest sequence in the batch. 
This padding ensures that all nodes are processed in parallel with consistent sequence lengths and corresponding KV cache, allowing for efficient batch processing across the tree search. \dyf{The details of padding and concatenating are given in Appendix Eq. (\ref{eq:kv}) and (\ref{eq:seq}).}

Besides data collecting and preparation, we also clean up the useless KV cache either the leaf node is terminated, or all the children's branches are exploited and finished. In this way, we release the memory, making room for new reasoning paths. 

\subsubsection{Adaptive Parallel Generation}
To further utilize the computational resources, we introduce an adaptive parallelism queue, which dynamically adjusts the number of parallel paths based on the available GPU memory. The parallelism queue size, denoted \( |P| \), is used to restrict the number of exploitation and exploration paths in Sec.~\ref{sec:method_searching}. It is calculated by the available and the peak memory usage during previous generations: 
% $|P| = \frac{O_{\text{max}} - O_{\text{init}}}{O_{\text{peak}} - O_{\text{init}}},$ \label{eq:queue_size}
\begin{equation}
\label{eq:queue_size}
|P| = \frac{O_{\text{max}} - O_{\text{init}}}{O_{\text{peak}} - O_{\text{init}}},
\end{equation}
where $O_{\text{max}}$ is the total memory budget, \( O_{\text{peak}} \) represents the peak memory usage from the previous generation, and \( O_{\text{init}} \) is the memory consumption during model initialization. 
\dyf{As the tree grows, the memory occupation of intermediate results continues to increase even with KV cache cleaning. Since memory overflow is one of the termination conditions, it is important to adaptively adjust the parallel number, preventing excessive memory allocation and early termination.} 
% mechanism ensures that the number of parallel paths is adjusted by the available memory resources, preventing excessive memory allocation and early termination of searching process}. 
 

After the generation phase, the newly generated sequences and KV caches are stored based on the tree width. The sequences for each node are completely stored, while the KV caches are stored partially with only the new tokens generated at this step (details are provided in Appendix~\ref{eq:n_new}). 
The new nodes are then added to the candidate node pool \( N \), where they will be available for subsequent selection processes in the tree search.

% \hspace{0pt}
\vspace{0.05in}

In summary, our Parallelism Streamline is a well-structured streamline to optimize both memory usage and parallel execution. 
% By carefully engineering the tree structure, KV cache, and parallel generation processes, we ensure that the reasoning capabilities of LLM are significantly enhanced while maintaining computational efficiency. 
The overall process is showcased in Algorithm~\ref{alg:algorithm_parallel}. More details can be found in Appendix~\ref{app:sec:parallel_reasoning} due to the limited length. 


\begin{algorithm}[ht]
\caption{Algorithmic process DPTS}
\label{alg:algorithm_parallel}
\begin{algorithmic}[1]
\REQUIRE LLM generation function $llm(x)$, PRM reward function $prm(x)$, Query $q$, Candidate Node Pool $N = \varnothing$, Parallel Queue $P = \varnothing$, Exploit Node Proportion $p$, Tree Width $w$. \\
\ENSURE End node with best path reward $n^*$.

{\color{ForestGreen}{// Step 1: Initialize the root node}} \\
\STATE $r \gets \text{generate\_node}(q, \mathrm{None})$
\STATE $N \gets N \cup \{r\}$

\WHILE{\text{within computational budget}} 

    % {\color{ForestGreen}{// Step 2: Adjust Parallel Queue}} \\
    \STATE $ P_\text{size} \gets \text{Eq. (\ref{eq:queue_size})}$  \\
    
    {\color{ForestGreen}{// Step 2: Perform searching}} \\
    \STATE $P \gets \text{Search}(P, P_\text{size}, N)$ (Algorithm~\ref{alg:searching})
    \STATE $\theta_{\mathrm{es}}, \theta_{\mathrm{ds}} \gets \text{Eq. (\ref{eq:theta})}$ \\
    
    {\color{ForestGreen}{// Step 3: Parallelism by Eq. (\ref{eq:kv}) and (\ref{eq:seq})}} \\
    \STATE $\mathbf{n} \gets \text{generate\_node}(
    \mathrm{Seq}^{all}, \mathrm{KV}^{all})$ \\ 
    {\color{ForestGreen}{// Step 4: Update new nodes by Eq.~(\ref{eq:n_new})}} \\
    % \STATE $\mathrm{KV}^{\text{all}}, \mathrm{Seq}^{\text{all}} \gets \text{Eq. (\ref{eq:kv}), Eq. (\ref{eq:seq})}$
    % \STATE $\mathbf{n} \gets \mathtt{generate\_node}(\mathrm{Seq}^{\text{all}}, \mathrm{KV}^{\text{all}})$
    {\color{ForestGreen}{// Step 5: Terminate and reward}} \\
    \STATE $N\gets \text{Reward}(P, N)$ (\text{Algorithm~\ref{app:algo:reward}}) \\
    {\color{ForestGreen}{// Step 6: Perform transition}} \\
    
    \STATE $P \gets \text{Transition}(P, \theta_{\mathrm{es}}, \theta_{\mathrm{ds}})$ (\text{Algorithm~\ref{alg:transition}})
\ENDWHILE

\RETURN $\max_{\text{reward}}(\forall n \in N)$
\end{algorithmic}
\end{algorithm}

% The dynamic adjustment of parallelism, coupled with effective padding strategies and efficient node management, allows for scalable and high-performance inference in complex tree search scenarios.  



% \textbf{Data Structure.} For preliminary, we first organize every thought steps as nodes of ToT. Each node in the parallel reasoning process is represented by a data structure that maintains essential information for efficient tree search. This structure includes the node's unique identifier, a reference to its parent node, prior confidence scores, the complete sequence of tokens from the root to the current node, and the node-specific key-value (KV) cache. To optimize memory usage, we avoid redundant storage of the entire sequence of past KV caches, retaining only the KV cache specific to each node. This ensures that the memory consumption is minimized without sacrificing the ability to track relevant state information.

% \The core of our parallel reasoning approach lies in the dynamic management of the parallelism queue, which plays a central role in balancing memory usage and computational efficiency. The size of this queue is adaptively adjusted based on the available GPU memory, ensuring that the framework can handle varying memory demands throughout the inference process. Specifically, 
% % the queue size is determined by the peak memory usage observed during prior generations, allowing us to dynamically allocate computational resources while avoiding memory overload.

% For data preparation phrase, to handle the variable path lengths in the tree search, input data is prepared by padding both the KV caches and input sequences to achieve uniform dimensions. Since the path lengths can differ significantly between nodes, we perform padding on both the KV caches and input sequences to ensure consistent input dimensions. The padding process involves left-padding the KV caches for shorter paths and right-padding the input sequences, resulting in a unified matrix that can be processed in parallel by the large language model (LLM). This approach allows the model to handle varying sequence lengths efficiently without introducing unnecessary complexity.

% Once the data is prepared, the framework proceeds to the generation and updating stage. In this phase, new output sequences and their corresponding KV caches are generated for each node in parallel. The results are then partitioned based on the tree width, with each partition containing only the newly generated information. This segmentation minimizes the memory overhead by retaining only the relevant KV cache segments associated with the new tokens. The output sequences, on the other hand, are stored in their entirety to avoid fragmentation, which would otherwise introduce unnecessary latency. Newly generated nodes are subsequently updated into the candidate node pool, where they become available for further exploitation in subsequent cycles.

% Together, these components of the parallel reasoning process allow the framework to efficiently leverage computational resources, maintain flexibility in handling diverse paths, and ensure high-speed inference while keeping memory consumption in check.
% }

\subsection{Search and Transition Mechanism}
\label{sec:search_and_transition}
In this section, we introduce the \textbf{Search} and \textbf{Transition} Mechanism in DPTS, which is a hybrid search algorithm that balances exploitation and exploration through separate management and dynamic conversion. 

\subsubsection{Search}
\label{sec:method_searching}

% To achieve a balance between exploitation and exploration, the parallel queue  $P$  is dynamically partitioned: the top  $p|P|$  highest-scoring nodes are designated as exploit nodes, while the remaining  $(1 - p)|P|$  nodes are explore nodes. These nodes are selected from the candidate node pool $N$ based on their confidence, as illustrated in Figure~\ref{fig:overview} (left). 

% \textbf{Exploit nodes updating} follows a dynamic node inheritance mechanism. if the new node confidence above the threshold, the best child of the exploit node can proceed to the next generation. However, if the confidence of new node falls below the threshold, or the number of exploit node less than $p|P|$, additional nodes are selected from the highest-scoring candidates in the pool  $N$  to ensure the total number of exploitation paths. This dynamic adjustment ensures that the exploitation process remains continuous and adaptive to the evolving search space. 

% \textbf{Explore nodes selection} focuses on extensive searching high-confidence solutions and refining reasoning direction. Unlike exploit nodes, explore nodes are not inherited from the previous generation cycle. Instead, before each expansion step, the highest-scoring nodes from pool $N$, excepting the exploit nodes that are already selected, are collected as explore nodes. This mechanism ensures that explore nodes are always updated based on the latest search progress, allowing for efficient and extensive exploration while avoid unnecessary detours into suboptimal regions. 

% \jwt{
The Search Mechanism aims to balance exploration and exploitation by dynamically partitioning the nodes in parallel queue \( P \) into two categories: \textit{explore nodes} and \textit{exploit nodes}. 
% By allocating resources efficiently to both types of nodes, the search process is able to explore new areas of the search space while exploiting high-confidence paths for refinement. 

As illustrated in Figure~\ref{fig:overview} (left), these nodes are selected from the candidate node pool \( N \).
% 设置了一个超参数p，这个超参数根据什么设置的 % 目前是 half-half
The partition ratio $p$ can be manually adjusted according to the task and memory budget. 
%And we simply set to $p=0.5$ for balance. 这句应该放实验里面 % okk
At initialization, the top \( p|P| \) highest-scoring nodes are assigned as \textit{exploitation nodes}, while the remaining \( (1 - p)|P| \) nodes are assigned as \textit{exploration nodes}. While during searching progress, the proportion of the two types of nodes dynamically fluctuates based on the transition mechanism in Sec.~\ref{sec:method_transition}. 
The primary distinction between these two nodes lies in their origins and roles during the search process.

\paragraph{Exploitation Nodes} 
 The exploitation nodes are primarily inherited from parent exploitation nodes, focusing on refining the most promising paths in the search space. When a child node’s confidence exceeds a predefined threshold, it inherits the status of its parent exploitation node and continues that path. This inheritance ensures that the most promising paths are deepened and further refined. Additionally, when the number of exploitation nodes falls below a predefined threshold, new high-confidence candidate nodes from the pool \( N \) are selected to fill the gap, ensuring that the number of exploitation nodes remains adequate for the search process. This strategy enables the exploitation of high-potential paths while maintaining the focus on areas with high confidence.
 % These nodes are selected before each expansion step from the candidate node pool \( N \), excluding those already designated as exploitation nodes. Exploitation nodes are not inherited from the previous generation; instead, they are always chosen based on the latest progress in the search space. This approach ensures that exploitation targets the most confident solutions while avoiding redundant exploration of paths that are unlikely to lead to better results.

\paragraph{Exploration Nodes} 
In contrast to exploitation nodes, the exploration nodes are not inherited from previous nodes but are dynamically selected from the candidate nodes pool. These nodes are responsible for discovering new paths that may have high potential but low current confidence in the search space. At each reasoning step, the exploration nodes are reselected from the candidate pool \( N \), choosing the highest-confidence nodes that are not already assigned as exploitation nodes. The dynamic re-selection of exploration nodes allows the search process to adapt to changing circumstances and uncover new regions of the search space that may lead to better solutions.

% These nodes are dynamically updated through an inheritance mechanism, where a new child node is inherited by an exploration node only if its confidence exceeds a certain threshold. If the confidence of the new child node falls below the threshold, or if the number of explore nodes drops below \( p|P| \), additional nodes are selected from the pool \( N \) to maintain the desired number of explore nodes. This dynamic selection ensures that exploitation  remains adaptive to the evolving search space, allowing the algorithm to discover new, potentially fruitful paths as it progresses.


% }


% \begin{algorithm}[ht]
%     \caption{Searching}
% \label{alg:searching}
% \begin{algorithmic}[1]
% \REQUIRE Parallel queue $P$, parallel number $\mathtt{queue\_size}$, candidate node pool $N$. 
% \ENSURE Updated $P$. 
% \IF{$|P|<\mathtt{queue\_size}$}
%     \STATE $N'\gets$ Descending $N$ based on conf. \\
%     % , $\forall n \in N$
%     \STATE $P \gets P \cup N'[:\mathtt{queue\_size}-|P|]$
%     \STATE $P'\gets$ Descending $P$ based on conf.
%     % , $\forall n \in P$
%     \STATE $n$.mode $\gets \mathtt{EXPLOIT}$, $\forall n \in P'[:p|P|]$
%     \STATE $n$.mode $\gets \mathtt{EXPLORE}$, $\forall n \in P'[p|P|:]$
% \ENDIF
% \RETURN $P$
% \end{algorithmic}
% \end{algorithm}

\subsubsection{Transition}
\label{sec:method_transition}

% While partitioning parallel nodes into exploit and explore helps mitigate redundant exploitation and shallow expansion issues, it does not entirely eliminate inefficiencies. One key limitation is that the initially selected exploit nodes are not guaranteed to be on the optimal path. Even when an exploit node is later found to be suboptimal, it continues expanding until reach the termination condition (timeout or out of memory), leading to unnecessary resource consumption. Additionally, before an exploit node terminates, newly identified high-confidence nodes cannot be deeply exploit since they are still classified as explore nodes. These weaken the flexibility and limit the efficiency of parallel reasoning. 

% To alleviate this issue, we introduce a bidirectional transition mechanism in the DPTS framework. As illustrated in Figure~\ref{fig:overview} (middle), it enables dynamic transition between exploit and explore nodes, enhancing flexibility and adaptability. It is a two-way transition: \textit{Early Stop (Exploit → Explore)} and \textit{Deep Seek (Explore → Exploit)}. 
% % These two strategies allow the search process to adaptively reallocate computational resources, ensuring efficient reasoning while maintaining a balance between exploitation and exploration.

% In the \textbf{Early Stop transition}, a threshold $\theta_{{early\_stop}}$ is introduced to dynamically evict low-potential exploit nodes. Specifically, we evaluate the best child node after each expansion. If the confidence is lower than $\theta_{{early\_stop}}$, none of the child would be added in the queue $P$ for the next cycle, preventing further redundant exploitation. Conversely, if the best child’s confidence exceeds $\theta_{{es}}$, it inherits the parent’s status and continues to exploit in the next cycle. As discussed in Sec.~\ref{sec:motivation}, the prior confidence is a reasonable indicator for forecasting the potential of the path. The threshold $\theta_{{es}}$ is empirically set as
% \begin{equation}
% \label{eq:theta}
% \theta_{{es}}  = \begin{cases}
% \lambda_1\frac{\sum^{N_\mathrm{Exp.}}_{n}n.\mathrm{conf.}}{|N_\mathrm{Exp.}|}, \text{if $t\le t^*$} \\ 
% \max(\forall_{n\in N_\mathrm{Exp.}} n.\mathrm{conf.}), \text{otherwise}
% \end{cases}
% \end{equation}
% where $N_\mathrm{Exp.}$ means all previously selected and expanded nodes, $\lambda_1$ is a coefficient, $t$ is the number of current terminated paths and $t^*$ is a predefined threshold to change the $\theta_{{early\_stop}}$. 
% We provide experimental evidence in Appendix~\ref{app:best_path_index} to show that early-terminated paths are more likely to be the optimal solution. 

% In contrast, the \textbf{Deep Seek transition} allows high-confidence explore nodes to convert into exploit nodes, enabling promising nodes to dig deeper. Specifically, explore node with a confidence exceeding the threshold $\theta_{{deep\_seek}}$ is promoted to an exploit node, where $\theta_{{deep\_seek}}$ has the similar formula as $\theta_{early\_stop}$ with $\lambda_{2}$. 
% % which is empirically set as $\theta_{{deep\_seek}}=\theta_{{early\_stop}}$ and we consistently obtain good results. 
% Using this mechanism, the number of exploit nodes may temporarily exceed $p|P|$. Fortunately, the increasing number of high-confidence nodes naturally raises $\theta_{{early\_stop}}$, which in turn a larger ratio of exploit nodes meet the Early Stop condition. This creates an adaptive system where the two types of nodes are dynamically balanced throughout the searching process. 

% \jwt{
While the Search Mechanism ensures a balance between exploration and exploitation, the redundant issue is not entirely mitigated. 
% there are cases where inefficiencies arise. 
One example is that the initial exploitation nodes are not guaranteed to be the optimal solution. However, they only stop exploiting till reach the termination condition. 
% may fail to lead to optimal paths.
Another issue occurs when high-confidence nodes are assigned as exploration nodes, but they will wait for the computation resources and do not roll out till the previous paths terminate. 
% which may waste resources because of over-exploring through already existing promising paths. 

To address these issues, we introduce the Transition Mechanism, which consists of two main strategies: \textit{Early Stop} (Exploitation → Exploration) and \textit{Deep Seek} (Exploration → Exploitation). As illustrated in Figure~\ref{fig:overview} (middle), these strategies allow an evolving search space with node transits between the two statuses. 
It helps the tree maintain focused reasoning, ensuring the efficient allocation and utilization of limited computational resources throughout the whole search process. 
% This flexibility enhances the search process by reallocating computational resources as needed to maintain an optimal balance between exploration and exploitation.

\paragraph{Early Stop (Exploitation → Exploration)} 
The Early Stop~\cite{yao2007early} strategy allows relatively low-confidence exploitation nodes to transition into explore nodes, eliminating redundant exploitation on suboptimal paths. 
During the expansion process, if the best child node of an explore node has a confidence lower than a certain threshold \( \theta_{\mathrm{es}} \), the child node will be excluded from the queue \( P \) in the next cycle. This prevents further exploration of paths that are unlikely to lead to optimal solutions, saving computational resources. Conversely, if the child node’s confidence exceeds \( \theta_{\mathrm{es}} \), it inherits the parent’s status and continues to explore in the next cycle. This mechanism ensures that only the most promising explore nodes continue to expand, optimizing both exploration and resource usage.
The threshold \( \theta_{\mathrm{es}} \) is defined as follows:
\begin{equation}
\label{eq:theta}
\mathsf{\theta}_{\mathrm{es}} = \begin{cases}
\lambda_\mathrm{es} \, \frac{1}{|\mathcal{N}|} \sum\limits_{i \in \mathcal{N}} c_i, & \text{if } t \leq t^* \\
\underset{i \in \mathcal{N}}{\max} \, c_i, & \text{otherwise}
\end{cases}
\end{equation}
where \( \mathcal{N} \) is the set of previously expanded nodes, \( c_i \) represents the confidence of node \( i \), \( \lambda_\mathrm{es} \) is a coefficient that adjusts the threshold, \( t \) is the number of currently terminated paths, and \( t^* \) is a predefined threshold after which \( \theta_{\mathrm{es}} \) is adjusted.


\paragraph{Deep Seek (Exploration → Exploitation)} 
The Deep Seek strategy addresses the issue of inefficient over-exploration and shallow thinking, ensuring promising exploration nodes can be dug deeper. Specifically, exploration nodes with confidence exceeding a threshold \( \theta_{\mathrm{ds}} \) with $\lambda_\mathrm{ds}$ are promoted to exploitation nodes. 
% This threshold is typically set to \( \theta_{\mathrm{ds}} = \theta_{\mathrm{es}} \), according to empirical results. % 这个实验里也打算加一下不等的，因为我想了想感觉等号不一定理论上合理。。
As a result, the number of exploitation nodes may temporarily exceed \( p|P| \). But as more high-confidence nodes are promoted, \( \theta_{\mathrm{es}} \) increases, and thus more exploration nodes are stopped under the Early Stop strategy. This creates a dynamic balance between exploration and exploitation throughout the search process. 

% \hspace{0pt}
\vspace{0.07in}

In a word, the proposed Search and Transition Mechanism in DPTS effectively manages the trade-off between exploitation and exploration with dynamic and bidirectional transition. 
% By dynamically partitioning the parallel queue and  the transition strategies, our approach adapts to the evolving search space and optimizes the utilization of computational resources. It enables the model to focus on the most promising branches while avoiding redundant or suboptimal expansions. 
Detailed algorithms in this part can be found in Appendix~\ref{app:sec:parallel_reasoning}.  

% \begin{algorithm}[ht]
%     \caption{Transition}
% \label{alg:transition}
% \begin{algorithmic}[1]
% \REQUIRE Parallel queue $P$, transition thresholds $\theta_{early\_stop}$ and $\theta_{deep\_seek}$. 
% \ENSURE Updated $P$. 
% \FORALL{$n \in P$}
%     \STATE $n^*=\max_\mathrm{conf.}$($n$.children)
%     \IF{$n$.mode $=$ $\mathtt{EXPLOIT}$ \AND $n^*$.conf. $<\theta_{early\_stop}$ \OR $n$.mode $=$ $\mathtt{EXPLORE}$ \AND $n^*$.conf. $<\theta_{deep\_seek}$}
%         \STATE $P \gets P \setminus n$
%     \ELSE
%         \STATE $P \gets P\cup \{n^*\}$
%         \STATE $n^*$.mode $\gets \mathtt{EXPLOIT}$ 
%     \ENDIF
% \ENDFOR
% \RETURN $P$
% \end{algorithmic}
% \end{algorithm}

