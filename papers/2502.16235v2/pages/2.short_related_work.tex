\section{Related Work}
% \label{sec:related_work}

\paragraph{Reasoning with LLMs. }
LLMs have evolved from System 1 tasks (e.g., translation)~\cite{Brown_2020_Language} to System 2 reasoning (e.g., math, logic)~\cite{Kojima_2022_Large}. CoT~\cite{Wei_2022_Chain} enhances multi-step reasoning, with variants like Self-Consistent CoT~\cite{wang2022self}, but its exploration scope remains constrained, limiting its effectiveness.~~\cite{chu2023survey}.
% \vspace{-4pt}
% \noindent\textbf{Tree Search for Reasoning. }
Furthermore, ToT~\cite{Yao_2023_Tree} enables multi-path exploration, leveraging MCTS~\cite{chaslot2008monte} for backtracking and heuristic rollouts~\cite{wan2024alphazero,wang2024q}. However, MCTS remains computationally expensive, with limited work on acceleration methods.

% \vspace{-pt}
\paragraph{LLM Inference Acceleration. }
While LLM inference has been optimized for linear decoding~\cite{lin2024awq}, tree-structured reasoning remains underexplored~\cite{li2024large}. Approaches like Deft~\cite{yao2024deft} optimize prefix sharing, while others use self-consistency for early stopping~\cite{li2024escape}. Efficient tree search for LLM reasoning remains an open challenge.

Due to page limit, we have included a more detailed discussion of related work 
% (1) Reasoning with LLMs, (2) Tree Search for Reasoning, and (3) LLM Inference Acceleration 
in Appendix~\ref{app:sec:related_work}. 