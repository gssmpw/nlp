\section{Threat Model}

In this work, we consider an adversary aiming to manipulate a deep learning-based traffic sign classification system using universal perturbations. The attacker has black-box access to the victim model, meaning they do not have direct knowledge of its architecture, parameters, or training data but can observe its predictions. The attack assumes that the adversary can place small, inconspicuous perturbations, such as stickers, on traffic signs in real-world environments to trigger the attack.

The primary objective of the attacker is to induce misclassification across multiple sign types using a single, common adversarial perturbation. This perturbation is designed to exploit shared vulnerabilities in the modelâ€™s feature extraction process, ensuring that the perturbation generalizes across different traffic signs.

We assume that the system relies solely on visual input for traffic sign recognition, without additional contextual verification mechanisms such as sensor fusion or map-based validation. Furthermore, we consider scenarios where standard adversarial defenses, such as adversarial training or input preprocessing, may be ineffective against universal physical perturbations. This threat model reflects real-world risks in autonomous driving and intelligent transportation systems, where adversarial manipulation of traffic signs could lead to incorrect or unsafe driving decisions.
