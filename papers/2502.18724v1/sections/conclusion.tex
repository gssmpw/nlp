\section{Conclusion}
\label{sec_conclusion}

This work introduced a novel method for generating universal perturbations that visually look like simple black and white stickers, and using them to cause incorrect street sign prediction. Unlike traditional adversarial perturbations, the adversarial universal stickers are designed to be applicable to any street sign: same sticker, or stickers, can be applied in same location to any street sign and cause it to be misclassified. Further, to enable safe experimentation with adversarial images and street signs, this work presented a virtual setting that leverages Street View images of street signs, rather than need to physically modify street signs, to test the attacks. The experiments in the virtual setting demonstrated that these stickers can consistently mislead deep learning model used commonly in street sign recognition, and achieve high attack success rates on dataset of US traffic signs. The findings highlight the practical security risks posed by simple stickers applied to traffic signs, and the ease with which adversaries can generate adversarial universal stickers that can be applied to many street signs.