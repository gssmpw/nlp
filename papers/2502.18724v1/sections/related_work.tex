\section{Related Work}
\label{sec:related_work}

This section provides an overview of existing work on universal perturbation attacks as well as attacks using adversarial stickers.

\subsection{Universal Perturbation Attacks}

Existing universal adversarial attacks can be broadly grouped within two categories: noise-based 
and the generator-based~\cite{weng2024comparative}.
Previous noise-based Adversarial Universal attacks overlooked the link between instance-level and universal-level attacks. To address this, Li et al.~\cite{Li_Yang_Wei_Yang_Huang_2022} introduced AE-UAP, which learns universal adversarial perturbations by incorporating the instance-level adversarial examples into the learning to learn more dominant perturbations~\cite{weng2024comparative}. Furthermore, the authors in ~\cite{Weng_Luo_Zhong_Lin_Li_2023} tackled the dominant bias in ensemble attacks using a non-target Kullback–Leibler (KL) loss. Additionally, the min-max optimization method employed dynamically adjusts ensemble weights, further boosting attack transferability across different models.

Regarding the generator-based category, building on the Transferable Targeted Perturbations (TTP) framework~\cite{naseer2021generating},Wang et al. introduced the Transferable Targeted Adversarial Attack (TTAA) method~\cite{10204634}. Unlike TTP, which relies on neighborhood similarity matching, TTAA incorporates a feature discriminator to align the feature distributions of adversarial samples with those of the real target class. 
In addition, the authors in~\cite{10205119} built on the TTP framework with Minimizing Maximum Model Discrepancy (M3D), a method that enhances the transferability of black-box targeted attacks. They demonstrated that the generalization attack error on a black-box target model primarily depends on the empirical attack error on the substitute model and the maximum model discrepancy among substitute models.

In contrast, in this work we focus on simple, yet effective approach where the perturbation is achieved by adding sticker(s) on the face of the street signs. Simple black stickers can be easily and practically deployed in practice to modify the classification of the~signs.

\subsection{Adversarial Stickers}

Prior work has explored adversarial stickers, but not in a universal perturbation setting. Prior work such as Robust Physical Perturbation (RP2)~\cite{eykholt2018robust} has exposed the vulnerability of deep neural networks (DNNs) to adversarial examples in real-world settings. Designed for safety-critical applications, RP2 generates robust visual perturbations that mislead classifiers under various conditions. Using road sign classification as a case study, it demonstrated high success rates in both lab and field tests, causing targeted misclassification with simple black-and-white sticker perturbations.

The authors in ~\cite{9779913} propsed the Meaningful Adversarial Sticker, a novel, physically feasible, and stealthy attack method for black-box attacks in real-world settings. This approach manipulates everyday stickers by adjusting their placement, rotation, and other parameters to deceive computer vision systems. To optimize these parameters efficiently, the authors propose the  Region based Heuristic Differential Evolution (RHDE) algorithm, which incorporates an offspring generation strategy that aggregates effective solutions and an adaptive adjustment mechanism for evaluation criteria. Extensive experiments in both digital and physical environments—including face recognition, image retrieval, and traffic sign recognition—validated the method’s effectiveness. Remarkably, even without prior knowledge of the target model, the attack successfully misleads vision systems in a covert manner, exposing potential security vulnerabilities.

In contrast, this work demonstrated how simple stickers can be placed on street signs, each sticker in the same location on all the street signs. This removes a need for street-sign specific perturbations.