\section{Background}
\label{background}

In this section, we provide a brief overview of adversarial attacks and the LISA dataset, which contains images of U.S. street signs. We also discuss LISA-CNN, the victim machine learning model used for street sign classification in this work. While we utilize LISA-CNN, we apply it to current Street View images of traffic signs.

\subsection{Adversarial Attacks}

Machine learning models, particularly deep neural networks (DNNs), are vulnerable to adversarial attacks -- small, carefully crafted modifications to input data that can alter predictions. These attacks operate in two primary settings: white-box, where the attacker has full access to the model’s parameters, and black-box, where the attacker has no direct knowledge of the target model~\cite{chakraborty2018adversarial,liang2022adversarial}.

One of the earliest attack methods, Fast Gradient Sign Method (FGSM), perturbs an image by adding the sign of the gradient of the loss function~\cite{goodfellow2014explaining}. More advanced iterative techniques, such as Projected Gradient Descent (PGD), refine this approach to generate stronger adversarial examples~\cite{mkadry2017towards}. While these attacks typically generate unique perturbations for each input, a more scalable approach is universal adversarial perturbations (UAPs)—a single perturbation designed to fool a model across multiple inputs~\cite{moosavi2017universal}.

Adversarial patches take this concept further by localizing perturbations to a specific region within an image rather than modifying the entire input~\cite{brown2017adversarial}. These patches are often designed to be universal, making them particularly practical for real-world attacks. To enhance their robustness against variations in viewing angles, lighting, and environmental factors, Expectation over Transformations (EoT) was introduced to generate patches that remain effective under common distortions~\cite{athalye2018synthesizing}. However, real-world studies, such as Robust Physical Perturbations (RP2), demonstrate that synthetic transformations alone fail to fully capture environmental complexity, necessitating real-world testing under diverse conditions~\cite{eykholt2018robust}.

Beyond noise-based and patch-based attacks, light and shadow-based adversarial attacks have emerged as more stealthy alternatives. Shadow attacks leverage natural or artificial shadows to create disruptions that are difficult for both human observers and AI models to detect~\cite{zhong2022shadows}. Similarly, light-based attacks manipulate natural light sources, such as sunlight or flashlights, to subtly distort object recognition~\cite{hsiao2024natural}. Leaf-based adversarial attacks leverage naturally occurring leaves to fool the machine learning model~\cite{etim2024fallleafadversarialattack}. These methods exploit everyday environmental factors, posing a significant challenge for AI-driven vision systems in real-world~settings.

\subsection{LISA-CNN and LISA Traffic Sign Dataset}

The LISA dataset is a collection of U.S. traffic sign images, covering 47 different road sign categories~\cite{lisa}. Due to class imbalances, prior research has focused on a subset of the $16$ most common signs, a selection that improves model training efficiency and classification performance~\cite{hsiao2024natural}.

LISA-CNN is a convolutional neural network designed specifically for traffic sign recognition using this dataset. The architecture consists of three convolutional layers followed by a fully connected layer, optimized to classify traffic signs under various environmental conditions and viewpoints~\cite{eykholt2018robust}. Models trained on LISA have demonstrated high accuracy and real-time detection capabilities, making them well-suited for autonomous vehicle perception~\cite{pavlitska2023adversarial} In this work, we utilize LISA-CNN to analyze adversarial universal perturbations on traffic signs. As this is the most common model used for traffic sign recognition, it is only one used in this work, but the ideas should apply equally to other models.

\subsection{Street View Street Sign Images}

Due to practical constraints, and considering potential safety issues, actual street signs are not used nor modified for this work. The attacks are evaluated by modifying images from Street View~\cite{googleExploreStreet}. Since the evaluation is done on real Street View images, we believe it emulates well real-world attacks on physical street signs without need to actually modify real street signs. Further, the use of Street View images ensures that the training set (LISA images) is different from the testing set (Street View images).

\begin{figure*}[t]
    \centering
    \includegraphics[width=15cm]{plots/universal_perturbation_workflow.pdf}
    \caption{\small Adversarial universal sticker attack workflow.}
\label{fig_universal_perturbation_workflow}
\end{figure*}
