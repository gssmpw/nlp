\section{Introduction}
\label{sec_introduction}

Deep Neural Networks (DNNs) have revolutionized machine learning applications, yet they remain inherently vulnerable to adversarial perturbations -- subtle, often imperceptible modifications to input data that can drastically alter model predictions~\cite{chakraborty2018adversarial}. First identified by Szegedy et al.~\cite{szegedy2013intriguing}, these adversarial examples expose fundamental weaknesses in deep learning models, particularly in safety-critical domains like autonomous driving, biometric authentication, and medical imaging~\cite{goodfellow2014explaining},~\cite{kumar2024medical}. As DNNs continue to be integrated into real-world applications, their susceptibility to adversarial attacks raises significant concerns about security, reliability, and trustworthiness.

One of key domains where DNNs can be applied is autonomous vehicles, where street sign recognition can be easily done by the DNN to help an autonomous vehicle navigate physical world. Unfortunately, adversarial perturbations can prevent correct street sign recognition or classification. Existing adversarial perturbation work has been applied in the context of traffic sign recognition and object detection, and attackers have demonstrated various perturbation strategies. Existing attacks include physical sticker attacks~\cite{eykholt2018robust} where various stickers are placed on street signs to cause the street sign to not be correctly recognized. More recent work on adversarial shadows~\cite{zhong2022shadows} shows that light shadows can achieve same effect; the shadows are simple, however, generating them in practice may be difficult, e.g., on a cloudy day. Other work has shown light-based manipulations~\cite{hsiao2024natural} where bright light spots are shone on street signs to cause them to be misclassified. All of these  attacks leverage model vulnerabilities to induce erroneous street sign predictions. These attacks highlight not only the fragility of DNN-based perception systems but also the practical feasibility of adversarial manipulations in physical environments.

One limitation of the attacks is that the perturbations are specific to each traffic sign. In other setting, universal adversarial perturbations (UAPs) have been shown to pose a particularly severe threat, as they can fool a model across multiple inputs with a single, input-agnostic perturbation~\cite{moosavi2017universal}. Unlike targeted adversarial attacks, including the existing ones that attack street signs~\cite{eykholt2018robust,zhong2022shadows,hsiao2024natural},  UAPs generalize across diverse samples, significantly compromising model robustness in real-world scenarios~\cite{weng2024comparative}. 

In this work, we investigate the impact of universal adversarial perturbations (UAPs) on the specific case of traffic sign classification systems. We focus on the design of simple black and white stickers, and find how these stickers can be placed at a common coordinate on any street sign to cause it to be misclassified. Our experiments demonstrate that a single location can be exploited across multiple sign types to consistently induce misclassification, highlighting a critical vulnerability in deep learning-based recognition systems. Without a need for a complex perturbation, a simple black or white sticker can cause a street sign to be misclassified and have the machine learning model generate confidence score of up to 90\%. Further, unlike targeted attacks that require sign-specific modifications, our findings reveal that single location where one or two stickers are located on any traffic sign can cause misclassification, enabling a universal attack strategy that generalizes across different street signs.

Further, to enable safe experimentation with adversarial images and street signs, this work presents a virtual setting that leverages Street View images of street signs, rather than need to physically modify street signs, to test the attacks. Due to practical constraints, and considering potential safety issues, actual street signs were not used nor modified for this work. The attacks are evaluated by modifying images from Street View. This further demonstrates that attackers can leverage easily accessible Street View images to test their attacks and find the locations for the adversarial universal stickers to place on the street signs.

Our insights have significant implications for the security of autonomous driving systems, as the suggests that attackers can design efficient, low-cost perturbations capable of misleading a model without requiring precise adjustments for each individual sign. Additionally, our results emphasize the need for more robust defense mechanisms that account for spatially invariant attack strategies, as current adversarial training and input transformation techniques may fail against universal perturbations that exploit shared model weaknesses.


\subsection{Contributions}

The contributions of this work are as follows:

\begin{enumerate}

    \item We demonstrate the first example of universal perturbation attacks targeting specifically street signs.

    \item We show that it is possible to enable safe experimentation with adversarial images and street signs by using a virtual setting that leverages Street View images of real street signs, without need to actually modify real signs -- which may be illegal or dangerous.

    \item We demonstrate that a simple black or white sticker located strategically on the face of the street sign can cause misclassification with confidence score (of the incorrect classification) as high as 90\%.

    \item We show that for single or double white or black stickers it is possible to find common coordinates on the tested street sign such that each street sign is misclassified.

\end{enumerate}


    



