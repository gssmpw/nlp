
\documentclass{article}
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{float}
\usepackage{amsmath} 
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{booktabs}
\usepackage[ruled]{algorithm2e}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage[noend]{algpseudocode}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}


\newcommand{\gr}{\rowcolor[gray]{.95}}
\usepackage{colortbl} 
\definecolor{mygray}{gray}{0.95} 
\newcommand{\wc}{\cellcolor{white}} 

\title{Dynamic Low-Rank Sparse Adaptation for Large Language Models}

\author{%
  Weizhong Huang$^{1}$ \quad Yuxin Zhang$^{1}$ \quad Xiawu Zheng$^{1,3,4}$ \quad Yang Liu$^{2}$ \quad Jing Lin$^{2}$ \\    \textbf{Yiwu Yao}$^2$ \quad \textbf{Rongrong Ji}$^{1,3}$\thanks{Corresponding author: rrji@xmu.edu.cn}
  \\[0.2cm] 
  $^1$Key Laboratory of Multimedia Trusted Perception and Efficient Computing, \\ \; Ministry of Education of China, Xiamen University, 361005, P.R. China. \; \\ $^2$ Huawei Technologies\;
  $^3$Institute of Artificial Intelligence, Xiamen University \;\\
  $^4$ Peng Cheng Laboratory, Shenzhen, China\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. 
%
Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios.
In this paper, we introduce dynamic \textbf{Lo}w-rank \textbf{S}parse \textbf{A}daptation \textbf{(LoSA)}, a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. 
%
In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training.
%
Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning.
%
Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs.
%
Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. 
%
For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by {\color{red} 68.73$\downarrow$} and increased zero-shot accuracy by {\color{red} 16.32$\%$$\uparrow$}, achieving a \textbf{2.60$\times$} speedup on CPU and \textbf{2.23$\times$} speedup on GPU, requiring only \textbf{45 minutes} of fine-tuning on \textbf{a single} NVIDIA A100 80GB GPU. Code is available at \url{https://github.com/wzhuang-xmu/LoSA}.
\end{abstract}

\section{Introduction}

The development of large language models (LLMs) \citep{zhang2022opt, touvron2023llama, touvron2023llama2} has marked substantial advancements in the field of natural language processing \citep{achiam2023gpt}. 
As the scale of these models increases, they demonstrate enhanced capabilities in understanding and generating across diverse contexts \citep{kaplan2020scaling, brown2020language}. Nevertheless, the exponential growth in model size presents formidable challenges for deployment and inference, primarily due to escalated computational demands and latency \citep{zhu2023survey}. 
To mitigate these issues, a variety of model compression strategies have been developed. Techniques such as sparsity \citep{frantar2023sparsegpt, sun2023simple, dong2024pruner, ma2023llm, xia2023sheared, an2024fluctuation}, quantization \citep{egiazarian2024extreme, xiao2023smoothquant, xu2024onebit, lin2023awq}, and knowledge distillation \citep{ko2024distillm, hsieh2023distilling, gu2023minillm, agarwal2024policy} have proven effective in reducing model size while largely preserving their original efficacy, thus enhancing the feasibility of deploying large models in practical applications.

\begin{figure}[!t]
\begin{center}
\includegraphics[height=0.3\linewidth,width=1\linewidth]{figures/fig1.pdf}
\end{center}
\caption{\label{fig:framework}Comparing traditional sparse LLM combined with LoRA to our LoSA method: (a) Traditional LLM sparsity methods employ uniform sparsity rates, and LoRA also uses uniform ranks. Additionally, LoRA weights cannot be merged into the sparse LLM weights. (b) LoSA performs dynamic sparse low-rank adaptation on LLMs, simultaneously applying sparsity to both LLM and low-rank adaptation. Moreover, LoSA dynamically determines the layer-wise sparsity rates based on representation mutual information and allocates the ranks of the low-rank adaptation according to the reconstruction errors of the sparse LLM.}
\vspace{-1.5em}
\end{figure}

Among the diverse array of model compression techniques, sparsity emerges as a prominent method for diminishing both the size and computational demands of LLMs \citep{li2023sparse, lu2024spp, frantar2023sparsegpt, sun2023simple}. Notable implementations such as SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple} have effectively demonstrated one-shot sparsity, achieving substantial reductions in model size while largely maintaining performance. 
These strategies also enhance inference speed on CPUs and GPUs through integration with specialized libraries such as DeepSparse 
 \citep{deepsparse} and nm-vllm \citep{nm-vllm}. 
However, these approaches encounter performance degradations at high sparsity levels. 
Consequently, fine-tuning are essential to recuperate the efficacy of sparse LLMs \citep{guo2024ebft}, thereby ensuring that they remain robust and effective in practical applications.

Leveraging parameter-efficient fine-tuning (PEFT) methods \citep{houlsby2019parameter, lester2021power, hu2021lora} offers a compelling approach to fine-tune sparse LLMs without necessitating the adjustment of the entire model's parameters. 
By incorporating only a minimal number of additional parameters, PEFT methods circumvent the resource-intensive demands typically associated with full-model fine-tuning. Among various PEFT strategies, Low-rank Adaptation (LoRA) \citep{hu2021lora} distinguishes itself through its innovative use of two low-rank matrices. 
These matrices are integrated during the fine-tuning phase and subsequently can be merged with the original model weights. This integration effectively preserves the original structure of the model while also eliminating delays during inference, thereby streamlining the deployment process and improving the model's performance after fine-tuning.

However, directly employing the existing LoRA method to fine-tune sparse LLMs faces several critical issues: \textbf{1) Incompatibility between sparse LLMs and LoRA.} The weights refined through LoRA cannot be seamlessly integrated into sparse LLMs \citep{zhang2023pruning}. Retaining LoRA weight matrices leads to increased inference delays in sparse LLMs (Table \ref{tab:speedup}), thereby compromising the compression and acceleration advantages initially gained from sparsity. \textbf{2) Static setup of uniform sparsity rates.} Existing SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple} methods predefine a uniform sparsity rate for each layer; however, the relative importance of each layer may change during the fine-tuning process. The setting of uniform layer-wise sparsity rates impairs the performance of the sparse model. \textbf{3) Static determination of the LoRA matrices' rank.} LoRA's approach of predetermining the rank for each layer fails to account for the variability in layer-wise reconstruction errors that arise during the sparsity of LLMs. Consequently, this results in a uniform allocation of fine-tuning parameters across different layers, which is insufficient for achieving satisfactory fine-tuning performance \citep{zhang2023adaptive}. 

To address the issues mentioned above, we propose dynamic \textbf{Lo}w-rank \textbf{S}parse \textbf{A}daptation \textbf{(LoSA)} for LLMs, a method that seamlessly integrates low-rank adaptation into LLM sparsity.
%
LoSA attains this objective through three primary innovations.
%
Firstly, to maintain compatibility between the sparse LLM weights and low-rank adaptation, we dynamically sparsify the low-rank adaptation, ensuring they align with the sparsity patterns of the LLM weights. 
%
Furthermore, we dynamically adjust the layer-wise sparsity rates of the LLM using Representation Mutual Information (RMI) \citep{tishby2000information, zheng2022neural}.  
Firstly, we derive that RMI can be used as a metric to determine the importance of each layer in LLMs according to the Information Bottleneck (IB) principle \citep{tishby2000information}. Furthermore, we approximate the calculation of RMI using normalized Hilbert-Schmidt Independence Criterion (HSIC) \citep{gretton2005measuring, kornblith2019similarity}. This means that we only need to obtain the feature map of LLM to calculate RMI, allowing us to determine the importance of each layer and efficiently set the layer-wise sparsity rate during fine-tuning.
Lastly, we dynamically determine the ranks for the low-rank adaptation based on reconstruction errors, which allows us to allocate a larger fine-tuning parameter budget to layers with greater reconstruction errors. This strategy not only achieves a rational distribution of fine-tuning parameters but also maximizes the reduction of reconstruction errors in sparse LLMs, enhancing overall fine-tuning performance.

Extensive experimental results demonstrate the efficiency and effectiveness of our proposed LoSA for sparsifying representative LLMs, including LLaMA-1 \citep{touvron2023llama}, LLaMA-2 \citep{touvron2023llama2}, LLaMA-3 \citep{llama3}, LLaMA-3.1 \citep{llama31}, OPT \citep{zhang2022opt} and Vicuna \citep{chiang2023vicuna} with their parameter sizes ranging from 7 billion to 70 billion. Remarkably, our approach reduces the perplexity of a sparse LLaMA-2-7B \citep{touvron2023llama2} model with sparsity ratio of 70$\%$, obtained using Wanda \citep{sun2023simple} method, by {\color{red} 68.73$\downarrow$}, while achieving an accuracy improvement of {\color{red} 16.32$\%$$\uparrow$} across seven downstream datasets. Moreover, it achieves a \textbf{2.60$\times$} speedup on CPU and \textbf{2.23$\times$} speedup on GPU, requiring only \textbf{45 minutes} of fine-tuning on a \textbf{single} NVIDIA A100 80GB GPU. 


\section{Methodology}
\subsection{Preliminaries}\label{sec:Preliminaries}
\paragraph{Notation.} In this study, we use uppercase letters (\emph{e.g.,}$X, Y$) to denote random variables. Bold typeface represents vectors (\emph{e.g.,}$\boldsymbol{x, y}$), matrices or tensors (\emph{e.g.,}$\boldsymbol{X, Y}$). Calligraphic font indicates loss functions (\emph{e.g.,}$\mathcal{L}$). 

\paragraph{Problem Formulation.}
Following the approach introduced by SparseGPT \citep{frantar2023sparsegpt}, we conceptualize the implementation of sparsity in LLMs as a layer-wise reconstruction problem. Our objective is to minimize the difference in output between each layer of a sparse LLM and its corresponding dense counterpart. Consider a dense LLM composed of \(n\) layers, where the weight matrix of the \(i\)-th layer is denoted as \(\boldsymbol{W}_i \in \mathbb{R}^{c_{\text{out}}\times c_{\text{in}}}\), \(c_{\text{in}}\) and \(c_{\text{out}}\) representing the number of input and output channels, respectively. The input feature maps are represented by \(\boldsymbol{X}_i \in \mathbb{R}^{c_{\text{in}} \times d}\), where \(d\) is the hidden dimension. The sparsity mechanism in an LLM involves applying a binary mask \(\boldsymbol{M}_i \in \{0,1\}^{c_{\text{out}} \times c_{\text{in}}}\) to the weight matrix  \(\boldsymbol{W}_i\), which selectively eliminates individual elements.

In this study, we explore the integration of low-rank adaptations with sparsity methods for LLMs. Our approach incorporates low-rank adaptations into the framework of layer-wise reconstruction error optimization. We introduce low-rank adaptation for the $i$-th layer as $\boldsymbol{B}_i \in \mathbb{R}^{c_{\text{out}}\times r_i}$ and $\boldsymbol{A}_i \in \mathbb{R}^{r_i \times c_{\text{in}}}$, with $r_i$ representing the rank of the adaptation. Simultaneously, we define the ranks of the low-rank adaptations for all $n$ layers collectively as $\boldsymbol{r} = (r_1, r_2, ..., r_n)$, and the sparsity rates for all $n$ layers as $\boldsymbol{s} = (s_1, s_2, ..., s_n)$. Consequently, the low-rank sparse adaptation for LLMs can be viewed as the following optimization problem:
\begin{equation}\label{eq:target}
\begin{split}
    & \min_{\boldsymbol{M}, \boldsymbol{BA}} \;  \sum_{i=1}^{n}||\underbrace{\boldsymbol{W}_i*\boldsymbol{X}_i-(\boldsymbol{M}_i \odot (\boldsymbol{W}_i+\boldsymbol{B}_i\boldsymbol{A}_i))*\boldsymbol{X}_i}_{\mathcal{L}_{i}}||_2^2,  
    \;\; \\
    & \emph{s.t.} \;\; \frac{\left\| \boldsymbol{M}_i \right\|_0}{c_{\text{out}} \cdot c_{\text{in}} } = s_i, \frac{1}{n}\sum_{i=1}^{n}s_i = {\Theta}, \frac{1}{n}\sum_{i=1}^{n}r_i = {\Omega},
\end{split}
\end{equation}
where \( * \) denotes matrix multiplication, \( \odot \) represents Hadamard product, \( \|\cdot\|_2 \) signifies the \(\ell_2\) norm, \(\mathcal{L}_i\) denotes reconstruction error of the \(i\)-th layer, and \(\left\| \boldsymbol{M}_i \right\|_0\) indicates the number of 0 elements in matrix $\boldsymbol{M}_i$.

The proposed formula integrates LLM sparsity and low-rank adaptation into a unified optimization objective, framing it as a constrained layer-wise reconstruction problem. This approach offers advantages over the conventional method of optimizing sparsity and fine-tuning separately. By employing joint optimization, we aim to improve the accuracy of sparse LLM and low-rank adaptation can be merged into sparse LLM.

Our optimization objective underscores the necessity of determining three critical parameters in the joint optimization process for sparse fine-tuning: sparsity mask $\boldsymbol{M}$, layer-wise sparsity rates $\boldsymbol{s}$, and the rank allocations for each layer $\boldsymbol{r}$. To derive the sparsity mask $\boldsymbol{M}$, we leverage existing sparsity methods such as SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple}. Notably, our approach is designed to be compatible with any sparsity method, offering the potential to enhance the accuracy of any sparse LLM. In the subsequent sections, we will elucidate our methodology for determining the layer-wise sparsity rates $\boldsymbol{s}$ and the layer-wise rank allocations $\boldsymbol{r}$, which are crucial components of our optimization framework.



\subsection{Layer-wise Sparsity Rate}\label{sec:layerwise_sparsity}
\paragraph{Motivation.} 
The current SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple} methods choose a uniform sparsity rate for each layer to sparsify LLMs, largely because determining layer-wise sparsity rates requires sorting the weight importance of each layer. For LLMs with billions of parameters, this is challenging and time-consuming due to computational bottleneck. However, uniform sparsity across layers is not optimal, as the contributions of each layer to the final performance vary significantly \citep{yin2023outlier}. Applying the same sparsity rate to all layers risks removing important weights \citep{lu2022learning, wang2020neural}. This leads us to consider how to overcome computational bottleneck and quickly determine the layer-wise sparsity rates for LLMs?

In this paper, we propose a metric based on Representation Mutual Information (RMI) \citep{bachman2019learning, tschannen2019mutual} for efficiently determining layer-wise sparsity rates. Specifically, the calculation of RMI relies on the feature map of each layer, allowing us to evaluate the importance of a layer by simply extracting its feature maps. This RMI-based approach for determining layer-wise sparsity rates significantly reduces computational complexity. We will provide a detailed explanation of this method below.

\paragraph{RMI for Sparsity.} 
The Information Bottleneck (IB) \citep{tishby2000information} principle elucidates the process of balancing mutual information between input and output representations in LLMs. For the hidden representation of the $i$-th layer $X_i$, the goal is to minimize the mutual information $\text{I}(X; X_i)$ between input $X$ and $X_i$ to reduce inter-layer redundancy, while simultaneously maximizing the mutual information $\text{I}(X_i; Y)$ between $X_i$ and output $Y$ to ensure the layer retains task-relevant information for accurate predictions of $Y$. Specifically, it can be formulated as:
\begin{equation}\label{eq:IB}
\min \text{I}(X; X_i) - \beta \text{I}(X_i; Y),
\end{equation}
where \( \beta \) is a trade-off parameter that balances information compression and the retention of task-relevant information. Given a LLM consist of $n$ layers, we aim to minimize redundancy not only between the input and individual layers but also across different layers within the model. To generalize the IB principle in this multi-layer context, we extend the objective as follows:
\begin{equation}
\min \sum_{i=1}^n \sum_{j=i+1}^n \left( \text{I}(X; X_i) + \text{I}(X_j; X_i) \right) - \beta \text{I}(X_i; Y).
\end{equation}\label{eq:expand_IB}

In this expanded formulation, the term \( \text{I}(X_j; X_i) (i \ne j, i, j = 1, \ldots, n) \) represents the mutual information between two distinct layers, capturing the redundancy between them. The objective is to minimize this inter-layer mutual information so that the representations learned by different layers are as independent as possible. This implies that layers that are highly correlated with others are less important. Therefore, the RMI between different layers \( \text{I}(X_j; X_i) \)  can serve as an accurate and robust indicator of the importance of LLM layers \citep{zheng2021information}.

\paragraph{Algorithm Design.} 
Now we describe how to obtain the layer-wise sparsity rate of LLM using RMI. We use $X_1, ..., X_n$ to represent the hidden representations in each layer, the RMI between the $i$-th layer and the $j$-th layer is denoted as $\text{I}(X_i, X_j)$. 
As mentioned above, a layer correlated to other layers is less important. Therefore, the importance of a specific layer $i$ is defined as:
\begin{equation}\label{eq:import}
	p_i = e^{-\sum_{j=1, j\neq i}^n\text{I}(X_i, X_j ) },
\end{equation}
where $e$ is natural constant. With the layer-wise importance $\boldsymbol{p} \in \mathbb{R} ^{n\times 1}$, determining the layer-wise sparsity rate is transformed into a linear programming problem. Formally, the layer-wise sparsity rate $\boldsymbol{s} \in \mathbb{R}^{n\times 1}$ can be determined as follows:
\begin{equation}\label{eq:HIS_prob}
	\min_{\boldsymbol{s}} \, \boldsymbol{p}^T\boldsymbol{s} \, s.t. \; \frac{1}{n}\sum_{i=1}^{n}s_i = {\Theta}.
\end{equation}

It should be noted that the RMI mentioned above is challenging to compute in practice, since the distribution in $I(X_i, X_j)$ is intractable and is time-consuming to estimate. Here, we introduce the normalized Hilbert-Schmidt Independence Criterion (HSIC) \citep{gretton2005measuring,zheng2021information,kornblith2019similarity}  \footnote{Normalized HSIC is also known as CKA \citep{kornblith2019similarity}, RV coefficient \citep{robert1976unifying}, and Tucker’s congruence coefficient \citep{lorenzo2006tucker}.} to address this issue. First, we obtain the feature maps of each layer in LLM, denoted as \(\boldsymbol{X}_1, \boldsymbol{X}_2, \ldots, \boldsymbol{X}_n\). Therefore, the RMI is calculated as:
\begin{equation}\label{eq:linear_HSIC}
\begin{aligned}
	\text{I}(X_i, X_j) \approx \text{nHSIC}_{\text{linear}}(X_i, X_j)
	&=\frac{||\boldsymbol{X}_j^T \boldsymbol{X}_i ||_F^2}{||\boldsymbol{X}_i^T \boldsymbol{X}_i ||_F ||\boldsymbol{X}_j^T \boldsymbol{X}_j ||_F},
\end{aligned}
\end{equation}
where \( || \cdot ||_F \) denotes the Frobenius norm. This means that in Eq.~\ref{eq:linear_HSIC}, we use the feature maps to estimate the RMI indicator, making the practical calculation of RMI possible. Combining Eq.~\ref{eq:HIS_prob} and Eq.~\ref{eq:linear_HSIC}, we can determine the importance of each layer of LLM across layers, thereby quickly determining the layer-wise sparsity rate of LLM. The end-to-end time to compute the layer-wise sparsity of LLaMA-2-7B \citep{touvron2023llama2} is only 48 seconds using one NVIDIA A100 80GB GPU, which is quite fast. We demonstrate the effectiveness of the above method in the ablation experiments in Section \ref{sec:AblationStudy}.

\subsection{Sparsity-Aware Rank Allocation}\label{sec:rank_allocation}
\paragraph{Problem Setup.}
Using low-rank adaptation can effectively restore the performance of sparse LLMs. However, once a LLM with non-uniform layer-wise sparsity is obtained, low-rank adaptation fine-tuning faces a critical challenge: how to allocate the layer-wise rank of low-rank adaptation for the non-uniform sparse LLM within a limited fine-tuning parameter budget? LoRA \citep{hu2021lora} uniformly assigns the same rank to all low-rank adaptations, which is inefficient because it fails to account for the variability in layer-wise reconstruction errors across the sparse LLM during fine-tuning \citep{frantar2023sparsegpt,xu2024besa}. Intuitively, layers with higher reconstruction errors should be allocated a larger fine-tuning budget, as this would help reduce the reconstruction errors more effectively.
Thus, we propose a sparsity-aware rank allocation algorithm that efficiently distributes the fine-tuning parameter budget across each layer, guided by the layer-wise reconstruction errors of the sparse LLM. The objective is to maximize the overall reduction in reconstruction error during the fine-tuning of the sparse LLM. Details of this algorithm are discussed below.

\paragraph{Algorithm Design.} 
The notation $(\mathcal{L}_{1}, \mathcal{L}_{2}, \ldots, \mathcal{L}_{n})$ denotes the reconstruction errors for $n$ layers, and the average value is $\mathcal{L}_{\text{avg}} = \frac{1}{n}\sum_{i=1}^{n}\mathcal{L}_{i}$. Meanwhile, $\Omega$ indicates the average rank of all $n$ layers. Consequently, the rank of the $i$-th layer is computed as:
\begin{equation}\label{eq:rank_allocation}
r_i = \lfloor\frac{\mathcal{L}_{i}}{\mathcal{L}_{\text{avg}}}\times \Omega \rceil
\end{equation}
where $\lfloor x \rceil$ rounds $x$ to the nearest integer. 
This formula ensures a rational allocation of the fine-tuning budget to each layer based on its reconstruction error. Layers with higher reconstruction errors are assigned larger ranks, while those with lower errors receive smaller ranks, maximizing the reduction in reconstruction error for sparse LLMs. In Section \ref{sec:AblationStudy}, we demonstrate the effectiveness of this sparsity-aware rank allocation method through ablation experiments. Additionally, we compare our approach with a strategy that allocates ranks based on sparsity rates, demonstrating that our method achieves better performance.

For training stability, when low-rank adaptations $\boldsymbol{B}, \boldsymbol{A}$ increase their rank, we concatenate random Gaussian initialized parameters $\mathcal{N} (0, \sigma^2)$ to $\boldsymbol{A}$ and zeros to $\boldsymbol{B}$. The above initialization operation is the same as LoRA \citep{hu2021lora}, so the layer’s output remains unchanged before and after new parameters added. When $\boldsymbol{B}, \boldsymbol{A}$ decrease their rank, the new adaptations directly inherit parameters from the original adaptations and the extra parameters are discarded.

\subsection{Dynamic Sparsity and Adaptation}\label{sec:DynamicSparsity}
To achieve better sparse fine-tuning LLMs, we further extend our algorithm to implement a dynamic sparsity and fine-tuning approach. This involves progressively sparsifying an increasing number of weights while simultaneously conducting low-rank adaptation fine-tuning. Dynamic sparsity and fine-tuning ensure the maximum integration of LLM sparsity and low-rank adaptation fine-tuning. We perform $T$ steps of sparsity and fine-tuning, and determine the progressive sparsity rate using the cubic sparsity schedule proposed by \citep{zhu2017prune}, as described below:
\begin{equation}\label{eq:iter_sparsity}
\Theta^{t}=\Theta^{f}-\Theta^{f}\left(1-\frac{t}{T}\right)^{3}, \enskip\enskip t=1,2,...,T 
\end{equation}
where $\Theta^{f}$ is final sparsity rate and $\Theta^{t}$ denotes the average sparsity rate of the $n$ layers at step $t$. Furthermore, since the reconstruction error tends to increase with the rising sparsity rate, we linearly increase the average rank $\Omega^{t}$ at each step, $\textit{i.e.}$, 
\begin{equation}\label{eq:iter_rank}
\Omega^{t+1}=\Omega^{t}+1, \enskip\enskip t=1,2,...,T 
\end{equation}

After calculating the average sparsity rate $\Theta^{t}$ at step $t$, we first establish the layer-wise sparsity rates $\boldsymbol{s}^{t}$ using the method outlined in Section \ref{sec:layerwise_sparsity}. Subsequently, we simultaneously sparsify the weights of LLM and low-rank adaptation by applying the sparse mask $\boldsymbol{M}^{t}$, which is derived using either the SparseGPT \citep{frantar2023sparsegpt} or Wanda \citep{sun2023simple} method. This coordinated approach ensures compatibility between the LLM weights and the low-rank adaptations, facilitating the integration of low-rank adaptations into the sparse weights of the LLM after fine-tuning. Once we have established sparse LLM, we then ascertain the layer-wise rank $\boldsymbol{r}^{t}$ for the low-rank adaptations, employing the rank allocation method described in Section \ref{sec:rank_allocation}. The full details of the algorithm are outlined in Algorithm \ref{alg:LoSA}.

\begin{figure}[h]
  \centering
  \begin{algorithm}[H]

  \KwIn{Dense weight of LLM $\boldsymbol{W}$, low-rank adaptation weight $\boldsymbol{BA}$, dynamic steps $T$, target sparsity rate $\Theta^{f}$, initial average rank $\Omega^1$.}
\KwOut{Sparse fine-tuning LLM.}
\vspace{0.5em}


\For{$t=1, \cdots, T$}{
Calculate the progressive sparsity rate $\Theta^{t}$ using Eq.~\ref{eq:iter_sparsity};\\
Obtain RMI between two layers using Eq.~\ref{eq:linear_HSIC};\\
Calculate the layer-wise importance $\boldsymbol{p}^t$ by Eq.~\ref{eq:import};\\
Obtain the layer-wise sparsity rate $\boldsymbol{s}^t$ by Eq.~\ref{eq:HIS_prob};\\
Get sparse mask $\boldsymbol{M}^t$ of $\boldsymbol{W^{t}+B^{t}A^{t}}$ through SparseGPT or Wanda;\\
Calculate the current average rank by Eq.~\ref{eq:iter_rank};\\
Allocate layer-wise rank $\boldsymbol{r}^t$ by Eq.~\ref{eq:rank_allocation};\\
Update low-rank adaptation weight $\boldsymbol{B^tA^t};$\\
}

Sparse low-rank adaptation is merged into sparse LLM weight to obtain the final sparse LLM.
\caption{Dynamic \textbf{Lo}w-rank \textbf{S}parse \textbf{A}daptation \textbf{(LoSA)}} 
\label{alg:LoSA}
\end{algorithm}
\end{figure}


\section{Experiments}\label{sec:experiments}
\label{Experiments}
\subsection{Experimental Settings}\label{sec:ExperimentalSettings}
\paragraph{Models and Baselines.} We have applied our method to several LLMs, including LLaMA-1 \citep{touvron2023llama}, LLaMA-2 \citep{touvron2023llama2}, LLaMA-3 \citep{llama3}, LLaMA-3.1 \citep{llama31}, Vicuna \citep{chiang2023vicuna}, and OPT \citep{zhang2022opt}, with parameter sizes ranging from 7 billion to 70 billion. To further validate the effectiveness of our approach in enhancing the accuracy of existing sparse methods, we selected Wanda \citep{sun2023simple} and SparseGPT \citep{frantar2023sparsegpt} as baselines, and also compare with LoRA \citep{hu2021lora}. We set the rank of LoRA to 8 and the remaining training settings for LoRA are the same as those for LoSA.

\paragraph{Evaluation.} We report perplexity of sparse LLM on WikiText-2 \citep{merity2016pointer} dataset and use lm-eval-harness \citep{gao2021framework} to evaluate the zero-shot accuracy on downstream datasets, including HellaSwag \citep{zellers2019hellaswag}, Winogrande \citep{sakaguchi2021winogrande}, BoolQ \citep{clark2019boolq}, OpenBookQA \citep{mihaylov2018can}, PIQA \citep{bisk2020piqa}, ARC-Easy, and ARC-Challenge \citep{clark2018think}. 

\paragraph{Datasets and Training Details.} 
We randomly sampled a 10K subset from the Alpaca-GPT4 \citep{peng2023instruction} to construct our fine-tuning dataset. We utilized the same calibration dataset as SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple}, which consists of 128 sequences sampled from the C4 training set \citep{raffel2020exploring} for sparsification. During the fine-tuning process, we employed the Paged AdamW optimizer \citep{dettmers2024qlora}, setting a maximum gradient norm of 0.3. The learning rate followed a linear learning rate schedule and set the learning rate to be $2\times 10^{-4}$. All experiments were conducted on NVIDIA A100 80GB GPUs. We use one GPU for the 7B, 13B, and 8B models, two GPUs for the 30B models, and three GPUs for the 70B models. We set the fine-tuning steps $T=5$ and initial average rank $\Omega^{1}=6$. 


\subsection{Language Modeling}
The perplexity results of fine-tuning the sparse LLMs at 50-70\% sparsity rate on the WikiText-2 dataset are presented in Table \ref{tab:ppl_results}. LoSA improves the performance of both the SparseGPT and Wanda methods across models of various parameter sizes and architectures. For instance, when fine-tuning 70 $\%$ sparse LLaMA-2-7B with LoSA, the perplexity of the sparse models obtained by SparseGPT and Wanda is reduced by 16.60 and 68.73, respectively. Additionally, our method significantly outperforms LoRA. These results highlight the effectiveness of our method in enhancing the language modeling capabilities of sparse LLMs.

\begin{table*}[t!]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Perplexity of \texttt{LoSA} for sparse LLMs on WikiText-2 dataset at 50/60/70$\%$ sparsity.}
\small
\label{tab:ppl_results}
\setlength{\tabcolsep}{4pt}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l|c| ccc | c c c|c|c |c}
\toprule 
 & \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{LLaMA-1} & \multicolumn{3}{c|}{LLaMA-2} & LLaMA-3 & LLaMA-3.1 &Vicuna  \\
 \midrule
   Sparsity & Method  & 7B & 13B & 30B &7B& 13B & 70B &8B & 8B & 13B \\
    \midrule
      0$\%$ & Dense  & 5.68 & 5.09 & 4.77 & 5.12 & 4.57 & 3.12 & 6.05 & 6.18 & 5.94  \\
            \midrule
    \multirow{7}{*}{50$\%$} & SparseGPT  &7.22 & 6.21 &5.31 & 6.51 &5.63 & 3.98 &9.30 & 9.18& 7.73  \\
    & w. \texttt{LoRA}   &6.91 & 6.04& 5.16&6.31 & 5.49 &3.91 &8.50  &8.49 &6.51  \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf6.86 & \bf  5.98& \bf 5.13 & \bf 6.27 & \bf 5.44 &  \bf 3.88 & \bf 8.38 & \bf 8.36 & \bf 6.46  \\
    \cmidrule{2-11}
    & Wanda  &7.26 &6.15 &5.24 &6.42& 5.56& 3.98 &9.59&9.53 & 7.29  \\
    & w. \texttt{LoRA}   &6.84 & 6.04&5.17 &6.33 &5.46 &3.94 &8.56  &8.53 & 6.53 \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf 6.82 & \bf 5.94 & \bf 5.13 & \bf 6.24 & \bf 5.41 &  \bf 3.93 & \bf 8.41 & \bf  8.42 & \bf 6.44  \\
        \midrule
 \multirow{7}{*}{60$\%$} & SparseGPT  &10.41 &8.43 & 6.81 & 10.14  &7.88 &5.10 & 14.85&15.10 & 10.02  \\
    & w. \texttt{LoRA}   & 8.29& 6.94 &6.18 &7.98 &6.75 &4.90 & 10.77 &10.73 &7.87 \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf 8.14& \bf6.81 & \bf 6.10 & \bf  7.82 & \bf 6.65 &  \bf4.88& \bf  10.58 & \bf 10.44 & \bf  7.54  \\
    \cmidrule{2-11}
       & Wanda  & 10.69&8.75 & 6.56  & 10.79& 8.40& 5.25& 20.02& 21.51&9.54  \\
    & w. \texttt{LoRA}   &8.38 &6.95 &5.99 &8.07 &6.78 &5.01 & 11.29 &11.09 & 7.82 \\
    \gr \wc &  w. \bf\texttt{LoSA} & \bf 8.20& \bf  6.75 & \bf 5.92 & \bf  7.88& \bf6.62  &  \bf 4.95 & \bf 10.85 & \bf10.59  & \bf7.59  \\
        \midrule
        \multirow{7}{*}{70$\%$} & SparseGPT  & 26.30 &19.24 & 12.56  &27.42 &20.57 & 9.46 &40.53 &39.76 & 21.95  \\
    & w. \texttt{LoRA}   &11.48 &8.95 &7.54 & 11.06&8.99 & 6.32&16.50  &16.05 &10.19  \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf 11.20 & \bf8.71  & \bf 7.21 & \bf10.82  &  \bf 8.82 & \bf 6.13 & \bf 15.74 & \bf 15.41 & \bf 9.92 \\
    \cmidrule{2-11}
      & Wanda   & 85.77 &55.90 &  17.37 &79.67 & 48.07 & 11.10 & 112.10 &109.99 &  44.89 \\
    & w. \texttt{LoRA}   &13.46 & 9.90&8.34& 12.57&9.65 &6.50 &20.25 & 18.98 &10.42  \\
    \gr \wc &  w. \bf\texttt{LoSA} & \bf11.75 & \bf8.79  & \bf 7.98 & \bf 10.94 & \bf8.86  &  \bf 6.30  & \bf 16.59 & \bf16.46 & \bf  9.94\\
     \bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\end{table*}

\subsection{Zero-shot Tasks}
We report the improvements in zero-shot accuracy on seven downstream tasks achieved by LoSA for sparse LLMs with 50-70$\%$ sparsity, obtained using SparseGPT and Wanda methods in Table \ref{tab:zero_shot_main_results}. Our LoSA method significantly enhances the zero-shot accuracy across different models ranging from 7 billion to 70 billion parameters. Notably, our LoSA method increases the average zero-shot accuracy of the 70$\%$ sparse LLaMA-2-7B obtained via Wanda by 16.32$\%$, surpassing LoRA by 3.47$\%$. These experimental results highlight the substantial enhancements in understanding and reasoning capabilities of sparse LLMs brought about by our LoSA approach.

\begin{table*}[t!]
\centering
\renewcommand{\arraystretch}{1.1}
\caption{Mean Zero-shot accuracy results of \texttt{LoSA} for sparse LLMs on the HellaSwag, Winogrande, BoolQ, OpenBookQA, PIQA, ARC-Easy and ARC-Challenge datasets at 50/60/70$\%$ sparsity. The detailed accuracy of each dataset can be found in Appendix \ref{app:DetailedZero-shotTaskResults}.}
\small
\label{tab:zero_shot_main_results}
\setlength{\tabcolsep}{4pt}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{l|c| ccc | c c c|c| c |c}
\toprule 
 & \multicolumn{1}{c|}{} & \multicolumn{3}{c|}{LLaMA-1} & \multicolumn{3}{c|}{LLaMA-2} & LLaMA-3 & LLaMA-3.1 &Vicuna  \\
 \midrule
   Sparsity & Method  & 7B & 13B & 30B &7B& 13B & 70B &8B &8B & 13B  \\
    \midrule
      0$\%$ & Dense  & 61.74 & 63.84 &  67.41 & 61.88& 65.00& 69.14 &  65.62 &65.93  & 65.53   \\
            \midrule
    \multirow{7}{*}{50$\%$} & SparseGPT  &57.96 & 60.93 &65.34&59.29  &62.53& 68.93 & 60.99&  61.22 & 63.40  \\
    & w. \texttt{LoRA}   &  59.83&  62.68&  66.31& 60.47&63.90 &69.56 & 63.20  & 64.25 &63.56  \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf 60.54 & \bf 63.09& \bf66.86 & \bf61.32 & \bf 64.23 &  \bf69.82 & \bf  64.20 & \bf 64.59 & \bf 64.28  \\
    \cmidrule{2-11}
    & Wanda  &57.08 & 61.39 &  65.59 &59.46 & 62.88 & 68.21&59.65 &59.58  & 63.74 \\
    & w. \texttt{LoRA}   &59.46 & 63.07 & 66.37 &60.51 &63.84 &69.04 & 62.65 & 63.05  & 64.16  \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf  60.09 & \bf 63.49 & \bf67.19  & \bf 60.85 & \bf 64.32 &  \bf 69.65 & \bf63.20  & \bf 63.63 & \bf64.41  \\
        \midrule
 \multirow{7}{*}{60$\%$} & SparseGPT  &52.72& 56.57& 61.63& 53.90&  58.20&  66.27&54.05 &55.80 &  60.25 \\
    & w. \texttt{LoRA}   & 55.92 & 60.00 &  64.90&  57.50 &61.08 &68.32 &58.98  &60.19 &61.26  \\
    \gr \wc &  w. \bf\texttt{LoSA} & \bf 57.38 & \bf61.06  & \bf 65.97 & \bf 58.52 & \bf 61.67 &  \bf 69.04 & \bf60.30  & \bf60.74  & \bf 61.67 \\
    \cmidrule{2-11}
       & Wanda  & 51.98 &56.19 & 62.46 &52.51 & 58.19 &66.27 & 48.90&49.78  & 60.19  \\
    & w. \texttt{LoRA}   &55.39 & 59.67 & 64.50&  56.83& 60.91& 68.24& 57.09& 57.61 &60.89 \\
    \gr \wc &  w. \bf\texttt{LoSA}  & \bf56.21 & \bf60.88 & \bf 65.86 & \bf 58.06 & \bf61.82  &  \bf69.08 & \bf 58.60 & \bf 58.64 & \bf 61.42 \\
        \midrule
        \multirow{7}{*}{70$\%$} & SparseGPT  &43.60 &48.00 &53.64 & 43.07   &  47.38 & 60.84 & 43.02& 42.83 & 48.53  \\
    & w. \texttt{LoRA}   & 50.41 &55.00 &61.63 & 50.76& 55.16 &65.72 & 50.36 & 52.33&  55.15  \\
    \gr \wc &  w. \bf\texttt{LoSA} & \bf 52.74& \bf 56.53 & \bf 62.37  & \bf52.51  & \bf57.16  &  \bf66.41 & \bf 51.99 & \bf 54.06 & \bf 56.40\\
    \cmidrule{2-11}
      & Wanda   & 37.45 &  40.79& 53.35 &  35.33& 38.88 & 58.48  & 35.42 & 36.10& 42.06 \\
    & w. \texttt{LoRA}   &48.30 & 51.83 &59.06 &48.18 &50.62 &65.27 &47.02 &47.52 & 52.34 \\
    \gr \wc &  w. \bf\texttt{LoSA} & \bf51.20 & \bf 55.31 & \bf 61.57 & \bf 51.65 & \bf53.00  &  \bf 66.12 & \bf 50.37 & \bf50.33  & \bf 56.37  \\
     \bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\end{table*}

\subsection{N:M Sparsity} \label{sec:NMSparsity}
We extend LoSA to N:M sparsity and adopt a mixed N:8 sparsity (N refers to non-zero weights) configuration following DominoSearch \citep{sun2021dominosearch}. We allow different layers to have distinct N values while maintaining a constant overall sparsity ratio. We assign lower N values to more important layers and the N value for each layer are determined using the method described in Section \ref{sec:layerwise_sparsity}. The results are presented in Table \ref{tab:NM}. It is evident that LoSA improves the accuracy of the LLaMA-2-7B under N:M sparsity and outperforms LoRA.

\begin{table}[ht]
  \begin{minipage}[b]{0.45\linewidth}
    \centering
    \caption{Perplexity and mean zero-shot accuracy of mixed N:M sparsity.}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lccc}
\toprule
Method & N:M Sparsity  & Perplexity & Accuracy \\ 
 \midrule
SparseGPT  &2:8  &103.76 & 33.27 \\
w. \texttt{LoRA} & 2:8  & 22.47 &41.46 \\
\gr  \bf w. \texttt{LoSA}  &\bf  Mixed 2:8 & \textbf{19.97} & \textbf{43.77} \\
\midrule
Wanda  &2:8  &3006.24 &32.71 \\
w. \texttt{LoRA} & 2:8  & 56.14 &38.72 \\
\gr   \bf w. \texttt{LoSA}  &  \bf Mixed 2:8 & \textbf{25.41} & \textbf{41.91} \\
\bottomrule
    \end{tabular}}\label{tab:NM}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.55\linewidth}
    \centering
    \caption{The perplexity of the LLaMA-2-7B at different sparsity rates.}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{l| cccccc}
\toprule 
   Sparsity & 40\% & 50\% & 60\% & 70\% & 80\% &  90\% \\
         \midrule
     SprseGPT &6.12& 6.51&10.14 & 27.42 & 115.50 & 1439.35  \\
    w. \texttt{LoRA}  &6.08&6.30 &7.98 & 11.06& 26.35 &93.16 \\
  \gr   \bf w. \texttt{LoSA}  &\bf 6.05& \bf 6.25 & \bf7.82 & \bf10.82  & \bf 21.54 & \bf 84.39 \\ 
\midrule
     Wanda &6.07&6.42&10.79 &79.67 &1980.85  &17151.30 \\
    w. \texttt{LoRA} &6.04 &6.31 & 8.07&12.57 & 36.43&335.43 \\
  \gr   \bf w. \texttt{LoSA} & \bf6.02  & \bf 6.21& \bf7.88 &\bf 10.94  & \bf 24.38 & \bf 168.71  \\ 
     \bottomrule
\end{tabular}}\label{tab:acrossrates}
  \end{minipage}
\end{table}

\subsection{Ablation Study}\label{sec:AblationStudy}
\paragraph{Robustness across Different Sparsity Rates.}
Table \ref{tab:acrossrates} presents the results of perplexity for sparse LLaMA-2-7B across different sparsity rate, ranging from 40$\%$ to 90$\%$. These results demonstrate that LoSA consistently reduces the perplexity of both SparseGPT and Wanda across all sparsity levels and consistently outperforms LoRA. This validates the robustness and effectiveness of LoSA method at various sparsity rates, ensuring reliable performance even as the pruning level varies.


\paragraph{Effectiveness of the Proposed Strategies.}
In this paper, we propose three strategies: Layer-wise Sparsity Rate (\textbf{LSR}, Section \ref{sec:layerwise_sparsity}), Sparsity-Aware Rank Allocation (\textbf{SRA, Section \ref{sec:rank_allocation}}), and Dynamic Sparsity and Adaptation (\textbf{DSA}, Section \ref{sec:DynamicSparsity}). To demonstrate the effectiveness of three strategies, we conduct an ablation study in Table \ref{tab:Ablation_Study}. The first row of the table presents the result of the 70$\%$ sparse LLaMA-2-7B obtained by Wanda and further fine-tuned by LoSA. We then progressively remove each strategy from LoSA \textcolor{green}{(rows 2-4)}, combinations of two strategies \textcolor{orange}{(rows 5-6)}, and all three strategies \textcolor{red}{(row 7)}. We can see that removing any strategy causes a decrease in the final accuracy. The severity of the accuracy drop follows the order: \textcolor{red}{removing three strategies} $>$ \textcolor{orange}{removing two strategies} $>$ \textcolor{green}{removing one strategy}. Additionally, among the three strategies, we found that DSA contributed the most to the final accuracy. The experimental results demonstrate that all three proposed strategies contribute to the final accuracy, and using all three strategies achieves the best results.

\begin{table}[ht]
  \begin{minipage}[b]{0.53\linewidth}
    \centering
    \caption{Ablation of the proposed strategies.}
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{lcc}
\toprule
Method  & Perplexity & Accuracy \\ 
 \midrule
 \gr  \bf \texttt{LoSA} & \bf10.94 & \bf 51.65 \\
  \midrule
\textcolor{green}{w/o LSR} &\textcolor{green}{11.36 (+0.42)} & \textcolor{green}{50.62 (-1.03)} \\
\textcolor{green}{w/o SRA} & \textcolor{green}{11.44 (+0.50)} &\textcolor{green}{50.94 (-0.71)}  \\
\textcolor{green}{w/o DSA} &\textcolor{green}{11.78 (+0.84)} &\textcolor{green}{49.90 (-1.75)}  \\
\textcolor{orange}{w/o LSR $\&$ SRA} & \textcolor{orange}{11.94 (+1.00)} &  \textcolor{orange}{48.81 (-2.84)} \\
\textcolor{orange}{w/o LSR $\&$ DSA} & \textcolor{orange}{12.23 (+1.29)}&   \textcolor{orange}{48.30 (-3.35)}\\
\textcolor{orange}{w/o SRA $\&$ DSA}&  \textcolor{orange}{12.03 (+1.09)} &  \textcolor{orange}{48.54 (-3.11)} \\
\textcolor{red}{w/o LSR $\&$ SRA $\&$ DSA} & \textcolor{red}{12.74 (+1.80)} & \textcolor{red}{47.76 (-3.89)} \\
\bottomrule
    \end{tabular}}\label{tab:Ablation_Study}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.47\linewidth}
\centering
\caption{Experimental results of OPT-13B.}
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{@{}lccc}
\toprule 
Method & Sparsity  & Perplexity & Accuracy \\ 
 \midrule
 Dense & 0$\%$ & 10.13& 55.22 \\
  \midrule
SparseGPT  &70$\%$  &20.26 & 47.68 \\
w. \texttt{LoRA} & 70$\%$  & 17.73 & 51.34 \\
\gr  \bf w. \texttt{LoSA}  & \bf 70$\%$ & \textbf{17.05} & \textbf{52.31} \\
\midrule
Wanda  &70$\%$  & 73.70& 41.51 \\
w. \texttt{LoRA} &70$\%$  &  20.54 & 49.16\\
\gr  \bf w. \texttt{LoSA}  & \bf 70$\%$ & \textbf{19.75} & \textbf{50.13} \\
\bottomrule
\end{tabular}
}\label{tab:opt}
\end{minipage}
\end{table}

\paragraph{Experimental results for OPT model.}
We show experimental results of LoSA fine-tuning 70$\%$ sparse OPT-13B \citep{zhang2022opt} in Table \ref{tab:opt}. LoSA effectively restores the accuracy of sparse model that are not based on the LLaMA architecture, and its performance is better than LoRA.

\begin{wraptable}{r}{4.4cm}
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\caption{Different ways to allocate rank. SR: Sparsity Rate RE: Reconstruction Error.}\label{tab:vsSparsityRate}
\begin{tabular}{@{}l cc}
\toprule 
   Method & Perplexity & Accuracy  \\
    \hline
   SR &11.37 & 50.97 \\
    \gr  \bf RE & \bf10.94 &\bf 51.65 \\
    \midrule
\end{tabular}
\end{wraptable}

\paragraph{Reconstruction Error vs. Sparsity Rate.} 
In Section \ref{sec:rank_allocation}, we allocate fine-tuning parameters per layer based on reconstruction error. Table \ref{tab:vsSparsityRate} compares this with an alternative method that assigns higher ranks to layers with higher sparsity. Specifically, we used the Wanda to obtain a 70$\%$ sparse LLaMA-2-7B model, and carried out LoSA fine-tuning. The results show that reconstruction error-based allocation outperforms sparsity-based allocation, likely because the optimization goal is to minimize error between the dense and sparse LLM, sparsity rate does not adequately reflect changes in reconstruction error. Addtionally, computing the reconstruction error takes only 46 seconds on an NVIDIA A100 80GB GPU, making it a better proxy for rank allocation.

\paragraph{Dynamic Steps.} 
We present the ablation study of the dynamic steps T in Figure \ref{fig:StepandRank}. Steps T determines the frequency at which the sparsity rate increases. A larger T means the sparsity rate increases more slowly, and fewer parameters are removed each time. We show the impact of different values of T on the final perplexity while keeping the number of fine-tuning samples constant. Specifically, we demonstrate the results of fine-tuning a 70$\%$ sparse LLaMA-2-7B model obtained using the Wanda method with LoSA. Increasing T appropriately can effectively reduce the model's perplexity. However, a larger T may result in insufficient training of the model after each sparsification, which in turn leads to a further increase in perplexity.

\paragraph{Rank Budget.} 
We show the impact of different rank budgets on the LLM's perplexity in Figure \ref{fig:StepandRank}, with \(\Omega^1 = 2, 6, 10, 16\), for the 70 $\%$ sparse LLaMA-2-7B obtained using Wanda. All experiments are conducted with a fixed set of 10K fine-tuning samples. Increasing the rank budget appropriately can effectively reduce perplexity, leading to a better recovery of the sparse model's performance. However, since the fine-tuning samples are fixed, further increasing the rank budget results in insufficient training of low-rank adaptation, which causes an increase in perplexity.

\subsection{Analysis}\label{sec:analysis}

\paragraph{Fine-tuning Efficiency.}
In Table \ref{tab:fine_tuning_time}, we demonstrate the fine-tuning efficiency of LoSA. We compared the fine-tuning parameters, time, and GPU memory usage between LoSA and LoRA. LoSA requires fewer fine-tuning parameters, only $1-s \%$ of LoRA's (where $1-s \%$ is the sparsity rate), and its GPU memory usage is similar to LoRA. However, since LoSA performs $T=5$ rounds of sparsification and calculates layer-wise sparsity rates and rank allocation, it takes more time for fine-tuning. Nevertheless, LoSA provides better accuracy and lower inference latency compared to LoRA, and it only requires about an hour of fine-tuning, which we believe is a worthwhile trade-off.

\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.43\linewidth}
        \centering
        \includegraphics[height=3cm]{./figures/fig2.pdf} 
        \caption{Effect of different steps and ranks on perplexity.}
        \label{fig:StepandRank}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.55\linewidth}
        \centering
        \captionof{table}{Fine-tuning efficiency of \texttt{LoSA}.}
        \resizebox{\linewidth}{!}{
            \begin{tabular}{@{}l cccc}
                \toprule 
                Method &  \makecell[c]{Fine-tuning\\Params (M)} & \makecell[c]{Fine-tuning\\Time (min)}  & \makecell[c]{Fine-tuning\\Memory (GB)}   \\
                \toprule 
                Wanda   & $0$  &  $0$ & $0$ \\
                w. \texttt{LoRA}  & $20.28$  &  $13.78$ & $53.59$  \\
                \gr   w. \texttt{LoSA} &  $20.28\times (1-s\%)$  &  $45.34$  &   $53.92$ \\
                \midrule
                SparseGPT  & $0$ &  $0$ &  $0$ \\
                w. \texttt{LoRA}   & $20.28$  &  21.40 & $53.59$ \\
                \gr  w. \texttt{LoSA}  & $20.28\times(1-s\%)$  & 73.91  &  $53.92$ \\
                \midrule
            \end{tabular}
        }
        \label{tab:fine_tuning_time}
    \end{minipage}
\end{figure}

\paragraph{Inference Speedup.} We analyzed the acceleration effect of the sparse LLaMA-2-7B, as shown in Table \ref{tab:speedup}. We measured the end-to-end time of the model generate tokens using the DeepSparse \citep{deepsparse} inference engine on an Intel(R) Xeon(R) Silver 4314 CPU and the nm-vllm \citep{nm-vllm} inference engine on a NVIDIA RTX 4090 24GB GPU. Compared to the dense model, our method achieves a remarkable 1.77-2.60$\times$ speedup on CPU and 1.71-2.23$\times$ speedup on GPU at 50-70$\%$ sparsity. In contrast, LoRA cannot merge weights into sparse weights which introduces additional inference latency that increases with higher sparsity rates. This demonstrates the advantage of our LoSA method in maintaining the acceleration performance of sparse LLMs.

\begin{table*}[h]
\vspace{-0.5em}
\centering
\caption{The end-to-end inference speedup of sparse LLaMA-2-7B on CPU and GPU.}\label{tab:speedup}
\setlength{\tabcolsep}{10pt}
\resizebox{0.9\textwidth}{!}{
\begin{tabular}{c|c|c|c>{\columncolor{mygray}}c|c>{\columncolor{mygray}}c|c>{\columncolor{mygray}}c}
\toprule
\bf \multirow{1}{*}{Device} & \bf \multirow{1}{*}{Sparsity} & \bf \multirow{1}{*}{Dense} & \multicolumn{2}{c|}{\bf 50\%} & \multicolumn{2}{c|}{\bf 60\%} & \multicolumn{2}{c}{\bf 70\%} \\&  & &\texttt{LoRA} & \bf \texttt{LoSA} &  \texttt{LoRA} & \bf \texttt{LoSA} & \texttt{LoRA}& \bf\texttt{LoSA} \\
\midrule
\multirow{2}{*}{CPU}  &\makecell[c]{Throughput\\(tokens/s)} $\uparrow$ & 3.43 &  5.68 & \bf6.08 &  6.64& \bf7.41 & 7.88 &\bf8.93   \\ 
\cmidrule{2-9}
&\makecell[c]{\bf Speedup} \bf $\uparrow$  &1.00$\times$ & 1.65$\times$ & \bf1.77$\times$ & 1.94$\times$ & \bf2.16$\times$ &2.29$\times$& \bf 2.60$\times$  \\
\midrule
\multirow{2}{*}{GPU}  &\makecell[c]{Throughput\\(tokens/s)} $\uparrow$ & 57.35 &  79.63 & \bf97.88 & 88.58 & \bf111.82 & 98.10 &\bf 127.69   \\ 
\cmidrule{2-9}
&\makecell[c]{\bf Speedup} \bf $\uparrow$  &1.00$\times$ & 1.39$\times$ & \bf1.71$\times$ & 1.54$\times$ & \bf1.95$\times$ &1.71$\times$& \bf 2.23$\times$  \\
\bottomrule
\end{tabular}}
\end{table*}

\vspace{-1em}
\section{Conclusion}
In this paper, we propose a novel dynamic low-rank sparse adaptation method for the efficient fine-tuning of sparse LLMs. Our method simultaneously sparsifies both LLM and low-rank adaptation, ensuring that low-rank adaptation can be merged into LLM weight post-training, thereby not increasing inference latency. Moreover, we introduce representation mutual information as an effective and efficient metric for dynamically determining the layer-wise sparsity rates during fine-tuning. Additionally, we dynamically adjust the rank of low-rank adaptation based on the layer-wise reconstruction error changes during sparsity, ensuring a efficient fine-tuning budget allocation for each layer. Extensive experiments demonstrate the effectiveness of our method in fine-tuning sparse LLMs.


\section*{Reproducibility Statements}
The experimental setup is described in Section \ref{sec:ExperimentalSettings} and code is available at \url{https://github.com/wzhuang-xmu/LoSA}.

\section*{Acknowledgments}
This work was supported by  the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. U23A20383, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No. 2021J06003, No.2022J06001).


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Limitation}
Although our LoSA method effectively enhances the accuracy of existing sparsity techniques under different sparsity settings, there is still a gap to achieving lossless high-ratio sparsity for LLMs. This underscores the need for future exploration of more efficient fine-tuning methods to further improve the accuracy of sparse LLMs.

\section{Related Work}
\label{RelatedWork}
\paragraph{LLM Sparsity.} 
Existing sparsity methods, including SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple}, enable training-free sparsity of LLMs, effectively eliminating non-essential weights while striving to preserve model performance as much as possible. However, existing sparsity methods can lead to significant accuracy degradation under high sparsity rates, partly because these methods pre-set uniform layer-wise sparsity rates, overlooking the fact that redundancy levels vary between different layers of LLMs \citep{song2024sleb,men2024shortgpt,chen2024compressing}. OWL \citep{yin2023outlier} has recognized this issue and employs heuristic metrics to set the sparsity rates of LLMs inversely proportional to the ratio of observed activation outliers within each layer, thereby achieving a non-uniformly sparse LLMs. While setting non-uniform pruning rates can partially improve the accuracy of sparse LLMs, the accuracy still remains unsatisfactory. Therefore, fine-tuning sparse LLMs to restore their accuracy is necessary. This paper proposes using low-rank sparse adaptation to restore the accuracy of sparse LLMs and dynamically determine the layer-wise sparsity rates using representation mutual information during the fine-tuning process. Representation mutual information \citep{bachman2019learning, tschannen2019mutual} have been successfully applied to prune small models such as CNNs and BERT \citep{zheng2021information,fan2021layer, hu2024mpruner}. However, its application in pruning LLMs has not been well explored. In this paper, we derive the use of the representation mutual information metric to efficiently and rapidly determine the relative importance of each layer in LLMs during sparse fine-tuning. Through extensive experiments, we validate the effectiveness of the representation mutual information metric in pruning LLMs.


\paragraph{Low-Rank Adaptation (LoRA).} 
LoRA \citep{hu2021lora} stands out as a highly effective parameter-efficient fine-tuning (PEFT) method \citep{houlsby2019parameter, pfeiffer2020adapterfusion, lester2021power, liu2021p}, which incorporates trainable low-rank matrices that seamlessly reintegrate into the original model weights post-tuning, ensuring maintained efficiency without added latency or memory overhead. In LoRA fine-tuning, a crucial rank parameter dictates the tuning budget for each layer. AdaLoRA \citep{zhang2023adaptive} underscores the importance of adaptive allocation, suggesting that this budget be tailored according to the significance score of each weight matrix. SoRA \citep{ding2023sparse} is to dynamically adjust the rank of low-rank adaptation in the training process with a sparse gating unit trained by proximal gradient method. ALoRA \citep{liu2024alora} evaluates the importance of each rank, iteratively prunes low-contribution ranks, and reallocates resources to achieve dynamic adjustment of ranks. Similarly, we have identified the issue with the distribution of fine-tuning parameters during the fine-tuning of sparse LLMs. Uniformly setting the rank size like LoRA, does not effectively restore the accuracy of sparse LLMs. Therefore, this paper advocates for the dynamic allocation of the rank parameter budget, based on the sparse reconstruction errors across different layers, to optimize tuning efficacy. Although both LoSA and previous related works propose adjusting the rank in LoRA to achieve efficient parameter allocation, LoSA's dynamic rank adjustment strategy is specifically designed for sparse LLMs. Allocating fine-tuning parameters based on reconstruction error helps minimize the reconstruction error of sparse LLMs.

\begin{table}[b]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\vspace{-0.2cm}
\caption{Experimental results of comparison between cubic and linear sparsity schedule.}\label{tab:linear_schedule}
\vspace{-0.2cm}
\begin{tabular}{@{}l cc}
\toprule 
   Method & Perplexity & Accuracy  \\
    \hline
   Linear &8.04 &  57.79 \\
    \gr  \bf Cubic & \bf 7.88 & \bf 58.06 \\
    \midrule
\end{tabular}
\end{table}

\paragraph{Joint Sparsity and LoRA.} 
Combining network sparsity with LoRA has been shown to effectively enhance the accuracy of sparse LLMs \citep{li2024nuteprune,li2024lorap,zhao2024apt}. For instance, LLM-Pruner \citep{ma2023llm} executes a one-shot structured pruning of LLMs, followed by fine-tuning using LoRA. LoRAPrune \citep{zhang2023pruning} implements iterative structured pruning, where weight importance is determined by replacing gradients on full weights with those calculated via LoRA. LoSparse \citep{li2023losparse} performs structured pruning on LLMs, using a combination of low-rank and sparse matrices to approximate the original weight matrix. LoRAShear \citep{chen2023lorashear} utilizes LoRA in conjunction with dynamic fine-tuning strategies to reinstate knowledge in structural pruning LLMs. All these studies apply LoRA to fine-tune structural pruning LLMs. Adjusting the input/output dimensions of the two low-rank adaptations in LoRA and integrating them into the structural pruning weights is straightforward \citep{zhao2024apt,guo2023compresso}. However, this approach is not viable for unstructured pruning (network sparsity). Unstructured pruning removes individual weights, resulting in sparse LLMs. In contrast, low-rank adaptations remain dense even after dimensional adjustments, making it impossible to merge them into sparse LLMs. Consequently, this paper aims to explore effective techniques for integrating low-rank adaptations into the sparse weights of LLM. The goal is to ensure that sparse LLMs and low-rank adaptations share the same sparse mask, thereby the model's sparsity is preserved and inference latency remains unaffected.

\vspace{-0.5cm}
\section{More Ablation Studies} \label{app:MoreAblationStudies}
\subsection{Cubic vs. Linear Sparsity Schedule}
In Section \ref{sec:DynamicSparsity}, we gradually increase the sparsity rate using cubic sparsity schedule, where we compare this with the setting of linearly increasing the sparsity rate. Linear sparsity schedule can be be expressed as:
\begin{equation}\label{eq:linear_schedule}
\Theta^{t}=\Theta^{f}-\Theta^{f}\left(1-\frac{t}{T}\right), \enskip\enskip t=1,2,...,T 
\end{equation}
where $\Theta^{f}$ is final sparsity rate and $\Theta^{t}$ denotes the average sparsity rate of the $n$ layers at step $t$.

We present the impact of using a cubic sparsity schedule versus a linear sparsity schedule on fìnal accuracy in Table \ref{tab:linear_schedule}. Specifically, we provide results of LoSA fìne-tuning LLaMA-2-7B at a 60$\%$ sparsity rate, as obtained using Wanda method. The cubic sparsity schedule consistently outperforms the linear sparsity schedule in terms of accuracy. Compared to the linear sparsity schedule, which removes redundant connections uniformly, the cubic sparsity schedule prunes the network more aggressively in the initial phase when redundant connections are abundant, and then gradually reduces the number of weights pruned each time as fewer weights remain in the network. Our experiments showed that the cubic sparsity schedule performed better than the linear sparsity schedule, which is why we adopted it. 

\subsection{Partial or All Linear Layers?} 
In our experiments, we applied low-rank adaptation to all linear layers in both the attention and MLP modules of the LLM. We present the experimental results of applying low-rank adaptation to only a subset of the linear layers in Table \ref{tab:partiallinear}. Specifically, we experimented with applying low-rank adaptation solely to the linear layers within the attention of 60$\%$ sparse LLaMA-2-7B, as obtained using Wanda. The results indicated that partial application of low-rank adaptation yielded worse performance compared to applying it to all linear layers. We believe that since all the linear layers are sparse, low-rank adaptation should be added to all of them to maximize the recovery of accuracy.

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\vspace{0cm}
\caption{Experimental results of adding low-rank adaptation to partial linear layers.}\label{tab:partiallinear}
\begin{tabular}{@{}l cc}
\toprule 
   Linear Layer & Perplexity & Accuracy  \\
    \hline
   Q,V & 8.22& 56.59 \\
   Q,K,V,O& 8.13& 56.95 \\
    \gr  \bf All &\bf 7.88 & \bf 58.06 \\
    \midrule
\end{tabular}
\end{table}

\subsection{Comparison with Structured Pruning.} 
We compare the performance of structured pruning and unstructured pruning in Table \ref{tab:vsStructuredPruning}. Specifically, for structured pruning, we use the LLM-Pruner \citep{ma2023llm} method to obtain the pruned LLM and then perform LoRA fine-tuning on the pruned LLM, with the fine-tuning settings referenced in Section \ref{sec:ExperimentalSettings}. The acceleration data for structured pruning is obtained from LLM-Pruner paper. From the experimental data in the table, we can see that unstructured pruning can achieve basically the same acceleration effect on the GPU as structured pruning, while maintaining significantly better accuracy than structured pruning.

\begin{table}[ht]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\caption{Comparison results of unstructured pruning and structured pruning.}\label{tab:vsStructuredPruning}
\begin{tabular}{@{}l cccc}
\toprule 
   Method &Sparsity & Perplexity & Accuracy  &Speedup \\
    \hline
    LLaMA-2-7B& 0$\%$ &5.12 & 61.88&1.00$\times$ \\
    \makecell[c]{LLM-Pruner \\ w. \texttt{LoRA}} &Structured 50$\%$ & 23.25  &  51.02 &\bf 1.85 $\times$\\
    \gr  \bf \makecell[c]{SparseGPT\\ w. \texttt{LoSA}} & \bf Unstructured 50$\%$  &\bf 6.25 & \bf61.32 &  1.71 $\times$\\
    \midrule
\end{tabular}
\end{table}

\subsection{LoSA vs. "Fine-tune first, then sparsify"} 
A key issue addressed by LoSA is that LoRA cannot be merged into sparse LLMs. However, there is a simple solution to this problem: first fine-tune the LLM using LoRA, and then apply sparsification methods such as SparseGPT or Wanda. We call the above method as "Fine-tune first, then sparsify". We compare LoSA with this "Fine-tune first, then sparsify" approach in Table \ref{tab:Fine-tunefirst}. We use the Wanda method to obtain sparse LLaMA-2-7B model and use LoSA or the "Fine-tune first, then sparsify" method to fine-tune. As shown in the experimental data from Table \ref{tab:Fine-tunefirst}, the accuracy of the "Fine-tune first, then sparsify" method is lower than that of LoSA, especially in high sparsity settings, where it leads to severe accuracy degradation. In contrast, LoSA adopts an iterative sparse fine-tuning approach that maintains good accuracy and can be merged into sparse LLMs, further demonstrating the superiority of the LoSA method.

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\vspace{0cm}
\caption{LoSA vs. "Fine-tune first, then sparsify".}\label{tab:Fine-tunefirst}
\begin{tabular}{@{}l ccc}
\toprule 
   Method & Sparsity &Perplexity & Accuracy  \\
    \hline
  LLaMA-2-7B & 0 $\%$&5.12 &61.88 \\
   \midrule
   "Fine-tune first, then sparsify" &50 $\%$ & 7.00&60.35 \\
    \gr  \bf LoSA &\bf 50 $\%$ &\bf 6.21 & \bf 60.85\\
    \midrule
       "Fine-tune first, then sparsify" &60 $\%$ &11.00 & 53.44\\
    \gr  \bf LoSA &\bf 60 $\%$ & \bf7.88&\bf 58.06 \\
    \midrule
    "Fine-tune first, then sparsify" &70 $\%$ &77.57 &34.99 \\
    \gr  \bf LoSA &\bf 70 $\%$ &\bf 10.94 & \bf 51.65 \\
    \midrule
\end{tabular}
\end{table}

\subsection{LoSA vs. Sparse LoRA}
We show the effect of fine-tuning the sparse LLMs using Sparse LoRA, where Sparse LoRA is an improved version of LoRA that has the same mask as sparse LLMs and can be merged into sparse LLMs. Specifically, we show the results for the 70$\%$ sparse LLaMA-2-7B \citep{touvron2023llama2} model in Table \ref{tab:SparseLoRA}.
\begin{table*}[h]
  \centering
  \small
  \caption{LoSA vs. Sparse LoRA.
  }\label{tab:SparseLoRA}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{@{}lccccccccc}
  \toprule
Method  &  Perplexity & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
 LLaMA-2-7B &5.12&57.17&68.90&77.74&31.40&78.07&76.39&43.52&61.88\\
  \cmidrule{1-10}
  SparseGPT&27.42&33.08&58.41&64.89&17.40&62.46&43.22&22.01&43.07\\
 w. \texttt{Sparse LoRA}&11.26&43.63&62.06&63.46&22.80&70.84&57.22&29.01&49.86\\
 w. \texttt{LoRA}&11.06&44.80&62.90&63.36&24.20&71.22&58.71&30.12&50.76\\
  \gr \bf w.\texttt{LoSA} &   \bf10.82  & \bf 46.06 & \bf 63.85 & \bf70.15   & \bf24.80 & \bf71.93 & \bf 60.44 &   \bf30.35 &\bf 52.51 \\ 
   \cmidrule{1-10}
  Wanda&79.67&27.92&49.33&52.87&12.60&55.33&30.60&18.69&35.33\\
 w. \texttt{Sparse LoRA}&12.74&40.53&56.84&64.08&22.20&68.53&55.77&26.37&47.76\\
 w. \texttt{LoRA}&12.57&40.77&57.22&64.19&22.40&68.55&57.32&26.79&48.18\\
  \gr \bf w. \texttt{LoSA} &   \bf10.94  & \bf 45.10 & \bf 60.93 & \bf67.65   & \bf25.20 & \bf71.06 & \bf 62.50 &   \bf29.10 &\bf 51.65 \\
\bottomrule
\end{tabular}
}
\end{table*}

Since Sparse LoRA's Low-rank adaptation is also sparse, its accuracy is worse than LoRA, and it is also much worse than LoSA. Additionally, since both Sparse LoRA and LoSA can be merged into sparse LLMs, their inference acceleration effects are basically the same, and both outperform LoRA. Overall, our proposed LoSA method outperforms both Sparse LoRA and LoRA in terms of accuracy and inference acceleration.

\subsection{Ablation experiment on OPT model.}
We present ablation experiments of our proposed strategies on the LLaMA-2-7B model in Section \ref{sec:AblationStudy}. In this section, we further demonstrate the effectiveness of our two proposed strategies, including Layer-wise Sparsity Rate (\textbf{LSR} and Section \ref{sec:layerwise_sparsity}) and Sparsity-Aware Rank Allocation (\textbf{SRA, Section \ref{sec:rank_allocation}}), on the OPT model which is non-LLaMA architecture. Specifically, we used the Wanda method to obtain a 70$\%$ sparse OPT-13B \citep{zhang2022opt} model. The experimental results are in Table \ref{tab:opt_ablation}.
\begin{table*}[h]
  \centering
  \small
  \caption{Ablation experiment results on the OPT model.
  }\label{tab:opt_ablation}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{@{}lccccccccc}
  \toprule
Method  &  Perplexity & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
  \gr \bf LoSA &   \bf19.75  & \bf 45.20 & \bf 59.91& \bf 60.96 & \bf24.80   & \bf73.39 & \bf57.65 & \bf 29.01 &   \bf50.13 \\ 
    w/o LSR &20.72&44.21&59.32&59.34&24.20&72.45&57.10&28.15&49.24\\
 w/o SRA &20.55&44.84&59.66&59.24&24.40&72.69&57.07&28.41&49.47\\
 w/o LSR \& SRA &21.48&43.35&58.78&58.65&23.90&72.09&56.45&27.56&48.68\\
\bottomrule
\end{tabular}
}
\end{table*}

We can see that removing either the LSR or SRA leads to a decrease in LoSA accuracy. The results demonstrate the soundness and effectiveness of LSR and SRA across different architectures.


\section{Extending LoSA to structured pruning}
Although LoSA focuses on fine-tuning unstructured pruned LLMs, we extended the LoSA method to fine-tune structured pruned LLMs in this section. We use the Wanda-sp \citep{an2024fluctuation} method to determine the mask of structured pruned LLMs. We compared LoSA with SliceGPT \citep{ashkboos2024slicegpt}, LLM-Pruner \citep{ma2023llm}, LoRAPrune \citep{zhang2023pruning} and LoRAShear \citep{chen2023lorashear} on the LLaMA-1-7B model \citep{touvron2023llama} with 20$\%$ pruning rate. The experimental results are reported in Table \ref{tab:structured_pruned_LLMs}.
\begin{table*}[h]
  \centering
  \small
  \caption{Experimental results of LoSA fine-tuning structured pruned LLMs.
  }\label{tab:structured_pruned_LLMs}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{@{}lccccccccc}
  \toprule
Method  &  Perplexity & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
    LLaMA-1-7B &5.69&73.18&78.35&72.99&67.01&67.45&41.38&42.40&63.25\\
  \cmidrule{1-10}
  SliceGPT&8.71&37.89&64.09&45.67&62.75&53.62&31.74&33.20&46.99\\
 LLM-pruner& 8.14& 69.54& 76.44& 68.11& 65.11& 63.43& 37.88& 40.00& 60.07\\
 LoRAPrune&7.63&65.82&79.31&70.00&62.76&65.87&37.69&39.14&60.05\\
 LoRAShear&/ &70.17&76.89&68.69&65.83&64.11&38.77&39.97&60.63\\
  \gr \bf LoSA &   \bf7.07  & \bf 71.67 & \bf 78.17 & \bf71.56   & \bf65.86 & \bf66.93 & \bf 40.66 &   \bf40.50 &\bf 62.19 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

Our LoSA method outperforms SliceGPT, LLM-Pruner, LoRAPrune and LoRAShear as shown above, demonstrating the superior performance of LoSA.

\section{Extending LoSA to fine-tune other training-free sparsity methods}
Our LoSA method is designed for fine-tuning sparse LLMs, which means that LoSA can be combined with any training-free sparsity methods to enhance their accuracy. We have demonstrated the improvement that LoSA brings to SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple}. We further show the performance improvements of LoSA on other training-free sparsity methods, including Pruner-Zero \citep{dong2024pruner} and ALPS \citep{meng2024alps}. All experimental data are based on a 70$\%$ sparse LLaMA-2-7B \citep{touvron2023llama2} model. The experimental results are reported in Table \ref{tab:other_training-free_sparsity_methods}.
\begin{table*}[h]
  \centering
  \small
  \caption{Experimental results of LoSA fine-tuning other training-free sparsity methods.
  }\label{tab:other_training-free_sparsity_methods}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{@{}lccccccccc}
  \toprule
Method  &  Perplexity & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
    LLaMA-2-7B &5.12&57.17&68.90&77.74&31.40&78.07&76.39&43.52&61.88\\
  \cmidrule{1-10}
  Pruner-Zero&103.15&27.56&50.99&41.93&13.00&56.90&34.47&18.60&34.78\\
 w. \texttt{LoRA}& 11.56&43.43&60.46&67.19&21.00&70.40&59.60&27.47&49.94\\
  \gr \bf w. \texttt{LoSA} &   \bf10.78  & \bf 45.56 & \bf62.10 & \bf69.15   & \bf25.00 & \bf71.73 & \bf 61.08 &   \bf29.45 &\bf 52.01 \\ 
  \cmidrule{1-10}
  ALPS&19.31&38.35&61.96&64.59&22.20&66.82&48.37&24.95&46.75\\
 w. \texttt{LoRA}& 10.83&47.54&62.88&69.11&27.00&73.23&61.70&29.78&53.03\\
  \gr \bf w. \texttt{LoSA} &   \bf10.28  & \bf 49.90 & \bf64.34 & \bf71.38   & \bf28.10 & \bf75.24 & \bf63.78 &   \bf31.27 &\bf 54.86 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

From the data above, we can observe that the accuracy of training-free sparse LLMs has significantly decreased compared to the dense model. LoSA effectively improves the accuracy of sparse LLMs, outperforming LoRA.

\section{Comparison of LoSA and more baselines}
We present a comparison of LoSA with AdaLoRA \citep{zhang2023adaptive} and SoRA \citep{ding2023sparse} which dynamically adjust the rank of LoRA like LoSA. Since AdaLoRA and SoRA have only been experimented on smaller models and there is no experimental data for LLMs such as LLaMA, we use the open source codes of AdaLoRA and SoRA to fine-tune a 70$\%$ sparse LLaMA-2-7B \citep{touvron2023llama2} model obtained by the Wanda \citep{sun2023simple} method. Since the rank of LoRA is 8, according to the original paper, the initial rank for each incremental matrix in AdaLoRA is 12. The rank of the SoRA method is set to 8, and other hyperparameters are set according to the original paper. Other experimental settings follow those in Section \ref{sec:ExperimentalSettings} and are aligned with the settings of LoRA and LoSA. The experimental results are reported in Table \ref{tab:more_baselines}.
\begin{table*}[h]
  \centering
  \small
  \caption{Comparison of AdaLoRA \citep{zhang2023adaptive} SoRA \citep{ding2023sparse} and LoSA.
  }\label{tab:more_baselines}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{@{}lccccccccc}
  \toprule
Method  &  Perplexity & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
    LLaMA-2-7B &5.12&57.17&68.90&77.74&31.40&78.07&76.39&43.52&61.88\\
  \cmidrule{1-10}
  Wanda&79.67&27.92&49.33&52.87&12.60&55.33&30.60&18.69&35.33\\
 w. \texttt{LoRA}&12.57&40.77&57.22&64.19&22.40&68.55&57.32&26.79&48.18\\
 w. \texttt{AdaLoRA}&12.08&41.01&57.78&64.73&23.00&69.09&57.77&26.90&48.61\\
 w. \texttt{SoRA}&11.89&41.37&57.87&64.95&23.40&68.78&58.25&27.17&48.83\\
  \gr \bf w. \texttt{LoSA} &   \bf10.94  & \bf 45.10 & \bf 60.93 & \bf67.65   & \bf25.20 & \bf71.06 & \bf62.50 &   \bf29.10 &\bf51.65 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

The results clearly show that our LoSA method outperforms AdaLoRA and SoRA, demonstrating the effectiveness of LoSA. This is evident because AdaLoRA and SoRA only dynamically adjust the rank, and the weights of AdaLoRA and SoRA cannot be merged into sparse LLMs. In contrast, LoSA dynamically adjusts the rank based on reconstruction error, determines layer-wise sparsity rates for sparse LLMs, and adopts dynamic sparse fine-tuning. Additionally, LoSA weights can be merged into sparse LLMs. These strategies ensure that LoSA achieves better accuracy than AdaLoRA and SoRA.

\section{More experimental results of N:M sparsity}
We demonstrated the accuracy of LoSA fine-tuning sparse LLMs with mixed 2:8 sparsity in Section \ref{sec:NMSparsity}. In this section, we further demonstrate the accuracy of LoSA fine-tuning sparse LLMs with mixed 2:4 sparsity and show the acceleration effect of the sparse LLMs with mixed 2:4 and mixed 2:8 sparsity on GPU. The results of LoSA fine-tuning the mixed 2:4 sparse LLaMA-2-7B \citep{touvron2023llama2} obtained by Wanda \citep{sun2023simple} method are shown in Table \ref{tab:mixed2:4}.
\begin{table*}[h]
  \centering
  \small
  \caption{Experimental results of LoSA fine-tuning the mixed 2:4 sparse LLaMA-2-7B obtained by Wanda method.
  }\label{tab:mixed2:4}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{@{}lcccccccccc}
  \toprule
Method  & Sparsity & Perplexity & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
    LLaMA-2-7B &0$\%$&5.12&57.17&68.90&77.74&31.40&78.07&76.39&43.52&61.88\\
  \cmidrule{1-11}
  Wanda&2:4&11.02&40.92&62.43&67.65&24.20&70.84&61.78&31.20&51.29\\
 w. \texttt{LoRA}&2:4&8.27&50.37&64.80&72.81&27.60&75.19&69.40&35.58&56.54\\
  \gr \bf w. \texttt{LoSA} & \bf Mixed 2:4 &  \bf7.72  & \bf 51.85 & \bf 66.01 & \bf74.51   & \bf29.70 & \bf76.54 & \bf71.08 &   \bf37.26 &\bf58.14 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

LoSA improves accuracy for mixed 2:4 sparsity and outperforms LoRA. Since mixed 2:4 and mixed 2:8 sparsity is a specific type of sparsity pattern, it can also leverage the nm-vllm \citep{nm-vllm} inference engine to achieve accelerated inference on GPUs. We also measured the inference acceleration effect of sparse LLaMA-2-7B with mixed 2:4 and mixed 2:8 sparsity on NVIDIA RTX 4090 24GB GPU. The results are shown in Table \ref{tab:speedup_nm}.
\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\vspace{0cm}
\caption{Speedup of LLaMA-2-7B with mixed N:M sparsity on GPU.}\label{tab:speedup_nm}
\begin{tabular}{@{}l ccc}
\toprule 
   Speed &Dense &Mixed 2:4 &Mixed 2:8  \\
    \hline
   Throughput (tokens/s) &57.35 &98.35 &133.40 \\
   Speedup & 1.00$\times$ &1.71$\times$ &2.33$\times$ \\
    \bottomrule
\end{tabular}
\end{table}


\section{Efficiency analysis as the model size increases}
We measured the time consumption of our proposed methods, Layer-wise Sparsity Rate (\textbf{LSR, Section \ref{sec:layerwise_sparsity}}) and Sparsity-Aware Rank Allocation (\textbf{SRA, Section \ref{sec:rank_allocation}}), on LLMs of different parameter sizes using a single NVIDIA A100 80GB GPU. The results are reported in Table \ref{tab:efficiency_analysis_as_the_model_size_increases}.

\begin{table}[h]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\vspace{0cm}
\caption{Time consumption of our proposed methods on models with different parameter sizes.}\label{tab:efficiency_analysis_as_the_model_size_increases}
\begin{tabular}{@{}l ccccc}
\toprule 
   Time (seconds) &7B & 8B &13B&30B&70B \\
    \hline
   LSR &48&34&74&140&332\\
   SRA & 46&32&71&132&321 \\
    \bottomrule
\end{tabular}
\end{table}

From the table, we can observe that as the model size increases, the required computation time also increases. However, for the largest 70B model, the computation time for LSR and SRA are only 332 seconds and 321 seconds, respectively, which are very fast and have minimal computational overhead.


\section{Analysis of sparsity rate}
We plot the layer-wise sparsity rate of the sparse LLMs obtained by our LoSA method in Figure \ref{fig:sparsityrate}.
\begin{figure}[!h]
\begin{center}
\includegraphics[height=0.4\linewidth,width=0.8\linewidth]{figures/fig3.pdf}
\end{center}
\caption{\label{fig:sparsityrate}Layer-wise sparsity rates of different LLMs.}
\end{figure}

We observed that the RMI metric tends to assign lower sparsity rates to the initial and final layers of LLMs while allocating higher sparsity rates to the middle layers. From the results, we can see that there is a lot of redundancy in the middle layer of LLMs.

\section{Detailed Zero-shot Task Results}\label{app:DetailedZero-shotTaskResults}
We evaluated a series of zero-shot learning tasks, as shown in Tables \ref{tab:zero_shot_main_results} and \ref{tab:NM}. These tasks include HellaSwag \citep{zellers2019hellaswag}, Winogrande \citep{sakaguchi2021winogrande}, BoolQ \citep{clark2019boolq}, OpenBookQA \citep{mihaylov2018can}, PIQA \citep{bisk2020piqa}, ARC-Easy, and ARC-Challenge \citep{clark2018think}. We present detailed task performance metrics in Tables \ref{tab:zero_shot_50}, \ref{tab:zero_shot_60}, \ref{tab:zero_shot_70}, \ref{tab:zero_shot_Vicuna_OPT} and \ref{tab:zero_shot_NM}, providing a comprehensive understanding of the zero-shot capabilities of the related models.

\begin{table*}[h]
\vspace{-1cm}
  \centering
  \small
  \caption{Zero-shot accuracy results of \texttt{LoSA} for sparse LLaMAs at 50$\%$ sparsity.
  }\label{tab:zero_shot_50}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{6}{*}{LLaMA-1-7B}   & Dense   & 56.92 &  69.93 & 75.05 & 34.40   & 78.67 &75.34 &41.89 & 61.74\\
  \cmidrule{2-10}
  & SparseGPT  & 51.43 &67.88 & 72.05 & 30.00 & 75.30 & 71.38 & 37.71 &57.96 \\ 
  &  w. \texttt{LoRA} &55.72 &68.59 &74.16 &31.00 &76.39 &71.46 & \bf 41.47 & 59.83 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf55.72  & \bf69.22 & \bf 76.06 & \bf32.00  & \bf 77.04 & \bf 72.90 &   40.87 &\bf60.54  \\ 
   \cmidrule{2-10}
 &   Wanda  & 51.85 & 66.06 & 71.22 &28.80 & 75.63 & 69.11 & 36.86 &57.08 \\ 
 &  w. \texttt{LoRA} &55.39 & 67.01& 72.72&32.00 & 77.11& 72.35&39.68 &  59.46\\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf  55.72& \bf68.03 & \bf73.36  & \bf 31.40 & \bf 77.15  & \bf 73.48 &   \bf 41.21 &\bf 60.09 \\ 
  \midrule
  \multirow{6}{*}{LLaMA-1-13B}   & Dense   &59.94  & 72.77  &77.89 & 33.20   & 79.16 & 77.40 &46.50 & 63.84\\
  \cmidrule{2-10}
  & SparseGPT  & 54.95 & 71.67 &76.97& 31.20 & 77.26 &72.47 & 41.98& 60.93\\ 
  &  w. \texttt{LoRA} &58.90 & 69.93& 80.00&33.40 &78.67 & 73.93& 43.88& 62.68 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 59.29 & \bf71.19 & \bf80.03  & \bf 33.60 & \bf 79.22 & \bf73.95 &   \bf  44.37&\bf 63.09 \\ 
   \cmidrule{2-10}
 &   Wanda  &55.71 & 71.98 &75.90  &32.20 & 77.26 & 73.19 & 43.52 &  61.39\\ 
 &  w. \texttt{LoRA} &58.64 & 71.03&79.69 &34.50 & 78.40& 74.41&44.79 & 63.07 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf58.85 & \bf71.82  & \bf  79.77& \bf 34.60 & \bf 78.78 & \bf 74.49 &   \bf 46.08 &\bf  63.49 \\ 
  \midrule   
  \multirow{6}{*}{LLaMA-1-30B}   & Dense & 63.35 &75.69 & 82.69 & 36.00 & 81.01 &80.30 & 52.82 &67.41\\
  \cmidrule{2-10}
  & SparseGPT   & 59.15 & 75.22 & 82.32 &35.00 & 78.20 & 78.96 &48.56  & 65.34 \\
  &  w. \texttt{LoRA} & 60.50&75.34&83.46 & 35.20&79.22 &80.05 &50.60 &66.31  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 61.16 & \bf 75.43 & \bf 83.55 & \bf 35.30 & \bf 79.81 & \bf 80.32 &   \bf 52.45 &\bf 66.86 \\ 
      \cmidrule{2-10}
  &  Wanda &60.93 & 73.48 & 81.90 &34.60 & 79.27 &79.29 & 49.66 &65.59 \\
  &  w. \texttt{LoRA} &62.37&73.72 &82.72 &\bf 36.60 &79.54&79.80 & 49.83& 66.37 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 63.02 & \bf 74.35& \bf85.20 &  35.20  & \bf 79.71 & \bf  80.22&   \bf 52.65 &\bf 67.19 \\ 
\midrule
  \multirow{6}{*}{LLaMA-2-7B}   & Dense  &57.17 & 68.90&	77.74&31.40 &78.07	&76.39 & 43.52&61.88\\
  \cmidrule{2-10}
  & SparseGPT & 52.37 & 69.85 &75.02&29.20 & 75.46 &73.27 &39.85 &59.29\\
  &  w. \texttt{LoRA} &55.63 &68.35&76.94 & 31.90&\bf 76.09 &73.32 & 41.04& 60.47 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 55.73 & \bf 68.67& \bf 77.19 & \bf32.00  &  75.71 & \bf 77.42 &   \bf 42.49 &\bf 61.32 \\ 
      \cmidrule{2-10}
  &  Wanda & 52.49&68.19&75.99 & 31.20&76.00& 72.77& 39.59 &59.46 \\
  &  w. \texttt{LoRA} & 55.31&68.11 &77.06 & 31.80&77.58 &72.73 &40.96 & 60.51 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 55.75 & \bf 68.13& \bf77.13  & \bf33.00  & \bf 77.59 & \bf73.06  & \bf 41.21 &\bf60.85  \\ 
  \midrule 
  \multirow{6}{*}{LLaMA-2-13B} & Dense & 60.06 &72.22&80.52  & 35.20 & 79.11 &79.42 & 48.46 &65.00 \\
  \cmidrule{2-10}
  & SparseGPT  &55.83 & 72.77 & 81.44 & 32.60 & 78.02 &74.83 &42.24& 62.53 \\
  &  w. \texttt{LoRA} & 58.68& 72.06&80.58 & 34.40&78.78 &77.48 &45.33  & 63.90 \\
   \gr \wc &  \bf w. \texttt{LoSA} & \bf 58.92 & \bf72.30 & \bf 81.44 & \bf 35.00 & \bf79.38  & \bf 77.68 &   \bf 45.88 &\bf  64.23\\ 
      \cmidrule{2-10}
  &  Wanda  &56.90 &71.35&81.84 & 32.00&  78.40&76.18 &43.52& 62.88 \\
  &  w. \texttt{LoRA} &58.77 & 71.51&81.77 & 33.20& 79.43&76.89 & 45.31& 63.84 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 59.00& \bf 71.59  & \bf81.83  & \bf33.80 & \bf 79.47 & \bf 77.57  &   \bf 47.01 &\bf 64.32 \\ 
  \midrule   
   \multirow{6}{*}{LLaMA-2-70B}   & Dense  & 66.10&78.06 &83.40	&37.20 &82.21&	82.55	&	54.44& 69.14\\
  \cmidrule{2-10}
  & SparseGPT    & 63.80& \bf 78.85 & 83.55 &38.20 & 81.94 & 82.40 &53.75  & 68.93\\
 &  w. \texttt{LoRA} &63.45 & 78.37&84.74 &39.20 &\bf 82.97 &83.08 &55.12 & 69.56 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 64.06 &  78.57 & \bf 85.34 & \bf39.20  & 82.54 & \bf 83.31 &   \bf 55.20 &\bf 69.82 \\ 
      \cmidrule{2-10}
  &  Wanda   &64.10 & 78.14 &82.50  &37.40 & 81.88 & 80.80 & 52.65 & 68.21 \\
  &  w. \texttt{LoRA} &64.06 &76.64 & 83.30&38.10 &82.54 &\bf 83.38 &55.29 & 69.04 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 64.16 & \bf77.74& \bf 85.57 & \bf38.20  & \bf82.59  & 83.16  &   \bf 56.14 &\bf  69.65\\ 
   \midrule   
   \multirow{6}{*}{LLaMA-3-8B}   & Dense 	& 60.19 & 72.77 & 81.35 & 34.80 &79.71 & 80.09 & 50.43 & 65.62\\
  \cmidrule{2-10}
  & SparseGPT    &  53.39 & 72.38 &79.27 & 30.80& 76.06 &73.02 &41.98 &60.99 \\
  &  w. \texttt{LoRA} &57.40 &71.82 & 81.91&31.40 &78.51 & 76.22&45.14 & 63.20 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 57.85 & \bf72.77 & \bf 81.99 & \bf32.80  & \bf 78.62 & \bf 76.98 &   \bf 48.38 &\bf 64.20 \\ 
      \cmidrule{2-10}
 &  Wanda   & 51.23 & 70.24 & 78.69 & 30.20 &  75.68 & 71.04 & 40.44 &59.65 \\
 &  w. \texttt{LoRA} &56.95 &72.22 &78.18 & 31.20& 78.18&76.01 &45.82 & 62.65 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 57.29 & \bf72.38 & \bf 78.75 & \bf 32.00 & \bf78.45  & \bf76.52  &   \bf48.21  &\bf 63.20 \\ 
       \midrule   
   \multirow{6}{*}{LLaMA-3.1-8B}   & Dense& 59.98 &73.32  &82.05 &33.20 &79.98& 81.57&51.45 &65.93 \\
  \cmidrule{2-10}
  & SparseGPT  &53.62 &72.14 &81.19 &29.20 &76.17  &74.49 &41.72 &61.22\\
  &  w. \texttt{LoRA}&57.46 &71.82 &82.37 & 32.00&79.33 &78.54 &48.29 &  64.25\\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf  57.65& \bf72.30 & \bf 82.54 & \bf 32.40 & \bf 79.76 & \bf 78.75 &   \bf 48.72 &\bf 64.59 \\ 
      \cmidrule{2-10}
 &  Wanda &51.19 &70.72 &78.62 &26.80 &75.08 &73.11 &41.55 &59.58\\
 &  w. \texttt{LoRA} &56.67& 71.19&79.02 &30.60 &78.51&77.48 &47.89 &63.05  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 57.17 & \bf71.35 & \bf 80.55 & \bf 31.60 & \bf  78.67& \bf 78.03 &   \bf47.97  &\bf 63.63 \\ 
\bottomrule
\end{tabular}
}
% \vspace{-0.5cm}
\end{table*}

\begin{table*}[h]
\vspace{-1cm}
  \centering
  \small
  \caption{Zero-shot accuracy results of \texttt{LoSA} for sparse LLaMAs at 60$\%$ sparsity.
  }\label{tab:zero_shot_60}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{6}{*}{LLaMA-1-7B}   & Dense   & 56.92 &  69.93 & 75.05 & 34.40   & 78.67 &75.34 &41.89 & 61.74 \\
  \cmidrule{2-10}
  & SparseGPT  & 44.86 & 63.61 &70.24  & 24.40&73.10  & 62.62 & 30.20 & 52.72\\ 
  &  w. \texttt{LoRA} & 52.27&65.82 &64.92 &29.20 & 75.84& 67.30&36.12 &  55.92\\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf52.30  & \bf65.85 & \bf 74.43 & \bf 29.20 & \bf  76.06& \bf67.55  &   \bf36.26  &\bf 57.38 \\ 
   \cmidrule{2-10}
 &   Wanda  &43.63 & 62.04 & 67.19 &25.00 &  73.02& 62.61 & 30.34 &51.98 \\ 
 &  w. \texttt{LoRA} &52.00 & 63.85& 66.67&29.40 &75.48 & 66.25& 34.13& 55.39 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 52.16 & \bf 64.17 & \bf 69.66 & \bf 29.70& \bf75.76  & \bf 66.41 &   \bf  35.58 &\bf 56.21 \\ 
  \midrule
  \multirow{6}{*}{LLaMA-1-13B}   & Dense &59.94  & 72.77  &77.89 & 33.20   & 79.16 & 77.40 &46.50 & 63.84\\
  \cmidrule{2-10}
  & SparseGPT &49.06 & 68.75 & 70.37 &27.60 & 75.63 &68.40 & 36.20 & 56.57  \\ 
  &  w. \texttt{LoRA} & 55.80&67.96 & 77.34&30.60 & 77.31&70.75 &40.27 &60.00  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 56.22 & \bf 69.77& \bf 78.93 & \bf 32.00 & \bf 77.64 & \bf 71.63 &   \bf41.21  &\bf 61.06 \\ 
   \cmidrule{2-10}
 &   Wanda  &48.92 & 68.19 &69.82  &27.64 & 74.91 & 68.92 & 34.93 &56.19 \\ 
 &  w. \texttt{LoRA} &55.34 &69.06 &76.27 &30.20 &76.88 &70.20 & 39.76& 59.67 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 55.81 & \bf 69.85& \bf76.29  & \bf31.00  & \bf 77.97 & \bf72.60  &   \bf 42.66 &\bf 60.88 \\ 
  \midrule   
  \multirow{6}{*}{LLaMA-1-30B}   & Dense & 63.35 &75.69 & 82.69 & 36.00 & 81.01 &80.30 & 52.82 &67.41 \\
  \cmidrule{2-10}
  & SparseGPT  & 55.03 & 72.80 &76.50 & 32.20 &76.83 & 74.71 & 43.32 & 61.63 \\
  &  w. \texttt{LoRA} &61.01 &72.69 &81.25 &35.00 &78.51 & 78.07& 47.78& 64.90 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 62.03 & \bf72.91 & \bf 81.67 & \bf 36.30 & \bf 79.32 & \bf 79.25 &   \bf 50.32 &\bf 65.97 \\ 
      \cmidrule{2-10}
  &  Wanda & 56.71& 72.30 & 76.24 & 31.60& 77.67 & 76.19 & 46.52 &62.46 \\
  &  w. \texttt{LoRA} & 60.31&72.65 &79.05&34.60 &78.89 &77.82 &48.21 & 64.50 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf  61.02 & \bf72.75 & \bf81.56  & \bf36.40  & \bf79.22  & \bf 79.12 &   \bf 50.94 &\bf 65.86 \\ 
\midrule
  \multirow{6}{*}{LLaMA-2-7B}   & Dense &57.17 & 68.90&	77.74&31.40 &78.07	&76.39 & 43.52&61.88\\
  \cmidrule{2-10}
  & SparseGPT    & 45.74 & 65.90 & 71.99 &25.80 &71.11 &64.02 &32.76 &53.90 \\
  &  w. \texttt{LoRA} &51.51 & 65.04& 73.79& 30.00&74.79 &68.82 &38.57 & 57.50 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 52.95 & \bf 67.32& \bf 73.82 & \bf31.00  & \bf  75.97& \bf 69.95 &   \bf38.63  &\bf 58.52  \\ 
      \cmidrule{2-10}
  &  Wanda   & 44.22 & 64.88 & 65.84 & 25.20& 72.09 & 64.56 &30.80 & 52.51 \\
  &  w. \texttt{LoRA} &51.28 & 65.82&70.43 & 30.20&74.97 &69.36 & 35.75& 56.83 \\
  \gr \wc &  \bf w. \texttt{LoSA} &\bf51.62  & \bf66.93 & \bf 74.04 & \bf31.40  & \bf 74.98 & \bf70.37  &   \bf 37.12 &\bf 58.06 \\ 
  \midrule 
  \multirow{6}{*}{LLaMA-2-13B}   & Dense  & 60.06 &72.22&80.52  & 35.20 & 79.11 &79.42 & 48.46 &65.00\\
  \cmidrule{2-10}
  & SparseGPT & 49.89& 70.88 & 77.28 & 28.80 & 75.41 & 68.98 &36.18 & 58.20 \\
  &  w. \texttt{LoRA} &55.85 & 69.01&78.13 &32.40 & 77.42&74.03 & 40.70& 61.08 \\
   \gr \wc &  \bf w. \texttt{LoSA} & \bf 56.13 & \bf69.69 & \bf 79.05 & \bf 32.60 & \bf 77.99 & \bf 74.74 &   \bf 41.47 &\bf61.67  \\ 
      \cmidrule{2-10}
  &  Wanda  & 48.82  & 68.75 & 77.28 & 29.00 & 75.84 & 69.87 &37.80 &58.19 \\
  &  w. \texttt{LoRA} &55.61 & 68.98&78.29 &31.20 & 77.42&72.35 & 42.49&60.91  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 55.90 & \bf 69.93& \bf 78.38 & \bf 32.40 & \bf 77.91 & \bf74.54  &   \bf 43.69 &\bf 61.82 \\ 
  \midrule   
   \multirow{6}{*}{LLaMA-2-70B}   & Dense & 66.10&78.06 &83.40	&37.20 &82.21&	82.55	&	54.44& 69.14\\
  \cmidrule{2-10}
  & SparseGPT    &59.41 & 76.64 &  83.85&35.60 &80.35  & 80.26 &49.49  & 66.52\\
 &  w. \texttt{LoRA} & 61.86& \bf77.74&84.83 & 38.00&81.66 &81.61 &52.56 & 68.32 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf  62.23& 77.43 & \bf 84.90 & \bf38.40  & \bf  82.69 & \bf 82.90 &   \bf 54.70 &\bf 69.04 \\ 
      \cmidrule{2-10}
  &  Wanda &59.43 & 76.16 &84.10 &36.20 &79.92  & 80.09 & 47.95 &66.27  \\
  &  w. \texttt{LoRA} &62.43 & 76.01& 84.25& 38.50&81.01 &82.20 & 53.24& 68.24 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 62.54 & \bf 76.72& \bf86.48  & \bf 38.60 & \bf 81.07 & \bf 83.04 &   \bf 55.12 &\bf  69.08 \\ 
   \midrule   
   \multirow{6}{*}{LLaMA-3-8B}   & Dense 	& 60.19 & 72.77 & 81.35 & 34.80 &79.71 & 80.09 & 50.43 & 65.62\\
  \cmidrule{2-10}
  & SparseGPT & 45.84 & 68.51 & 77.77& 22.80&70.57 &62.16 &30.72&54.05 \\
  &  w. \texttt{LoRA} & 52.91&68.67 &76.91 & 26.20&75.17 &72.22 & 40.78& 58.98 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 53.48 & \bf70.24 & \bf 80.37 & \bf28.20 & \bf 75.19 & \bf 72.64 &   \bf 41.98 &\bf 60.30 \\ 
      \cmidrule{2-10}
 &  Wanda & 38.02 & 60.14 & 68.56 & 20.00 & 67.95 & 59.93 &27.73 & 48.90 \\
 &  w. \texttt{LoRA} &50.32 & 65.43&73.76 &26.80 & 74.37&69.44 & 39.51& 57.09 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf  51.54 & \bf 68.51 & \bf 75.57 & \bf 27.00 & \bf75.57 & \bf71.42 &   \bf 40.61 &\bf 58.60 \\ 
     \midrule   
   \multirow{6}{*}{LLaMA-3.1-8B}   & Dense & 59.98 &73.32  &82.05 &33.20 &79.98& 81.57&51.45 &65.93\\
  \cmidrule{2-10}
  & SparseGPT  &45.53 &68.75 & 77.65&24.20 &71.38 &68.10 & 34.98& 55.80 \\
  &  w. \texttt{LoRA} & 53.21&68.35 &78.75 &28.00 &75.79 & 73.91&43.34 & 60.19 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 53.62 & \bf 69.22& \bf 79.97 & \bf28.10  & \bf  76.17& \bf74.66  &   \bf43.47  &\bf  60.74 \\ 
      \cmidrule{2-10}
 &  Wanda  & 38.75&60.85 &70.67 & 21.40& 69.59&60.19 &27.05 &49.78  \\
 &  w. \texttt{LoRA} &51.47 &65.90  &73.85&  27.80 &75.24&70.29 & 38.74&57.61  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf51.85  & \bf 67.40& \bf 74.83 & \bf 28.80 & \bf75.34  & \bf72.01  &   \bf 40.27 &\bf 58.64 \\ 
\bottomrule
\end{tabular}
}
% \vspace{-0.5cm}
\end{table*}


\begin{table*}[h]
\vspace{-1cm}
  \centering
  \small
  \caption{Zero-shot accuracy results of \texttt{LoSA} for sparse LLaMAs at 70$\%$ sparsity.
  }\label{tab:zero_shot_70}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
\midrule 
   \multirow{6}{*}{LLaMA-1-7B}   & Dense  & 56.92 &  69.93 & 75.05 & 34.40   & 78.67 &75.34 &41.89 & 61.74 \\
  \cmidrule{2-10}
  & SparseGPT & 34.58  &  56.43 & 64.80&16.80 & 64.25 &45.24 &23.12& 43.60\\ 
  &  w. \texttt{LoRA} &45.86 &60.93 &63.12 &23.20 & 70.62& 58.67&30.46 & 50.41 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 46.45 & \bf 64.09& \bf 68.65 & \bf24.80  & \bf72.14  & \bf60.77  &   \bf 32.25 &\bf 52.74 \\ 
   \cmidrule{2-10}
 &   Wanda &28.86  &52.80  &59.69 &14.20 &  57.56 &31.27 &17.75 & 37.45\\ 
 &  w. \texttt{LoRA} &41.70 & 57.85& 65.05&22.60 &69.48 & 54.12& 27.30&48.30  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf45.13 & \bf 60.06& \bf67.65  & \bf24.40  & \bf71.65  & \bf59.72 &   \bf29.78  &\bf 51.20 \\ 
  \midrule
  \multirow{6}{*}{LLaMA-1-13B}   & Dense &59.94  & 72.77  &77.89 & 33.20   & 79.16 & 77.40 &46.50 & 63.84 \\
  \cmidrule{2-10}
  & SparseGPT & 37.51  & 63.30 &68.78 &20.80 & 67.63 &52.78 &25.17 &48.00\\ 
  &  w. \texttt{LoRA} & 50.10& 63.61&72.29 & 26.20&74.43 &64.56 &33.79 &55.00  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf51.28  & \bf 64.88& \bf74.59  & \bf 29.20 & \bf 75.35 & \bf 64.62 &   \bf35.76  &\bf 56.53 \\ 
   \cmidrule{2-10}
 &   Wanda & 31.06 & 54.38 &61.59 &16.20 & 62.68  &42.05 & 17.58& 40.79\\ 
 &  w. \texttt{LoRA} &47.56 &61.01 &66.02 &23.00 &72.74 & 61.57&30.89 &51.83  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 50.21 & \bf64.95 & \bf 71.16 & \bf 25.60 & \bf 74.10 & \bf66.16 &   \bf34.98  &\bf 55.31 \\ 
  \midrule   
  \multirow{6}{*}{LLaMA-1-30B}   & Dense & 63.35 &75.69 & 82.69 & 36.00 & 81.01 &80.30 & 52.82 &67.41 \\
  \cmidrule{2-10}
  & SparseGPT  & 44.56 & 69.30 &65.35 &25.80 & 72.42& 65.78&32.25&53.64 \\
  &  w. \texttt{LoRA} &55.66 & 72.13& 79.36&30.40 &77.15 &73.86 &42.83 &61.63  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf56.32  & \bf 73.40& \bf80.09  & \bf 32.30 & \bf 77.42 & \bf 73.94 &   \bf 43.10 &\bf 62.37 \\ 
      \cmidrule{2-10}
  &  Wanda   & 44.23 &67.01 & 66.70 &26.40 & 72.03 & 64.86&32.25  & 53.35 \\
  &  w. \texttt{LoRA} &53.78 &67.72 & 77.74& 30.80& 76.44& 68.22&38.74 & 59.06 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 56.21 & \bf 69.77& \bf78.50  & \bf32.20  & \bf 77.31 & \bf 73.90 &   \bf 43.09 &\bf 61.57 \\ 
\midrule
  \multirow{6}{*}{LLaMA-2-7B}   & Dense &57.17 & 68.90&	77.74&31.40 &78.07	&76.39 & 43.52&61.88\\
  \cmidrule{2-10}
  & SparseGPT   &33.08 &  58.41&  64.89&  17.40 & 62.46 &43.22  &22.01  &43.07\\
  &  w. \texttt{LoRA} & 44.80&62.90 &63.36 &  24.20 & 71.22 &58.71 & 30.12& 50.76 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 46.06 & \bf 63.85 & \bf 70.15 & \bf 24.80 & \bf 71.93 & \bf 60.44 &   \bf 30.35 &\bf52.51  \\ 
      \cmidrule{2-10}
  &  Wanda  & 27.92  & 49.33 & 52.87   & 12.60 &55.33  & 30.60 & 18.69 &35.33\\
  &  w. \texttt{LoRA} &40.77 &57.22 & 64.19&22.40 &68.55 &57.32 &26.79 & 48.18 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 45.10 & \bf60.93& \bf67.65  & \bf25.20  & \bf71.06  & \bf62.50  &   \bf 29.10 &\bf 51.65 \\ 
  \midrule 
  \multirow{6}{*}{LLaMA-2-13B} & Dense & 60.06 &72.22&80.52 & 35.20 & 79.11 &79.42 & 48.46 &65.00 \\
  \cmidrule{2-10}
  & SparseGPT  & 36.90 & 61.64 &66.02 & 21.00 &67.57 &52.61 &25.94 &47.38\\
  &  w. \texttt{LoRA} &49.86 &66.77 &71.99 &26.40 &74.21 & 63.97& 32.94&55.16  \\
   \gr \wc &  \bf w. \texttt{LoSA} & \bf 50.57 & \bf 67.56& \bf 76.42 & \bf28.20  & \bf 74.27 & \bf 67.47 &   \bf 35.67 &\bf 57.16 \\ 
      \cmidrule{2-10}
  &  Wanda   & 29.60 & 51.70 &62.32&13.60 &58.65  & 37.21& 19.11 &  38.88\\
  &  w. \texttt{LoRA} &45.70 &60.93 &62.20  &24.00&71.98 & 60.23& 29.27& 50.62 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 46.79 & \bf 62.90& \bf 68.20 & \bf 25.20 & \bf 73.65 & \bf63.38  & \bf30.80  &\bf 53.00 \\ 
  \midrule   
   \multirow{6}{*}{LLaMA-2-70B}   & Dense  & 66.10&78.06 &83.40	&37.20 &82.21&	82.55	&	54.44& 69.14 \\
  \cmidrule{2-10}
  & SparseGPT   &50.98 &\bf 75.45  & 80.06 &30.00  & 75.24 & 73.57 &  40.61& 60.84\\
 &  w. \texttt{LoRA} &59.50 &74.98 & 84.04& 34.00&79.71&78.41 &49.40 & 65.72 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 60.21 & 75.09& \bf 84.72 & \bf 35.00 & \bf 79.80 & \bf 79.43 &   \bf 50.60 &\bf 66.41 \\ 
      \cmidrule{2-10}
  &  Wanda  & 48.16  & 73.88 & 74.46  & 27.00 &74.86  & 72.69 &38.31 &58.48 \\
  &  w. \texttt{LoRA} & 59.06&\bf 75.45& 82.48&34.00 &79.05 &78.41 &48.46 & 65.27 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 60.10 & 74.66& \bf84.92  & \bf 34.10 & \bf  79.16 & \bf 79.38&   \bf 50.51 &\bf 66.12 \\ 
   \midrule   
   \multirow{6}{*}{LLaMA-3-8B}   & Dense 	& 60.19 & 72.77 & 81.35 & 34.80 &79.71 & 80.09 & 50.43 & 65.62\\
  \cmidrule{2-10}
  & SparseGPT   & 34.26 & 56.75 & 66.51& 16.80 & 63.28 &42.09&21.42 & 43.02\\
  &  w. \texttt{LoRA} &44.93& 62.35&61.59 &21.80 &70.57 &60.06 &31.23 & 50.36 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 46.09 & \bf 62.98& \bf62.87 & \bf23.80  & \bf72.09  & \bf 62.96 &   \bf33.11  &\bf51.99  \\ 
      \cmidrule{2-10}
 &  Wanda  & 27.36  & 49.96 & 53.33 & 12.00 & 56.04 & 31.86 & 17.41& 35.42 \\
 &  w. \texttt{LoRA}  & 39.52&57.22 &61.92 &17.40 & 68.44&56.40 &28.24 & 47.02 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf43.12  & \bf61.01 & \bf 62.61 & \bf 23.20 & \bf 70.95 & \bf 60.35 &   \bf 31.40 &\bf50.37  \\ 
       \midrule   
   \multirow{6}{*}{LLaMA-3.1-8B}   & Dense & 59.98 &73.32  &82.05 &33.20 &79.98& 81.57&51.45 &65.93 \\
  \cmidrule{2-10}
  & SparseGPT &33.97 &54.70 & 67.34&14.80 &61.92 &45.33 & 21.76&42.83\\
  &  w. \texttt{LoRA} &45.44 & 61.33& 71.74& 21.80&71.60 & 60.73&33.64 & 52.33 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 46.32 & \bf64.48 & \bf74.19  & \bf25.20  & \bf  71.82 & \bf62.04 &   \bf 34.34 &\bf 54.06 \\ 
      \cmidrule{2-10}
 &  Wanda   &27.43 &48.70 &57.71 & 13.60& 55.01&31.86 &18.43 & 36.10\\
 &  w. \texttt{LoRA} &39.75 &56.67 &64.51 &19.40& 68.99& 55.72&27.65 & 47.52 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 42.12 & \bf 58.88& \bf65.34 & \bf21.20 & \bf72.31 & \bf 61.53 &   \bf  30.97&\bf 50.33 \\ 
\bottomrule
\end{tabular}
}
\end{table*}


\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of \texttt{LoSA} for sparse Vicuna/OPT at 50/60/70$\%$ sparsity.
  }\label{tab:zero_shot_Vicuna_OPT}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Model  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
  \midrule 
    \multirow{6}{*}{Vicuna-13B(50$\%$)}   & Dense  & 59.64&71.59 &	85.26 &36.80 &79.00 & 78.66&	47.78& 65.53\\
  \cmidrule{2-10}
  & SparseGPT   &56.88 & 69.82 & 83.58 & 36.00& 77.22 & 75.08 & 45.71 & 63.40\\
  &  w. \texttt{LoRA} & 57.00& 70.09& 84.10& 36.20&78.13 &75.05 &44.37 & 63.56 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 57.39 & \bf 72.22& \bf84.77 & \bf 36.30 & \bf 78.45 & \bf 75.38 &   \bf 45.39 &\bf 64.28 \\ 
      \cmidrule{2-10}
  &  Wanda  & 56.67&70.96 &83.68 & 36.00& 77.69 & 75.55 & 45.65 &63.74 \\
  &  w. \texttt{LoRA} & 57.30 &71.51 & 83.67& 36.20&78.45 & 76.18& 45.82& 64.16 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf57.34  & \bf71.88 & \bf 83.82 & \bf 36.60 & \bf78.67  & \bf 76.72 &   \bf 45.85 &\bf 64.41 \\ 
    \midrule   
    \multirow{6}{*}{Vicuna-13B(60$\%$)}   & Dense  & 59.64&71.59 &	85.26 &36.80 &79.00 & 78.66&	47.78& 65.53\\
  \cmidrule{2-10}
  & SparseGPT   & 51.75& 68.27 & 81.09 &32.20& 74.80 & 71.65 & 42.02 & 60.25 \\
  &  w. \texttt{LoRA} & 54.23&69.77 & 81.56&32.30 & 77.15&71.21 & 42.61& 61.26 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf54.67  & \bf70.09 & \bf 81.62 & \bf 32.40 & \bf 77.31 & \bf 72.90 &   \bf 42.71 &\bf 61.67 \\ 
      \cmidrule{2-10}
  &  Wanda  & 51.46 & 68.43&81.71 & 31.60& 74.65 & 71.84 & 41.64 &60.19 \\
  &  w. \texttt{LoRA} & 54.08& 68.55&80.31 & 32.00&76.93 & 71.91&42.41 & 60.89 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf54.85  & \bf 69.14& \bf 81.28 & \bf32.80  & \bf77.20  & \bf 71.97 &   \bf 42.66 &\bf 61.42 \\ 
    \midrule   
  \multirow{6}{*}{Vicuna-13B(70$\%$)}   & Dense  & 59.64&71.59 &	85.26 &36.80 &79.00 & 78.66&	47.78& 65.53\\
  \cmidrule{2-10}
  & SparseGPT   & 38.52& 61.01 &  73.30& 17.80 & 67.30 & 53.91 & 27.90 &48.53\\
  &  w. \texttt{LoRA} &49.00 &64.80 & 73.91& 24.40& 73.61 &65.74 &34.56 &55.15  \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 49.40 & \bf64.96 & \bf 76.85 & \bf26.40  & \bf 74.54 & \bf 66.58 & \bf36.09  &\bf  56.40\\ 
      \cmidrule{2-10}
  &  Wanda  & 31.84 &54.70  & 62.78 & 16.40 & 61.75 & 44.87 &22.10  &42.06\\
  &  w. \texttt{LoRA} & 45.77 &60.93& 68.32&24.20 & 71.87& 62.37& 32.94& 52.34 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 49.03 & \bf65.35 & \bf 76.12 & \bf 27.00 & \bf 73.94 & \bf 65.40 &   \bf 37.71 &\bf 56.37 \\ 
  \midrule   
  \multirow{6}{*}{OPT-13B(70$\%$)}   & Dense  & 52.43& 65.04&	65.93& 27.20&75.84 &	67.13&32.94 &55.22\\
  \cmidrule{2-10}
  & SparseGPT   &40.94 & 61.40 & 63.65 & 21.00 & 69.10 & 52.44 & 25.94&47.68 \\
  &  w. \texttt{LoRA} & 46.79&60.77 &63.79 &26.40 &72.96 & 59.55&29.10& 51.34 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 46.83 & \bf61.01 & \bf 68.65 & \bf 27.00 & \bf72.99  & \bf60.02  &   \bf 29.69 &\bf  52.31\\ 
      \cmidrule{2-10}
  &  Wanda  & 34.36 & 55.09 & 55.02 & 15.60 & 62.89 &43.73 & 23.89 &41.51\\
  &  w. \texttt{LoRA} &44.81 &59.69 & 60.52& 22.60&71.93 & 55.85& 28.67& 49.16 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf45.20  & \bf59.91 & \bf 60.96 & \bf24.80  & \bf 73.39   & \bf 57.65&   \bf 29.01 &\bf 50.13 \\ 
\bottomrule
\end{tabular}
}
\end{table*}

\begin{table*}[h]
  \centering
  \small
  \caption{Zero-shot accuracy results of \texttt{LoSA} for sparse LLaMA-2-7B at N:M sparsity.
  }\label{tab:zero_shot_NM}
  \setlength{\tabcolsep}{5.5pt}
  \resizebox{1.0\textwidth}{!}{
  \begin{tabular}{clcccccccc}
  \toprule
Sparsity  &  Method & HellaSwag \hspace{-0.2cm} & Winogrande \hspace{-0.2cm} & BoolQ & OBQA & PIQA & ARC-e & ARC-c & \hspace{-0.2cm}  Mean \\
   \midrule   
        \multirow{6}{*}{2:8}  & Dense &57.17 & 68.90&	77.74&31.40 &78.07	&76.39 & 43.52&61.88\\
  \cmidrule{2-10}
  & SparseGPT   & 27.82& 47.99 & 43.67 &13.20 &54.62  & 28.62 &  16.98& 33.27\\
  &  w. \texttt{LoRA} &33.26 & 52.09& 62.20& 19.00&  61.21&42.13 &20.31 & 41.46 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf36.14  & \bf54.85  & \bf62.32 & \bf   19.20 & \bf63.44  & \bf 47.72 &   \bf22.70  &\bf43.76  \\ 
      \cmidrule{2-10}
  &  Wanda  & 26.19 & 50.83& 37.83&13.80 & 52.88 & 26.52 &20.90  & 32.71\\
  &  w. \texttt{LoRA} &29.10 &51.54 &62.20 &14.40 &58.22 &37.33 &18.26 & 38.72 \\
  \gr \wc &  \bf w. \texttt{LoSA} & \bf 33.24 & \bf54.30 & \bf62.28  & \bf 17.00 & \bf61.64  & \bf 43.06 &   \bf 21.84 &\bf 41.91 \\ 
\bottomrule
\end{tabular}
}
\end{table*}


\end{document}
