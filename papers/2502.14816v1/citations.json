[
  {
    "index": 0,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "song2024sleb",
        "author": "Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon",
        "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks"
      },
      {
        "key": "men2024shortgpt",
        "author": "Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng",
        "title": "Shortgpt: Layers in large language models are more redundant than you expect"
      },
      {
        "key": "chen2024compressing",
        "author": "Chen, Xiaodong and Hu, Yuxuan and Zhang, Jing",
        "title": "Compressing large language models by streamlining the unimportant layer"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yin2023outlier",
        "author": "Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei",
        "title": "Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bachman2019learning",
        "author": "Bachman, Philip and Hjelm, R Devon and Buchwalter, William",
        "title": "Learning representations by maximizing mutual information across views"
      },
      {
        "key": "tschannen2019mutual",
        "author": "Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K and Gelly, Sylvain and Lucic, Mario",
        "title": "On mutual information maximization for representation learning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zheng2021information",
        "author": "Zheng, Xiawu and Ma, Yuexiao and Xi, Teng and Zhang, Gang and Ding, Errui and Li, Yuchao and Chen, Jie and Tian, Yonghong and Ji, Rongrong",
        "title": "An information theory-inspired strategy for automatic network pruning"
      },
      {
        "key": "fan2021layer",
        "author": "Fan, Chun and Li, Jiwei and Ao, Xiang and Wu, Fei and Meng, Yuxian and Sun, Xiaofei",
        "title": "Layer-wise model pruning based on mutual information"
      },
      {
        "key": "hu2024mpruner",
        "author": "Hu, Seungbeom and Park, ChanJun and Ferraiuolo, Andrew and Ko, Sang-Ki and Kim, Jinwoo and Song, Haein and Kim, Jieung",
        "title": "MPruner: Optimizing Neural Network Size with CKA-Based Mutual Information Pruning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "houlsby2019parameter",
        "author": "Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain",
        "title": "Parameter-efficient transfer learning for NLP"
      },
      {
        "key": "pfeiffer2020adapterfusion",
        "author": "Pfeiffer, Jonas and Kamath, Aishwarya and R{\\\"u}ckl{\\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna",
        "title": "Adapterfusion: Non-destructive task composition for transfer learning"
      },
      {
        "key": "lester2021power",
        "author": "Lester, Brian and Al-Rfou, Rami and Constant, Noah",
        "title": "The power of scale for parameter-efficient prompt tuning"
      },
      {
        "key": "liu2021p",
        "author": "Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie",
        "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhang2023adaptive",
        "author": "Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo",
        "title": "Adaptive budget allocation for parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ding2023sparse",
        "author": "Ning Ding and Xingtai Lv and Qiaosen Wang and Yulin Chen and Bowen Zhou and Zhiyuan Liu and Maosong Sun",
        "title": "Sparse Low-rank Adaptation of Pre-trained Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liu2024alora",
        "author": "Liu, Zequan and Lyn, Jiawen and Zhu, Wei and Tian, Xing and Graham, Yvette",
        "title": "Alora: Allocating low-rank adaptation for fine-tuning large language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "li2024nuteprune",
        "author": "Li, Shengrui and Han, Xueting and Bai, Jing",
        "title": "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models"
      },
      {
        "key": "li2024lorap",
        "author": "Li, Guangyan and Tang, Yongqiang and Zhang, Wensheng",
        "title": "LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models"
      },
      {
        "key": "zhao2024apt",
        "author": "Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing",
        "title": "Apt: Adaptive pruning and tuning pretrained language models for efficient training and inference"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "ma2023llm",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "Llm-pruner: On the structural pruning of large language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2023pruning",
        "author": "Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others",
        "title": "Pruning meets low-rank parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "li2023losparse",
        "author": "Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo",
        "title": "Losparse: Structured compression of large language models based on low-rank and sparse approximation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "chen2023lorashear",
        "author": "Chen, Tianyi and Ding, Tianyu and Yadav, Badal and Zharkov, Ilya and Liang, Luming",
        "title": "Lorashear: Efficient large language model structured pruning and knowledge recovery"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "zhao2024apt",
        "author": "Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing",
        "title": "Apt: Adaptive pruning and tuning pretrained language models for efficient training and inference"
      },
      {
        "key": "guo2023compresso",
        "author": "Guo, Song and Xu, Jiahang and Zhang, Li Lyna and Yang, Mao",
        "title": "Compresso: Structured pruning with collaborative prompting learns compact large language models"
      }
    ]
  }
]