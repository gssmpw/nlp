@article{bachman2019learning,
  title={Learning representations by maximizing mutual information across views},
  author={Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{chen2023lorashear,
  title={Lorashear: Efficient large language model structured pruning and knowledge recovery},
  author={Chen, Tianyi and Ding, Tianyu and Yadav, Badal and Zharkov, Ilya and Liang, Luming},
  journal={arXiv preprint arXiv:2310.18356},
  year={2023}
}

@article{chen2024compressing,
  title={Compressing large language models by streamlining the unimportant layer},
  author={Chen, Xiaodong and Hu, Yuxuan and Zhang, Jing},
  journal={arXiv preprint arXiv:2403.19135},
  year={2024}
}

@article{fan2021layer,
  title={Layer-wise model pruning based on mutual information},
  author={Fan, Chun and Li, Jiwei and Ao, Xiang and Wu, Fei and Meng, Yuxian and Sun, Xiaofei},
  journal={arXiv preprint arXiv:2108.12594},
  year={2021}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{guo2023compresso,
  title={Compresso: Structured pruning with collaborative prompting learns compact large language models},
  author={Guo, Song and Xu, Jiahang and Zhang, Li Lyna and Yang, Mao},
  journal={arXiv preprint arXiv:2310.05015},
  year={2023}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International conference on machine learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{hu2024mpruner,
  title={MPruner: Optimizing Neural Network Size with CKA-Based Mutual Information Pruning},
  author={Hu, Seungbeom and Park, ChanJun and Ferraiuolo, Andrew and Ko, Sang-Ki and Kim, Jinwoo and Song, Haein and Kim, Jieung},
  journal={arXiv preprint arXiv:2408.13482},
  year={2024}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@inproceedings{li2023losparse,
  title={Losparse: Structured compression of large language models based on low-rank and sparse approximation},
  author={Li, Yixiao and Yu, Yifan and Zhang, Qingru and Liang, Chen and He, Pengcheng and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={20336--20350},
  year={2023},
  organization={PMLR}
}

@article{li2024lorap,
  title={LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models},
  author={Li, Guangyan and Tang, Yongqiang and Zhang, Wensheng},
  journal={arXiv preprint arXiv:2404.09695},
  year={2024}
}

@article{li2024nuteprune,
  title={NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models},
  author={Li, Shengrui and Han, Xueting and Bai, Jing},
  journal={arXiv preprint arXiv:2402.09773},
  year={2024}
}

@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{liu2024alora,
  title={Alora: Allocating low-rank adaptation for fine-tuning large language models},
  author={Liu, Zequan and Lyn, Jiawen and Zhu, Wei and Tian, Xing and Graham, Yvette},
  journal={arXiv preprint arXiv:2403.16187},
  year={2024}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{men2024shortgpt,
  title={Shortgpt: Layers in large language models are more redundant than you expect},
  author={Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
  journal={arXiv preprint arXiv:2403.03853},
  year={2024}
}

@article{pfeiffer2020adapterfusion,
  title={Adapterfusion: Non-destructive task composition for transfer learning},
  author={Pfeiffer, Jonas and Kamath, Aishwarya and R{\"u}ckl{\'e}, Andreas and Cho, Kyunghyun and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2005.00247},
  year={2020}
}

@article{song2024sleb,
  title={SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks},
  author={Song, Jiwon and Oh, Kyungseok and Kim, Taesu and Kim, Hyungjun and Kim, Yulhwa and Kim, Jae-Joon},
  journal={arXiv preprint arXiv:2402.09025},
  year={2024}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{tschannen2019mutual,
  title={On mutual information maximization for representation learning},
  author={Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K and Gelly, Sylvain and Lucic, Mario},
  journal={arXiv preprint arXiv:1907.13625},
  year={2019}
}

@article{yin2023outlier,
  title={Outlier weighed layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity},
  author={Yin, Lu and Wu, You and Zhang, Zhenyu and Hsieh, Cheng-Yu and Wang, Yaqing and Jia, Yiling and Pechenizkiy, Mykola and Liang, Yi and Wang, Zhangyang and Liu, Shiwei},
  journal={arXiv preprint arXiv:2310.05175},
  year={2023}
}

@inproceedings{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{zhang2023pruning,
  title={Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}

@article{zhao2024apt,
  title={Apt: Adaptive pruning and tuning pretrained language models for efficient training and inference},
  author={Zhao, Bowen and Hajishirzi, Hannaneh and Cao, Qingqing},
  journal={arXiv preprint arXiv:2401.12200},
  year={2024}
}

@article{zheng2021information,
  title={An information theory-inspired strategy for automatic network pruning},
  author={Zheng, Xiawu and Ma, Yuexiao and Xi, Teng and Zhang, Gang and Ding, Errui and Li, Yuchao and Chen, Jie and Tian, Yonghong and Ji, Rongrong},
  journal={arXiv preprint arXiv:2108.08532},
  year={2021}
}

