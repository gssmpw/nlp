\section{Related Work}
\label{RelatedWork}
\paragraph{LLM Sparsity.} 
Existing sparsity methods, including SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple}, enable training-free sparsity of LLMs, effectively eliminating non-essential weights while striving to preserve model performance as much as possible. However, existing sparsity methods can lead to significant accuracy degradation under high sparsity rates, partly because these methods pre-set uniform layer-wise sparsity rates, overlooking the fact that redundancy levels vary between different layers of LLMs \citep{song2024sleb,men2024shortgpt,chen2024compressing}. OWL \citep{yin2023outlier} has recognized this issue and employs heuristic metrics to set the sparsity rates of LLMs inversely proportional to the ratio of observed activation outliers within each layer, thereby achieving a non-uniformly sparse LLMs. While setting non-uniform pruning rates can partially improve the accuracy of sparse LLMs, the accuracy still remains unsatisfactory. Therefore, fine-tuning sparse LLMs to restore their accuracy is necessary. This paper proposes using low-rank sparse adaptation to restore the accuracy of sparse LLMs and dynamically determine the layer-wise sparsity rates using representation mutual information during the fine-tuning process. Representation mutual information \citep{bachman2019learning, tschannen2019mutual} have been successfully applied to prune small models such as CNNs and BERT \citep{zheng2021information,fan2021layer, hu2024mpruner}. However, its application in pruning LLMs has not been well explored. In this paper, we derive the use of the representation mutual information metric to efficiently and rapidly determine the relative importance of each layer in LLMs during sparse fine-tuning. Through extensive experiments, we validate the effectiveness of the representation mutual information metric in pruning LLMs.


\paragraph{Low-Rank Adaptation (LoRA).} 
LoRA \citep{hu2021lora} stands out as a highly effective parameter-efficient fine-tuning (PEFT) method \citep{houlsby2019parameter, pfeiffer2020adapterfusion, lester2021power, liu2021p}, which incorporates trainable low-rank matrices that seamlessly reintegrate into the original model weights post-tuning, ensuring maintained efficiency without added latency or memory overhead. In LoRA fine-tuning, a crucial rank parameter dictates the tuning budget for each layer. AdaLoRA \citep{zhang2023adaptive} underscores the importance of adaptive allocation, suggesting that this budget be tailored according to the significance score of each weight matrix. SoRA \citep{ding2023sparse} is to dynamically adjust the rank of low-rank adaptation in the training process with a sparse gating unit trained by proximal gradient method. ALoRA \citep{liu2024alora} evaluates the importance of each rank, iteratively prunes low-contribution ranks, and reallocates resources to achieve dynamic adjustment of ranks. Similarly, we have identified the issue with the distribution of fine-tuning parameters during the fine-tuning of sparse LLMs. Uniformly setting the rank size like LoRA, does not effectively restore the accuracy of sparse LLMs. Therefore, this paper advocates for the dynamic allocation of the rank parameter budget, based on the sparse reconstruction errors across different layers, to optimize tuning efficacy. Although both LoSA and previous related works propose adjusting the rank in LoRA to achieve efficient parameter allocation, LoSA's dynamic rank adjustment strategy is specifically designed for sparse LLMs. Allocating fine-tuning parameters based on reconstruction error helps minimize the reconstruction error of sparse LLMs.

\begin{table}[b]
\renewcommand{\arraystretch}{1.2}
\small
\centering
\setlength\tabcolsep{0.53em}
\vspace{-0.2cm}
\caption{Experimental results of comparison between cubic and linear sparsity schedule.}\label{tab:linear_schedule}
\vspace{-0.2cm}
\begin{tabular}{@{}l cc}
\toprule 
   Method & Perplexity & Accuracy  \\
    \hline
   Linear &8.04 &  57.79 \\
    \gr  \bf Cubic & \bf 7.88 & \bf 58.06 \\
    \midrule
\end{tabular}
\end{table}

\paragraph{Joint Sparsity and LoRA.} 
Combining network sparsity with LoRA has been shown to effectively enhance the accuracy of sparse LLMs \citep{li2024nuteprune,li2024lorap,zhao2024apt}. For instance, LLM-Pruner \citep{ma2023llm} executes a one-shot structured pruning of LLMs, followed by fine-tuning using LoRA. LoRAPrune \citep{zhang2023pruning} implements iterative structured pruning, where weight importance is determined by replacing gradients on full weights with those calculated via LoRA. LoSparse \citep{li2023losparse} performs structured pruning on LLMs, using a combination of low-rank and sparse matrices to approximate the original weight matrix. LoRAShear \citep{chen2023lorashear} utilizes LoRA in conjunction with dynamic fine-tuning strategies to reinstate knowledge in structural pruning LLMs. All these studies apply LoRA to fine-tune structural pruning LLMs. Adjusting the input/output dimensions of the two low-rank adaptations in LoRA and integrating them into the structural pruning weights is straightforward \citep{zhao2024apt,guo2023compresso}. However, this approach is not viable for unstructured pruning (network sparsity). Unstructured pruning removes individual weights, resulting in sparse LLMs. In contrast, low-rank adaptations remain dense even after dimensional adjustments, making it impossible to merge them into sparse LLMs. Consequently, this paper aims to explore effective techniques for integrating low-rank adaptations into the sparse weights of LLM. The goal is to ensure that sparse LLMs and low-rank adaptations share the same sparse mask, thereby the model's sparsity is preserved and inference latency remains unaffected.

\vspace{-0.5cm}