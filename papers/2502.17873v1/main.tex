% \documentclass[sigconf,review]{acmart}
\documentclass[sigconf, nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
\acmConference[KDD '25]{Proceedings of the 31th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}{}{}

\acmISBN{978-1-4503-XXXX-X/18/06}




\usepackage{pifont} % For checkmark and cross symbols
\usepackage{array}  % For better table formatting
\usepackage{subcaption}
\usepackage{multirow}
\newcommand{\JH}[1]{\textcolor{red}{#1}}

\begin{document}


\title{EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling}


\author[1]{Jiazhen Hong}
% \authornote{Both authors contributed equally to this research.}
\authornotemark[1]
% \orcid{0000-0003-2475-6040}
\affiliation{%
  \institution{Emotiv Research}
  \city{Melbourne}
  \country{Australia}
}
\email{jiazhen@emotiv.com}
% ------------------------------------------------------------------
\author{Geoffrey Mackellar}
\affiliation{%
  \institution{Emotiv Research}
  \city{Sydney}
  \country{Australia}
}
\email{geoff@emotiv.com}
% ------------------------------------------------------------------
\author{Soheila Ghane}
\affiliation{%
  \institution{Emotiv Research}
  \city{Melbourne}
  \country{Australia}
}
\email{soheila@emotiv.com}
% ------------------------------------------------------------------

\renewcommand{\shortauthors}{Jiazhen Hong et al.}

\begin{abstract}
Deep learning has achieved significant progress in the development of electroencephalogram (EEG) foundation models, with Transformer-based architectures excelling at capturing long-range dependencies. However, their quadratic computational complexity presents challenges in memory efficiency, training, and inference speed, limiting their scalability and generalizability as a foundation model. In this paper, we propose EEGM2, a self-supervised framework based on structured state space duality (SSD) that overcomes these limitations. EEGM2 introduces three key innovations: (1) a reconstruction-based framework that captures both local and global EEG features through Mamba-2 structured state space models, (2) a spatiotemporal-aware loss function that enhances robustness to noise and preserves spectral information, and (3) a multi-branch receptive field input embedding strategy that improves cross-subject generalization and stability for EEG sequences of varying lengths. In comparison to traditional pretraining methods, on raw EEG or latent representation spaces, EEGM2 shows superior performance on long-sequence tasks, where conventional models struggle. Our experimental results on six EEG datasets validate that EEGM2 not only achieves state-of-the-art cross-domain accuracy but also reduces computational overhead, making it a more efficient solution for deployment on resource-constrained BCI devices. 
\end{abstract}


\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\keywords{EEG self-supervised Learning, Foundation Models}
\maketitle

\section{Introduction}
%BCI, EEG, challenge
Brain-computer interfaces (BCIs) provide new opportunities to improve the quality of life for individuals with disabilities by enabling control through mental activities \cite{hong2024chatbci}. Electroencephalography (EEG), as a non-invasive, low-cost, and easy-to-use method, is widely used in BCIs to record and analyze brain activity \cite{hong2022deep}. 
% EEG reflects the brain’s functional state by measuring electrical signals from the cerebral cortex.
However, scalp EEG signals are inherently noisy, exhibit spatiotemporal dependencies, and vary significantly across individuals, posing challenges for accurate modeling and interpretation \cite{acharya2013automated}. Additionally, BCI devices have strict resource limitations, requiring deployed models to be memory-efficient and fast in inference while maintaining high accuracy.
 
Self-supervised models 
% have demonstrated superior performance in modeling EEG signals 
% They enabled pretraining on large unlabeled datasets, followed by fine-tuning on smaller labeled datasets, enhancing the quality of learned features and reducing the reliance on extensive labeled data. 
have achieved significant success in natural language processing (NLP), computer vision (CV), and speech analysis \cite{wangeegpt}, and have recently demonstrated good performance in modeling EEG signals. In previous studies \cite{kostas2021bendr, yang2024biot, wangeegpt, eeg2rep2024}, Transformer-based models have shown strong capabilities for global sequence modeling, achieving superior performance on small to medium range EEG sequences \cite{gu2021efficiently}. 
However, as the length of the EEG signal increases, the computational complexity of these models grows quadratically, significantly increasing both training and inference costs \cite{dai2023multichannelsleepnet}. Although Transformer variants have been developed to capture long dependencies, they still struggle to scale efficiently to very long sequences of 10,000 or more steps \cite{gu2021efficiently}.

%Mamba-2
% To address quadratic scaling in computational complexity, recent advances in sequence modeling have introduced efficient architectures such as 
Structured state-space models (SSMs) \cite{gu2021efficiently} and their derivatives including Mamba \cite{gu2023mamba} and Mamba-2 \cite{dao2024transformers} were recently introduced to overcome the limitation of Transformers in long sequences. They leverage selective state-space mechanisms to balance computational efficiency with capacity of learning and representing rich, complex dependencies in the data. Mamba-2, in particular, combines the modified parallel Mamba block with attention mechanism, named structured state-space duality (SSD), and achieves linear complexity in sequence length while maintaining high accuracy across diverse sequence modeling tasks. Mamba-2 demonstrates Pareto dominance over both Mamba and Transformer layer types in terms of accuracy and computational efficiency, making it highly promising for processing long sequences \cite{dao2024transformers}. 

SSM-type layers are designed to efficiently capture features in EEG signals, however, the final performance and complexity of the model depend heavily on the architecture of the self-supervised framework. Similar to Transformer-based models, it is crucial to design architectures that maximize the benefits of SSM-type layers while integrating them with other components to achieve a model that is both highly accurate and efficient, meaning it maintains a low memory footprint and fast inference time, ensuring deployability on BCI devices with limited computational resources.

%Novelty
In this paper, we introduce EEGM2, a self-supervised framework designed to leverage Mamba-2 blocks \cite{dao2024transformers} to accurately model sequences of various lengths in EEG signals while minimizing computational complexity for resource-limited environments. The main contributions of EEGM2 are as follows:
\begin{itemize}
    % \item EEGM2 employs an encoder-decoder architecture with fewer parameters, specifically 4.5 million, to reconstruct EEG signals and learn meaningful EEG features for downstream tasks. 
    \item \textbf{EEG Mamba-2 (EEGM2).} We present EEGM2 an encoder-decoder architecture built on Mamba-2 blocks, designed to efficiently capture both local and global features of EEG signals while minimizing memory usage and inference time.
    % \item By incorporating the state-space duality of Mamba-2 for the first time, EEGM2 showcases high efficiency in modeling long sequences while maintaining computational scalability. \JH{memory-usage, time}
    \item \textbf{Knowledge transfer across subject and domain.} The spatiotemporal loss function and multi-branch embedding strategy in EEGM2 enable knowledge transfer across subjects and domains, supporting the development of large foundation models for EEG signals.
    \item \textbf{Robust Empirical Results.} We evaluated EEGM2 on multiple EEG datasets across various tasks in both supervised and unsupervised settings. The results demonstrate that pretrained EEGM2, with a 18-times smaller size and an inference time that scales linearly with sequence length, outperforms baseline models and effectively leverages pretrained models to improve performance on new tasks.
    \item \textbf{Efficient Deployment Strategy.} We showed that fine-tuning EEGM2 using only the encoder component reduces model size by 18-times while enhancing performance. This suggests an effective approach for deploying large foundation models on resource-constrained BCI devices, ensuring both efficiency and high performance.
\end{itemize}

\section{Related Work and Objectives }
Self-supervised learning has gained traction in the field of EEG representation learning, offering a way to derive meaningful representations from sparsely labeled EEG data \cite{deng2022boosting}. With the increase in EEG data availability, the development of foundation models for EEG has become a realistic goal. EEG2Rep~\cite{eeg2rep2024} and MAEEG~\cite{chien2211maeeg} demonstrated that pretraining models using self-supervised objectives significantly improves performance, especially when labeled data is scarce. BIOT~\cite{yang2024biot} introduced a tokenization module designed for general biosignal processing, allowing for cross-domain learning on EEG, ECG, and human sensory signals. Contrastive learning-based approaches~\cite{mohsenvand2020contrastive} have been explored to learn EEG representations by maximizing the agreement between differently augmented views of the same signal. BENDR~\cite{kostas2021bendr} extended this idea by applying masked autoencoder-based training and contrastive learning, providing a scalable way to pretrain EEG models. MAEEG~\cite{chien2211maeeg}, on the other hand, focuses purely on reconstruction-based learning, emphasizing the utility of reconstructing raw EEG signals as a pretraining strategy. Masking-based methods, EEGPT~\cite{wangeegpt} and EEG2Rep~\cite{eeg2rep2024} adopt mask-based dual self-supervised learning objectives, improving feature generalization across multiple EEG tasks.  Despite these advancements, most self-supervised EEG models struggle with long-sequence processing due to the inherent computational challenges of Transformer-based architectures. While Transformers excel at modeling long-range dependencies, their quadratic computational complexity in sequence length makes them inefficient for high-resolution EEG data. This issue is particularly pronounced in foundation models, which require scalability across different EEG paradigms. To address this, recent work has begun to explore structured state-space models (SSMs) as an alternative to Transformer mechanisms. Mamba, a recent SSM-based model, has demonstrated linear-time complexity while maintaining strong sequence modeling capabilities~\cite{gu2023mamba}. A memory usage and inference speed analysis comparing Transformer-based and Mamba-based models can be found in Section \ref{sec:4.5}.

%Inspired by the encoder-decoder structure of U-Net~\cite{ronneberger2015unet} and EEG signal reconstruction in MAEEG~\cite{chien2211maeeg}, EEGM2 is built upon a reconstruction-based encoder-decoder self-supervised architecture. Considering the challenges posed by long sequences, where Transformer-based architectures may struggle, EEGM2 incorporates the structured state-space model (SSM) blocks, specifically Mamba-2 blocks, which are highly effective in long-sequence modeling~\cite{dao2024transformers}. This reconstruction-based framework enables EEGM2 to leverage both local and global EEG features, making it robust to noise and suitable for transfer learning in downstream tasks. Moreover, to investigate whether EEGM2 learns meaningful EEG representations, we first train the model without labels using raw EEG signals, comparing the reconstructed signals to the originals. Subsequently, we extract the features from the encoder layer, referred to as the latent representation.



\section{Methodology}

\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{Figure/Figure1.pdf}
  \caption{EEGM2 Architecture.}\label{fig:1}
\end{figure*}

Figure \ref{fig:1} illustrates the EEGM2 architecture. Below, in this section, we describe: (1) the overall structure of EEGM2, (2) the Multi-Branch Receptive Field Input Embedding, (3) the Mamba-2 Block, and (4) the Spatiotemporal Loss Function.

\subsection{EEGM2 architecture}
EEGM2 mainly consists of three modules: the encoder, mediator, and decoder. Following the input embedding layer, the encoder module adopts a hierarchical structure comprising one Mamba-2 block and two 1D convolutional layers, interspersed with downsampling operations via max pooling. The mediator module utilizes an additional Mamba-2 block to project temporal information and model sequential latent representations. 

The decoder module mirrors the encoder’s structure, employing upsampling operations and skip connections to reconstruct the input signal. Skip connections between corresponding encoder and decoder layers help retain spatial and temporal features ~\cite{ronneberger2015unet}.
% , inspired by the architecture of U-Net~\cite{ronneberger2015unet}. 
The reconstruction of EEG signals in the decoder is achieved using 1D convolutions and Mamba-2 blocks. Finally, the output embedding is generated through a $1 \times 1$ convolution. Upsampling is performed via linear interpolation, expanding the temporal dimension to match the resolution of the target feature map.

\subsection{Multi-branch receptive field input Embedding}
To accommodate the multi-scale temporal characteristics of EEG signals, a Multi-Branch receptive field input Embedding strategy is employed. Three parallel convolutional layers with kernel sizes of 1, 3, and 7 are used to extract short-, medium-, and long-term temporal patterns. The outputs of these branches are concatenated along the channel dimension and fused using a $1 \times 1$ convolution to form the initial feature map. This multi-scale embedding enhances the model's robustness to varying EEG sequence lengths and subject-specific variations.

\subsection{Mamba-2 block}
Each Mamba-2 block, illustrated in Figure~\ref{fig:1}, is equipped with LayerNorm and residual connections to enhance stability and performance during training. Mamba-2~\cite{dao2024transformers} is an advanced structured state-space model (SSM) specifically designed for efficient long-sequence modeling. Compared to Transformer-based architectures, Mamba-2 achieves linear time complexity while preserving the ability to capture long-range dependencies, making it particularly well-suited for EEG data. Its structured state-space duality (SSD) mechanism enables both parallel computation and selective information propagation, significantly improving scalability and computational efficiency.

Given an input sequence $\mathbf{x} \in \mathbb{R}^{T \times d}$, where $T$ represents the sequence length (i.e., the number of time steps) and $d$ denotes the feature dimension (i.e., the number of input channels), Mamba-2 operates using a structured state-space representation:
\begin{equation}
    \mathbf{h}_t = A_t \mathbf{h}_{t-1} + B_t \mathbf{x}_t,
\end{equation}
\begin{equation}
    \mathbf{y}_t = C_t \mathbf{h}_t,
\end{equation}
where $t \in \{1, \dots, T\}$ is the current time step, and $A_t, B_t, C_t$ are parameterized matrices that are dynamically updated at each time step. Unlike conventional recurrent models, Mamba-2 employs a selective update mechanism, enhancing its expressivity and robustness in capturing long-range dependencies. By integrating Mamba-2 blocks into EEGM2, we harness these advancements to efficiently model long-sequence EEG signals while ensuring computational scalability.

\subsection{Spatitemproal loss functioin}
Unlike representation-based methods in previous works, EEGM2 employs a reconstruction-based learning strategy to extract latent EEG representations. To effectively preserve both temporal and spectral characteristics of EEG signals, we introduce a Spatiotemporal Loss Function, which combines a Mean Absolute Error (L1) loss in the time domain with a spectral loss computed in the frequency domain. Given a predicted EEG signal \(\hat{y} \in \mathbb{R}^{N \times T}\) and the corresponding ground truth EEG signal \(y \in \mathbb{R}^{N \times T}\), the Spatitemproal loss function is defined as:

\begin{equation}
\mathcal{L}_{\text{Spatiotemporal}} = \alpha \cdot \mathcal{L}_{\text{L1}} + \beta \cdot \mathcal{L}_{\text{Spectral}},
\end{equation}

where \(\alpha\) and \(\beta\) are hyperparameters that control the relative contribution of each component. In this paper, $\alpha =\beta=1$.

\subsubsection{L1 Loss (Time Domain Reconstruction)}

The first term, \(\mathcal{L}_{\text{L1}}\), is the Mean Absolute Error (MAE), which minimizes the absolute difference between the predicted and ground truth signals in the time domain:

\begin{equation}
\mathcal{L}_{\text{L1}} = \frac{1}{B T} \sum_{i=1}^{B} \sum_{t=1}^{T} \left| y_{i,t} - \hat{y}_{i,t} \right|,
\end{equation}

where \(B\) is the batch size, and \(T\) is the number of time steps in the EEG sequence. Compared to L2 (Mean Squared Error) loss, L1 avoids excessive penalization of large deviations, leading to better generalization\cite{mazilu2011l1}. However, L1 loss has a non-smooth gradient at zero, which can slow down convergence. To mitigate this, we employ a OneCycle learning rate schedule to facilitate stable training.

%L1 loss is robust to outliers, making it suitable for EEG signals, which are often contaminated with noise and artifacts. 

\subsubsection{Spectral Loss (Frequency Domain Reconstruction)}

The second term, \(\mathcal{L}_{\text{Spectral}}\), ensures that the model accurately reconstructs EEG signals in the frequency domain by comparing the Fourier-transformed representations of the predicted and ground truth signals:

% \begin{equation}
% \mathcal{L}_{\text{Spectral}} = \frac{1}{B T} \sum_{i=1}^{B} \sum_{f=1}^{F} \left\| \mathcal{F}(y_i)_{f} - \mathcal{F}(\hat{y}_i)_{f} \right\|^2,
% \end{equation}

\begin{equation}
\mathcal{L}{\text{Spectral}} = \frac{1}{B T} \sum{i=1}^{B} \sum_{j=1}^{T/2+1} \left| \mathcal{F}(y_i)_j - \mathcal{F}(\hat{y}_i)_j \right|^2,
\end{equation}

where \(\mathcal{F}(\cdot)\) represents the real-valued discrete Fourier transform (rFFT) applied along the temporal axis. Unlike conventional magnitude-based spectral losses, which extract specific frequency bands, this loss directly minimizes the squared difference in spectral amplitude across all non-negative frequency components. This ensures that the network learns to reconstruct not only the time-domain waveform but also the signal’s global spectral structure, which is crucial for EEG data containing oscillatory patterns such as alpha, beta, and theta rhythms.

By incorporating spectral loss, EEGM2 enhances the robustness of its learned representations by preserving frequency-domain consistency, mitigating the effects of local noise while ensuring the retention of meaningful oscillatory features. This complements the time-domain reconstruction loss, leading to improved generalization in downstream tasks.




\section{Experimental Results}
\subsection{Datasets} %and Pre-processing}
\begin{table}
\centering
\caption{Overview of Dataset}
\label{tab:1}
\begin{tabular}{l|c|c|c|c}
\toprule
\textbf{Datasets}  & \textbf{Chan.} &  \textbf{Sub.} & \textbf{Samples}  & \textbf{Seq. Length} \\ \hline
%TUAB - 2 s           & 16 &  2383     & 2,178,242                           & 256               \\ 
\textbf{TUAB} - 10 s          & 16 &  2383     & 409,455                           & 1280              \\ 
\textbf{TUAB} - 30 s          & 16 &  2383     & 135,702                            & 3840              \\ 
\textbf{TUAB} - 60 s          & 16 &  2383     & 56,290                              & 7680              \\ 
\textbf{TUAB} - 100 s         & 16 &  2383     & 39,810                              & 12800              \\ \hline
\textbf{Crowdsourced}      & 14 &  13       & 12,296                             & 256               \\ 
\textbf{STEW}           & 14 &  48       & 28,512                             & 256               \\ 
\textbf{DriverDistraction}   & 14 &  17       & 66,197                             & 256               \\ 
\textbf{Alpha}                & 14 & 59 & 11,866                             & 256               \\ 
\textbf{Attention}            & 14 & 27 & 21,894                             & 256               \\ \hline
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:1} summarizes the datasets used in this study.
% , along with their corresponding number of channels (Chan.), number of subjects (Sub.), number of data samples, and sequence length (Seq. length). 
To evaluate the modeling capability of EEGM2, we conducted experiments on six EEG datasets, including the TUH Abnormal EEG Corpus (TUAB)\cite{lopez2015automated}
% —one of the largest publicly available EEG repositories—
and various Emotiv-collected datasets covering diverse real-world scenarios. 
TUAB provides 16-channel EEG signals sampled at 200 Hz, while Emotiv datasets are recorded at 128 Hz with 14 channels. We resample all data to 128 Hz for consistency. We explore TUAB with four different sequence length settings (from 10-100 seconds) to assess model performance across varying temporal resolutions to evalue models ability in learning long-sequence EEG data. The five Emotiv datasets were collected from different real-world applications, including: Crowdsourced Eye Open/Close Detection, STEW, focusing on mental workload estimation, Driver Distraction Detection, Alpha wave Eye Open/Close, and Attention state classification. Crowdsourced Eye Open/Close Detection \cite{williams2023crowdsourced} and STEW \cite{lim2018stew} are publicly available, while other datasets are proprietary datasets provided by Emotiv. Experiments on the Emotiv datasets are conducted to assess the generalization and robustness of EEGM2, as these datasets cover different Emotiv headset types, each introducing unique signal noise characteristics and cross-subject variations further test the model’s adaptability to individual differences in EEG recordings. Additional details on data descriptions and preprocessing are provided in Appendix \ref{app:1}. 

We follow the same dataset splitting strategy as BIOT \cite{yang2024biot} for the TUAB dataset. For datasets collected using the Emotiv headset, we applied a bandpass filter and segmented the data into 2-second windows, each containing 256 time steps. We split the Emotiv datasets into subject-wise training, validation and test sets that presents a challenging setup due to inter-subject variability, allowing us to evaluate the cross-subject generalization capability of EEGM2.


\subsection{Implementation and Setup}
\subsubsection{\textbf{Pretraining \& Evaluation Strategy}}
We employed the AdamW optimizer with an initial learning rate of $2.5 \times 10^{-4}$ and a weight decay of $1 \times 10^{-2}$. A OneCycle learning rate schedule~\cite{smith2019super} was used, with a maximum learning rate of $5 \times 10^{-4}$ and a minimum of approximately $3.13 \times 10^{-5}$, following a cosine annealing strategy. The learning rate was warmed up for the first 30\% of training steps and then gradually decayed, starting at $\frac{max\_lr}{10}$ and reaching $\frac{max\_lr}{10000}$ by the end of training. 
The model undergoes a two-stage training process. At first stage, it is pretrained in a self-supervised approach for 500 epochs with a batch size of 64, optimizing a reconstruction loss without relying on labeled data. After pretraining, in the second stage, we used task-specific labelled datasets and employed two strategies: (a) the entire encoder-decoder architecture is fine-tuned, or (b) the pretrained encoder is extracted from the framework and is used for probing. The two strategies are explained in the following sections. Training is conducted using 32-bit mixed precision on a single NVIDIA RTX 6000 Ada GPU. To ensure the reliability of the results, each experiment is repeated three times, and we report the mean and standard deviation across all tables presented in this paper to maintain consistency.

\subsubsection{\textbf{Fine-Tuning - Encoder-Decoder}} \label{sec:4.2.2}
In this setting, the full capability of EEGM2 is evaluated, denoted as EEGM2(Fine). After pretraining, the entire encoder-decoder architecture, along with an additional MLP classification layer, is fine-tuned for a 10 epochs using cross-entropy loss. The AdamW optimizer is used for fine-tuning.
% employed during fine-tuning for effective adaptation of learned representations to task-specific objectives.

\subsubsection{\textbf{Probing - Encoder Only}}\label{sec:4.2.3}
In this setting, the pretrained encoder remains frozen and serves solely as a feature extractor. To effectively reduce dimension while preserving the statistical characteristics of EEG representations, we compute key summary statistics along the feature dimension, including the minimum, maximum, mean, and standard deviation. We also extract quantile-based features (e.g., 5th, 25th, 50th, 75th, and 95th percentiles) to characterize the properties of distribution of the encoded representations. 
% \JH{[talk about future work, could be removed]}We adopt a universal representation learning strategy, assuming that all downstream tasks share similar underlying patterns, thereby making our approach broadly applicable across various EEG tasks. However, future improvements could involve incorporating task-specific representation learning techniques, such as Common Spatial Patterns (CSP)~\cite{ramoser2000optimal} for motor imagery EEG, to further enhance performance in specialized applications.

\textbf{Linear Probing}, denoted as EEGM2(Linear), we apply logistic regression on the extracted representations to assess their linear separability. 
% We utilize a \texttt{OneVsRestClassifier} wrapped around a logistic regression model, treating each class as an independent binary classification problem. To ensure stable training, 
The input features are standardizes using \texttt{StandardScaler}. 
% before fitting the logistic regression model. 
%Given the potential scale of the training data, we employ a subsampling strategy, capping the maximum number of training samples at 100,000. If a dataset exceeds this threshold, we perform stratified sampling to maintain the class distribution while reducing computational costs. The logistic regression model is trained for a maximum of one million iterations to ensure convergence.

\textbf{Non-Linear Probing}, denoted as EEGM2 (Light), we introduce a multi-layer perceptron (MLP) classifier to process the extracted representations. 
% Unlike linear probing, which assumes that the features are linearly separable, non-linear probing applies 
The additional non-linearity enhances feature expressivity. The MLP consists of two fully connected layers with ReLU activation and a dropout rate of 0.5.
% between them to mitigate overfitting. 
The computed statistical and quantile-based features are concatenated and passed through the MLP classification head, allowing the model to capture complex feature interactions that a simple linear classifier might overlook. This approach enhances downstream task performance by leveraging non-linear transformations. 
% to better model complex patterns in the EEG data.



\begin{table*}
\centering
\caption{Performance comparison of EEGM2 variants in self-supervised long-sequence EEG modeling.}
\label{tab:2}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Models}                            & \textbf{Mamba-2 Block} & \textbf{Spatiotemporal Loss} & \textbf{Multi-branch} & \textbf{ACMSE} & \textbf{Averaged Training Time} \\ \hline
EEGM2                              & \ding{51}              & \ding{51}                    & \ding{51}                             & 6.87e-13       & 299.43 seconds/epoch                \\ \hline
EEGM2-S1       & \ding{51}              & \ding{51}                    & \ding{55}                             & 9.24e-12       & 296.45 seconds/epoch                   \\ \hline
EEGM2-S2        & \ding{51}              & \ding{55} (L1 Loss)          & \ding{51}                             & 7.74e-12       & 300.12 seconds/epoch                   \\ \hline
EEGM2-S3   & Mamba-1                & \ding{51}                    & \ding{51}                             & 6.95e-13       & 435.95 seconds/epoch                   \\ \hline
EEGM2-S4                       & Mamba-1                & \ding{51}                    & \ding{55}                             & 1.06e-11       & 438.05  seconds/epoch                  \\ \hline
EEGM2-S5  & Transformer              & \ding{51}                    & \ding{51}                             & Out of Memory  & Out of Memory          \\ \hline
\end{tabular}
\end{table*}

\subsection{Long-Sequence EEG Modeling} \label{sec:4.3}
In this section, we discuss the self-supervised phase with five different configurations of EEGM2 to evaluate the contribution of each component in long-sequence modeling and showcase the performance of proposed architecture. The six differen configuratons used are: \textbf{EEGM2} (The full version of EEGM2, representing the most powerful configuration. It integrates all components in \ref{fig:1}), \textbf{EEGM2-S1} (A variant without multi-branch receptive field), \textbf{EEGM2-S2} (A variant that replaces spatiotemporal loss with L1 loss), \textbf{EEGM2-S3} (A variant where the Mamba-2 block is replaced with the Mamba-1 block), \textbf{EEGM2-S4} (Similar to EEGM2-S3, but without multi-branch receptive field), and \textbf{EEGM2-S5} (A variant where the Mamba-2 block is replaced with a Transformer block).

Table \ref{tab:2} presents an ablation study evaluating the performance of EEGM2 and its variants in a self-supervised reconstruction task for long-sequence EEG modeling. To assess the capability of EEGM2 in handling long-sequence EEG signals, we use the TUAB dataset with a sequence length of 100 seconds (12,800 samples). All experiments were conducted under the same conditions on a single NVIDIA RTX 6000 Ada GPU. To better understand the contribution of each component in EEGM2, we report the Averaged Single-Channel Mean Squared Error (ACMSE) and the average training speed in epochs per second. The ACMSE is computed by first calculating the Mean Squared Error (MSE) for each EEG channel between the raw signal and its reconstructed counterpart. The final ACMSE is then obtained by averaging across all channels.

As shown in Table \ref{tab:2}, EEGM2 achieves the lowest ACMSE, demonstrating its ability to effectively model long-sequence EEG data. Due to the quadratic computational complexity of transformer-based models, EEGM2-S5, where the Mamba-2 block is replaced with a Transformer mechanism, runs out of memory when processing sequences of 100 seconds. This highlights the advantage of Mamba-2’s linear scaling, which allows EEGM2 to efficiently handle long sequences while maintaining superior reconstruction accuracy. A more comprehensive discussion on memory usage can be found in Section \ref{sec:4.5}. Comparing EEGM2 with EEGM2-S4, we observe that while the ACMSE improves only slightly, the Mamba-2 block significantly reduces training time compared to the Mamba-1 block, demonstrating its computational efficiency. Conversely, comparing EEGM2 with EEGM2-S1, and EEGM2-S4 with EEGM2-S5, we find that while the multi-branch receptive field introduces only a minimal increase in training time, it significantly enhances reconstruction performance, further emphasizing its effectiveness. Moreover, for EEGM2-S2, which excludes the spatiotemporal loss, although the average training time remains similar, the training process becomes unstable. Further details on this instability can be found in Appendix \ref{app:3}.

\subsection{Downstream Tasks}
This section is divided into three parts:
(1) Long-sequence EEG tasks, which assess EEGM2’s capability in modeling long-sequence EEG signals.
(2) Short-sequence EEG tasks across various real-world scenarios, which evaluate EEGM2’s stability and cross-subject generalization.
(3) Cross-domain analysis, which explores the potential of EEGM2 toward a foundation model for diverse EEG applications.

\subsubsection{\textbf{Long-Sequence Task}}

\begin{table}
\centering
\caption{Performance on TUAB across different sequence lengths.}
\label{tab:3}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c}
\toprule
\textbf{Models} & \textbf{Balanced ACC} & \textbf{AUROC} \\ \hline

\multicolumn{3}{l}{\textbf{10 Seconds:}} \\ \hline
CNN-LSTM \cite{li2022motor, wangeegpt}       & 78.48$\pm$0.38          & 0.8569$\pm$0.0051 \\ 
CNNTransformer \cite{peh2022transformer, wangeegpt}              & 77.77$\pm$0.22          & 0.8461$\pm$0.0013 \\ 
BIOT \cite{yang2024biot}             & 79.59$\pm$0.57          & 0.8815$\pm$0.0043 \\ 
EEGPT \cite{wangeegpt}                       & 79.83$\pm$0.30          & 0.8718$\pm$0.0050 \\
MAEEG \cite{chien2211maeeg, eeg2rep2024}                  &   77.56$\pm$3.56      & 0.8656$\pm$0.0333 \\ 
BENDR \cite{kostas2021bendr, eeg2rep2024}                     &  76.96$\pm$3.98     & 0.8397$\pm$0.0344 \\ 
EEG2Rep \cite{eeg2rep2024}             &   80.52$\pm$2.22     & 0.8843$\pm$0.0309 \\ \hline
%EEGM2 (Linear)  & 77.15$\pm$0.29 & 0.8331$\pm$0.00\\ 
EEGM2 (Light)  & 79.14$\pm$0.21 & 0.8559$\pm$0.00  \\
%EEGM2 (Scratch)  & 79.75$\pm$0.94 & 0.8630$\pm$0.01 \\
EEGM2 (Fine) & \textbf{80.87}$\pm$0.54 & \textbf{0.8864}$\pm$0.00\\ \hline

\multicolumn{3}{l}{\textbf{30 Seconds:}} \\ \hline
EEGM2 (Light)  & 78.97$\pm$0.25 & 0.8575$\pm$0.00  \\
EEGM2 (Fine) & \textbf{81.71}$\pm$0.12 & \textbf{0.8932}$\pm$0.00 \\ \hline

\multicolumn{3}{l}{\textbf{60 Seconds:}} \\ \hline
EEGM2 (Light)  & 76.94$\pm$0.33 & 0.8257$\pm$0.00  \\
EEGM2 (Fine)  & \textbf{80.68}$\pm$0.45 & \textbf{0.8803}$\pm$0.00 \\ \hline

\multicolumn{3}{l}{\textbf{100 Seconds:}} \\ \hline
EEGM2 (Light)  & 74.57$\pm$0.27 & 0.7986$\pm$0.00    \\ 
EEGM2 (Fine) & \textbf{81.08}$\pm$0.28 & \textbf{0.8869}$\pm$0.00 \\ \hline
\bottomrule
\end{tabular}
}
\end{table}

Most prior works on the TUAB dataset focus on short-sequence settings (e.g., 10 seconds) due to memory constraints. Studies such as BIOT~\cite{yang2024biot}, EEGPT~\cite{wangeegpt}, and EEG2Rep~\cite{eeg2rep2024} have demonstrated the effectiveness of self-supervised learning for EEG representation learning, achieving high AUROC, while utilizing various architectures like CNN-LSTM and Transformer-based models. However, these methods often struggle to capture long-range dependencies due to quadratic computational costs or memory inefficiencies. Benefiting from the integration of the Mamba-2 block, EEGM2 overcomes these limitations. To assess its capability in modeling long-sequence EEG signals, we conduct experiments on the TUAB dataset with varying durations ranging from 10 to 100 seconds, corresponding to sequence lengths from 1,280 to 12,800 samples.

Table~\ref{tab:3} presents the balanced accuracy and AUROC (the evaluation metrics can be found in Appendix~\ref{app:2}) of EEGM2 and state-of-the-art EEG models on the TUAB dataset across different sequence durations (10, 30, 60, and 100 seconds). For each duration, the best performance is highlighted in bold. We evaluate two alternative solutions of EEGM2: EEGM2 (Light), a lightweight version with a reduced model size of 0.45M parameters, and EEGM2(Fine), which retains the full architecture with 4.5M parameters. For a fair comparison, we report the best results achieved and reported in their paper in Table~\ref{tab:3}. However, note that BENDER, MAEEG, and EEG2Rep report accuracy instead of balanced accuracy, and it is highly likely that their balanced accuracy is lower due to the class imbalance in the TUAB dataset. Notably, EEGM2(Fine) consistently outperforms prior methods, achieving state-of-the-art performance at 10 seconds, with a balanced accuracy of 80.87\% and AUROC of 0.8864. Meanwhile, EEGM2 (Light), despite being 18 times smaller, still achieves comparable performance with a balanced accuracy of 79.14\% and AUROC of 0.8559. By analyzing performance across different sequence durations, interestingly we observe that the EEGM2’s performance does not decrease with increasing sequence length but instead improves, achieving its best results at 30 seconds (81.71\% balanced accuracy, 0.8932 AUROC), outperforming all other models. Even at 100 seconds, EEGM2(Fine) maintains high performance (81.08\% balanced accuracy, 0.8869 AUROC), showcasing its superior ability to model long-sequence EEG data. However, EEGM2 (Light), which doesn't have the mediator and decoder modules, and has a significantly smaller size, shows a decline in performance as sequence length increases. This could be due to the probing method applied after the encoder layer, which may lead to information loss for long sequences. On another hand, EEGM2(Fine) benefits from the full architecture, including three Mamba-2 blocks in the mediator and decoder modules, allowing it to capture the long-range dependencies and achieve superior results with long-sequence EEG.

\begin{table}
\centering
\caption{Performance Comparison of EEGM2 Variants in Long-Sequence EEG Downstream Task.}
\label{tab:4}
\begin{tabular}{lccc}
\toprule
\textbf{Models} & \textbf{Balanced ACC} & \textbf{AUROC} \\
\midrule
\textbf{EEGM2}    & \textbf{81.08} & \textbf{0.8869}   \\ \hline
\multicolumn{3}{l}{\textbf{w/o multi-branch}} \\ \hline
EEGM2-S1 & 79.06 ($\downarrow$ 2.02) & 0.8546 ($\downarrow$ 0.03) \\
\hline
\multicolumn{3}{l}{\textbf{w/o spatiotemporal loss}} \\ \hline
EEGM2-S2 & 77.01 ($\downarrow$ 4.07) & 0.8247 ($\downarrow$ 0.06)   \\
\hline
\multicolumn{3}{l}{\textbf{w/o mamba-2 block}} \\ \hline
EEGM2-S3 & 76.08 ($\downarrow$ 5.00) & 0.8176 ($\downarrow$ 0.07)  \\
EEGM2-S4 & 75.38 ($\downarrow$ 5.70) & 0.8339 ($\downarrow$ 0.05)  \\ 
EEGM2-S5 & out of memory & out of memory \\ \hline
\bottomrule
\end{tabular}
\end{table}

Table \ref{tab:4} presents an ablation study evaluating the performance of EEGM2 and its variants in the Long-Sequence EEG Downstream Task, corresponding to the pre-trained models analyzed in Table \ref{tab:2}. Compared to EEGM2, Table \ref{tab:4} reveals that removing the multi-branch receptive field input embedding has the smallest impact, resulting in a 2.02 decrease in balanced accuracy, highlighting its importance in enhancing robust EEG feature extraction. Removing the spatiotemporal loss leads to a 4.07 decrease in balanced accuracy, indicating its critical role in preserving spectral information and mitigating noise. Meanwhile, replacing the Mamba-2 block with Mamba-1 causes a significant 5.0 decrease in balanced accuracy, demonstrating its essential contribution to handling long-sequence dependencies. The most substantial decline is observed in EEGM2-S4, where both the multi-branch receptive field input embedding is removed, and the Mamba-2 block is replaced with Mamba-1, leading to a 5.7 drop in performance, highlighting the combined importance of these components.

\subsubsection{\textbf{Short-Sequence \& Multiple Tasks}}

\begin{table*}
\centering
\caption{Performance comparison of EEGM2 and state-of-the-art models on downstream tasks using the Emotiv dataset.}\label{tab:5}
\resizebox{\textwidth}{!}{
\begin{tabular}{l cc cc cc cc cc}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multicolumn{2}{c}{\textbf{Crowdsourced}} & \multicolumn{2}{c}{\textbf{DriverDistraction}} & \multicolumn{2}{c}{\textbf{STEW}} & \multicolumn{2}{c}{\textbf{Alpha}} & \multicolumn{2}{c}{\textbf{Attention}} \\ 
\cline{2-11} 
& \multicolumn{1}{c}{\textbf{ACC}} & \textbf{AUROC} & \multicolumn{1}{c}{\textbf{ACC}} & \textbf{AUROC} & \multicolumn{1}{c}{\textbf{ACC}} & \textbf{AUROC} & \multicolumn{1}{c}{\textbf{ACC}} & \textbf{AUROC} & \multicolumn{1}{c}{\textbf{ACC}} & \textbf{AUROC} \\ 
\hline
MAEEG \cite{chien2211maeeg} & \multicolumn{1}{c}{86.75$\pm$3.50} & 0.8621$\pm$0.03 & \multicolumn{1}{c}{74.58$\pm$2.16} & 0.6079$\pm$0.03 & \multicolumn{1}{c}{72.46$\pm$3.67} & 0.7250$\pm$0.03 & \multicolumn{1}{c}{69.18$\pm$1.54} & 0.7949$\pm$0.02 & \multicolumn{1}{c}{82.61$\pm$0.01} & 0.5282$\pm$0.03 \\ 

BENDR \cite{kostas2021bendr} & \multicolumn{1}{c}{83.78$\pm$2.35} & 0.8380$\pm$0.03 & \multicolumn{1}{c}{74.31$\pm$2.38} & 0.5986$\pm$0.03 & \multicolumn{1}{c}{69.74$\pm$2.11} & 0.6977$\pm$0.02 & \multicolumn{1}{c}{65.75$\pm$2.50} & 0.6764$\pm$0.02 & \multicolumn{1}{c}{76.93$\pm$4.04} & 0.5464$\pm$0.04 \\ 

EEG2Rep \cite{eeg2rep2024} & \multicolumn{1}{c}{94.13$\pm$2.11} & 0.9413$\pm$0.02 & \multicolumn{1}{c}{80.07$\pm$2.63} & 0.6614$\pm$0.02 & \multicolumn{1}{c}{73.60$\pm$1.47} & 0.7440$\pm$0.02 & \multicolumn{1}{c}{73.10$\pm$2.76} & 0.8118$\pm$0.07 & \multicolumn{1}{c}{75.41$\pm$3.20} & 0.6635$\pm$0.03 \\ 

BIOT \cite{yang2024biot} & \multicolumn{1}{c}{87.95$\pm$3.52} & 0.8778$\pm$0.03 & \multicolumn{1}{c}{74.34$\pm$3.57} & 0.6121$\pm$0.04 & \multicolumn{1}{c}{69.88$\pm$2.15} & 0.7011$\pm$0.03 & \multicolumn{1}{c}{70.72$\pm$1.32} & 0.7698$\pm$0.03 & \multicolumn{1}{c}{73.59$\pm$7.24} & 0.6326$\pm$0.13 \\ 

\hline
EEGM2(Linear) & \multicolumn{1}{c}{84.84$\pm$0.05} & 0.9185$\pm$0.00 & \multicolumn{1}{c}{73.99$\pm$0.06} & 0.7046$\pm$0.00 & \multicolumn{1}{c}{68.59$\pm$0.00} & 0.7341$\pm$0.00 & \multicolumn{1}{c}{66.75$\pm$0.13} & 0.7971$\pm$0.00 & \multicolumn{1}{c}{76.05$\pm$0.02} & 0.6479$\pm$0.00 \\ 

EEGM2(Light)                                  & \multicolumn{1}{c}{86.13$\pm$0.21}          & 0.9245$\pm$0.01          & \multicolumn{1}{c}{\textbf{81.11$\pm$0.13}} & 0.6825$\pm$0.01          & \multicolumn{1}{c}{70.24$\pm$0.69}          & 0.7523$\pm$0.01                      & \multicolumn{1}{c}{{75.69$\pm$1.20}}    & { 0.8563$\pm$0.02}                & \multicolumn{1}{c}{\textbf{82.81$\pm$0.35}} & { 0.6708$\pm$0.01}    \\ 
EEGM2(Scratch)                                   & \multicolumn{1}{c}{84.19$\pm$3.83}          & 0.9302$\pm$0.02          & \multicolumn{1}{c}{73.44$\pm$2.93}          & 0.6445$\pm$0.01          & \multicolumn{1}{c}{72.39$\pm$2.39}          & {0.7891$\pm$0.00}                & \multicolumn{1}{c}{68.12$\pm$3.70}          & 0.8137$\pm$0.06                      & \multicolumn{1}{c}{76.58$\pm$1.05}          & 0.6262$\pm$0.06         \\ 

EEGM2(Fine) & \multicolumn{1}{c}{\textbf{94.51$\pm$1.31}} & \textbf{0.9881$\pm$0.00} & \multicolumn{1}{c}{76.54$\pm$1.21} & \textbf{0.7097$\pm$0.01} & \multicolumn{1}{c}{\textbf{74.26$\pm$1.48}} & \textbf{0.7901$\pm$0.02} & \multicolumn{1}{c}{\textbf{77.49$\pm$4.27}} & \textbf{0.8856$\pm$0.02} & \multicolumn{1}{c}{79.14$\pm$3.15} & \textbf{0.6885$\pm$0.01} \\ 
\hline
\bottomrule
\end{tabular}
}
\end{table*}

Table \ref{tab:5} presents the performance of EEGM2 and state-of-the-art models on downstream tasks using the Emotiv dataset, evaluating EEGM2’s stability and cross-subject generalization. The table reports classification accuracy (ACC) and AUROC as performance metrics, the best performance for each dataset are marked in bold. Unlike random sample-based splits, the Emotiv datasets follow a subject-wise split, where entire subjects (individual participants) are assigned exclusively to either the training, validation, or testing sets. This setup enables evaluating the cross-subject generalization, as models must learn robust EEG representations that generalize across different individuals rather than memorizing subject-specific patterns. Additionally, this approach allows us to assess model robustness against the unique signal noise characteristics of different subjects.

For comparison, Table \ref{tab:5} includes EEGM2(Linear) and EEGM2(Light), the two variants of EEGM2 that are based on pretrained encoder only and updated by probing techniques, as described in Section \ref{sec:4.2.2}. EEGM2(Fine) is the fine-tuned model utilizing the complete encoder-decoder structure (Section \ref{sec:4.2.3}). EEGM2(Scratch) serves as a baseline trained directly from random initialization without any pertaining. Furthermore, we reproduce state-of-the-art Transformer-based models, including MAEEG \cite{chien2211maeeg}, BENDR \cite{kostas2021bendr}, EEG2Rep \cite{eeg2rep2024}, and BIOT \cite{yang2024biot}, ensuring that all models follow the same experimental setup for fair comparison. 

From Table \ref{tab:5}, we observe that the best performance is consistently achieved by either EEGM2(Fine) or EEGM2(Light), outperforming all other models, including the baseline EEGM2(Scratch). Notably, EEGM2(Linear), despite its linear probing assumption and being 18 times smaller than EEGM2(Scratch), still achieves competitive or superior results in the Crowdsourced, DriverDistraction, and Attention (AUROC) datasets, demonstrating the effectiveness of the learned latent representations in EEGM2’s encoder. One major challenge in EEG classification is handling imbalanced datasets, where class distributions are highly skewed, leading to biased predictions in traditional supervised models. This issue is particularly evident in DriverDistraction, which exhibits severe class imbalance. While EEGM2 did not achieve the highest accuracy, EEGM2(Fine) obtained the highest AUROC of 0.7097, surpassing all other models, despite having a lower accuracy of 76.54\%. Since AUROC is a more robust metric for evaluating imbalanced classification tasks, this result highlights EEGM2's capability to extract meaningful EEG representations rather than relying on majority-class predictions. Overall, the results in Table \ref{tab:5} highlight EEGM2's superior ability to capture transferable EEG representations, consistently outperforming prior self-supervised and fine-tuned models across multiple EEG classification tasks. %This demonstrates EEGM2’s effectiveness in handling short-sequence adaptability, cross-subject variability, and imbalanced data distributions, further reinforcing its potential toward foundation model.

\subsubsection{\textbf{Cross-Domain}}
\begin{table*}
\centering
\caption{EEGM2 performance, pre-trained on DriverDistraction, in cross-domain and in-domain settings.}
\label{tab:6}
\begin{tabular}{l cc cc cc cc cc} 
\toprule
\multirow{2}{*}{\textbf{Initilization}} & \multicolumn{2}{c}{\textbf{Crowdsourced}}            & \multicolumn{2}{c}{\textbf{STEW}}                          & \multicolumn{2}{c}{\textbf{Alpha}}                         & \multicolumn{2}{c}{\textbf{Attention}} & \multicolumn{2}{c}{\textbf{Average}}          \\ \cline{2-11}
  & \textbf{Acc}   & \textbf{AUROC} & \textbf{Acc}   & \textbf{AUROC}        & \textbf{Acc}   & \textbf{AUROC}        & \textbf{ACC}   & \textbf{AUROC}  & \textbf{ACC}   & \textbf{AUROC}  \\ \hline
Random & 84.19 & 0.9302 & 72.96 & 0.7842&	68.12 &	0.8137&	76.58&	0.6262&	75.46	& 0.7886  \\ 
Cross-domain &      ($\uparrow$ 8.33) &	($\uparrow$ 0.0506) &	($\uparrow$ 0.98) &	($\uparrow$ 0.0129) &	($\uparrow$ 1.61) &	($\uparrow$ 0.0597) &	($\uparrow$ 0.07) &	($\downarrow$ 0.0153) &	($\uparrow$ 2.75) &	($\uparrow$ 0.0270)     \\

In-domain   & ($\uparrow$ 10.32)  &	($\uparrow$ 0.0579)& ($\uparrow$ 1.86) &	($\uparrow$ 0.0227) &	($\uparrow$ 9.38)	 & ($\uparrow$ 0.0720)	 & ($\uparrow$ 0.89) &	($\uparrow$ 0.0288) &	($\uparrow$ 5.61)	 & ($\uparrow$ 0.0453)   \\\hline
%\bottomrule
\end{tabular}
\end{table*}

To evaluate the cross-domain generalization of EEGM2 and its potential toward building an EEG foundational model, we conducted experiments across multiple datasets. Given that DriverDistraction contains the largest number of samples, we employed a cross-domain transfer learning strategy, where EEGM2 is first pre-trained on DriverDistraction and then fine-tuned on the remaining Emotiv datasets. For comparison, we also provide the result from in-domain pre-training, where EEGM2 is trained directly on the corresponding dataset.

The results from Table \ref{tab:6} demonstrate that EEGM2 significantly outperforms random initialization in both in-domain and cross-domain settings. When pre-trained on driver distraction detection, EEGM2 retains transferable knowledge, leading to consistent performance gains across diverse downstream tasks, including eye state classification (open/closed) (Crownsource), mental workload estimation (STEW), alpha wave eye open/close (Alpha) and attention detection. Notably, EEGM2 achieves an average improvement of 2.75\% in accuracy and 0.027 in AUROC under cross-domain pre-training, highlighting its strong generalization capability across different EEG-based tasks. Additionally, in-domain pre-training consistently achieves the best performance, indicating the importance of alignment between training and test distributions. Specifically, in-domain pre-training yields a 5.61\% accuracy gain and a 0.0453 AUROC improvement compared to training from scratch. Overall, the cross-domain experiments confirm that EEGM2 serves as an effective EEG representation learner, enabling to transfer knowledge across diverse cognitive and neurological tasks. By expanding pre-training datasets to cover a wider range of EEG signals, subjects, and experimental conditions, EEGM2 can further enhance its robustness and position itself toward foundation model for future EEG applications.


\subsection{Memory Usage \& Inference Speed} \label{sec:4.5}
\begin{table}
\centering
\caption{State-of-the-art EEG models and their size.}
\label{tab:7}
\begin{tabular}{l l c}
\hline
\textbf{Reference}    & \textbf{Model} & \textbf{Model Size} \\ \hline
\cite{yang2024biot}   & BIOT   &  3.2M          \\ 
\cite{chien2211maeeg} & MAEEG  &  2.5M          \\ 
\cite{kostas2021bendr}& BENDR  &  33M           \\ 
\cite{eeg2rep2024}    & EEG2Rep&  0.1M           \\ 
This paper & EEGM2 (Light) & 0.25M \\ 
This paper & EEGM2-S5   & 4.5M  \\ 
This paper & EEGM2   & 4.5M  \\ \hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%
% Model/Model size summary
%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we analyze the memory usage and inference time of recent state-of-the-art self-supervised EEG models, along with the proposed EEGM2, through simulation experiments. Table \ref{tab:7} summarizes the number of parameters of these models under "Model Size." Among them, all models except EEGM2 and EEGM2 (Light), which are based on the Mamba-2 block, utilize Transformer-based architectures. BIOT adopts a linear Transformer mechanism to efficiently capture complex token interactions while maintaining linear complexity. As a variant of EEGM2, EEGM2-S5 replaces the Mamba-2 blocks in EEGM2 with Transformer blocks, as described in Section \ref{sec:4.3}. MAEEG and BENDR share a Transformer-based architecture with a masking mechanism but differ in their learning objectives. BENDR combines contrastive and predictive losses to enhance EEG feature representation, while MAEEG follows a Masked Autoencoder (MAE) framework, where the model reconstructs masked EEG signals as its primary training objective. EEG2Rep, on the other hand, leverages predictive self-supervised learning with masking strategies to further improve EEG feature representations.

%%%%%%%%%%%%%%%%%%
% simulation setup
%%%%%%%%%%%%%%%%%%
The simulation experiment evaluates the performance of various self-supervised EEG models, including the proposed EEGM2, on a 16-channel simulated EEG signal with sequence lengths ranging from 50 to 12,000. Each model undergoes a warm-up phase with 15 inference runs to ensure stable GPU performance before evaluation. All experiments are conducted on a single NVIDIA RTX 6000 Ada GPU with a maximum memory capacity of 51,546 MB. For each sequence length, the experiment measures memory usage (MB), defined as the peak memory consumption during model inference, and inference speed (samples/ms), representing the number of samples processed per millisecond, calculated as the inverse of the per-sample inference time. The inference time is averaged over 10 trials, and results are visualized as memory usage and inference speed curves as a function of sequence length. Transformer-based models, Mamba-based models, and hybrid architectures are compared under the same conditions to assess their efficiency and scalability.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.9\linewidth} 
    \centering
    \includegraphics[width=\linewidth]{Figure/Figure2a.pdf}
    \caption{Memory Usage.}
    \label{fig:2a}
  \end{subfigure}
  \hfill 
  \begin{subfigure}[b]{0.9\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figure/Figure2b.pdf}
    \caption{Inference Speed.}
    \label{fig:2b}
  \end{subfigure}
    \caption{Memory usage and inference speed across varying sequence lengths. }
  \label{fig2}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Memory Usage Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%
As shown in Figure \ref{fig:2a}, parameter size is not the key factor affecting memory usage. For example, EEG2Rep, despite having only  0.1M parameters, runs out of memory when the sequence length exceeds 6000, while, EEGM2 benefits from the Mamba-2 block, demonstrating a linear increase in memory usage as sequence length grows. Compared to EEGM2, EEGM2 (Light) has only 0.25M parameters, yet its reduction in memory usage is minimal, indicating that the main factor affecting memory is not parameter count but computational complexity. MAEEG and BENDR, due to their similar Transformer-based architectures, exhibit comparable memory consumption. Since Transformers have \(\mathcal{O}(N^2)\) complexity, their memory usage increases quadratically with sequence length, leading to out-of-memory errors when the sequence length exceeds approximately 3000. BIOT, on the other hand, achieves the lowest memory usage due to its linear Transformer mechanism and simplified Transformer block strategy. Instead of traditional Transformer, BIOT employs a linear Transformer layer combined with a lightweight fully connected network and residual connections, avoiding the quadratic complexity of standard Transformers and significantly reducing memory overhead. EEGM2-S5, a variant of EEGM2 where the Mamba-2 block is replaced with an Transformer block, also encounters out-of-memory when the sequence length surpasses 6000, demonstrating the significant contribution of the Mamba-2 block on memory scaling. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Inference Speed Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%
In terms of inference speed, as shown in Figure \ref{fig:2b}, we evaluate model efficiency using samples per millisecond (samples/ms) to assess their suitability for deployment on resource-limited edge BCI devices. Transformer-based models, including MAEEG, EEG2Rep, and EEGM2-S5, experience a significant decline in inference speed as sequence length increases. BENDR, due to its large model size and high number of parameters, exhibits the slowest inference speed. Although BIOT employs a linear Transformer mechanism, its inference speed is not particularly impressive, likely due to its relatively large parameter size of 3.2M. On the other hand, EEGM2 (Light) demonstrates the highest inference speed in terms of samples per millisecond. This advantage stems from its efficient Mamba-2 block and small parameter size of 0.25M, highlighting its potential toward an efficient foundation model for downstream tasks, making it a practical solution for real-time BCI inference.


\section{Conclusion}
In this paper, we introduce EEGM2, a Mamba-2-based self-supervised framework for efficient modeling of various range sequences in EEG data. By leveraging structured state-space duality, a spatiotemporal loss function, and a multi-branch receptive field embedding, EEGM2 effectively captures long-range dependencies while maintaining computational efficiency. Experimental results on multiple EEG datasets demonstrate its superiority over Transformer-based models, achieving state-of-the-art accuracy with significantly lower memory usage and faster inference. To address potential resource constraints on edge devices, we propose a lightweight version, EEGM2 Light, which delivers strong performance compared to the original model. Both EEGM2 and EEGM2 Light provide accurate results, offering flexibility based on the trade-off between accuracy, memory usage, and inference time. 

%Future work will focus on enhancing feature aggregation for long-sequence tasks and expanding pretraining across diverse EEG domains to further improve generalization.



% \section{Acknowledgments}






%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.

\newpage
% \bibliographystyle{ACM-Reference-Format}
\bibliographystyle{IEEEtran}
\bibliography{Reference}


%%
%% If your work has an appendix, this is the place to put it.
\appendix
\section{Emotiv Dataset}\label{app:1}
\subsection{Attention Dataset} 
The Attention Dataset was collected through an experiment where subjects completed four tasks—two visual and two auditory—designed to assess attention in classifying repeated stimuli. In visual tasks, participants viewed four-digit numbers and clicked when the same number appeared consecutively, with a total duration of 640 seconds. In auditory tasks, they listened to three words and clicked when a word was repeated in sequence, lasting 540 seconds. Each subject completed a total stimulus time of 19 minutes and 40 seconds. Data were recorded using a 14-channel Emotiv Epoc headset, generating multivariate time-series data. After preprocessing and manual labeling, data from 31 subjects were collected, with 4 excluded due to poor quality.

\subsection{Crowdsourced}
Crowdsourced is a publicly available dataset \cite{williams2023crowdsourced} collected while participants performed a resting-state task, alternating between two-minute intervals with eyes open and eyes closed. Among the 60 participants, only 13 successfully completed both conditions using 14-channel EPOC$+$, EPOC X, and EPOC devices. The data was originally recorded at 2048 Hz and later downsampled to 128 Hz. The raw EEG recordings from these 13 participants, along with pre-processing, analysis, and visualization scripts, are publicly accessible on the Open Science Framework (OSF).

\subsection{DriverDistraction}
DriverDistraction was obtained by recording EEG brain activity from 17 participants while they engaged in a driving simulation for around 40 minutes. During the simulation, participants carried out various distraction tasks, which can be categorized into three main types: (1) conversing with a passenger, (2) interacting with a mobile phone (including texting and calling), and (3) engaging in problem-solving activities. EEG signals were captured at a sampling rate of 128 Hz using the Emotiv Epoc EEG headset, which records data from 14 channels. The resulting dataset is a multivariate time series with 14 input variables and approximately 5.5 million records. Each time point in the dataset was manually labeled according to the specific activity being performed.

\subsection{STEW}
The STEW dataset is a publicly available dataset \cite{lim2018stew} that consists of raw EEG recordings collected from 48 participants who took part in a multitasking workload experiment using the SIMKAP multitasking test. Prior to the test, baseline brain activity at rest was also recorded. EEG signals were captured using a 14-channel Emotiv EPOC headset at a sampling rate of 128 Hz, resulting in 2.5 minutes of recorded data per participant. After each stage of the experiment, participants assessed their perceived mental workload on a scale from 1 to 9, with these ratings stored in a separate file. Additionally, the dataset includes binary class labels, where workload ratings greater than 4 are categorized as high, while ratings of 4 or below are classified as low. These labels are utilized for specific analytical purposes. The STEW dataset is available upon request via IEEE DataPort.

\section{AUROC \& Balanced ACC}\label{app:2}
Balanced Accuracy (ACC) represents the mean recall across all classes, offering a more reliable assessment of model performance, particularly in imbalanced datasets. The Area Under the Receiver Operating Characteristic Curve (AUROC) measures a model’s ability to differentiate between classes by condensing the ROC curve into a single value.

\section{Ablation Study - Long-Sequence Modeling} \label{app:3}
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.48\linewidth} 
    \centering
    \includegraphics[width=\linewidth]{Figure/Figure3a.png}
    \caption{EEGM2 and EEGM2-S1.}
    \label{fig:app1}
  \end{subfigure}
  \hfill 
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figure/Figure3b.png}
    \caption{EEGM2 and EEGM2-S2.}
    \label{fig:app2}
  \end{subfigure}
  \hfill 
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figure/Figure3c.png}
    \caption{EEGM2 and EEGM2-S3.}
    \label{fig:app3}
  \end{subfigure}
  \hfill 
  \begin{subfigure}[b]{0.48\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Figure/Figure3d.png}
    \caption{EEGM2 and EEGM2-S4.}
    \label{fig:app4}
  \end{subfigure}
  \caption{Training time of EEGM2 and its variants across four different settings.}
  \label{fig:3}
\end{figure}

Below, we detail the five different configurations of EEGM2 described in Section~\ref{sec:4.3}:
\begin{itemize}
 \item \textbf{EEGM2}: The full version of EEGM2, representing the most powerful configuration. It integrates all components, including the Mamba-2 block, spatiotemporal loss, and a multi-branch receptive field input embedding.
 \item \textbf{EEGM2-S1 (w/o multi-branch receptive field)}: A variant of EEGM2 that retains the Mamba-2 block and spatiotemporal loss but removes the multi-branch receptive field.
 \item \textbf{EEGM2-S2 (w/o spatiotemporal loss)}: A variant of EEGM2 that includes the Mamba-2 block and multi-branch receptive field but replaces the spatiotemporal loss with L1 loss.
 \item \textbf{EEGM2-S3 (Mamba-2 replaced with Mamba-1)}: A variant of EEGM2 where the Mamba-2 block is replaced with the Mamba-1 block, while maintaining the spatiotemporal loss and multi-branch receptive field.
 \item \textbf{EEGM2-S4 (Mamba-2 replaced with Mamba-1, w/o multi-branch)}: Similar to EEGM2-S3, but with the multi-branch receptive field removed.
 \item \textbf{EEGM2-S5 (Mamba-2 replaced with Transformer)}: A variant of EEGM2 where the Mamba-2 block is replaced with an Transformer block.
\end{itemize}

Following the ablation study of EEGM2 in self-supervised learning for TUAB 100-second duration modeling, Figure~\ref{fig:3} illustrates EEGM2 with four different settings. From Figure~\ref{fig:app1}, we can see that incorporating a multi-branch structure does not significantly impact training time. From Figure~\ref{fig:app2}, we observe that the spectral loss preserves spectral information, and the spatiotemporal loss stabilizes training. Figures~\ref{fig:app3} and~\ref{fig:app4} show that replacing Mamba-2 with Mamba-1 significantly reduces training speed and increases computational cost.

% \section{Implementation of State-of-the-Art EEG Models} \label{app:4}
% To ensure a fair comparison, we reproduced the state-of-the-art EEG models MAEEG~\cite{chien2211maeeg}, BENDR~\cite{kostas2021bendr}, EEG2Rep~\cite{eeg2rep2024}, and BIOT~\cite{yang2024biot} according to their original architectures. All models were trained and evaluated using the same datasets and preprocessing pipeline as EEGM2. For BENDR, EEG2Rep, and BIOT, we utilized their official GitHub repositories for implementation. For MAEEG, since no official code was provided, we followed the architecture description in the paper and referenced the structure of BENDR to ensure an accurate reproduction. In the Memory Usage and Inference Speed analysis, we deactivated the downsampling functions in BENDR and MAEEG, to maintain fairness in affecting the computational efficiency analysis across different sequence lengths.



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
