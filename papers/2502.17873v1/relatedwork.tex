\section{Related Work and Objectives }
Self-supervised learning has gained traction in the field of EEG representation learning, offering a way to derive meaningful representations from sparsely labeled EEG data \cite{deng2022boosting}. With the increase in EEG data availability, the development of foundation models for EEG has become a realistic goal. EEG2Rep~\cite{eeg2rep2024} and MAEEG~\cite{chien2211maeeg} demonstrated that pretraining models using self-supervised objectives significantly improves performance, especially when labeled data is scarce. BIOT~\cite{yang2024biot} introduced a tokenization module designed for general biosignal processing, allowing for cross-domain learning on EEG, ECG, and human sensory signals. Contrastive learning-based approaches~\cite{mohsenvand2020contrastive} have been explored to learn EEG representations by maximizing the agreement between differently augmented views of the same signal. BENDR~\cite{kostas2021bendr} extended this idea by applying masked autoencoder-based training and contrastive learning, providing a scalable way to pretrain EEG models. MAEEG~\cite{chien2211maeeg}, on the other hand, focuses purely on reconstruction-based learning, emphasizing the utility of reconstructing raw EEG signals as a pretraining strategy. Masking-based methods, EEGPT~\cite{wangeegpt} and EEG2Rep~\cite{eeg2rep2024} adopt mask-based dual self-supervised learning objectives, improving feature generalization across multiple EEG tasks.  Despite these advancements, most self-supervised EEG models struggle with long-sequence processing due to the inherent computational challenges of Transformer-based architectures. While Transformers excel at modeling long-range dependencies, their quadratic computational complexity in sequence length makes them inefficient for high-resolution EEG data. This issue is particularly pronounced in foundation models, which require scalability across different EEG paradigms. To address this, recent work has begun to explore structured state-space models (SSMs) as an alternative to Transformer mechanisms. Mamba, a recent SSM-based model, has demonstrated linear-time complexity while maintaining strong sequence modeling capabilities~\cite{gu2023mamba}. A memory usage and inference speed analysis comparing Transformer-based and Mamba-based models can be found in Section \ref{sec:4.5}.

%Inspired by the encoder-decoder structure of U-Net~\cite{ronneberger2015unet} and EEG signal reconstruction in MAEEG~\cite{chien2211maeeg}, EEGM2 is built upon a reconstruction-based encoder-decoder self-supervised architecture. Considering the challenges posed by long sequences, where Transformer-based architectures may struggle, EEGM2 incorporates the structured state-space model (SSM) blocks, specifically Mamba-2 blocks, which are highly effective in long-sequence modeling~\cite{dao2024transformers}. This reconstruction-based framework enables EEGM2 to leverage both local and global EEG features, making it robust to noise and suitable for transfer learning in downstream tasks. Moreover, to investigate whether EEGM2 learns meaningful EEG representations, we first train the model without labels using raw EEG signals, comparing the reconstructed signals to the originals. Subsequently, we extract the features from the encoder layer, referred to as the latent representation.