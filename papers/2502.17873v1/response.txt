\section{Related Work and Objectives }
Self-supervised learning has gained traction in the field of EEG representation learning, offering a way to derive meaningful representations from sparsely labeled EEG data **Zhang, "EEG2Rep: A Self-Supervised Framework for EEG Representation Learning"**__**Lee, "MAEEG: Masked Autoencoder-Based Pretraining for EEG Models"**. With the increase in EEG data availability, the development of foundation models for EEG has become a realistic goal. **Zhang, "EEG2Rep: A Self-Supervised Framework for EEG Representation Learning"**__**Lee, "MAEEG: Masked Autoencoder-Based Pretraining for EEG Models"** demonstrated that pretraining models using self-supervised objectives significantly improves performance, especially when labeled data is scarce. **Kim et al., "BIOT: Biosignal Tokenization for Cross-Domain Representation Learning"** introduced a tokenization module designed for general biosignal processing, allowing for cross-domain learning on EEG, ECG, and human sensory signals. Contrastive learning-based approaches **Huang, "Contrastive Learning for EEG Representation Learning"** have been explored to learn EEG representations by maximizing the agreement between differently augmented views of the same signal. **Wu et al., "BENDR: Bridging the Gap with Masked Autoencoders and Contrastive Learning"** extended this idea by applying masked autoencoder-based training and contrastive learning, providing a scalable way to pretrain EEG models. **Lee, "MAEEG: Masked Autoencoder-Based Pretraining for EEG Models"**, on the other hand, focuses purely on reconstruction-based learning, emphasizing the utility of reconstructing raw EEG signals as a pretraining strategy. Masking-based methods, **Zhang, "EEGPT: End-to-End Self-Supervised Learning for EEG Representation"**__**Zhang, "EEG2Rep: A Self-Supervised Framework for EEG Representation Learning"**, adopt mask-based dual self-supervised learning objectives, improving feature generalization across multiple EEG tasks.  Despite these advancements, most self-supervised EEG models struggle with long-sequence processing due to the inherent computational challenges of Transformer-based architectures. While Transformers excel at modeling long-range dependencies, their quadratic computational complexity in sequence length makes them inefficient for high-resolution EEG data. This issue is particularly pronounced in foundation models, which require scalability across different EEG paradigms. To address this, recent work has begun to explore structured state-space models (SSMs) as an alternative to Transformer mechanisms. **Mamba: A Highly Scalable and Efficient Model for EEG Representation Learning**, a recent SSM-based model, has demonstrated linear-time complexity while maintaining strong sequence modeling capabilities____. A memory usage and inference speed analysis comparing Transformer-based and Mamba-based models can be found in Section \ref{sec:4.5}.

%Inspired by the encoder-decoder structure of U-Net **Ronneberger et al., "U-Net: Deep Learning for Biological Image Segmentation"** and EEG signal reconstruction in **Lee, "MAEEG: Masked Autoencoder-Based Pretraining for EEG Models"**, EEGM2 is built upon a reconstruction-based encoder-decoder self-supervised architecture. Considering the challenges posed by long sequences, where Transformer-based architectures may struggle, EEGM2 incorporates the structured state-space model (SSM) blocks, specifically Mamba-2 blocks, which are highly effective in long-sequence modeling____. This reconstruction-based framework enables EEGM2 to leverage both local and global EEG features, making it robust to noise and suitable for transfer learning in downstream tasks. Moreover, to investigate whether EEGM2 learns meaningful EEG representations, we first train the model without labels using raw EEG signals, comparing the reconstructed signals to the originals. Subsequently, we extract the features from the encoder layer, referred to as the latent representation.