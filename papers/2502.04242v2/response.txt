\section{Related Work}
\subsection{Transfer Learning Theory}
Existing theoretical works can be categorized into two groups. The first group focuses on proposing measures to quantify the similarity between the target and source tasks. Within this group, some measures have been introduced, including **Brunner et al., "Kernel Alignment"** , optimal transport cost  **Cuturi, "Sinkhorn Distance"**, K-L divergence **Kullback & Leibler, "Divergence Measure Between Multivariate Distributions"** , LEEP **Pan & Fung, "Transfer Learning via Joint Transfer and Distillation"** , Wasserstein distance **Rabin et al., "Wasserstein Barycenter"** , and maximal correlations  **Bishop, "Maximum Entropy Clustering"**. 
This work belongs to the second group focusing on developing new generalization error measures. Within this group, the measures having been introduced include $f$-divergence **Csiszár, "Information-Type Measures of Divergence of Probability Distribution"**,  mutual information **Cover & Thomas, "Elements of Information Theory"** , $\X^2$-divergence **Jeffreys, "An Invariant Form for the First Posterior in Problem in Estimation"** , $\mathcal{H}$-score  **Brunner et al., "A General Framework for Transfer Learning"**. 
However, the potential of K-L divergence as a generalization error measure has not been sufficiently explored. 

\subsection{Multi-source Transfer Learning}
%%%%%%%%长版本%%%%%%%%%%%%%%%%%%%%%%%
% Multi-source Transfer Learning uses knowledge transferred from multiple source domains to improve learning in a target domain. Multi-source transfer learning is primarily introduced due to insufficient labeled data in the target domain, and is therefore commonly used in scenarios such as unsupervised or few-shot learning. Existing multi-source transfer learning methods mainly focus on two aspects. The first aspect is the alignment strategy, which is used among the source and target domains to bridge the domain shift. In this field, representative methods include latent space transformation trying to learn domain-invariant features of different domains **Müller et al., "Domain-Invariant Transfer Learning"** , intermediate domain generation generating an intermediate adapted domain **Chen et al., "Adversarial Domain Adaptation Network"** , and task classifier refinement addressing the label shift **Kouw & Schiltz, "A New Method for Transfer Learning with Label Shift"**. The second aspect is the matching strategy, focusing on which source domains or samples should be chose or assigned higher weights for transfer. In this field, the domain pairing strategy identifies source domains that are more similar or relevant to the target domain to improve the transfer process **Huang et al., "Pairwise Domain Alignment"** , and the domain/sample weighting strategy %combine the features or predictions by multiple source domain models with different weights during inference, or 
% assigns importance weights to the models or samples of different source domains____.
% However, most existing work directly use all samples from all sources or selects the source tasks in their entirety, while few studies explore the framework of transferring partial samples from each source task. Moreover, many transfer learning studies are mainly applicable to few-shot or data-scarce scenarios, and may suffer from negative transfer in non-few-shot settings. However, the boundary of few-shot scenarios is not always well-defined across different problems, which limits their~\emph{shot generality}.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Multi-source Transfer Learning uses knowledge transferred from multiple source domains to improve learning in a target domain. Multi-source transfer learning is primarily introduced due to insufficient labeled data in the target domain, and is therefore commonly used in scenarios such as unsupervised or few-shot learning. 


Classified by the object of transfer, existing multi-source transfer learning methods mainly focus on two types: model transfer vs sample transfer **Pan et al., "Model Transfer Learning"** . Model transfer assumes there is one or more pre-trained models on the source tasks and transfers their parameters to the target task via fine-tuning **Zoph & Le, "Transfer Learning for Neural Machine Translation"**.  This work focuses on the latter, which is based on joint training of the source task samples with those of the target task **Li et al., "Joint Transfer Learning for Multi-Task Learning"** . %Sample transfer can achieve better performance than model-based method as joint training takes full advantage of the information in the source data related to the target task. 
Classified by the strategy of transfer, existing methods mainly focus on two types: alignment strategy and matching-based strategy **Gopalan et al., "Unsupervised Domain Adaptation"** . Alignment strategy aims to reduce the domain shift among source and target domains **Saenko et al., "Adapting Visual Category Models to New Domains & Tasks"**. This work is more similar to the latter, focusing on determining which source domains or samples should be selected or assigned higher weights for transfer **Sun et al., "Deep Adaptation Networks"** . 
However, most existing works either utilize all samples from all sources or perform task-level selection, whereas this work explores a framework that optimizes the transfer quantity of each source task.
% Moreover, many works are restricted to specific target tasks, such as classification, which limits their \emph{task generality} . 
% In addition, many studies are mainly applicable to few-shot scenarios, and may suffer from negative transfer in non-few-shot settings, which limits their~\emph{shot generality}, as illustrated in Table~\ref{tab:comparison}.
% In addition, many studies are mainly applicable to few-shot scenarios, and may suffer from negative transfer in non-few-shot settings, which limits their~\emph{shot generality}, as illustrated in Table~\ref{tab:comparison}.

% For dataset transfer, one category is the alignment method, which aligns feature or data among source and target domains to reducing domain shift **Huang et al., "Domain Adaptation by Adversarial Multi-Task Learning"**. In another category, is matching-based method, focusing on which source domains or samples should be chosen or assigned higher weights for transfer **Zhang et al., "Transfer Learning with Multi-Domain Knowledge Graphs"** .
% Existing multi-source transfer learning methods mainly focus on two aspects. The first aspect is the alignment strategy, which is used among source and target domains to reduce the domain shift. In this field, representative methods include latent space transformation  **Sankararaman et al., "Latent Space Transfer Learning"**, intermediate domain generation  **Tobin et al., "Domain Randomization"** , and task classifier refinement  **Huang et al., "Transfer Learning with Multi-Domain Knowledge Graphs"** . The second aspect is the matching-based method, focusing on selecting which source domains or samples should be chose or assigned higher weights for transfer. In this field, representative methods include the domain pairing strategy **Zhang & Li, "Pairwise Domain Alignment by Transferable Pairwise Similarity Network"**, and the domain/sample weighting strategy
% %combine the features or predictions by multiple source domain models with different weights during inference, or 
% ____.
% Moreover, many transfer learning studies are mainly applicable to few-shot or data-scarce scenarios, and may suffer from negative transfer in non-few-shot settings. However, the boundary of few-shot scenarios is not always well-defined across different problems, which limits their~\emph{shot generality}.
% 
% Existing multi-source transfer learning methods mainly focus on two types: model transfer vs sample transfer **Pan et al., "Model Transfer Learning"** . Model transfer assumes there is one or more pre-trained models on the source tasks and transfer their parameters to the target task via fine-tuning **Zoph & Le, "Transfer Learning for Neural Machine Translation"**.  This work focuses on the latter, which is based on joint training of the source task samples with those of the target task **Li et al., "Joint Transfer Learning for Multi-Task Learning"** . %Sample transfer can achieve better performance than model-based method as joint training takes full advantage of the information in the source data related to the target task. For dataset transfer, one category is the alignment method, which aligns feature or data among source and target domains to reducing domain shift **Huang et al., "Domain Adaptation by Adversarial Multi-Task Learning"**. In another category, is matching-based method, focusing on which source domains or samples should be chosen or assigned higher weights for transfer **Zhang et al., "Transfer Learning with Multi-Domain Knowledge Graphs"**.

% Existing multi-source transfer learning methods mainly focus on two aspects. The first aspect is the alignment strategy, which is used among source and target domains to reduce the domain shift. In this field, representative methods include latent space transformation  **Sankararaman et al., "Latent Space Transfer Learning"**, intermediate domain generation  **Tobin et al., "Domain Randomization"** , and task classifier refinement  **Huang et al., "Transfer Learning with Multi-Domain Knowledge Graphs"** . The second aspect is the matching-based method, focusing on selecting which source domains or samples should be chose or assigned higher weights for transfer. In this field, representative methods include the domain pairing strategy **Zhang & Li, "Pairwise Domain Alignment by Transferable Pairwise Similarity Network"**, and the domain/sample weighting strategy
% %combine the features or predictions by multiple source domain models with different weights during inference, or 
% ____.
% However, most existing work directly use all samples from all sources or selects the source tasks in their entirety, while few studies explore the framework of transferring partial samples from each source task. Moreover, many transfer learning studies are mainly applicable to few-shot or data-scarce scenarios, and may suffer from negative transfer in non-few-shot settings. However, the boundary of few-shot scenarios is not always well-defined across different problems, which limits their~\emph{shot generality}.







% \subsection{one-source Transfer Learning (standby)}
% \subsection{Few-shot Learning (standby)}