\section{Related Work}
\subsection{Transfer Learning Theory}
Existing theoretical works can be categorized into two groups. The first group focuses on proposing measures to quantify the similarity between the target and source tasks. Within this group, some measures have been introduced, including $l_2$-distance  \cite{long2014transfer}, optimal transport cost  \cite{courty2016optimal}, 
% K-L divergence~\cite{ganin2016domain,tzeng2017adversarial}, 
LEEP~\cite{nguyen2020leep}, Wasserstein distance~\cite{shui2021aggregating_WADN}, and maximal correlations~\cite{lee2019learning_MCW}. 
This work belongs to the second group focusing on developing new generalization error measures. Within this group, the measures 
having been introduced include $f$-divergence~\cite{harremoes2011pairs},  mutual information 
~\cite{bu2020tightening}, $\X^2$-divergence~\cite{tong2021mathematical}, $\mathcal{H}$-score~\cite{bao2019information,wu2024h_Hensemble}. 
However, the potential of K-L divergence as a generalization error measure has not been sufficiently explored. 
% In addition, some theoretical studies yield target models with constrained spaces, such as a convex combination of source models~\cite{tong2021mathematical}. 


\subsection{Multi-source Transfer Learning}
%%%%%%%%长版本%%%%%%%%%%%%%%%%%%%%%%%
% Multi-source Transfer Learning uses knowledge transferred from multiple source domains to improve learning in a target domain. Multi-source transfer learning is primarily introduced due to insufficient labeled data in the target domain, and is therefore commonly used in scenarios such as unsupervised or few-shot learning. Existing multi-source transfer learning methods mainly focus on two aspects. The first aspect is the alignment strategy, which is used among the source and target domains to bridge the domain shift. In this field, representative methods include latent space transformation trying to learn domain-invariant features of different domains~\cite{li2021multi, wang2022self}, intermediate domain generation generating an intermediate adapted domain~\cite{zhao2021madan,he2021multi}, and task classifier refinement addressing the label shift~\cite{li2022dynamic}. The second aspect is the matching strategy, focusing on which source domains or samples should be chose or assigned higher weights for transfer. In this field, the domain pairing strategy identifies source domains that are more similar or relevant to the target domain to improve the transfer process~\cite{guo2020multi}, and the domain/sample weighting strategy
% %combine the features or predictions by multiple source domain models with different weights during inference, or 
% assigns importance weights to the models or samples of different source domains~\cite{shui2021aggregating}.
% However, most existing work directly use all samples from all sources or selects the source tasks in their entirety, while few studies explore the framework of transferring partial samples from each source task. Moreover, many transfer learning studies are mainly applicable to few-shot or data-scarce scenarios, and may suffer from negative transfer in non-few-shot settings. However, the boundary of few-shot scenarios is not always well-defined across different problems, which limits their~\emph{shot generality}.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Multi-source Transfer Learning uses knowledge transferred from multiple source domains to improve learning in a target domain. Multi-source transfer learning is primarily introduced due to insufficient labeled data in the target domain, and is therefore commonly used in scenarios such as unsupervised or few-shot learning. 


Classified by the object of transfer, existing multi-source transfer learning methods mainly focus on two types: model transfer vs sample transfer~\cite{zhuang2020comprehensive}. Model transfer assumes there is one or more pre-trained models on the source tasks and transfers their parameters to the target task via fine-tuning~\cite{wan2022uav}.  This work focuses on the latter, which is based on joint training of the source task samples with those of the target task~\cite{zhang2024revisiting_MADA,shui2021aggregating_WADN,li2021dynamic}. %Sample transfer can achieve better performance than model-based method as joint training takes full advantage of the information in the source data related to the target task. 
Classified by the strategy of transfer, existing methods mainly focus on two types: alignment strategy and matching-based strategy \cite{zhao2024more}. Alignment strategy aims to reduce the domain shift among source and target domains~\cite{li2021multi,zhao2021madan,li2021dynamic}. This work is more similar to the latter, focusing on determining which source domains or samples should be selected or assigned higher weights for transfer~\cite{guo2020multi,shui2021aggregating_WADN,tong2021mathematical,wu2024h_Hensemble}. 
However, most existing works either utilize all samples from all sources or perform task-level selection, whereas this work explores a framework that optimizes the transfer quantity of each source task.
% Moreover, many works are restricted to specific target tasks, such as classification, which limits their \emph{task generality}. 
% In addition, many studies are mainly applicable to few-shot scenarios, and may suffer from negative transfer in non-few-shot settings, which limits their~\emph{shot generality}, as illustrated in Table~\ref{tab:comparison}.

% In addition, many studies are mainly applicable to few-shot scenarios, and may suffer from negative transfer in non-few-shot settings, which limits their~\emph{shot generality}, as illustrated in Table~\ref{tab:comparison}.


% For dataset transfer, one category is the alignment method, which aligns feature or data among source and target domains to reducing domain shift~\cite{zhang2024revisiting_MADA,shui2021aggregating_WADN,li2021dynamic}. In another category, is matching-based method, focusing on which source domains or samples should be chosen or assigned higher weights for transfer~\cite{shui2021aggregating_WADN}. 

% Existing multi-source transfer learning methods mainly focus on two aspects. The first aspect is the alignment strategy, which is used among source and target domains to reduce the domain shift. In this field, representative methods include latent space transformation~\cite{li2021multi, wang2022self}, intermediate domain generation~\cite{zhao2021madan,he2021multi}, and task classifier refinement~\cite{li2022dynamic}. The second aspect is the matching-based method, focusing on selecting which source domains or samples should be chose or assigned higher weights for transfer. In this field, representative methods include the domain pairing strategy~\cite{guo2020multi}, and the domain/sample weighting strategy
% % %combine the features or predictions by multiple source domain models with different weights during inference, or 
% % ~\cite{shui2021aggregating_WADN}.


% Moreover, many transfer learning studies are mainly applicable to few-shot or data-scarce scenarios, and may suffer from negative transfer in non-few-shot settings. However, the boundary of few-shot scenarios is not always well-defined across different problems, which limits their~\emph{shot generality}.





% Existing multi-source transfer learning methods mainly focus on two types: model transfer vs sample transfer. Model transfer assumes there is one or more pre-trained models on the source tasks and transfer their parameters to the target task via fine-tuning~\cite{wan2022uav}.  This work focuses on the latter, which is based on joint training of the source task samples with those of the target task. It can achieve better performance than model-based method as joint training takes full advantage of the information in the source data related to the target task. For dataset transfer, one category is the alignment method, which aligns feature or data among source and target domains to reducing domain shift~\cite{zhang2024revisiting_MADA,li2021dynamic}. In another category, is matching-based method, focusing on which source domains or samples should be chosen or assigned higher weights for transfer~\cite{shui2021aggregating_WADN}.

% Existing multi-source transfer learning methods mainly focus on two aspects. The first aspect is the alignment strategy, which is used among source and target domains to reduce the domain shift. In this field, representative methods include latent space transformation~\cite{li2021multi, wang2022self}, intermediate domain generation~\cite{zhao2021madan,he2021multi}, and task classifier refinement~\cite{li2022dynamic}. The second aspect is the matching-based method, focusing on selecting which source domains or samples should be chose or assigned higher weights for transfer. In this field, representative methods include the domain pairing strategy~\cite{guo2020multi}, and the domain/sample weighting strategy
% %combine the features or predictions by multiple source domain models with different weights during inference, or 
% ~\cite{shui2021aggregating_WADN}.
% However, most existing work directly use all samples from all sources or selects the source tasks in their entirety, while few studies explore the framework of transferring partial samples from each source task. Moreover, many transfer learning studies are mainly applicable to few-shot or data-scarce scenarios, and may suffer from negative transfer in non-few-shot settings. However, the boundary of few-shot scenarios is not always well-defined across different problems, which limits their~\emph{shot generality}.







% \subsection{one-source Transfer Learning (standby)}
% \subsection{Few-shot Learning (standby)}