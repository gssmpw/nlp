The explainability of deep neural networks (DNNs) has received increasing attention in recent years. However, there has long been a pessimistic view regarding the possibility of faithfully explaining DNNs's inference logics**Zeiler et al., "Visualizing and Understanding Convolutional Networks"**. Fortunately, recent advancements in interaction-based explanations, as surveyed by **Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"**, have made the first attempt to tackle the mathematical feasibility of explaining a DNN's inference logics using a small number of inference patterns. Specifically, (1) **Zhang and Manning, "MuProp: Unifying Forward and Reverse Backpropagation"** discovered and **Zhang et al., "Explanation Methods in Deep Learning: A Review"** proved that there exists an AND-OR logical model, which contains only a small number of interactions, can faithfully explain the inference logics of DNNs, regardless of how the input samples are masked. (2) **Montavon et al., "Explaining Non-linear Classification Decisions with Deep Taylor Decomposition"** used the complexity of interactions to explain the generalization power of DNNs. (3) **Shrikumar et al., "Learning Important Feasures through Propagating Activation Differences back into a Network"** demonstrated that fourteen attribution methods can all be explained as a reallocation of interaction effects.

In this way, compared to previous studies, this paper provides further insights into the underlying factors contributing to the overfitting of DNNs and identifies the key factor that determines the composition of confusing samples in DNNs. The lottery ticket hypothesis**Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"** suggests that a DNN's representation is largely influenced by a small subset of randomly initialized parameters, known as winning tickets. Building on this, our experiments showed that the low-layer parameters of a DNN are the primary determinant of the composition of confusing samples. In contrast, other factors, such as high-layer parameters and network architecture, have significantly less impact.