\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{fig/fig_overview_fig.pdf}
  \caption{The diagram illustrates the MHA, MLA, and our MHA2MLA. It can be seen that the ``cached'' part is fully aligned with MLA after MHA2MLA. The input to the attention module is also completely aligned with MLA (the \colorbox{lightgray!60}{aligned region below}). Meanwhile, the parameters in MHA2MLA maximize the use of pre-trained parameters from MHA (the \colorbox{lightgray!60}{aligned region above}).}
  \vspace{-0.4cm}
  % https://1drv.ms/p/c/e248d5c415e14c2d/ETjEatb4CaVLpZfGpurOYKsB41mBrk6_He7OiQfC5mh_Vg?e=DJMN0j
  \label{fig:overview}
\end{figure*}

% MHA、MLA以及我们的MHA2MLA的示意图。可以看到，Cached部分在MHA2MLA后完全对齐MLA。注意力模块的输入也完全对齐了MLA（下方的对齐区域）。而参数部分最大化利用了MHA中预训练的参数（上方的对齐区域）。