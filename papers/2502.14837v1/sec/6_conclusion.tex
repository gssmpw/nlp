
% 1. Our main contributions are:

% - **First Data-Efficient MHA-to-MLA Transition Framework**: We propose MHA2MLA, the first parameter-efficient fine-tuning method enabling seamless adaptation of pre-trained MHA-based LLMs to MLA architecture without full pretraining, achieving performance recovery with merely 0.3%-0.6% of typical training data.

% - **Contribution-Aware Partial RoPE Removal**: A novel dimension selection strategy that strategically removes rotary position embeddings (RoPE) from low-contribution subspaces, achieving optimal balance between KV cache compression (92.19% reduction) and capability preservation (0.5% performance drop on LongBench).

% - **Pre-Training Aligned Low-Rank Projection**: A joint SVD-based approximation method that compresses NoPE key/value matrices into latent spaces while maximally preserving pre-trained knowledge, ensuring compatibility with downstream compression techniques like quantization.


\section{Conclusion}  
This work addresses the critical challenge of adapting pre-trained MHA-based LLMs (or variants) to the KV-cache-efficient MLA architecture. By introducing MHA2MLA with contribution-aware partial-RoPE removal and SVD-driven low-rank projection, we achieve near-lossless compression of KV cache (up to 96.87\% size reduction for Llama2-7B) while requiring only 3\textperthousand~to~6\textperthousand of training data. The framework demonstrates strong compatibility with existing compression techniques and maintains commonsense reasoning and long-context processing capabilities, offering a practical pathway for deploying resource-efficient LLMs without sacrificing performance. Our results underscore the feasibility of architectural migration for LLMs through targeted parameter reuse and data-efficient fine-tuning.

\section*{Limitations}  

\paragraph{Verification on More LLMs}  
Considering that MHA2MLA can significantly reduce inference costs, it is worthwhile to validate it on larger and more diverse open-source LLMs. However, constrained by our computation resources, models like Llama3 require fine-tuning on a 128K context length to mitigate performance degradation from continued training, so we did not perform such experiments. Furthermore, since Deepseek has not yet open-sourced the tensor-parallel inference framework for MLA, it is currently challenging to explore models larger than 7B. This will be addressed in our future work.

\paragraph{Parameter-Efficient MHA2MLA Fine-tuning}  
This paper primarily focuses on the data efficiency of MHA2MLA. Since the architectural transformation does not involve the Feed-Forward (FFN) module, future work could explore parameter-efficient MHA2MLA fine-tuning, for example by freezing the FFN module and/or freezing the parameters in the queries and keys that correspond to the retained RoPE. This could further reduce the cost of the MHA2MLA transition.