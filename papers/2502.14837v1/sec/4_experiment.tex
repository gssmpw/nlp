\section{Experiment}
\label{sec:exper}


\input{tab/tab_comm_sense}


% 我们在135M，360M，1B7，7B大小的LLM上进行了MHA2MLA微调实验，其中135M，360M，1B7来自Huggingface完全开源（包括预训练数据集smollm-corpus）的模型，保证了微调数据和预训练数据一致，7B是llama2-7B，由于它的预训练数据没有公开，我们使用了前面的smollm-corpus，因此存在数据分布差异的潜在影响。
% 实验的目标是1.如何最小化MHA2MLA在架构改动带来的性能下降，2. MHA2MLA能减少多少KV cache存储占用？ 3. MHA2MLA和KV cache量化是否能集成？

We evaluate our method on LLMs of varying scales (SmolLM-135M/360M/1B7, Llama2-7B) pre-trained with MHA or GQA.
We chose the SmolLM-series\footnote{\url{https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966}}
because its pretraining data and framework are both open-source, which can minimize the gap in fine-tuning data and processes. 
We chose Llama2-7B\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}} because it is one of the widely used open-source LLMs (but its pretraining data is not open-source, there is a potential gap in fine-tuning data).

We denote the architectural migration using MHA2MLA and GQA2MLA, respectively.\footnote{
The details of the fine-tuning process (including data and hyperparameters) are provided in \Cref{app:ft_details}.
}
Both adopt \textit{data-efficient full-parameter fine-tuning}, with the head-wise 2-norm selection ($\mathcal{S}_{\text{2-norm}}$, $r=\frac{d_h}{16}$) for Partial-RoPE and joint SVD factorization (\textbf{SVD\textsubscript{joint}}) for low-rank approximation as default configurations. 
% 对于
% For transparency:  
% - **135M, 360M, 1B7**: Fully open-source models from HuggingFace, fine-tuned on their original pretraining corpus (smolLM-corpus \cite{smolLM2023}) to ensure data consistency.  
% - **7B**: LLaMA-2-7B \cite{touvron2023llama}, fine-tuned on smolLM-corpus due to undisclosed pretraining data. We mitigate potential distribution shift via knowledge distillation from the original model.  
Our experiments address three critical questions:  
\begin{enumerate}[leftmargin=*,itemsep=0pt, topsep=0pt, parsep=0pt]
    \item How does MHA2MLA minimize accuracy degradation induced by architectural shifts? 
    \item What does MHA2MLA achieve in the KV cache reduction ratio?  
    \item Can MHA2MLA integrate with KV cache quantization for compound gains?  
\end{enumerate}  


\subsection{Commonsense Reasoning Tasks}  
\label{ssec:general_res}

\paragraph{Main Results}
% 表1汇报了本文方法在四种基座上的效果，特别是在不同KV cache压缩比例下的结果（即不同维度的d_{kv}超参）。
% 首先，对比直接继续微调和原始LLM的性能，我们发现4个基座上的性能变化很微弱（135M上-0.25%，360M上+0.03%， 1B7上+0.03%，7B上+0.37%）。这表明我们选择的微调数据不会显著伤害或增强原始基座的性能，为MHA2MLA提供了合适的实验环境。
% 其次，随着d_{kv}的减小（例如32 \to 16 \to 8），虽然KV cache节省比例提升，但是性能损失也更难通过微调快速恢复。
% 我们还观察到，参数量越大的模型，转变成MLA架构的损失就更小。
% 最后，从135M到7B模型，我们微调所需的Token数量仅仅是预训练的千分之三到千分之六，体现出方法的数据高效性。
% 总而言之，无论是GQA2MLA还是MHA2MLA，都能成功的以微小的代价实现架构转变，从而获得高效且经济的推理。

As shown in Table 1, our method achieves efficient architectural migration across four model scales (135M to 7B) under varying KV cache compression ratios (via latent dimension \( d_{kv} \)). 
First, when comparing the performance of our fine-tuning approach with the original LLM, we observe only minor changes in performance across the four base models: a -0.25\% decrease on the 135M, +0.03\% on the 360M, +0.03\% on the 1B7, and +0.37\% on the 7B. 
This suggests that the fine-tuning data does not significantly degrade or improve the performance of the original model, providing an appropriate experimental setting for the MHA2MLA framework.


% Next, as \(d_{kv}\) decreases (e.g., from 32 to 16 to 8), the KV cache reduction increases, but the performance loss becomes more challenging to recover through fine-tuning. We also find that larger models experience less performance degradation when transitioning to the MLA architecture.
Next, as \(d_{kv}\) decreases (e.g., from 32 to 16 to 8), the KV cache reduction increases (i.e., from -68.75\% to -81.25\% to -87.5\%), but the performance loss becomes more challenging to recover through fine-tuning. 
\Cref{fig:rank_loss} shows the fine-tuning loss curves of 135M (representing GQA) and 7B (representing MHA) under different compression ratios. As the compression ratio increases, the loss difference from the baseline becomes larger. Additionally, we observe that the fluctuation trends of the loss curves are \textit{almost identical}, indicating that our architecture migration does not significantly harm the model's internal knowledge.

\input{fig/fig_rank_loss}



% We also find that larger models experience less performance degradation when transitioning to the MLA architecture.
We also find that larger models experience less performance degradation when transitioning to the MLA architecture. For example, with compression down to 18.75\%, the performance drops by 2.41\% for 135M, 2.69\% for 360M, 1.28\% for 1B7, and 0.61\% for 7B, revealing the \textbf{potential scaling law of MHA2MLA}.
Finally, from the 135M model to the 7B model, the number of tokens required for fine-tuning is only about 0.3\% to 0.6\% of the pretraining tokens, demonstrating the data efficiency of our method.

Overall, whether using GQA2MLA or MHA2MLA, the architecture transition is achieved with minimal cost, resulting in efficient and economical inference.

% Fine-tuned models exhibit minimal performance deviation from their original counterparts, with accuracy changes ranging from -0.25\% (135M) to +0.37\% (7B), confirming that our data-efficient protocol preserves base capabilities while enabling structural shifts. Although reducing \( d_{kv} \) (e.g., \( 32 \to 8 \)) amplifies performance recovery challenges—smaller models (135M) suffer >3\% degradation at \( d_{kv}=8 \), whereas larger models (7B) sustain <1\% loss—the method demonstrates remarkable scalability, requiring only **0.3\%–0.6\%** of pretraining tokens (e.g., 3B tokens for 7B models) to achieve up to **97\% KV cache reduction**. Crucially, larger models (1.7B/7B) show 1.8–2.4× higher inference throughput than smaller counterparts under equivalent compression, highlighting the synergy between model capacity and compression robustness. These results validate that MHA2MLA/GQA2MLA enables cost-effective transitions to MLA, balancing memory efficiency, accuracy, and hardware practicality.

% \paragraph{Main Results}  
% Table 1 presents the results of our method on four base models, with a particular focus on the performance under different KV cache compression ratios (i.e., varying the \(d_{kv}\) hyperparameter). 

\subsection{Long Context Tasks}
\input{tab/tab_LongBench}
% \input{tab/tab_longbench_v2}
\label{ssec:longbench_res}

\paragraph{Settings}
% 为了测试模型的生成能力，我们选择LongBench作为模型生成能力的基准测试。我们使用贪心解码的策略来测试所有模型。我们根据模型微调时的序列长度来设置上下文窗口大小。我们使用HQQ和Quanto来设置不同精度的cache测试原始模型的能力作为baseline。由于我们的方法和kv cache量化是可以叠加的，我们还额外测试了两者叠加的效果。
To evaluate the generative capabilities of the model, we adopt LongBench~\cite{acl/BaiLZL0HDLZHDTL24} as the benchmark for generation performance. All models are tested using a greedy decoding strategy. The context window size is determined based on the sequence length used during model fine-tuning. We use HQQ \cite{badri2023hqq} and Quanto\footnote{\url{https://huggingface.co/blog/quanto-introduction}} to set caches with different levels of precision to evaluate the performance of the original model as the baseline. Since our method is compatible with KV cache quantization, we also conduct additional experiments to assess the combined effect of both approaches.

\paragraph{Main Results}
% 表2展示了Llama2-7B以及MHA2MLA在longbench上的结果。
% 首先与Int4量化方法相比，MHA2MLA在压缩比例相当时性能可比（差距为-0.2%或-0.4%），压缩比例更大时性能损失更多。但是Int2量化会在取得87.5%的压缩比例时，显著减少模型的性能（HQQ方法为-6.2%， Quanto方法为-9%）。
% 而MHA2MLA在相同或更多的压缩比例下，性能全部高于Int2量化。
% 特别是 d_{kv} = 64 结合 Int4_{HQQ}推理，压缩率为92.19\%时，性能仅减少0.5\%.
% d_{kv} = 16 结合 Int4_{HQQ}推理，压缩率能够高达为96.87\%，性能减少3.3\%，仍然优于Int2量化。
% 它表明MHA2MLA能够很好地与KV cache量化方法相结合，实现更经济地推理。
As evidenced in \Cref{tab:long_bench}, MHA2MLA achieves competitive or superior efficiency-accuracy profiles compared to post-training quantization methods on LongBench. While 4-bit quantization incurs modest degradation (-0.2\% to -0.4\%) at comparable compression ratios, aggressive 2-bit quantization suffers severe performance collapse (-6.2\% to -9\%) despite 87.5\% KV cache reduction. In contrast, MHA2MLA alone attains 87.5\% compression (at \( d_{kv}\!=\!16 \)) with only 3\% accuracy loss, and further synergizes with 4-bit quantization to reach 92.19\%/96.87\% compression (\( d_{kv}\!=\!64/16 \)+Int4$_\text{HQQ}$) while limiting degradation to -0.5\%/-3.2\%, outperforming all 2-bit baselines. This highlights that MHA2MLA's latent space design remains orthogonal to numerical precision reduction, enabling \textbf{compound efficiency gains} without destructive interference.  



\subsection{Ablation Study}
\label{ssec:ablation_study}

\input{tab/tab_partial_rope}
\paragraph{Four Partial-RoPE strategies: $\mathcal{S}_{\text{high}}$, $\mathcal{S}_{\text{low}}$, $\mathcal{S}_{\text{uniform}}$, $\mathcal{S}_{\text{2-norm}}$}
% 表二展示了四种Full-RoPE to Partial-RoPE的策略，我们选择SmolLM-135M作为GQA架构的代表，SmolLM-1B7作为MHA架构的代表。
% 首先对比四种策略与full-RoPE，我们发现保留低频的S_low损失了最多的性能（-0.XX%），而保留高频的S_high的损失明显较少（-0.YY）。说明了高频子空间的重要性。
% S_uniform和S_{2-norm}的表现要更好，前者均匀的保留了频率的子空间，后者按照注意力分数的贡献值保留高贡献的频率。
% 它们两者的差异并不显著，本文选择S_{2-norm}作为默认的配置。
% 其次，当结合partial-rope和SVD低秩近似时，我们观察到上述发现仍然一致。
\Cref{tab:partial_rope} presents the results of four strategies for converting full-RoPE to partial-RoPE. %In our experiments, we selected SmolLM-135M as the representative for the GQA architecture and SmolLM-1B7 for the MHA architecture. 
First, when comparing the four strategies with full-RoPE, we observed that the low-frequency retention strategy, \(\mathcal{S}_{\text{low}}\), incurred the greatest performance loss (a reduction of -6.49\%@135M and -1.21\%@1B7), whereas the high-frequency retention strategy, \(\mathcal{S}_{\text{high}}\), experienced significantly less degradation (a reduction of -0.85\%@135M and -0.76\%@1B7), underscoring the importance of high-frequency subspaces. Both \(\mathcal{S}_{\text{uniform}}\) and \(\mathcal{S}_{2\text{-norm}}\) yielded better performance, the \(\mathcal{S}_{\text{uniform}}\) preserves subspaces across the frequency spectrum, while the \(\mathcal{S}_{2\text{-norm}}\) retains subspaces based on their contribution to the attention scores. 
We choose \( \mathcal{S}_{2\text{-norm}} \) as the default configuration because the removed subspaces (i.e., NoPE) are more suitable for the (SVD-based) low-rank approximation.
% Since the performance difference between these two strategies is marginal, we adopt \(\mathcal{S}_{2\text{-norm}}\) as the default configuration. Furthermore, when combining partial-RoPE with the SVD-based low-rank approximation, we observed that the above findings remain consistent.

% \input{tab/tab_low_rank}
\paragraph{Two SVD-based low-rank approximations: SVD\textsubscript{split}, SVD\textsubscript{joint}}
% 表三以SmolLM-135M作为GQA架构的代表，SmolLM-1B7作为MHA架构的代表，对比了两种SVD方法的效果。
% 我们发现两种LLM上均是SVD_{joint}更优，平均性能高于SVD_{split}XX%在135M上，YY%在1B7上。
% 此外，我们发现MHA2MLA/GQA2MLA的整体性能损失中，partial-rope的损失占比Z1%，低秩近似带来的损失占比Z2%（并且会随着压缩比例提升）。
% 综上，SVD_{joint}可以作为毫无疑问的默认选择。
The last two rows of each group in \Cref{tab:partial_rope} compare the effects of the two SVD methods. 
%, using SmolLM-135M (representing GQA) and SmolLM-1B7 (representing MHA)
We observe that, on both LLMs, the SVD\(_{\text{joint}}\) method consistently outperforms SVD\(_{\text{split}}\), yielding an average performance improvement of 0.92\% on the 135M model and 0.74\% on the 1B7 model. 
%Furthermore, our analysis reveals that among the overall performance degradation observed in the MHA2MLA/GQA2MLA transition, the loss attributable to Partial-RoPE accounts for Z1\% while the degradation due to low-rank approximation constitutes Z2\% (with the latter increasing as the compression ratio intensifies). 
%In summary, SVD\(_{\text{joint}}\) emerges as the clear default choice.
It indicates that SVD\(_{\text{joint}}\) emerges as the clear default choice.

