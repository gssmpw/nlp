\newpage
\section{The Calculation of 2-norm Score}
\label{app:2_norm}

% 为了计算每个head的2-norm分数，我们从训练集中选了1024条数据。各个子集的比例和微调的设置是一样的。首先计算每个head的query向量和key向量，然后针对向量的每个旋转子空间计算2-norm分数，最后根据子空间把query和key的2-norm分数加在一起。如果模型使用的注意力是GQA，那么会在一个GQA组里面平均2-norm分数并且组内共享。

To compute the 2-norm scores for each attention head, we selected 1,024 samples from the training dataset. The proportions of the subsets and sequence length used during the 2-norm computation are consistent with those used during fine-tuning. First, we calculate the query vectors and key vectors for each head. Then, for each rotational subspace of the vectors, we compute the 2-norm scores. Finally, the 2-norm scores of the query and key vectors are aggregated within each subspace. If the model employs Grouped-Query Attention (GQA), the 2-norm scores are averaged within each GQA group, and the scores are shared between the groups.

\section{Inference Process of MHA2MLA}
\label{app:mha2mla_infer}
% 在MHA2MLA模型推理时，我们的输入有第h个head中第i个token的隐层表示x_i，以及保存在KV cache中的前i-1个token的\bm{k}_{<i, rope}^{(h)}和\bm{c}_{<i, kv}^{(h)}.
% 我们的目标是计算两个部分的点积：
During inference in the MHA2MLA model, our input includes the hidden representation \( x_i \) of the \( i \)-th token, as well as the previously stored \(\bm{k}_{<i, \text{rope}}^{(h)}\) and \(\bm{c}_{<i, \text{kv}}\) in the KV cache for the first \( i-1 \) tokens.  

During the inference, our goal is to compute the \( h \)-th head's dot product of these two parts $\bm{q}_{i,\text{rope}}^{(h)} \bm{k}_{\le i,\text{rope}}^{(h)\top}$ and $\bm{q}_{i,\text{nope}}^{(h)} \bm{k}_{\le i,\text{nope}}^{(h)\top}$.
For the RoPE part, we can easily extract \( \bm{W}_{q, \text{rope}}^{(h)} \) and \( \bm{W}_{k, \text{rope}}^{(h)} \) from the pre-trained parameter matrices \( \bm{W}_q^{(h)} \) and \( \bm{W}_k^{(h)} \) (i.e., the rows corresponding to the subspace that retains RoPE) and then obtain the result through a linear transformation:
% 其中
\begin{align*}  
\bm{q}_{i,\text{rope}}^{(h)} &= \bm{x}_i\bm{W}_{q,
\text{rope}}^{(h)}\\
\bm{k}_{i,\text{rope}}^{(h)} &= \bm{x}_i\bm{W}_{k,
\text{rope}}^{(h)}\\
\bm{k}_{\le i,\text{rope}}^{(h)} &= [\bm{k}_{<i, \text{rope}}^{(h)}, ~\bm{k}_{i,\text{rope}}^{(h)}]
\\ \to ~&\bm{q}_{i,\text{rope}}^{(h)} \bm{k}_{\le i,\text{rope}}^{(h)\top}.
\end{align*}
Note that \(\bm{k}_{<i, \text{rope}}^{(h)}\) is already stored in the KV cache and can be directly retrieved.

For the NoPE part, \(\bm{q}_{i,\text{nope}}^{(h)}\) can still be easily obtained through a linear transformation $\bm{W}_{q,\text{nope}}^{(h)}$ which extracted from the pre-trained parameter matrix \( \bm{W}_q^{(h)} \) by separating the rows corresponding to the subspace with RoPE removed.  
However, \(\bm{k}_{i,\text{nope}}^{(h)}\) requires two linear transformations:  a \textit{dimensionality reduction} transformation using \(\bm{W}_{dkv}\), and a \textit{dimensionality expansion} transformation using \(\bm{W}_{uk}^{(h)}\).  
Note that \(\bm{W}_{dkv}\) is shared across all heads in the current layer, and both \(\bm{W}_{dkv}\) and \(\bm{W}_{uk}^{(h)}\) are constrained by the SVD decomposition of the pre-trained parameter matrices \(\bm{W}_{k,\text{nope}}^{(h)}\) and \(\bm{W}_{v}^{(h)}\), preserving most of the pre-trained knowledge:
\begin{align*}  
\bm{q}_{i,\text{nope}}^{(h)} &= \bm{x}_i\bm{W}_{q,
\text{nope}}^{(h)}\\
\bm{c}_{i, kv} &= \bm{x}_i\bm{W}_{dkv,
}\\
\bm{k}_{i,\text{nope}}^{(h)} &= \bm{c}_{i, kv}\bm{W}_{uk}^{(h)}\\
\bm{k}_{<i, \text{nope}}^{(h)} &= \bm{c}_{<i, kv}\bm{W}_{uk}^{(h)}.
\end{align*}
During inference, the NoPE part can also leverage the standard MLA matrix merging algorithm to reduce memory consumption:
\begin{align*}
\bm{k}_{\le i, \text{nope}}^{(h)} &= [\bm{c}_{<i, kv},~ \bm{c}_{i, kv}]\bm{W}_{uk}^{(h)}\\
 \bm{q}_{i,\text{nope}}^{(h)} \bm{k}_{\le i,\text{nope}}^{(h)\top} & = (\bm{x}_i\bm{W}_{q,
\text{nope}}^{(h)})  (\bm{c}_{\le i, kv}\bm{W}_{uk}^{(h)})^\top \\
 & = \bm{x}_i (\bm{W}_{q,
\text{nope}}^{(h)} \bm{W}_{uk}^{(h)\top}) \bm{c}_{\le i, kv}^\top.
\end{align*}
We can pre-multiply the parameter matrices $(\bm{W}_{q,
\text{nope}}^{(h)} \bm{W}_{uk}^{(h)\top})$, and let $\bm{c}_{ i, q}^{(h)} = \bm{x}_i (\bm{W}_{q, \text{nope}}^{(h)} \bm{W}_{uk}^{(h)\top})$.
In the end, the output of MHA2MLA is as follows:
\begin{align*}    
& \bm{v}_i^{(h)} = \\
&\bm{o}_i^{(h)}\!=\!\text{Softmax}\!\left(\bm{q}_{i,\text{rope}}^{(h)}\bm{k}_{\le i,\text{rope}}^{(h)\top}\!+\!\bm{c}_{i, q}^{(h)}\bm{c}_{\le i, kv}^\top\!\right) \bm{c}_{\le i, kv} \nonumber 
    \\ &\text{MHA2MLA}(\bm{x}_i) = \left[\dots, \bm{o}_i^{(h)}\bm{W}_{uv}^{(h)},  \dots\right]  \bm{W}_o.
\end{align*}
Where $\bm{W}_{uv}^{(h)}$ and $\bm{W}_o$ can also perform matrix merging to make inference more economical.

\paragraph{Why doesn't MHA2MLA perform low-rank representation on the query as DeepSeek does?}
Firstly, we found that the economical inference of MLA is not affected even if $\bm{W}_{q,\text{nope}}^{(h)}$ is not decomposed into a dimension-reducing matrix (e.g., $\bm{W}_{dq}$) and a dimension-increasing matrix (e.g., $\bm{W}_{uq}^{(h)}$). 
Secondly, decomposing $\bm{W}_{q,\text{nope}}^{(h)}$ introduces additional architectural migration loss (approximation loss) and further reduces the number of LLM parameters. 
Therefore, we believe there is no need to decompose $\bm{W}_{q,\text{nope}}^{(h)}$ within the MHA2MLA framework.


\section{The Details of Fine-tuning}
\label{app:ft_details}

\paragraph{Data}

% 我们使用Smollm1预训练的语料来微调我们的模型。数据集由fineweb-edu-dedup、cosmopedia-v2、python-edu、open-web-math和stackoverflow构成。前三个语料都来自HuggingFaceTB整理的smollm-corpus。fineweb-edu-dedup是HuggingFaceTB从教育相关网页过滤的高质量数据。HuggingFaceTB也使用类似的方法对来自The Stack的python片段进行了过滤获得python-edu数据集。cosmopedia-v2是HuggingFaceTB根据BISAC书籍分类定义的3.4万个主题使用模型生成的高质量数据集。open-web-math和stackoverflow分别来源于网络上的高质量数学文本和Stackoverflow的帖子。

We fine-tune our model using the pretraining corpus from SmolLM\footnote{\url{https://huggingface.co/blog/smollm}}. 
The dataset consists of fineweb-edu-dedup, cosmopedia-v2, python-edu, open-web-math, and StackOverflow. The first three datasets are part of the smollm-corpus\footnote{\url{https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus}} curated by HuggingFaceTB. Fineweb-edu-dedup is a high-quality dataset filtered by HuggingFaceTB from education-related webpages. Similarly, HuggingFaceTB filtered Python code snippets from The Stack to construct the python-edu dataset. Cosmopedia-v2 is a high-quality dataset generated by a model based on 34,000 topics defined by BISAC book classifications. Additionally, open-web-math\footnote{\url{https://huggingface.co/datasets/open-web-math/open-web-math}} and StackOverflow\footnote{\url{https://huggingface.co/datasets/bigcode/stackoverflow-clean}} are sourced from high-quality mathematical texts available online and posts from StackOverflow, respectively.

\paragraph{Hyperparameters}
\input{tab/tab_hp}

% 当训练 1B7$_{\text{SmolLM}}$  7B$_{\text{Llama2}}$ 的时候，global batch size设置为256，学习率设置为1e-4，训练步数设置为12000，warmup步数设置为1000。学习率在10000步后开始下降。lr_warmup_style和lr_decay_style保持不变。

The fine-tuning hyperparameters for models of all sizes are listed in \Cref{tab:Hyperparameters}. The training process employs a warmup phase followed by a decay strategy. A 1-sqrt decay strategy is applied to ensure a smooth and gradual reduction.

\input{tab/tab_pe_dim}

\section{Ablation Study on Partial-RoPE Dimensions}
\label{app:pe-dim}



% 为了更好的选择partial-rope的策略和维度，我们在135M$_{\text{SmolLM}}$上进行了关于RoPE维度数目的消融实验，实验结果如表3所示。分别对比四种策略的性能与维度数目的关系，我们发现低频的S_low在维度数目较低时（<=4）时，出现了比较严重的性能损失（-14.7%）。随着维度数目的变化，S_uniform和S_{2-norm}始终保持着较优的性能。维度数目从4增加到8带来的收益并不显著，因此我们选择partial-RoPE的维度数目为4。
To better determine the strategy and dimensionality for partial-RoPE, we conducted an ablation study on the number of RoPE dimensions using the 135M$_{\text{SmolLM}}$ model. The experimental results are presented in \Cref{tab:pe_dim}. By comparing the performance of four different strategies in varying dimensionalities, we observed that the low-frequency strategy, $\mathcal{S}_{\text{low}}$, suffered significant performance degradation (-14.7\%) when the dimensionality was relatively low ($\leq 4$). In contrast, both $\mathcal{S}_{\text{uniform}}$ and $\mathcal{S}_{\text{2-norm}}$ consistently demonstrated superior performance regardless of dimensionality. Furthermore, increasing the dimensionality from 4 to 8 provided negligible performance gains. Based on these findings, we selected a dimensionality of 4 for partial-RoPE.


\section{Detailed Results}
\label{app:other_lb}
\input{tab/tab_other_LongBench}
\input{tab/tab_partial_rope_full}
\input{fig/fig_res_rank_loss}
\input{fig/fig_pe_loss}
\input{fig/fig_svd_loss}

%  Both \(\mathcal{S}_{\text{uniform}}\) and \(\mathcal{S}_{2\text{-norm}}\) yielded better performance 

In this section, we present the detailed results. 
\paragraph{Detailed LongBench evaluation} is reported in \Cref{tab:other_long_bench}.

\paragraph{Detailed ablation experiment} is reported in \Cref{tab:partial_rope_full}.

\paragraph{Additional visualizations of fine-tuning loss} 
We present the loss of the other two models fine-tuned, excluding the ones mentioned in the main text, in \Cref{fig:res_rank_loss}. 
We observe that as fine-tuning progresses, the gap in loss between our approach and the baseline gradually decreases, and both exhibit similar fluctuations, demonstrating the effectiveness of our approach. 
In \Cref{fig:pe_loss}, we show the loss under different partial-RoPE strategies. Except for $\mathcal{S}_{\text{low}}$, the other three partial-RoPE schemes show little difference from the baseline. Additionally, $\mathcal{S}_{\text{low}}$ has a higher probability of convergence failure. In \Cref{fig:svd_loss}, we show the loss under different SVD strategies. The loss curves on both 1B7$_{\text{SmolLM}}$ and 135M$_{\text{SmolLM}}$ reveal that SVD\textsubscript{joint} outperforms SVD\textsubscript{split}.


% The evaluation results of ablation experiment and results of all smollm models on the LongBench benchmark, which are documented in the \Cref{tab:partial_rope_full} and \Cref{tab:other_long_bench}. We can draw similar conclusions to those in the main text from the results in \Cref{tab:partial_rope_full}: both \(\mathcal{S}_{\text{uniform}}\) and \(\mathcal{S}_{2\text{-norm}}\) yielded better performance and $\mathcal{S}_{\text{2-norm}}$ + SVD\textsubscript{joint} can achieve better results compared to $\mathcal{S}_{\text{2-norm}}$ + SVD\textsubscript{split}.  

