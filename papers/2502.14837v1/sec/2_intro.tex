\section{Introduction}
\label{sec:intro}

\input{fig/fig_overview}

% P1:
% 近年来，大型语言模型（LLMs）快速推动了人工智能通用智能（AGI）的发展，其智能水平随着参数数量的增加而提高。
% 然而，这种提升是以更大的训练计算资源和潜在的推理吞吐量下降为代价的，带来了海量的电力消耗和碳排放。
% 这些限制对LLM的广泛采用和利用构成了重大挑战。
% 为了实现经济高效的部署和推理，一系列工作不断探索创新的Transformer架构。
% Deepseek提出的多头潜在注意力（MLA）架构取得了引人注目的成功。

The rapid advancement of large language models (LLMs) has significantly accelerated progress toward artificial general intelligence (AGI), with model capabilities scaling predictably with parameter counts \cite{corr/abs-2001-08361}. 
However, these gains come at a steep cost: escalating computational demands for training and degraded inference throughput, resulting in substantial energy consumption and carbon emissions \cite{strubell-etal-2019-energy}. 
% A series of studies have been exploring innovative Transformer architectures to enable cost-effective deployment and inference. 

% P2:
% 随着下游任务越来越复杂，长上下文和推理时计算越来越重要。
% 在注意力机制方面，多头注意力（MHA）的键值（KV）缓存对LLM的推理效率构成了重大障碍。
% 为了解决这一问题，人们探索了多种方法，包括GQA和MQA。
% 然而，这些方法在试图减少KV缓存时同时也减少了注意力网络的参数量，导致性能损失。
% Deepseek提出的MLA是一种配备了低秩键值联合压缩的新颖注意力机制。
% 实证结果表明，与MHA相比，MLA在性能上取得了显著提升，同时在推理过程中显著减少了KV缓存，从而提高了推理效率。

As downstream tasks grow increasingly complex, long-context processing and computationally intensive inference have become central to LLM applications
\cite{acl/AnG0ZLZKQ24}. 
A key bottleneck lies in the memory footprint of the Key-Value (KV) cache inherent to the Multi-Head Attention (MHA, \citeyear{nips/VaswaniSPUJGKP17}) mechanism, which scales linearly with sequence length and model size. 
To mitigate this, variants like Grouped-Query Attention (GQA, \citeyear{emnlp/AinslieLJZLS23})  and Multi-Query Attention (MQA, \citeyear{corr/abs-1911-02150})  have been explored. 
% reduce KV cache size by sharing keys or values across grouped or all attention heads. 
However, these methods reduce not only the KV cache size but also the number of parameters in the attention, leading to performance degradation.
The DeepSeek introduces Multi-Head Latent Attention (MLA, \citeyear{corr/abs-2405-04434}), an attention mechanism equipped with low-rank key-value joint compression. 
Empirically, MLA achieves superior performance compared with MHA, and meanwhile significantly reduces the KV cache during inference, thus boosting the inference efficiency. 

% P3:
% 一个很自然的问题是：当前已经well-trained的LLMs（例如LLaMA-3）虽然以MHA训练，能否enabling MLA推理？
% 由于MHA和MLA在网络结构上存在显著差异，零样本迁移不现实，从头预训练的巨大成本又难以接受，使得MHA to MLA极具挑战并且没有被人研究过。
% 本文首次提出一个精心设计的MHA2MLA框架，最大化利用预训练好的MHA的架构和参数，同时kv cache的存储以及推理过程对齐到MLA（如图1所示）。
% MHA2MLA包含partial rope和低秩近似两个关键环节。我们的目标是让MHA2MLA使用尽可能少的数据（即数据高效）复原因为架构改动带来的性能损失。

A critical yet unexplored question arises: \textbf{Can LLMs originally well-trained for MHA be adapted to enabling MLA for inference?} 
% The structural divergence between MHA and MLA renders direct zero-shot migration impractical. Meanwhile, the prohibitive cost of full pretraining from scratch makes this transition economically infeasible. 
The inherent architectural disparities between MHA and MLA render zero-shot transfer impractical, while the prohibitive cost of pretraining from scratch makes this transition both technically challenging and underexplored in existing research.
% This tension underscores the unique challenge of MHA-to-MLA adaptation: achieving parameter-efficient transformation without sacrificing pre-trained knowledge, a problem that remains unaddressed in prior work.
To address this gap, we propose the first carefully designed MHA2MLA framework that maximizes parameter reuse from pre-trained MHA networks while aligning the KV cache storage and inference process with MLA's paradigm (\Cref{fig:overview}). Our framework features two pivotal technical innovations: partial rotary position embedding (partial RoPE) and low-rank approximation.
The primary objective of MHA2MLA is to achieve data-efficient performance recovery - restoring architecture-induced capability degradation using minimal fine-tuning data.

% P4
% 由于MLA的推理加速和RoPE不兼容，因此deepseek选择了一个折中方案，只在少量维度保留了位置编码，对其余维度进行压缩。
% 因此，为了对齐MLA，我们需要在MHA中移除一定比例的RoPE维度，即把它们变成NoPE。
% 移除的比例越高，压缩率越高，但是性能损失也越难以恢复。
% 为了取得最佳的平衡，我们全面探索各种移除RoPE的方法，最终发现根据每个维度对注意力得分的贡献值进行排序，仅保留top-k的维度的方法最佳。
% 尽管有些工作研究过从头预训练partial rope LLM，但是研究通过数据高效微调实现full RoPE to partial rope是我们工作的首创。

The inherent incompatibility between MLA's inference acceleration mechanism and RoPE necessitates architectural compromises. 
DeepSeek's solution preserves PEs in limited dimensions while compressing others, requiring strategic removal of RoPE dimensions (converting them to NoPE) in MHA to achieve MLA alignment. 
While higher removal ratios enhance compression efficiency, they exacerbate performance degradation, creating an efficiency-capability trade-off. 
Through systematically exploring RoPE removal strategies, we identify that contribution-aware dimension selection (retaining top-k dimensions ranked by attention score impact) optimally balances these competing objectives. 
Although previous studies have investigated training partial-RoPE LLMs from scratch~\cite{gpt-neo,corr/abs-2410-06205}, our work pioneers data-efficient fine-tuning for full-to-partial RoPE conversion in LLMs.

% P5
% MLA使用了一个低秩的表示空间（存储在cache中）大幅压缩了key和value的显存占用。
% 对于已经去掉RoPE的维度，MHA2MLA也可以对它们进行低秩近似。
% 我们探索了两种近似方式，一是对参数矩阵进行SVD低秩分解，使用n→r→n的线性变换近似原始矩阵。它虽然实现简单，但是减少了一些模型参数，使得性能损失较多。
% 二是增加一个降维的自编码器层，实证研究发现该方法效果更好。

% MLA使用了一个低秩的表示空间（存储在cache中）大幅压缩了key和value的显存占用。
% MHA2MLA也可以对values和去掉RoPE的keys进行低秩近似。
% 我们基于已经训练好的参数矩阵W_v和W_k中对应NoPE的部分进行SVD近似。在获得低秩表示的同时，尽可能保留原始模型学到的知识。
%探索了两种近似方式，一是对参数矩阵进行SVD低秩分解，使用n→r→n的线性变换近似原始矩阵。它虽然实现简单，但是减少了一些模型参数，使得性能损失较多。
% 二是增加一个降维的自编码器层，实证研究发现该方法效果更好。

MLA reduces memory footprint by projecting keys and values into a low-rank latent representation space (stored in the KV cache). 
MHA2MLA can also apply low-rank approximation to the values and keys stripped of RoPE (NoPE dimensions). By performing  Singular Value Decomposition (SVD) on the pre-trained parameter matrices \( \bm{W}_v \) and \( \bm{W}_k \) corresponding to the NoPE subspaces, we compress these components into a latent space while maximizing the retention of knowledge learned by the original model.  



% MLA reduces memory footprint by projecting keys and values into a low-rank latent representation space (stored in the KV cache). 
% To further optimize dimensions where RoPE have been removed, MHA2MLA introduces two distinct low-rank approximation strategies:  
% 1. \textbf{SVD-Based Decomposition}: This approach applies singular value decomposition (SVD) to the original parameter matrices, approximating them through linear transformations of the form \( W \approx U_r \Sigma_r V_r^T \), where \( r \ll n \). While straightforward to implement, this method reduces the effective model parameters, leading to non-negligible performance degradation.  
% 2. \textbf{Autoencoder-Enhanced Projection}: We propose inserting a lightweight autoencoder layer to learn compressed representations without discarding parameters. 
% Empirical studies demonstrate that this approach better preserves model capacity, achieving superior performance compared to SVD-based approximation.  


% P6
% 两个关键环节相结合下，我们验证了MHA或者GQA都能够只使用X%的预训练数据（即数据高效）进行二次微调转化成MLA，享受高效且经济的推理。


Our main contributions are:
\begin{itemize}[leftmargin=*,itemsep=0pt, topsep=0pt, parsep=0pt]
    \item we introduce MHA2MLA, the first parameter-efficient fine-tuning framework that adapts pre-trained MHA-based LLMs to the MLA architecture using only 3\textperthousand~to~6\textperthousand~of training data without training from scratch.
    \item we demonstrate that the MHA2MLA architecture can be integrated with KV-cache quantization to achieve more economical inference (up to a 96.87\% reduction).
    \item we conduct experiments across four model sizes (from 135M to 7B, covering both MHA and GQA), and detailed ablation studies to provide guidance and insights for MHA2MLA.
\end{itemize}
% - **First Data-Efficient MHA-to-MLA Transition Framework**: 

% - **Contribution-Aware Partial RoPE Removal**: A novel dimension selection strategy that strategically removes rotary position embeddings (RoPE) from low-contribution subspaces, achieving optimal balance between KV cache compression (92.19% reduction) and capability preservation (0.5% performance drop on LongBench).

% - **Pre-Training Aligned Low-Rank Projection**: A joint SVD-based approximation method that compresses NoPE key/value matrices into latent spaces while maximally preserving pre-trained knowledge, ensuring compatibility with downstream compression techniques like quantization.