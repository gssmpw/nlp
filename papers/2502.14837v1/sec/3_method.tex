\section{Preliminary}
\label{sec:preliminary}

\subsection{Multi-Head Attention (MHA)}  
Given an input sequence \(\{\bm{x}_1,\dots, \bm{x}_l\} \in \mathbb{R}^{l \times d}\), standard MHA \cite{nips/VaswaniSPUJGKP17} projects each token \(\bm{x}_i\) into queries \(\bm{q}_i^{(h)} = \bm{x}_i \bm{W}_q^{(h)}\), keys \(\bm{k}_i^{(h)} = \bm{x}_i \bm{W}_k^{(h)}\), and values \(\bm{v}_i^{(h)} = \bm{x}_i \bm{W}_v^{(h)}\), where \( \bm{W}_q^{(h)}, \bm{W}_k^{(h)}, \bm{W}_v^{(h)} \in \mathbb{R}^{d \times d_h} \) for each head \( h \in \{1, \dots, n_h\} \). The Rotary positional encoding (RoPE, \citeyear{journals/ijon/SuALPBL24}) is applied to queries and keys (e.g., $\bm{q}_{i,\text{rope}}^{(h)}=\text{RoPE}(\bm{q}_{i}^{(h)})$), followed by scaled dot-product attention\footnote{
We ignore here the $\frac{1}{\sqrt{d}}$ scaling factor for ease of notation.
}:  
\begin{align}    
    &\bm{o}_i^{(h)} = \text{Softmax}\left(\bm{q}_{i,\text{rope}}^{(h)} \bm{k}_{\le i,\text{rope}}^{(h)\top}\right) \bm{v}_{\le i}^{(h)}, \nonumber \\
    &\text{MHA}(\bm{x}_i) = \left[\bm{o}_i^{(1)}, \dots, \bm{o}_i^{(n_h)}\right] \bm{W}_o, 
\end{align}
where $\bm{W}_o \in \mathbb{R}^{(n_h d_h) \times d}$ and $[\cdot,\cdot]$ means vector concatenate.
% where $\bm{W}_o \in \mathbb{R}^{(n\times d_h) \times d}$.
During autoregressive inference, MHA stores the KV cache \(\{\bm{k}_{\text{rope}}^{(h)}, \bm{v}^{(h)}\}_{h=1}^{n_h}\) of size \( \mathcal{O}(2 l n_h d_h) \), growing linearly with sequence length \( l \), posing memory bottlenecks.  

\paragraph{Variants:}  
Grouped-Query Attention (GQA, \citeyear{emnlp/AinslieLJZLS23}) shares keys/values across \( n_g \) groups (\( n_g \ll n_h \)) to reduce the KV cache. For each head \( h \), it maps to group \( g=\lfloor h/n_g \rfloor \):  
\begin{align}
    &\bm{o}_i^{(h)} = \text{Softmax}\left(\bm{q}_{i,\text{rope}}^{(h)} \bm{k}_{\le i,\text{rope}}^{(g)\top}\right) \bm{v}_{\le i}^{(g)}, \nonumber \\
    &\text{GQA}(\bm{x}_i) = \left[\bm{o}_i^{(1)}, \dots, \bm{o}_i^{(n_h)}\right] \bm{W}_o.
\end{align}
Multi-Query Attention (MQA, \citeyear{strubell-etal-2019-energy}) is a special case of GQA with \( n_g = 1 \), i.e., all heads share a single global key/value. 
While reducing the KV cache to \( \mathcal{O}(2 l n_g d_h) \), these methods degrade performance due to parameter pruning.


\subsection{Multi-Head Latent Attention (MLA)}  
% DeepSeek's MLA \cite{corr/abs-2405-04434} introduces a paradigm shift by compressing the KV cache into a low-rank latent vector. 
% For each head, instead of storing full-rank \( K, V \in \mathbb{R}^{n \times d} \), MLA learns a latent projection:
% \begin{align}    
%     \bm{q}_{i,\text{rope}}^{(h)}, \bm{k}_{i,\text{rope}}^{(h)} &= \text{RoPE}\left(\bm{x}_i \bm{W}_{dq}^{(h)} \bm{W}_{qr}^{(h)}, \bm{x}_i \bm{W}_{kr}^{(h)}\right) \nonumber\\
%     \bm{q}_{i,\text{nope}}^{(h)} &= \bm{x}_i \bm{W}_{dq}^{(h)} \bm{W}_{qc}^{(h)} \nonumber\\
%     \bm{c}_{i,kv}^{(h)} &= \bm{x}_i \bm{W}_{dkv}^{(h)}  \nonumber\\
%     \bm{k}_{i,\text{nope}}^{(h)}, \bm{v}_{i}^{(h)} &= \bm{c}_{i, kv}^{(h)} \bm{W}_{uk}^{(h)}, \bm{c}_{i, kv}^{(h)} \bm{W}_{uv}^{(h)}  \nonumber\\
%     \bm{o}_i^{(h)}=\text{Softm}&\text{ax}\left(\bm{q}_{i,\text{rope}}^{(h)} \bm{K}_{\le i,\text{rope}}^{(h)\top}+\bm{q}_{i,\text{nope}}^{(h)} \bm{K}_{\le i,\text{nope}}^{(h)\top}\right) \nonumber\\
%     &\quad \quad \quad \cdot \bm{V}_{\le i}^{(h)} \nonumber \\
%     \text{MLA}(\bm{x}_i) &= \left[\bm{o}_i^{(1)}, \dots, \bm{o}_i^{(n)}\right] \cdot \bm{W}_o,
% \end{align}


MLA~\cite{corr/abs-2405-04434} introduces a hybrid architecture that decouples PE from latent KV compression. 
For each head $h$, the input $\bm{x}_i$ is projected into two complementary components:  

\paragraph{Position-Aware Component}
A subset of dimensions retains PE to preserve positional sensitivity:  
$$
 \bm{q}_{i,\text{rope}}^{(h)}, \bm{k}_{i,\text{rope}} = \text{RoPE}
    \left( 
        \bm{x}_i \bm{W}_{dq}
        %^{(h)} 
        \bm{W}_{qr}^{(h)}, \bm{x}_i \bm{W}_{kr} 
    \right),
$$
where $\bm{W}_{dq} \in \mathbb{R}^{d \times d_q}$, $\bm{W}_{qr}^{(h)} \in \mathbb{R}^{d_q\times d_r}$, $\bm{W}_{kr} \in \mathbb{R}^{d \times d_r}$ project queries/keys into a RoPE-preserved component of dimension $d_r$.  

\paragraph{Position-Agnostic Component}
The remaining dimensions $d_c$ are stripped of PE (i.e., NoPE), $\bm{k}_{i,\text{nope}}^{(h)}$ and $\bm{v}_{i}^{(h)}$ and compressed into a shared latent vector $\bm{c}_{i,kv}^{(h)}$:
\begin{align}
   \bm{q}_{i,\text{nope}}^{(h)} &= \bm{x}_i \bm{W}_{dq} \bm{W}_{qc}^{(h)},\nonumber\\ \bm{c}_{i,kv} &= \bm{x}_i \bm{W}_{dkv}, \nonumber\\
   \bm{k}_{i,\text{nope}}^{(h)}, \bm{v}_{i}^{(h)} &= \bm{c}_{i,kv} \bm{W}_{uk}^{(h)}, \bm{c}_{i,kv} \bm{W}_{uv}^{(h)} \nonumber,
\end{align}
where $\bm{W}_{qc}^{(h)} \in \mathbb{R}^{d_q \times d_c}$, 
$\bm{W}_{dkv} \in \mathbb{R}^{d \times d_{kv}}$, 
$\bm{W}_{uk}^{(h)} \in \mathbb{R}^{d_{kv} \times d_c}$, 
$\bm{W}_{uv}^{(h)} \in \mathbb{R}^{d_{kv} \times d_h}$.
Note that $d_r+d_c = d_h$.
% The latent vector \( \bm{c}_{i,kv}^{(h)} \) is then linearly decoded into NoPE keys/values.
The attention output of MLA combines both components:  
\begin{align}    
    &\bm{o}_i^{(h)}=\text{Softm}\text{ax}\left(\bm{q}_{i,\text{rope}}^{(h)} \bm{k}_{\le i,\text{rope}}^{(h)\top}+\bm{q}_{i,\text{nope}} \bm{k}_{\le i,\text{nope}}^{(h)\top}\right) \nonumber\\
    &\quad \quad \quad \cdot \bm{v}_{\le i}^{(h)} \nonumber \\
    &\quad \quad \text{MLA}(\bm{x}_i) = \left[\bm{o}_i^{(1)}, \dots, \bm{o}_i^{(n_h)}\right] \cdot \bm{W}_o.
\end{align}
Unlike MHA and its variants, MLA stores the latent vector $\bm{c}_{kv}$ and $\bm{k}_{i,\text{rope}}^{(h)}$ (\( \mathcal{O}\left(ld_r+ld_{kv})\right) \)) instead of full-rank \( \bm{k}_i, \bm{v}_i \) (\( \mathcal{O}(2ln_hd_h) \)), where \( (d_r+d_{kv}) \ll 2n_hd_h \). 

\paragraph{Why does MLA need to separate RoPE and NoPE?}
MLA introduces matrix merging techniques for the NoPE portion during inference, 
effectively reducing memory usage. 
For the dot product operation \(\bm{q}_{i,\text{nope}}^{(h)} \bm{k}_{j,\text{nope}}^{(h)\top}\), the following identity transformation can be applied
\footnote{To simplify the notation, we omit the superscript $^{(h)}$. 
Matrices $\bm{W}_{uv}$ and $\bm{W}_{o}$ can also be merged, please refer to Appendix C by \citet{corr/abs-2405-04434}.}:
\begin{align}    
    \bm{q}_{i,\text{nope}} \bm{k}_{j,\text{nope}}^{\top} 
    &= \left(\bm{x}_i \bm{W}_{dq} \bm{W}_{qc}\right)\left(\bm{c}_{j,kv} \bm{W}_{uk}\right)^\top \nonumber\\
    &= \bm{x}_i \left(\bm{W}_{dq} \bm{W}_{qc}\bm{W}_{uk}^\top\right)\bm{c}_{j,kv}^\top \nonumber
\end{align}
where $\left(\bm{W}_{dq} \bm{W}_{qc}\bm{W}_{uk}^\top\right)$ can be pre-merged into a single matrix, and $\bm{c}_{j,kv}$ is already stored in the KV cache. 
As for the RoPE portion, the RoPE($\cdot$) function multiplies the input vector by the rotation matrix (e.g., RoPE($\bm{q}_i$) = $\bm{q}_i\bm{R}_i$, $\bm{R}_i$'s specific form will be introduced in \Cref{ssec:partial_rope}). 
Therefore, the identity transformation becomes as follows:
\begin{align}    
    \bm{q}_{i,\text{rope}} \bm{k}_{j,\text{rope}}^{\top} 
    &= \left(\bm{x}_i \bm{W}_{dq} \bm{W}_{qr} \bm{R}_i\right)\left(\bm{x}_j \bm{W}_{kr} \bm{R}_j\right)^\top \nonumber\\
    &= \bm{x}_i \left(\bm{W}_{dq} \bm{W}_{qc}\bm{R}_{j-i}\bm{W}_{kr}^\top\right)\bm{x}_{j}^\top \nonumber
\end{align}
Since \(\left(\bm{W}_{dq} \bm{W}_{qc} \bm{R}_{j-i} \bm{W}_{kr}^\top\right)\) is related to the relative position \(j-i\), it cannot be merged into a fixed matrix. 
Considering that the relative distances in LLMs can be very long, such as 128K, the RoPE portion is better suited to be computed using the original form.
% :  
% (1) Storing the latent vector $\bm{c}_{kv}^{(h)}$ and $\bm{k}_{i,\text{rope}}^{(h)}$ (\( \mathcal{O}\left(ln_h(d_r+d_{kv})\right) \)) instead of full-rank \( \bm{K}, \bm{V} \) (\( \mathcal{O}(2ln_hd_h) \)), where \( (d_r+d_{kv}) \ll d_h \).  
% (2) Preserving all projection matrices (\( \bm{W}_{dq}, \bm{W}_{qr}, \bm{W}_{qc}, \ldots \)) to retain model capacity.  
% Empirically, MLA reduces KV cache memory by \( \times 8\) while matching or exceeding MHA’s accuracy on knowledge-intensive tasks \cite{corr/abs-2405-04434}.  


\section{MHA2MLA}


\subsection{Partial-RoPE}
\label{ssec:partial_rope}


% Since MLA's KV cache compression is incompatible with RoPE, DeepSeek adopts a trade-off approach by retaining PEs in only a limited subset of dimensions while compressing the remainder.  
To enable migration from standard MHA to MLA, we propose partial-RoPE finetuning, a strategy that removes RoPE from a targeted proportion of dimensions and converts them into NoPE. 
Critically, while prior work has explored training LLMs with partial-RoPE from scratch (achieving marginally better perplexity than full-RoPE \cite{gpt-neo,corr/abs-2410-06205}), no existing method addresses how to efficiently adapt pre-trained full-RoPE models (e.g., Llama) to partial-RoPE without costly retraining. 
Our work bridges this gap by systematically evaluating partial-RoPE variants to identify the most data-efficient fine-tuning protocol for recovering model performance post-adaptation.

\input{fig/fig_partial_rope}
\input{fig/fig_7b_2norm}


% 首先我们介绍一下RoPE。给定需要嵌入位置信息的表示向量q和k，We decompose queries and keys into 2-dimensional chunks，\bm{q}_i = [\bm{q}_i^{[2k, 2k+1]}]_{0\le k< \frac{d}{2}}，\bm{k}_i = [\bm{k}_i^{[2k, 2k+1]}]_{0\le k< \frac{d}{2}}.
% In other words, we denote by \bm{q}_i^{[2k, 2k+1]} \in \mathbb{R}^2 the $k$-th 2-dimensional chunk of the query vector of the i-th token, using analogous notation for the key vectors.
% RoPE considers a sequence of angles \(\theta_k = \beta^{-2k/d},  0\le k< \frac{d}{2}\)，where k=0 is the fastest rotating component at 1 radian per token and k=\frac{d}{2}-1 is the slowest rotating component at approximately \frac{1}{\theta} rotations per token. 
% The parameter θ is called the base wavelength, which by default is 10,000 (Su et al., 2024), although works have explored increasing it to, for instance, 500,000 (Xiong et al., 2023; Roziere et al., 2023; Dubey et al., 2024).

\paragraph{MHA's Full-RoPE} encodes positional information into queries and keys through frequency-specific rotations. Formally, given a query vector \(\bm{q}_i \in \mathbb{R}^{d_h}\) and key vector \(\bm{k}_i \in \mathbb{R}^{d_h}\), we partition them into 2D chunks:  
\[
\bm{q}_i,\bm{k}_i = \left[\bm{q}_i^{[2k, 2k+1]}\right]_{0 \leq k < \frac{d_h}{2}}, \left[\bm{k}_i^{[2k, 2k+1]}\right]_{0 \leq k < \frac{d_h}{2}},
\]  
where \(\bm{q}_i^{[2k, 2k+1]} \in \mathbb{R}^2\) denotes the \(k\)-th 2D subspace. Each chunk undergoes a rotation by position-dependent angles \(\theta_k = \beta^{-2k/{d_h}}\), forming a spectrum of wavelengths.
High-frequency components, e.g., $k=0$, rotate rapidly at 1 radian per token.  
Low-frequency components, e.g., $k=\frac{d_h}{2}-1$, rotate slowly at \(\sim \beta^{1/d_h}\) radians per token.  
The base wavelength \(\beta\), typically set to \(10^4\) \cite{journals/ijon/SuALPBL24} or \(5\!\times\!10^5\).

% 我们表示旋转矩阵为\bm{R}^{[2k, 2k+1]}为
% \begin{align}
%     \bm{R}^{[2k, 2k+1]}(\theta_k)=\left[\begin{array}{cr}
% \cos \left(\theta_k\right) & -\sin \left(\theta_k\right) \\
% \sin \left(\theta_k\right) & \cos \left(\theta_k\right)
% \end{array}\right],
% \end{align}
% highlighting that \bm{R}^{[2k, 2k+1]} is a 2-dimensional orthogonal transformation (rotation). 
% One can view \bm{R}^{[2k, 2k+1]} as a ‘unit rotation’ by gk radians. The RoPE technique amounts to the construction of a blockdiagonal matrix \bm{R} where each $2\times2$ block on the diagonal is a rotation by a different frequency of RoPE. 
% The \bm{R}_i denotes in fact matrix exponentiation by an integer i which is the position of $\bm{x}_i$. 
% We can exploit a nice property of rotation matrices, i.e. that R “ ρpigkq to avoid the computation of the matrix power. 
% As this matrix is block diagonal, computing Riqi means that the rotations act only on 2-dimensional chunks of the query (or key), i.e.


Formally, for each 2D chunk \( \bm{q}_i^{[2k, 2k+1]} \) and \( \bm{k}_i^{[2k, 2k+1]} \), the rotation matrix at position \( i \) is defined as:  
\[
\bm{R}_i^{[2k, 2k+1]}(\theta_k) = 
\begin{bmatrix} 
\cos(i\theta_k) & -\sin(i\theta_k) \\ 
\sin(i\theta_k) & \cos(i\theta_k) 
\end{bmatrix}.
\]  
Thus, applying RoPE to queries and keys becomes:  
\begin{align*}
\bm{q}_{i, rope} = \left[\bm{R}_i^{[2k, 2k+1]}(\theta_k)\bm{q}_i^{[2k, 2k+1]}\right]_{0 \leq k < \frac{d_h}{2}},  \\
\bm{k}_{i, rope} = \left[\bm{R}_i^{[2k, 2k+1]}(\theta_k)\bm{k}_i^{[2k, 2k+1]}\right]_{0 \leq k < \frac{d_h}{2}}.
\end{align*}

% Crucially, \( \bm{R}_i^{[2k, 2k+1]} \) is an orthogonal matrix representing a rotation by \( i\theta_k \) radians in the \( k \)-th subspace.  

% The full RoPE transformation is implemented via a block-diagonal matrix, where each \( 2 \times 2 \) block rotates its corresponding query/key subspace by progressively slower frequencies. 
% Thus, applying RoPE to queries and keys becomes:  
% \[
% \bm{q}_{i,\text{rope}} = \bm{R}_i \bm{q}_i, \quad \bm{k}_{i,\text{rope}} = \bm{R}_i \bm{k}_i,
% \]  
% preserving relative positional information through rotational invariance \cite{su2024roformer}.  

\paragraph{Full-RoPE to Partial-RoPE Strategies}  
Given $r$ retained rotational subspaces($r=\frac{d_r}{2}\ll$ total subspaces $\frac{d_h}{2}$, we propose four strategies (illustrated in \Cref{fig:partial_rope}) to select which \( r \) subspaces preserve RoPE encoding:

 \textbf{High-Frequency Preservation} retain the \( r \) fastest-rotating (high-frequency) subspaces:
    \[
    \mathcal{S}_{\text{high}} = \left\{ k \,\vert\, 0 \leq k < r \right\}.
    \]
It is consistent with the p-RoPE method proposed in \citet{corr/abs-2410-06205}, where they explored settings in which \(r\) constituted 25\%, 50\%, and 75\% of the total subspaces, and observed a slight advantage over full-RoPE in LLMs trained from scratch.
    
 \textbf{Low-Frequency Preservation} retain the \( r \) slowest-rotating (low-frequency) subspaces:
\[
\mathcal{S}_{\text{low}} = \left\{ k \,\big|\, \frac{d_h}{2} - r \leq k < \frac{d_h}{2} \right\}.
\]
It was chosen as a controlled experiment for the high-frequency strategy.
    
 \textbf{Uniform Sampling} select \( r \) subspaces with equidistant intervals:
\[
\mathcal{S}_{\text{uniform}} = \left\{ \left\lfloor k \frac{d_h}{2r} \right\rfloor \,\bigg|\, 0 \leq k < r \right\}
\]
This balances high- and low-frequency components through geometric spacing.
In practice, \(2r\) typically divides \(d_h\).
It is similar to the partial RoPE used in GPT-Neo~\cite{gpt-neo}.
    
\textbf{Head-wise 2-norm Contribution} 
\citet{corr/abs-2410-06205} were the first to propose the 2-norm contribution to investigate whether these frequencies are utilized and how they are helpful. 
This approach is based on the observation that, according to the Cauchy-Schwarz inequality, the influence of the \( k \)-th frequency subspace on the attention logits is upper-bounded by the 2-norm of the corresponding query and key components, i.e., $\left|\left\langle\mathbf{q}_i^{[2k,2k+1]}, \mathbf{k}_j^{[2k,2k+1]}\right\rangle\right| \leqslant\left\|\mathbf{q}_i^{[2k,2k+1]}\right\|\left\|\mathbf{k}_j^{[2k,2k+1]}\right\|$.
For each head \( h \), we compute the mean 2-norm score for each subspace in an LLM over long sequences
\footnote{
The 2-norm calculation detail is placed in \Cref{app:2_norm}.
}.
Then, we propose to rank all subspaces by their 2-norm score and select the top-$r$:
\begin{align*}
    \mathcal{S}_{\text{2-norm}}\!=\!\underset{0\le k<\frac{d_h}{2}}{\text{top-}r} \left( \left\|\mathbf{q}_*^{[2k,2k+1]}\right\|\left\|\mathbf{k}_*^{[2k,2k+1]}\right\| \right).
\end{align*}
This head-specific selection adaptively preserves rotation-critical subspaces.
\Cref{fig:7b_2norm} visualizes the 2-norm of  Llama2-7B's four heads.

We will analyze the effectiveness of the four strategies in \Cref{ssec:ablation_study} and conduct an ablation study on the essential hyperparameter $r$ in \Cref{app:pe-dim}.
For all strategies, non-selected subspaces (\( k \notin \mathcal{S} \)) become NoPE dimensions, enabling seamless integration with MLA's latent compression.

\input{fig/fig_svd_overview}
\subsection{Low-rank Approximation}
\label{sec:low_rank_appro}

% 在完成full-rope to partial rope后，我们已经得到了MLA的KV cache中的第一项，即\left[\bm{R}_i^{[2k, 2k+1]}(\theta_k)\bm{k}_i^{[2k, 2k+1]}\right]_{k \in \mathcal{S}}. 
% 接下来，我们的目标是获得第二项c_{i,kv} \in R^{d_{kv}}，它是k_{i,nope}和v_i的低秩表示。

% 给定MHA中的keys和values k_i = x_i W_k，v_i = x_i W_v，首先提取W_k中对应k_{i,nope}的维度，即不在\mathcal{S}中的子空间，得到k_{i,nope} = x_i W_{k,nope}。
% 为了在获得c_{i,kv}的同时最大化利用已经预训练好的参数矩阵W_{k,nope}和W_v，我们提出两种SVD分解的低秩近似方案。
% 方案一叫做SVD_{分解}，是把W_{k,nope}和W_v分开处理，各自使用d_{kv}中的一部分，简单起见，可以让它两各使用一半。
% $$W_{k,nope} = U_{k[:, :\frac{d_kv}{2}]} S_{k[:\frac{d_kv}{2}, :\frac{d_kv}{2}]} V_{k[:\frac{d_kv}{2},:]}$$
% $$W_v = U_{v[:, :\frac{d_kv}{2}]} S_{v[:\frac{d_kv}{2}, :\frac{d_kv}{2}]} V_{v[:\frac{d_kv}{2},:]}$$
% 定义降维矩阵W_{dk}和升维矩阵W_{uk}如下（同样的方式得到W_{dv}，W_{uv}），
% $$W_{dk} = U_{k[:, :\frac{d_kv}{2}]} \sqrt{S_{k[:\frac{d_kv}{2}, :\frac{d_kv}{2}]}}$$
% $$W_{uk} = \sqrt{S_{k[:\frac{d_kv}{2}, :\frac{d_kv}{2}]}} V_{v[:\frac{d_kv}{2},:]} $$

% 方案二是采用MLA的理念，在子空间中联合表示k_{i,nope}和v_i，我们叫做SVD_{joint}。
% $$[ W_{k,nope}, W_v] = U_{[:, :d_kv]} S_{[:d_kv, :d_kv]} V_{k[:d_kv,:]}$$
% $$W_{dkv} = U_{k[:, :d_{kv}]} \sqrt{S_{k[:d_{kv}, :d_{kv}]}}$$
% $$W_{uk} = \sqrt{S_{k[:d_{kv}, :d_{kv}]}} V_{v[:d_{kv},:-d_v]} $$
% $$W_{uv} = \sqrt{S_{k[:d_{kv}, :d_{kv}]}} V_{v[:d_{kv},d_v:]} $$
% 实证发现，SVD_{joint}对LLM的性能损失更小。


After transitioning from full RoPE to partial RoPE, we obtain the first component of the KV cache in MLA, represented as: \(\bm{k}_{i, rope} = \left[\bm{R}_i^{[2k,2k+1]}(\theta_k)\bm{k}_i^{[2k,2k+1]}\right]_{k \in \mathcal{S}}\). 
Our next goal is to derive the second component, \(\bm{c}_{i,kv} \in \mathbb{R}^{d_{kv}}\), which serves as a low-rank representation of \(\bm{k}_{i,\text{nope}}\) and \(\bm{v}_i\).


Given the keys $\bm{k}_i = \bm{x}_i \bm{W}_k$ and values $\bm{v}_i = \bm{x}_i \bm{W}_v$ in MHA,
we first extract the subspace of $\bm{W}_k$ corresponding to \(\bm{k}_{i,\text{nope}}\), i.e., the dimensions not included in \(\mathcal{S}\), yielding: $\bm{k}_{i,\text{nope}} = \bm{x}_i \bm{W}_{k,\text{nope}}$.
We propose two Singular Value Decomposition (SVD)-based strategies (Illustrated in \Cref{fig:svd_overview}) to preserve pre-trained knowledge while achieving rank reduction:


\paragraph{Decoupled SVD (SVD\textsubscript{split})}  
Separately decompose \(\bm{W}_{k,\text{nope}}\) and \(\bm{W}_v\) into truncated SVDs, allocating \(d_{kv}/2\) dimensions to each:  
\[
\bm{W}_{k,\text{nope}} = \bm{U}_k \bm{\Sigma}_k \bm{V}_k^\top, \quad \bm{W}_v = \bm{U}_v \bm{\Sigma}_v \bm{V}_v^\top,
\]  
where \(\bm{U}_k, \bm{U}_v,\bm{V}_{k},\bm{V}_{v}\in \mathbb{R}^{d_h \times \frac{d_{kv}}{2}}\), \(\bm{\Sigma}_k, \bm{\Sigma}_v \in \mathbb{R}^{\frac{d_{kv}}{2} \times \frac{d_{kv}}{2}}\). The down-projection matrices $\bm{W}_{d*}$ and up-projection matrices $\bm{W}_{u*}$ become:  
\begin{gather*}
\bm{W}_{dk} = \bm{U}_k \bm{\Sigma}_k^{1/2}, \quad \bm{W}_{uk} = \bm{\Sigma}_k^{1/2} \bm{V}_k^\top, \\ 
\bm{W}_{dv} = \bm{U}_v \bm{\Sigma}_v^{1/2}, \quad \bm{W}_{uv} = \bm{\Sigma}_v^{1/2} \bm{V}_v^\top.
\end{gather*}
The low-rank representation \( \bm{c}_{i, kv} \) can be constructed using $\bm{c}_{i,kv} = \left[ \bm{x}_i \bm{W}_{dk}, \bm{x}_i \bm{W}_{dv} \right]$.


\paragraph{Joint SVD (SVD\textsubscript{joint})}  
To preserve interactions between \(\bm{K}_{\text{nope}}\) and \(\bm{V}\), we jointly factorize the concatenated matrix:  
\[
[\bm{W}_{k,\text{nope}}, \bm{W}_v] = \bm{U}_{kv} \bm{\Sigma}_{kv} \bm{V}_{kv}^\top,
\]  
where \(\bm{U}_{kv},\bm{V}_{kv} \in \mathbb{R}^{d_h \times d_{kv}}\), \(\bm{\Sigma}_{kv} \in \mathbb{R}^{d_{kv} \times d_{kv}}\). The latent projection is then:  
\begin{gather*}
\bm{W}_{dkv} = \bm{U}_{kv} \bm{\Sigma}_{kv}^{1/2}, \\ 
\bm{W}_{uk}\!=\!\bm{\Sigma}_{kv}^{1/2} \bm{V}_{kv}[:, :-d_v], \bm{W}_{uv}\!=\!\bm{\Sigma}_{kv}^{1/2} \bm{V}_{kv}[:, d_v:].    
\end{gather*}
This jointly optimizes the latent space for both keys and values, i.e., $\bm{c}_{i,kv} = \bm{x}_i \bm{W}_{dkv}$, retaining cross-parameter dependencies critical for autoregressive generation
\footnote{
We describe the economical inference process of MHA2MLA in \Cref{app:mha2mla_infer}.
}.  
% \paragraph{Empirical Analysis}  
\Cref{ssec:ablation_study} shows \textbf{SVD\textsubscript{joint}} outperforming \textbf{SVD\textsubscript{split}}, validating that joint factorization better preserves pre-trained knowledge.  

