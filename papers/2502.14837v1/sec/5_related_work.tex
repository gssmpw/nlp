\section{Related Work}
\label{sec:related-work}


% \paragraph{Attention Architecture}
% 注意力模块有着全局感知野，是Transformer模型的关键模块。
% MHA，MQA，GQA，MLA。

% 为了解决句子长度二次方的问题，线性注意力：star-transformer，Linear Transformer (Katharopoulos et al.,
% 2020; Wang et al., 2020),  RWKV (Peng et al.,
% 2023), and Mamba (Gu and Dao, 2023), Ring Attention。

% 当前的LLM中主流是MHA极其变种，我们的实验验证了他们都能快速适配到MLA。对于线性注意力为代表的其他架构，只要其中存在类似Query*Key点积的形式，MHA2MLA的思路也能适配。

% \paragraph{Economical Key-Value Cache}
% KV cache在推理中成为瓶颈
% 架构设计：MLA。Sharing KV representations across layers, as in YONO (Sun et al., 2024), MiniCache (Liu et al., 2024b), and MLKV (Zuhri et al., 2024), reduces memory by reusing the same KV cache across multiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance.

% 量化方法：KV quantization techniques like GPTQ
% (Frantar et al., 2022), Kivi (Liu et al., 2024c), and
% KVQuant (Hooper et al., 2024) reduce the precision of the KV vectors by storing them in lower-bit
% formats. This reduces memory usage and computational overhead, enabling longer contexts and faster
% inference with minimal performance loss.

% 剪枝：dynamic token pruning, employed by LazyLLM (Fu et al., 2024), A2SF (Jo and Shin, 2024), and SnapKV (Li et al., 2024). These methods selectively prune less important tokens from the KV cache, reducing memory usage without sacrificing performance. Although dynamic pruning improves efficiency, it risks discarding essential tokens, especially for tasks requiring a detailed understanding of distant context. Additionally, dynamic pruning introduces complexity in determining which tokens to prune and often
% requires fine-tuning or retraining. Pruning head dimensions, seen in approaches like SliceGPT (Ashkboos et al., 2024), Sheared (Xia et al., 2023), and Simple Pruning (Sun et al., 2023), reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the model’s ability to capture important token relationships.

% 我们的MHA2MLA方法实现了标准Transformer架构的LLM迁移到MLA这种经济性的优秀架构，还证明了能够和KV量化方法集成，实现97\%的cache节省，理论上也能与剪枝方法集成。



\paragraph{Efficient Attention Architectures}  
The standard Multi-Head Attention (MHA, \citeyear{nips/VaswaniSPUJGKP17}) mechanism's quadratic complexity in context length has spurred numerous efficiency innovations. 
While MHA remains foundational, variants like Multi-Query Attention (MQA) and Grouped-Query Attention (GQA, \citeyear{emnlp/AinslieLJZLS23}) reduce memory overhead by sharing keys/values across heads—albeit at the cost of parameter pruning and performance degradation. 
Parallel efforts, such as Linear Transformers \cite{naacl/GuoQLSXZ19,icml/KatharopoulosV020, iclr/ChoromanskiLDSG21}, RWKV \cite{emnlp/PengAAAABCCCDDG23}, and Mamba \cite{corr/abs-2312-00752}, replace softmax attention with linear recurrences or state-space models, but struggle to match the expressiveness of standard attention in autoregressive generation.  

Multi-Head Latent Attention (MLA, \citeyear{corr/abs-2405-04434})  distinguishes itself by compressing KV caches into low-rank latent vectors without pruning attention parameters. 
Our work bridges MLA with mainstream architectures (MHA/GQA), enabling seamless migration via data-efficient fine-tuning.  
Notably, while many linear attention variants abandon softmax query-key interactions (e.g., through kernel approximations), architectures preserving a query-key dot product structure—even in factorized forms—remain compatible with our MHA2MLA framework. 


\paragraph{Economical Key-Value Cache}  
The memory footprint of KV caches has become a critical bottleneck for long-context inference. 
Recent advances fall into three categories:  

\textit{Innovative Architecture} methods like MLA~\cite{corr/abs-2405-04434}, MiniCache~\cite{nips/LiuLPHHZ24}, and MLKV~\cite{corr/abs-2406-09297} share or compress KV representations across layers or heads. 
While effective, cross-layer sharing risks conflating distinct attention patterns, potentially harming task-specific performance. 
Only MLA has been successfully validated in Deepseek's LLMs.

\textit{Quantization} techniques such as GPTQ~\cite{corr/abs-2210-17323}, FlexGen~\cite{icml/0007ZYLRCLRSZ23}, and KIVI~\cite{icml/LiuYJZXBC024} store KV caches in low-bit formats (e.g., 2-bit), achieving memory savings with precision loss.  

\textit{Dynamic Pruning} approaches like A2SF~\cite{corr/abs-2407-20485} and SnapKV~\cite{nips/LiHYVLYCLC24} prune ``less important'' tokens from the KV cache. However, token pruning risks discarding critical long-range dependencies, while head pruning (e.g., SliceGPT~\cite{iclr/AshkboosCNHH24}, Sheared~\cite{conf/iclr/XiaGZ024}, and Simple Pruning~\cite{conf/iclr/Sun0BK24}) irreversibly reduces model capacity.  

Our MHA2MLA method achieves the migration of standard Transformer-based LLMs to the more economical MLA architecture and has demonstrated its ability to integrate with KV quantization techniques to realize a \textasciitilde 97\% cache saving. It is also theoretically compatible with other methods like pruning. 
