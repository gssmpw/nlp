\begin{table*}[t]
\centering
\small
\setlength\tabcolsep{3pt}
% \resizebox{\textwidth}{!}{
\begin{tabular}{llrrrrrrrrrrrrrrrrrr}
  \toprule
  % {\textbf{Model}} & \textbf{Precision} & \textbf{KV Mem.} & \textbf{Avg@LB} \\
  \multirow{2}{*}{\textbf{$d_{kv}$}} & \multirow{2}{*}{\textbf{Precision}} & \multirow{2}{*}{\textbf{KV}} & \multirow{2}{*}{\textbf{Avg.}} & \multicolumn{3}{c}{\textbf{S-Doc QA}} & \multicolumn{3}{c}{\textbf{M-Doc QA}} & \multicolumn{3}{c}{\textbf{Summ.}} & \multicolumn{3}{c}{\textbf{Few-shot}} & \multicolumn{2}{c}{\textbf{Synth.}} & \multicolumn{2}{c}{\textbf{Code}} \\
  \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16} \cmidrule(lr){17-18} \cmidrule(lr){19-20} 
  & & & & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} & \textbf{E} & \textbf{F} & \textbf{G} & \textbf{H} & \textbf{I} & \textbf{J} & \textbf{K} & \textbf{L} & \textbf{M} & \textbf{N} & \textbf{O} & \textbf{P} \\ 
  \midrule
  \rowcolor{gray!10} \multicolumn{20}{c}{\textit{\textbf{7B$_{\text{Llama2}}$ (Length=4K)}}} \\
  & \raggedright BF16 & 100.0\% & 27.4 & 15.1 & 9.6 & 21.1 & 7.5 & 9.7 & 3.7 & 26.7 & 20.5 & 3.2 & 65.5 & 87.5 & 34.1 & 1.9 & 6.6 & 66.5 & 59.4 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-75.00\%} 
  & 27.5 & 16.1 & 9.1 & 22.0 & 7.3 & 9.9 & 3.6 & 26.5 & 21.1 & 3.4 & 65.5 & 87.2 & 34.3 & 1.5 & 6.7 & 66.0 & 59.9 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & 27.3 & 14.4 & 9.5 & 20.5 & 7.5 & 9.7 & 3.5 & 25.8 & 20.7 & 3.1 & 65.5 & 87.7 & 34.3 & 1.4 & 7.3 & 66.8 & 59.3 \\
    \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int2$_{\text{HQQ}}$ & \multirow{2}{*}{-87.50\%} & 21.2 & 18.0 & 5.5 & 12.6 & 7.5 & 8.4 & 3.2 & 12.6 & 18.6 & 0.9 & 56.5 & 73.3 & 27.0 & 1.8 & 6.1 & 34.5 & 52.9 \\
  & \raggedright Int2$_{\text{Quanto}}$ &  & 18.5 & 9.4 & 6.2 & 12.7 & 6.8 & 6.7 & 3.3 & 5.9 & 17.2 & 0.4 & 61.0 & 63.9 & 26.0 & 1.4 & 2.7 & 42.4 & 30.5 \\
  % \arrayrulecolor{gray!20}
  \arrayrulecolor{black}
  \hline
  \multirow{3}{*}{$64$} 
  & \raggedright BF16 & -68.75\% & 27.1 & 13.3 & 9.6 & 23.2 & 7.2 & 10.9 & 3.5 & 24.6 & 20.0 & 22.1 & 62.5 & 83.5 & 32.4 & 0.9 & 8.7 & 56.9 & 53.7 \\
  \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-92.19\%}  & \bf 26.9 & 13.4 & 9.1 & 25.6 & 7.3 & 10.2 & 3.4 & 24.6 & 20.0 & 20.9 & 62.5 & 83.8 & 32.3 & 0.6 & 9.6 & 55.3 & 52.7 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & \bf 26.8 & 13.8 & 9.2 & 24.6 & 7.4 & 10.5 & 3.5 & 24.6 & 19.8 & 21.4 & 62.0 & 84.3 & 31.8 & 1.2 & 7.5 & 56.1 & 51.8 \\
  % \arrayrulecolor{gray!20}
  \arrayrulecolor{black}
  \hline
  \multirow{3}{*}{$32$} 
  & \raggedright BF16 & -81.25\% & 26.3 & 14.9 & 9.1 & 27.0 & 7.3 & 9.9 & 3.1 & 24.6 & 19.1 & 22.5 & 60.5 & 81.6 & 26.9 & 0.0 & 8.2 & 53.4 & 52.6 \\
  \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-95.31\%}  &\bf  26.1 & 14.7 & 9.5 & 26.6 & 7.9 & 10.7 & 3.4 & 23.6 & 19.0 & 20.5 & 60.5 & 80.8 & 28.3 & 0.0 & 7.6 & 51.9 & 52.0 \\
  & \raggedright Int4$_{\text{Quanto}}$ &  & \bf 26.1 & 14.7 & 9.5 & 26.6 & 7.9 & 10.7 & 3.4 & 23.6 & 19.0 & 20.5 & 60.5 & 80.8 & 28.3 & 0.0 & 7.6 & 51.9 & 52.0 \\
  % \arrayrulecolor{gray!20}
  \arrayrulecolor{black}
  \hline
  \multirow{3}{*}{$16$} 
  & \raggedright BF16 & -87.50\% & \bf 24.4 & 14.7 & 9.5 & 24.3 & 7.8 & 10.2 & 3.8 & 22.8 & 19.1 & 24.6 & 61.0 & 82.8 & 20.2 & 0.2 & 8.6 & 39.9 & 41.4 \\
  \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-96.87\%}   & \bf 24.2 & 15.2 & 9.4 & 25.2 & 7.4 & 10.2 & 3.9 & 22.9 & 19.8 & 20.6 & 61.0 & 82.5 & 21.7 & 0.1 & 9.0 & 38.0 & 41.2 \\
  & \raggedright Int4$_{\text{Quanto}}$ &  & \bf 23.4 & 15.6 & 8.4 & 22.7 & 7.3 & 10.2 & 3.8 & 20.2 & 18.7 & 18.6 & 61.0 & 81.9 & 21.7 & 0.5 & 8.0 & 36.9 & 38.3\\
  \arrayrulecolor{black}
  
  % \midrule
  \rowcolor{gray!10} \multicolumn{20}{c}{\textit{\textbf{1B7$_{\text{SmolLM}}$ (Length=2K)}}} \\
  & \raggedright BF16 & 100.0\% & 18.7 & 2.6 & 6.3 & 19.9 & 5.4 & 8.6 & 2.7 & 23.5 & 18.4 & 20.2 & 46.5 & 70.2 & 32.4 & 2.2 & 3.2 & 21.3 & 16.5 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-75.00\%} 
  & 18.6 & 2.5 & 6.2 & 19.1 & 5.5 & 8.2 & 2.7 & 23.4 & 18.3 & 20.0 & 46.5 & 69.4 & 32.1 & 2.7 & 3.2 & 21.5 & 16.0 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & 18.6 & 2.6 & 6.2 & 17.4 & 5.1 & 8.6 & 2.6 & 23.0 & 18.1 & 20.1 & 46.0 & 70.2 & 31.9 & 2.9 & 3.6 & 21.9 & 16.7 \\
  & \raggedright Int2$_{\text{HQQ}}$ & \multirow{2}{*}{-87.50\%} & 16.3 & 2.5 & 5.6 & 13.0 & 4.8 & 7.5 & 2.7 & 14.8 & 16.3 & 9.3 & 46.0 & 70.4 & 26.9 & 2.6 & 3.4 & 18.3 & 16.8 \\
  & \raggedright Int2$_{\text{Quanto}}$ &  & 13.3 & 1.6 & 3.8 & 10.3 & 3.9 & 7.3 & 1.4 & 5.9 & 13.4 & 6.3 & 40.0 & 64.3 & 14.6 & 3.1 & 3.5 & 15.6 & 17.5 \\
  % \arrayrulecolor{gray!20}
  \hline
  \multirow{3}{*}{$32$} 
  & \raggedright BF16 & -68.75\% & 16.0 & 2.6 & 6.1 & 16.9 & 4.6 & 9.3 & 2.0 & 22.8 & 15.1 & 19.9 & 50.0 & 57.1 & 29.8 & 1.7 & 2.4 & 9.4 & 6.7 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-92.19\%}  & 15.9 & 2.7 & 5.7 & 16.3 & 5.0 & 8.5 & 1.8 & 23.0 & 15.0 & 18.5 & 50.0 & 56.2 & 30.2 & 1.8 & 3.2 & 10.0 & 6.8 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & 15.4 & 2.5 & 5.7 & 16.1 & 5.7 & 8.7 & 2.1 & 20.9 & 13.8 & 17.6 & 50.0 & 55.0 & 29.5 & 1.7 & 2.8 & 9.6 & 5.4 \\
  % \arrayrulecolor{gray!20}
  \hline
  \multirow{3}{*}{$16$} 
  & \raggedright BF16 & -81.25\% & 16.5 & 2.6 & 6.2 & 17.2 & 4.5 & 9.7 & 2.1 & 22.0 & 15.3 & 21.0 & 47.5 & 55.5 & 31.7 & 1.2 & 3.3 & 15.8 & 8.5 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-95.31\%} & 16.2 & 2.5 & 6.1 & 16.2 & 4.5 & 8.9 & 2.0 & 20.6 & 15.4 & 19.7 & 47.5 & 55.6 & 30.6 & 1.2 & 4.0 & 16.3 & 8.0 \\
  & \raggedright Int4$_{\text{Quanto}}$ &  & 15.6 & 2.5 & 5.7 & 15.6 & 4.3 & 8.8 & 1.6 & 21.2 & 15.7 & 17.6 & 47.0 & 55.7 & 27.4 & 1.7 & 3.6 & 15.6 & 6.2 \\
  % \arrayrulecolor{gray!20}
  \hline
  \multirow{3}{*}{$8$} 
  & \raggedright BF16 & -87.50\% & 15.3 & 2.4 & 5.9 & 17.9 & 4.8 & 10.1 & 1.8 & 25.1 & 15.2 & 20.6 & 42.5 & 49.0 & 31.4 & 2.7 & 3.3 & 7.1 & 4.4\\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-96.87\%}   & 15.0 & 2.4 & 5.7 & 16.9 & 4.7 & 10.1 & 2.0 & 23.5 & 14.7 & 20.3 & 42.5 & 47.6 & 30.6 & 2.6 & 3.6 & 7.7 & 4.5\\
  & \raggedright Int4$_{\text{Quanto}}$ &  & 14.2 & 2.7 & 5.4 & 16.9 & 4.1 & 8.8 & 1.5 & 22.2 & 14.4 & 17.2 & 42.0 & 47.9 & 29.9 & 1.5 & 3.3 & 7.0 & 3.0 \\
  \arrayrulecolor{black}
  % \midrule
\rowcolor{gray!10} \multicolumn{20}{c}{\textit{\textbf{360M$_{\text{SmolLM}}$ (Length=2K)}}} \\
  & \raggedright BF16 & 100.0\% & 13.5 & 2.4 & 6.4 & 14.3 & 5.0 & 8.8 & 2.5 & 18.0 & 17.5 & 7.1 & 47.5 & 37.5 & 24.9 & 1.5 & 3.4 & 8.1 & 10.4 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-75.00\%} 
  & 13.4 & 2.7 & 6.1 & 14.1 & 5.5 & 8.4 & 3.0 & 16.2 & 15.4 & 11.2 & 47.5 & 37.5 & 23.4 & 1.3 & 3.7 & 9.0 & 10.1 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & 13.3 & 2.4 & 6.2 & 13.7 & 5.4 & 8.7 & 2.6 & 15.4 & 17.4 & 7.3 & 47.5 & 37.3 & 24.4 & 1.0 & 3.7 & 8.4 & 11.0 \\
  & \raggedright Int2$_{\text{HQQ}}$ & \multirow{2}{*}{-87.50\%} & 10.8 & 2.7 & 4.7 & 8.3 & 5.4 & 5.9 & 1.9 & 9.9 & 10.0 & 8.4 & 45.2 & 27.5 & 14.2 & 2.1 & 4.2 & 10.0 & 11.9 \\
  & \raggedright Int2$_{\text{Quanto}}$ &  & 8.6 & 2.6 & 2.2 & 4.4 & 3.9 & 4.8 & 1.4 & 5.6 & 8.9 & 2.9 & 44.0 & 26.8 & 9.6 & 1.0 & 1.9 & 7.2 & 9.7 \\
  % \arrayrulecolor{gray!20}
  \hline
  \multirow{3}{*}{$32$} 
  & \raggedright BF16 & -68.75\% & 13.5 & 2.3 & 5.9 & 13.4 & 5.5 & 9.8 & 2.7 & 20.4 & 14.5 & 11.5 & 41.0 & 31.2 & 29.6 & 1.2 & 3.5 & 15.4 & 7.9 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-92.19\%}  & \bf 12.5 & 2.6 & 5.7 & 12.1 & 5.1 & 10.2 & 2.7 & 14.6 & 12.5 & 8.8 & 41.0 & 30.3 & 27.8 & 1.9 & 2.7 & 14.5 & 7.6 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & \bf 12.3 & 2.0 & 5.2 & 11.9 & 5.0 & 9.1 & 3.0 & 15.4 & 14.9 & 8.3 & 41.0 & 28.3 & 27.0 & 0.9 & 3.9 & 13.8 & 7.8 \\
  % \arrayrulecolor{gray!20}
  \hline
  \multirow{3}{*}{$16$} 
  & \raggedright BF16 & -81.25\% & 11.6 & 2.2 & 5.2 & 13.0 & 4.8 & 9.5 & 3.2 & 13.4 & 13.4 & 11.3 & 32.0 & 26.1 & 22.5 & 1.1 & 5.0 & 14.9 & 7.7 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-95.31\%}  & \bf 11.2 & 2.6 & 5.6 & 12.0 & 5.1 & 8.8 & 2.9 & 13.4 & 12.4 & 10.8 & 32.0 & 24.8 & 21.8 & 2.1 & 3.7 & 14.0 & 7.2 \\
  & \raggedright Int4$_{\text{Quanto}}$ &  & \bf 10.9 & 1.9 & 4.9 & 11.5 & 4.2 & 8.8 & 2.6 & 12.2 & 12.2 & 9.5 & 32.5 & 25.8 & 18.5 & 1.4 & 4.6 & 15.5 & 7.8 \\
  % \arrayrulecolor{gray!20}
  \hline
  \multirow{3}{*}{$8$} 
  & \raggedright BF16 & -87.50\% & 9.9 & 1.9 & 4.7 & 11.7 & 4.5 & 8.5 & 2.8 & 13.0 & 12.9 & 9.4 & 34.0 & 17.2 & 15.3 & 1.4 & 3.2 & 11.4 & 6.9 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-96.87\%}   & 10.0 & 2.2 & 4.8 & 11.0 & 4.2 & 8.2 & 2.6 & 13.1 & 12.8 & 11.7 & 33.5 & 17.3 & 14.8 & 0.8 & 4.4 & 11.6 & 7.4\\
  & \raggedright Int4$_{\text{Quanto}}$ &  & 9.3 & 1.8 & 3.6 & 11.3 & 4.0 & 8.0 & 3.0 & 10.6 & 12.0 & 7.4 & 31.5 & 19.8 & 10.3 & 0.8 & 4.8 & 12.1 & 7.6 \\
  \arrayrulecolor{black}
  \midrule


\end{tabular}
% }
\caption{Evaluation results of all models on LongBench, including Task A: narrativeqa, B: qasper, C: multifieldqa\_en, D: hotpotqa, E: 2wikimqa, F: musique, G: gov\_report, H: qmsum, I: multi\_news, J: trec, K: triviaqa, L: samsum, M: passage\_count, N: passage\_retrieval\_en, O: lcc, P: repobench-p. \textbf{Bold} indicates compression ratios greater than or equal to Int2 quantization while also achieving performance higher than Int2.}
\label{tab:other_long_bench}
\end{table*}
