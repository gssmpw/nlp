\begin{table}[t]
\centering
% \small
\begin{tabular}{l@{\hskip 5pt}p{2cm}@{\hskip 5pt}c@{\hskip 5pt}c}
  \toprule
  {\textbf{Model}} & \textbf{Precision} & \textbf{KV Mem.} & \textbf{Avg@LB} \\
  \midrule
  \rowcolor{gray!10}7B$_{\text{Llama2}}$
  & \raggedright BF16 & 100.0\% & 27.4 \\
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-75.00\%} 
  & 27.5 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & 27.3 \\
    \arrayrulecolor{gray!20}
  \hline

  & \raggedright Int2$_{\text{HQQ}}$ & \multirow{2}{*}{-87.50\%} & 21.2 \\
  & \raggedright Int2$_{\text{Quanto}}$ &  & 18.5 \\
  % \arrayrulecolor{gray!20}
  \arrayrulecolor{black}
  \hline
  \multirow{3}{*}{~$d_{kv}\!=\!64$} 
  & \raggedright BF16 & -68.75\% & 27.1 \\
  \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-92.19\%}  & \bf 26.9 \\
  & \raggedright Int4$_{\text{Quanto}}$ & & \bf 26.8 \\
  % \arrayrulecolor{gray!20}
  \arrayrulecolor{black}
  \hline
  \multirow{3}{*}{~$d_{kv}\!=\!32$} 
  & \raggedright BF16 & -81.25\% & 26.3 \\
  \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-95.31\%}  &\bf  26.1 \\
  & \raggedright Int4$_{\text{Quanto}}$ &  & \bf 26.1 \\
  % \arrayrulecolor{gray!20}
  \arrayrulecolor{black}
  \hline
  \multirow{3}{*}{~$d_{kv}\!=\!16$} 
  & \raggedright BF16 & -87.50\% & \bf 24.4 \\
  \arrayrulecolor{gray!20}
  \hline
  & \raggedright Int4$_{\text{HQQ}}$ & \multirow{2}{*}{-96.87\%}   & \bf 24.2 \\
  & \raggedright Int4$_{\text{Quanto}}$ &  & \bf 23.4 \\
  \arrayrulecolor{black}
    \bottomrule
\end{tabular}
\caption{Evaluation results of Llama2-7B and MHA2MLA on LongBench. \textbf{Bold} indicates compression ratios greater than or equal to Int2 quantization while also achieving performance higher than Int2.}
\vspace{-0.3cm}
\label{tab:long_bench}
\end{table}