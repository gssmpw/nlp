\documentclass{article}

\usepackage{iclr2025_conference,times}

% ready for submission
\input{defs.tex}


\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{anyfontsize}


\title{Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors} 


  
\author{
  Milad Sefidgaran$^{\:\nmid}$,\quad Abdellatif Zaidi$\:^{\dagger}$$\:^{\nmid}$,\quad Piotr Krasnowski$^{\:\nmid}$\\
  $^{\:\nmid}$ Paris Research Center, Huawei Technologies France \quad 
  $\:^{\dagger}$ Universit\'e Gustave Eiffel, France \\
  \texttt{\{milad.sefidgaran2,piotr.g.krasnowski\}@huawei.com,}\\ \hspace{0.2 cm}\texttt{abdellatif.zaidi@univ-eiffel.fr}
}

\iclrfinalcopy

% \author{
%   Milad Sefidgaran,\quad Abdellatif Zaidi,\quad Piotr Krasnowski
% }
\begin{document}

\maketitle
\begin{abstract}
We establish in-expectation and tail bounds on the generalization error of representation learning type algorithms. The bounds are in terms of the relative entropy between the distribution of the representations extracted from the training and ``test'' datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for the training and test datasets. Our bounds are shown to reflect the ``structure'' and ``simplicity'' of the encoder and significantly improve upon the few existing ones for the studied model. We then use our in-expectation bound to devise a suitable data-dependent regularizer; and we investigate thoroughly the important question of the selection of the prior. We propose a systematic approach to simultaneously learning a data-dependent Gaussian mixture prior and using it as a regularizer. Interestingly, we show that a \textit{weighted attention mechanism} emerges naturally in this procedure. Our experiments show that our approach outperforms the now popular Variational Information Bottleneck (VIB) method as well as the recent Category-Dependent VIB (CDVIB). 
\end{abstract}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Introduction %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:intro}
One major problem in learning theory pertains to how to guarantee that a statistical learning algorithm performs on new, unseen data as well as on the used training data, i.e., it has good \textit{generalization} properties. This key question, which has roots in various scientific disciplines, has been studied using seemingly unrelated approaches, including compression-based  \citep{littlestone1986relating,blumer1987occam, arora2018stronger, blum2003pac,suzuki2018spectral,hsu2021generalization,barsbey2021heavy,hanneke2019sharp,hanneke2019sample,bousquet2020proper, hanneke2021stable,hanneke2020universal,cohen2022learning,Sefidgaran2022,sefidgaran2024data}, information-theoretic \citep{russozou16,xu2017information,steinke2020reasoning,esposito2020,Bu2020,haghifam2021towards,neu2021informationtheoretic,aminian2021exact,harutyunyan2021,Zhou2022,lugosi2022generalization,hellstrom2022new}, PAC-Bayes \citep{seeger2002pac,langford2001not,catoni2003pac,maurer2004note,germain2009pac,tolstikhin2013pac,begin2016pac,thiemann2017strongly,dziugaite2017computing,neyshabur2018pacbayesian,rivasplata2020pac,negrea2020defense,negrea2020it,viallard2021general}, and intrinsic dimension-based \citep{simsekli2020hausdorff,birdal2021intrinsic,hodgkinson2022generalization,lim2022chaotic} approaches. 

In practice, a common approach advocates the usage of a two-part, or \emph{encoder-decoder}, model, often referred to as \textit{representation learning}. In this approach, the encoder part of the model shoots for extracting a ``minimal'' \emph{representation} of the input (i.e., small generalization error), whereas the decoder part shoots for small empirical risk. One popular approach is the information bottleneck (IB), which was first introduced in ~\citep{tishby2000information} and then extended in various ways \citep{shamir2010learning,alemi2016deep,estella2018distributed,kolchinsky2019nonlinear,fischer2020conditional,rodriguez2020convex,kleinman2022gacs}. The IB principle is mainly based on the assumption that Shannon's mutual information (MI) between the input and the representation is a good indicator of the generalization error. However, this assumed relationship has been refuted in several works \citep{kolchinsky2018caveats, rodriguez2019information, amjad2019learning,geiger2019information, dubois2020learning,lyu2023recognizable,sefidgaran2023minimum}. As shown in these works, the few existing theoretical MI-based generalization bounds (\eg \citep{vera2018role,kawaguchi23a}) become vacuous in most reasonable setups. Also, in practice, no consistent relation between the generalization error and the MI has been observed experimentally so far. Rather, recent works \citep{blum2003pac,geiger2019information,sefidgaran2023minimum} have shown that the generalization error of representation learning algorithms is related to the \emph{minimum description length} (MDL) of the latent variable and the so-called \emph{geometric compression}. Geometric compression occurs when latent vectors are designed so as to concentrate around a limited number of representatives which form centroid vectors of associated clusters~\citep{amjad2019learning, geiger2019information}. In such settings, inputs can be mapped to the centroids of the clusters that are closest to their associated latent vectors (i.e., lossy compression); and this yields non-vacuous bounds at the expense of only a small (distortion) penalty. The benefit of this lossy compression approach can be appreciated when opposed to classic MI-based bounds \citep{vera2018role,kawaguchi23a} which are known to be vacuous when the latent vectors are deterministic functions of the inputs.

In this work, we study the problem of representation learning depicted in Fig.~\ref{fig:setup} from a generalization error perspective. Then, we use the obtained generalization bound to design and discuss various choices of generalization-inspired regularizers using data-dependent Gaussian mixture priors. To the best knowledge of the authors, generalization error bounds that account suitably for the encoder-decoder structure of the representation learning problem of Fig.~\ref{fig:setup} are very scarce; and, in fact, with the exception of~\citep{sefidgaran2023minimum}, no non-vacuous bounds for these settings have been reported so far. 


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{Fig/CRL.png}
    \caption{Studied representation learning setup.} 
    % \vspace{-0.2 cm}
    \label{fig:setup}
\end{figure}

\textbf{Contributions:} Our main contributions in this work are summarized as follows. 
\begin{itemize}[leftmargin=0.9em,topsep=0pt]
  \setlength\itemsep{0.1 cm}
\item We establish in-expectation and tail bounds on the generalization error of the representation learning algorithms. Our bounds are expressed in terms of the relative entropy between the distribution of the representations extracted from training and ``test'' datasets and a data-dependent symmetric prior $\vc Q$, i.e., the Minimum Description Length ($\text{MDL}(\vc Q)$) of the latent variables for training and test datasets -- (Bounds that depend on $\text{MDL}(\vc Q)$ are arguably \textit{better} bounds because they capture the structure and simplicity of the encoders in sharp contrast with IB-based approaches~\citep{blum2003pac}). However,  our bounds are shown to be possibly tighter than those of \citep{sefidgaran2023minimum}. For instance, while the bounds of~\citep{sefidgaran2023minimum} are of the order of $\sqrt{\text{MDL}(\vc{Q})/n}$, where $n$ designates the size of the used training dataset, ours is approximately of the order of $\text{MDL}(\vc{Q})/n$ for the realizable setup.
 
\item We propose a systematic approach to finding a suitable ``data-dependent'' prior that we then use to construct judiciously a regularizer during training (based on our newly established bounds). Specifically, first, we observe that if the latent variables are generated according to a Gaussian distribution, then the prior $\vc{Q}$ that minimizes the \emph{empirical} $\text{MDL}(\vc{Q})$ term is a Gaussian mixture distribution. Then, using this and the known fact that Gaussian mixture distributions can approximate sufficiently well any arbitrary distribution when the number of mixture components is large enough~\citep{dalal1983approximating,goodfellow2016deep,nguyen2022admixture}, we propose two methods for simultaneously finding a Gaussian mixture prior and using it as a regularizer along the optimization iterations. The methods are coined `lossless Gaussian mixture prior'' and ``lossy Gaussian mixture prior'', respectively.  In essence, the procedure consists of finding the underlying ``structure'' of the latent variables in the form of a Gaussian mixture prior; and, simultaneously, steers the latent variables to best fit with this found structure. Interestingly, in the lossy version of the approach, which is shown to generally yield better performance, the components of the Gaussian mixture are updated using a mechanism that is similar to the self-attention mechanism. In particular, the components are updated according to the extent they each ``\textit{attend}'' to the latent variables statistically.


\item We validate the advantages of our generalization-aware regularizer in practice through experiments using various datasets (CIFAR10, CIFAR100, INTEL, and USPS) and encoder architectures (CNN4 and ResNet18). In particular, we show that our approach outperforms the popular VIB of~\citep{alemi2016deep} and the recent Category-Dependent VIB of~\citep{sefidgaran2023minimum}. The reader is referred to Section~\ref{sec:experiments} and Appendix~\ref{sec:details_exp} for details on the datasets, models, and experiments.
\end{itemize}

We emphasize once more that our approach here, which measures complexity using MDL of the involved latent variables, has two appealing features: (i) it yields generalization bounds that only depend on the encoder part of representation type statistical learning algorithms, and (ii) the employed lossy compression enables the yielded bounds to only take finite values, i.e., not vacuous, in reasonable setups, by opposition to the MI bounds of~\citep{vera2018role,kawaguchi23a}. The described approach and results must be contrasted with classes of prior-art bounds that measure complexity differently. The first class of bounds involves the complexity of the hypothesis (model) space and includes, e.g., MI-based, PAC-Bayes, and some of the compression-based bounds (e.g. \citep{arora2018stronger}). Such bounds mostly involve ``data-independent'' priors on the model; and seldom use ``data-dependent'' priors~\citep{dziugaite2018data,perez2021tighter} -- see \citep[Section~3.3]{alquier2021} for a detailed review. Generalization bounds that use model complexity do not seem to be amenable to using them for regularization since, in practice, one has only a single instance of the posterior. The second class of bounds are intrinsic dimension-based bounds that measure the complexity of the model along the optimization trajectories. Although in this approach multiple instances of the posterior are available, measuring the trajectory complexity of large models is not practical. The third class of bounds uses prediction complexity such as with f-CMI~\citep{harutyunyan2021,hellstrom2022new} - see also the related~\citep{blum2003pac,sefidgaran2023minimum}. In such bounds, typically the complexity appears in both the loss function and the regularizer; and this is generally not reasonable in practice.



\textbf{Notations.} We denote the random variables and their realizations by upper and lower case letters and use Calligraphy fonts to refer to their support set \eg $X$, $x$, and $\mathcal{X}$. The distribution of $X$ is denoted by $P_X$,\footnote{We, however, make an exception for the input data, whose distribution is denoted by $\mu$, as it is common in theoretical papers, e.g. \citep{xu2017information,Bu2020,lugosi2022generalization}.} which for simplicity, is assumed to be a \emph{probability mass function} for a random variable with discrete support set and to be \emph{probability density function} otherwise. With this assumption, the Kullback–Leibler (KL) between two distributions $P$ and $Q$ is defined as $D_{KL}\left(P\|Q\right)\coloneqq \mathbb{E}_P\left[\log(P/Q)\right]$ if $P\ll Q$, and $\infty$ otherwise. Lastly, we denote the set $\{1,\ldots,n\}$, $n\in \mathbb{N}^*$, by $[n]$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Problem Setup %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vspace{-0.2cm}
\section{Problem setup} \label{sec:setup}
% \vspace{-0.2 cm}
We consider a $C$-class classification setup, as described below.

\textbf{Data.} We assume that the \emph{input data} $Z$,  which take value % in the \emph{input space} $\mathcal{Z}$ 
according to an unknown distribution $\mu$, is composed of two parts $Z=(X,Y)$, where \textbf{(i)} $X$ represents the \emph{feature} of the input data, taking values in the \emph{feature space} $\mathcal{X}$, and \textbf{(ii)} $Y\in \mathcal{Y}$ represents the label ranging from 1 to $C$, \ie $\mathcal{Y}=[C]$. We denote the underlying distribution of $X$ and $Y$ by $\mu_X$ and $\mu_Y$, respectively, and their joint distribution by $\mu \coloneqq \mu_{X|Y} \mu_Y \coloneqq \mu_{X} \mu_{Y|X}$. 

\textbf{Training dataset.} To learn a model, we assume the availability of a \emph{training dataset} $S=\{Z_1,\ldots,Z_n\}\sim \mu^{\otimes n}\eqqcolon P_S$, composed of $n$ i.i.d. samples $Z_i=(X_i,Y_i)$ of the input data. In our analysis, we often use a  \emph{test} dataset (known also as \emph{ghost} dataset \citep{steinke2020reasoning}) $S'=\{Z'_1,\ldots, Z'_n\}\sim \mu^{\otimes n}\eqqcolon P_{S'}$, where $Z'_i=(X'_i,Y'_i)$.  To simplify the notation, we denote the features and labels of $S$ and $S'$ by $\vc{X}\coloneqq X^n\sim \mu_X^{\otimes n}$, $\vc{Y}\coloneqq Y^n \sim \mu_Y^{\otimes n}$, $\vc{X}'\coloneqq X^{\prime n} \sim \mu_X^{\otimes n}$, and $\vc{Y}'\coloneqq Y^{\prime n} \sim \mu_Y^{\otimes n}$, respectively. 

\textbf{Encoder-decoder model.} We assume that the model (hypothesis) is composed of two parts: an encoder and a decoder part. The encoder $w_{e}\in \mathcal{W}_{e}$ takes as input the feature $x$ and generates as output the \emph{representation} or the \emph{latent variable} $U \in \mathcal{U}$, possibly stochastically. For simplicity, we assume that  $\mathcal{U} = \mathbb{R}^{d}$, for some $d\in \mathbb{N}^*$. The decoder $w_d\in \mathcal{W}_{d}$ takes $U$ as input and outputs an estimate $\hat{Y}$ of the true label $Y$. The overall model is denoted by $w\coloneqq (w_e,w_d)\in \mathcal{W} = \mathcal{W}_e \times \mathcal{W}_d$. The setup is shown in Fig.~\ref{fig:setup}.

\textbf{Learning algorithm.} We consider a general stochastic learning framework in which the learning algorithm $\mathcal{A}\colon \mathcal{Z}^n \to \mathcal{W}$ has access to a training dataset $S$ and uses it to choose a model (or hypothesis) $\mathcal{A}(S)=W\in \mathcal{W}$, where $W=(W_e,W_d)$. The distribution induced by the learning algorithm $\mathcal{A}$ is denoted by $P_{W|S}=P_{W_e,W_d|S}$. Also, the joint distribution of $(S,W)$ is denoted by $P_{S,W}$ and the marginal distribution of $W$ under this distribution is denoted by $P_W$. Furthermore, we denote the induced conditional distribution of the latent variable $U$ given the encoder and the input by $P_{U|X,W_e}$. Finally, we denote the conditional distribution of the model's prediction $\hat{Y}$, conditioned on the decoder 
and the latent variable, by $P_{\hat{Y}|U,W_d}$. It is easy to see that $  P_{\hat{Y}|X,W} = \mathbb{E}_{U\sim P_{U|X,W_e}}\big[ P_{\hat{Y}|U,W_d}\big]$. Lastly and as a general rule, we use the following shorthand notation 
\begin{align}
    P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \coloneq \bigotimes\nolimits_{i\in[n]} \left\{P_{U_i|X_i,W_e} P_{U'_i|X'_i,W_e}\right\}.
\end{align}

Similar notation is used to shorten products of distributions, \eg $P_{\vc{U}|\vc{X},W_e}$ and $P_{\hat{\vc{Y}}|\vc{X},W}$.
%, $P_{\vc{U}'|\vc{X}',W_e}$, $P_{\hat{\vc{Y}}|\vc{X},W}$, and $P_{\hat{\vc{Y}}'|\vc{X}',W}$. 

\textbf{Risks.} The quality of a model $w$ is assessed by the below 0-1 loss function $\ell\colon \mathcal{Z} \times \mathcal{W} \to \{0,1\}$:
\begin{equation}
    \ell(z,w) \coloneqq \mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|x,w}} [\mathbbm{1}_{\{y \neq \hat{Y}\}}] = \mathbb{E}_{U\sim P_{U|x,w_e}}\mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|U,w_d}} \left[\mathbbm{1}_{\{y \neq \hat{Y}\}}\right].
\end{equation}

In learning theory, the ultimate goal is to find a model that minimizes the \emph{population risk}, defined as $\mathcal{L}(w)\coloneqq\mathbb{E}_{Z\sim \mu}\left[\ell(Z,w)\right]$. However, since the underlying distribution $\mu$ is unknown, only the \emph{empirical risk}, defined as $\hat{\mathcal{L}}(s,w)\coloneqq\frac{1}{n}\sum_{i\in[n]}\ell(z_i,w)$, is accessible and can be minimized. Therefore, a central question in learning theory and this paper is to control the difference between these two risks,  known as \emph{generalization error}: 
\begin{align}
    \gen(s,w) \coloneqq \mathcal{L}(w) - \hat{\mathcal{L}}(s,w).
\end{align}
In our results, for simplicity, we also use the following shorthand notations:
\begin{align}
   \mathcal{\hat{L}}(\vc{y},\vc{
   \hat{y}}) \coloneqq  \frac{1}{n}\sum\nolimits_{i\in[n]} \mathbbm{1}_{\{\hat{y}_i\neq y_i\}},&\quad \quad
   \mathcal{\hat{L}}(\vc{y}',\vc{
   \hat{y}}') \coloneqq  \frac{1}{n}\sum\nolimits_{i\in[n]} 
   \mathbbm{1}_{\{\hat{y}'_i\neq y'_i\}},
\end{align}
Note that 
\begin{align}
 \mathcal{\hat{L}}(s,w)=\mathbb{E}_{\hat{\vc{Y}}\sim  P_{\vc{\hat{Y}}|\vc{x},w}}\left[\mathcal{\hat{L}}(\vc{y},\vc{\hat{Y}})\right],\quad \quad \mathcal{\hat{L}}(s',w)=\mathbb{E}_{\hat{\vc{Y}}'\sim  P_{\vc{\hat{Y}}'|\vc{x}',w}}\left[\mathcal{\hat{L}}(\vc{y}',\vc{\hat{Y}}')\right].
\end{align}

\textbf{Symmetric prior.}  Our results are stated in terms of the KL-divergence between a posterior (\eg $ P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e}$) and a prior $\vc{Q}$ that needs to satisfy some symmetry property.

\begin{definition}[Symmetric prior] \label{def:symmetry} A conditional prior $\vc{Q}(U^{2n}|Y^{2n},X^{2n},W_e)$ is said to be symmetric if $\vc{Q}(U^{2n}_{\pi}|Y^{2n},X^{2n},W_e)$ is invariant under all permutations $\pi\colon [2n]\mapsto [2n]$ for which $\forall i \colon Y_{i}{=}Y_{\pi(i)}$.
\end{definition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Generalization Bound - Expectation%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Generalization bounds for representation learning algorithms} \label{sec:Bounds}
In this section, we establish novel in-expectation and tail bounds on the generalization error of representation learning algorithms for the setup of Fig.~\ref{fig:setup}.

\subsection{In-expectation bound}

Define the function $h_D\colon [0,1]\times[0,1]\to [0,2]$ as
\begin{equation*}
    h_D(x_1,x_2) \coloneqq 2h_b\Big(\frac{x_1+x_2}{2}\Big)-h_b(x_1)-h_b(x_2),
\end{equation*}
where $h_b(x) = -x\log_2(x)-(1-x)\log_2(1-x)$ is the binary Shannon entropy function. It is easy to see that $h_D(x_1,x_2)/2$ equals the Jensen-Shannon
divergence between two binary Bernoulli distributions with parameters $x_1 \in [0,1]$ and $x_2 \in [0,1]$. Also, let the function $h_C\colon [0,1] \times [0,1]\times \mathbb{R}^+ \to \mathbb{R}^+$ be defined as
\begin{equation}
h_{C}(x_1,x_2;\epsilon) \coloneqq \max_{\epsilon' } \Big\{h_b(x_{1\land 2}+\epsilon')-h_b(x_{1\land 2})+h_b(x_{x_{1\lor 2}}-\epsilon')-h_b(x_{x_{1\lor 2}})\Big\},
\label{definition-function-hC}
\end{equation}
where $x_{1\land 2}=\min(x_1,x_2)$, $x_{1\lor 2}=\max(x_1,x_2)$, and  the maximization in ~\eqref{definition-function-hC} is over all  
\begin{equation}
\epsilon' \in \Big[0,\min\Big(\epsilon,\frac{x_{1\lor 2}-x_{1\land 2}}{2}\Big)\Big].    
\end{equation}
Hereafter we sometimes use the handy notation 
\begin{equation}
       h_{\vc{y},\vc{y}',\vc{
   \hat{y}},\vc{\hat{y}}'}(\epsilon)\coloneqq h_C\left( \mathcal{\hat{L}}(\vc{y},\vc{\hat{y}}),\mathcal{\hat{L}}(\vc{y}',\vc{
   \hat{y}}') ;\epsilon\right).
\end{equation}

Now, we state our in-expectation generalization bound for representation learning algorithms.

\begin{theorem}\label{th:generalizationExp_hd} Consider a $C$-class  classification problem and a learning algorithm $\mathcal{A}\colon \mathcal{Z}^n\to \mathcal{W}$ that induces the joint distribution $(S',S,W,\vc{U},\vc{U'},\hat{\vc{Y}},\hat{\vc{Y}}') \sim P_{S'} P_{S,W}   P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e}   P_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{U},\vc{U}',W_d}$. Then, for any symmetric conditional distribution $\vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e)$ and for $n\geq 10$, we have
\begin{align}    \mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[h_D\Big(&\mathcal{\hat{L}}(\vc{Y}',\vc{
\hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{\hat{Y}})\Big) \Big] \leq \nonumber \\
&\frac{ \,\textnormal{MDL}(\vc{Q})+\log(n)}{n}+\mathbb{E}_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right], \label{eq:bound_hd}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively, and 
\begin{align}
  \textnormal{MDL}(\vc{Q}) \coloneqq   \mathbb{E}_{S,S',W_e} \big[ D_{KL}\big(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \big) \big]. \label{eq:MDL_original}
\end{align}
\end{theorem}
The proof of Theorem~\ref{th:generalizationExp_hd}, which appears in Appendix~\ref{pr:generalizationExp_hd}, consists of two main proof steps, a change of measure argument followed by the computation of a moment generation function (MGF). Specifically, we use the Donsker-Varadhan's lemma \citep[Lemma~2.1]{donsker1975asymptotic} to change the distribution of the latent variables from $P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e}$ to $\vc{Q}$. This change in measure results in a penalty term equal to $\text{MDL}(\vc{Q})$. Let $f$ be given by $n$ times the difference of $h_D$ and the term on the right-hand-side (RHS) of~\eqref{eq:bound_hd} , i.e., $f=n(h_D-\text{RHS}\eqref{eq:bound_hd})$. We apply the Donsker-Varadhan change of measure on the function $f$, in sharp contrast with related proofs in MI-based bounds literature~\citep{xu2017information,steinke2020reasoning,alquier2021}. The second step consists of bounding the MGF of $nf$. For every label $c\in[C]$, let $\mathcal{B}_c$ denote the set of those samples of $S$ and $S'$ that have label $c$. By construction, any arbitrary reshuffling of the latent variables associated with the samples in the set $\mathcal{B}_c$ preserves the labels. In addition, such reshuffling does not change the value of the symmetric prior $\vc{Q}$. The rest of the proof consists of judiciously bounding the MGF of $nf$ under the uniform distribution induced by such reshuffles.

It is easy to see that the left hand side (LHS) of~\eqref{eq:bound_hd} is related to the expected generalization error. For instance, since by \citep[Lemma~1]{sefidgaran2023minimum} the function $h_D(x_1,x_2)$ is convex in both arguments, $h_D(x_1,0)\geq x_1$, and $h_D(x_1,x_2)\geq (x_1-x_2)^2$ for $x_1,x_2 \in [0,1]$, one has that 
\begin{equation*}
\mathbb{E}_{\vc{S},W}\big[\gen(S,W)\big]\leq \mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\big[h_D\big(\mathcal{\hat{L}}(\vc{Y}',\vc{
   \hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{
   \hat{Y}})\big)\big],    
\end{equation*}
and
\begin{equation*}
\mathbb{E}_{\vc{S},W}\big[\gen(S,W)\big]^2\leq \mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\big[h_D\big(\mathcal{\hat{L}}(\vc{Y}',\vc{
   \hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{
   \hat{Y}})\big)\big],    
\end{equation*}
for the ``realizable'' and ``unrealizable'' cases, respectively. 

Several remarks are now in order. First, note that the generalization gap bound of Theorem~\ref{th:generalizationExp_hd}  does \textit{not} depend on the classification head; it only depends on the encoder part! In particular, this offers a theoretical justification of the intuition that in representation-type neural architectures the main goal of the encoder part is to seek a good generalization capability whereas the main goal of the decoder part is to seek to minimize the empirical risk. Also, it allows the design of regularizers that depend only on the encoder, namely the complexity of the latent variables, as we will elaborate on thoroughly in the next section. (2) The dominant term of the RHS of~\eqref{eq:bound_hd} is $\textnormal{MDL}(\vc{Q})/n$.  This can be seen by noticing that the total variation term $\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1$ is of the order $\sqrt{C/n}$ as shown in~\citep[Theorem~2]{berend2012convergence}; and, hence, the residual 
\begin{equation}
    B_{\text{emp\_diff}} \coloneqq \mathbb{E}_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\bigg[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\bigg(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\bigg)\bigg],
    \label{residual-term-RHS-bound-theorem1}
   \end{equation}
is small for large $n$ (see below for additional numerical justification of this statement). (3) The term $\textnormal{MDL}(\vc{Q})$, as given by~\eqref{eq:MDL_original}, expresses the average (w.r.t. data and training stochasticity) of KL-divergence terms of the form $D_{KL}(\mathbf{P} \| \mathbf{Q})$ where $\mathbf{P}$ is the distribution of the representation in the training samples $n$ and the test samples $n$ conditioned on the features of the $2n$ examples for a given encoder, while $\mathbf{Q}$ is a fixed symmetric prior distribution for representations given $2n$ samples for the given encoder. As stated in Definition~\ref{def:symmetry}, $\mathbf{Q}$ is symmetric  for any permutation $\pi$; and, in a sense, this means that $\mathbf{Q}$ induces a distribution on $(\mathbf{U},\mathbf{U'})$ conditionally given $(\mathbf{Y},\mathbf{Y'},\mathbf{X},\mathbf{X'},W_e)$ that is invariant under all permutations that preserve the labels of training and ghost samples. (4) The minimum description length of the representations arguably reflects the encoder's ``structure'' and ``simplicity''~\citep{sefidgaran2023minimum}. In contrast, mutual information (MI) type bounds and regularizers, used, e.g., in the now popular IB method, are known to fall short of doing so~\citep{geiger2021information,amjad2019learning,rodriguez2019information,dubois2020learning,lyu2023recognizable}. In fact, as mentioned in these works, most existing theoretical MI-based generalization bounds (\eg \citep{vera2018role,kawaguchi23a}) become vacuous in reasonable setups. In addition, no consistent relation between the generalization error and MI has been reported experimentally so far. Therefore, MDL is a better indicator of the generalization error than the mutual information used in the IB principle. 

As we already mentioned, the total variation $\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1$ is of the order $\sqrt{C/n}$~\citep[Theorem~2]{berend2012convergence}; and for this reason, the second term on the RHS of~\eqref{eq:bound_hd} is negligible in practice. Figure~\ref{fig:h_C} shows the values of the term inside the expectation of $B_{\text{emp\_diff}}$ as given by~\eqref{residual-term-RHS-bound-theorem1} for the CIFAR10 dataset for various values of the generalization error. The values are obtained for empirical risk of $0.05$ and $\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1$ set to be of the order $\sqrt{C/n}$. As it is visible from the figure, the term inside the expectation of $B_{\text{emp\_diff}}$ is the order of magnitude smaller than the generalization error. This illustrates that even for settings with moderate dataset size such as CIFAR, the generalization bound of Theorem~\ref{th:generalizationExp_hd} is mainly dominated by $\textnormal{MDL}(\vc{Q})/n$. 

As stated in the Introduction section, generalization bounds for the representation learning setup of Fig.~\ref{fig:setup} are rather scarce; and, to the best of our knowledge, the only non-vacuous existing in-expectation bound was provided recently in~\citep[Theorem~4]{sefidgaran2023minimum}. This bound states that
% \vspace{-0.1 cm}
\begin{equation}
    \mathbb{E}_{\vc{S},W}\left[\gen(S,W)\right] \leq \sqrt{\frac{ 2\,\textnormal{MDL}(\vc{Q})+C+2}{n}},
    \label{in-expectation-bound-sefidgaran-al-2023}
\end{equation}
where $C$ is the number of classes.
\begin{itemize}[leftmargin=*,topsep=0pt]
       \item[i.] Investigating~\eqref{eq:bound_hd} and~\eqref{in-expectation-bound-sefidgaran-al-2023}, it is easy to see that, order-wise,  while the bound of 
       ~\citep[Theorem~4]{sefidgaran2023minimum} evolves as $\mathcal{O}\left(\sqrt{\textnormal{MDL}(\vc{Q})/n}\right)$ our bound of Theorem~\ref{th:generalizationExp_hd} is tighter comparatively and it evolves approximately as 
 $\mathcal{O}\left(\textnormal{MDL}(\vc{Q})/n\right)$ for realizable setups with large $n$ (i.e., for most settings in practice).
   \item[ii.] Figure~\ref{fig:bounds} depicts the evolution of both bounds as a function of $\text{MDL}(\vc{Q})/n$ for the CIFAR10 dataset and for different values of the empirical risk. %when the empirical risk is set to $0.05$. 
   It is important to emphasize that, in doing so, we account for the contribution of all terms of the RHS of~\eqref{eq:bound_hd}, including the residual $B_{\text{emp\_diff}}$ which is then \textit{not} neglected. As is clearly visible from the figure, our bound of Theorem~\ref{th:generalizationExp_hd} is tighter comparatively. Also, the advantage over~\eqref{in-expectation-bound-sefidgaran-al-2023} becomes larger for smaller values of the empirical risk and larger values of $\text{MDL}(\vc{Q})/n$.
   \end{itemize}  

\begin{figure}
    \centering
    \begin{minipage}{0.48\textwidth}
    \vspace{0.05 cm}
        \includegraphics[width=\textwidth]{Fig/hC_gen_bigger.png}
    \caption{Values of $h_C\left( \mathcal{\hat{L}}(\vc{y},\vc{
   \hat{y}}),\mathcal{\hat{L}}(\vc{y}',\vc{
   \hat{y}}') ;\epsilon\right)$ for various values of the generalization error for the CIFAR10 dataset.}
    \label{fig:h_C}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.48\textwidth}
      % \includegraphics[width=\textwidth]{Fig/gen_bounds.png}
      \vspace{0.15 cm}
      \includegraphics[width=0.95\textwidth]{Fig/gen_bounds_multiple.png}
    \caption{Comparison of the generalization bounds of Theorem~\ref{th:generalizationExp_hd} (for various values of $\hat{\mathcal{L}}(S,W)$) and \citep[Theorem~4]{sefidgaran2023minimum} for the CIFAR10 dataset.}
    \label{fig:bounds}
    \end{minipage}\hfill
\end{figure}


\subsection{Tail bound}

The following theorem provides a probability tail bound on the generalization error of the representation learning setup of Fig.~\ref{fig:setup}.

\begin{theorem}\label{th:generalization_tail_hd_exp} Consider the setup of Theorem~\ref{th:generalizationExp_hd} and consider some symmetric conditional distribution $\vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e)$. Then, for any $\delta \geq 0$ and for $n\geq 10$, with probability at least $1-\delta$ over choices of $(S,S',W)$, it holds that

\begin{align}    
h_D\left(\mathcal{\hat{L}}(S',W),\mathcal{\hat{L}}(S,W)\right) \leq  &\frac{ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right)
     +\log(n/\delta)}{n}\nonumber\\
&+\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right], \label{eq:tail_hd}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively.
\end{theorem}

The proof of Theorem~\ref{th:generalization_tail_hd_exp} appears in Appendix~\ref{pr:generalization_tail_hd_exp}.

\subsection{Lossy generalization bounds} \label{sec:lossy}
The bounds of the previous section can be regarded as lossless versions of ones that are more general, and which we refer to as \textit{lossy} bounds. The lossy bounds are rather easy extensions of the corresponding lossless versions, but they have the advantage of being guaranteed to stay non-vacuous even when the encoder is set to be deterministic. Also, such bounds are useful to explain the empirically observed \emph{geometrical compression} phenomenon \citep{geiger2021information}. For comparison, MI-based bounds, such as Xu-Raginsky \citep{xu2017information} are known to suffer both shortcomings \citep{haghifam2023limitations,livni2023information}. The aforementioned shortcomings have been shown that can be addressed using the lossy approach \citep{Sefidgaran2022,sefidgaran2024data}. For the sake of brevity, in the rest of this section we only illustrate how the bound~\eqref{in-expectation-bound-sefidgaran-al-2023} can be extended to a corresponding lossy one.   Let $\hat{W}_e\in \mathcal{W}_e$ be any quantized model defined by $P_{\hat{W}_e|S}$, that satisfy the \emph{distortion} criterion $ \mathbb{E}_{P_{S,W}P_{\hat{W}_e|S}}\left[\gen(S,W)-\gen(S,\hat{W})\right] \leq \epsilon$, where $\hat{W}=(\hat{W}_e,W_d)$. Then, we get 
\begin{equation}
    \mathbb{E}_{\vc{S},W}\left[\gen(S,W)\right] \leq \sqrt{\frac{ 2\,\textnormal{MDL}(\vc{Q})+C+2}{n}}+\epsilon, \label{eq:lossy_bound}
\end{equation}
where now $\text{MDL}(\vc{Q})$ is considered for the quantized encoder, \ie 
\begin{equation}
  \textnormal{MDL}(\vc{Q}) \coloneqq   \mathbb{E}_{S,S',\hat{W}_e} \Big[ D_{KL}\Big(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',\hat{W}_e} \big\| \vc{Q}(\vc{U},\vc{U'}|S,S',\hat{W}_e) \
     \Big) \Big]. \label{eq:MDL_original_quantized}
\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Regularizer %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Regularization using data-dependent Gaussian mixture priors} \label{sec:GM_centeralized}
Theorems~\ref{th:generalizationExp_hd} and \ref{th:generalization_tail_hd_exp} essentially mean that if for a given learning algorithm the minimum description length $\textnormal{MDL}(\vc{Q})$ is small, then the algorithm is guaranteed to generalize well. Hence, it is natural to use the term $\textnormal{MDL}(\vc{Q})$ as a suitable regularizer. The question of the choice of the prior $\vc{Q}$ is pivotal for this. In this section, we propose an effective method to simultaneously find a data-dependent $\vc{Q}$ and use it to build a suitable regularizer term along the optimization iterations.

We assume that for a given input $x$ the encoder outputs the mean $\mu_x \in \mathbb{R}^{d}$ and standard deviation $\sigma_x \in \mathbb{R}^{d}$. Also, we assume that the latent variable $U$ is distributed according to a multivariate Gaussian distribution with a diagonal covariance matrix, \ie $U \sim \mathcal{N}\big(\mu_x,\diag(\sigma_x^2)\big)$ where $\diag(\sigma_x^2)$ denotes a $d\times d$ diagonal matrix with diagonal elements $\sigma_x^2$. With this assumption, we have
\begin{equation*}
    P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} = \bigotimes\nolimits_{i\in[n]} \Big\{\mathcal{N}\big(\mu_{x_i},\diag(\sigma_{x_i}^2)\big)\mathcal{N}\big(\mu_{x'_i},\diag(\sigma_{x'_i}^2)\big)\Big\}.
\end{equation*}
In our approach, we model the prior $\vc{Q}$ as a suitable \emph{Gaussian mixture}, with the mixture coefficients chosen judiciously in a manner that is training-data dependent and along the optimization iterations. The rationale for this choice is two-fold: (i) The Gaussian mixture distribution is known to possibly approximate well enough any arbitrary distribution provided that the number of mixture components is sufficiently large \citep{dalal1983approximating,goodfellow2016deep} (see also \citep[Theorem~1]{nguyen2022improving}); and (ii) given distributions $\{p_{i}\}_{i\in[N]}$, the distribution $q$ that minimizes $\sum_{i\in[N]} D_{KL}(p_i\|q)$ is $q=\frac{1}{N}\sum_{i\in[N]}p_i$. Thus, if all distributions $p_{i}$ are Gaussian, the minimizer is a Gaussian mixture. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Regularizer-GM-Single-view %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Let, for $c\in[C]$, $Q_c$ denote the data-dependent Gaussian mixture prior $Q_c$ for label $c$. Also, let $\vc{Q}(\vc{U},\vc{U'}|S,S',\hat{W}_e)=\prod_{i\in[n]} Q_{Y_i}(U_i)Q_{Y'_i}(U'_i)$. It is easy to see that this prior satisfies the symmetry property of Definition~\ref{def:symmetry}. In what follows, we explain how the priors $\{Q_c\}$ are chosen and updated along the optimization iterations. As it will become clearer, our method is somewhat reminiscent of the expectation-maximization (EM) algorithm for finding Gaussian mixture priors that maximize the log-likelihood,  but with notable major differences: \textbf{(i)} In our case the prior must be learned along the optimization iterations with the underlying distribution of the latent variables possibly changing at every iteration.  \textbf{(ii)} The Gaussian mixture prior is intended to be used in a regularizer term, not to maximize the log-likelihood; and, hence, the approach must be adapted accordingly. \textbf{(iii)} Unlike the usual scenario where the goal is to find an appropriate Gaussian mixture given a set of points, here we are given a set of distributions \ie $\mathcal{N}\left(\mu_{x_i},\diag(\sigma_{x_i}^2)\right)$ that generate such points. \textbf{(iv)} The found prior must satisfy (at least partially)\footnote{While the bounds of Theorems~\ref{th:generalizationExp_hd} and \ref{th:generalization_tail_hd_exp} require the prior $\vc{Q}$ to satisfy the exact symmetry of Definition~\ref{def:symmetry}, it can be shown that these bounds still hold (with a small penalty) if such exact symmetry requirement is relaxed partially. The reader is referred to Appendix~\ref{sec:partial_symmetric}, where formal results and their proofs are provided for the case of ``almost symmetric'' priors.} certain ``symmetry'' properties.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Lossless-GM %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lossless Gaussian mixture prior} \label{sec:lossless_brief}
For each label $c\in[C]$, we let the prior $Q_c$ to be defined as
\begin{equation}
   Q_{c} = \sum\nolimits_{m\in[M]} \alpha_{c,m}\, Q_{c,m},
\end{equation}
over $\mathbb{R}^d$, where $\alpha_{c,m} \in [0,1]$, $\sum_{m\in[M]} \alpha_{c,m}=1$ for each $c\in[C]$, and where $\{Q_{c,m}\}_{c,m}$ are multivariate Gaussian distributions with a diagonal covariance matrix:
\begin{equation*}
Q_{c,m}=\mathcal{N}\left(\mu_{c,m},\diag\left(\sigma_{c,m}^2\right)\right),\quad m\in[M], c\in [C].
\end{equation*}
With the above prior choice, the regularizer term simplifies as $\sum\nolimits_{i\in[b]} D_{KL}\big(P_{U_i|X_i,W_e}\big\| Q_{Y_i} \big)$. However, since the KL-divergence between a Gaussian and a Gaussian mixture distributions does not have a closed-form expression, we estimate it using a slightly adapted method from \citep{hershey2007approximating}. Our estimate is an average of the upper and lower bounds of the KL-divergence, denoted as $D_{\text{var}}$ and $D_{\text{prod}}$. Please refer to Appendix~\ref{sec:KL_estimation} for more details on this estimation. For better readability, we present the approximation of the KL-divergence by its upper bound $D_{\text{var}}$ in the main part of this paper and we refer the reader to Appendix~\ref{sec:average_estimate} for the approach using $\big(D_{\text{var}}+D_{\text{prod}}\big)/2$.


Finally, similar to \citep{alemi2016deep,sefidgaran2023minimum}, we consider only the part of the upper bound $\textnormal{MDL}(\vc{Q})$ corresponding to the training dataset $S$, simply because the test dataset $S'$ is not available during the training phase. With this assumption and for a mini-batch $\mathcal{B}=\{z_1,\ldots,z_b\}\subseteq S$, the regularizer term is equal to
\begin{align}
    \textnormal{Regularizer}(\vc{Q}) \coloneqq  D_{KL}\big(P_{\vc{U}_{\mathcal{B}}|\vc{X}_{\mathcal{B}},W_e} \big\| \vc{Q}_{\mathcal{B}} \
     \big), \label{eq:regularizer}
\end{align}
where the indices $\mathcal{B}$ indicate the restriction to the set $\mathcal{B}$. For better exposition, we will drop the notation dependence on $\mathcal{B}$ in the rest of this section.  Now, we are ready to explain how the Gaussian mixtures are initialized, updated, and used as a regularizer simultaneously and along the optimization iterations. In what follows, the superscript $(t)$ denotes the optimization iteration $t\in \mathbb{N}^*$.

\textbf{Initialization.} First, we initialize the priors as $Q_c^{(0)}$ by initializing $\alpha_{c,m}^{(0)}$ and the parameters $\mu_{c,m}^{(0)}$ $\sigma_{c,m}^{(0)}$ of the components $Q_{c,m}^{(0)}$, for $c\in[C], m\in[M]$, similar to the method of initializing the centers in k-means++ \citep{arthur2007k}. The reader is referred to Appendix~\ref{sec:initialization_single_view} for further details.

\textbf{Update of the priors.} Let the mini-batch picked at iteration $t$ be $\mathcal{B}^{(t)}=\{z_1^{(t)},\ldots,z_b^{(t)}\}$. By dropping the dependence on $(t)$ for better readability, the regularizer \ref{eq:regularizer}, at iteration $(t)$, can be written as  
\begin{align}
    \textnormal{Regularizer}(\vc{Q}) =& \sum\nolimits_{i\in[b]} D_{KL}\big(P_{U_i|x_i,w_e} \big\|\sum\nolimits_{m\in[M]} \alpha_{y_i,m}^{(t)} Q^{(t)}_{y_i,m}(U_i)\big) \nonumber \\
     \stackrel{(a)}{\leq}& \sum\limits_{i\in[b]} \sum\limits_{m\in[M]} \gamma_{i,m} \left(D_{KL}\big(P_{U_i|x_i,w_e} \big\| Q^{(t)}_{y_i,m}(U_i)\big)-\log\big(\alpha_{y_i,m}^{(t)}/\gamma_{i,m}\big)\right),  \label{eq:reg_lossless_single_not_simplified}
\end{align}
where the last step holds for any choices of $\gamma_{i,m}\geq 0$ such that $\sum_{m\in[M]} \gamma_{i,m} =1$, for every $i\in[b]$. To see why the step $(a)$ holds, we refer the reader to Appendix~\ref{sec:KL_estimation} to see how the variational bound $D_{var}$ is derived.

Now, to update the components of the priors, first (similar to `E'-step) note that the coefficients $\gamma_{i,m}$ that minimizes the above upper bound are equal to
\begin{align}
    \gamma_{i,m} = \frac{\alpha_{y_i,m}^{(t)} e^{-D_{KL}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m}^{(t)}\big)}}{\sum\nolimits_{m'\in [M]} \alpha_{y_i,m'}^{(t)} e^{-D_{KL}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m'}^{(t)}\big)}}, \quad i\in[b],m\in[M]. \label{eq:gamma_i}
\end{align}
Let $\gamma_{i,c,m} = \gamma_{i,m}$ if $c=y_i$ and $\gamma_{i,c,m} =0$ otherwise. Next, (similar to $M$-step) we treat $\gamma_{i,m}$ as constants, and find the parameters $\mu_{c,m}^*$, $\sigma^{*}_{c,m}$, $\alpha_{c,m}^*$ that minimizes the upper bound \eqref{eq:reg_lossless_single_not_simplified}, by simply taking the partial derivatives and equating them to zero. Simple calculations show that the closed-form solutions are
\begin{align}
    \mu_{c,m}^* =& \frac{1}{b_{c,m}}\sum\nolimits_{i\in[b]}  \gamma_{i,c,m} \mu_{x_i},  &{\sigma^{*}_{c,m,j}}^2 = \frac{1}{b_{c,m}}\sum\nolimits_{i\in[b]}  \gamma_{i,c,m} \left(\sigma_{x_i,j}^2+(\mu_{x_i,j}-\mu_{c,m,j}^{(t)})^2\right),\nonumber\\
    \alpha_{c,m}^* = & b_{c,m}/b_c, \quad &b_{c,m}=\sum\nolimits_{i\in[b]}  \gamma_{i,c,m}, \quad\quad  b_c =\sum\nolimits_{m\in[M]} b_{c,m}.\label{eq:optimal_updates_lossless_single} \hspace{1.5 cm} 
\end{align}
where $j\in[d]$ denotes the index of the coordinate in $\mathbb{R}^d$ and $\sigma_{c,m}^*=(\sigma_{c,m,1}^*,\ldots,\sigma_{c,m,d}^*)$. Finally, to reduce the dependence of the prior on the dataset and to \emph{partially} preserve the symmetry property, let
\begin{align}
    \mu_{c,m}^{(t+1)} =& (1-\eta_1) \mu_{c,m}^{(t)}+\eta_1 \mu_{c,m}^*+\mathfrak{Z}_{1}^{(t+1)}, \quad {\sigma_{c,m}^{(t+1)}}^2 = (1-\eta_2) {\sigma_{c,m}^{(t)}}^2+\eta_2 {\sigma_{c,m}^*}^2+\mathfrak{Z}_{2}^{(t+1)},\nonumber \\
    \alpha_{c,m}^{(t+1)} = &(1-\eta_3) \alpha_{c,m}^{(t)}+\eta_3 \alpha_{c,m}^*, \label{eq:updates_lossless_single}
\end{align}
where  $\eta_1,\eta_2,\eta_3 \in [0,1]$  are some fixed coefficients and $\mathfrak{Z}_{j}^{(t+1)}$, $j\in[2]$, are i.i.d. multivariate Gaussian random variables distributed as $\mathcal{N}(\vc{0}_d,\zeta_j^{(t+1)}\mathrm{I}_d)$. Here $\vc{0}_d=(0,\ldots,0)\in \mathbb{R}^d$ and $\zeta_j^{(t+1)}\in \mathbb{R}^+$ are some fixed constants.

\textbf{Regularizer.} Finally, using \eqref{eq:gamma_i}, the upper bound \eqref{eq:reg_lossless_single_not_simplified} that we use as a regularizer can be simplified as 
\begin{equation}
    - \sum\nolimits_{i\in[b]} \log\Big(\sum\nolimits_{m\in [M]} \alpha_{y_i,m}^{(t)} e^{-D_{KL}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m}^{(t)}\big)} \Big).\label{eq:reg_lossless_single}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Lossy-GM %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lossy Gaussian mixture prior} \label{sec:lossy_brief}
The lossy case is explained in Appendix~\ref{sec:lossy_single_view} when the KL-divergence estimate $\left(D_{\text{prod}}+D_{\text{var}}\right)/2$ is considered. Similar to Section~\ref{sec:lossless_brief}, it can be shown that if only $D_{\text{var}}$ is considered for the KL-divergence estimate, then the regularizer term becomes equal to
\begin{equation}
    - \sum\nolimits_{i\in[b]} \log\left(\sum\nolimits_{m\in [M]} \alpha_{y_i,m}^{(t)} e^{-D_{KL,Lossy}\big(P_{U_i|x_i,\hat{w}_e}\|Q_{y_i,m}^{(t)}\big)} \right),\label{eq:reg_lossy_single}
\end{equation}
where $D_{KL,Lossy}\big(P_{U|x,\hat{w}_e}\|Q_{y,m}\big)$ is defined as
\begin{align}
    {D_{KL}}\bigg(\mathcal{N}\bigg(\mu_x, \frac{\sqrt{d}}{2} \mathrm{I}_d\bigg) \Big\| \mathcal{N}\bigg(\mu_{c,m}, \frac{\sqrt{d}}{2}  \mathrm{I}_d\bigg)\bigg) {+} D_{KL}\Big(\mathcal{N}\big( \vc{0}_d, \diag(\sigma_x^2{+}\boldsymbol{\epsilon}) \big) \big\| \mathcal{N}\big(\vc{0}_d, \diag(\sigma_{c,m}^2{+}\boldsymbol{\epsilon}) \big)\Big),
\end{align}
where $\boldsymbol{\epsilon}=(\epsilon,\ldots,\epsilon)\in \mathbb{R}^d$ and $\epsilon\in \mathbb{R}^+$ is a fixed hyperparameter.

Furthermore the components are updated according to \eqref{eq:updates_lossless_single}, where $\gamma_{i,c,m}$,  $\mu_{c,m}^*$, and $\alpha_{c,m}^*$ are defined as before, but ${\sigma^{*}_{c,m,j}}^2 = \frac{1}{b_{c,m}}\sum\nolimits_{i\in[b]}  \gamma_{i,c,m} \sigma_{x_i,j}^2$ and $ \gamma_{i,m}$ is equal to  
\begin{align}
    \gamma_{i,m} =& \frac{\alpha_{y_i,m}^{(t)} e^{-D_{KL,Lossy}\big(P_{U_i|x_i,\hat{w}_e}\|Q_{y_i,m}^{(t)}\big)}}{\sum\nolimits_{m'\in [M]} \alpha_{y_i,m'}^{(t)} e^{-D_{KL,Lossy}\big(P_{U_i|x_i,\hat{w}_e}\|Q_{y_i,m'}^{(t)}\big)}} = \frac{\beta_{y_i,m}^{(t)} e^{\frac{\langle \mu_{x_i},\mu_{y_i,m}^{(t)}\rangle}{\sqrt{d}}}}{\sum\nolimits_{m'\in [M]} \beta_{y_i,m'}^{(t)} e^{\frac{\langle \mu_{x_i},\mu_{y_i,m'}^{(t)}\rangle}{\sqrt{d}}}}, \nonumber
\end{align}
where $\beta_{y_i,m}^{(t)} =  \alpha_{y_i,m}^{(t)} e^{-\frac{\|\mu_{y_i,m}^{(t)}\|^2}{\sqrt{d}}}e^{-\sum_{j\in[d]}(\log(\sigma_{y_i,m,j}^{(t)}/\sigma_{x_i,j})+\sigma_{x_i,j}^2/(2{\sigma_{y_i,m,j}^{(t)}}^2))}$. In cases where the means of the components are normalized and the variances are fixed, $\beta_{y_i,m}^{(t)}  \propto \alpha_{y_i,m}^{(t)}$.

The parameters $\gamma_{i,m}$ measure the contribution of the component $m$ in $Q_{y_i}$ in generating the latent variable $U_i$. One can observe a similarity between how these parameters are chosen in our approach and the attention mechanism, with the difference that here we are considering a \emph{weighted} version of this mechanism, and without key and query matrices since we do not consider projections to other spaces. Intuitively, to measure the contribution of each component, we measure how much that component ``\emph{attend}'' to $U_i$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Experiments %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments} \label{sec:experiments}
In this section, we present the results of our simulations. The reader is referred to Appendix~\ref{sec:details_exp} for additional details, including used datasets, models, and training hyperparameters. 

For the experiments, we considered the lossy regularizer approach with a Gaussian mixture prior and the KL-divergence estimate of $\left(D_{\text{prod}}+D_{\text{var}}\right)/2$, as detailed in Appendix~\ref{sec:lossy_single_view}. In this section, we refer to our regularizer as \emph{Gaussian mixture MDL} (GM-MDL). To verify the practical benefits of the introduced regularizer, we conducted several experiments considering different datasets and encoder architectures as summarized below and detailed in Appendix~\ref{sec:details_exp}:
\begin{itemize}    
\item \textbf{Datasets:} CIFAR10, CIFAR100, INTEL, and USPS image classification,   
\item \textbf{Encoder architectures:} CNN4 and ResNet18. \end{itemize}

To compare our approach with the previous literature, in addition to the no-regularizer case, we also considered the Variational Information Bottleneck (VIB) of \citep{alemi2016deep} and the Category-dependent VIB (CDVIB) of \citep{sefidgaran2023minimum}.
\begin{figure}[t!]
  \centering
    % \vspace{-0.5 cm}
  \includegraphics[height=5 cm]{Fig/accuracy.png}
  \captionof{figure}{Test performance of the CNN-based encoder trained on CIFAR10 using standard VIB \citep{alemi2016deep} regularization, Category-dependent VIB (CDVIB) \citep{sefidgaran2023minimum} regularization, and our proposed Gaussian Mixture MDL (GM-MDL) regularization.
  }
  \label{fig:accuracy_single}
\end{figure}

The results presented in Fig.~\ref{fig:accuracy_single} and Table~\ref{tab:accuracy_single} clearly show the practical advantages of our proposed approach. All experiments are run independently 5 times and the reported values and plots are the average over 5 runs. In Fig.\ref{fig:accuracy_single}, we plotted the performance of different regularizers as a function of the trade-off regularization parameter $\beta$. In Table~\ref{tab:accuracy_single}, we reported the best achieved average test accuracy for each regularizer.



\begin{table}[ht!]
 \renewcommand{\arraystretch}{1.5}
\caption{Test performance of representation learning models with different encoder architectures, and trained on selected datasets using VIB \citep{alemi2016deep}, Category-dependent VIB (CDVIB) \citep{sefidgaran2023minimum}, and our proposed Gaussian Mixture MDL (GM-MDL).}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\# & Encoder  & Dataset & no reg. & VIB   & CDVIB & GM-MDL \\ \hline
1  & CNN4     & CIFAR10 & 0.612          & 0.626 & 0.649 & \textbf{0.681}           \\ \hline
2  & CNN4     & USPS   &  0.948 & 0.952  & 0.955 & \textbf{0.963}  \\        \hline
3  & CNN4     & INTEL   &      0.756 & 0.759 & 0.763 & \textbf{0.776}           \\ \hline
4  & ResNet18 & CIFAR10 & 0.824          & 0.829 & 0.835 & \textbf{0.848}           \\ \hline
5  & ResNet18 & CIFAR100 &     0.454   & 0.458 & 0.463 & \textbf{0.497}           \\ \hline
\end{tabular}
\label{tab:accuracy_single}
\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{abbrvnat}
\bibliography{biblio}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% Appendices %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\begin{center}
    {\huge \textbf{Appendices}}
\end{center}
The appendices are organized as follows:

\begin{itemize}
\item In Appendix~\ref{sec:lossy_intuition}, we provide the intuition behind the lossy generalization bounds and we present an extension of Theorem~\ref{th:generalizationExp_hd} to lossy compression settings.
\item In Appendix~\ref{sec:partial_symmetric}, we show how the established generalization bounds of this work can be extended to cases where the prior violates the symmetry condition.
\item In Appendix~\ref{sec:average_estimate}, we explain in detail our approach to finding the Gaussian mixture priors and how to use them in a regularizer term.  This subsection is further divided into three parts, describing 
\begin{itemize}
    \item our initialization method in Appendix~\ref{sec:initialization_single_view},
    \item the lossless approach in Appendix~\ref{sec:lossless_single_view},
    \item and the lossy approach in Appendix~\ref{sec:lossy_single_view}.
\end{itemize}
        \item In Appendix~\ref{sec:limitations_futur_works}, we discuss the potential future directions.
     \item Appendix~\ref{sec:details_exp} explains the details of our experiments.
    \item Appendix~\ref{sec:KL_estimation} contains the used approximation method for the KL divergence between a Gaussian distribution and a Gaussian mixture distribution, and also between two Gaussian mixture distributions.
   \item Finally, the deferred proofs are presented in Appendix~\ref{sec:proofs}.
    \end{itemize}

\appendix


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% Further discussions %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\section{Tail bound on the generalization error of representation learning algorithms} \label{sec:tail}
In this section, we propose a novel tail bound on the generalization error of representation learning algorithms.

\begin{theorem}\label{th:generalization_tail_hd_exp} Consider the setup of Theorem~\ref{th:generalizationExp_old} and consider some symmetric conditional distribution $\vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e)$. Then, for any $\delta \geq 0$ and for $n\geq 10$, with probability at least $1-\delta$ over choices of $(S,S',W)$, we have
\begin{align}    
h_D\left(\mathcal{\hat{L}}(S',W),\mathcal{\hat{L}}(S,W)\right) \leq  &\frac{ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right)
     +\log(n/\delta)}{n}\nonumber\\
+&\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right], \label{eq:tail_hd}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively.
\end{theorem}
This result is proved in Appendix~\ref{pr:generalization_tail_hd_exp}. 

Similar to Theorems~\ref{th:generalizationExp_old} and \ref{th:generalizationExp_hd}, this result shows that minimizing $\text{MDL}(\vc{Q})$ (more precisely $D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right)$) can result in a good generalization performance.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Multi-task representation learning algorithms %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalization bound for the Multi-task representation learning algorithms} \label{sec:multi_task}



In representation learning, the extracted representations will often be used for multiple tasks. Hence, in such situations, it is desirable to guarantee a good generalization performance in all of those tasks. In this section, we show how our results can be extended to include such cases. For this, we start by stating the problem setup for this $L$ parallel classification task.


\vspace{5 cm}
\textbf{Data.} We assume that $Z=(X,Y^L)$, where $X\in \mathcal{X}$ is the feature, similar as before, while $Y^L=(Y_1,\ldots,Y_L)$, where $Y_l\in \mathcal{Y}_l$ represents the label for task $l\in[L]$, ranging from 1 to $C_l$, \ie $\mathcal{Y}=[C_l]$. We denote the underlying distribution of $X$ and $Y^L$ by $\mu_X$ and $\mu_{Y^L}=\mu_{Y_1,\ldots,Y_L}$, respectively, and their joint distribution by $\mu \coloneqq \mu_{X|Y^L} \mu_{Y^L} \coloneqq \mu_{X} \mu_{Y^L|X}$. 

\textbf{Training dataset.} To learn a model, we assume the availability of a \emph{training dataset} $S=\{Z_1,\ldots,Z_n\}\sim \mu^{\otimes n}\eqqcolon P_S$, composed of $n$ i.i.d. samples $Z_i=(X_i,Y^L_i)$ of the input data, where $Y^L_i=(Y_{i,1},\ldots,Y_{i,L})$. The test dataset $S'$ is defined similarly. To simplify the notation, we denote the features and labels of $S$ and $S'$ by $\vc{X}\coloneqq X^n\sim \mu_X^{\otimes n}$, $\vc{Y}^L\coloneqq (Y^L)^n \sim \mu_Y^{\otimes nL}$, $\vc{X}'\coloneqq X^{\prime n} \sim \mu_X^{\otimes n}$, and $Y^{L\prime}\coloneqq Y^{\prime n} \sim \mu_Y^{\otimes n}$, respectively. 

\textbf{Encoder-decoder model.} We assume that the model (hypothesis) is composed of two parts: an encoder and a decoder part. The encoder $w_{e}\in \mathcal{W}_{e}$ takes as input the feature $x$ and generates as output the \emph{representation} or the \emph{latent variable} $U \in \mathcal{U}$, possibly stochastically. For simplicity, we assume that $\mathcal{U} = \mathbb{R}^{d}$, for some $d\in \mathbb{N}^*$. The decoder $w_d\in \mathcal{W}_{d}$ takes $U$ as input and outputs an estimate $\hat{Y}$ of the true label $Y$. The overall model is denoted by $w\coloneqq (w_e,w_d)\in \mathcal{W} = \mathcal{W}_e \times \mathcal{W}_d$. The setup is shown in Fig.~\ref{fig:setup}.


%$\mu_{X_1 | Y}$
%$\mu_{X_K | Y}$
%$W=(W_{e,1}, ..., W_{e,K},W_{d})$
%$P_{U_1|X_1,W_{e,1}}$
%$P_{U_K|X_K,W_{e,K}}$
%$U_1$ $U_K$ $X_1$ $X_K$ $\hat{Y}$ $S$ $Y$
%$P_{\hat{Y}|U_1, ..., U_K,W_{d}}$

\textbf{Learning algorithm.} We consider a general stochastic learning framework in which the learning algorithm $\mathcal{A}\colon \mathcal{Z}^n \to \mathcal{W}$ has access to a training dataset $S$ and uses it to choose a model (or hypothesis) $\mathcal{A}(S)=W\in \mathcal{W}$, where $W=(W_e,W_d)$. The distribution induced by the learning algorithm $\mathcal{A}$ is denoted by $P_{W|S}=P_{W_e,W_d|S}$. Also, the joint distribution of $(S,W)$ is denoted by $P_{S,W}$ and the marginal distribution of $W$ under this distribution is denoted by $P_W$. Furthermore, we denote the induced conditional distribution of the latent variable $U$ given the encoder and the input by $P_{U|X,W_e}$. Finally, we denote the conditional distribution of the model's prediction $\hat{Y}$, conditioned on the decoder 
and the latent variable, by $P_{\hat{Y}|U,W_d}$. It is easy to see that $  P_{\hat{Y}|X,W} = \mathbb{E}_{U\sim P_{U|X,W_e}}\big[ P_{\hat{Y}|U,W_d}\big]$. Lastly, and as a general rule, we use the following shorthand notation 
\begin{align}
    P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \coloneq \bigotimes\nolimits_{i\in[n]} \left\{P_{U_i|X_i,W_e} P_{U'_i|X'_i,W_e}\right\}.
\end{align}
Similar notation is used to shorten products of distributions, \eg $P_{\vc{U}|\vc{X},W_e}$ and $P_{\hat{\vc{Y}}|\vc{X},W}$.
%, $P_{\vc{U}'|\vc{X}',W_e}$, $P_{\hat{\vc{Y}}|\vc{X},W}$, and $P_{\hat{\vc{Y}}'|\vc{X}',W}$. 

\textbf{Risks.} The quality of a model $w$ is assessed by the below 0-1 loss function $\ell\colon \mathcal{Z} \times \mathcal{W} \to \{0,1\}$:
\begin{align}
    \ell(z,w) \coloneqq \mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|x,w}} [\mathbbm{1}_{\{y \neq \hat{Y}\}}] = \mathbb{E}_{U\sim P_{U|x,w_e}}\mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|U,w_d}} \left[\mathbbm{1}_{\{y \neq \hat{Y}\}}\right].
\end{align}
In learning theory, the ultimate goal is to find a model that minimizes the \emph{population risk}, defined as $\mathcal{L}(w)\coloneqq\mathbb{E}_{Z\sim \mu}\left[\ell(Z,w)\right]$. However, since the underlying distribution $\mu$ is unknown, only the \emph{empirical risk}, defined as $\hat{\mathcal{L}}(s,w)\coloneqq\frac{1}{n}\sum_{i\in[n]}\ell(z_i,w)$, is accessible and can be minimized. Therefore, a central question in learning theory and this paper is to control the difference between these two risks,  known as \emph{generalization error}: 
\begin{align}
    \gen(s,w) \coloneqq \mathcal{L}(w) - \hat{\mathcal{L}}(s,w).
\end{align}

In our results, for simplicity, we also use the following shorthand notations:
\begin{align}
   \mathcal{\hat{L}}(\vc{y},\vc{
   \hat{y}}) \coloneqq  \frac{1}{n}\sum\nolimits_{i\in[n]} \mathbbm{1}_{\{\hat{y}_i\neq y_i\}},&\quad \quad
   \mathcal{\hat{L}}(\vc{y}',\vc{
   \hat{y}}') \coloneqq  \frac{1}{n}\sum\nolimits_{i\in[n]} 
   \mathbbm{1}_{\{\hat{y}'_i\neq y'_i\}},
\end{align}
Note that 
\begin{align}
 \mathcal{\hat{L}}(s,w)=\mathbb{E}_{\hat{\vc{Y}}\sim  P_{\vc{\hat{Y}}|\vc{x},w}}\left[\mathcal{\hat{L}}(\vc{y},\vc{\hat{Y}})\right],\quad \quad \mathcal{\hat{L}}(s',w)=\mathbb{E}_{\hat{\vc{Y}}'\sim  P_{\vc{\hat{Y}}'|\vc{x}',w}}\left[\mathcal{\hat{L}}(\vc{y}',\vc{\hat{Y}}')\right].
\end{align}
\vspace{5 cm}

\begin{theorem}\label{th:generalizationExp_hd_multi_task} Consider the setup of Theorem~\ref{th:generalizationExp_old}. Then, for any symmetric conditional distribution $\vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e)$ and for $n\geq 10$, we have
\begin{align}    \mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[h_D\Big(&\mathcal{\hat{L}}(\vc{Y}',\vc{
\hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{\hat{Y}})\Big) \Big] \leq \nonumber \\
&\frac{ \,\textnormal{MDL}(\vc{Q})+\log(n)}{n}+\mathbb{E}_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right], \label{eq:bound_hd_multi_task}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively, and $ \textnormal{MDL}(\vc{Q})$ is defined in \eqref{eq:MDL_original}.
\end{theorem}

   Finally, in representation learning, a good representation can often be used for different inference tasks. For example, if we consider that the representation $U$ is used for $L$ different classification tasks, each containing $C$ possibly different labels, then it can be shown that 
\begin{align}
    \max_{l\in[L]} h_D\left(\mathbb{E}_{W}\left[\mathcal{L}_l(W)\right],\mathbb{E}_{(S,W)}\left[\hat{\mathcal{L}_l}(S,W)\right]\right) \leq \frac{2 \,\textnormal{MDL}(\vc{Q})+\log(n)+L+C+2}{n}, \label{eq:multi_labels_bound_hd}
\end{align}
where here $\mathcal{L}_l(W)$ and $\hat{\mathcal{L}_l}(S,W)$ represent the population and empirical risks associated with the $l$'th classification task. Note that in this setup each sample is composed of $Z=(X,Y)$, where $Y=(Y_1,\ldots,Y_L)$ contains $L$ different labels.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Intuition behind lossy generalization bounds %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Intuition behind lossy generalization bounds} \label{sec:lossy_intuition}

The bounds of Theorems~\ref{th:generalizationExp_hd} and \ref{th:generalization_tail_hd_exp} for the deterministic encoders may become vacuous due to the KL-divergence term, and the bounds cannot explain the empirically observed \emph{geometrical compression} phenomenon \citep{geiger2021information}. These issues can be addressed using the \emph{lossy} compressibility approach, as opposed to the \emph{lossless} compressibility approach considered in previous sections. To provide a better intuition for these approaches, we first briefly explain their counterparts in information theory, i.e., lossless and lossy source compression.


Consider a \emph{discrete} source $V \sim P_V$ and assume that we have $n$ i.i.d. realizations $V_1,\ldots,V_n$ of this source. Then, for sufficiently large values of $n$, the classical lossless source coding result in information theory states that this sequence can be described by approximately $nH(V)$ bits, where $H(V)$ is the Shannon entropy function. Thus, intuitively, $H(V)$ is the complexity of the source $V$. Now, suppose that $V$ is no longer discrete. Then $V_1,\ldots,V_n$ can no longer be described by any \emph{finite} number of bits. However, if we consider some ``vector quantization'' instead, a sufficiently close vector can be described by a finite number of bits. This concept is called \emph{lossy compression}. The amount of closeness is called the distortion, and the minimum number of needed bits (per sample) to describe the source within a given distortion level is given by the rate-distortion function. 

Similar to \citep[Section~2.2.1 and Appendix~C.1.2]{sefidgaran2023minimum}, we borrow such concepts to capture the ``lossy complexity'' of the latent variables in order to avoid non-vacuous bounds, which can also explain the geometrical compression phenomenon \citep{geiger2021information,sefidgaran2023minimum}. This is achieved by considering the compressibility of ``quantized'' latent variables derived using the ``distorted'' encoders $\hat{W}_e$. Note that $\hat{W}_e$ is distorted only for the regularization term to measure the lossy compressibility (rate-distortion), and the undistorted latent variables are passed to the decoder. This is different from approaches that simply add noise to the output of the encoder and pass it to the decoder.

Finally, we show how to derive similar lossy bounds to \eqref{eq:lossy_bound} in terms of the function $h_D$. We first define the inverse of the function $h_D$ as follows. For any $y\in[0,2]$ and $x_2\in[0,1]$, let
 \begin{align}
        h_D^{-1}(y|x_2) = \sup \left\{x_1 \in[0,1]\colon h_D(x_1,x_2) \leq y\right\}.
\end{align}
Let $\hat{W}_e\in \mathcal{W}_e$ be any quantized model defined by $P_{\hat{W}_e|S}$, that satisfy the \emph{distortion} criterion $ \mathbb{E}_{P_{S,W}P_{\hat{W}_e|S}}\left[\gen(S,W)-\gen(S,\hat{W})\right] \leq \epsilon$, where $\hat{W}=(\hat{W}_e,W_d)$.  Then, using Theorem~\ref{th:generalizationExp_hd} for the quantized model, we have
\begin{align}    \mathbb{E}_{\vc{S},\vc{S}',\hat{W},\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[h_D\Big(&\mathcal{\hat{L}}(\vc{Y}',\vc{
\hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{\hat{Y}})\Big) \Big] \leq \nonumber \\
&\frac{ \,\textnormal{MDL}(\vc{Q})+\log(n)}{n}+\mathbb{E}_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right] \eqqcolon \Delta(\hat{W},\vc{Q}).
\end{align}
Next, using the Jensen inequality, we have 
\begin{align}    h_D\Big(\mathbb{E}_{\hat{W}}[\mathcal{L}(\hat{W})],\mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})]\Big) \Big] \leq \mathbb{E}_{\vc{S},\vc{S}',\hat{W},\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[h_D\Big(\mathcal{\hat{L}}(\vc{Y}',\vc{
\hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{\hat{Y}})\Big) \Big].
\end{align}
Combining the above two inequalities yields
\begin{align}    
h_D\Big(\mathbb{E}_{\hat{W}}[\mathcal{L}(\hat{W})],\mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})]\Big) \Big] \leq  \Delta(\hat{W},\vc{Q}).
\end{align}
Finally, we have
\begin{align}    
\mathbb{E}_{\vc{S},W}\left[\gen(S,W)\right]  \leq &\mathbb{E}_{\vc{S},\hat{W}}\left[\gen(S,\hat{W})\right] +\epsilon \nonumber \\
= &\mathbb{E}_{\hat{W}}[\mathcal{L}(\hat{W})] - \mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})] +\epsilon \nonumber \\
\leq & h_D^{-1}\left(\min(2,\Delta(\hat{W},\vc{Q})) \big|\mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})] \right) - \mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})] +\epsilon 
\end{align}
In particular, for negligible values of $\mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})]$, $h_D^{-1}\left(\min(2,\Delta(\hat{W},\vc{Q})) \big|\mathbb{E}_{S,\hat{W}}[\hat{\mathcal{L}}(S,\hat{W})] \right) \approx \min(2,\Delta(\hat{W},\vc{Q}))  \lesssim  \frac{\textnormal{MDL}(\vc{Q})+\log(n)}{n}$, which gives
\begin{align*}
    \mathbb{E}_{\vc{S},W}\left[\gen(S,W)\right]  \lesssim \frac{\textnormal{MDL}(\vc{Q})+\log(n)}{n} + \epsilon.
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Partial Symmetry %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Generalization bounds via non-symmetric priors} \label{sec:partial_symmetric}
 In this section, we discuss how the bounds of Theorems~\ref{th:generalizationExp_hd} and \ref{th:generalization_tail_hd_exp} can be extended to settings in which the requirement of symmetry is relaxed partially.  We focus on  ``differentially private'' and ``partially symmetric'' data-dependent priors.


\subsection{Differentially private data-dependent priors}
One way to extend the results to include the partially symmetric data-dependent priors is by leveraging the differential privacy tools \citep{dwork2006differential,dwork2014algorithmic,dwork2015generalization,dziugaite2018data}. The reader is referred to \citep[Section~3.3]{alquier2021} for more on differentially private priors.

Recall that given the dataset $S$ we train a model $W$ using the learning algorithm $\mathcal{A}(\cdot)$, \ie $W=\mathcal{A}(S)$. %, which has two parts $(W_e,W_d)$. %For simplicity of notations, let $W_e = \mathcal{A}_e(S)$, \ie $\mathcal{A}_e(\colon)$ is the learning algorithm that by taking $S$, outputs the encoder part. 
Now, assume that by having the dataset $S$ and the trained model $W= \mathcal{A}(S)$  we choose the prior $\vc{Q}^{S,W}$  using a potentially stochastic mechanism $\mathcal{T}\colon \mathcal{S} \times \mathcal{W} \to \mathcal{Q}$, where $\mathcal{Q}$ denotes the space of all conditional distributions of $\vc{U},\vc{U'}$ given $(\vc{Y},\vc{Y}')$, that is ``strongly'' symmetric. To state the definition of strongly symmetric prior, we first recall the notations of $\vc{U}_{\pi},\vc{U'}_{\pi}$ and $\vc{Y}_{\pi},\vc{Y}'_{\pi}$ for any permutation $\pi:[2n]\to[2n]$.  Let $Y^{2n}\coloneqq (\vc{Y},\vc{Y}')$. Then, we define $\vc{Y}_{\pi}$ and $\vc{Y}'_{\pi}$ as
\begin{align}
\vc{Y}_{\pi}\coloneqq& Y_{\pi(1)},\ldots,Y_{\pi(n)}, \nonumber \\
\vc{Y}_{\pi}\coloneqq& Y_{\pi(n+1)},\ldots,Y_{\pi(2n)}.
\end{align}
The variables $\vc{U}_{\pi}$ and $\vc{U'}_{\pi}$ are defined in a similar manner.

\begin{definition}[Strongly symmetric prior]
    A conditional distribution $\vc{Q}$ of $\vc{U},\vc{U'}$ given $(\vc{Y},\vc{Y}')$ is strongly symmetric, if for every $(\vc{U},\vc{U'},\vc{Y},\vc{Y}')$ and every permutation $\pi \colon [2n] \to [2n]$ that preserves the labeling (\ie $\vc{Y}_{\pi}=\vc{Y}$ and $\vc{Y}'_{\pi}=\vc{Y}'$) we have
\begin{align}
    \vc{Q}\left(\vc{U},\vc{U'}|\vc{Y},\vc{Y}'\right) = &\vc{Q}\left(\vc{U}_{\pi},\vc{U'}_{\pi}|\vc{Y},\vc{Y}'\right).
\end{align}
\end{definition}
Note that any strongly symmetric prior satisfies the symmetry condition of Definition~\ref{def:symmetry}. In addition, the per-label Gaussian-mixture prior of Section~\ref{sec:GM_centeralized} meets the strongly symmetric condition. To show this, recall that for any $c\in[C]$, the Gaussian mixture prior for label $c$ is denoted by $Q_c$. Given these per-label priors, the prior $\vc{Q}$ is defined as
\begin{align}
    \vc{Q}(\vc{U},\vc{U'}|S,S',\hat{W}_e) = & \vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y}')\nonumber \\
    = & \prod_{i\in[n]} Q_{Y_i}(U_i)Q_{Y'_i}(U'_i). \nonumber
\end{align}
It is immediate to see that this prior is strongly symmetric under any permutation that preserves the labeling.

Next, we define the notion of learning the prior in a differentially private manner. For simplicity, we consider the case where $\mathcal{A}(S)$ can be written as a deterministic function $g(S,V)$, where $V$ represents all the stochasticity in the learning algorithm that is independent of $S$. An example of such a learning algorithm is the Stochastic Gradient Descent (SGD) algorithm.
\begin{definition}[Differentially private prior] \label{def:diff_private} We say $\mathcal{T}\colon \mathcal{S} \times \mathcal{W} \to \mathcal{Q}$ is $\varepsilon_p$-differentially private if for any fixed $V$, and all datasets $S$ and $S_1$ that are different in only one coordinate and for all measurable subsets $B \subseteq \mathcal{Q}$, we have
\begin{align}
    \mathbb{P}\left(\vc{Q}^{S,\mathcal{A}(S)} \in B\right) \leq e^{\varepsilon_p} \mathbb{P}\left(\vc{Q}^{S_1,\mathcal{A}(S_1)} \in B \right),
\end{align}    
where $\mathcal{A}(S)=g(S,V)$ and $\mathcal{A}(S_1)=g(S_1,V)$.
\end{definition}

Now, we state our tail-bound result for $\varepsilon_p$-differentially private prior.

\begin{proposition}\label{prop:generalization_tail_hd_exp_partial} Consider the setup of Theorem~\ref{th:generalizationExp_hd} and suppose the prior $\vc{Q}^{S,\mathcal{A}(S)}$ is chosen using an $\varepsilon_p$-differentially private mechanism $\mathcal{T}\colon \mathcal{S} \times \mathcal{W} \to \mathcal{Q}$. Then, for any $\delta \geq 0$ and for $n\geq 10$, with probability at least $1-\delta$ over choices of $(S,S',W)$, it holds that
\begin{align}    
h_D\left(\mathcal{\hat{L}}(S',W),\mathcal{\hat{L}}(S,W)\right) \leq  &\frac{ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q}^{S,\mathcal{A}(S)} \
     \right)
     +\log(2n/\delta)}{n}\nonumber\\
&+\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right]\nonumber \\
&+\frac{1}{2}\varepsilon_p^2+\varepsilon_p \sqrt{\frac{\log(4/\delta)}{2n}}, \label{eq:tail_hd_partial}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively.
\end{proposition}
The proof stated in Appendix~\ref{pr:generalization_tail_hd_exp_partial} is an extension of Theorem~\ref{th:generalization_tail_hd_exp} using \citep[Theorems~18\&19]{dwork2015generalization} and \citep[Theorem~4.2]{dziugaite2018data}.


\subsection{Partially symmetric data-dependent priors}
In this section, we show an alternative way to extend our generalization bound results by defining the partially symmetric priors.

\begin{definition}[Partially symmetric prior] \label{def:partial_symmetry} The prior $\vc{Q}$ is $(\epsilon,\delta)$-partially symmetric for the learning algorithm $\mathcal{A}\colon \mathcal{Z}^n\to \mathcal{W}$, if with probability at least $1-\delta$ over choices of $(S',S,W_e,\vc{U},\vc{U'}) \sim P_{S'} P_{S,W_e} \vc{Q}$,
\begin{align}
    \forall \pi_{\vc{Y},\vc{Y}'}\colon \quad \vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e) \leq e^{\epsilon} \vc{Q}(\vc{U}_{\pi_{\vc{Y},\vc{Y}'}},\vc{U'}_{\pi_{\vc{Y},\vc{Y}'}}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e),
\end{align}
where this should hold for any permutation $\pi_{\vc{Y},\vc{Y}'}$ (which could potentially depend on $\vc{Y},\vc{Y}'$) that satisfies the labeling.
\end{definition}
Note that the partially symmetric prior can potentially depend on $(S,W)$.

\begin{proposition}\label{prop:generalizationExp_hd_partial} Consider the setup of Theorem~\ref{th:generalizationExp_hd}. Then, for any $(\epsilon,\delta)$-partially symmetric conditional distribution $\vc{Q}$ and for $n\geq 10$, we have
\begin{align}    \mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[h_D\Big(\mathcal{\hat{L}}(\vc{Y}',\vc{
\hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{\hat{Y}})\Big) \Big] \leq &\frac{ \,\textnormal{MDL}(\vc{Q})+\log\big(\delta e^{2n}+n e^{\epsilon}\big)}{n}\nonumber \\
&+\mathbb{E}_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right]. \label{eq:bound_hd_partial}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively and $\textnormal{MDL}(\vc{Q})$ is defined in \ref{eq:MDL_original}.
\end{proposition}

This result is proved in Appendix~\ref{pr:generalizationExp_hd_partial}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% GM-aproximation (D_KL+D_prod)/2 %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian mixture prior approximation and regularization} \label{sec:average_estimate}
In this section, we explain in detail our approach to finding an appropriate data-dependent Gaussian mixture prior and how to use it in a regularizer term along the optimization trajectories. The section is subdivided into three parts: the first part explains how we initialize the components of the Gaussian mixture prior, and the other two parts explain the lossless and lossy versions of our approach.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%% Single-View-GM-aproximation (D_KL+D_prod)/2 %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Recall that we are considering a regularizer term equal to 
\begin{align}
    \textnormal{Regularizer}(\vc{Q}) \coloneqq  D_{KL}\big(P_{\vc{U}_{\mathcal{B}}|\vc{X}_{\beta},W_e} \big\| \vc{Q}_{\mathcal{B}} \
     \big), \label{eq:regularizer_single}
\end{align}
where the indices $\mathcal{B}$ indicate the restriction to the set $\mathcal{B}$. However, for the sake of simplicity, we will drop the dependence on $\mathcal{B}$ in the rest of this section. Also, in the following, the superscript $(t)$ is used to denote the optimization iteration $t\in \mathbb{N}^*$.

We choose a Gaussian mixture prior $\vc{Q}$ in lossless and lossy ways. In both approaches, we initialize three sets of parameters $\alpha_{c,m}^{(0)}$, $\mu_{c,m}^{(0)}$, and $\sigma_{c,m}^{(0)}$, for $c\in[C]$ and $m\in[M]$, similarly. We will explain this first.

\subsection{Initialization of the components} \label{sec:initialization_single_view}
We let $\alpha_{c,m}^{(0)}=1/M$, for $c\in[C]$ and $m\in[M]$. The standard deviation values $\sigma_{c,m}^{(0)}$ are randomly chosen from the distribution $\mathcal{N}(0,\mathrm{I}_d)$. 

The means of the components $\mu_{c,m}^{(0)}$ are initialized in a way that the centers are initialized in the k-means++ method \citep{arthur2007k}. More specifically, they are initialized as follows.

\begin{itemize}
    \item[1.] The model's encoder $W_{e}$ is initialized.
 
\item[2.] A mini-batch $\vc{Z}=\{Z_1,\ldots,Z_{\tilde{b}}\}$, with a large mini-batch size $\tilde{b}\gg b$, of the training data is selected. Let $\vc{X}$ and $\vc{Y}$ be the set of features and labels of this mini-batch.
    
    For simplicity, we denote by $\vc{X}_c=\{X_{c,1},\ldots,X_{c,b_c}\}\subseteq \vc{X}$ the subset of features of the mini-batch with label $c\in[C]$. Note that $\sum_{c\in[C]} b_c = \tilde{b}$.
    
    Using the initialized encoder, we compute the corresponding parameters of the latent spaces for this mini-batch. Denote their mean vector as $\boldsymbol{\mu}_c=\{\mu_{c,1},\ldots,\mu_{c,b_c}\}$. For each $c\in[C]$, we let $\mu_{c,1}^{(0)}$ be equal to one of the elements in $\boldsymbol{\mu}_c$, uniformly.

    \item[3.] For $2 \leq m \leq M$, we take a new mini-batch $\vc{Z}$, with per-label features and latent variable means $\vc{X}_c$ and $\boldsymbol{\mu}_c$. Then, for all $c\in[C]$, we compute the below distances:
    \begin{align*}
        d_{\min,c}(i) = \min_{m'\in[m-1]} \left\| \mu_{c,i}-\mu_{c,m'}^{(0)} \right\|^2,\quad \quad i\in[b_c].
    \end{align*}
    Then, we randomly sample an index $i^*$ from the set $[b_c]$ according to a weighted probability distribution, where the index $i$ has a weight proportional to $d_{\min,c}(i)$. We let $\mu_{c,m}^{(0)}$ be equal to $\mu_{c,i^*}$.
\end{itemize}

\subsection{Lossless Gaussian mixture prior} \label{sec:lossless_single_view}
We start with the lossless version, which is easier to explain. Based on our observations in the experiments, the final population accuracy achieved when using the lossless regularizer is better than when using VIB \citep{alemi2016deep} or CDVIB \citep{sefidgaran2023minimum} but worse than when using the lossy version, explained in Appendix~\ref{sec:lossy_single_view}.


\textbf{Update of the priors.} Suppose the mini-batch picked at iteration $t$ is $\mathcal{B}^{(t)}=\{z_1^{(t)},\ldots,z_b^{(t)}\}$. We drop the dependence of the samples on $(t)$ for better readability. Then, the regularizer (\ref{eq:regularizer_single}), at iteration $(t)$, can be written as  
\begin{align}
    \textnormal{Regularizer}(\vc{Q}) = & \sum\nolimits_{i\in[b]} D_{KL}\big(P_{U_i|x_i,w_e} \big\| Q^{(t)}_{y_i}(U_i)\big).  \label{eq:reg_lossless_single_not_simplified_total}
\end{align}
We propose upper and lower bounds on this term. The upper bound is already presented in \eqref{eq:reg_lossless_single_not_simplified}, denoted as $D_{\text{var}}$:
\begin{align}
     \textnormal{Regularizer}(\vc{Q})\leq D_{\text{var}} \coloneqq \sum_{i\in[b]} \sum_{m\in[M]} \gamma_{i,m} \left(D_{KL}\big(P_{U_i|x_i,w_e} \big\| Q^{(t)}_{y_i,m}(U_i)\big)-\log\Big(\frac{\alpha_{y_i,m}^{(t)}}{\gamma_{i,m}}\Big)\right).  \label{eq:reg_lossless_single_var}
\end{align}
The upper bound holds for all choices of $\gamma_{i,m}\geq 0$ such that $\sum_{m\in[M]} \gamma_{i,m} =1$, for any $i\in[b]$.  As explained in Section~\ref{sec:GM_centeralized}, the coefficients $\gamma_{i,m}$ that minimize the above upper bound and thus make it tighter are equal to
\begin{align}
    \gamma_{i,m} = \frac{\alpha_{y_i,m}^{(t)} e^{-D_{KL}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m}^{(t)}\big)}}{\sum\limits_{m'\in [M]} \alpha_{y_i,m'}^{(t)} e^{-D_{KL}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m'}^{(t)}\big)}}, \quad i\in[b],m\in[M].
\end{align}
Denote $\gamma_{i,c,m} =\begin{cases} \gamma_{i,m},& \text{if } c=y_i,\\
0,& \text{otherwise.}.\end{cases}$. 

Next, we establish an estimated lower bound on the regularizer as
\begin{align}
     \textnormal{Regularizer}(\vc{Q})\geq  &
     -\sum_{i\in[b]} \left( \frac{1}{2} \log\Big((2\pi e)^d \prod\nolimits_{j
    \in[d]} \sigma_{x_i,j}^2\Big)+ \log\Big( \sum_{m=1}^M \alpha_{y_i,m}^{(t)} t_{i,m}\Big)\right)\nonumber\\
     \approx & -\sum_{i\in[b]} \left( \frac{1}{2} \log\Big((2\pi e)^d \prod\nolimits_{j
    \in[d]} \sigma_{x_i,j}^2\Big)+ \log\Big( \sum_{m=1}^M \alpha_{y_i,m}^{(t)} t'_{i,m}\Big)\right)\nonumber\\
    \eqqcolon & D_{\text{prod}}, \label{eq:reg_lossless_single_prod}
\end{align}
where
\begin{align}
    t_{i,m} \coloneqq& \mathbb{E}_{U\sim P_{U_i|x_i,w_e}}\left[Q_{y_i,m}^{(t)}\right]\stackrel{(a)}{=} \frac{ e^{-\sum_{j\in[d]}\frac{\left(\mu_{x_i,j} - \mu_{y_i,m,j}^{(t)}\right)^2}{2\left(\sigma^2_{x_i,j}+{\sigma_{y_i,m,j}^{(t)}}^2\right)}}}{\sqrt{\prod_{j\in[d]}  \left(2\pi\left(\sigma^2_{x_i,j}+{\sigma_{y_i,m,j}^{(t)}}^2\right)\right)}},\nonumber\\
    t'_{i,m} \coloneqq&  \frac{ e^{-\sum_{j\in[d]}\frac{\left(\mu_{x_i,j} - \mu_{y_i,m,j}^{(t)}\right)^2}{2{\sigma_{y_i,m,j}^{(t)}}^2}}}{\sqrt{\prod_{j\in[d]}  \left(2\pi{\sigma_{y_i,m,j}^{(t)}}^2\right)}}, \label{eq:Gaussian_integral}
\end{align}
where the step $(a)$ is derived from \citep{bromiley2003products}. The reader is referred to Appendix~\ref{sec:KL_estimation} for details on how this upper bound is derived.

It has already been observed in \citep{hershey2007approximating} for the case of two Gaussian mixture distributions that the KL-divergence is better estimated by considering the average of the \emph{product} lower bound and the \emph{variational} upper bound. We then consider the following estimate as the regularizer term
\begin{align}
     \textnormal{Regularizer}(\vc{Q}) \approx \frac{D_{\text{var}} + D_{\text{prod}} }{2} \eqqcolon D_{\text{est}},
    \label{eq:reg_lossless_single_estimate},
\end{align}
where $D_{\text{var}}$ and $D_{\text{prod}}$ are defined in \eqref{eq:reg_lossless_single_var} and \eqref{eq:reg_lossless_single_prod}, respectively. 

Next, we treat $\gamma_{i,m}$ as constants and find the parameters $\mu_{c,m}^*$, $\sigma^{*}_{c,m}$, $\alpha_{c,m}^*$ that minimize $D_{\text{est}}$ by solving the following equations
\begin{align*}
    \frac{\partial D_{est}}{\partial \mu_{c,m,j}} =0,\quad\frac{\partial D_{est}}{\partial \sigma_{c,m,j}} =0,\quad\frac{\partial D_{est}}{\partial \alpha_{c,m}} =0,
\end{align*}
with the constraint that $\sum_{m}\alpha_{c,m}=1$ for each $c\in[C]$. The above equations have the following optimal solutions $\mu_{c,m}^*$ and $\alpha_{c,m}^*$, and $\sigma^{*}_{c,m}$:
%but unlike the lossy version in the next subsection, they do not have a closed-form solution for $\sigma^{*}_{c,m}$. However, by a simple approximation trick of treating the denominator in \eqref{eq:Gaussian_integral} as constant, we can get a closed-form solution for $\sigma^{*}_{c,m,j}$ as well. Thus, simple algebra leads to the following exact closed-form solutions $\mu_{c,m}^*$ and $\alpha_{c,m}^*$ and approximate $\sigma^{*}_{c,m}$:
\begin{align}
    \mu_{c,m}^* =& \frac{1}{\tilde{b}_{c,m}}\sum\nolimits_{i\in[b]}  \tilde{\gamma}_{i,c,m} \mu_{x_i},\nonumber\\
    {\sigma^{*}_{c,m,j}}^2 =& \frac{1}{b_{c,m}}\sum\nolimits_{i\in[b]}   \left( \gamma_{i,c,m} \sigma_{x_i,j}^2+2 \tilde{\gamma}_{i,c,m}(\mu_{x_i,j}-\mu_{c,m,j}^{(t)})^2\right),\nonumber\\
    \alpha_{c,m}^* = & \tilde{b}_{c,m}/\tilde{b}_c, \nonumber \\ \tilde{b}_{c,m}=&\sum\nolimits_{i\in[b]}  \tilde{\gamma}_{i,c,m},\nonumber\\ \tilde{b}_c =&\sum\nolimits_{m\in[M]} \tilde{b}_{c,m}\nonumber \\
    b_{c,m}=&\sum\nolimits_{i\in[b]}  \gamma_{i,c,m}, .\label{eq:optimal_updates_lossless_single_comp} \hspace{1.5 cm} 
\end{align}
where
\begin{align}
\tilde{\gamma}_{i,c,m} \coloneqq & \frac{\gamma_{i,c,m}+\beta_{i,c,m}}{2},\nonumber\\
    \beta_{i,c,m} = & \begin{cases}
         \frac{\eta_{i,m}}{\sum_{m'\in[M]}\eta_{i,m'}},& \quad {if }c=y_i,\\
         0,&\quad {otherwise}.
     \end{cases}  \nonumber \\
     \eta_{i,m} \coloneqq & \alpha_{y_i,m}^{(t)} e^{-\sum_{j\in[d]}\frac{\left(\mu_{x_i,j} - \mu_{y_i,m,j}^{(t)}\right)^2}{2{\sigma_{y_i,m,j}^{(t)}}^2}}.
\end{align}
Note that $j\in[d]$ denotes the index of the coordinate in $\mathbb{R}^d$ and $\sigma_{c,m}^*=(\sigma_{c,m,1}^*,\ldots,\sigma_{c,m,d}^*)$. Finally, to reduce the dependence of the prior on the dataset, we choose the updates as
\begin{align}
    \mu_{c,m}^{(t+1)} =& (1-\eta_1) \mu_{c,m}^{(t)}+\eta_1 \mu_{c,m}^*+\mathfrak{Z}_{1}^{(t+1)}, \quad {\sigma_{c,m}^{(t+1)}}^2 = (1-\eta_2) {\sigma_{c,m}^{(t)}}^2+\eta_2 {\sigma_{c,m}^*}^2+\mathfrak{Z}_{2}^{(t+1)},\nonumber \\
    \alpha_{c,m}^{(t+1)} = &(1-\eta_3) \alpha_{c,m}^{(t)}+\eta_3 \alpha_{c,m}^*, \label{eq:updates_lossless_single_app}
\end{align}
where  $\eta_1,\eta_2,\eta_3 \in [0,1]$  are some fixed coefficients and $\mathfrak{Z}_{j}^{(t+1)}$, $j\in[2]$, are i.i.d. multivariate Gaussian random variables distributed as $\mathcal{N}(\vc{0}_d,\zeta_j^{(t+1)}\mathrm{I}_d)$. Here $\vc{0}_d=(0,\ldots,0)\in \mathbb{R}^d$ and $\zeta_j^{(t+1)}\in \mathbb{R}^+$ are some fixed constants.

\textbf{Regularizer.} Finally, the regularizer estimation \eqref{eq:reg_lossless_single_estimate} can be simplified as 
\begin{align}
     \textnormal{Regularizer}(\vc{Q}) =&- \frac{1}{2}\sum_{i\in[b]} \log\Big(\sum\nolimits_{m\in [M]} \alpha_{y_i,m}^{(t)} e^{-D_{KL}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m}^{(t)}\big)} \Big)\nonumber \\&-\frac{1}{2}\sum_{i\in[b]} \left( \frac{1}{2} \log\Big((2\pi e)^d \prod\nolimits_{j
    \in[d]} \sigma_{x_i,j}^2\Big)+ \log\Big( \sum_{m=1}^M \alpha_{y_i,m}^{(t)} t'_{i,m}\Big)\right).\label{eq:reg_lossless_single_complete}
\end{align}


\subsection{Lossy Gaussian mixture prior} \label{sec:lossy_single_view}
Now, we proceed with the lossy version of the regularizer. For this, we consider the MDL of the ``perturbed'' latent variable while passing the unperturbed latent variable to the decoder. Fix some $\epsilon\in \mathbb{R}^+$ and let $\boldsymbol{\epsilon}=(\epsilon,\ldots,\epsilon)\in \mathbb{R}^d$.

For the regularizer, we first consider the perturbed $U$ as
\begin{align}
    \hat{U}=U +\tilde{Z} = (\mu_X + Z_1) + Z_2 \eqqcolon \hat{U}_1 + \hat{U}_2,
\end{align}
where $\tilde{Z}$, $Z_1$, and $Z_2$ are independent multi-variate random variables, drawn from the distributions $\mathcal{N}\left(\vc{0}_d,\sqrt{d/4}\,\mathrm{I}_d+\diag\big(\boldsymbol{\epsilon}\big)\right)$, $\mathcal{N}\left(\vc{0}_d,\sqrt{d/4}\,\mathrm{I}_d\right)$, and $\mathcal{N}\left(\vc{0}_d,\diag\big(\sigma_{X,j}^2+\epsilon\big)\right)$, respectively. Consequently, $\hat{U}_1\sim \mathcal{N}(\mu_X,\sqrt{d/4}\,\mathrm{I}_d)$ is independent from $\hat{U}_2 \sim \mathcal{N}(\vc{0}_d,\diag(\sigma_X^2+\boldsymbol{\epsilon}))$, given $(X,W_e)$. 

For each label $c\in[C]$, we consider two Gaussian mixture priors $Q_{c,1}$ and $Q_{c,2}$ for $\hat{U}_1$ and $\hat{U}_2$, respectively, as follows:
\begin{align}
   Q_{c,1} = & \sum\nolimits_{m\in[M]} \alpha_{c,m}\, Q_{c,m,1},\\
    Q_{c,2} = & \sum\nolimits_{m\in[M]} \alpha_{c,m}\, Q_{c,m,2}
\end{align}
over $\mathbb{R}^d$, where $\alpha_{c,m} \in [0,1]$, $\sum_{m\in[M]} \alpha_{c,m}=1$ for each $c\in[C]$, and where $\{Q_{c,m,1}\}_{c,m}$ and  $\{Q_{c,m,2}\}_{c,m}$ are multivariate Gaussian distributions with a diagonal covariance matrix:
\begin{align*}
Q_{c,m,1}=&\mathcal{N}\left(\mu_{c,m},\sqrt{d/4} \, \mathrm{I}_d\right),\\
Q_{c,m,2}=&\mathcal{N}\left(\vc{0}_d,\diag\left(\sigma_{c,m}^2+\boldsymbol{\epsilon}\right)\right).
\end{align*}
Note that the Gaussian mixture priors $Q_{c,1}$ and $Q_{c,2}$ have the same parameters of $\alpha_{c,m}$. Now, let the prior $Q_{c}$ be the distortion of $\hat{U}=\hat{U}_1+\hat{U}_2$, when $\hat{U}_1\sim Q_{c,1}$ and $\hat{U}_2\sim Q_{c,2}$.

Now, for the variation upper bound $D_{\text{var}}$ for the regularizer, we first consider the inequality
\begin{align}
    D_{KL}\left(P_{\hat{U}|x,w_e} \| Q_{y_i}\right) \leq & D_{KL}\left(\mathcal{N}(\mu_x,\sqrt{d/4}\,\mathrm{I}_d) \| Q_{y_i,1}\right)+D_{KL}\left(\mathcal{N}(\vc{0}_d,\diag(\sigma_x^2+\boldsymbol{\epsilon})) \| Q_{y_i,2}\right) \nonumber  \\
    \eqqcolon& D_{KL,Lossy}\left(P_{\hat{U}|x,w_e} \| Q_{y_i}\right).\label{eq:lossy_upper}
\end{align}
Using the same arguments as in the lossless version but for $D_{KL,Lossy}\left(P_{\hat{U}|x,w_e} \| Q_{y_i}\right)$ instead of $D_{KL}\left(P_{\hat{U}|x,w_e} \| Q_{y_i}\right)$, we derive the following upper bound, denoted as $D_{\text{var}}$:\begin{align}
     \textnormal{Regularizer}(\vc{Q})\leq D_{\text{var}} \coloneqq \sum_{i\in[b]} \sum_{m\in[M]} \gamma_{i,m} \left(D_{KL,lossy}\big(P_{U_i|x_i,w_e} \big\| Q^{(t)}_{y_i,m}(U_i)\big)-\log\Big(\frac{\alpha_{y_i,m}^{(t)}}{\gamma_{i,m}}\Big)\right),  \label{eq:reg_lossy_single_var}
\end{align}
which is minimized for 
\begin{align}
    \gamma_{i,m} = \frac{\alpha_{y_i,m}^{(t)} e^{-D_{KL,Lossy}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m}^{(t)}\big)}}{\sum\limits_{m'\in [M]} \alpha_{y_i,m'}^{(t)} e^{-D_{KL,Lossy}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m'}^{(t)}\big)}}, \quad i\in[b],m\in[M].
\end{align}
Denote $\gamma_{i,c,m} =\begin{cases} \gamma_{i,m},& \text{if } c=y_i,\\
0,& \text{otherwise.}.\end{cases}$. 

% For the lower bound, we first consider the inequality
% \begin{align}
%     D_{KL}\left(P_{\hat{U}|x,w_e} \| Q_{y_i}\right) \geq & D_{KL}\left(\mathcal{N}(\mu_x,\sqrt{d/4}\mathrm{I}_d) \| Q_{y_i,1}\right).\label{eq:lossy_lower}
% \end{align}
% Then we apply a similar lower bound as in the lossless case to the RHS of \eqref{eq:lossy_lower}. This lower bound, denoted by $D_{\text{prod}}$, is equal to
 For the lower bound, we apply a similar lower bound as in the lossless case. This (estimated) lower bound, denoted by $D_{\text{prod}}$, is equal to
\begin{align}
      D_{\text{prod}} & 
      \coloneqq 
     -\sum_{i\in[b]} \left( \frac{d}{2} \log\left(\pi e \sqrt{d}\right)+ \log\Big( \sum_{m=1}^M \alpha_{y_i,m}^{(t)} \tilde{t}_{i,m}\Big)\right), \label{eq:reg_lossy_single_prod}
\end{align}
where
\begin{align}
    \tilde{t}_{i,m} \coloneqq& \frac{1}{\sqrt{(2\pi\sqrt{d})^d}} e^{-\frac{\left\|\mu_{x_i} - \mu_{y_i,m}^{(t)}\right\|^2}{2\sqrt{d}}}, \label{eq:Gaussian_integral_lossy}
\end{align}
We then consider the following estimate as the regularizer term
\begin{align}
     \textnormal{Regularizer}(\vc{Q}) \approx \frac{D_{\text{var}} + D_{\text{prod}} }{2} \eqqcolon D_{\text{est}},
    \label{eq:reg_lossy_single_estimate},
\end{align}
where $D_{\text{var}}$ and $D_{\text{prod}}$ are defined in \eqref{eq:reg_lossy_single_var} and \eqref{eq:reg_lossy_single_prod}, respectively. 

Next, similar to the lossless case, we treat $\gamma_{i,m}$ as constants and find the parameters $\mu_{c,m}^*$, $\sigma^{*}_{c,m}$, $\alpha_{c,m}^*$ that minimize $D_{\text{est}}$ by solving the following equations
\begin{align*}
    \frac{\partial D_{est}}{\partial \mu_{c,m,j}} =0,\quad\frac{\partial D_{est}}{\partial \sigma_{c,m,j}} =0,\quad\frac{\partial D_{est}}{\partial \alpha_{c,m}} =0,
\end{align*}
with the constraint that $\sum_{m}\alpha_{c,m}=1$ for each $c\in[C]$. The exact closed-form solutions $\mu_{c,m}^*$ and $\alpha_{c,m}^*$ and $\sigma^{*}_{c,m,j}$ are equal to :
\begin{align}
    \mu_{c,m}^* =& \frac{1}{\hat{b}_{c,m}}\sum\nolimits_{i\in[b]}  \hat{\gamma}_{i,c,m} \mu_{x_i},\nonumber\\
    {\sigma^{*}_{c,m,j}}^2 =& \frac{1}{b_{c,m}}\sum\nolimits_{i\in[b]}   \gamma_{i,c,m} \sigma_{x_i,j}^2,\nonumber\\
    \alpha_{c,m}^* = & \tilde{b}_{c,m}/\tilde{b}_c, \nonumber \\ \tilde{b}_{c,m}=&\sum\nolimits_{i\in[b]}  \tilde{\gamma}_{i,c,m},\nonumber\\ \tilde{b}_c =&\sum\nolimits_{m\in[M]} \tilde{b}_{c,m}\nonumber \\
    b_{c,m}=&\sum\nolimits_{i\in[b]}  \gamma_{i,c,m},\nonumber \\
    \hat{b}_{c,m}=&\sum\nolimits_{i\in[b]}  \hat{\gamma}_{i,c,m}.\label{eq:optimal_updates_lossy_single} \hspace{1.5 cm} 
\end{align}
where
\begin{align}
\tilde{\gamma}_{i,c,m} \coloneqq & \frac{\gamma_{i,c,m}+\beta_{i,c,m}}{2},\nonumber\\
\hat{\gamma}_{i,c,m} \coloneqq & \frac{2\gamma_{i,c,m}+\beta_{i,c,m}}{3},\nonumber\\
    \beta_{i,c,m} = & \begin{cases}
         \frac{\eta_{i,m}}{\sum_{m'\in[M]}\eta_{i,m'}},& \quad {if }c=y_i,\\
         0,&\quad {otherwise}.
     \end{cases}  \nonumber \\
     \eta_{i,m} \coloneqq & \alpha_{y_i,m}^{(t)} e^{-\frac{\left\|\mu_{x_i} - \mu_{y_i,m}^{(t)}\right\|^2}{2\sqrt{d}}}.
\end{align}
Note that $j\in[d]$ denotes the index of the coordinate in $\mathbb{R}^d$ and $\sigma_{c,m}^*=(\sigma_{c,m,1}^*,\ldots,\sigma_{c,m,d}^*)$. Finally, to reduce the dependence of the prior on the dataset, we choose the updates 
\begin{align}
    \mu_{c,m}^{(t+1)} =& (1-\eta_1) \mu_{c,m}^{(t)}+\eta_1 \mu_{c,m}^*+\mathfrak{Z}_{1}^{(t+1)}, \quad {\sigma_{c,m}^{(t+1)}}^2 = (1-\eta_2) {\sigma_{c,m}^{(t)}}^2+\eta_2 {\sigma_{c,m}^*}^2+\mathfrak{Z}_{2}^{(t+1)},\nonumber \\
    \alpha_{c,m}^{(t+1)} = &(1-\eta_3) \alpha_{c,m}^{(t)}+\eta_3 \alpha_{c,m}^*, \label{eq:updates_lossy_single_app}
\end{align}
where  $\eta_1,\eta_2,\eta_3 \in [0,1]$  are some fixed coefficients and $\mathfrak{Z}_{j}^{(t+1)}$, $j\in[2]$, are i.i.d. multivariate Gaussian random variables distributed as $\mathcal{N}(\vc{0}_d,\zeta_j^{(t+1)}\mathrm{I}_d)$. Here $\zeta_j^{(t+1)}\in \mathbb{R}^+$ are some fixed constants.

\textbf{Regularizer.} Finally, the regularizer estimation \eqref{eq:reg_lossy_single_estimate} can be simplified as  
\begin{align}
     \textnormal{Regularizer}(\vc{Q}) =&- \frac{1}{2}\sum_{i\in[b]} \log\Big(\sum\nolimits_{m\in [M]} \alpha_{y_i,m}^{(t)} e^{-D_{KL,Lossy}\big(P_{U_i|x_i,w_e}\|Q_{y_i,m}^{(t)}\big)} \Big)\nonumber \\&-\frac{1}{2}\sum_{i\in[b]} \left( \frac{d}{2} \log\left(\pi e \sqrt{d}\right)+ \log\Big( \sum_{m=1}^M \alpha_{y_i,m}^{(t)} \tilde{t}_{i,m}\Big)\right).\label{eq:reg_lossy_single_complete}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Limitaitons and Future directions %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future directions} \label{sec:limitations_futur_works}
In this work, we have established generalization bounds in terms of the minimum description length (MDL) of the latent variables. These bounds are particularly suitable for encoder-decoder architectures since they depend only on the encoder part of the model. The bounds improve the state-of-the-art results from $\sqrt{\text{MDL}(\vc{Q})/n}$ to $\text{MDL}(\vc{Q})/n$ in some cases. 

Inspired by our established bounds, we propose a systematic approach to finding a data-dependent prior and using it as a regularizer. The approach consists of first finding the underlying ``structure'' of the latent variable space, modeling it as a Gaussian mixture and then steering the latent variables in order to fit that mixture model. Conducted on various datasets and with various encoder architectures, reported experiments show promising results.

Our work opens up the door for several interesting future work directions, which we summarize hereafter.

\begin{itemize}[leftmargin=*]
    \item[1.] In the main body of this work, we have established generalization bounds in terms of symmetric priors. However, the proposed practical approach for the design of the prior slightly violates the symmetry condition. While it is not uncommon (sometimes preferred ?) to stretch the technical assumptions for practical designs a little, in Appendix~\ref{sec:partial_symmetric} we resolve the tension by showing that small deviations from the required technical symmetry only yield a small penalty in the bound. In the context of this paper, this result could be made more precise by studying the exact deviation of the proposed Gaussian mixture prior from the required symmetry and the caused deterioration of the bound.


    \item[2.] The introduced regularizer depends on the dimension of the latent variable, rather than on the dimension of the model or the input data, which are often much larger. This is a major advantage of our approach. In addition, our approach is relatively easy to implement. Nevertheless, similar to many other regularizers, this comes at the expense of some additional computational overhead. Possible means of reducing that overhead include: (i) using the regularizer only in the first $K$ epochs (which, generally, are the most critical \citep{keskar2016large,achille2017critical}) and (ii) applying the regularizer in a suitable lower-dimensional space, e.g., after proper projection of the latent vector onto that space.
    

    \item[3.] In Section~\ref{sec:GM_centeralized}, we have shown how a weighted attention mechanism emerges naturally in the process of finding the data-dependent Gaussian mixture prior. This may be particularly interesting; and is worth further exploration especially when our approach is applied to self-attention layers.

    
\item[4.] In Section~\ref{sec:GM_centeralized}, proper selection of the number of components of the Gaussian mixture ($M$) should depend, among other factors, on the dimension $d$ of the problem and the number of hidden ``subpopulations'' in the latent vector (which itself depends on the used encoder!). Thus, suitable values of $M$ seem difficult to obtain beforehand; and, instead, one can resort to simply treating it as a hyper-parameter. One approach to circumventing this could be to explore the ``structure'' of the training data using some common dimensionality reduction and unsupervised clustering techniques, such as the t-SNE of~\citep{chan2018t} or the method of~\citep{yang2012robust}. 

\item[5.] Finally, we mention that in this work, we focused primarily on the application to classification tasks. However, the approach and results of this paper can be extended to other setups, such as semi-supervised and transfer learning settings.
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% Experiments details %%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Details of the experiments} \label{sec:details_exp}

This section provides additional details about the experiments that were conducted. The code used in the experiments is available at \url{https://github.com/PiotrKrasnowski/Gaussian_Mixture_Priors_for_Representation_Learning}. 

\subsection{Datasets}
\label{experiments:datasets}

In all experiments, we used the following image classification datasets:

\textbf{CIFAR10} \citep{krizhevsky2009learning} - a dataset of 60,000 labeled images of dimension $32 \times 32 \times 3$ representing 10 different classes of animals and vehicles.

\textbf{CIFAR100} \citep{krizhevsky2009learning} - a dataset of 60,000 labeled images of dimension $32 \times 32 \times 3$ representing 100 different classes. 

\textbf{USPS} \citep{hull1994database}\footnote{\url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html\#usps}} - a dataset of 9,298 labeled images of dimension $16 \times 16 \times 1$ representing 10 classes of handwritten digits.

\textbf{INTEL}\footnote{https://www.kaggle.com/datasets/puneet6060/intel-image-classification} - a dataset of over 24,000 labeled images of dimension $150 \times 150 \times 3$ representing 6 classes of different landscapes (`buildings',  `forest', `glacier', `mountain', `sea', `street').

All images were normalized before feeding them to the encoder.


\subsection{Architecture details}
\label{experiments:architecture}

The experiments were conducted using two types of encoder models: a custom convolutional encoder and a pre-trained ResNet18 followed by a linear layer (more specifically, the model ``ResNet18\_Weights.IMAGENET1K\_V1'' in PyTorch). The architecture of the CNN-based encoder can be found in Table~\ref{tab::tab1}. This custom encoder is a concatenation of four convolutional layers and two linear layers. We apply max-pooling and a LeakyReLU activation function with a negative slope coefficient set to $0.1$. The encoders take re-scaled images as input and generate parameters $\mu_x$ and variance $\sigma_x^2$ of the latent variable of dimension $m=64$. Latent samples are produced using the reparameterization trick introduced by \citep{kingma2014auto}. Subsequently, the generated latent samples are fed into a decoder with a single linear layer and softmax activation function. The decoder's output is a soft class prediction.

Our tested encoders were complex enough to make them similar to ``a universal function approximator'', in line with \citep{dubois2020learning}. Conversely, we employ a straightforward decoder akin to \citep{alemi2016deep} to minimize the unwanted regularization caused by a highly complex decoder. This approach allows us to emphasize the advantages of our regularizer in terms of generalization performance. However, note that the used ResNet18 model is already pre-trained using various regularization and data augmentation techniques. Therefore, the effect of a new regularizer is naturally less visible.

\begin{table}[ht]
 \renewcommand{\arraystretch}{1.2}
\centering
\caption{The architecture of the convolutional encoder used in the experiments. The convolutional layers are parameterized respectively by the number of input channels, the number of output channels, and the filter size. The linear layers are defined by their input and output sizes.\vspace{0.2 cm}}
\begin{tabular}{|cc|cc|cc|}
\hline
\multicolumn{2}{|c|}{Encoder}             & \multicolumn{2}{c|}{Encoder cont'd}        & \multicolumn{2}{c|}{Encoder cont'd}       \\ \hline
\multicolumn{1}{|c|}{Number} & Layer           & \multicolumn{1}{c|}{Number} & Layer             & \multicolumn{1}{c|}{Number} & Layer           \\ \hline
\multicolumn{1}{|c|}{1}  & Conv2D(3,8,5)  & \multicolumn{1}{c|}{6}  & Conv2D(16,16,3)  & \multicolumn{1}{c|}{11} & LeakyReLU(0.1)  \\ \hline
\multicolumn{1}{|c|}{2}  & Conv2D(3,8,5)  & \multicolumn{1}{c|}{7}  & LeakyReLU(0.1)   & \multicolumn{1}{c|}{12} & Linear(256,128) \\ \hline
\multicolumn{1}{|c|}{3}  & LeakyReLU(0.1) & \multicolumn{1}{c|}{8}  & MaxPool(2,2)     & \multicolumn{2}{c|}{Decoder}              \\ \hline
\multicolumn{1}{|c|}{4}  & MaxPool(2,2)   & \multicolumn{1}{c|}{9}  & Flatten          & \multicolumn{1}{c|}{1}  & Linear(64,10)   \\ \hline
\multicolumn{1}{|c|}{5}  & Conv2D(8,16,3) & \multicolumn{1}{c|}{10} & Linear(N,256) & \multicolumn{1}{c|}{2}  & Softmax         \\ \hline
\end{tabular}
\label{tab::tab1}
\end{table}

\subsection{Implementation and training details}
\label{experiments:implementation}

The PyTorch library \citep{paszke2019pytorch} and a GPU Tesla P100 with CUDA 11.0 were utilized to train our prediction model. We employed the PyTorch Xavier initialization scheme \citep{glorot2010understanding} to initialize all weights, except biases set to zero. For optimization, we used the Adam optimizer \citep{KingmaB14} with parameters $\beta_1 = 0.5$ and $\beta_2 = 0.999$, an initial learning rate of $10^{-4}$, an exponential decay of 0.97, and a batch size of 128.

We trained the encoder and decoder models for 200 epochs five times independently for each considered regularization loss and for each value of the regularization parameter $\beta$ ranging between zero and one. The training was done using conventional cross-entropy loss for image category classification at the decoder’s output, and regularization of the encoder’s output based on either the standard VIB, the Category-dependent VIB, or our Gaussian mixture objective functions. For the Gaussian mixture objective function, we selected $M$=20 priors for each class category. The Gaussian mixture priors were initialized using the approaches in~\ref{sec:initialization_single_view}. The priors were updated after each training iteration using the procedure in~\ref{sec:lossy_single_view} with a moving average coefficient $\eta_1 =1\mathrm{e}{-2}$ for the priors’ means $\mu_{c,m}$, $\eta_2 = 5\mathrm{e}{-4}$ for the priors’ variances $\sigma^2_{c,m}$, and $\eta_3 = 1\mathrm{e}{-2}$ for the mixture weights $\alpha_{c,m}$. Following the approach outlined in \citep{alemi2016deep}, we generated one latent sample per image during training and 12 samples during testing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%% KL estimation %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{KL-divergence estimation} \label{sec:KL_estimation}
In this section, we first recall the KL-divergence estimation of two Gaussian mixture distributions developed in \citep{hershey2007approximating,durrieu2012lower}. Then, we adapt these approaches to the case where the KL-divergence estimation of a Gaussian distribution and a Gaussian mixture distribution is considered.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{KL-divergence estimation of two Gaussian mixture distributions} \label{sec:KL_GM_GM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this section, we recall the results of \citep{hershey2007approximating,durrieu2012lower}. We give the results only for the case where the covariance matrices of the Gaussian components are diagonal, for simplicity and because only diagonal covariance matrices are considered in our work. However, the results hold for the general form of the covariance matrix.

Consider two Gaussian mixture distributions $P$ and $Q$, defined as 
\begin{align*}
    P = & \sum_{j=1}^N \beta_j P_j,\\
    Q = & \sum_{i=1}^M \alpha_i Q_i,
\end{align*}
where $\alpha_i,\beta_j \geq 0$, $\sum_{j\in[N]} \beta_j =1$, and $\sum_{i\in[M]} \alpha_i =1$. In addition, each component is a multivariate Gaussian distribution with diagonal covariance matrices.
\begin{align*}
 P_j =& \mathcal{N}(\boldsymbol{\mu}_{p,j},\diag(\boldsymbol{\sigma}_{p,j}^2)),\\
    Q_i = &\mathcal{N}(\boldsymbol{\mu}_{q,i},\diag(\boldsymbol{\sigma}_{q,i}^2)).
\end{align*}

\subsubsection{Product of Gaussian approximation}
In this approximation, $D_{KL}(P\|Q)$ is approximated as \citep{hershey2007approximating}:
\begin{align}
    D_{\text{prod}}(P\|Q) \coloneqq \sum_{j\in[N]} \beta_j \log\left(\frac{\sum_{j'\in[M]}\beta_{j'} \mathbb{E}_{P_j}\left[P_{j'}\right]}{\sum_{i\in[M]}\alpha_{i} \mathbb{E}_{P_j}\left[Q_{i}\right]}\right). \label{eq:KL_GM_GM_prod}
\end{align}
Note that this approximation is generally neither an upper bound nor a lower bound.

\subsubsection{Variational approximation}
In this approximation, $D_{KL}(P\|Q)$ is approximated as \citep{hershey2007approximating}:
\begin{align}
    D_{\text{var}}(P\|Q) \coloneqq \sum_{j\in[N]} \beta_j \log\left(\frac{\sum_{j'\in[M]}\beta_{j'} e^{-D_{KL}\left(P_j\|P_{j'}\right)}}{\sum_{i\in[M]}\alpha_{i} e^{-D_{KL}\left(P_j\|Q_{i}\right)}}\right). \label{eq:KL_GM_GM_var}
\end{align}
Note that this approximation is again not an upper or lower bound in general.

\subsubsection{Average of two approximations}
It has been shown in \citep{hershey2007approximating,durrieu2012lower}, that the average of the product and variational approximation provides a better estimate of the KL-divergence between two Gaussian prior distributions.
\begin{align}
    D_{\text{est}}(P\|Q)=\frac{ D_{\text{prod}}(P\|Q)+ D_{\text{var}}(P\|Q)}{2}.
\end{align}

\subsection{KL-divergence estimation between a Gaussian and a Gaussian mixture distribution}
\label{sec:KL_G_GM}

In this section, we adapt the approaches of \citep{hershey2007approximating} for the setup where $P$ is a $d$-dimensional Gaussian distribution with a diagonal covariance matrix and $Q$ is a Gaussian mixture of $M$ of $d$-dimensional Gaussians with a diagonal covariance matrix.

Formally, let $$P=\mathcal{N}(\boldsymbol{\mu},\diag(\boldsymbol{\sigma}_p^2)),$$ and $Q$ be a Gaussian mixture
\begin{align*}
    Q = \sum_{i=1}^M \alpha_i Q_i,
\end{align*}
where $\alpha_i \geq 0$, $\sum_{i\in[M]} \alpha_i =1$, and
\begin{align*}
    Q_i = \mathcal{N}(\boldsymbol{\mu}_i,\diag(\boldsymbol{\sigma}_{q,i}^2)).
\end{align*}


\subsubsection{Product of Gaussian bound}
Denoting $L_P(f) \coloneqq \mathbb{E}_P[\log(f)]$, we have $D_{KL}(P\|Q)= L_P(P)-L_P(Q)$. Note that
\begin{align*}
    L_P(P)=-h(P) = -\frac{1}{2} \log\Big((2\pi e)^d \prod\nolimits_{j
    \in[d]} \sigma_{p,j}^2\Big),
\end{align*}
where $h(\cdot)$ is the differential entropy. Next, to bound $L_P(Q)$, using the idea of \citep{hershey2007approximating}, we have
\begin{align}
L_P(Q) =&  \mathbb{E}_P\Big[\log\big( \sum_{i=1}^M \alpha_i Q_i\big)\Big] \leq \log\Big( \sum_{i=1}^M \alpha_i \mathbb{E}_P[Q_i]\Big) = \log\Big( \sum_{i=1}^M \alpha_i t_i\Big),
\end{align}
where
\begin{align}
    t_i = \mathbb{E}_P[Q_i] = \int_x P(x) Q_i(x) \der x ,
\end{align}
is the normalization constant of the product of the Gaussians (refer to \citep[Appendix~A]{durrieu2012lower}). Note that by choice of the diagonal covariance matrices, these constants can be written as the product of $m$ coordinate-wise constants. 

Thus, we have
\begin{align}
    D_{KL}(P\|Q) \geq  -\frac{1}{2} \log\Big((2\pi e)^d \prod\nolimits_{j
    \in[d]} \sigma_{p,j}^2\Big) - \log\Big( \sum_{i=1}^M \alpha_i t_i\Big)\eqqcolon D_{\text{prod}}(P\|Q).
\end{align}
Note that, unlike the KL divergence estimation of two Gaussian mixture priors, here the product of Gaussian approaches provides a lower bound.

\subsubsection{Variational bound}
 Fix some $\gamma_i\geq 0$, $i\in[M]$ such that $\sum_i \gamma_i=1$. Then,
\begin{align}
L_P(Q) =&  \mathbb{E}_P\Big[\log\big( \sum_{i=1}^M \alpha_i Q_i\big)\Big] \nonumber \\
       = & \mathbb{E}_P\Big[\log\Big( \sum_{i=1}^M \alpha_i \gamma_i \frac{Q_i}{\gamma_i}\Big)\Big] \nonumber \\
       \geq & \sum_{i\in[M]} \gamma_i \mathbb{E}_P\Big[\log\Big(\frac{\alpha_i Q_i}{\gamma_i}\Big)\Big] \nonumber \\
       =& \sum_{i\in[M]} \gamma_i \big(L_P(Q_i) +\log(\alpha_i/\gamma_i)\big). \label{eq:KL_G_GM_Var}
\end{align}
Maximizing this lower bound with respect to $\gamma_i$ gives
\begin{align}
    \gamma_i = \frac{\alpha_i e^{-D_{KL}(P\|Q_i)}}{\sum_{i'\in [M]} \alpha_{i'} e^{-D_{KL}(P\|Q_{i'})}}.
\end{align}
Using this choice, \eqref{eq:KL_G_GM_Var} simplifies as
\begin{align}
L_P(Q) =&  \mathbb{E}_P\Big[\log\big( \sum_{i=1}^M \alpha_i Q_i\big)\Big] \geq \log\bigg(\sum_{i\in[M]} \alpha_i e^{L_P(Q_i)}\bigg).
\end{align}
Hence, overall
\begin{align}
     D_{KL}(P\|Q) \leq & L_P(P) - \log\bigg(\sum_{i\in[M]} \alpha_i e^{L_P(Q_i)}\bigg) \\
    =&- \log\bigg(\sum_{i\in[M]} \alpha_i e^{-D_{KL}(P\|Q_i)}\bigg)\\
    \eqqcolon & D_{\text{var}}(P\|Q).
\end{align}

Note that again, unlike the KL divergence estimation of two Gaussian mixture priors, where the variation approach provides only an approximation, this approach provides an upper bound.

\subsubsection{Average of two approximations}
Finally, to estimate the KL-divergence between a Gaussian distribution and a Gaussian mixture distribution, we consider the average of the product of the Gaussian lower bound and the variational upper bound.
\begin{align}
    D_{\text{est}}(P\|Q)=\frac{ D_{\text{prod}}(P\|Q)+ D_{\text{var}}(P\|Q)}{2}.
\end{align}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%% Proofs %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proofs} \label{sec:proofs}
In this section, we present the deferred proofs.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Proof: Generalization Expectation %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{th:generalizationExp_hd}} \label{pr:generalizationExp_hd}
Fix some symmetric conditional prior $\vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e)$. We will  show that 
\begin{align}
\mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\left[h_D\left(\mathcal{\hat{L}}(\vc{Y}',\vc{
   \hat{Y}}'),\mathcal{\hat{L}}(\vc{Y},\vc{
   \hat{Y}})\right) -h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right] \leq \frac{ \,\textnormal{MDL}(\vc{Q})+\log(n)}{n}, \label{eq:bound_hd_dupp}
\end{align}
where $\hat{p}_{\vc{Y}}$ and $\hat{p}_{\vc{Y}'}$ are empirical distributions of $\vc{Y}$ and $\vc{Y}'$, respectively, 
\begin{align}
  \textnormal{MDL}(\vc{Q}) \coloneqq   \mathbb{E}_{S,S',W_e} \left[ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \Big\| \vc{Q} \
     \right) \right], \label{eq:MDL_original_dupp}
\end{align}
and
\begin{align*}
     (\vc{S},\vc{S}',\vc{U},\vc{U'},\vc{\hat{Y}},\vc{\hat{Y}}',W)\sim P_{S,W}P_{S'} P_{\vc{U}|\vc{X},W_e}P_{\vc{U}'|\vc{X}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d}.
\end{align*}

Denote 
\begin{align*}
P_1 \coloneqq & P_{S,W}P_{S'} P_{\vc{U}|\vc{X},W_e}P_{\vc{U}'|\vc{X}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d}, \\
P_2\coloneqq & P_{S,W}P_{S'} Q_{\vc{U},\vc{U}'|\vc{X},\vc{X}',\vc{Y},\vc{Y}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d},\\
f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) \coloneqq &  h_D\left(\mathcal{\hat{L}}(\vc{Y}',\hat{\vc{Y}}'),\mathcal{\hat{L}}(\vc{Y},\hat{\vc{Y}})\right)-h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right).
\end{align*}

Next, similar to information-theoretic (e.g. \citep{xu2017information,steinke2020reasoning,sefidgaran2023minimum}) and PAC-Bayes-based approaches (e.g. \citep{alquier2021,rivasplata2020pac}) we use Donsker-Varadhan's inequality to change the measure from $P_1$ to $P_2$. The cost of such a change is $D_{KL}(P_1\|P_2) = \text{MDL}(\vc{Q})$. We apply Donsker-Varadhan on the function $nf$.  Concretely, we have
\begin{align*}
\mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right)\Big]  \leq  & D_{KL}\left(P_1\|P_2\right) + \log \left(\mathbb{E}_{P_2}\left[e^{nf\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right)}\right]\right)\\
 = & \text{MDL}(\vc{Q}) + \log \left(\mathbb{E}_{P_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right]\right).
\end{align*}
Hence, it remains to show that 
\begin{align}
    \mathbb{E}_{P_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right] \leq n. \label{eq:pr_thm1_2}
\end{align}
Let $\tilde{\vc{Q}}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}$ be the conditional distribution of $(\hat{\vc{Y}},\hat{\vc{Y}}')$ given $(\vc{Y},\vc{Y}')$, under the joint distribution $P_2$. It can be easily verified that $\tilde{\vc{Q}}$ satisfies the symmetry property since $\vc{Q}$ is symmetric (as defined in Definition~\ref{def:symmetry}). For better clarity, we re-state the symmetry property of $\tilde{\vc{Q}}$ and define some notations that will be used in the rest of the proof. 

Let $Y^{2n}\coloneqq (\vc{Y},\vc{Y}')$ and $\hat{Y}^{2n}\coloneqq (\hat{\vc{Y}},\hat{\vc{Y}}')$. For a given permutation $\tilde{\pi}\colon [2n] \to [2n]$, the permuted vectors $Y^{2n}_{\tilde{\pi}}$  and $\hat{Y}^{2n}_{\tilde{\pi}}$ are defined as 
\begin{align}
Y^{2n}_{\tilde{\pi}}\coloneqq Y_{\tilde{\pi}(1)},\ldots,Y_{\tilde{\pi}(2n)}, \nonumber\\
\hat{Y}^{2n}_{\tilde{\pi}}\coloneqq \hat{Y}_{\tilde{\pi}(1)},\ldots,\hat{Y}_{\tilde{\pi}(2n)}. \label{def:y_concat_pi}
\end{align}
Furthermore, under the permutation $\tilde{\pi}$, we denote the first $n$ coordinates of $Y^{2n}_{\tilde{\pi}}$ 
 and $\hat{Y}^{2n}_{\tilde{\pi}}$ by
\begin{align}
\vc{Y}_{\tilde{\pi}} \coloneqq& Y_{\tilde{\pi}(1)},\ldots,\hat{Y}_{\tilde{\pi}(n)}, \nonumber \\
\hat{\vc{Y}}_{\tilde{\pi}} \coloneqq& \hat{Y}_{\tilde{\pi}(1)},\ldots,\hat{Y}_{\tilde{\pi}(n)}, \label{def:y_pi} 
\end{align}
respectively, and the next $n$ coordinates of $Y^{2n}_{\tilde{\pi}}$ 
 and $\hat{Y}^{2n}_{\tilde{\pi}}$ by
\begin{align}
\vc{Y}'_{\tilde{\pi}} \coloneqq Y_{\tilde{\pi}(n+1)},\ldots,Y_{\tilde{\pi}(2n)}, 
\nonumber \\
\hat{\vc{Y}}'_{\tilde{\pi}} \coloneqq\hat{Y}_{\tilde{\pi}(n+1)},\ldots,\hat{Y}_{\tilde{\pi}(2n)}.\label{def:y_p_pi} 
\end{align}
respectively. By $\tilde{\vc{Q}}$ being symmetric, we mean that $\tilde{\vc{Q}}_{\hat{\vc{Y}}_{\tilde{\pi}},\hat{\vc{Y}}'_{\tilde{\pi}}|\vc{Y},\vc{Y}'}$ remains invariant under all permutations such that $Y_{i}=Y_{\tilde{\pi}(i)}$ for all $i\in[2n]$. In other words, all permutations such that $\vc{Y}=\vc{Y}_{\tilde{\pi}}$ and $\vc{Y}'=\vc{Y}'_{\tilde{\pi}}$.

Hence, we can write
\begin{align}
     \mathbb{E}_{P_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right] = &  \mathbb{E}_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right], \label{eq:pr_thm1_3}
\end{align}
where $\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'\sim \mu_{Y}^{\otimes 2n}\tilde{\vc{Q}}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}$.

Fix some $\vc{Y}$ and $\vc{Y}'$. Without loss of generality and for simplicity, assume that $\vc{Y}$ and $\vc{Y}'$ are \emph{ordered}, in the sense that for $r\in[R]$, $Y_r=Y'_r$, and $\left\{Y_{R+1},\ldots,Y_{n}\right\} \bigcap \left\{Y'_{R+1},\ldots,Y'_{n}\right\}=\varnothing$, where
$$R= n-\frac{n}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1.$$
Otherwise, it is easy to see that the following analysis holds by proper (potentially non-identical) re-orderings of $\vc{Y}$ and $\vc{Y}'$ and corresponding predictions $\hat{\vc{Y}}$ (according to the way $\vc{Y}$ is re-ordered) and $\hat{\vc{Y}}'$ (according to the way $\vc{Y}'$ is re-ordered), such that $\vc{Y}$ and $\vc{Y}'$ coincidence in all first $R$ coordinates and do not have any overlap in the remaining $n-R$ coordinates. 

Furthermore, for $r\in[n]$, let $J_r\in \{r,n+r\}\sim \text{Bern}(\frac{1}{2})$ be a uniform binary random variable and define $J^c_r$ as its complement, \ie $J_r \cup J_r^c = \{r,n+r\}$. Define the mapping $\pi_R\coloneqq [2n] \to [2n]$ as following: For $r\in[R]$, $\pi_R(r)=J_r$ and $\pi_R(r+n)=J^c_r$. For $r \in [R+1,n]$, $\pi_R(r)=r$ and $\pi_R(n+r)=n+r$. Note that $\pi_{R}$ depends on $(\vc{Y},\vc{Y}')$ and under $\pi_R$, $\vc{Y}=\vc{Y}_{\pi_R}$ and $\vc{Y}'=\vc{Y}'_{\pi_R}$, where $\vc{Y}_{\pi_R}$ and $\vc{Y}'_{\pi_R}$ are defined in \eqref{def:y_pi} and \eqref{def:y_p_pi}, respectively. Hence, $\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1=\left\|\hat{p}_{\vc{Y}_{\pi_R}}-\hat{p}_{\vc{Y}'_{\pi_R}}\right\|_1$. To simplify the notations, in what follows we denote the coordinates of $\vc{Y}_{\pi_R}$ by $$\vc{Y}_{\pi_R} \coloneqq (Y_{\pi_R,1},\ldots,Y_{\pi_R,n}),$$ and the coordinates of $\vc{Y}'_{\pi_R}$ by $$\vc{Y}'_{\pi_R} \coloneqq (Y'_{\pi_R,1},\ldots,Y'_{\pi_R,n}).$$ Note that by \eqref{def:y_pi} and \eqref{def:y_p_pi}, we have $Y_{\pi_R,i}=Y^{2n}_{\pi_R(i)}$ and $Y'_{\pi_R,i}=Y^{2n}_{\pi_R(i+n)}$ for $i\in[n]$, where $Y^{2n}_{\pi_R(i)}$ is defined in \eqref{def:y_concat_pi}. Similar notations are used for the prediction vectors, \ie
\begin{align*}
   \hat{\vc{Y}}_{\pi_R} \coloneqq & (\hat{Y}_{\pi_R,1},\ldots,\hat{Y}_{\pi_R,n}),\\
   \hat{\vc{Y}}'_{\pi_R} \coloneqq & (\hat{Y}'_{\pi_R,1},\ldots,\hat{Y}'_{\pi_R,n}).
\end{align*}


With these notations, for a fixed ordered $\vc{Y}$ and $\vc{Y}'$ we have
\begin{align}
    \mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[e^{n f\left(\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'\right) }\right] = &\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\mathbb{E}_{J_1,\ldots,J_R \sim\text{Bern}(\frac{1}{2})^{\otimes R}} \left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}}_{\pi_R},\vc{\hat{Y}'}_{\pi_R}\right) }\right]\nonumber \\
     = & \mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\mathbb{E}_{J_1,\ldots,J_R \sim\text{Bern}(\frac{1}{2})^{\otimes R}} \left[e^{n f\left(\vc{Y}_{\pi_R},\vc{Y}'_{\pi_R},\vc{\hat{Y}}_{\pi_R},\vc{\hat{Y}'}_{\pi_R}\right) }\right]. \label{eq:pr_thm1_4}
\end{align}
where the first step follows due to the symmetric property of $\tilde{\vc{Q}}$ and the second step follows since $\vc{Y}=\vc{Y}_{\pi_R}$ and $\vc{Y}'=\vc{Y}'_{\pi_R}$.


Now, consider another mapping $\pi\coloneqq [2n]\to [2n]$ such that $\pi$ is identical to $\pi_R$ for the indices in the range $[1:R]\cup[n+1:n+R]$, \ie for $r\in[R]$,
\begin{align*}
    \pi(r)=\pi_R(r)=J_r,\quad \quad \pi(r+n)=\pi_R(r+n)=J_r^c.
\end{align*}
Furthermore, for the indices in the range in $[R+1:n]\cup[n+R+1:2n]$, $\pi$ is defined as follows: for $r \in [R+1,n]$, 
\begin{align*}
    \pi(r)=J_r,\quad \quad \pi(n+r)=J_{r}^c,
\end{align*}
where as previously defined, $J_r\in \{r,n+r\}\sim \text{Bern}(\frac{1}{2})$ is a uniform binary random variable and $J^c_r$ is its complement. Denote $$J_{R+1}^n \coloneqq J_{R+1},\ldots,J_n.$$

With the above definitions, we have
\begin{align}
    &\hspace{-0.4 cm}e^{n f\left(\vc{Y}_{\pi_R},\vc{Y}'_{\pi_R},\vc{\hat{Y}}_{\pi_R},\vc{\hat{Y}'}_{\pi_R}\right) } \nonumber\\
    = & \mathbb{E}_{J_{R+1}^n\sim \text{Bern}(\frac{1}{2})^{\otimes (n-R)} } \bigg[ e^{nh_D\left(\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}'_{\pi,i}\neq Y'_{\pi,i}\}},\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}_{\pi,i}\neq Y_{\pi,i}\}}\right)} \nonumber\\
    &\hspace{3.3cm}\times e^{nf\left(\vc{Y}_{\pi_R},\vc{Y}'_{\pi_R},\vc{\hat{Y}}_{\pi_R},\vc{\hat{Y}'}_{\pi_R}\right) - nh_D\left(\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}'_{\pi,i}\neq Y'_{\pi,i}\}},\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}_{\pi,i}\neq Y_{\pi,i}\}}\right) } \bigg] \nonumber \\
    \stackrel{(a)}{\leq}& \mathbb{E}_{J_{R+1}^n\sim \text{Bern}(\frac{1}{2})^{\otimes (n-R)} } \left[ e^{nh_D\left(\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}'_{\pi,i}\neq Y'_{\pi,i}\}},\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}_{\pi,i}\neq Y_{\pi,i}\}}\right)}  \right], \label{eq:pr_thm1_5}
\end{align}
where $(a)$ holds due to the following Lemma, shown in Appendix~\ref{pr:claim}.

\begin{lemma} \label{lem:claim} The below relation holds:
\begin{align}
f\left(\vc{Y}_{\pi_R},\vc{Y}'_{\pi_R},\vc{\hat{Y}}_{\pi_R},\vc{\hat{Y}'}_{\pi_R}\right) \leq h_D\left(\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}'_{\pi,i}\neq Y'_{\pi,i}\}},\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}_{\pi,i}\neq Y_{\pi,i}\}}\right).
\end{align}
\end{lemma}

Hence, for a fixed ordered $\vc{Y}$ and $\vc{Y}'$, combining \eqref{eq:pr_thm1_4} and \eqref{eq:pr_thm1_5} yields
\begin{align}
    \mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}&\left[e^{n f\left(\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'\right) }\right] \nonumber \\
    &= \text{$\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}$}
    \mathbb{E}_{J_1,\ldots,J_n \sim\text{Bern}(\frac{1}{2})^{\otimes n}} \left[ e^{n h_D\Big(\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}'_{\pi,i}\neq Y'_{\pi,i}\}},\frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\{\hat{Y}_{\pi,i}\neq Y_{\pi,i}\}}\Big)}  \right] \nonumber\\
& \leq n, \label{eq:pr_thm1_4_combined}
\end{align}
where the last step is derived by using \citep[Proof of Theorme~3]{sefidgaran2023minimum}. As mentioned before, it is easy to see that the above analysis holds for non-ordered  $\vc{Y}$ and $\vc{Y}'$, by simply considering proper (potentially non-identical) re-orderings of $\vc{Y}$ and $\vc{Y}'$ and corresponding predictions $\hat{\vc{Y}}$ (according to the way $\vc{Y}$ is re-ordered) and $\hat{\vc{Y}}'$ (according to the way $\vc{Y}'$ is re-ordered), such that $\vc{Y}$ and $\vc{Y}'$ coincidence in all first $R$ coordinates and do not have any overlap in the remaining $n-R$ coordinates. 

Combining \eqref{eq:pr_thm1_3}, \eqref{eq:pr_thm1_4}, and \eqref{eq:pr_thm1_4_combined}, shows \eqref{eq:pr_thm1_2} which completes the proof.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Proof: Generalization Tail %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem~\ref{th:generalization_tail_hd_exp}} \label{pr:generalization_tail_hd_exp}
First note that by convexity of the function $h_D$ (\citep[Lemma~1]{sefidgaran2023minimum}), we have
\begin{align}
    h_D\left(\mathcal{\hat{L}}(S',W),\mathcal{\hat{L}}(S,W)\right) \leq \mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_D\left(\mathcal{\hat{L}}(\vc{Y}',\hat{\vc{Y}}'),\mathcal{\hat{L}}(\vc{Y},\hat{\vc{Y}})\right)\right].
\end{align}
Hence, it suffices to show that with probability at least $1-\delta$ over choices of $(S,S',W)$, 
\begin{align}    
\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_D\left(\mathcal{\hat{L}}(\vc{Y}',\hat{\vc{Y}}'),\mathcal{\hat{L}}(\vc{Y},\hat{\vc{Y}})\right)\right] \leq &\frac{ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right)
     +\log(n/\delta)}{n}\nonumber\\
+&\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right]. \label{eq:tail_hd_proof_1}
\end{align}
Similar to the proof of Theorem~\ref{th:generalizationExp_hd}, define
\begin{align*}
P'_1 \coloneqq & P_{\vc{U}|\vc{X},W_e}P_{\vc{U}'|\vc{X}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d}, \\
P'_2\coloneqq & Q_{\vc{U},\vc{U}'|\vc{X},\vc{X}',\vc{Y},\vc{Y}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d},\\
f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) \coloneqq &  h_D\left(\mathcal{\hat{L}}(\vc{Y}',\hat{\vc{Y}}'),\mathcal{\hat{L}}(\vc{Y},\hat{\vc{Y}})\right)-h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right).
\end{align*}
Using Donsker-Varadhan's inequality, we have
\begin{align}
n\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\Big[f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right)\Big]  \leq  & D_{KL}\left(P'_1\|P'_2\right) + \log \left(\mathbb{E}_{P'_2}\left[e^{nf\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right)}\right]\right)\nonumber \\
 = & D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right) + \log \left(\mathbb{E}_{P'_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right]\right). \label{eq:dv_tail}
\end{align}
Hence,
\begin{align}
\mathbb{P}\Bigg(\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right)\right]>&\frac{  D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right) +\log(n/\delta)}{n}\Bigg)\nonumber \\
&\hspace{1 cm}\stackrel{(a)}{\leq} \mathbb{P}\left(\log \left(\mathbb{E}_{P'_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right]\right)>\log(n/\delta)\right) \nonumber \\
&\hspace{1 cm}= \mathbb{P}\left(\mathbb{E}_{P'_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right]>n/\delta\right) \nonumber \\
&\hspace{1 cm}\stackrel{(b)}{\leq} \frac{\mathbb{E}_{S,S',W_e}\mathbb{E}_{P'_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right]}{n/\delta}  \nonumber \\
&\hspace{1 cm}\stackrel{(c)}{\leq} \delta,
\end{align}
where
\begin{itemize}
    \item $(a)$ follows by \eqref{eq:dv_tail},
    \item  $(b)$ is derived using the Markov inequality,
    \item and $(c)$ is shown in \eqref{eq:pr_thm1_2}.
\end{itemize}
This completes the proof.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Proof: differentially private %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition~\ref{prop:generalization_tail_hd_exp_partial}} \label{pr:generalization_tail_hd_exp_partial}
To state the proof, first, we need to recall the notion of $\beta$-approximate max-information; as previously defined in \citep[Definition~3.2]{dziugaite2018data} and \citep[Definition~3.2]{dziugaite2018data}. Here, we state the definition adapted to our setup. For ease of notation, denote
\begin{align}
    e_V(S) \coloneqq (S,\mathcal{A}(S))=(S,g(S,V)).
\end{align}

\begin{definition} \label{def:max_inf} Let $\beta\geq 0$. Then, define the $\beta$-max-information between $S$ and $\vc{Q}^{e_V(S)}$, denoted by $I_{\infty}^{\beta}$, as the minimal value $k$ such that for all product events $E$ and all fixed $V$, we have
\begin{align}
    \mathbb{P}\left(\big(S,\vc{Q}^{e_V(S)}\big)\in E\right) \leq e^{k} \mathbb{P}\left(\big(S,\vc{Q}^{e_V(\tilde{S})}\big)\in E\right)+\beta,
\end{align}
where $\tilde{S}$ is an independent dataset with the same distribution as $S$. %The max-information $I_{\infty}$ is defined as $I_{\infty}^{\beta}$ for $\beta=0$.    
\end{definition}
Fix some $\delta'>0$, which will be made explicit in the following. Now, ``similar'' to the proof of \citep[Theorem~4.2]{dziugaite2018data}, for any $\vc{Q} \in \mathcal{Q}$, define
\begin{align}
    R(\vc{Q}) = \left\{(S,S',W)\colon h_D\left(\mathcal{\hat{L}}(S',W),\mathcal{\hat{L}}(S,W)\right) > \Delta(S,S',W,\vc{Q},\delta')\right\},
\end{align}
where
\begin{align}    
\Delta(S,S',W,\vc{Q},\delta') \coloneqq & \frac{ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q} \
     \right)
     +\log(n/\delta')}{n}\nonumber\\
&+\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right].
\end{align}
Fix some $\beta>0$. For every fixed $S'$ and $V$, by Definition~\ref{def:max_inf}, we know that 
\begin{align*}
    \mathbb{P}\left((S,W,S') \in R(\vc{Q}^{e(S)})|S',V\right) \leq e^{I_{\infty}^{\beta}} \mathbb{P}\left((S,W,S') \in R(\vc{Q}^{e_V(\tilde{S})})|S',V\right)+\beta,
\end{align*}
where $\tilde{S}$ is independent of $(e(S),S')$. Hence,
\begin{align}
    \mathbb{P}_{S,W,S'}\left((S,W,S') \in R(\vc{Q}^{e_V(S)})\right) \leq &e^{I_{\infty}^{\beta}} \mathbb{P}_{S,W,S'}\left((S,W,S') \in R(\vc{Q}^{e_V(\tilde{S})})\right)+\beta \nonumber \\
    \stackrel{(a)}{\leq} & e^{I_{\infty}^{\beta}} \delta'+\beta,
\end{align}
where $(a)$ is derived since by Theorem~\ref{th:generalization_tail_hd_exp}, we know that $\mathbb{P}\left(R(\vc{Q})\right) \leq \delta'$ for every $\vc{Q}$ independent of $S$ and $S'$. Recall that strong symmetry implies symmetry.

Let $\beta=\delta/2$ and $\delta \coloneqq e^{I_{\infty}^{\delta/2}} \delta'+\delta/2$. Equivalently, 
\begin{align*}
    \delta' \coloneqq  \frac{\delta e^{-I_{\infty}^{\delta/2}} }{2}.
\end{align*}
With these choices, with probability $1-\delta$ over choices of $(S,S',W)$, we have
\begin{align}
    h_D\left(\mathcal{\hat{L}}(S',W),\mathcal{\hat{L}}(S,W)\right) \leq &\Delta(S,S',W,\vc{Q},\delta') \nonumber \\
    = & \frac{ D_{KL}\left(P_{\vc{U}, \vc{U}'|\vc{X},\vc{X}',W_e} \big\| \vc{Q}^{e(S)} \
     \right)
     +\log(2n/\delta)+I_{\infty}^{\delta/2}}{n}\nonumber\\
&+\mathbb{E}_{\hat{\vc{Y}},\hat{\vc{Y}}'|\vc{Y},\vc{Y}'}\left[h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right)\right].
\end{align}
The final result follows by \citep[Theorem~20]{dwork2015generalization}, where they showed that
\begin{align*}
    I_{\infty}^{\delta/2} \leq \frac{n}{2}\varepsilon_p^2+\varepsilon_p \sqrt{\frac{n\log(4/\delta)}{2}}.
\end{align*}
This completes the proof.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Proof: Partial symmetry %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition~\ref{prop:generalizationExp_hd_partial}}\label{pr:generalizationExp_hd_partial}
Recall the following notations in the proof of Theorem~\ref{th:generalizationExp_hd}:
\begin{align*}
P_1 \coloneqq & P_{S,W}P_{S'} P_{\vc{U}|\vc{X},W_e}P_{\vc{U}'|\vc{X}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d}, \\
P_2\coloneqq & P_{S,W}P_{S'} Q_{\vc{U},\vc{U}'|\vc{X},\vc{X}',\vc{Y},\vc{Y}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d},\\
f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) \coloneqq &  h_D\left(\mathcal{\hat{L}}(\vc{Y}',\hat{\vc{Y}}'),\mathcal{\hat{L}}(\vc{Y},\hat{\vc{Y}})\right)-h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right).
\end{align*}
Using the identical steps as in the proof Theorem~\ref{th:generalizationExp_hd}, we have
\begin{align*}
\mathbb{E}_{\vc{S},\vc{S}',W,\hat{\vc{Y}},\hat{\vc{Y}}'}\Big[f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right)\Big]  \leq 
  & \text{MDL}(\vc{Q}) + \log \left(\mathbb{E}_{P_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right]\right).
\end{align*}
Hence, it remains to show that 
\begin{align}
    \mathbb{E}_{P_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right] \leq \delta e^{2n}+n e^{\epsilon}. \label{eq:pr_partial_2}
\end{align}
Let $\Pi_{\vc{Y},\vc{Y}'}$ denote the set of all permutations that preserve the labeling. Denote the size of this set as $N_{\pi,\vc{Y},\vc{Y'}}\coloneqq N$. Then, the prior
\begin{align}
    \tilde{\vc{Q}}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e) \coloneqq \frac{1}{N}\sum_{\pi \in \Pi_{\vc{Y},\vc{Y}'}} \vc{Q}(\vc{U}_{\pi},\vc{U'}_{\pi}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e),
\end{align}
is symmetric in the sense of Definition~\ref{def:symmetry}. Furthermore, by Definition~\ref{def:partial_symmetry}, we have with probability at least $1-\delta$ over choices of $(S',S,W_e,\vc{U},\vc{U'}) \sim P_{S'} P_{S,W_e} \vc{Q}$,
\begin{align}
     \vc{Q}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e) \leq e^{\epsilon} \tilde{\vc{Q}}(\vc{U},\vc{U'}|\vc{Y},\vc{Y'},\vc{X},\vc{X'},W_e).
\end{align}
Hence, since $f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) \leq 2$, we have that
\begin{align}
    \mathbb{E}_{P_2}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right] \leq \delta e^{2n}+e^{\epsilon} \mathbb{E}_{P_3}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right], \label{eq:pr_partial_3}
\end{align}
where
\begin{align*}
P_3 \coloneqq &  P_{S,W}P_{S'} \tilde{Q}_{\vc{U},\vc{U}'|\vc{X},\vc{X}',\vc{Y},\vc{Y}',W_e}P_{\vc{\hat{Y}}|\vc{U},W_d}P_{\vc{\hat{Y}}'|\vc{U}',W_d}.
\end{align*}
The result now follows since $\tilde{Q}$ is symmetric and hence identical to the proof of Theorem~\ref{th:generalizationExp_hd}, we have
\begin{align}
    \mathbb{E}_{P_3}\left[e^{n f\left(\vc{Y},\vc{Y}',\vc{\hat{Y}},\vc{\hat{Y}'}\right) }\right] \leq n. 
\end{align}
This completes the proof.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Proof: Lemma %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:claim}} \label{pr:claim}
 For ease of notations, for $i\in[n]$, denote 
\begin{align*}
    \ell_{i,\pi_R} \coloneqq& \frac{1}{n}\mathbbm{1}_{\{\hat{Y}_{\pi_R,i}\neq Y_{\pi_R,i}\}}, \\
    \ell'_{i,\pi_R} \coloneqq& \frac{1}{n}\mathbbm{1}_{\{\hat{Y}'_{\pi_R,i}\neq Y'_{\pi_R,i}\}}.
\end{align*}
Consider similar notations for the mapping $\pi$ to define $\ell_{i,\pi}$ and $\ell'_{i,\pi}$. Furthermore, denote
\begin{align*}
    \Delta \ell \coloneqq & \sum_{i=1}^n \left(\ell_{i,\pi_R}-\ell_{i,\pi}\right)=\sum_{i=R+1}^n \left(\ell_{i,\pi_R}-\ell_{i,\pi}\right),\\
    \Delta \ell' \coloneqq & \sum_{i=1}^n \left(\ell'_{i,\pi_R}-\ell'_{i,\pi}\right)=\sum_{i=R+1}^n \left(\ell'_{i,\pi_R}-\ell'_{i,\pi}\right).
\end{align*}
It is easy to verify that $ \Delta \ell= - \Delta \ell'$ and 
\begin{align}
    \left| \Delta \ell\right| \leq \frac{1}{n}(n-R) = \frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1.
\end{align} 

With these notations,
\begin{align}
f\left(\vc{Y}_{\pi_R},\vc{Y}'_{\pi_R},\vc{\hat{Y}}_{\pi_R},\vc{\hat{Y}'}_{\pi_R}\right) = & h_D\left(\sum_{i=1}^n \ell'_{i,\pi_R} ,\sum_{i=1}^n\ell_{i,\pi_R}\right)-h_{\vc{Y},\vc{Y}',\hat{\vc{Y}},\hat{\vc{Y}}'}\left(\frac{1}{2}\left\|\hat{p}_{\vc{Y}}-\hat{p}_{\vc{Y}'}\right\|_1\right) \nonumber \\
\stackrel{(a)}{\leq} & h_D\left(\sum_{i=1}^n \ell'_{i,\pi_R} -\Delta \ell' ,\sum_{i=1}^n\ell_{i,\pi_R}-\Delta \ell\right)\nonumber \\
=&  h_D\left(\sum_{i=1}^n \ell'_{i,\pi} , \sum_{i=1}^n \ell_{i,\pi}\right),
\end{align}
which completes the proof, assuming the step (a) holds. 

It then remains to show the step $(a)$. To show this step, it is sufficient to prove that for every $x_1,x_2\in[0,1]$, $\tilde{\epsilon}\in \mathbb{R}^+$, and $\epsilon\in \mathbb{R}$ such that $(x_1+\epsilon),(x_2-\epsilon)\in [0,1]$ and $|\epsilon| \leq \tilde{\epsilon}$, the below inequality holds:
\begin{align}
 h_{D}\left(x_1 , x_2\right) -h_C\left(x_1,x_2;\tilde{\epsilon}\right) \leq h_{D}\left(x_1+\epsilon , x_2-\epsilon\right) .
\end{align}
Without loss of generality, assume that $x_1 \leq x_2$. We show the above inequality for different ranges of $\epsilon$, separately.
\begin{itemize}[leftmargin=*] 
\item If $\epsilon\leq 0$, then since by \citep[Lemma~1]{sefidgaran2023minimum}, $h_D(x;x_2)$ is decreasing in the real-value range of $x\in[0,x_2]$ and $h_D(x_1;x)$ is increasing in the real-value range of $x\in[x_1,1]$, we have
\begin{align*}
     h_{D}\left(x_1 , x_2\right) - h_{D}\left(x_1+\epsilon , x_2-\epsilon\right) \leq & 0 \\
     \leq & h_C\left(x_1,x_2;\tilde{\epsilon}\right),
\end{align*}
where the last inequality follows using the fact that $h_C$ is non-negative. 


    \item If $\epsilon \geq x_2-x_1$, then by letting $\epsilon'= (x_2-x_1)-\epsilon\leq 0$, we have
\begin{align*}
     h_{D}\left(x_1 , x_2\right) - h_{D}\left(x_1+\epsilon , x_2-\epsilon\right) = & h_{D}\left(x_1 , x_2\right) - h_{D}\left(x_2-\epsilon' , x_1+\epsilon'\right)  \\
    \stackrel{(a)}{=}& h_{D}\left(x_1 , x_2\right) - h_{D}\left(x_1+\epsilon',x_2-\epsilon' \right)\\
     \stackrel{(b)}{\leq}  & 0 \\
     \stackrel{(c)}{\leq} & h_C\left(x_1,x_2;\tilde{\epsilon}\right),
\end{align*}
where $(a)$ is deduced by the symmetry of $h_D$ and steps $(b)$ and $(c)$ are deduced similar to the case $\epsilon \leq 0$ above.

\item If $\epsilon\in [0,(x_2-x_1)/2]$, then we have
\begin{align*}
     h_{D}\left(x_1 , x_2\right) - h_{D}\left(x_1+\epsilon , x_2-\epsilon\right) =&h_b(x_1+\epsilon)+h_b(x_2-\epsilon)- h_b(x_1)-h_b(x_2)\nonumber\\
     \leq &h_C(x_1,x_2;\tilde{\epsilon}),
\end{align*}
where the last step follows by definition of the function $h_C$, and since $\epsilon$ belongs to the below interval:
\begin{align}
[0,\tilde{\epsilon}] \cap [0,(x_{1\lor 2}-x_{1\land 2})/2].
\end{align}


\item If $\epsilon\in [(x_2-x_1)/2,(x_2-x_1)]$, then by letting $\epsilon'= (x_2-x_1)- \epsilon$, we have $\epsilon'\in [0,(x_2-x_1)/2]$ and
\begin{align*}
     h_{D}\left(x_1 , x_2\right) - h_{D}\left(x_1+\epsilon , x_2-\epsilon\right) =&h_b(x_1+\epsilon')+h_b(x_2-\epsilon')- h_b(x_1)-h_b(x_2)\nonumber\\
     \leq &h_C(x_1,x_2;\tilde{\epsilon})
\end{align*}
where the last step follows by definition of the function $h_C$, and since $\epsilon$ belongs to the below interval:
\begin{align}
[0,\tilde{\epsilon}] \cap [0,(x_{1\lor 2}-x_{1\land 2})/2].
\end{align}
Note that $\epsilon'\leq \tilde{\epsilon}$, since $\epsilon'\in [0,(x_2-x_1)/2]$ and $\epsilon\in [(x_2-x_1)/2,(x_2-x_1)]$. Hence, $\epsilon'\leq \epsilon$, and by assumption $\epsilon\leq \tilde{\epsilon}$.
\end{itemize}

This completes the proof of the lemma.






\end{document}