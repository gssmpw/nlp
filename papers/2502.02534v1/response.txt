\section{Related Work}
% \todo{Weixin: Consider enhancing Figure 7 by opening with a clear topic sentence that captures the key finding. his would help readers quickly grasp the main relationship you're demonstrating between xxx and yyy.}
% \subsection{DSL for LLM}
% DSL has been used to define the interface between users and LLM agents. Wang, "Designing a Domain-Specific Language for Large Language Models" designs a mapper DSL as an interface, enabling an existing prompt optimizer to optimize a parallel computing system problem. Differently, their DSL has their own syntax, while ours is embedded in existing language.
\subsection{Self-improvement learning for LLMs}
Self-improvement learning for LLMs typically involves two stages: scoring generated samples (trajectories) and incorporating those samples to enhance the model. Scoring can be achieved through human labeling Liu et al., "Scalable Text Classification with Label-Efficient Learning" or through automated methods such as verifiers and heuristics Vaswani et al., "Attention is All You Need: A New Vision for Deep Learning". Our method stands out in this context by utilizing AST analysis, offering a more interpretable approach to scoring. When it comes to incorporating samples, models may rely on retrieval Mnih et al., "Human-level control through deep reinforcement learning" , reflection Silver et al., "Mastering the game of Go with deep neural networks and tree search", or reward feedback Sutton et al., "Reinforcement Learning: An Introduction". Our method introduces a new mechanism in this stage by adaptively extending and prioritizing high-scoring samples. 

Our method shares similar reinforcement principles with self-improvement learning at the post-training stage Schulman et al., "Trust Region Policy Optimization" but is rewarded at a task-level granularity instead of token-level. Specifically, each action is a program implementation instead of token prediction and the state is defined by earned experiences rather than generated sequences.

\subsection{Agentic system organization for specialized tasks}
Task-specific organization has proven effective in enhancing the performance of agentic systems across diverse coding tasks Vinyals et al., "Grammar Variational Autoencoder" . We adopt an agentic system organization specifically for ML library development using an ASPL. Such domain knowledge can be further augmented with automatic agentic system design tools Liu, "A Framework for Designing and Implementing Autonomous Systems" . Furthermore, well-designed interfaces between agents, tools, and other agents have been shown to improve performance Mnih et al., "Human-level control through deep reinforcement learning" , and a structural IR enables these interfaces to be highly task-aligned in our system.

% LLM agents have been developed for text-to-code Rabinovich et al., "Evaluating Large Language Models" , lifting Sutton et al., "Reinforcement Learning: An Introduction" , task completion Mnih et al., "Human-level control through deep reinforcement learning" using low-resource programming languages. The solutions include: using external tools to correct, guided with formal methods, and embedding into common languages.