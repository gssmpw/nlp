 %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\usepackage{listings}
\lstdefinelanguage{step}{
    keywords={def, inputs, fns, outputs, impl},
    keywordstyle=\color{blue}\bfseries,
    identifierstyle=\color{black}\bfseries,
    sensitive=false,
    comment=[l]{\#},
    commentstyle=\color{purple}\ttfamily,
    stringstyle=\color{red}\ttfamily,
    alsoletter={=},
}
\newcommand{\step}[1]{\lstinline[language=step, basicstyle=\scriptsize\ttfamily]{#1}\xspace}

\newcommand{\primitive}[1]{\textcolor{blue}{#1}}
\newcommand{\algorithmicparam}{\textbf{param}}
\newcommand{\PARAM}{\item[\algorithmicparam]}
\newcommand{\algorithmicfunc}{\textbf{func}}
\newcommand{\FUNC}{\item[\algorithmicfunc]}
\newcommand{\ITYPE}[1]{\STATE[] \textbf{input:} #1 \addtocounter{ALC@line}{-1}}
\newcommand{\OTYPE}[1]{\STATE[] \textbf{output:} #1 \addtocounter{ALC@line}{-1}}
\newcommand{\FTYPE}[1]{\STATE[] \textbf{type:} #1 \addtocounter{ALC@line}{-1}}
\newcommand{\FN}[1]{\STATE[] \textbf{fn} #1 \addtocounter{ALC@line}{-1}}
\newcommand{\INIT}[1]{\STATE[] \textbf{init: } #1 \addtocounter{ALC@line}{-1}}

\definecolor{customblue}{HTML}{719AAC}
\newcommand{\dblue}[1]{\textcolor{customblue}{#1}}
\definecolor{customorange}{HTML}{E29135}
\newcommand{\dorange}[1]{\textcolor{customorange}{#1}}
\definecolor{customgreen}{HTML}{72B063}
\newcommand{\dgreen}[1]{\textcolor{customgreen}{#1}}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Adaptive Self-improvement LLM Agentic System for ML Library Development}



\usepackage{xcolor}   % For text coloring
% \usepackage{sfmath}   % For sans-serif math font (optional, for \textsf)
\usepackage{xspace}
\newcommand{\weixin}[1]{\noindent{\textcolor{blue}{\textbf{Weixin: } {#1} }}} % Just for comments 
\newcommand{\genghan}[1]{\noindent{\textcolor{blue}{\textbf{Genghan: } {#1} }}} % Just for comments


\begin{document}

\twocolumn[
\icmltitle{Adaptive Self-improvement LLM Agentic System for ML Library Development}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}





\begin{icmlauthorlist}
\icmlauthor{Genghan Zhang}{yyy}
\icmlauthor{Weixin Liang}{yyy}
\icmlauthor{Olivia Hsu}{yyy}
\icmlauthor{Kunle Olukotun}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Computer Science, Stanford University, Stanford, USA}

\icmlcorrespondingauthor{Genghan Zhang}{zgh23@stanford.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.



\begin{abstract}
ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\times$ over a baseline single LLM.
\end{abstract}

\section{Introduction}
\label{intro}
\begin{figure}[htb]
\centering
\includegraphics[width=\linewidth]{figs/system-diagram.pdf}
\caption{We propose an adaptive self-improvement LLM agentic system. Similar to human experiential learning~\cite{kolb2014experiential}, LLM agents start from their base knowledge and accumulate experiences through parallel sampling. Our adaptive self-improvement learning algorithm filters high-quality answers, stratifies the earned experiences by difficulty, and adaptively selects demonstrations to enhance LLM agents. 
}
\label{fig:system-diagram}
\end{figure}
With the ending of Dennard Scaling and Mooreâ€™s Law, computer architectures are specializing in domain applications to achieve greater performance and efficiency and will continue to do so~\cite{hennessy2019new}.
New domain-specific architectures (DSA) typically come with new architecture-specific programming languages (ASPL), such as CUDA for NVIDIA GPUs~\cite{cuda}, HIP for AMD GPUs~\cite{hip}, and Pallas for Google TPUs~\cite{pallas}. Even existing ASPLs change as generations of DSAs evolve because new DSAs introduce specialized functions to these existing ASPLs~\cite{choquette2023nvidia}. 
% For example, NVIDIA's latest Hopper GPU architecture contains multiple programmable hardware features, including asynchronous units on top of its original general-purpose vector processors~\cite{choquette2023nvidia}. 
Efficiently utilizing these new functions requires fundamentally different programming styles and thus new ASPLs~\cite{cutlass2023,hagedorn2023graphene}. 
% \todo{Weixin: For Figure 1 caption, Maybe good to start the first sentence direclty with "We propose", and also, good to explicitly put our methods' name in the caption as well. }

Each DSA needs a corresponding ML library, a collection of ML operators written in its ASPL, before programmers can effectively use the DSA to accelerate ML applications. 
%such as cuDNN~\cite{chetlur2014cudnn} and MIOpen~\cite{miopen}.
ML library development is challenging because it requires expertise in both ML algorithms and the target ASPL. Essentially, library development is a generation process that composes low-level ASPL primitives into high-level ML operators~\cite{dong2024flex,ye2025flashinfer}. 


% Stream value is of generic data type or reference type, and thus a chunk of memory can be randomly accessed by the reference like tensor.  


% LLMs have shown surprising in-context learning capabilities~\cite{brown2020language}. In-context learning is a good fit for library development because data are expensive to collect for a new ASPL. To improve such in-context learning process, people have proposed various recipes such as zero/few/many-shot~\cite{kojima2022large,brown2020language,agarwal2024many}, chain-of-thought~\cite{wei2022chain}, sampling with verifiers~\cite{uesato2022solving,lightman2023let}, and agents~\cite{yang2024swe,anthropic:agent} in order to better leverage the test time computation. However, it is unclear what test-time compute recipes are effective on the ML library development task. 

% LLM agent means using LLM's text generation capability to solve domain-specific tasks~\cite{wu2023autogen,yang2024swe,anthropic:agent}. We construct the \textit{agentic system organization} by decomposing the task into multiple LLM agents that generate and self-correct. The task is to generate a STeP program that implements the ML operator using given specialized functions. Besides, we augment the native text interface among LLM agents with a \textit{structural intermediate representation} that serves as a concise interface among users, LLM agents, and verifiers.

% LLMs have shown surprising learning capabilities through in-context learning without requiring parametric updates~\cite{brown2020language}. In-context learning is a good fit for library development because data are expensive to collect for a new ASPL and the sunk cost of updating parameters can be high because of the versatility of ASPLs. The challenge lies in generating more data and effectively utilizing data produced by LLMs. To address this, we propose a search strategy called \textit{adaptive sampling} that combines parallel sampling with adaptive selection of good examplesâ€”defined as hard tasks paired with succinct answers, inspired by the sampling and filtering mechanism of AlphaCode~\cite{li2022competition,leblond2023alphacode2}. Taking one step further, these sampled good examples facilitate \textit{self-improvement learning}, which tailors the reinforced self-training~\cite{gulcehre2023reinforced,singh2023beyond} idea to in-context learning, where demonstrations include completed tasks generated by LLMs themselves. Instead of including all completed tasks in the prompt, adaptive sampling first selects the hard tasks where LLMs exhibit high uncertainty and gradually incorporates tasks that LLMs handle with higher confidence. The adaptive granularity is controlled by a hyperparameter. Adaptive sampling also strategically filters good answers from those discovered by parallel sampling, with a preference for succinctness to gain denser information from the generated data.
ML library development using ASPLs requires complex reasoning while minimizing data requirements. % Developers create modern ASPLs with simulation before the physical chip of a DSA is ready
ML libraries are developed simultaneously as the chip is manufactured to meet the production timeline~\cite{villa2021need}. This library--chip co-design process creates such a tight timeline that ASPLs have only a limited number of code examples. Moreover, this task is complicated even for experienced human programmers. 
For example, the publication of FlashAttention-3~\cite{shah2024flashattention} lagged behind the release of the H100 by two years. Directly adapting FlashAttention-2~\cite{dao2023flashattention} from A100 to H100 GPU witnessed a 47\% performance drop~\cite{spector2024thunderkittens}.

The challenges in ML library development call for more automatic solutions. Furthermore, these automatic solutions need to self-improve to perform complex reasoning starting from simple and limited data. Large language models (LLMs) have demonstrated emerging capabilities in code generation~\cite{kaplan2020scaling,wei2022emergent}. Moreover, empirical evidence implies that LLMs already have the base knowledge of ML algorithms~\cite{ouyang2024kernelbench}. Therefore, we explore the use of LLM agents to develop ML libraries with emerging ASPLs.

Current self-improvement methods for LLM agents fall short because of limited exploration or low data efficiency. LLM agents can enhance their performance by synthesizing semantically similar data~\cite{yu2023metamath,shinn2024reflexion,zhao2024expel}.
%similarity-based techniques such as paraphrasing questions~\cite{yu2023metamath}, interpreting answers~\cite{shinn2024reflexion}, or summarizing experiences~\cite{zhao2024expel}.
Although these methods are effective for local exploration~\cite{chen2024self}, they are insufficient for tasks that require substantial cognitive effort~\cite{huang2023large}. Self-improvement learning can significantly improve reasoning ability through reinforcement learning~\cite{cobbe2021training,bai2022constitutional,singh2023beyond}. This approach, however, currently requires hundreds of effective trajectories sampled from LLM agents for each problem~\cite{wang2024math}, making it unsuitable for complex scenarios with limited data availability. 

% Self-improvement learning that improves models with self-generated data has shown efficacy on LLM agents~\cite{zhao2024expel,shinn2024reflexion}. We address a different challenge compared to these general methods. While their focus is on condensing extensive knowledge into a limited context window, our challenge lies in completing tasks with minimal data. This is due to the high cost of collecting high-quality ASPL data, which requires training humans to become experts. To address this challenge, we introduce adaptivity into the learning process, resulting in adaptive self-improvement learning. 

%This also causes different principles of \textit{good} demonstrations for a task: similarity vs. difficulty.

% To enable complex reasoning with limited data, 
To address these limitations, we design an adaptive self-improvement learning algorithm integrated with an agentic system organization. This approach not only produces a self-improving agentic system to assist humans but also generates high-quality ML operators that can be leveraged by other systems. We show our system in~\cref{fig:system-diagram}. Our techniques create a self-improvement cycle: LLM agents evolve through earned experiences and these evolved agents can earn more experiences. This self-improvement cycle is fully automated, involving no human experts beyond the ASPL designers themselves, who initially tell the models how to use the ASPL primitives. 

Inspired by curriculum learning~\cite{bengio2009curriculum}, our algorithm prioritizes hard-earned experiences gained from challenging task completion. When these hard-earned experiences are used up, the algorithm adaptively increases the number of demonstrations by mixing experiences earned from less challenging tasks. \Cref{sec:exp-learn} shows that hard-earned experiences improve LLM agents more efficiently than mixed ones. Mixed experiences, while sometimes diluting demonstrations, can help agents overcome learning obstacles and complete more tasks. As a byproduct, the algorithm adaptively increases test-time compute on challenging tasks until they are finished or the data is used up.

% \todo{owhsu: the key feedback is that the following paragraph doesn't really fit here. Figure out a new organization of the introduction paragraphs}
To emulate the library--chip co-design process, we choose Streaming Tensor Programs (STeP) as the target ASPL for library generation. STeP is an emerging ASPL designed for next-generation reconfigurable dataflow architectures~\cite{prabhakar2017plasticine}, a family of DSAs for AI~\cite{prabhakar2021sambanova,chen2023ai,prabhakar2024sambanova}. 
The only public document of STeP is a non-archival three-page workshop publication~\cite{sohn2024streaming}, which defines its semantics without any code examples or execution environments. Therefore, STeP programs do not exist in the training corpus of any LLM. 
\begin{figure}[htb]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figs/all-models-methods.pdf}}
\caption{Portion of completed tasks (Pass@n) across models using single LLM, agentic system, and adaptive self-improvement agentic system, highlighting performance improvement.}
\label{fig:all-methods}
\end{center}
\end{figure}

Putting all these together, our system solves up to 96\% of the tasks in our benchmark and achieves up to $3.9\times$ improvements over a baseline single LLM, as shown in~\cref{fig:all-methods}. The contributions of this paper are: 
(1) an adaptive self-improvement
learning algorithm that enables LLM agents to continuously
construct ML libraries through adaptive experience-driven
evolution; (2) an end-to-end agentic system that uses
adaptive self-improvement to develop an ML library for
STeP, an ASPL for a next-generation AI accelerator; (3)
a complete evaluation of the adaptive self-improvement learning algorithm and the integrated agentic system on a realistic benchmark constructed
from common ML operators.
%(1) an adaptive self-improvement learning that enables LLM agents to continuously construct ML libraries through adaptive experience-driven evolution; (2) an end-to-end agentic system with adaptive self-improvement for ML library development using STeP, an ASPL for a next-generation AI accelerator; (3) insights into the adaptive self-improvement learning integrated with the agentic system on a benchmark we construct from common ML operators.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/bench.pdf}
    \caption{We benchmarked 8 groups of ML operators for a common LLM layer~\cite{jiang2024mixtral}. They can be categorized into \dgreen{dynamic}, \dorange{static matrix}, and \dblue{static vector} operators where parentheses indicate the number of tasks associated with each group of ML operators.}
    \label{fig:bench}
\end{figure}


\section{Background}
% \weixin{
% Nice background intro! Perhaps it might be helpful to consider adding a reference table that maps technical abbreviations of the key concepts (ASPL, STeP, RDA) to their full names? This could help readers more easily follow the technical discussions, especially for those new to the field.}
In this section, we provide background on how DSAs are programmed through their ASPLs, describe how these ASPLs are used to create end-user ML libraries, and identify the key challenges of this library generation process. We also establish STeP as the target ASPL to explore LLM techniques for ML library development. Key concepts related to the background are listed in~\cref{tab:abbv} for reference.

\subsection{Architecture-specific programming languages}
ASPLs describe the low-level programming interface of a DSA using primitives and specialized functions. Primitives model the basic execution pattern similar to general-purpose programming language constructs, and specialized functions control specialized accelerator units that are optimized for domain applications on the DSA. 
Unlike domain-specific languages (DSLs), which are a top-down distillation of the domain algorithms, ASPLs refer to a bottom-up abstraction of the underlying chip architecture. 
% In the library--chip co-design process, ASPL is created with simulation before the physical chip is ready.
% \todo{Weixin: Figure 3 caption: Maybe start directly with a first sentence with "We benchmarked" }
% \todo{Weixin: Figure 4 caption: add a short phrase before (a). What this whole figure is about. }

% Instruction Set Architecture (ISA) describes a similar concept but only for instruction-driven architectures like CPU and GPU~\cite{asanovic2014instruction}. The core concept of ASPLs are similar to ``Abstract Machines'' or ``Virtual Machines'', but with an emphasis on being directly programmable~\cite{lattner2004llvm}.

\subsection{ML library development using ASPLs}
ML libraries developed in ASPLs face portability challenges because ASPLs rapidly evolve to align with DSA updates to meet the demand of growing ML workloads.
% than before because the modification of architecture has to be visible in the language for better efficiency. 
% In the old days, ASPL was portable across architectures, where libraries only needed some parameter tuning for new architectures.  However, the free lunch of data parallelism ends. 
For example, the matrix multiplication units on NVIDIA GPUs and their corresponding MMA instructions have been updated every generation since their first introduction~\cite{nvidia:v100}. 
Consequently, every library function that uses MMA instructions must be rewritten in a new ASPL per generation. Moreover, ML libraries need to be shipped at the same time as the chip. In this case, library development costs are no longer negligible. % However, application users' expectation of the pace of efficiency improvement remains constant if not increasing.

To solve these challenges, we propose to enhance users' learning capabilities for a given ASPL. This approach offers an alternative to current automation techniques that focus on simplifying the learning curve for new ASPLs. Mainstream automation techniques compromise by optimizing ML operators whose performance is most significantly affected by the ASPL update~\cite{tillet2019triton,cutlass2023}. 
%such as CUTLASS for matrix multiplication and convolution~\cite{cutlass2023}, FlashAttention~\cite{dao2022flashattention,dao2023flashattention,shah2024flashattention}, and Triton for matrix multiplication with epilogue computation~\cite{tillet2019triton}. 
Only focusing on certain ML operators does not fully incorporate some ASPL updates, such as memory optimizations, which could potentially accelerate any ML operator. Meanwhile, new ML operators are being proposed~\cite{gu2023mamba,sun2024learning}. Given these factors, we need better automation to improve the productivity of ML library development using APSLs.

\begin{figure}[thb]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figs/step-with-rda.pdf}}
\caption{
STeP is an ASPL for next-generation RDAs. (a) diagrams a STeP program for a simplified MoE module. More details can be found in~\cref{sec:step-appdix}. (b) illustrates the streaming dataflow execution of (a) on an RDA where ``MU'' stands for memory unit, and ``CU'' stands for compute unit. (c) is the SN40L, a deployed RDA chip.}
\label{fig:rda}
\end{center}
\end{figure}

\subsection{STeP for next-generation RDA}
\label{sec:step-intro}
We chose STeP as our target ASPL due to its potential for better efficiency and its status as a research prototype ASPL. STeP's efficiency potential stems from its role as the ASPL for next-generation RDAs, which have emerged as a promising alternative to GPUs. The SN40L, a deployed RDA implementation shown in \Cref{fig:rda}(c), demonstrates record-breaking inference speeds for the Llama 3.1 405B model~\cite{sn:record}. Although STeP does not yet have a path to a fabricated chip, developing ML libraries in STeP still presents similar challenges as other ASPLs. Writing STeP programs requires complex reasoning about streaming dataflow execution, and our work began without any existing executable STeP programs to reference.
% We choose STeP instead of ASPLs with native hardware like CUDA because LLMs have already learned them~\cite{ouyang2024kernelbench,chaturvedi2024hpc}. Generations of ASPLs can share the same basis because DSAs usually evolve incrementally. Although designed for next-generation RDAs, STeP still follows the basic dataflow execution model of an RDA as shown in~\cref{fig:rda}(b). The memory and compute units can be programmed to execute different functions. These functions are fused on chip, allowing better operator-level pipeline parallelism. For example, Type 4 and 5 operators in~\cref{fig:bench} are automatically fused into a complete attention operator on chip.

Like other ASPLs, STeP's operational semantics are expressed by primitives and specialized functions. Specifically, STeP primitives describe different stream token manipulation strategies, and STeP specialized functions express different configurations of RDA units. STeP adds streaming to the conventional dataflow execution model of RDAs as shown in~\cref{fig:rda}(b). Streaming dataflow unifies both data values and control signals as stream tokens, baking the control flow directly into the data, for better dynamism. The streaming dataflow execution model in STeP is inspired by parallel patterns and array programming~\cite{hsu2023sparse,rucker2024revet}. A stream can be consumed by at most one primitive because of the queueing nature of dataflow~\cite{zhang2021high}, which is called an affine type constraint in programming language theory~\cite{wiki:affinetype}. The affine type constraint is a global property of the program since it counts the usage of a variable in the whole program.

STeP primitives are categorized as either arithmetic or shape manipulation. Arithmetic primitives apply computations and control flow to stream tokens. Shape manipulation primitives reshape the data within the stream by changing the control tokens. ~\Cref{fig:rda}(a) and~\Cref{fig:example-pattern} are example STeP programs that contain 5 arithmetic primitives and only shape manipulation primitives, respectively.
%~\cref{sec:exp-agent} analyzes the agentic system's performance on these two kinds of primitives.

% Implementing~\cref{eq:moe-module} with~\cref{fig:step-intro} exemplifies ML library construction using an ASPL. STeP simplifies real-world ASPLs by omitting architecture parameters and memory interface. However, STeP captures the key gradients of an ASPL: programmable DSA interface and low-level primitives, which is enough to emulate the challenge of developing ML libraries to LLMs.
% Therefore, we need a calculator instead of a human staring at the program.
% We express ML operators in PyTorch so that LLM's base knowledge of ML algorithms can be potentially better utilized. After these steps, the task is eventually transformed to \textit{translate PyTorch to equivalent STeP implementation} given the shape and data type of input and output streams and specialized functions.

% \subsection{Functional correctness and Type checking}
% \label{sec:verifer}
% As shown in~\cref{fig:step-intro}, STeP types are data-dependent. However, no general verification method exists for data-dependent types. Therefore, we have to compromise with unit test methods. We test the results on a single set of shapes with random input values. The fidelity of our unit test method builds on practice~\cite{jia2019taso} and theory~\cite{gulwani2003discovering}. The type checking can be done by static analysis on the abstraction syntax tree of the STeP program with the ast Python library.
\begin{algorithm}[tbh]
\caption{Adaptive self-improvement learning}
\label{alg:selsa}
\begin{algorithmic}[1]
\INPUT $\mathcal{X}$: task set, $m$: adaptive granularity
\REQUIRE $\theta$: LLM agentic system, $r$: reward from verifier, $\sigma$: filter function, $\beta$: selection function 
% \STATE $\mathcal{C}_0 \leftarrow \{\mathbb{E}_{y\sim p_{\theta}(y|x_i)} [r(x_i, y)] \mid x_i\in \mathcal{X}\}$ \hfill $\triangleright$ success rate
% \STATE $\mathcal{B}_0 \leftarrow \{(x_i, y) \mid r(x_i, y) = 1, x_i\in \mathcal{X}\}$ \hfill $\triangleright$ replay buffer
% \STATE $\mathcal{S}_0 \leftarrow \{x_i \mid c_i > 0, c_i\in \mathcal{C}_0\}$ \hfill $\triangleright$ newly solved tasks
% \STATE $\mathcal{D} \leftarrow \sigma(\mathcal{B}_0, \mathcal{C}_0, \mathcal{S}_0)$ \hfill $\triangleright$ select examples
% \STATE $\mathcal{X} \leftarrow \mathcal{X}\setminus \mathcal{S}_0$ \hfill $\triangleright$ remove solved tasks
\STATE $\mathcal{D} \leftarrow \emptyset$
\STATE $t\leftarrow 0$ \hfill $\triangleright$ iteration
\REPEAT
% \STATE \dblue{// Stratify}
\STATE $\mathcal{E} \leftarrow \beta(\mathcal{D}, m)$ \hfill $\triangleright$ stratification
\FOR{$e_j\in \mathcal{E}$}
% \STATE \dblue{// Select}
\STATE $d_j \leftarrow [e_0,e_1,...e_j]$ \hfill $\triangleright$ selection
\STATE \dblue{// Parallel sampling}
\STATE $\mathcal{C}_t \leftarrow \{\mathbb{E}_{y\sim p_{\theta}(y|x_i, d_j)} [r(x_i, y)] \mid x_i\in \mathcal{X}\}$ 
\STATE $\mathcal{B}_t \leftarrow \{(x_i, y) \mid r(x_i, y) = 1, x_i\in \mathcal{X}\}$ 
\STATE $\mathcal{S}_t \leftarrow \{x_i \mid c_i > 0, c_i\in \mathcal{C}_t\}$
\IF{$\mathcal{B}_t\neq \emptyset$}
% \STATE \dblue{// Filter}
\STATE $\mathcal{D} \leftarrow \mathcal{D} \cup \sigma(\mathcal{B}_t, \mathcal{C}_t, \mathcal{S}_t)$ \hfill $\triangleright$ filtering
\STATE $\mathcal{X} \leftarrow \mathcal{X}\setminus \mathcal{S}_t$
\STATE $t \leftarrow t+1$ 
\STATE \textbf{break}
\ENDIF
\STATE $t \leftarrow t+1$
\ENDFOR
\UNTIL {$\mathcal{X}=\emptyset \lor (d_j=\mathcal{D} \land \mathcal{B}_{t-1}=\emptyset)$}
\OUTPUT Solutions: $\mathcal{D}$
\end{algorithmic}
\end{algorithm}

\begin{figure*}[thb]
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{figs/self-improvement.pdf}}
\caption{Running example of~\cref{alg:selsa} with $m=3$ and $|\mathcal{X}|=5$. Orange circles are demonstrations $d_j$ for the current iteration. Green circles are finished tasks and white ones are not. $m=3$ means $\beta$ stratifies demonstrations into 3 levels: hard, medium, and easy. Iter 1 and 3 consider hard-only examples, and the other iterations consider mixed examples.
}
\label{fig:self-improve}
\end{center}
\end{figure*}

\section{Adaptive self-improvement learning}
\label{sec:method}
Our adaptive self-improvement learning evolves LLM agentic systems with data generated by themselves. This algorithm parallel samples the agentic system for correct answers with the success rate, filters high-quality correct answers, stratifies the earned experiences, and adaptively updates demonstrations until all the tasks are solved or the demonstrations are used up. \Cref{alg:selsa} shows the complete algorithm for adaptive self-improvement learning. As a byproduct, the algorithm adaptively assigns more test-time compute to harder tasks because it excludes a task from the task set after completion. 
\Cref{fig:self-improve} shows a running example, where only the unfinished tasks are fed to the agentic system. This algorithm is independent of the specific organization of the agentic system, as shown in~\cref{fig:system-diagram}. 

\begin{figure*}[htb]
\begin{center}
\centerline{\includegraphics[width=2\columnwidth]{figs/workflow-color.pdf}}
\caption{Details of our agentic system organization. (a) shows a single LLM. (b) shows the system components and their representations. The user is either a human or a self-improvement learning process. The filled colors align with the text colors in~\cref{sec:implementation}.}
\label{fig:workflow}
\end{center}
\end{figure*}

\subsection{Filtering high-quality answers}
The filter function, $\sigma$, collects earned experience $\mathcal{D}$ by filtering one answer for each newly solved task in $\mathcal{S}_t$ and records the success rate of this answer. $\sigma$ first groups the correct answers $\mathcal{B}_t$ by the isomorphic abstract syntax tree~\cite{knuth1968semantics}. Then, it randomly selects one answer from each isomorphic group. After that, it selects the one with the minimal length of pure code (excluding comments and empty lines) from these canonical representatives as the final answer for the task in $\mathcal{S}_t$. It also stores the success rate from $\mathcal{C}_t$ for $\beta$. The minimal length selection follows the Minimum Description Length principle for higher information density~\cite{rissanen1978modeling}. On the other hand, shorter text might lose chain-of-thought comments~\cite{wei2022chain}. To balance these two contradicting intuitions, we introduce randomness to the selection of representative answers for each isomorphic group. 

\subsection{Stratification and selection}
The selection function $\beta$ stratifies the earned experiences $\mathcal{D}$ by binning them into $m$ levels of difficulty and demonstrations $d_j$ are selected incrementally from the stratified experiences $\mathcal{E}$. We define the difficulty as the opposite of the success rate following~\cite{lightman2023let}. $\beta$ sorts $\mathcal{D}$ in ascending order of success rate and then bins the tasks as evenly as possible to get the boundaries of each bin. Then tasks are rebinned using these boundary values.
% uses the success rate of the last task in each segment as the bin edges. 
This selection strategy can cause repetitive steps as exemplified by the dash line circles (Iter 4\&5 in~\cref{fig:learning-curve}(a)) 
% Iter 3\&4, 5\&6 and 7\&9 ~\cref{fig:learning-curve}(b) and 
when the newly finished tasks are easier than the demonstrations. Our algorithm keeps these repetitive steps instead of avoiding them because the tasks with low success rates have a better chance of getting one correct answer with the number of samples doubled. For example, Iter 4 performs better than Iter 3 in~\cref{fig:learning-curve}(b) with the same demonstrations. This method can also cause later iterations to have fewer tokens but with higher quality than the previous iteration (Iter 2\&3 in~\cref{fig:learning-curve}(a)) when the boundary value crosses two bins and the newly finished tasks are easier.

\subsection{Discussion}
To use this technique in a continuous learning setting, users can add new tasks to the initial $\mathcal{X}$ and apply~\cref{alg:selsa}. This approach, however, does not guarantee success and might exceed context window constraints. If the task set $\mathcal{X}$ is finite, as in our case, the algorithm will terminate. 

% \begin{algorithm}
% \caption{Iterative Task Performance Improvement}
% \begin{algorithmic}[1]
% \REQUIRE{$k$ (number of groups), $\sigma$ (testbench), $\mathcal{M}$ (agentic system), $\pi_0$ (base prompt)}
% \OUTPUT{$R_t$}
% \STATE $R_0 \gets$ SAMPLE($\pi_0$)

% \STATE $t \gets 1$ \COMMENT{Iteration counter}
% \WHILE{TRUE}
%     \STATE $\Theta \gets$ PARTITION\_SUCCESSES($R_{t-1}, k$) \COMMENT{Get max success thresholds $\{\theta_1, ..., \theta_k\}$}
%     \FOR{$\theta_i$ in $\Theta$}
%         \STATE $F_t \gets \{x \in R_{t-1} : \sigma(x) = 0\}$ \COMMENT{Failed tasks where success $\sigma(x) = 0$}
%         \STATE $H_t \gets \{x \in R_{t-1} : 0 < \sigma(x) \leq \theta_i\}$ \COMMENT{Hard tasks}
%         \STATE $\pi_t \gets$ CONSTRUCT\_PROMPT($\pi_0, H_t$) \COMMENT{Base prompt $\pi_0$ with examples from $H_t$}
        
%         \STATE $R'_t \gets$ SAMPLE($\pi_t$) \COMMENT{Merge results across temperatures}
%         \STATE $R_t \gets$ MERGE\_BEST($\{R_{t-1}, R'_t\}$) \COMMENT{Merge with previous iteration}
        
%         \STATE $\lambda_{t-1} \gets |\{x \in R_{t-1} : \sigma(x) > 0\}|$ \COMMENT{Previous success count}
%         \STATE $\lambda_t \gets |\{x \in R_t : \sigma(x) > 0\}|$ \COMMENT{Current success count}
%         \STATE $t \gets t + 1$
        
%         \IF{$\lambda_t > \lambda_{t-1}$}
%             \STATE \textbf{break}
%         \ENDIF
%     \ENDFOR
    
%     \IF{$\theta_i = \theta_k$ \AND $\lambda_t = \lambda_{t-1}$}
%         \STATE \textbf{break}
%     \ENDIF
% \ENDWHILE
% \end{algorithmic}
% \end{algorithm}

% We formulate the ML library development using STeP as a prompt engineering problem by expressing target ML operators and available specialized functions in PyTorch and enabling LLMs to write STeP programs in Python. 
% This section explains how we formulate ML library development using an ASPL as a prompt engineering problem and build a simulation environment for parallel sampling and verification. 
% We first embed STeP into Python. Beneath this Python frontend is a functional simulator that calculates the result of STeP programs. Then we test the functional correctness against the ML operator written in PyTorch using unit tests and check the affine type constraint using static analysis. 

% It is shape-agnostic because the shape transformation is implemented symbolically using Sympy. 
% To the best of our knowledge, we are also the first to articulate the boundary between streaming programs that can and cannot be expressed by tensor language.

\section{Agentic system organization}
\label{sec:implementation}
In this section, we introduce a specific agentic system organization tailored for ML library development using STeP as shown in~\cref{fig:workflow}. It includes LLM agents, a code generator, and verifiers with a structural intermediate representation as the interface between users and system components.
\subsection{LLM agents}
\label{sec:agentic-system}
As shown in~\cref{fig:workflow}(a), each single LLM is designed for a purpose assigned by the domain expert through the \dblue{base prompt}. The base prompt contains the task description, base knowledge, and demonstrations. The \dgreen{prompt composer} chains the base prompt and task-specific input, which is then fed into a configured LLM serving \dorange{API}.  

% The agentic system organization is composed of \dblue{LLM agents}, a \dgreen{code generator}, and \dorange{verifiers} as shown in~\cref{fig:workflow}(b). Parallel samplers execute the agentic system with the same task concurrently and store the output in earned experiences. In each sampler, LLM agents generate the answer, and the code generator wraps the answer with data preparation and testing logic into a stand-alone pytest file of which the verifiers check correctness and type constraint.

We design two agents, \textit{a proposer and a guardian}. As explained in~\cref{sec:step-intro}, the affine type constraint in STeP is a global property that requires thinking back and forth beyond step-by-step reasoning, which is challenging for the causal generation of LLMs. Therefore, we design a guardian agent to check and correct the affine type error globally. We exclude the demonstration tasks in the base prompt of the agents from the benchmark to avoid the model directly copying the answer. 

The \dblue{proposer agent} generates a candidate STeP implementation whose base prompt comes from ASPL designers. The base prompt is composed of STeP references and usage patterns. 
% STeP reference contains the name, description of functionality, and example usage for each primitive. Usage pattern refers to common ways of composing STeP primitives. 
\Cref{fig:reference-accum} shows the reference for the \texttt{Accum} primitive, and~\cref{fig:example-pattern} exemplifies one of the usage patterns for shape manipulation. 

The \dblue{guardian agent} decides whether the output of the proposer violates the affine type constraint and corrects the implementation when necessary. The guardian agent consists of a fused gate and corrector.~\Cref{fig:prompt-agent-2} shows the base prompt for the guardian agent, which provides input and output examples of variable reuse where the variable is reused zero, one, or two times. 

\subsection{Code generator}
After the LLM agents, a \dgreen{code generator} takes in the generated implementation and outputs a self-contained pytestable Python script. With this code generator, LLM agents only need to output implementations without other helper code. 
% Users express target ML operators and available specialized functions in PyTorch, and LLM agents write STeP programs in Python.  
We embed the STeP specifications written in natural language to Python (as exemplified in~\cref{fig:accum-english} to \cref{fig:accum-python}). Beneath this Python frontend is a functional simulator that calculates the result of STeP programs. Since the essence of each ASPL is the programming abstraction (semantics) instead of its syntax~\cite{liskov2008}, we choose to prompt LLMs with their familiar Python syntax. In this way, LLMs can focus more on reasoning about the STeP programming abstraction without being distracted by alien syntax. 

\subsection{Verifier}
Fast verification is vital because it bottlenecks adaptive self-improvement learning. ASPLs further increase this complexity by requiring a simulator for the library--chip co-design process. Simulating STeP as a general dataflow system in Python would be slow because of its high dynamism~\cite{zhang2024dataflow}. Since we only focus on functional correctness in this work, we meticulously limit the level of dynamism to the degree that ML operators require. Consequently, our simulator emulates stream execution using tensor computation with necessary control flows.

Our system organization contains two verifiers. As a result, the reward in~\cref{alg:selsa} is $r(x,y)=1$ for task $y$ if answer $x$ passes these two verifiers and $r(x,y)=0$ otherwise. One \dorange{verifier} checks for functional correctness. Users program PyTorch to express their ML operators, which elicits LLMs' base knowledge of ML algorithms. The verifier compares the execution results of our simulator with the result tensors of the corresponding PyTorch program on a single set of shapes with random input values. The fidelity of our unit test method builds on practice~\cite{jia2019taso} and theory~\cite{gulwani2003discovering}. The other \dorange{verifier} checks the affine type constraint by performing static analysis on the abstraction syntax tree of the STeP program with the Python ast module~\cite{python:ast}. 

%The STeP program and the reward are stored together in the earned experiences. 
\subsection{Structual intermediate representation}
\label{sec:structual-ir}
Good interfaces can improve the performance of agentic coding systems~\cite{yang2024swe,wei2024improving}. We borrow the intermediate representation (IR) technique from compiler literature~\cite{lattner2021mlir} and use a structural IR to unify the interfaces of our agentic system. Specifically, the structural IR is used as the interface between users and the agentic system and as the interface between LLM agents and the code generator within the system.
% We justify structural IR in~\cref{sec:exp-agent}.

Our structural IR encodes necessary information using a data serialization language. It externalizes and condenses programs instead of simply formatting the prompt without changing the content. 
% Comparing the structural IR in~\Cref{fig:impl-attn0-yaml} with the equivalent bare Python in~\cref{fig:impl-attn0-python} for the same task, structural IR can externalize and condense the information, helping LLMs to reason by reducing the search space. To be specific, without structural IR, users have to input all the code before \texttt{body} function of~\cref{fig:impl-attn0-python}, which includes redundant glue strings.
Comparing the structural IR in~\cref{fig:impl-attn0-yaml} with the equivalent bare Python in~\cref{fig:impl-attn0-python} for the same task, 
%structural IR can externalize and condense the information by removing redundant glue strings.
users only need to state two things: the ML operator to implement and the specialized functions as in~\cref{fig:attn-task0-desc} without any redundant glue string. 
Moreover, structural IR saves tokens by reducing redundant prompts, allowing for more demonstrations. 

% We borrow the IR technique from compiler literature for its effectiveness in scaling compiler infrastructure~\cite{lattner2021mlir}. The code generator needs additional functionality that converts the IR to Python. We trade the system complexity for better performance as shown in~\cref{fig:all-methods-pass@k}.


% \begin{figure}[htb]
% \begin{center}
% \centerline{\includegraphics[width=0.8\columnwidth]{figs/agent.pdf}}
% \caption{LLM agent.}
% \label{fig:agent}
% \end{center}
% \end{figure}

\section{Benchmark}
\label{sec:benchmark}
We construct a set of tasks to measure the adaptive self-improvement agentic system proposed in~\cref{sec:method} and~\cref{sec:implementation}. This benchmark should cover a diverse set of popular ML operators and specialized functions. In total, we collect 26 tasks covering 8 groups of ML operators in common LLM model architectures, as shown in~\cref{fig:bench}.

\subsection{Metric}
We choose pass@k~\cite{chen2021evaluating} as the metric for task completion. Pass@k is calculated as~\cref{eq:pass_at_k} given $T$ tasks, $n$ samples of the agentic system, and $c_i$ correct responses for each task $i$. It is useful for us to analyze the metric at two extremes: pass@1 and pass@n. Pass@1 is the expectation of the success rate across tasks. Pass@n is the expectation of the portion of tasks that can be solved given all samples. 
% For all our experiments, T=26 and n=64.
\begin{equation}
        \text{pass@k} \coloneqq \frac{1}{T}\sum_{i=1}^{T}\left[ 1 - \frac{\binom{n-c_i}{k}}{\binom{n}{k}} \right]
    \label{eq:pass_at_k}
\end{equation}
% \begin{align}
%     \text{pass@k} &\coloneqq \frac{1}{T}\sum_{i=1}^{T}\left[ 1 - \frac{\binom{n-c_i}{k}}{\binom{n}{k}} \right]
%     \label{eq:pass_at_k} \\
%     \text{pass@1} &\coloneqq \text{pass@k } (k=1) = \frac{1}{T}\sum_{i=1}^{T}\frac{c_i}{n} \label{eq:pass_at_1} \\
%     \text{pass@n} &\coloneqq \text{pass@k } (k=n) =\frac{1}{T}\sum_{i=1}^{T}\mathbb{I}(c_i>0) \label{eq:pass_at_n}
% \end{align}

\subsection{Construct the benchmark suite}
We construct the benchmark from first principles and do not favor any kind of task. Firstly, the number of tasks in each group is nearly the same as shown in~\cref{fig:bench}. Secondly, the benchmark has an even distribution of difficulties.~\Cref{tab:sonnet-analysis-fused} shows that both tasks requiring shape and arithmetic primitives and tasks with and without reused variables distribute fairly evenly.

We provide a reference implementation for each task. These oracle implementations ensure that each task has at least one correct answer. Each task of one type has either different specialized functions for the same operator or different operators with different specialized functions. More details on the benchmark are in~\cref{sec:bench-info}.

\section{Experiments}
\label{sec:exp}
Detailed experimental settings are in~\cref{sec:exp-setting}. We also benchmark the tasks with OpenAI-o1 in~\cref{tab:all-models} but do not include it in the following experiments to control for test-time compute. All prompts are formatted in YAML because structural prompts generally benefit~\cite{he2024does}. 

\subsection{Analysis of adaptive self-improvement learning}
This section uses the agentic system organization described in~\cref{sec:agentic-system} for the best possible base learning capability. 

\label{sec:exp-learn}
\textbf{The hard-only examples improve the performance more effectively than examples mixed with easier ones.} As shown in~\cref{fig:learning-curve}, the Pareto optimal is composed of hard examples (denoted by ``H'') for all three models. Moreover, hard examples bring the most significant improvement along the learning curve. In some cases, fewer hard examples may perform better than more examples mixed with easier examples. For example, Iter 3 has better performance than Iter 2 for gpt-4o.

\textbf{Mixed examples are required to generate better hard-only examples.} In DeepSeek-V3, although HM$_1$ performs the same as H$_1$ and HME$_1$ performs worse than H$_2$ while taking more tokens, H$_2$ would not be discovered without HM$_1$ and HME$_1$. Although the new hard-only examples do not necessarily improve the performance, they can save input tokens, as exemplified by HM$_4$\&H$_5$ of gpt-4o and HM$_5$\&H$_6$ of llama.


% Why does sonnet consume fewer tokens?  Answer: the input tokens are counted by the least necessary number of tokens averaged across tasks. A task can take increasing input tokens but still fail. Sonnet only has 3 unsolved tasks left. Therefore, although it has the most example tokens, the average number of input tokens across tasks is still less than others. Actually, Iter 5\&6\&8 of llama use the same set of examples, but Iter 8 uses more tokens because more tasks have been finished by Iter 7 which has more examples and thus more tokens than Iter 6.

% Why can two sampling cycles have the same input tokens (Iter 3\&4, 5\&6 and 7\&9 of llama and Iter 4\&5 of gpt-4o)? Answer: the newly selected examples belong to easier bins; thus, the current bin does not change. For example, a list [1, 2, 3, 5] is binned to [1, 2] (H), [3] (M), and [5] (S). Assuming 5 is added, then [1, 2, 3, 4, 5] will be binned to [1, 2] (H), [3, 4] (M), and [5] (S) with the same ``H'' as the prior.

%``H'' is the same as the previous. Assuming 0.5 is further added, then [0.1,0.2,0.3,0.4,0.5,0.6] to [0.1,0.2] (H), [0.3,0.4] (M), and [0.5,0.6] (S). ``HM'' sets are the same. 

% Why can later iterations have fewer tokens than the prior (Iter 2\&3 of gpt-4o)? Answer: The current implementation uses the success rate of the last task in each segment as the boundary values. Therefore, for [1,2,2,4], ``H'' will be [1,2,2] because the boundary value is 2. Assuming 1 is added, then [1,1,2,2,4] will have [1,1] for ``H'', which is shorter than the prior [1,2,2]. This pair also shows that \textbf{fewer hard examples can perform better than more easy examples}.

\begin{figure*}[htb]
\centering
\includegraphics[width=2\columnwidth]{figs/learning-curves-three-86.pdf}
\caption{Adaptive self-improvement learning improves the agentic system with data generated by itself. The input tokens are averaged across tasks. A task takes increasing input tokens until success or data is used up.  ``9: HME$_5$'' means 9-th iteration, 5-th cycle of adaptive sampling, and demonstrations contain hard (H), medium (M), and easy (E)-earned experiences. ``None'' means no examples for the first iteration. For Iter $i$, if Pass@n$_{i-1} >$ Pass@n$_{i-2}$, then a new cycle of adaptive sampling starts from ``H''. Otherwise, the current cycle continues in the order of H$\rightarrow$HM$\rightarrow$HME. The dash lines are connected in the iteration order. Claude 3.5 Sonnet result is in~\cref{fig:sonnet-learning-curve}.}
\label{fig:learning-curve}
\end{figure*}

\textbf{Adaptive granularity affects token efficiency and peak performance.} If the adaptive granularity $m$ is too small, then easy examples might dilute the difficulty of training data. If $m$ is too large, then exploration steps might be too conservative and thus waste input tokens. Therefore, there is a sweet spot that balances the training data difficulty and cost of input tokens. As shown in~\cref{fig:4o-non-adaptive}, $m=3$ is that spot. $m=3$ saves $1.07\times$ tokens over $m=4$ while maintaining performance. Additionally, $m=3$ improves the performance by $1.5\times$ at a similar input token cost when compared to $m=1$. 
\begin{figure}[htb]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figs/4o_m_plot_comp.pdf}}
\caption{Hyperparameter tuning of adaptive granularity $m$ on GPT-4o. This supports using m=3 for~\cref{fig:learning-curve}.}
\label{fig:4o-non-adaptive}
\end{center}
\end{figure}

\subsection{Ablation study of agentic system organization}
The experiments below study the base learning capability of agentic systems without the self-improvement process.

\label{sec:exp-agent}
\textbf{Agentic system can discover non-trivial STeP programs.}
Surprisingly, the agentic system composes attention operators with over 50\% Pass@1 as shown in~\cref{fig:all-methods-sonnet}. That means the agentic system can discover online softmax~\cite{milakov2018online} and memory-free streaming attention~\cite{sohn2024implementing} with specialized functions provided in~\cref{fig:attn-tasks}, which is considered challenging for ordinary programmers. 

\textbf{Our structual IR improves performance by increasing the sample diversity.}~\Cref{fig:all-methods-pass@k} shows the efficacy of our structural IR and agentic method. We calculate the semantic diversity of sampled answers. A group of answers is considered to have the same semantics if their abstract syntax trees are isomorphic. We calculate success diversity as the number of semantically different correct answers divided by the total number of correct answers, averaged across all tasks. Failure diversity is calculated in a similar way but for wrong answers. Overall, diversity combines both metrics. As shown in~\cref{tab:diversity-analysis}, Pass@n has a positive correlation with failure, overall diversity, and the complexity of the methods. However, structural IR can hurt success diversity.
% \todo{Weixin: Table 1, 2: good to textbf the best numbers}


% \begin{table}[htb]
% \centering
% \begin{tabular}{c c c c c c}
% \toprule
% \multirow{2}{*}{Method} & \multirow{2}{*}{Pass@1} & \multirow{2}{*}{Pass@n} & \multirow{2}{*}{Success div.} & \multirow{2}{*}{Failure div.} & \multirow{2}{*}{Overall div.} \\
% \midrule
% Single & 0.20 & 0.46 & 0.32 & 0.47 & 0.41 \\
% Single-S & 0.30 & 0.62 & 0.27 & 0.64 & 0.50 \\
% Agent-S & 0.37 & 0.85 & 0.34 & 0.68 & 0.52 \\
% \bottomrule
% \end{tabular}
% \caption{Analysis of the correlation between answer diversity and the performance.}
% \label{tab:diversity-analysis}
% \end{table}
\begin{table}[htb]
\centering
\begin{tabular}{c c c c c c}
\toprule
\multirow{2}{*}{Method} & Success & Failure &  Overall & \multirow{2}{*}{Pass@n}\\
 & diversity & diversity & diversity & \\
\midrule
Single-WS & 0.32 & 0.47 & 0.41 & 0.46\\
Single & 0.27 & 0.64 & 0.50 & 0.62\\
Agent & \textbf{0.34} & \textbf{0.68} & \textbf{0.52} & \textbf{0.85}\\
\bottomrule
\end{tabular}
\caption{Analysis of the correlation between semantic diversity of answers and the performance. ``-WS'' means ``without structural IR''. Higher values indicate higher semantic diversity. 
}
\label{tab:diversity-analysis}
\end{table}
\textbf{The guardian agent can correct affine type errors but might also corrupt correct answers output by the proposer agent.}
% ~\Cref{tab:sonnet-analysis-single} and~\cref{tab:sonnet-analysis-agent} analyze the source of improvement brought by the agentic method. 
\begin{figure}[htb]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figs/pass_k-methods.pdf}}
\caption{Pass@k against the number of samples for a single LLM without structural IR, baseline single LLM, and agentic system.}
\label{fig:all-methods-pass@k}
\end{center}
\end{figure}
% \begin{figure}[ht]
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{figs/heatmap.pdf}}
% \caption{Pass@1 of different test-time compute strategies with n=8 for the cost and fairness. The heatmaps exclude 2 tasks that none of the models succeed. The average numbers of input, output, and total token counts of each task sample are: (5430, 725, 6155), (4349, 5647, 9996), and (5103, 504, 5607), respectively.}
% \label{fig:heatmap}
% \end{center}
% \end{figure}
As shown in~\Cref{tab:sonnet-analysis-fused}, the guardian agent effectively corrects the proposer agent, solving 5 extra reuse tasks (Pass@n of Avg-Reuse from 6/12 to 11/12). Notably, all the Arith-Reuse tasks can be solved by the agentic system (Pass@n of Arith-Reuse from 5/8 to 8/8). Surprisingly, the agentic system also helps with non-reuse tasks (Pass@n of Shape-Once from 3/7 to 4/7). However, the agentic system reduces the Pass@1 of Arith-Once from 0.685 to 0.663, implying that the guardian agent might corrupt the proposer's output. The agentic system can compensate for such corruption by finishing more tasks, resulting in an unchanged Pass@1 of Avg-Once.
% \Cref{tab:sonnet-analysis-agent} shows that the best agentic system can already solve all the tasks that only involve arithmetic computation. However, it still struggles with the tasks with shape manipulation. 
% \begin{table}[htb]
% \centering
% \begin{tabular}{c c c c | c}
% \toprule
% Attribute & Metric & Once & Reuse & Avg \\
% \midrule
% \multirow{2}{*}{Arithmetics} & Pass@1 & 0.6629 & 0.4551 & 0.5521 \\
% & Pass@n & 7/7 & 8/8 & 15/15 \\
% \midrule
% \multirow{2}{*}{Shape} & Pass@1 & 0.1674 & 0.0508 & 0.1250 \\
% & Pass@n & 4/7 & 3/4 & 7/11 \\
% \midrule
% \multirow{2}{*}{Avg} & Pass@1 & 0.4152 & 0.3203 & 0.3714 \\
% & Pass@n & 11/14 & 11/12 & 22/26 \\
% \bottomrule
% \end{tabular}
% \caption{Analysis of the performance based on the attributes for the Claude-3-5-sonnet agentic system.}
% \label{tab:sonnet-analysis-agent}
% \end{table}
% \begin{table}[htb]
% \centering
% \begin{tabular}{c c c c | c}
% \toprule
% Attribute & Metric & Once & Reuse & Avg \\
% \midrule
% \multirow{2}{*}{Arith} & Pass@1 & 0.685 & 0.232 & 0.444 \\
% & Pass@n & 7/7 & 5/8 & 12/15 \\
% \midrule
% \multirow{2}{*}{Shape} & Pass@1 & 0.145 & 0.004 & 0.094 \\
% & Pass@n & 3/7 & 1/4 & 4/11 \\
% \midrule
% \multirow{2}{*}{Avg} & Pass@1 & 0.415 & 0.156 & 0.296 \\
% & Pass@n & 10/14 & 6/12 & 16/26 \\
% \bottomrule
% \end{tabular}
% \caption{Single proposer agent. ``Arith'' means the oracle implementation only involves arithmetic primitives. ``Shape'' means the oracle implementation involves shape manipulation primitives as introduced in~\cref{sec:step-intro}. ``Once'' means all the streams are used once, while ``Reuse'' means some streams are used more than once and thus requires \texttt{Copy} primitive for the affine type constraint.
% }
% \label{tab:sonnet-analysis-single}
% \end{table}
% \begin{table}[htb]
% \centering
% \begin{tabular}{c c c c | c}
% \toprule
% Attribute & Metric & Once & Reuse & Avg \\
% \midrule
% \multirow{2}{*}{Arith} & Pass@1 & 0.663 & 0.455 & 0.552 \\
% & Pass@n & 7/7 & 8/8 & 15/15 \\
% \midrule
% \multirow{2}{*}{Shape} & Pass@1 & 0.167 & 0.051 & 0.125 \\
% & Pass@n & 4/7 & 3/4 & 7/11 \\
% \midrule
% \multirow{2}{*}{Avg} & Pass@1 & 0.415 & 0.320 & 0.371 \\
% & Pass@n & 11/14 & 11/12 & 22/26 \\
% \bottomrule
% \end{tabular}
% \caption{Self-corrected system with proposer and guardian agents. Attributes are the same as~\cref{tab:sonnet-analysis-single}.}
% \label{tab:sonnet-analysis-agent}
% \end{table}

\begin{table}[htb]
\centering
\begin{tabular}{c c c | c c c}
\toprule
Mode & Metric & Method & Once & Reuse & Avg \\
\midrule
\multirow{4}{*}{Arith} & \multirow{2}{*}{Pass@1} & Single & \textbf{0.685} & 0.232 & 0.444 \\
& & Agent & 0.663 & \textbf{0.455} & \textbf{0.552} \\
& \multirow{2}{*}{Pass@n} & Single & \textbf{7}/7 & 5/8 & 12/15 \\
& & Agent & \textbf{7}/7 & \textbf{8}/8 & \textbf{15}/15 \\
\midrule
\multirow{4}{*}{Shape} & \multirow{2}{*}{Pass@1} & Single & 0.145 & 0.004 & 0.094 \\
& & Agent & \textbf{0.167} & \textbf{0.051} & \textbf{0.125} \\
& \multirow{2}{*}{Pass@n} & Single & 3/7 & 1/4 & 4/11 \\
& & Agent & \textbf{4}/7 & \textbf{3}/4 & \textbf{7}/11 \\
\midrule
\multirow{4}{*}{Avg} & \multirow{2}{*}{Pass@1} & Single & \textbf{0.415} & 0.156 & 0.296 \\
& & Agent & \textbf{0.415} & \textbf{0.320} & \textbf{0.371} \\
& \multirow{2}{*}{Pass@n} & Single & 10/14 & 6/12 & 16/26 \\
& & Agent & \textbf{11}/14 & \textbf{11}/12 & \textbf{22}/26 \\
\bottomrule
\end{tabular}
\caption{Analysis of improvement brought by the agentic method. ``Arith'' and ``Shape'' mean the oracle implementation only involves arithmetic primitives and involves shape manipulation primitives as introduced in~\cref{sec:step-intro}, respectively. ``Once'' and ``Reuse'' mean all the streams are used once and more than once, respectively.
}
\label{tab:sonnet-analysis-fused}
\end{table}

% \begin{table}[htb]
% \centering
% \begin{tabular}{c c c c | c}
% \toprule
% Attribute & Metric & Once & Reuse & Avg \\
% \midrule
% \multirow{2}{*}{Arithmetics} & Pass@1 & 0.6853 & 0.2324 & 0.4437 \\
% & Pass@n & 7/7 & 5/8 & 12/15 \\
% \midrule
% \multirow{2}{*}{Shape} & Pass@1 & 0.1451 & 0.0039 & 0.0938 \\
% & Pass@n & 3/7 & 1/4 & 4/11 \\
% \midrule
% \multirow{2}{*}{Avg} & Pass@1 & 0.4152 & 0.1562 & 0.2957 \\
% & Pass@n & 10/14 & 6/12 & 16/26 \\
% \bottomrule
% \end{tabular}
% \caption{Analysis of the performance based on the attributes for a single Claude-3-5-sonnet proposer.}
% \label{tab:sonnet-analysis-single}
% \end{table}


% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

\section{Related Work}
% \todo{Weixin: Consider enhancing Figure 7 by opening with a clear topic sentence that captures the key finding. his would help readers quickly grasp the main relationship you're demonstrating between xxx and yyy.}
% \subsection{DSL for LLM}
% DSL has been used to define the interface between users and LLM agents. ~\cite{wei2024improving} designs a mapper DSL as an interface, enabling an existing prompt optimizer to optimize a parallel computing system problem. Differently, their DSL has their own syntax, while ours is embedded in existing language.
\subsection{Self-improvement learning for LLMs}
Self-improvement learning for LLMs typically involves two stages: scoring generated samples (trajectories) and incorporating those samples to enhance the model. Scoring can be achieved through human labeling~\cite{cobbe2021training,lightman2023let} or through automated methods such as verifiers and heuristics~\cite{wang2024math,singh2023beyond}. Our method stands out in this context by utilizing AST analysis, offering a more interpretable approach to scoring. When it comes to incorporating samples, models may rely on retrieval~\cite{zhao2024expel,park2023generative}, reflection~\cite{shinn2024reflexion,liu2023reflect}, or reward feedback~\cite{opsahl2024optimizing,fernando2023promptbreeder}. Our method introduces a new mechanism in this stage by adaptively extending and prioritizing high-scoring samples. 

Our method shares similar reinforcement principles with self-improvement learning at the post-training stage~\cite{bai2022constitutional,gulcehre2023reinforced,tian2024toward} but is rewarded at a task-level granularity instead of token-level. Specifically, each action is a program implementation instead of token prediction and the state is defined by earned experiences rather than generated sequences.

\subsection{Agentic system organization for specialized tasks}
Task-specific organization has proven effective in enhancing the performance of agentic systems across diverse coding tasks~\cite{zhang2024caravan,fang2024assertllm,guan2024intelligent}. We adopt an agentic system organization specifically for ML library development using an ASPL. Such domain knowledge can be further augmented with automatic agentic system design tools~\cite{khattab2023dspy,hu2024automated}. Furthermore, well-designed interfaces between agents, tools, and other agents have been shown to improve performance~\cite{schick2023toolformer,yang2024swe,wu2023autogen}, and a structural IR enables these interfaces to be highly task-aligned in our system.

% LLM agents have been developed for text-to-code~\cite{batten2024pyhdl,elmaaroufi2024scenicnl}, lifting~\cite{bhatia2024verified}, task completion~\cite{barke2024hysynth,wei2024improving} using low-resource programming languages. The solutions include: using external tools to correct, guided with formal methods, and embedding into common languages.


\section{Conclusion}
% The techniques and findings might benefit commercial RDAs in the future. In the rule-based era~\cite{newell1959report}, programming language research benefited AI research by abstracting the thought process~\cite{mccarthy1962towards}. This paper shows early evidence that programming abstraction can benefit agentic AI systems in the foundation model era~\cite{bommasani2021opportunities}. New ASPLs are being proposed to simplify users' learning curve to SOL performance. Taking the other direction, this paper improves the learning capability of the users for a given ASPL. LLMs are not to replace any compiler or metaprogramming techniques but to boost the productivity of programmers using these methods so that ML operators can be optimized fairly.

ML library development using ASPLs is a critical component of the ML ecosystem, but it remains poorly automated. To address this limitation, we co-design the learning process and agentic system around a central objective: enabling complex reasoning with limited data. Our methods simultaneously implement non-trivial ML operators and produce a self-improving agent. Consequently, our system provides not only support to experts in developing ML libraries but also valuable resources for other systems.

\clearpage
\section*{Impact Statement}
Our work on adaptive self-improvement learning LLM agentic systems for automating ML library development using ASPLs has several potential societal implications. The primary impact is the potential to significantly enhance the productivity of ML library developers, which could accelerate the development of more efficient ML systems. This advancement could democratize access to ML development tools and reduce the technical barriers to entry in the field. Additionally, our technique offers an approach for deploying LLM agents in scenarios requiring complex reasoning with limited data availability. While these developments primarily aim to advance the field of Machine Learning, we acknowledge that increased automation in software development could impact the nature of programming work and skills required in the field. We believe these potential implications warrant ongoing discussion and careful consideration as the technology develops.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
