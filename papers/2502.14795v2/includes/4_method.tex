\section{Humanoid-VLA}


\subsection{Preliminary} 


\textbf{Definition of humanoid control. }
With the growing availability of human data in the graphics community, recent humanoid control has increasingly adopted methods that learn from human data. 
Specifically, given a target body pose from physical teleoperation (e.g., a motion capture system) and the humanoid's proprioception, the whole-body controller $\mathcal{P}$ generates joint torques to control the humanoid robot. Formally, this can be expressed as
\begin{equation}
    j_t = \mathcal{P}(s_t, p_t),
\end{equation}
where $s_t, p_t, j_t$ means target body
pose, humanoid's proprioception and joint torques at time $t\in \mathbb{N}^+$. 
% p_t dim 242

\textbf{Limitation of humanoid control. } 

However, developing a general-purpose robot requires purposive learning, which involves extracting meaningful intentions from human data and adapting prior experiences to novel tasks or environments. 
Current data acquisition methods, focusing mainly on human joint poses, lack integration with egocentric vision. Thus, they can only teach robots what actions are performed, not the underlying intent or context. Consequently, pose-level imitation is inherently limited in generalizability due to environmental discrepancies.



\begin{figure*}[ht]
  \centering
   \includegraphics[width=0.97\linewidth]{imgs/model1.pdf}
   \caption{\textbf{Overview of Humanoid-VLA}. Humanoid-VLA includes two main parts: language-Motion Pre-alignment and vision-conditioned fine-tuning. }
   \label{fig:model}
   \vspace{-1.5em}
\end{figure*}



\textbf{Our solution.}
We present Humanoid-VLA, the first VLA model for humanoid robots, seamlessly integrating language understanding, scene perception, and motion control into a unified system to address previous limitations in humanoid control. Next, we demonstrate the framework from two main parts: \textbf{Language-Motion Pre-Alignment} and \textbf{Vision-conditioned Fine-tuning}.


\subsection{Language-Motion Pre-Alignment}
In this stage, we align non-egocentric human motion data with language descriptions. This alignment enables the model to learn motion patterns and action semantics from non-egocentric data sources, laying a robust foundation for motion generation without requiring egocentric visual input. 



\begin{figure*}
  \centering
   \includegraphics[width=1\linewidth]{imgs/aug_pipline.pdf}
   \caption{\textbf{Data Acquisition Pipeline}. We propose a cost-effective, self-supervised data augmentation approach that converts abundant pure motion data from videos into annotated motion data with captions. The framework consists of two key modules: a compositional motion quantization method and an autonomous data augmentation approach, which together enable scalable expansion of the dataset.}
   \label{fig:templtes}
   \vspace{-1.5em}
\end{figure*}


\subsubsection{Data Acquisition}
\begin{table}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{@{}c|cc|ccc@{}}
\toprule
Category  & Text & Motion & Clips & Frames & Hours \\
\midrule
Motion capture  &\checkmark&\checkmark & 29K & 0.3M & 4.1 \\
Online Video &\ding{55}& \checkmark& 0.8M & 541M & 7515.7 \\
{Synthetic Data}  &\checkmark& \checkmark& 100K & 16M & 227.7 \\
% Manual Annotated  & \checkmark& \checkmark& 4K & 0.7M & 10.3 \\
% Manual Annotated & Video &  &  & 4K & 0.7M & 10.3 \\
\midrule
\multicolumn{3}{c}{Total}    & \textbf{0.929M} & \textbf{557.3M} & \textbf{7790.2} \\
% Total (A)&  &  &  & \textbf{20.7M} & \textbf{560M} & \textbf{7790.2}\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Datasets Statistics}}
\label{tab:data statistics}
\end{table}
\textbf{Limitations of data acquisition.}
Previous studies have predominantly utilized well-curated datasets that pair motion trajectories with language descriptions to train text-conditioned motion generation models. While these datasets facilitate effective training, they are limited in both quantity and diversity, which constrains their ability to achieve better alignment. In contrast, large-scale online video datasets (as shown in Table~\ref{tab:data statistics}) offer abundant and diverse motion data. However, the absence of corresponding language annotations significantly limits their applicability for this task.

Recent efforts to address this bottleneck have focused on annotating large-scale video datasets manually or using video large language models (VLLMs)\cite{zhang2023video}. However, manual labeling is prohibitively expensive, and VLLMs often produce noisy, incomplete, or imprecise annotations due to their inability to capture fine-grained motion details or describe complex actions. These limitations undermine the effectiveness of the resulting datasets for aligning language and motion.


\textbf{Self-supervised data augmentation.}
Instead of relying on explicit motion descriptions, we propose a cost-effective annotation method by designing various self-supervised tasks directly derived from motion data. For instance, one representative approach involves temporarily masking specific body joints within motion sequences and training the model to reconstruct the occluded movements. Instructional prompts such as "missing left arm <Occlusion> motion data. Please complete the motion" can be generated for these tasks, paired with the corresponding ground truth motions as target outputs. This automatic approach eliminates the need for explicit annotations and is more accurate than adding extra annotation for motion data from video sources.

Next, we explain how this is achieved through two key modules:
\textbf{compositional motion quantitation} and  \textbf{automatic data augmentation.}




\textbf{Compositional motion quantitation.}
As shown in Figure~\ref{fig:model}, we propose a decompositional compression method for body pose representation.
Specifically, we decompose each body pose into five body-based tokens corresponding to five distinct parts: the left leg, right leg, torso, left arm, and right arm. 
We independently train each encoder $\mathcal{E}_b$ and its corresponding codebook $V_b$ for each body part to compress the body part data at time $t$, denoted as $c_t$, into a quantized representation $z_t \in \mathbb{R}^{5}$. 

Formally, we define the motion encoder as $\mathcal{E}_m=\{\mathcal{E}_b\}_{b=1}^5$, which compresses $c_t$ into $z_t$.
\begin{equation}
    \hat{z_t} = \mathcal{E}_m(c_t),
\end{equation}
where $\hat{z_t}=\{\hat{z_b}\}_{b=1}^5$ is the collective discrete vector obtained from $\mathcal{E}_m$, which are the most similar elements to the quantization of $c_t$ in vocabulary $V_m=\{V_b\}_{b=1}^5$. Similar to the encoder, we employ a motion decoder to project the latent variable back onto the action space:
\begin{equation}
    \hat{c_t} = \mathcal{D}_m(\hat{z_t}).
\end{equation}



The optimize goal $\mathcal{L}_{hvq}$ can be expressed as the combination of reconstruction loss $\mathcal{L}_{\text{rec}}$, embedding loss $\mathcal{L}_{\text{emb}}$ and commitment loss $\mathcal{L}_{\text{com}}$:
\begin{equation}
      \mathcal{L}_{hvq} = \underbrace{\|{c_t} - \hat{{c_t}}\|_2}_{\mathcal{L}_{\text{rec}}} + \underbrace{\|\text{sg}({z_t})-\hat{{z_t}}\|_2}_{\mathcal{L}_{\text{emb}}} + \underbrace{\|{z_t}-\text{sg}(\hat{z_t})\|_2}_{\mathcal{L}_{\text{com}}}.
\end{equation}


This compositional encoding method is crucial, allowing for flexible editing of motion sequences. The advantage of decomposing a body pose into multiple parts and encoding them separately lies in that we can form flexible operations on the motion sequence at the token level. For instance, we can replace, perturb, or rearrange the tokens corresponding to specific body parts to generate new motion patterns. This flexibility significantly enhances control over motion data, laying the foundation for further task design.



\textbf{Automatic data augmentation. }
As illustrated in Figure~\ref{fig:templtes}, we introduce four types of augmentations—<Track>, <Time>, <Occlusion>, and <State>—to extract diverse features from raw motion data. For example, in the <Track> augmentation, we isolate the temporal trajectory of a specific joint (e.g., the root joint) and encode it as a corresponding motion token. To create meaningful question-answer pairs, we pair this motion feature with an instructional prompt, such as “Please move your center position along the trajectory of <Track>,” while using the complete motion sequence as the answer. This approach effectively augments datasets that initially lacked linguistic annotations, enabling their use in tasks requiring text-motion alignment.

\textbf{Discussion. }
This method presents several key advantages. 
1) It is highly flexible and extensible: augmentation types like <Track> can be combined with other conditions (e.g., <Time>) to create more complex tasks, while linguistic diversity can be further enriched by rephrasing the same instruction through tools like GPT-4\cite{achiam2023gpt}.
2) The framework leverages motion data's inherent temporal and spatial dynamics, allowing models to learn richer and more robust motion-language relationships.
3) Lastly, the use of interleaved datasets enhances cross-modal alignment by incorporating both motion and text in inputs and outputs. As demonstrated by prior work such as VILA~\cite{lin2024vila}, such training paradigms enable models to better capture the interplay between motion and language without compromising performance on their original tasks.

Using this augmentation approach, we collect the largest motion-language interleaved dataset to date, with a scale that is 25 times larger than previous work~\cite{mao2024learning}. This effectively addresses the data scarcity issue for training foundational human motion models.

\subsubsection{Training.}
When we acquire enough data with language annotations, we still need to consider the quality of raw motion data from video sources.
Therefore, we divide our whole training process into two stages. First, we leverage low-quality data to establish initial alignment between motion and language. Even if they are not precise, the large-scale data could also lay a foundation. 
Later, we continue to train the model using a smaller but high-quality dataset from Mocap, ensuring that it conforms to proper human kinematics.




\textbf{Details.}
We utilize LLMs to map input conditions to generate motion sequences effectively. Our data augmentation approach and compositional motion encoding allow LLMs to seamlessly embed motion conditions into input descriptions.
For instance, an instruction \( l_t \) for motion generation can be structured as: "Plan a sequence of actions ending with <State> over <Time> seconds." Here, <State> corresponds to the discrete action representation token \( z_t \), which is derived from the motion pose \( c_t \) at timestep \( t \) in the motion sequence, while <Time> specifies the motion duration. By unifying the motion codebook \( V_m \) and the language codebook \( V_l \) into a shared vocabulary \( V = \{V_l, V_m\} \), we can encode the instruction \( l_t \) alongside the motion representations \( z_t \) and temporal representations \( d_t \) as language tokens \( X_d = \{x_d^i\}_{i=1}^N \), where \( x_d \in V \) and \( N \) represents the length of the input description. This transformation makes the combined motion and temporal data compatible with LLMs, enabling precise and flexible input encoding.



\textbf{Loss function.}
Motion generation can thus be framed as an autoregressive process that predicts the dictionary index of the next action token, ultimately producing the final motion output \( X_o = \{x_o^i\}_{i=1}^L \), where \( x_o \in V \) and \( L \) denotes the output sequence length. The training objective is defined as maximizing the log-likelihood of the data distribution:
\begin{equation} 
\mathcal{L}_\text{LLM} = -\sum_{i} \log p(x_o^i \mid x_o^{<i}, x_d).
\end{equation}
Finally, the predicted discrete motion sequence \( \hat{z_t} \) can be derived from the LLM's output sequence \( X_o \) through vocabulary mapping. This sequence can then be used to reconstruct the final predicted motion \( S = \{s_t\}_{t=1}^{T} \), where \( T \) represents the length of the motion sequence.


\subsection{Vision-Conditioned Fine-Tuning}
Visual information provides humanoids with detailed object-aware insights, helping them not only understand how to act but also decide what actions to take. While previous research has trained humanoids using large datasets of human motion, the lack of egocentric visual data limits their ability to react based on autonomous perception. To address this, we collect real-world motion capture data paired with egocentric visuals, enabling the transfer of learned motion knowledge to real-world, visually grounded scenarios.

\textbf{Details.}
We copy and freeze the transformer layers from the language-motion pre-alignment phase to integrate visual information with language descriptions. Additionally, we introduce a vision encoder and utilize cross-attention layers to fuse visual features \( X_v \) with language features \( X_d \) into a unified embedding \( X_u \). 
Specifically, the decoder comprises \( L \) layers, with the \( l \)-th layer consisting of a copied transformer decoder layer and a cross-attention layer. In the cross-attention layer, the tokenized language tokens \( X_d^l \) are used as the query, while the encoded visual tokens \( X_v^l \) serve as both the key and the value:
\begin{equation}
\label{fig:overall}
\begin{aligned}
    Q_l = X_d^l W_{Q}^l,\ \
    K_l = X_v^l W_{K}^l,\ \
    V_l = X_v^l W_{V}^l, 
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
{X_u^l} = \text{Softmax}(\frac{Q_lK_l^T}{\sqrt{D}})V_l,
\end{aligned}
\end{equation}
where $D$ represents the hidden dimension size, $W_{Q}^l\in\mathbb{R}^{D_d\times D}$ represents the linear transformation matrix of language token, and $W_{K}^l, W_{V}^l\in\mathbb{R}^{D_v\times D}$ represents the transformation of visual tokens.

\textbf{Loss function.} Here, we optimize the model in the same way as the former language-action pre-alignment phase.

\subsection{Whole-Body Controller}


\textbf{Details.}
Once the two training phases are completed, the model can be integrated with a whole-body controller to enable control of a humanoid robot. 
The whole-body controller $\mathcal{P}$ is essentially a goal-conditioned RL policy that maps human motion onto the joints of a humanoid robot $j_t\in \mathbb{R}^{24}$. We define a reward strategy $\mathcal{R}$, which takes the observation $\mathcal{O}$ and the given goal $\mathcal{G}$ as input, and outputs the target positions for the proportional-derivative (PD) controller in the action space $\mathcal{A}$. We use the proximal policy optimization (PPO) \citep{schulman2017proximal} to maximize the accumulated reward.



