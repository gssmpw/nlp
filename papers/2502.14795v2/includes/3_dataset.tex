\section{Dataset}









\textbf{Dataset sources.}

We have collected seven datasets, which include motion data generated from real-person videos, the open-source motion capture dataset AMASS ~\cite{ma2019amass}, and motions generated from random texts ~\cite{shen2024gvhmr,jiang2024motiongpt,tevet2023human,chen2023executing}. In addition, we used two datasets as test benchmarks, namely HumanML3D ~\cite{guo2022humanml3d} and a manually annotated video dataset.

\textbf{Obtaining SMPL Data.}
The Skinned Multi-Person Linear model (SMPL) data serves as a fundamental building block for our humanoid robot's action learning process ~\cite{SMPL2015}. Besides from AMASS and text generated motion dataset, we utilized GVHMR ~\cite{shen2024gvhmr} method to extract the SMPL skeletal information from the collected videos. By accurately representing the human body's shape and motion in a parametric model, SMPL data enables our model to understand the kinematic relationships between different body joints.




\textbf{Keypoint Extracting.}
To ensure that the input data for our model pertains to a uniform human body type, we undertook the task of transforming all SMPL data into an identical neutral body type. Specifically, to map the human body joints information $K_h$ onto a uniformly formatted body type data $K_u$, we selected 15 shared joints common to both robots and humans, namely pelvis, left and right hip, knee, ankle, foot, elbow, wrist, and hand. Subsequently, we employed the {\color{red}XX method} to optimize different bodies into a set of highly accurate 3D relative coordinates, which can be formulated as $F_s$:

This unification of the SMPL body format standardized the input form of the model, eliminating potential interferences caused by different body types. The unified processing approach allows the model to learn from a wide variety of human motion data, rather than being confined to specific motion-captured data, thus endowing the generated motions of the model with greater diversity.


\textbf{Automatic Data Augmentation}

We propose a data augmentation method to generate synthetic motion captions for videos, which can expand the training data and improve the performance of downstream tasks.

We mask joints of known motion sequences or part of movements, enabling our model to fill in missing joints, motion sequences, or body joints considering specific trajectories. The enhanced motion-text data from augmentation could allow the model to capture correlations among different moves. Rich forms of instruction enhance the robustness of model ability towards generative language inputs, enabling various generation methods to improve the fluency and generalize ability of generated motions.





\textbf{Visual Integration. }

We also collected a scene-centered reaction dataset $D_v$ in order to foster the ability of the VLA model to understand the environment. This is crucial for robot operation autonomy, especially in situations where external instructions may be vague or ambiguous.

Data collection was performed in the humanVLA simulation environment. Combining with IsaacGym, we are able to create realistic and various scenarios for robot-object interaction. First-person view images are taken from the camera while the robot interacts with different objects and environments; at the same time, joint states will be recorded in real time to provide a detailed representation of how the robot will be moving. This combination of sensory data ensures that the model receives rich, multimodal input, enabling it to learn how to understand and respond to various environmental cues.












