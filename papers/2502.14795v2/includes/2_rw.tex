\section{Related Works}

% \begin{figure*}
%   \centering
%    \includegraphics[width=1.0\linewidth]{imgs/data.pdf}
%    \vspace{-5mm}
%    \caption{\textbf{Train}. }
%    \label{fig:}
%    \vspace{-3mm}
% \end{figure*}

% V2
\subsection{Humanoid Control}
Traditional humanoid control methods \cite{li2023dynamic, kuindersma2016optimization, elobaid2023online, dantec2021whole, dai2014whole} like MPC provide accuracy and stability but lack adaptability, while learning-based methods offer flexibility but rely on human motion data due to limited humanoid datasets.
Works like Exbody \citep{cheng2024expressive}, Exbody2 \citep{ji2024exbody2}, HARMON \citep{jiang2024harmon}, and mobile-television \citep{lu2024mobile} perform upper-body motion retargeting for humanoid robots using the SMPL model \citep{loper2023smpl} and root velocity tracking for lower-body locomotion.
To achieve flexible and complex motions, methods such as PHC \citep{luo2023perpetual}, H2O \citep{he2024learning}, and OmniH2O \citep{he2024omnih2o} use the SMPL model to extend motion retargeting to whole-body control.
Additionally, approaches like OmniH2O \citep{he2024omnih2o}, HARMON \citep{jiang2024harmon}, and UH-1 \citep{mao2024learning} enable language-guided motion generation. However, these methods are reactivate, meaning that the models generate various motions passively based on text or key points. 
To perform more advanced tasks autonomously in dynamic and complex environments, \textbf{egocentric visual information} is indispensable.




\subsection{Humanoid Dataset}
Apart from the first-person visual information, aligning motion with semantically relevant textual information is crucial for the construction of a foundational humanoid robot model.
Previous human datasets, such as AMASS \citep{mahmood2019amass},  HumanML3D \citep{Guo_2022_CVPR}, Motion-X \citep{lin2023motion}, and Human3.6M \citep{h36m_pami, IonescuSminchisescu11} provide large-scale human motion data. While some works use human motion retargeting to develop humanoid robot datasets \citep{he2024omnih2o, he2024learning, cheng2024expressive, ji2024exbody2}, these datasets often suffer from sparse text annotations and limited scale, restricting their use in training foundational models. 
Even though some methods can mitigate this issue, they generally suffer from high costs~\cite{mao2024learning} and a lack of precision~\cite{tevet2023human}.
In contrast, we propose a self-suprvised data augmentation method that circumvents the need for manually
annotated textual descriptions while effectively utilizing
large-scale, unlabeled motion data extracted from video repositories for the training of robot foundation model.

\subsection{VLA for Robotics Learning}

In recent years, VLA models have advanced robot learning, particularly for robotic arms and quadrupeds, by integrating vision, language, and action to enhance task and environment generalization. For robotic arms, models like RT-2~\cite{brohan2023rt}, OpenVLA~\cite{kim2024openvla}, GR-2~\cite{cheang2024gr}, RoboMamba~\cite{liurobomamba}, and RDT-1B~\cite{liu2024rdt} leverage visual and language inputs for efficient task execution. In quadrupeds, models such as QUAR-VLA~\cite{ding2025quar} and QUART-Online~\cite{tong2024quart} improve generalization and adaptability in dynamic environments, while $\pi_0$~\cite{black2024pi_0} enables multi-embodied robots to perform diverse tasks. Despite these advances, due to the scarcity of datasets that combine first-person visual information, textual motion descriptions, and whole-body motion data for humanoid robots, VLA models have yet to be applied to humanoid robots. This paper takes the first step in building the Humanoid-VLA model to enable humanoid robots to autonomously perform loco-manipulation tasks.






