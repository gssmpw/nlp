\newpage
\appendix
\onecolumn

\section{Data collection}

Our motion dataset is derived from three primary sources: Motion Capture, Online Videos, and Synthetic Data. Specifically, the motion capture data is sourced from the open-source AMASS dataset, which includes textual annotations. Synthetic Data is generated by inputting random text into an open-source motion model to produce corresponding movements. Online Videos are collected from the web, with human motions extracted using the method described in ~\cite{wang2025tram}.

It is important to note that we select 15 universal humanoid joint points from the standard 22 SMPL joints in these open-source datasets. The framework enhances flexibility and extensibility by seamlessly integrating augmentation types with other conditions to create complex tasks, while advanced tools such as GPT-4 expand linguistic diversity through instruction rephrasing. By leveraging the inherent temporal and spatial dynamics of motion data, it enables models to learn more comprehensive and robust motion-language relationships. Additionally, improved cross-modal alignment is achieved through interleaved datasets that incorporate both motion and text, allowing models to better capture the interplay between motion and language.



\section{Data templates}

\begin{table}[ht]
  \centering
  \resizebox{1\linewidth}{!}
  {
  \begin{tabular}{@{}c|l@{}}
    \toprule
    \textbf{Task} & \textbf{\makecell[c]{Template descriptions}} \\
    \midrule
\multirow{9}{*}{\texttt{<Occlusion>} $\to$ \texttt{<Motion>}} 
& \textbf{Your help is crucial for us. Please complete the missing left leg data: \texttt{<Occlusion>}.}\\
&\quad1. \texttt{<Occlusion>} is part of the left leg motion. Please predict its complete data.\\
&\quad2. The left leg data: \texttt{<Occlusion>} is incomplete. Please help us fill in this part.\\
&\quad3. Please provide the complete left leg data: \texttt{<Occlusion>}.\\
&\quad...\\
&\quad N. To perfect the motion, we need the complete left leg data: \texttt{<Occlusion>}. Please assist us.\\
\\
& Your help is crucial for us. Please complete the missing right leg data: \texttt{<Occlusion>}. (\texttimes N)\\
& \texttt{<Occlusion>} is part of the left arm motion. Please predict its complete data. (\texttimes N)\\
& The right arm data: \texttt{<Occlusion>} is missing. Could you help us fill in this part? (\texttimes N)\\
\midrule
\multirow{3}{*}{\texttt{<Track>} $\to$ \texttt{<Motion>}} & Please ensure your root moves along \texttt{<Track>} for the next action. (\texttimes N)\\
& Please move your left hand according to the trajectory of \texttt{<Track>}. (\texttimes N) \\
& Please keep the movement of your right hand consistent with \texttt{<Track>}. (\texttimes N)\\
\midrule
\multirow{3}{*}{\texttt{<Motion>} $\to$ \texttt{<Track>}} 
& I would like a detailed analysis of the trajectory of the central position in this action: \texttt{<Motion>}. (\texttimes N)\\
& Tell me the path of the left hand: \texttt{<Motion>}. (\texttimes N)\\
& I would like a detailed analysis of the trajectory of the right hand in this action: \texttt{<Motion>}. (\texttimes N)\\

\midrule
\multirow{2}{*}{\texttt{<Time>} $\to$ \texttt{<Motion>}} 
& Can you make a motion that lasts for \texttt{<Time>} frames, with a certain percentage of variability? (\texttimes N)\\
& Show me a motion that is longer than \texttt{<Time>} seconds in duration. (\texttimes N)\\

\midrule
\multirow{2}{*}{\texttt{<Motion>} $\to$ \texttt{<Time>}} 
& Calculate the frame duration for \texttt{<Motion>}'s poses. (\texttimes N)\\
& Compute the duration in seconds for \texttt{<Motion>}'s poses. (\texttimes N)\\

\midrule
\multirow{3}{*}{\texttt{<State>} $\to$ \texttt{<Motion>}} 
& Randomly generate an entire action sequence from \texttt{<State1>}. (\texttimes N)\\
& Create actions randomly using the last state \texttt{<StateN>}. (\texttimes N)\\
& Move from the initial position \texttt{<State1>} to the final position \texttt{<StateN>}. (\texttimes N)\\
\midrule

\multirow{2}{*}{\texttt{<Caption>}+\texttt{<Occlusion>} $\to$ \texttt{<Motion>}} 
 & Explain the state before executing \texttt{<Motion>} actions. (\texttimes N)\\
& Explain the final conditions following \texttt{<Motion>} actions. (\texttimes N)\\
\midrule

\multirow{2}{*}{\texttt{<State>}+\texttt{<Time>} $\to$ \texttt{<Motion>}} &
Design a full action sequence culminating in \texttt{<StateN>} across \texttt{<Time>} frames. (\texttimes N)\\
& Plan a sequence of actions ending with \texttt{<StateN>} over \texttt{<Time>} seconds. (\texttimes N)\\
\midrule

\multirow{4}{*}{\texttt{<Occlusion>}+\texttt{<Caption>} $\to$ \texttt{<Motion>}} &
 To complete the \texttt{<Caption>} action, we need the missing center motion data: \texttt{<Occlusion>}. Please assist us. (\texttimes N)\\
& To complete the \texttt{<Caption>} action, we need the missing left leg motion data: \texttt{<Occlusion>}. Please assist us. (\texttimes N)\\
& To finish the \texttt{<Caption>} action, we need to fill in the missing left arm motion data: \texttt{<Occlusion>}. Please assist us. (\texttimes N)\\
& Your assistance is crucial. Please help us complete the missing right arm data: \texttt{<Occlusion>} for the \texttt{<Caption>} action. (\texttimes N)\\
\midrule

\multirow{3}{*}{\texttt{<Track>}+\texttt{<Caption>} $\to$ \texttt{<Motion>}} &
 Please keep your root on the trajectory of \texttt{<Track>} while performing the actions described by \texttt{<Caption>}. (\texttimes N)\\
& While your left hand is moving along \texttt{<Track>}, perform the action described by \texttt{<Caption>}. (\texttimes N)\\
& Generate an action according to the description \texttt{<Caption>} while ensuring your right hand remains on the trajectory of \texttt{<Track>}. (\texttimes N)\\
\midrule

\multirow{3}{*}{\texttt{<State>}+\texttt{<Track>}+\texttt{<Caption>} $\to$ \texttt{<Motion>}} &
Starting from \texttt{<State1>}, follow the direction of your root guided by \texttt{<Track>} to perform the dynamic described by \texttt{<Caption>}. (\texttimes N)\\
& Starting from \texttt{<State1>}, your right hand needs to follow the path of \texttt{<Track>} and complete the dynamic described in \texttt{<Caption>}. (\texttimes N)\\
& Starting from \texttt{<State1>}, follow the direction of your root guided by \texttt{<Track>} to perform the dynamic described by \texttt{<Caption>}. (\texttimes N)\\
\midrule
\multirow{1}{*}{\texttt{<Motion>}+\texttt{<Time>}+\texttt{<Caption>} $\to$ \texttt{<Motion>}} &
Produce an action sequence for \texttt{<Caption>} incorporating a partial motion sequence described as \texttt{<Motion>} spanning \texttt{<Time>} frames. (\texttimes N)\\
    \bottomrule
  \end{tabular}
  }
  \caption{\textbf{Examples of conditional language description.} All task descriptions could be expanded $N$ times similar to the first example. }
  \label{tab:app_templates}
\end{table}


% \begin{table}[ht]
%   \centering
%   \resizebox{1\linewidth}{!}
%   {
%   \begin{tabular}{@{}c|l@{}}
%     \toprule
%     \textbf{Task} & \textbf{\makecell[c]{Template descriptions}} \\
%     \midrule
% \multirow{9}{*}{<Occlusion> → <Motion>} 
% & \textbf{Your help is crucial for us. Please complete the missing left leg data: <Occlusion>.}\\
% &\quad1. <Occlusion> is part of the left leg motion. Please predict its complete data.\\
% &\quad2. The left leg data: <Occlusion> is incomplete. Please help us fill in this part.\\
% &\quad3. Please provide the complete left leg data: <Occlusion>.\\
% &\quad...\\
% &\quad N. To perfect the motion, we need the complete left leg data: <Occlusion>. Please assist us.\\
% \\
% & Your help is crucial for us. Please complete the missing right leg data: <Occlusion>.(\times N)\\
% & <Occlusion> is part of the left arm motion. Please predict its complete data.(\times N)\\
% & The right arm data: <Occlusion> is missing. Could you help us fill in this part? (\times N)\\
% \midrule
% \multirow{3}{*}{<Track> → <Motion>} & Please ensure your root moves along <Track> for the next action. (\times N)\\
% & Please move your left hand according to the trajectory of <Track>. (\times N) \\
% & Please keep the movement of your right hand consistent with <Track>. (\times N)\\
% \midrule
% \multirow{3}{*}{<Motion> → <Track>} 
% & I would like a detailed analysis of the trajectory of the central position in this action: <Motion>. (\times N)\\
% & Tell me the path of the left hand: <Motion>. (\times N)\\
% & I would like a detailed analysis of the trajectory of the right hand in this action: <Motion> (\times N)\\

% \midrule
% \multirow{2}{*}{<Time> → <Motion>} 
% & Can you make a motion that lasts for <Time> frames, with a certain percentage of variability? (\times N)\\
% & Show me a motion that is longer than <Time> seconds in duration. (\times N)\\

% \midrule
% \multirow{2}{*}{<Motion> → <Time>} 
% & Calculate the frame duration for <Motion>'s poses. (\times N)\\
% & Compute the duration in seconds for <Motion>'s poses. (\times N)\\

% \midrule
% \multirow{3}{*}{<State> → <Motion>} 
% &Randomly generate an entire action sequence from <State1>. (\times N)\\
% & Create actions randomly using the last state <StateN>. (\times N)\\
% & Move from the initial position <State1> to the final position <StateN>. (\times N)\\
% \midrule

% \multirow{2}{*}{<Caption>+<Occlusion> → <Motion>} 
%  &Explain the state before executing <Motion> actions. (\times N)\\
% & Explain the final conditions following <Motion> actions. (\times N)\\
% \midrule

% \multirow{2}{*}{<State>+<Time> → <Motion>} &
% Design a full action sequence culminating in <StateN> across <Time> frames. (\times N)\\
% & Plan a sequence of actions ending with <StateN> over <Time> seconds. (\times N)\\
% \midrule

% \multirow{4}{*}{<Occlusion>+<Caption> → <Motion>} &
%  To complete the <Caption> action, we need the missing center motion data: <Occlusion>. Please assist us. (\times N)\\
% &To complete the <Caption> action, we need the missing left leg motion data: <Occlusion>. Please assist us. (\times N)\\
% &To finish the <Caption> action, we need to fill in the missing left arm motion data: <Occlusion>. Please assist us. (\times N)\\
% &Your assistance is crucial. Please help us complete the missing right arm data: <Occlusion> for the <Caption> action. (\times N)\\
% \midrule

% \multirow{3}{*}{<Track>+<Caption> → <Motion>} &
%  Please keep your root on the trajectory of<Track> while performing the actions described by <Caption>. (\times N)\\
% & While your left hand is moving along <Track>, perform the action described by <Caption>. (\times N)\\
% &Generate an action according to the description <Caption> while ensuring your right hand remains on the trajectory of<Track>. (\times N)\\
% \midrule

% \multirow{3}{*}{<State>+<Track>+<Caption> → <Motion>} &
% Starting from <State1>, follow the direction of your root guided by <Track> to perform the dynamic described by <Caption>. (\times N)\\
% &Starting from <State1>, your right hand needs to follow the path of<Track> and complete the dynamic described in <Caption>. (\times N)\\
% &Starting from <State1>, follow the direction of your root guided by <Track> to perform the dynamic described by <Caption>. (\times N)\\
% \midrule
% \multirow{1}{*}{<Motion>+<Time>+<Caption> → <Motion>} &
% Produce an action sequence for <Caption> incorporating a partial motion sequence described as <Motion> spanning <Time> frames. (\times N)\\
%     \bottomrule
%   \end{tabular}
%   }
%   \caption{\textbf{Examples of conditional language description.} All task descriptions could be expand N times similar to the first example. }
%   \label{tab:app_templates}
% \end{table}


The input descriptions for our subtasks are presented in Table ~\ref{tab:app_templates}. Specifically, for each category of subtasks, such as "Complete left leg data," we have developed N variations of expressions. This allows our self-supervised augmentation approach to significantly expand the range of task descriptions, increasing the original 59 subtasks at N times.




\section{Simulation Performance}

\begin{figure*}[h]
  \centering
   \includegraphics[width=1\linewidth]{imgs/humanvla.pdf}
   \caption{\textbf{Simulation robot experiment}.}
   \label{fig:humanvla}
\end{figure*}


To enhance humanoid robots' environmental interaction capabilities, we selected 2 representative object interaction tasks from the HITR dataset referencing HumanVLA ~\cite{xu2024humanvla}. For each task, we collected comprehensive data including egocentric visual frames, natural language instructions, and robot control signals. Our approach extends beyond real-world scenarios into simulation environments, where we followed HumanVLA's setup to extract 15 key joints from stick figure trajectories for constructing our training dataset. This methodology leverages the abundance of existing stick figure/humanoid datasets, enabling efficient data acquisition through retargeting for both visual and motion aspects. Although joint retargeting to humanoid forms may not match the quality of motion capture data, it presents promising opportunities for continued research using large-scale simulation-based VLA data for downstream task training.

A key strength of our universal framework lies in its demonstrated adaptability across different robot configurations. Unlike previous approaches that are often constrained to specific platforms, our framework successfully generalizes between distinct robot architectures in simulation and real-world implementations. This universal applicability represents a significant advancement over prior work such as HumanVLA, which, while providing excellent policy foundations, was limited in achieving universal humanoid control and real robot deployment. Our framework transcends these limitations by establishing a universal bridge between simulation and physical robot systems, paving the way for truly generalizable humanoid control strategies that can be deployed across diverse platforms and tasks.


\section{More Details}
It is noteworthy that the joint points for motion generation in our model are not the conventional 22 joints, but rather a set of 15 joints that are common to both humanoid robots and humans. This approach ensures universality across different configurations. However, an additional optimization step is required to adapt the poses generated by the large model for downstream applications. Specifically, generating a motion sequence with 15 joints necessitates an optimization process to map it onto the target downstream configuration.

To acquire training data, we extract 15 joint points from standard datasets for training purposes. This ensures that the model learns from a consistent and universally applicable set of joint points.

To enable humanoid robots to execute corresponding motions through joint-point mapping, we employ the Adam optimizer similar to ~\cite{mao2024learning, jiang2024harmon}, which map the positions of the 15 joints from keypoints to the 24 joints of the humanoid robot. By maintaining the end-effector positions as closely aligned as possible with the existing joint node positions, the overall motion pattern of the humanoid robot remains consistent with the keypoint representation.

We define the problem as training a goal-conditioned RL policy $\pi$ that maps human motion onto the joints of a humanoid robot $j_t\in \mathbb{R}^{24}$. We define a reward strategy $\mathcal{R}$, which takes the observation $\mathcal{O}$ and the given goal $\mathcal{G}$ as input, and outputs the target positions for the proportional-derivative (PD) controller in the action space $\mathcal{A}$. We use the proximal policy gradient (PPO) to maximize the accumulated reward.



\section{Limitation}

\textbf{Robustness of the RL Policy}. Although we have developed a general RL policy, its performance lacks sufficient robustness. We plan to further refine the policy to enhance task completion.

\textbf{Limited Availability of High-Quality Data}. The availability of high-quality data is limited, including manually annotated data and execution data from real-world humanoid robots. While we considered leveraging datasets from works like Mimicking-Bench~\cite{liu2024mimicking}, the restricted robot configurations render them unsuitable for general robotics tasks. Consequently, we will undertake the collection of data for general humanoid robot tasks and encourage the community to recognize and address this data scarcity.

\textbf{Training Approach}. Our current training methodology is relatively simple, and not fully leveraging the available data. We have identified several strategies to enhance the training of motion generation models, we will incorporate these techniques into our future work.