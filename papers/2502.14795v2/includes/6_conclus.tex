\section{Conclusion}
This paper introduces Humanoid-VLA, a novel framework designed to address the challenges of humanoid robot control with egocentric visual integration. The framework aligns language and motion using human motion datasets, enables context-aware motion generation through cross-attention mechanisms, and tackles data scarcity using self-supervised pseudo-annotations. Built on whole-body control architectures, Humanoid-VLA facilitates adaptive object interaction and exploration with enhanced contextual understanding.
The effectiveness of Humanoid-VLA has been validated through evaluations of motion generation quality and execution success rates on real humanoid robots, demonstrating high executability. In the future, we aim to enhance the success rate of humanoid robots in performing more complex loco-manipulation tasks.

