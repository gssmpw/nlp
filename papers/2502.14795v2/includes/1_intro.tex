\section{Introduction}


\begin{figure}
  \centering
   \includegraphics[width=1\linewidth]{imgs/teaser.pdf}
   \caption{\textbf{Comparison between previous works and our approach.} With the capability of autonomous perception, Humanoid-VLA can perform tasks to interact with objects, significantly advancing beyond previous methods that rely on mimicking human demonstrations for motion execution.}
   \label{fig:teaser}
\end{figure}

Humanoid robots are poised to transform diverse industries—from healthcare to manufacturing—by combining human-like dexterity with adaptability to execute complex tasks. Building on extensive human motion datasets from computer graphics research \cite{mahmood2019amass,guo2020action2motion}, recent advances have established data-driven frameworks for humanoid motion skill acquisition. 

Initial research \cite{cheng2024expressive,ji2024exbody2} developed whole-body controllers that translate basic human kinematic sequences into humanoid motion. The field has since progressed to integrate multimodal perception, enabling humanoids to perform real-time mimicry of human demonstrations \cite{he2024learning} and respond fluidly to natural language commands \cite{mao2024learning}. 
While these approaches achieve high-fidelity motion control in humanoid robots, they operate primarily through reactive mechanisms, dynamically adjusting motions in response to external inputs. They cannot perceive autonomously and infer potential interaction targets within their immediate surroundings. This limitation substantially impedes their deployment in scenarios that demand object manipulation or adaptive exploration in complex environments.
To this end, this paper aims to investigate universal humanoid control with egocentric visual integration. 

However, developing such a system faces a significant bottleneck: \textbf{data scarcity}.
Existing motion capture datasets lack synchronized first-person visual information, making direct transfer to egocentric tasks impossible. Moreover, while teleoperation offers a theoretical pathway for collecting visuomotor data, its prohibitive costs severely constrain large-scale acquisition. These constraints lead to inadequate training datasets in quantity and diversity, which hinders the development of a foundation model for humanoid control with egocentric visual integration. 

As to the challenge of data scarcity, we propose a feasible and cost-effective paradigm.
Specifically, we begin by establishing a language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions. This enables the model to learn universal motion patterns and action semantics from diverse third-person observations, yielding a robust, generalizable motion representation that does not rely on egocentric visual input.
Next, we incorporate egocentric visual context through a parameter-efficient cross-attention module. This adaptive mechanism preserves the integrity of the pretrained model while allowing for the dynamic fusion of first-person visual features, thereby enabling context-aware motion generation. 
Our framework essentially reduces the dependence on egocentric datasets, making combining language understanding and egocentric scene perception with motion control feasible.

Furthermore, the existing training paradigm alone is insufficient to ensure optimal model performance, primarily due to limitations in the alignment between motion and language. Drawing inspiration from the success of MLLMs \cite{liu2023llava, zhang2023video}—where robust large language models (LLMs) serve as foundational components—we argue that the effectiveness of our framework hinges on the pre-alignment of motion and language representations. Achieving this alignment, however, heavily depends on the availability of large-scale and high-quality data. Unfortunately, current motion datasets are insufficient in scale to meet this need. While video sources offer a vast reservoir of human data, their utility for model training is constrained by the lack of motion description annotations.


To address this limitation, we propose a self-supervised data augmentation framework that generates pseudo-annotations through automated motion analysis.
Our solution features an automatic annotation pipeline that extracts semantic meaning directly from motion sequences via carefully designed self-supervised tasks.
A representative implementation involves temporarily masking specific body joints within motion sequences and training the model to reconstruct the occluded movements. We automatically generate instructional prompts for such tasks as "missing left arm <Occlusion> motion data. Please complete the motion" paired with corresponding ground truth motions as target outputs. This automated process systematically converts raw motion data into meaningful question-answer pairs.
By integrating these self-supervised learning objectives, our approach circumvents the need for manually annotated textual descriptions while effectively utilizing large-scale and unlabeled motion data extracted from video repositories.

Finally, integrating a whole-body controller following previous work~\cite{he2024learning}, our proposed \textbf{Humanoid-VLA} seamlessly combines language understanding, scene perception, and motion control into a unified system. 
Extensive experiments show our approach significantly enhances humanoid robots' autonomous interaction capabilities in real-world environments, paving the way for practical deployment across diverse applications.









