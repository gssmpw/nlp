\section{Humanoid-VLA}








\textcolor{red}{In this section, we first present an overview of the proposed solution to address the key challenge of data scarcity in training VLA models for humanoid control.
We then provide a detailed explanation of the proposed solution, outlining its methodology and implementation.}


\subsection{Preliminary. } 

To begin with, we review the definition and limitation of humanoid control and then identify the challenge of integrating ego-centric visual information into this task.

\textbf{Definition. }
With the increasing availability of human data in computer graphics and computer vision, recent humanoid control has increasingly adopted methods that learn from human data. 
Specifically, given a target body pose from physical teleoperation (e.g., motion capture system) and the humanoid's proprioception, the whole body controller $\mathcal{P}$ generates joint torques to control the humanoid robot. Formally, this can be expressed as
\begin{equation}
    j_t = \mathcal{P}(s_t, p_t),
\end{equation}
where $s_t, p_t, j_t$ means body
pose, humanoid's proprioception and joint torques at time $t\in \mathbb{N}^+$. 

\textbf{Limitation. } 
Some studies have expanded the application of humanoid control using methods like pose estimation from video data, but these approaches are limited to mimicking 3D human pose trajectories. However, developing a general-purpose robot requires purposive learning, which involves extracting meaningful intentions from human data and adapting prior experiences to novel tasks or environments. Current data acquisition methods, focusing mainly on human joint poses, lack integration with egocentric vision. Thus, they can only teach robots what actions are performed, not the underlying intent or context. Consequently, trajectory-level imitation is inherently limited in generalizability due to environmental discrepancies.

\textbf{Challenge. } 
In other domains like robot arms, the integration of visual information is often addressed through vision-language-action (VLA) models.
However, applying such models to humanoid systems remains impractical due to a critical limitation: \textbf{data scarcity}. First, existing large-scale human datasets lack egocentric vision, i.e., first-person perspective information, which is essential for training models tailored to humanoid tasks. 
Second, even if paired vision-language-action datasets could be collected, the high cost of collection limits their current scale and diversity, which are insufficient to support the development of high-quality humanoid VLA models.


\subsection{A Holistic View of Our Method. } 

% Building upon the success of MLLMs,
% we propose a solution that can effectively leverage the available large-scale non-egocentric human data to train a VLA model without relying entirely on limited data collected from first-person perspectives.
Building on the success of MLLMs, we propose a solution that effectively utilizes large-scale non-egocentric human data to train a VLA model, minimizing reliance on scarce first-person perspective data.

\textbf{Solution. } 
Our method combines non-egocentric and egocentric data in two stages: \textbf{Language-Motion Pre-Alignment} and \textbf{Vision-Conditioned Fine-Tuning}.
1) In the first stage, we use a large language model (LLM) to align non-egocentric human motion data with language descriptions, enabling the model to learn motion patterns and action semantics without relying on egocentric vision. This stage builds a robust foundation for motion generation using abundant non-visual data.
2) In the second stage, we incorporate first-person visual information through efficient fine-tuning on vision-language-action pairs. This integration refines the motion generation process, while significantly reducing the need for extensive egocentric data.
By combining non-egocentric and egocentric data in a complementary manner, our approach mitigates data scarcity and improves the model’s ability to generalize across diverse tasks.

% \textbf{Solution. } 
% % 想一个更好的词，概述我们这种训练范式
% We propose an approach that strategically combines non-egocentric and egocentric data in a staged learning process:  \textbf{language-motion pre-alignment} and \textbf{vision-conditioned fine-tuning}.
% Specifically, we first pre-align existing non-egocentric human motion data with language descriptions, enabling the model to learn basic motion patterns and action semantics. 
% Decoupling the initial learning of motion and language from visual integration allows for establishing a robust foundation for motion generation. 
% Once this foundation is built, egocentric vision data, even if limited, can be incorporated during later stages of training through efficient fine-tuning techniques. 
% This approach mitigates the data scarcity problem and enables the model to generalize across diverse tasks by combining non-egocentric and egocentric data in a complementary manner. Specifically,

% \textbf{Language-Motion Pre-Alignment.}
% In the first stage, we leverage a large language model (LLM) as the foundational model and align it with diverse human motion data through a language-motion pre-alignment process. By doing so, the model learns to understand and generate human motions without requiring egocentric vision information. This stage utilizes a large amount of non-visual human motion data, preserving fundamental coarse-grained motion generation capabilities.

% \textbf{Vision-Conditioned Fine-Tuning.}
% In the second stage, we introduce first-person visual information as a condition for motion generation. Using parameter-efficient fine-tuning on first-view vision-language-action pairs, we integrate egocentric vision into the model. This approach refines the motion generation process by incorporating visual context while significantly reducing the dependence on extensive data.



% By decoupling the learning of basic motion generation from visually conditioned fine-tuning, our method ensures a balance between data efficiency and performance. The two-stage strategy allows the model to generalize across diverse motion scenarios in the first stage, while leveraging visual information in the second stage to enhance fine-grained motion generation.

\textbf{Overall Framework. } 
Combined with raw whole-body controller $\mathcal{P}$, we could build a humanoid system that can autonomously perceive, interpret, and act in real-world environments with visual context and language-driven instructions, which is our proposed \textbf{Humanoid-VLA}.
Formally, given an observation $o_t$
at time $t \in \mathbb{N}^+$,
the cognition model $\mathcal{C}$ produces an \textcolor{red}{body pose $s_t \in \mathbb{R}^{}$}
to achieve the goal instruction \textcolor{red}{$g_t$}.
\[
     j_t = \mathcal{P}(s_t)=\mathcal{P}(\mathcal{C}(o_t, g_t)).
\]
where the observation \textcolor{red}{$o_t$} 
represents the RGB observation
while the goal \textcolor{red}{$g_t := (l_{t}; d_t; c_t)$} 
consists of different language descriptions about
motion descriptions $l_{t}$, 
motion duration time $d_t$ 
and other conditions $c_t$.
Next, we will illustrate how to accomplish this framework.

\begin{figure*}[ht]
  \centering
   \includegraphics[width=0.97\linewidth]{imgs/model.jpg}
   \caption{\textbf{Overview}. }
   \label{fig:teaser}
\end{figure*}

% \begin{figure*}[t]
%   \centering
%    \includegraphics[width=0.95\linewidth]{imgs/template_new.pdf}
%    \vspace{-5mm}
%    \caption{\textbf{Train}. }
%    \label{fig:real_robot_exp}
%    \vspace{-3mm}
% \end{figure*}

% % 前情提要
% \subsection{Preliminary}
% % 对于整个数据集
% \textbf{Imitation Learning.} Language instructions allow the agent to learn manipulation plans from a set of $N$ motion sequences $\{m_i\}_{i=0}^N$. Each individual motion sequence $m$ contains a series of state-action tuples $m=\{(s_t, a_t)\}_{t=0}^{T}$ lasting $T$ time steps, where $s_t$ and $k_t$ represent the state and action at the $t$ step respectively. The learning objective can be concluded as maximizing the log-likelihood across the dataset complying with the imitation learning manner:
% % ~\cite{1988imitation}
% \begin{equation}
%     \mathcal{L} = \mathbb{E}_{(m,l)_i\sim D}(\sum_{t=0}^{|m|}\log \pi_{\theta}(a_t|s_t,l)),
% \end{equation}
% where $D=\{(m,l)_i\}_{i=0}^N$, and $l$ is the language instruction for the trajectory. As shown in Figure xxx, the policy aims to map to the action space $\pi_{\theta}(a| s,l)$ by learning parameters $\theta$ from model $\pi_{\theta}$. 
% % ~\ref{fig:vla_arch}
\subsection{Language-Motion Pre-Alignment.}

\begin{table}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{@{}c|cc|ccc@{}}
\toprule
Category  & Text & Keypoint & Clips & Frames & Hours \\
\midrule
Motion capture  &\checkmark&\checkmark & 29K & 0.3M & 4.1 \\
Online Video &\ding{55}& \checkmark& 0.8M & 541M & 7515.7 \\
{Synthetic Data}  &\checkmark& \checkmark& 100K & 16M & 227.7 \\
% Manual Annotated  & \checkmark& \checkmark& 4K & 0.7M & 10.3 \\
% Manual Annotated & Video &  &  & 4K & 0.7M & 10.3 \\
\midrule
\multicolumn{3}{c}{Total}    & \textbf{0.929M} & \textbf{557.3M} & \textbf{7790.2} \\
% Total (A)&  &  &  & \textbf{20.7M} & \textbf{560M} & \textbf{7790.2}\\
\bottomrule
\end{tabular}
}
\caption{\textbf{Datasets Statistics}}
\label{tab:data statistics}
\end{table}

This stage focuses on building a model that generates pose trajectories based on non-egocentric goal instructions. The first step toward achieving this goal is addressing the issue of data acquisition.

\subsubsection{Limitations of data acquisition.}

% 需要确认下是humanml3d还是amass
% 需要解决引用的问题
% 目前写的有点适合放在intro里面
% \textbf{Prior Bottleneck in Data Utilization. } 
% A key limitation in training language-motion alignment models lies in the gap between data availability and annotation completeness. Datasets like HumanML3D, which pair motion trajectories with language descriptions, enable effective training but remain too small to drive significant progress. In contrast, large-scale online video datasets, as shown in Table~\ref{tab:data statistics}, offer abundant and diverse motion data. However, their lack of corresponding language annotations severely limits their utility for this task.

One of the key limitations in training language-motion alignment models lies in the gap between data availability and annotation completeness. Datasets such as HumanML3D, which pair motion trajectories with language descriptions, enable effective training but remain too small to support significant progress. In contrast, large-scale online video datasets (as shown in Table~\ref{tab:data statistics}) provide abundant and diverse motion data. However, the lack of corresponding language annotations severely restricts their utility for this task.

Recent efforts to mitigate this bottleneck focus on annotating large-scale video datasets either manually or through video large language models (VLLMs). While these approaches have shown some promise, they face two main challenges:
%
1) High Cost of Manual Annotation: Manual labeling is prohibitively expensive, especially for large-scale video datasets, making it impractical for widespread use.
2) Low Quality of VLLM-Generated Annotations: Current VLLMs struggle to capture fine-grained motion details and fail to accurately describe complex human actions. As a result, the annotations they generate are often noisy, incomplete, or imprecise.
%
These challenges limit the effectiveness of the resulting datasets for training language-motion alignment models and hinder progress in this area.


% For example, while models like \textcolor{red}{MDM~\cite{}} or T2M~\cite{} can generate motions for simple commands like "go forward," they fail to produce feasible motions for more complex instructions such as "go forward like a drunken man." 

% In essence, limited data restrict the model's ability to generate arbitrary motions, posing a significant barrier to universal humanoid control.

% 加入验证实验，用anootaion的数据集用，效果会很差。
% \textbf{Huge Unpaired Data. } On the contrary, as shown in Table~\ref{tab:data statistics}, large-scale online video datasets can serve as a valuable resource for motion data. However, these datasets lack corresponding language annotations, which limits their utility in aligning language with motion modalities.

% Recently, some studies have sought to address this issue by leveraging video large language models (video LLMs) to automatically annotate large-scale video datasets. While this approach holds promise, it faces several significant challenges. First, the cost of automated annotation is prohibitively high, especially when applied to massive video datasets. Second, the quality of annotations generated by current video LLMs is often inadequate due to their limited ability to capture fine-grained motion details.
% In particular, current video LLMs struggle to accurately interpret and describe complex human actions, resulting in annotations that are noisy, incomplete, or imprecise. These shortcomings ultimately hinder the effectiveness of the collected data.

% The purpose of this stage is to leverage the capabilities of large language models (LLMs) to understand and generate universal motion patterns. By aligning language with motion, the goal is to enable the model to generalize across diverse motion scenarios and establish a strong foundation for motion generation.

% However, as shown in Table~\ref{tab:data statistics}, the data available for training LLMs in this domain is limited primarily to motion capture (mocap) data and some synthetic datasets. While these datasets provide both motion sequences and corresponding motion descrption, they lack the diversity and scale necessary to fully exploit the potential of LLMs. Another critical limitation is that large-scale online video datasets, which could serve as a rich resource for motion data, do not include corresponding language annotations. This lack of language annotation prevents the effective use of such datasets for training VLA models and, consequently, hinders progress in scaling up motion generation models.

% This situation highlights a fundamental bottleneck in the field of motion generation: \textbf{the inability to scale models to larger datasets due to the lack of high-quality, cost-effective language annotations for motion-related data.}

\subsubsection{Our solution to data acquisition.}
\begin{figure*}
  \centering
   \includegraphics[width=1\linewidth]{imgs/template_new.png}
   \caption{\textbf{Augmentation Templates}. }
   \label{fig:templtes}
\end{figure*}

\textbf{Self-Supervised Data Augmentation. }
To address the issue of missing language annotations in human motion data extracted from video sources, we propose an automatic language annotation augmentation approach based on designing various self-supervised tasks directly derived from motion data.
For example, one of the tasks we design involves selectively masking certain joints in a motion sequence and formulating a motion completion objective, where the model predicts the missing portions of the sequence.
Through this self-supervised data augmentation strategy, we can eliminate the dependence on explicit textual motion descriptions.

Next, we will now explain how this is achieved through two key modules:
: \textbf{1) Compositional Motion Quantitation} and 2) \textbf{Interleaved Data Generation.}

\begin{figure}
  \centering
   \includegraphics[width=1.0\linewidth]{imgs/vqvae.jpg}
   \vspace{-5mm}
   \caption{\textbf{Motion VQ}. }
   \label{fig:motion vq}
   \vspace{-3mm}
\end{figure}

\textbf{Compositional Motion Quantitation.}
As shown in Figure~\ref{fig:motion vq}, we propose a decompositional compression method for body pose representation.
Specifically, we decompose each body pose into five body-based tokens corresponding to five distinct parts: the left leg, right leg, torso, left arm, and right arm. 
We independently train an encoder and its corresponding codebook for each body part to compress the body part data, denoted as $K_h$, into a quantized representation $Z_h \in \mathbb{R}^{5}$.
Formally, we define the motion encoder $\mathcal{E}_m$, which compresses $K_h$ into five tokens $Z_h$, as follows:
\begin{equation}
    Z_h = \mathcal{E}_m(K_h).
\end{equation}


This compositional encoding method is crucial, allowing for flexible editing of motion sequences. The advantage of decomposing a body pose into multiple parts and encoding them separately lies in that we can form flexible operations on the motion sequence at the token level. For instance, we can replace, perturb, or rearrange the tokens corresponding to specific body parts to generate new motion patterns. This flexibility significantly enhances control over motion data, laying the foundation for further task design.


% % Additionally, this approach generates consistent, data-driven supervision signals by leveraging the inherent temporal and spatial dynamics of motion data. This also enables models to learn richer and more robust representations of motion-language relationships. 

% Previous work often directly used VQ coding for motion to expand motion data flexibly and effectively without description. However, this encoding method makes it challenging to perform subsequent expansion operations on the motion. 
% Therefore, we adopt a decompositional compression method to prevent the model from generating overly long motion representations. By doing so, we compress the data into five body-based tokens according to their positions.

% % 具体而言，为了方便对帧数和身体部位的定位，对于每帧给定15个关节点 R^15*3，我们首先将K_s分为左腿，右腿，torso，左手，右手五个部位。
% Specifically, for easier tracking of frames and body parts, given 15 joint points $K_h\in \mathbb{R}^{15\times3}$ for each frame, we decompose keypoints into five parts: left leg, right leg, torso, left arm, and right arm. 


% % 对于每一个身体部位数据，我们分别训练encoder及对应的codebook，将K_s压缩为R^5。
% For the data of each body part, we separately train an encoder and its corresponding codebook to compress $K_h$ into $Z_h \in \mathbb{R}^{5}$ by mapping the body part data into quantized tokens. Formally, we define this motion encoder $\mathcal{E}_m$ which compresses $K_s$ into five tokens as follows:
% \begin{equation}
%     Z_h = \mathcal{E}(K_h).
% \end{equation}

% % 这个方法不仅可以减少模型的输出维度，同时可以配合数据增强方式，更加灵活的使用不同模板进行有控制的输出。
%  The loss function of {\color{red}humanVQVAE} can be expressed as the combination of reconstruction loss, vector quantization and commitment loss:
% \begin{equation}
%  \mathcal{L} = \mathcal{L}_{\text{rec}} +\mathcal{L}_{\text{VQ}}+\mathcal{L}_{\text{com}}.
% \end{equation}
% This approach not only reduces the output dimensionality of the model, but also enables more flexible and controllable motion generation.

\textbf{Interleaved Data Generation. }
As shown in Figure~\ref{fig:templtes}, we introduce four augmentation types—<Track>, <Time>, <Absent>, and <State>—to extract diverse features from raw motion data. For instance, in the <Track> augmentation, we isolate the temporal trajectory of a specific joint (e.g., the root joint) and encode it as a corresponding motion token. By pairing this motion feature with an instructional text, such as “Please move your center position along the trajectory of <Track>,” and using the complete motion sequence as the answer, we construct meaningful question-answer pairs. 
This procedure enables the augmentation of datasets originally devoid of linguistic annotations.

This method offers several advantages. First, it is highly flexible and extensible: augmentation types like <Track> can be combined with other conditions (e.g., <Time>) to create more complex tasks, while linguistic diversity can be expanded by rephrasing the same instruction via GPT4 in multiple ways. Second, the framework utilizes motion data's inherent temporal and spatial dynamics, enabling models to learn richer and more robust motion-language relationships. Finally, interleaved datasets enhance cross-modal alignment, where both inputs and outputs contain motion and text. As supported by prior work, such as VILA~\cite{}, training paradigms of this kind allow models to better capture the interplay between motion and language without impairing performance on their original tasks.

Using this data augmentation approach, we collected the largest motion-language interleaved dataset to date, with a scale that is 100 times larger than previous work. This effectively addresses the data scarcity issue for training foundational human motion models. Next, we will detail the training process.

\subsubsection{Training.}

\textbf{Model Selection.}

\textbf{Training Strategy.}
We first pre-train on a large-scale video dataset to fully leverage the abundance of data, followed by fine-tuning on the AMASS dataset for enhanced performance on downstream tasks.
It is worth noting that we also apply the previously mentioned template-based augmentation method to the AMASS dataset. With the inclusion of motion descriptions, the variety of template augmentations increases, enabling a more thorough exploration of the model's potential.

\textbf{Parameter scaling.}

\textbf{Implementation Details.}
% 我们使用了llama-instruct 8B和llama-vision 70B作为foundation model。在训练阶段设置了学习率为2e-5，lr scheduler type为consine。每个device的batch size设置为4。对于每一个关节部位的encoder，我们设置它的codebook大小为1024.我们使用H100进行模型训练，训练时间为xxxh。warm up ratio被设置为0.01。
We utilize llama3 as the foundation models. In the training phase, warm up ratio is set at 0.01, with learning rate is configured at 2e-5, and the cosine learning scheduler. The batch size per device is set to 4. For the encoder of each joint part, its codebook size is set to 1024. We conduct model training using 8 NVIDIA H100 Tensor Core GPUs through \textcolor{red}{XXX} hours. 


% % 我们进一步在高质量人工标注数据集上进一步进行微调，目标使模型能够生成更自然、复杂且长时间连续的动作。
% We conduct further fine-tuning of the model on the meticulously curated, high-quality manually annotated dataset, which is feature as wide range and human annotated. 
% The primary objective of fine-tuning process is to empower the model to generate actions that are considerably more natural in appearance, highly complex in composition, and capable of performing continuously over long durations.
% Through fine-tuning, we aim to enhance the model's performance in handling tasks that demand high-fidelity and long-term coherent action generation, thereby making it more applicable and reliable in practical applications.


\subsection{Vision-Conditioned Fine-Tuning.}

% \subsection{Vision Integration}
%为了让机器人更好地与环境交互，我们在HITR数据集（引humanvla）中选择了20个具有代表性的rearrangement任务，收集第一视角视觉、自然语言指令、机器人控制信息数据。
To enhance humanoid's ability to interact effectively with environment, we selected 20 representative rearrangement tasks from the HITR dataset. For each task, we collected data encompassing heading camera's view frames, natural language instructions, and proprioception information.

% 我们集成这些数据并生成多模态对话结构，对其应用定制的对话模板格式化，并将图像和文本encode为模型所需的输入格式。使用这些数据利用LoRA对模型进行微调。
We integrated these data and generated a multimodal dialogue structure, formatting it using a predefined template. The images and instructions were encoded into the input format required by the model. Using processed data, the model was fine-tuned through LoRA. 
% 在推理时，实时渲染的第一视角相机图片V、自然语言指令L被输入我们的 model to produce 15 joint points Kh。这些关键点 are then 被整理成五个tokens， 用于下游RL policy去控制机器人运动。
During inference, real-time rendered heading camera view frames \( V \) and natural language instruction \( L \) are input into our model to generate 15 joint key points \( K_h \):
\[
K_h = \text{Model}(V, L)
\]
These key points are then organized into five tokens:
% \[
% \text{Tokens} = \text{Organize}(K_h)
% \]
The resulting tokens are then used as inputs for the downstream reinforcement learning policy to control the humanoid’s movements. 

% 这一方法使模型能够有效地学习视觉环境与文本指令之间的关联，从而提升其理解自然语言并在环境中交互完成任务的能力。
This method enables the model to interpret visual inputs, allowing it to effectively learn the associations between the environment and  instructions. Consequently, it enhances the model's ability to comprehend natural language and interact with the environment to complete tasks.
% This method enables the model to effectively learn the relationships between the environment and instructions, thereby enhancing its ability to comprehend natural language and interact with the environment to accomplish tasks.
% This method allows the model to effectively learn the relationships between visual and language inputs, enhancing its ability to comprehend natural language and interact with the environment to complete tasks.


\subsection{Humanoid Retargeting}
% 为了使人形机器人可以通过关节点映射实现相应的动作，我们将15个关节位置从keypoint通过Adam优化器映射到24个人形机器人上。通过将末端执行器位置与已有关节点位置尽可能保持一致，使得整体人形机器人的运动模式与关节点保持一致。
In order to enable the humanoid robot to perform corresponding actions through joint-point mapping, we map the positions of 15 joints from keypoints to 24 humanoid robots \jc{?} using the Adam optimizer. By keeping the position of the end-effector as consistent as possible with the positions of the existing joint nodes, the overall motion pattern of the humanoid robot will be resemble with the keypoint representation.
% 我们将问题定义为一个训练一个goal-conditioned RL的policy π，将人类motion映射到人形机器人关节 R^24上。我们定义了一个reward策略R，根据输入的observation O和给定的goal G，将的humanoid goal keypoints和 robot proprioception 输出为proportional-derivative (PD) controller 的目标位置。
% 我们使用了proximal policy gradient（PPO）将累计reward最大化。
We define the problem as training a goal-conditioned RL policy $\pi$ that maps human motion onto the joints of a humanoid robot $K_r\in \mathbb{R}^{24}$. We define a reward strategy $\mathcal{R}$, which takes the observation $\mathcal{O}$ \jc{proprioception} and the given goal $\mathcal{G}$ as input, and outputs the target positions for the proportional-derivative (PD) controller in the action space $\mathcal{A}$. We use the proximal policy gradient (PPO) to maximize the accumulated reward.
