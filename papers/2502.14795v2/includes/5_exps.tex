\section{Experiments}
We evaluate the proposed \textbf{Humanoid-VLA} in terms of its ability to enable universal humanoid control.
We structure the experiments to answer
the following questions:
\textbf{1) RQ1}: Does \textbf{Humanoid-VLA} generate kinematically accurate and physically plausible motions?
\textbf{2) RQ2}: How effective is the humanoid control with vision integration?

\subsection{Evaluation on motion generation}
In this section, we take a comprehensive evaluation of the quality of the pose trajectory generated by the model.
To comprehensively demonstrate the effectiveness of our approach, we access motion quality from two perspectives:
\textbf{1) Kinematic fidelity:}
This metric evaluates the kinematic performance, measuring positional changes without considering physical dynamics.
Following~\cite{mao2024learning}, we evaluate our model on the standard text-to-motion (T2M) task, which generates motion sequences based on textual action descriptions. It highlights the model's core capability of translating natural language into human motion.  

\textbf{2) Physical plausibility:}
Unlike the above metric, this evaluation assesses the physical feasibility of generated poses in real-world environments.
Beyond standard T2M tasks, we evaluate our model on more challenging scenarios that exceed the capabilities of existing models, particularly tasks incorporating diverse input conditions such as joint trajectories. This comprehensive assessment demonstrates the robustness and versatility of the model across a broad spectrum of applications.

\vspace{-4pt}
\subsubsection{Kinematic Fidelity }
\vspace{-4pt}
\textbf{Setup.} 
We evaluate the motion quality using a widely used dataset HumanML3D~\cite{guo2022humanml3d} and our collected dataset Humanoid-S, which contains manually annotated action descriptions for human pose extracted from 4646 video clips.
%
While HumanML3D focuses on basic locomotion patterns such as running, swimming, and dancing, Humanoid-S encompasses more complex human actions.
%
We choose the whole testing dataset and randomly select one textual description per clip to serve as the input for evaluation. 
%
For a fair comparison, we evaluate all models using 15 joints consistent with our model's configuration, selected for their presence in both humans and humanoid robots to enhance generalizability.

\begin{table}
\centering
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{@{}lccccccc@{}}
\toprule
\multirow{2}{*}{Types} &  
\multirow{2}{*}{Input} & 
\multicolumn{4}{c}{Accuracy}
\\ \cmidrule(lr){3-6} 
&
&$E_{\text{mpjpe}}^{g}\downarrow$ 
% &$E_{\text{mpjpe}}^{l}\downarrow$ 
&$E_{\text{mpjpe}}^\text{pa}\downarrow$ 
& $E_{\text{accel}} \downarrow$ 
& $E_{\text{vel}}\downarrow$ 
\\ \midrule
\multirow{4}{*}{Easy}&

{D} &
36.13 & 1.53 & 34.42 & 18.73
\\
&
{T} &
 36.57 & 1.48 & 35.10 & 18.53
\\
&
{A} &
 39.02  & 1.32 & 34.32 & 17.91
\\
& 
 {$S_n$} &
 36.29 & 1.55 & 34.93 & 18.88
\\
\midrule

\multirow{3}{*}{Medium}&
{D + T} &
 {31.07} & {1.18}  & {27.84} & {14.76}
\\
&
 {D + A} &
 36.98& 1.30& 34.87& 18.16
\\
&
 {D + $S_n$} &
 35.75 & {1.18} &33.41 &17.18
\\
\midrule
\multirow{1}{*}{Hard}& 
 {D + $S_1$ + $S_N$} 
 & 37.14 & 1.34 & 34.69 & 18.08
\\
   \bottomrule
\end{tabular}
}
\caption{\textbf{Physical plausibility of generated motion under versatile conditions.} Humanoid-VLA provides four conditional input types: motion description (\textbf{D}), motion time duration (\textbf{T}), motion sequence with absent body parts (\textbf{A}), and motion state (\textbf{$S_n$}) at time n within total N timesteps.
Based on input combinations, we establish three tiers of motion generation tasks with increasing complexity.}
\label{tab:controllable}
\vspace{-1.em}
\end{table}

\begin{table}
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{@{}ccccccc@{}}
\toprule
\multirow{2}{*}{Method} & 
\multicolumn{2}{c}{HumanML3D}& \multicolumn{2}{c}{Humanoid-S}\\
\cmidrule(lr){2-3}
\cmidrule(lr){4-5}  
% & MM Dist$\downarrow$ 
& FID$\downarrow$ 
% & MModality$\uparrow$
& DIV$\uparrow$ 
% & MMDist$\downarrow$ 
& FID$\downarrow$ 
% & MModality$\uparrow$
& DIV$\uparrow$ 
\\ \midrule
MDM&
% $\boldsymbol{3.740}^{\pm.095}$ &
${0.889}^{\pm.026}$ &
${3.855}^{\pm.053}$ &
${2.351}^{\pm.590}$ &
${4.111}^{\pm.261}$ &
\\
T2M-GPT&
% $4.512^{\pm.165}$ &
${0.531}^{\pm.020}$ &
${4.555}^{\pm.058}$ &
${1.101}^{\pm.189}$ &
${4.199}^{\pm.218}$ &
\\\midrule
% 10 epoch微调版本结果
\modelname &
% $4.270^{\pm.206}$ &
$\boldsymbol{0.467}^{\pm.018}$ & 
$\boldsymbol{4.585}^{\pm.086}$ &
$\boldsymbol{1.037}^{\pm.147}$ & 
$\boldsymbol{4.466}^{\pm.213}$ &
\\
 \midrule
\end{tabular}%
}
\caption{
\textbf{Kinematic fidelity of generated motion in HumanML3D and Humanoid-S.} We use FID score and Diversity to evaluate the quality of the motion generated by the model, where bold values indicate the best results.
}
\label{tab:t2m}
\vspace{-1.5em}
\end{table}

\textbf{Metrics.}
We follow the evaluation framework from ~\cite{guo2022fid}, utilizing two established metrics to evaluate the quality of motion generation: \textbf{(1) FID} measuring distribution similarity between generated and real motions, and \textbf{(2) Diversity} quantifying action variation degree, and calculating the average Euclidean distance between 200 randomly generated motions.
Lower FID indicates better distribution matching and higher DIV scores reflect superior diversity.

\textbf{Baselines.}
We consider two baselines commonly used in humanoid control:
{\textbf{(1) MDM}~\cite{tevet2023human}}: A diffusion-based generation model that utilizes a classifier-free paradigm to produce natural and diverse motions.
{\textbf{(2) T2M-GPT}~\cite{zhang2023t2m}}: A transformer-based generation model that combines VQ-VAE\cite{van2017neural} with an autoregressive approach to generate human motions from text.

\textbf{Implementation details.}
We utilize Llama3-70B~\cite{dubey2024llama} as the foundation model. In the training phase, warm up ratio is set at 0.01, with learning rate configured at 2e-5, and the cosine learning scheduler. The batch size per device is set to 4. For the encoder of each body part, its codebook size is set to 1024. We conduct model training using 8 NVIDIA H100 GPUs through {216} hours.


\begin{figure*}[t]
  \centering
   \includegraphics[width=1\linewidth]{imgs/real.pdf}
   \caption{\textbf{Robot experiments in real world}. \modelname demonstrates its ability to interact with objects, showcasing robust performance in real-world environments. The humanoid model successfully executes precise object-kicking tasks and avoids obstacle task in real-world scenarios.}
   \label{fig:real}
   \vspace{-1.5em}
\end{figure*}





\textbf{Results.}
The comparative evaluation results between \modelname and the baseline models are presented in Table~\ref{tab:t2m}. On the HumanML3D dataset, \modelname achieves a significant FID score of 0.467, representing substantial improvements of 47.5\% and 12\% over MDM and T2M-GPT respectively, which indicates its superior capability in capturing real motion distributions. Furthermore, \modelname demonstrates remarkable performance in motion diversity, attaining a diversity score of 4.466 on the Humanoid-S dataset, outperforming MDM by 6\%. This achievement is particularly noteworthy as it reflects the model's ability to generate diverse motions under challenging linguistic constraints. The comprehensive experimental results demonstrate that \modelname excels in high-quality action generation, establishing its effectiveness in text-to-motion synthesis tasks.


\begin{table}
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{@{}cccccccc@{}}
\toprule
& 
Low-quality data &
\multicolumn{1}{c}{High-quality Data} &
\multirow{2}{*}{FID$\downarrow$} &
\multirow{2}{*}{DIV$\uparrow$ }
% \multicolumn{3}{c}{Motion-to-Text}& 
\\ \cmidrule(lr){2-2}\cmidrule(lr){3-3}& w aug & w aug   
% MModality$\uparrow$

% & MMDist$\downarrow$ 
\\ \midrule
&\checkmark & &
${0.698}^{\pm.037}$ & 
${4.576}^{\pm.098}$ &\\
& &\checkmark &
${0.557}^{\pm.016}$ & 
${3.867}^{\pm.062}$ &\\
 & \checkmark&\checkmark &
$\boldsymbol{0.467}^{\pm.018}$ & 
$\boldsymbol{4.585}^{\pm.086}$ &\\
   \bottomrule
\end{tabular}%
}
\caption{\textbf{Ablation on data augmentation.} Here, low-quality data refers to motion data extracted through human motion recovery, which tends to lack precision. In contrast, high-quality data refers to motion data obtained directly from physical devices, ensuring greater accuracy.}
\label{tab:aug}
\vspace{-1.5em}
\end{table}



\subsubsection{Physical Plausibility }
\textbf{Setup.} 
We evaluate this metric in the IsaacGym physics simulator~\cite{makoviychuk2021isaac}.
Following~\cite{he2024omnih2o,ji2024exbody2}, we assess the humanoid's tracking accuracy in executing the model-generated kinematic trajectories to quantify physical plausibility.



\textbf{Metrics.}
Our metrics are designed across two dimensions: (1) \textit{\textbf{State-related.}} The global Mean Per-Joint Position Error (MPJPE) $E_{\text{mpjpe}}^{g}$ (mm) quantifies the average positional error of individual joints. The Procrustes-aligned MPJPE (PA-MPJPE) $E_{\text{mpjpe}}^\text{pa}$ (mm) eliminates global scale and rotational discrepancies to assess shape accuracy specifically. (2) \textit{\textbf{Transition-related.}} We evaluate acceleration error $E_{\text{accel}}$ (mm/s²) and velocity error $E_{\text{vel}}$ (mm/s) metrics to assess physical plausibility by computing the average joint-level distances in acceleration and velocity respectively.
For all metrics, lower values correspond to better performance.



\textbf{Baselines.}
As these conditional motion tasks are uniquely solvable by our model, conventional baselines are not applicable. Therefore, we focus on evaluating our approach independently, adopting the tracking error widely accepted in ~\cite{ji2024exbody2} to evaluate the effectiveness of our method.


\textbf{Results.}
As shown in Table \ref{tab:controllable}, our RL policy achieves robust motion imitation joint control with mean position errors \(E_{\text{mpjpe}}^g\) consistently below 40 mm, and minimum score 31.07mm under medium difficulty with caption and time conditions. The policy achieves remarkably low errors in pose accuracy $E_{\text{mpjpe}}^\text{pa}$ at {1.18mm}, acceleration $E_{\text{accel}}$ at {27.84mm}, and velocity $E_{\text{vel}}$ at {14.76mm}, demonstrating smooth and physically consistent motion generation. This experiment validates our method's ability to generate high-quality motions across diverse control conditions while preserving physical plausibility and control fidelity.


\textbf{Ablation on data augmentation.} 

As shown in Table \ref{tab:aug}, incorporating extensive video motion data reduces the FID from 0.557 to 0.467, representing a 16\% improvement. This significant enhancement demonstrates that large-scale motion data extracted from videos strengthens the alignment between motion and language. 
We can still achieve comparable results Even with low-quality mocap data for fine-tuning. These two points strongly validate the significance of incorporating video data to expand the training process.
These findings underscore the effectiveness of our self-supervised data augmentation strategy. 



\vspace{-10pt}
\subsection{Evaluation on vision integration }

\vspace{-5pt}
\textbf{Experimental setup.}

We evaluate the performance of our \modelname model in real-world environments utilizing visual information. An RGB camera is employed to capture first-person view images, and experiments are conducted using the Unitree G1 robot across four task categories: upper-body motion, lower-body motion, full-body motion, and object interaction. These tasks, such as approaching targets, kicking a ball, and obstacle navigation, require visual guidance for accurate positioning, aiming to validate the effectiveness of our VLA approach.




\textbf{Results.}
For each task category, we carefully select 4 representative tasks and evaluate 10 tests on each task, using success rate as the evaluation metric.
Our humanoid-VLA model shows great performance in various object interaction tasks shown in Table ~\ref{tab:vis_SR}.
Selected tasks are shown in Figure~\ref{fig:real}. 
In the "kick ball" task, our humanoid model enables the robot to effectively utilize visual information to accurately approach the object and execute a kicking motion. 
In the "avoid obstacles" task, our robot successfully navigates around obstacles to reach the desired target position. 
These results demonstrate that our VLA model effectively leverages visual information to generate appropriate motions.





\begin{table}[ht]
  \centering
  \resizebox{0.45\linewidth}{!}{
  \begin{tabular}{@{}c|cc@{}}
    \toprule
    \textbf{Task} & \textbf{\makecell[c]{SR}} \\
    \midrule
 Turn to an object & 10/10 \\
 Hold an object  & 9/10 \\

 Wave to people & 10/10 \\


Avoid an obstacle  & 9/10 \\  
Jump over an object & 9/10 \\  
Dance with a partner  & 8/10 \\ 
 Punch an obstacle & 10/10 \\ 
 Kick a ball  & 9/10 \\


    \bottomrule
  \end{tabular}
  }
  \caption{\textbf{Evaluation on Vision Integration.} }
  \label{tab:vis_SR}
  \vspace{-2em}
\end{table}











