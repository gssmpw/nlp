\documentclass{article}

\title{Systems and Algorithms for Convolutional\\ Multi-Hybrid Language Models at Scale}

\author{%
  Jerome Ku$^{2,*}$, 
  Eric Nguyen$^{1,*}$, 
  David W. Romero$^{3,*}$, 
  Garyk Brixi$^{1}$, 
  Brandon Yang$^{5}$, \\
  Anton Vorontsov$^{3}$, 
  Ali Taghibakhshi$^{3}$, 
  Amy X. Lu$^{4}$, 
  Dave P. Burke$^{2}$, \\
  Greg Brockman$^{5,\dagger}$, 
  Stefano Massaroli$^{7, 8}$, 
  Christopher RÃ©$^{1}$, 
  Patrick D. Hsu$^{2,4}$, \\
  Brian L. Hie$^{1,2}$, 
  Stefano Ermon$^{1}$, 
  Michael Poli$^{1,7,\ddagger}$
}
\date{\small
  $^1$Stanford University,
  $^2$Arc Institute,
  $^3$NVIDIA, \\
  $^4$University of California, Berkeley,
  $^5$Independent Researcher,
  $^7$Liquid AI,
  $^8$RIKEN
}
\input{template/new_style}
\input{utils/math}
\begin{document}

\maketitle

\begin{abstract}
    %
    \noindent
    We introduce\let\thefootnote\relax\footnotetext{\hspace{-0.6cm}$^*$ These authors contributed equally to this work. \\$^\dagger$ Current address: OpenAI. \\$^{\ddagger}$Corresponding author: poli@stanford.edu.} \textit{convolutional multi-hybrid} architectures, with a design grounded on two simple observations. First, operators in hybrid models can be tailored to token manipulation tasks such as in-context recall, multi-token recall, and compression, with input-dependent convolutions and attention offering complementary performance. Second, co-designing convolution operators and hardware-aware algorithms enables efficiency gains in regimes where previous alternative architectures struggle to surpass Transformers. At the 40 billion parameter scale, we train end-to-end 1.2 to 2.9 times faster than optimized Transformers, and 1.1 to 1.4 times faster than previous generation hybrids. On H100 GPUs and model width 4096, individual operators in the proposed multi-hybrid StripedHyena 2 architecture achieve two-fold throughput improvement over linear attention and state-space models. Multi-hybrids excel at sequence modeling over byte-tokenized data, as demonstrated by the Evo 2 line of models. We discuss the foundations that enable these results, including architecture design, overlap-add blocked kernels for tensor cores, and dedicated all-to-all and point-to-point context parallelism strategies.  
\end{abstract}
%

\input{sections/1_introduction}
\input{sections/architecture.tex}
\input{sections/kernel_optimizations.tex}
\input{sections/context_parallelism.tex}
\input{sections/discussion.tex}

\bibliography{main}
\bibliographystyle{style}

\newpage
\clearpage

\appendix

\rule[0pt]{\columnwidth}{1pt}
\begin{center}
    \Large{Systems and Algorithms for Convolutional \\ Multi-Hybrid Language Models at Scale} \\
    \vspace{0.15cm}
    \emph{Supplementary Material}
\end{center}
\rule[0pt]{\columnwidth}{1.2pt}

\doparttoc
\tableofcontents

\input{sections/appendix/A_details}
\input{sections/appendix/B_figures}
\input{sections/appendix/C_protocol}
%\input{sections/appendix}

\end{document}


