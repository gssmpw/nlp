\documentclass[10pt]{article}
\usepackage{tmlr} 
\usepackage[accepted,preprint]{style}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{authblk}
\usepackage{algorithm}
\usepackage{algorithmic}
%
\input{template/style}
\input{utils/math}

\title{ML Plan}
\author{}
\date{}


\begin{document}
%


This document summarizes our plan for technical writing and pushing on the final features and tasks needed for release.
%

Alongside the {\tt Evo2} line of models and the paper, we are releasing an open-source version of our training and inference stacks, {\tt savanna} and {\tt vortex}. 

\section{Technical Writing for The Paper}
%

\subsection{Main text}

\paragraph{Model description}\textbf{[Michael]} Usual high-level description of the model present in general pretraining technical reports, similar to the Evo1 paper. Model information, token counts, pretraining stages, basic parallelism choices. FLOP counts and rough comparison to other foundation models. High-level motivation 
behind architecture design. 

This time we could put more emphasis on the inference piece compared to Evo1, given its increasing importance in our narrative. High-level description of how we achieve fast inference (connect to Brian's test-time guided sampling). \textbf{Stretch goal:} decode tok/s with SH2 + speculative decoding, compared to SH.

\paragraph{Panels for Fig 1:}TODO:
\begin{itemize}
    \item Throughputs 7B and 40B, scaling context length (see candidate figure on slack)
\end{itemize}


\subsection{Methods}

\paragraph{Work log}\textbf{[Everyone]} 
\begin{itemize}
    \item Pretraining, midtraining, post-training stages. Two stages of pretraining 1k -> 8k. "Midtraining" / extension stages. Finetune recipes. Evaluation loop to test extension.
    \item Exact pretraining configurations. Motivation behind sharding strategy.
    \item Ablations: extension methods (hybrid, ABF, PI), model architecture
    \item Datasets and token counts. Used in which version of the model (table)
    \item Hardware / cluster description, monitoring (if ok with NVIDIA)
    \item Short description of issues found during training: activation, projection interleaving, and how we addressed them
\end{itemize}

%

\paragraph{Algorithms: architecture}\textbf{[Michael]}

Architecture description: multihybrid, specifics of convolution parametrization for each hyena variant

\paragraph{Algorithms: context parallelism}\textbf{[Michael, David, Brandon]}  

We developed custom sharding infrastructure to scale to effectively scale to 1M context.

Algorithmic description + diagram for context parallel for StripedHyena2 operators. Naive a2a (long), full p2p (short). \textbf{Stretch goal:} pipelined a2a for long hyena, tested as proof of concept of scaling outside training. Profiling and isolated throughput table. 

High-level informal  description of sequence and context parallelism implementation in savanna, outside of the new operators

\paragraph{Algorithms: kernels}\textbf{[Michael, Jerome]}  

We worked on some custom kernels for the architecture.

Algorithmic description of cgcg kernel and implementation for short hyena. Impact on prefilling speed in vortex 

\paragraph{Algorithms: efficient inference}\textbf{[Michael, Amy]}  
%

How caching with new operators (maybe simple schematic), \textbf{stretch goal:} nucleotide-level specdec in vortex, measurements

\section{Features and Software Release}
%


\paragraph{{\tt savanna v1.0.0}}
We aim to polish a release version of savanna that could serve as the basis for (a) reproduction of Evo2 at smaller scale and (b) research work on additional features. The release version does not need to be at feature parity with all branches of savanna dev.

\begin{itemize}
    \item 01/11 \textbf{[Michael]} README, style changes, general cleanup  
    \item 01/12 \textbf{[Michael]} Documentation
    \item 01/13 \textbf{[Michael]} Design quickstart test configs
\end{itemize}
%

\paragraph{{\tt vortex v1.0.0}} 

\begin{itemize}
    \item 01/13 \textbf{[Michael]} Finalize packaging of vortex (easier installation), miscellaneous refactor chores
    \item 01/13 \textbf{[Michael]} Improve documentation
    \item 01/13 \textbf{[Michael]} Integration of prefill kernel
    \item 01/15 \textbf{[Michael]} Support larger batch sampling 
    \item 01/15 \textbf{[Michael]} Support longer prefills
    \item 01/13 \textbf{[Amy]} Speculative decode PR open, no draft model (sampling completions)
    \subitem Michael to test implementation and speedups
    
\end{itemize}


\paragraph{Additional experiments and misc}
\begin{itemize}
    \item 01/12 \textbf{[Michael, Jerome]} Launch configs
    \item 01/18 \textbf{[Jerome]} Collect throughputs with Transformer, SH1, SH2, for Fig 1 
    \subitem Idea: x-axis sequence length, y-axis throughput, display as bar chart or line plot
    \item By release \textbf{[Everyone]} Model checkpoint release: prepare list of checkpoints and strategy for hosting (HF, dedicated...). Examples and notebook?
\end{itemize}

\section{Writing}

Writing drafts can go here. Everyone has access to this overleaf.

\paragraph{Model description}

\subsection{Context Parallelism}
%
\paragraph{Contents}
%

%
\begin{itemize}
    \item Context parallel algorithm tailored to each subtype hyena layers in StripedHyena2, since the filter length changes. Since cache size of convolutional variants is proportional to filter length, communication overheads will vary
    \item  Generally, we wish to minimize communication overhead, we adopt two main strategies (parallel to context parallelism in self-attention) one based on p2p primitives and one based on a2a. 
    \item Description of p2p (different data layouts). Diagram 
    \item Description and limitations a2a naive (different data layouts), can be addressed with overlapping. Diagram
    \item Diagram showing where each type of communication algorithm was used inside each of the three building blocks of StripedHyena2
    \item Extension 1: p2p FFT. Diagram
    \item Extension 2: pipelined a2a. Diagram
    \item Profiling looks different depending on operator size / number of "batched" computations. Isolated latency plots (x-axis sequence length). Maybe do 7B   
\end{itemize}

Diagram inspirations:
\begin{itemize}
    \item https://arxiv.org/pdf/2408.16978
    \item https://arxiv.org/pdf/2309.14509
    \item https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method 
\end{itemize}

\paragraph{Story}
\begin{itemize}
    \item Start with an overview of what context parallelism is and why we need it
    \item Existing approaches with attention, and high-level on what we use for the attention in SH2
    \item p2p and a2a descriptions  
\end{itemize}
\end{document}


