% \section{Details}

% This section contains information on pretraining and midtraining stages, configurations, and other details specific to Evo 2.

% \begin{center}
%     \begin{table}[H]
%         \centering
%         \begin{tabular}{lrrr}
%         \toprule
%         \textbf{\textsf{Model}} & \textbf{\textsf{Model Size}} & \textbf{\textsf{Pretrain Tokens}} & \textbf{\textsf{FLOPS est.}} \\
%         \midrule
%         \addlinespace[0.5em]
%         \multicolumn{4}{l}{\textbf{Fully open: data, infrastructure, weights}} \\
%         \cmidrule(lr){1-4}
%         Evo 2 40B & 40.3B & 9.3T & $2.25 e^{24}$ \\
%         Pythia Large & 12B & 300B & $2.16e^{22}$ \\
%         OLMo 2 Large & 13B & 5.6T & $4.37e^{23}$ \\
%         \addlinespace[1.5em]
%         \multicolumn{4}{l}{\textbf{Open data, weights}} \\ 
%         \cmidrule(lr){1-4}
%         Evo 1 & 7B & 300B & $1.26e^{22}$ \\
%         Falcon 180B & 180B & 3.5T & $3.78e^{24}$ \\
%         StarCoder 2 15B & 15B & 4.3T & $3.87e^{23}$ \\
%         \addlinespace[1.5em]
%         \multicolumn{4}{l}{\textbf{Open weights only}} \\
%         \cmidrule(lr){1-4}
%         DeepSeek V3 & 37B (671B total) & 14.8T & $3.28e^{24}$ \\
%         Llama 3.1 Large & 405B & 15T & $3.64e^{25}$ \\
%         \addlinespace[1.5em]
%         \multicolumn{4}{l}{\textbf{Closed}} \\
%         \cmidrule(lr){1-4}
%         ESM 3 Large & 98B & 771B & $1.07 e^{24}$ \\
%         xTrimo Large & 100B & 1T & $6.00 e^{23}$ \\
%         \bottomrule
%         \end{tabular}
%         \caption{Comparison of model statistics across selected language and biology models. *Values are approximated (official numbers unavailable; computed from available data). B = billion ($10^9$), T = trillion ($10^{12}$).}
%     \end{table}
% \end{center}

% \begin{center}
%     \begin{table}[H]
%         \centering
%         \begin{tabular}{lrrr}
%         \toprule
%         \rowcolor{blue!10}\textbf{\textsf{Model}} & \textbf{\textsf{Model Size}} & \textbf{\textsf{Pretrain Tokens}} & \textbf{\textsf{FLOPS est.}} \\
%         \midrule
%         \addlinespace[0.5em]
%         \multicolumn{4}{l}{\textbf{Fully open: data, infrastructure, weights}} \\
%         \cmidrule(lr){1-4}
%         Evo 2 40B & 40.3B & 9.3T & $2.25 e^{24}$ \\
%         Pythia Large & 12B & 300B & $2.16e^{22}$ \\
%         OLMo 2 Large & 13B & 5.6T & $4.37e^{23}$ \\
%         \addlinespace[1.5em]
%         \multicolumn{4}{l}{\textbf{Open data, weights}} \\ 
%         \cmidrule(lr){1-4}
%         Evo 1 & 7B & 300B & $1.26e^{22}$ \\
%         Falcon 180B & 180B & 3.5T & $3.78e^{24}$ \\
%         StarCoder 2 15B & 15B & 4.3T & $3.87e^{23}$ \\
%         \addlinespace[1.5em]
%         \multicolumn{4}{l}{\textbf{Open weights only}} \\
%         \cmidrule(lr){1-4}
%         DeepSeek V3 & 37B (671B total) & 14.8T & $3.28e^{24}$ \\
%         Llama 3.1 Large & 405B & 15T & $3.64e^{25}$ \\
%         \addlinespace[1.5em]
%         \multicolumn{4}{l}{\textbf{Closed}} \\
%         \cmidrule(lr){1-4}
%         ESM 3 Large & 98B & 771B & $1.07 e^{24}$ \\
%         xTrimo Large & 100B & 1T & $6.00 e^{23}$ \\
%         \bottomrule
%         \end{tabular}
%         \caption{Comparison of model statistics across selected language and biology models. *Values are approximated (official numbers unavailable; computed from available data). B = billion ($10^9$), T = trillion ($10^{12}$).}
%     \end{table}
% \end{center}
% \subsection{Distributed Strategy}

% \begin{itemize}
%     \item We use a 3D mesh: data parallel, tensor parallel, context parallel.
%     \item We shard X (ZeRO-3)
%     \item Our training infrastrustructre is built on Megatron-DeepSpeed, GPT-NeoX and Transformer Engine. We implemented new context parallel algorithms for StripedHyena 2, detailed in Section X.
%     \item We train in mixed precision, with fp8 on all dense GEMMs and fp8 on the RMSNorm (Transformer Engine). We generally find no quality degradation (see Table X).
% \end{itemize}

% \subsection{Evo 2 Training and Loss}

% Evo 2 is trained auto-regressively on a byte-tokenized dataset of nearly 9T DNA nucleotide tokens, comprising of DNA sequences from eukaryote, prokaryote, archaea, organelle, and phage genomes.

% We train Evo 2 in two phases: a pretraining phase at 8192 token context focused more on functional elements and midtraining phase during which we extend up to 1M token context length with more entire genomes in the data mix. Evo 2 40B's pretraining stage is further split into two stages, first training at 1024 context for 6.6T tokens before extending to 8192 context for 1.1T tokens.

% The model is trained with a reweighted cross entropy loss, which weighs the loss contribution of repetitive portions of DNA by 0.1. This affects the genomic window and whole genome portions of the data which contain these annotations. This loss has been found in other DNA models to improve performance on downstream tasks and better calibrate likelihoods between repetitive and non-repetitive DNA, which we found to be true for downstream tasks in small scale experiments\ref{tab:loss_downweighting}
% \[
% \begin{aligned}
%     \ell_{\tt CE} &= y_t \log(p_t) - (1-y_t)\log(1-p_t) \\ 
%     \ell_{\tt wCE} &= \frac{1}{Z} \sum_t w_t \ell_{\tt CE}
% \end{aligned}
% \]
% %
% with weighting 
% \[ 
% \begin{aligned}
%     w_t &= \begin{cases}
%         0.1 & \text{if position }t\text{ is in repetitive region} \\
%         1.0 & \text{otherwise}
%         \end{cases} \\ 
%     Z &= 0.1 * {\rm num}_{\rm repeat} + {\rm num}_{\rm non\_repeat} \\ 
% \end{aligned}
% \]
% where:
% $w_t$ is the weight applied to each position, ${\rm num}_{\rm repeat}$ represents the number of positions in repetitive regions within a batch and ${\rm num}_{\rm non\_repeat}$ is the number of non repetitive regions, and $Z$ is the normalization factor that ensures consistent loss scaling regardless of the proportion of repetitive regions.

% For any base pair, the model is tasked with predicting the uppercase character. For the first 3T tokens of pretraining, lowercase tokens are input to the model to add information on which portions of DNA are repetitive. For any additional pretraining and for midtraining, all inputs to the model are uppercase. This is done to further help learn different representations for interspersed repeats, which are very common in many eukaryotic genomes.

% Model size information and hyperparameters used for pretraining Evo 2 models are shown in Table \ref{tab:model-architecture}. Each Evo 2 model uses a repeating pattern of operator types, see figure \ref{sec:architecture}, with the number of repetitions scaling with model size. The \textsf{Hyena-SE} uses filters of length 7 in the inner convolution, \textsf{Hyena-MR} uses length 128.

% \begin{table}[H]
% \centering
% \begin{tabular}{l|ccc}
% \toprule
% & Evo 2 40B & Evo 2 7B & Evo 2 1B base \\
% \midrule
% Parameters & 40.3B & 6.5B & 1.1B \\
% Total Layers & 50 & 32 & 25 \\
% Hidden Size & 8,192 & 4,096 & 1,920 \\
% Num Heads & 64 & 32 & 15 \\
% \midrule
% Total Tokens & 9.3T & 2.4T & 1T \\
% \bottomrule
% \end{tabular}
% \caption{Model architecture configurations for Evo 2 models}
% \label{tab:model-architecture}
% \end{table}

% % \begin{table}[H]
% % \centering
% % \begin{tabular}{l|ccccc}
% % \toprule
% % & Evo 2 40B & Evo 2 7B & Evo 2 1B base & Evo 2 100M base & Evo 2 50M base \\
% % \midrule
% % Parameters & 40.3B & 6.5B & 1.1B & 107M & 54M \\
% % Total Layers & 50 & 32 & 25 & 15 & 11 \\
% % Hidden Size & 8,192 & 4,096 & 1,920 & 768 & 640 \\
% % Num Heads & 64 & 32 & 15 & 12 & 10 \\
% % \midrule
% % Total Tokens & 9.3T & 2.4T & 1T & 1T & 1T \\
% % \bottomrule
% % \end{tabular}
% % \caption{Model architecture configurations for the Evo 2 model family. Each model uses a repeating pattern of layer types, with the number of repetitions scaling with model size. The Hyena Short Conv uses length 7, Hyena Medium Conv uses length 128.}
% % \label{tab:model-architecture}
% % \end{table}

% \subsection{Pretraining Stage}

% \begin{table}[H]
% \centering
% \begin{tabular}{l|ccc}
% \toprule
% & Evo 2 40B base & Evo 2 7B base & Evo 2 1B base \\
% \midrule
% Learning Rate & 2.0e-4 & 3.0e-4 & 1.5e-4 \\
% Training Batch & 16M & 4M & 2M \\
% Total Iterations & 516K & 500K & 490K \\
% Pretraining Tokens & 8.7T & 2.1T & 1T \\
% Sequence length & 1024 (6.6T), 8192 (1.1T) & 8192 & 8192 \\
% \bottomrule
% \end{tabular}
% \caption{Pretraining hyperparameters for Evo 2 models. Each model uses the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.95$, and cosine learning rate decay.}
% \label{tab:model-training}
% \end{table}

% % \begin{table}[H]
% % \centering
% % \begin{tabular}{l|ccccc}
% % \toprule
% % & Evo 2 40B base & Evo 2 7B base & Evo 2 1B base & Evo 2 100M base & Evo 2 50M base \\
% % \midrule
% % Learning Rate & 2.0e-4 & 3.0e-4 & 1.5e-4 & 3.0e-4 & 3.0e-4 \\
% % Training Batch & 16M & 4M & 2M & 2M & 2M \\
% % Total Iterations & 516K & 500K & 490K & 500K & 450K \\
% % Pretraining Tokens & 8T & 2.1T & 1T & 1T & 0.9T \\
% % \bottomrule
% % \end{tabular}
% % \caption{Pretraining hyperparameters for Evo 2 models. Each model uses the AdamW optimizer with $\beta_1=0.9$, $\beta_2=0.95$, and cosine learning rate decay.}
% % \label{tab:model-training}
% % \end{table}

% \subsection{Midtraining Stages}

% \paragraph{Context extension}

% \begin{itemize}
%     \item We follow a multi-stage midtraining procedure, gradually extending the context length. 
%     \item We explore three different methods to adapt rotary attention embeddings to longer sequences. At each stage, we validate on downstream benchmarks and custom long-context recall benchmark designed for the new modality.
%     \item We generally find that hybrids and multihybrid deep signal processing models can be successfully extended with similar procedures to dense Transformer models, with consistent improvements in performance (perplexity lowering with scale and input context length).
% \end{itemize}

% After each extension stage, model performance was evaluated using loss, loss on short sequence data, performance on short sequence downstream tasks, and performance on the long context needle in haystack evaluation. We did not find significant differences between different extension protocols.

% We developed a long-context recall eval inspired by needle in a haystack. A 100 token long DNA sequence needle is placed into a random DNA sequence at fixed intervals. An identical 100 token long query is added at the end of the DNA sequence. We then perturb the needle and measure the predicted delta on the predicted sequence logits of the query using a sparse categorical jacobian to assess the models ability to recall the needle and change it's predictions based on single token changes to the needle sequence (method in main paper).
