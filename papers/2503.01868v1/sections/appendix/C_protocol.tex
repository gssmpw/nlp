
\section{Appendix: Methods}


\begin{table}[H]
    \centering
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \begin{tabular}{ll}
        \toprule
        \rowcolor{blue!10} \textbf{\textsf{Setting}} & \textbf{\textsf{Value}} \\
        \midrule
        \textsf{Tensor Parallel}  & 2, 2, 8, 8, 16, 16, 32 \bigstrut \\
        \textsf{Sequence Parallel} & True \\
        \textsf{Context Parallel} & 1, 1, 1, 1, 1, 2, 2 \bigstrut \\
        \textsf{Global Batch Size} & 4M tokens \bigstrut \\
        \textsf{Hardware} & H100 SXM \bigstrut \\
        \textsf{GPU Count} & 256 \bigstrut \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \begin{tabular}{ll}
        \toprule
        \rowcolor{blue!10} \textbf{\textsf{Setting}} & \textbf{\textsf{Value}} \\
        \midrule
        \textsf{Tensor Parallel} & 8, 8, 8, 8, 16, 32, 64 \bigstrut \\
        \textsf{Sequence Parallel} & True \bigstrut \\
        \textsf{Context Parallel} & 1, 1, 1, 2, 2, 2, 2 \bigstrut \\
        \textsf{Global Batch Size} & 8M tokens \bigstrut \\
        \textsf{Hardware} & H100 SXM \bigstrut \\
        \textsf{GPU Count} & 2048 \bigstrut \\
        \bottomrule
    \end{tabular}
    \end{minipage}
    \caption{\textbf{[Left]:} Baseline distributed configuration used for $7$B parameter model measurements in Figure \ref{fig:all_scaling} at 16K, $33$K, $65$K, $131$K, $262$K, $524$K, $1$M sequence length. 
    \textbf{[Right]:} Baseline distributed training configuration used for $40$B parameter model measurements in Figure \ref{fig:all_scaling} at $16$K, $33$K, $65$K, $131$K, $262$K, $524$K, $1$M sequence length. We also verify scaling on $2048$ with the same settings, doubling batch size $16$M. We observe similar throughput multipliers even on $2048$.}
    \label{tab:appendix_scaling}
\end{table}


\subsection{Pretraining Experiments}\label{sup-sec:grouping}

\paragraph{Methodology}

For all training experiments, we use Savanna. Configuration files are available in the repository, at the following frozen commit hash: \href{github.com/Zymrael/savanna/commit/5f9fdbf1bff7c3ff6e225d46d32d4d7eb97fead9}{{\tt 5f9fdb}}. We train on the {\tt OpenGenome 2} dataset \citep{brixievo2}. For throughput measurements, we report the settings in dedicated tables (Table \ref{tab:appendix_scaling}). We use critical batch size estimation \citep{mccandlish2018empirical} to determine a batch size for the 7 billion parameter runs. We train in mixed precision using FP8 for dense layers and norms.

\paragraph{Effect of grouped convolution}
 
We train 7 billion parameter StripedHyena 2 \textsf{SE-MR-LI} models on 400 billion tokens {\tt OpenGenome2}, with group sizes $1$ (baseline) and $16$, and observe no significant difference in convergence. We also explore the effect of filter grouping in Hyena on smaller models, including simpler hybrids using only \textsf{Hyena-SE} and \textsf{Hyena-MR}. Group sizes larger than $64$ introduce minimal degradation in quality, more visible in smaller models. A smaller number of independent filters also reduces the granularity of the regularization in \textsf{Hyena-MR}, initialized to span different values across filters. The configuration files to replicate the experiments are available in Savanna: \href{https://github.com/Zymrael/savanna/tree/main/configs/7b-final-test/model_configs/group}{{\tt configs/7b-final-test/}}.

\paragraph{Context extension}

Full configuration files are available in \href{https://github.com/Zymrael/savanna/tree/main/configs/context-scaling}{{\tt configs/context-scaling}}. We evaluate recall during midtraining context extension with the needle-in-a-haystack task described in \citep{brixievo2}. Results are shown in Figure \ref{fig:niah_extension}.

\paragraph{Replacing feed-forward-layers with convolutions}

In early designs, we also trained variants of the architecture where every feed-forward layer (MLP, SwiGLU) following every hyena or MHA operator had been replaced with \textsf{Hyena-SE}. We observe generally improved convergence for these models with a small decrease in throughput. Given these findings, coupled with the higher throughput of \textsf{Hyena-SE} compared to MHA or state-space models, we expect future multi-hybrid designs to also optimize the ratio of MLPs and \textsf{Hyena-SE} (or their MoE variants). Configuration files are available at: \href{https://github.com/Zymrael/savanna/tree/main/configs/model/evo2/ablations}{{\tt configs/model/evo2/ablations}}.
% \newpage

\subsection{Profiling}

\paragraph{Operator profiling}

All operators use their official kernels\footnote{We use the mLSTM kernels developed for \href{https://huggingface.co/NX-AI/xLSTM-7b}{xLSTM-7B}.}. For DeltaNet, we report the latency of the kernel provided in the Flash Linear Attention \citep{yang2024fla} repository. For Mamba2 \citep{dao2024transformers}, we use the official kernels provided by the authors.

Figure \ref{fig:operator_profiles} and \ref{fig:tflop-app} shows the results, with \textsf{Hyena-SE} and \textsf{Hyena-MR} providing graceful scaling to longer sequence lengths, with high throughput even at batch size 1. Figure \ref{fig:hyenamr_variants} shows a direct comparison of latencies and TFLOPS / second for the \textsf{Hyena-MR} with different underlying implementations. Direct convolutions using the proposed two-stage approach generally outperform PyTorch convolutions.

% \subsection{Repeat Down Weighting}

% We conducted experiments to assess the impact of repeat loss reweighting on downstream performance using two Striped Hyena 1 models (550M parameters, 8192 sequence length). Both models were trained on data mix v0, which was enriched for complete eukaryotic genomes to better evaluate the effects of repeat downweighting. The models differed only in their treatment of repetitive sequences: one applied a loss reweighting factor of 0.1 to lowercase sequences, while the other used no reweighting.

% Performance was evaluated using the ClinVar pathogenic versus benign classification benchmark. The model without loss reweighting achieved an AUROC of 0.63 at 40,000 training steps, compared to 0.73 for the model with reweighting. Training of the non-reweighted model was discontinued after we observed no improvement in ClinVar performance beyond 30,000 steps, while the reweighted model continued to show improvements until the experiment concluded at 100,000 steps. Based on these results, we adopted a loss reweighting factor of 0.1 for repetitive sequences in subsequent experiments.

% \begin{table}[h]
% \centering
% \caption{ClinVar AUROC at 40,000 Steps}
% \begin{tabular}{ll}
% \hline
% \textbf{Model} & \textbf{AUROC} \\
% \hline
% No reweighting & 0.63 \\
% Repeat Reweighting 0.1 & 0.73 \\
% \hline
% \end{tabular}
% \label{tab:loss_downweighting}
% \end{table}

% \subsection{Data Composition}

% To investigate the effect of data composition on downstream performance, we compare Evo 2 7B base with a 7B parameter StripedHyena 2 model trained on a ablation data composition \ref{tab:dataset_comparison} at 8192 context length. The ablation dataset did include the mRNA the same weight as the final pretraining dataset, but used whole genomes instead of the Eukaryotic genomic windows and augmented mRNA. We trained for 1.9T tokens until the loss plateaued.

% Evaluation on the BRCA1 mutagenesis dataset demonstrated superior performance for the final pretraining data composition compared to the data ablation, highlighting the benefits of focusing pretraining on functionally enriched regions around genes.  Zero-shot AUROC improves for BRCA1 variants from 0.793 to 0.891. We find the performance increases most for overall and for noncoding variants, with smaller improvements on coding. While the data ablation had more weight to noncoding regions, these high effect noncoding variants near the BRCA1 gene are better modeled by the Evo 2 7B base model which was pretrained on datasets focused around genic windows, and learned to better calibrate these effects with coding effects.

% \begin{table}[h]
% \centering
% \caption{BRCA1 Zero-shot Comparison Between Models}
% \begin{tabular}{llrrr}
% \hline
% \textbf{Metric} & \textbf{Region} & \textbf{7B Data Ablation} & \textbf{Evo 2 7B base} & \textbf{Difference} \\
% \hline
% Spearman r & Overall & 0.358 & 0.513 & +0.155 \\
% AUROC & Overall & 0.793 & 0.891 & +0.098 \\
% AUPRC & Overall & 0.543 & 0.713 & +0.170 \\
% AUROC & Coding & 0.769 & 0.797 & +0.028 \\
% AUPRC & Coding & 0.522 & 0.534 & +0.012 \\
% AUROC & Non-coding & 0.867 & 0.962 & +0.095 \\
% AUPRC & Non-coding & 0.658 & 0.869 & +0.211 \\
% \hline
% \end{tabular}
% \end{table}

% At the same time, Evo 2 7B base performs the same or within 0.01 at DART-eval zero shot evaluations of performance on identifying noncoding regions, showing that more weight on whole genomes suggesting it did not help even with zero-shot tasks on distal regulatory regions. More analysis is needed to draw conclusions on enhancer specific tasks.

% \begin{table}[h]
% \centering
% \begin{tabular}{l|rrr}
% \hline
% Dataset & Pretraining (\%) & Midtraining (\%) & Ablation (\%) \\
% \hline
% GTDB + IMG/PR & 18 & 24 & 9 \\
% Metagenomics (MGD DB) & 24 & 5.00 & 15 \\
% IMG/VR & 3 & 2 & 1.7 \\
% ncRNA & 1.99 & 1.00 & 2 \\
% Euk Promoters & 0.02 & 0.01 & 0.02 \\
% Organelles & 0.5 & 0.25 & 0 \\
% Euk mRNAs & 9 & 4.50 & 9 \\
% Euk mRNA augmented & 9 & 4.5 & 0 \\
% Euk 5kb windows & 35 & 5 & 0 \\
% NCBI: Animalia & 0 & 36 & 45 \\
% NCBI: Plantae & 0 & 12 & 8.6 \\
% NCBI: Fungi & 0 & 4 & 8 \\
% NCBI: Protista & 0 & 0.8 & 0.8 \\
% NCBI: Chromista & 0 & 0.8 & 0.6 \\
% hg38 & 0 & 0 & 0.30 \\
% \hline
% \end{tabular}
% \caption{Pretraining and midtraining data compositions compared with ablation data}
% \label{tab:dataset_comparison}
% \end{table}