\section{Conclusion}

In this paper, we introduce systems and algorithms for convolutional multi-hybrids, a new class of architecture for sequence modeling at scale. We discuss architecture design, block layout, kernels for fast convolutions on GPUs based on overlap-add schemes, and context parallelism strategies. Multi-hybrids excel at efficient modeling of byte and character-level data, and we expect their utilization to unlock new applications for foundation models. Effectiveness of StripedHyena 2 is verified at scale (40 billion parameter, over 9 trillion tokens and 1 million context) with the Evo 2 line of models.

\section*{Acknowledgements}

We thank Armin W. Thomas and Keshigeyan Chandrasegaran for helpful discussions and feedback. 
% \subsection{Autoregressive Inference}

% If time permits, we can discuss the following: cache structure of hyena layers, prefill variants, blurb on simple decode, speculative decoding.

% \paragraph{Prefill}

% \begin{itemize}
%     \item Prefilling for {\tt Hyena-SE} and {\tt Hyena-MR} is simple. We can use the same kernels
%     \item {\tt Hyena-LI} is compatible with linear scans, parallel scans, or FFT-based prefills 
%     \item Our implementation uses a custom FFT prefill
%     \item If time permits: talk about FFT prefill kernel
% \end{itemize}

% \paragraph{Decode}

% \begin{itemize}
%     \item Visualization of decode and cache management
% \end{itemize}