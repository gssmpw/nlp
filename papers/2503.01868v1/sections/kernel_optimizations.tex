\section{Hardware-Aware Convolution Algorithms}\label{sec:kernel_optimizations}
%
Discrete convolutions are mathematically equivalent to matrix multiplications with Toeplitz matrices\footnote{Algorithms for fast convolutions correspond to fast matrix multiplications schemes for Toeplitz matrices.}. Convolutional operators in StripedHyena 2 operate in computational regimes that are quite different from traditional convolutional neural networks (CNNs), requiring custom algorithms:
\begin{itemize}
    \item {\tt Hyena-SE}: \textit{short-explicit} hyena layers are based on a combination of grouped depthwise convolutions with explicitly parametrized shorter filters (e.g., length 7).
    \item {\tt Hyena-MR}: \textit{medium-regularized} hyena layers are based on a combination of grouped depthwise convolutions with longer filters (e.g., length 128), obtained by applying a regularization term to the filter weights.
    \item {\tt Hyena-LI}: \textit{long-implicit} hyena layers are based on a combination of grouped depthwise convolutions with longer \textit{implicit} filters (as long as the sequence length) \citep{romero2021ckconv,poli2023hyena}, in the same family as other long convolutions.
\end{itemize} 
%
Libraries such as PyTorch provide optimized implementations of short explicit convolutions via a variety of backends due to their common utilization in CNN architectures. These implementations span approaches such as im2col and Winograd algorithms \citep{chetlur2014cudnn,vasudevan2017parallel}. However, existing implementations are not fully optimized for the convolution operators used in multi-hybrids, which rely mostly on depthwise convolutions of different lengths. When the convolution filter is very long, most algorithms rely on FFT-based methods. These approaches are known to suffer from lower hardware utilization, despite modifications to leverage tensor cores \citep{li2021tcfft,fu2023flashfftconv}. Instead of im2col or Winograd GEMM algorithms for convolutions, we focus on a direct multi-pass blocked approach that is co-designed to exploit filter grouping in Hyena.

% We note that one could opt to accelerate convolutional multi-hybrids primitives with other classes of algorithms e.g., modified Winograd {\color{red}CIT} to run on tensor cores, instead of a chunked algorithm for direct convolution.

% 

\subsection{Block Convolution}
\label{sec:block_conv_background}
For a causal FIR filter of length $\ell_h$ applied to an input signal $x$ of length $\ell$ (with typical $\ell \gg \ell_h$), the output at index $t$ is
%
\begin{equation}\label{eq:fir_def}
    y_t = \sum_{k=0}^{t} h_{t-k}\,x_k, \quad \text{where } h_k=0 \text{ for } k<0 \text{ or } k\ge \ell_h.
\end{equation}
%
This can be written in matrix form:
%
\begin{equation}\label{eq:toeplitz_matrix}
    \begin{bmatrix}
        y_0 \\
        y_1 \\ 
        y_2 \\
        \vdots \\
        y_{\ell-1}
    \end{bmatrix}
    =
    \underbrace{
        \begin{bmatrix}
            h_0   & 0     & 0     & 0     & 0     & \cdots & 0     \\
            h_1   & h_0   & 0     & 0     & 0     & \cdots & 0     \\
            \vdots& h_1   & h_0   & 0     & 0     & \cdots & 0     \\
            h_{\ell_h-1} & \vdots & h_1   & h_0   & 0     & \cdots & 0     \\
            0     & h_{\ell_h-1} & \vdots & h_1   & h_0   & \cdots & 0     \\
            \vdots     & \ddots & \ddots & \ddots & \ddots & \ddots & 0 \\
            0     & \cdots & 0     & h_{\ell_h-1} & \cdots  & h_1 & h_0   
        \end{bmatrix}
    }_{\displaystyle T}
    \begin{bmatrix}
        x_0 \\
        x_1 \\
        x_2 \\
        \vdots \\
        x_{\ell-1}
    \end{bmatrix}.
\end{equation}
%
Classical digital signal processing often handles FIR filters using \emph{block convolution} methods \citep{burrus1985convolution}. One partitions both the input and the output signals into chunks of size $\ell_b$, then multiplies $\ell_b \times \ell_b$ sub-blocks of $T$ against these smaller segments. Once the filter's support is exceeded, many sub-blocks are purely zeros and can be skipped -- an advantage when $\ell_h \ll \ell$.


Concretely, the input and output sequences are chunken into $x=(\hat x_0, \hat x_1, \dots)$, $y=(\hat y_0, \hat y_1, \dots)$ with  
%
\begin{equation}
    \hat x_k = (x_{k\ell_b},  \dots, x_{k\ell_b+\ell_b-1}), \quad 
    \hat y_k = (y_{k\ell_b}, \dots, y_{k\ell_b+\ell_b-1}), \quad k = 0, 1, 2, \dots, \lceil \ell/\ell_b \rceil-1.
\end{equation}
%
The fully partitioned Toeplitz matrix factors into submatrices $H_0, H_1, \dots, H_{\lceil \ell/\ell_b \rceil-1}$ of size $\ell_b \times \ell_b$. For instance,
%
\begin{equation}
    H_0 = \begin{bmatrix}
        h_0 & 0 & \cdots & 0 \\
        h_1 & h_0 & \ddots & \vdots \\
        \vdots & \ddots & \ddots & 0 \\
        h_{\ell_b-1} & \cdots & h_1 & h_0
    \end{bmatrix}, 
    \quad
    H_1 = \begin{bmatrix}
        h_{\ell_b} & h_{\ell_b-1} & \cdots & h_1 \\
        h_{\ell_b+1} & h_{\ell_b} & \ddots & \vdots \\
        \vdots & \ddots & \ddots & h_{\ell_b-1} \\
        h_{2\ell_b-1} & \cdots & h_{\ell_b+1} & h_{\ell_b}
    \end{bmatrix}, \quad \dots
\end{equation}
%
%
Hence,
%
\begin{equation}
    T = \begin{bmatrix}
        H_0      &        &        & \\
        H_1      & H_0    &        & \\
        H_2      & H_1    & H_0    & \\
        \vdots   & \vdots & \vdots & \ddots \\
        
    \end{bmatrix}.
\end{equation}
%
Since $h_t=0$ for $t\ge \ell_h$, any blocks with index greater than $\left\lceil (\ell_h-1)/\ell_b \right\rceil+1$ yields a zero submatrix. Hence, we only need to construct and multiply the non-zero submatrices $H_k, k=0, 1, \dots, \left\lceil (\ell_h-1)/\ell_b \right\rceil$.
%
The output blocks then obey:
%
\begin{equation}\label{eq:block_conv_def}
    \hat y_n = \sum_{k=0}^{n} H_{n-k}\,\hat x_k = \sum_{k=0}^{\lceil (\ell_h-1)/\ell_b \rceil} H_{k}\,\hat x_{n-k}.
\end{equation}
%

Note that \emph{block convolution} \eqref{eq:block_conv_def} can be seen as a “convolution of convolutions”, since each block $H_k$ is itself a Toeplitz matrix and can be implemented efficiently by direct multiplication or by using fast convolution techniques (e.g., FFT-based methods when $\ell_b$ is large) within each block. 

\subsection{Simple Two-Stage Block Algorithm}
\label{sec:two_stage_algo}

For many \texttt{Hyena-SE} or \texttt{Hyena-MR} use cases, $\ell_h$ (the filter length) is much smaller than $\ell$ (the sequence length). When $\ell_h$ is also within about twice the chosen block size $\ell_b$, a particularly efficient two-stage block algorithm can be used. 

Let $T$ be the Toeplitz matrix that applies a grouped depthwise FIR filter of length $\ell_h$. Suppose we choose a block size $\ell_b$ such that $\ell_h \le 2 \ell_b$. Under this condition, $T$ can be decomposed into a block-diagonal part plus an off-diagonal part (or ``stage''):
%
\begin{equation}
    T = \underbrace{\begin{bmatrix}
       H_0 &  &  & \\
         & H_0 &  & \\
         &  & \ddots &  \\
         &  &  & H_0
    \end{bmatrix}}_{\text{first stage}}
    \;+\;
    \underbrace{\begin{bmatrix}
         &  &  & \\
        H_1 &  &  & \\
         &  \ddots&  &  \\
         &  &  H_1 & 
    \end{bmatrix}}_{\text{second stage}}.
\end{equation}
%
In particular:
\begin{itemize}
    \item $H_0$ covers the points of the filter that align with the current chunk $\hat x_k$.
    \item $H_1$ covers the points of the filter that spills over from the previous chunk $\hat x_{k-1}$, capturing taps that straddle the boundary between adjacent chunks.
\end{itemize}
%



\begin{note}{colback=teal!10}
As an illustrative example, consider $\ell=6$ (sequence length), $\ell_h=4$ (filter length), and $\ell_b=3$ (block size). The filter coefficients $h_0, h_1, h_2, h_3$ form the blocks:
%
\[
H_0 = \begin{bmatrix}
h_0 & 0 & 0 \\
h_1 & h_0 & 0 \\
h_2 & h_1 & h_0
\end{bmatrix}, \quad
H_1 = \begin{bmatrix}
h_3 & h_2 & h_1 \\
0 & h_3 & h_2 \\
0 & 0 & h_3
\end{bmatrix}.
\]
%
The full Toeplitz matrix $T$ decomposes as:
%
\[
T = \underbrace{\begin{bmatrix}
H_0 & 0 \\
0 & H_0
\end{bmatrix}}_{\text{first stage}}
+
\underbrace{\begin{bmatrix}
0 & 0 \\
H_1 & 0
\end{bmatrix}}_{\text{second stage}}
= \begin{bmatrix}
h_0 & 0 & 0 & 0 & 0 & 0 \\
h_1 & h_0 & 0 & 0 & 0 & 0 \\
h_2 & h_1 & h_0 & 0 & 0 & 0 \\
h_3 & h_2 & h_1 & h_0 & 0 & 0 \\
0 & h_3 & h_2 & h_1 & h_0 & 0 \\
0 & 0 & h_3 & h_2 & h_1 & h_0
\end{bmatrix}.
\]
\end{note}

This approach presents a number of advantages.  Once loaded, $H_0$ and $H_1$ can be reused across multiple chunks of the input and, with our grouped operator design, also across multiple channels within the same group. This provides a convenient way to turn small {\tt GEMV} operations into {\tt GEMMs}, compared to other {\tt GEMM} approaches for convolutions that rely on forming strided views of the input. Furthermore, the decomposition separates “current chunk” vs. “previous chunk” computations, which can be run in parallel or as a pipeline. 

% Finally, larger, contiguous blocks (of size $\ell_b$) align well with hardware accelerators such as specialized tensor cores, effectively turning 
% %


%\begin{note}{colback=green!10}
\paragraph{Analysis of the two-stage multiplication.}
Suppose we denote by $\hat{X}_n \in \RR^{\ell_b \times d}$ the $n$-th input chunk (as in \eqref{eq:block_conv_def}), where $d$ denotes both the group size and the tensor core dimension, and let $\hat{Y}_n \in \RR^{\ell_b \times d}$ be the corresponding output chunk. Under a two-stage block convolution, each $\hat{Y}_n$ is computed as
%
\begin{equation}\label{eq:two_stage_formula}
    \hat{Y}_n = H_0\,\hat{X}_n \;+\; H_1\,\hat{X}_{n-1},
    \quad
    n = 0, 1, \dots, \lceil \ell/\ell_b \rceil - 1,
    \quad
    \text{(with $\hat{X}_{-1} = 0$ for $n=0$).}
\end{equation}
%
Here, $H_0$ captures the filter taps interacting with the “current” chunk $\hat{X}_n$, while $H_1$ handles the “spillover” from the preceding chunk $\hat{X}_{n-1}$. By construction, $\ell_h \le 2\,\ell_b$ ensures that no additional off-diagonal blocks appear beyond $H_1$.

In the setting where all channels within a group share the same filter, the matrices $H_0 \in \RR^{\ell_b \times \ell_b}$ and $H_1 \in \RR^{\ell_b \times \ell_b}$ are common to every channel in the group. Consequently, instead of forming a block-diagonal structure over separate channels, one can directly operate on the full input block $\hat{X}_n \in \RR^{\ell_b \times d_g}$, where $d_g$ is the group size. In particular, if the tensor core is of size $d_g$ and $\ell_b = d_g$, then \eqref{eq:two_stage_formula} can be implemented as two full {\tt GEMM} operations. This approach maximizes throughput and fully leverages tensor core utilization by processing the entire group in one efficient matrix multiplication. 
%\end{note}


\subsubsection{Implementation}
% using {\tt Triton}\footnote{Triton, a Python DSL (domain specific language) that enables composition of performant CUDA kernels with {\tt numpy}-like syntax. For example, a kernel for loading the Toeplitz matrix directly from the dense filter can be written in under 50 LOC while still maintaining IO-efficiency.
% }.

We report the kernel implementation of our two-stage blocked algorithm in Algorithm \ref{alg:two_stage_chunked}. The crux lies in the grouping structure, which enables data reuse and efficient computation using tensor cores dedicated hardware units on NVIDIA GPUs specialized for high throughput matrix multiplication. Without grouping, one would need to adopt a different strategy to transform depthwise convolutions from {\tt GEMVs} to {\tt GEMMs} to avoid lower throughput CUDA cores, for example forming an input view that parallelizes the first-stage multiplication with $H_0$ across all blocks.

% insight, a channel-independent, depthwise convolution (GEMV) would be limited to lower throughput CUDA cores. Algorithm \ref{alg:two_stage_chunked} shows the implementation of the forward pass for a two-stage chunked convolution in the depthwise case.


% structure of grouped filters and channel-independence of depthwise convolutions, which enables data reuse and efficient computation using tensor cores, dedicated hardware units on NVIDIA GPUs specialized for high throughput matrix multiplication.  

\begin{algorithm}[H]
\caption{Simple Two-Stage Blocked Hyena Convolution (Forward)}
\begin{algorithmic}[1]
\Require Input $v,~q,~k \in \RR^{\ell \times d}$, filter $h \in \RR^{\ell_h}$, block size $\ell_b$
\Ensure Output $y \in \RR^{\ell \times d_g}$ 
\State Chunk inputs $v,~q,~k$ into blocks $v_i,~q_i,~k_i$ of size $\ell_b \times d_g$
\For{block $i = 0$ to $\lceil \ell/\ell_b \rceil - 1$}
    \State Load $v_i,~q_i,~k_i,~H_0,~H_1$ to on-chip memory
    \State Initialize $y_i = 0$
    \State Optional: $v_i \gets k_i \odot v_i$
    \State $y_i \gets H_0 v_i$ \Comment{First {\tt GEMM}: block-diagonal}
    \If{$i > 0$}
        \State Load $x_{i-1}$ to on-chip memory
        \State $y_{i} \gets y_{i} + H_1 v_{i-1}$ \Comment{Second {\tt GEMM}: off-diagonal}
    \EndIf
    \State Optional: Compute $y_{i} \gets q_i \odot y_i$
\EndFor
\State \Return $y$
\end{algorithmic}
\label{alg:two_stage_chunked}
\end{algorithm}



% and thus cannot be done in a single kernel without the use of atomic operators, limiting concurrency. Instead, 

\subsubsection{Profiling}

Convolutional multi-hybrids models are designed to be efficient across a wide range of regimes, compared to both full attention and other subquadratic operators. We optimize for both short and long sequences, as pretraining is often performed at shorter sequence lengths to maximize throughput.

\paragraph{Measurement protocol} 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/hyenase_variants_latency.pdf}
    \includegraphics[width=0.49\textwidth]{figures/hyenase_variants_tflops.pdf}
    \vspace{-2.5mm}
    \caption{Forward latency and TFLOPS / second of \textsf{Hyena-MR} variants with filter length $128$. We compare a baseline implementation using PyTorch convolutions and our two-stage blocked kernel, showing substantial improvements in latency and throughput.}
    \label{fig:hyenamr_variants}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/operator_profiles.pdf} \\
    % \includegraphics[width=0.8\textwidth]{figures/operator_profiles_tflops.pdf}
    \caption{Forward latency and TFLOPs / second of \textsf{Hyena-SE}, \textsf{Hyena-MR} and other common operators in architecture design: \textit{multi-head attention} (MHA) and linear attention variants. All values are collected at operator width $4096$ (corresponding to model width at $7$B parameters), on H100s. For MHA, we report both a highly optimized implementation for Hopper GPUs (PyTorch SDPA) as well as a previous generation implementation not optimized for Hopper GPUs (FlashAttention2) \citep{dao2023flashattention}. All other operators use their official auto-tuned Triton kernels. Convolutional primitives remain efficient across sequence lengths, with substantially higher throughput than other operators, including efficient alternatives to MHA.}
    \label{fig:operator_profiles}
\end{figure}

We measure the latency and throughput of common operators in a batch size 1, varying sequence length, model width 4096 setting (corresponding to common operator width for 7B models), including input and output projections. We do not keep the total number of tokens constant, in contrast to the protocol used in FlashAttention 3 \citep{shah2024flashattention}. Figure \ref{fig:operator_profiles} and \ref{fig:tflop-app} provide the results.







% \begin{center}
% \begin{tikzpicture}[
%     %Global config
%     >=latex,
%     line width=1pt,
%     %Styles
%     Brace/.style={
%         decorate,
%         decoration={
%             brace,
%             raise=-7pt
%         }
%     },
%     Matrix/.style={
%         matrix of nodes,
%         text height=2.5ex,
%         text depth=0.75ex,
%         text width=3.25ex,
%         align=center,
%         left delimiter={[},
%         right delimiter={]},
%         %column sep=5pt,
%         %row sep=5pt,
%         %nodes={draw=black!10}, % Uncoment to see the square nodes.
%         nodes in empty cells,
%     },
%     DA/.style={
%         fill,
%         opacity=0.2,
%         rounded corners,
%         inner sep=-3pt,
%         line width=1pt,
%     },
%     DL/.style={
%         left delimiter=[,
%         right delimiter=],
%         inner sep=-2pt,
%     },
%     DG/.style={
%         line cap= round,
%         line width =15pt,
%         opacity=0.2,
%     }
% ] 
% \matrix[Matrix] at (0,0) (M){ % Matrix contents  
% $h_0$ & $0$ & $0$ & $0$ \\% & $0$ & $0$ \\ 
% $h_1$ & $h_0$ & $0$ & $0$ \\%& $0$ & $0$ \\
% $0$ & $h_1$ & $h_0$ & $0$ \\
% $0$ & $0$ & $h_1$ & $h_0$ \\
% };
% \begin{scope}[on background layer] 
%     %FOR MATRIX M
%     %To delimit internal braces
%    %\node[DL,fit=(M-1-1)(M-4-4)](subM-1){};
%     %To delimit internal area groups
%     \node[DA,blue,fit=(M-1-1)(M-2-2)](subM-2){};
%     \node[DA,blue,fit=(M-3-3)(M-4-4)](subM-2){};
%     \node[DA,green,fit=(M-3-1)(M-4-2)](subM-2){};

%     %\node[DA,green,fit=(M-6-4)(M-7-6)](subM-3){};
%     % For line sectors
%     %\draw[DG,red](M-2-2.center) --(M-6-6.center);
%     %\draw[DG,orange](M-1-7.center) --(M-7-7.center);
%     %\draw[DG,black](M-8-1.center) --(M-8-6.center);    
% \end{scope}
% \end{tikzpicture}
% \end{center}


% \subsection{Generalized Multi-Pass Chunked Algorithm}

% In principle, \textsf{Hyena-MR} with longer filters, as well as {\tt Hyena-LI} can be computed using a multi-stage extension of the two-stage algorithm. Combined with a grouped design, this can ensure higher hardware utilization by exploitingtensor cores. 

% For a general filter length $\ell_h$ and block size $\ell_b$, we can decompose the Toeplitz matrix $T_h$ into a sum of $\lceil \ell_h/\ell_b \rceil$ block matrices:

% \begin{align}
%     T_h &= T_h^{(0)} + T_h^{(1)} + \cdots + T_h^{(\lceil \ell_h/\ell_b \rceil - 1)} \\
%     \text{where } T_h^{(k)} &= \begin{bmatrix}
%         0_{kb \times \ell} \\
%         \tilde{T}_h^{(k)} \\
%         0_{(\ell - kb - r_k) \times \ell}
%     \end{bmatrix}
% \end{align}

% Here $\tilde{T}_h^{(k)}$ is a block-diagonal matrix containing the $k$-th set of filter coefficients:
% \begin{align}
%     \tilde{T}_h^{(k)} &= \begin{bmatrix}
%         H_k & & & \\
%         & H_k & & \\
%         & & \ddots & \\
%         & & & H_k
%     \end{bmatrix} \\
%     H_k &= \begin{bmatrix}
%         h_{kb} & 0 & \cdots & 0 \\
%         h_{kb+1} & h_{kb} & \cdots & 0 \\
%         \vdots & \vdots & \ddots & \vdots \\
%         h_{kb+r_k-1} & h_{kb+r_k-2} & \cdots & h_{kb}
%     \end{bmatrix}
% \end{align}

% where $r_k = \min(\ell_b, \ell_h - kb)$ is the remaining filter length for block $k$. This decomposition allows us to compute the convolution in $\lceil \ell_h/\ell_b \rceil$ passes, with each pass processing a different set of filter coefficients. With this factorization, one can devise a multi-stage version of the two-stage algorithm presented earlier.
%


% \begin{itemize}
%     \item Get {\tt cgcg} roughly on par with causal conv1d fused at short filter lengths (< 4), then show how it out performs naive PyTorch conv kernels at longer filters (from short to medium)
%     \item Operator level profiling (and MFU) for fwd against linear attention and state-space model variants
%     \item Some finegrained analyses (group size)?
%     \item If time permits: (@Jerome) mention work on backward and further optimizations (e.g., Toeplitz reuse)
% \end{itemize}



%\begin{algorithm}[H]
% \caption{Two-Pass Chunked Convolution}
% \begin{algorithmic}[1]
% \Require Input signal $u \in \RR^{\ell}$, filter $h \in \RR^{\ell_h}$, block size $b$
% \Ensure Output signal $y \in \RR^{\ell}$ 
% \State Split input $u$ into blocks $u^{(i)}$ of size $b$
% \State Initialize output $y = 0$
% \For{block $i = 0$ to $\lceil \ell/b \rceil - 1$}
%     \State \textbf{Local Pass:} Compute diagonal block contribution
%     \State $y^{(i)} \gets T^{(i)}_{\text{diag}} u^{(i)}$ \Comment{$T^{(i)}_{\text{diag}}$ is block-diagonal Toeplitz matrix}
%     \If{$i > 0$}
%         \State \textbf{Correction Pass:} Add contribution from previous block
%         \State $y^{(i)} \gets y^{(i)} + T^{(i)}_{\text{corr}} u^{(i-1)}$ \Comment{$T^{(i)}_{\text{corr}}$ contains overlap terms}
%     \EndIf
% \EndFor
% \State \Return $y$
% \end{algorithmic}
% \end{algorithm}

% CHECK:
% When $b < \ell_h$, additional correction blocks would be needed to account for the longer filter response overlapping multiple blocks.




% \paragraph{Two-Pass Chunked Convolutions}

% \paragraph{Background.}
% We consider signals $\mathbf{u} \in \mathbb{R}^{d \times \ell}$ and use subscripts to index in the time dimension, and Greek superscripts to index in the space (or width) dimension.
% Discrete (causal) convolutions of a bank of filters $\mathbf{h} \in \mathbb{R}^{d \times d \times \ell}$ over a signal $\mathbf{u} \in \mathbb{R}^{d \times \ell}$ are defined as
% \[
% y^\alpha_t \;=\; \sum_{\beta=0}^{d} (h^{\alpha \beta} * u^\beta )_t \;=\; \sum_{\beta=0}^{d} \sum_{j=0}^{t} h^{\alpha \beta}_{t-j}\,u^\beta_j \,.
% \]
% We are mostly interested in generalized grouped (depthwise) convolutions, i.e., the number of filters is smaller than $d \times d$ and depends on a particular interconnection topology between channels. We start with the depthwise case since that covers many interesting scenarios for us.
% In the depthwise case, the result is simply obtained as
% \[
% y^\alpha_t \;=\; \sum_{j=0}^{t} h^\alpha_{t-j}\,u^\alpha_j \,.
% \]
% Note: in practice, $h_t$ will be non-zero only for $t$ such that $t<\ell_h$. Our implementations leverage various effective filter lengths.

% \paragraph{Two-Pass Algorithm.}
% A discrete convolution can be equivalently expressed as a GEMM with a Toeplitz (circulant) matrix. Let $\ell_h = 3$ and $\ell = 4$ for clarity. Then,
% \[
% y \;=\; T\,u \;=\;
% \begin{bmatrix}
% h_0 & h_1 & h_2 & 0 \\
% 0 & h_0 & h_1 & h_2 \\
% 0 & 0 & h_0 & h_1 \\
% 0 & 0 & 0 & h_0
% \end{bmatrix}
% \begin{bmatrix}
% u_0 \\[6pt]
% u_1 \\[6pt]
% u_2 \\[6pt]
% u_3
% \end{bmatrix}.
% \]

% \textbf{Step 1: Local.}
% The main idea is to compute GEMMs with blocks of $T$, then correct by using computation from the previous block.

% Introduce groups (blocks) with superscripts:
% \[
% \hat{y}^{(0)} \;=\;
% \begin{bmatrix}
% h_0 & h_1\\[6pt]
% 0   & h_0
% \end{bmatrix}
% \begin{bmatrix}
% u_0\\[3pt]
% u_1
% \end{bmatrix},
% \quad
% \hat{y}^{(1)} \;=\;
% \begin{bmatrix}
% h_0 & h_1\\[6pt]
% 0   & h_0
% \end{bmatrix}
% \begin{bmatrix}
% u_1\\[3pt]
% u_2
% \end{bmatrix}.
% \]
% Note that the matrices are shared, and the computation can be trivially parallelized.

% \textbf{Step 2: Correction.}
% The full output is then computed by correcting $\hat{y}^{(1)}$ by using $\hat{y}^{(0)}$. Specifically,
% \[
% y^{(0)} \;=\; \hat{y}^{(0)}, \quad
% y^{(1)} \;=\; \hat{y}^{(1)} + \hat{T}\,u^{(0)},
% \]
% where
% \[
% \hat{T} \;=\;
% \begin{bmatrix}
% h_2 & 0\\[6pt]
% h_1 & h_2
% \end{bmatrix}.
% \]
% This is extended to $k$ steps as
% \[
% y^{(k)} \;=\; \hat{y}^{(k)} \;-\; \hat{T}\,u^{(k-1)},
% \]
% and can be generalized to essentially any type of GEMM, with the main advantages being the ability to leverage structure (e.g., locality in this case, or low-rank structure of blocks in recurrent layers, etc.).



% \paragraph{Grouping and Toeplitz chunks}

% {\color{red}TODO: figure with three panels: latency over sequence length of individual operators, inset for smaller context, throughputs 40B 1M}

