\begin{figure}[H]
    \centering
    \includegraphics[width=0.99\textwidth]{figures/fig1_final_v2.pdf}
    \vspace{-3mm}
    \caption*{Figure 1: Scaling experiments, showing differences in perplexity and throughput of Transformers, multi-hybrids (StripedHyena 2), and other alternative operators.}
    \label{fig:first}
\end{figure}


\section{Introduction}


Architecture improvements for training language models at scale can be broadly categorized into several main groups. Tweaks to the attention mechanism to reduce the size of the kv cache such as GQA, MQA, MLA, sliding window and linear attention \citep{shazeer2019fast,brown2020language,vaswani2021scaling,katharopoulos2020transformers,ainslie2023gqa,liu2024deepseek}. Changes to the model for numerical stability, resilience to outliers and quantization such as pre-norm, SwiGLU, QK normalization \citep{zhang2019root,xiong2020layer,shazeer2020glu}. Finally, modifications that improve model capacity or recall at longer context such as RoPE, MoE \citep{shazeer2017outrageously,su2024roformer}. Despite the broad interest in architecture improvement, remarkably few proposals, outside of the aforementioned methods, have delivered consistent gains at scale.

A different approach is to introduce new \textit{classes} of input-dependent operators to the standard mix of layers (self-attention and feed-forward) layers and optimize their composition, resulting in \textit{hybrid} architectures. Hybrids promise improvements on both quality and efficiency, and have been proposed in various domains and with various mixtures of operators, typically with some combination of convolution and attention \citep{dai2021coatnet,polistripedhyena}, linear attention and attention \citep{fu2022hungry,fathi2023block,lieber2024jamba,glorioso2024zamba}, or local attention and attention \citep{child2019generating,beltagy2020longformer}. In language modeling at scale, hybrids of convolutions, linear attention and attention have been validated through dedicated scaling laws \citep{poli2024mechanistic} and large-scale model releases \citep{glorioso2024zamba,nguyen2024sequence,team2024jamba}.

Despite being a promising alternative, hybrids based on operators such as linear attention or state-space models have struggled to replace baseline Transformers as the de facto standard for language modeling due to a variety of reasons. One limitation is that these fixed-state operators realize efficiency gains only when applied to very long sequences, which is also where they drastically underperform full self-attention \citep{arora2023zoology,jelassi2024repeat} in quality. Compared to Transformers, these methods are generally slower in common pretraining regimes: shorter contexts with larger and wider models. Furthermore, most of these approaches have been developed with the explicit goal of matching self-attention performance on in-context recall over longer sequences, but require hybridization with self-attention to perform competitively in practice. This has introduced redundancy in architectures, as multiple operators are optimized for the same capability: in-context recall.

% Furthermore, the development of these approaches has been driven by the goal of matching self-attention performance on in-context recall over longer sequences. Yet, they have only been successfully deployed in hybrids due to quality gaps, introducing a redundancy with multiple operators optimizing for the same in-context recall capability.



% and have been validated in practice through scaling laws and a variety of large-scale model releases. In language modeling, by far the most popular class of hybrids relies on the introduction of modern linear attention operators or local attention in a Transformer backbone (jamba, samba, etc.) with variants offering very similar performance and quality. In this setting, some convolutional hybrids have also been shown to perform well (stripedhyena). 

% Hybrids also frequently appear outside language modeling (computer vision, diffusion, audio, biology). In domains where data is not tokenized as aggressively, convolutional hybrids tend to be the most popular and have a longer history. 


% gqa, mla, sliding window) (b) changes to the model for stability, removal of outliers, resilience to quantization (norms, swiglu, qk normalization) (c) changes that improve quality per parameter or quality at longer context(rotary embeddings, mixture of experts, etc.).

% Hybrids are a class of model architectures that introduce additional types of operators in the baseline mix of self-attention and feed-forward layers (MLPs, SwiGLUs, MoEs), such as linear recurrences (linear RNNs, linear attention, state-space models). So far, there is skibidi

% a stack of self-attention and feed-forward layers, and fall into a few different categories: (a) tweaks to the attention mechanism to improve compression of the kv cache (gqa, mla, sliding window) (b) changes to the model for stability, removal of outliers, resilience to quantization (norms, swiglu, qk normalization) (c) changes that improve quality per parameter (rotary embeddings, mixture of experts, etc.).




    % New operators in hybrids have been designed to replace self-attention in its strongest subtask of in-context recall, but this leads to suboptimal designs choices in hybrid models. Why not choose operators that complement each other? Convolutions excel at filtering information, especially at longer context where naive softmax attention tends to attend to noise.

% Understanding the interplay between quality and efficiency of a model architecture is fundamental, yet challenging due to the multitude of factors that affect: experimental protocol (ablations, scaling laws), relative efficiency of the implementation and baselines, dataset and optimization hyperparameters.

This paper explores a fundamentally different approach. We advocate for model architecture designs that are both hybridization-aware and hardware-aware, combining different types of operators with complementary capabilities and computational costs, across a range of input and model sizes. Our approach is motivated by work on synthetics \citep{akyurek2024context} and mechanistic design \citep{poli2024mechanistic}, showing how different operators in hybrids can specialize to subtasks such as recall, compression, multi-query recall, and fuzzy recall. For example, input-dependent convolutions excel at filtering noise and performing multi-token recall, useful for modeling byte-level data, whereas attention is optimized for targeted recall of information across longer sequences. We introduce \textit{multi-hybrids}, architectures that combine strengths of multiple operator types. 

We focus on StripedHyena 2, the first example of a convolutional multi-hybrid architecture for sequence modeling validated on a series of experiments at scale (40 billion parameters, 9 trillion tokens). StripedHyena 2 is based on three different types of input-dependent convolutional operators: (i.) short, explicitly-parametrized hyena operators that maximize hardware utilization, specializing in local multi-token recall, (ii.) medium-length, regularized hyena operators tailored to efficient modeling across hundreds of tokens, and (iii.) long, implicit hyena operators that aggregate information over the entire sequence. We describe the algorithmic foundations of convolutional multi-hybrids, focusing on architecture design, kernels, and context parallelism algorithms. As a motivating example, we will use the experiments behind the Evo 2 line of models \citep{brixievo2}, built on top of StripedHyena 2. Evo 2 40B is a state-of-the-art foundation model for genomics, trained on byte-tokenized (nucleotide) sequences.


% that has first shown that operators in hybrid models specialize to subtasks such as recall, compression, multi-query recall, fuzzy recall\footnote{For example, input-dependent convolutions excel at filtering noise and performing multi-token recall, whereas attention is optimized for longer-range in-context recall}.

% based on the combination of multiple types of operators with complementary capabilities and computational profiles.


% Instead, we argue for model architecture design that is hybridization-aware and hardware-aware, based on the combination of multiple types of operators with complementary capabilities and computational profiles.


% designing directly for hybridization can result in more efficient models, by allowing for the introduction of operators with capabilities that complement each other e.g., convolution and attention. 

% This specialization is key to also enable efficiency gains at a broader set of input regimes e.g., by making heavy use of shorter input-dependent convolutions to perform local multi-token recall. We push these ideas by introducing a new class of \textit{multi-hybrid} architectures, where many operators are interleaved. 


% This is motivated by work on mechanistic design (CIT).

% and push this concept further by introducing a new class of convolutional multi-hybrid architectures. This is motivated by work on mechanistic design (CIT).

% MAD line of work: synthetics tell us that operators in hybrid models specialize to subtasks such as recall, compression, multi-query recall, fuzzy recall etc. While historically efficient operators were primarily designed to \textit{replace} self-attention (i.e., perform as well as attention in its strongest subtask of in-context recall), this leads to suboptimal designs choices in hybrid models. Why not choose operators that complement each other? Convolutions excel at filtering information, especially at longer context where naive softmax attention tends to attend to noise.


% In 

% architecture design, efficient implementation with two-pass kernels for convolutions, custom context parallelism methods, and inference cache management. 



% We discuss the algorithmic foundations of multi-hybrids: this report, we describe the algorithmic foundations of multi-hybrid convolutional models, focusing on StripedHyena 2, the architecture at the core of the Evo 2 line of models. StripedHyena 2 is designed from the ground up to exploit the synergy between different operators, at multiple levels. This is the main idea of multihybrid models. Notably, multi-hybrid designs allow us to go beyond the understood tradeoffs between efficiency and quality, and to further optimize the Pareto frontier of efficiency and quality (most existing efficient hybrids based on RNNs, linear attention and state-space layers offer similar performance and quality, because they are all based on the same underlying operator choices).


\paragraph{Outline}

%

In Section \ref{sec:architecture}, we introduce the basic design ideas and describe the three primary operators behind convolutional multi-hybrids. We then discuss composition, filter grouping for improved hardware utilization, and showcase scaling at the thousand GPU and 40 billion parameter scale. In Section \ref{sec:kernel_optimizations}, we focus on architecture and algorithm co-design. Using filter grouping, we adapt overlap-add algorithms \citep{burrus1985convolution} to tensor cores, introducing our implementation of a two-stage blocked kernel. We measure the performance gains at short and long context compared to efficient attention implementations (FlashAttention3 \citep{shah2024flashattention}, SDPA) and other alternative operators such as linear attention and state-space models: Mamba2 \citep{dao2024transformers}, xLSTM \citep{beck2024xlstm} and DeltaNet \citep{yang2024parallelizing}. In Section \ref{sec:context_parallelism}, we develop custom context parallelism methods for the different types of convolutions in our models. We introduce both peer-to-peer and all-to-all algorithms, including new channel-pipelined variants and FFT-based methods.


