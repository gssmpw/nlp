\section{evaluation}
% justification
We conducted an exploratory evaluation of \textit{Polymind}, focusing on the usability, creativity, and usefulness of its parallel collaboration workflow.
More specifically, the study aimed to answer the following two questions:
\begin{enumerate}
    \item Is \textit{Polymind} easy to use, and useful for prewriting?
    \item How effective are \textit{Polymind}'s parallel collaboration workflow and microtasking features for supporting creativity in prewriting?
\end{enumerate}

% \input{demographics}

\subsection{Participants}
We used convenience sampling to recruit 10 participants (4 male, 6 female) mainly from local universities. All participants are L2 English speaker.
\revision{Similar to our formative study, we mainly reached out to participants with creative writing experience or related majors, though not necessarily expert writers. All participants had experience using prewriting or diagramming tools such as Figma or Miro, but reported limited knowledge or experience of AI or programming.}
We refer to them as V1-10. For each participant, we offered a coupon equivalent to 50 HKD.

\subsection{Study Design}

\subsubsection{Tasks}
To evaluate our \textit{Polymind} in both divergent and convergent thinking phases, we divide a creative pre-writing task into two sessions: story ideation, and story outlining. In the story ideation, participants were required to brainstorm as many distinct storylines as possible. Each storyline only needs to be one or two sentences long that specifies main characters, events, locations, time, etc. In the story outlining session, participants were asked to pick one favourite storyline from the previous session, and draft a rough outline with as many details as possible. An outline needs to specify a clear structure (such as the classic beginning-climax-ending structure), and key events along the structure.

\subsubsection{Conditions}
The study compared our system, \textit{Polymind} to a turn-taking, conversational interface, plus \textit{Polymind}'s diagramming canvas, as a baseline using two creative writing prompts. \revision{The baseline allows users to take notes and keep track of conversational results using the canvas, but does not require nor allow users to interact with the AI via diagrams}. Specifically, two system conditions were used:
\begin{itemize}
    \item \textbf{GPT-4 \& Canvas} OpenAI ChatGPT-4 interface plus \textit{Polymind}'s diagramming canvas. All microtasking features were turned off.
    \item \textbf{\textit{Polymind:}} full version of \textit{Polymind} with six predefined default microtasks.
\end{itemize}
Two creative writing prompts were chosen:
\begin{itemize}
    \item Write a story where your character is traveling a road that has no end, either literally or metaphorically.
    \item Write a story in which a character is running away from something, literally or metaphorically.
\end{itemize}
We used a Latin square experimental design~\cite{ryan2007modern} to achieve a balanced sequence of writing prompts and system conditions.

\subsubsection{Study Procedure}
After giving consent to our study, users were invited to use two systems in turn to complete two sessions: story ideation and story outlining, given two different writing prompts. Each session lasted 12 minutes, and users were given time to transfer their prewriting results (generations, diagrams, or merely thoughts and ideas in their minds) to another document after each session concluded. Before using \textit{Polymind}, we walked the participants through all of its features and offered approximately 10 minutes for them to try out the system.

After two sessions concluded for a system condition, each participant was required to complete a survey, including a NASA Task Load Index (NASA-TLX)~\cite{hart1986nasa}, and three dimensions (2 questions each) of Creativity Support Index (CSI)~\cite{cherry2014quantifying}: Enjoyment, Exploration, and Expressiveness. Two dimensions (Collaboration \& Immersion) of the original CSI were dropped because they were irrelevant to the two questions we sought to answer.

To evaluate the final results (outlines), we invited two expert writers to score participants outlines using Torrance Test of Creative Writing (TTCW)~\cite{chakrabarty2023art}, instead of the self-rated score of CSI. \revision{We did a quick interview after the scoring to ask about their general feedback, and to compare results of two conditions and pick out examples with most noticeable differences.}
One of our expert is a professional fiction writer and has a doctoral degree in film studies. She used to be a screenwriter before becoming a fiction writer. The other expert is an AO3 (Archive of Our Own) writer that has posted over 400K words and accumulated over 250K views.

After using two system conditions (4 sessions in total), each participant was then required to complete a survey to rate the usefulness of each \textit{Polymind} features. We then conducted a brief interview (5-10 min) to ask about their overall use experience, feedback on \textit{Polymind}'s workflow, perceptions of creativity support, and perceived differences between the two workflows and their impact on the final results.

The whole study procedure lasted around 2 hours, and was screen recorded. The interviews were audio-taped and transcribed for analysis.

% \input{usability_figures}

\subsection{Study Results}
In this subsection, we report the findings of our study. On balance, \textit{Polymind}'s parallel microtasking workflow granted more customizability and was more controllable. Therefore users reported a stronger sense of agency, ownership of results, and a higher level of expressiveness. The microtasking workflow could also help quickly expand idea trees through ``chaining''-like effects~\cite{wu2022ai}.

\input{usability_figures}

\subsubsection{Usability \& Usefulness}
Despite efforts of microtask and diagram management and the potential learning curve, to our surprise, \textit{Polymind} was almost perceived as easy to use as the ChatGPT interface, and significantly reduced frustration towards generated results (as shown \autoref{fig:usability}). ChatGPT interface was demanding mainly due to efforts of digesting longer text information (e.g., V4-5), and typing and iteratively refining lengthy prompts (e.g., V1-2). \revision{For \textit{Polymind}, the main cause of demand was said by V2-3, \& V10 to be the efforts of mannually managing the canvas, adjusting its layout, and progressing through diagrams.}
Notably, V1 \& V2 said that \textit{Polymind}'s interface was easier to navigate, and easier to read, because its generations were mainly short phrases, and had structures (including lines, sections, \& microtask colours). 
Besides, the randomness of ChatGPT generations also caused higher frustration level and worse perceived performance than \textit{Polymind} among some participants (e.g., V5, V8).

The key features of \textit{Polymind} were mainly perceived useful for prewriting, as shown in \autoref{fig:usefulness}. Of them awareness-related features, such as notifications, previews, and initiative modes, were found most controversial, which revealed the tension of our \textit{Goal 2.1} and being overall non-intrusive. V3 felt a proactive microtask was particularly annoying and intrusive, but we observed that all other participants left key microtasks proactive. Some said (e.g., V5, V7) they would need proactive microtasks in divergent thinking phases for quick ideas, but sometimes did not want to be interrupted while thinking. Therefore, most participants thought the feature of switching intiative modes particularly helpful.

% \input{usability_usefulness}
\input{usefulness_figures}

\subsubsection{Creativity Support}
In terms of creativity, participants generally felt \textit{Polymind} was more supportive (see \autoref{fig:CSI}), but the results were not significant ($P_{Enjoyment}=0.67$, $P_{Exploration}=0.19$, $P_{Expressiveness}=0.05$). Notably, the expressiveness dimension has almost shown significance, as many (e.g., V3 \& V5) reported that \textit{Polymind}'s diagramming interface and microtasking workflow put them in dominant roles that encouraged them to freely express their brief ideas. In terms of results, two conditions produced similar number of ideas ($Polymind_{median}=3$, $Polymind_{stdev}=1.06$, $Baseline_{median}=3$, $Baseline_{stdev}=8.51$) in the ideation session.

In addition, experts' scores showed that the baseline condition produced outlines that were able to pass 5.7 TTCW tests, as compared to \textit{Polymind}'s 4 tests ($P=0.23$) (see \autoref{fig:CSI}).
\revision{
Experts did not particularly mention any noticeable differences in quality between two conditions except that \textit{Polymind}'s results were much shorter and lacked details to pass some tests. Besides, they both expressed concern of overused or clich√©d results. One expert said she was initially interested by V2's story (baseline), but only to find out that it was from \textit{The Vampire Diaries}
}
This is expected, as ChatGPT interface could quickly generate ``\textit{complete and detailed outlines with simple prompts}'' (V2), while \textit{Polymind} usually encouraged users to make progress in diagrams with limited words.
Although some (e.g., V8) noted that they could still generate a complete outline using \textit{Polymind}, but they simply did not want to, because they would like to take control, and create a story from their own fragmented ideas, instead of borrowing all results from ChatGPT.
\input{CSI}

\subsubsection{Microtask Usage: Quick Chaining and Idea Expansion}
\revision{All default microtasks have been applied by 10 participants to produce their final results, as shown in \autoref{tab:usage}. In some cases users might have default microtasks slightly edited. For example, V2 changed the prompt of \BboxS{\textcolor{white}{Brainstorm}} and switched the output type to \textbf{\textcolor{sticky_note}{\textit{sticky note}}} to request detailed settings of a story. Four participants have delegated a total of 8 customized microtasks. For example, V8 delegated \CboxS{\textcolor{white}{Beginning}} \& \CCboxS{\textcolor{white}{Climax}} to generate a beginning and climax of a given storyline. V4 delegated \CboxS{\textcolor{white}{Juice}} to juice up a given story in a \textbf{\textcolor{sticky_note}{\textit{sticky note}}} with more details.}

\revision{We also found participants came up with creative and efficient ways of using a combination of microtasks}. By leaving some microtasks in the proactive mode, \textit{Polymind} can easily perform the ``chaining'' operation~\cite{wu2022ai} to expand users' ideas in a tree-like structure. During this process, multiple distinct microtasks could contribute simultaneously in parallel to users' main operations, which made the collaboration more efficient and creative.
Some participants complimented that the parallel microtasks were like ``\textit{a mature pipeline that needs little efforts}'' (V5), ``\textit{as if splitting (brainstorming) indefinitely}'' (V6). For example, V2 mainly used two proactive microtasks: \FboxS{\textcolor{white}{Freewrite}} \& \SboxS{\textcolor{white}{Summarise}} during the story ideation session.
\revision{He later explained that \FboxS{\textcolor{white}{Freewrite}} was used to quickly generate stories in a \textbf{\textcolor{sticky_note}{\textit{sticky note}}} given a few keywords or concepts within a \textbf{\textcolor{section}{\textit{section}}}, while \SboxS{\textcolor{white}{Summarise}} presented brief summaries in a \textbf{\textcolor{sticky_note}{\textit{sticky note}}} of \FboxS{\textcolor{white}{Freewrite}}'s generations so that he would not need to read whole stories.}

V10 instead was mainly using \BboxS{\textcolor{white}{Brainstorm}} and \EboxS{\textcolor{white}{Elaborate}} to expand her ideas in brief keywords and concepts. \revision{She used \BboxS{\textcolor{white}{Brainstorm}} to request related ideas and \EboxS{\textcolor{white}{Elaborate}} to provide concrete examples of an idea.} In a comparison to the ChatGPT interface, she commented that,
\begin{quote}
    ``\textit{I feel that ChatGPT often generated something irrelevant, and missed my expectations. But this system (Polymind) stuck to my main concept by generating relevant ideas. Although the results were only brief keywords, but it was fast. It could produce a huge idea tree within a short period of time, and you could easily find something intriguing and figure out a coherent story.}''.
\end{quote}

\input{microtasks_usage}

V3 was one of the three participants that showed clear preference for the ChatGPT interface, but she also added that experimenting ideas with \textit{Polymind} were much easier. She explained that \textit{Polymind} ``\textit{had a structure}'' and could perform chaining-like operations easily ``\textit{by using sections}'' and parallel microtasks, while for ChatGPT, ``\textit{combining elements (like some characters, events, or settings) from its generations to re-prompt it was challenging}''. Similarly, V6 noted that brainstorming associations between key events or scenes was much easier with \textit{Polymind} by ``\textit{using several proactive microtasks operating on nodes or sections}''.

\subsubsection{Polymind is More Controllable}
While ChatGPT-4's conversational interface was able to generate long pieces of text with many ideas and details (V2-4, V7-9), most of our participants (V1-2, V4-5, V7-8, V10) mentioned that \textit{Polymind} felt more controllable in a prewriting task. This is because \textit{Polymind} directly operated on diagrams that were often shorter and \revision{thus easier to digest and re-prompt} than ChatGPT-4's conversations, and the canvas progressed in a structured manner with parallel microtasks handling very specific requirements.

The longer generations of the conversational interface were often criticised for being hallucinatory (e.g., ``\textit{not that creative or sensible as it appears}'' -- V7), too random (e.g., ``\textit{not what I expected}'' -- V5, ``\textit{irrelevant}'' -- V10), or ``\textit{mediocre}'' (V2) during a story pre-writing session. In comparison, \textit{Polymind} generations were perceived by many to be relevant (V2, V5, V10), and its microtasks more responsive to users' requests (V5, V8, V10), although it might require some efforts to manage or configure them (V4). \revision{This echoes with the usability score of the two conditions, where \textit{Polymind} was on the same level with the baseline, despite the efforts of managing a diagramming interface.}
V8 said in retrospect that,
\begin{quote}
    ``\textit{I think this (Polymind) would be very helpful for coming up with a story. Cause you can specify your beginning, you can specify your climax, and the ending too... And I think customizing microtasks is also a nice feature... You can really outline everything. While for GPT, you often don't know what is beginning, what is climax or ending.}''
\end{quote}

Additionally, V1, V2 and V8 noted that \textit{Polymind}'s microtasking workflow made it easier to re-prompt. V8 said,
\begin{quote}
    ``\textit{If I want to change something, I know where the part is. Like the character, I only need to change several keywords, like, Oh I'd like the character to be a dragon... I felt that my prompts were actually considered (by microtasks), while GPT sometimes doesn't process all my prompts.}''
\end{quote}
For the conversational interface, it often took multiple iterations to reach a decent draft (V4-5), and each prompt had to be lengthy to change the context (V1-2, V5), which was demanding. V2 also added that the \textit{Polymind} interface was neater because with some parallel microtasks it required little to no efforts of note taking to ask multiple follow-up questions of different ideas.

It is worth noting that, three participants that disliked \textit{Polymind}'s workflows mentioned that it was quite demanding sometimes to configure microtasks (V3-4), and progress in diagrams (V9). V9 said, ``\textit{GPT could generate a lot with a single prompt, while \textit{Polymind} only little by little.}''
\revision{V10 shared similar sentiments. She noted prompting ChatGPT would be much easier than using \textit{Polymind}'s diagrammatic workflow if its generations were not random. However, she added that \textit{Polymind} was in reality less demanding because you could explore more options and easily drop random results.}

\subsubsection{Polymind Affords Agency}
Our participants almost unanimously said that \textit{Polymind} put users in a dominant role, while with the conversational interface they were completely guided by the GPT. This aligns with our \textit{Goal 2} that aims to put humans in a role of managing all microtasks. Participants without any ideas, such as V3 \& V4, generally did not mind following the ChatGPT. This is expected, and agrees with our formative study. However, same as almost all other participants, they particularly mentioned that they felt these results were not their ideas, as ChatGPT generated almost everything. While using \textit{Polymind}, participants said that they had more freedom and control (V3, V6-8), and needed to think a lot (V4-5, V8). 

One of the participants, V5, particularly said he had no trust in AI because ``\textit{it could not be truly creative}''. He therefore became very annoyed with the ChatGPT interface when it did not generate what he expected, saying it was ``\textit{bad usability}'', while attributing ``\textit{good usability}'' to the task management workflow. V7 also expressed concerns for using the conversational interface for brainstorming,
\begin{quote}
    ``\textit{At first sight, it might seem it had generated everything you could think of, but then you'd find many were indeed non-sensical. But I felt I was confined to these generations after reading them. It was especially hard for a novice writer like me to come up with other possibilities. So it felt like it was GPT that was composing a fiction, rather than me.}''
\end{quote}
She later added that her own results from \textit{Polymind} felt more ``\textit{logical}'', and ``\textit{rigorous}''.

Of the participants that said very positively of the ChatGPT interface, V3 stressed that \textit{Polymind} encouraged her to express her own ideas, while ChatGPT did not. That was why she assigned a very low score of expressiveness in the CSI survey when using ChatGPT.