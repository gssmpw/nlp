\section{Discussion}
\subsection{Limitation of rotation prediction task}
\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.1\textwidth}
        \includegraphics[width=\linewidth]{figure/ship_sample.png}
        \caption{"Ship"}
        \label{fig:ship_sample}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_main_likelihood_ship.pdf}
        \caption{Likelihood of all 10 classes over steps}
        \label{fig:all_classes_main_likelihood_hard_ship}
    \end{subfigure}
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_aux_likelihood_ship.pdf}
        \caption{Likelihood of all 4 rotation classes over steps}
        \label{fig:all_class_likelihood_aux_hard_ship}
    \end{subfigure}
    \caption{Likelihood across iteration of a level 5 Gaussian Noise sample in class "Ship"}
    \label{fig:likelihood_ship}
\end{figure}
Figure \ref{fig:likelihood_ship} illustrates the likelihood of different classes across iterations, using a sample (Figure \ref{fig:ship_sample}) from the "Ship" class with a rotation angle of 0 degrees. The sample's features, which include many edge features, allow the model to predict rotation effectively and maintain stability over iterations (Figure \ref{fig:all_class_likelihood_aux_hard_ship}). This stability and accuracy in rotation prediction enable the recurrent model to better estimate the optimal iteration for the main task.

However, predicting the rotation angle becomes more challenging for samples with fewer edge features or isotropic characteristics, such as the sample from the "Cat" class in Figure \ref{fig:cat_sample}. Figure \ref{fig:all_classes_main_likelihood_hard_cat}, \ref{fig:all_class_likelihood_aux_hard_cat} shows that while the likelihood of the "Cat" class continues to increase over iterations and remains significantly higher than other classes, the likelihood of the ground truth rotation (0 degrees) in the self-supervised task is very low . Instead, the model tends to predict the class corresponding to a 270-degree rotation. This behavior negatively impacts the estimation of the optimal iteration for the main task based on the self-supervised task.

We acknowledge this as a limitation of using the rotation prediction task as a self-supervised task.

\subsection{Converge to a fixed point does not ensure to mitigate "overthinking"}

\cite{bansal2022endtoend} highlights the relationship between changes in feature maps across iterations and the issue of "overthinking".
Specifically, they showed that for recurrent models where the norm $\|h_t - h_{t-1}\|$ converges to $0$, it can be assumed that the feature map has converged to a fixed point, and thus the prediction results of iterations beyond this fixed point remain unchanged, effectively addressing "overthinking." 

However, we observe that this assumption is not entirely accurate. Figure \ref{fig:visualize norm} illustrates that the norm $\|h_t - h_{t-1}\|$ of Conv-GRU converges to $0$ after more than $20$ iterations. Nevertheless, Figure \ref{fig:visualize loss} reveals that both the classification loss and self-supervised loss of the model exhibit divergence, corresponding to Conv-GRU encountering "overthinking" on corruption test sets, as shown in Figure \ref{fig:cnn_gru_gn_alpha_0.0}. 

In contrast, Conv-LiGRU demonstrates greater stability. Specifically, not only does the norm $\|h_t - h_{t-1}\|$ converge to $0$ (Figure \ref{fig:visualize norm}), but both the main loss and auxiliary loss also converge smoothly to $0$ (Figure \ref{fig:visualize loss}). Additionally, Figure \ref{fig:cnn_ligru_alpha_0.0} shows that Conv-LiGRU significantly mitigates the "overthinking" phenomenon compared to Conv-GRU. 

In conclusion, we assert that the convergence of $\|h_t - h_{t-1}\|$ alone does not guarantee that the model is free from "overthinking."

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.1\textwidth}
        \includegraphics[width=\linewidth]{figure/cat_sample.png}
        \caption{"Cat"}
        \label{fig:cat_sample}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_main_likelihood_cat.pdf}
        \caption{Likelihood of all 10 classes over steps}
        \label{fig:all_classes_main_likelihood_hard_cat}
    \end{subfigure}
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_aux_likelihood_cat.pdf}
        \caption{Likelihood of all 4 rotation classes over steps}
        \label{fig:all_class_likelihood_aux_hard_cat}
    \end{subfigure}
    \caption{Likelihood across iteration of a level 5 Gaussian Noise sample in class "Cat"}
    \label{fig:likelihood_cat}
\end{figure}

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \includegraphics[width=\linewidth]{figure/visualize_norm.pdf}
        \caption{The $\|h_t - h_{t-1}\|$ across iterations}
        \label{fig:visualize norm}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.55\textwidth}
        \includegraphics[width=\linewidth]{figure/visualize_loss.pdf}
        \caption{The loss value across iterations}
        \label{fig:visualize loss}
    \end{subfigure}
    \caption{\textbf{Left:} The change in norm of feature maps of Conv-GRU and Conv-LiGRU. \textbf{Right:} Loss value across iterations of Conv-GRU and Conv-LiGRU}
    \label{fig:layer_norm}
\end{figure*}
% \begin{algorithm}[H]
% \caption{Incremental Progress Training Algorithm}
% \label{alg:incremental_progress}
% \begin{algorithmic}[1]
% \Require Parameter vector $\theta$, integer $m$, weight $\alpha$
% \For{batch\_idx = 1, 2, \dots}
%     \State Choose $n \sim U\{0, m-1\}$ and $k \sim U\{1, m-n\}$
%     \State Compute $\phi_n$ with $n$ iterations without tracking gradients
%     \State Compute $\hat{y}_{\text{prog}}$ with additional $k$ iterations
%     \State Compute $\hat{y}_m$ with a new forward pass of $m$ iterations
%     \State Compute $\mathcal{L}_{\text{max\_iters}}$ with $\hat{y}_m$
%     \State Compute $\mathcal{L}_{\text{progressive}}$ with $\hat{y}_{\text{prog}}$
%     \State Compute $\mathcal{L} = (1 - \alpha) \cdot \mathcal{L}_{\text{max\_iters}} + \alpha \cdot \mathcal{L}_{\text{progressive}}$
%     \State Compute $\nabla_\theta \mathcal{L}$ and update $\theta$
% \EndFor
% \end{algorithmic}
% \end{algorithm}

% \begin{figure*}[htbp]
%     \centering
%     \begin{subfigure}{0.48\textwidth}
%         \includegraphics[width=\linewidth]{figure/cnn_gru_gn_alpha_0.1.png}
%         \caption{Caption for Graph 2}
%         \label{fig:cnn_gru_gn_alpha_0.1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}{0.48\textwidth}
%         \includegraphics[width=\linewidth]{figure/cnn_ligru_alpha_0.1.png}
%         \caption{Caption for Graph 2}
%         \label{fig:cnn_ligru_alpha_0.1}
%     \end{subfigure}
%     \caption{A horizontal arrangement of three graphs.}
%     \label{fig:alpha0.1}
% \end{figure*}


% To mitigate the problem of overthinking in recurrent models, \cite{bansal2022endtoend} proposed progressive loss (\ref{alg:incremental_progress}). We trained Conv-GRU and Conv-LiGRU using \ref{alg:incremental_progress} with $\alpha = 0.1$. 

% Figures \ref{fig:cnn_gru_gn_alpha_0.1} and \ref{fig:cnn_ligru_alpha_0.1} indicate that progressive loss does not limit overthinking in the Conv-GRU and Conv-LiGRU models and also reduces the accuracy stability between iterations. However, we figure out that the Progressive Loss can buff the accuracy of RNNs (Table 3). 