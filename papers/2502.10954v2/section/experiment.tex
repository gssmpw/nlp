\section{Experiment}
\subsection{Datasets}  
We train our models on the CIFAR10 and CIFAR100 and evaluate them using CIFAR10-C and CIFAR100-C datasets.  

\textbf{CIFAR10 \& CIFAR100} 
are standard benchmarks for image classification. 
CIFAR10 has 50,000 training and 10,000 testing images across 10 classes, while CIFAR100 spans 100 classes with the same dataset size. 
We use both for training and validation.  
\\
\textbf{CIFAR10-C \& CIFAR100-C} 
are corrupted versions of CIFAR10 and CIFAR100. 
They are used to assess models' robustness to distribution shifts, featuring 15 corruption types (e.g., Gaussian noise, blur, contrast) at five severity levels. 
We use them to evaluate generalization under real-world distortions.
\subsection{Models and Training}
\textbf{Model Architecture.} 
\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\linewidth]{figure/visualize_adaptive.pdf}
    \caption{We evaluate the ability of networks to classify objects on two test sets with higher noise severity than those used during training, measuring accuracy on level 3 inputs (purple) and level 5 inputs (red). Recurrent models are compared against the best feed-forward models.}
    \label{fig:visualize-adaptive}
\end{figure}
We kept the Image Transformation and Output Prediction components (Section~\ref{output_pred}) to evaluate extrapolation, varying the Thinking Processing between feed-forward and recurrent architectures.

We used ResNet \citep{he2016residual} with 4 layers, each layer includes 6 convolution blocks, a total of 24 convolutional blocks for the feed-forward model, maintaining equal input-output channels. 
We compare 3 main architectures: Recall architecture, Conv-GRU, and Conv-LiGRU. 
All models used 128 feature channels, and recurrent models were trained for \( T_{\text{train}} = 30 \) iterations, for testing we use the $T_{test} = 100$.  

Training used the Adam optimizer \citep{adam} with a weight decay of 0.0002. 
Datasets were split 80\%/20\% for training and validation. 
Following \citet{aug_cifarc}, Gaussian noise\todo[disable]{what does $\sigma$ means? Is it the variance or the weight of the noise?} \todo[disable]{Fixed} was added to enhance generalization. For self-supervised learning,  each input image was randomly rotated by the function $f_{rotate}$ at one of four angles (0°, 90°, 180°, 270°), so the final augmentation as: 
\begin{equation}
    x = f_{rotate}(clip(x + \delta))
\end{equation}
where $\delta \sim \mathcal{N}(0, \sigma^2 \mathbf{I})$, $\sigma$ is the standard deviation of the Gaussian noise, and $x + \delta$ is clipped to the input range $[0, 1]^N$. We set $\sigma = 0.04$ equivalent to level-1 Gaussian noise corruption in CIFAR10-C.

We trained these models for 200 epochs on CIFAR10 and for 600 epochs on CIFAR100.  

\subsection{Extrapolation Capability of Recurrent Models}
\begin{figure}[t!]   
    \centering
    \includegraphics[width=\linewidth]{figure/bar_char_cnn_ligru_act.pdf}
        \caption{Accuracy (\%) on CIFAR10-C, at level 5, Conv-LiGRU with and without ACT}
        \label{fig:bar_chart_act_ligru}
\end{figure}
Recurrent networks are well-known for their generalization capabilities in logical tasks. 
In our study, we extend this observation to computer vision tasks. 
By training various recurrent models on clean datasets augmented from CIFAR10 and CIFAR100, and testing them on 15 corruption types from CIFAR10-C and CIFAR100-C, we found that recurrent models consistently outperform feed-forward networks in generalization. 

Unlike feed-forward networks, which operate with fixed computation costs, recurrent models adapt dynamically, iterating more when encountering challenging samples with higher corruption levels. 
This "thinking deeper" ability enhances their generalization performance. 
Figure \ref{fig:bar_chart_cifar100c} highlights this, showing that Conv-GRU and Conv-LiGRU outperform ResNet on most corruption test sets.
More results with CIFAR-10C are demonstrated in Appendix~\ref{sec:addition_result}.

Recurrent models also demonstrate remarkable parameter efficiency, as shown in Table \ref{tab:params}, using only one-sixth the parameters of feed-forward networks while achieving superior generalization. 
Figure \ref{fig:visualize-adaptive} further illustrates their adaptability. 
For example, at noise level 3, the model requires just 6 iterations to achieve 60\% accuracy, while at noise level 5, it needs 13 iterations to achieve the same accuracy. 
Notably, a recurrent model with 24 iterations outperforms a feed-forward network with equivalent computational depth, emphasizing the effectiveness of dynamic depth adjustment. 

Beyond adapting computation during inference, recurrent models learn invariant features that are reusable across iterations. 
This synergy of dynamic computation and feature reuse makes them powerful for tackling complex computer vision tasks.

\subsubsection{Iterative Outputs}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{figure/hidden_states.pdf}
    \caption{A "Cat" sample input, and outputs from different iterations are shown to illustrate the model’s sequential reasoning process on CIFAR10-C (level 1). We visualize the norm of vector feature \( h^{(i, j)}_t \) (row \( i \), column \( j \)) of the feature map \( h_t \), demonstrating the model’s feature extraction over iterations. This is a representative example from a Conv-LiGRU model trained on CIFAR10 with \( T_{\text{train}} = 30 \).}
    \label{fig:hidden_states}
\end{figure}

To understand the model's thinking process, we visualize the hidden feature maps \( h_t \) at each iteration. Figure \ref{fig:hidden_states} presents the Gaussian heatmap of the norm vector feature \( h^{(i, j)}_t \) (at row \( i \), column \( j \)) for each feature map \( h_t \). This figure reveals two notable insights. First, the model progressively detects features from local to global, gradually capturing the entire object. Second, it prioritizes identifying key distinguishing features, such as the face and tail, before detecting less critical ones, like the legs. This suggests that the model’s thinking process can be reasoned about, and it exhibits a naive yet intuitive recognition process similar to human perception.

\begin{table}[t!]
\centering
\caption{The number of parameters}
\begin{tabular}{l|cc}
\hline
\textbf{Model} & \textbf{Params} & \textbf{Compress} \\ \hline
Resnet & 7.8M & $1.0\times$\\
Recall & 0.9M & $8.3\times$ \\
Conv-GRU & 1.6M & $4.9\times$ \\
Conv-LiGRU & 1.3M & $6.0\times$ \\ \hline
\end{tabular}
\label{tab:params}
\end{table}

\subsection{Analysis Of The Underthinking Problem}
\todo[disable]{change the problem's name to underthinking}

ACT aims to optimize the iteration count in recurrent models but often limits their reasoning ability. 
Overemphasis on the "ponder cost" can cause RNNs to halt prematurely, effectively reducing them to feedforward networks. 
To examine the impact of adaptive computation, we incorporated the ACT mechanism into Conv-GRU and Conv-LiGRU and compared its performance with our proposed self-supervised accuracy estimation method.  

We trained Conv-GRU model by the ACT method with different values for the hyperparameters $\tau$ and $\epsilon$, where $\tau$ is the weight of the "ponder cost" term in the loss function. A larger $\tau$ encourages the model to minimize the number of "thinking" steps during training. 
Additionally, if the cumulative stopping probability at each "thinking" step exceeds $1 - \epsilon$, the model terminates the thinking process. Therefore, a smaller $\epsilon$ leads to a longer thinking process. 
We provide a more detailed explanation of ACT and the hyperparameters $\tau$ and $\epsilon$ in the Appendix \ref{sec:background}. 
% \todo{a brief discussion of $\tau, \epsilon$ is needed. 2 sentences are fine} \todo{Fixed??}
With $\tau = 0.5$ and $\epsilon = 1\text{e-5}$, the model halted after the third iteration (Figure \ref{fig:cnn_gru_gn_act_0.5}). 
Lowering $\tau$ to $2\text{e-4}$ extended its iterations to 18 (Figure \ref{fig:cnn_gru_gn_act_0.0002}). 
However, applying the same parameters to Conv-LiGRU still led to immediate halting after 2 iteration (Figure \ref{fig:cnn_ligru_act_0.0002}).

This early termination significantly restricts the extrapolation capabilities of recurrent models. 
Instead, we propose allowing models to compute adaptively with an upper limit defined by $T_{test}$ and estimating the optimal iteration ($t_{opt}$) via a self-supervised task. 

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figure/cnn_gru_gn_act_0.5.pdf}
        \caption{Conv-GRU with ACT, $\tau = 0.5$ }
        \label{fig:cnn_gru_gn_act_0.5}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figure/cnn_gru_gn_act_0.0002.pdf}
        \caption{Conv-GRU with ACT, $\tau = 0.0002$ }
        \label{fig:cnn_gru_gn_act_0.0002}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figure/cnn_ligru_act_0.0002.pdf}
        \caption{Conv-LiGRU with $\tau = 0.0002$}
        \label{fig:cnn_ligru_act_0.0002}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figure/dt_net_recall_2d_alpha_0.0.pdf}
        \caption{Recall}
        \label{fig:dt_net_recall_2d_alpha_0.0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figure/cnn_gru_gn_alpha_0.0.pdf}
        \caption{Conv-GRU}
        \label{fig:cnn_gru_gn_alpha_0.0}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \includegraphics[width=\linewidth]{figure/cnn_ligru_alpha_0.0.pdf}
        \caption{Conv-LiGRU}
        \label{fig:cnn_ligru_alpha_0.0}
    \end{subfigure}
    
    \caption{Accuracy across iterations of the diverse models on level 5 of corruption CIFAR10-C test sets.}
    \label{fig:arch_and_hyper}
\end{figure*}

\begin{table}[t!]
\setlength{\tabcolsep}{3.5pt} 
\centering
\caption{The average estimated accuracy and maximum accuracy across all 15 types of corruption at level 5 in the CIFAR10-C dataset. ($\tau_1 = 0.5, \tau_2=0.0002$ are the values of $\tau$ when we apply ACT)}
\begin{tabular}{l|cc|cc}
\hline
\textbf{Model} & \textbf{Est \%} & \textbf{Max \%} & \textbf{Avg $t_{opt}$} & \textbf{Var $t_{opt}$} \\ \hline
resnet & 39.58 & 39.58 & 24.0 & 0.0 \\
recall & 42.76 & 44.43 & 21.0 & 90.0 \\
conv-gru, $\tau_1$ & 49.89 & 49.89 & 3 & 0.0 \\
conv-gru, $\tau_2$ & 50.60 & 50.60 & 17.8 & 0.45 \\
conv-ligru, $\tau_2$ & 50.42 & 50.42 & 2.0 & 0.06 \\
conv-gru & 51.47  & 52.50 & 19.6 & 12.97 \\
conv-ligru & \textbf{52.54} & \textbf{52.89} & 23.6 & 38.11 \\ \hline
\end{tabular}
\label{tab:acc_gap}
\end{table}

Our approach allows recurrent models to "think" freely, leading to notable performance improvements. 
From Table~\ref{tab:acc_gap}, we observe that Conv-GRU and Conv-LiGRU, when not constrained by ACT, tend to take more thinking steps and achieve superior performance compared to when ACT restricts them. Figure~\ref{fig:bar_chart_act_ligru} also demonstrates that the estimation of optimal iterations consistently outperforms the computation constraint by ACT, achieving superior results on almost 15 corruption types in CIFAR10-C.

\subsection{Efficiency of Conv-LiGRU}
\textbf{Conv-LiGRU Mitigates Overthinking.} 
Figure~\ref{fig:cnn_gru_gn_alpha_0.0} shows that Conv-GRU suffers from overthinking on level-5 corruption test sets in CIFAR10-C, affecting both main and auxiliary tasks.

To address this, Conv-LiGRU removes the reset gate to better retain information across iterations. 
This is crucial for handling corrupted data, where feature extraction is more challenging. 
Figure~\ref{fig:cnn_ligru_alpha_0.0} confirms that Conv-LiGRU significantly reduces overthinking across all corruption test sets, offering a robust solution.

While Recall demonstrates resistance to overthinking (Figure \ref{fig:dt_net_recall_2d_alpha_0.0}), it requires maintaining full feature map resolution, leading to higher computational costs. Furthermore, its lack of long-term memory results in inferior performance in CIFAR10-C compared to GRU-based models (Table \ref{tab:acc_gap}).

\textbf{Outstanding Performance of Conv-LiGRU.} Tables \ref{tab:acc_gap} demonstrate that Conv-LiGRU achieves higher estimated and peak accuracy across diverse models on the CIFAR10-C dataset. 

Figure \ref{fig:bar_chart_cifar100c} further illustrates Conv-LiGRU's superiority, outperforming Conv-GRU on 11 out of 15 corruption types at level 5 in CIFAR100-C. With an average accuracy improvement of $0.98\%$ (as shown in Table \ref{tab:acc_avg_cifar100}), these findings underscore Conv-LiGRU's remarkable effectiveness in handling image data and its clear advantage over Conv-GRU.

\subsection{Limitation of rotation prediction task}
\begin{figure}[!t]
    \centering
    \begin{subfigure}{0.1\textwidth}
        \includegraphics[width=\linewidth]{figure/ship_sample.png}
        \caption{"Ship"}
        \label{fig:ship_sample}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_main_likelihood_ship.pdf}
        \caption{Likelihood of all 10 classes over steps}
        \label{fig:all_classes_main_likelihood_hard_ship}
    \end{subfigure}
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_aux_likelihood_ship.pdf}
        \caption{Likelihood of all 4 rotation classes over steps}
        \label{fig:all_class_likelihood_aux_hard_ship}
    \end{subfigure}
    \caption{Likelihood across iteration of a level 5 Gaussian Noise sample in class "Ship"}
    \label{fig:likelihood_ship}
\end{figure}
Figure \ref{fig:likelihood_ship} illustrates the likelihood of different classes across iterations, using a sample (Figure \ref{fig:ship_sample}) from the "Ship" class with a rotation angle of 0 degrees. The sample's features, which include many edge features, allow the model to predict rotation effectively and maintain stability over iterations (Figure \ref{fig:all_class_likelihood_aux_hard_ship}). This stability and accuracy in rotation prediction enable the recurrent model to better estimate the optimal iteration for the main task.

However, predicting the rotation angle becomes more challenging for samples with fewer edge features or isotropic characteristics, such as the sample from the "Cat" class in Figure \ref{fig:cat_sample}. Figure \ref{fig:all_classes_main_likelihood_hard_cat}, \ref{fig:all_class_likelihood_aux_hard_cat} shows that while the likelihood of the "Cat" class continues to increase over iterations and remains significantly higher than other classes, the likelihood of the ground truth rotation (0 degrees) in the self-supervised task is very low. Instead, the model tends to predict the class corresponding to a 270-degree rotation. This behavior negatively impacts the estimation of the optimal iteration for the main task based on the self-supervised task.

We acknowledge this as a limitation of using the rotation prediction task as a self-supervised task.

\subsection{Converge to a fixed point does not ensure to mitigate "overthinking"}
\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.08\textwidth}
        \includegraphics[width=\linewidth]{figure/cat_sample.png}
        \caption{"Cat"}
        \label{fig:cat_sample}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_main_likelihood_cat.pdf}
        \caption{Likelihood of all 10 classes over steps}
        \label{fig:all_classes_main_likelihood_hard_cat}
    \end{subfigure}
    \begin{subfigure}{0.22\textwidth}
        \includegraphics[width=\linewidth]{figure/all_classes_aux_likelihood_cat.pdf}
        \caption{Likelihood of all 4 rotation classes over steps}
        \label{fig:all_class_likelihood_aux_hard_cat}
    \end{subfigure}
    \caption{Likelihood across iteration of a level 5 Gaussian Noise sample in class "Cat"}
    \label{fig:likelihood_cat}
\end{figure}

\begin{table}[t!]
\centering
\caption{The average estimated accuracy, and maximum accuracy across all 15 types of corruption at level 5 in the CIFAR100-C dataset.}
\begin{tabular}{l|ccc}
\hline
\textbf{Model} & \textbf{Est \%} & \textbf{Max \%} & \textbf{Gap \%} \\ \hline
Resnet & 17.56 & 17.56 & 0 \\
Conv-GRU & 23.27  & 23.78 & -0.51 \\
Conv-LiGRU & \textbf{24.25} & \textbf{24.99} & -0.74 \\ \hline
\end{tabular}
\label{tab:acc_avg_cifar100}
\end{table}

\cite{bansal2022endtoend} highlights the relationship between changes in feature maps across iterations and the issue of "overthinking".
% Specifically, they showed that for deep thinking models where $\|h_t - h_{t-1}\|_2$ converges to $0$, it can be assumed that the feature map has converged to a fixed point, and thus the prediction results of iterations beyond this fixed point remain unchanged, effectively addressing "overthinking." 
they demonstrate that if \(\|h_t-h_{t-1}\|_2\) converges to $0$ in deep thinking models, the feature map reaches a fixed point, making later predictions unchanged and mitigating "overthinking."

However, we observe that this assumption is not entirely accurate. Figure \ref{fig:visualize norm} illustrates that $\|h_t - h_{t-1}\|_2$ of Conv-GRU converges to $0$ after more than $20$ iterations. 
Nevertheless, Figure \ref{fig:visualize loss} reveals that both the classification loss and self-supervised loss of the model exhibit divergence, corresponding to Conv-GRU encountering "overthinking" on corruption test sets, as shown in Figure \ref{fig:cnn_gru_gn_alpha_0.0}. 

In contrast, Conv-LiGRU demonstrates greater stability. Specifically, not only does $\|h_t - h_{t-1}\|_2$ converge to $0$ (Figure \ref{fig:visualize norm}), but both the main loss and auxiliary loss also converge smoothly to $0$ (Figure \ref{fig:visualize loss}). Additionally, Figure \ref{fig:cnn_ligru_alpha_0.0} shows that Conv-LiGRU significantly mitigates the "overthinking" phenomenon compared to Conv-GRU. 

In conclusion, the convergence of $\|h_t - h_{t-1}\|_2$ does not ensure that the model is free from "overthinking."

\begin{figure}[t!]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{figure/visualize_norm.pdf}
        \caption{The $\|h_t - h_{t-1}\|_2$ across iterations}
        \label{fig:visualize norm}
    \end{subfigure}
    \vfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{figure/visualize_loss.pdf}
        \caption{The loss value across iterations}
        \label{fig:visualize loss}
    \end{subfigure}
    \caption{\textbf{Left:} The change in norm of feature maps of Conv-GRU and Conv-LiGRU. \textbf{Right:} Loss value across iterations of Conv-GRU and Conv-LiGRU}
    \label{fig:layer_norm}
\end{figure}