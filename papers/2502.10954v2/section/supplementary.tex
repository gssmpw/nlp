\onecolumn
\title{Learning to Stop Overthinking at Test Time\\(Supplementary Material)}
\maketitle

\appendix

\section{Background: Adaptive Computation Time (ACT)}
\label{sec:background}
ACT \cite{Graves2016AdaptiveComutationTime} is a mechanism designed to dynamically determine the number of recurrent steps required to process each input. Unlike its original formulation, which handles variable-length sequences, our work applies ACT to static inputs in visual reasoning tasks.  

At each time step, the model generates a halting score $p_t$ through a learned convolutional layer. The cumulative sum of these scores, $P_t$, determines whether the computation should continue. When $P_t$ reaches a predefined threshold $(1 - \epsilon)$, iteration stops, and the final hidden state is computed as a weighted sum of the intermediate states.  

A key component of ACT is the ponder cost, an auxiliary loss term that encourages the model to minimize the number of recurrent steps while maintaining accuracy. The total loss function consists of the task loss $L_{task}(y, \hat{y}_{act})$ and the ponder cost, weighted by a hyperparameter $\tau$ (Equal \ref{eq:act_loss}). By tuning $\tau$, we control the trade-off between computational efficiency and performance. In our study, we analyze the limitations of ACTâ€™s early stopping heuristic and propose a self-supervised approach to better estimate the optimal number of iterations.  
\begin{equation}
    \mathcal{L} = \sum_{i=0}^{|\mathcal{D}|} \frac{1}{|\mathcal{D}|} L_{task}(y^i , \hat{y}^i_{\text{act}}) - \tau \sum_{t=1}^{t^i_{\text{halt}} - 1} p^i_t
    \label{eq:act_loss}
\end{equation}


\section{Additional simulation results}
\label{sec:addition_result}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figure/bar_chart_cifar10c.pdf}
        \caption{Accuracy (\%) on CIFAR10-C, at level 5, Resnet, Conv-GRU, Conv-LiGRU}
        \label{fig:bar_chart_cifar10c}
\end{figure*}

% Figure~\ref{fig:bar_chart_cifar10c} demonstrated blabla...
% Figure~\ref{fig:bar_chart_act} demonstrated blabla...
% \subsection{Additional Results on the Common Corruptions Dataset}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figure/bar_char_cnn_gru_gn_act.pdf}
        \caption{Accuracy (\%) on CIFAR10-C, at level 5, Conv-GRU with and without ACT}
        \label{fig:bar_chart_act}
\end{figure*}

\begin{table*}[t]
\centering
\caption{Test accuracy (\%) on CIFAR10-C, level 5}
\begin{tabular}{l|c|c|c|c|c|c|c}
\toprule
Corruptions & resnet & recall & cnn gru act 0.5 & cnn gru act 0.0002 & cnn gru & cnn ligru act 0.0002 & cnn ligru \\
\midrule
Gauss & 58.24 & 53.03 & 59.86 & 63.44 & 65.80 & 59.85 & 63.50 \\
Shot & 57.42 & 55.32 & 60.88 & 65.13 & 67.45 & 61.61 & 64.71 \\
Impul & 42.81 & 23.80 & 42.43 & 48.91 & 47.10 & 41.97 & 48.25 \\
Defoc & 28.55 & 28.83 & 37.96 & 37.50 & 35.36 & 41.17 & 39.61 \\
Glass & 39.53 & 42.78 & 53.30 & 54.21 & 53.78 & 50.24 & 54.26 \\
Motn & 28.19 & 28.14 & 38.70 & 36.36 & 35.81 & 40.97 & 40.97 \\
Zoom & 29.82 & 34.34 & 42.70 & 41.66 & 43.08 & 46.72 & 45.47 \\
Snow & 49.12 & 58.34 & 63.52 & 63.92 & 67.45 & 64.99 & 66.86 \\
Frost & 46.09 & 51.84 & 59.56 & 63.48 & 65.22 & 62.10 & 66.47 \\
Fog & 22.23 & 28.40 & 35.42 & 37.43 & 41.28 & 32.54 & 32.66 \\
Brit & 50.13 & 65.76 & 66.06 & 68.28 & 69.14 & 69.81 & 73.51 \\
Contr & 15.28 & 15.72 & 18.86 & 15.45 & 11.32 & 15.91 & 14.55 \\
Elast & 36.54 & 47.78 & 52.63 & 52.20 & 52.31 & 53.39 & 55.86 \\
Pixel & 41.27 & 28.89 & 51.30 & 43.57 & 48.16 & 45.68 & 51.82 \\
JPEG & 48.50 & 61.05 & 65.12 & 64.76 & 68.81 & 68.37 & 69.73 \\
\bottomrule
\end{tabular}
\label{tab:acc_all}
\end{table*}

\begin{table*}[t]
\setlength{\tabcolsep}{3pt} 
\centering
\caption{Test accuracy (\%) on CIFAR100-C, level 5, ResNet, Conv-GRU, Conv-LiGRU}
\begin{tabular}{l|ccccccccccccccc}
\toprule
& gauss & shot & impul & defoc & glass & motn & zoom & snow & frost & fog & brit & contr & elast & pixel & jpeg \\
\midrule
rn & 22.52 & 23.01 & 9.74 & 7.47 & 12.38 & 7.87 & 8.92 & 24.97 & 12.13 & 4.24 & 27.18 & 2.49 & 11.29 & 11.36 & 14.13 \\
cg & 26.02 & 25.85 & 10.73 & 16.64 & 23.00 & 18.70 & 20.00 & 35.26 & 32.01 & 10.88 & 39.61 & 4.41 & 31.28 & 19.74 & 34.95 \\
clig & 29.16 & 30.48 & 10.26 & 17.69 & 23.44 & 19.97 & 20.70 & 34.92 & 32.85 & 10.39 & 36.97 & 4.56 & 29.63 & 23.07 & 37.23 \\
\bottomrule
\end{tabular}
\label{tab:acc_cifar100}
\end{table*}

Figure \ref{fig:bar_chart_cifar10c} compares the performance of feedforward (ResNet) and recurrent architectures (Conv-GRU, Conv-LiGRU) in deep thinking networks on 15 corruption types at level 5 from the CIFAR10-C test set. The results show that recurrent models outperform feedforward networks in 14 out of 15 corruption types. Detailed accuracy values are provided in Table \ref{tab:acc_all}.

Additionally, we compare feedforward and recurrent models on CIFAR100-C, with Table \ref{tab:acc_cifar100} listing the accuracy of ResNet, Conv-GRU, and Conv-LiGRU on 15 corruption types at level 5. Both tables confirm the superiority of recurrent models over feedforward networks. Furthermore, Conv-LiGRU surpasses Conv-GRU on most test sets (11 out of 15 on CIFAR10-C and 12 out of 15 on CIFAR100-C), demonstrating its effectiveness and suitability for deep thinking models.

Figure \ref{fig:bar_chart_act} compares the performance of Conv-LiGRU with and without ACT. The results show that without ACT, Conv-LiGRU outperforms the ACT variant on 9 out of 15 level 5 test sets of CIFAR10-C. Detailed accuracy values for each test set are provided in Table \ref{tab:acc_all}. These findings reinforce that deep thinking models can achieve better performance when allowed to think freely rather than being constrained.

% In the framework of \citet{bansal2022endtoend}, Progressive Loss helps mitigate the "forgetting" phenomenon by enforcing consistency between deep and shallow reasoning outputs. 
% However, we found that introducing Progressive Loss during training in our setup destabilizes reasoning performance. 
% We demonstrate this in Section~\ref{}.
% \textcolor{red}{Add the progressive experiment in ablation study.}