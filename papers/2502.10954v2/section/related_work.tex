\section{Related Works}
\label{sec:relatedwork}

\textbf{Thinking in Language vs. Latent Space:} 
Recent studies, most notably OpenAI-O1 \cite{jaech2024openai}, have demonstrated that large language models (LLMs) can handle more complex tasks by thinking longer before answering in natural language.  
While it brings large language models closer to human-like thinking, this approach activates all layers at any time. 
Furthermore, its performance depends largely on the thought sequence's length, making it challenging to perform more difficult tasks with fewer computations.
Recently, Deepseek-R1 \cite{guo2025deepseek} has utilized a fraction of the large model for inference, allowing adaptation to edge devices. 
In this study, we focus on implicit reasoning embedded within recurrent models. 
Reasoning in the latent space allows models to synthesize and refine instance interpretability, akin to a conventional deep model.
\citet{xu2024lars} extends the latent reasoning to In-Context Learning, showing greater robustness with a faster inference time. 
As opposed to CoT reasoning, \citet{hao2024training} demonstrates that the continuous space can represent multiple alternative reasoning steps, thereby significantly expanding the model's search space.

% \todo{Fixed??}\todo{More about the benefits of latent reasoning models. One approach can benefit the other}

\textbf{Test-time Training (TTT):} 
Another line of work coinciding with ours is learning at test time by directly updating model parameters on test data without supervision. 
Previous work has shown that TTT is robust to distribution shifts \citep{sun2020test,gandelsman2022test}, while \citet{muennighoff2025s1simpletesttimescaling} shows that a simple test-time training can beat OpenAI-O1 on the math question.
The connection between the RNN update mechanisms, the attention mechanisms, and the TTT has been highlighted in \citet{sun2024learning}, therefore a new TTT layer is proposed for the generation of long sequences. 
Our study leverages self-supervised tasks to estimate the optimal reasoning depth, improving the model's computational efficiency and reasoning performance.

\textbf{Deep Thinking:} 
\citet{schwarzschild2021can} demonstrated that recurrent models trained on simple tasks can generalize to harder ones simply by repeating a set of layers more times during testing. 
However, \citet{bansal2022endtoend} identified the "overthinking" problem, where longer inference time leads to worse performance.
To mitigate this issue, they proposed Recall architecture and Progressive Loss. 
The "Recall" architecture adds residual connections from the original input to every recurrent input, effectively preventing ``overthinking'' due to vanishing gradient.
Progressive Loss forces the outputs across iterations to be consistent, allowing the hidden representations to converge to a fixed point after \textit{some iterations}. 
\todo[disable]{should be more on the point: DT+RC models prevent overthinking with residual connections and forcing the hidden states to converge to fixed points}
\todo[disable]{Fixed??}
\citet{bansal2022endtoend} used a very large number of steps for all difficulty levels to ensure that the representations converged because they could not determine the optimal number of steps.
To address this, \citet{veerabadran2023Adaptive,Ballas2016Iclr} applied ACT \cite{Graves2016AdaptiveComutationTime}, using a sigmoidal unit to decide when to stop iterating. 
Since stopping probabilities cannot be predetermined, ACT added a "ponder cost" to encourage early termination. 
Building on this, \citet{PonderNet} restricted the stopping probabilities to a predefined prior, allowing control over the termination point by adjusting the prior.

\citet{schwarzschild2021can, bansal2022endtoend,veerabadran2023Adaptive} conducted experiments on logical tasks such as prefix sum, maze solving, chess, and pathfinding, demonstrating the logical extrapolation capabilities of recurrent models. 
However, the extrapolation ability of recurrent models has yet to be explored in classical computer vision tasks like object recognition.
\citet{cifarC} developed corruption datasets CIFAR10-C, CIFAR100-C, and ImageNetC corruption datasets to generalize the model by introducing 15 types of corruption at five different severity levels.
Noting the similarity between the experimental setup and the logical tasks, we show that recurrent models trained on lower noise level data can achieve robustness on harder noise levels. 
We further demonstrate that recurrent models can exhibit extrapolation capabilities in object recognition tasks, mirroring human perception. Under ideal, noise-free conditions, recognition is swift and effortless, whereas challenging conditions demand additional time for pattern identification.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/act-pipeline.pdf}
    \caption{Deep thinking pipeline with training includes both main (classification) and secondary (self-supervised) tasks. 
    During inference, the iteration with peak self-supervised performance is selected for the halting of the classification task. 
    The reset gate in Conv-GRU is simplified and $\phi$ is replaced from Tanh to ReLu, resulting in Conv-LiGRU.}
    \label{fig:enter-label}
\end{figure}