\section{Methodology}
\subsection{Deep Thinking Model Overview}
% \todo{Add background information about deep thinking} \todo{Fixed??}
We study the deep thinking network that processes input images $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$ in three main stages to explore the behavior of recurrent models under adaptive computation and extrapolation:

\textbf{Input Transformation}: The input image $\mathbf{X}$ is transformed via a convolutional layer, producing the initial state $\mathbf{h}_0$:
   \begin{equation}
       \mathbf{h}_0 = \sigma(\mathbf{W}_\text{in} \ast \mathbf{X} + \mathbf{b}_\text{in}),
   \end{equation}
where \( \mathbf{W}_\text{in} \) and \( \mathbf{b}_\text{in} \) are the weights and bias of the convolution, \( \ast \) denotes the convolution operator, and \( \sigma(\cdot) \) is an activation function such as ReLU.
\label{input_transform}


\textbf{Thinking Processing}: The thinking process employs a recurrent layer with identical input and output shapes, allowing the model to iteratively apply this layer multiple times, similar to an RNN.
With this architecture, the model can adjust the number of iterations based on the complexity of the task, a process we refer to as "thinking". We formulate this architecture as follows:
\begin{equation}
   % \mathbf{h}_t = f_\text{rec}(\mathbf{h}_{t-1}, \mathbf{x}_t), \quad t = 1, \dots, T_\text{train},
   \mathbf{h}_t = f_\text{rec}(\mathbf{h}_{t-1}), \quad t = 1, \dots, T_\text{train}
\end{equation}
where \( f_\text{rec}(\cdot) \) represents the recurrent function, and $T_{train}$ is
the maximum number of "thinking" steps during training. 

To ensure that the model does not "forget" the initial input, \citet{bansal2022endtoend} proposed Recall, which concatenates the input \( X \) with \( \textbf{h}_{t-1} \) at each "thinking" step, resulting in \( \textbf{h}_t = f_{\text{rec}}([\textbf{h}_{t-1}, X]) \). 
However, this approach requires the hidden feature map size to match the input \( X \), increasing computational complexity. 
Instead, we integrate \( \textbf{h}_{t-1} \) and \( \textbf{h}_0 \) at each iteration, with \( f_{\text{rec}} \) designed as a recurrent unit, leading to \( \textbf{h}_t = f_{\text{rec}}(\textbf{h}_{t-1}, \textbf{h}_0) \). 
The hidden feature map \( \textbf{h}_i \) can be downsampled compared to \( X \), reducing computational complexity compared to Recall. 
In this study, we experiment with different architectures for \( f_{\text{rec}} \) and propose a novel design introduced in Section~\ref{subsec:arch}.

%, the initial state \( \mathbf{h}_0 \) is iteratively updated in \( T_\text{train} \) iterations during training, with $\mathbf{x}_t$ is the income signal at iteration $t$.
% In language modeling, $T_{\text{train}}$ is commonly demonstrated as the length of the sentence, while the hidden states $\mathbf{h}_t$ capture the representation features of each word in the sentence.

% Following \citet{bansal2022endtoend}, $f_{\text{rec}}$ is defined as an implicit function representing the reasoning process, with $T_{\text{train}}$ now interpreted as the budget or the maximum reasoning steps. 
% \begin{equation}
%    \mathbf{h}_t = f_\text{rec}(\mathbf{h}_{t-1},\mathbf{h}_0), \quad t = 1, \dots, T_\text{train},
% \end{equation}

% They introduce\todo{change this word} "initial state" information $h_0$ through a residual connection, referred to as a "Recall" architecture. 
% This input incorporation ensures that the output remains relevant to the function's input, even with deep reasoning steps.
% We describe the architecture selection for the recurrent block in Section~\ref{subsec:arch}.

\textbf{Output Prediction}: The output at each iteration \( \hat{y}_t \) is generated by applying a readout function:
   \begin{equation}
       \hat{\mathbf{y}}_t = f_\text{out}(\mathbf{h}_t),
   \end{equation}
   where \( f_\text{out}(\cdot) \) maps the recurrent state to the desired task output.
\label{output_pred}

By constraining a consistent target at each iteration, these stages enable us to examine the effect of varying the number of iterations $t$ on the model's performance, highlighting the adaptability and extrapolation capacity of recurrent models.

\subsection{Accuracy-Iteration Relationship Estimation}

Since ground-truth labels are unavailable during testing, assessing the performance on the main task ($\mathcal{T}_\text{main}$) across iterations ($t$) is nontrivial. 
To address this, we introduce a simple self-supervised auxiliary task ($\mathcal{T}_\text{aux}$) as a proxy, leveraging its strong correlation with $\mathcal{T}_\text{main}$ to estimate main task's performance.
The auxiliary task should satisfy the following assumptions: 

\textbf{Core Assumptions}:  
1. \( \mathcal{T}_\text{aux} \) shares semantic and structural similarities with \( \mathcal{T}_\text{main} \), ensuring a positive correlation between their accuracies over \( t \):  
   \[
   \textnormal{corr}\left(\text{Accuracy}_{\mathcal{T}_\text{aux}}(t), \text{Accuracy}_{\mathcal{T}_\text{main}}(t)\right) > 0 \ \forall\ t
   \]
2. The difficulty of \( \mathcal{T}_\text{aux} \) positively correlates with \( \mathcal{T}_\text{main} \) under both in-distribution (ID) and out-of-distribution (OOD) conditions. 
% \todo{Fixed??}
% \todo{Very good but why do we choose rotation loss?}

\textbf{Auxiliary Task Design}: 
We use the rotation prediction task \cite{Balaji2018MetaRegTD} as \( \mathcal{T}_\text{aux} \), a simple and effective pretext task that allows the model to learn features that are useful for many downstream tasks. 
Furthermore, we hypothesize that similar to the recognition task, rotation prediction becomes more difficult under more challenging conditions. 
The input image \( X \) is rotated by one of four angles \( \{0^\circ, 90^\circ, 180^\circ, 270^\circ\} \), and the model predicts the rotation angle as a four-class classification problem.
The auxiliary loss with $t$ iteration is:
\[
\mathcal{L}_t^{\text{aux}} = -\sum_{k=1}^{4} \mathbf{y}_t^{\text{a}}[\text{k}] \log \hat{\mathbf{y}}_t^{\text{a}}[\text{k}],
\]
where $\mathbf{y}_t^{\text{a}}[\text{k}]$ is the probability for each angle.

During training, at each update, we use the output of the last iteration $T_{train}$ to calculate the loss function. 
Combining with the main task loss \( \mathcal{L}_{T_{train}}^\text{main} \), which is the cross-entropy loss in classification, we obtain the total training loss:
\begin{align}
    \mathcal{L} = \mathcal{L}_{T_{train}}^\text{main} + \mathcal{L}_{T_{train}}^\text{aux}.
\end{align}

\textbf{Iteration Search for Main Task Improvement}:  
During testing, \( \text{Accuracy}_{\mathcal{T}_\text{aux}}(t) \) is used to estimate the optimal iteration \( t_\text{opt} \) for \( \mathcal{T}_\text{main} \). 
Given a fixed budget $T_{\text{test}}$,
% This proportionality holds even on OOD data, enabling the model to determine the optimal iteration \( t_\text{opt} \) for \( \mathcal{T}_\text{main} \):
\begin{align}
    t_\text{opt} = \underset{{t\in[T_{\text{test}}]}}{\arg\max} \big(\text{Accuracy}_{\mathcal{T}_\text{aux}}(t)\big).
\end{align}
Hence, comparing to the theoretical optimal accuracy given $t_*$, i.e., $A_*=\text{Accuracy}_{\mathcal{T}_\text{train}}(t_*)$, our hypothesis estimates the optimal accuracy as
\begin{align}
    \hat{A}_* = \text{Accuracy}_{\mathcal{T}_\text{main}}(t_\text{opt}).
\end{align}

% We assume:
% \[
% \text{Accuracy}_{\mathcal{T}_\text{main}}(t_\text{opt}) \approx \max_{t} \text{Accuracy}_{\mathcal{T}_\text{main}}(t).
% \]
This framework offers a scalable and adaptive method for estimating the accuracy of the main task and optimizing performance efficiently in diverse test scenarios.
The inference pipeline is described at Algorithm \ref{alg:estimate_acc}.

\subsection{Conv-LiGRU}
\label{subsec:arch}
\begin{algorithm}[t!]
\caption{Testing Phase: Accuracy-Iteration Relationship Estimation}
\begin{algorithmic}[1]
\State \textbf{Input:} Trained model, test data with $D$ batches, maximum iterations $T_\text{test}$
\State \textbf{Output:} Optimal iteration $t_\text{opt}$ 
% and estimated $\text{Accuracy}_{\mathcal{T}_\text{main}}(t_\text{opt})$

\Statex
\State \textbf{Step 1: Initialize variables}
\State Initialize \texttt{Correct} as a zero array of length $T_\text{test}$
\Statex

\State \textbf{Step 2: Compute $\text{Accuracy}_{\mathcal{T}_\text{aux}}(t)$ during testing}
\For{$i = 1, 2, \dots, D$} \Comment{Iterate over $D$ test batches}
    \State $h^{(0)} \gets \text{InputTransformation}(\text{Batch}_i)$
    \For{$t = 1, 2, \dots, T_\text{test}$} \Comment{Iterate over $T_\text{test}$ steps}
        \State $h_t \gets f_\text{rec}(h_{t-1}, h_0)$ \Comment{Recurrent computation}
        \State $\hat{y}_t^{\text{main}}, \hat{y}_t^{\text{aux}} \gets f_\text{out}(h_t)$ \Comment{Output predictions}
        \State $\texttt{Correct}[t] \mathrel{+}=  \text{\# Correct auxiliary samples}$
    \EndFor
\EndFor
\State $\text{Accuracy}_{\mathcal{T}_\text{aux}}(t) \gets \texttt{Correct}[t] / \text{TotalSamples}$
\Statex

\State \textbf{Step 3: Estimate optimal iteration}
\State $t_\text{opt} \gets \arg\max_t \big(\text{Accuracy}_{\mathcal{T}_\text{aux}}(t)\big)$
% \State $\text{EstimatedAccuracy}_{\mathcal{T}_\text{main}} \gets \text{Accuracy}_{\mathcal{T}_\text{aux}}(t_\text{opt})$

\Statex
\State \textbf{Return:} $t_\text{opt}$
% , $\text{EstimatedAccuracy}_{\mathcal{T}_\text{main}}$
\end{algorithmic}
\label{alg:estimate_acc}
\end{algorithm}

Existing research has explored various recurrent architectures for image input, including Recursive Convolutional Networks \cite{recursiveCnn2014lecun}, GRU-based models \cite{Ballas2016Iclr}, and Deep Thinking Recall. 
However, these architectures often fail to achieve stable accuracy across iterations or miss the long term memory.

In this study, we propose a novel GRU-based model, Conv-LiGRU, inspired by the light-gated recurrent unit (LiGRU) \cite{ligru2018}, which simplifies the GRU by removing the reset gate, replacing tanh with ReLU activation, and applying batch normalization. 

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/bar_chart_cifar100c.pdf}
        \caption{Accuracy (\%) on CIFAR100-C, at level 5, Resnet, Conv-GRU, Conv-LiGRU}
        \label{fig:bar_chart_cifar100c}
\end{figure*}

Key features of Conv-LiGRU include:

\textbf{Removal of the Reset Gate}: By eliminating the reset gate, Conv-LiGRU streamlines the gating mechanism, reducing the number of parameters, and improving computational efficiency. LiGRU was originally designed for audio data, where the authors argued that the reset gate in GRU might disrupt intermediate features, particularly for continuous data like audio. 
Similarly, the reasoning process consists of a sequence of thinking steps, where skipping even a single step can lead to incorrect conclusions. Therefore, removing the reset gate is a reasonable choice to ensure a stable flow of information and a consistent, uninterrupted thinking process.

% \todo{Reasoning is a chain of steps, skipping one step likely causes errors.} \todo{Fixed??}

\textbf{Normalization and Activation Function}: Similar to LiGRU, Conv-LiGRU replaces \( \tanh \) with ReLU to mitigate the vanishing gradient problem and enhance the model's ability to capture long-range dependencies. Additionally, instead of layer normalization, Conv-LiGRU employs batch normalization \cite{he2016residual} to ensure stable performance during iterative computations and to better suit image-based data and convolution operations.

\textbf{Convolutional State Transitions}: 
Conv-LiGRU adapts LiGRU for image tasks by replacing fully connected layers with convolutions, preserving spatial structure and enhancing performance.

The state update equation for Conv-LiGRU is defined as:
   \begin{equation}
       \textbf{h}_t = \textbf{z}_t \odot \tilde{\textbf{h}}_t + (1 - \textbf{z}_t) \odot \textbf{h}_{t-1},
   \end{equation}
   \begin{equation}
       \textbf{z}_t = \sigma(U_z \ast \textbf{h}_{t-1} + BN(W_z \ast \textbf{h}_0)),
   \end{equation}
   \begin{equation}
       \tilde{\textbf{h}}_t = RELU(BN(W_h \ast \textbf{h}_0) + U_h \ast \textbf{h}_{t-1})
   \end{equation}
where \( z_t \) is the update gate, \( \tilde{h}_t \) is the candidate state, and \( \odot \) denotes element-wise multiplication, \( \ast \) denotes the convolution operation, and \( W_z \), \( W_h \), \( U_z \), and \( U_h \) are learnable parameters, $RELU$, $BN$ are relu activation function and bach normalize layer perspective.

These enhancements allow Conv-LiGRU to maintain stability and achieve robust performance across ID and OOD scenarios. 
Reduced computational complexity and enhanced stability make Conv-LiGRU highly effective for iterative image-based tasks.

