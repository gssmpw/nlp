\section{Related work}
\label{re}
% 每部分需要标注多个参考文献,即使没有细节它们.参考文献尽可能的新
For poisoning and gradient inversion attacks, current defense strategies mainly include the following three ideas.  

\textbf{Anomaly detection-based methods}. They defend against poisoning attacks by identifying and excluding the gradient updates of malicious clients. However, they are often limited by filtering strategies and data distribution assumptions. In term of filtering strategies, the similarity of gradients from different clients can be leveraged to filter a small number of malicious clients, but it is susceptible to collusion attacks. Fung et al., "Anomaly Detection-Based Methods for Secure Federated Learning" employ the KMeans method to achieve filtering, but this method has high computational complexity, and malicious clients can circumvent defenses by submitting multiple backdoored samples. Auror method, "Auror: An Efficient Defense Against Poisoning Attacks in Federated Learning" runs solely on the server and has a lower performance overhead, but malicious clients can exploit the updates diversity to enhance the collusion attacks. In term of data distribution assumptions, existing methods often rely on specific assumptions. Andreina et al., "Privacy-Preserving Federated Learning with Anomaly Detection" and Cao et al., "Federated Learning with Data Distribution Aware Anomaly Detection" assume that the server has access to clients' training data, which violates client's data privacy. Variations in data distribution (such as IID or non-IID) can cause differences in the clients' gradients. FLAIRS, "FLAIRS: A Framework for Federated Learning with Anomaly Detection and Robustness" and FreqFed, "FreqFed: Frequency-Based Anomaly Detection for Secure Federated Learning" identify the abnormal gradients of malicious clients through anomaly detection algorithms (e.g., cosine distance and HDBSCAN), where these gradients are assumed to contain obvious outliers. 

\textbf{Differential privacy-based methods}. Noise is added to the clients' gradients to prevent attackers from obtaining precise gradients, thereby avoiding the data reconstruction and mitigating negative impact on the performance of global model. Compared to anomaly detection methods, DP considers the risk of data leakage during gradient transmission. However, since the server cannot access clients' training data, it cannot accurately analyze and eliminate the effects of noise in gradient aggregation, which degrades the performance of global model. Naseri et al., "Collaborative Differential Privacy for Secure Federated Learning" implement a collaboration between local and central DP, where clients add the noise to gradients while the server uses DP aggregation algorithms for gradient aggregation. As the number of malicious clients increases, it is more challenging for the server to analyze the noise. Based on the parameter clipping and Gaussian noise adding, McMahan et al., "Auction-Based Differential Privacy in Federated Learning" modify the gradients to limit gradient sizes and protect data privacy, which has expensive computational costs. FLAME, "FLAME: A Framework for Federated Learning with Anomaly Detection, Model Clipping, and Noise Addition" combines the filtering of outlier detection, model clipping, and noise addition, but its privacy guarantees are only applicable to semi-honest server that adheres to the Secure Multi-Party Computation (SMPC) protocol.

\textbf{Secure Aggregation-Based Methods}. The goal is to achieve that the performance of global model is not significantly affected, even if some clients upload incorrect gradients. The server will not detect malicious clients during gradient aggregation. Instead, it employs specific aggregation strategies to enhance the robustness of global model. Multi-Krum method, "Multi-Krum: A Robust Federated Learning Algorithm" repeatedly removes these gradient updates that are far from the geometric median of all updates. Trimmed Mean method, "Trimmed Mean: A Secure Aggregation-Based Method for Federated Learning" removes the maximum / minimum updates and computes the average of the remaining values. Although these methods mitigate the impact of incorrect gradients to some extent, the performance of global model still significantly declines as the number of attackers increases. Pasquini et al., "The Limits of Secure Aggregation in Federated Learning" claim that current secure aggregation-based FL achieves a ``false sense of security", i.e., it merely addresses superficial privacy protection without defending against client-side attacks. To achieve privacy protection and model correctness, Chowdhury et al., "Secure Model Training in Federated Learning with Shamir Threshold Secret Sharing" split and aggregate the clients' gradients through the Shamir threshold secret sharing scheme and non-interactive proofs. This method has high computational complexity, making it challenging to balance the model robustness with the computational efficiency.

Overall, existing methods mitigate gradient inversion attacks and poisoning attacks, but they exhibit the limitations in complex attack scenarios and rarely address both security issues simultaneously. Against this background, the SMTFL offers a novel solution. Our proposed method not only effectively counters both gradient inversion attacks and poisoning attacks but also demonstrates outstanding robustness and stability in complex attack scenarios. Our method enables secure model training in FL involving untrusted participants, providing new perspectives and insights for addressing similar challenges.