\section{Experimental Setup}

In this section, we discuss the experimental setup for validating our proposed method, \method{}. We check the feasibility of \method{} using \textbf{UltraFeedback}~\cite{cui2023ultrafeedback}, compared to the diverse direct alignment algorithms~\cite{rafailov2023direct, yuan2023rrhf, zhao2023slic, azar2024general, xu2024contrastive, ethayarajh2024kto, hong2024orpo, park2024disentangling, meng2024simpo} as a method for general chatbot alignment. We also use \textbf{Anthropic-HH}~\cite{bai2022training} for a detailed comparison with existing methods for KL penalty relaxation of DPO~\cite{wu2024beta, gorbatovski2024learn}. The implementation details for each experimental setting are in \Cref{app:implementation_details}.

\subsection{UltraFeedback}

UltraFeedback~\cite{cui2023ultrafeedback} is an AI feedback dataset where GPT-4~\cite{achiam2023gpt} rates responses obtained from four different language models. We follow the experimental setting of SimPO~\cite{meng2024simpo} for comparison with various direct alignment algorithms, including DPO~\cite{rafailov2023direct}, RRHF~\cite{yuan2023rrhf}, SLiC-HF~\cite{zhao2023slic}, IPO~\cite{azar2024general}, CPO~\cite{xu2024contrastive}, KTO~\cite{ethayarajh2024kto}, ORPO~\cite{hong2024orpo}, and R-DPO~\cite{park2024disentangling}. Specifically, we use the \texttt{Instruct} setting starting from instruction-tuned language models~\cite{jiang2023mistral, dubey2024llama}. We evaluate resulting models by AlpacaEval 2~\cite{dubois2024length}, Arena-Hard~\cite{li2024crowdsourced}, and MT-Bench~\cite{jiang2023llm}, which are widely used for general chatbot benchmarks. 

\subsection{Anthropic-HH}

Anthropic-HH~\cite{bai2022training} is a human preference dialogue dataset containing two subsets based on the helpfulness and harmlessness principle. Here, we use \texttt{helpful-base} and \texttt{harmless-base} splits to validate the criterion using logit monotonicity for instance-level $\beta$ control used in \method{} and the efficiency in terms of trade-off between performance and KL divergence~\cite{rafailov2024scaling}. We choose \texttt{gemma-2-2B}~\cite{team2024gemma} to obtain the reference policy through Supervised Fine-tuning with chosen responses. Following DPO~\cite{rafailov2023direct}, we evaluate the models trained with each method under various $\beta$ in the single-turn dialogue setting. We regard PairRM~\cite{jiang2023llm} as an evaluator for checking performance by win rate comparing their responses and chosen responses in the test splits.