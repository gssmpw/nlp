\section{Related Works}

\paragraph{Direct Alignment Algorithms} 
Many variants of direct alignment algorithms perform alignment on offline preference datasets without an external reward model. DPO~\cite{rafailov2023direct} performs alignment through preference modeling with the implicit reward derived from the optimal policy of reward maximization under the KL penalty objective. RRHF~\cite{yuan2023rrhf} performs alignment by training to maintain the likelihood margin between preference ranks. KTO~\cite{ethayarajh2024kto} changes the assumptions of the Bradley-Terry model~\cite{bradley1952rank} used by DPO and introduces Prospect Theory~\cite{kahneman2013prospect}, and IPO~\cite{azar2024general} converts to the root-finding problem for strengthening the KL constraint. SLiC-HF~\cite{zhao2023slic}, CPO~\cite{xu2024contrastive}, ORPO~\cite{hong2024orpo}, and SimPO~\cite{meng2024simpo} train without reference models utilizing behavior cloning, margin loss, contrastive loss, odds ratio loss, and fixed margin by replacing the implicit rewards.

\paragraph{Reward Over-optimization and KL Penalty}
Since RLHF~\cite{ziegler2020finetuning} utilizes a trained reward model, it amplifies the limitations of the reward model as it is optimized for an imperfect reward, according to Goodhartâ€™s Law~\cite{hoskin1996awful}, and this is called reward over-optimization~\cite{gao2023scaling}. However, \citet{rafailov2024scaling} finds that direct alignment algorithms also experience similar reward over-optimization, regardless of the variant. Direct alignment algorithms commonly show humped curves of performance according to the increase of KL divergence from the reference model during training. TR-DPO~\cite{gorbatovski2024learn} argues that this is due to the Hessian of the loss landscape converging to zero as the implicit reward margin grows during training, so they update the reference model for mitigating this phenomenon. On the other hand, $\beta$-DPO~\cite{wu2024beta}, which also performs relaxation of KL penalty, claims that adaptively changing $\beta$ through the statistics of the implicit reward margin is required to reflect the quality of the preference pair.

\input{figures/reward_margin}

\paragraph{Combining Sampling Distribution}
Combining sampling distributions of language models can be utilized to estimate a new sampling distribution with specific characteristics. Contrastive Decoding~\cite{li2022contrastive} shows that the log-likelihood margins of the expert and amateur language models can enhance response diversity by penalizing incorrect response patterns favored by the amateur language model. \citet{sanchez2023stay} shows that classifier-free guidance~\cite{ho2022classifier} can enhance prompt relativity in language modeling by treating prompts as conditions and sharpening the conditional sampling distribution. Combining the change from instruction-tuning in a small language model with a large language model can approximate fine-tuning. \citet{liu2024tuning} utilizes the instruction-tuned small language model as the logit offset, and \citet{mitchell2023emulator} estimates the importance sampling ratio of the optimal distribution defined by the objective of RLHF from it. Inspired by the theoretical motivation of \citet{mitchell2023emulator}, \citet{liu2024decoding} shows that the sampling distribution of the policy trained under the near $\beta$ by DPO can be approximated by policy obtained under $\beta$ and the reference policy.

\input{figures/kl_trade_off}