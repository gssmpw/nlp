\section{Introduction}

\input{figures/monotonic_logit}
\input{figures/overview}

Aligning large language models with human preferences for helpfulness and harmless principles~\cite{askell2021general, bai2022training, cui2023ultrafeedback} is a crucial requirement for general chatbot agents. Reinforcement Learning from Human Feedback (RLHF)~\cite{ziegler2020finetuning} is the pioneering approach that regards the alignment of large language models as a reward maximization problem and solves it by reinforcement learning~\cite{schulman2017proximal}. However, the complicated training pipeline of RLHF increases the training complexity and computation cost of the rollout for online reinforcement learning, in addition to the difficulty of collecting human preference datasets. Moreover, introducing a trained reward model as a proxy reward function to replace the intractable ground-truth human preference reward function makes large language models suffer from the side effect of reward over-optimization~\cite{gao2023scaling} inherited from the reward models.

Direct Preference Optimization (DPO)~\cite{rafailov2023direct} proposes an approach to reform the limitation of RLHF by converting the policy optimization problem into a preference modeling problem and performing alignment using only offline learning. It shows comparable performance while skipping the reward modeling process required by RLHF and has become an effective alternative approach for alignment. In particular, subsequent studies with various modifications to the DPO loss objective open a new research domain called direct alignment algorithms~\cite{rafailov2024scaling}, which perform alignment directly from offline preference datasets without training separated reward models.

However, DPO assumes that $\beta$ and the reference policy, which define a KL penalty that prevents excessive deviations from the reference model in RLHF, are fixed for exploiting the existence of a closed-form solution derived from the objective function of the RLHF. However, this assumption can lead to suboptimal results, since the KL penalty can be regarded as a Lagrangian relaxation of the constraint optimization defined by the trust region~\cite{schulman2017proximal}. In this regard, $\beta$-DPO~\cite{wu2024beta} argues that $\beta$ should be adaptively chosen according to the quality of the preference pair but fails to control $\beta$ at the instance-level and proposes a batch-level control method. On the other hand, TR-DPO~\cite{gorbatovski2024learn} claims to periodically update the reference policy to reduce over-optimization~\cite{rafailov2024scaling}, but it may induce unnecessary KL divergence for improvement since the update is not adaptive.

In this paper, we present \textbf{\methodbfull{} (\methodb{})}, an instance-level adaptive KL penalty control for DPO that neither TR-DPO nor $\beta$-DPO achieves. Specifically, we check the advantage of adjusting $\beta$ for each preference pair by observing the monotonicity of the log-likelihood ratio between the chosen response and the rejected response when the $\beta$ used during training is perturbed, as described in \Cref{fig:monotonic_logit}. Here, the criterion for controlling $\beta$ does not require batch-level statistics, and the policy under the perturbed $\beta$ can be estimated by reusing the current policy and reference policy logits. This criterion results in independence from the choice of micro-batch size and no additional computation requirements for model updates, unlike $\beta$-DPO and TR-DPO.

Experimental results demonstrate that \method{} outperforms $\beta$-DPO, TR-DPO, and most direct alignment algorithms that modify DPO loss objective~\cite{yuan2023rrhf, zhao2023slic, azar2024general, xu2024contrastive, ethayarajh2024kto, hong2024orpo, park2024disentangling, meng2024simpo}, highlighting the importance of adequate KL penalty relaxation for DPO. Furthermore, we confirm that the variation of $\beta$ determined by the adaptive criterion in \method{} reflects the confusion as a preference model, which is not addressed by the adaptive criterion of $\beta$-DPO. We also find that the adaptive KL penalty control of \method{} is crucial for an efficient KL trade-off compared to TR-DPO, which is not an adaptive KL penalty control.

In summary, our work shows the following:
\begin{itemize}
    \setlength\itemsep{0em}
  \item \method{} provides a simple criterion to improve DPO through KL penalty relaxation.
  \item \method{} adaptively adjusts $\beta$ in instance-level reflecting confusion as a preference model.
  \item \method{} efficiently controls $\beta$ in trade-off between KL divergence and performance.
\end{itemize}