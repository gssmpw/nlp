\appendix
\onecolumn
\section{Proof of \Cref{prop:dera}}\label{app:dera}

\begin{proposition*}[\citet{liu2024decoding}]
Under the assumption of optimal autoregressive policy $\pi^*$ where the prompt $x \in \mathcal{X}$, response vocabulary $y_i \in \mathcal{V}$, and logit $f: \mathcal{X} \times \mathcal{V}^{i-1} \rightarrow \mathbb{R}^{|\mathcal{V}|}$, the optimal policy $\pi^*_\frac{\beta}{\lambda}$ can be approximated by the arithmetic mean of logits between $\pi^*_\beta$ and reference policy $\pi_{\textnormal{ref}}$,
\begin{equation*}
\begin{split}
\pi^*_{\frac{\beta}{\lambda}} (y_{1:n}|x) &= \prod_{i=1}^n \pi^*_{\frac{\beta}{\lambda}} (y_i|x, y_{1:i-1}) \\
&\approx \prod_{i=1}^n \textnormal{Softmax} \big(\lambda f^*_\beta (x, y_{1:i-1}) + (1 - \lambda) f_{\textnormal{ref}} (x, y_{1:i-1}) \big)_{y_i}. \\
\end{split}
\end{equation*}
\end{proposition*}

\textit{Proof of \Cref{prop:dera}}. Recall that optimal policy $\pi^*_\beta$ has a closed-form solution and ground-truth reward function $r^*$ can be reparameterized using the normalizing constant $Z^*_\beta$,
\begin{equation*}
\begin{split}
\pi^*_\beta (y | x) &= \frac{1}{Z^*_\beta(x)}\pi_{\text{ref}}(y | x) \exp \big(\frac{1}{\beta}r^*(x, y)\big), \\
Z^*_\beta(x) &= \sum_{y} \pi_{\text{ref}}(y | x) \exp \big(\frac{1}{\beta}r^*(x, y)\big), \\
r^*(x, y) &= \beta \log \frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } + \beta \log Z^*_\beta(x). \\
\end{split}
\end{equation*}

Here, we plug the reparameterization of $r^*$ to the close-form solution of $\pi^*_\frac{\beta}{\lambda}$ and simple algebra yield,
\begin{equation*}
\begin{split}
\pi^*_\frac{\beta}{\lambda} (y | x) &= \frac{1}{Z^*_\frac{\beta}{\lambda}(x)}\pi_{\text{ref}}(y | x) \exp \big(\frac{\lambda}{\beta}r^*(x, y)\big) = \frac{\pi_{\text{ref}}(y | x) \exp \big(\frac{\lambda}{\beta}r^*(x, y)\big)}{\sum_{y} \pi_{\text{ref}}(y | x) \exp \big(\frac{\lambda}{\beta}r^*(x, y)\big)} \\
&= \frac{\pi_{\text{ref}}(y | x) \exp \big(\lambda \log \frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } + \lambda \log Z^*_\beta(x)\big)}{\sum_{y} \pi_{\text{ref}}(y | x) \exp \big(\lambda \log \frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } + \lambda \log Z^*_\beta(x)\big)} = \frac{\pi_{\text{ref}}(y | x) \big(\frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } + Z^*_\beta(x) \big)^\lambda}{\sum_{y} \pi_{\text{ref}}(y | x) \big(\frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } + Z^*_\beta(x) \big)^\lambda} \\
&= \frac{\pi_{\text{ref}}(y | x) \big(\frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } \big)^\lambda}{\sum_{y} \pi_{\text{ref}}(y | x) \big(\frac{\pi^*_\beta (y | x)}{\pi_{\text{ref}}(y | x) } \big)^\lambda} = \frac{\pi^*_\beta (y | x)^\lambda \pi_{\text{ref}}(y | x)^{1-\lambda}}{\sum_{y} \pi^*_\beta (y | x)^\lambda \pi_{\text{ref}}(y | x)^{1-\lambda}} = \frac{1}{Z(x)} \pi^*_\beta (y | x)^\lambda \pi_{\text{ref}}(y | x)^{1-\lambda}, \\
\end{split}
\end{equation*}

where $Z$ denotes the normalizing constant of reparameterized form of $\pi^*_\frac{\beta}{\lambda}$. Now, we use the assumption of autoregressive policy $\pi^*_\beta$. This assumption gives us to evade intractable normalizing constant $Z$,
\begin{equation*}
\begin{split}
\pi^*_\frac{\beta}{\lambda} (y_i | x, y_{1:i-1}) &\approx \frac{1}{Z(x, y_{1:i-1})} \pi^*_\beta (y_i | x, y_{1:i-1})^\lambda \pi_{\text{ref}}(y_i | x, y_{1:i-1})^{1-\lambda} \\
&= \frac{\pi^*_\beta (y_i | x, y_{1:i-1})^\lambda \pi_{\text{ref}}(y_i | x, y_{1:i-1})^{1-\lambda}}{\sum_{v \in \mathcal{V}}\pi^*_\beta (v | x, y_{1:i-1})^\lambda \pi_{\text{ref}}(v | x, y_{1:i-1})^{1-\lambda}} \\
&= \frac{\textnormal{Softmax} \big(f^*_\beta (x, y_{1:i-1}) \big)^\lambda_{y_i} \textnormal{Softmax} \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)^{1 - \lambda}_{y_i}}{\sum_{v \in \mathcal{V}} \textnormal{Softmax} \big(f^*_\beta (x, y_{1:i-1}) \big)^\lambda_v \textnormal{Softmax} \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)^{1 - \lambda}_v} \\
&= \frac{\exp \big(f^*_\beta (x, y_{1:i-1}) \big)^\lambda_{y_i} \exp \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)^{1 - \lambda}_{y_i}}{\sum_{v \in \mathcal{V}} \exp \big(f^*_\beta (x, y_{1:i-1}) \big)^\lambda_v \exp \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)^{1 - \lambda}_v}, \\
\end{split}
\end{equation*}

with eliminating $ \big( \sum_{v \in \mathcal{V}} \exp \big(f^*_\beta (x, y_{1:i-1}) \big)_v \big)^\lambda \big( \sum_{v \in \mathcal{V}} \exp \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)_v \big)^{1 - \lambda} $ from nominator and denominator at the last line. Note that the geometric mean acts as the arithmetic mean in log-scale,
\begin{equation*}
\begin{split}
&\frac{\exp \big(f^*_\beta (x, y_{1:i-1}) \big)^\lambda_{y_i} \exp \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)^{1 - \lambda}_{y_i}}{\sum_{v \in \mathcal{V}} \exp \big(f^*_\beta (x, y_{1:i-1}) \big)^\lambda_v \exp \big(f_{\textnormal{ref}} (x, y_{1:i-1}) \big)^{1 - \lambda}_v} \\
&= \frac{\exp \big( \lambda f^*_\beta (x, y_{1:i-1})_{y_i} + (1-\lambda) f_{\textnormal{ref}} (x, y_{1:i-1})_{y_i} \big)}{\sum_{v \in \mathcal{V}} \exp \big( \lambda f^*_\beta (x, y_{1:i-1})_v + (1-\lambda) f_{\textnormal{ref}} (x, y_{1:i-1})_v \big)} \\
&= \textnormal{Softmax} \big(\lambda f^*_\beta (x, y_{1:i-1}) + (1-\lambda) f_{\textnormal{ref}} (x, y_{1:i-1}))_{y_i}. \\
\end{split}
\end{equation*}

Therefore, $\pi^*_\frac{\beta}{\lambda}$ can be approximated by the arithmetic mean of logit between $\pi^*_\beta$ and $\pi_{\textnormal{ref}}$,
\begin{equation*}
\begin{split}
\pi^*_{\frac{\beta}{\lambda}} (y_{1:n}|x) &= \prod_{i=1}^n \pi^*_{\frac{\beta}{\lambda}} (y_i|x, y_{1:i-1}) \\
&\approx \prod_{i=1}^n \textnormal{Softmax} \big(\lambda f^*_\beta (x, y_{1:i-1}) + (1 - \lambda) f_{\textnormal{ref}} (x, y_{1:i-1}) \big)_{y_i}.\\
 \\
\end{split}
\end{equation*}
\hfill $\square$

\section{Implementation Details}
\label{app:implementation_details}

The implementation of \method{} and experiments are all based on the TRL\footnote{\href{https://github.com/huggingface/trl}{github.com/huggingface/trl}} library. Here, we explain the experimental settings for UltraFeedback~\cite{cui2023ultrafeedback} and Antropic-HH~\cite{bai2022training} in detail.

\subsection{UltraFeedback}

For a fair comparison with direct alignment algorithms and existing approaches for KL penalty relaxation, we follow the \texttt{Instruct} setting suggested by SimPO~\cite{meng2024simpo}. The \texttt{Instruct} setting starts with \texttt{Mistral-7B-Instruct-v0.2}\footnote{\href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}{huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}, Apache 2.0 License, Copyright (c) 2023 Mistral AI}~\cite{jiang2023mistral} and \texttt{Meta-Llama-3-8B-Instruct}\footnote{\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}, LLAMA 3 Community License, Copyright (c) 2024 Meta Platforms}~\cite{dubey2024llama} as reference policies, each named as \texttt{Mistral-Instruct} and \texttt{Llama-3-Instruct}. First, rollouts using prompts from UltraFeedback~\cite{cui2023ultrafeedback} are performed, then PairRM~\cite{jiang2023llm} serves as an external evaluator to build preference datasets for approximating the on-policy learning~\cite{tajwar2024preference, lee2024aligning}. We use corresponding datasets publicly released by SimPO, each denoted as \texttt{mistral-instruct-ultrafeedback}\footnote{\href{https://huggingface.co/datasets/princeton-nlp/mistral-instruct-ultrafeedback}{huggingface.co/datasets/princeton-nlp/mistral-instruct-ultrafeedback}} and \texttt{llama3-ultrafeedback}\footnote{\href{https://huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback}{huggingface.co/datasets/princeton-nlp/llama3-ultrafeedback}}. We perform hyperparameter searches for the learning rate within the range of [3e-7, 5e-7, 7e-7, 1e-6] and  $\varepsilon$ within the [0.005, 0.01, 0.02] range while $\beta$ is fixed to 0.01, following the best hyperparameter of DPO reported from SimPO. Other common hyperparameters are fixed in the same way as SimPO. Every experiment is conducted using 16 NVIDIA A100-SXM4-40GB GPUs within 2 hours. We evaluate resulting models through AlpacaEval 2~\cite{dubois2024length}, Arena-Hard~\cite{li2024crowdsourced}, and MT-Bench~\cite{jiang2023llm} following the same sampling configuration settings reported by SimPO. \Cref{tables:ultrafeedback} summarizes the training configurations for \texttt{Mistral-Instruct} and \texttt{Llama-3-Instruct}.

\input{tables/ultrafeedback}

\subsection{Anthropic-HH}

We use \texttt{helpful-base} and \texttt{harmless-base} splits for experiments using Anthropic-HH\footnote{\href{https://huggingface.co/datasets/Anthropic/hh-rlhf}{huggingface.co/datasets/Anthropic/hh-rlhf}, MIT License, Copyright (c) 2022 Anthropic}~\cite{bai2022training}. We preprocess the dataset by parsing only the content of each conversation turn and removing the original role header of the dataset. We use \texttt{gemma-2-2b}\footnote{\href{https://huggingface.co/google/gemma-2-2b}{huggingface.co/google/gemma-2-2b}, Apache 2.0 License, Copyright (c) 2024 Google LLC}~\cite{team2024gemma} as a base model for obtaining the reference policy through Supervised Fine-tuning (SFT) with chosen responses by applying the chat template of \texttt{gemma-2-2b-it}~\cite{team2024gemma}\footnote{\href{https://huggingface.co/google/gemma-2-2b-it}{huggingface.co/google/gemma-2-2b-it}, Apache 2.0 License, Copyright (c) 2024 Google LLC}. We fix all hyperparameters except $\beta$ for a fair comparison between methods. We use $\varepsilon=0.01$ in \method{} and $\tau=128$, $\alpha=0.6$ in TR-DPO~\cite{gorbatovski2024learn} as the method-specific hyperparameter and $\beta$ within the [0.01, 0.05, 0.1, 0.5] range. Following DPO~\cite{rafailov2023direct}, we evaluate resulting models in the single-turn dialogue setting by comparing with chosen responses from test split through PairRM\footnote{\href{https://huggingface.co/llm-blender/PairRM}{huggingface.co/llm-blender/PairRM}}~\cite{jiang2023llm} as an external evaluator to check the win rate. We set the temperature as 1.0 and max token length as 1024 when sampling responses from each model for evaluation. Every experiment is conducted using 4 NVIDIA A100-SXM4-40GB GPUs within 7 hours. \Cref{tables:anthropic_hh} shows the common training configurations for each experiment.

\input{tables/anthropic_hh}

\section{Qualitative Analysis of Logit Monotonicity and Implicit Reward Margin}
\label{app:qualitative_analysis}

We compare preference pairs whose implicit reward margin is maximized among the preference pairs showing monotonically increasing or decreasing logits in the \texttt{helpful-base} split of Antropic-HH~\cite{bai2022training}. Similarly, we compare preference pairs whose implicit reward margin is minimized among the preference pairs showing monotonically increasing or decreasing logits. We obtain these preference pairs through the policy trained with DPO under $\beta=0.1$. If we follow the claim of $\beta$-DPO, higher $\beta$ should be selected for both preference pairs that sufficiently maximize the implicit reward margin regardless of logit monotonicity. However, \Cref{tables:dec_max} shows the case close to the label flipping compared to the case of \Cref{tables:inc_max} in which the adaptive control decision of \method{} and $\beta$-DPO matches in high implicit reward margin. In this case of label flipping, increasing $\beta$ results in a negative effect by increasing the scale of the loss gradient~\cite{rafailov2023direct} on the noisy preference pair. On the other hand, \Cref{tables:inc_min} shows the case of the rejected response with a significantly lower quality than the chosen response compared to the case of \Cref{tables:dec_min} in which the adaptive control decision of \method{} and $\beta$-DPO matches in low implicit reward margin. However, $\beta$-DPO will assign a low $\beta$ to the corresponding example contrary to the original claim since it shows a low implicit reward margin. These qualitative examples demonstrate that the claim of $\beta$-DPO, preference data quality can be estimated through the implicit reward margin, may not be empirically valid.


\input{tables/dec_max}
\input{tables/inc_max}


\input{tables/inc_min}
\input{tables/dec_min}
