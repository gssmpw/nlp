\section{Conclusion}

In this paper, we present \methodfull{} (\method{}), an instance-level adaptive KL penalty control for DPO, adjusting $\beta$ by observing the monotonicity of the log-likelihood ratio between the chosen response and the rejected response when the $\beta$ used during training is perturbed. The criterion for instance-level adaptive control of $\beta$ only requires estimating the policy under the perturbed $\beta$, which can be efficiently estimated by reusing the current policy and reference policy logits without relying on batch-level statistics and requiring additional computation cost. Resulting models obtained through \method{} perform better than resulting models from existing methods under general chatbot benchmarks. In particular, the criterion used in \method{} shows a more efficient KL trade-off than the non-adaptive KL penalty relaxation while reflecting the confusion on preference pairs, emphasizing the importance of an appropriate instance-level KL penalty relaxation.