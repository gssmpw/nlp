\section{\methodsfull{}}
In this section, we describe our proposed method, \textbf{\methodbfull{} (\methodb{})}, that adaptively controls KL penalty coefficient $\beta$ at the instance-level based on the logit monotonicity as a preference model according to the perturbation of $\beta$. \Cref{fig:overview} illustrates the difference between \method{} and existing KL penalty relaxation methods.

\subsection{Relaxation of KL Penalty in DPO}
The KL penalty introduced by RLHF can be regarded as an approach to solve the constrained optimization problem in the trust region~\cite{schulman2015trust} defined near the reference policy $\pi_\text{ref}$ as an unconstrained optimization by treating $\beta$ as a Lagrange multiplier~\cite{schulman2017proximal}. From this perspective, even though DPO reformulates the problem of finding an optimal policy under fixed $\pi_\text{ref}$ and $\beta$ as a preference modeling problem, using a single $\beta$ and a fixed trust region for all instances may lead to suboptimal results. This hypothesis regarding relaxation of KL penalty can be supported by the experimental results of $\beta$-DPO~\cite{wu2024beta} that adaptively control $\beta$ based on the statistics of implicit reward margin during the training process and TR-DPO~\cite{gorbatovski2024learn} that updates $\pi_{\text{ref}}$ during the training process for preventing over-optimization~\cite{rafailov2024scaling} from the vanishing curvature of the loss landscape.

However, $\beta$-DPO fails to perform instance-level $\beta$ control despite claiming that the quality of each preference pair should determine $\beta$. Instead, it performs batch-level $\beta$ control using momentum-based estimation of batch-level margin disparities, which is strongly affected by the micro-batch size. In addition, TR-DPO updates the reference model without adaptive criteria, which can lead to inefficient KL divergence trade-off between performance and incur computational costs for updating the reference model. Therefore, instance-level adaptive KL penalty control without requiring additional computational cost that achieves an efficient KL trade-off is still undiscovered for DPO.

\subsection{Logit Monotonicity under Perturbation}
Establishing a criterion to adaptively change the KL penalty for each instance of preference dataset is not a trivial problem. As a proxy criterion, we can exploit that the policy obtained via DPO can function as a preference model $\mathbb{P}_{\theta, \beta}$. Formally, $\mathbb{P}_{\theta, \beta}$ can be expressed as a binary classifier with logit $z_\theta$ and margin $\gamma$ for a preference triplet $(x, y^w, y^l) \in \mathcal{D}$,
\begin{equation*}
\begin{split}
& \mathbb{P}_{\theta, \beta}(\cdot | \cdot) = \sigma \Big( \beta \big( z_\theta(\cdot) - \gamma(\cdot) \big) \Big), \\
& z_\theta(x, y^w, y^l) \coloneqq \log \frac{\pi_\theta (y^w | x)}{\pi_\theta (y^l | x)}, \\
& \gamma(x, y^w, y^l) \coloneqq \log \frac{\pi_\text{ref} (y^w | x)}{\pi_\text{ref} (y^l | x)}. \\
\end{split}
\end{equation*}

This shows that $\beta$ serves as an inverse temperature of a binary classifier. For a given $\beta$, we define $\beta^-_\varepsilon$ and $\beta^+_\varepsilon$ with a positive constant $\varepsilon>0$. That is, $\beta^-_\varepsilon$ and $\beta^+_\varepsilon$ refer to values that have been \textit{perturbed} to be slightly larger or slightly smaller than the $\beta$,
\begin{equation*}
\begin{split}
& \beta^-_\varepsilon \coloneqq \frac{\beta}{1+\varepsilon}, \beta^+_\varepsilon \coloneqq \frac{\beta}{1-\varepsilon}. \\
\end{split}
\end{equation*}

Let us denote the parameters obtained via DPO as a function of $\beta$, $ \theta(\cdot): \mathbb{R}^+ \rightarrow \Theta$. Consider the case we observe the strict \textit{monotonicity} of logits happens according to the perturbation of $\beta$ on $\theta(\cdot)$,
\begin{equation}\label{eqn:overegularization}
\begin{split}
& z_{\theta(\beta^-_\varepsilon)}(\cdot) > z_{\theta(\beta)}(\cdot) > z_{\theta(\beta^+_\varepsilon)}(\cdot), \\
\end{split}
\end{equation}
\begin{equation}\label{eqn:underregularization}
\begin{split}
& z_{\theta(\beta^-_\varepsilon)}(\cdot) < z_{\theta(\beta)}(\cdot) < z_{\theta(\beta^+_\varepsilon)}(\cdot). \\
\end{split}
\end{equation}

Intuitively, this corresponds to observing monotonic changes in preference confidence under the same test-time temperature scaling~\cite{guo2017calibration}. If the logits monotonically decrease with increasing $\beta$, then raising the training temperature (i.e., lowering $\beta$) yields a clearer separation of $y^w$ and $y^l$ in the neighborhood of $\tfrac{1}{\beta}$, despite having a softer decision boundary. Conversely, if they monotonically increase, a higher training temperature harms the separation of $y^w$ and $y^l$. From this, we can estimate the benefit of adjusting $\beta$ for each instance within the neighborhood defined by $\varepsilon$.

\subsection{Estimating KL Penalty Perturbation}
Note that $\theta(\cdot)$ is intractable since it is equivalent to having access to models trained on each $\beta$. However, \citet{liu2024decoding} shows that optimal policy under $\frac{\beta}{\lambda}$ can be expressed by $\pi_\text{ref}$ re-weighted with importance ratio using $\pi^*_\beta$. If we assume the autoregressive prior of optimal policy, then the optimal policy under $\frac{\beta}{\lambda}$ can be estimated by the optimal policy under $\beta$ and the reference policy, as we respecify \Cref{prop:dera} from \citet{liu2024decoding},
\begin{proposition}[\citet{liu2024decoding}]\label{prop:dera}
Under the assumption of optimal autoregressive policy $\pi^*$ where the prompt $x \in \mathcal{X}$, response vocabulary $y_i \in \mathcal{V}$, and logit $f: \mathcal{X} \times \mathcal{V}^{i-1} \rightarrow \mathbb{R}^{|\mathcal{V}|}$, the optimal policy $\pi^*_\frac{\beta}{\lambda}$ can be approximated by the arithmetic mean of logits between $\pi^*_\beta$ and reference policy $\pi_{\textnormal{ref}}$,
\begin{equation*}
\begin{split}
&\pi^*_{\frac{\beta}{\lambda}} (y_{1:n}|x) = \prod_{i=1}^n \pi^*_{\frac{\beta}{\lambda}} (y_i|x, y_{1:i-1}) \\
&\begin{split}
    \approx \prod_{i=1}^n \textnormal{Softmax} \big( &\lambda f^*_\beta (x, y_{1:i-1}) \\
    + &(1 - \lambda) f_{\textnormal{ref}} (x, y_{1:i-1}) \big)_{y_i}. \\
\end{split}
\end{split}
\end{equation*}
Proof. See \Cref{app:dera}.
\end{proposition}

Using \Cref{prop:dera}, we can approximate $\pi_{\theta(\beta^-_\varepsilon)}$ and $\pi_{\theta(\beta^+_\varepsilon)}$ by trained policy and reference policy without accessing $\theta(\cdot)$ since they are the approximated policies for $\pi^*_{\beta^-_\varepsilon}$ and $\pi^*_{\beta^+_\varepsilon}$. To adaptively control $\beta$ for each preference triplet $(x, y^w, y^l)$ during the training process, we regard the policy $\pi_\theta$ obtained in the current step as the best approximation of the optimal policy under current $\beta$ and estimate $\pi_{\theta(\beta^-_\varepsilon)}$ and $\pi_{\theta(\beta^+_\varepsilon)}$ for $z_{\theta(\beta^-_\varepsilon)}$ and $z_{\theta(\beta^+_\varepsilon)}$,
\begin{equation}\label{eqn:p_epsilon_estimate}
\begin{split}
& \pi_{\theta(\beta^-_\varepsilon)} (y_{1:n}|x) = \prod_{i=1}^n \pi_{\theta(\beta^-_\varepsilon)} (y_i|x, y_{1:i-1}) \\
&\begin{split}
    \approx \prod_{i=1}^n \textnormal{Softmax} \big(
    (1 + \varepsilon) &f_{\theta} (x, y_{1:i-1}) \\
    - \varepsilon &f_{\text{ref}} (x, y_{1:i-1}) \big)_{y_i}, \\
\end{split}
\end{split}
\end{equation}
\begin{equation}\label{eqn:n_epsilon_estimate}
\begin{split}
& \pi_{\theta(\beta^+_\varepsilon)} (y_{1:n}|x) = \prod_{i=1}^n \pi_{\theta(\beta^+_\varepsilon)} (y_i|x, y_{1:i-1}) \\
&\begin{split}
    \approx \prod_{i=1}^n \textnormal{Softmax} \big(
    (1 - \varepsilon) &f_{\theta} (x, y_{1:i-1}) \\
    + \varepsilon &f_{\text{ref}} (x, y_{1:i-1}) \big)_{y_i}.
\end{split}
\end{split}
\end{equation}

Recall that we need not only the logit of the current policy $f_\theta$ but also the logit of the reference policy $f_\text{ref}$ to compute the estimated log-likelihood ratio. However, in order to compute the loss function of DPO, $\mathcal{L}_\text{DPO}$, the log-likelihood from the reference policy must be computed for each training instance, which allows us to simply reuse $f_\text{ref}$ for estimation without any additional computation cost of model forward passes. Therefore, we determine the $\tilde{\beta}$, which is used for the KL penalty coefficient in the current training step for each instance,
\begin{equation}\label{eqn:beta_selection}
\begin{split}
\tilde{\beta}(x, y^w, y^l; \theta) &= 
  \begin{cases}
    \beta^-_{\varepsilon} & \text{if } ~\eqref{eqn:overegularization},\\
    \beta^+_{\varepsilon} & \text{if } ~\eqref{eqn:underregularization},\\
    \beta & \text{otherwise}.
  \end{cases}\\
\end{split}
\end{equation}

After the model update, the $\beta$ corresponds to the optimal policy that the current policy is targeting changes depending on $\tilde{\beta}$ used in $\mathcal{L}_\text{DPO}$ for each instance. Therefore, we need to modify the baseline $\beta$ for the next training step, and we simply update the $\beta$ with the mean statistics of $\tilde{\beta}$ determined across the batch used in the update as follows:
\begin{equation}\label{eqn:updates}
\begin{split}
& \beta \leftarrow \mathbb{E}_{x, y^w, y^l}[\tilde{\beta}(x, y^w, y^l; \theta)]. \\
\end{split}
\end{equation}

Note that $\tilde{\beta}$ is determined independently with the batch-level statistic, so the adaptive control of $\beta$ in \method{} can be performed independently with the choice of micro-batch size. \Cref{alg:edpo} summarizes the entire training process of \method{}.
\begin{algorithm}[h!]
    \caption{$\varepsilon$-Direct Preference Optimization}\label{alg:edpo}
    \begin{algorithmic}[1]
    \REQUIRE reference policy $\pi_{\text{ref}}$, initial KL penalty coefficient $\beta$, and perturbation size $\varepsilon$
    \STATE Initialize model $\pi_{\theta}$ with $\pi_{\text{ref}}$.
    \WHILE{not converged}
        \STATE Sample preference triplets ($x$, $y^w, y^l$).
        \STATE Estimate $\pi_{\hat{\theta}(\beta^-_\varepsilon)}, \pi_{\hat{\theta}(\beta^+_\varepsilon)}$ using \ref{eqn:p_epsilon_estimate} and \ref{eqn:n_epsilon_estimate}.
        \STATE Determine instance-level $\tilde{\beta}$ according to \ref{eqn:beta_selection}.
        \STATE Update $\pi_{\theta}$ by $\mathcal{L}_{\text{DPO}}$ with instance-level $\tilde{\beta}$.
        \STATE Update the current $\beta$ using $\tilde{\beta}$ by \ref{eqn:updates}.
    \ENDWHILE
\RETURN aligned policy $\pi_{\theta}$.
\end{algorithmic}
\end{algorithm}