\section{Experimental Results}

\input{tables/main}

\paragraph{Main Results of \methodb{}}
In \Cref{tables:main}, we observe that \method{} shows notable performances across AlpacaEval 2~\cite{dubois2024length}, Arena-Hard~\cite{li2024crowdsourced}, and MT-Bench~\cite{jiang2023llm} using UltraFeedback. In particular, we find that the performance of \method{} outperforms most direct alignment algorithms, which generally modify the loss objective, highlighting that the major assumption of fixed KL penalty in DPO is overlooked. In addition, we observe that \method{} performs better than other KL penalty relaxation approaches~\cite{wu2024beta, gorbatovski2024learn} from \Cref{tables:kl_relaxation}. Thus, we can find that instance-level KL penalty control significantly impacts the final performance.

\input{tables/kl_relaxation}

\paragraph{Influence of $\bm{\varepsilon}$ on Training Dynamics}
The perturbation $\varepsilon$ is used for checking logit monotonicity as a preference model in the neighborhood of the current $\beta$. Therefore, it can be chosen within a reasonable range to estimate the approximated policies corresponding to $\beta^-_\varepsilon$ and $\beta^+_\varepsilon$. However, $\varepsilon$ can influence training dynamics since $\varepsilon$ determines the sizes of instance-level KL penalty coefficient $\tilde{\beta}$. We further analyze the intra-epoch training dynamics on \texttt{Llama-3-Instruct} settings according to $\varepsilon$. We compare the forward KL divergence $\mathbb{D}_\text{KL}(\pi_\text{ref}||\pi_\theta)$~\cite{rafailov2024scaling} and performance on AlpacaEval 2 using checkpoints obtained at 0.2 intervals during the training, along with the changes of in-batch ratio of $\beta^-_\varepsilon$ and $\beta^+_\varepsilon$, as shown in \Cref{fig:epsilon}. We find that adaptive control occurs more frequently for both $\beta^-_\varepsilon$ and $\beta^+_\varepsilon$ as $\varepsilon$ increased, leading to accelerating the increase of KL divergence and performance. We also observe that the performance at the beginning of training tends to be lower when higher $\varepsilon$. We speculate that the trained policy at the beginning of training is insufficient to estimate the optimal policy, making the approximation unstable at the high $\varepsilon$ level.

\input{figures/epsilon}

\paragraph{Analysis of Logit Monotonicity}
$\beta$-DPO~\cite{wu2024beta} chooses higher $\beta$ for preference pairs with larger implicit reward margins to update the current policy conservatively from the reference policy. This is motivated by the claim that large implicit reward margins reflect higher quality gaps of response pairs corresponding to meaningless training signals. In this respect, we analyze the implicit reward margin of preference pairs where logit monotonicity according to the perturbation of $\beta$ happened in policies trained by DPO using Antropic-HH, as shown in \Cref{fig:reward_margin}. We find that \method{} performs opposite decisions compared to $\beta$-DPO from the observation that preference pairs with monotonically increasing logits have smaller average implicit reward margins than those with monotonically decreasing logits. Also, this implies that \method{} enhances training signals for confusing examples because the implicit reward margin is proportional to the preference confidence, and the increase of $\beta$ scales up the gradient of DPO loss~\cite{rafailov2023direct}. Furthermore, we confirm that implicit reward margins do not always represent the quality of preference pairs through qualitative analysis in \Cref{app:qualitative_analysis}. Therefore, we suspect that $\beta$-DPO fails on the instance-level adaptive KL penalty control because it assigns low gradient weights to confusing examples and strongly relies on the implicit reward margins that do not always represent the quality of preference pairs.

\paragraph{Efficiency in KL Trade-off}
As TR-DPO~\cite{gorbatovski2024learn} claims, increasing KL divergence would be desirable as a trade-off when deviating from the reference policy improves performance. However, the over-optimization~\cite{rafailov2024scaling} of direct alignment algorithms emphasizes that it is necessary to check the Pareto frontier to determine whether performance improvements can be achieved without indiscriminately expanding the KL divergence. \Cref{fig:kl_trade_off} depicts the Pareto frontier of models trained under various beta using Antropic-HH by DPO, \method{} and two variants of TR-DPO, TR-DPO$^\tau$ which hard-updates the reference policy by the fixed interval and TR-DPO$^\alpha$ which soft-updates the reference policy through weight merging. We can see that regardless of the two variants, TR-DPO induces more KL divergence than DPO and \method{} and cannot achieve similar performance under the same KL budget as \method{}. This highlights the efficiency of \method{} in KL trade-offs and implies that controlling the KL penalty in a non-adaptive manner can induce excessive relaxation for performance improvements.