\section{Preliminaries}

\paragraph{Reinforcement Learning from Human Feedback}

To obtain a language model that aligns with human preference, RLHF~\cite{ziegler2020finetuning} introduces reinforcement learning. It is equivalent to approaching preference alignment as a reward maximization problem, where we find a policy $\pi$ that maximizes a ground-truth reward function $r^*$ representing human reward for a response $y$ obtained from a corresponding policy for a given prompt $x$. However, since the ground-truth reward function cannot be accessed, a reward model trained from the preference dataset is introduced as a proxy reward function. On the other hand, to prevent the policy update from deviating too much from the current policy from the initial policy, the KL divergence from the reference policy $\pi_{\text{ref}}$ serves as a penalty and regards the initial policy as a reference policy. At this time, coefficient $\beta$ controls the strength of the penalty. The optimal policy that satisfies the maximization of the modified objective function under $\beta$ has a closed-form solution $\pi^*_{\beta}$,
\begin{equation*}
\begin{split}
& \pi^*_\beta \coloneqq \text{arg}\max_{\pi} \{ \mathbb{E}_{x, y}[r^*(x, y)] - \beta \mathbb{D}_{\text{KL}} (\pi || \pi_{\text{ref}}) \}, \\
& \pi^*_\beta (y | x) = \frac{1}{Z^*_\beta(x)}\pi_{\text{ref}}(y | x) \exp \big(\frac{1}{\beta}r^*(x, y)\big), \\
& Z^*_\beta(x) = \sum_{y} \pi_{\text{ref}}(y | x) \exp \big(\frac{1}{\beta}r^*(x, y)\big). \\
\end{split}
\end{equation*}

\paragraph{Direct Preference Optimization}

RLHF has a limitation in efficiency due to the additional training step of the reward model. In this respect, DPO~\cite{rafailov2023direct} proposes an approach that can perform preference alignment without training the reward model. DPO focuses on the fact that the ground-truth reward function can be implicitly reparameterized by the closed-form solution $\pi^*_\beta$ and reference policy $\pi_{\text{ref}}$ with an intractable normalizing constant $Z^*_\beta$. If we assume the Bradley-Terry model~\cite{bradley1952rank} for the ground-truth human preference function, then the human preference can be modeled by the margin between the reward of the chosen response $y^w$ and the rejected response $y^l$ with the sigmoid function $\sigma$, which can ignore the intractable term $Z^*_\beta$ by cancellation. From this observation, DPO performs preference alignment through preference model optimization using an offline dataset in the sense that obtaining an optimal policy through policy optimization in RLHF can be obtained by training a preference model given by the implicit reward $r_{\theta, \beta}$,
\begin{equation*}
\begin{split}
& r_{\theta, \beta}(x, y) \coloneqq \beta \log \frac{ \pi_{\theta} (y | x)}{\pi_{\text{ref}} (y | x)} + Z_\beta(x;\theta), \\
& \mathbb{P}_{\theta, \beta}(y^w \succ y^l | x) \coloneqq \sigma \big( r_{\theta, \beta}(x, y^w) - r_{\theta, \beta}(x, y^l) \big), \\
& \mathcal{L}_{\text{DPO}}(x, y^w, y^l; \theta, \beta) \coloneqq - \log \mathbb{P}_{\theta, \beta}(y^w \succ y^l | x). \\
\end{split}
\end{equation*}