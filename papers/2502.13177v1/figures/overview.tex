\begin{figure*}[!t]
  \includegraphics[width=\textwidth]{figures/overview_image.pdf}
  \caption{Comparison between \method{} and existing KL penalty relaxation methods. $\beta$-DPO~\cite{wu2024beta} adaptively adjusts $\beta$ based on the batch-level statistics of implicit reward margin through momentum, failing to adjust at the instance-level and being affected by micro-batch size. TR-DPO~\cite{gorbatovski2024learn} updates reference policy periodically by the current policy, so the updates are not adaptive and require computational costs for reference policy updates. On the other hand, \method{} can adaptively control $\beta$ at the instance-level by checking the monotonicity of the log-likelihood ratio under perturbation of $\beta$ by simply reusing logits from the policies.}
  \label{fig:overview}
\end{figure*}