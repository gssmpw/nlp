\begin{table*}[h]
    \centering
    % \footnotesize
    \begin{tabular}{lcc}
        \toprule
        \textbf{Configuration} & \textbf{Mistral-Instruct} & \textbf{Llama-3-Instruct} \\ 
        \midrule
        Model & \texttt{Mistral-7B-Instruct-v0.2} & \texttt{Meta-Llama-3-8B-Instruct} \\
        Dataset & \texttt{mistral-instruct-ultrafeedback} & \texttt{llama3-ultrafeedback} \\
        Optimizer & AdamW & AdamW \\
        Epoch & 1 & 1\\
        Batch Size & 128 & 128 \\
        Learning Rate & [\underline{3e-7}, 5e-7, 7e-7, 1e-6] & [3e-7, 5e-7, \underline{7e-7}, 1e-6] \\
        Scheduler & cosine & cosine \\
        Warm-up Ratio & 0.1 & 0.1 \\
        Weight Decay & 0 & 0 \\
        $\beta$ & 0.01 & 0.01 \\
        $\varepsilon$ & [0.005, \underline{0.01}, 0.02] & [0.005, \underline{0.01}, 0.02] \\
        \bottomrule
    \end{tabular}
    \caption{Training configurations for \texttt{Mistral-Instruct} and \texttt{Llama-3-Instruct} in the experiment settings using Ultrafeedback~\cite{cui2023ultrafeedback}. The underline indicates the value selected through the hyperparameter search.}
    \label{tables:ultrafeedback}
\end{table*}