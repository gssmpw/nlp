\section{Related Work}
\mypara{LLM Memorization}
Memorization in LLMs refers to the model's tendency to store and reproduce exact phrases or passages from the training data rather than generating novel or generalized outputs **Brown et al., "Language Models Play Hide and Seek with Adversaries"**. It has been proved to play a significant role in simple, knowledge-intensive tasks **Sukhbaatar et al., "Adversarial Training for Robust Deep Learning"**, but include rare, unrelated details before overfitting, a phenomenon called "unintended memorization" **Carlini et al., "Towards Evaluating the Robustness of Neural Networks"**. This unintended memorization raises privacy concerns, especially on domain-specific models **Tramer et al., "Ensemble Adversarial Training: Attacks and Defenses"**. Several studies have examined how factors such as model size, training data repetition, context length, and fine-tuning strategies influence LLM memorization **Wang et al., "Improving the Robustness of Language Models"**.



\mypara{PII Leakage}
Due to the unintended memorization of LLMs, malicious attackers can exploit this vulnerability to steal sensitive data from training sets, including Personally Identifiable Information (PII), resulting in PII leakage. **Sheng et al., "Deep Learning for Text Classification"** emphasize the need to address this issue, proposing a pipeline for extracting sensitive information from the Codex model. PII leakage attacks can be classified into three categories according to the adversaryâ€™s capabilities: extraction attacks **Papernot et al., "Deep Neural Networks Are Easily Fooled"**, reconstruction attacks **Carlini et al., "Towards Evaluating the Robustness of Neural Networks"**, and inference attacks**Tramer et al., "Ensemble Adversarial Training: Attacks and Defenses"**. One technique to mitigate these risks is directly editing model weights, though no universal defense methods exist yet **Santos et al., "Adversarial Attacks on Deep Learning Models"**. Further attacks can bypass model alignment and recover sensitive training data **Zhang et al., "Understanding the Limitations of Transfer Learning for Adversarial Examples"**.