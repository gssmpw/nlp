[
  {
    "index": 0,
    "papers": [
      {
        "key": "satvaty2024undesirablememorizationlargelanguage",
        "author": "Ali Satvaty and Suzan Verberne and Fatih Turkmen",
        "title": "Undesirable Memorization in Large Language Models: A Survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wang2024generalizationvsmemorizationtracing",
        "author": "Xinyi Wang and Antonis Antoniades and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang",
        "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "carlini2019secret",
        "author": "Carlini, Nicholas and Liu, Chang and Erlingsson, {\\'U}lfar and Kos, Jernej and Song, Dawn",
        "title": "The secret sharer: Evaluating and testing unintended memorization in neural networks"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "yang2024memorization",
        "author": "Xinyu Yang and Zichen Wen and Wenjie Qu and Zhaorun Chen and Zhiying Xiang and Beidi Chen and Huaxiu Yao",
        "title": "Memorization and Privacy Risks in Domain-Specific Large Language Models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "carlini2023quantifyingmemorizationneurallanguage",
        "author": "Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang",
        "title": "Quantifying Memorization Across Neural Language Models"
      },
      {
        "key": "mireshghallah2022empirical",
        "author": "Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David K and Berg-Kirkpatrick, Taylor",
        "title": "An empirical analysis of memorization in fine-tuned autoregressive language models"
      },
      {
        "key": "zeng2024exploringmemorizationfinetunedlanguage",
        "author": "Shenglai Zeng and Yaxin Li and Jie Ren and Yiding Liu and Han Xu and Pengfei He and Yue Xing and Shuaiqiang Wang and Jiliang Tang and Dawei Yin",
        "title": "Exploring Memorization in Fine-tuned Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "291327",
        "author": "Liang Niu and Shujaat Mirza and Zayd Maradni and Christina P{\\\"o}pper",
        "title": "{CodexLeaks}: Privacy Leaks from Code Generation Language Models in {GitHub} Copilot"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "carlini2021extracting",
        "author": "Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others",
        "title": "Extracting training data from large language models"
      },
      {
        "key": "mireshghallah2022empirical",
        "author": "Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David K and Berg-Kirkpatrick, Taylor",
        "title": "An empirical analysis of memorization in fine-tuned autoregressive language models"
      },
      {
        "key": "yu2023bagtrickstrainingdata",
        "author": "Weichen Yu and Tianyu Pang and Qian Liu and Chao Du and Bingyi Kang and Yan Huang and Min Lin and Shuicheng Yan",
        "title": "Bag of Tricks for Training Data Extraction from Language Models"
      },
      {
        "key": "zhang2023ethicisttargetedtrainingdata",
        "author": "Zhexin Zhang and Jiaxin Wen and Minlie Huang",
        "title": "Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "inan2021training",
        "author": "Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and R{\\\"u}hle, Victor and Withers, James and Sim, Robert",
        "title": "Training data leakage analysis in language models"
      },
      {
        "key": "lukas2023analyzing",
        "author": "Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-Beguelin, Santiago",
        "title": "Analyzing Leakage of Personally Identifiable Information in Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "mireshghallah2022quantifying",
        "author": "Mireshghallah, Fatemehsadat and Goyal, Kartik and Uniyal, Archit and Berg-Kirkpatrick, Taylor and Shokri, Reza",
        "title": "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks"
      },
      {
        "key": "fu2024membership",
        "author": "Fu, Wenjie and Wang, Huandong and Gao, Chen and Liu, Guanghua and Li, Yong and Jiang, Tao",
        "title": "Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "patil2023sensitiveinformationdeletedllms",
        "author": "Vaidehi Patil and Peter Hase and Mohit Bansal",
        "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "nasr2025scalable",
        "author": "Milad Nasr and Javier Rando and Nicholas Carlini and Jonathan Hayase and Matthew Jagielski and A. Feder Cooper and Daphne Ippolito and Christopher A. Choquette-Choo and Florian Tram{\\`e}r and Katherine Lee",
        "title": "Scalable Extraction of Training Data from Aligned, Production Language Models"
      }
    ]
  }
]