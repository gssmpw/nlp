\section{Related Work}
\mypara{LLM Memorization}
Memorization in LLMs refers to the model's tendency to store and reproduce exact phrases or passages from the training data rather than generating novel or generalized outputs ~\citep{satvaty2024undesirablememorizationlargelanguage}. It has been proved to play a significant role in simple, knowledge-intensive tasks ~\citep{wang2024generalizationvsmemorizationtracing}, but include rare, unrelated details before overfitting, a phenomenon called "unintended memorization" \citep{carlini2019secret}. This unintended memorization raises privacy concerns, especially on domain-specific models ~\citep{yang2024memorization}. Several studies have examined how factors such as model size, training data repetition, context length, and fine-tuning strategies influence LLM memorization ~\citep{carlini2023quantifyingmemorizationneurallanguage, mireshghallah2022empirical, zeng2024exploringmemorizationfinetunedlanguage}.



\mypara{PII Leakage}
Due to the unintended memorization of LLMs, malicious attackers can exploit this vulnerability to steal sensitive data from training sets, including Personally Identifiable Information (PII), resulting in PII leakage. ~\citealp{291327} emphasize the need to address this issue, proposing a pipeline for extracting sensitive information from the Codex model. PII leakage attacks can be classified into three categories according to the adversaryâ€™s capabilities: extraction attacks ~\citep{carlini2021extracting, mireshghallah2022empirical, yu2023bagtrickstrainingdata, zhang2023ethicisttargetedtrainingdata}, reconstruction attacks ~\citep{inan2021training, lukas2023analyzing}, and inference attacks~\citep{mireshghallah2022quantifying, fu2024membership}. One technique to mitigate these risks is directly editing model weights, though no universal defense methods exist yet ~\citep{patil2023sensitiveinformationdeletedllms}. Further attacks can bypass model alignment and recover sensitive training data ~\citep{nasr2025scalable}.