@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th USENIX security symposium (USENIX security 19)},
  pages={267--284},
  year={2019}
}

@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}

@misc{carlini2023quantifyingmemorizationneurallanguage,
    title={Quantifying Memorization Across Neural Language Models},
    author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
    year={2023},
    eprint={2202.07646},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2202.07646},
}

@inproceedings{fu2024membership,
  title={Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration},
  author={Fu, Wenjie and Wang, Huandong and Gao, Chen and Liu, Guanghua and Li, Yong and Jiang, Tao},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{inan2021training,
  title={Training data leakage analysis in language models},
  author={Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and R{\"u}hle, Victor and Withers, James and Sim, Robert},
  journal={arXiv preprint arXiv:2101.05405},
  year={2021}
}

@inproceedings{lukas2023analyzing,
  title={Analyzing Leakage of Personally Identifiable Information in Language Models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-Beguelin, Santiago},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={346--363},
  year={2023},
  organization={IEEE Computer Society}
}

@inproceedings{mireshghallah2022empirical,
  title={An empirical analysis of memorization in fine-tuned autoregressive language models},
  author={Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David K and Berg-Kirkpatrick, Taylor},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1816--1826},
  year={2022}
}

@inproceedings{mireshghallah2022quantifying,
  title={Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks},
  author={Mireshghallah, Fatemehsadat and Goyal, Kartik and Uniyal, Archit and Berg-Kirkpatrick, Taylor and Shokri, Reza},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={8332--8347},
  year={2022}
}

@misc{patil2023sensitiveinformationdeletedllms,
	title={Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks},
	author={Vaidehi Patil and Peter Hase and Mohit Bansal},
	year={2023},
	eprint={2309.17410},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2309.17410},
}

@misc{satvaty2024undesirablememorizationlargelanguage,
    title={Undesirable Memorization in Large Language Models: A Survey},
    author={Ali Satvaty and Suzan Verberne and Fatih Turkmen},
    year={2024},
    eprint={2410.02650},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2410.02650},
}

@misc{wang2024generalizationvsmemorizationtracing,
    title={Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data},
    author={Xinyi Wang and Antonis Antoniades and Yanai Elazar and Alfonso Amayuelas and Alon Albalak and Kexun Zhang and William Yang Wang},
    year={2024},
    eprint={2407.14985},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2407.14985},
}

@misc{yu2023bagtrickstrainingdata,
	title={Bag of Tricks for Training Data Extraction from Language Models},
	author={Weichen Yu and Tianyu Pang and Qian Liu and Chao Du and Bingyi Kang and Yan Huang and Min Lin and Shuicheng Yan},
	year={2023},
	eprint={2302.04460},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2302.04460},
}

@misc{zeng2024exploringmemorizationfinetunedlanguage,
    title={Exploring Memorization in Fine-tuned Language Models},
    author={Shenglai Zeng and Yaxin Li and Jie Ren and Yiding Liu and Han Xu and Pengfei He and Yue Xing and Shuaiqiang Wang and Jiliang Tang and Dawei Yin},
    year={2024},
    eprint={2310.06714},
    archivePrefix={arXiv},
    primaryClass={cs.AI},
    url={https://arxiv.org/abs/2310.06714},
}

@misc{zhang2023ethicisttargetedtrainingdata,
	title={Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation},
	author={Zhexin Zhang and Jiaxin Wen and Minlie Huang},
	year={2023},
	eprint={2307.04401},
	archivePrefix={arXiv},
		primaryClass={cs.CL},
	url={https://arxiv.org/abs/2307.04401},
}

