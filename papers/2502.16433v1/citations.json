[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "chung2022scaling",
        "author": "Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others",
        "title": "Scaling instruction-finetuned language models"
      },
      {
        "key": "sanh2021multitask",
        "author": "Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others",
        "title": "Multitask prompted training enables zero-shot task generalization"
      },
      {
        "key": "zhou2023lima",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "arora2022exposure",
        "author": "Arora, Kushal and Asri, Layla El and Bahuleyan, Hareesh and Cheung, Jackie Chi Kit",
        "title": "Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "schmidt2019generalization",
        "author": "Schmidt, Florian",
        "title": "Generalization in generation: A closer look at exposure bias"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2020exposure",
        "author": "Wang, Chaojun and Sennrich, Rico",
        "title": "On exposure bias, hallucination and domain shift in neural machine translation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bengio2015scheduled",
        "author": "Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam",
        "title": "Scheduled sampling for sequence prediction with recurrent neural networks"
      },
      {
        "key": "ranzato2015sequence",
        "author": "Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech",
        "title": "Sequence level training with recurrent neural networks"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2022brio",
        "author": "Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham",
        "title": "BRIO: Bringing order to abstractive summarization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "pang2020text",
        "author": "Pang, Richard Yuanzhe and He, He",
        "title": "Text generation by learning from demonstrations"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shen2015minimum",
        "author": "Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang",
        "title": "Minimum risk training for neural machine translation"
      },
      {
        "key": "zhang2019bridging",
        "author": "Zhang, Wen and Feng, Yang and Meng, Fandong and You, Di and Liu, Qun",
        "title": "Bridging the gap between training and inference for neural machine translation"
      },
      {
        "key": "duckworth2019parallel",
        "author": "Duckworth, Daniel and Neelakantan, Arvind and Goodrich, Ben and Kaiser, Lukasz and Bengio, Samy",
        "title": "Parallel scheduled sampling"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "stiennon2020learning",
        "author": "Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F",
        "title": "Learning to summarize with human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "korbak2022rl",
        "author": "Korbak, Tomasz and Perez, Ethan and Buckley, Christopher L",
        "title": "RL with KL penalties is better viewed as Bayesian inference"
      },
      {
        "key": "deng2020residual",
        "author": "Deng, Yuntian and Bakhtin, Anton and Ott, Myle and Szlam, Arthur and Ranzato, Marc'Aurelio",
        "title": "Residual energy-based models for text generation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "kumar2022gradient",
        "author": "Kumar, Sachin and Paria, Biswajit and Tsvetkov, Yulia",
        "title": "Gradient-based constrained sampling from language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pace2024west",
        "author": "Pace, Aliz{\\'e}e and Mallinson, Jonathan and Malmi, Eric and Krause, Sebastian and Severyn, Aliaksei",
        "title": "West-of-N: Synthetic Preference Generation for Improved Reward Modeling"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "bachmann2024pitfalls",
        "author": "Bachmann, Gregor and Nagarajan, Vaishnavh",
        "title": "The pitfalls of next-token prediction"
      }
    ]
  }
]