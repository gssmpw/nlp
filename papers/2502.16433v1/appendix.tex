\section{Appendix}

\subsection{A brief introduction to DPO, RLHF, and EBM}

\paragraph{The equivalence of RLHF and EBM} For the completeness of this paper, we include the result of the equivalence between RLHF and EBM. For the full proofs, we refer the reader to \cite{rafailov2023direct,korbak2022rl}.

The RLHF objective is the following:
\begin{align}\label{eq:rlhf_objective}
\begin{split}
		& \max_{\pi_\theta} \E_{\vx\sim\gD, \vy\sim\pi_\theta(\vy|\vx)}[r(\vx, \vy)] \\
		& \quad-\beta \KL\p{\pi_\theta(\vy|\vx)||\pr(\vy|\vx)},
\end{split}
\end{align}
where $\vx\sim\mathcal{D}$ is a given prefix, $\vy\sim\pi_\theta(\vy|\vx)$ is a sampled continuation from the trainable model $\pi_\theta$, and $r(\vx, \vy)\in \R$ is the reward. Meanwhile, we want to control the divergence between $\pi_\theta$ and $\pr$, where $\pr$ is usually an already pretrained or finetuned LLM.
The RLHF optimum is achieved at the following EBM:
\begin{align}\label{eq:rlhf_ebm}
	\pi^*(\vy|\vx) = \frac{1}{Z(\vx)}\pr(\vy|\vx)\exp\p{\frac{1}{\beta}r(\vx,\vy)},
\end{align}
where $Z(\vx)=\sum_{\vy}\pr(\vy|\vx)\exp\p{\frac{1}{\beta}r(\vx,\vy)}$ is the partition function.


\citet{rafailov2023direct} assume that the preference over two sequences $\vy_w$ and $\vy_l$ given $\vx$ is parameterized by the Bradley-Terry model:
\[
 P(\vy_w\succ\vy_l|\vx)=\frac{e^{r(\vx, \vy_w)}}{e^{r(\vx, \vy_l)}+e^{r(\vx, \vy_w)}}.
\]
% The optimal policy $\pi^*$ takes the aforementioned EBM form \cref{eq:rlhf_ebm}. and this EBM reprametrization 
Under the Bradley-Terry model, DPO establishes the equivalence between the original RLHF objective \cref{eq:rlhf_objective} and the following supervised objective:
%\begin{align}
%	\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\text {ref }}\right)=-\mathbb{E}_{\left(\vx, \vy_w, \vy_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(\vy_w \mid \vx\right)}{\pi_{\text {ref }}\left(\vy_w \mid \vx\right)}-\beta \log \frac{\pi_\theta\left(\vy_l \mid \vx\right)}{\pi_{\text {ref }}\left(\vy_l \mid \vx\right)}\right)\right],
%\end{align}

\begin{align}
\begin{split}
	&\mathcal{L}_{\mathrm{DPO}}(\pi_\theta ; \pi_{\text{ref}})=\\
	&\mathbb{E}_{(\vx, \vy_w, \vy_l) \sim \mathcal{D}}\Big[ \log \sigma\Big(\beta \log \frac{\pi_\theta(\vy_w \mid \vx)}{\pi_{\text{ref}}(\vy_w \mid \vx)} \\
	&\quad -\beta \log \frac{\pi_\theta(\vy_l \mid \vx)}{\pi_{\text{ref}}(\vy_l \mid \vx)}\Big)\Big],
\end{split}
\end{align}

where $\sigma(\cdot)$ is the Sigmoid function.

They also generalize the formulation to the Plackett-Luce model, where we have a linear ordering $\tau(\cdot)$ among $K$ sequences:

%\begin{equation}\label{eq:rankcpo}
%\begin{split}
%		&\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)\\
%		&=-\mathbb{E}_{\tau, \vy_1, \ldots, \vy_K, \vx \sim \mathcal{D}}\left[\log \prod_{k=1}^K \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(k)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(k)} \mid \vx\right)}\right)}{\sum_{j=k}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(j)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(j)} \mid \vx\right)}\right)}\right].
%\end{split}
%\end{equation}

\begin{equation}\label{eq:rankdpo}
\resizebox{\columnwidth}{!}{%
$
\begin{split}
		&\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)=\\
		&\underset{\substack{\tau, \vx \sim \mathcal{D} \\  \vy_1, \ldots, \vy_K} }{\mathbb{E}}\left[\log \prod_{k=1}^K \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(k)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(k)} \mid \vx\right)}\right)}{\sum_{j=k}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(j)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(j)} \mid \vx\right)}\right)}\right].
\end{split}
$
}
\end{equation}
Here, $\tau(1),\ldots, \tau(K)$ induce a ranking among $K$ sequences.


\subsection{Derivation of the CPO objective function}
Here we give a full derivation of the CPO objective function in \cref{eq:norankcpo}.

Let $\vy_1,\ldots,\vy_K$ be $K$ continuations of a given prefix $\vx$. Without loss of generality, let $\vy_1$ be the best candidate. We are interested in the MLE of the event $P(\vy_1 \text{ is the best among $K$ candidates}|\vx)$.

We start from the sequence-level (RLHF) objective, notice that here $r(\cdot)$ is a reward over language quality, not human preference.
\begin{align}
\begin{split}
		& \max_{\pi_\theta} \E_{\vx\sim\gD, \vy\sim\pi_\theta(\vy|\vx)}[r(\vx, \vy)] \\
		& \quad-\beta \KL\p{\pi_\theta(\vy|\vx)||\pr(\vy|\vx)},
\end{split}
\end{align}

Its optimum is achieved at the following EBM:
\begin{align}\label{eq:rlhf_ebm_appendix}
	\pi^*(\vy|\vx) = \frac{1}{Z(\vx)}\pr(\vy|\vx)\exp\p{\frac{1}{\beta}r(\vx,\vy)},
\end{align}
where $Z(\vx)=\sum_{\vy}\pr(\vy|\vx)\exp\p{\frac{1}{\beta}r(\vx,\vy)}$ is the partition function. See the proof in \cite{rafailov2023direct,korbak2022rl}.



Now we consider the natural extension of the Bradley-Terry model to $K$ candidates:
\begin{align}\label{eq:mle_k_appendix}
\begin{split}
	&P(\vy_1 \text{ is the best among $K$ candidates}|\vx)\\
	&=\frac{\exp\p{r^*(\vx, \vy_1)}}{\sum_{k\in[K]}\exp\p{r^*(\vx, \vy_k)}}.
\end{split}
\end{align}

Now assuming we have the optimal policy $\pi^*$, we can reparameterize $r$ by rearranging \cref{eq:rlhf_ebm_appendix}:
\begin{align}\label{eq:implicit_r_appendix}
\begin{split}
	r^*(\vx, \vy)=\beta \log \frac{\pi^*(\vy \mid \vx)}{\pi_{\mathrm{ref}}(\vy \mid \vx)}+\beta \log Z(\vx).
\end{split}
\end{align}
Plugging \cref{eq:implicit_r_appendix} into \cref{eq:mle_k_appendix}, we get \cref{eq:norankcpo}.

\subsection{Query template of Dolly and Wiki text generation}
The query template for the Dolly instruction-following is the following: ``\texttt{For the following query to a chatbot, which response is more helpful?$\backslash$n
    Query: \{\}$\backslash$n
    Response A: \{\}$\backslash$n
    Response B: \{\}$\backslash$n
    State only "A" or "B" to indicate which response is more helpful.$\backslash$n
    More helpful:}''
    
The template for Wiki is the following: ``\texttt{For the following prefix, which continuation is better?$\backslash$n
    Prefix: \{\}$\backslash$n
    Continuation A: \{\}$\backslash$n
    Continuation B: \{\}$\backslash$n
    State only "A" or "B" to indicate which continuation is more helpful.$\backslash$n
    Better:}''
    
\subsection{DPO generation template and example}

When generating unhelpful responses for DPO, we query GPT with the following template:
\texttt{Given the ground truth instruction and response, can you generate a not helpful response?$\backslash$n Instruction: \{\}$\backslash$nResponse: \{\}$\backslash$nNot helpful response:}. 

One example of the generated response is the following:
\textbf{instruction:} \texttt{When did Virgin Australia start operating?}
\textbf{chosen response:} \texttt{Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.}
\textbf{rejected response:} \texttt{Virgin Australia definitely exists and has airplanes that fly to different places.}


\subsection{Connection to noise contrastive estimation}
Noise contrastrive estimation (NCE) \citep{gutmann2010noise} is a novel estimation technique introduced to tackle the computational infeasibility of traditional likelihood-based methods in large-scale machine learning models, particularly those involving high-dimensional data. NCE diverges from typical maximum likelihood estimation by transforming the problem into a classification task, which is deeply connected to both DPO and CPO. In NCE, the model is trained to distinguish between real data and noise/synthetic data. Beyond binary classification, RankingNCE \footnote{Despite the name, it means the model is ranking the real data highest among all data, rather than learning a total ordering.} also trains the model to rank the real data higher than all noise samples \citep{ma2018noise}. 

There are two important distinctions between CPO and NCE. First, instead of training the model to distinguish between real data and noise (at which any reasonable language model should already be good), we train the model to distinguish \textit{better than a reference model does}, hence making the model better at recognizing natural text. Second, we also introduce a denser ranking signal by incorporating the similarity among embeddings of different samples. The experiments in this paper demonstrate that such a dense training signal consistently improves text generation quality.
