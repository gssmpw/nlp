\section{Related work}

LLMs trained with next token prediction loss \citep{radford2019language,chung2022scaling,sanh2021multitask,zhou2023lima} have demonstrated many fascinating capabilities, including the ability to perform zero-shot or few-shot tasks \citep{radford2019language,brown2020language} and the ability to reason \citep{wei2022chain}. 

Several works have investigated the shortcomings of MLE and exposure bias. \citet{arora2022exposure} measured the accumulation of errors in language generation due to exposure bias. \citet{schmidt2019generalization} connected exposure bias to generalization. \citet{wang2020exposure} studied how exposure bias leads to hallucination in neural machine translation. To mitigate exposure bias, there exists a long line of work that has explored sequence-level training methods. \citet{bengio2015scheduled,ranzato2015sequence} proposed to train RNN with RL or RL-related algorithms rather than teacher-forcing. BRIO \citet{liu2022brio} targeted the summarization task with the ROUGE signal. \citet{pang2020text} trained the language models with an offline RL learning algorithm. 

Recently, RLHF \citep{stiennon2020learning,ouyang2022training} was developed. Although the primary goal of RLHF is model alignment, it is effectively a sequence-level training technique. For the RLHF training, we usually need to gather a pair of continuations for each prefix, where one continuation aligns with human preference and the other does not. This pair of sequences is used to train a reward model, which is later used to supervise the samples generated by the RL-trained model. The model is typically optimized by REINFORCE \citep{williams1992simple} or PPO \citep{schulman2017proximal}. 

Sequence-level training is also closely related to energy-based models (EBM) \citep{korbak2022rl}, for example, the RLHF objective can be reframed as a supervised learning algorithm coined as direct preference optimization (DPO) \citep{rafailov2023direct} under the assumption of the Bradley-Terry model \citep{bradley1952rank} or the Plackett-Luce model \citep{plackett1975analysis,luce2012individual}. The particular formulation of the EBM in DPO mimics the formulation in \citet{deng2020residual}, with the reward function being the energy function. However, \citet{deng2020residual} directly treat the EBM as a language model, which is computationally heavy for sampling and inference (due to the estimation of the partition function). This EBM form has also been studied in controlled text generation. \citet{kumar2022gradient} adopted the Langevin dynamics technique to directly sample from the EBM, with different energy functions that characterize toxicity, fluency, and diversity. All of these methods can be viewed as sequence-level algorithms for different purposes. 