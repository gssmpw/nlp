\section{Our approach}
While the RL penalty with KL control \cref{eq:rlhf_objective} is widely adopted in RLHF, it can also be used directly to train LLMs: instead of a preference reward, we can use any metric that measures text qualities as the reward $r$, including ROUGE, BLEU, MAUVE, etc. The benefit of \cref{eq:rlhf_objective} over \cref{eq:mle_objective} is that $r$ guides the model over a whole sequence $\vy$, rather than just a single token. This motivates our work to investigate the possibility of using a sequence-level objective in the pretraining stage and the SFT stage of LLM.

Following \citet{rafailov2023direct}, we assume that the preference over two sequences $\vy_w$ and $\vy_l$ given $\vx$ is parameterized by the Bradley-Terry model:
\[
 P(\vy_w\succ\vy_l|\vx)=\frac{e^{r(\vx, \vy_w)}}{e^{r(\vx, \vy_l)}+e^{r(\vx, \vy_w)}}.
\]
% The optimal policy $\pi^*$ takes the aforementioned EBM form \cref{eq:rlhf_ebm}. and this EBM reprametrization 
Under the Bradley-Terry model, DPO establishes the equivalence between the original RLHF objective \cref{eq:rlhf_objective} and the following supervised objective:
%\begin{align}
%	\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta ; \pi_{\text {ref }}\right)=-\mathbb{E}_{\left(\vx, \vy_w, \vy_l\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_\theta\left(\vy_w \mid \vx\right)}{\pi_{\text {ref }}\left(\vy_w \mid \vx\right)}-\beta \log \frac{\pi_\theta\left(\vy_l \mid \vx\right)}{\pi_{\text {ref }}\left(\vy_l \mid \vx\right)}\right)\right],
%\end{align}

\begin{align}
\begin{split}
	&\mathcal{L}_{\mathrm{DPO}}(\pi_\theta ; \pi_{\text{ref}})=\\
	&\mathbb{E}_{(\vx, \vy_w, \vy_l) \sim \mathcal{D}}\Big[ \log \sigma\Big(\beta \log \frac{\pi_\theta(\vy_w \mid \vx)}{\pi_{\text{ref}}(\vy_w \mid \vx)} \\
	&\quad -\beta \log \frac{\pi_\theta(\vy_l \mid \vx)}{\pi_{\text{ref}}(\vy_l \mid \vx)}\Big)\Big],
\end{split}
\end{align}

where $\sigma(\cdot)$ is the Sigmoid function.

We can also generalize the formulation to the Plackett-Luce model, where we have a linear ordering $\tau(\cdot)$ among $K$ sequences:

%\begin{equation}\label{eq:rankcpo}
%\begin{split}
%		&\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)\\
%		&=-\mathbb{E}_{\tau, \vy_1, \ldots, \vy_K, \vx \sim \mathcal{D}}\left[\log \prod_{k=1}^K \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(k)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(k)} \mid \vx\right)}\right)}{\sum_{j=k}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(j)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(j)} \mid \vx\right)}\right)}\right].
%\end{split}
%\end{equation}

\begin{equation}\label{eq:rankdpo}
\resizebox{\columnwidth}{!}{%
$
\begin{split}
		&\mathcal{L}_{\mathrm{DPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)=\\
		&\underset{\substack{\tau, \vx \sim \mathcal{D} \\  \vy_1, \ldots, \vy_K} }{\mathbb{E}}\left[\log \prod_{k=1}^K \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(k)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(k)} \mid \vx\right)}\right)}{\sum_{j=k}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(j)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(j)} \mid \vx\right)}\right)}\right].
\end{split}
$
}
\end{equation}
Here, $\tau(1),\ldots, \tau(K)$ induce a ranking among $K$ sequences. To simplify the notation, from now on we always assume that $\vy_1\sim\mathcal{D}$ is the most preferred text appearing in the training data. 

Investigating the DPO objective, we notice two caveats for its use in the pretraining and SFT stages: 
\begin{enumerate*}[series = tobecont, itemjoin = \quad]
	\item We need human labelers to gather $\vy_w, \vy_l$.
	\item There may not be a natural ranking among negative sequences $\vy_2,\ldots\vy_K$ in terms of text quality.
\end{enumerate*}
To tackle the first point, we sample $\vy_l\sim\mathcal{A}$ where $\mathcal{A}$ is some noise distribution from which it is cheap to sample; to tackle the second point, we provide a variant objective that models a ``best-of-$K$'' event: $\vy_1$ is the best among $K$ sequences, rather than a linear ordering event $\vy_{\tau(1)}\succ \vy_{\tau(2)}\succ \ldots\succ \vy_{\tau(K)}$.
These modifications lead to our proposed CPO objective:
%\ch{I have a hard time going from the equation above to \ref{eq:rankcpo}. Is there an assumption to make here, or just a re-lableing? If it's only relabeling based on $\tau$, let's say that explicitly. Also, what is the random variable/distribution $A$? It's essential that this part is clear before the equation is built since it's one of the main contributions of the work. I suggest re-structing the formulation as follows 1/Say what you are going to do to with $A$ and $\tau$ and why then  2/Present \ref{eq:rankcpo}. Right now you make the modifications and then explain the modifications so the reader is confused.}

\begin{align}\label{eq:norankcpo}
\begin{split}
		&\mathcal{L}_{\mathrm{CPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)=\\
		&\underset{\substack{ (\vx,\vy_1) \sim \mathcal{D} \\  \vy_2, \ldots, \vy_K\sim\mathcal{A}} }{\mathbb{E}} \left[\log \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_1 \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_1 \mid \vx\right)}\right)}{\sum_{j=1}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_j \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_j \mid \vx\right)}\right)}\right].
\end{split}
\end{align}

%\begin{equation}\label{eq:rankingnce}
%\resizebox{\columnwidth}{!}{%
%$
%\begin{split}
%		&\mathcal{L}_{\mathrm{NCE}}\left(\pi_\theta, \pi_{\mathrm{noise}}\right)=\\
%		&\underset{\substack{ (\vx,\vy_1) \sim \mathcal{D} \\  \vy_2, \ldots, \vy_K\sim \pi_{\mathrm{noise}(\vy|\vx)} } }{\mathbb{E}} \left[\log \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_1 \mid \vx\right)}{\pi_{\mathrm{noise}}\left(\vy_1 \mid \vx\right)}\right)}{\sum_{j=1}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_j \mid \vx\right)}{\pi_{\mathrm{noise}}\left(\vy_j \mid \vx\right)}\right)}\right].
%\end{split}
%$
%}
%\end{equation}

If ranking information is desired, we have the following CPO objective with ranking:

\begin{equation}\label{eq:rankcpo}
\resizebox{\columnwidth}{!}{%
$
\begin{split}
		&\mathcal{L}_{\mathrm{CPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)=\\
	&\underset{\substack{\tau, (\vx,\vy_1) \sim \mathcal{D} \\  \vy_2, \ldots, \vy_K\sim\mathcal{A}} }{\mathbb{E}} \left[\log {\displaystyle \prod_{k=1}^K} \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(k)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(k)} \mid \vx\right)}\right)}{{\displaystyle \sum_{j=k}^K} \exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(j)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(j)} \mid \vx\right)}\right)}\right].
\end{split}
$
}
\end{equation}


We will later discuss some possible choices of ranking signals and show that the ranking can indeed further improve the text generation quality.
%
%Unlike the similar formulation to the DPO objectives, importantly in CPO, we do not assume $\vy_1,\ldots,\vy_K\sim \mathcal{D}$, rather it can be sampled from any distribution $\mathcal{A}$. The gist of this work is how to efficiently construct $\mathcal{A}$ to improve text generation quality.

The crucial aspect of CPO is how to generate negative sequences $\vy_2,...,\vy_k \sim \mathcal{A}$. For RLHF, negative sequences are simply the ones that humans dislike. For the qualities of text generation, we implicitly model the sequence-level signal $r(\vx, \vy)$ such that $r(\vx, \vy_k)<r(\vx, \vy_1)$, $\forall k\in\{2,\ldots, K\}$. In other words, the reward $r(\cdot)$ prefers the ground truth to any other sequence. Importantly, the actual signal $r$ is not parameterized explicitly, instead it is represented by the log density ratio $\log\frac{\pi_\theta}{\pr}$. 

\subsection{Connection to noise contrastive estimation}

Noise contrastrive estimation (NCE) \citep{gutmann2010noise} is a novel estimation technique introduced to tackle the computational infeasibility of traditional likelihood-based methods in large-scale machine learning models, particularly those involving high-dimensional data. NCE diverges from typical maximum likelihood estimation by transforming the problem into a classification task, which is deeply connected to both DPO and CPO. In NCE, the model is trained to distinguish between real data and noise/synthetic data. Beyond binary classification, RankingNCE \footnote{Despite the name, it means the model is ranking the real data highest among all data, rather than learning a total ordering.} also trains the model to rank the real data higher than all noise samples \citep{ma2018noise}. 

There are two important distinctions between CPO and NCE. First, instead of training the model to distinguish between real data and noise (at which any reasonable language model should already be good), we train the model to distinguish \textit{better than a reference model does}, hence making the model better at recognizing natural text. Second, we also introduce a denser ranking signal by incorporating the similarity among embeddings of different samples. The experiments in this paper demonstrate that such a dense training signal consistently improves text generation quality.

\subsection{Synthetic negative samples}
In this work, we propose four ways to generate synthetic negative samples. The first is to autoregressively generate continuations from a reference model (trained with the next token prediction loss). We fixed the synthetic data generation strategy to be top$-k$ sampling with $k=50$. The advantage of this strategy over the forthcoming strategies is that the generated continuations are of higher quality and lead to better downstream performance, while the disadvantage is that sampling is slow. We denote these negative samples as $\textbf{autoregressive negatives (AN)}$.  One can speed up the sampling process via speculative sampling \citep{chen2023accelerating} or using a smaller or distilled model, this direction is orthogonal to our approach and can be directly incorporated into our framework. 

The second way is to directly use the continuations to other (possibly unrelated) prefixes within the same mini-batch as the negative samples. More specifically, given a batch of prefixes and continuations $\{\vx_i, \vy_i\}_{i=1}^b$, the negative samples to the prefix $\vx_i$ are composed of $\{\vy_j\}_{j\neq i}$. Although these negative samples are not difficult to distinguish, they are very simple to create and can be easily scaled up. We denote these as $\textbf{batch negatives (BN)}$.

The third way is to perform a token-level perturbation. Given a sequence $\vy = \{\vy_1,\ldots, \vy_T\}$, we randomly select $c$ percent of the positions $\{t_1,\ldots, t_j\}\subseteq [T]$, and substitute each $\vy_{t_i}$ independently based on $\pi_\theta(\vy_{t_i}|\vy_1,\ldots,\vy_{t_i})$. We call these $\textbf{meanfield negatives (MN)}$. The name is based on the fact that we use a fully separable distribution to approximate the autoregressive distribution, but note that this is not the optimal mean-field approximation. Nevertheless, computing this particular meanfield approximation does not take an additional cost, compared to estimating the best meanfield approximation. This method does not generate semantically meaningful sentences, but it does generate hard negative samples, as the model tends to give them high probabilities. 

Lastly, for each ground truth continuation, we can truncate the continuation at a random position and append an extra EOS token to the end. We denote this by $\textbf{truncation negatives (TN})$.

\subsection{Possible ranking signals}
As mentioned above, our reward implicitly prefers the ground truth over other sequences, and we do not explicitly model the reward parametrically. 
%Obviously, the suitable reward function is not unique if we were to write this function explicitly. To name a few, given a ground truth continuation $\vy_w$ and a model-generated text $\vy_l$ to a given prefix $\vx$, the empirical training data distribution $\wh P(\vy_w|\vx)>\wh P(\vy_l|\vx)$ is one such function, since the latter empirical probability is most likely $0$; since $\wh P$ is an approximation to the real language distribution $P$, likely $P$ is also a function that is consistent with the afore-mentioned implicit preference. 
The upside of implicit representation of the reward is that it bypasses the shortcuts \citep[e.g.][]{krishna2021hurdles} that are known to other explicit metrics \footnote{Note that here we are not claiming CPO does not exist any shortcuts. The statement here simply means that other existing metrics have known shortcuts.}. However, since we do not have access to a concrete score for the text quality, when presented with more than one negative sample, we do not have a direct ranking among them.

Previous work on sequence-level training \citep{liu2022brio,bengio2015scheduled} has suggested a variety of signals, including BLEU, ROGUE, and BertScore. These signals are usually specific to certain downstream tasks such as translation or summarization. In the modern era of LLMs, they have been shown to no longer align with human evaluations \citep{goyal2022news}. Since our goal is to improve text generation or instruction-following, the cosine similarity between embeddings is a more intuitive signal to measure the distance between sequences. The use of embedding for text generation quality measurement is also suggested in the MAUVE metric \citep{pillutla2021mauve}.

When presented $K$ sequences and a ranking is desired, the sequences are ranked based on their cosine similarity to the ground truth. Let $\ve_1,\ldots, \ve_K$ be the embeddings of given sequences $\vy_1,\ldots, \vy_K$ and without loss of generality assume that $\ve_1$ is the ground truth, we define $\tau(\vy_i)<\tau(\vy_j)$ if $\frac{\ip{\ve_i}{\ve_1}}{\|\ve_i\|\|\ve_1\|}>\frac{\ip{\ve_j}{\ve_1}}{\|\ve_j\|\|\ve_1\|}$, with the lower ranking index indicating the better sample. Using the objective \cref{eq:rankcpo}, this process gives us denser signals during training and can lead to better downstream performance.

Another good candidate for the ranking signal is the reward model score. In fact, since the downstream performance is judged by a reward model, this will probably yield the best test performance as well. However, one has to train and host an extra reward model, creating extra memory and computation overhead. Therefore, we did not include such a signal during training in this work.

\subsection{Approximate reverse-KL}\label{subsec:approx_reverse_kl}
In the experiment, we also show how CPO improves reverse-KL. As we discuss previously, an unavoidable issue of calculating the reverse-KL is that we do not have access to the probability of the generated sequences under the true language distribution. However, if we agree that the ability to model natural language scales with the model size, then we can approximate the true language distribution $P$ with a more capable model $\widehat P$, hence approximating the reverse-KL divergence. Since many of our tasks are conditional by nature, for example, the instruction-following task is to generate a response conditioned on the input instruction, we further consider the expected reverse-KL divergence:
\begin{align}\label{eq:reversekl}
\begin{split}
		&\E_\vx\left[\KL\p{Q(\cdot|\vx)||\widehat P(\cdot|\vx)}\right] \\
		&\approx \frac{1}{|\gX|}\sum_{\vx\in\gX}\sum_{\vy\in \gY}Q(\vy|\vx)\log\p{\frac{Q(\vy|\vx)}{\widehat P(\vy|\vx)}},
\end{split}
\end{align}
where $\gX$ is the set of inputs (e.g. instructions) in the test set, and $\gY$ is the set of generated continuations (e.g. responses).
During our evaluation, we also notice that a more capable $Q$ tends to generate sequences $\vy$ with a lower probability $Q(\vy|\vx)$, compared to a less capable $Q$. This phenomenon is indeed expected, since a more capable model should be able to generate more diverse continuations. To overcome the numerical instability with a vanishing $Q$, we also use the following surrogate: 
\begin{align}\label{eq:condprob}
\begin{split}
		&\frac{1}{|\gX|}\sum_{\vx\in\gX}\sum_{\vy\in \gY}\frac{-\log\widehat P(\vy|\vx)}{|\vy|}, \\
		&\qquad \text{where } |\vy| \text{ is the length of $\vy$.}
\end{split}
\end{align}
This is the log conditional probability normalized by length, and its usage has been justified in \citet{cho2014properties,liu2022brio,fu2023gptscore}. In particular, \citet{fu2023gptscore} has discussed the use of normalized conditional probability with the GPT evaluator.

 