\section{Preliminary}

\paragraph{Notation} Consider a sentence of $T$ tokens $\vx=\{\vx_1,\ldots, \vx_T\}\in\gX$, and let $P$ be the unknown target language distribution, $\tilde P(\vx)$ be the empirical distribution of the training data (which is an approximation of $P$), and $Q$ be the distribution of our model at hand. Since our paper is also closely related to RLHF, we will also use $\pi$ to represent the distributions. In particular, we sometimes write $\pi_\theta$ for a distribution that is parameterized by $\theta$, where $\theta$ is usually the set of trainable parameters of the LLM; we write $\pr$ for a reference distribution that should be clear given the context. The next token prediction loss is minimizing the forward-KL between $P$ and $Q$. 

\paragraph{Forward-KL vs. reverse-KL} The forward-KL is formally defined as the following:

\begin{align*}
\begin{split}
	&\argmin_Q \KL(P||Q) \\
        & \approx \argmin_Q \KL(\tilde P||Q) \\
    & =\argmin_Q -\frac{1}{|\gX|}\sum_{\vx\in\gX}\log{Q(\vx)}.
\end{split}
\end{align*}

Since we are only optimizing $Q$, minimizing the forward-KL is equivalent to the maximum likelihood estimation (MLE) $\max \log Q$. Further decomposing $Q(\vx)=\prod_i Q(\vx_i|\vx_1^{i-1})$, we get the next token prediction loss function
\begin{align}\label{eq:mle_objective}
	\argmax_Q\sum_{\vx\in\gX} \sum_{x_t\in\vx} \log Q(\vx_t|\vx_{1}^{t-1}).
\end{align}

To actually measure the quality of the generated text, typically, we will first generate several sequences and then evaluate the quality of these generated sequences. Here we look closely at the reverse-KL:
\begin{align}
	\KL(Q||P) = \sum_{\vx\in\gX}Q(\vx)\log\p{\frac{Q(\vx)}{P(\vx)}},
\end{align}
however, since $\vx\sim Q$, and we do not have access to $P$, the reverse-KL cannot be computed exactly.

\paragraph{The equivalence of RLHF and EBM} For the completeness of this paper, we include the result of the equivalence between RLHF and EBM. For the full proofs, we refer the reader to \cite{rafailov2023direct,korbak2022rl}.

The RLHF objective is the following:
\begin{align}\label{eq:rlhf_objective}
\begin{split}
		& \max_{\pi_\theta} \E_{\vx\sim\gD, \vy\sim\pi_\theta(\vy|\vx)}[r(\vx, \vy)] \\
		& \quad-\beta \KL\p{\pi_\theta(\vy|\vx)||\pr(\vy|\vx)},
\end{split}
\end{align}
where $\vx\sim\mathcal{D}$ is a given prefix, $\vy\sim\pi_\theta(\vy|\vx)$ is a sampled continuation from the trainable model $\pi_\theta$, and $r(\vx, \vy)\in \R$ is the reward. Meanwhile, we want to control the divergence between $\pi_\theta$ and $\pr$, where $\pr$ is usually an already pretrained or finetuned LLM.
The RLHF optimum is achieved at the following EBM:
\begin{align}\label{eq:rlhf_ebm}
	\pi^*(\vy|\vx) = \frac{1}{Z(\vx)}\pr(\vy|\vx)\exp\p{\frac{1}{\beta}r(\vx,\vy)},
\end{align}
where $Z(\vx)=\sum_{\vy}\pr(\vy|\vx)\exp\p{\frac{1}{\beta}r(\vx,\vy)}$ is the partition function.


