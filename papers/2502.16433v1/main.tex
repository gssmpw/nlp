% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{caption} 
\captionsetup[table]{skip=10pt}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{mdframed}
\usepackage{adjustbox}
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable, skins}
\include{math_command}
\include{custom_command}
\usepackage{cleveref}

\NewTColorBox{NewBox}{ s O{!htbp} m m }{%
  floatplacement={#2},
  IfBooleanTF={#1}{float*,width=\textwidth}{float},
  colframe=green!50!black,colback=green!10!white, % any tcolorbox options here
  after upper={\captionof{figure}{#3}\label{#4}}, % Caption and label
}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcommand{\zhilif}[1]{\textcolor{blue}{#1}}
\title{Sequence-level Large Language Model Training with Contrastive Preference Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Zhili Feng \\
  Carnegie Mellon University\thanks{Work done as an intern at Amazon.} \\
  \texttt{zhilif@andrew.cmu.edu} \\\And
  Dhananjay Ram \\
  AWS AI \\ \texttt{radhna@amazon.com} \\\And
  Cole Hawkins \\
  AGI Foundations\\ \texttt{colehawk@amazon.com} \\\AND
  Aditya Rawal \\
  AGI Foundations\\ \texttt{adirawal@amazon.com} \\\And
  Jinman Zhao \\
  AGI Foundations\\ \texttt{jinming@amazon.com} \\\And
  Sheng Zha \\
  AGI Foundations\\ \texttt{zhasheng@amazon.com}
  }

\begin{document}
\maketitle
\begin{abstract}
The next token prediction loss is the dominant self-supervised training objective for large language models and has achieved promising results in a variety of downstream tasks. However, upon closer investigation of this objective, we find that it lacks an understanding of sequence-level signals, leading to a mismatch between training and inference processes. To bridge this gap, we introduce a contrastive preference optimization (CPO) procedure that can inject sequence-level information into the language model at any training stage without expensive human labeled data. Our experiments show that the proposed objective surpasses the next token prediction in terms of win rate in the instruction-following and text generation tasks. 
%Specifically, using OpenLlama-3B, our method achieves a $13.8\%$ improvement in an instruction-following task and a $3\%$ increase in a text-generation task.
\end{abstract}



\input{intro}

\input{related_work}
% \input{prelim}
\input{cpo}
\input{experiment}



\section{Conclusions and Limitations}
In this paper, we propose an auxiliary CPO loss function for LLM training, which can be used with or without ranking signals, depending on the quality of the negative samples. We investigated several ways to generate negative samples. One limitation of this work is that the synthetic data are very noisy unless generated autoregressively; it is interesting to explore other ways to efficiently generate high-quality negative data beyond the autoregressive fashion. 
%One possible direction is to consider Langevin dynamic sampling, which samples all tokens in parallel. 

% Entries for the entire Anthology, followed by custom entries
% \bibliographystyle{acl_natbib}
\bibliography{custom_citation.bib}
\appendix
\input{appendix}

\end{document}
