\section{Preliminary}

\paragraph{Notation} Consider a sentence of $T$ tokens $\vx=\{\vx_1,\ldots, \vx_T\}\in\gX$, and let $P$ be the unknown target language distribution, $\tilde P(\vx)$ be the empirical distribution of the training data (which is an approximation of $P$), and $Q$ be the distribution of our model at hand. Since our paper is also closely related to RLHF, we will also use $\pi$ to represent the distributions. In particular, we sometimes write $\pi_\theta$ for a distribution that is parameterized by $\theta$, where $\theta$ is usually the set of trainable parameters of the LLM; we write $\pr$ for a reference distribution that should be clear given the context. The next token prediction loss is minimizing the forward-KL between $P$ and $Q$. 



