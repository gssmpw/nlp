%\section{Experiment}
\section{Experimental Setup}

Throughout this section, $\textbf{BN, AN, MN, TN}$ represents batch negatives, autoregressive negatives, meanfield negatives, and truncation negatives respectively. \textbf{MixN} represents a mixed negative sampling strategy for which the details can be found in its context. We use \textbf{ANR} for models trained with autoregressive negatives and ranking signals, similarly we can denote $\textbf{MixNR}$, etc. We always randomly swap $15\%$ tokens using \textbf{MN}. Although this choice is mainly heuristic, such a ratio appears quite frequently since BERT \citep{wettig2022should}.

\vspace{-5pt}
\paragraph{Task and model.} We consider two tasks in this paper. The first is an instruction-following task, trained and evaluated on the Dolly dataset \citep{DatabricksBlog2023DollyV2}. This dataset is composed of $15011$ total instruction and response pairs. We train with $7505$ sequences and test with the rest $7506$. We use pre-trained GPT2-XL \citep{radford2019language} and OpenLlama-3B\citep{touvron2023llama,openlm2023openllama} as the base model. The second is an open-ended text generation task on Wikidump data~\citep{wikidump}. We train the OpenLlama-3B model to predict $85\%$ tokens per sample given the leading $15\%$ tokens.

\begin{figure}[tb]
\centering
	\includegraphics[width=0.9\columnwidth]{figure/win_rate_gpt3.5_llama_horizontal}
 \vspace{-0.5em}
	\caption{The effect of different generation configuration during inference and different negative sampling methods during training. Unless otherwise specified, greedy decoding is used. Win rate is evaluated by GPT-3.5 against the ground truth continuations.}
	\label{fig:llama_comparison_neg_sample}	
\end{figure}

\vspace{-5pt}
\paragraph{Baselines.} We consider three main baselines: MLE, DPO, and parallel scheduled sampling (\textbf{PSS}, \citep{duckworth2019parallel}). Importantly, the difference between DPO and CPO lies in their negative samples. For DPO, we query GPT-3.5 to generate \textit{unhelpful} response to the Dolly instructions. PSS is trained to sample 3 sequences for each training data, and each token is replaced with $p=0.5$ (see \citet{duckworth2019parallel} for more details).
\begin{table}[tb]

 \caption{The win rate of GPT2-XL against the ground truth, samples generated by greedy decoding, evaluated by GPT-3.5.}
 \vspace{-0.5em}
\resizebox{\columnwidth}{!}{

\begin{tabular}{ c|c|c|c|c|c|c|c|c } 
  \toprule
  & \textsc{MLE} & PSS & DPO & \textsc{ANR} & \multicolumn{4}{c}{\textsc{MixNR}}  \\ 
  \midrule
  $\alpha$ & - & - & - & - & 0 & 0.5 & 0.7 & 0.9\\
  \midrule
  WinRate & 0.471 & 0.086 & 0.383 & \bf 0.506 & 0.476 & 0.479 & 0.487 & 0.485\\ 
  \bottomrule
  \hline
\end{tabular}
}
\vspace{-0.5em}
\label{table:gpteval}
\end{table}
\vspace{-5pt}
\paragraph{Training details.} Throughout the experiment, we fix the learning rate to be $1e-5$, we use the AdamW optimizer with weight decay of $0.05$. We keep the batch size to be $64$. Unless otherwise specified, for the baseline model, we train GPT2-XL and OpenLlama-3B with the next token prediction loss for $2000$ steps. Using these models as the reference model $\pr$, we continue to train with the CPO objective either with or without ranking signals, with $\beta=5$, for $1000$ steps. For both models, each training data in a batch contains $11$ negative samples in total. For MixN and MixNR, we also use a negative sample size of $11$, consisting of $3$ BN, $5$ MN, and $3$ TN. The MLE models used for evaluation are continually trained for the same number of steps from the reference model, like the CPO models. All experiments are conducted on two AWS machines, each with $8$ A100 GPUs.

\vspace{-5pt}
\paragraph{Evaluation.} As discussed in \citet{goyal2022news}, almost all automated evaluation metrics have been shown to not align with human evaluations in the modern era of LLMs, so we decide to use GPT \citep{brown2020language} as the evaluator. See the query template in the appendix.
% The query template for the Dolly instruction-following is the following: ``\texttt{For the following query to a chatbot, which response is more helpful?$\backslash$n
%     Query: \{\}$\backslash$n
%     Response A: \{\}$\backslash$n
%     Response B: \{\}$\backslash$n
%     State only "A" or "B" to indicate which response is more helpful.$\backslash$n
%     More helpful:}''
For efficiency, we generate and evaluate $1000$ samples chosen from the 7506 test set. A similar template is used for Wiki text generation, see the detail in the appendix. During inference, we consider greedy decoding, top-$p$ and top-$k$ sampling, as well as beam search.


\begin{table*}[tb]
\centering
\caption{The win rate of OpenLlama-3B trained with CPO and MLE against the ground truth data in Dolly, sampled by greedy decoding, evaluated by GPT-3.5. $ \normalfont\textsc{MLE}_1$, \textsc{ANR} and \textsc{AN} are trained for 200 steps, the rest models are trained for 1000 steps. The best CPO model outperforms the MLE baseline by 13.8\% win rate.}
\vspace{-0.5em}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ c|c|c|c|c|c|c|c|c|c|c|c|c|c } 
  \toprule
  &$ \normalfont\textsc{MLE}_1$ & PSS & DPO & \textsc{ANR} & \textsc{AN} & $ \normalfont\textsc{MLE}_2$ & \multicolumn{6}{c|}{\textsc{MixNR}} & \textsc{MixN} \\ 
  \midrule
  $\alpha$ &  -  &   -  & - & - & - & - &0 & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 & -\\
  \midrule
  WinRate &0.505 &0.270 & 0.555 & \bf 0.643 & 0.56 & 0.522 & 0.608 & 0.620 & 0.614 & 0.610 & 0.601 & 0.550 & 0.576 \\ 
  \bottomrule
  \hline
\end{tabular}
}
\vspace{-0.5em}
\label{table:openllama3beval}
\end{table*} 

\vspace{-5pt}
\paragraph{Weight-space ensemble.} Previous works \citep{liu2022brio} have also suggested to combine the auxilliary loss function with the MLE training objective $\alpha\mathcal{L}_{\mathrm{MLE}}+\mathcal{L}_{\mathrm{CPO}}$, the downside of combining loss functions in this way is that for a different choice of $\alpha$ one will have to retrain the model. To investigate the importance of loss combination, we instead perform a weight-space ensemble \citep{wortsman2022robust}. In particular, denote $\theta_{\mathrm{CPO}}$ and $\theta_{\mathrm{MLE}}$ the model parameters trained solely with CPO or MLE respectively, we generate with the interpolated weights $\theta = \alpha \theta_{\mathrm{MLE}} + (1-\alpha) \theta_{\mathrm{CPO}}$. 


%\subsection{Analysis}
\section{Experimental Analysis}
\subsection{Instruction-Following Task}
Our proposed CPO method with various negative sampling strategies consistently outperforms the MLE baseline models on the Dolly instruction-following task. Using greedy sampling with GPT2-XL, the CPO model has a clear margin over the MLE model, and CPO+ANR has a $3.5\%$ higher win rate, see \cref{table:gpteval}. Note that CPO incurs very little computation overhead during the actual training: the overhead only comes a larger batch size, and even if we generate the negative samples autoregressively, it is a one-time offline cost. 

The improvement in OpenLlama-3B is more significant: CPO+ANR has a $13.8\%$ higher win rate than the MLE baseline, and CPO+MixNR has a $9.8\%$ higher win rate in \cref{table:openllama3beval}. We also observe that weight-space ensemble has a positive impact on the model. Heuristically, for OpenLlama-3B, a smaller $\alpha$ is preferred (more emphasis on the CPO weights) (\cref{table:openllama3beval}), but the reverse holds for GPT2-XL (\cref{table:gpteval}). We hypothesize that the choice of $\alpha$ should depend on the model: if the model is more capable, then it can benefit more from CPO. Here, we show the existence of a good $\alpha$, and we leave further exploration to future research. 
\vspace{-5pt}
\paragraph{Comparison with DPO and PSS.} The proposed CPO method performs better than other two baseline methods: DPO and PSS   (see Table~\ref{table:openllama3beval}).
%Interestingly, DPO does not outperform CPO (note that the difference between DPO and CPO lies in the negative samples). 
We believe that DPO performs poorly because unhelpful/irrelevant continuations (even generated by ChatGPT) do not provide a very strong signal as human generated samples. Unlike in alignment, where the toxic/harmful samples provide a clear indication of what not to generate, here it is not clear what DPO can gain from merely a single irrelevant sample. On the other hand, CPO can benefit from larger negative sample size. 
\vspace{-5pt}
\paragraph{Sampling Strategy.} In addition to greedy decoding, we also experiment with different choice of sampling strategies. In all settings, CPO has consistently demonstrated superior performance over MLE, see \cref{fig:llama_comparison_neg_sample}.

%To test whether the proposed CPO objective and the negative sampling strategies scale up to larger models, we also conduct experiments with OpenLlama-3b \citep{touvron2023llama,openlm2023openllama}. Using the same training and testing split of Dolly, we train the OpenLlama model with MLE loss for $2000$ steps as the baseline model, and continually train it with CPO for $1000$ steps. We use a negative sample size of $11$, consisting $3$ BN, $5$ MN, and $3$ TN. The greedy decoding results are listed in \cref{table:openllama3beval}. Compare to GPT2-XL, OpenLlama-3B benefits more from CPO.

\vspace{-5pt}
\paragraph{Effect of different negative samples.} We perform a study on the effects of different negative sampling strategies; the results are presented in \cref{fig:llama_comparison_neg_sample}. We first train the OpenLlama-3B model with MLE loss for 1000 steps, then continue to train with CPO for 200 steps. For all ground truth sequences, we use $4$ negative sequences. In this setting, we always use the ranking information to train CPO. We observe that the effects of BNR and TNR on the reward model preference are similar and that they perform slightly better than MNR. 





\begin{table}[tb]
\centering 
\caption{OpenLlama-3B's win rate against the ground truth continuation on Wikidump. The model is trained with either MLE or CPO+BNR. Weight ensemble is adopted. The best CPO model outperforms the MLE baseline by 3\% win rate.}
\vspace{-0.5em}
\resizebox{0.9\columnwidth}{!}{

\begin{tabular}{ c|c|c|c|c|c } 
  \toprule
  & \textsc{MLE} &  \multicolumn{4}{c}{\textsc{BNR}}  \\ 
  \midrule
  $\alpha$ & - & 0 & 0.5 & 0.7 & 0.9\\
  \midrule
  WinRate & 0.508 & 0.455 & 0.505 & 0.5 & \bf 0.538\\ 
   \bottomrule
  \hline
\end{tabular}
}
\vspace{-0.5em}
\label{table:wikitexteval}
\end{table}



\subsection{Open-ended Text Generation Task}
We further test OpenLlama-3B's ability on an open-ended text generation task with CPO. Using Wikidump data \citep{wikidump}, for each test sample, we take its first $15\%$ tokens as the prefix and train the model with CPO on the rest $85\%$. For negative sampling, we use four BNR examples. The results in~\cref{table:wikitexteval} show that 
%with optimal weight interpolation coefficient $\alpha$, CPO can greatly 
CPO can improve the model's win rate against the MLE baseline by $3\%$. 
%The results also have a different pattern compared to the instruction-following task: the optimal choice of $\alpha$ shows a reverse trend. With the Dolly dataset we observes a small optimal $\alpha$, but on the Wiki dataset we see a large optimal $\alpha$. It is likely because the negative samples here are too noisy, since only $15\%$ prefixes are provided.
We observe that increasing $\alpha$ improves the score, the opposite of the instruction-following task. It is likely because the negative samples here are too noisy, since only $15\%$ prefixes are provided.

Additionally, we test the MAUVE score \citep{pillutla2021mauve} of MLE and CPO compared to the ground truth. See the results in \cref{fig:mauve} and \cref{table:wikimauve}.

\begin{figure}
	\centering
	\includegraphics[width=0.8\columnwidth]{figure/mauve_curve.pdf}
	\caption{MAUVE score of MLE and CPO on Wiki data.}
	\label{fig:mauve}
\end{figure}

\begin{table}[tb]
\centering 
\caption{MAUVE score of MLE and CPO on Wiki data.}
\vspace{-0.5em}
\resizebox{\columnwidth}{!}{
\begin{tabular}{ c|c|c|c|c|c|c|c } 
  \toprule
  & \textsc{MLE} &  \multicolumn{4}{c}{\textsc{CPO BNR}}  \\ 
  \midrule
  $\alpha$ & - & 0 & 0.1 & 0.3 & 0.5 & 0.7 & 0.9\\
  \midrule
  WinRate & 0.524 & 0.610 & 0.627 & \bf 0.673 & 0.645 & 0.651 & 0.668\\ 
   \bottomrule
  \hline
\end{tabular}
}
\vspace{-0.5em}
\label{table:wikimauve}
\end{table}


