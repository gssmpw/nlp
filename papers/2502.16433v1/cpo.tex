\section{Proposed approach}\label{sec:approach}
Consider a sentence of $T$ tokens $\vx=\{\vx_1,\ldots, \vx_T\}\in\gX$,
% and let $P$ be the unknown target language distribution, $\tilde P(\vx)$ be the empirical distribution of the training data (which is an approximation of $P$), and $Q$ be the distribution of our model at hand. Since our paper is also closely related to RLHF, 
 we use $\pi(\vx)$ to represent the distribution of $\vx$ under some language policy $\pi$. In particular, we write $\pi_\theta$ for a distribution that is parameterized by $\theta$, where $\theta$ is usually the set of trainable parameters of the LLM; we write $\pr$ for a reference distribution that should be clear given the context. Inspired by DPO, we introduce our CPO objective:
\begin{equation}\label{eq:norankcpo}
\resizebox{0.85\columnwidth}{!}{%
$
\begin{split}
		\mathcal{L}_{\mathrm{CPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)=
		\underset{\substack{ (\vx,\vy_1) \sim \mathcal{D} \\  \vy_2, \ldots, \vy_K\sim\mathcal{A}} }{\mathbb{E}} \left[\log \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_1 \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_1 \mid \vx\right)}\right)}{\sum_{j=1}^K \exp \left(\beta \log \frac{\pi_\theta\left(\vy_j \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_j \mid \vx\right)}\right)}\right].
\end{split}
$
}
\end{equation}
Here $(\vx, \vy_1)$ is the ground truth prefix-continuation pair from the natural language distribution $\mathcal{D}$, and $\vy_2,\ldots, \vy_K$ are $K-1$ negative continuations sampled from a to-be-discussed distribution $\mathcal{A}$. The derivation is deferred to the appendix. If some ranking of the data quality is presented, i.e. $\tau:[K]\to [K]$ where $\tau(i)<\tau(j)$ means $\vy_i$ is preferred over $\vy_j$, we also have the following CPO objective with ranking:
\begin{equation}\label{eq:rankcpo}
\resizebox{0.85\columnwidth}{!}{%
$
\begin{split}
		\mathcal{L}_{\mathrm{CPO}}\left(\pi_\theta, \pi_{\mathrm{ref}}\right)=
	\underset{\substack{\tau, (\vx,\vy_1) \sim \mathcal{D} \\  \vy_2, \ldots, \vy_K\sim\mathcal{A}} }{\mathbb{E}} \left[\log {\displaystyle \prod_{k=1}^K} \frac{\exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(k)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(k)} \mid \vx\right)}\right)}{{\displaystyle \sum_{j=k}^K} \exp \left(\beta \log \frac{\pi_\theta\left(\vy_{\tau(j)} \mid \vx\right)}{\pi_{\mathrm{ref}}\left(\vy_{\tau(j)} \mid \vx\right)}\right)}\right].
\end{split}
$
}
\end{equation}

Unlike RLHF or DPO, which require human preference data $\vy_1\geq \vy_2\geq\cdots\geq\vy_K$, CPO requires only ground truth data $(x, \vy_1)\sim \mathcal{D}$, and $K-1$ synthetic negative samples $\vy_2,\ldots,\vy_K\sim\mathcal{A}$. Possibly, we can also get a ranking among the $K-1$ synthetic samples in a fully automatic way. On a high level, CPO implicitly rewards the ground truth more than the synthetic negative samples.

We consider four ways to generate synthetic data. (1) \textbf{autoregressive negatives (AN)}: We use the language model to autoregressively generate the negative samples given a prefix. We fixed the synthetic data generation strategy to be top-$k$ sampling with $k=50$. (2) $\textbf{batch negatives (BN)}$: given a batch of prefixes and continuations $\{\vx_i, \vy_i\}_{i=1}^b$, the negative samples to the prefix $\vx_i$ are composed of $\{\vy_j\}_{j\neq i}$. (3) $\textbf{meanfield negatives (MN)}$: given a sequence $\vy = \{y_1,\ldots, y_T\}$, we randomly select $c$ percent of the positions $\{t_1,\ldots, t_j\}\subseteq [T]$, and substitute each $y_{t_i}$ independently based on $\pi_\theta(y_{t_i}|y_1,\ldots,y_{t_i})$, i.e. we independently resample $c\%$ of the tokens according to their original autoregressive distribution. (4) $\textbf{truncation negatives (TN})$: for each ground truth continuation, we truncate them at a random position and append an extra EOS token at the end.
% \begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
% 	\item \textbf{autoregressive negatives (AN)}: We use the language model to autoregressively generate the negative samples given a prefix. We fixed the synthetic data generation strategy to be top-$k$ sampling with $k=50$.
% 	\item $\textbf{batch negatives (BN)}$: given a batch of prefixes and continuations $\{\vx_i, \vy_i\}_{i=1}^b$, the negative samples to the prefix $\vx_i$ are composed of $\{\vy_j\}_{j\neq i}$.
% 	\item $\textbf{meanfield negatives (MN)}$: given a sequence $\vy = \{y_1,\ldots, y_T\}$, we randomly select $c$ percent of the positions $\{t_1,\ldots, t_j\}\subseteq [T]$, and substitute each $y_{t_i}$ independently based on $\pi_\theta(y_{t_i}|y_1,\ldots,y_{t_i})$. 
% 	\item $\textbf{truncation negatives (TN})$: for each ground truth continuation, we truncate the continuation at a random position and append an extra EOS token at the end.
% \end{enumerate}


In our experiments, we observe that CPO can often benefit from a ranking among $K$ samples, where the ranking is based on their cosine similarity to the ground truth. Let $\ve_1,\ldots, \ve_K$ be the embeddings of given sequences $\vy_1,\ldots, \vy_K$ and without loss of generality assume that $\ve_1$ is the ground truth, we define $\tau(i)<\tau(j)$ if $\frac{\ip{\ve_i}{\ve_1}}{\|\ve_i\|\|\ve_1\|}>\frac{\ip{\ve_j}{\ve_1}}{\|\ve_j\|\|\ve_1\|}$, with the lower ranking index indicating the better sample. Using the objective \cref{eq:rankcpo}, this process gives us denser signals during training and can lead to better downstream performance.


 