\section{Introduction}


Next token prediction is now the predominant way for pre-training and supervised fine-tuning (SFT) of large language models (LLM). This loss function can be easily scaled up to train models with trillions of parameters and tokens, and it has demonstrated the ability to generate coherent and contextually relevant text. Let $P$ be the unknown target language distribution and let $Q$ be the distribution of our model at hand. The goal of next token prediction is to minimize the \textit{forward-KL} divergence between $P$ and $Q$. This training process only supervises the prediction of one token at a time, given the full context of the ground truth. On the other hand, during inference, the model needs to generate a whole sequence (for a given prompt) relying on its own prior predictions. This mismatch between the training and inference stage is known as \textit{exposure-bias} in the literature of RNN and sequence-to-sequence model \citep{bengio2015scheduled,ranzato2015sequence}.

In other words, next token prediction injects only \textit{token-level} information into the model, but missing \textit{sequence-level} signal. The latter requires a generation of a longer horizon, which often relies on reinforcement learning algorithms; for example, reinforcement learning with human feedback (RLHF) \citep{ouyang2022training}; and is computationally expensive.
In this work, we ask the following question:
	\textit{Can we introduce sequence-level information in LLM pre-training / SFT with a small computational cost?}

We answer the question affirmatively with our proposed \textsc{\textbf{C}ontrastive \textbf{P}reference \textbf{O}ptimization} (CPO) method. The goal of CPO is to improve \textbf{generation quality}. Unlike RLHF, the proposed CPO method does not require human preference information as the training signal. While we demonstrate CPO in the SFT case, the loss can be seemlessly applied to the late stage of pretraining as well.

%Another related method that optimizes the quality of generated text is BRIO \citep{liu2022brio}. Although unlike BRIO, the proposed CPO method does not necessarily rely on autoregressively sampled negative sequences from the model and therefore is much more computational efficient and easier to scale up. In addition, CPO is also derived from a more principled statistical perspective. Our experiments demonstrate that CPO is able to improve the quality of text generation in terms of reward model scores and reverse-KL divergence.
