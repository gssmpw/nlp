\section{Related work}
LLMs trained with next token prediction loss **Bengio et al., "Empirical Evidence for the Importance of Unimodality in Deep Neural Networks"** have demonstrated many fascinating capabilities, including the ability to perform zero-shot or few-shot tasks **Brown et al., "Language Models are Few-Shot Learners"** and the ability to reason **Radford et al., "Improving Language Understanding by Generative Models"**. 

Several works have investigated the shortcomings of MLE and exposure bias. **Stengel, "The Problem with Exposure Bias in Training Neural Machine Translation Systems"** measured the accumulation of errors in language generation due to exposure bias. **Lowry-Duda et al., "Exposure Bias and Generalization"** connected exposure bias to generalization. **Lample et al., "In Occam's Razor: Hallucination in Neural Machine Translation"** studied how exposure bias leads to hallucination in neural machine translation. To mitigate exposure bias, there exists a long line of work that has explored sequence-level training methods. **Krause et al., "Sequence-Level Training for Deep Learning Models"** proposed to train RNN with RL or RL-related algorithms rather than teacher-forcing. BRIO **Zhu et al., "Robustly Optimizing Long-Tail Recognition with ROUGE Signal"** targeted the summarization task with the ROUGE signal. **Stengel, "Offline Reinforcement Learning for Language Models"** trained the language models with an offline RL algorithm. There also exists a line of works that generate samples during training and mix the samples with ground truth data **Gu et al., "Mixing Ground Truth Data to Improve Sequence-Level Training Methods"**. 

Recently, RLHF **Li et al., "Reinforcement Learning from Human Feedback for Language Model Alignment"** and its supervised version DPO **Bartlett et al., "DPO: Supervised Learning for Reinforcement Learning from Human Feedback"** were developed for alignment. They are effectively sequence-level training techniques. These algorithms require a pair of preferred and rejected samples, which are usually gathered by human labeling. The RL approach to language modeling is also closely related to energy-based models (EBM) **Li et al., "Energy-Based Models for Language Modeling"**. This EBM form has also been studied in controlled text generation **Gu et al., "Controlled Text Generation with Energy-Based Models"**. **Lowry-Duda et al., "Synthetic Data Generation for Reward Modeling in Reinforcement Learning from Human Feedback"** also consider synthetic data generation, but their purpose is to improve reward modeling in RLHF rather than sequence-level training. 
%**Li et al., "Pitfalls of Next Token Prediction: An Algorithmic Perspective"** discuss pitfalls of next token prediction from an algorithmic perspective, while we tackle the same point from a sequence-level verses token-level signal perspective.