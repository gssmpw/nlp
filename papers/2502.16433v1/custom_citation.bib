@article{bengio2015scheduled,
  title={Scheduled sampling for sequence prediction with recurrent neural networks},
  author={Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@article{ranzato2015sequence,
  title={Sequence level training with recurrent neural networks},
  author={Ranzato, Marc'Aurelio and Chopra, Sumit and Auli, Michael and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1511.06732},
  year={2015}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}
@article{feng2023leveraging,
  title={Leveraging multiple descriptive features for robust few-shot image learning},
  author={Feng, Zhili and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2307.04317},
  year={2023}
}
@article{menon2022visual,
  title={Visual classification via description from large language models},
  author={Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2210.07183},
  year={2022}
}
@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}
@article{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}
@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}
@article{stiennon2020learning,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3008--3021},
  year={2020}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{korbak2022rl,
  title={RL with KL penalties is better viewed as Bayesian inference},
  author={Korbak, Tomasz and Perez, Ethan and Buckley, Christopher L},
  journal={arXiv preprint arXiv:2205.11275},
  year={2022}
}
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}
@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}
@article{plackett1975analysis,
  title={The analysis of permutations},
  author={Plackett, Robin L},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={24},
  number={2},
  pages={193--202},
  year={1975},
  publisher={Oxford University Press}
}
@book{luce2012individual,
  title={Individual choice behavior: A theoretical analysis},
  author={Luce, R Duncan},
  year={2012},
  publisher={Courier Corporation}
}
@article{deng2020residual,
  title={Residual energy-based models for text generation},
  author={Deng, Yuntian and Bakhtin, Anton and Ott, Myle and Szlam, Arthur and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:2004.11714},
  year={2020}
}
@inproceedings{kumar2022gradient,
  title={Gradient-based constrained sampling from language models},
  author={Kumar, Sachin and Paria, Biswajit and Tsvetkov, Yulia},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={2251--2277},
  year={2022}
}
@article{liu2022brio,
  title={BRIO: Bringing order to abstractive summarization},
  author={Liu, Yixin and Liu, Pengfei and Radev, Dragomir and Neubig, Graham},
  journal={arXiv preprint arXiv:2203.16804},
  year={2022}
}
@article{pillutla2021mauve,
  title={Mauve: Measuring the gap between neural text and human text using divergence frontiers},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4816--4828},
  year={2021}
}
@article{chen2023accelerating,
  title={Accelerating large language model decoding with speculative sampling},
  author={Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  journal={arXiv preprint arXiv:2302.01318},
  year={2023}
}
@article{goyal2022news,
  title={News summarization and evaluation in the era of gpt-3},
  author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
  journal={arXiv preprint arXiv:2209.12356},
  year={2022}
}
@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}
@InProceedings{pmlr-v162-ethayarajh22a,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher = {PMLR},
}
@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}
@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}
@inproceedings{wortsman2022robust,
  title={Robust fine-tuning of zero-shot models},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7959--7971},
  year={2022}
}
@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}
@article{fu2023gptscore,
  title={Gptscore: Evaluate as you desire},
  author={Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
  journal={arXiv preprint arXiv:2302.04166},
  year={2023}
}
@inproceedings{gutmann2010noise,
  title={Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author={Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={297--304},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@article{ma2018noise,
  title={Noise contrastive estimation and negative sampling for conditional models: Consistency and statistical efficiency},
  author={Ma, Zhuang and Collins, Michael},
  journal={arXiv preprint arXiv:1809.01812},
  year={2018}
}
@ONLINE{wikidump,
    author = "Wikimedia Foundation",
    title  = "Wikimedia Downloads",
    url    = "https://dumps.wikimedia.org"
}
@inproceedings{10.5555/2969239.2969370,
author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used succesfully in our winning entry to the MSCOCO image captioning challenge, 2015.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
pages = {1171–1179},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}
@article{krishna2021hurdles,
  title={Hurdles to progress in long-form question answering},
  author={Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
  journal={arXiv preprint arXiv:2103.06332},
  year={2021}
}
@article{pang2020text,
  title={Text generation by learning from demonstrations},
  author={Pang, Richard Yuanzhe and He, He},
  journal={arXiv preprint arXiv:2009.07839},
  year={2020}
}
@article{arora2022exposure,
  title={Why Exposure Bias Matters: An Imitation Learning Perspective of Error Accumulation in Language Generation},
  author={Arora, Kushal and Asri, Layla El and Bahuleyan, Hareesh and Cheung, Jackie Chi Kit},
  journal={arXiv preprint arXiv:2204.01171},
  year={2022}
}
@article{wang2020exposure,
  title={On exposure bias, hallucination and domain shift in neural machine translation},
  author={Wang, Chaojun and Sennrich, Rico},
  journal={arXiv preprint arXiv:2005.03642},
  year={2020}
}
@article{schmidt2019generalization,
  title={Generalization in generation: A closer look at exposure bias},
  author={Schmidt, Florian},
  journal={arXiv preprint arXiv:1910.00292},
  year={2019}
}
@article{shen2015minimum,
  title={Minimum risk training for neural machine translation},
  author={Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
  journal={arXiv preprint arXiv:1512.02433},
  year={2015}
}
@article{zhang2019bridging,
  title={Bridging the gap between training and inference for neural machine translation},
  author={Zhang, Wen and Feng, Yang and Meng, Fandong and You, Di and Liu, Qun},
  journal={arXiv preprint arXiv:1906.02448},
  year={2019}
}
@article{duckworth2019parallel,
  title={Parallel scheduled sampling},
  author={Duckworth, Daniel and Neelakantan, Arvind and Goodrich, Ben and Kaiser, Lukasz and Bengio, Samy},
  journal={arXiv preprint arXiv:1906.04331},
  year={2019}
}
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}
@article{wettig2022should,
  title={Should you mask 15\% in masked language modeling?},
  author={Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2202.08005},
  year={2022}
}
@article{pace2024west,
  title={West-of-N: Synthetic Preference Generation for Improved Reward Modeling},
  author={Pace, Aliz{\'e}e and Mallinson, Jonathan and Malmi, Eric and Krause, Sebastian and Severyn, Aliaksei},
  journal={arXiv preprint arXiv:2401.12086},
  year={2024}
}
@article{bachmann2024pitfalls,
  title={The pitfalls of next-token prediction},
  author={Bachmann, Gregor and Nagarajan, Vaishnavh},
  journal={arXiv preprint arXiv:2403.06963},
  year={2024}
}