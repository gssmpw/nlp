% \begin{table*}
% \caption{\label{tab:overall}
%     Results of evaluating LLMs on \name dataset. The overall average (Avg.) score is bold in each row.
% }
% \begin{threeparttable}
% \centering
% \begin{tabular}{
%   l % Left align for Models
%   % l % Left align for Metrics
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for DF
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for MF
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for Avg.
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for IP
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for AP
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for Avg.
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for LI
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for SI
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for IT
%   >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for Avg.
%   c % Centered for Overall Avg.
% }
%     \toprule
%     \toprule
%     \multirow{2}{*}{Models} &
%     \multicolumn{3}{c}{Sequence understanding} & \multicolumn{3}{c}{Action prediction} & \multicolumn{4}{c}{Interest perception} & \multirow{2}{*}{Overall Avg.} \\
%     & DF & MF & Avg. & IP & AP & Avg. & LI  & SI & IT & Avg. & \\
%     \cmidrule(lr){1-12}
%     Text-based & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx \\
%     \cmidrule(lr){1-12}
%     MLP4Rec 
%     & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx & x.xx \\
%     \cmidrule(lr){1-12}
%     GRU4Rec 
%     & 33.67 & 23.44 & 32.86 & 34.94 & 30.02 & 32.20 & 46.98 & 41.33 & 45.56 & 43.97 & \textbf{37.15} \\ 
%     \cmidrule(lr){1-12}
%     SASRec 
%     & 40.41 & 24.22 & 39.14 & 42.49 & 35.27 & 38.47 & 59.20 & 53.32 & 56.39 & 55.72 & \textbf{45.66} \\
%     \cmidrule(lr){1-12}
%     Mamba4Rec 
%     & 30.54 & 18.75 & 29.62 & 34.25 & 28.10 & 30.83 & 38.82 & 34.69 & 35.64 & 36.06 & \textbf{32.69} \\ 
%     \cmidrule(lr){1-12}
%     HSTU 
%     & 43.29 & 25.00 & 41.86 & 47.30 & 36.53 & 41.31 & 70.85 & 60.06 & 65.67 & 64.45 & \textbf{50.88} \\
%     \cmidrule(lr){1-12}
%     Trm++
%     & 47.28 & 27.34 & 45.72 & 48.67 & 37.35 & 42.38 & 75.25 & 64.14 & 70.10 & 68.71 & \textbf{53.88} \\
%     % Trm++
%     % & 44.02 & 23.44 & 42.41 & 47.64 & 36.12 & 41.23 & 73.87 & 63.48 & 75.07 & 69.33 & 52.96 \\
%     \bottomrule
%     \bottomrule
% \end{tabular}
% \end{threeparttable}
% \end{table*}



\section{Experiments}

We assess the performance of various user embeddings in prompting LLMs for personalized question answering. Specifically, we want to reveal the answers to the following research questions:
\begin{itemize}[leftmargin=*]
\item \textbf{RQ1}: What is the overall performance of user embeddings in prompting LLMs for personalized question answering?
\item \textbf{RQ2}: How is the contribution of each component?
\item \textbf{RQ3}: What is the scaling law of user embeddings in prompting LLMs for personalized question answering?
\item \textbf{RQ4}: How is the efficiency of embedding-based GRs?
% \item \textbf{RQ4}: Can user embeddings assist LLMs in considering user personalization in free question answering?
\end{itemize}

\subsection{Experimental Setup}
\subsubsection{\textbf{Datasets}}
We randomly split $31,317,087$ interactions from source data into $9:1$ for the training and test sets. The training set serves in the pre-training and fine-tuning stages. Based on the test set, we generate $7,192$ personalized Q\&A  for the evaluating stage.
\subsubsection{\textbf{Models}}
The user embeddings to be evaluated are produced by encoder models. 
We experiment on several state-of-the-art models to comprehensively evaluate embedding-based GRs. 
\begin{itemize}[leftmargin=*]
\item \textbf{GRU4Rec}~\cite{hidasi2015session}: leverages the gated recurrent units (GRUs), the famous class in recurrent neural networks (RNNs) to learn the user sequential representation.
\item \textbf{SASRec}~\cite{kang2018self}: applies self-attention, the core of transformers, to embed user interests from interaction sequences.
\item \textbf{Mamba4Rec}~\cite{liu2024mamba4rec}: captures user temporal dynamics using state space models (SSMs). Employing a selective mechanism, it identifies important states while ignoring irrelevant ones. 
\item \textbf{HSTU}~\cite{zhai2024actions}: redefines recommendations as sequential transduction tasks within GRs. It introduces a novel module, HSTU, specifically designed for industrial recommendation scenarios.
\item \textbf{Trm++}~\cite{zhai2024actions}: is a prominent Transformer variant~\cite{touvron2023llama} leverages upgraded architecture accompanied by RoPE~\cite{su2024roformer}, SwiGLU.
\end{itemize}
As illustrated in Figure~\ref{fig:overview}, the user embeddings will serve as the soft prompts to contextualize LLMs to generate personalized responses conforming to user interests. To guarantee fairness, we utilize a uniform backbone LLM, Qwen2.5-3b-instruct~\cite{yang2024qwen2}, for all encoder models. We also consider text-based GRs for comparison as follows.
\begin{itemize}[leftmargin=*]
\item \textbf{Text20 \& Text50}: leverages user sequence of $20$ and $50$ interactions, respectively, as text prompt of LLMs to generate responses.
\end{itemize}
% \begin{equation}
% \text{ROUGE-N} = \frac{\sum_{S \in \text{References}} \sum_{\text{ngram} \in S} \text{Count}_{\text{match}}(\text{ngram})}{\sum_{S \in \text{References}} \sum_{\text{ngram} \in S} \text{Count}(\text{ngram})}
% \end{equation}
% % Calculate individual n-gram precision
% \begin{equation}
% \text{Precision}_n = \frac{\sum_{\text{ngram}_n \in \text{Candidate}} \text{Count}_{\text{clip}}(\text{ngram}_n)}{\sum_{\text{ngram}_n \in \text{Candidate}} \text{Count}(\text{ngram}_n)}
% \end{equation}
% % Calculate BLEU without brevity penalty
% \begin{equation}
% \text{BLEU} = \exp \left( \frac{1}{N} \sum_{n=1}^{N} \log(\text{Precision}_n) \right)
% \end{equation}


% In our experiments, we employ both ROUGE and BLEU metrics to evaluate the quality of the generated text.
% ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics primarily used for evaluating automatic summarization and other text generation tasks. It measures the overlap between the n-grams in the generated text and the reference text. Specifically, ROUGE-N evaluates the recall of n-grams, with common variants including ROUGE-1 (unigrams) and ROUGE-2 (bigrams). ROUGE-L, on the other hand, considers the longest common subsequence, capturing sequence-level matches between the generated and reference texts.
% BLEU (Bilingual Evaluation Understudy) is a metric widely used in machine translation to assess the quality of text generation. BLEU calculates the precision of n-grams between the generated output and one or more reference texts. It combines precision across n-gram levels (typically from 1-gram to 4-gram) using a geometric mean and incorporates a brevity penalty to account for overly short translations. This ensures that the metric reflects not only precise matches but also encourages outputs that are sufficiently detailed.
% Both metrics offer complementary insights into the effectiveness of the text generation model, with ROUGE providing a recall-oriented perspective and BLEU emphasizing precision. Together, they facilitate a comprehensive evaluation of our model's performance.

% In \name\footnote{\url{https://github.com/ming429778/\name}}, we assess the e-commerce capabilities of existing advanced LLMs, including five close-sourced LLMs, i.e., Gemini-1.5-pro, GPT-4 turbo/GPT-4~\citep{OpenAI2023GPT4}, Claude-3, Qwen2-max~\cite{yang2024qwen2}, and seven open-sourced LLMs, i.e., ChatGLM3-6B~\cite{du2021glm}, Yi-1.5-6B/Yi-1.5-34B~\citep{young2024yi}, Llama3-8B/Llama3-70B~\citep{touvron2023llama}, Qwen2-7B/Qwen2-72B~\cite{yang2024qwen2}. We apply the API for large-scale generation for close-sourced LLMs. For open-sourced LLMs, we download and evaluate the model from Huggingface\footnote{\url{https://huggingface.co}}. Notice that we use \textbf{fine-tuned models} (e.g., Chat, Instruct version) for fairness. 
% To comprehensively evaluate LLMs' capabilities, we employ two prompting approaches: \textbf{zero-shot} and \textbf{few-shot} settings in two dimensions of e-commerce knowledge: \textbf{common} knowledge and \textbf{abstract} knowledge. 

\subsubsection{\textbf{Implementation Details}}
Following the previous research~\cite{sun2019bert4rec,kang2018self}, we initialize the trainable parameters using Gaussian distribution. We employ the AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of 1e-4 and mini-batch size $256$ to optimize all models. Based on the configurations in prior studies~\cite{sun2019bert4rec, zhang2019feature, kang2018self} and maintaining comparability, we ensure that all model sizes are on the same order of magnitude (approximately 20M) and globally adjust the hyperparameters. For instance, regarding GRU4Rec, we utilize a $3$-layer GRU module with a hidden size of $1024$, and for SASRec, the number of layers is set to $L=6$, each with $h=8$ attention heads and dimension size $d=1024$, with an inner size (i.e., of FNN layers) of $1536$. We utilize $N=512$ as the global sequence length. 
We zero-pad sequences shorter than $N$ and truncate those longer than $N$.
The input and output layers share an item embedding matrix, and the dropout rate is set to $0.1$ to alleviate overfitting. The maximum numbers of epochs for pre-training and fine-tuning are $10$ and $3$, respectively. 
Our implementations\footnote{We release code and data at~\url{https://github.com/OpenStellarTeam/UQABench} to encourage reproducibility and facilitate further research on this topic.} are conducted in the environment using PyTorch 2.4.0 and Python 3.10.13 with $8\times$ GPUs.

% Based on previous research~\cite{kang2018self,sun2019bert4rec}, we initialize the trainable parameters using a Gaussian distribution. To optimize all encoder models, we employ the AdamW optimizer~\cite{kingma2014adam} with learning rate 1e-3 and mini-batch size $256$. Consistent with the configurations used in prior studies~\cite{sun2019bert4rec, zhang2019feature, kang2018self} and maintaining comparability, we uniformly adjust the hyperparameters across all models: Transformer layers are set to $L=2$, each with $h=8$ attention heads, and an inner size (i.e., FNN layers) of $256$. Specifically, we set the dimension size as $d=128$ and the maximum sequence length as $N=200$. Short sequences (i.e., those with $n_i<N$) are padded with zeros, and long sequences (i.e., those with $n_i>N$) are clipped by discarding early interactions. The input and output layers share one item embedding matrix to alleviate the overfitting problem. Training is capped at a maximum of $100$ epochs. Additionally, we utilize an early-stopping criterion, halting training if the NDCG@10 metric on the validation dataset shows a decline for $10$ consecutive epochs. Our implementation leverages PyTorch 1.13 and Python 3.7.15.

% \subsubsection{Metrics}
% In our experiments, we use two frequently used metrics in NLP, ROUGE and BLEU, to evaluate the quality of LLMs' responses.
% ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap between the generated text and the reference text by counting matching n-grams, mainly focusing on recall.
% BLEU (Bilingual Evaluation Understudy) evaluates the precision of n-grams in the generated text compared to reference texts. 
% Using both ROUGE and BLEU allows us to assess the performance of generated text regarding both recall (coverage) and precision (accuracy).

% Suggested by previous works~\cite{kang2018self,sun2019bert4rec}, the trainable parameters are initialized with a Gaussian distribution. We optimize all encoder models with AdamW~\cite{kingma2014adam} and use the default learning rate of 1e-3 and the default mini-batch size of 256. Following the setting of the original paper~\cite{sun2019bert4rec, zhang2019feature, kang2018self} and ensuring a fair comparison, we set the hyperparameters for all models, including Transformer layer as $L=2$, attention head as $h=8$, and inner size (e.g., FNN layers) as $256$. In particular, we set dimension size as $d=128$ and maximum sequence length $N=200$. We further pad short-term sequences (i.e., those with $n_i<N$) by zero to ensure $n_i>d$ for long-term sequential recommendation. 
% The maximum number of training epochs is $100$. 
% Moreover, we adopt the early-stopping training strategy \textcolor{black}{if the NDCG@10 performance on the validation set decreases for $10$ continuous epochs.}
% We implement our model in PyTorch 1.13, Python 3.7.15.

\subsection{Overall Comparison (RQ1)}

We evaluate several advanced user encoder models on proposed \name following the standardized evaluation process (Figure~\ref{fig:overview}) and report the overall performances in Table~\ref{tab:overall}. In general, \name shows excellent discriminative power for different models, demonstrating its evaluation effectiveness. We draw some interesting observations as follows:

\begin{itemize}[leftmargin=*]

\item \textbf{Embedding-based GRs \textit{vs.} Text-based GRs}: 
The best model of embedding-based GRs (i.e., using Trm++) can achieve satisfactory performance on par with text-based GRs, showing the effectiveness of this method in personalization. Remarkably, embedding-based GRs surpass text-based counterparts in item prediction tasks since they are pre-trained to capture more prior information. However, embedding-based GRs perform significantly worse than text-based methods in interest perception tasks, indicating room for improvement in the framework.

\item Among embedding-based methods, HSTU and Trm++ present superior performance, providing high-quality user embeddings to prompt LLMs to give relatively satisfied responses, conforming to user preferences.  
The model performances exhibit anisotropy, meaning the performance of models on each task is not linearly correlated. For example, SASRec performs close to HSTU on sequence understanding and action prediction tasks yet far worse than HSTU on interest perception.

% Among embedding-based methods, HSTU and Trm++ present superior performance, providing high-quality user embeddings to prompt LLMs to give relatively satisfied responses, conforming to user preferences.  
% The models' performances exhibit anisotropy in different tasks, meaning the performance of models on each task is not linearly correlated. For example, SASRec performs par with HSTU on sequence understanding and action prediction tasks, yet far worse than HSTU on interest perception tasks.

\item Regarding evaluation tasks, most encoder models perform significantly well in interest perception tasks, showcasing that the LLM-based system is inclusive and diverse to perceive user personalized interest instead of only popular items. The models' performance on action prediction tasks is also quite acceptable, suggesting that the embedding-based GRs paradigm has potential in traditional recommendation tasks. Nevertheless, the sequence understanding task results are not good enough, especially the sub-task match feature, reflecting that the current framework is immature to completely substitute the industrial feature engineering and needs future improvements.

% \item In general, the results show \name discriminates well between user embeddings produced by different encoder models, demonstrating its evaluation effectiveness. In most cases, HSTU and Trm++ achieve superior performance, which means providing high-quality user embeddings to prompt LLMs to give relatively satisfied responses, according to user personalization.  

% \item Regarding evaluation tasks, all encoder models perform significantly well in interest perception tasks, showcasing the LLM-based system to be more diverse in perceiving user interest instead of just popular items. The performance of all models on action prediction tasks is not bad, which shows that the embedding-based GRs paradigm has the potential to undertake traditional recommendation tasks. Nevertheless, the sequence understanding task results are not good enough, especially the sub-task match features, which reflects that the current framework is immature to substitute the industrial feature engineering and needs future improvements.

\end{itemize}



\subsection{Ablation Study (RQ2)}


\begin{table}[t]
    % \fontsize{8}{11}\selectfont
    \caption{Ablation in pre-training stage.}
        \vspace{-2mm} 
    \begin{threeparttable}
    \begin{tabular}{cccccc}
        \toprule
        \toprule
        \multirow{1}{*}{Training}&\multirow{1}{*}{Inputs}&
        SU & AP & IP & Avg. \cr
        \cmidrule(lr){1-6}
        \multirow{3}{*}{SL}
        & Full Info             & 45.72 & 42.38 & 68.71 & 53.88 \cr
        & \textit{w/o} Text     & 42.08 & 39.27 & 66.84 & 51.16 \cr
        & \textit{w/o} Side     & 39.55 & 37.83 & 61.47 & 47.87 \cr
        \cmidrule(lr){1-6}
        \multirow{3}{*}{CL}
        & Full Info             & 42.41 & 39.25 & 62.37 & 49.40 \cr
        & \textit{w/o} Text     & 37.06 & 35.35 & 58.02 & 44.99 \cr
        & \textit{w/o} Side     & 36.50 & 32.84 & 51.96 & 41.47 \cr

        % & \textit{w/o} Text     & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
        % & \textit{w/o} Side     & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
        \bottomrule
        \bottomrule
    \end{tabular}
    % \vspace{0cm}
    \end{threeparttable}
        \label{tab:ablation_1}
        \vspace{-2mm}
\end{table}


\begin{table}[t]
    % \fontsize{8}{11}\selectfont
    \caption{Ablation in fine-tuning stage.}
        \vspace{-2mm} 
    \begin{threeparttable}
    \begin{tabular}{cccccc}
        \toprule
        \toprule
        \multirow{1}{*}{Integration}&\multirow{1}{*}{Strategy}&
        SU & AP & IP & Avg. \cr
        \cmidrule(lr){1-6}
        \multirow{2}{*}{Mean Pooling}
        & Full FT               & 43.70 & 38.83 & 72.70 & 53.75 \cr
        % & \textit{w/o} Adapter  & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
        & Adapter      & 45.72 & 42.38 & 68.71 & 53.88 \cr
        \cmidrule(lr){1-6}
        \multirow{2}{*}{Q-Former}
        & Full FT               & 45.35 & 44.37 & 69.89 & 55.00 \cr
        % & \textit{w/o} Adapter  & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
        & Adapter      & 31.15 & 29.92 & 31.80 & 30.96 \cr
        \bottomrule
        \bottomrule
    \end{tabular}
    % \vspace{0cm}
    \end{threeparttable}
        \label{tab:ablation_2}
        \vspace{-2mm}
\end{table}

To guarantee evaluation reliability and fairness, we conduct an ablation study in terms of pre-training and fine-tuning stages to determine the optimal settings of the experiments.
\subsubsection{\textbf{Pre-training}}
\label{sec:ablation_1}
The two critical factors in pre-training are model inputs and training methods. We consider three variants of model inputs: (1) \textbf{Full Info}: inputs entire information, including item ID and side information (e.g., category ID and category name), (2) \textbf{\textit{w/o} Text}: leverages the item ID and ID-based side information (e.g., category ID), meaning without textual information, and (3) \textbf{\textit{w/o} Side}: only uses item ID without side information. Furthermore, we consider two training methods: (1) \textbf{SL}: uses the CE loss to supervise training, and (2) \textbf{CL}: utilizes InfoNCE loss to perform contrastive training on encoders. According to the results in Table~\ref{tab:ablation_1}, we observe that the combination of \textbf{Full Info} + \textbf{SL} yields the best performance. Consequently, we utilize such a setting for our experiments.

% The two critical factors in pre-training are model inputs and training methods. We consider three variants of model inputs: (1) \textbf{Full Info}: inputs entire information, including item ID and all side information (e.g., category ID and category name) into the encoder model, (2) \textbf{\textit{w/o} Text}: leverages item ID and ID-based side information (e.g., category ID), meaning without textual information, and (3) \textbf{\textit{w/o} Side}: uses only the item ID as model inputs, meaning without side information. Furthermore, we consider two training methods: (1) \textbf{SL}: uses a supervised learning means (i.e., CE loss) to learn user embeddings, and (2) \textbf{CL}: utilizes a contrastive learning method (i.e., InfoNCE) to train user embeddings. According to the results in Table~\ref{tab:ablation_1}, we find that the settings using \textbf{Full Info} + \textbf{SL} achieve the best performance. Consequently, we utilize such settings for our experiments.

\subsubsection{\textbf{Fine-tuning}}
\label{sec:ablation_2}
We explore the optimal settings in fine-tuning.
We compare two compression approaches in the adapter: (1) \textbf{Mean Pooling}: uses a mean pooling layer to compress user sequence embeddings into a single embedding, (2) \textbf{Q-Former}: leverages Q-Former to generate fixed-length embeddings, set to 16 by default. 
Moreover, we consider two fine-tuning strategies: (1) \textbf{Full FT}: fine-tunes the adapter and LLM together, and (2) \textbf{Adapter}: fine-tunes only the adapter. Table~\ref{tab:ablation_2} shows that the setting of \textbf{Full FT} + \textbf{Q-Former} achieves optimal performance. However, we observe that the Q-Former method is sensitive to the hyperparameters (e.g., learning rate), leading to instability (i.e., emerging NaN loss). Therefore, we opt for a more stable configuration, \textbf{Mean Pooling} + \textbf{Adapter}, as our experiment's essential fine-tuning configuration for consistency and reliability.

% We explore the optimal settings in fine-tuning.
% We compare two compression approaches in the adapter: (1) \textbf{Mean Pooling}: regards the user embeddings as soft prompt and combines it with LLMs' text embedding linearly, (2) \textbf{Q-Former}: integrates the user embeddings with LLMs' text embedding within the cross-attention module. 
% Moreover, we consider two fine-tuning strategies: (1) \textbf{Full FT}: fine-tunes the adapter and LLM together, and (2) \textbf{Adapter}: fine-tunes only the adapter. Table~\ref{tab:ablation_2} shows that the setting of \textbf{Full FT} + \textbf{Cross} achieves optimal performance. However, we observe that the cross-attention method is sensitive to the hyperparameters (e.g., learning rate) and usually collapses. Therefore, we choose a more stable setting, \textbf{Linear} + \textbf{Adapter}, as the essential fine-tuning setting of the entire experiments.



% All tasks: 30.96; Sequence understanding: 31.15; Action prediction: 29.92; Interest perception: 31.80; df: 31.94; mf: 21.88; ip: 31.50; ap: 28.65; li: 31.53; si: 31.56; it: 32.51
% All tasks: 55.00; Sequence understanding: 45.35; Action prediction: 44.37; Interest perception: 69.89; df: 46.65; mf: 29.33; ip: 51.14; ap: 38.92; li: 73.47; si: 68.80; it: 68.03
% All tasks: 56.31; Sequence understanding: 47.49; Action prediction: 46.34; Interest perception: 70.14; df: 49.27; mf: 26.56; ip: 53.73; ap: 40.44; li: 73.49; si: 67.57; it: 71.28
% All tasks: 57.37; Sequence understanding: 46.94; Action prediction: 46.76; Interest perception: 72.67; df: 49.14; mf: 21.09; ip: 51.50; ap: 42.97; li: 76.01; si: 69.75; it: 74.41


% 53.75; 43.70; 38.83; 72.70; 44.49; 34.38; 44.29; 34.48; 77.51; 67.64; 76.76

% To verify the contribution of each component of the proposed \name, we conduct an ablation study with two variants of the LinRec+SASRec (i.e., Eq.~(\ref{eq:LinRec})) over the Gowalla dataset, including (1) \textit{w/o L2 norm}: without the L2 normalization layer, and (2) \textit{w/o ELU}: without the ELU activation layer. Figure~\ref{fig:ablation} shows the performances of different variants in terms of Recall@10, MRR, and NDCG@10. It can be observed that each component contributes to performance. The L2 normalization is particularly crucial for \name, justifying that satisfying the normalized property (i.e., attention score summations less than or equal to 1) is the most critical part of preserving the learning capabilities of attention mechanisms. Besides, the ELU activation layer is indispensable for achieving encouraging results. Furthermore, LinRec+SASRec consistently improves performance in all cases, confirming the correctness of our design choice.  


\begin{table}[t]
    % \fontsize{8}{11}\selectfont
    \caption{The scalability in terms of model size.}
        \vspace{-2mm} 
    \begin{threeparttable}
    \begin{tabular}{cccccc}
        \toprule
        \toprule
        \multirow{2}{*}{Model size}&\multirow{1}{*}{Architecture}&
        \multicolumn{4}{c}{Evaluation tasks} \cr
        \cmidrule(lr){3-6}
        & $(l, d, h)$ & SU & AP & IP & Avg. \cr
        \cmidrule(lr){1-6}
            % 1.6M   & (2, 256, 4)     & 32.74 & 32.93 & 46.69 & 38.50\cr
            3.2M   & (4, 256, 4)     & 39.00 & 34.74 & 58.16 & 45.26\cr
            13M    & (4, 512, 8)     & 38.62 & 37.08 & 67.55 & 49.86\cr
            25M    & (8, 512, 8)     & 45.72 & 42.38 & 68.71 & 53.88\cr
            57M    & (8, 768, 12)    & 45.29 & 42.15 & 74.74 & 56.16\cr
            85M    & (12, 768, 12)   & 47.12 & 44.32 & 76.48 & 58.08\cr
            303M   & (24, 1024, 8)   & 50.43 & 46.65 & 77.30 & 60.01\cr
            680M   & (24, 1536, 12)  & 52.20 & 48.67 & 83.74 & 63.78\cr
            1.2B   & (24, 2048, 16)  & 53.98 & 49.16 & 84.97 & 64.86\cr
     
        \bottomrule
        \bottomrule
    \end{tabular}
    % \vspace{0cm}
    \end{threeparttable}
        \label{tab:scale_size}
        \vspace{-2mm}
\end{table}

            % 1.6M   & (2, 256, 4) & 27.94 & 27.45 & 35.39 & 30.03 \cr
            % 3.2M   & (4, 256, 4) & 26.13 & 26.65 & 26.38 & 26.37 \cr
            % 13M    & (4, 512, 8) & 26.39 & 27.93 & 30.55 & 28.12 \cr
            % 25M    & (8, 512, 8) & 30.40 & 30.02 & 39.40 & 32.98 \cr
            % 57M    & (8, 768, 12) & 40.23 & 40.45 & 65.78 & 47.97 \cr
            % 85M    & (12, 768, 12) & 41.14 & 36.44 & 64.44 & 46.67 \cr
            % 303M   & (24, 1024, 8) & 42.04 & 43.82 & 67.28 & 50.18 \cr
            % 680M   & (24, 1536, 12) & 39.20 & 40.61 & 67.78 & 48.22 \cr
            % 1.2B   & (24, 2048, 16) & 42.30 & 40.93 & 74.29 & 51.48 \cr

\begin{figure}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.75\linewidth]{pics/uqa_scaling_law.pdf}
    \vspace{-3mm}
    \caption{
    Scaling law in terms of the model size.
    }
    \label{fig:scaling_law}
    \vspace{-3mm}
\end{figure}

% l2_d256_h4 1,606,656
% l4_d256_h4 3,213,312
% l4_d512_h8 12,652,544
% l8_d512_h8 25,305,088
% l8_d768_h12 56,635,392
% l12_d768_h12 84,953,088
% l24_d1024_h8 303,611,904
% l24_d1536_h12 679,550,976
% l24_d2048_h16 1,214,349,312



% \begin{table}[t]
%     % \fontsize{8}{11}\selectfont
%     \caption{Ablation in fine-tuning process.}
%         \vspace{-2mm} 
%     \begin{threeparttable}
%     \begin{tabular}{cccccc}
%         \toprule
%         \toprule
%         \multirow{2}{*}{Encoder size}&\multirow{2}{*}{LLM size}&
%         \multicolumn{4}{c}{Sequence length} \cr
%         \cmidrule(lr){3-6}
%         & & 32 & 64 & 128 & 256 \cr
%         \cmidrule(lr){1-6}
%         \multirow{2}{*}{10M}
%         & 1.5B      & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
%         & 3.0B        & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
%         \cmidrule(lr){1-6}
%         \multirow{2}{*}{300M}
%         & 1.5B      & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
%         & 3.0B        & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
%         \cmidrule(lr){1-6}
%         \multirow{2}{*}{2000M}
%         & 1.5B      & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
%         & 3.0B        & $xx.xx$ & $xx.xx$ & $xx.xx$ & $xx.xx$ \cr
%         \bottomrule
%         \bottomrule
%     \end{tabular}
%     % \vspace{0cm}
%     \end{threeparttable}
%         \label{tab:ablation_1}
%         \vspace{-2mm}
% \end{table}

% \subsection{Online Experiments (RQ4)}
% We validate the effectiveness of \name in guiding the online deployment of encoder models. Specifically, we design an LLM-based sequential user encoder to produce semantic-related user embeddings, which aim to complement semantic features to the deployed model (i.e., DIN). However, the previous intermediate metrics, such as Recall, cannot reflect the semantic quality of user embeddings and sometimes are inconsistent with downstream metrics, such as AUC and GAUC. Therefore, we apply \name to assess the semantic quality of user embeddings. On \name, we observe a substantial advancement of semantic-related user embeddings compared to the original ones (i.e., deployed version). Then, we integrate semantic-related user embeddings into the main flow of CTR prediction and jointly train the whole system in the offline environment, gaining 0.12\% in AUC. Subsequently, we conduct an online A/B test with 1\% traffic for the experimental group, observing the growth of 0.xx\% CTR and 0.xx\% GMV.

\subsection{Scalability Study (RQ3)}
We explore the scalability of user embeddings in prompting LLMs for personalization. We employ Trm++ as the backbone encoder and follow the proposed evaluation process. Our scalability study considers two critical factors of encoders: model size and sequence length. The experimental details and results are as follows.

\subsubsection{\textbf{Model Size}} 
We scale the model size of the encoders, ranging from 3M to 1.2B. The results are reported in Table~\ref{tab:scale_size}, where we specify the architectural details, i.e., the number of layers $l$, dimension size $d$, and the number of heads $h$, of the encoder. The experimental results show a clear scaling law between model performance and model size, illustrated in Figure~\ref{fig:scaling_law}. Such a result is significant to the application scenario: We can strengthen encoder models offline (i.e., scaling up model size) to consistently enhance the personalization performance of LLMs in the online environment without impairing the inference efficiency.

\subsubsection{\textbf{Sequence Length}} 
We also investigate the impact of sequence length on personalized performance by adjusting this parameter during pre-training. Specifically, we experiment with various maximum sequence lengths ranging from $32$ to $512$ and present the corresponding outcomes in Table~\ref{tab:scale_length}. We find that while embedding-based GRs enhance performance as the sequence length increases, this improvement rate diminishes beyond a certain length (e.g., $256$). This trend is reasonable, given that the actual length of user interactions is generally limited.

% We also investigate the impact of sequence length on personalized performance by adjusting it during pre-training. Specifically, we experiment with sequence lengths ranging from \{32, 64, 128, 256, 512\}, presenting the corresponding outcomes in Table~\ref{tab:scale_length}. The result indicates that embedding-based LLMs enhance performance as the sequence length increases. Initially, it shows substantial performance gains with each increment in sequence length. However, the improvement rate diminishes when the sequence length becomes sufficiently large (e.g., 256). This trend is reasonable, given that the actual length of user interactions is generally limited.

\begin{table}[t]
    % \fontsize{8}{11}\selectfont
    \caption{The scalability in terms of sequence length.}
        \vspace{-2mm} 
    \begin{threeparttable}
    \begin{tabular}{ccccc}
        \toprule
        \toprule
        \multirow{2}{*}{Sequence length}&
        \multicolumn{4}{c}{Evaluation tasks} \cr
        \cmidrule(lr){2-5}
        & SU & AP & IP & Avg. \cr
        \cmidrule(lr){1-5}
            32    & 27.78 & 30.83 & 38.79 & 33.38 \cr
            64    & 37.21 & 35.67 & 53.75 & 43.40 \cr
            128   & 40.88 & 38.64 & 63.97 & 49.49 \cr
            256   & 42.41 & 41.23 & 69.33 & 52.96 \cr
            512   & 45.72 & 42.38 & 68.71 & 53.88 \cr
        \bottomrule
        \bottomrule
    \end{tabular}
    % \vspace{0cm}
    \end{threeparttable}
        \label{tab:scale_length}
        \vspace{-2mm}
\end{table}

\subsection{Efficiency Study (RQ4)}

We compare the efficiency of text-based and embedding-based GRs to provide insights into deployment scenarios. When deploying LLMs for online serving, the number of tokens is critical to inference efficiency. Therefore, we report the average number of tokens for the methods tested, as shown in Table~\ref{tab:efficiency}, where \textbf{Trm-M} and \textbf{Trm-Q} correspond to Trm++ leveraging two compression methods (\textbf{Mean Pooling} and \textbf{Q-former}). The results indicate that embedding-based GRs significantly reduce the number of tokens compared to their text-based counterparts, achieving increased efficiency by a factor of \(8\times\) to \(19\times\). This efficiency gain is achieved by compressing the user context, i.e., interactions, into fewer tokens (1 for Trm-M and 16 for Trm-Q). In the future, refining question prompts may substantially increase efficiency as they occupy relatively more tokens than soft prompts (i.e., user embeddings).

% We compare the efficiency of text- and embedding-based GRs in deployments. When deploying LLMs for online serving, the number of tokens is critical to inference efficiency. Therefore, we report the average token number of tested methods in Table~\ref{tab:efficiency}, where Trm-M and Trm-Q are respective to Trm++ leveraging two compression methods (Mean Pooling and Q-former). The results show that embedding-based GRs highly reduce the tokens compared to text-based counterparts, achieving superior efficiency (\(10\times\) to \(20\times\)). The efficiency is obtained by compressing the interaction sequence into much fewer tokens (1 for Trm-M and 16 for Trm-Q).

\begin{table}[t]
    % \fontsize{8}{11}\selectfont
    \caption{Efficiency comparison on UQABench.}
        \vspace{-2mm} 
    \begin{threeparttable}
    \begin{tabular}{cccc}
        \toprule
        \toprule
        \multirow{1}{*}{Methods}&\multirow{1}{*}{Models}&
        Performance & Avg. Tokens \cr
        \cmidrule(lr){1-4}
        \multirow{2}{*}{Text-based}
        & Text20               & 53.02 & 1171.23  \cr
        & Text50               & 59.32 & 2498.19  \cr
        \cmidrule(lr){1-4}
        \multirow{2}{*}{Emb-based}
        & Trm-M                & 53.88 & 133.28  \cr
        & Trm-Q                & 55.00 & 148.28  \cr
        \bottomrule
        \bottomrule
    \end{tabular}
    % \vspace{0cm}
    \end{threeparttable}
        \label{tab:efficiency}
        \vspace{-2mm}
\end{table}

% \subsection{Case Study (RQ5)}


% \begin{table}[t]
%     \centering
%     % \vspace{-2mm}
%     \begin{tabular}{ccc}
%         \toprule
%         Variants & GPT-4 eval & Human eval \\
%         \cmidrule(lr){1-3}
%         original & \textbf{0.6104} & \textbf{0.92} \\
%         w/o prompt designing & 0.2074 & 0.34 \\
%         w/o relation templating & 0.3590 & 0.51 \\
%         w/o negative sampling & 0.5410 & 0.85 \\
%         \bottomrule
%     \end{tabular}
%     % \vspace{-2mm} % 在表格和标题之间减少间距
%     \caption{Ablation of question generation.}
%     \vspace{-4mm} % 在标题后减少间距
%     \label{tab:ablation}
% \end{table}

% Moreover, we define \textit{Inconsistency} to evaluate the reliability: measure whether the model's answers to two questions are inconsistent, where the questions are generated by fixing KG triples and conducting negative sampling twice (i.e., two questions have different false choices).

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{pics/Ablation.pdf}
%     \caption{
%     Results of inconsistency rates. The lower, the better. The green and blue represent the random sampling and our sampling methods, respectively.  
%     }
%     \label{fig:ablation}
% \end{figure}


% \begin{table*}
% \centering
%     \begin{tabular}{cccccccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Temperatures}&
%         \multicolumn{3}{|c}{{Precision@5}} & \multicolumn{3}{|c}{Recall@5} & \multicolumn{3}{|c}{SC@5}\cr
%         & Common & Abstract & Avg. & Common & Abstract & Avg. & Common & Abstract & Avg. \cr
%         \cmidrule(lr){1-10}
% $T=0.01$   & 50.22 & 43.58 & 47.17 & 50.65 & 44.25 & 47.71 & 49.78 & 43.22 & 46.77 \cr
% $T=0.1$ & 49.61 & 43.63 & 46.86 & 55.65 & 49.10 & 52.64 & 43.26 & 36.06 & 39.95 \cr
% $T=0.2$ & 50.00 & 42.97 & 46.77 & 61.96 & 53.96 & 58.28 & 38.91 & 30.43 & 35.02 \cr
% $T=0.3$ & 50.26 & 42.25 & 46.58 & 66.30 & 60.36 & 63.57 & 32.39 & 28.64 & 30.67 \cr
% $T=0.5$ & 46.57 & 41.84 & 44.39 & 70.65 & 65.47 & 68.27 & 23.26 & 17.90 & 20.80 \cr
%         \bottomrule
%     \end{tabular}
%   \caption{\label{tab:rq3}
%      Results of exploring knowledge boundary of Qwen-7B. We report Precision, Recall, and SC in different temperatures. 
%   }
% \end{table*}

% \subsection{Exploring Knowledge Boundary (RQ3)}
% Then, we explore the knowledge boundary of Qwen-7B. Notice that we use the base model here. In Table~\ref{tab:rq3}, we report the previously defined metrics SC@5, Precision@5, and Recall@5, tuning the model temperature in search range $\{0.01, 0.1, 0.2, 0.3, 0.5\}$.
% We observe that due to model generation being encouraged to be diversified by increasing temperatures,  the precision gets slightly reduced, and the Recall and SC (strict correct) increase and reduce significantly. 
% Determining reference temperature $T_0$ is empirical, with often a small value such as $0.2$ or $0.3$. 
% Here, we set $T_0=0.2$, as in which the Recall is $58.28$, close to the Accuracy $58.52$ of its fine-tuned version (Qwen-7B-Instruct), conforming to our analysis that the Recall reflects the potential knowledge boundary, which post-training methods can reach. We can also know the essential boundary of the model according to SC value $35.02$, inside which the knowledge is sufficiently trained.


% \begin{table*}[t]
% \centering
% % \resizebox{\linewidth}{!}
% {
%     \begin{tabular}{ccccccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Model}&
%         \multicolumn{4}{|c}{{Zero-shot}} & \multicolumn{4}{|c}{Few-shot} \cr
%         & Input & Output & Avg. RT & Max. RT & Input & Output & Avg. RT & Max. RT \cr
%         \cmidrule(lr){1-9}
% Gemini    & 91.21  & 87.63 & 2.80  & 5.76 & 225.21 & 45.56 & 2.36  & 6.22\cr
% Claude3   & 142.77 & 30.69 & 18.06 & 29.97& 319.26 & 95.40 & 17.83 & 29.51\cr
% Llama3-70B& 120.38 & 26.42 & 2.09  & 8.63 & 268.02 & 4.84  & 0.88  & 6.92\cr
% Qwen2-72B & 93.55  & 12.24 & 1.05  & 9.72 & 236.55 & 11.00 & 1.76  & 41.77\cr
% Qwen2-max & 108.07 & 2.68  & 0.71  & 4.51 & 233.55 & 2.48  & 0.72  & 3.73\cr
% GPT-4     & 147.38 & 6.53  & 1.74  & 5.56 & 332.38 & 4.95  & 1.61  & 7.42 \cr
%         \cmidrule(lr){1-9}
% Avg.      & 117.23 & 27.70 & 4.41 & 10.69 & 269.16 & 27.37 & 4.19 & 16.00 \cr
%         \bottomrule
%     \end{tabular}}
    
%   \caption{\label{tab:rq4}
%      Results of efficiency test. We report the average token number of inputs and outputs and the average response time (Avg. RT). In addition, we record the maximum response time (Max. RT), which is decisive for the time cost of batch API calls. 
%   }
% \end{table*}

% \subsection{Efficiency Study (RQ4)}
% Efficiency is critical in evaluating LLMs due to the high costs associated with their inference. In this context, we examine the efficiency of the proposed \name. The results are presented in Table~\ref{tab:rq4}, illustrating the effectiveness of our evaluation framework in reducing cost.
% Notably, in most cases, the average response times under zero-shot and few-shot settings are below 3 seconds, with maximum response times capped at under 10 seconds. This rapid assessment significantly enhances the usability of \name without sacrificing reliability. Moreover, our question refinement effectively controls the input and output token numbers, contributing to overall efficiency. 
