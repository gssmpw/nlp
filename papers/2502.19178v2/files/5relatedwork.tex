\section{Related Work}
\subsection{Sequential Recommendations}
Sequential recommendations (SRs) learn the user representation from the historical interaction sequence, then calculate the recommendation scores of candidate items and choose top-$k$ as the recommendation result~\cite{fang2020deep,wang2019sequential}. 
Previous works try various deep learning modules to enhance user modeling performance. For example, GRU4Rec~\cite{hidasi2015session} leverages the GRU layers, Caser~\cite{tang2018personalized} uses the CNN layers, and HGN~\cite{ma2019hierarchical} adopts GLU layers. The recent research direction of SRs gradually converges to prevalent Transformer~\cite{vaswani2017attention}. The Transformer-based methods, such as SASRec~\cite{kang2018self}, BERT4Rec~\cite{sun2019bert4rec}, FDSA~\cite{zhang2019feature}, BST~\cite{chen2019behavior}, and CORE~\cite{hou2022core}, leverage attention layers to capture the node correlation within the interaction sequence, which remarkably elevates the representation learning.
Despite the performance effectiveness of Transformer-based methods, the main challenges are efficiency~\cite{tay2022efficient}. The attention module of Transformers brings quadratic computational complexity with sequence length, which is unrealistic for deployment in industrial recommender systems that need low inference time. The most recent works, such as LinRec~\cite{liu2023linrec}, LRURec~\cite{yue2024linear}, Mamba4Rec~\cite{liu2024mamba4rec}, provide efficient solutions that can par with or even outperform Transformer-based methods. However, one essential issue is that the CF framework of SRs may lead to information cocoons and jeopardize user experience.

% Sequential recommendations (SRs) learn the user representation from the historical interaction sequence, then calculate the recommendation scores of candidate items and choose top-$k$ as the recommendation result~\cite{fang2020deep,wang2019sequential}. 
% Previous works try various deep learning modules to enhance user modeling performance. For example, GRU4Rec~\cite{hidasi2015session} leverages the GRU layers, Caser~\cite{tang2018personalized} uses the CNN layers, and HGN~\cite{ma2019hierarchical} adopts GLU layers. The recent research direction of SRs gradually converges to prevalent Transformer~\cite{vaswani2017attention}. The Transformer-based methods, such as SASRec~\cite{kang2018self}, BERT4Rec~\cite{sun2019bert4rec}, FDSA~\cite{zhang2019feature}, BST~\cite{chen2019behavior}, and CORE~\cite{hou2022core}, leverage attention layers to capture the node correlation and dependency within the interaction sequence, which remarkably elevates the representation learning.
% Despite the performance effectiveness of Transformer-based methods, the main challenges are efficiency and scalability~\cite{tay2022efficient}. The attention module of Transformers brings quadratic computational complexity with sequence length, which is unrealistic for deployment in industrial recommender systems that need low inference time. The most recent works, such as LinRec~\cite{liu2023linrec}, LRURec~\cite{yue2024linear}, Mamba4Rec~\cite{liu2024mamba4rec}, provide some efficient solutions that can par with or even outperform transformer-based methods. However, one essential issue is that the CF framework of SRs may lead to information cocoons and jeopardize user experience.

\subsection{Generative Recommendations}
Generative recommendations (GRs) leverage the generative capabilities of LLMs to generate more personalized and diverse results besides candidate items~\cite{xu2024prompting,zhao2023recommender,zhang2023recommendation,wu2024survey}. The mainstream of GRs, text-based GRs, leverage the text information in user interaction sequences as the user context to prompt LLMs to generate personalized results ~\cite{liu2023chatgpt,petrov2023generative,kang2023llms,geng2022recommendation,lyu2023llm}. 
Some frequently used prompting methods in the LLM area can be adapted to text-based GRs, such as in-context learning (ICL)~\cite{gao2020making}, and chain-of-thought (CoT)~\cite{wei2022chain}. The ICL-based methods~\cite{liu2023chatgpt,li2023bookgpt} incorporate the task description and in-context demonstrations (i.e., few-shot). While the CoT-based approaches~\cite{huang2023recommender,wang2023recmind} design task-specific reasoning steps to assist LLMs in generating personalized responses step-by-step. 
Nevertheless, employing the whole user interaction sequence as the text context of LLMs is impractical and unscalable. A regular user interaction sequence with hundreds of items can produce 10k to 100k tokens, which may cause an unacceptably high inference time and cost and even exceed the LLMs' context window limit. Therefore, a line of GRs tries to transform the user interaction sequence into a compact form, user embedding, to softly prompt LLMs for personalization~\cite{li2023prompt,ning2024user,li2023personalized,hebert2024persoma}. The embedding-based GRs highly reduce the token number compared to directly using text information, improving the efficiency and scalability of GRs. 

% With rich worldwide knowledge and diversity, LLMs provide a chance to break the information cocoons of users and bring sufficient personalization~\cite{xu2024prompting,zhao2023recommender,zhang2023recommendation,wu2024survey}. One promising direction LLMs can take to provide personalization is incorporating user history interactions as contextual information~\cite{liu2023chatgpt,petrov2023generative,kang2023llms,geng2022recommendation,lyu2023llm}.

% GRs aim to employ LLMs to generate more diverse and personalized recommendation results than SRs.
% One branch of GRs, text-based GRs, directly leverages the text information of user interactions. The interacted items' text information, such as ID, title, and category, will be filled in the reserved positions chronologically in the pre-designed prompt, serving as the user's context. Then, given specific questions like "What item will the user click next," the LLMs will generate a response such as "item $j$."
% As the user interactions are usually lengthy and noisy, embedding-based GRs transform the interaction sequence into an information-dense and refined form, i.e., user embeddings. The user embedding will be aligned to semantic space by the adapter and then act as the soft prompt for LLMs, prompting them to be more personalized. Compared to text-based GRs, this approach needs fewer tokens for the user context, meaning improved efficiency and scalability. The remaining concern is, "Can user embedding-based GRs provide comparable performance to text-based GRs?" Which is also the core viewpoint of this paper. 


