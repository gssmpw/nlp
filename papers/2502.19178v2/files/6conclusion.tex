\section{Conclusion}
In the LLM era, the user interaction data provide a rich source to contextualize LLM to be more personalized, which implies a promising recommendation paradigm, GRs. By compressing the interaction sequence into a more compact form, user embeddings, the embedding-based GRs outperform text-based GRs with less inference pressure and are friendly to real-world deployments.  
To address the concerns about the effectiveness of embedding-based GRs, we propose a novel benchmark, \name, evaluating user embeddings from the personalization perspective. We propose a standardized three-stage evaluation process, including pre-training, fine-tuning, and evaluating. Furthermore, to comprehensively assess the quality of user embedding, we design three evaluation tasks and corresponding sub-tasks. We extensively experiment on state-of-the-art encoder models to explore the embedding-based GRs' effectiveness and provide some insights. 


% In the LLM era, the user interaction data provide a rich source to contextualize LLM to be more personalized, which implies a promising recommendation paradigm, generative recommendations (GRs). By compressing the interaction sequence into a more compact form, user embeddings, the embedding-based GRs outperform text-based GRs with less inference pressure (fewer tokens) and are friendly to real-world deployments.  
% To address the concerns about the effectiveness of embedding-based GRs, we propose a novel benchmark, \name, for evaluating user embeddings from the personalization perspective. We propose a standardized three-stage evaluation process, including pre-training, fine-tuning, and evaluating. Furthermore, to comprehensively assess the quality of user embedding, we design three evaluation tasks and corresponding sub-tasks. We extensively experiment on state-of-the-art encoder models to explore the embedding-based GRs' effectiveness and provide some insights. 