\begin{figure*}[t]
    \centering
    % \vspace{-4mm}
    \includegraphics[width=0.9\linewidth]{pics/UQABench_overview.pdf}
    % \vspace{-4mm}
    \caption{
    An overview of the standardized evaluation process of user embeddings on \name. The left-bottom part demonstrates the encoder model encodes the historical interactions into user embeddings, which requires pre-training. The left-top part shows that user embeddings act as the soft prompt for LLMs to generate personalized responses, which needs fine-tuning. The right side demonstrates the proposed tasks in the evaluating stage and the corresponding Q\&A demo.  
    }
    \label{fig:overview}
    % \vspace{-4mm}
    % \captionsetup{belowskip=-20pt}
\end{figure*}

\section{\name}
The overview of \name is shown in Figure~\ref{fig:overview}. Based on the industrial user click log and item catalog, we construct the user interaction data as the evaluation source. The evaluation process comprises three critical steps: \textbf{pre-training}, \textbf{fine-tuning}, and \textbf{evaluating}. 
First, we pre-train the encoder models on user interactions to learn initial user embeddings.
Next, we align these user embeddings to the semantic space through fine-tuning, enabling them to act as soft prompts for LLMs. Finally, we assess the quality of the embeddings by evaluating the performance of LLMs prompted by these embeddings in personalized question answering tasks.

\subsection{Source Data}
To build a benchmark specialized in user personalization, we require an open source dataset that includes sufficient user interactions and item information. Unfortunately, two crucial reasons hinder us from directly leveraging existing datasets, such as MovieLens, Yelp, and Amazon Review Data.
First, such datasets mainly focus on specific fields (e.g., movies, merchants, books, and sports) and specific tasks (e.g., next-item and rating predictions), jeopardizing evaluation comprehensiveness.
Second, these datasets suffer from issues related to information richness, such as limited users and items, insufficient side information, and short interaction sequences, all of which reduce the reliability of the evaluation.
To address the issues above, we construct and release a brand-new user interaction dataset as the source data for \name. This dataset is collected and processed from \textbf{Taobao's User Click Log}. Statistically, it includes $184,520$ users, $994,447$ items, and $31,317,087$ interactions, along with extensive item-side information (e.g., item title, category name, brand name, shop name) sourced from \textbf{Taobao's Item Catalog}. 
To ensure user privacy and comply with regulations, we perform necessary operations, detailed in Section~\ref{sec:data}, on data without destroying the sequence patterns. Then, we launch the evaluation flow leveraging the well-prepared source data. 

% To build a benchmark specialized in user personalization, we require an open-source dataset that includes user interactions and item information. 
% Unfortunately, two crucial reasons hinder us from directly leveraging existing datasets, such as MovieLens, Yelp, and Amazon Review Data. 
% First, such datasets mainly focus on specific fields (e.g., movies, merchants, books, and sports) and specific tasks (e.g., next-item prediction and rating prediction), jeopardizing evaluation comprehensiveness. 
% Second, these datasets have issues with information richness, such as limited users and items, insufficient side information, and short-length interaction sequences, which are inconducive to the evaluation reliability. 
% To address the issues above, we construct and release a brand-new user interaction dataset as the source data of \name. The dataset is collected and processed from \textbf{Taobao's User Click Log}. Statistically, the dataset includes $184,520$ users, $994,447$ items, and $31,317,087$ interactions, accompanied by sufficient item-side information (e.g., item title, category name, brand name, shop name) joined from \textbf{Taobao's Item Catalog}. To protect user privacy and comply with regulations, we conduct the necessary operations on data without destroying the sequence pattern. Then, we launch the evaluation flow leveraging the prepared source data.

\subsection{Stage 1: Pre-training}
The first step of evaluation on \name is the pre-training stage, which is essential for encoder models to learn to encode noisy and lengthy user interaction sequences into denser representations (i.e., user embeddings). In this stage, we focus on three critical factors that greatly affect the upper bound of performance: model inputs, training tasks, and training methods. To determine the optimal settings of these three critical parts, we conduct a corresponding ablation study in Section~\ref{sec:ablation_1}.

\subsubsection{\textbf{Model Inputs.}}
Model inputs are fundamental in pre-training encoder models. The item ID, serving as the unique identifier for each item (e.g., book, movie, music), remains the most critical feature in traditional recommendations. Additionally, side information such as title, category, brand, and shop provides crucial contextual information that enhances recommendation performance. The item side information includes ID-based information (e.g., category ID) and text-based information (e.g., category name). 
We utilize item ID as the necessary input and consider the ID-based and text-based side information as the optional inputs. 

\subsubsection{\textbf{Training Tasks.}}
Determining the appropriate training task is vital for the pre-training stage. Although different encoder models may use various tasks (e.g., sequential or bidirectional order, next prediction or cloze tasks, item or attribute prediction) in the original works, we standardize the training task to ensure fairness and improve efficiency. We adopt the next-item prediction (NIP)~\cite{hidasi2015session,kang2018self} as the uniform pre-training task for all encoder models. NIP is a widely used paradigm in sequential recommendations and has proven its effectiveness in learning user representations~\cite{fang2020deep}.

\subsubsection{\textbf{Training Methods.}}
Regarding encoder training methods, two common approaches in recommender systems are employed: \textbf{supervised learning} and \textbf{contrastive learning}. 
Supervised learning (SL), typically optimized via classification losses such as cross-entropy (CE), achieves strong performance when labeled data is abundant and reliable.
However, in real-world scenarios where explicit labels are scarce, and the item pool is vast (leading to data sparsity), SL struggles to generalize. In contrast, contrastive learning (CL) methods such as InfoNCE are preferred in industrial settings.
Contrastive learning operates under a self-supervised paradigm: it learns user and item representations by contrasting positive pairs (e.g., user-item interactions) against negative pairs (e.g., randomly sampled non-interactions).
This method achieves efficiency via in-batch negative sampling and robustness in cold-start/long-tail scenarios by leveraging intrinsic data structures.


% Regarding the training methods of encoders, we utilize two prevalent approaches in recommender systems: \textbf{supervised learning} and \textbf{contrastive learning}. Supervised learning (SL), particularly supervised classification loss like cross-entropy (CE) loss, is highly effective when labels are abundant and precise. However, due to practical challenges such as a lack of explicit labels and the vast item pool, industrial applications prefer contrastive learning (CL) methods like InfoNCE. Contrastive learning self-supervises user and item representation learning by maximizing similarity with positive samples and minimizing similarity with negative samples. This method is advantageous for its efficiency in in-batch learning and transferability for cold start and long-tail recommendation.

% \subsection{Stage 1: Pre-training}
% The first step of evaluation on \name is the pre-training stage, an essential process for encoder models to learn to encode the noisy and lengthy user interaction sequence into a denser representation (i.e., user embedding). In the pre-training stage, we mainly focus on three critical factors closely related to the performance upper bound: model inputs, training tasks, and training methods. 

% \subsubsection{\textbf{Model Inputs.}}
% The item ID is the unique identifier of each item (e.g., book, movie, music) in recommender systems, which is the most critical feature in traditional recommendations. 
% The side information, e.g., title, category, brand, and shop, which provides contextual information about the item, is also vital in enhancing recommendation performance. Conclusively, the item side information includes ID-based information (e.g., category ID) and text-based information (e.g., category name). 
% Consequently, we utilize item ID as the necessary input and consider the ID-based and text-based side information as the options for model inputs. 

% \subsubsection{\textbf{Training Tasks.}}
% We need to determine the training task of the pre-training stage. However, the original training tasks across different encoder models can vary. For example, the order may be sequential or bidirectional, the task paradigm may be the next prediction or cloze tasks, and the prediction target may be the item or its attribute.
% To guarantee the evaluation fairness of the encoder models and improve efficiency, we apply the most prevalent training paradigm in sequential recommendations, next-item prediction (NIP)~\cite{hidasi2015session,kang2018self}, as the uniform training task in our standardized evaluation process. Notably, the NIP task has been frequently used in massive works and has demonstrated its effectiveness in learning user representations~\cite{fang2020deep}. 

% \subsubsection{\textbf{Training Methods.}}
% Regarding the training methods of encoders, we consider two prevalent means in recommender systems: \textbf{supervised learning} and \textbf{contrastive learning}. Supervised learning is the essential deep learning (DL) training method. Supervised classification loss, such as cross-entropy (CE) loss, is frequently used in recommendations when the labels are sufficient and precise. However, due to practical issues, such as a lack of labels and massive items, the industry favors contrastive learning methods, such as InfoNCE. Contrastive learning self-supervised learns user and item representations by maximizing the similarity with positive samples and minimizing the similarity with negative samples. Its advantages include its efficiency in in-batch learning and transferability for cold-start users.

\subsection{Stage 2: Fine-tuning}
The pre-trained encoders are capable of producing user embeddings, capturing user interests and sequential patterns.
Unlike serving collaborative filtering functions (e.g., calculating user-item similarity) in traditional recommendations, user embeddings will act as a soft prompt to activate the personalization of LLMs. However, the discrepancy between the dimensions of the user embeddings and semantic space obstructs direct use. 
To this end, the adapter is needed to transform user embeddings into word embeddings' dimensions. Additionally, compression methods are adopted to reduce the length of user embeddings, where we consider two approaches: \textbf{Mean Pooling} and \textbf{Q-Former}, specified in Section~\ref{sec:ablation_2}. In this way, the user embeddings are converted to several ``soft-prompt tokens.''
A fine-tuning stage of the framework is necessary for (1) transferring the information encoded in user embeddings to LLMs and (2) generating responses consistent with the answer paradigm to improve evaluation efficiency.
We freeze the pre-trained encoders to avoid the knowledge of LLMs overwhelming them and causing catastrophic forgetting of learned user sequential information.
We explore two fine-tuning strategies: full fine-tuning (LLM+adapter) and partial fine-tuning (only adapter) in Section~\ref{sec:ablation_2}.

% The pre-trained encoders are capable of producing user embeddings, capturing user interests and sequential patterns.
% Unlike serving collaborative filtering functions (e.g., calculating user-item similarity) in traditional recommendations, user embeddings will act as a soft prompt to activate the personalization of LLMs. However, the discrepancy between the dimensions of the user embeddings and semantic space obstructs direct use. 
% To this end, the adapter is needed to transform user embeddings into word embeddings' dimensions. Additionally, compression methods are adopted to reduce the length of user embeddings, where we consider two approaches: \textbf{Mean Pooling} and \textbf{Q-Former}, specified in Section~\ref{sec:ablation_2}. In this way, the user embeddings are converted to several ``soft-prompt tokens''.
% A fine-tuning stage of the framework is necessary for (1) transferring the information encoded in user embeddings to LLMs and (2) generating responses consistent with the answer paradigm to improve evaluation efficiency.
% We freeze the pre-trained encoders to avoid the knowledge of LLMs overwhelming them and causing catastrophic forgetting of learned user sequential information.
% We explore two fine-tuning strategies: full fine-tuning (LLM+adapter) and partial fine-tuning (only adapter).

\begin{table*}
% \fontsize{8}{11}\selectfont
\caption{\label{tab:task_intro}
Details and interpretations of evaluation tasks.}
    \centering
    \begin{threeparttable}
    \begin{tabular}{lll}
        \toprule
        \toprule
        \textbf{Task} & \textbf{Sub-task} & \textbf{Interpretation} \\
        \cmidrule(lr){1-3}
        {Sequence understanding} 
        & Direct feature (DF)       & Directly recovering the features in user interaction sequence. \cr
        & Match feature (MF)        & Counting the statistics of features matching the target feature.  \cr
        \cmidrule(lr){1-3}
        {Action prediction} 
        & Item prediction (IP)      & Predicting the next item the user will most likely interact with. \cr
        & Attribute prediction (AP) & Predicting the attribute of the next item the user will most likely interact with. \cr
        \cmidrule(lr){1-3}
        {Interest perception} 
        & Long-term interest (LI)   & Perceiving the user's long-term interest category/brand/store. \cr
        & Short-term interest (SI)  & Perceiving the user's current interest category/brand/store. \cr
        & Interest trajectory (IT)  & Detecting the interest changes of the user. \cr
        \bottomrule
        \bottomrule
    \end{tabular}
    \end{threeparttable}
    \vspace{-2mm}
\end{table*}

\subsection{Stage 3: Evaluating}
Next, we prompt the LLMs with aligned user embeddings to generate personalized responses and evaluate their quality. 
The critical challenge is scaling up the evaluation while guaranteeing objectivity simultaneously. 
To this end, we manually design some question templates as seeds according to the interaction data patterns and expertise. 
This way, massive question prompts (with their ground truths) can be automatically generated based on user historical interactions.
We propose three critical evaluation tasks to assess user embeddings from multi-dimensional perspectives: sequence understanding, action prediction, and interest perception, as illustrated in Figure~\ref{fig:overview}. 
As specified in Table~\ref{tab:task_intro}, we subdivide each task into two or three sub-tasks for a more fine-grained evaluation and design various task-specific templates for each sub-task. Some details of template designing are mentioned in Section~\ref{sec:prompt}.
We then introduce each evaluation task from a conceptual perspective.

% Next, we prompt the LLMs with aligned user embeddings to generate personalized responses and evaluate their quality. 
% The critical challenge is scaling up the evaluation while guaranteeing objectivity simultaneously. 
% To this end, we manually design some question templates as seeds according to the interaction data patterns and expertise. 
% This way, massive question prompts (with their ground truths) can be automatically generated based on user historical interactions.
% We propose three critical evaluation tasks to assess user embeddings from multi-dimensional perspectives: sequence understanding, action prediction, and interest perception, as illustrated in Figure~\ref{fig:overview}. 
% We subdivide each task into two or three sub-tasks, as shown in Table~\ref{tab:task_intro}, for a more fine-grained evaluation and design various task-specific templates for each sub-task. Some details of template designing are mentioned in Section~\ref{sec:prompt}.
% We then introduce each evaluation task from a conceptual perspective.

\subsubsection{\textbf{Sequence Understanding.}}
Sequence understanding involves using LLMs to extract and restore historical user information from user embeddings.
The primary goal of this task is to assess how well user embeddings can serve as a bridge to convey necessary information from user interaction sequences to LLMs.
This task is also of significant interest to the industry since it is related to whether user embeddings in the LLM era can substitute for massive feature engineering on the user side.
Specifically, for a given user interaction sequence, we can query LLMs for  \textbf{direct features} like item IDs, titles, and other side information such as category, shop, and brand.
For example, we can construct the question prompt such that, "\textit{what are the last three items the user has clicked?}" or, "\textit{what is the brand of the last item the user has clicked?}"
In user modeling of industrial recommendation scenarios, the \textbf{match features} play a critical role in feature engineering that captures high-order user sequence patterns.   
To this end, we design questions focusing on match features, defined as the statistical attributes of the user sequence that match the target feature, like item, category, or brand.
For example, "\textit{given a category, tell me the times the user has clicked on this category in history}." Although the questions about match features are more complicated than direct features, it is a practical approach for exploring the upper bound of user embeddings in the sequence understanding task.

% The task of sequence understanding is asking LLMs to recover user historical information from user embeddings. The core target of this task is evaluating how well the user embeddings capture and convey the important information from the user interaction sequence to LLMs.
% This task is also of significant interest to the industry since it is related to whether user embeddings in the LLM era can substitute for massive feature engineering on the user side.
% Specifically, given the user interaction sequence, we may ask LLMs for some \textbf{direct features}, such as the item ID, title, and other side information, including category, shop, and brand.
% For example, we can construct the question prompt such that, \textit{what are the last three items the user has clicked?} or, \textit{what is the brand of the last item the user has clicked?}
% In user modeling of recommendation or searching scenarios, the \textbf{match features} are also important in feature engineering that captures user sequence patterns.   
% Consequently, we design some question prompts related to the match features. The match features are defined as the statistics of the user sequence that match the target feature (e.g., item, category, brand). For example, \textit{Given an item, tell me the times the user has clicked on this item in history}. Although the questions about match features are more complicated than direct features, it is a practical approach for exploring the upper bound of user embeddings in the sequence understanding task.


\subsubsection{\textbf{Action Prediction.}}
We are interested in how user embeddings can assist LLMs in completing traditional industrial tasks like top-$k$ recommendations and click-through rate (CTR) prediction, which are highly related to revenue.
An essential task proposed by us for evaluating these industrial capabilities is action prediction, which includes \textbf{next-item} and \textbf{next-attribute} predictions, the pivotal focus of user sequence modeling.
Traditional next-item prediction involves inputting a user interaction sequence into the model and outputting the item the user will most likely interact with next time from the candidate items.
Two key differences exist between our proposed task and the traditional next-item prediction task.
First, in our approach, we use the compressed user sequence, represented by user embeddings, as a soft prompt for LLMs to generate predictions rather than directly using the original user sequence.
Additionally, we expand the prediction target beyond the item itself to include other attributes (i.e., side information) of the next items, such as title, category, brand, and shop. 
For example, we may ask, "\textit{what item brand is the user most likely to click at next time?}"
Such capability of providing more diverse responses is a significant advantage of LLMs.


% Besides the capabilities of user embeddings in assisting LLMs in understanding the user interaction sequence, we care about their capabilities in prompting LLMs to finish industrial tasks, such as traditional recommendation and click-through-rate (CTR) tasks, which are highly related to revenue.  
% One highly corresponding task to evaluate the above industrial capabilities is action prediction, including \textbf{next-item} and \textbf{next-attribute} predictions, the most critical points in traditional sequential recommendations. The traditional next-item prediction task is defined as inputting a user interaction sequence into the model, then outputting the item the user will most likely interact with next time from the candidate items. Two main differences exist between this proposed task and the traditional next-item prediction task.
% Here, we do not directly use the original user sequence as input; instead, we input the compressed user sequence, user embedding, as the soft prompt for LLMs to generate predictions. In addition, we do not restrict the prediction target to the item itself; in contrast, we may ask LLMs for other attributes (i.e., side information) of the next items, such as title, category, brand, and shop. This is a significant advantage of LLMs. 

\subsubsection{\textbf{Interest Perception.}}
One revolutionary improvement of LLM-based recommendations compared to traditional recommendations should be introducing remarkable diversity. Restricted by the training paradigm and CF framework, traditional recommendations will concentrate on popular items and users with frequent interactions. We hope that user embeddings can assist LLM-based approaches in recalling diverse user interest items to improve personalization and enhance user experience. 
To evaluate the effect of user embeddings in enhancing personalization, we design some unique question prompts to assess how well LLMs prompted by user embeddings know the user's interest. 
As the interaction sequence may be noisy and overlong, the user embedding should highlight the core and indispensable parts (e.g., long- and short-term interests) and reveal the user's interest trajectory. 
Regarding user \textbf{long- and short-term interests}, we build the prompts such as "\textit{the user wants to purchase products from a certain category/brand/store, based on user's history information, recommends three options}," and "\textit{what is the user's recent interest category/brand/store}." The ground truth of the former is related to the whole interaction sequence, while the latter consists of the recently interacted items.  
Considering the user's \textbf{interest trajectory}, we may ask, "\textit{has the user's interest recently changed, and what is the trajectory of interest change}." 

\subsection{Technical Details}
Besides introducing the main evaluation process, several technical details merit attention, which we discuss below. 

\subsubsection{\textbf{Data Preprocessing}} 
\label{sec:data}
First, for privacy protection and compliance regulations, we anonymize the user IDs to prevent locating users and filter out sensitive items. Furthermore, the original user interaction data is lengthy and noisy, and the number of items is massive, which is inconducive to encoder models' training efficiency and stability. To address these issues, we conduct necessary data preprocessing on both the user and item sides. 
On the user side, we filter out users with interaction sequences that are either too long or too short, thereby enhancing the efficiency and stability of mini-batch training.
Regarding items, we remove those with fewer than three user interactions to reduce the size of the item pool, significantly improving the training effectiveness of user encoders.

\subsubsection{\textbf{Prompt Designing}} 
\label{sec:prompt}
We design templates of question prompts to enhance evaluation efficiency and objectivity. Given a task-specific template and user data, we can automatically generate the corresponding question and answer based entirely on user interactions. 
For instance, a template designed for the direct feature task might be "\textit{what are the last $k$ items the user has clicked?}", and inputting the interaction sequence $s_i = [v_1,\cdots,v_{n_i}]$ of user $u_i$ would automatically yield the answer $v_{n_i-k+1},\cdots,v_{n_i}$. 
Since it is unrealistic for LLMs to generate complete answers to highly specialized questions, we provide candidates for such questions. 
Furthermore, we adopt some filter rules to avoid trivial or overly burdensome questions, for example, discarding questions with an answer of $0$ (i.e., no match features) in match feature tasks and ensuring $k$ is less than $4$ in direct feature tasks. 
Some necessary instructions are also provided to guide LLMs' generation, like arranging multiple answers in chronological order for direct feature tasks or sorting the answers by user interest level for interest perception tasks. However, these instructions only play an auxiliary role, as the fine-tuning stage plays the leading role.


% \subsection{Technical Details}
% Besides the main evaluation process, some technical details need attention, and we introduce them as follows. 
% \subsubsection{\textbf{Data Preprocessing}} 
% \label{sec:data}
% First, for privacy protection and compliance regulations, we anonymize the user IDs to prevent locating users and filter out some sensitive items. Furthermore, the original user interaction data is lengthy and noisy, and the number of items is massive, which is inconducive to encoder models' training efficiency and stability.
% To address this issue, we conduct necessary data preprocessing on both the user and item sides. 
% Regarding user-side preprocessing, we filter out users with too long or too short interaction sequences, enhancing mini-batch training efficiency and stability. In terms of items, we filter out items with fewer than $3$ interactions with users. 
% This measure helps reduce the item pool size, significantly improving the training effectiveness of user encoders.

% \subsubsection{\textbf{Prompt Designing}} 
% \label{sec:prompt}
% We design templates of question prompts to enhance evaluation efficiency and objectivity. Given a task-specific template and user, we can automatically generate the corresponding question and answer based entirely on user interactions. For example, one template of a direct feature task is "\textit{what are the last $k$ items the user has clicked?}", for which if we input the interaction sequence $s_i = [v_1,\cdots,v_{n_i}]$ of user $u_i$, it automatically outputs the answer $v_{n_i-k+1},\cdots,v_{n_i}$. Since requiring LLMs to generate complete answers to some specialized questions is unrealistic, we provide candidates for these questions. 
% Furthermore, we adopt some filter rules to avoid meaningless or extremely tough questions, such that in the match feature task, we get rid of the question whose answer is $0$ (i.e., no match features); in the direct feature task, we make sure $k$ is less than $4$. Moreover, some necessary instructions are added to guide LLMs' generation, like arranging the answers in chronological order (direct feature task) or sorting the answers by user interest level (interest perception task). However, the instructions here only play an auxiliary role since we have the fine-tuning stage.

% We design templates of question prompts to enhance evaluation efficiency and objectivity. Given a task-specific template and user, we can automatically generate the corresponding question and answer based entirely on user interactions. For example, one template of direct feature task is "\textit{what are the last $k$ items the user has clicked?}", for which if we input the interaction sequence $s_i = [v_1,\cdots,v_{n_i}]$ of user $u_i$, it automatically outputs the answer $v_{n_i-k+1},\cdots,v_{n_i}$. 
% Furthermore, we adopt some filter rules to avoid meaningless or extremely hard questions, such that in the match feature task, we get rid of the question whose answer is $0$ (i.e., no match features); in the direct feature task, we make sure $k$ is less than $4$. Moreover, some necessary instructions are added to guide LLMs' generation, like arranging the answers in chronological order (direct feature task) or sorting the answers by user interest level (interest perception task). However, the instructions here only play an auxiliary role since we have the fine-tuning stage.

% \subsubsection{Evaluation metrics.} 
% Contrary to applying metrics in recommendations, such as Recall and NDCG, we use two frequently used metrics in NLP, ROUGE, and BLEU, to evaluate the quality of LLMs' responses.
% ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap between the generated text and the reference text by counting matching n-grams, mainly focusing on recall.
% BLEU (Bilingual Evaluation Understudy) evaluates the precision of n-grams in the generated text compared to reference texts. 
% Specifically, we directly calculate the ROUGE and BLEU metrics if the answer is a unique item or feature. If the answer has multiple items or features, we split the answers and calculate the average metrics.  
% One exception is that we utilize the Exact Match metric for the match feature task since the answers of this task are digits. 



\begin{table*}
\caption{\label{tab:overall}
    Results of evaluating LLMs on \name dataset. The overall average (Avg.) score is bold in each row.
}
\begin{threeparttable}
\centering
\begin{tabular}{
  l % Left align for Methods
  l % Left align for Models
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for DF
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for MF
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for Avg.
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for IP
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for AP
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for Avg.
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for LI
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for SI
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for IT
  >{\centering\arraybackslash}p{0.8cm} % Centered and fixed-width for Avg.
  c % Centered for Overall Avg.
}
    \toprule
    \toprule
    \multirow{2}{*}{Methods} & \multirow{2}{*}{Models} &
    \multicolumn{3}{c}{Sequence understanding} & \multicolumn{3}{c}{Action prediction} & \multicolumn{4}{c}{Interest perception} & \multirow{2}{*}{Overall Avg.} \cr
    & & DF & MF & Avg. & IP & AP & Avg. & LI  & SI & IT & Avg. & \cr
    \cmidrule(lr){1-13}
    \multirow{2}{*}{Text-based}
    & Text20 
    & 38.25 & 28.91 & 37.52 & 44.81 & 40.58 & 42.45 & 63.82 & 79.96 & 62.79 & 71.10 & \textbf{53.02} \cr
    \cmidrule(lr){2-13}
    & Text50 
    & 44.22 & 32.03 & 43.27 & 45.58 & 38.04 & 41.39 & 79.27 & 89.80 & 79.63 & 84.29 & \textbf{59.32} \cr
    \cmidrule(lr){1-13}
    \multirow{6}{*}{Emb-based}
    & GRU4Rec 
    & 33.67 & 23.44 & 32.86 & 34.94 & 30.02 & 32.20 & 46.98 & 41.33 & 45.56 & 43.97 & \textbf{37.15} \cr 
    \cmidrule(lr){2-13}
    & SASRec 
    & 40.41 & 24.22 & 39.14 & 42.49 & 35.27 & 38.47 & 59.20 & 53.32 & 56.39 & 55.72 & \textbf{45.66} \cr
    \cmidrule(lr){2-13}
    & Mamba4Rec 
    & 30.54 & 18.75 & 29.62 & 34.25 & 28.10 & 30.83 & 38.82 & 34.69 & 35.64 & 36.06 & \textbf{32.69} \cr
    \cmidrule(lr){2-13}
    & HSTU 
    & 43.29 & 25.00 & 41.86 & 47.30 & 36.53 & 41.31 & 70.85 & 60.06 & 65.67 & 64.45 & \textbf{50.88} \cr
    \cmidrule(lr){2-13}
    & Trm++ 
    & 47.28 & 27.34 & 45.72 & 48.67 & 37.35 & 42.38 & 75.25 & 64.14 & 70.10 & 68.71 & \textbf{53.88} \cr
    \bottomrule
    \bottomrule
\end{tabular}
\end{threeparttable}
\end{table*}


\subsection{In-depth Analysis: Scaling Law}

We aim to reveal the scaling law of using embeddings to prompt LLMs for personalization, which benefits resource distribution in industrial scenarios. Following valuable experiences from LLMs and recommender systems, we focus on two factors: \textbf{model size} (of user encoders) and \textbf{sequence length} (of interaction data in the pre-training stage).
Scaling up the model size is validated to improve model performance in various domains, e.g., NLP~\cite{kaplan2020scaling}. 
In addition, the previous experience in SRs~\cite{fang2020deep} is that scaling up the sequence length can consistently improve the recalling and ranking performance. We will explore whether such conclusions still hold for the effect of user embeddings in prompting LLMs for personalization.  
As the core of LLMs, Transformers are proven to possess excellent scaling capabilities. Therefore, our scaling experiments will apply Transformers as the backbone models.

% We aim to reveal the scaling law of using embeddings to prompt LLMs for personalization, which benefits resource expectation and distribution in industrial scenarios. Following valuable training experiences from LLMs and recommender systems, we focus on two factors: \textbf{model size} (of user encoders) and \textbf{sequence length} (of interaction data in the pre-training stage).
% Scaling up the model size is validated to improve model performance in various domains, e.g., NLP~\cite{kaplan2020scaling}. 
% In addition, the previous experience in SRs~\cite{fang2020deep} is that scaling up the sequence length can consistently improve the recalling and ranking performance. We will explore whether this conclusion still holds for the effect of user embeddings in prompting LLMs for personalization.  
% As the core of the LLMs, Transformers also perform well in sequential recommendations. Moreover, it has been proven that the Transformers framework possesses scaling capabilities in various areas. 
% Therefore, our scaling experiments will apply Transformers as the backbone models.

% \begin{Verbatim}[fontsize=\small]
% \end{Verbatim}

% \begin{lstlisting}
% *Question*: Select the most suitable one from among the *Choices* to fill in the blank space to complete the following *Sentence*, making it comply with the logic and knowledge in e-commerce and daily consumption. Just output the selected choice. The terms in brackets are the descriptions of words.
% *Sentence*: Rapid heating (function) is very important for ___ (category).
% *Choice*: ['microwave oven', 'flashlight', 'electric fan', 'speaker']
% *Answer*: microwave oven
% \end{lstlisting}


% \begin{table*}[t]
% \centering
%     % \vspace{-2mm}
%     \begin{tabular}{cccccccc}
%         \toprule
%         % \toprule
%         \multirow{2}{*}{Models}&
%         \multicolumn{3}{|c}{Zero-shot} & \multicolumn{3}{|c|}{Few-shot} & \multirow{2}{*}{Overall Avg.}\cr
%         & Common & Abstract & Avg. & Common & Abstract  & Avg. \cr
%         \cmidrule(lr){1-8}
        
%         ChatGLM3-6B     & 38.24 & 29.61 & 34.27 & 37.14 & 32.43 & 34.98 & 34.63 \cr
%         Gemini-1.5-pro  & 45.87 & 45.27 & 45.59 & 40.65 & 31.20 & 36.31 & 40.95 \cr
%         Claude3         & 53.83 & 50.00 & 52.09 & 53.04 & 46.80 & 50.18 & 51.14 \cr
%         % Baichuan2-13B   & & & & & & & \cr
%         Yi-1.5-6B       & 42.10 & 48.34 & 45.01 & 40.43 & 42.71 & 41.48 & 43.25\cr
%         Yi-1.5-34B      & 54.13 & 52.94 & 53.58 & 58.48 & 50.64 & 54.88 & 54.23\cr
%         Llama3-8B       & 32.61 & 37.34 & 34.78 & 45.87 & 53.96 & 49.59 & 42.19 \cr
%         Llama3-70B      & 57.98 & 46.94 & 52.02 & 51.36 & 59.71 & 55.25 & 53.64 \cr
%         % Qwen2-7B        & 48.91 & 45.01 & 47.12 & 59.13 & 51.41 & 55.58 & 51.35 \cr
%         Qwen2-7B        & 60.43 & 56.27 & 58.52 & 58.91 & 50.90 & 55.23 & 56.88 \cr
%         Qwen2-72B       & 68.48 & 58.97 & 64.12 & \textbf{68.85} & 62.15 & 65.76 & 64.94 \cr
%         Qwen2-max       & \textbf{70.65} & 62.66 & \textbf{66.98} & 68.04 & 64.96 & 66.63 & \textbf{66.81} \cr
%         GPT-4 turbo     & 62.83 & 60.87 & 61.93 & 66.52 & 65.47 & 66.04 & 63.99 \cr
%         GPT-4           & 63.91 & \textbf{67.77} & 65.69 & 67.61 & \textbf{67.26} & \textbf{67.45} & 66.57 \cr
%         \bottomrule
%     \end{tabular}
%   \caption{\label{tab:overall}
%     Overall comparison results of evaluating LLMs on ECKGBench dataset.  
%     % Results of evaluating LLMs on ECKGBench dataset. Each column represents a single evaluation test that leverages one prompting approach (i.e., zero-shot or few-shot) in one e-commerce dimension (i.e., common or abstract). 
%   }
%   \vspace{-8mm}
% \end{table*}

% \subsection{Prompt designing}
% We are ready to design the prompt and assemble the questions.
% \subsubsection{Paradigm.}
% Letting LLM generate without restriction requires additional assessment from humans or a judge model (e.g., another LLM), leading to excess cost and jeopardizing objectivity.
% To address this issue, we apply multiple-choice form, which provide better differentiation of LLMs' capabilities compared to true or false.

% \subsubsection{Instruction.}
% Instruction is needed to activate the LLMs' knowledge of e-commerce and guide them in answering special questions (e.g., multiple-choice). 
% Therefore, we first draft the question instructions manually. Afterward, we leverage GPT-4 to refine the details to guarantee LLMs' instruction-following. 

% \subsubsection{Assembly.}
% The prompt's main body is an unfinished declarative sentence consisting of source entity $s$, relation template $r^t$, and the uncompleted part.
% The candidate choices consist of the ground truth $t$ and false choices $\{t_1^2,t_2^2,t_3^2\}$. 
% The question has a standard multi-choice paradigm with instruction, formed as: [\textit{instruction, sentence, choices}]. We filter out low-quality questions manually. 

% \subsection{Question assembly}
% In this step, we are ready to assemble the questions in final form. 
% \subsubsection{Paradigm.}
% Letting LLM generate without restriction requires additional assessment from humans or a judge model (e.g., another LLM), leading to excess cost and may bring intense subjectivity.
% To address the issue, we select the question paradigm from two classical types: true or false and multiple-choice. To enhance the discrimination of LLMs' capability, we utilize multiple-choice, which is more confusing and informative than true or false. 

% \subsubsection{Instruction.}
% Instruction is needed to activate the LLMs' memory and knowledge in the e-commerce domain and guide them in answering the multiple-choice question (i.e., selecting only one suitable choice from candidate choices). 
% Therefore, we first draft the instructions of the question manually. Afterward, we leverage GPT-4 to refine the instruction to guarantee the instruction-following of models and reveal their true capabilities. 

% \subsubsection{Assembly.}
% Given a triple in KG, the main body of a question is an unfinished declarative sentence, constructed by \{\textit{source entity (description), relation templates, part to be completed (description)}\}.
% The candidate choices consist of the ground truth $t$ and false choices $t_1^2,t_2^2,t_3^2$. The task for LLMs is completing the sentences by selecting one choice from the candidate choices according to the instructions. Thus, the final prompt is formed as: [\textit{instruction, sentence, choices}]. We demonstrate an example in Table~\ref{tab:prompt}. 

% \begin{table}[ht]
%     \centering
%     \fontsize{9}{11}\selectfont
%     \begin{tabular}{@{}p{8cm}@{}} % Adjust widths as needed
%         \toprule
%         \textbf{*Instruction*:} Select the most suitable one from among the *Choices* to fill in the blank space to complete the following *Sentence*, making it comply with the logic and knowledge in e-commerce and daily consumption. Just output the selected choice. The terms in brackets are the descriptions of words. \cr
%         \textbf{*Sentence*:} Rapid heating (function) is very important for \underline{\hspace{1cm}}  (name of category). \cr
%         %\underline{\hspace{1cm}}(category). \cr
%         \textbf{*Choices*:} Microwave oven, Flashlight, Electric fan, Speaker \cr
%         \textbf{*Answer*:} Microwave oven \cr
%         \bottomrule
%     \end{tabular}
%     \caption{An example of assembled question prompt.}
%     \label{tab:prompt}
% \end{table}

% \subsection{Evaluation Dimensions}
% \label{sec:dimension}
% We assess LLMs' capabilities in e-commerce through two dimensions: common knowledge (dim\_1) and abstract knowledge (dim\_2).
% \textbf{Common knowledge} includes frequently asked user concepts like product functions, target groups, and seasonal suitability, essential for LLMs to shape positive first impressions in e-commerce.
% \textbf{Abstract knowledge} involves complex relationships, such as color associations and style similarities, which are less noticeable. This advanced understanding enhances LLMs' ability to boost implicit revenue by recommending products aligned with user preferences.

