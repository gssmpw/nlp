\section{Related Works}
Researchers have been exploring two primary directions to efficiently improve response quality under a frozen pre-trained LLM. 
One of the most commonly known technique is to increase the number of reasoning steps to encourage the LLMs to think more before the final output is provided \cite{DBLP:conf/nips/Wei0SBIXCLZ22}. Following this direction, people have been using search to improve the performance of the inference of LLMs, and the recently released o1 models are one of the products that lead on this \cite{jaech2024openai}. Following the categorization in \citet{zeng2024scaling}, we define search as the process of LLMs making multiple attempts to solve the problem based on internal or external guidance, and the search strategy can be mainly categorized into tree-search and sequential revisions.

From the perspective of tree search, researchers try to generate a few responses at the same time and use external guidance to choose the one that is the best, i.e., tree search is searching inter-responses. Based on this classification, the simplest method in this direction is the Best-of-N (BoN) sampling \cite{cobbe2021gsm8k, brown2024large}. However, because of its efficiency compared to other actual search algorithms that involve a value estimation, there are many work that focuses on this setting to make it more efficient and effective. For example, \citet{sun2024fast} proposed to use speculative rejection in BoN to reject bad responses through early score. \citet{chen2024flaming} proposed to use extremely high temperature on the first token to greatly improve the Best-of-N performance on math and coding tasks. 
Besides BoN sampling, actual search that happens on the tree is also very popular. 
Inspired by the success of AlphaGo, many researchers believe Monte Carlo Tree Search can be a very efficient search algorithm to be combined with machine learning, and it is a recently hot topic in test-time computation as it has a more accurate value estimation and naturally balances exploration and exploitations \cite{zeng2024scaling}. Research for MCTS can be further classified into three classes based on how a node is defined, namely token-level actions \cite{zhang2023planning, liu2023making}, step-level actions \cite{hao2023reasoning, chen-etal-2024-step, zhou2023language}, and solution-level actions \cite{zhang2024accessing, zhang2024llama}.  
On the other hand, beam search, as a very classic sampling algorithm, has been shown to be also efficient, and even better than MCTS in test-time scaling \cite{chen2024alphamathzeroprocesssupervision}. 
Similar to the different design in MCTS, \citet{xie2024self} proposed to let the LLM themselves be the value function and conduct beam search on a step level. 
TreeBoN\cite{qiu2024treebon} proposed to use a similar step-level beam search data in a DPO training. 
\citet{snell2024scaling} proposed a k-step rollout to estimate the partial sequence in beam-search. 
In this paper, we adopt the simplest tree-search setting, Best-of-N (BoN) sampling, due to the absence of a highly accurate process reward model (PRM).

The sequential revisions in the search strategy are more about self-evaluation and self-correction that happen sequentially in a single response, i.e, searching inner-responses. This direction is previously known as self-correction, and have also been known as iterations based on reflections. Starting from Self-Refine \cite{madaan2024self}, there are many works that studied how to effectively use LLM to give themselves their own evaluation and reflection \cite{yao2022react, shinn2023reflexion}, and study how this reflection-based mechanism can be adapted to different applications \cite{chen2023teaching, gou2023critic, chen2024reprompt}. While there have previously been a lot of discussions on whether LLM can actually self-evaluate and self-correct themselves \cite{huang2023large, chen2024tree, verma2024brittle}, recent studies have shown that by training with high-quality data that involve such process, LLM can actually achieve a useable level of capability and help self-correct in the process \cite{huang2024o1, zeng2024scaling, deepseekai2025deepseekr1incentivizingreasoningcapability}. 

While the research in the two search strategies is relatively separate, their methods are orthogonal and can be used together to make the sampling process more efficient \cite{snell2024scaling}, and there have also already been some trial that define a complete solution as a node in the tree search \cite{zhang2024accessing, zhang2024llama}. This is especially important when generating training data for newer generations of models. 
In this paper, we focus on the intersection of both search strategies and study how to strategically trigger the sequential revision capability of LLMs more efficiently when tree search strategies are also used in the process.