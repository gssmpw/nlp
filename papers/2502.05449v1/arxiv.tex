
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %
\usepackage{multirow}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[preprint]{icml2025}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\definecolor{myred}{RGB}{251, 214, 211}
\definecolor{myblue}{RGB}{47, 110, 186}
\definecolor[named]{ACMPurple}{cmyk}{0.55,1,0,0.15}
\hypersetup{
  colorlinks=true,
  citecolor=ACMPurple
}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{Iterative Deepening Sampling for Large Language Models}

\begin{document}

\twocolumn[
\icmltitle{Iterative Deepening Sampling for Large Language Models}




\begin{icmlauthorlist}
\icmlauthor{Weizhe Chen}{usc}
\icmlauthor{Sven Koenig}{uci}
\icmlauthor{Bistra Dilkina}{usc}
\end{icmlauthorlist}

\icmlaffiliation{usc}{Thomas Lord Department of Computer Science, University of Southern California, United States}
\icmlaffiliation{uci}{Department of Computer Science, University of California, Irvine, United States}

\icmlcorrespondingauthor{Weizhe Chen}{weizhech@usc.edu}
\icmlcorrespondingauthor{Sven Koenig}{sven.koenig@uci.edu}
\icmlcorrespondingauthor{Bistra Dilkina}{dilkina@usc.edu}

\icmlkeywords{Machine Learning, ICML, LLM}

\vskip 0.3in
]



\printAffiliationsAndNotice{} %

\begin{abstract}
The recent release of OpenAI's o1 models and other similar frameworks showcasing test-time scaling laws has demonstrated their exceptional capability to tackle complex reasoning tasks. Inspired by this, subsequent research has revealed that such test-time scaling laws hinge on the model's ability to search both within a single response (intra-response) and across multiple responses (inter-response) during training. Crucially, beyond selecting a single optimal response, the model must also develop robust self-correction capabilities within its own outputs. However, training models to achieve effective self-evaluation and self-correction remains a significant challenge, heavily dependent on the quality of self-reflection data. In this paper, we address this challenge by focusing on enhancing the quality of self-reflection data generation for complex problem-solving, which can subsequently improve the training of next-generation large language models (LLMs). Specifically, we explore how manually triggering a model's self-correction mechanisms can improve performance on challenging reasoning tasks. To this end, we propose a novel iterative deepening sampling algorithm framework designed to enhance self-correction and generate higher-quality samples. Through extensive experiments on Math500 and AIME benchmarks, we demonstrate that our method achieves a higher success rate on difficult tasks and provide detailed ablation studies to analyze its effectiveness across diverse settings.
\end{abstract}

\section{Introduction}

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/ID_sampling_workflow.png}
    \caption{An illustration of our method, Iterative Deepening Sampling (ID-sampling), where $B_0$ is the initial sampling budget.}
    \label{fig:workflow}
\end{figure*}

Since ChatGPT, large language models (LLMs) have been a rapidly evolving domain that tries to solve problems beyond traditional language tasks like summarization or question answering \cite{chen2023teaching, yao2022react, chen2024alphamathzeroprocesssupervision, chen2024solving}. Significantly, the newly released OpenAI O1 has demonstrated its strong capability in complex problem-solving through its detailed reasoning steps before outputting the final answer \cite{jaech2024openai}. Since then, many researchers have studied how to replicate success from an open-source perspective and how to train models that are even better at efficiently solving problems that still remain unsolvable by the current LLMs \cite{huang2024o1, zeng2024scaling, deepseekai2025deepseekr1incentivizingreasoningcapability}. 

The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained—there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of OpenAI's o1 series \cite{jaech2024openai}, researchers have been exploring a new class of scaling laws that govern inference-time performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities. Moreover, this paradigm not only enhances inference-time efficiency but also facilitates the generation of high-quality synthetic data, which can be leveraged to train the next generation of models and evaluate their performance \cite{guan2025rstar}.

A key insight in this line of research is that high-quality data capable of eliciting self-reflection in LLMs is crucial for enhancing their reasoning capabilities. Notably, o1-like models demonstrate improved performance when longer reasoning chains are utilized \cite{huang2024o1, deepseekai2025deepseekr1incentivizingreasoningcapability}. One straightforward yet effective approach to collecting such data involves manually inserting prompts such as "Wait! Maybe I made some mistakes! I need to rethink from scratch." These interventions have been shown to successfully trigger self-reflection, thereby improving the model’s reasoning depth and accuracy. However, as this remains an emerging area, many design choices in implementing self-reflection mechanisms lack systematic analysis. In particular, fundamental questions—such as when and how self-reflection should be introduced—remain largely unexplored.

In this paper, we address the challenge of efficiently generating high-quality reasoning data using a fixed model, without additional training on complex reasoning tasks such as mathematical problem-solving. Specifically, we focus on manually introducing self-reflection triggers into generated data to improve the pass rate of a fixed model.
To achieve this, we propose Iterative Deepening Sampling (ID-Sampling), a novel algorithmic framework that iteratively increases the sampling budget following a geometric progression, while incorporating self-reflection mechanisms at each expansion step. We theoretically demonstrate that ID-Sampling effectively balances computational efficiency and response quality, ensuring that the budget is not excessively wasted while still improving model performance.
We evaluate ID-Sampling on two challenging reasoning benchmarks, MATH-500 and AIME-24, and demonstrate its effectiveness in Best-of-N sampling and majority voting settings. Additionally, we provide an ablation study analyzing how the rate of budget increase per iteration impacts both pass rate and inference time. Our results highlight the potential of ID-Sampling as a scalable approach for improving LLM reasoning performance through adaptive self-reflection mechanisms.


\section{Related Works}

Researchers have been exploring two primary directions to efficiently improve response quality under a frozen pre-trained LLM. 
One of the most commonly known technique is to increase the number of reasoning steps to encourage the LLMs to think more before the final output is provided \cite{DBLP:conf/nips/Wei0SBIXCLZ22}. Following this direction, people have been using search to improve the performance of the inference of LLMs, and the recently released o1 models are one of the products that lead on this \cite{jaech2024openai}. Following the categorization in \citet{zeng2024scaling}, we define search as the process of LLMs making multiple attempts to solve the problem based on internal or external guidance, and the search strategy can be mainly categorized into tree-search and sequential revisions.

From the perspective of tree search, researchers try to generate a few responses at the same time and use external guidance to choose the one that is the best, i.e., tree search is searching inter-responses. Based on this classification, the simplest method in this direction is the Best-of-N (BoN) sampling \cite{cobbe2021gsm8k, brown2024large}. However, because of its efficiency compared to other actual search algorithms that involve a value estimation, there are many work that focuses on this setting to make it more efficient and effective. For example, \citet{sun2024fast} proposed to use speculative rejection in BoN to reject bad responses through early score. \citet{chen2024flaming} proposed to use extremely high temperature on the first token to greatly improve the Best-of-N performance on math and coding tasks. 
Besides BoN sampling, actual search that happens on the tree is also very popular. 
Inspired by the success of AlphaGo, many researchers believe Monte Carlo Tree Search can be a very efficient search algorithm to be combined with machine learning, and it is a recently hot topic in test-time computation as it has a more accurate value estimation and naturally balances exploration and exploitations \cite{zeng2024scaling}. Research for MCTS can be further classified into three classes based on how a node is defined, namely token-level actions \cite{zhang2023planning, liu2023making}, step-level actions \cite{hao2023reasoning, chen-etal-2024-step, zhou2023language}, and solution-level actions \cite{zhang2024accessing, zhang2024llama}.  
On the other hand, beam search, as a very classic sampling algorithm, has been shown to be also efficient, and even better than MCTS in test-time scaling \cite{chen2024alphamathzeroprocesssupervision}. 
Similar to the different design in MCTS, \citet{xie2024self} proposed to let the LLM themselves be the value function and conduct beam search on a step level. 
TreeBoN\cite{qiu2024treebon} proposed to use a similar step-level beam search data in a DPO training. 
\citet{snell2024scaling} proposed a k-step rollout to estimate the partial sequence in beam-search. 
In this paper, we adopt the simplest tree-search setting, Best-of-N (BoN) sampling, due to the absence of a highly accurate process reward model (PRM).

The sequential revisions in the search strategy are more about self-evaluation and self-correction that happen sequentially in a single response, i.e, searching inner-responses. This direction is previously known as self-correction, and have also been known as iterations based on reflections. Starting from Self-Refine \cite{madaan2024self}, there are many works that studied how to effectively use LLM to give themselves their own evaluation and reflection \cite{yao2022react, shinn2023reflexion}, and study how this reflection-based mechanism can be adapted to different applications \cite{chen2023teaching, gou2023critic, chen2024reprompt}. While there have previously been a lot of discussions on whether LLM can actually self-evaluate and self-correct themselves \cite{huang2023large, chen2024tree, verma2024brittle}, recent studies have shown that by training with high-quality data that involve such process, LLM can actually achieve a useable level of capability and help self-correct in the process \cite{huang2024o1, zeng2024scaling, deepseekai2025deepseekr1incentivizingreasoningcapability}. 

While the research in the two search strategies is relatively separate, their methods are orthogonal and can be used together to make the sampling process more efficient \cite{snell2024scaling}, and there have also already been some trial that define a complete solution as a node in the tree search \cite{zhang2024accessing, zhang2024llama}. This is especially important when generating training data for newer generations of models. 
In this paper, we focus on the intersection of both search strategies and study how to strategically trigger the sequential revision capability of LLMs more efficiently when tree search strategies are also used in the process. 


\section{Preliminaries}

\subsection{Best-of-N Sampling}

For reasoning-intensive tasks such as mathematical problem-solving and coding, Best-of-N sampling is one of the most widely used strategies for data generation. This approach involves sampling N outputs from the same model using predefined sampling parameters—typically with a higher temperature than single-sample settings—followed by a selection process to determine the best response. The selection criteria depend on the intended use of the samples. During training, responses are typically evaluated using rule-based checkers for mathematical problems or online judges for coding tasks to identify correct answers within the sampled set. At test time, a reward model is often employed to score the generated responses, with the highest-scoring output selected as the final answer. This methodology effectively balances exploration and optimization, making it a fundamental component in enhancing the performance of LLMs on reasoning tasks.

The BoN sampling is a simple yet effective method that can be fully parallelized to enhance performance. Increasing N guarantees improved results during training when a ground-truth checker is available and generally leads to better performance at inference time, provided that the reward model is sufficiently accurate. However, a key limitation of BoN is that all N samples are generated using the same sampling parameters, which restricts diversity in the output space. This lack of diversity often leads to performance saturation, preventing BoN from effectively scaling as N increases. Addressing this limitation remains an important research challenge for improving the robustness and scalability of BoN-based approaches.

\subsection{Majority Voting}

Similar to BoN sampling, majority voting, also known as self-consistency, provides an alternative approach for aggregating N different responses \cite{wang2022self}. As the name suggests, this method involves generating N responses, counting the frequency of each unique answer, and selecting the most frequently occurring response as the final output. This approach leverages the inherent redundancy in multiple generations to improve robustness and reliability, making it particularly useful for tasks requiring high confidence in correctness.

Vanilla majority voting offers the advantage of aggregating responses efficiently without relying on a reward model. Additionally, it can be extended to a weighted version, where weights are assigned based on PRM scores or confidence estimations of the generated answers \cite{wang2022self}. While majority voting benefits from not requiring a highly accurate reward model, it faces challenges in identifying equivalent answers in complex reasoning tasks. For instance, in mathematical problems, expressions such as $\frac{1}{\sqrt{3}} $ and $\frac{\sqrt{3}}{3}$ are equivalent but must be recognized as such to ensure correct vote counting. A common solution in mathematical domains involves using symbolic-based checkers to compare answer pairs and identify equivalences. However, this process can be computationally expensive, particularly when N responses exhibit high diversity, requiring up to $O(N^2)$ comparisons. In some cases, this comparison overhead can be as time-consuming as the GPU inference itself, posing a significant bottleneck in large-scale reasoning tasks.






\section{Iterative Deepening Sampling}


\begin{algorithm}[thb]
\caption{Iterative Deepening Sampling}
\label{alg:id_sampling}
\begin{algorithmic}[1]
\STATE \textbf{function} \textsc{ID-Sample}(LLM, Dataset, $B_0$, $\gamma$):
\STATE $Prefixes \gets \textit{Questions from Dataset}$
\STATE $Budget \gets B_0$
\WHILE{$Prefixes$ is not empty}
    \STATE $NewPrefixes \gets \{\}$
    \STATE $Outputs \gets \text{LLM.Search}(Prefixes, Budget)$
    \FOR{each $Output$ in $Outputs$}
        \IF{$Output$ is finished}
            \STATE \textsc{LogAnswer}($Output$)
        \ELSE
            \STATE $NewPrefix \gets \textsc{PadTrigger}(Output)$
            \STATE $NewPrefixes.\text{append}(NewPrefix)$
        \ENDIF
    \ENDFOR
    \STATE $Budget \gets Budget \times \gamma$
    \STATE $Prefixes \gets NewPrefixes$
\ENDWHILE
\STATE \textbf{return} $Prompt$
\STATE \textbf{end function}
\end{algorithmic}
\end{algorithm}

In this paper, we explore a setting where tree-search-based strategies are combined with sequential revision. Given a fixed computational budget, an important challenge is determining how much additional budget should be allocated to refining a given prefix $x_0$ that the model has already sampled. Efficient budget allocation is crucial, as any saved resources can be redirected to increasing the number of N in Best-of-N sampling or deepening tree search, ultimately improving overall performance. Understanding this trade-off is key to optimizing both search efficiency and model output quality.

More specifically, in the setting where sequential revision is considered, the self-evaluation and self-correction process can be manually triggered by introducing a predefined \textit{trigger sentence}, such as "Wait! Maybe I made some mistakes! I need to rethink from scratch." which is used in our experiment. In most cases, LLMs respond to this trigger by restarting their reasoning process and making self-corrections. Repeatedly inserting this sentence increases the overall length of the reasoning trajectory, potentially improving problem-solving accuracy by facilitating iterative refinement. In this paper, we propose a method for strategically placing the trigger sentence at increasing intervals within longer contextual windows, aiming to balance computational efficiency and the effectiveness of self-correction.

Suppose we have already used a budget of $b$ to generate a prefix $x_0$ and are now considering whether to immediately introduce a trigger sentence. Iterative Deepening (ID) sampling allocates an additional budget of $\gamma \dots b$ before inserting the next trigger sentence, where $\gamma > 1$ is a tunable hyperparameter. This iterative process continues until reaching a maximum budget $B$, beyond which no further trigger sentences are introduced.
If the reasoning process reaches a natural stopping point—i.e., a complete answer is generated within the allocated budget—the process terminates early. This is because generating a full response from scratch generally leads to more reliable outputs than attempting to refine an already complete solution. Additionally, continuing the reasoning process after an answer has been formed often requires the use of process reward models (PRMs), which tend to be less accurate in evaluating partial completions. Moreover, expecting an LLM to generate a different answer solely through self-correction assumes a highly robust self-revision capability, whereas the same computational cost could instead be used to generate an entirely new response from scratch.
The complete procedure is outlined in Algorithm~\ref{alg:id_sampling}, where $B_0$ represents the initial budget. The function LLM.search performs tree search within a given budget and is adaptable to different tree-search strategies. The function PadTrigger handles the insertion of trigger sentences while ensuring redundancy is minimized if necessary.

The definition and allocation of computational budget depend on the specific tree-search framework employed, leading to variations in implementation strategies. In this paper, we focus on the setting where N responses are sampled in parallel, which we will discuss in detail later.  We have provided an illustration of ID-sampling in Fig.~\ref{fig:workflow}.

Before going into the details, a key challenge in ID-sampling is that budget control occurs before each manually triggered self-evaluation and self-correction step without explicitly analyzing the actual generated responses. This can lead to unnecessary iterations, potentially increasing computational costs. However, due to the design of our ID-sampling method, we establish important theoretical guarantees. In particular, we provide a bound on the total number of tokens generated before reaching the final answer, as formalized in the following theorem.

\begin{theorem}
    Suppose the final answer obtained through ID-Sampling needs a budget of $L$. Then the total number of budget used is no more than $\frac{\gamma * L}{\gamma + 1}$.
\end{theorem}

\paragraph{Proof Sketch}
Note that the budget for each iteration of generation is a geometric sequence with a common ratio of $\gamma$, the theorem is a direct conclusion by using a summarization of the geometric sequence.

It is important to note that Iterative Deepening (ID) sampling does not impose any assumptions or constraints on the model’s inherent self-correction or self-evaluation capabilities. In some cases, the model may naturally generate a response that already includes a trigger sentence before an explicit manual insertion. We observe that the built-in reasoning capabilities of recent state-of-the-art models, such as OpenAI-o1 \cite{jaech2024openai} and DeepSeek-R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, significantly impact the effectiveness of ID-sampling. To better understand these effects, we conduct a comprehensive study using DeepSeek-R1, which we present in the experimental section.

In this paper, we instantiate the proposed algorithm in a setting where N responses are sampled in parallel and later aggregated—a common framework used in Best-of-N sampling and majority voting. Since response generation is independent and typically performed in parallel, the computational cost primarily depends on the total number of generated responses $N$ and their respective lengths.
To manage computational efficiency, we define the budget as the maximum number of tokens generated in a given round, which corresponds to the $max\_token$ parameter in LLM serving engines such as vLLM \cite{kwon2023efficient}. Additionally, to avoid inserting the trigger sentence mid-sentence, we extend generation until the completion of a reasoning step. Here, a step is defined similarly to its usage in beam search and Monte Carlo Tree Search (MCTS), typically identified by token splits such as `\textbackslash n' or `\textbackslash n\textbackslash n'. This ensures that trigger sentence placement aligns with the logical structure of the generated response, preserving coherence and stability in the reasoning process.

Leveraging modern serving engines to efficiently allocate the KV-cache and execute inference in batches using data parallelism and pipeline parallelism significantly enhances the efficiency of Iterative Deepening (ID) sampling. By setting a maximum generation token limit in the sampling parameters, the computational cost scales linearly with generation time, ensuring that ID-sampling remains efficient even as the number of iterations increases.





\section{Experiments}

\subsection{Experiment Setup}

In our experiments, we evaluate Iterative Deepening (ID) sampling on a set of mathematical problems. Given the strong problem-solving capabilities of modern models, which inherently possess self-evaluation and self-correction mechanisms, we focus on more challenging benchmarks and omit relatively easier datasets such as GSM8K \cite{cobbe2021gsm8k} and MATH \cite{hendrycksmath2021}. Instead, we directly assess performance on competition-level and Olympiad-level benchmarks, including MATH-500 \cite{lightman2023lets} and AIME-24. Specifically, MATH-500 is a curated subset of 500 problems from the original MATH dataset, designed to evaluate process reward models trained for mathematical reasoning. Meanwhile, AIME (American Invitational Mathematics Examination) is a highly competitive exam aimed at identifying top-performing high school students in the U.S. The AIME-24 dataset comprises 30 problems sourced from the 2024 AIME I and II exams, providing a rigorous benchmark for assessing reasoning capabilities under competition-level constraints. 
For both datasets, answers are evaluated using symbolic-based checkers to ensure that all mathematically equivalent responses are recognized as correct.

For our experiments, we primarily use LLAMA-3.1-1B-Instruct \cite{dubey2024llama} and Phi-4 \cite{abdin2024phi} before transitioning to the latest DeepSeek-R1-Distill-Qwen-7B \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}. This allows us to investigate how the strong built-in self-correction capabilities of recently released R1 models influence the performance of our algorithm.
For reward models, we employ the newly released Qwen-2.5-Math-PRM-7B \cite{prmlessons}, which demonstrates superior accuracy compared to other available models. This choice ensures a robust evaluation of reasoning performance in our study.

For baselines, we compare our approach against vanilla Best-of-N (BoN) sampling. For evaluation, we report three key metrics:
\begin{enumerate}
    \item Best-of-N (BoN) – The accuracy when a reward model selects the best response from N generated samples.
    \item Majority Voting (cons@N) – The accuracy when responses are aggregated via unweighted majority voting, selecting the most frequent answer.
\end{enumerate}
For both BoN and majority voting, a single aggregated answer is compared against the ground-truth solution to measure accuracy. Note that when $N=1$, BoN is necessarily reporting the Pass@1 score.

All experiments are conducted on an NVIDIA A100 GPU (80GB memory) using the vLLM serving engine \cite{kwon2023efficient}.
By default, we use a carefully selected set of hyperparameters to balance efficiency and performance:
\begin{itemize}
    \item Based on the typical sampling budget used in training (10–40), we set the maximum number of N to 32 for Best-of-N (BoN) sampling.
    \item For ID-sampling with BoN, we set the initial budget $B_0$ as 256 and terminate when the maximum sequence length reaches 8192 for Deepseek-R1-distill-Qwen-7B and 4096 otherwise.
    \item By default, we use $\gamma=2.0$ for ID-sampling, and we will later provide an ablation study results on this.
\end{itemize}

\subsection{Results}

Before evaluating the pass rate, we first analyze the runtime overhead of ID-sampling. We observe that ID-sampling requires approximately 1.6–1.9× the runtime of a baseline approach without the trigger sentence. Given that this additional computational cost could instead be allocated to generating more responses and selecting the best one, we present our results in terms of an equivalent N. Specifically, if the original results correspond to Bo8, we report them as equivalent 
$N=16$, as ID-sampling consistently completes within twice the runtime of the original method.

\subsubsection{Math-500}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/phi4_math.png}
    \caption{Results for Phi4 on Math-500 dataset. The x-axis is the equivalent N after considering the additional time used by ID-sampling.}
    \label{fig:phi4_math}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/r1_math.png}
    \caption{Results for Deepseek-R1-Distill-Qwen-7B on Math-500 dataset. The x-axis is the equivalent N after considering the additional time used by ID-sampling.}
    \label{fig:r1_math}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/llama_math.png}
    \caption{Results for Llama-3.1-1B-Instruct on Math-500. The x-axis is the equivalent N after considering the additional time used by ID-sampling.}
    \label{fig:llama_math}
\end{figure}

The MATH-500 dataset is a curated subset of the MATH dataset, selected to retain its more challenging problems. Originally used by OpenAI to train process reward models (PRMs) \cite{lightman2023lets}, this dataset benefits from the high accuracy of recently released reward models. As a result, in this setting, Best-of-N (BoN) sampling can serve as a reliable sampling method even in the absence of a ground-truth checker.

We present our results for different models on Math-500 in Fig.\ref{fig:phi4_math}, \ref{fig:r1_math}, \ref{fig:llama_math}. Across all scenarios, we observe that ID-sampling provides a significant improvement at the same $N$ and remains effective in most scenarios even when considering the additional computational cost as the equivalent $N$. However, the majority voting score from ID-sampling does not always outperform. 
The performance gap in majority voting can be attributed to the relative self-correction capabilities of different models and their built-in ability to trigger self-correction. For models with limited self-correction capabilities, manually inserting a trigger sentence, as done in ID-sampling, can sometimes degrade performance, potentially turning previously correct responses into incorrect ones.
Because of this, in our setting where N responses are sampled independently, ID-sampling results in a more uniform response distribution. As a consequence, incorrect answers now have a higher probability of being mistakenly selected as the final output under majority voting.
While BoN mitigates this issue by leveraging a separate reward model, majority voting relies solely on frequency-based aggregation, making it less effective in such cases.

Our results also indicate that the performance gap between ID-sampling and vanilla sampling increases as model capabilities improve. Notably, even with the latest DeepSeek-R1-Distill-Qwen-7B, a state-of-the-art reasoning model with built-in self-evaluation and self-correction, ID-sampling consistently outperforms vanilla sampling. This is because while stronger models excel at self-correction, they remain suboptimal at determining when to initiate the self-correction process. Compared to earlier models, each trigger sentence has a more pronounced effect, allowing ID-sampling to correct errors that might otherwise persist without explicit intervention.


\subsubsection{AIME-24}

\begin{table}[tb]
\centering
\begin{tabular}{c|cc|cc}
\multicolumn{5}{c}{\textbf{Phi-4}} \\[3pt]
\hline
$N$
  & \multicolumn{2}{c|}{\textbf{Vanilla}} 
  & \multicolumn{2}{c}{\textbf{ID-sampling}} \\
\cline{2-5}
& \textbf{BoN} & \textbf{cons@N} & \textbf{BoN} & \textbf{cons@N} \\
\hline
1  & 17.24 & 17.24 & \textbf{20.69} & \textbf{20.69} \\
4  & 13.79 & 17.24 & \textbf{24.14} & \textbf{24.14} \\
8  & 13.79 & 20.69 & \textbf{27.59} & \textbf{24.14} \\
16 & 13.79 & 20.69 & \textbf{27.59} & \textbf{20.69}         \\
32 & 13.79 & 20.69 & \textbf{27.59} & \textbf{20.69}         \\
\hline
\end{tabular}

\vspace{8pt} %

\begin{tabular}{c|cc|cc}
\multicolumn{5}{c}{\textbf{Llama}} \\[3pt]
\hline
$N$
  & \multicolumn{2}{c|}{\textbf{Vanilla}}
  & \multicolumn{2}{c}{\textbf{ID-Sampling}} \\
\cline{2-5}
& \textbf{BoN} & \textbf{cons@N} 
& \textbf{BoN} & \textbf{cons@N} \\
\hline
1  & 0.00           & 0.00           
   & \textbf{3.45}  & \textbf{3.45}   \\
4  & 0.00           & 0.00           
   & 0.00           & \textbf{3.45}  \\
8  & 0.00           & 0.00           
   & 0.00           & \textbf{3.45}   \\
16 & \textbf{3.45}  & 0.00           
   & 0.00           & \textbf{3.45}   \\
32 & 3.45           & 0.00           
   & \textbf{10.34} & \textbf{6.90}  \\
\hline
\end{tabular}

\caption{ Results for different models with different number of samples on AIME-24 dataset.
\textbf{Top:} Phi-4 comparison. 
\textbf{Bottom:} Llama-3.1-1B-Instruct comparison (BoN vs.\ ID-BoN). 
Best results in each setting is highlighted in bold. Here, the number of samples $N$ is not calculated as equivalent $N$.
}
\label{tab:aime_non_r1}
\end{table}

\begin{figure}[tb]
    \centering
        \includegraphics[width=0.8\linewidth]{figs/r1_BoN_aime.png}
        \caption{BoN results for DeepSeek-R1-Distill-Qwen-7B on AIME with different $\gamma$, compared to the vanilla sampling. The numbers in the bracket represent the $\gamma$ used in ID-sampling. Given the diverse time used for different $\gamma$, the x-axis is the actual $N$ used rather than the equivalent one.}
        \label{fig:r1_bon_aime}
\end{figure}

\begin{figure}[tb]
    \centering
        \centering
        \includegraphics[width=0.8\linewidth]{figs/r1_cons_aime.png}
    \caption{
cons@N results for DeepSeek-R1-Distill-Qwen-7B on AIME with different $\gamma$, compared to the vanilla sampling. The numbers in the bracket represent the $\gamma$ used in ID-sampling. Given the diverse time used for different $\gamma$, the x-axis is the actual $N$ used rather than the equivalent one.}
    \label{fig:r1_cons_aime}
\end{figure}

\begin{table}[tb]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Method  & Relative Time \\ \midrule
Vanilla & 1.00              \\
ID(2.5) & 1.17           \\
ID(2.0) & 1.88           \\
ID(1.5) & 2.29           \\
ID(1.2) & 3.53           \\ \bottomrule
\end{tabular}
\caption{The relative time cost for ID-sampling for different $\gamma$ from Deepseek-R1-distill-Qwen-7B on AIME-24.}
\label{table:time_gamma}
\end{table}

The AIME-24 dataset is a recently introduced, highly challenging benchmark that is explicitly excluded from model training data. In this setting, reward models exhibit significantly lower accuracy, and the pass rates of various LLMs are notably lower compared to MATH-500 \cite{prmlessons}. This makes AIME-24 a more rigorous test of a model’s reasoning and generalization capabilities.

We present our results in Figures \ref{fig:r1_bon_aime}, \ref{fig:r1_cons_aime}, and Table \ref{tab:aime_non_r1}. Notably, for Best-of-N (BoN) sampling, we observe a counterintuitive trend: performance decreases as $N$ increases and more samples are collected. This decline is attributed to false comparisons introduced by the reward model, which may misrank responses. However, even at the largest $N$ setting (i.e., $N=32$), BoN still underperforms compared to ID-sampling across all three models.

This result underscores the effectiveness of ID-sampling on challenging datasets like AIME, where the self-correction mechanism significantly enhances the model's reasoning and iterative refinement capabilities. On the other hand, due to the intrinsic difficulty of the dataset, we do not observe a substantial benefit from scaling $N$ in the current setting. Specifically, if an LLM can solve a given problem, it tends to succeed consistently; conversely, if it fails, generating more responses does not improve accuracy. This finding highlights the necessity of generating more diverse responses, even when self-correction mechanisms are incorporated.

\paragraph{Ablation Study} Given the significant performance gains of ID-sampling on AIME, we conducted an ablation study to analyze the impact of the scaling factor 
$\gamma$ on ID-sampling. We present the pass rate results in Figures \ref{fig:r1_bon_aime} and \ref{fig:r1_cons_aime}, and report the relative inference time for each setting in Table \ref{table:time_gamma}.

Our find that adjusting $\gamma$ substantially affects both performance and computational cost. The worst-performing setting was $\gamma=2.5$, which failed to provide a stable and significant improvement over vanilla sampling. The best performance was achieved at  $\gamma=1.2$, where self-evaluation and self-correction were triggered frequently. We also tested smaller values $\gamma=0.8,1.0)$, but these settings were computationally infeasible, failing to complete within 10× the runtime of vanilla sampling, and were thus omitted.

Interestingly, we observed a non-convex relationship between $\gamma$ and performance, with the second-best setting being $\gamma=2.0$. This non-convex behavior complicates hyperparameter selection. From our perspective, although increasing the number of responses does not provide a clear scaling advantage in the current setting, runtime efficiency remains a critical factor. Notably, selecting  $\gamma=1.2$ nearly doubles the runtime compared to  $\gamma=2.0$. Given this trade-off, we adopt $\gamma=2.0$ as the default setting, as it balances inference speed and output quality.

\section{Discussions}

Due to computational constraints and the sudden release of the DeepSeek-R1 series, which introduced new phenomena that deserve investigation, our study primarily focuses on ID-sampling without integrating it into tree-search algorithms. However, we emphasize that our proposed framework can be directly extended to methods such as beam search and Monte Carlo Tree Search (MCTS). In these cases, the budget in Algorithm~\ref{alg:id_sampling} can be directly interpreted as the computational budget in MCTS or the number of iterations in beam search.
One challenge in this extension is that self-correction mechanisms reduce the reliance on early model outputs being correct, distinguishing the need for a new generation of PRMs from previously released ones.
However, future researchers who develop or have access to more accurate PRMs that explicitly model the self-correction process can leverage our ID-sampling framework to enhance the reasoning capabilities of fixed models.

Beyond combining ID-sampling with tree-search algorithms, several promising research directions could further enhance performance. However, these avenues remain largely unexplored due to the substantial computational resources required. 
First of all, in our study, we employ a fixed trigger sentence—``Wait! Maybe I made some mistakes! I need to rethink from scratch."—as introduced by \citet{zhao2024marcoo1openreasoningmodels}. However, recent findings from DeepSeek-R1 suggest that self-evaluation and self-correction can emerge spontaneously without explicit manual prompting, implying that trigger sentences can be highly flexible. Moreover, DeepSeek-R1 models demonstrate a tendency to self-correct locally, often requiring only minimal prompts—such as a single word (``Wait!")—followed by context-specific revisions. This suggests a significant opportunity: modifying the trigger sentence could substantially impact both the computational efficiency and the pass rate performance of ID-sampling. In our preliminary experiments, we tested a minimal trigger sentence—``Wait!"—but found that it was insufficient to consistently induce self-correction, often introducing noise rather than meaningful refinements. As reasoning models like o1 continue to evolve, future research could provide deeper insights into optimizing trigger sentences—both in form and function—to maximize the effectiveness of self-reflection mechanisms.
A second promising research direction is exploring alternative frequencies for inserting trigger sentences beyond the current geometric sequence approach. While ID-sampling provides a key advantage—offering a theoretical guarantee of bounded additional computational cost compared to one-shot generation—it remains possible that alternative designs, even without strong theoretical guarantees, could yield superior empirical performance. However, the design space for such strategies is vast, making exhaustive exploration infeasible. We believe that our proposed algorithm establishes a strong baseline for future research aimed at optimizing the frequency of self-evaluation and self-correction triggers, paving the way for more efficient and effective reasoning mechanisms. 
Lastly, in the early stages of our project, we investigated whether the initial budget impacts the performance of ID-sampling. However, we did not observe any consistent effect, particularly because the geometrically increasing budget quickly equalizes differences across initial budget settings after a few trigger sentences are introduced. Nevertheless, it remains possible that different models exhibit varying sensitivity to this hyperparameter, suggesting an avenue for further exploration.

Our proposed method also has several clear limitations. The most significant is that, as highlighted in the latest DeepSeek-R1 technical report, self-correction capabilities can emerge spontaneously through reinforcement learning. This suggests that our algorithm has a more constrained range of applications. In particular, as discussed in the experimental section, the performance of our method varies significantly depending on the underlying model, and the improvement margin of ID-sampling is notably smaller on newer models. However, we argue that given the demonstrated effectiveness of using supervised fine-tuning (SFT) for distillation from existing models \cite{deepseekai2025deepseekr1incentivizingreasoningcapability}, along with theoretical guarantees that bound the additional computational cost, ID-sampling remains a valuable approach for efficiently generating data in certain scenarios. 
Moreover, incorporating trigger sentences into the generation process requires ID-sampling to invoke multiple generation steps. While this theoretically incurs no additional cost on the KV-cache, in practice, it can lead to increased inference time unless one modifies the inference engine—such as vLLM—to manually store and reuse the KV-cache. 
Additionally, due to the need for multiple sampling steps, ID-sampling introduces extra assistant tokens during generation, which may cause a slight distribution shift when applied to black-box models. However, our experiments indicate that the impact of this shift is generally minimal and can be ignored in most cases.


\section{Conclusions}

In this paper, we introduce Iterative Deepening (ID) Sampling, a simple yet effective sampling algorithm designed to adaptively allocate computational budget for sequential revisions in large language models (LLMs). We demonstrate that ID-sampling effectively enhances inference-time performance by improving the pass rate on challenging mathematical reasoning tasks across various models. Our results indicate that while current models exhibit strong self-correction capabilities, they still have significant room for improvement in determining when to trigger self-correction themselves without human intervention.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.

\bibliography{example_paper}
\bibliographystyle{icml2025}




\end{document}
