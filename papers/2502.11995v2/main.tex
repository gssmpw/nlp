\pdfoutput=1

\documentclass[11pt]{article}
\usepackage{amssymb}


\usepackage[preprint]{acl}
\DeclareUnicodeCharacter{2060}{}
\usepackage[most]{tcolorbox}  

\tcbuselibrary{breakable}

\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx} %
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{url}
\usepackage{cuted}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}


\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx} %
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{margin=1in}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
keywordstyle=\color{magenta},numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,                  
    showspaces=false,              
    showstringspaces=false,
    showtabs=false,                 
    tabsize=2
}

\lstset{style=mystyle}






\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{multirow}


\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{hyperref}

\usepackage{inconsolata}
\usepackage[justification=centering, labelfont=bf, font=small]{caption}


\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}

\title{Presumed Cultural Identity: How Names Shape LLM Responses}



\author{
  \textbf{Siddhesh Pawar\textsuperscript{1}}, 
  \textbf{Arnav Arora\textsuperscript{1}}, 
  \textbf{Lucie-Aimée Kaffee\textsuperscript{2}}, 
  \textbf{Isabelle Augenstein\textsuperscript{1}}\\[6pt]
  \textsuperscript{1}University of Copenhagen, Denmark,\quad
  \textsuperscript{2}Hugging Face\\[4pt]
  \texttt{\{sipa, aar\}@di.ku.dk},\quad
  \texttt{lucie.kaffee@huggingface.co},\quad
  \texttt{augenstein@di.ku.dk}
}
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true,citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}








\begin{document}
\maketitle
\begin{abstract}
Names are deeply tied to human identity.
They can serve as markers of individuality, cultural heritage, and personal history. %
However, using names as a core indicator of identity can lead to over-simplification of complex identities.
When interacting with LLMs, %
user names are an important point of information for personalisation. 
Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation.
We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation \footnote{Code available at: \href{https://github.com/copenlu/cultural-name-bias}{https://github.com/copenlu/cultural-name-bias}}.
\end{abstract}





\section{Introduction}


Large language models (LLMs) are increasingly being integrated into personalized applications like virtual assistants, where providing helpful suggestions requires tailoring responses to individual users.
 To build this understanding, models have to undergo a process of implicit personalisation, i.e., changing the answer based on implicit assumptions about the user~\cite{jin2024implicit}. Popular platforms offering virtual assistants also have features where they store `memories' about the user~\citep{openai2024-memories} or mimic the writing style~\citep{claude-style} to tailor the response to a specific user. 
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/Name-bias-eg.png}
    \caption{Example of an interaction with an LLM with an identity presumption based on the name}
    \label{fig:enter-label}
\end{figure}

Names carry deep cultural and personal identity, playing a central role in human communication. Sociological research indicates that names are imbued with culturally loaded meanings that can trigger stereotypes and discriminatory responses— evidence of which is seen in field experiments, where individuals with ethnically distinctive names receive fewer opportunities ~\citep{bertrand2003emily, fryer2004causes}. However, names do not always equate to a singular cultural identity. People may have names that reflect heritage from one culture while having grown up in a completely different cultural context, such as in cases of immigration, diaspora communities, or multicultural families. In human interaction, there is usually a larger context or other cues that provide a signal to a speaker about the other person's identity. However, such cues may be missing when a user is interacting with an LLM, making the limited information available in the prompts and stored in memory very important. Indeed, in analyzing LLM memory traces, \citet{openai2024-fairness} found that the most common single memory is: ``User’s name is <NAME>''], and that users often explicitly mention their own name in their interactions with models. Therefore, names could serve as a rich signal for personalisation to the models. However, erroneous assumptions about a name’s associated identity can lead to biased or misleading personalisation, reinforcing stereotypes.


LLMs are trained on vast and heterogeneous datasets – often comprising Web-scraped text, literature, and digital communications – that inherently include personal information, linking names with various identifying attributes and identities \citep{plant2022you}. This linking leads to a name bias, which alters the output when a name is mentioned in the prompt \citep{haim2024s, wei2024uncovering}. While prior work has examined gender and race presumptions based on names \citep{haim2024s, wolfe-caliskan-2021-low}, there has been no work on investigating cultural presumptions in LLMs. Examining name-biased cultural presumptions reveals how models represent, propagate and flatten cultural stereotypes, but also provides insights for developing more equitable, culturally sensitive AI systems \citep{naous-etal-2024-beer}. 

Our \textbf{contributions} are thus as follows.
We study \textbf{name bias with respect to cultural presumptions in LLMs} with 900 names %
across 30 cultures and 4 LLMs and questions spanning multiple cultural facets including food, clothing, and rituals. We prompt LLMs with different information-seeking questions with a name included in the prompt and assess cultural presumption in the responses.
Our analysis shows \textbf{strong evidence of cultural identity assumption} and significant \textbf{asymmetries in how LLMs associate names with cultural elements}, with particularly strong biases for some cultures (e.g. East Asian and Russian names), while showing weaker associations for names from certain other cultures.
Finally, there is also \textbf{substantial disparity between the names themselves}. Some names lead to much more biased responses compared to others. 
This has substantial implications for future work. How LLMs should adapt to output based on user 
names and assumed culture presents a complex interplay between beneficial customisation and the inadvertent reinforcement of biases, and requires fundamental and nuanced considerations.


\section{Background}

\paragraph{LLM personalisation}
The recent uptake of chat interfaces for LLM has led to attempts to personalise LLM interactions by tailoring model outputs to individual user preferences and contexts ~\cite{zhang2024personalization}. Recent studies have explored various approaches to enhance LLM personalisation, such as reducing redundancy and creating more personalized interactions by remembering user conversations \cite{magister2024way, salemi2023lamp}.

However, personalisation can also lead to over-simplifying user identity and reproduce or amplify model bias. This problem has been observed across various technical fields, e.g. \citet{greene2019personal} discusses how personalisation often reduces individuals to feature vectors, neglecting the complex facets of personal identity and potentially reinforcing biases present in the data. However, in the context of LLMs research on personalisation has just started.
Previous work found that when LLMs are assigned personas, they exhibit bias, perpetuating stereotypes \cite{DBLP:conf/iclr/GuptaSDKCSK24}, even when those identities are implicit \cite{kantharuban2024stereotype,jin2024implicit}. In our work, we examine these implicit biases through the lense of names, i.e. the output of models being influenced by the addition of names across cultures.

\paragraph{Bias in LLMs}
Names are deeply intertwined with personal and cultural identity \cite{watzlawik2016first,dion1983names}. \citet{tajfel2010social}'s \textit{Social Identity Theory} posits that individuals derive a significant part of their self-concept from their membership in social groups, with names acting as identifiers of these affiliations. However, there can be a disconnect between one's name and cultural background, leading to complex implications for one's sense of belonging \cite{deaza2019impact}. Names not always being a simple indicator of identity is exemplified by name assimilation, the adoption of common Western names by minority ethnic groups and immigrants \cite{carneiro2020please}.


As names can lead to simplified assumptions about user identity, names have been used across a variety of studies investigating bias in LLMs. For example, \citet{haim2024s} prompt LLMs with scenarios involving individuals with names associated with various racial and gender groups in the American cultural context. Their findings reveal that the models systematically disadvantage names commonly linked to racial minorities and women, with names associated with Black women receiving the least favorable outcomes. 
Names have been used as a proxy for gender \citet{DBLP:conf/ci2/KotekDS23,DBLP:conf/emnlp/WanPSGCP23} and ethnic identity bias \cite{DBLP:conf/acl/NadeemBR20}, and cultural personas \cite{DBLP:journals/corr/abs-2406-13993}. There has been a recent increase in work on cultural biases in LLMs \citep{pawar2024survey}. \citet{openai2024-fairness} evaluate the bias introduced by names in ChatGPT. They state that users often share their own names with chat assistants for tasks such as writing e-mails. Similar to our work, they examine first-person bias. While their work focuses on the propagation of harmful stereotypes related to race and gender, our study focuses on general cultural stereotypes. %


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/name-bias-setup.png}
    \caption{Experimental Setup}
    \label{fig:setup}
\end{figure*}


\section{Methodology}
\label{sec:methodology}

\subsection{Names}
We use a dataset from Facebook~\citep{NameDataset2021} to obtain names from across the world, based on names of Facebook users. It includes the most popular names, their gender, and the country from which the name was sourced. We only use  first names for our task and select the top 30 names (based on popularity) from the dataset with an equal mix of male and female names (genders are marked in the dataset). %



\subsection{Cultural information}
We also need information about different cultures as ground truth to identify presumed cultures in LLM responses and to create information-seeking questions that require some cultural assumptions. We leverage assertions about cultures in the knowledge graph (KG) CANDLE~\citep{candle2023} to do this. The KG has 1.1 million assertions about cultural common-sense knowledge across 5 facets of culture - food, drinks, tradition, clothing, and rituals. Qualitative analyses reveal that CANDLE contains numerous generic assertions about cultures that do not meaningfully contribute to our information-seeking setting, e.g. statements such as `The Chinese civilization has been a long and enduring one.' To filter these out, we develop an LLM-based approach that identifies whether an assertion contains a concrete, distinctive cultural element (such as a specific food, tradition, ritual, or practice) rather than general claims about a culture's history, values or characteristics. More details can be found in \autoref{app:assert_filtering}.

\subsection{Cultures}
To decide which cultures to use for our study, we take an intersection of the two data sources we list above, i.e. the source of names and the source of cultural information. We take the cultures with at least 30 names in the names dataset and at least 200 (filtered) assertions pertaining to the cultures in CANDLE-KG. Taking the intersection of the two, results in 30 countries, see \autoref{fig:bias_default}. %
For the scope of this study, we adopt a one-to-one mapping between cultures and countries to align with our names dataset and CANDLE's organization, while acknowledging that cultural identities often transcend national boundaries.

\subsection{Questions}
To create the questions to probe LLMs, we use a semi-automatic approach. For a set of seed questions, the authors of this study manually crafted a list of pertaining to the categories used in the KG, i.e., clothing, food/drinks, tradition/rituals. This was done by qualitatively going through insights about what kind of questions are asked in real-world LLM interactions~\citep{zhao2024wildchat1mchatgptinteraction, ouyang-etal-2023-shifted}.


To expand this set of seed questions and remove potential biases from manual curation, we add questions from a list of candidate questions generated by an LLM. For generating candidate questions that are related to the assertions, we prompt an LLM to generate candidate questions from clusters of assertions. Specifically, we remove country names (to ensure that clusters are about concepts rather than about cultures) from the assertions and cluster using BERTopic~\citep{grootendorst2022bertopic} into clusters of topically similar assertions. From each cluster, we generated open-ended questions for which CANDLE assertions could serve as informative answers. We used an LLM with a prompt (shown in  \autoref{prompt-question-gen}) that converts 5 assertions from a cluster into a generic, culture-agnostic question. For example, an assertion like `Traditional Finnish breakfast includes porridge' would generate a question like: `What are some traditional breakfast foods in different cultures?'; this process resulted in 1,935 candidate questions. The authors then manually selected questions from these candidates and expanded the seed question list. The final question list is provided in~\autoref{tab:que_list}.
\subsection{Models}
We evaluate five different models to analyse name-based bias. Our selection includes four open-weights models: Aya \citep{ustun2024aya}, DeepSeek \citep{guo2025deepseek}, Llama \citep{dubey2024llama}, and Mistral-Nemo \citep{mistral2023nemo}
and one closed model: GPT-4o-mini \citep{openai20244o}. We provide details of the exact model checkpoints and names in \autoref{tab:model_codes} in the Appendix. This diverse set of models ensures representation from various geographical backgrounds, allowing us to explore how training data origins and model design impact biases in personalisation. By evaluating this mix of models, we aim to uncover differences in name-related biases influenced by pre-training data sources, fine-tuning methodologies, and the geographic origins of model development. We do all our analysis for generations in English.


\subsection{Experimental setting}

We outline our experimental setup in \autoref{fig:setup} -- we generate responses to different questions using prompts with and without names in them. We then assess bias in responses in the form of cultural presumptions through two methodologies and compare their performance. The details of various parts of our pipeline are as follows. 

\subsubsection{Response generation}
For generating responses to probe LLMs, we add the name to the system prompt, in the format: ``My name is <Name>. Help me with the following questions''. We add questions to the user prompt. 




\subsubsection{Cultural presumption detection}
\label{sec: bias-det}
We formulate a presumed culture, when responses to a question have an overt bias through particular cultural information included within them. As shown in \autoref{fig:setup}, we use two methodologies for cultural presumption detection. One using a pure LLM-as-a-judge approach where the model is tasked with detecting if the generated response is biased towards a given culture. The second, where an assertion is provided from CANDLE and the model is tasked with checking if that assertion is \textit{contained} within the model response. The prompts used for both these tasks are provided in \autoref{fig:assertion-prompt}  and \autoref{fig:bias-check-prompt} in the Appendix. We evaluate both these approaches manually.

\subsubsection{Human evaluation}
For analysing bias evaluation through our method, we conduct a human evaluation of the performance of the detection classifiers on 300 responses. Two PhD students are asked to (in tandem) annotate a randomly sampled set of model responses stratified by model type. We provide annotation guidelines and details in the Appendix (\ref{Anno}).
We evaluate both our approaches through the labeled set above. Our LLM-as-a-judge cultural presumption classifier has a 95\% accuracy. For our entailment classifier, when compared against the second question, we achieved an 85.4\% accuracy. This is because the labels for the second question are at times `yes' even when the first one is `no', due to assertion being contained but the response being tailored towards several cultures, such as recommendations of dishes from around the world. While the assertion-based approach is grounded in real-world data, with assertions drawn from human generated text, the labels overpredict bias when measuring cultural presumption. For this reason, we report results with our LLM-as-a-judge approach in our paper.


\subsection{Robustness validation using CANDLE}
Though the results of our assertion-based approach overpredicts bias, as reported in the previous section, we conduct a correlation analysis between the response bias calculated through the two approaches. We calculate Pearson correlation and Spearman rank correlation between bias values of countries for each model and facet pair.

While the overall correlations are moderate (Pearson = 0.218, Spearman = 0.423), a deeper examination shows stronger correlations between top-10 and bottom-10 values. For the highest-bias instances, examining the union of top-10 biased cultures from each method, we find a sizable correlation (Pearson = 0.782, Spearman = 0.755), with food-related biases showing near-perfect correlation (Pearson = 1.000, Spearman = 0.988). Even for the bottom-10 values, we find a strong correlation (Pearson = 0.967, Spearman = 0.800).

Food-related responses show the strongest correlation (Spearman = 0.585), followed by clothing (Spearman = 0.440) while tradition and ritual shows moderate correlations (Spearman = 0.307 and 0.361, respectively), reflecting a high degree of variance in answers.

\subsection{Bias calculation}
\label{sec: bias_cal}
We calculate cultural bias in model responses using LLM-as-a-judge (detailed in ref \ref{sec: bias-det}). We measure bias by calculating how frequently responses show cultural preferences for each combination of culture, model, and facet. These frequencies are then averaged across different names and questions to obtain a final bias score. We find that even prompts without names show cultural bias. To isolate the impact of names, we measure this `default bias' in responses without names and subtract it from the bias found in responses with names. This gives us a clearer measure of the additional bias introduced by cultural names.

Mathematically, for each combination of culture \(c\), model \(m\), and facet \(f\), the measured bias is defined as:

\begin{equation}
\resizebox{\columnwidth}{!}{$
    \text{Bias}(c_s, c, m, f) = \frac{1}{N_{c_s, m, f}} \sum_{i=1}^{N_{c_s, m, f}} \mathbb{I}\!\left\{ r_i(c, m, f) = 1 \right\}
$}
\end{equation}
where \(N_{c_s, m, f}\) is the number of responses associated with names sourced from culture \(c_s\) for model \(m\) and facet \(f\) (across all questions of that facet), and \(r_i(c, m, f)\) is a binary indicator (with respect to checking culture \(c\)) that equals 1 if the \(i\)th response is biased.

For responses without names, the default bias is computed as:
\begin{equation}
\resizebox{\columnwidth}{!}{$
    \text{Bias}_0(c, m, f) = \frac{1}{N_{m, f}^{(0)}} \sum_{i=1}^{N_{m, f}^{(0)}} \mathbb{I}\!\left\{ r_i^{(0)}(c, m, f) = 1 \right\}
$}
\end{equation}
where \(N_{m, f}^{(0)}\) is the number of responses (without names) for model \(m\) and facet \(f\). Finally, the adjusted bias (which we report and analyse) is defined as:
\begin{equation}
\resizebox{\columnwidth}{!}{$
    \text{Bias}_{\text{adj}}(c_s, c, m, f) = \text{Bias}(c_s, c, m, f) - \text{Bias}_0(c, m, f)
$}
\end{equation}


\section{Results}
\label{sec:results}
\begin{figure}[t]
  \centering
  \includegraphics[width=1\columnwidth, trim={0.3cm 0 0 0},clip]{figures/avg_bias_by_country_def.png}
  \caption{Default Bias values averaged over Models and Facets. For details refer to \autoref{sec: bias_cal}.}
  \label{fig:bias_default}
\end{figure}


\subsection{Default bias}

We calculate default bias (see \autoref{sec: bias_cal}) and observe that model responses show inherent bias towards certain cultures even without names in prompts. When prompted with open-ended information-seeking questions, models disproportionately generate suggestions drawing from East and South Asian cultural elements, with Japanese and Indian references appearing most frequently. This pattern aligns with recent studies \citep{khandelwal2023casteist, li2024culture} that show default responses disproportionately include culture-specific symbols from these regions. While this bias persists across all models, its magnitude varies significantly: DeepSeek shows the lowest average bias (0.035), while Mistral exhibits the highest (0.071), followed by Llama (0.068) and Aya (0.061). 




\begin{figure*}[t]
  \centering
  \includegraphics[width=0.95\textwidth,height=0.8\textheight,keepaspectratio]{figures/vertical_bias_plots.png}
  \caption{Bias across models above the default bais. For calculation of bais refer to section \ref{sec: bias_cal}}
  \label{fig:bias_boxplot}
\end{figure*}




\subsection{Cultural presumptions based on names}

To understand how LLMs associate names with cultures, we analyse the difference between cultural bias (associations) in responses when prompts contain names and when no names are mentioned as discussed in \autoref{sec: bias_cal}. The graph shown in \autoref{fig:bias_boxplot} represents the degree to which a model associates a particular culture to a name from that culture, over the case when no name is provided. For instance, both Korea and Russia show notably high positive differences in Llama3 (around 0.4-0.5), indicating that when presented with Korean or Russian names, the model generates significantly more Korean and Russian specific suggestions respectively, compared to when no name is mentioned. This suggests that names from these cultures lead to high cultural presumption in Llama's responses. Conversely, for countries such as Ireland, Brazil, and the Philippines, we observe negative values, particularly for Llama and Aya. These negative values indicate that when presented with names from these cultures, the models generate more random, diverse suggestions. This results in a lower proportion of culture-specific suggestions compared to the default case where no name is mentioned, suggesting that the models may not have learned strong associations between these names and their corresponding cultural elements (suggesting low resource or flattened cultures). 

\paragraph{Model-based comparison of name bias}

The pattern of biases is not uniform across models as highlighted in~\autoref{fig:bias_boxplot}. DeepSeek and Aya32b exhibit some similarities to Llama (with positive spikes for countries like Russia), yet display lower magnitudes of biases overall. Meanwhile, MistralNemo has the highest bias overall, suggesting that it encodes strong name‐driven associations. Certain countries (e.g., Korea, Russia, India) consistently elicit culture‐specific outputs across models when names from those cultures are mentioned in the prompts. Others (Ireland, Brazil, the Philippines) often lead to more random or generalized suggestions, indicating weaker learned associations between their names and cultural elements. The trends also hold for GPT-4o-mini, which we add in the appendix as experiments were conducted in a more constrained setup (\autoref{openai}). 









  
\paragraph{Facet-based comparison} To understand how cultural bias differs between different categories of cultural questions, we analyse model behavior across three facets: clothing, food, and ritual \& tradition.~\autoref{fig:bias_boxplot_aspect} compares the default bias (without names) and name-induced bias in the responses across these facets. The introduction of culturally-associated names consistently amplifies these biases across all facets, but with varying intensities. Clothing-related queries show the most dramatic increase, with bias rising from 0.071 to 0.121, representing a roughly 70\% increase. This may be because fashion is imbued with overt cultural signifiers and deeply localised traditions that are immediately recognisable and context-specific—often reflecting unique regional aesthetics as compared to other facets \cite{davis1994fashion,chandler2002basics}. Similarly, tradition-related queries see a substantial increase from 0.061 to 0.098. Notably, East Asian countries, particularly Japan, Korea, and India, consistently show the strongest associations across all facets, appearing as outliers in the boxplot with high bias values ranging from 0.3 to 0.45.


\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{figures/aspect_boxplot.png}
  \caption{Box plot showing comparison of  bias for countries values (averaged over 4 models) for each facet.}
  \label{fig:bias_boxplot_aspect}
\end{figure}









\section {Analysis}




 








\subsection{Cross-cultural bias evaluation}



To study cross-cultural biases, we analyse potential bias in responses with respect to other cultures. ~\autoref{fig:model_bias_boxplot} shows cross-cultural bias for all countries above the default bias (averaged across models and facets). One observation across all countries is that mentions of names decrease the diversity of responses. For countries such as Japan, China, and India, this phenomenon is distinctly visible. The responses to questions without names, have predominance of suggestions from these countries. When names from other countries are mentioned, the number of suggestions from these three countries reduces significantly. This leads to bias values  towards these countries being negative (less bias than default). 

\begin{figure}[!t]
  \centering
  \includegraphics[width=1\columnwidth, height=1.1\textheight, keepaspectratio]{figures/overall_diff_heatmap.png}
  \caption{Cross-cultural bias heatmap for bias values over the default (\ref{sec: bias_cal}). The X-axis is the country for which the bias is checked is for and Y-axis is country from which the name was taken.} %
  \label{fig:model_bias_boxplot}
\end{figure}







\subsection{Name-wise comparison}

\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{figures/names_biased_responses_plot.png}
  \caption{Distribution of biased responses per name [Names are omitted from the x-axis to avoid clutter]}
  \label{fig:KDE}
\end{figure}

Not all names elicit biased responses from the models. In fact, the distribution is quite skewed. We show this in \autoref{fig:KDE}. The distribution of biased responses per name is heavily skewed, with most names having relatively few biased responses and a smaller subset having substantially higher counts. 
We list the set of top biased names across all countries and their frequencies in \autoref{tab:all_biased_names}.








\subsection{Names present in more than one culture}
\input{tables/cross-cultural-names}
To study cross-cultural associations, we consider the names present in more than one culture, grouping them based on \citet{hanks2006}. The cross-cultural names in our dataset fall into five broad clusters based on common countries: Anglophone, Hispanic/Latin, European, Middle Eastern/North African, and East Asian names —- with each cluster reflecting different patterns in country association as highlighted in \autoref{tab:name_bias}. %

A key observation is that the models tend to flatten cultural identities by stereotyping names—disproportionately linking them to one dominant country within each group.  For instance, within the Anglophone group, names like Mark and James consistently receive suggestions biased towards the United States (typically 10–12\%), while Canada, despite being an English-speaking country, is assigned very low values (below 1–1.5\%). In the Hispanic/Latin cluster, although names such as Juan, Maria, and Carlos show significant associations with both the US and Mexico, there is a notable bias towards the US, with Spain moderately represented and Portugal almost negligible. 

\subsection{A closer look at the questions}

We examine what words lead to the highest bias when a name is mentioned in the prompt (\autoref{fig:model_bias_words}). The plot reveals that the word `tradition', when mentioned in the question, leads to disproportionally high bias in the responses compared to other words. We also consider bias elicited by the word for each country before and after the mention of the name in \autoref{fig:model_bias_words}. While the proportion of bias elicited by the word `tradition' is extremely low with prompts without names, it becomes sizable when names are mentioned in the prompt.


\begin{figure}[!t]
  \centering
  \includegraphics[width=\columnwidth]{figures/grouped_percentage_comparison.png}
  \caption{Percentage contribution of each word's biased responses relative to the overall number of biased responses}
  \label{fig:model_bias_words}
\end{figure}





















\section {Discussion}
\label{sec:discuss:personalisation}
Through our experiments, we demonstrate that LLMs implicitly personalise their responses by inferring user background from names. Further, simple wording can further strengthen these influences. The mention of the word \textit{tradition}, \textit{cultural} or \textit{family} along with a person's name in the query can lead to responses heavily biased towards some cultures over others. 
Relying solely on a name to determine cultural identity can be problematic as it can introduce biases in model responses towards
underrepresented groups \citep{kantharuban2024stereotype, das-etal-2023-toward}. 
We find that some names clearly introduce more bias than others, raising questions about how AI interaction is inadvertently influenced by a user's name. While we establish this in a template-based single-turn setting, how such response bias would manifest itself in a more naturalistic multi-turn setting remains to be explored. 


How LLMs should adapt output based on user names and assumed culture presents a complex interplay between beneficial customisation and the inadvertent reinforcement of biases. While personalisation aims to enhance user experience by tailoring interactions, it can also lead to the oversimplification of identities, resulting in the perpetuation of stereotypes~\cite{kirk2024benefits}.
The problem of implicit personalisation as a moral problem is defined by \citet{jin2024implicit}, encouraging future discussions of the issues on a case-by-case basis. 
The distinction between beneficial and detrimental personalisation hinges on the model's ability to respect the multifaceted nature of individual identities. These considerations should particularly be made based on deployment context. \citet{kirk2025human} argue that as AI systems become more personalised and agentic, there is a pressing need for socioaffective alignment to ensure that AI behaviors support users' psychological and social well-being. 
Provided the anthropomorphic and relationship building behaviour~\citep{ibrahim2025multiturnevaluationanthropomorphicbehaviours} that models are trained to interact with, above all, it is crucial for models to be trained to be transparent in the assumptions they are making and convey the implicit personalisation taking place. This provides the user with agency, which in the case of an error would allow the user to change the behaviour. 

\section{Conclusion}
Our study establishes and quantifies the change in LLM responses and suggestions (to information seeking questions) when names are mentioned in the context.  
We find strong evidence of cultural identity assumptions, particularly for names from East Asian, Russian, and Indian cultures, while names from Ireland, Brazil, and the Philippines lead to more diverse and generic responses.
We also find disparities between names themselves, with some leading to much more biased responses than others.
Furthermore, a facet-based analysis indicates that clothing and tradition queries amplify bias most dramatically, especially when key terms such as `tradition' are present.
Our cross-cultural analysis highlights the issue of cultural flattening -- that models  
consistantly favour some cultures over others. %
We hope this study will serve as a useful reference for considerations on the utility vs. harms of names-based personalisation of LLMs. 


\section{Limitations}

A limitation of our study is the methodological choice to equate countries with cultures, which is a simplification of complex cultural identities. This one-to-one mapping, while being the prevailing approach work on cultural NLP, fails to capture important nuances such as cultural groups that span multiple countries, multiple distinct cultures within a single country, diaspora communities, and regional cultural variations. While this simplification was necessary because of the nature of the names dataset and CANDLE, it potentially masks more nuanced cultural associations and biases in the models' responses.

Another limitation stems from our source of names and its inherent sampling bias. Countries with high internet penetration and digital presence are better represented both in our names dataset and in LLMs' training data. For instance, names from South Korea or Japan, countries with high internet usage, appear frequently in model responses with specific cultural suggestions, while names from regions with lower digital representation might elicit more generic responses. This data skew could explain why certain cultures consistently show stronger associations in model outputs, reflecting broader digital accessibility disparities rather than purely cultural biases.

\section{Ethical Implications}

In conducting this study, we carefully considered privacy implications by using only first names rather than full names, preventing potential identification of individuals while maintaining authenticity in our experiments. However, this methodological choice, while protective, still enables us to uncover significant ethical concerns about how LLMs make cultural assumptions based on names.
These findings raise ethical concerns about the real-world impact of name-based cultural presumptions in LLMs. When models flatten cultural identities by linking certain names to specific cultural contexts, they risk stereotyping users and misrepresenting individual preferences. In applications like customer service and content recommendation, such misassumptions can lead to misguided personalization that not only reinforces cultural homogenization but also harms user sentiment—potentially causing frustration, feelings of alienation, and even user dropout, particularly among underrepresented groups.

\section*{Acknowledgments}
We thank the Carlsberg Foundation and the Danish e-Infrastructure Consortium (DeiC) for supporting the work through grants and compute. We also thank Nadav Borenstein, Sarah Masud, Greta Warren, and other members of the COPENLU lab for providing feedback on the project in various stages of the project. 


\bibliography{custom}
\clearpage
\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Model details and Experiment Details}
For all our experiments, we use the vLLM library for efficient inference~\citep{kwon2023efficient}. We use the hyperparameters, we provide specific model codes in \autoref{tab:model_codes}.


\textbf{Llama}: We used Meta-Llama-3.1-8B-Instruct available via HuggingFace\footnote{https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct}. We used vLLM for inference with parameters temperature=0.7, top\_p=0.9, max\_tokens=2048, dtype=`half' and max\_model\_len=8096.

\textbf{Aya}: We used Aya-expanse-32b available via HuggingFace\footnote{https://huggingface.co/CohereForAI/aya-expanse-32b}. We used vLLM for inference with parameters temperature=0.8, top\_k=50, max\_tokens=2048, dtype=`half' and max\_model\_len=8096.

\textbf{Mistral}: We used Mistral-Nemo-Instruct-2407 available via HuggingFace\footnote{https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407}. We used vLLM for inference with parameters temperature=0.6, top\_p=0.8, max\_tokens=2048, dtype=`half' and max\_model\_len=8096. 

\textbf{DeepSeek}: We used DeepSeek-R1-Distill-Llama-8B available via HuggingFace\footnote{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B}. We used vLLM for inference with parameters temperature=0.6, top\_p=0.8, max\_tokens=2048, dtype=`half' and max\_model\_len=8096. 

For generating responses (with and without names), we used the above four models, and total number of generations were around 90k per-model, which required around 1 day on 8 A100s. For calculating the bias, we ran LLM-as-a-Judge (using meta-llama/Llama-3.1-70B) to check for bais towards all 30 countries on the 360k responses, which required around 8 days on 8 Nvidia A100s. For robustness analysis, we carried out assertion-checking using meta-llama/Llama-3.1-8B which required  around 10 days on 6 Nvidia H100s (as for each response, to check for bias towards a country, we checked on average 200 Assertions). Hyper-paramters for the LLM-as-a-judge were similar to the ones mentioned above. The names dataset used in the paper is released under  Apache-2.0 license which is a permissive open-source license. allows anyone to freely use, modify, and distribute the licensed software. For the openweight models, we signed the terms of use on HuggingFace which allow to use the models to generate and analyze the data for publications. 


\begin{table}[h]
   \centering
   \begin{tabular*}{\columnwidth}{l@{\extracolsep{\fill}}p{4.5cm}}
   \toprule
       \textbf{Model} & \textbf{HuggingFace Repository} \\
   \midrule
        Aya & CohereForAI/aya-expanse-32b \\
        Mistral & mistralai/Mistral-Nemo-Instruct-2407 \\
        DeepSeek & deepseek-ai/DeepSeek-R1-Distill-Llama-8B \\
        Llama & meta-llama/Meta-Llama-3.1-8B-Instruct \\
   \bottomrule
   \end{tabular*}
   \caption{Models used in this study and their corresponding HuggingFace repository code}
   \label{tab:model_codes}
\end{table}


\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/country_differences_baroai.png}  \caption{OpenAI GPT-4o-mini name bias over the default responses}
  \label{fig:model_bias_openai}
\end{figure}


\subsection{Closed Source Models}
\label{openai}

We also conduct experiments with one closed-source model: gpt-4o-mini, but with 15 names instead of 30 due to resource constraints.~\autoref{fig:model_bias_openai}, highlights bias in responses for prompts with names over the the default bias (bias when no name is mentioned in the prompt). The findings are at par with those of open weights models, and we observe high cultural bias in outputs towards countries like Japan, Korea, India, and Turkey when respective names are mentioned in the prompt. Total cost of generations was around \$30 for around 10k generations. 



\subsection{Assertion filtering}
\label{app:assert_filtering}

As mentioned in \autoref{sec:methodology}, we filter generic assertions about cultures from CANDLE KG. We also observed high overlap between the facets {food, drink} and {tradition, ritual}. Subsequently, questions related to these topics had answers in both sets. To make our comparison fair, we decided to merge the assertions from these facets. Post selection of the countries from the names dataset and the assertion filtering, we have 23k high quality assertions. The prompt for the LLM based assertion filtering can be found in \autoref{prompt_asser_filter}. For the classification, we used an Mistral-instruct-v0.3 model with a temperature of 0.2.

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\textwidth]{figures/grid_bias_plots_base.png}
  \caption{Default Bias across models, for calculation and discussion about default bias refer to section \ref{sec: bias_cal} }
  \label{fig:bias_boxplot_baseline}
\end{figure*}



    
    
    






\section{Method details}
\subsection{Annotation Guidelines}
\label{Anno}
Given a triplet of ${C_i, A_i, R_j}$ where $C_i$ is the \textit{culture} towards which the bias should be checked, $A_i$ is an assertion about that culture from CANDLE, and $R_j$ is a model's response to a question with a name from the same culture $i$ or a different culture $j$, the annotators provided labels for two questions:  (1) Is the response biased towards the country? (2) Is the response biased towards the country, based strictly on the assertion provided? The first question matches our research goal explicitly, though is more subjective. The second is tailored towards the specific assertions from CANDLE and, hence, more grounded. While annotating the questions, following guidelines are shown in \ref{fig:annotation-guidelines}

\input{prompts/Anno-guidelines}

\enlargethispage{28.6pt}
\input{tables/all-multi-cult-names}

\subsection{Prompts}
We provide a list of prompts used for evaluation in \autoref{fig:assertion-prompt} and \autoref{fig:bias-check-prompt} 

\begin{figure*}
\tiny
\lstinputlisting[label=prompt_asser_filter, caption=Prompt used for Assertion Filtering]{prompts/assertion_filtering.tex}
\end{figure*}
\input{prompts/assert-based-checking}
\input{prompts/bias-checking}
\input{tables/biased-names-full}

\begin{figure*}
\lstinputlisting[label=prompt-question-gen, caption=Prompt used for candidate question generation]{prompts/question_generation.tex}
\end{figure*}



\include{tables/question_list}
\end{document}
