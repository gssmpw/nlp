\section{Related Work}
\subsection{Data cleaning}
Data cleaning plays a vital role in enhancing data quality by identifying and rectifying inconsistencies, errors, and redundancies. This process ensures that the data is more accurate and reliable, ultimately supporting better analysis and  decision-making____.   Extensive research has explored various aspects of data cleaning, including addressing key issues by  deduplication, outlier detection, and logical errors____, as well as enhancing the scalability of cleaning algorithms____. Further efforts have focused on developing specialized algorithms tailored to the semantics of specific datasets such as time series____, medical____, Chinese-language e-business____, and GPS  trajectory____ datasets; To achieve these objectives, various approaches have been employed, including  rule-based methods____, probabilistic techniques____, or hybrid models combining both____. More sophisticated requirements, such as privacy considerations, have also been introduced into the data cleaning process____. Additionally, some researchers have proposed incorporating user input into the cleaning process____. A large amount of data exists in semi-structured and unstructured formats. However, limited effort has been devoted to addressing the data quality issues inherent in these formats____, which is the primary focus of our work.   Ilyas and Chu provide a comprehensive review of data cleaning____.



\subsection{Updatability of Extracted Views} 

Expectations from extractors have risen as requirements have become more diversified, from the point that there were no  criteria to evaluate their performance____ to the point that extraction algorithms need to work under various stresses such as noisy data, low response time, and diverse types of input and output____. The body of research related to the problem of updatable views over unstructured data is relatively limited. However, we can draw insights from existing work on updatable views over semi-structured data, such as XML, which is slightly relevant to our study. Similar to the relational setting, creating views over XML databases can provide various advantages, including faster query processing and convenient access control over specific sections of a larger XML database ____. Kozankiewicz et al.____ propose to incorporate  information about forseeable updates over views into the  view definition. Therefore,  the ultimate affects of updates are specified solely by the query designer which, if not verified, might leave the XML database in an incorrect state .  
\subsection{Updatability of Relational Views}
Our research question involves translating updates on  the content of an extracted view to updates on the content of the associated  document. This is similar to the problem of updatability of relational views that is thoroughly studied in relational databases____. 

In the relational setting, the problem of a view updates is defined as finding a translation of a view update   to a database update  such that running the same view definition query on the updated relation produces the updated view regardless of the database instance. Interesting research  challenges are raised from this definition such as how to deal with the problem of multiple possible translations? Are views always updatable? if not how to identify views that cannot be updated? how to derive a specific translation mechanism for a given view definition, database schema, and update specification? 


Two general approaches are proposed for updatability of relational views.  First, along with a view definition, all authorized updates and their corresponding translations should be provided. However, the provided translation mechanism also needs to be verified. The second approach is to exploit the information provided by the view definition, the update mechanism, and database constraints to derive conditions on the legitimacy of a translator. For example, the view dependency graph that is constructed using only the view definition and database schema is used to verify a translator for some classes of deletions, insertions, and replacements____. Our solution to the extracted view updates is aligned with the latter approach.  We limit ourselves to a class of view updates that is realized by a domain-preserving function that maps each extracted value to a value from the same domain, similar to perturbing values of a table to protect privacy.  However,  we do not impose any constraints on the input documents. 
We pick the most natural translation which is to substitute old values in the source document with new values, and we expect to see them extracted by running the same extractor (Figure~\ref{fig:exvu}).\par
In summary, applying solutions proposed in the relational setting to the information extraction domain poses significant challenges. The relational setting benefits from various constraints, including schema-based constraints such as data types, referential integrity constraints, key constraints, and functional dependency constraints, among others. These constraints serve to structure and regulate the problem space. However, in the context of information extraction, such constraints are generally not present: there are usually no inherent limitations on the content of the source documents.

\subsection{Rule-based  versus LLM-based Extraction}\label{subsubsec:whyrulebase}  


When using extractors as document cleaning tools, it is essential to trace updates back to the original documents and predict how changes in the documents will affect the extracted relations. We meet these two requirements through deterministic and computable extractors. While we can ensure these properties with rule-based extractors, it remains unclear how to enforce such guarantees in extractors based on pre-trained language models (PLMs), including large language models (LLMs):

\begin{description}
    
\item [Computable Extractor:] Being a strict extractor implies that the extracted items must occur in the input text, and computability requires that the extraction process is capable of generating the necessary provenance. Because, not all instances of a term or phrase in the source document may correspond to those that are extracted, we require a mechanism for identifying the corresponding positions, i.e., fine-grained data lineage. Extractors written in certain rule-based extraction languages, such as AQL (used in SystemT) and JAPE (used in GATE),  are inherently computable, i.e., the lineage of extracted items are available as a by-product of the extraction process. However, extractors expressed as PLMs do not come with these inherent capabilities. PLMs are  complex and   perceived as black-box solutions. As a result, a significant body of research is dedicated to inventing novel techniques to explain how PLMs operate____. The fine-grained lineage of extracted items can be considered a \textit{local explanation} mechanism,\footnote{For a comprehensive overview of explainability,  refer to____.} which aims to provide insight into how a model responds to a specific input instance. Several techniques belong to this category, among which explanations based on \textit{attribution} might offer a viable mechanism for pinpointing  corresponding positions in the input. An attribution-based explainer assigns a relevancy score to each input word, highlighting its contribution to generating the output (extracted items, in the extraction case). In our work,  computability is treated as a non-probabilistic property, whereas lineage based on attribution is inherently probabilistic____, so current PLMs do not meet our requirement for computability.

\item [Deterministic Extractor:] The generated outputs in PLMs are the outcomes of a combination of multiple stochastic and/or heuristic steps. Consequently, running a PLM multiple times for the same input can yield different responses____, which characterizes LLMs as non-deterministic extractors. Again they do not meet the requirements required for extracting updatable views.  

\end{description}





\subsection{Fine-grained Data Lineage} 
Data lineage, or provenance, has been defined and formalized for structured and semi-structured data____. Given a value that is the outcome  of executing a well-defined query over some data sources, often relational tables, provenance determines three aspects related to the value: data points in the source that contribute to form the value, the way that data points collaborate to produce the value, and the exact location(s) in the data source from which the value originates. The last aspect is similar to the notion of lineage that we use in this work, i.e., we require the extractor to provide the positions in a document from which a value is extracted.

Provenance-based techniques have also been applied to information extraction problems. Roy et al.____ propose a provenance-based technique to improve the quality of extraction by refining the dictionaries that are used in a rule-based extraction system. A set of entries from the dictionaries that have been involved in generating the output are analyzed to determine which should be removed to improve the extractor's performance most.  In other work, Liu et al.____ use provenance techniques to determine the most effective rule refinements, i.e., those that result in removing undesirable tuples and keeping correct ones.
Chai et al.____ examine the provenance of a multi-stage extraction program that can include relational operators on intermediate tables. Users' feedback is expressed as updates over tuples that appear at any stage of the extraction process, and these updates are translated into modifications of the corresponding extraction program.    

\subsection{Static Analysis of Programs Using Regular Languages}
 We use an extended form of finite-state automata to statically analyse an extraction program and the update mechanism. Similar static analyses of regular expressions or deterministic finite automata   have been used in diverse areas, including  access control, feature interactions, and vulnerability detection of programs. For example, Murata et al.____ propose an automaton-based access control mechanism for XML database systems. Regular expressions are derived  from given queries, access-control policies, and  schemas. Based on the characteristics of the derived automata, element/attribute level access requests  by  queries are determined to be either  granted, denied, or statically indeterminate, independently of any actual input XML documents. An event-based framework is introduced by Kin et al.____ for developing and maintaining new gestures that can be used in multi-touch environments. Using that framework, application developers express each gesture as a regular expression over some predefined touch events. Regular expressions associated with  gestures are then statically analyzed to identify possible conflicts between various gestures. Yu et al.____ present a method for detecting security vulnerabilities in programs that use string manipulation operators such as \emph{concatenation} and \emph{replacement}. In essence, their approach involves constructing DFAs to represent the program's data dependency graph. Subsequently, they perform static analysis on the provided attack pattern, expressed as a DFA, and the graph's DFAs to detect potential vulnerabilities.
 Dynamiclly generated SQL queries can enhance the flexibility of programs, written in  JAVA or other languages,  by allowing them to adapt to changing conditions and requirements without the need to write multiple static queries. However, the host compiler, i.e., JAVA compiler,  does not test the generated query strings for possible errors such as type errors.

 To this end, Wassermann et al.____ propose a static analyzer to verify the correctness of dynamically constructed SQL queries embedded in programs. Their approach involves creating a DFA representation of the generated query strings and   performing static analysis on the  DFA.