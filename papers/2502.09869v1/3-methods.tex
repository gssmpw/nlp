\section{Methods}
To answer the research questions, we conducted a semi-structured interview study for 34 users who had used personalized recommendation platforms (i.e., Xiaohongshu, Douyin, Kuaishou\footnote{Kuaishou is a content and social platform known for its short video sharing and emphasis on authentic expression. It more focuses on community engagement and live-streaming interactions.}, and BiliBili Shorts\footnote{Bilibili is a Chinese video streaming platform where users can share and interact through video uploads and comments. Its mobile application offers a short video section similar to YouTube Shorts, where users can create and share brief, engaging videos, but with a stronger emphasis on anime and gaming due to its community nature.}). We used both inductive and deductive approaches to analyze the data. This study was approved by the Institutional Review Board of Syracuse University and conducted in compliance with ethical guidelines of the respective institutions of all authors.  

\subsection{Participant Recruitment}
We targeted active users who had used personalized recommendation platforms. We distributed pre-screening surveys through social media platforms, online forums, and personal networks. The eligibility criteria included (1) being 18 years or older and (2) having at least six months of experience using the personalized recommendation platforms. We asked participants to self-identify the personalized recommendation platforms they commonly used. The pre-screening surveys helped select a diverse group of participants, ensuring variation in gender, age, educational backgrounds, and daily usage patterns. Additionally, we employed snowball sampling, asking initial participants to refer other users who met the study's criteria. This approach was effective in reaching individuals with varying levels of engagement, from lurkers who primarily consumed content to heavy users who actively posted and interacted with content.

We recruited 34 participants in total. The majority of the participants were aged 18-25 ($n = 25$), with a higher representation of females ($n = 19$). Participants reported using multiple personalized recommendation platforms. Douyin ($n = 29$, 85\% of participants) and Xiaohongshu ($n = 24$, 71\% of participants) were the most commonly used; other platforms used included Bilibili Shorts ($n = 10$), Kuaishou ($n = 7$), and TikTok ($n = 1$). All these platforms provide personalized recommendation content including short videos, images, and texts. The primary interfaces of Xiaohongshu and Douyin are shown in \autoref{fig:interface}. Interfaces of Kuaishou (shown in \autoref{fig:ksmain} \& \autoref{fig:ksvideo} in \autoref{appendix:interface}) are similar to Xiaohongshu, and interfaces of Bilibili Shorts (shown in \autoref{fig:bili} in \autoref{appendix:interface}) are akin to Douyin. The duration of usage for their most frequently used platform ranged from less than a year to over four years. According to reports, the majority of users on these platforms are younger generation under 35~\cite{flow2023demographic,marketingtochina2024douyin,kuaishou2022user}, and over 70\% of Xiaohongshu's users are female ~\cite{flow2023demographic}. Our sample of participants exhibits an age and gender distribution that aligns with the general user demographics of these platforms. More participants information are presented in \autoref{tab:participants} in \autoref{appendix:demographics}.

\subsection{Interview Procedure}
We conducted semi-structured interviews from December 2022 to April 2023. Each interview lasted between 40 to 60 minutes. The interviews were conducted in person or via video conferencing platforms to accommodate participants' schedules and geographic locations. All interviews were conducted in Mandarin Chinese. Participants received a compensation of 25 Chinese Yuan. Participants were informed that they could withdraw from the study at any time without penalty.

The interview protocol was designed to explore participants' engagement with personalized recommendation platforms. First, we asked the participants about their platform usage, including the platforms they were using, the content they were interested in, and the interactions with content and platforms (e.g., browsing, posting, liking, and searching). Then, we asked their understanding of and attitudes toward personalized recommendation and platformsâ€™ algorithms, as well as perceived impacts of platform use. Particularly, we delved into how participants responded to the algorithms, such as their strategies for managing content exposure and content preferences and avoiding undesirable content. We wrapped up the interviews by asking their overall suggestions to the design of personalized recommendation platforms.

Participants were informed about the study's goal and procedure before the interviews began. Each interview was audio-recorded with the participants' consent, and detailed notes were taken to capture key points. The recordings were transcribed verbatim and anonymized for analysis. 

\subsection{Data Analysis}
We conducted a codebook thematic analysis of the interview data~\cite{braun_thematic_2019,emeline_brule_thematic_2020}. We first conducted inductive coding, allowing themes to emerge from the data. We then incorporated deductive analysis by integrating feedback concepts from existing literature to further refine and apply the codes. Finally, we conducted code co-occurrence analysis~\cite{namey2008data} to explore the relationship between user feedback behaviors and purposes for interacting with platforms.

The data analysis was conducted locally on MAXQDA, with the codebook shared among researchers for discussion. All interviews were analyzed in Mandarin Chinese to retain the original nuances and meanings during interpretation. Selected quotes were then translated into English for presentation in this paper. 

In the inductive coding phase, our researchers first read through all transcripts to familiarize themselves with the data, and then each of them independently conducted an open coding for a different portion of the transcripts, allowing themes to emerge through constant comparison and memo-ing~\cite{miles1994qualitative}. Throughout this process, the researchers held regular discussions to compare their codes and refine the open coding scheme. They synthesized the codes into categories and composed an initial codebook. The categories include: user behaviors, interaction purposes, perceptions and attitudes, folk theories, and challenges of interacting with algorithms.

Transitioning to the deductive phase, we compared the coding results with existing literature. We identified that the ways users interacted with the algorithms could be interpreted into user providing feedback to recommendation systems. Based on existing literature, recommendation system feedback is usually categorized into explicit and implicit feedback~\cite{jannach2018recommending,jawaheer2014modeling,kelly2003implicit}. Explicit feedback requires users additional input beyond their normal behavior, such as rating and answering questions about their interests, while implicit feedback is derived unobtrusively from users' natural interactions with the system, such as viewing, selecting, and forwarding~\cite{kelly2003implicit}. We observed that, in our study, participants intentionally leveraged implicit feedback mechanisms (e.g., clicking a post) to provide feedback to their personalized recommendation. While some of the strategic behaviors to shape recommendation feeds have been documented in prior literature~\cite{Cen_Ilyas_Allen_Li_Madry_2024}, limited research has connected users' perceptions with system feedback mechanisms. 

By comparing to the literature, we refined the codebook. First, we narrowed down to focus on two code categories: \textit{interaction purposes} and \textit{user behaviors}. We referred to the established categories of \textit{explicit feedback} and \textit{implicit feedback} in ~\cite{jannach2018recommending} to categorize user interaction with systems. Then, we found that within implicit feedback behaviors, users consciously and proactively shape the recommendation feeds, which contradicts the original definition of implicit feedback. Therefore, we divided implicit feedback into \textit{intentional implicit feedback} and \textit{unintentional implicit feedback} to differentiate whether users' intention is present or absent during implicit feedback behaviors. Overall, we categorized users' feedback behaviors into three types:
\begin{itemize}
    \item \textit{Explicit feedback}: users' direct input to express their preferences or interests.
    
    \item \textit{Intentional implicit feedback}: behaviors that users consciously perform to influence recommendation content, with their knowledge that these actions might be interpreted by the platforms to infer their interests.
    
    \item \textit{Unintentional implicit feedback}: users' natural interactions with the platforms without any deliberate intention to influence recommendation content.
\end{itemize}

%Then, within each feedback categories, we refined the specific codes of user behaviors by referring to existing literature ~\cite{jannach2018recommending, Cen_Ilyas_Allen_Li_Madry_2024}, while retaining the codes of behaviors (e.g., initiate a new search, using or collecting hashtags, and ignoring or swiping past a post) that were identified in inductive coding and specific to our study context.

To further understand and interpret feedback behaviors, we mapped three key properties to each identified behavior: features (i.e., specific platform features such as the ``like'' button or search box that afford feedback behaviors), polarity ~\cite{jawaheer2014modeling,hu2008collaborative} (i.e., ``positive'' or ``negative'' feedback), and minimum scope ~\cite{oard2001modeling} (i.e., the minimum level at which feedback is applied: ``segment,'' ``object,'' or ``class''). We carefully analyzed all the mentioned platforms to identify the corresponding feature(s) for each behavior and assigned polarity and scope based on interpretation of participant transcripts and platform functionalities.

Then, two researchers completed the deductive coding based on the refined codebook. For the first eight transcripts, they coded them independently and reviewed the coding together to any discrepancies and refined the coding guidelines accordingly. This iterative process helped to ensure consistency in interpretations. Then, they independently coded the remaining transcripts, 13 transcripts for each. During the process, the two researchers continued to share summary memos and addressed any ambiguities during weekly discussions with the research team.

We identified potential correlation patterns between feedback behavior types and purposes for interacting with platforms (i.e., content consumption, directed information seeking, content creation and promotion, and feed customization) as well as specific feed customization sub-purposes. To explore their relationships, we used code co-occurrence analysis. Specifically, we identified instances where a user behavior code and a user purpose code appear together within the same interview segment. To avoid duplication, we counted each behavior-purpose co-occurrence only once per participant using MAXQDA. For example, if a behavior-purpose co-occurrence was mentioned multiple times in a single participantâ€™s interview, it was treated as a single co-occurrence instance for that participant.  %By focusing on the co-occurrence of codes rather than individual cases, we ensured that our analysis reflected the behavior-purpose patterns across participants, rather than the frequency of mentions within one interview. 
We then aggregated these co-occurrence instances within each of the three feedback types, i.e., explicit feedback, intentional implicit feedback, and unintentional implicit feedback. 
We also conducted co-occurrence analysis between the sub-purposes (i.e., improving recommendation relevance, increasing content diversity, reducing inappropriate content, and protecting privacy) and user feedback behaviors. Notably, we only counted instances within intentional implicit feedback and explicit feedback for the sub-purposes, because participants did not specify unintentional implicit feedback behaviors corresponding to the four sub-purposes. 
