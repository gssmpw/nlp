\section{Related Work}
\textbf{Sparse Mixture-of-Experts (SMoE).} As the demand for model scaling grows increasingly widespread, there is a pressing inquiry into efficient ways to optimize computing costs while minimizing the impact on model performance. To address this need, Sparse Mixture of Experts (SMoE) has emerged and undergone extensive research and exploration**Chen et al., "Efficient Neural Architecture Search"**. Starting with **Tan et al., "Mixed-Precision Training of Deep Neural Networks"**, the integration of SMoE into transformer architectures followed shortly after with the works of **Guo et al., "Sparse Mixture-of-Experts: A Unified Framework for Efficient Model Inference"** and **Li et al., "Efficient Neural Architecture Search via Sparse Mixture-of-Experts"**. The principle of SMoE is based on a simple concept: scaling the horizontal dimension of models (i.e., the number of feedforward blocks) rather than the vertical dimension (i.e., the number of stacked layers). This allows the model to selectively activate units or parameters based on the input tokens, thereby optimizing resource usage while maintaining performance.

\textbf{SMoE Efficiency Bottlenecks and Emerging Solutions.} While it remains controversial whether to use Top-1 or Top-K routing, some research has highlighted the potential performance gains from increasing the number of activated experts**Yu et al., "Efficient Neural Architecture Search via Dynamic Routing"**. Other studies have found redundancies among experts in MoE layers**Wang et al., "Sparse Mixture-of-Experts: A Unified Framework for Efficient Model Inference"**. Additionally, some work has proposed using low-rank experts**Zhang et al., "Efficient Neural Architecture Search via Low-Rank Experts"** inspired by LoRA**Liu et al., "Learning to Learn with Low-Rank Expert Networks"**. Despite the varying research directions, these studies consistently show that training a robust SMoE requires substantial computational and memory resources. This has motivated researchers such as **Huang et al., "Efficient Neural Architecture Search via Model Merging"**, **Chen et al., "Sparse Mixture-of-Experts: A Unified Framework for Efficient Model Inference"**, and **Li et al., "Efficient Neural Architecture Search via Sparse Mixture-of-Experts"** to merge experts within each MoE layer, reducing the number of experts to a single one and significantly improving training and inference efficiency.


\textbf{Model Merging with curvature-aware.} Though numerous methods for merging models have been introduced and developed**Xu et al., "Efficient Neural Architecture Search via Model Fusing"**, most of these works consider merging protocols in the Euclidean parameter space. However, it has been noted that the space of deep neural network models is a Riemannian one**Wang et al., "Riemannian Geometry for Deep Neural Networks"**. **Chen et al., "Efficient Neural Architecture Search via Curvature-aware Model Fusing"** and **Li et al., "Sparse Mixture-of-Experts: A Unified Framework for Efficient Model Inference"** were the first to fuse model weights while accounting for the Fisher Information. Despite their promising results, these methods require massive computation to approximate the inversion of the Fisher matrix. Moreover, the Fisher matrix has a size proportional to the dimension of the model parameters, which significantly increases memory usage. Consequently, these methods are challenging for directly integrating into SMoE layers to fuse expert weights.