[
  {
    "index": 0,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "key": "lepikhin2021gshard",
        "author": "Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen",
        "title": "{\\{}GS{\\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      },
      {
        "key": "fedus2022switch",
        "author": "William Fedus and Barret Zoph and Noam Shazeer",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "lepikhin2021gshard",
        "author": "Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen",
        "title": "{\\{}GS{\\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "William Fedus and Barret Zoph and Noam Shazeer",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "shazeer2017outrageously",
        "author": "Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "key": "chen2023sparse",
        "author": "Tianlong Chen and Zhenyu Zhang and AJAY KUMAR JAISWAL and Shiwei Liu and Zhangyang Wang",
        "title": "Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2024merge",
        "author": "Pingzhi Li and Zhenyu Zhang and Prateek Yadav and Yi-Lin Sung and Yu Cheng and Mohit Bansal and Tianlong Chen",
        "title": "Merge, Then Compress: Demystify Efficient {SM}oE with Hints from Its Routing Policy"
      },
      {
        "key": "lu2024expertsequalefficientexpert",
        "author": "Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li",
        "title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wu2024mixture",
        "author": "Xun Wu and Shaohan Huang and Furu Wei",
        "title": "Mixture of Lo{RA} Experts"
      },
      {
        "key": "liu2024mixturelowrankexpertstransferable",
        "author": "Zihan Liu and Hanyi Wang and Yaoyu Kang and Shilin Wang",
        "title": "Mixture of Low-rank Experts for Transferable AI-Generated Image Detection"
      },
      {
        "key": "Wu_2024_CVPR",
        "author": "Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu",
        "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hu2022lora",
        "author": "Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "Lo{RA}: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li2024merge",
        "author": "Pingzhi Li and Zhenyu Zhang and Prateek Yadav and Yi-Lin Sung and Yu Cheng and Mohit Bansal and Tianlong Chen",
        "title": "Merge, Then Compress: Demystify Efficient {SM}oE with Hints from Its Routing Policy"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "he-etal-2023-merging",
        "author": "He, Shwai  and\nFan, Run-Ze  and\nDing, Liang  and\nShen, Li  and\nZhou, Tianyi  and\nTao, Dacheng",
        "title": "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhong2024lory",
        "author": "Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis",
        "title": "Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "yadav2023ties-merging",
        "author": "Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal",
        "title": "Resolving Interference When Merging Models"
      },
      {
        "key": "cai2023robust",
        "author": "Cai, Ruisi and Zhang, Zhenyu and Wang, Zhangyang",
        "title": "Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights?"
      },
      {
        "key": "ilharco2022patching",
        "author": "Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig",
        "title": "Patching open-vocabulary models by interpolating weights"
      },
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      },
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      },
      {
        "key": "don2022cold",
        "author": "Don-Yehiya, Shachar and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem",
        "title": "Cold fusion: Collaborative descent for distributed multitask finetuning"
      },
      {
        "key": "rame2023model",
        "author": "Rame, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, L{\\'e}on and Lopez-Paz, David",
        "title": "Model ratatouille: Recycling diverse models for out-of-distribution generalization"
      },
      {
        "key": "Tuan2024Hypertrans",
        "author": "Tran Anh Tuan and Nguyen Viet Dung and Tran Ngoc Thang",
        "title": "A Hyper-Transformer model for Controllable Pareto Front Learning with Split Feasibility Constraints"
      },
      {
        "key": "lu2024twinmerging",
        "author": "Zhenyi Lu and Chenghao Fan and Wei Wei and Xiaoye Qu and Dangyang Chen and Yu Cheng",
        "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "amari1998natgrad",
        "author": "Amari, Shun-ichi",
        "title": "Natural Gradient Works Efficiently in Learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      }
    ]
  }
]