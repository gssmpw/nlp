
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{ color, colortbl}
% \usepackage[table]{xcolor}
\usepackage{multicol, multirow}
\usepackage{subcaption}
\newcommand{\revise}[1]{{#1}}
\usepackage{hyperref}
\definecolor{myblue}{rgb}{0.1,0.2,0.75}
\definecolor{lightblue}{HTML}{A6E5FF}
% \setlength{\intextsep}{5pt}
\hypersetup{ %
pdftitle={},
pdfauthor={},
pdfsubject={},
pdfkeywords={},
pdfborder=0 0 0,
pdfpagemode=UseNone,
colorlinks=true,
linkcolor=myblue,
citecolor=myblue,
filecolor=myblue,
urlcolor=myblue,    
pdfview=FitH}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{pygreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{pyred}{rgb}{0.7, 0.0, 0.0}
\definecolor{pyblue}{rgb}{0.0, 0.0, 0.7}
\definecolor{pygray}{rgb}{0.5, 0.5, 0.5}
\definecolor{pydarkgray}{rgb}{0.3, 0.3, 0.3}
% \definecolor{blue}{rgb}{0.9,0.9,0.9}
\newcommand*{\B}[1]{\mathbf{#1}}
\newcommand*{\C}[1]{\mathcal{#1}}
\newcommand*{\T}[1]{\textrm{#1}}
\newcommand{\methodshort}[1]{\textsc{Ties-Merging}}
\newcommand{\method}[1]{\textsc{TrIm, Elect Sign \& Merge}}
\newcommand{\hlight}[1]{\textcolor{magenta}{#1}}
% \usepackage{hyperref}
\definecolor{color1}{HTML}{006EB8}
\definecolor{color2}{HTML}{009B55}
\newcommand{\myblue}[1]{\textcolor{azureblue}{~#1}}
\newcommand{\myorange}[1]{\textcolor{orange}{~#1}}
% \hypersetup{colorlinks=true,urlcolor=color2,linkcolor=color2,citecolor=color2}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{caption}
\usepackage{tcolorbox}
\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{listings}
\lstset{
  frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{pygray},
  keywordstyle=\color{pyblue},
  commentstyle=\color{pygreen},
  stringstyle=\color{pyred},
  rulecolor=\color{pygray},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage[]{graphicx}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{amsmath}
\usepackage{algpseudocode}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{url}
% Using this package for saving 1/2 page
\usepackage[subtle]{savetrees}
\usepackage[normalem]{ulem}
\newcommand\tsout{\bgroup\markoverwith{\textcolor{red}{\rule[0.5ex]{2pt}{0.4pt}}}\ULon}

\newcommand{\tanm}[1]{\textcolor{red}{(Tan: #1)}}

\usepackage{titletoc}
% \usepackage[subtle]{savetrees} 

\newcommand\DoToC{%
  \startcontents
  \printcontents{}{1}{\textbf{Table of Contents}\vskip3pt\hrule\vskip5pt}
  \vskip3pt\hrule\vskip5pt
}


\title{CAMEx: Curvature-aware Merging of Experts}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Dung V. Nguyen\textsuperscript{1}\thanks{Co-first authors} \quad 
  Minh H. Nguyen\textsuperscript{1}\footnotemark[1] \quad
  Luc Q. Nguyen\textsuperscript{2}\footnotemark[1] \quad
  Rachel S.Y. Teo\textsuperscript{3} \\
  \textbf{Tan M. Nguyen\textsuperscript{3}\thanks{Co-last authors} \quad
  Linh Duy Tran\textsuperscript{2}\footnotemark[2]}\\
  \textsuperscript{1}Faculty of Mathematics and Informatics, Hanoi University of Science and Technology \\
  \textsuperscript{2}Viettel AI, Viettel Group \\
  \textsuperscript{3}Department of Mathematics, National University of Singapore \\
  \texttt{\{dung.nv232215M, minh.nh232331M\}@sis.hust.edu.vn} \\
  \texttt{\{lucnq1,linhtd15\}@viettel.com.vn} \\
  \texttt{rachel.tsy@u.nus.edu,tanmn@nus.edu.sg}
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the model's generalization ability, especially during the pre-training phase, where the parameter manifold might exhibit more complex curvature. Curvature-aware merging methods typically require additional information and computational resources to approximate the Fisher Information Matrix, adding memory overhead. In this paper, we introduce CAMEx (\textbf{C}urvature-\textbf{A}ware \textbf{M}erging of \textbf{Ex}perts), a novel expert merging protocol that incorporates natural gradients to account for the non-Euclidean curvature of the parameter manifold. By leveraging natural gradients, CAMEx adapts more effectively to the structure of the parameter space, improving alignment between model updates and the manifold's geometry. This approach enhances both pre-training and fine-tuning, resulting in better optimization trajectories and improved generalization without the substantial memory overhead typically associated with curvature-aware methods. Our contributions are threefold: (1) CAMEx significantly outperforms traditional Euclidean-based expert merging techniques across various natural language processing tasks, leading to enhanced performance during pre-training and fine-tuning; (2) we introduce a dynamic merging architecture that optimizes resource utilization, achieving high performance while reducing computational costs, facilitating efficient scaling of large language models; and (3) we provide both theoretical and empirical evidence to demonstrate the efficiency of our proposed method. The code is publicly available at: \url{https://github.com/kpup1710/CAMEx}.
\end{abstract}


\section{Introduction}

Sparse Mixture of Experts (SMoE)~\citep{Jacobs1991,shazeer2017outrageously} is currently a core component for constructing foundation and large language models (LLMs), whose parameters count can rise up to billions and trillions~\citep{devlin2019bert,yang2019xlnet,liu2019roberta,raffel2020exploring,nguyen2022fourierformer,fedus2022switch,wei2022chain,teo2025molex}. Nevertheless,  \citep{Hoffmann2024scaling, kaplan2020scalinglawsneurallanguage} recognized a scaling law that underpins the LLM's evolution, which is larger models require exponentially more computational resources and data to continue improving, and without sufficient scaling in all dimensions, performance gains may plateau. Thus, identifying and implementing efficient methodologies for the sustainable scaling of LLMs is imperative. SMoE addresses this challenge by sparsely activating parameters of large models, which can boost model performance with only minor losses in computational efficiency. The methodology is integrated chiefly into feedforward layers of transformers, processing tokens by selectively activating a small number of experts and hence trimming down the computing memory and FLOPS~\citep{fedus2022switch,lepikhin2021gshard}. 

Since its debut in~\citep{shazeer2017outrageously}, SMoE has gone through numerous explorations and advancements in routing mechanism development and expert architecture design.~\citep{dai2022stablemoe} proposes a two-phase training strategy for stabilizing the gate function so that the expert's selection of one token does not fluctuate between different inference times.~\citep{zhou2022mixtureofexperts} changes the perspective of the router to experts with experts choice routing, ensuring a balancing load between experts.~\citep{teo2024momentumsmoe} incorporates heavy-ball momentum in SMoE to enhance the modelâ€™s stability and robustness.~\citep{chi2022representation} and ~\citep{chen2023sparse} address the concern of representation collapse in SMoE by proposing cosine scoring and a fixed random initialized router, respectively. Some other works view the routing mechanism as a reinforcement learning, clustering,  or optimal transport problem~\citep{nielsen2025tight}. In terms of expert design orientation,~\citep{rajbhandari2022deepspeedmoe} and~\citep{dai2024deepseekmoe} introduce the concept of shared experts wherein each token is processed by a fixed expert and another selected through gating, achieving two experts engagement per layer without increasing the communication cost beyond that of top-1 gating.~\citep{muqeeth2023soft} proposes to merge experts by taking the weighted mean of the expert's parameters with respect to router scores. This methodology is then extended in~\citep{he-etal-2023-merging}, ~\citep{zhong2024lory}, and ~\citep{li2024merge} for causal language modeling pretraining and fine-tuning tasks.

Among existing rigorous research on SMoE, our work focuses on the experts merging lines of research. Specifically, we systemically integrate natural gradient into task-specific merging protocol for SMoE. To the best of our knowledge, the current merging protocol applied for SMoE still deems the parameter space of the expert's parameters as Euclidean ones. Nevertheless, it has been shown that the space of neural network parameters brings the characteristic of the Riemannian manifold \citep{amari1998natgrad}. Therefore, it is natural for us to make an effort in such a direction for merging experts. Although some existing works on merging models have already leveraged the Fisher Information Matrix \citep{matena2022merging, jin2023dataless}, we find that they require large computational space and complicated steps to perform well. In contrast, our merging protocol is simple and straightforward to implement while still taking into account the curvature of the parameters manifold. We discover the superior performance of curvature-aware merging in our method compared to the regular merging procedure applied to SMoE. Our main contributions are three-fold:
\begin{enumerate}[leftmargin=25pt]
    \item We introduce a novel rapid and efficient merging technique named Curvature-
Aware Merging of Experts (CAMEx) for SMoE that includes information about the curvature of the expert's parameters manifold.
    \item We propose a new architecture based on CAMEx, which dynamicalizes the merging protocol along with parameters reduction. Our empirical experiments prove the dominant performance of this architecture on pre-training tasks.  %\tanm{is it CAMEx or something else?}.
    \item We theoretically prove that our CAMEx obtains better alignment between experts and the training task domain.
    % We provide a thorough theoretical analysis for our proposed curvature-aware merging and conclude that our method can benefit the merged experts in convergence speed and considering the alignment between experts and the training task domain.
\end{enumerate}

% We perform extensive experiments to prove our proposed methodology's effectiveness empirically.

We empirically demonstrate that 1) our proposed merging method can add in rapidness of convergence speed for pre-training and 2) when combined with other merging protocols, it can boost the model's performance on a variety of practical tasks, including language modeling, text classification, question answering, and image classification.

\section{Curvature-aware Merging of Experts}
\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{model_arc.pdf}
\end{center}
\caption{Overview of CAMEx for a causal language modeling SMoE. The experts are merged through the router scores and the curvature-matrix $\B{M}$. During the merging protocol, we can generate the masks for the domain-vectors, denoted as $\gamma_i$, such as Ties or Dare. We follow the causal segmenting pipeline from \citep{zhong2024lory} to achieve both memory efficiency and causal information constraints. Note that \textit{stop gradient} operator is applied for the first segment router scores.}
% \vspace{-5mm}
\end{figure}
This section aims to give an overview of model merging methods and their integration into SMoE architecture. We then introduce our curvature-aware merging protocol stamping from the natural gradient. Finally, we perform a theoretical analysis to support our proposal.

\subsection{Background: Expert Merging in Sparse Mixture of Experts}
It is convenient to recall the concept of SMoE and a few well-known experts merging methods for SMoE. From this point to the rest of our paper, let us use the notations summarized in Table~\ref{tab:notation}.
\begin{table}[t!]
\caption{Notations and Definitions.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ll|ll}
\toprule
\textbf{Symbol} & \textbf{Description} & \textbf{Symbol} & \textbf{Description} \\ 
\midrule
$T$ & Number of tokens & $k$ & Number of selected experts\\
$N$ & Total number of experts & $\mathbf{E}_i \in \mathbb{R}^{d \times h}$ & Weights for the $i$th expert \\
$\mathbf{h} \in \mathbb{R}^{T \times d}$ & Input tokens or hidden states & $\mathbf{G}(\cdot, \cdot) \in \mathbb{R}^{T \times N}$ & Gating function output\\ 
$\mathcal{S}_t$ & Set of top-k experts for token $\mathbf{h}_t$ & $\alpha \in [0,1] $ & Rescalling factor \\ \bottomrule
\end{tabular}}
\label{tab:notation}
\end{table}

{\bf Sparse Mixture of Experts.} A SMoE layer processes the tokens series as follows:
\begin{equation}
    \begin{cases}
    \mathbf{y}_t = \sum_{i \in \mathcal{S}_t} \mathbf{G}(t, i) \cdot \mathbf{E}_i \mathbf{h}_t \\
    \mathbf{G}(t,\cdot) = \text{softmax}(\mathbf{W}_g \mathbf{h}_t)\\
    \mathcal{S}_t = \text{top-k}(\mathbf{G}(t, \cdot))
\end{cases}
\end{equation}
{\bf SMEAR.} \citep{muqeeth2023soft} introduces the ensemble of expert parameters through weighted average computation with the factors being the router scores. 

{\bf Task-specific merging in SMoE.} Our work will follow the scheme of task-specific merging \citep{ilharco2023editing}. In such a setting, we assume the existence of $N$ pre-trained models parameterized by $\theta_i$, each of which was pre-trained on a different task. We then define the task-vector for each pre-trained model through the merged model $\theta_m$ as $\tau_i = \theta_i - \theta_m$. The merging protocol will be performed by Eqn.~\ref{merging}. Under the context of SMoE, each expert learns to handle a particular subset of the input space or specializes in a specific type of feature or pattern \citep{Jacobs1991,dai2024deepseekmoe}. We believe it is more suitable to reference this technique as \textbf{domain-specific merging}. We, therefore, will rename the tensors $\tau_i = \B{E}_i - \B{E}_m$ as \textit{domain-vector}. Additionally, to take the router information into account, we will define the formulation for domain-specific merging in a SMoE layer as follows:
\begin{equation}
\label{ep:domain-specific}
    \B{\hat{E}}_m = \B{E}_m + \alpha\sum_{i = 1}^{N-1} s_i \tau_i
\end{equation}
where $s_i$ denotes the score of the router for the $i$th expert. We want to note that with $0 < \alpha < 1$, domain-specific merging aligns with soft merging.

\subsection{Background: Other Model Merging Methods}
In this section, we discuss other recent and widely-adopted model merging methods outside the context of SMoE that we will combine with our curvature-aware merging method in our experiments in Section~\ref{sec:experiment}.

{\bf TIES merging.} \citep{yadav2023ties-merging} improves upon task arithmetic by removing interference between the task vectors. Specifically, TIES zeros out entries in a given task vector with low magnitude and resolves sign conflicts across different task vectors. 

{\bf DARE merging.} Different from TIES, DARE merging randomly zeros out the neurons like a Dropout layer~\citep{yu2024language}.

{\bf Fisher merging.} Existing work on Fisher merging suffers from computational complexity since computing and inverting the Fisher Information Matrix, especially for large neural networks, is often intractable. Even when using approximations like diagonal or block-diagonal Fisher matrices, these methods can still be computationally expensive and challenging to apply at scale. Furthermore, the accuracy of Fisher approximations, such as diagonal or block-diagonal, can be problematic~\citep{matena2022merging}.
\subsection{Gradient interpretation of models merging}
We want to emphasize the alignments between the paradigm of gradient descent and model merging. For this, we denote $\theta \in \mathbb{R}^N$, $\mathcal{L}(\theta)$, and $\eta$ as the model's parameters, the empirical loss function, and the learning rate, respectively. During the training process of a deep learning model, the parameters are updated following the gradient descent formula:
\begin{equation}
    \label{gd} \tag{GD}
    \theta_{n+1} = \theta_n + \eta (-\nabla \mathcal{L}(\theta_n))
\end{equation}
In the aspect of deep models merging, we also have an update rule in a similar manner, which is
\begin{equation}
    \label{merging} \tag{Merg}
    \hat{\theta}^m = \theta^m + \alpha \sum_{i=1}^n \underbrace{(\theta^i - \theta^m)}_{\substack{\text{gradient-like} \\ \text{ update direction}}}
\end{equation}
where $\theta^m$ denotes the merged model's parameters, and \(\theta^i\) denotes the parameters of the $i$th expert. Here, we interpret \(\theta^i\) as the optimal parameters of the model for a specific task or domain, and then the update rule gives us a direction toward optimizing for all tasks. 

However, it has been pointed out by~\citep{amari1998natgrad} that the parameter space structure of deep learning models has Riemannian characteristics. Therefore, a more \textit{natural} gradient updating scheme was proposed,
\begin{equation}
\label{natgrad}\tag{NGD}
    \theta_{n+1} = \theta_n + \eta \underbrace{G(\theta_n) (-\nabla \mathcal{L}(\theta_n))}_{\text{natural gradient}}
\end{equation}
In this formula, $G(\theta_n) \in \mathbb{R}^{N \times N}$ denotes the \textit{Riemannian metric tensor}~\citep{amari1998natgrad, amari1998whynat}, which characterizes the intrinsic curvature of a particular manifold in $N$-dimensional space~\citep{Martens2020} or sometimes, the inversed Fisher Information Matrix. The same ideology was introduced for merging large language models in Fisher merging~\citep{matena2022merging} and Regmean~\citep{jin2023dataless}. However, both methods suffer from the bottleneck in the computation cost of approximating the Fisher Information. Moreover, these methods are challenging to apply in sparse layers of SMoE since they would introduce huge latency, FLOPS, and memory for computing and storing matrices whose number of entries is proportional to a number of expert parameters. 

\subsection{Experts merging with curvature-aware}
In this section, for the sake of conciseness, we focus on the language modeling task; a similar methodology can be applied to other tasks, such as classification. We introduce an efficient way to merge experts within SMoE layers, based on the causal segmenting approach proposed by \citep{zhong2024lory}. The goal of the causal segment routing strategy is to enhance the efficiency of expert merging operations while maintaining the autoregressive nature of language models.  More details about this algorithm can be found in Appendix \ref{sec:alg_imp} and Algorithm \ref{alg:main}. We then perform the following merging protocols:
\begin{equation}
\label{eq:ca-merge}
\tag{CA-Merg}
    \B{\hat{E}}_m^l = \B{E}_m^l + \alpha\sum_{i=1}^{N-1} \B{M}_i\cdot (s^l_i*\tau_i^l)
\end{equation}
where $\B{M}_I \in \mathbb{R}^{d_{in} d_{out} \times d_{in} d_{out}}$ denote the curvature matrix which performs matrix product with the gradient-like component. The curvature of the parameters manifold will be learned through these tensors while optimizing the empirical loss. This approach has also proven its effectiveness in meta-learning for few-shot classification \citep{park2019meta}. 
We further explore the computing efficiency of merging experts by proposing a novel dynamic merging formula
\begin{equation}
 \begin{cases}
 \label{eq:dynamic}
\tag{Dynamic-Merg}
    \B{E}_m^{l+1} &= \B{E}_m^l + \dfrac{\alpha}{N-1}\displaystyle\sum_{i=1}^{N-1} \B{M}_i \cdot \tau_i^l\\
    \B{\hat{E}}_m^{l+1} &= \B{E}_m^{l+1} + \displaystyle\alpha\sum_{i=1}^{N-1} \B{M}_i \cdot (s^{l+1}_i*\tau_i^{l+1})
\end{cases}   
\end{equation}
The architecture corresponding to this recurrent representation can be found in Figure \ref{fig:dynamic}. The architecture contains a global expert that traverses through the SMoE layers by the updating rule in Eqn.~\ref{eq:dynamic}. Not only will this allow a notable reduction in model size and GFLOPS, but it also ensures the number of experts in each SMoE is the same as in the full-expert setting, where each layer has the same number of experts. {We refer to Appendix \ref{walkthrough} for a step-by-step walkthrough of key equations in CAMEx.}
\begin{figure}[t!]
    \centering
     \includegraphics[width=0.8\linewidth]{model_arc-2.pdf}
    \caption{\textbf{Overall architecture of different SMoE layers}. The figure presents the vanilla SMoE layer on the left, the merging expert layer in the middle, and our proposed dynamic merging SMoE layer on the right. Our architecture reduces the number of parameters compared to the other two, while maintaining the same number of activated neurons per layer. Importantly, despite the dynamic merging mechanism, our architecture preserves the same number of experts at each layer as the other SMoE architectures, ensuring comparable model capacity, i.e., the number of activated parameters per layer.}
    \label{fig:dynamic}
    \vspace{-5mm}
\end{figure}
\subsection{Efficency}
{\bf Parameter efficient approximation of curvature matrix.} Storing and computing a curvature matrix requires a whopping memory and time complexity of \(O(n^4)\) and \(O(n^4)\), respectively. This is infeasible even for a simple SMoE layer, as one layer can contain many experts. To mitigate this problem, we follow \citep{Martens15} and approximate the curvature matrix using the Kronecker product. It has been proven by \citep{Hameed_Tahaei_Mosleh_Partovi_Nia_2022} that we can approximate an arbitrary matrix using a finite sum of Kronecker products. For a curvature matrix \(\B{M}_i \in \mathbb{R}^{d_{in}d_{out}\times d_{in}d_{out}} \), we present the rank-1 approximation as below:
\begin{align}
    \B{M}_i \approx \B{M}_i^{in} \otimes \B{M}_i^{out}
\end{align}
with \(\B{M}_i^{in} \in \mathbb{R}^{d_{in}\times d_{in}}\) and \(\B{M}_i^{out} \in \mathbb{R}^{d_{out}\times d_{out}}\). Still, this form of approximation is too large to compute and store during training time, so we further decompose \(\B{M}_i^{in}\) and \(\B{M}_i^{out}\) using Kronecker product because of the efficient computation using tensor algebra. This form of approximation reduces the number of parameters added and only puts negligible memory and computational overhead to the training process at the cost of additional $O(n)$ memory complexity and $O(n^{2.5})$ computational complexity. Although this might limit the representative capacity of the curvature matrix, we empirically find that the performance of our method still surpasses other merging methods. 

{\bf Efficient test-time inference with reparameterization.} We focus on the case where $\alpha = 1$. To further optimize the computation of curvature-aware merging, we embed the curvature matrices into the domain-vectors using the following reparameterization trick:
\begin{equation}
    \B{E}'_i \gets \B{E}_m + \B{M}_i \cdot \tau_i
\end{equation}
In this case, the merging formula at test time becomes:
    \[
    \B{\hat{E}}_m = \B{E}_m + \sum_{i=1}^{N-1} s_i \cdot (\B{E}'_i - \B{E}_m) = \B{E}_m + \sum_{i=1}^{N-1} \B{M}_i \cdot (s_i \cdot \tau_i)
    \]
Thus, during inference, we avoid storing the curvature matrices and recomputing their product with domain vectors, reducing the total FLOPs. This explains the computational efficiency seen in Section \ref{sec:experiment}.

\subsection{Theoretical Analysis}
\label{sec:theo}
In this section, we investigate how optimizing the curvature matrix $\B{M}$ in Eqn.~\ref{eq:ca-merge} can improve the generalization of the expert merging process for downstream tasks. We first recall the formulation for domain-specific merging
\begin{equation}
    \B{\hat{E}}_m = \B{E}_m + \alpha\sum_{j=1}^{N-1} s_j \cdot \tau_j
\end{equation}
We begin by calculating the gradient of the downstream task loss function, denoted by \(\mathcal{L}\), with respect to $\B{M}_j$ at a specific \( l \)th layer as follows:
\begin{equation}
\label{M-grad}
    \dfrac{\partial \mathcal{L}}{\partial \B{M}_j} 
    = \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}_m} \cdot \dfrac{\partial \B{\hat{E}}_m}{\partial \B{M}_j} 
    = \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}_m} \cdot \left(\alpha s_j * \tau_j \right) 
    = \alpha s_j * \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}_m} \cdot (\B{E}_j - \B{E}_m)
\end{equation}
This is the outer product of the gradients of the task loss and the domain vectors. It is important to note the connection with the Fisher Information Matrix. For a downstream task, if we define the loss function as the negative log-likelihood, such as in a supervised classification task \(\mathcal{L} (\theta) = \mathbb{E}_{(x,y) \sim p} [-\log_\theta p(y|x)]\), then the empirical Fisher Information Matrix can be defined as
\[
\mathbf{F} = \mathbb{E}_{(x,y) \sim p} [\nabla_\theta \log_\theta p(y|x) \nabla_\theta \log_\theta p(y|x)^\top]
\]

Next, we consider how the gradient of the curvature matrix can contribute to better performance. Given a time step \( t \), the standard gradient descent from an initial point \(\B{M}_j\) with learning rate $\beta$ yields the following update:
\begin{align}
\B{M}^{t+1}_{j} &= \B{M}^{t}_{j} - \beta * \dfrac{\partial \mathcal{L}}{\partial M^{t}_{j}} = \B{M}^{t}_{j} - \alpha\beta * s^{t}_j * \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}^{t}_m} \cdot (\B{E}^{t}_j - \B{E}^{t}_m)
\end{align}
We assume standard gradient descent for simplicity, but the argument extends to other advanced gradient algorithms, such as momentum~\citep{polyak1964some,sutskever2013importance,nguyen2020momentumrnn,he2020momentum,wang2022does,nguyen2022improving} and ADAM~\citep{kingma2014adam,loshchilov2018decoupled}. We then apply \(\B{M}_j\) to the merging process in Eqn.~\ref{eq:ca-merge} and get
\begin{align}
    \B{\hat{E}}_m = \underbrace{\B{E}_m + \alpha\sum_{j=1}^{N-1} s^{t+1}_j * \B{M}^{t}_j \cdot \tau^{t+1}_j}_{\text{domain-specific merging with curvature-aware}} - \alpha^2\beta \sum_{j=1}^{N-1} s^{t}_j s^{t+1}_j * \left(\tau^{t^\top}_j \cdot \tau^{t+1}_j   \right)\cdot \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}^{t}_m} \label{eq: ca-merge8}
\end{align}
The detail for the derivation can be found in Appendix \ref{sec:derviation}. We can see that the first term in Eqn.~\ref{eq: ca-merge8} is the classic domain-specific merging formula with the guidance of the learned curvature. Furthermore, the second term contains the direction from the task loss gradient and the inner product between domain-vectors from two consecutive gradient steps. If \(\B{M}_j = \B{I} \;\forall j\), this term can be seen as an auxiliary signal from task loss of the previous update step guiding the merging direction. The term \(s^{t}_j s^{t+1}_j \cdot \left(\tau^{t^\top}_j \tau^{t+1}_j   \right)\) modeling the agreement of the merging direction between updating steps: if there are conflicts between current and the previous updating direction, then this signal will be alleviated, thus dampening the harm to the merging direction of the current step; otherwise if they show strong agreement, this amplifies the impact of the updating direction toward minimizing the task loss with respect to the previous step, thus accelerate the training process while implicitly helping current merging direction with additional \textit{experience}. 

On the other hand, we can rewrite Eqn.~\ref{eq: ca-merge8} as follows:
\begin{equation}
     \B{\hat{E}}_m = \B{E}_m + \alpha\sum_{j=1}^{N-1} s^{t+1}_j * \B{M}^{t}_j \cdot \tau^{t+1}_j - \alpha^2\beta \sum_{j=1}^{N-1} s^{t}_j s^{t+1}_j * \underbrace{\left(\tau^{t^\top}_j \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}^{t}_m}  \right)}_{\text{gradient matching}} \cdot \tau^{t+1}_j 
\end{equation}
We now have the inner-product between the gradient of the task loss and the domain-vector. This can be interpreted as the matching between the update from the task loss gradient and the domain-specific direction. We then have the updated domain-specific direction for each expert whose weighting factors are calculated by the inner-product. Therefore, we are performing a soft nearest distance voting to find the experts that agree the most with the task loss and enhance the merged experts with the corresponding domain-vector.
\section{Experimental results}
\label{sec:experiment}
\begin{table*}[t!]
\caption{\textbf{Performance of T5-base variants {on the fine-tuning tasks for} GLUE}. All SMoE variants have 8 experts per layer. We follow \citep{devlin2019bert} in conducting experiments on the GLUE benchmark. Our curvature-aware methods outperform all baselines across tasks, while maintaining the same number of parameters and FLOPs as the SMoE models.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
\toprule
\textbf{Methods} & \textbf{Params} & \textbf{TFLOPs} & \textbf{SST-2} & \textbf{MRPC} & \textbf{CoLA} & \textbf{QQP} & \textbf{STSB} & \textbf{QNLI} & \textbf{RTE} & \textbf{MNLI} \\
\midrule
Vanilla & 220M & 4.65 & 93.34 & 89.70 & 58.06 & 88.76 & 89.06 & 92.34 & 74.36 & 86.36 \\
SMoE & 1.0B & 4.65 & 94.26 & 90.87 & 56.78 & 88.69 & 89.44 & 92.07 & 70.75 & 86.38 \\
\midrule
Domain-Specific & 1.0B & 4.65 & 93.57 & 90.19 & 58.07 & 88.77 & 89.40 & 92.51 & 72.56 & 86.40 \\
Ties & 1.0B & 4.65 & 93.92 & \underline{91.44} & 58.54 & 86.47 & 88.58 & 91.87 & 75.54 & 86.39 \\
Dare & 1.0B & 4.65 & 93.80 & 89.46 & 58.33 & 88.72 & 89.13 & 92.29 & 73.64 & 86.20 \\
\midrule
\rowcolor[gray]{0.9} 
\textbf{Domain-specific-CA} &1.0B& 4.65& 93.80 & 91.16 & \underline{58.57} & \textbf{88.86} & 89.47 & \underline{92.60} & 74.72 & \underline{86.44} \\
\rowcolor[gray]{0.9} 
\textbf{Dare-CA}& 1.0B & 4.65 & \underline{94.49} & 91.15 & 58.56 & 88.76 & \textbf{89.56} & \textbf{92.80} & \textbf{78.70} & 86.34 \\
\rowcolor[gray]{0.9} 
\textbf{Ties-CA}& 1.0B & 4.65 & \textbf{94.61} & \textbf{92.49} & \textbf{60.06} & \underline{88.83} & \underline{89.54} & 91.89 & \underline{75.81} & \textbf{86.45} \\
\bottomrule
\end{tabular}}
\label{tab:glue}
\end{table*}

We perform evaluations on \textbf{four} major tasks, including language modeling, text classification, question answering, and image classification. For language modeling, we use the Wikitext-2 and Wikitext-103 \citep{merity2016pointer} benchmarks. For text classification, we employ a subset of the GLUE \citep{wang2019glue} benchmark, a collection of \textbf{eight} diverse tasks designed to test different aspects of language understanding. For question answering, we employ two famous benchmarks: SQuAD \citep{rajpurkar-etal-2016-squad} and WikiQA \citep{yang-etal-2015-wikiqa}. Finally, the ImageNet-1k \citep{deng2009imagenet} dataset is chosen for image classification evaluation.

\begin{wrapfigure}{r}{0.45\linewidth}
  \centering
  \includegraphics[width=\linewidth]{plot_per.pdf}
  \caption{\small Perplexity of GPT2-small variants starting at the tenth epoch.}
  \label{fig:conv}
\end{wrapfigure}
We choose GPT-2 \citep{radford2019language} small and Swin-Transformer small \citep{liu2021Swin} as our backbones for language modeling and image classification, respectively. Regarding GLUE and question-answering tasks, T5 base \citep{raffel2020exploring} is chosen.

Our experimental results confirm that the proposed merging method accelerates pre-training convergence and, when combined with other merging protocols, enhances model performance across tasks and settings. All results are averaged over 5 runs with different random seeds. Detailed information on the datasets, models, training procedures, and hyperparameters is provided in Appendix \ref{datasets} and Appendix \ref{sec:more_ex_detail}. {For additional experiments on different routers and merging methods, we refer to Appendix  \ref{appendix: twin-merging}, and \ref{appendix: router}}. 
\subsection{Training and evaluation details}
 We fix the number of epochs for all models on each task. For each text-related task, we first undertake a comprehensive hyper-parameter search. This encompasses batch sizes from \{$8$, $16$, $32$, $64$\}, learning rates from \{$3e{-4}$, $1e{-4}$, $3e{-5}$, $1e{-5}$\}, to pinpoint the optimal fine-tuned models. Regarding image classification tasks, a batchsize of 96 was chosen for all models. In addition, we choose AdamW \citep{loshchilov2018decoupled} as the default optimizer and conduct all experiments on NVIDIA A100 GPUs.
We compare our proposal to three merging baselines, including domain-specific, Ties, and Dare merging. There exists prior works on merging methods with the aid of the Fisher Information Matrix, such as \citep{matena2022merging}, which rely on access to a validation set used to compute the Fisher matrix or fine-tune hyperparameters. To eliminate the need for a validation set, \citep{jin2023dataless} proposes storing and transmitting inner product matrices derived from the training data for each task, which are of the same size as the original model. However, this approach becomes costly for large models, as storage and transmission demands increase linearly with model size and the number of tasks as well as the number of experts. Therefore, we choose baselines that are needless of extra information and computational cost to perform comparisons. {More details about theoretical comparison between CAMEx and Fisher-based merging methods can be found in Appendix \ref{sec: camex_fisher_comparison}}. We want to note that our merging protocol can be easily integrated into other works such as merge then compress protocol \citep{li2024merge}.

\subsection{Results}
In the following tables, the row with our method's results is highlighted in \textit{grey}. Results with the best and second best performance are written in \textbf{bold} and \underline{underline}, respectively. In addition, methods with the postfix ``-CA'' denote the curvature-aware version of the corresponding baseline.

In Table \ref{tab:glue}, the results demonstrate that CA-augmented models consistently outperform their non-CA counterparts. Ties-CA achieves the highest scores on SST-2 (94.61), MRPC (92.49), CoLA (60.06), and MNLI (86.45), showing considerable improvements over both the vanilla and standard Ties models. Similarly, Dare-CA performs best on RTE (78.70), surpassing Dare (73.64), indicating that CA improves performance on smaller datasets and tasks with higher variability. Furthermore, Domain-specific-CA exceeds the non-CA version on QNLI and MNLI, demonstrating the broader applicability of curvature-aware methods. {We provided a significant t-test at Appendix \ref{t-test}.}

\setlength{\intextsep}{2pt}
\setlength{\columnsep}{10pt}
\begin{wraptable}{r}{0.5\textwidth}
\centering
\caption{Performance of GPT-2 small variants for the pre-training task on Wikitext-103.}
 \resizebox{1\linewidth}{!}{
\begin{tabular}{l c c c}
\toprule
\textbf{Methods} & \textbf{Perplexity}$\downarrow$ & \textbf{Params (M)} & \textbf{GFLOPS} $\downarrow$\\
\midrule
Vanilla & 23.03 & 125 & 292.5\\
SMoE & 22.42 & 522 & 292.5\\
Domain-specific & 21.64 & 522 & 292.5\\
\midrule 
\rowcolor[gray]{0.9} 
\textbf{Domain-specific-CA} & \textbf{21.50} & 522 &  292.5 \\
\rowcolor[gray]{0.9} 
\textbf{Dynamic} & \underline{21.55} & \textbf{470} & 292.5\\
\bottomrule
\end{tabular}
}
\label{tab:wik103}
\end{wraptable}
In Table \ref{tab:wik103}, the Domain-specific-CA and Dynamic outperform the Vanilla, SMoE, and Domain-specific baselines, with lower perplexity values. Domain-specific-CA achieves the lowest perplexity score of 21.50, showcasing superior performance in language modeling tasks when compared to all other methods. The Dynamic architecture follows closely with a perplexity of 21.55 while also reducing the parameter count by 9\%, compared to the other methods. This highlights the Dynamic architecture's efficiency in maintaining strong performance with fewer parameters, making it ideal for resource-constrained environments. Moreover, the Dynamic architecture is competitive with Domain-specific-CA and outperforms the rest in terms of convergence speed, which is shown in Figure \ref{fig:conv}. 

\setlength{\intextsep}{2pt}
\setlength{\columnsep}{10pt}
\begin{wraptable}{r}{0.5\textwidth}
\caption{Performance of GPT-2 small variants for the supervised fine-tuning task on Wikitext-2}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{l c c c}
\toprule
\textbf{Methods} & \textbf{Perplexity}$\downarrow$ & \textbf{Params (M)} & \textbf{GFLOPS} $\downarrow$\\
\midrule
Vanilla & 21.84 & 125 & 292.5\\
SMoE  & 21.60 & 522 & 292.5\\ \midrule
Domain-specific & 21.56 & 522 & 292.5\\
Ties & 21.45 & 522 & 292.5\\
Dare & 21.60 & 522 & 292.5\\ \midrule
\rowcolor[gray]{0.9} 
\textbf{Domain-specific-CA} & \textbf{21.06} & 522& 292.5\\
\rowcolor[gray]{0.9} 
\textbf{Dare-CA} & 21.42 & 522& 292.5\\
\rowcolor[gray]{0.9} 
\textbf{Ties-CA} & \underline{21.11} & 522 &292.5\\
\bottomrule
\end{tabular}
}
\label{tab:wik2}
\end{wraptable}

In Table \ref{tab:wik2}, the Vanilla model reaches a perplexity of 21.84. Despite increasing parameters, SMoE only slightly improves to 21.60. Domain-specific, Ties, and Dare methods show small gains, with Ties reaching 21.45. However, curvature-aware (CA) methods outperform all others. Domain-specific-CA achieves the best perplexity at 21.06, followed by Ties-CA (21.11) and Dare-CA (21.42), each significantly improving over their non-CA counterparts. All models beyond Vanilla share the same computational cost, indicating that CA methods enhance performance without added complexity. Domain-specific-CA stands out, demonstrating the clear advantage of curvature-aware optimization.

 

\setlength{\intextsep}{1pt}
\setlength{\columnsep}{10pt}
\begin{wraptable}{r}{0.5\textwidth}
\caption{Performance of T5-base variants on question answering tasks.}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\textbf{Params}} & \multirow{2}{*}{\textbf{TFLOPs}} & \textbf{SQuAD} & \textbf{WikiQA} & \\
         &   &  & \textbf{Em/F1} & \textbf{Accuracy} & \\ \midrule
Vanilla         & 222M  & 2.86   & 81.01/88.14 & 96.06   \\ 
SMoE            & 1.0B  & 2.86   & 81.25/88.50 & 96.04   \\\midrule
Domain-specific & 1.0B  & 2.86   & 80.21/87.44 & 95.32 \\ 
Ties            & 1.0B  & 2.86   & 80.76/88.11 & 95.87   \\ 
Dare           & 1.0B   & 2.86   & 80.88/88.03 & 96.01   \\ \midrule
\rowcolor[gray]{0.9} 
\textbf{Domain-specific-CA} & 1.0B & 2.86 & 80.44/87.69 & 95.72 \\
\rowcolor[gray]{0.9} 
\textbf{Ties-CA} & 1.0B   & 2.86   & \underline{81.52}/\textbf{88.60} & \textbf{96.55}   \\ 
\rowcolor[gray]{0.9} 
\textbf{Dare-CA} & 1.0B   & 2.86   & \textbf{81.76}/\textbf{88.60} & \underline{96.23}   \\ \bottomrule
\end{tabular}
}
\label{tab:QA}
\end{wraptable}
In Table~\ref{tab:QA}, the baseline models, including Vanilla, SMoE, and non-CA versions of Ties and Dare, achieve solid results but show diminishing improvements as model complexity increases. In contrast, our curvature-aware methods significantly outperform their counterparts. For instance, on the SQuAD dataset, Dare-CA achieves the highest Exact Match (EM) score of 81.76\% and an F1 score of 88.60\%, surpassing all other methods. Similarly, on WikiQA, Ties-CA attains the highest accuracy of 96.55\%, with Dare-CA closely following at 96.23\%. 

In Table~\ref{tab:img}, while Vanilla and SMoE exhibit solid accuracy scores, they are surpassed by the curvature-aware (CA) enhanced versions of the models. Notably, Ties-CA delivers the best top-1 accuracy at 83.38\% and the highest top-5 accuracy at 96.96\%, slightly edging out Dare-CA, which achieves 83.38\% and 96.94\%, respectively. 
\section{Ablation}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{plot_alpha.pdf}
    \caption{Impact of the $\alpha$ parameter on Curvature-Aware method performance across NLP tasks. We observe that the scaling factors that are within the range $[0.8,1]$ consistently improve model's performance.}
    \label{fig:plot_alpha}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{plot_rank.pdf}
    \caption{Impact of the Kronecker rank of curvature matrix on model's performance. We observe that as the rank increases the performance drops and then saturates. However, we would like to note that this curve might change depending on the downstream tasks and the merging protocol.}
    \label{fig:enter-label}
    \vspace{-5mm}
\end{figure}

\textbf{Impact of the scaling factor.} The plot in Figure \ref{fig:plot_alpha} illustrates the impact of the $\alpha$ parameter on the performance of three curvature-aware (CA) model variants Domain-specific-CA, Ties-CA, and Dare-CA across three natural language processing tasks: STSB, MRPC, and RTE. The $\alpha$ parameter ranges from 0.5 to 1.0. The overall trend suggests that increasing $\alpha$ leads to better generalization, particularly for complex tasks such as RTE, where sentence-level entailment and similarity benefit from stronger curvature-aware representations. Moreover, across all tasks, the model reaches its peak performance when $\alpha$ is inside the range $[0.8, 1]$. This observation aligns with that indicated by \citep{yadav2023ties-merging}. {For a more comprehensive analysis on the impact of $\alpha$ and number of experts, we direct the readers to Appendix \ref{ablation:alpha} and \ref{ablation:nexpeerts}.}
\setlength{\intextsep}{2pt}
\setlength{\columnsep}{10pt}
\begin{wraptable}{r}{0.5\textwidth}
\vspace{5mm}
\centering
\caption{Comparison of Accuracy for Swin-Transformer small variants on ImageNet-1k.}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Methods} & \textbf{Params (M)} & \textbf{GFLOPs} & \textbf{Acc@1} & \textbf{Acc@5} \\ \midrule
Vanilla & 50  & 6.75  & 83.14   & 96.90  \\ 
SMoE    & 157 & 6.75  & 83.15   & 96.71  \\ \midrule
Domain-specific & 157 & 6.75 & 83.15   & 96.91          \\ 
Ties  & 157 & 6.75 & 83.28   & 96.93          \\ 
Dare  & 157 & 6.75 & 83.13   & 96.88     \\ \midrule
\rowcolor[gray]{0.9} 
\textbf{Domain-specific-CA} & 157 & 6.75 & \underline{83.29}   & \underline{96.95}   \\ 
\rowcolor[gray]{0.9} 
\textbf{Ties-CA} &157 & 6.75 & \textbf{83.38}   & \textbf{96.96} \\
\rowcolor[gray]{0.9} 
\textbf{Dare-CA} &157 & 6.75 & \textbf{83.38} & 96.94  \\
\bottomrule
\end{tabular}
}
\label{tab:img}
\end{wraptable}
\begin{wraptable}{r}{0.5\textwidth}
\centering
\caption{Comparison for Swin-Transformer small variants on corrupted ImageNet.}
\resizebox{1\linewidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Methods}   & \textbf{ImageNet-O} & \textbf{ImageNet-A}   & \textbf{ImageNet-R} \\ \midrule
Vanilla                     & 45.88   & 23.68/53.10 & 37.34/52.34    \\ 
SMoE                        & 43.34  &  23.72/53.15 & 38.02/55.17   \\ \midrule
\rowcolor[gray]{0.9} 
% \textbf{Domain-specific-CA}   & 44.88 & \underline{24.75}/\underline{53.78} & \underline{38.23}/\underline{55.34}   \\ 
\rowcolor[gray]{0.9} 
\textbf{Ours}      & \textbf{50.69}   &  \textbf{25.45}/\textbf{54.24}  & \textbf{38.37}/\textbf{55.42}  \\
% \rowcolor[gray]{0.9} 
% \textbf{Dare-CA} &  44.88  & 24.74/53.77 & \underline{38.23}/55.24     \\
\bottomrule
\end{tabular}
}
\label{tab:img_corrup}
\end{wraptable}

\textbf{Improved performance with higher Kronecker rank.} Across all three tasks (STSB, MRPC, and RTE), the evaluation metrics tend to improve as the rank increases from 1 to 8. This indicates that higher-ranked models generally perform better, suggesting a positive correlation between rank and task performance. Notably, the Domain-specific-CA model consistently achieves high performance across all tasks, especially in STSB, where metrics approach 0.90. Although MRPC and RTE show slightly lower metrics, ranging from 0.50 to 0.75, there is a clear improvement in performance as rank increases, particularly in the lower-to-mid ranks. However, we observed a decline in performance for Ties-CA and Dare-CA as the rank increases. We hypothesize that this is due to the masking mechanism employed by these methods, which may interfere with the learning process of the curvature matrices. 

\textbf{Robustness against noise.} Table \ref{tab:img_corrup} demonstrates that curvature-aware models offer superior performance on corrupted ImageNet datasets compared to both Vanilla and SMoE variants. Among the models, our best configuration (Ties-CA) stands out as the best performer, showcasing robustness to corruptions across all datasets. These results suggest that incorporating curvature-awareness can substantially improve model robustness in challenging conditions.
\section{Related Work}
\textbf{Sparse Mixture-of-Experts (SMoE).} As the demand for model scaling grows increasingly widespread, there is a pressing inquiry into efficient ways to optimize computing costs while minimizing the impact on model performance. To address this need, Sparse Mixture of Experts (SMoE) has emerged and undergone extensive research and exploration~\citep{shazeer2017outrageously,lepikhin2021gshard,fedus2022switch}. Starting with \citep{shazeer2017outrageously}, the integration of SMoE into transformer architectures followed shortly after with the works of \citep{lepikhin2021gshard} and \citep{fedus2022switch}. The principle of SMoE is based on a simple concept: scaling the horizontal dimension of models (i.e., the number of feedforward blocks) rather than the vertical dimension (i.e., the number of stacked layers). This allows the model to selectively activate units or parameters based on the input tokens, thereby optimizing resource usage while maintaining performance.

\textbf{SMoE Efficiency Bottlenecks and Emerging Solutions.} While it remains controversial whether to use Top-1 or Top-K routing, some research has highlighted the potential performance gains from increasing the number of activated experts~\citep{shazeer2017outrageously,chen2023sparse}. Other studies have found redundancies among experts in MoE layers~\citep{li2024merge, lu2024expertsequalefficientexpert}. Additionally, some work has proposed using low-rank experts~\citep{wu2024mixture,liu2024mixturelowrankexpertstransferable,Wu_2024_CVPR} inspired by LoRA~\citep{hu2022lora}. Despite the varying research directions, these studies consistently show that training a robust SMoE requires substantial computational and memory resources. This has motivated researchers such as \citep{li2024merge}, \citep{he-etal-2023-merging}, and \citep{zhong2024lory} to merge experts within each MoE layer, reducing the number of experts to a single one and significantly improving training and inference efficiency.


\textbf{Model Merging with curvature-aware.} Though numerous methods for merging models have been introduced and developed~\citep{yadav2023ties-merging,cai2023robust,ilharco2022patching,matena2022merging,jin2022dataless,don2022cold,rame2023model, Tuan2024Hypertrans,  lu2024twinmerging}, most of these works consider merging protocols in the Euclidean parameter space. However, it has been noted that the space of deep neural network models is a Riemannian one~\citep{amari1998natgrad}. \citep{matena2022merging} and \citep{jin2022dataless} were the first to fuse model weights while accounting for the Fisher Information. Despite their promising results, these methods require massive computation to approximate the inversion of the Fisher matrix. Moreover, the Fisher matrix has a size proportional to the dimension of the model parameters, which significantly increases memory usage. Consequently, these methods are challenging for directly integrating into SMoE layers to fuse expert weights.

\section{Limitation and conclusion}
In this work, we introduced CAMEx, a curvature-aware approach to expert merging in Mixture of Experts architectures. By leveraging natural gradients to account for the parameter manifold's curvature, CAMEx enhances model performance and reduces computational costs during both pre-training and fine-tuning, outperforming traditional Euclidean-based methods. Additionally, our dynamic merging architecture optimizes resource usage by incorporating a global expert across layers, thus minimizing model size without sacrificing accuracy. Despite the overall improvements, a minor limitation is that curvature-aware merging demonstrates reduced compatibility with Ties and Dare merging at higher Kronecker ranks. Future work could dive deeper into this phenomenon and extend CAMEx to other expert merging methods and explore curvature-aware approaches in broader neural network models to further enhance our dynamic architecture. This research lays the groundwork for developing more efficient and scalable models in large-scale machine learning.

\newpage
\subsubsection*{Acknowledgments}
Dung V. Nguyen was funded by the Master, PhD Scholarship Programme of Vingroup Innovation Foundation (VINIF), code VINIF.2023.ThS.024.

This research / project is supported by the National Research Foundation Singapore under the AI
Singapore Programme (AISG Award No: AISG2-TC-2023-012-SGIL). This research / project is
supported by the Ministry of Education, Singapore, under the Academic Research Fund Tier 1
(FY2023) (A-8002040-00-00, A-8002039-00-00). This research / project is also supported by the
NUS Presidential Young Professorship Award (A-0009807-01-00) and the NUS Artificial Intelligence Institute--Seed Funding (A-8003062-00-00).

\textbf{Reproducibility Statement:} Source codes for our experiments are provided in the supplementary materials of the paper. The details of our experimental settings and computational infrastructure are given in Section \ref{sec:experiment} and the Appendix \ref{sec:more_ex_detail}. All datasets that we used in the paper are published, and they are easy to find in the Internet.

\textbf{Ethics Statement:} Given the nature of the work, we do not foresee any negative societal and ethical impacts of our work.

\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage

\appendix

\begin{center}
{\bf \Large{Supplement to ``CAMEx: Curvature-aware Merging of Experts''}}
\end{center}
\label{sec:appendix}

\DoToC



\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{merging-crop.pdf}
    \caption{{CAMEx merging pipeline vs Fisher-based merging pipeline. Note that Fisher merging requires the storing of the $\nabla_{E_i} \log p_{E_i}(y|x)$ for all experts and all $x$ in training dataset. Furthermore, it has been pointed out that Fisher merging will have poor performance while using fewer examples to estimate the Fisher.} }
    \label{fig:camex_fisher}
\end{figure}

{
\section{Comprision of CAMEx and Fisher-based merging methods}
\label{sec: camex_fisher_comparison}
The pipeline comparison between CAMEx and Fisher-based merging methods is shown in Figure \ref{fig:camex_fisher}. Both approaches aim to capture the curvature of the parameter space during the merging process.
(Diagonal) Fisher Merging \citep{matena2022merging} applies a diagonal approximation to the Fisher information matrix. In this work, they estimate the diagonal of the Fisher matrix as:
}
\begin{equation}
{
    \bf{F} = \mathop{\mathbb{E}}_{x \sim D_m} \left[ \mathop{\mathbb{E}}_{y \sim p_{\bf{\theta}}(y|x)} \left[ \nabla_{\bf{\theta}} \log p_{\bf{\theta}}(y|x)  \nabla_{\bf{\theta}} \log p_{\bf{\theta}}(y|x)^{\top} \right] \right],
    }
\end{equation}
{
The expectation over $y$ can be estimated via sampling from $p_\theta(y|x_i)$ or computed exactly when the number of classes is small. The closed-form solution for Fisher merging (without necessarily applying the diagonal approximation) is given by:}

\begin{equation}
{
\bf{\hat{E}}^l_m =   \bigg(\displaystyle \sum_{m=1}^M  F^l_m \bigg)^{-1} \bigg( \displaystyle\sum_{i=1}^N  F^l_i \bf{E}^l_i \bigg) .}
\end{equation}
{
Thus, to approximate the Fisher Information Matrix for SMoE models, Fisher merging requires storing $\nabla_{E_i} \log p_{E_i}(y|x)$ for all experts and all $x$ in the training dataset. Additionally, it has been noted that Fisher merging can suffer from poor performance when fewer examples are used to estimate the Fisher matrix \citep{matena2022merging}.\\}
{
In the case of our method (depicted in Figure \ref{fig:camex_fisher}a), by denoting $\mathbf{M}_i$ as the curvature matrix of the $i$-th expert, CAMEx utilizes the formula for merging experts derived from the natural gradient descent update as:
}
\begin{equation}
{
\tag{CA-Merg}
        \bf{\hat{E}}_m^l = \bf{E}_m^l + \alpha\sum_{i=1}^{N-1} \bf{M}_i\cdot (s^l_i*\tau_i^l)}
\end{equation}
{
CAMEx implicitly implements the gradient-based matching between the task loss gradient and domain-vector of the corresponding expert to approximate the empirical Fisher through the dynamic of gradient descend update of $\bf{M}_i$:}
{
\begin{align}
\bf{M}^{t+1}_{i} &= \bf{M}^{t}_{i} - \beta * \dfrac{\partial \mathcal{L}}{\partial M^{t}_{i}} = \bf{M}^{t}_{i} - \alpha\beta * s^{t}_i * \dfrac{\partial \mathcal{L}}{\partial \bf{\hat{E}}^{t}_m} \cdot (\bf{E}^{t}_i - \bf{E}^{t}_m),  
\end{align}
}
{
where the term $\dfrac{\partial \mathcal{L}}{\partial \bf{\hat{E}}_m} \cdot (\bf{E}_i - \bf{E}_m)$ represents the outer product of the gradients of the task loss and the domain vectors. This operation contributes to capturing the curvature of the expert parameter space, ensuring curvature awareness during the merging process. This approach eliminates the need to compute the inversion of the empirical Fisher Information Matrix, thereby reducing computational overhead while maintaining sensitivity to parameter space curvature.}

\section{Additional Details on datasets}
\label{datasets}

This section provides detailed information on the datasets and evaluation metrics used in the experiments in Section \ref{sec:experiment}.
\subsection{Language Modeling on WikiText}
\textbf{The WikiText-103} dataset consists of Wikipedia articles designed to capture long-range contextual dependencies. The training set includes approximately 28,000 articles, totaling around 103 million words. The validation and test sets have 218,000 and 246,000 words, respectively, spread across 60 articles per set, with each set comprising roughly 268,000 words. Our experiments follow the standard procedure described in \citep{DBLP:conf/iclr/MerityX0S17}.

\textbf{WikiText-2} is a smaller version of WikiText-103, containing 2 million tokens and a vocabulary of 33,000 words.

\subsection{Text classification on GLUE benchmark}
These tasks include MNLI \citep{williams2018mnli}, which assesses a model's ability to determine entailment between pairs of sentences; QQP \citep{quora2017qqp} and MRPC \citep{dolan2005mrpc}, which focus on identifying sentence similarity and paraphrase detection; SST-2 \citep{socher2013recursive} for sentiment analysis; CoLA \citep{warstadt2019cola} for grammaticality judgment; and QNLI \citep{wang2019glue} for question-answer classification. Additionally, STSB \citep{cer2017stsb} evaluates the model's ability to measure sentence similarity, while RTE \citep{dagan2006rte} tests logical reasoning. 

\subsection{Question-answering on SQuAD and WikiQA}
\textbf{SQuADv1.1} \citep{rajpurkar-etal-2016-squad} (Stanford Question Answering Dataset) is a widely used benchmark for reading comprehension and question answering tasks. It contains over \textbf{100,000 question-answer pairs} sourced from more than 500 Wikipedia articles. Each question is paired with a paragraph from the article, where the answer is a span of text extracted from the passage. The dataset consists of natural language questions that cover a wide range of topics, context paragraphs from Wikipedia, and answers marked by their start and end positions within the context. The primary task is to extract the correct answer span based on the posed question. Key features of the dataset include the need for exact span extraction, the large dataset size, and its task design focused on reading comprehension. Evaluation is typically done using Exact Match (EM), which measures the percentage of predictions that exactly match the ground-truth answers, and the F1 score, which measures the overlap between predicted and ground-truth answers by calculating the harmonic mean of precision and recall.

\textbf{WikiQA} \citep{yang-etal-2015-wikiqa} is an open-domain question answering dataset designed for answer sentence selection tasks. It consists of natural language questions primarily extracted from search engine queries, with candidate sentences sourced from Wikipedia articles. Each candidate sentence is labeled as either a correct or incorrect answer for the given question. The dataset contains 3,047 questions and 29,258 candidate sentences. The main challenge is selecting the correct sentence from a set of candidates, unlike SQuADv1.1, where the task focuses on extracting a text span. Key features include its real-world query origins, the sentence selection task, and the open-domain nature, which requires models to identify relevant sentences from diverse topics. WikiQA is evaluated using Accuracy.


\subsection{Image Classification on Imagenet}
\textbf{ImageNet-1k}, the most widely utilized subset of the ImageNet dataset introduced by \citep{deng2009imagenet}, comprises 1.28 million images for training and 50,000 images for validation, across 1,000 categories. Performance evaluation is typically based on top-1 and top-5 accuracy metrics.

\subsection{Adversarial Examples and  Out-of-distribution datasets}
\textbf{ImageNet-A}: The ImageNet-A dataset~\citep{hendrycks2021natural} contains real-world images specifically curated to fool ImageNet classifiers. It focuses on 200 classes, a subset of the 1,000 classes in ImageNet-1k. Errors made within these 200 classes are considered particularly significant, as they represent a wide variety of categories from ImageNet-1k.

% \textbf{ImageNet-C}: To evaluate model robustness against common image distortions, we utilize the ImageNet-C dataset~\citep{hendrycks2019benchmarking}, which includes 15 types of artificially generated corruptions at five different severity levels. Models are assessed using the mean corruption error (mCE) metric, where lower mCE values reflect greater robustness to corruption.

\textbf{ImageNet-O}: This dataset consists of examples adversarially filtered to challenge out-of-distribution (OOD) detectors on ImageNet~\citep{hendrycks2021natural}. It includes images from the larger ImageNet-22k dataset but excludes those present in ImageNet-1k. The selected samples are those that a ResNet-50 model confidently misclassifies as an ImageNet-1k class, and the primary evaluation metric is the area under the precision-recall curve (AUPR).

\textbf{ImageNet-R}: ImageNet-R contains a variety of artistic renditions of object classes found in the original ImageNet dataset~\citep{hendrycks2021many}. This dataset includes 30,000 artistic representations of images from 200 classes, selected from the ImageNet-1k subset. The dataset was created to challenge models with non-standard visual interpretations of the classes.

\section{Algorithm and implementation details}
\subsection{Causal segmenting}
\label{sec:alg_imp}
\begin{algorithm}[t!]
\small{
\caption{The Overall Procedures of \texttt{CAMEx}.}
\begin{algorithmic}[1]
\setstretch{1.1}
\State \textbf{Initialize:} A model $\mathcal M$ with $l$ SMoE layers, the total number of original experts $N$.
\State Let $\mathtt{H}^t \in \mathbb{R}^{B\times L \times N}$ and $\mathtt{T}^t \in \mathbb{R}^{B \times L \times d} $ denote the \textit{router logits} and the \textit{sequence of tokens} at intermediate layer $t$, respectively.
\For{layer $t=1,\ldots,l$}
\State $K = L / S $, $T^l \gets \texttt{RESHAPE}(T, B*K,S,d)$ \Comment{\textcolor{gray}{Begin Causal Segmenting}}
\State $\mathtt{H}^l \gets \B{G}\left(T^l \right)$ 
\State $\mathtt{H}^l \gets \texttt{ROLLandDETACH}\left(\mathtt{H}^l \right)$ 
\If{\texttt{TIES-MERGING}}   \Comment{\textcolor{gray}{Generate mask for merging}}
\For{expert $i=1,\ldots,N-1$} 
\State $\tau_i \gets \B{E}_i - \B{E}_m $
\State $\gamma_i \gets sgn(\tau_i)$
\EndFor
\State $\gamma^m = sgn(\sum_{i=1}^{N-1} \tau_i)$
\For{expert $i=1,\ldots,{N-1}$} 
\State $\tau^m_i \gets \gamma_i \wedge \gamma^m$
\State $\tau_i \gets \tau_i \cdot \B{M}_i$
\EndFor
\Else
\State Generate mask for \texttt{DARE-MERGING}
\EndIf
\State $\B{E}_m \gets \B{E}_m + \gamma_m * \sum_{i=1}^{N-1} \texttt{H}^l_i * \tau_i  $ \Comment{\textcolor{gray}{Merge Experts}}
\EndFor
\end{algorithmic}
\label{alg:main}}
\end{algorithm}
{
Background of Causal Segmenting: 
A significant advancement in SMoE design centers on fully differentiable architectures that eliminate the need for additional loss terms to stabilize training. In \citep{muqeeth2023soft}, a model was introduced that computes a weighted average of expert feed-forward networks (FFNs). For an input \(x\) with corresponding routing weights, the output is defined as:
 }
$$
{
o_x = \text{FFN}\left(h_x; \sum_{i=1}^{N} s_i \cdot \mathbf{E}_i\right), \quad \text{where} \quad s_i = \text{Softmax}(\B{G}(h_x))_i. }
$$ 
{
However, applying this approach to autoregressive language models is computationally costly, as the merged FFN must be computed for each token in the sequence, leading to costs that scale linearly with the number of experts. An alternative based on poolingâ€”routing via the sequence's average representation, as follows: }
$$
{
s_i = \text{Softmax}\left(\B{G}\left(\frac{\sum_{j=1}^L h_{x_j}}{L}\right)\right)_i. }
$$ 
{
This, however, disrupts the autoregressive property essential for pre-training. To address this, \citep{zhong2024lory} introduced causal segment routing. This technique merges FFNs in an MoE layer by utilizing information from the preceding segment to process the current segment. Specifically, given a training instance \(X\) consisting of \(L\) tokens (e.g., \(L = 4096\)), we divide the instance into \(N\) segments, each containing \(T\) (e.g., \(T = 256\)) consecutive tokens. For the \(k\)-th segment \(S_k\), where \(k > 1\), we compute the average of the hidden representations from the previous segment \(S_{k-1}\), denoted as \(\bar{h}_{k-1}\). By using the average hidden representation, the model can adapt to prompts of varying lengths during inference. The average hidden representation \(\bar{h}_{k-1}\) is then employed to determine the routing weights, leading to a merged expert \(\bar{\bf{E}}\): }
{
\begin{align}
\bar{h}_{k-1} &= \frac{1}{T} \sum_{x \in S_{k-1}} h_x, \quad s_i = \text{Softmax}(\B{G}(\bar{h}_{k-1})), \quad \bar{\bf{E}} = \sum_i s_i \cdot \bf{E}_i.
\end{align}
}
{
The merged expert \(\bar{\mathbf{E}}\) is then used to process all tokens in the current segment \(S_k\), i.e., \(o_x = \text{FFN}(h_x; \bar{\mathbf{E}}), \forall x \in S_k\). This approach ensures that the model's routing decisions rely exclusively on data from preceding positions. For the first segment \(S_1\), the segment's own representation is used to compute the merging weights for its FFN. To prevent information leakage, a stop-gradient operation is applied to \(\B{G}(\bar{h}_1)\):}
\begin{equation}
    {\bar{h}_0 = \dfrac{1}{T}\sum_{x \in S_0}^T h_x}
\end{equation}
{
These tokens are then used to calculate the scores for the merging procedure}
{
\begin{align*}
\tag{\texttt{ROLLandDETACH}}
    s_0 &= \texttt{DETACH}\left(\B{G}(\bar{h}_1, k)\right) \\
    s_i &= \B{G}(\bar{h}_{i-1}), \quad i=1,\dots, S-1
\end{align*}
}
\subsection{Some implementations}
\textbf{Implementation of Kronecker product} We consider the case where experts are linear layers
\begin{lstlisting}
    # Calculating domain-specific vectors
    taus = weights - weight_m 

    # output_size = dim_out1 * dim_out2, input_size = dim_in1 * dim_in2
    taus = taus.view(1, -1, dim_out1, dim_out2, dim_in1, dim_in2).repeat(rank, 1, 1, 1, 1, 1)
    # Calculate Kronecker-product
    taus = torch.einsum("rbij, rbjklm->rbiklm", curve1_out, taus)
    taus = torch.einsum("rbik, rbjklm->rbjilm", curve2_out, taus)
    taus = torch.einsum("rbil, rbjklm->rbjkim", curve1_in, taus)                    
    taus = torch.einsum("rbim, rbjklm->rbjkli", curve2_in, taus)
    # Summation along the Kronecker rank dimension and reshape
    taus = taus.sum(0) 
    taus = taus.reshape(-1, output_size, input_size)  
\end{lstlisting}
\section{More Experiment Details} \label{sec:more_ex_detail}

\paragraph{Supervised Fine-Tuning Hyper-Parameters} Besides $\{$batch~size, learning~rate, epoch~counts$\}$ which vary for each task, we keep other hyper-parameters of supervised fine-tuning fixed for all tasks. These are shown in  Table~\ref{tab:ft-hyper}.

\begin{table}[htbp]
  \centering
  \arrayrulecolor{black}
  \caption{\small Fine-tuning hyper-parameters of all models in Section~\ref{sec:experiment}}
  \resizebox{0.4\textwidth}{!}{
    \begin{tabular}{lr}
    \toprule
    Hyper-Parameters & Values \\
    \midrule
    Optimizer & \textsc{AdamW} \\
    Adam $\epsilon$ & $1e\mathrm{-}6$ \\
    Adam $\beta$ & ($0.9$, $0.98$) \\
    Warm-up steps & $16$ \\
    Weight decay & $0.01$ \\
    LR scheduler & \textsc{Linear decay} \\
    \midrule
    Scaling factor $\alpha$ & $1$ \\
    Kronecker rank $r$ & $1$ \\
    \bottomrule
    \end{tabular}}
  \label{tab:ft-hyper}%
\end{table}
\section{Derivation}
\label{sec:derviation}
This is the derivation for Eqn~\ref{eq: ca-merge8} in Section~\ref{sec:theo}
\begin{align}
    \B{\hat{E}}_m &= \B{E}_m + \alpha\sum_{j=1}^{N-1} \B{M}^{t+1}_{j} \cdot (s^{t+1}_j * \tau^{t+1}_j) \\
        &= \B{E}_m + \alpha\sum_{j=1}^{N-1} \left[\B{M}^{t}_{j} - \alpha\beta * s^{t}_j * \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}^{t}_m} \cdot \tau^{t}_j \right] \cdot (s^{t+1}_j * \tau^{t+1}_j) \\
        &= \underbrace{\B{E}_m + \alpha\sum_{j=1}^{N-1} s^{t+1}_j * \B{M}^{t}_j \cdot \tau^{t+1}_j}_{\text{domain-specific merging with curvature-aware}} - \alpha^2\beta \sum_{j=1}^{N-1} s^{t}_j s^{t+1}_j * \left(\tau^{t^\top}_j \cdot \tau^{t+1}_j   \right)\cdot \dfrac{\partial \mathcal{L}}{\partial \B{\hat{E}}^{t}_m}
\end{align}

{\section{Step-by-step walkthrough for key equations of CAMEx}
\label{walkthrough}
\subsection{Key equation for merging of CAMEx}}
\begin{equation}
{
\tag{CA-Merg}
    \bf{\hat{E}}_m^l = \bf{E}_m^l + \alpha\sum_{i=1}^{N-1} \bf{M}_i\cdot (s^l_i*\tau_i^l)}
\end{equation}

{In (CA-Merg) equation, we consider the merging of experts at $l$-th layer of the model. $\bf{E}_m^l$ denotes the "base" expert that is not included in the routing process. $\tau^l_i = \bf{E}_i^l - \bf{E}_m^l$ denotes $i$-th the domain-vector that adapts the "base" expert to the corresponding domain. Finally, $s^l_i$ denotes the score of the $i$-th domain vector w.r.t the input. We view the merging of experts as a optimization problem where $\alpha*s^l_i$ acts as the adaptive learning rate. Therefore, it is straightforward to integrate natural gradient approach into this equation by introducing curvature matrices $\bf{M}_i$. Due to the challenging tractability of the Fisher Matrix in the intermediate layers of deep models, we proposed to learn them empirically through backpropagation as indicated by Eqn. \ref{M-grad} in the main text and a simmilar approach using meta-learning \citep{park2019meta}.}

{\subsection {Key equation for merging in dynamic architecture}}
\begin{equation}
 {\begin{cases}
\tag{Dynamic-Merg}
    \bf{E}_m^{l+1} &= \bf{E}_m^l + \dfrac{\alpha}{N-1}\displaystyle\sum_{i=1}^{N-1} \bf{M}_i \cdot \tau_i^l\\
    \bf{\hat{E}}_m^{l+1} &= \bf{E}_m^{l+1} + \displaystyle\alpha\sum_{i=1}^{N-1} \bf{M}_i \cdot (s^{l+1}_i*\tau_i^{l+1})
\end{cases}}   
\end{equation}

{The (Dynamic-Merg) system perform two steps which are calculating base expert for the next layer and perform (CA-Merge), respectively. For the first step, we eliminate the score and take the average of curvure-aware domain vector instead to avoid information leakage. The result then takes the role as the base expert for the next layer.} 

{\subsection{Curvature update}
 In the main text, we try to give an explaination of how our method we update the curvature matrix with the curvature information of the parameters space. To achive that, we first take the derivative of equation (CA-Merge) w.r.t the curvature matrix $\bf{M}_i$:}
\begin{equation}
    {\dfrac{\partial \bf{\hat{E}}_m}{\partial \bf{M}_j} 
    =  \left(\alpha s_j * \tau_j \right) 
    = \alpha s_j * (\bf{E}_j - \bf{E}_m)}
\end{equation}
{To evaluate the gradient of the task loss $\mathcal{L}$ w.r.t $\bf{M}_i$ we apply the chain-rule:}
\begin{equation}
    {\dfrac{\partial \mathcal{L}}{\partial \bf{M}_j} 
    = \dfrac{\partial \mathcal{L}}{\partial \bf{\hat{E}}_m} \cdot \dfrac{\partial \bf{\hat{E}}_m}{\partial \bf{M}_j} 
    = \alpha s_j * \dfrac{\partial \mathcal{L}}{\partial \bf{\hat{E}}_m} \cdot (\bf{E}_j - \bf{E}_m)}
\end{equation}

{
\section{Student's t-test for experiments on GLUE dataset} 
\label{t-test}
We report the t-test results, beginning with the null hypothesis $H_0$: \textit{The performance between each pair of T5-Ties-CA vs T5-Ties, and T5 on GLUE SST-2, MRPC, CoLA, and MNLI are the same.}. In this test, we choose the significant value to be 0.05.}
 \begin{table}[htbp]
\centering
{
\caption{{Evaluation results on SST-2 with different random seeds.}}
\begin{tabular}{cccc}
\toprule
\textbf{Index} & \textbf{Ties\_CA} & \textbf{Ties} & \textbf{Vanilla} \\ \midrule
1 & 94.44 & 93.77 & 93.31 \\ 
2 & 94.86 & 94.13 & 93.33 \\ 
3 & 94.62 & 93.90 & 93.21 \\ 
4 & 94.60 & 94.12 & 93.46 \\ 
5 & 94.54 & 93.70 & 93.41 \\ 
6 & 94.55 & 93.87 & 93.56 \\ 
7 & 94.37 & 94.03 & 93.67 \\ \bottomrule
\end{tabular}
}
\label{tab:sst2_results}
\end{table}

\begin{table}[htbp]
\centering
{
\caption{{T-statistic and p-value when evaluating on SST-2.}}
\begin{tabular}{lcc}
\toprule
\textbf{Test} & \textbf{t-statistic} & \textbf{p-value} \\ \midrule
Ties-CA vs Vanilla & 13.72 & 1.08e-8 \\ 
Ties-CA vs Ties    & 7.36  & 8.74e-6 \\ \bottomrule
\end{tabular}
}
\label{tab:t_statistic}
\end{table}
\vspace{1cm}
\begin{table}[htbp]
\centering
\caption{{Evaluation results on MRPC with different random seeds.}}
{\begin{tabular}{lccc}
\toprule
\textbf{Index} & \textbf{Ties\_CA} & \textbf{Ties} & \textbf{Vanilla} \\ \midrule
1 & 92.35 & 91.35 & 89.85 \\ 
2 & 92.61 & 91.30 & 89.65 \\ 
3 & 92.55 & 91.40 & 89.74 \\ 
4 & 92.40 & 91.55 & 89.49 \\ 
5 & 92.54 & 91.62 & 89.76 \\ 
6 & 92.44 & 91.77 & 89.85 \\ 
7 & 92.33 & 91.43 & 89.62 \\ \bottomrule
\end{tabular}}
\label{tab:mrpc_results}
\end{table}


\begin{table}[h!]
\centering
\caption{{T-statistic and p-value when evaluating on MRPC.}}
{\begin{tabular}{lcc}
\toprule
\textbf{Test} & \textbf{t-statistic} & \textbf{p-value} \\ \midrule
Ties-CA vs Vanilla & 42.91 & 1.67e-14 \\ 
Ties-CA vs Ties    & 12.95 & 2.06e-8  \\ \bottomrule
\end{tabular}}
\label{tab:t_statistic_mrpc}
\end{table}

\begin{table}[h!]
\centering
\caption{{Evaluation results on CoLA with different random seeds.}}
{\begin{tabular}{lccc}
\toprule
\textbf{Index} & \textbf{Ties\_CA} & \textbf{Ties} & \textbf{Vanilla} \\ \midrule
1 & 61.01 & 57.95 & 57.74 \\ 
2 & 59.53 & 58.63 & 57.82 \\ 
3 & 60.36 & 58.90 & 58.03 \\ 
4 & 60.13 & 58.92 & 58.23 \\ 
5 & 59.41 & 58.31 & 58.51 \\ 
6 & 59.33 & 57.38 & 57.36 \\ 
7 & 60.03 & 58.53 & 58.40 \\ \bottomrule
\end{tabular}}
\label{tab:cola_results}
\end{table}

\begin{table}[h!]
\centering
\caption{{T-statistic and p-value when evaluating on CoLA.}}
{\begin{tabular}{lcc}
\toprule
\textbf{Test} & \textbf{t-statistic} & \textbf{p-value} \\ \midrule
Ties-CA vs Vanilla & 7.14 & 1.18e-5 \\ 
Ties-CA vs Ties    & 5.16 & 2.00e-4 \\ \bottomrule
\end{tabular}}
\label{tab:t_statistic_cola}
\end{table}

\begin{table}[h!]
\centering
\caption{{Evaluation results on MNLI with different random seeds.}}
{\begin{tabular}{lccc}
\toprule
\textbf{Index} & \textbf{Ties\_CA} & \textbf{Ties} & \textbf{Vanilla} \\ \midrule
1 & 86.52 & 86.25 & 86.22 \\ 
2 & 86.45 & 86.32 & 86.31 \\ 
3 & 86.37 & 86.39 & 86.36 \\ 
4 & 86.59 & 86.46 & 86.41 \\ 
5 & 86.32 & 86.53 & 86.50 \\ 
6 & 86.54 & 86.38 & 86.34 \\ 
7 & 86.47 & 86.41 & 86.34 \\ \bottomrule
\end{tabular}}
\label{tab:mnli_results}
\end{table}

\begin{table}[h!]
\centering
\caption{{T-statistic and p-value when evaluating on MNLI.}}
{\begin{tabular}{lcc}
\toprule
\textbf{Test} & \textbf{t-statistic} & \textbf{p-value} \\ \midrule
Ties-CA vs Vanilla & 2.29 & 0.04 \\ 
Ties-CA vs Ties    & 1.49 & 0.16 \\ \bottomrule
\end{tabular}}
\label{tab:t_statistic_mnli}
\end{table}
\vspace{3cm}
{{Based on the p-values in the tables above, we draw the following conclusions:
\begin{itemize}
    \item The T5-Ties-CA variant significantly outperforms T5-Ties and T5-Vanilla on SST-2, MRPC, and CoLA.
    \item While T5-Ties-CA does not statistically significantly outperform T5-Ties on MNLI, it still demonstrates significant improvement over T5-Vanilla.
\end{itemize}}

{
\section{Additional Experiments}
\subsection{Integrating CAMEx into Twin-Merging}
\label{appendix: twin-merging}
}
{
We expand our experiments to include a broader range of most recent merging expert methods. Specifically, we integrated our CAMEx method with the Twin-Merging approach \citep{lu2024twinmerging}. Key distinctions between CAMEx and Twin-Merging lie in their core mechanisms:
\begin{itemize}
    \item Our method is a non-Euclidean merging method, which utilizes the curvature-aware matrix, whereas Twin-Merging is a model merging method, which relies on Euclidean merging.
    \item Our approach is specifically designed for finetuning, in contrast to Twin-Merging, which is intended for post-training.
    \item Finally, our dynamic mechanism performs inter-layer to form the merged expert, unlike Twin-Merging, which uses within-layer pre-calculations for merging.
To integrate our method with Twin-Merging, we first fine-tune the Curvature Aware model for a specific GLUE task. At test time, we apply the Twin-Merging algorithm to merge experts, referring to our approach as Twin-CA. Notably, we found Twin-Merging to be a simple yet powerful technique that is easy to implement and helps reduce memory usage during inference. We adhere to the original implementation settings, using a sparsity density value of 0.2.
\end{itemize}
\begin{table}[ht]
\centering
\caption{{Performance of Twin-Merging and its Curvature Aware (CA) variant on GLUE tasks.}}
{
\begin{tabular}{lccc}
\toprule
Method & MRPC & RTE & STSB \\
\midrule
Twin-Merging & 91.97 & 72.20 & 88.56 \\
Twin-CA (\textbf{Ours}) & \textbf{92.30} & \textbf{74.73} & \textbf{89.55} \\
\bottomrule
\end{tabular}
\label{tab:twin}
}
\end{table}
The results in Table \ref{tab:twin} demonstrate the effectiveness of our CAMEx approach when integrated with the Twin-Merging mechanism on GLUE tasks, highlighting its strong potential for incorporation into more advanced merging techniques.
}
{
\subsection{Experiments on Token-choice vs Expert-choice routing}
\label{appendix: router}
We also demonstrate our merging approach with the following routing mechanisms. We compare the baseline performance (i.e., the ties merging expert without Curvature Aware) under different routing mechanisms with the corresponding Curvature-Aware counterparts to see how different routing functions affect CAMEx performance. It is worth noting that Expert Choice routing is not compatible with the experts merging method, as discussed by Lory \citep{zhong2024lory} in their Subsection 5.3.: 
\begin{itemize}
    \item Stable MoE routing \citep{dai2022stablemoe}.
    \item Naive routing \citep{shazeer2017outrageously}.
    \item X-MoE routing \citep{chi2022representation}.
\end{itemize}
Note that the Curvature Aware model leverages the segment routing strategy (the causal segmenting strategy) proposed in Lory \citep{zhong2024lory}, enabling a direct comparison between our model and the expert choice method. The results in Table \ref{tab:routers} suggest that Curvature-Aware merging benefit from more advanced routing strategies. The CA model consistently outperforms the baseline with Ties merging across all routing mechanisms. Additionally, we observe that both Naive routing CA and X-MoE routing CA deliver robust performance across GLUE tasks, while Stable MoE routing CA emerges as the most reliable choice overall.
}

\begin{table}[ht]
\centering
\caption{{Performance of T5-base variants on the finetuning GLUE tasks}}
{\begin{tabular}{lcccc}
\toprule
Method & MRPC & RTE & STSB & SST-2 \\
\midrule
Expert Choice MoE & 93.10 & 66.78 & 89.19 & 93.80 \\
\midrule
Stable MoE routing Ties & 91.92 & 75.48 & 89.48 & 93.37 \\
\textbf{Stable MoE routing CA} & 92.96 & 78.76 & 89.64 & 94.63 \\
\midrule
Naive routing Ties & 91.44 & 75.54 & 88.58 & 93.92 \\
\textbf{Naive routing CA} & 92.49 & 78.70 & 89.56 & 94.61 \\
\midrule
X-MoE routing Ties & 91.99 & 75.29 & 88.42 & 93.26 \\
\textbf{X-MoE routing CA} & 92.79 & 78.20 & 89.26 & 94.38 \\
\bottomrule
\end{tabular}}
\label{tab:routers}
\end{table}

{\subsection{Longer training for Wikitext-103 pre-training}
\label{Appendix: longer_wiki}
We conduct additional experiments by training for longer iterations on the Wikitext-103 dataset. The performance gaps between methods remain stable starting around epoch 40. \\}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{plot_wikitext_60.pdf}
    \caption{{Performance of Vannila, Domain-specific, Domain-specific-CA under longer pre-training.}}
    \label{fig:wiki-60}
\end{figure}

{As shown in Figure \ref{fig:wiki-60} the trends demonstrate consistent improvements of our method over the baseline, with the gap remaining significant even after prolonged training.
\subsection{More comprehensive ablation study on hyperparameters}
\subsubsection{Ablation study on $\alpha$}
\label{ablation:alpha}
We extend the range of $\alpha$ for the ablation study, specifically evaluating Dare-CA and Ties-CA with $\alpha \in [0.1, 1.6]$. The evaluation is conducted using 5 different seeds, and the results are averaged.\\}
\begin{figure}[ht]
    \centering
    % First figure
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{RTE-alpha-crop.pdf}
    \end{subfigure}
    % Second figure
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{MRPC-alpha-crop.pdf}
    \end{subfigure}
    % Third figure
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{STSB-alpha-crop.pdf}
    \end{subfigure}
    \vspace{-0.8\baselineskip}
    \caption{{Test performance of Curvature-Aware methods under varying settings of $\alpha$.}}
    \label{fig:more_alpha}
\end{figure}

{The results in Figure \ref{fig:more_alpha} lead to the following observations:
\begin{itemize}
    \item The performance of the models is suboptimal or even worse than the vanilla baseline when $\alpha$ is either too small ($\alpha \in [0.1, 0.4]$) or too large ($\alpha > 1.1$).
    \item Dare-CA is more sensitive to the choice of $\alpha$, showing sharper improvements and declines across the range.
    \item Ties-CA exhibits more gradual changes, suggesting it is more robust to variations in $\alpha$. The optimal range for $\alpha$ is $[0.8,1.0]$. 
\end{itemize}}

{\subsubsection{Ablation study on number of experts}
\label{ablation:nexpeerts}
We conduct additional studies on our method using different numbers of experts in the T5 backbone.\\}

\begin{figure}[ht]
    \centering
    % First figure
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{RTE-num-experts-crop.pdf}
        \label{fig:figure1}
    \end{subfigure}
    % Second figure
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{MRPC-num-experts-crop.pdf}
        \label{fig:figure2}
    \end{subfigure}
    % Third figure
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{STSB-num-experts-crop.pdf}
        \label{fig:figure3}
    \end{subfigure}
    \vspace{-1.5\baselineskip}
    \caption{{Test performance of Curvature-Aware methods under varying number of experts.}}
    \label{fig:more_experts}
\end{figure}

{The following conclusions can be drawn from Figures \ref{fig:more_experts}:
\begin{itemize}
    \item Increasing the number of experts generally improves accuracy up to a certain point: Accuracy improves as the number of experts increases, with the most significant gains occurring from 4 to 8 experts.
    \item After 12 experts, the accuracy either saturates or slightly decreases.
    \item We suggest using eight experts, as it provides a balanced trade-off between performance and efficiency.
\end{itemize}} 

{\subsection{Results on large backbone (Phi-3)}
To evaluate the effectiveness of CAMEx with large backbone, we experiment with Phi-3.}
\begin{table}[ht]
\centering
\caption{{Performance of Phi-3-mini variants on the fine-tuning tasks for the GLUE benchmark.}}
{\begin{tabular}{lccccccccc}
\toprule
Model & Params & SST-2 & MRPC & CoLA & STSB & RTE & QQP & QNLI & MNLI \\
\midrule
Phi-3          & 3.8B & 95.76     & 90.12     & 61.36  & 88.7   & 80.51 & 92.38 & 94.84 & 90.39 \\
Phi3-Ties      & 7.4B & 96.56     & 92.25     & 62.33  & 89.99  & 87.73 & 94.13 & 95.58 & 91.28 \\
Phi3-Ties-CA   & 7.4B & \textbf{97.23} & \textbf{94.04} & \textbf{63.43} & \textbf{90.27} & \textbf{88.09} & \textbf{94.80} & \textbf{95.82} & \textbf{92.13} \\
\bottomrule
\end{tabular}}
\end{table}
{\begin{itemize}
    \item The Ties-CA and Ties variants remarkably outperform the vanilla version, creating a substantial performance gap.
    \item Ties-CA further enhances the performance of Ties in all listed tasks.
\end{itemize}
Thus, we believe that curvature awareness has potential for improving other language models (LMs).}
\end{document}

