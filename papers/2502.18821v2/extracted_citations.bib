@article{Tuan2024Hypertrans,
title = {A Hyper-Transformer model for Controllable Pareto Front Learning with Split Feasibility Constraints},
journal = {Neural Networks},
volume = {179},
pages = {106571},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106571},
author = {Tran Anh Tuan and Nguyen Viet Dung and Tran Ngoc Thang},
}

@InProceedings{Wu_2024_CVPR,
    author    = {Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu},
    title     = {Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14205-14215}
}

@article{amari1998natgrad,
  author={Amari, Shun-ichi},
  journal={Neural Computation}, 
  title={Natural Gradient Works Efficiently in Learning}, 
  year={1998},
  volume={10},
  number={2},
  pages={251-276},
  keywords={},
  doi={10.1162/089976698300017746}

}

@article{cai2023robust,
  title={Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights?},
  author={Cai, Ruisi and Zhang, Zhenyu and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2302.12480},
  year={2023}
}

@article{don2022cold,
  title={Cold fusion: Collaborative descent for distributed multitask finetuning},
  author={Don-Yehiya, Shachar and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
  journal={arXiv preprint arXiv:2212.01378},
  year={2022}
}

@article{fedus2022switch,
  author  = {William Fedus and Barret Zoph and Noam Shazeer},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  url     = {http://jmlr.org/papers/v23/21-0998.html}
}

@inproceedings{he-etal-2023-merging,
    title = "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts",
    author = "He, Shwai  and
      Fan, Run-Ze  and
      Ding, Liang  and
      Shen, Li  and
      Zhou, Tianyi  and
      Tao, Dacheng",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2023.emnlp-main.907",
    pages = "14685--14691",
}

@inproceedings{hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{ilharco2022patching,
  title={Patching open-vocabulary models by interpolating weights},
  author={Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29262--29277},
  year={2022}
}

@article{jin2022dataless,
  title={Dataless knowledge fusion by merging weights of language models},
  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},
  journal={arXiv preprint arXiv:2212.09849},
  year={2022}
}

@inproceedings{lepikhin2021gshard,
    title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@inproceedings{li2024merge,
title={Merge, Then Compress: Demystify Efficient {SM}oE with Hints from Its Routing Policy},
author={Pingzhi Li and Zhenyu Zhang and Prateek Yadav and Yi-Lin Sung and Yu Cheng and Mohit Bansal and Tianlong Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=eFWG9Cy3WK}
}

@misc{liu2024mixturelowrankexpertstransferable,
      title={Mixture of Low-rank Experts for Transferable AI-Generated Image Detection}, 
      author={Zihan Liu and Hanyi Wang and Yaoyu Kang and Shilin Wang},
      year={2024},
      eprint={2404.04883},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.04883}, 
}

@misc{lu2024expertsequalefficientexpert,
      title={Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models}, 
      author={Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li},
      year={2024},
      eprint={2402.14800},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14800}, 
}

@inproceedings{lu2024twinmerging,
title={Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging},
author={Zhenyi Lu and Chenghao Fan and Wei Wei and Xiaoye Qu and Dangyang Chen and Yu Cheng},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=81YIt63TTn}
}

@article{matena2022merging,
  title={Merging models with fisher-weighted averaging},
  author={Matena, Michael S and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17703--17716},
  year={2022}
}

@article{rame2023model,
  title={Model ratatouille: Recycling diverse models for out-of-distribution generalization},
  author={Rame, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, L{\'e}on and Lopez-Paz, David},
  year={2023},
  journal={arXiv preprint arXiv:2212.10445}
}

@article{shazeer2017outrageously,
    title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
    author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
    journal={arXiv preprint arXiv:1701.06538},
    year={2017},
}

@inproceedings{wu2024mixture,
title={Mixture of Lo{RA} Experts},
author={Xun Wu and Shaohan Huang and Furu Wei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uWvKBCYh4S},
}

@inproceedings{yadav2023ties-merging,
      title={Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
    booktitle = "NeurIPS",
    year = "2023",
    address = "New Orleans, USA",
    publisher = "Proceedings of Machine Learning Research",
}

