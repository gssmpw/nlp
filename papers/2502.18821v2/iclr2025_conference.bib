@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 volume = {30},
 year = {2017}
}

@article{Tuan2024Hypertrans,
title = {A Hyper-Transformer model for Controllable Pareto Front Learning with Split Feasibility Constraints},
journal = {Neural Networks},
volume = {179},
pages = {106571},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106571},
author = {Tran Anh Tuan and Nguyen Viet Dung and Tran Ngoc Thang},
}

@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{Dosovitskiy2021AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.11929}
}

@article{Han2020ASO,
  title={A Survey on Visual Transformer},
  author={Kai Han and Yunhe Wang and Hanting Chen and Xinghao Chen and Jianyuan Guo and Zhenhua Liu and Yehui Tang and An Xiao and Chunjing Xu and Yixing Xu and Zhaohui Yang and Yiman Zhang and Dacheng Tao},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.12556}
}

@article{Mao2022SingleFA,
  title={Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model},
  author={Zhiyuan Mao and Ajay Jaiswal and Zhangyang Wang and Stanley H. Chan},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.10040}
}

@article{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      volume={abs/1810.04805},
      journal={ArXiv},
}

@InProceedings{Liu_2021_ICCV,
    author    = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
    title     = {Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {10012-10022}
}

@article{Zheng2021EndtoEndOD,
  title={End-to-End Object Detection with Adaptive Clustering Transformer},
  author={Minghang Zheng and Peng Gao and Renrui Zhang and Xiaogang Wang and Hongsheng Li and Hao Dong},
  journal={ArXiv},
  year={2021},
  volume={abs/2011.09315}
}

@inproceedings{Touvron2021TrainingDI,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herv'e J'egou},
  booktitle={ICML},
  year={2021}
}


@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{he2021fastmoe,
  title={Fastmoe: A fast mixture-of-expert training system},
  author={He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  journal={arXiv preprint arXiv:2103.13262},
  year={2021}
}

@inproceedings{he2022fastermoe,
    author = {He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
    title = {FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models},
    year = {2022},
    isbn = {9781450392044},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3503221.3508418},
    doi = {10.1145/3503221.3508418},
    booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
    pages = {120â€“134},
    numpages = {15},
    keywords = {parallelism, distributed deep learning, performance modeling},
    location = {Seoul, Republic of Korea},
    series = {PPoPP '22}
}

@inproceedings{ortega2022diversity,
  title={Diversity and generalization in neural network ensembles},
  author={Ortega, Luis A and Caba{\~n}as, Rafael and Masegosa, Andres},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={11720--11743},
  year={2022},
  organization={PMLR}
}

@InProceedings{Zhu_2019_CVPR,
author = {Zhu, Shilin and Dong, Xin and Su, Hao},
title = {Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@article{choshen2022fusing,
  title={Fusing finetuned models for better pretraining},
  author={Choshen, Leshem and Venezian, Elad and Slonim, Noam and Katz, Yoav},
  journal={arXiv preprint arXiv:2204.03044},
  year={2022}
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{jin2022dataless,
  title={Dataless knowledge fusion by merging weights of language models},
  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},
  journal={arXiv preprint arXiv:2212.09849},
  year={2022}
}

@article{matena2022merging,
  title={Merging models with fisher-weighted averaging},
  author={Matena, Michael S and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17703--17716},
  year={2022}
}

@article{stoica2023zipit,
      title={ZipIt! Merging Models from Different Tasks without Training}, 
      author={George Stoica and Daniel Bolya and Jakob Bjorner and Taylor Hearn and Judy Hoffman},
      year={2023},
      journal={arXiv preprint arXiv:2305.03053}
}

@article{ilharco2022patching,
  title={Patching open-vocabulary models by interpolating weights},
  author={Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29262--29277},
  year={2022}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@article{cai2023robust,
  title={Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights?},
  author={Cai, Ruisi and Zhang, Zhenyu and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2302.12480},
  year={2023}
}

@article{don2022cold,
  title={Cold fusion: Collaborative descent for distributed multitask finetuning},
  author={Don-Yehiya, Shachar and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
  journal={arXiv preprint arXiv:2212.01378},
  year={2022}
}

@article{rame2023model,
  title={Model ratatouille: Recycling diverse models for out-of-distribution generalization},
  author={Rame, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, L{\'e}on and Lopez-Paz, David},
  year={2023},
  journal={arXiv preprint arXiv:2212.10445}
}

@article{komatsuzaki2022sparse,
  title={Sparse upcycling: Training mixture-of-experts from dense checkpoints},
  author={Komatsuzaki, Aran and Puigcerver, Joan and Lee-Thorp, James and Ruiz, Carlos Riquelme and Mustafa, Basil and Ainslie, Joshua and Tay, Yi and Dehghani, Mostafa and Houlsby, Neil},
  journal={arXiv preprint arXiv:2212.05055},
  year={2022}
}

@article{chen2021dsee,
  title={Dsee: Dually sparsity-embedded efficient tuning of pre-trained language models},
  author={Chen, Xuxi and Chen, Tianlong and Cheng, Yu and Chen, Weizhu and Wang, Zhangyang and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2111.00160},
  year={2021}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}


@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{freeman2016topology,
  title={Topology and geometry of half-rectified network optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={arXiv preprint arXiv:1611.01540},
  year={2016}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ainsworth2022git,
  title={Git re-basin: Merging models modulo permutation symmetries},
  author={Ainsworth, Samuel K and Hayase, Jonathan and Srinivasa, Siddhartha},
  journal={arXiv preprint arXiv:2209.04836},
  year={2022}
}

@article{jordan2022repair,
  title={Repair: Renormalizing permuted activations for interpolation repair},
  author={Jordan, Keller and Sedghi, Hanie and Saukh, Olga and Entezari, Rahim and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2211.08403},
  year={2022}
}

@inproceedings{pena2023re,
  title={Re-basin via implicit Sinkhorn differentiation},
  author={Pe{\~n}a, Fidel A Guerrero and Medeiros, Heitor Rapela and Dubail, Thomas and Aminbeidokhti, Masih and Granger, Eric and Pedersoli, Marco},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20237--20246},
  year={2023}
}

@article{jolicoeur2023population,
  title={PopulAtion Parameter Averaging (PAPA)},
  author={Jolicoeur-Martineau, Alexia and Gervais, Emy and Fatras, Kilian and Zhang, Yan and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2304.03094},
  year={2023}
}

@article{gueta2023knowledge,
  title={Knowledge is a region in weight space for fine-tuned language models},
  author={Gueta, Almog and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
  journal={arXiv preprint arXiv:2302.04863},
  year={2023}
}

@article{shazeer2017outrageously,
    title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
    author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
    journal={arXiv preprint arXiv:1701.06538},
    year={2017},
}

@article{mittal2022modular,
  title={Is a modular architecture enough?},
  author={Mittal, Sarthak and Bengio, Yoshua and Lajoie, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28747--28760},
  year={2022}
}

@inproceedings{lepikhin2021gshard,
    title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@article{fedus2022switch,
  author  = {William Fedus and Barret Zoph and Noam Shazeer},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {120},
  pages   = {1--39},
  url     = {http://jmlr.org/papers/v23/21-0998.html}
}

@article{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      journal={arXiv preprint arXiv:2005.14165},
}

@article{chung2022scaling,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and Albert Webson and Shixiang Shane Gu and Zhuyun Dai and Mirac Suzgun and Xinyun Chen and Aakanksha Chowdhery and Alex Castro-Ros and Marie Pellat and Kevin Robinson and Dasha Valter and Sharan Narang and Gaurav Mishra and Adams Yu and Vincent Zhao and Yanping Huang and Andrew Dai and Hongkun Yu and Slav Petrov and Ed H. Chi and Jeff Dean and Jacob Devlin and Adam Roberts and Denny Zhou and Quoc V. Le and Jason Wei},
      year={2022},
      journal={arXiv preprint arXiv:2210.11416}
}

@article{fan2022m3vit,
  title={M$^3$vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design},
  author={Fan, Zhiwen and Sarkar, Rishov and Jiang, Ziyu and Chen, Tianlong and Zou, Kai and Cheng, Yu and Hao, Cong and Wang, Zhangyang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28441--28457},
  year={2022}
}

@article{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      journal={arXiv preprint arXiv:2001.08361}
}

@misc{yadav2023compeft,
      title={ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization}, 
      author={Prateek Yadav and Leshem Choshen and Colin Raffel and Mohit Bansal},
      year={2023},
      eprint={2311.13171},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{koishekenov2023moepruning,
    title = "Memory-efficient {NLLB}-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model",
    author = "Koishekenov, Yeskendir  and
      Berard, Alexandre  and
      Nikoulina, Vassilina",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.198",
    doi = "10.18653/v1/2023.acl-long.198",
    pages = "3567--3585",
    abstract = "The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that enables the removal of up to 80{\%} of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts.",
}

@article{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      journal={arXiv preprint arXiv:2204.02311}
}


@article{chi2022representation,
  title={On the Representation Collapse of Sparse Mixture of Experts},
  author={Chi, Zewen and Dong, Li and Huang, Shaohan and Dai, Damai and Ma, Shuming and Patra, Barun and Singhal, Saksham and Bajaj, Payal and Song, Xia and Wei, Furu},
  journal={arXiv preprint arXiv:2204.09179},
  year={2022}
}

@article{raffel2020exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      journal={arXiv preprint arXiv:1910.10683}
}

@inproceedings{yadav2023ties-merging,
      title={Resolving Interference When Merging Models}, 
      author={Prateek Yadav and Derek Tam and Leshem Choshen and Colin Raffel and Mohit Bansal},
    booktitle = "NeurIPS",
    year = "2023",
    address = "New Orleans, USA",
    publisher = "Proceedings of Machine Learning Research",
}

@article{rajbhandari2022deepspeedmoe,
      title={DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale}, 
      author={Samyam Rajbhandari and Conglong Li and Zhewei Yao and Minjia Zhang and Reza Yazdani Aminabadi and Ammar Ahmad Awan and Jeff Rasley and Yuxiong He},
      year={2022},
      journal={arXiv preprint arXiv:2201.05596}
}

@article{artetxe2022efficient,
      title={Efficient Large Scale Language Modeling with Mixtures of Experts}, 
      author={Mikel Artetxe and Shruti Bhosale and Naman Goyal and Todor Mihaylov and Myle Ott and Sam Shleifer and Xi Victoria Lin and Jingfei Du and Srinivasan Iyer and Ramakanth Pasunuru and Giri Anantharaman and Xian Li and Shuohui Chen and Halil Akin and Mandeep Baines and Louis Martin and Xing Zhou and Punit Singh Koura and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Mona Diab and Zornitsa Kozareva and Ves Stoyanov},
      year={2022},
      journal={arXiv preprint arXiv:2112.10684},
}

@article{chen2022taskspecific,
      title={Task-Specific Expert Pruning for Sparse Mixture-of-Experts}, 
      author={Tianyu Chen and Shaohan Huang and Yuan Xie and Binxing Jiao and Daxin Jiang and Haoyi Zhou and Jianxin Li and Furu Wei},
      year={2022},
      journal={arXiv preprint arXiv:2206.00277},
}

@article{
muqeeth2023soft,
title={Soft Merging of Experts with Adaptive Routing},
author={Mohammed Muqeeth and Haokun Liu and Colin Raffel},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=7I199lc54z},
note={Featured Certification}
}
@article{wang2023cuttlefish,
      title={Cuttlefish: Low-Rank Model Training without All the Tuning}, 
      author={Hongyi Wang and Saurabh Agarwal and Pongsakorn U-chupala and Yoshiki Tanaka and Eric P. Xing and Dimitris Papailiopoulos},
      year={2023},
      journal={arXiv preprint arXiv:2305.02538}
}

@article{li2023losparse,
      title={LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation}, 
      author={Yixiao Li and Yifan Yu and Qingru Zhang and Chen Liang and Pengcheng He and Weizhu Chen and Tuo Zhao},
      year={2023},
      journal={arXiv preprint arXiv:2306.11222}
}

@inproceedings{socher-etal-2013-recursive-sst2,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1170",
    pages = "1631--1642",
}

@inproceedings{dolan-brockett-2005-automatically-mrpc,
    title = "Automatically Constructing a Corpus of Sentential Paraphrases",
    author = "Dolan, William B.  and
      Brockett, Chris",
    booktitle = "Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)",
    year = "2005",
    url = "https://aclanthology.org/I05-5002",
}

@inproceedings{khashabi-etal-2018-looking-multirc,
    title = "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
    author = "Khashabi, Daniel  and
      Chaturvedi, Snigdha  and
      Roth, Michael  and
      Upadhyay, Shyam  and
      Roth, Dan",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1023",
    doi = "10.18653/v1/N18-1023",
    pages = "252--262"
}

@inproceedings{gordon-etal-2012-semeval-copa,
    title = "{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning",
    author = "Gordon, Andrew  and
      Kozareva, Zornitsa  and
      Roemmele, Melissa",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1052",
    pages = "394--398",
}

@article{sakaguchi2019winogrande,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      journal={arXiv preprint arXiv:1907.10641},
}

@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{yang-etal-2015-wikiqa,
    title = "{W}iki{QA}: A Challenge Dataset for Open-Domain Question Answering",
    author = "Yang, Yi  and
      Yih, Wen-tau  and
      Meek, Christopher",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1237",
    doi = "10.18653/v1/D15-1237",
    pages = "2013--2018",
}

@inproceedings{yang-etal-2018-hotpotqa,
    title = "{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    author = "Yang, Zhilin  and
      Qi, Peng  and
      Zhang, Saizheng  and
      Bengio, Yoshua  and
      Cohen, William  and
      Salakhutdinov, Ruslan  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1259",
    doi = "10.18653/v1/D18-1259",
    pages = "2369--2380"
}

@article{han2016deep,
      title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}, 
      author={Song Han and Huizi Mao and William J. Dally},
      year={2016},
      journal={arXiv preprint arXiv:1510.00149},
}

@article{ma2023llmpruner,
      title={LLM-Pruner: On the Structural Pruning of Large Language Models}, 
      author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
      year={2023},
      journal={arXiv preprint arXiv:2305.11627}
}

@article{nie2022hetumoe,
      title={HetuMoE: An Efficient Trillion-scale Mixture-of-Expert Distributed Training System}, 
      author={Xiaonan Nie and Pinxue Zhao and Xupeng Miao and Tong Zhao and Bin Cui},
      year={2022},
      journal={arXiv preprint arXiv:2203.14685}
}

@article{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      journal={arXiv preprint arXiv:1804.07461}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}

@article{gao2022parameterefficient,
      title={Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models}, 
      author={Ze-Feng Gao and Peiyu Liu and Wayne Xin Zhao and Zhong-Yi Lu and Ji-Rong Wen},
      year={2022},
      journal={arXiv preprint arXiv:2203.01104}
}

@article{kim2021scalable,
      title={Scalable and Efficient MoE Training for Multitask Multilingual Models}, 
      author={Young Jin Kim and Ammar Ahmad Awan and Alexandre Muzio and Andres Felipe Cruz Salinas and Liyang Lu and Amr Hendy and Samyam Rajbhandari and Yuxiong He and Hany Hassan Awadalla},
      year={2021},
      journal={arXiv preprint arXiv:2109.10465}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{Kuhn1955Hungarian,
  added-at = {2011-12-12T19:01:16.000+0100},
  author = {Kuhn, Harold W.},
  biburl = {https://www.bibsonomy.org/bibtex/20bbf339729b509a2836d225e7dd174bb/gergie},
  doi = {10.1002/nav.3800020109},
  file = {:Kuhn1955Hungarian.pdf:PDF},
  groups = {public},
  interhash = {4aaf0e5b3c9a5c33fc97d9a29b5a8f04},
  intrahash = {0bbf339729b509a2836d225e7dd174bb},
  journal = {Naval Research Logistics Quarterly},
  keywords = {},
  month = {March},
  number = {1--2},
  pages = {83--97},
  timestamp = {2011-12-12T19:01:16.000+0100},
  title = {{The Hungarian Method for the Assignment Problem}},
  username = {gergie},
  volume = 2,
  year = 1955
}

@misc{zhang2022platon,
      title={PLATON: Pruning Large Transformer Models with Upper Confidence Bound of Weight Importance}, 
      author={Qingru Zhang and Simiao Zuo and Chen Liang and Alexander Bukharin and Pengcheng He and Weizhu Chen and Tuo Zhao},
      year={2022},
      eprint={2206.12562},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhu2017prune,
      title={To prune, or not to prune: exploring the efficacy of pruning for model compression}, 
      author={Michael Zhu and Suyog Gupta},
      year={2017},
      eprint={1710.01878},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{sanh2020movement,
      title={Movement Pruning: Adaptive Sparsity by Fine-Tuning}, 
      author={Victor Sanh and Thomas Wolf and Alexander M. Rush},
      year={2020},
      eprint={2005.07683},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zafrir2021prune,
      title={Prune Once for All: Sparse Pre-Trained Language Models}, 
      author={Ofir Zafrir and Ariel Larey and Guy Boudoukh and Haihao Shen and Moshe Wasserblat},
      year={2021},
      eprint={2111.05754},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{li2024merge,
title={Merge, Then Compress: Demystify Efficient {SM}oE with Hints from Its Routing Policy},
author={Pingzhi Li and Zhenyu Zhang and Prateek Yadav and Yi-Lin Sung and Yu Cheng and Mohit Bansal and Tianlong Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=eFWG9Cy3WK}
}

@misc{lu2024expertsequalefficientexpert,
      title={Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models}, 
      author={Xudong Lu and Qi Liu and Yuhui Xu and Aojun Zhou and Siyuan Huang and Bo Zhang and Junchi Yan and Hongsheng Li},
      year={2024},
      eprint={2402.14800},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14800}, 
}


@inproceedings{hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}
@inproceedings{wu2024mixture,
title={Mixture of Lo{RA} Experts},
author={Xun Wu and Shaohan Huang and Furu Wei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uWvKBCYh4S},
}


@misc{liu2024mixturelowrankexpertstransferable,
      title={Mixture of Low-rank Experts for Transferable AI-Generated Image Detection}, 
      author={Zihan Liu and Hanyi Wang and Yaoyu Kang and Shilin Wang},
      year={2024},
      eprint={2404.04883},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.04883}, 
}

@inproceedings{he-etal-2023-merging,
    title = "Merging Experts into One: Improving Computational Efficiency of Mixture of Experts",
    author = "He, Shwai  and
      Fan, Run-Ze  and
      Ding, Liang  and
      Shen, Li  and
      Zhou, Tianyi  and
      Tao, Dacheng",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2023.emnlp-main.907",
    pages = "14685--14691",
}

@inproceedings{
zhong2024lory,
title={Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training},
author={Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=LKEJPySnlt}
}


      
@article{amari1998natgrad,
  author={Amari, Shun-ichi},
  journal={Neural Computation}, 
  title={Natural Gradient Works Efficiently in Learning}, 
  year={1998},
  volume={10},
  number={2},
  pages={251-276},
  keywords={},
  doi={10.1162/089976698300017746}

}

@InProceedings{Wu_2024_CVPR,
    author    = {Wu, Jialin and Hu, Xia and Wang, Yaqing and Pang, Bo and Soricut, Radu},
    title     = {Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {14205-14215}
}

@inproceedings{Hoffmann2024scaling,
author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack W. and Sifre, Laurent},
title = {Training compute-optimal large language models},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2176},
numpages = {15},
location = {New Orleans, LA, USA},
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@inproceedings{dai2022stablemoe,
    title = "{S}table{M}o{E}: Stable Routing Strategy for Mixture of Experts",
    author = "Dai, Damai  and
      Dong, Li  and
      Ma, Shuming  and
      Zheng, Bo  and
      Sui, Zhifang  and
      Chang, Baobao  and
      Wei, Furu",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2022.acl-long.489",
    pages = "7085--7095",
}

@inproceedings{
chen2023sparse,
title={Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers},
author={Tianlong Chen and Zhenyu Zhang and AJAY KUMAR JAISWAL and Shiwei Liu and Zhangyang Wang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=w1hwFUb_81}
}
@inproceedings{
zhou2022mixtureofexperts,
title={Mixture-of-Experts with Expert Choice Routing},
author={Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Y Zhao and Andrew M. Dai and Zhifeng Chen and Quoc V Le and James Laudon},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=jdJo1HIVinI}
}

@misc{dai2024deepseekmoe,
      title={DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models}, 
      author={Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
      year={2024},
      eprint={2401.06066},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06066}, 
}

@article{Martens2020,
author = {Martens, James},
title = {New insights and perspectives on the natural gradient method},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {146},
numpages = {76},
}
@INPROCEEDINGS{amari1998whynat,
  author={Amari, S. and Douglas, S.C.},
  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)}, 
  title={Why natural gradient?}, 
  year={1998},
  volume={2},
  number={},
  pages={1213-1216 vol.2},
  keywords={Cost function;Parameter estimation;Newton method;Iterative methods;Convergence;Information systems;Cities and towns;Geometry;Optimization methods;Adaptive filters},
  doi={10.1109/ICASSP.1998.675489}}
@inproceedings{jin2023dataless,
title={Dataless Knowledge Fusion by Merging Weights of Language Models},
author={Xisen Jin and Xiang Ren and Daniel Preotiuc-Pietro and Pengxiang Cheng},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=FCnohuR6AnM}
}

@inproceedings{park2019meta,
 author = {Park, Eunbyung and Oliva, Junier B},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Meta-Curvature},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf},
 volume = {32},
 year = {2019}
}
@inproceedings{lu2024twinmerging,
title={Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging},
author={Zhenyi Lu and Chenghao Fan and Wei Wei and Xiaoye Qu and Dangyang Chen and Yu Cheng},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=81YIt63TTn}
}

@inproceedings{yu2024language,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{ilharco2023editing,
title={Editing models with task arithmetic},
author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=6t0Kwf8-jrj}
}

@ARTICLE{Jacobs1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@article{Hameed_Tahaei_Mosleh_Partovi_Nia_2022, 
    title={Convolutional Neural Network Compression through Generalized Kronecker Product Decomposition}, 
    volume={36}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/19958}, 
    DOI={10.1609/aaai.v36i1.19958}, 
    number={1}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Hameed, Marawan Gamal Abdel and Tahaei, Marzieh S. and Mosleh, Ali and Partovi Nia, Vahid}, 
    year={2022}, 
    month={Jun.}, 
    pages={771-779} }

@inproceedings{Martens15,
author = {Martens, James and Grosse, Roger},
title = {Optimizing neural networks with Kronecker-factored approximate curvature},
year = {2015},
publisher = {JMLR.org},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {2408â€“2417},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@article{merity2016pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@inproceedings{deng2009imagenet,
  title={ImageNet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{liu2021Swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@inproceedings{williams2018mnli,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={1112--1122},
  year={2018}
}

@misc{quora2017qqp,
  title={Quora Question Pairs},
  author={{Quora}},
  note={\url{https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs}},
  year={2017}
}

@inproceedings{dolan2005mrpc,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005}
}

@inproceedings{warstadt2019cola,
  title={Neural network acceptability judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  booktitle={Proceedings of the 33rd AAAI Conference on Artificial Intelligence},
  pages={6732--6739},
  year={2019}
}

@inproceedings{cer2017stsb,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
}

@inproceedings{dagan2006rte,
  title={The PASCAL Recognizing Textual Entailment Challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Proceedings of the First PASCAL Challenges Workshop on Recognising Textual Entailment},
  year={2006}
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15262--15271},
  year={2021}
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}

@inproceedings{DBLP:conf/iclr/MerityX0S17,
  author    = {Stephen Merity and
               Caiming Xiong and
               James Bradbury and
               Richard Socher},
  title     = {Pointer Sentinel Mixture Models},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Byj72udxe},
  timestamp = {Thu, 25 Jul 2019 14:25:57 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MerityX0S17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
teo2024momentumsmoe,
title={Momentum{SM}oE: Integrating Momentum into Sparse Mixture of Experts},
author={Rachel Teo and Tan Minh Nguyen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=y929esCZNJ}
}

@inproceedings{
nielsen2025tight,
title={Tight Clusters Make Specialized Experts},
author={Stefan Nielsen and Rachel Teo and Laziz Abdullaev and Tan Minh Nguyen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=Pu3c0209cx}
}

@inproceedings{
teo2025molex,
title={Mo{LE}x: Mixture of Layer Experts for Fine-tuning with Sparse Upcycling},
author={Rachel Teo and Tan Minh Nguyen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=rWui9vLhOc}
}

@inproceedings{
nguyen2023a,
title={A Primal-Dual Framework for Transformers and Neural Networks},
author={Tan Minh Nguyen and Tam Minh Nguyen and Nhat Ho and Andrea L. Bertozzi and Richard Baraniuk and Stanley Osher},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=U_T8-5hClV}
}

@article{nguyen2022fourierformer,
  title={Fourierformer: Transformer meets generalized fourier integral theorem},
  author={Nguyen, Tan and Pham, Minh and Nguyen, Tam and Nguyen, Khai and Osher, Stanley and Ho, Nhat},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29319--29335},
  year={2022}
}

@inproceedings{kingma2014adam,
title={Adam: A method for stochastic optimization},
author={Diederik P Kingma and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2015}
}

@article{nguyen2020momentumrnn,
  title={Momentumrnn: Integrating momentum into recurrent neural networks},
  author={Nguyen, Tan and Baraniuk, Richard and Bertozzi, Andrea and Osher, Stanley and Wang, Bao},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1924--1936},
  year={2020}
}

@article{nguyen2022improving,
  title={Improving neural ordinary differential equations with nesterov's accelerated gradient method},
  author={Nguyen, Ho Huu Nghia and Nguyen, Tan and Vo, Huyen and Osher, Stanley and Vo, Thieu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7712--7726},
  year={2022}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{wang2022does,
  title={How does momentum benefit deep neural networks architecture design? a few case studies},
  author={Wang, Bao and Xia, Hedi and Nguyen, Tan and Osher, Stanley},
  journal={Research in the Mathematical Sciences},
  volume={9},
  number={3},
  pages={57},
  year={2022},
  publisher={Springer}
}