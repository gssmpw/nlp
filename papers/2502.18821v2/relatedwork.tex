\section{Related Work}
\textbf{Sparse Mixture-of-Experts (SMoE).} As the demand for model scaling grows increasingly widespread, there is a pressing inquiry into efficient ways to optimize computing costs while minimizing the impact on model performance. To address this need, Sparse Mixture of Experts (SMoE) has emerged and undergone extensive research and exploration~\citep{shazeer2017outrageously,lepikhin2021gshard,fedus2022switch}. Starting with \citep{shazeer2017outrageously}, the integration of SMoE into transformer architectures followed shortly after with the works of \citep{lepikhin2021gshard} and \citep{fedus2022switch}. The principle of SMoE is based on a simple concept: scaling the horizontal dimension of models (i.e., the number of feedforward blocks) rather than the vertical dimension (i.e., the number of stacked layers). This allows the model to selectively activate units or parameters based on the input tokens, thereby optimizing resource usage while maintaining performance.

\textbf{SMoE Efficiency Bottlenecks and Emerging Solutions.} While it remains controversial whether to use Top-1 or Top-K routing, some research has highlighted the potential performance gains from increasing the number of activated experts~\citep{shazeer2017outrageously,chen2023sparse}. Other studies have found redundancies among experts in MoE layers~\citep{li2024merge, lu2024expertsequalefficientexpert}. Additionally, some work has proposed using low-rank experts~\citep{wu2024mixture,liu2024mixturelowrankexpertstransferable,Wu_2024_CVPR} inspired by LoRA~\citep{hu2022lora}. Despite the varying research directions, these studies consistently show that training a robust SMoE requires substantial computational and memory resources. This has motivated researchers such as \citep{li2024merge}, \citep{he-etal-2023-merging}, and \citep{zhong2024lory} to merge experts within each MoE layer, reducing the number of experts to a single one and significantly improving training and inference efficiency.


\textbf{Model Merging with curvature-aware.} Though numerous methods for merging models have been introduced and developed~\citep{yadav2023ties-merging,cai2023robust,ilharco2022patching,matena2022merging,jin2022dataless,don2022cold,rame2023model, Tuan2024Hypertrans,  lu2024twinmerging}, most of these works consider merging protocols in the Euclidean parameter space. However, it has been noted that the space of deep neural network models is a Riemannian one~\citep{amari1998natgrad}. \citep{matena2022merging} and \citep{jin2022dataless} were the first to fuse model weights while accounting for the Fisher Information. Despite their promising results, these methods require massive computation to approximate the inversion of the Fisher matrix. Moreover, the Fisher matrix has a size proportional to the dimension of the model parameters, which significantly increases memory usage. Consequently, these methods are challenging for directly integrating into SMoE layers to fuse expert weights.