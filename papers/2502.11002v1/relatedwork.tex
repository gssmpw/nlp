\section{Related Work}
\label{sec:relatedwork}

\subsection{Dual-Pixel Sensors}
\label{subsec:dualpixelsensor}

Fig.~\ref{fig:dualpixelsensor} shows the image formation in a traditional camera sensor and a DP sensor. A DP sensor contains two photodiodes at each pixel, allowing the pixel reading to split into two halves. As a result, any points which do not lie on the focal plane get distributed across multiple pixels, and exhibit disparity in DP left and right images. Considering the DP left image as a reference, the disparity direction of a point depends on whether the point lies in front (\textcolor{yellow}{yellow} in Fig.~\ref{fig:dualpixelsensor}) or behind (\textcolor{green}{green} in Fig.~\ref{fig:dualpixelsensor}) the focal plane. The amount of disparity of a point depends on the number of sensor pixels it gets spread. This point spread function (PSF) depends on the aperture size and the distance of the point from the focal plane. The defocus disparity information in DP images is a valuable cue to determine the deblurring required for a given pixel or image region. The reader is encouraged to refer to \cite{dpdnet_eccv2020,rdpd_iccv2021} for a detailed technical discussion on DP image formation.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.42]{images/dp_sensor}
	\caption{DP sensor image formation.}
	\label{fig:dualpixelsensor}
	\vspace{-14pt}
\end{figure}

\subsection{Defocus Deblurring}
\label{subsec:defocusdeblurmethods}

The defocus deblurring methods can be classified into two categories in the literature. The methods in the first category adopt a two-stage approach. The first stage estimates a defocus map, and the second uses the defocus map to perform non-blind deconvolution to restore the all-in-focus image. The methods in the second category adopt an end-to-end learning-based approach to generate deblurred output images directly. Representative methods from the first category include \cite{jnb_cvpr2015,ebdb_tip2017,dme_cvpr2019}. \cite{ebdb_tip2017} used image gradients to compute the blur difference between the original and re-blurred images. \cite{dme_cvpr2019} proposed a deep learning based approach for defocus map estimation.

Recently, Abuolaim \textit{et al.} \cite{dpdnet_eccv2020} proposed and performed DP based defocused deblurring for the first time. They adopt a UNet\cite{unet_miccai2015} style encoder-decoder network, which takes the concatenated DP left and right images as input. Abuolaim \textit{et al.} \cite{rdpd_iccv2021} recently also proposed a method to generate a realistic synthetic DP dataset to solve misalignment issues in real-world DP dataset in \cite{dpdnet_eccv2020}. Apart from these methods, a recent method \cite{single_defocus_deblur_wacv2022} adopts a multi-task learning approach using three decoders. The three decoders learn to estimate the left, right, and all-in-focus images, respectively. Lee \textit{et al.} \cite{iter_filter_adapt_cvpr2021} incorporate an auxiliary DP view supervision based disparity estimation task for improving the performance of the main defocus deblurring task.

In contrast to the existing methods, we focus on incorporating an explicit cross-correlation within the network architecture to guide the network about the amount of deblurring required for a given image region.