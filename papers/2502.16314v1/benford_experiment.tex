\documentclass[12pt]{article}

% Packages for math symbols and fonts
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathrsfs}
\usepackage{bm}

% Additional packages for better typography and layout
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{epigraph}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Custom theorem environments (optional)
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

% Custom commands (optional)
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

% Using the bibliography file
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{references.bib}

% Hyphenate URLs in the bibliogrpahy / references 
\setcounter{biburllcpenalty}{9000}

% Title and author information
\title{\vspace{-2cm}Optimality and Renormalization \\ imply Statistical Laws}

\author{Alexander Kolpakov \\ \href{akolpakov@uaustin.org}{akolpakov@uaustin.org} 
   \and Aidan Rocke \\ \href{rockeaidan@gmail.com}{rockeaidan@gmail.com} }


\date{\today}

% Page layout settings
\usepackage[a4paper, margin=2.5cm]{geometry}

% Typography settings
\setlength{\parindent}{0em}  % No paragraph indentation
\setlength{\parskip}{1ex}    % Add some space between paragraphs

% Fonts for text and math
\usepackage{charter}         % For text font (Charter)
%\usepackage[charter]{newtxmath}  % For math font (Math Design Charter)

% Begin the document
\begin{document}

\maketitle

\begin{abstract}
Benford's Law is an important instance of experimental mathematics that appears to constrain the information--theoretic behavior of numbers. Elias' encoding for integers is a remarkable approach to universality and optimality of codes. In the present analysis we seek to deduce a general law and its particular implications for these two cases from optimality and renormalization as applied to information--theoretical functionals. Both theoretical and experimental results corroborate our conclusions. 
\end{abstract}

\section{The Information Theory underlying Benford's Law}

\subsection{A probabilistic model for binary numbers}\label{proba}

Herein, we present a Maximum Entropy formulation of Benford's Law. We shall assume that a large integer may be represented as an unbounded binary code of $0$s and $1$s which is limited by the introduction of a stop symbol $S$. We shall also observe a threshold phenomenon associated to Benford's Law applicability to number so generated. 

Let a binary string $\beta$ be produced by the following probabilistic program:

\begin{itemize}
    \item[(1)] $\beta=``0"$ with probability $p_0$, or $\beta=``1"$ with probability $p_1$, otherwise;
    \item[(2)] For codes of length $\lvert \beta \rvert > 1$, set $\beta=``1"\gamma$, where $\gamma$ is generated by spawning '0' with probability $p_0$, $``1"$ with probability $p_1$, and $S$ with probability $p_S$. The generation of $\gamma$ is abandoned if $S$ is obtained and $S$ is not explicitly included at the end of the code.
\end{itemize}

Here we assume that the event of spawning $``0"$, $``1"$ and $``S"$ are independent and the probabilities satisfy 
\begin{equation}
p_0 + p_1 + p_S = 1.
\end{equation}

There are no additional assumptions besides the equality $p_0 = p_1$ due to the apparent symmetry. 

\subsection{Large deviations}\label{dev}

Given the above probabilistic program, any binary number with $k>1$ bits may be expressed as a code $\beta$ starting with $``1"$ and continued with $k$ random symbols until $S$ is generated. Thus, the probability of $\beta$ is given by the geometric distribution 
\begin{equation}
P(k) = (1-p_S)^{k-1}\cdot p_S,
\end{equation}

and the expected length of $\beta$ is given by 
\begin{equation}
\mathbb{E}[|\beta|] = 1 + \mathbb{E}[k-1] = \mathbb{E}[k] = \frac{1}{p_S}.
\end{equation}

The order of magnitude $\Omega(k)$ of a binary number with $k\geq 1$ bits is $2^{k-1}$, if we consider as such the largest value it may take. It follows that 
\begin{equation}
\mathbb{E}[\Omega] = \sum_{k=1}^\infty \Omega(k) \cdot P(k) = p_S \cdot \sum_{k=0}^\infty 2^k \cdot (1-p_S)^k = 
\begin{cases}
\frac{p_S}{2p_S-1}, & \text{if $p_S > \frac{1}{2}$},\\
\infty, & \text{$p_S \leq \frac{1}{2}$}.
\end{cases}
\end{equation}

While $\mathbb{E}[\Omega]$ diverges due to large deviations, we may estimate the typical stopping time and hence the typical order of magnitude using the geometric mean: 
\begin{equation}
\mathbb{E}[\log_2 \Omega] = \mathbb{E}[|\beta|] - 1 \sim \frac{1}{p_S}.
\end{equation}

It follows that the typical number $\beta$ is on the scale of 
\begin{equation}
\chi = 2^{\mathbb{E}[\log_2 \Omega]} \sim 2^{\frac{1}{p_S}}.
\end{equation}
which we call the characteristic scale. 

\subsection{From Maximum Entropy to Benford's Law}\label{ben}

Using the Principle of Maximum Entropy, we would like to constrain the model parameters so that the numbers are distributed approximately uniformly up to binary length $N$.  

Hence, given the cumulative distribution function of $k=|\beta|-1$
\begin{equation}
F(k)=1-(1-p_S)^k,
\end{equation}

we apply the constraint 
\begin{equation}
F(k) \approx \frac{k}{N},
\end{equation}
for $0 \leq k \leq N$. 

As we may only satisfy this constraint on average, we have 

\begin{equation}
(1-p_S)^N = \frac{1}{N} \sum_{k=1}^N (1-p_S)^N = \frac{1}{N} \sum_{k=1}^N \big(1-\frac{k}{N}\big)^{N/k} \approx \int_{0}^1 (1-u)^{\frac{1}{u}} du,
\end{equation}
for large $N \gg 1$. 

Now, we may define the auxiliary parameter 
\begin{equation}
\lambda = \int_{0}^1 (1-u)^{\frac{1}{u}} du \approx 0.23075...,
\end{equation}

so that
\begin{equation}
p_S = 1- \lambda^{1/N},
\end{equation}
where $p_S \rightarrow 0$ as $N \to \infty$. 

On the other hand, we find: 
\begin{equation}
p_0 = p_1 = \frac{\lambda^{1/N}}{2} = p_{\lambda, N},
\end{equation}
where $p_{\lambda, N} \rightarrow \frac{1}{2}$ as $N \to \infty$. 

Finally, we may deduce the cumulative distribution

\begin{equation}
F(k) = 1- \lambda^{k/N} \approx \frac{k}{N} \cdot \ln \frac{1}{\lambda}
\end{equation}

using the first term in the expansion 
\begin{equation}
x^{k/N} = \sum_{n=0}^\infty \frac{k^n (\frac{\ln x}{N})^n}{n!},
\end{equation}
as higher order terms become negligible for $N \gg 1$.

Within each order of magnitude $k$ all strings have the same probability 
\begin{equation}\label{eq:inside_order_of_mag}
p(k, \lambda, N) = p_{\lambda, N}^{k-1},
\end{equation}
to be generated, and thus for any fixed $k$ the distribution is uniform.   

Since
\begin{equation}\label{eq:orders_of_mag}
P(k) = F(k) - F(k-1) = \frac{1}{N}\cdot \ln\frac{1}{\lambda}
\end{equation}
does not depend on $k$, the distribution is uniform (up to a multiple) throughout all orders of magnitude $1 \leq k \leq N$. 

In other words, if $X$ is generated by our model, then $\log_2(X)$ is distributed roughly uniformly. In fact, the order of magnitude $k = \lfloor \log_2(X) \rfloor$ is distributed uniformly because of \eqref{eq:orders_of_mag}, and within each order of magnitude $\{ \log_2(X) \}$ tends to the uniform distribution because we have a large and large amount of numbers generated each with the same probability \eqref{eq:inside_order_of_mag}. 

Then, 
\begin{equation}
    d \cdot 10^m \leq X < (d+1)\cdot 10^m
\end{equation}
is equivalent to 
\begin{equation}
    \log_2 d + m\cdot \log_2 10 \leq \log_2 X < \log_2(d+1) + m\cdot \log_2 10,
\end{equation}
which, by the above reasoning, has probability proportional to the length of the interval:
\begin{equation}
    P(d) \sim \log_2\left( 1 + \frac{1}{d} \right).
\end{equation}
This is, after an appropriate normalization, equivalent to Benford's Law. It is also important to point out that the arguments above are largely heuristic, and not entirely rigorous. However, our numerical experiments suggest that the model's behavior is largely as expected.

\section{Numerical experiments}\label{exp}

We perform some experiments to support our conclusions: the corresponding Python code is available on Github \cite{github-benford}. First, we generate binary strings as prescribed by our model, convert them to decimals, and take the leading digit. In each case we generate $10000$ binary strings with $N = 64$ (which is the largest binary length for Benford's law to hold \textit{a priori}, see Section~\ref{ben}) and $\lambda \in \{10^{-12}, 10^{-6}, 0.1, 0.230759776818,$ $0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\}$. 

It turns out that 
\[
\lambda_* = \int^1_0 (1-u)^{1/u}\,du \approx 0.230759776818\ldots 
\]
relates to a threshold effect, see Figure~\ref{fig:lambda}. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{pics/tvd_vs_lambda.png}
    \caption{TVD from the model distribution of first significant digits to Benford's law. The model parameter $\lambda$ responsible for the halting probability is on the horizontal axis: $\lambda = 10^{-12}, 10^{-6}, 0.1, 0.23076 (\approx \lambda_*),$ $0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9$. Another parameter is set to $N=64$ throughout. The plateau starts at $\lambda = \lambda_*$. }
    \label{fig:lambda}
\end{figure}

Namely, for $\lambda < \lambda_*$ we do not observe that the numbers generated conform very strictly to Benford's law (though they do compare to the real--life dataset we use. However, for $\lambda \geq \lambda_*$ we see that the total variation distance (TVD) drops down significantly (and apparently plateaus). 

Second, we choose the dataset of city populations available from \cite{city-data}. However, this dataset turns out not to follow Benford's law very closely:  the TVD between the empirical distribution and Benford's law in this case is about $0.279507$. 

The interesting observation is that were the dataset modeled by our approach, it would have halting probability $p_S = 0.054689$, as the average binary length equals $18.285235$. The maximum binary length of the dataset is $N=25$.

If we use the ternary model with $p_S = 0.054689$, we get a close average binary length, as expected. However, the maximum binary length is much larger $N > 200$. In this case, the TVD between the model distribution and Benford's law is about $0.02287$, which is a much closer match.

\section{Non--ergodicity of large partitions}\label{kafri}

The following analysis is based off  Oded Kafri's statistical mechanical model for Benford's Law, though it elucidates a diffeent aspect and also aims to correct some mathematical inconsistencies of the original argument provided in \cite{Kafri2009EntropyPI}.

An $N$--digit number in base $\Omega \in \mathbb{N}$ may be modeled as a random vector with $N$ components where each component has value between $0$ and $\Omega - 1$.
 
Let us consider the function $\phi(\sigma)$ that counts the number of components of $\vec{X}_N$ that equal $\sigma$. Also, we have
\begin{equation}
    \frac{1}{N} \sum^{\Omega-1}_{\sigma=0} \phi(\sigma) = 1, 
\end{equation} 
which means that $\phi$ plays the role of a counting measure. 

Let the ``average digit'' of  $\vec{X}_N$ be
\begin{equation}
    \mu = \frac{1}{N} \sum^{\Omega-1}_{i=0} \sigma \cdot \phi(\sigma). 
\end{equation} 

Then, at time step $0 \leq i \leq N$, we scale the average back by $i/N$ and get ``on average'' $\phi(i \cdot \mu/N)$ components. Here, $\phi$ should be understood as a distribution over the possible component values on a continuous rather than discrete range. This can be achieved by looking at $\phi$ as a generalized function, or an interpolant over a discrete set of values. 

Then the time average of digits of $\vec{X}_N$ is
\begin{equation}
    \frac{1}{N} \sum^N_{i=0} \mu \cdot \phi\left( \frac{i\cdot \mu}{N} \right) \rightarrow \int^\mu_0 \phi(t) dt.
\end{equation}

If we assume that $\vec{X}_N$ is generated in ergodic manner, then the spatial and temporal averages of our dynamical system would coincide. Setting the time average equal to the spatial average, as ergodicity would imply via Birkhoff's theorem, results in the trivial solution $\phi(\mu) = 1$. Thus, we must essentially employ the \textit{non--ergodicity} of the process. 

Let us constrain first the spatial entropy of our sequence:
\begin{equation}
    \sum^{\Omega-1}_{i=0} \sigma \cdot \phi(\sigma) = \mu N = \mathcal{E} = const.
\end{equation}

To estimate the number of vectors $\vec{X}_N$ with entropy constraint $\mathcal{E}$, it suffices to consider the combinatorial expression: 
\begin{equation}
\Lambda(N, \mathcal{E}) = \frac{(N+\mathcal{E}-1)!}{(N-1)! \mathcal{E}!}.
\end{equation}

By using Stirling's approximation, we find
\begin{equation}\label{entropy:simplified}
\ln \Lambda \approx N \big((1+\mu)\cdot \ln (1+\mu) - \mu \cdot \ln \mu \big).
\end{equation}

However, the temporal entropy should also be constrained in some way. Here, however, we posit that we can only constrain the time average of digits. This means that the entropy rate is limited: the generation of each next digit has bounded entropy, while the whole process is open--ended and unbounded in time. 

Thus, we have
\begin{equation}
    \int^\mu_0 \phi(t) dt = \varepsilon = const.
\end{equation}

Now, to find $\phi(\mu)$ that maximizes entropy we may specify a Lagrangian in terms of the entropy $\ln \Lambda (\mu)$ and the marginal entropy constraint $\mathcal{E}$:

\begin{equation}
\mathcal{L}(\mu) = \ln \Lambda - \beta \cdot \left(\int^\mu_0 \phi(t) dt - \varepsilon\right).
\end{equation}

By using \eqref{entropy:simplified} we find
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \mu} = 0 \implies \phi(\mu) = \frac{N}{\beta}\ln \big(1+ \frac{1}{\mu}\big),
\end{equation}

as well as
\begin{equation}
\langle \phi \rangle = \frac{1}{N} \sum_{\mu=0}^{\Omega-1} \phi(\mu) = \frac{1}{\beta} \ln \Omega.
\end{equation} 

As the probability of the first digit ought to be normalized in order to be dimensionless, we find 
\begin{equation}
P(n) = \frac{\phi(n)}{\sum_{n=1}^{\Omega-1} \phi(n)} = \frac{\ln \left(1+\frac{1}{n}\right)}{\ln \Omega} = \log_{\Omega} \left(1+\frac{1}{n}\right),
\end{equation}
which is equivalent to Benford's Law. 

\section{Optimal codes from Lagrangians and Renormalization}

\subsection{Lagrangian Optimization}

Some interesting and practically common statistical laws appear to follow as optimal solutions of Lagrangians combining together the Minimal Entropy Principle with various combinatorial or physical constraints. One notable example is Visser's derivation of Zipf's in \cite{Visser2013}. 

Let us consider codes as probability spaces of codewords $\mathcal{C}$, with probability being a generalized function. This is done for the sake of convenience more than for any other purpose, and one may recast all arguments in the discrete case as well. 

For us, the codewords will be numbers, not necessarily naturals, drawn from the space $\mathcal{C} = (0, +\infty)$. Given a code with probability $p(x)$ for a codeword $x$ of length $\ell(x)$, its expected word length is
\begin{equation}
    \mathbb{E}[\ell] = \int_\mathcal{C} p(x) \ell(x) dx. 
\end{equation}

The Kraft--McMillan inequality can as well be recast in the continuous form:
\begin{equation}
    \int_\mathcal{C} \exp(-\ell(x)) dx \leq 1.
\end{equation}

We shall add this condition as a constraint to the Lagrangian below:
\begin{equation}
    \mathcal{L}(\ell) = \int_\mathcal{C} p(x) \ell(x) dx - \lambda \left( 1 - \int_\mathcal{C} \exp(-\ell(x)) dx \right) \to \min,
\end{equation}
with $\lambda > 0$. This implies that violating the Kraft--McMillan inequality would imply a penalty on the Lagrangian being minimized. 

Taking the functional derivative gives
\begin{equation}
    \frac{\delta \mathcal{L}}{\delta \ell} = p(x) - \lambda \exp(-\ell(x)) = 0,
\end{equation}
which is equivalent to $p(x)$ being the code's implied probability
\begin{equation}
    p(x) = \lambda \exp(-\ell(x)),
\end{equation}
for some $\lambda > 0$.

It follows from the complementary slackness that the code so derived has to saturate the Kraft inequality, and thus be complete. Indeed, we assume that $\lambda \neq 0$, so that the equality should take place in the corresponding constraint. 

Let us define the renormalization operator $\mathcal{R}$ for probabilities aligned with exponentiation. Namely, we want to be able to renormalize for $\exp(x)$ if we know the probabilities for $x$. This, on the surface, does not imply the universality property shown in \cite{Elias1975}, but rather guarantees that the conditional distribution is the same over all orders of magnitude.

Thus, put
\begin{equation}
    \mathcal{R} p(\exp(x)) = p(x) \cdot \exp(-x), 
\end{equation}
with the obvious fixed point given by a distribution $p(x)$ satisfying
\begin{equation}
    p(\exp(x)) = p(x) \cdot \exp(-x).
\end{equation}

Note that the above can also be rewritten as a statement about the code length instead:
\begin{equation}
    \ell(\exp(x)) = \ell(x) + x.
\end{equation}
The above, together with the natural constraint $\ell(x) \geq 0$, implies that $\ell(x)$ is given by the sum of iterated logarithms $\log(\log(\ldots\log(x)\ldots)$ that continues until we obtain a negative number.

This implies that 
\begin{equation}
    \ell(x) = \ln x + \ln(\ln(x))) + o(\ln(\ln(x))))
\end{equation}
which in its turn yields 
\begin{equation}
    \mathbb{E}[\ell] = \int_\mathcal{C} P(x) \ell(x)\, dx = \int_\mathcal{C} P(x) \ln x\, dx + O(1) \leq H(P) + O(1),
\end{equation}
where the last step is Wyner's inequality. Now, this is asymptotic optimality. 

Above, $P$ is any probability distribution on $\mathcal{C}$, and
\begin{equation}
    H(P) = \int_\mathcal{C} P(x) \ln P(x) \, dx
\end{equation}
is its Shannon entropy. 

Naturally, if we now recast our observation into the discrete setting, Elias' Omega Coding \cite{Elias1975} will provide a practical realization with very close properties. 

\subsection{Elias' encoding}

Among several possible encoding procedures for integers, in his paper \cite{Elias1975} Elias introduces the so--called Omega Encoding. This is a prefix code obtained by a simple iterative procedure below. Let $s = \text{bin}(x)$ be the binary string representation of a decimal number $x$, and let $x = \text{dec}(s)$ be the decimal value corresponding to a binary string $s$.

\begin{algorithm}[H]
\caption{Elias' Omega: Encoding }
\label{alg:omega-encoding}
\begin{algorithmic}[1]
\Require A positive integer $N \geq 1$
\Ensure Encoded binary string for $N$

\State Initialize an empty code string $C = \text{""}$
\State Append a "0" to the end of $C$: $C \gets C + "0"$ \Comment{Termination marker}
\While{$N > 1$}
    \State Prepend the binary representation of $N$ to $C$: $C \gets \text{bin}(N) + C$
    \State Set $N \gets \text{length}(\text{bin}(N)) - 1$ \Comment{Number of bits in $\text{bin}(N)$, minus one}
\EndWhile
\State \Return $C$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Elias' Omega: Decoding}
\label{alg:omega-decoding}
\begin{algorithmic}[1]
\Require Encoded binary string $S$
\Ensure Decoded positive integer $N$

\State Initialize $N \gets 1$
\State Initialize a pointer $i \gets 1$ (to track the current position in $S$)
\While{$S[i] \neq 0$}
\State Read the next $N + 1$ bits from $S$ starting at position $i$ 
\State $N \gets \text{dec}(S[i : i + N])$
\State Move the pointer $i \gets i + N + 1$
\EndWhile
\State \Return $N$
\end{algorithmic}
\end{algorithm}

In the previous section, we mentioned that Elias' Omega encoding has to be complete. Indeed, let $L(n)$ be the length of $n$ in Elias' Omega encoding, and let $\beta(n) = \lfloor \log_2 n \rfloor + 1$ be the binary length of $n$. Then we have 
\begin{equation}
    L(n) = L(\beta(n)-1) + \beta(n).
\end{equation}

Let also
\begin{equation}
    I_k = \{ n \in \mathbb{N} \,|\, \beta(n) = k  \} = \{ n \in \mathbb{N} \,|\, 2^{k-1} \leq n \leq 2^k - 1\}. 
\end{equation}

We have that $I_k$ contains $2^{k-1}$ elements, and thus 
\begin{equation}
    S_k = \sum_{n \in I_k} 2^{-L(n)} = \sum_{n\in I_k} 2^{-L(k-1) - k} = 2^{-L(k-1)-1}.
\end{equation}

Then for the Kraft's sum $S = \sum_{n\in\mathbb{N}} 2^{-L(n)}$ we get
\begin{equation}
    S = \sum^\infty_{k=1} S_k = S_1 + \sum^\infty_{k=2} S_k = \frac{1}{2} + \sum^\infty_{k=2} 2^{-L(k-1) - 1} = \frac{1}{2} + \frac{1}{2} \sum^\infty_{k=1} 2^{-L(k)} = \frac{1+S}{2}.
\end{equation}

This readily implies that $S = 1$. The real rate of convergence is very slow though: in our computations we have reached (see \cite{github-benford})
\begin{equation}
    \sum^{2^{2^{24}}}_{n=1} 2^{-L(n)} \approx 0.9697265625 \ldots
\end{equation}

\section*{Acknowledgments} This material is based upon work supported by the Google~Cloud Research Award number GCP19980904.

\printbibliography

\end{document}