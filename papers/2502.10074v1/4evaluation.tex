\section{Evaluation}
\label{sec:evaluation}

%\subsection{Implementation}
%\label{sec:implementation}

We implemented \sys on top of Block-STM~\cite{blockstm} and \basesys~\cite{chiron} in Rust to evaluate its performance impact on both an optimistic execution engine and a guided execution engine, covering two of the most widely adopted approaches to parallel execution in the blockchain space.
The implementation is publicly available on Github~\footnote{https://github.com/ISTA-SPiDerS/Anthemius}.
As \basesys is built on top of Block-STM, this simplifies the implementation and allows for an easier comparison of the results.
Furthermore, we use the parallel execution benchmarks proposed in \basesys~\cite{chiron}. 

%We picked \basesys to represent guided execution engines as it is built on top of BlockSTM which makes comparing the two approaches easier.
%Therefore, aside from evaluating how BlockSTM performs with our block scheduling approach, we also leverage the implementation of \basesys~\cite{chiron} to evaluate the performance of guided execution engines which can be found in blockchains such as Solana~\cite{solana} or Sui~\cite{sui}. 

%Due to this, we can evaluate how \sys affects the performance of two popular approaches to parallel smart contract execution.

Finally, we implemented the batch handler ($\sim70$ lines of code) and the batch scheduler ($\sim120$ lines of code) to assemble blocks and then forward these blocks to the respective execution engines.


%% LIMIT is 1000 (10%)
% MAX relax num = 2, max relax rate = 100
% TARGETINCRATE = At least double of numtx/c
% MAXLEN = 10k
% hot read limit = 4

%The constructed blocks are then passed to the execution engine.

%Depending on the approach to consensus, the block construction in \sys can be optimized.
%For instance, if the same leader proposes multiple consecutive blocks, as in Narwahl~\cite{narwahl}, Kauri~\cite{kauri}, or PBFT~\cite{pbft}, the leader can optimize scheduling by tracking whether a batch primarily consisted of contended transactions at a given point and then skipping that batch in subsequent rounds. The leader then waits until the batch becomes the first batch, to start including the transactions again.

\subsection{Benchmark}

The experiments were executed on a Debian GNU/Linux 12 server with two AMD EPYC 7763 64-Core Processors and 1024 GB of RAM. We generated batches of transactions with different distributions of read/write-accesses and different user distributions with the help of \basesys~\cite{chiron} for all five proposed workloads. Namely, one peer-to-peer workload (P2PTX), two Decentralized Exchange Workloads (DEXAVG and DEXBURSTY), one NFT workload (NFT), and one mixed workload (MIXED). 
These workloads are derived from real-world data from Ethereum and Solana and are designed to evaluate parallel transaction execution engines under realistic levels of contention. 
Each workload has a unique and realistic resource access pattern, along with a varying count of read and write operations per transaction.

Each experiment was executed a total of 10 times and the results we outline in this section present the average of all 10 runs. Furthermore, in each workload, we vary the number of worker threads from 4 to 32 in increments of 4.
Finally, we are interested in two key metrics: throughput, to assess the performance improvement introduced by \sys, and latency, to determine the average delay introduced by \sys.


We set the following parameters for the batch handler and batch scheduler:
First, we evaluate the execution engines using blocks of up to $\textsc{maxlen} = 10{,}000$ transactions, as this block size represents a sweet spot for both engines, where the execution setup overhead (e.g., virtual machine initialization) becomes negligible. Accordingly, we configured the batch size to match the target block size, as smaller batch sizes increase block construction overhead, while larger batch sizes reduce the batch handler's flexibility to adapt to the workload's characteristics.

Next, to minimize tail latency for transactions accessing hot resources, we allow the first and last $\textsc{lim} = 1{,}000$ transactions to be included freely without restrictions.  
Furthermore, we permit up to $\textsc{maxrelaxnum} = 2$ relaxations of the inclusion rate as we observed diminishing returns from additional relaxations and large scheduling costs beyond this point.  
We set the relaxation rate to a maximum of $\textsc{maxrelaxrate} = 100$, targeting an inclusion rate of $\textsc{targetincrate} = 2\frac{\text{maxlen}}{c}$. This accounts for the higher returns from a more aggressive target inclusion rate as the concurrency potential increases.  
Finally, we configure $\textsc{maxhotr} = 4$ to avoid uniting too many critical paths of transactions, ensuring manageable contention levels.




%As previously stated, we evaluate the performance of \sys with Optimistic Parallel Execution using Block-STM~\cite{blockstm} and Guided Parallel Execution using \basesys~\cite{chiron}. 


%Given the number of worker threads, we created an equal number of batches for the batch handler.

%The batch handler first tries to include transactions from the first batch and then attempts to include transactions from the following batches. Following that, the execution finishes once the first batch is fully exhausted. 
%Thus, each run of \sys presents the average run overall for several blocks with varying block sizes up to 10.000 transactions. For vanilla BlockSTM and \basesys, there is a stable block size of 10.000 transactions.
%To measure the average latency, we only evaluated the latency for the transactions in the first batch, adding up the execution time and scheduling time of all blocks until the given transaction was successfully included in a block and executed.

\subsection{Throughput}



As \sys delays the inclusion of some transactions in favor of others to enhance system performance, we provide the batch handler with several batches of $10{,}000$ transactions to saturate the system and measure the maximum throughput. Each batch is generated with the same distribution of resource accesses, both within and across batches. We then evaluate \sys by passing all batches to the batch handler and run \sys until all transactions from the first batch are successfully executed. Consequently, the evaluation for \sys spans multiple blocks, where the reported throughput represents the average throughput over the entire runtime and accounts for scheduling and execution time.
For the baseline versions of Block-STM and \basesys, we use a single block containing $10{,}000$ transactions that also fully saturates the system, with runtime variations dependent solely on the specific workload.  


As blockchains such as Aptos or Sui decouple consensus from execution, block scheduling could be moved outside of the critical path of consensus. This can significantly reduce the overhead, as scheduling requires only a single thread and only has to be done at the proposer node. Due to this, we display two lines for \sys. First, one that serves as a ceiling on performance, where we assume that there is an idle thread that can be used for scheduling outside of the critical path of consensus, denoted \textit{Decoupled \sys}. Second, one that serves as a floor on performance where we count the full scheduling overhead on the critical path of consensus, referred to as \textit{\sys}.

\begin{figure*}[t]
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=1\linewidth]{figures/tputpygoodblock.pdf} 
\caption{Throughput per Second - \basesys}
\label{fig:tputpythia}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=1\linewidth]{figures/tputstmgoodblock.pdf}
\caption{Throughput per Second - Block-STM}
\label{fig:tputblockstm}
\end{subfigure}
\caption{Throughput per Second}
\label{fig:image2}
\end{figure*}


The results for \sys with \basesys are shown in Figure~\ref{fig:tputpythia}, with the throughput in transactions per second on the y-axis and the number of worker threads on the x-axis. With the NFT workload, we only see a small speedup from creating good blocks. This is due to the account distribution in this workload, where transactions from users appear very frequently in several batches. Due to this, once a transaction of a given user is skipped, the following transactions also have to be skipped, resulting in long scheduling times and leaving very few transactions behind that can be included in the block. In comparison, in the peer-to-peer workload there is already a significant improvement, where with an increasing number of worker threads, we can reach almost twice the initial throughput. Following that, with increasing contention and less repetitive users, the decentralized exchange workloads reach over 240\% performance boost compared to vanilla \basesys. While in the average DEX workload, the scheduling overhead is very small, with increasing contention and increasing number of worker threads we can also see an increased scheduling overhead.
Finally, in the mixed workload, we also see a large performance advantage. This is also due to the much higher overall execution complexity compared to the scheduling overhead. Due to the complexity of the workload, the overhead is constant after 12 cores, but \sys under this workload shows over 200\% performance advantage compared to vanilla \basesys.


The throughput results for \sys with Block-STM are shown in Figure~\ref{fig:tputblockstm}, with the throughput in transactions per second on the y-axis and the number of worker threads on the x-axis. 
Compared to the results with \basesys, the results for Block-STM vary more as the high contention within each block results in a large re-execution overhead. 
As such, even when we build better blocks with \sys, the contention in the block is still so high, that Block-STM struggles to take advantage of that.
We can still see the largest disadvantage in the NFT workload, due to the user distribution preventing us from building better blocks. Furthermore, we can see that in the peer-to-peer workload, once we reach 20 threads, \sys is starting to be able to compensate for the re-execution overhead of Block-STM and reach a speed-up of up to 25\%. Similarly, for the DEX workloads, there is an initial performance drop due to the re-execution overhead, which is only compensated with more worker threads later.
Finally, in the MIXED workload, \sys shows a constant speed up compared to vanilla Block-STM up to 200\% the original performance.

%For both approaches the total scheduling overhead was only between 5m and 10m per batch

\subsection{Latency}

\begin{figure}[t]
\includegraphics[width=1\linewidth]{figures/anthemiuslatency.pdf} 
\caption{Tail Latency for \basesys and Block-STM}
\label{fig:latency}
\end{figure}


As we are delaying the inclusion of some transactions that access hot resources, we expect a latency overhead increase at the tail. 
Similarly to the throughput evaluation, we send several batches of transactions to the batch handler. To fully assess the effect of \sys, we evaluate how the tail latency develops when awaiting the finished execution of up to five batches for all workloads with a fixed number of $16$ cores.
The results of this evaluation are shown in Figure~\ref{fig:latency}, where the yellow line indicates the 50th percentile (median), the box represents the 25th and 75th percentiles (interquartile range), and the whiskers denote the 10th and 90th percentiles.

The results mirror what we saw in the throughput evaluation where in almost all workloads and configurations where \sys shows a significant speedup the average transaction latency is significantly lower. Furthermore, thanks to the large throughput advantage in these settings, especially when paired with \textit{Chiron}, \sys has a latency advantage for up to the 90\% percentile of transactions.

On the other hand, as expected, \sys shows a growing tail latency with an increasing number of batches. This
is expected since the congestion caused by the highly contended workloads results in different scheduling decisions. Nevertheless, we can see that the growing tail latency affects not only \sys but also the reference systems, although for certain workloads the effects of \sys are more prominent at the p90 percentile. 

This is a tradeoff the blockchain needs to take into account based on their expected workload and tune \sys parameters to better match the chracteristics of the transactions expected.




%We evaluate the tail latency of \sys compared to the vanilla single block latency in Block-STM and \basesys. We split transactions into three categories: p50, p75, and p90, relative to the latency percentile they were in. As such, the presented graphs show the latency for the 3 given percentiles.

%The results for \basesys are displayed in Figure~\ref{fig:latpythia}, with the latency in seconds on the y-axis and the different latency percentiles on the x-axis. We can see that for all workloads excluding the mixed workload, 50\% of all transactions were executed faster than in vanilla \basesys. This is the case as they were paired with less contended transactions such that one or even multiple blocks can be executed in the same time frame as a single block in vanilla \basesys.
%In the mixed workload, the latency is mostly on par with a slight latency overhead. 

%In p75 the results vary more. In the P2P and DEX workloads, the latency shows an advantage or is on par with vanilla \basesys. While in the MIXED and NFT workloads, there is a slight latency overhead.

%Finally, in the p90 group, there is a latency overhead in almost all settings. This is expected as we are delaying some transactions to improve the overall throughput and to improve the latency of transactions that are not accessing hot resources.
%However, we would like to point out that even in the p90, none of the transactions took more than twice as long in any setting. This indicates that for \basesys, \sys only presents a small tradeoff from system throughput to tail latency overhead.



%The results for Block-STM are displayed in Figure~\ref{fig:latpythia} and are very similar to the ones of \basesys. For the p50 group, we see an advantage compared to vanilla execution, and, analogous to \basesys, in the p75 group we see some workloads with a small advantage while others show a small overhead.
%However, finally, in the p90 group, for workloads with very high contention, we can see a more significant latency overhead. Nonetheless, in no setting, this is significantly larger than three times the vanilla latency.

\subsection{Summary}

In this section, we evaluated the throughput improvement \sys can provide across different execution engines. Our findings demonstrate that while \sys improves throughput for both types of execution engines under several of the workloads, its impact is significantly larger when combined with guided execution engines. In this case, \sys provides a large throughput improvement across all but one of the workloads. The only exception is the NFT workload, where many high-frequency users appear across multiple blocks, preventing \sys from effectively rescheduling their transactions.

When it comes to latency, we analyzed the tail latency percentiles of delayed transactions. Our results show that for most workloads the majority of transactions (over 75\%) have lower or similar latency compared to the vanilla execution, while only the slowest 25\% of transactions sustain a latency overhead.
This indicates that \sys can be a valuable addition to any blockchain with a parallel execution engine where the workload does not primarily stem from a very small set of users.
