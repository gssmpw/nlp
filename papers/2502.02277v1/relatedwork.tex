\section{Related Work}
In the field of imbalanced classification, solutions are generally categorized into data-centric and algorithmic approaches. Data-centric methods address class imbalance by altering dataset composition through techniques such as minority oversampling or majority undersampling \cite{chawla_smote_2011,he_adasyn_2008}, with SMOTE being a prominent technique for generating synthetic instances via linear interpolation \cite{chawla_smote_2011,zhang_mixup_2018,verma_manifold_2019}. Algorithmic methods enhance classifier performance by adjusting the learning process, including loss function weighting \cite{cao_learning_2019,cui_class-balanced_2019}, transfer learning \cite{singh_imbalanced_2021}, and meta-learning \cite{shu_meta-weight-net_2019}. Recent studies highlight that semi-supervised and self-supervised learning paradigms can significantly improve classification outcomes in imbalanced datasets \cite{yang_rethinking_2020}.

Imbalanced regression presents distinct challenges and has received relatively less attention in the literature compared to classification tasks. Adaptations from classification approaches have been applied, such as generating synthetic observations for sparsely populated target regions through interpolation \cite{camacho_wsmoter_2024} or by introducing Gaussian noise \cite{branco_smogn_2017}. Moreover, ensemble methods that integrate multiple preprocessing strategies have also been investigated \cite{branco_rebagg_2018}. Nonetheless, existing methods often fail to account for the varying complexity of regression across different regions, leading to differing data requirements that are not adequately addressed, an issue this paper resolves by proposing a novel approach to effectively capture and manage these regional complexities.