%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
 
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
SIGMA: Sheaf-Informed Geometric Multi-Agent Pathfinding
}


\author{Shuhao Liao$^{1}$, Weihang Xia$^{2}$, Yuhong Cao$^{3}$, Weiheng Dai$^{3}$,\\ Chengyang He$^{3}$, Wenjun Wu$^{1,*}$, Guillaume Sartoretti$^{3}$%
\thanks{$^{1}$Hangzhou International Innovation Institute, Beihang University, China}%
\thanks{$^{2}$CoreControl Inc, Hangzhou, China}
\thanks{$^{3}$Department of Mechanical Engineering, National University of Singapore, Singapore}
\thanks{*Corresponding author: Wenjun Wu (wwj09315@buaa.edu.cn).}
% \thanks{$^{2}$Institute of Artificial Intelligence, Beihang University, China}%
% \thanks{$^{2}$Department of Mechanical Engineering, College of Design and Engineering, National University of Singapore.}
% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small b.d.researcher@ieee.org}}%
}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{subcaption}
\usepackage{cite}
\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

The Multi-Agent Path Finding (MAPF) problem aims to determine the shortest and collision-free paths for multiple agents in a known, potentially obstacle-ridden environment. It is the core challenge for robotic deployments in large-scale logistics and transportation. Decentralized learning-based approaches have shown great potential for addressing the MAPF problems, offering more reactive and scalable solutions. However, existing learning-based MAPF methods usually rely on agents making decisions based on a limited field of view (FOV), resulting in short-sighted policies and inefficient cooperation in complex scenarios. There, a critical challenge is to achieve consensus on potential movements between agents based on limited observations and communications. To tackle this challenge, we introduce a new framework that applies sheaf theory to decentralized deep reinforcement learning, enabling agents to learn geometric cross-dependencies between each other through local consensus and utilize them for tightly cooperative decision-making. In particular, sheaf theory provides a mathematical proof of conditions for achieving global consensus through local observation. Inspired by this, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. During the task, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature, leading to efficient cooperation on pathfinding and collision avoidance. As a result, our proposed method demonstrates significant improvements over state-of-the-art learning-based MAPF planners, especially in relatively large and complex scenarios, demonstrating its superiority over baselines in various simulations and real-world robot experiments.
% utilize the underlying geometry. 
% In particular, the sheaf theory provides a rigorous mathematical method to distributedly formulate consensus between agents. By doing so, we allow agents to achieve efficient consensus on their local relationships and global distributions, leading to efficient implicit collaboration in MAPF.
% We provide a rigorous mathematical approach to describe local agent relationships and their global structure, describe complex relationships between different agents, and implement sophisticated types of consensus dynamics.
% Our approach demonstrates significant potential in improving the efficiency and effectiveness of MAPF in large-scale robotic systems.

\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

As intelligent robots advance, the application of large-scale Multi-Agent Path Finding (MAPF) has become increasingly important in scenarios such as warehouse automation, airport management, and robotic fleets~\cite{honig2019persistent, rekik2019multi, li2021lifelong, he2024social}. MAPF involves planning collision-free paths for multiple agents from their start positions to designated goals. This NP-hard problem presents significant challenges in scalability and computational efficiency due to the exponential growth of complexity with respect to the number of agents. 

Recently, the MAPF community has started looking to Multi-Agent Reinforcement Learning (MARL) to generate fast and scalable solutions~\cite{primal2,scrimp,g2rl}. Moreover, MARL has gained significant traction in multi-robot systems, where agents collaborate in decentralized settings to achieve global objectives~\cite{yu2023esp,yu2024adaptaug,yu2024leveraging,yu2021swarm,feng2024safe,feng2023mact}. These learning-based approaches rely on decentralized planning under partial observability (i.e., each agent only observes its nearby environment, usually 11$\times$11 grid world), reducing computational complexity and enabling the network to tackle large-scale scenarios effectively~\cite{feng2024hierarchical}. However, the solutions generated by these learning-based methods are usually suboptimal, since they restrict the information available to agents, hindering their ability to avoid local minima and perform delicate joint behaviors. To improve the performance of learning-based MAPF planners, recent methods tried to augment the information available to agents by incorporating expert paths, designing communication schemes, or providing global map encodings~\cite{primal, dhc, alpha}. These methods show significant improvements over previous learning-based methods, but there is still a remarkable performance gap between learning-based solutions and centralized optimization-based solutions~\cite{odrm}.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.22]{pic/pic1.png}
  \caption{A sheaf structure in MAPF, where the sheaf structure provides multilayer views of system structure, stalks collate high-dimensional data associated with agents, and restriction maps describe complex relationships between different agents. This sheaf structure aids agents in implementing consensus.}
  \label{pic1}
  \vspace{-5mm}
\end{figure}

This work builds upon the observation that existing learning-based planners typically lack the ability to let agents reason about and reach an explicit \textit{consensus} (i.e., a general agreement between agents on their future movements). Such consensus is naturally achieved in centralized methods, which significantly helps agents avoid blockage and deadlock, leading to a better success rate and shorter makespan. However, achieving consensus is non-trivial in decentralized methods, as each agent makes its decision individually.
As a result, current state-of-the-art learning-based methods still struggle with dense environments that require tight cooperation among agents.
% Consequently, current state-of-the-art learning-based methods still perform poorly in large-scale and large-team-size MAPF problems (e.g., the current SOTA learning-based planner~\cite{alpha} only achieves a 25\% success rate in 60*60 room-liked environments with 128 agents).    


To address this problem, we introduce SIGMA, a novel sheaf-informed MAPF planner that explicitly trains consensus by learning the underlying geometric cross-dependencies between agents. 
% To the best of our knowledge, this work is the first to help agents explicitly achieve consensus between them. 
To the best of our knowledge, this work is the first learning-based method in MAPF that helps agents explicitly achieve consensus among themselves.
Our method enhances the capability of the trained planner for modeling complex potential interactions between agents. In particular, we integrate \textit{sheaf theory}~\cite{hansen2019toward} into decentralized deep reinforcement learning, enabling agents to learn consensus/global consistency by modeling geometric cross-dependencies between each other, where these geometric cross-dependencies represent the consensus. Sheaf theory broadens the concept of graphs and studies the global consistency of high dimensional data. Particularly, it provides mathematical proof of conditions for achieving global consensus through local observation. Inspired by sheaf theory, we incorporate a neural network to approximately model the consensus in latent space based on sheaf theory and train it through self-supervised learning. By doing so, in addition to normal features for MAPF as in previous works, each agent distributedly reasons about a learned consensus feature for consensus-aware path-finding and collision avoidance.  We present exhaustive numerical comparisons with existing conventional and learning-based planners, which show that SIGMA outperforms state-of-the-art learning-based MAPF planners. Notably, SIGMA excels in large-scale scenarios with larger team sizes, where it significantly surpasses existing methods.

% In this work, we introduce sheaf structures into MAPF by examining the problem from a geometric perspective. From this perspective, traditional graphs, used to model agent interactions, are 1-cell complexes. 

% Sheaf theory extends this concept by introducing sections for local data representation, cohomology for ensuring global consistency, and sheaf morphisms for advanced data transformations and comparisons~\cite{hansen2019toward}. 
% Accordingly, our proposed SIGMA framework utilizes networks to model sheaf structures, including stalks and restriction maps, and designed a global section loss to measure the consistency among agents to achieve consensus. Based on experimental results, SIGMA outperforms state-of-the-art learning-based MAPF solvers in both random, warehouse, and highly structured maps such as room-like environments with space-limited doors and narrow corridors. Notably, SIGMA excels in large-scale scenarios with a high number of agents, significantly surpassing existing methods. Overall, by leveraging sheaf structures, our approach more effectively models environmental information and inter-agent relationships, demonstrating superior performance in both simulations and real-world experiments.


\section{RELATED WORK}

\subsection{Deep Reinforcement Learning-based MAPF}

Recent years have seen a growing interest in solving MAPF problems using MARL. The pioneering work, PRIMAL, introduced a combination of RL and imitation learning to plan paths through fully decentralized policies within a partially observable environment~\cite{primal}. PRIMAL utilized the Asynchronous Advantage Actor-Critic algorithm as underlying RL algoritihm, with all agents sharing the same parameters, while imitation learning was based on behavior cloning from data generated by the ODrM* planner. This approach was later extended in PRIMAL2 to address lifelong MAPF scenarios, incorporating learned conventions to enhance cooperation among agents, particularly in highly structured environments~\cite{primal2}. Subsequent research has explored communication learning as a promising approach to further enhance solution quality. For instance, works like MAGAT and DHC~\cite{magat, dhc} introduced Graph Neural Networks~\cite{peng2024graphrare} for communication learning, where each agent is treated as a node, and decisions are made based on aggregated information from neighboring agents. DCC, on the other hand, developed a selective communication strategy that determines whether an agent's decision should be influenced by its neighbors~\cite{dcc}. Moreover, PICO integrated planning priorities from a classical coupled planner into an ad-hoc communication topology, aiming to produce policies that reduce collisions~\cite{pico}. More recently, SCRIMP introduced a scalable method where agents learn from small FOV with a modified transformer for communication, improving performance in dense scenarios~\cite{scrimp}. Similarly, ALPHA combined local and global information, using a Graph Transformer to enhance decision-making and cooperation, addressing the limitations of limited FOV~\cite{alpha}. However, these methods often face challenges of scalability and complexity, struggling to handle instances with agent-dense environments.

\subsection{Sheaf Applications}
Sheaf theory addresses the local-to-global problem in multi-agent systems by allowing the coherent integration of local data into global structures~\cite{sheafori}. Grothendieck then extended its application in algebraic geometry through scheme theory, enabling the handling of complex structures like singularities~\cite{grothendieck1955general}. In distributed systems, sheaf theory provides a framework for consensus on complex data structures~\cite{hansen2020laplacians}. In signal processing, it manages distributed data with complex dependencies, and in network communication, it optimizes information flow and reduces communication costs~\cite{robinson2014topological}. In network science, sheaves provide enhanced descriptions of network structures, capturing the nature of relationships between nodes. Within this framework, opinion dynamics uses discourse sheaves to model how opinions evolve and interact within social networks~\cite{hansen2021opinion}. Additionally, Bodnar proposed neural sheaf diffusion to learn sheaf laplacians from lower-order data, providing a novel topological perspective on heterophily and oversmoothing in GNNs~\cite{bodnar2022neural,10872817,peng2023clgt}.


\begin{figure*}[t]
  \centering
  \includegraphics[scale=0.74]{pic/framework.pdf}
  \caption{Network structure of SIGMA. The observation encoder encodes observations in stalks (orange), the section FC learns restriction maps (green), and \(M(s)\) is included in the advantage function to enhance action evaluation. A global section loss (red) is then integrated into the network updating to align the policy with the sheaf structure.}
  \label{framework}
\end{figure*}

\section{MAPF AS AN RL PROBLEM}

\subsection{MAPF Problem Statement}

% The MAPF problem is formalized as follows. Given an undirected graph $G=(V, E)$ and an agent set $N$, each agent $i$ is assigned a unique start vertex $\xi_i \in V$ and a unique goal vertex $g_i \in V$. At each discrete time step $t=0, \ldots, \infty$, each agent can either move to an adjacent vertex or wait at its current vertex. A path for agent $i$ contains a sequence of adjacent (indicating a moving) or identical (indicating a waiting) vertices beginning at the start vertex $\xi_i$ and terminating at the goal vertex $g_i$. A collision between agents is either a vertex collision, which is a tuple $\langle i, j, v, t\rangle$ where agents $i$ and $j$ reaching at the same vertex $v$ at time $t$, or an edge collision, defined as a tuple $\langle i, j, u, v, t\rangle$ where agents $i$ and $j$ traverse the same edge $(u, v)$ in opposite directions at time $t$. A solution to MAPF is a set of collisionfree path, one for each agent. The quality of a solution is measured by the sum of arrival time of all agents at their goal vertices. 

The \textit{classical} MAPF problem considers a set of agents $N=\left\{\alpha_1, \ldots, \alpha_n\right\}$ and an undirected graph \(\mathbf{G} = (\mathbf{V}, \mathbf{E})\), where \(\mathbf{V}\) represents the set of vertices and \(\mathbf{E}\) represents the set of edges. Each agent \(i\) is assigned a distinct start vertex (\(\xi_i \in \mathbf{V}\)) and a distinct goal vertex (\(g_i \in \mathbf{V}\)). Time is discretized into uniform steps. At each time step \(t = 0, 1, 2, \ldots\), an agent has the option to either move to an adjacent vertex or remain stationary at its current vertex. A path for agent \(\alpha_i\) is defined as a sequence of vertices, either adjacent (indicating movement) or identical (indicating waiting), starting from the agent's start vertex \(\xi_i\) and ending at its goal vertex \(g_i\). Collisions between agents are categorized as either vertex collisions, which occur when two agents \(\alpha_i\) and \(\alpha_j\) occupy the same vertex \(v\) at the same time \(t\), or edge collisions, which occur when two agents \(\alpha_i\) and \(\alpha_j\) simultaneously traverse the same edge \((u, v)\) in opposite directions at time \(t\). A valid solution to the MAPF problem is a set of collision-free paths, one for each agent. The optimality of the solution is typically evaluated by the sum of the arrival times of all agents at their respective goal vertices.


\subsection{RL Environment Setup}
Remaining consistent with the standard MAPF problem, we use the following setup: the map is a 2D discrete 4-connectivity grid world,  where each grid is a vertex connected to its neighbors by edges, and agents can move to the free cell adjacent to their location or stay idle at each time step. An episode terminates when all agents are on their goals at the end of a time step (success) or when the number of time steps reaches the pre-defined limit (failure). We utilize the room-like map generator proposed by ALPHA. The generated room maps contain corridors of varying widths, which closely resemble real offices, warehouses, and other environments. Unlike the temporal impact of loose obstacles in random maps, continuous obstacles in such highly structured maps may significantly affect current decision-making even across substantial distances.
%1. What's the standard MAPF? give paper citation
%2. RL formulation of this problem, partially observable, then give the concept of an episode
%3. 

\subsubsection{Observation}
In our settings, each agent can only partially observe the environment limited by the size of the FOV \(\ell \times \ell\), where \(\ell\) is smaller than the total environment size \(m\). To ensure the agent remains centered within its FOV, \(\ell\) is selected as an odd number. The observation data is organized into two primary channels: the first channel is a binary matrix that represents obstacles within the FOV, and the second channel is another binary matrix that indicates the positions of other agents when they fall within the FOV. Additionally, the input to our model includes four heuristic channels, each corresponding to one of the possible movement directions: Up, Down, Left, and Right. These heuristic channels share the same dimensions as the FOV, and each cell is marked as 1 if taking the associated action would move the agent closer to its goal from that location. By incorporating these heuristic channels, the agent can infer the best direction to move without the need to explicitly include the goal location in the input~\cite{dhc}.

\subsubsection{Action Space}
In our grid-world environment, agents operate within a discrete action space. At each time step, an agent can choose to move to one of the adjacent grid cells or remain stationary. We do not consider diagonal movements thus each agent has a total of five possible actions. During training or execution, agents may occasionally select invalid actions, such as moving into an obstacle or causing a collision with another agent. To handle such situations, invalid actions are not filtered out; instead, if an invalid action occurs, the agent and any involved agents are recursively returned to their previous states until no collisions remain~\cite{dhc}.

\subsubsection{Reward}

We follow the DHC setup~\cite{dhc}, assigning the same penalty for each agent's movement and for staying away from the goal. The same reward setting ensures a fairer comparison. Our reward structure is shown in Table~\ref{tab:rewards}.

% To encourage agents to reach their goals efficiently, the reward function applies a penalty for every time step if an agent is not on its goal. Unlike designs that impose a heavier penalty for staying still, we treat all movements and stationary actions equally. This equality of reward is particularly useful in complex scenarios to avoid wrongly penalize agents who stop and allow another to pass in order to avoid collisions, promoting better coordination and overall pathfinding success. Our reward structure is shown in Table~\ref{tab:rewards}.

\begin{table}[ht]
    \centering
    \caption{Reward Structure}
    \label{tab:rewards}
    \begin{tabular}{c||c}
        \hline
        \textbf{Actions} & \textbf{Reward} \\
        \hline
        Move (Up/Down/Left/Right) & $-0.075$ \\
        \hline
        Stay (on goal, away goal) & $0,-0.075$ \\
        \hline
        Collision (obstacle/agents) & $-0.5$ \\
        \hline
        Finish & $3$ \\
        \hline
    \end{tabular}
\end{table}

\section{LEARNING TECHNIQUES}
\subsection{Dynamic Agent Graph}

We define a dynamic agent graph $G=(V, E)$ to represent the relationships between agents based on their FOV, as illustrated in Figure~\ref{agent graph}. Nodes $V$ represent $n$ agent $\alpha_i, i=(1,2,...,n)$. An edge $e \in E$ is established between two agents if they are within each other's FOV, indicating that they can potentially interact or need to consider each other's presence while planning their paths. This dynamic graph reflects the changing visibility and proximity of agents as they move through the environment, making it crucial for coordinating their actions and avoiding collisions. 
% The agent graph's ability to adapt to the agents' movements and interactions is essential for efficient and scalable MAPF.

\begin{figure}[thpb]
  \centering
  \includegraphics[scale=0.07]{pic/agent_graph.png}
  \caption{Dynamic agent graph shows dynamic connections among homogeneous agents, with edges representing mutual visibility within each other's FOV.}
  \label{agent graph}
\end{figure}

\subsection{Cellular Sheaf in MAPF}
A cellular sheaf is an algebraic-topological structure associated with a graph that attaches spaces of data to nodes and edges~\cite{snn}. To be precise, a cellular sheaf $(G, \mathcal{F})$ on a dynamic agent graph $G=(V, E)$ consists of:
\begin{itemize}
\item  A vector space $\mathcal{F}(v)$ for each $v \in V$,
\item  A vector space $\mathcal{F}(e)$ for each $e \in E$,
\item  A linear map $\mathcal{F}_{v \unlhd e}: \mathcal{F}(v) \rightarrow \mathcal{F}(e)$ for each incident node-edge pair $v \unlhd e$.
\end{itemize}

According to the sheaf theory, vector spaces is termed as \textit{stalks} and the linear map as \textit{restriction map}~\cite{Sheaftheory}. The stalk originates from the analogy where a sheaf in the agricultural sense is a collection of stalks of grain bound together by twine; similarly, in mathematical terminology, a cellular sheaf on a graph is a collection of stalks of data bound together by restriction maps.

% stalk：observation
% restrained map: network
% gamma space: training target

Here we focus on the concept of consensus and help readers to understand how it works in MAPF. Agents achieve global consensus via local observations, where the local observations of agents are in node stalks, and the model dependences between agents are in edge stalks. In our case,
agents reach consensus when their independent observations map to consistent features via learned restriction maps.
% when observations of independent agents can be mapped onto consistent features through learned restriction maps, it indicates these agents have reached a consensus. 
Specifically, if for any two agents $ v $ and $ u $ with observation vectors $ x_v $ and $ x_u $ agree on the edge $ e $, the condition $ \mathcal{F}_{v \unlhd e} x_v = \mathcal{F}_{u \unlhd e} x_u $ should be  satisfied. Here, the subspace of direct sum of all the node stalks that satisfy this condition is referred to as the space of global sections $\Gamma(G, \mathcal{F})$ \cite{globalsection}. We regard elements in the space of global sections as representatives of consensus, where the entire agents exhibit no-contradiction behavior in how to map the observations of agents across the graph to global sections. In our work, our target is to utilize self-supervising learning to train a neural networ to model the space of global sections, achieving consensus to avoid congestion and crowding among agents.




\subsection{Sheaf-Informed DQN}

As shown in Figure~\ref{framework}, we use a observation encoder to encode the observations into stalks (orange). Since MAPF agents are homogeneous, meaning they have identical characteristics, their corresponding restriction maps are also identical. Thus, we denote the same restriction maps (green) for all agents as $ M $, which is learned through the section FC's training process. Additionally, we incorporate the global section loss(red) directly into the network updating, ensuring that the learned policy respects the underlying sheaf structure and maintains consistency across the observation space.

DQN learns the action value function using neural networks. To incorporate the advantage and value functions, we can define the advantage function \( A(s, a) \) and the value function \( V(s) \), where $Q(s, a) = V(s) + A(s, a)$.
The agents access the current states \( s_t \in \mathcal{S} \) and selects an action \( a_t \in \mathcal{A} \) according to a policy \( \pi \) at each time step \( t \). The agent's objective is to maximize the expectation of the discounted total return \( R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots \), where \( r_t \) is the reward received at time \( t \).

Q-Learning utilizes an action value function for policy \( \pi \) as \( Q^\pi(s, a) = \mathbb{E}[R_t \mid s_t = s, a_t = a] \) and can be recursively defined by $Q^\pi(s, a) = \mathbb{E}_{s'}[r + \gamma \mathbb{E}_{a' \sim \pi}[Q^\pi(s', a')]]$
The optimal action value, \( Q^*(s, a) = \max_\pi Q^\pi(s, a) \), satisfies the Bellman optimality equation $Q^*(s, a) = \mathbb{E}_{s'}[r + \gamma \max_{a'} Q^*(s', a') \mid s, a]$. The optimal policy is trained by minimizing the loss $\mathcal{L_Q}$. Here, the parameters of the target network are updated periodically. In partially observable environments, agents generally need to condition on an state-action history $\mathcal{L_Q} = \mathbb{E}_{(s, a, r, s')}[(Q(s, a) - y)^2]$, 
where $y = r + \gamma \max_{a'} Q(s', a')$.


\begin{figure}[thpb]
  \centering
  \includegraphics[scale=0.1]{pic/sheaf.png}
  \caption{The observation vectors $o_1$ and $o_2$ of neighboring agents $\alpha_1$ and $\alpha_2$ are encoded into stalks $\mathcal{F}(\alpha_1)$ and $\mathcal{F}(\alpha_2)$, which are mapped onto the stalk of edge $e$ between them via restriction maps $M$.}
  \label{sheaf}
\end{figure}

\begin{table*}[!ht]
  \vspace{2mm}
  \centering
  \caption{Experimental Results. The symbol "$\uparrow$" indicates that a higher value is desirable, and vice versa.}
  \vspace{-2mm}
  \scalebox{0.75}{
    \input{tab_result}
  }
  \label{tab:results}
\end{table*}


As illustrated in Figure~\ref{sheaf}, \( o_1 \) and \( o_2 \) are the observation vectors of neighboring agents $\alpha_1$ and $\alpha_2$, and their corresponding stalks \( \mathcal{F}(\alpha_1) \) and \( \mathcal{F}(\alpha_2) \) should map to the same stalk $\mathcal{F}(e)$ on the edge $e$ via the restriction maps $M$, and they should match on the edge $\mathcal{F}(e)$ by definition of global sections, i.e., \( M (o_1) = M (o_2) \).
To satisfy these conditions and measure how close the current observations are to being within the space of global sections, we designed a self-supervise global section loss \( l_{\text{sec}} \). Specifically, \( l_{\text{sec}}^{\alpha_1} \) can be expressed as follows: 

\begin{equation}
    l_{{\rm{sec}}}^{{\alpha_1}} = \sum\limits_{\alpha_i \: {\rm in} \:\alpha_1\: \rm FOV } {{{\left| {M\left( {{o_i}} \right) - M\left( {{o_1}} \right)} \right|}^2}}
\end{equation}

\begin{equation}
    l_{\text{sec}} = \frac{1}{n} \sum_{i=1}^{n} l_{\text{sec}}^{\alpha_i}
\end{equation}

Where \( o_i \) represents the observation vectors of agent $\alpha_i$ within the FOV of agent $\alpha_1$, and \( M \) denotes the restriction map, $n$ is the number of agents. The function \( l_{\text{sec}} \) sums the mapping discrepancies between stalks, and by minimizing \( l_{\text{sec}} \), we ensure that the global section conditions are satisfied. 
When training converges, the global section loss is minimized, fulfilling the conditions set by sheaf theory, and ensuring that the agents achieve \textbf{consensus}.
% This minimization effectively aligns the agents' actions with the required section constraints, maintaining consistency across the network and ensuring that the agents achieve \textbf{consensus}.


We incorporate \(l_{\text{sec}}\) into the network 
updating to assist the agent in making decisions that adhere to the section conditions. As a result, the learned consensus among agents should be considered during learning the advantage value, we include \(M(s)\) in the advantage function to enable better evaluation of actions by the agents. The resulting Q-function in SIGMA is expressed as follows:

\begin{equation}
    Q(s, a) = V_{\theta_1}(s) +  A_{\theta_2}(s', a) - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} A_{\theta_2}(s', a')
\end{equation}
\begin{equation}
    s'=[M(s), s]
\end{equation}

\begin{equation}
    \mathcal{L}_{SIGMA} = \mathcal{L_Q} + \Lambda  \cdot {l_{{\rm{sec}}}}
\end{equation}


% Here, \( Q(s, a) \) represents the Q value for action \(a\) in state \(s\), \( V_{\theta}(s) \) denotes the value function of state \(s\), and \( A_{\theta}(s', a) \) signifies the advantage function of action \(a\) in state \(s'\). The term \(|\mathcal{A}|\) represents the size of the action space, $\mathcal{L}_{SIGMA}$ is the loss function, $\Lambda$ is a hyperparameter.

Here, \( Q(s, a) \) represents the Q value for action \(a\) in state \(s\), \( V_{\theta_1}(s) \) denotes the value function of state \(s\), which is parameterized by \(\theta_1\), and \( A_{\theta_2}(s', a) \) signifies the advantage function of action \(a\) in the enhanced state \(s'\), parameterized by \(\theta_2\). 
% Both \(V_{\theta_1}(s)\) and \(A_{\theta_2}(s', a)\) are neural networks, where \(\theta_1\) and \(\theta_2\) represent the sets of parameters for these networks, respectively. 
The term \(|\mathcal{A}|\) represents the size of the action space, \(\mathcal{L}_{SIGMA}\) is the loss function, and \(\Lambda\) is a hyperparameter.


\section{EXPERIMENTS}
In this section, we evaluate SIGMA through comprehensive simulation experiments, comparing its performance with SOTA baselines. We also conduct ablation studies to assess the impact of each component of our approach. Additionally, we test the robustness of the trained model by deploying it in simulation and real world environment.

\subsection{Main Results Comparison}

For our experiments, we train our models on structured environments of varying sizes, randomly chosen from a uniform distribution between 10 and 40, while consistently deploying 5 agents. During testing, we explore environments of 20, 40, and 60 sizes, scaling the number of agents from 4 up to 128.

Our evaluation include a comparison with 6 SOTA MAPF solutions: PRIMAL~\cite{primal}, MAPPER~\cite{mapper},
% G2RL~\cite{g2rl},
DHC~\cite{dhc}, DCC~\cite{dcc}, SCRIMP~\cite{scrimp}, and ALPHA~\cite{alpha}. Additionally, we benchmark against the searchbased, bounded-optimal centralized planner ODrM* with an inflation factor of \(\epsilon=2.0\)~\cite{odrm}. For a fair comparison, each planner was tested on the same set of 200 randomly-generated environments.

We employ three metrics to assess performance: 1) \textbf{Episode Length(EL)}: This measures the efficiency of a solution by counting the number of actions agents take to reach their goals within a single episode. 2) \textbf{Arrival Rate(AR)}: This is the percentage of agents that reach their goals across all episodes. 3) \textbf{Success Rate(SR)}: This metric evaluates a planner’s ability to completely fulfill a task. Notably, learning-based methods may show a low SR but still have a high AR, which highlights the importance of AR in evaluating episodes that nearly reach completion without being deemed total failures. The results are presented in Table~\ref{tab:results}.

In our experiments, SIGMA consistently outperforms other learning-based planners in terms of SR across all tasks. Notably, as the number of agents increases, SIGMA's improvement in SR significantly exceeds that of the baseline planners. For instance, in complex scenarios where most learning-based planners struggle or fail to solve the problems, such as with 128 agents on a 40$\times$40 map, SIGMA achieves a SR of 69\%. Even more impressively, on a larger 60$\times$60 map, SIGMA's SR reaches 80\%. These results highlight the effectiveness of our approach where the agents, through achieving consensus, successfully avoid congestion and overcrowding, demonstrating robust performance even under challenging conditions. Additionally, it is noteworthy that on a smaller 20x20 map, although SIGMA exhibits a high SR, the AR isn't as impressive. This indicates that while consensus effectively prevents congestion, it does not necessarily aid in navigating out of such congested scenarios efficiently.


% Compared to $\mathrm{ODrM}^*$, all learning-based planners exhibit significant advantages in terms of AR, which is crucial for life-long MAPF. ALPHA consistently outperforms PRIMAL in nearly all cases, likely due to PRIMAL's sole reliance on local information. As the agent team size increases, the effectiveness of rigid global guidance provided by MAPPER and G2RL diminishes, leading to a decline in their performance. Notably, even in our more complex tasks ( 128 agents, $60 \times 60$ map size, 0.2 obstacle density) demanding high agent cooperation, which MAPPER and G2RL were completely unable to solve, ALPHA achieved a $25 \%$ SR. This is likely due to the fact that MAPPER and G2RL, to some degree, force agents to follow paths/waypoints computed by $\mathrm{A}^*$, and this possibly reduces their effectiveness in denser environments.

% Another notable finding is that DHC and SCRIMP policies, at times, lead to certain agents getting stuck in corners because of the other agents. We believe that the same heuristic map used by DHC and SCRIMP causes this issue. ALPHA, with its comprehensive grasp of the global map and policy flexibility, does not face this limitation. Additionally, SCRIMP employs a tie-breaking strategy to enhance coordination, which is particularly effective in crowded cases. However, this also results in SCRIMP needing much more time to compute a solution. For example, in a MAPF problem with 64 agents in a map of size $20 \times 20$, ten episodes of SCRIMP take 12.29 s , while those of ALPHA take only 7.21 s ( $41 \%$ less time). The test results of random and warehouse maps are shown in the additional video.

\subsection{Ablation Analysis}

Our method focuses on encoding the sheaf structure(stalks and restriction maps) and integrating global section loss. These elements are applied to both the input of the advantage function and the loss function for updating the network to ensure the correctness of the sheaf structure. To analyze the importance of these elements, we experimented with three ablation variants of SIGMA: 1) \textbf{Encoded Stalk(ES)}: This variant tested only the stalks' encoding effectiveness by removing global section loss and restriction maps. 2) \textbf{Weighted Penalty(WP)}: This setup assessed the influence of global section loss within the loss calculation by excluding them. 3) \textbf{Feature Impact(FI)}: We evaluated the impact of removing restriction maps from the advantage function while keeping other elements constant.

\begin{figure}[thpb]
  \centering
  \includegraphics[scale=0.35]{pic/ablation.png}
  \caption{Success rates of ablation variants on 40x40 and 60x60 maps.}
  \label{ablation}
\end{figure}

The results of the ablation variants are shown in Figure~\ref{ablation}. Encoded stalk results in performance similar to the baselines, indicating that this component alone doesn't significantly influence success rates. The introduction of the restriction maps leads to a slight improvement in success rates, suggesting a beneficial but limited role in the overall system's performance. A notable enhancement is observed with the incorporation of global section loss, particularly as the number of agents increases. This enhancement significantly boosts success rates, demonstrating that global section loss effectively guide agents towards explicit consensus. When all components of the SIGMA framework are utilized, including both stalks, restriction maps, and global section loss, there is a further increase in performance. This indicates that the full integration of the sheaf structure is crucial for achieving consensus.


\subsection{Experimental Validation}
\begin{figure}[ht]
     \centering
     \begin{subfigure}[b]{0.22\textwidth}        
         \includegraphics[width=\textwidth]{pic/sim.png}
         \caption{Simulation Environment}
         \label{sim}
     \end{subfigure}
     \hspace{5mm}
     \begin{subfigure}[b]{0.22\textwidth}       
         \includegraphics[width=\textwidth]{pic/real.png}
         \caption{Real World}
         \label{real}
     \end{subfigure}
     \caption{Experiments with real robots on room-like map.}
     \label{expv}
\end{figure}

As shown in Figure \ref{expv}, Figure \ref{sim} represents the simulation environment, and Figure \ref{real} represents the real world environment. In this setup, each cell in the real world environment measures 0.3m on each side, slightly larger than the agents to ensure each agent occupies only one cell on the map. We employ 3 robots, each equipped with Mecanum wheels and measuring approximately 0.23m $\times$ 0.2m. The accurate positions of these robots are tracked using the \textit{OptiTrack Motion Capture System}. The starting and goal positions of the agents are randomly configured. In this experiment, although the robots recognize the virtual positions of obstacles and are programmed to avoid these areas, there are no physical obstacles in the real environment, preventing any interference with the line of sight needed for the OptiTrack motion capture system.

\section{CONCLUSIONS}

In this paper, we introduce SIGMA, a novel MAPF planner that pushes beyond the constraints of limited FOV commonly found in existing learning-based MAPF planners by utilizing sheaf theory to let all agents reason about and reach a team-wide consensus. Our approach efficiently encodes the sheaf structure within MAPF, incorporating global section loss to measure consistency. Through extensive experiments conducted on highly structured maps with varying team sizes and environmental complexities, SIGMA consistently outperforms SOTA learning-based MAPF planners and a bounded-optimal search-based planner in most scenarios. By rigorously proving the transition from local observation to global consensus, this work proposes a novel perspective to the MAPF research community.

In future work, we plan to focus on enriching the intricate relationships among agents by building consensus on more complex graphs. This will involve exploring advanced models and techniques that can handle and interpret the interactions and dependencies within larger, more complex network structures.

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}

This work was supported by the National Science and Technology Major Project(Grant No.2022ZD0116401), the National Natural Science Foundation of China(Grant No.62441617), and an Amazon Research Award.

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{honig2019persistent}
W.~H{\"o}nig, S.~Kiesel, A.~Tinka, J.~W. Durham, and N.~Ayanian, ``Persistent and robust execution of mapf schedules in warehouses,'' \emph{IEEE Robotics and Automation Letters}, vol.~4, no.~2, pp. 1125--1131, 2019.

\bibitem{rekik2019multi}
I.~Rekik and S.~Elkosantini, ``A multi agent system for the online container stacking in seaport terminals,'' \emph{Journal of Computational Science}, vol.~35, pp. 12--24, 2019.

\bibitem{li2021lifelong}
J.~Li, A.~Tinka, S.~Kiesel, J.~W. Durham, T.~S. Kumar, and S.~Koenig, ``Lifelong multi-agent path finding in large-scale warehouses,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~35, no.~13, 2021, pp. 11\,272--11\,281.

\bibitem{he2024social}
C.~He, T.~Duhan, P.~Tulsyan, P.~Kim, and G.~Sartoretti, ``Social behavior as a key to learning-based multi-agent pathfinding dilemmas,'' \emph{arXiv preprint arXiv:2408.03063}, 2024.

\bibitem{primal2}
M.~Damani, Z.~Luo, E.~Wenzel, and G.~Sartoretti, ``Primal $ \_2 $: Pathfinding via reinforcement and imitation multi-agent learning-lifelong,'' \emph{IEEE Robotics and Automation Letters}, vol.~6, no.~2, pp. 2666--2673, 2021.

\bibitem{scrimp}
Y.~Wang, B.~Xiang, S.~Huang, and G.~Sartoretti, ``Scrimp: Scalable communication for reinforcement-and imitation-learning-based multi-agent pathfinding,'' in \emph{2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 9301--9308.

\bibitem{g2rl}
B.~Wang, Z.~Liu, Q.~Li, and A.~Prorok, ``Mobile robot path planning in dynamic environments through globally guided reinforcement learning,'' \emph{IEEE Robotics and Automation Letters}, vol.~5, no.~4, pp. 6932--6939, 2020.

\bibitem{yu2023esp}
X.~Yu, R.~Shi, P.~Feng, Y.~Tian, J.~Luo, and W.~Wu, ``Esp: Exploiting symmetry prior for multi-agent reinforcement learning,'' in \emph{ECAI 2023}.\hskip 1em plus 0.5em minus 0.4em\relax IOS Press, 2023, pp. 2946--2953.

\bibitem{yu2024adaptaug}
X.~Yu, Y.~Tian, L.~Wang, P.~Feng, W.~Wu, and R.~Shi, ``Adaptaug: Adaptive data augmentation framework for multi-agent reinforcement learning,'' in \emph{2024 IEEE International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 10\,814--10\,820.

\bibitem{yu2024leveraging}
X.~Yu, R.~Shi, P.~Feng, Y.~Tian, S.~Li, S.~Liao, and W.~Wu, ``Leveraging partial symmetry for multi-agent reinforcement learning,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~38, no.~16, 2024, pp. 17\,583--17\,590.

\bibitem{yu2021swarm}
X.~Yu, W.~Wu, P.~Feng, and Y.~Tian, ``Swarm inverse reinforcement learning for biological systems,'' in \emph{2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 274--279.

\bibitem{feng2024safe}
P.~Feng, R.~Shi, S.~Wang, J.~Liang, X.~Yu, S.~Li, and W.~Wu, ``Safe and efficient multi-agent collision avoidance with physics-informed reinforcement learning,'' \emph{IEEE Robotics and Automation Letters}, 2024.

\bibitem{feng2023mact}
P.~Feng, X.~Yu, J.~Liang, W.~Wu, and Y.~Tian, ``Mact: Multi-agent collision avoidance with continuous transition reinforcement learning via mixup,'' in \emph{International Conference on Swarm Intelligence}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2023, pp. 74--85.

\bibitem{feng2024hierarchical}
P.~Feng, J.~Liang, S.~Wang, X.~Yu, X.~Ji, Y.~Chen, K.~Zhang, R.~Shi, and W.~Wu, ``Hierarchical consensus-based multi-agent reinforcement learning for multi-robot cooperation tasks,'' in \emph{2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 642--649.

\bibitem{primal}
G.~Sartoretti, J.~Kerr, Y.~Shi, G.~Wagner, T.~S. Kumar, S.~Koenig, and H.~Choset, ``Primal: Pathfinding via reinforcement and imitation multi-agent learning,'' \emph{IEEE Robotics and Automation Letters}, vol.~4, no.~3, pp. 2378--2385, 2019.

\bibitem{dhc}
Z.~Ma, Y.~Luo, and H.~Ma, ``Distributed heuristic multi-agent path finding with communication,'' in \emph{2021 IEEE International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 8699--8705.

\bibitem{alpha}
C.~He, T.~Yang, T.~Duhan, Y.~Wang, and G.~Sartoretti, ``Alpha: Attention-based long-horizon pathfinding in highly-structured areas,'' in \emph{2024 IEEE International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 14\,576--14\,582.

\bibitem{odrm}
G.~Wagner and H.~Choset, ``Subdimensional expansion for multirobot path planning,'' \emph{Artificial intelligence}, vol. 219, pp. 1--24, 2015.

\bibitem{hansen2019toward}
J.~Hansen and R.~Ghrist, ``Toward a spectral theory of cellular sheaves,'' \emph{Journal of Applied and Computational Topology}, vol.~3, pp. 315--358, 2019.

\bibitem{magat}
Q.~Li, W.~Lin, Z.~Liu, and A.~Prorok, ``Message-aware graph attention networks for large-scale multi-robot path planning,'' \emph{IEEE Robotics and Automation Letters}, vol.~6, no.~3, pp. 5533--5540, 2021.

\bibitem{peng2024graphrare}
T.~Peng, W.~Wu, H.~Yuan, Z.~Bao, Z.~Pengru, X.~Yu, X.~Lin, Y.~Liang, and Y.~Pu, ``Graphrare: Reinforcement learning enhanced graph neural network with relative entropy,'' in \emph{2024 IEEE 40th International Conference on Data Engineering (ICDE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 2489--2502.

\bibitem{dcc}
Z.~Ma, Y.~Luo, and J.~Pan, ``Learning selective communication for multi-agent path finding,'' \emph{IEEE Robotics and Automation Letters}, vol.~7, no.~2, pp. 1455--1462, 2021.

\bibitem{pico}
W.~Li, H.~Chen, B.~Jin, W.~Tan, H.~Zha, and X.~Wang, ``Multi-agent path finding with prioritized communication learning,'' in \emph{2022 International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2022, pp. 10\,695--10\,701.

\bibitem{sheafori}
H.~Miller, ``Leray in oflag xviia: the origins of sheaf theory, sheaf cohomology, and spectral sequences,'' \emph{Kantor 2000}, pp. 17--34, 2000.

\bibitem{grothendieck1955general}
A.~Grothendieck, \emph{A general theory of fibre spaces with structure sheaf}.\hskip 1em plus 0.5em minus 0.4em\relax University of Kansas, Department of Mathematics, 1955, no.~4.

\bibitem{hansen2020laplacians}
J.~Hansen, ``Laplacians of cellular sheaves: Theory and applications,'' Ph.D. dissertation, University of Pennsylvania, 2020.

\bibitem{robinson2014topological}
S.~Sardellitti and S.~Barbarossa, ``Topological signal processing over generalized cell complexes,'' \emph{{IEEE} Trans. Signal Process.}, vol.~72, pp. 687--700, 2024.

\bibitem{hansen2021opinion}
J.~Hansen and R.~Ghrist, ``Opinion dynamics on discourse sheaves,'' \emph{SIAM Journal on Applied Mathematics}, vol.~81, no.~5, pp. 2033--2060, 2021.

\bibitem{bodnar2022neural}
C.~Bodnar, F.~Di~Giovanni, B.~Chamberlain, P.~Lio, and M.~Bronstein, ``Neural sheaf diffusion: A topological perspective on heterophily and oversmoothing in gnns,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 18\,527--18\,541, 2022.

\bibitem{10872817}
T.~Peng, H.~Yuan, Y.~Zhang, Y.~Li, P.~Dai, Q.~Wang, S.~Wang, and W.~Wu, ``Tagrec: Temporal-aware graph contrastive learning with theoretical augmentation for sequential recommendation,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, pp. 1--14, 2025.

\bibitem{peng2023clgt}
T.~Peng, Y.~Liang, W.~Wu, J.~Ren, Z.~Pengrui, and Y.~Pu, ``Clgt: A graph transformer for student performance prediction in collaborative learning,'' in \emph{Proceedings of the AAAI conference on artificial intelligence}, vol.~37, no.~13, 2023, pp. 15\,947--15\,954.

\bibitem{snn}
J.~Hansen and T.~Gebhart, ``Sheaf neural networks,'' \emph{CoRR}, vol. abs/2012.06333, 2020.

\bibitem{Sheaftheory}
G.~E. Bredon, \emph{Sheaf theory}.\hskip 1em plus 0.5em minus 0.4em\relax Springer Science \& Business Media, 2012, vol. 170.

\bibitem{globalsection}
J.~Hansen and R.~Ghrist, ``Toward a spectral theory of cellular sheaves,'' \emph{Journal of Applied and Computational Topology}, vol.~3, no.~4, pp. 315--358, 2019.

\bibitem{mapper}
Z.~Liu, B.~Chen, H.~Zhou, G.~Koushik, M.~Hebert, and D.~Zhao, ``Mapper: Multi-agent path planning with evolutionary reinforcement learning in mixed dynamic environments,'' in \emph{2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 11\,748--11\,754.

\end{thebibliography}

% \bibliographystyle{IEEEtran}
% \bibliography{IEEEexample}


\end{document}
