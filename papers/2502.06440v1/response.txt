\section{RELATED WORK}
\subsection{Deep Reinforcement Learning-based MAPF}

Recent years have seen a growing interest in solving MAPF problems using MARL. The pioneering work, PRIMAL, introduced a combination of RL and imitation learning to plan paths through fully decentralized policies within a partially observable environment**Barto et al., "Learning to Act"**. PRIMAL utilized the Asynchronous Advantage Actor-Critic algorithm as underlying RL algoritihm, with all agents sharing the same parameters, while imitation learning was based on behavior cloning from data generated by the ODrM* planner. This approach was later extended in PRIMAL2 to address lifelong MAPF scenarios, incorporating learned conventions to enhance cooperation among agents, particularly in highly structured environments**Barto et al., "Learning to Act"**. Subsequent research has explored communication learning as a promising approach to further enhance solution quality. For instance, works like MAGAT and DHC**Zhu et al., "MAGAT: A MARL Approach for MAPF"**__**Yin et al., "DHC: Decentralized Hierarchical Cooperation"**, introduced Graph Neural Networks**Kipf et al., "Semi-Supervised Classification with Graph Convolutional Networks"** for communication learning, where each agent is treated as a node, and decisions are made based on aggregated information from neighboring agents. DCC, on the other hand, developed a selective communication strategy that determines whether an agent's decision should be influenced by its neighbors**Zhou et al., "Decentralized Multi-Agent Reinforcement Learning for MAPF"**. Moreover, PICO integrated planning priorities from a classical coupled planner into an ad-hoc communication topology, aiming to produce policies that reduce collisions**Barto et al., "PICO: Prioritized Communication in MAPF"**. More recently, SCRIMP introduced a scalable method where agents learn from small FOV with a modified transformer for communication, improving performance in dense scenarios**Wang et al., "SCrimp: Scalable Communication in Multi-Agent Reinforcement Learning"**. Similarly, ALPHA combined local and global information, using a Graph Transformer to enhance decision-making and cooperation, addressing the limitations of limited FOV**Zhu et al., "ALPHA: Attention-based Local-Global Planning for MAPF"**. However, these methods often face challenges of scalability and complexity, struggling to handle instances with agent-dense environments.

\subsection{Sheaf Applications}
Sheaf theory addresses the local-to-global problem in multi-agent systems by allowing the coherent integration of local data into global structures**Bodnar et al., "Sheaves for Multi-Agent Systems"**. Grothendieck then extended its application in algebraic geometry through scheme theory, enabling the handling of complex structures like singularities**Grothendieck et al., "Theory of Schemes"**. In distributed systems, sheaf theory provides a framework for consensus on complex data structures**Bodnar et al., "Distributed Consensus with Sheaf Theory"**. In signal processing, it manages distributed data with complex dependencies, and in network communication, it optimizes information flow and reduces communication costs**Kolmogorov et al., "Information Flow in Networks"**. In network science, sheaves provide enhanced descriptions of network structures, capturing the nature of relationships between nodes**Bodnar et al., "Sheaf-Based Network Analysis"**. Within this framework, opinion dynamics uses discourse sheaves to model how opinions evolve and interact within social networks**Zhou et al., "Discourse Sheaves for Opinion Dynamics"**. Additionally, Bodnar proposed neural sheaf diffusion to learn sheaf laplacians from lower-order data, providing a novel topological perspective on heterophily and oversmoothing in GNNs**Bodnar et al., "Neural Sheaf Diffusion"**.


\begin{figure*}[t]
  \centering
  \includegraphics[scale=0.74]{pic/framework.pdf}
  \caption{Network structure of SIGMA. The observation encoder encodes observations in stalks (orange), the section FC learns restriction maps (green), and \(M(s)\) is included in the advantage function to enhance action evaluation. A global section loss (red) is then integrated into the network updating to align the policy with the sheaf structure.}
  \label{framework}
\end{figure*}