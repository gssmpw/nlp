%

\documentclass{article}

%
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} %
\usepackage{subcaption}
\usepackage[most]{tcolorbox}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{colortbl}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{tablefootnote}
\usepackage[para,online,flushleft]{threeparttable}
\setlist{label=\textbullet}
%
%
%
%
\usepackage[endLComment=,italicComments=false]{algpseudocodex}


%
%
%
%
%


%
\newcommand{\theHalgorithm}{\arabic{algorithm}}

%
%
%
\usepackage[accepted]{icml2025}

\usepackage{basic_commands}


%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

%
\usepackage[capitalize,noabbrev]{cleveref}
\renewcommand*{\ttdefault}{cmtt}

%
%
%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%
%
%
\usepackage[textsize=tiny]{todonotes}

\definecolor{myred}{HTML}{F54254}
\definecolor{myorange}{HTML}{FFB135}
\definecolor{mygreen}{HTML}{10BD35}
\definecolor{myblue}{HTML}{598BE7}
\definecolor{mypurple}{HTML}{9A1C6B}
\definecolor{plgray}{HTML}{999999}
\newcommand{\sh}[1]{{\color{myred}{\small\bf\sf [SP: #1]}}}
\newcommand{\pl}[1]{{\color{plgray} #1}}

\newcommand{\myo}{%
  \begin{tikzpicture}[baseline=-3.2pt, line width=1pt, color=myblue]
    \def\r{3.5pt}
    \draw circle (\r);
  \end{tikzpicture}%
}
\newcommand{\myx}{%
  \begin{tikzpicture}[baseline=-3.2pt, line width=1pt, color=myred]
    \def\r{3.5pt}
    \draw (-\r,-\r) -- (\r,\r) (-\r,\r) -- (\r,-\r);
  \end{tikzpicture}%
}
\newcommand{\mytri}{%
  \begin{tikzpicture}[baseline=-3.2pt, line width=1pt, color=myorange]
    \def\r{3.5pt}
    \draw (0, {sqrt(3)/2*\r}) -- (-\r,-{sqrt(3)/2*\r}) -- (\r,-{sqrt(3)/2*\r}) -- cycle;
  \end{tikzpicture}%
}

\hypersetup{urlcolor=myblue,citecolor=myblue,linkcolor=myblue}

%
%
%
%
\linepenalty=1000
%
%
%
%
%
%
%






%
%
\icmltitlerunning{Flow Q-Learning}

\begin{document}

\twocolumn[
\icmltitle{Flow Q-Learning}

%
%
%
%

%
%
%
%

%
%
%
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Seohong Park}{ucb}
\icmlauthor{Qiyang Li}{ucb}
\icmlauthor{Sergey Levine}{ucb}
\end{icmlauthorlist}

\icmlaffiliation{ucb}{University of California, Berkeley}

\icmlcorrespondingauthor{Seohong Park}{seohong@berkeley.edu}

%
%
%
%
\icmlkeywords{}

\vskip 0.3in
]

%

%
%
%
%
%

\printAffiliationsAndNotice{}  %
%

\begin{abstract}
We present flow Q-learning (FQL),
a simple and performant offline reinforcement learning (RL) method that leverages an expressive \emph{flow-matching} policy
to model arbitrarily complex action distributions in data.
Training a flow policy with RL is a tricky problem, due to the iterative nature of the action generation process.
We address this challenge
by training an expressive \emph{one-step} policy with RL,
rather than directly guiding an iterative flow policy to maximize values.
This way, we can completely avoid unstable recursive backpropagation,
eliminate costly iterative action generation at test time,
yet still mostly maintain expressivity.
We experimentally show that FQL leads to strong performance across
$73$ challenging state- and pixel-based OGBench and D4RL tasks
in offline RL and offline-to-online RL.

\url{https://seohong.me/projects/fql/}
%
%
%
%
%
\end{abstract}




\section{Introduction}
\label{sec:intro}





%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

Offline reinforcement learning (RL) enables training an effective decision-making policy
from a previously collected dataset without costly environment interactions~\citep{batch_lange2012, offline_levine2020}.
The essence of offline RL is constrained optimization:
the agent must maximize returns while staying within the dataset's state-action distribution~\citep{offline_levine2020}.
As datasets have grown larger and more diverse~\citep{oxe_collaboration2024},
their behavioral distributions have become more complex and multimodal,
and this often necessitates an expressive policy class~\citep{robomimic_mandlekar2021}
capable of capturing these complex distributions and implementing a more precise behavioral constraint.
In this work, we aim to develop a scalable offline RL method by
leveraging \emph{flow matching}~\citep{flow_lipman2023, flow_liu2023, flow_albergo2023},
a simple yet powerful generative modeling technique alternative to denoising diffusion~\citep{diffusion_sohl2015, ddpm_ho2020}.
By employing an expressive flow policy,
we can effectively model the arbitrarily complex action distribution of the dataset
and thus enforce an accurate behavioral constraint,
which is central to many offline RL algorithms~\citep{awac_nair2020, td3bc_fujimoto2021, rebrac_tarasov2023}.

However, leveraging flow or diffusion models to parameterize policies for offline RL is not a trivial problem.
Unlike with simpler policy classes, such as Gaussian policies, there is no straightforward way to train the flow or diffusion policies to maximize a learned value function,
%
due to the iterative nature of these generative models.
This is an example of a \emph{policy extraction} problem,
which is known to be a key challenge in offline RL in general~\citep{bottleneck_park2024}. 
Previous works have devised diverse ways to extract an iterative generative policy from a learned value function,
based on weighted regression, reparameterized policy gradient, rejection sampling, and other techniques.
While they have shown promising initial results,
these extraction schemes are often limited or not necessarily scalable to more complex problems,
due to their inherent drawbacks
(\eg, unstable backpropagation through time,
limited use of samples, and high computational cost; \Cref{sec:prev_solutions}).
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%


\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/fql.pdf}
    \vspace{-10pt}
    \caption{
    \footnotesize
    \textbf{Flow Q-learning.}
    Flow-matching policies can model complex action distributions,
    but training an iterative flow policy with RL is challenging.
    To address this, we train an expressive \emph{one-step} policy $:\mu_\omega(\pl{s}, \pl{z}): \gS \times \sR^d \to \gA$
    to maximize Q values,
    while regularizing it with distillation from a BC flow policy.
    %
    }
    \label{fig:fql}
\end{figure}



%
%
%
%
%
%
%
%
%
%
%
%

In this work, we propose a simple and effective way to leverage an expressive flow policy for offline RL.
Our main idea is to train an iterative flow policy \emph{only} with behavioral cloning (BC).
Instead, we train a separate, expressive \emph{one-step} policy
that maximizes values while \emph{distilling} from the flow model (\Cref{fig:fql}).
%
By lifting the burden of value maximization from the flow model,
we completely avoid the problems associated with steering the iterative process,
while fully leveraging the expressivity of the flow model.
Moreover, this procedure yields an expressive one-step policy as the output,
which eliminates costly iterative flow steps at evaluation time.
We call this method \textbf{flow Q-learning (FQL)}, which constitutes our main contribution.



FQL is simple: thanks to the simplicity of flow matching (especially compared to denoising diffusion),
it can be implemented within a few lines on top of the standard actor-critic framework (\Cref{alg:fql}).
%
%
%
Yet, FQL is highly effective and efficient.
Especially on complex tasks involving highly multimodal action distributions,
FQL often leads to significantly better performance than
both Gaussian and diffusion policy-based offline RL methods,
without requiring iterative flow steps at test time.
Moreover, FQL can be directly fine-tuned with online rollouts,
often outperforming existing offline-to-online RL methods.
We empirically show the effectiveness of FQL on $\mathbf{73}$ diverse state- and pixel-based tasks across
the recently proposed OGBench~\citep{ogbench_park2025} and standard D4RL~\citep{d4rl_fu2020} benchmarks.

%
%
%
%
%
%
%
%
%

%


%


\section{Preliminaries}
\label{sec:preliminaries}

\textbf{Offline RL.}
In this work, we assume a Markov decision process $\gM$~\citep{rl_sutton2005} defined by a tuple $(\gS, \gA, r, \rho, p)$,
where $\gS$ is the state space, $\gA = \sR^d$ is the $d$-dimensional action space,
$r(\pl{s}, \pl{a}): \gS \times \gA \to \sR$ is the reward function,
$\rho(\pl{s}) \in \Delta(\gS)$ is the initial state distribution,
and $p(\pl{s'} \mid \pl{s}, \pl{a}): \gS \times \gA \to \Delta(\gS)$ is the transition dynamics distribution,
where we denote the set of probability distributions over a space $\gX$ as $\Delta(\gX)$ and use \pl{gray} to denote placeholder variables.
The goal of offline RL is to find the parameter $\theta$ of a policy \mbox{$\pi_\theta(\pl{a} \mid \pl{s}):$} $\gS \to \Delta(\gA)$
%
that maximizes the average discounted return
$R(\pi_\theta) = \E_{\tau \sim p^{\pi_\theta}(\pl{\tau})}[\sum_{h=0}^H \gamma^h r(s_h, a_h)]$
from a dataset $\gD = \{\tau^{(n)}\}_{n \in \{1, 2, \dots, N\}}$ without environment interactions,
where $\tau$ denotes a trajectory $(s_0, a_0, \ldots, s_H, a_H)$,
%
$\gamma$ denotes a discount factor,
and $p^{\pi_\theta}(\tau)$ is defined as $\rho(s_0) \pi_\theta(a_0 \mid s_0)p(s_1 \mid s_0, a_0) \cdots \pi_\theta(a_H \mid s_H)$.
In this work,
we also consider offline-to-online RL, whose goal is to further fine-tune the offline pre-trained policy with a modest amount of online environment interactions. %

%
%
%
\textbf{Behavior-regularized actor-critic.}%
\footnote{Here, we use the term ``behavior-regularized actor-critic'' to refer to
a general framework encompassing a family of approaches, not solely the specific BRAC method~\citep{brac_wu2019}.}
Behavior-regularized actor-critic~\citep{brac_wu2019, td3bc_fujimoto2021, rebrac_tarasov2023}
is one of the simplest (yet effective) offline RL frameworks.
In its most basic form, it minimizes the following actor-critic losses:
\begin{align}
    \mkern-10mu\gL_Q(\phi) &= \E_{\substack{s, a, r, s' \sim \gD, \\ a' \sim \pi_\theta}}
    [
        (Q_\phi(s, a) - r - \gamma Q_{\bar \phi}(s', a'))^2
    ], \label{eq:brac_critic}
    \raisetag{2.22\normalbaselineskip}
    \\
    \mkern-10mu\gL_\pi(\theta) &=
    \E_{\substack{s, a \sim \gD, a^\pi \sim \pi_\theta}}[\underbrace{-Q_\phi(s, a^\pi)}_{\texttt{Q loss}}
    + \underbrace{\alpha \log \pi(a \mid s)}_{\texttt{BC loss}}], \label{eq:brac_actor}
    \raisetag{2.22\normalbaselineskip}
\end{align}
%
%
where $Q_\phi(\pl{s}, \pl{a}): \gS \times \gA \to \sR$ is a state-action value function with parameter $\phi$,
$Q_{\bar \phi}(\pl{s}, \pl{a})$ is a target network~\citep{dqn_mnih2013},
$\alpha$ is a hyperparameter that controls the strength of the behavioral cloning (BC) regularizer,
and $s, a, r, s' \sim \gD$ denotes uniform sampling over the dataset's transition tuples.
Intuitively, the critic loss $\gL_Q(\phi)$ minimizes the standard Bellman error,
while the actor loss $\gL_\pi(\theta)$ maximizes values with reparameterized gradients through $a^\pi$,
For the actor, the BC loss is additionally applied to
prevent the policy from deviating too much from the behavioral policy's distribution.
The policy is typically modeled by a Gaussian distribution to enable effective reparameterization.
Perhaps surprisingly,
despite its simplicity, behavior-regularized actor-critic is one of the most performant frameworks
on standard D4RL tasks~\citep{rebrac_tarasov2023}.
In this work, we build our flow-based offline RL method on a variant of the behavior-regularized actor-critic framework.
%
%

%
%
%

%
%
\textbf{Flow matching.}
Flow matching~\citep{flow_lipman2023, flow_liu2023, flow_albergo2023}
is a simpler alternative to denoising diffusion~\citep{diffusion_sohl2015, ddpm_ho2020, score_song2021}
for training iterative generative models.
Unlike denoising diffusion models, which are based on stochastic differential equations (SDEs),
flow models are rooted in deterministic ordinary differential equations (ODEs),
which enable significantly simpler training and faster inference,
while often achieving better quality~\citep{sd3_esser2024, flow_lipman2024}.

%
Given a data distribution $p(\pl{x}) \in \Delta(\sR^d)$ on a $d$-dimensional Euclidean space,
flow matching aims to fit the parameter $\theta$ of
a time-dependent velocity field $v_\theta(\pl{t}, \pl{x}): [0, 1] \times \sR^d \to \sR^d$
such that its corresponding \emph{flow}~\citep{smooth_lee2002} $\psi_\theta(\pl{t}, \pl{x}): [0, 1] \times \sR^d \to \sR^d$,
defined by the unique solution to the ODE
\begin{align}
    \frac{\de}{\de t} \psi_\theta(t, x) = v_\theta(\psi_\theta(t, x)),
\end{align}
transforms a simple distribution (\eg, unit Gaussian) at $t = 0$
into the target data distribution $p(\pl{x})$ at $t = 1$.


In this work, we consider the simplest variant of flow matching based on linear paths and uniform time sampling~\citep{flow_lipman2024}.
The objective is as follows:
\begin{align}
    \min_\theta \ \ \E_{\substack{x^0 \sim \gN(0, I_d), \\ x^1 \sim p(\pl{x}), \\ t \sim \mathrm{Unif}([0, 1])}}
    \left[\|v_\theta(t, x^t) - (x^1 - x^0)\|_2^2\right],
\end{align}
where $\gN(0, I_d)$ is the $d$-dimensional standard normal distribution,
$\mathrm{Unif}([0, 1])$ denotes the uniform distribution over the unit interval,
and $x^t = (1-t)x^0 + t x^1$ is the linear interpolation between $x^0$ and $x^1$.
Intuitively, the velocity field $v_\theta$ is trained to match
the average direction from randomly sampled $x^0$ and $x^1$.
At optimum, this objective produces a vector field that generates the data distribution $p(\pl{x})$.
At inference time, we generate samples by numerically solving the ODE defined by $v_\theta$.
In this work, we use the simplest Euler method, which we find to be sufficient.
%
See \citet{flow_lipman2024} for further details about flow matching.

\textbf{Flow policies.}
In this work, we use flow matching to train policies.
The most basic flow-matching objective for behavioral cloning is as follows:
\begin{align}
    \gL_\mathrm{Flow}(\theta) &= \E_{\substack{s, a=x^1 \sim \gD, \\ x^0 \sim \gN(0, I_d), \\ t \sim \mathrm{Unif}([0, 1])}}
    \left[
        \|v_\theta(t, s, x^t) - (x^1 - x^0)\|_2^2
    \right], \label{eq:flow_policy} \raisetag{1.3\normalbaselineskip}
\end{align}
where $v_\theta(\pl{t}, \pl{s}, \pl{x}): [0, 1] \times \gS \times \sR^d \to \sR^d$
is a state- and time-dependent vector field with parameter $\theta$.
Recall that $\gA$ is defined as $\sR^d$, and flow matching happens in the action space.
The state-dependent vector field generates a state-dependent flow $\psi_\theta(\pl{t}, \pl{s}, \pl{x}): [0, 1] \times \gS \times \sR^d \to \sR^d$, which serves as a policy.
For $s \in \gS$ and $z \in \sR^d$,
we simply denote the ODE's output $\psi_\theta(1, s, z)$ by $\mu_\theta(s, z)$.
Intuitively, $\mu_\theta$ maps the noise $z = x^0$ (sampled from the standard normal distribution)
to the action $a = \mu_\theta(s, z)$ by the ODE.

\textbf{\color{myblue}Notational warning:}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Note that $\mu_\theta(\pl{s}, \pl{z})$ is a \emph{deterministic function} from $\gS \times \sR^d$ to $\gA$,
but serves as a \emph{stochastic policy} from $\gS$ to $\gA$ due to the stochasticity of $z \sim \gN(0, I_d)$.
We denote the corresponding induced stochastic policy as $\pi_\theta(\pl{a} \mid \pl{s})$,
and loosely refer to both $\mu_\theta$ and $\pi_\theta$ as ``policies.''
%


\section{Flow Q-Learning}
\label{sec:method}

%
%
%
%
%
%

We now introduce our method for effective data-driven decision-making, \textbf{flow Q-learning (FQL)}.
Our desiderata are twofold:
we want to leverage an expressive flow-matching policy to deal with complex behavioral action distributions;
%
we also want to keep the method as simple as possible so that practitioners can easily implement and use it.
%
%
%
%

\textbf{Na\"ive approach.}
Perhaps the simplest way to train a flow policy for offline RL is
to replace the BC loss with a flow-matching loss (\Cref{eq:flow_policy})
in the behavior-regularized actor-critic framework (\Cref{eq:brac_actor}).
Formally, this na\"ive approach minimizes the actor loss $\gL_\pi(\theta)$ defined by
%
\begin{align}
    \gL_\pi(\theta) &=
    \underbrace{\E_{s \sim \gD, a^\pi \sim \pi_\theta}[-Q_\phi(s, a^\pi)]}_{\texttt{Q loss}}
    + \underbrace{\alpha \gL_\mathrm{Flow}(\theta)}_{\texttt{BC loss}}. \label{eq:dql_actor}
\end{align}
Intuitively, the corresponding flow policy $\pi_\theta$
is ``steered'' to maximize the value function while minimizing the BC loss.
This is analogous to Diffusion-QL~\citep{dql_wang2023} for diffusion policies.
However, unlike the Gaussian case,
%
the flow or diffusion objective requires \emph{backpropagation through time} in the Q loss (\Cref{eq:dql_actor})
due to the recursion in numerical ODE solvers (\eg, the Euler method) (\Cref{fig:diagram}a).
Unfortunately, this is often unstable and costly in practice, potentially leading to suboptimal performance,
as we will show in our experiments.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/diagram.pdf}
    %
    \caption{
    \footnotesize
    \textbf{The idea.}
    Offline RL is essentially a tug-of-war between behavioral regularization and value maximization.
    \emph{(a)} Na\"ively doing this with a flow policy involves costly and unstable backpropagation through time (BPTT).
    \emph{(b)} We resolve this by training a separate \emph{one-step} policy,
    which maximizes values without BPTT while being regularized by a distillation loss from a BC flow policy.
    }
    \label{fig:diagram}
\end{figure}





\textbf{Solution.}
%
Our main idea is to \textbf{not} steer the original flow policy at all.
Instead, we will train the flow policy only with the BC loss,
and train a separate expressive \emph{one-step} policy to maximize the value function
while regularizing it by a \emph{distillation} loss from the full BC flow policy.
Since the one-step policy does not involve any iterative procedures,
we can completely avoid backpropagation through time in the Q loss (\Cref{eq:dql_actor}).
We call this idea \textbf{one-step guidance}.
%

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/onestep.pdf}
    \vspace{-10pt}
    \caption{
    \footnotesize
    \textbf{One-step policy.}
    The one-step policy $\mu_\omega$ learns the \emph{direct} mapping from $z$ to $a$
    of the flow policy $\mu_\theta$, while simultaneously maximizing values (this part is omitted in the figure).
    }
    \label{fig:onestep}
\end{figure}
%

More formally,
we train a flow policy $\mu_\theta(\pl{s}, \pl{z})$ only with the BC flow-matching loss (\Cref{eq:flow_policy}).
Alongside,
we train a one-step prediction model $\mu_\omega(\pl{s}, \pl{z}): \gS \times \sR^d \to \gA$ with parameter $\omega$,
whose main role is to learn
the \emph{direct} mapping from noise $z$ to the output action of the full ODE flow policy $a = \mu_\theta(s, z)$,
%
%
while simultaneously maximizing values (\Cref{fig:onestep}).
The distillation loss is defined as follows:
\begin{align}
    \gL_\mathrm{Distill}(\omega) =
    \E_{\substack{s \sim \gD, \\ z \sim \gN(0, I_d)}}
    \left[\|\mu_\omega(s, z) - \mu_\theta(s, z)\|_2^2\right]. \label{eq:distill}
\end{align}
Recall that $\mu_\theta(s, z)$ denotes the output of the ODE defined by the vector field $v_\theta$ (\Cref{sec:preliminaries}).
Importantly, we note that it is possible to train an \emph{expressive} one-step model
that generates high-quality samples
with this sort of distillation loss~\citep{flow_liu2023, shortcut_frans2025}.
%
%
%
%
%
%
%
%
%
%




We are now ready to describe the complete objective of our method, \textbf{flow Q-learning (FQL)}.
FQL has three components: critic $Q_\phi(\pl{s}, \pl{a})$,
BC flow policy $\mu_\theta(\pl{s}, \pl{z})$, and one-step policy $\mu_\omega(\pl{s}, \pl{z})$.
%
First, as discussed above, the BC flow policy is trained \emph{only} with the BC flow-matching loss (\Cref{eq:flow_policy}).
The critic is trained with the original critic loss of behavior-regularized actor-critic (\Cref{eq:brac_critic}),
%
except that
we use the one-step policy $\pi_\omega$ in place of $\pi_\theta$.
%
%
Finally, the one-step policy is trained with the following actor loss:
\begin{align}
    \gL_\pi(\omega) &=
    \underbrace{\E_{s \sim \gD, a^\pi \sim {\color{myblue}\pi_\omega}}[-Q_\phi(s, a^\pi)]}_{\texttt{Q loss}}
    + \underbrace{\alpha \gL_\mathrm{Distill}(\omega)}_{\texttt{{\color{myblue}"BC"} loss}}. \label{eq:fql_actor}
\end{align}
Similar to the na\"ive flow actor loss above (\Cref{eq:dql_actor}),
this objective maximizes both the Q and BC losses with a hyperparameter $\alpha$.
However, it does not involve backpropagation over time as $\pi_\omega$ is a one-step policy.
Note also that the distillation loss now serves as a behavioral regularizer based on the BC flow policy (\Cref{fig:diagram}b).
The output of this algorithm is the one-step policy $\pi_\omega$,
which is what is deployed at test time.
We provide a pseudocode for FQL in \Cref{alg:fql}, in which $M$ denotes the number of steps for the Euler method,
and describe the full implementation details in \Cref{sec:impl_details}.
%


%
%
\begin{algorithm}[t!]
\caption{Flow Q-Learning (FQL)}
\label{alg:fql}
\begin{algorithmic}
\footnotesize

\BeginBox[fill=myblue!8]
\Function{$\mu_\theta(s, z)$}{} \Comment{\color{myblue} BC flow policy}
\For{$t = 0, 1, \dots, M-1$}
\State $z \gets z + v_\theta(t/M, s, z) / M$ \Comment{Euler method}
\EndFor
\State \Return $z$
\EndFunction
\EndBox

\vspace{5pt}

\While{not converged}

\State Sample batch $\{(s, a, r, s')\} \sim \gD$

\BeginBox[fill=white]
\LComment{\color{myblue} Train critic $Q_\phi$}
\State $z \sim \gN(0, I_d)$
\State $a' \gets \mu_\omega(s', z)$
\State Update {\color{myblue}$\phi$} to minimize $\E[(Q_{\color{myblue}\phi}(s, a) - r - \gamma Q_{\bar{\phi}}(s', a'))^2]$
%
\EndBox
\BeginBox[fill=white]
\LComment{\color{myblue} Train vector field $v_\theta$ in BC flow policy $\pi_\theta$}
\State $x^0 \sim \gN(0, I_d)$
\State $x^1 \gets a$
\State $t \sim \mathrm{Unif}([0, 1])$
\State $x^t \gets (1-t) x^0 + t x^1$
\State Update {\color{myblue}$\theta$} to minimize $\E[\|v_{\color{myblue}\theta}(t, s, x^t) - (x^1 - x^0)\|_2^2]$
\EndBox
\BeginBox[fill=white]
\LComment{\color{myblue} Train one-step policy $\pi_\omega$}
\State $z \sim \gN(0, I_d)$
\State $a^\pi \gets \mu_{\color{myblue}\omega}(s, z)$
\State Update {\color{myblue}$\omega$} to minimize $\E[-Q_\phi(s, a^\pi) + \alpha \|a^\pi - \mu_\theta(s, z)\|_2^2]$
\EndBox
\EndWhile
\Return One-step policy $\pi_\omega$

\end{algorithmic}
\end{algorithm}
%
%

%
%
\begin{tcolorbox}[
  enhanced,
  breakable,
  float,
  floatplacement=t!,
  title=\textbf{Remark:} Connection to Wasserstein Regularization,
  colframe=myblue,
  colback=myblue!8,
  coltitle=white,
  parbox=false,
  left=5pt,
  right=5pt,
  grow to left by=3pt,
  grow to right by=3pt,
  %
  %
  toprule=2pt,
  titlerule=1pt,
  leftrule=1pt,
  rightrule=1pt,
  bottomrule=1pt,
]
Our distillation loss in \Cref{eq:distill} has an intriguing connection to \emph{Wasserstein} behavioral regularization.
Let $\xi$ be a random variable following the $d$-dimensional standard normal distribution, $\gN(0, I_d)$.
For $s \in \gS$,
let $\nu_\theta(s), \nu_\omega(s) \in \Delta(\gA)$ be the push-forward distributions
of $\xi$ by $\mu_\theta(s, \cdot)$ and $\mu_\omega(s, \cdot)$, respectively.
Then, the distillation loss in \Cref{eq:distill}
is an upper bound on the squared $2$-Wasserstein distance between $\nu_\omega(s)$ and $\nu_\theta(s)$:
\begin{align}
    \gL_\mathrm{Distill}(\omega) &=
    \E_{\substack{s \sim \gD, \\ z \sim \gN(0, I_d)}}
    \left[\|\mu_\omega(s, z) - \mu_\theta(s, z)\|_2^2\right] \nonumber \\
    &\geq \E_{s \sim \gD}\left[\inf_{\lambda \in \Lambda(\nu_\omega, \nu_\theta)} \E_{x, y \sim \lambda}[\|x - y\|_2^2]\right] \nonumber \\
    &= \E_{s \sim \gD}\left[W_2(\nu_\omega, \nu_\theta)^2\right],
\end{align}
where $\Lambda(\nu_\omega, \nu_\theta)$ denotes the set of coupling distributions of $\nu_\omega$ and $\nu_\theta$,
and $W_2$ denotes the $2$-Wasserstein distance with the Euclidean metric in the action space. %

\begin{table}[H]
\caption{
\footnotesize
\textbf{Behavioral regularizers in offline RL.}
}
\label{table:reg}
\centering
\vspace{5pt}
\scalebox{0.8}
{
\begin{tabular}{ccc}
\toprule
\textbf{Offline RL Method} & \textbf{Behavioral Regularizer} & \textbf{Metric-Aware?} \\
\midrule
TD3+BC & $D_\mathrm{KL}$ & \myx \\
AWAC & $D_\mathrm{KL}$ & \myx \\
CQL & $\chi^2$ & \myx \\
FQL (ours) & $W_2^2$ & \myo \\
\bottomrule
\end{tabular}
}
\end{table}

Hence, the BC term in the FQL actor loss (\Cref{eq:fql_actor}) can be interpreted as
an upper bound on the squared $2$-Wasserstein distance between
the current policy $\pi_\omega$ and the data-collecting policy approximated by $\pi_\theta$.
This Wasserstein regularizer is analogous to
the KL behavioral regularizer in TD3+BC~\citep{td3bc_fujimoto2021} and AWAC~\citep{awac_nair2020},
and the $\chi^2$ behavioral regularizer in CQL~\citep{cql_kumar2020, xql_garg2023}.
However, unlike the KL and $\chi^2$ divergences, which are (in principle) invariant and agnostic to any metric structures,%
\footnote{While the original KL and $\chi^2$ divergences are entirely metric-agnostic,
this property may be lost in practice with variational approximation
%
(\eg, with a Gaussian parameterization).}
our $2$-Wasserstein distance is \emph{aware} of the metric structure over actions (which we impose as the Euclidean distance) (\Cref{table:reg}).
%
This metric-aware property potentially incorporates a better inductive bias about the similarity between actions,
akin to how Wasserstein distances improve upon metric-agnostic divergences in other contexts in machine learning~\citep{wgan_arjovsky2017, metra_park2024}.

\end{tcolorbox}
%
%


\clearpage




\textbf{Why is it a good idea?}
FQL has three benefits.
First, it leverages reparameterized policy gradient (\ie, directly maximizing the Q function with gradients through $a^\pi$),
%
which is known to be one of the most effective policy extraction methods~\citep{bottleneck_park2024},
while entirely avoiding unstable and costly backpropagation through time.
We will revisit this point in more detail in \Cref{sec:prev_solutions},
and empirically show its effectiveness through our experiments (\Cref{sec:results}).
Second, FQL yields an efficient one-step policy as the output,
which eliminates iterative flow generation processes at inference time,
while maintaining most of the expressivity of the full flow model~\citep{flow_liu2023, shortcut_frans2025}.
%
Third, FQL is easy-to-implement and easy-to-tune:
thanks to the simplicity of flow-matching,
it can be implemented in a few lines on top of the standard behavior-regularized actor-critic framework,
and has only one major hyperparameter $\alpha$,
without requiring tuning a noise schedule.

%

\section{Prior Work}
\label{sec:related}

\textbf{Offline RL and offline-to-online RL.}
%
The goal of offline RL is to train a policy using only previously collected data.
Hundreds of offline RL methods and techniques have been proposed so far,
and many of them are based on a single central idea:
maximizing the return while minimizing a discrepancy measure between
the state-action distribution of the dataset and that of the learned policy~\citep{offline_levine2020, dualrl_sikchi2024}.
Previous works have implemented this high-level objective in diverse ways through
behavioral regularization~\citep{awac_nair2020, td3bc_fujimoto2021, rebrac_tarasov2023},
conservatism~\citep{cql_kumar2020},
in-sample maximization~\citep{iql_kostrikov2022, sql_xu2023, xql_garg2023},
out-of-distribution detection~\citep{mopo_yu2020, morel_kidambi2020, edac_an2021, sacrnd_nikulin2023},
dual RL~\citep{optidice_lee2021, dualrl_sikchi2024},
and generative modeling~\citep{dt_chen2021, tt_janner2021, diffuser_janner2022}.
After finishing offline RL training, we can further fine-tune the policy with additional online rollouts.
This setting is often referred to as offline-to-online RL,
for which several techniques have been proposed~\citep{o2o_lee2021, hybridrl_song2023, calql_nakamoto2023, rlpd_ball2023, aca_yu2023}.
Our method, FQL, is mainly designed for offline RL,
but we show that it can also be directly fine-tuned with online rollouts without any algorithmic changes.
%
%
%

\textbf{RL with diffusion and flow models.}
Motivated by the recent successes of iterative generative modeling techniques,
such as denoising diffusion~\citep{diffusion_sohl2015, ddpm_ho2020, cg_dhariwal2021}
and flow matching~\citep{flow_lipman2023, sd3_esser2024},
researchers have developed diverse ways to integrate them into RL.
Previous works have applied iterative generative models to
planning and hierarchical learning~\citep{diffuser_janner2022, dd_ajay2023, guidedflow_zheng2023, adaptdiffuser_liang2023, hdmi_li2023, sgp_suh2023, ldcq_venkatraman2024, hd_chen2024},
world modeling and data augmentation~\citep{synther_lu2023, dwm_ding2024, pgd_jackson2024, diamond_alonso2024},
and policy modeling (\Cref{sec:prev_solutions}).
Our method belongs to the third category,
where we model a policy with an expressive flow network
to capture the arbitrarily complex distribution of the behavioral policy.
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{How Have Previous Works Trained Diffusion and Flow Policies with RL?}
\label{sec:prev_solutions}

%


%
%
%
%
%

Various approaches have been proposed for training diffusion or flow policies with RL.
In this section, we provide an in-depth review of these methods, discuss their advantages and limitations,
and explain how FQL relates to prior work.
Prior methods can be categorized into several groups based on their \emph{policy extraction} strategies~\citep{bottleneck_park2024}.

%
%
%
%
%
%

%
%
%
%
%
%
%
%

%

\textbf{(1) Weighted behavioral cloning.}
One straightforward approach to modulating a diffusion or flow policy is
to assign \emph{weights} to transition samples based on the corresponding learned values. 
The most basic form uses advantage-weighted regression (AWR)~\citep{rwr_peters2007, awr_peng2019, awac_nair2020}
with the following objective:
\begin{align}
    %
    \max_\theta \ \ \E_{s, a \sim \gD}\left[e^{\alpha(Q(s, a) - V(s))} \gL_\mathrm{Flow}(\theta)\right],
    \label{eq:flow_awr}
\end{align}
where $\alpha$ is an inverse temperature hyperparameter,
and $Q(\pl{s}, \pl{a}): \gS \times \gA \to \sR$ and $V(\pl{s}): \gS \to \sR$ are
state-action and state value functions, respectively~\citep{rl_sutton2005}.
For diffusion policies, $\gL_\mathrm{Flow}(\theta)$ is replaced with a diffusion loss.
Intuitively, this objective makes the policy selectively clone transitions with high advantages.
Among previous works, 
QGPO~\citep{qgpo_lu2023},
EDP~\citep{edp_kang2023},
QVPO~\citep{qvpo_ding2024},
and QIPO~\citep{qipo_anon2025} are mainly based on weighted behavioral cloning.

Weighted behavioral cloning is simple and easy to implement.
However, it is known to be one of the least effective policy extraction methods~\citep{closer_fu2022, bottleneck_park2024},
due to the small number of effective samples and limited expressivity.%
%
\footnote{See \citet{bottleneck_park2024} for further discussions.}
In our experiments, we empirically show that weighted behavioral cloning
generally leads to subpar performance, especially on complex tasks.

%
%
%


\textbf{(2) Reparameterized policy gradient.}
Another popular approach to guide an iterative generative model is
to directly maximize the value function $Q(\pl{s}, \pl{a})$ with reparameterized gradients,
while regularizing it with a flow or diffusion loss,
as in \Cref{eq:dql_actor}.
Among previous approaches,
Diffusion-QL~\citep{dql_wang2023},
DiffCPS~\citep{diffcps_he2023},
Consistency-AC~\citep{consistencyac_ding2024},
SRDP~\citep{srdp_ada2024},
and EQL~\citep{entropydql_zhang2024}
implement this scheme with backpropagation through time.

Reparameterized policy gradient is known to be one of the most effective policy extraction methods for Gaussian policies~\citep{bottleneck_park2024}.
However, when na\"ively applied to iterative generative models,
it requires backpropagation through time (\Cref{eq:fql_actor}),
which often incurs stability issues %
and leads to suboptimal performance (\Cref{sec:results}).

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\textbf{(3) Rejection sampling.}
The third category is rejection sampling.
Instead of adjusting the parameter of the generative model,
we can sample $N$ actions from a \emph{fixed} BC policy, and select the action that has the highest value.
In other words, we treat the following formula as a policy:
\begin{align}
    \argmax_{\substack{a_1, \dots, a_N \text{$:$ } a_i \sim \pi^\beta}} \ \ Q(s, a_i), \label{eq:rejection}
\end{align}
where $\pi^\beta$ is a BC policy trained by a flow or diffusion objective.
Among previous works, 
SfBC~\citep{sfbc_chen2023},
IDQL~\citep{idql_hansenestruch2023},
and AlignIQL~\citep{aligniql_he2024}
are based on (variants of) rejection sampling.

Rejection sampling is simple and stable.
However, it requires querying the policy and value function $N$ times
at \emph{every} environment step during inference (and possibly during training as well, depending on the method).
This can be prohibitive with larger models or a larger number of samples.
%

%

%
%
%
%
%
%
%

\textbf{(4) Others.}
Besides these three major categories,
other techniques have also been proposed to guide a diffusion policy to maximize the learned value function,
based on some combination of the above strategies~\citep{diffusiondice_mao2024},
action gradients~\citep{dipo_yang2023, ddiffpg_li2024, parl_mark2024, dac_anonymous2025},
bi-level MDPs~\citep{dppo_ren2025},
value alignment~\citep{eda_chen2024},
and implicit Q-learning~\citep{srpo_chen2024, dtql_chen2024}.

\textbf{Contextualizing FQL in prior work.}
Our approach, FQL, falls into the second category, reparameterized policy gradient,
which is known to be one of the most effective policy extraction schemes~\citep{bottleneck_park2024}.
However, unlike the previous methods discussed above in the same category, which use backpropagation through time,
we entirely bypass recursive backpropagation
by only steering the one-step policy to maximize values (\Cref{eq:fql_actor}),
while training the flow policy solely with the BC loss.
%
%
%
%
Among previous works,
Consistency-AC~\citep{consistencyac_ding2024}, SRPO~\citep{srpo_chen2024}, and DTQL~\citep{dtql_chen2024}
also employ distillation,
and in particular, Consistency-AC~\citep{consistencyac_ding2024} shares a conceptually similar high-level objective to our method
(but with consistency models instead of direct one-step distillation).
However, they either still use backpropagation through time~\citep{consistencyac_ding2024}
or are based on implicit Q-learning~\citep{iql_kostrikov2022},
which is known to be less effective than actor-critic learning~\citep{rebrac_tarasov2023}.
In contrast, we train a \emph{one-step} policy within a more effective actor-critic framework,
with no backpropagation through time.
In our experiments,
we empirically show that our approach leads to significantly better performance
than previous distillation-based methods (Consistency-AC and SRPO) as well as other policy extraction schemes.
%

\begin{table*}[t!]
\vspace{-10pt}
\caption{
\footnotesize
\textbf{Offline RL results.}
FQL achieves the best or near-best performance on most of the $\mathbf{73}$ diverse, challenging benchmark tasks.
%
%
The performances are averaged over $\mathbf{8}$ seeds ($\mathbf{4}$ seeds for pixel-based tasks),
but the cells without the ``$\pm$'' sign indicate that the numbers are taken
from prior works~\citep{corl_tarasov2023, idql_hansenestruch2023, srpo_chen2024}.
See \Cref{table:offline_full} for the full results.
}
\label{table:offline}
\centering
\vspace{5pt}
\scalebox{0.76}
{
%
\begin{threeparttable}
\begin{tabular}{lcccccccccc}
\toprule
%
%
%
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\texttt{Gaussian Policies}} & \multicolumn{3}{c}{\texttt{Diffusion Policies}} & \multicolumn{4}{c}{\texttt{Flow Policies}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-11}
\texttt{Task Category} & \texttt{BC} & \texttt{IQL} & \texttt{ReBRAC} & \texttt{IDQL} & \texttt{SRPO} & \texttt{CAC} & \texttt{FAWAC} & \texttt{FBRAC} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\midrule



\texttt{OGBench antmaze-large-singletask ($\mathbf{5}$ tasks)} & $11$ {\tiny $\pm 1$} & $53$ {\tiny $\pm 3$} & $\mathbf{81}$ {\tiny $\pm 5$} & $21$ {\tiny $\pm 5$} & $11$ {\tiny $\pm 4$} & $33$ {\tiny $\pm 4$} & $6$ {\tiny $\pm 1$} & $60$ {\tiny $\pm 6$} & $28$ {\tiny $\pm 5$} & $\mathbf{79}$ {\tiny $\pm 3$} \\
\texttt{OGBench antmaze-giant-singletask ($\mathbf{5}$ tasks)} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 1$} & $\mathbf{26}$ {\tiny $\pm 8$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 4$} & $3$ {\tiny $\pm 2$} & $9$ {\tiny $\pm 6$} \\
\texttt{OGBench humanoidmaze-medium-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 1$} & $33$ {\tiny $\pm 2$} & $22$ {\tiny $\pm 8$} & $1$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $53$ {\tiny $\pm 8$} & $19$ {\tiny $\pm 1$} & $38$ {\tiny $\pm 5$} & $\mathbf{60}$ {\tiny $\pm 14$} & $\mathbf{58}$ {\tiny $\pm 5$} \\
\texttt{OGBench humanoidmaze-large-singletask ($\mathbf{5}$ tasks)} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 0$} & $\mathbf{11}$ {\tiny $\pm 2$} & $4$ {\tiny $\pm 2$} \\
\texttt{OGBench antsoccer-arena-singletask ($\mathbf{5}$ tasks)} & $1$ {\tiny $\pm 0$} & $8$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $12$ {\tiny $\pm 4$} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 4$} & $12$ {\tiny $\pm 0$} & $16$ {\tiny $\pm 1$} & $33$ {\tiny $\pm 6$} & $\mathbf{60}$ {\tiny $\pm 2$} \\
\texttt{OGBench cube-single-singletask ($\mathbf{5}$ tasks)} & $5$ {\tiny $\pm 1$} & $83$ {\tiny $\pm 3$} & $91$ {\tiny $\pm 2$} & $\mathbf{95}$ {\tiny $\pm 2$} & $80$ {\tiny $\pm 5$} & $85$ {\tiny $\pm 9$} & $81$ {\tiny $\pm 4$} & $79$ {\tiny $\pm 7$} & $79$ {\tiny $\pm 2$} & $\mathbf{96}$ {\tiny $\pm 1$} \\
\texttt{OGBench cube-double-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 1$} & $7$ {\tiny $\pm 1$} & $12$ {\tiny $\pm 1$} & $15$ {\tiny $\pm 6$} & $2$ {\tiny $\pm 1$} & $6$ {\tiny $\pm 2$} & $5$ {\tiny $\pm 2$} & $15$ {\tiny $\pm 3$} & $14$ {\tiny $\pm 3$} & $\mathbf{29}$ {\tiny $\pm 2$} \\
\texttt{OGBench scene-singletask ($\mathbf{5}$ tasks)} & $5$ {\tiny $\pm 1$} & $28$ {\tiny $\pm 1$} & $41$ {\tiny $\pm 3$} & $46$ {\tiny $\pm 3$} & $20$ {\tiny $\pm 1$} & $40$ {\tiny $\pm 7$} & $30$ {\tiny $\pm 3$} & $45$ {\tiny $\pm 5$} & $30$ {\tiny $\pm 3$} & $\mathbf{56}$ {\tiny $\pm 2$} \\
\texttt{OGBench puzzle-3x3-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 1$} & $21$ {\tiny $\pm 1$} & $10$ {\tiny $\pm 2$} & $18$ {\tiny $\pm 1$} & $19$ {\tiny $\pm 0$} & $6$ {\tiny $\pm 2$} & $14$ {\tiny $\pm 4$} & $19$ {\tiny $\pm 1$} & $\mathbf{30}$ {\tiny $\pm 1$} \\
\texttt{OGBench puzzle-4x4-singletask ($\mathbf{5}$ tasks)} & $0$ {\tiny $\pm 0$} & $7$ {\tiny $\pm 1$} & $14$ {\tiny $\pm 1$} & $\mathbf{29}$ {\tiny $\pm 3$} & $10$ {\tiny $\pm 3$} & $15$ {\tiny $\pm 3$} & $1$ {\tiny $\pm 0$} & $13$ {\tiny $\pm 1$} & $25$ {\tiny $\pm 5$} & $17$ {\tiny $\pm 2$} \\
\texttt{D4RL antmaze ($\mathbf{6}$ tasks)} & $17$ & $57$ & $78$ & $79$ & $74$ & $30$ {\tiny $\pm 3$} & $44$ {\tiny $\pm 3$} & $64$ {\tiny $\pm 7$} & $65$ {\tiny $\pm 7$} & $\mathbf{84}$ {\tiny $\pm 3$} \\
\texttt{D4RL adroit ($\mathbf{12}$ tasks)} & $48$ & $53$ & $\mathbf{59}$ & $52$ {\tiny $\pm 1$} & $51$ {\tiny $\pm 1$} & $43$ {\tiny $\pm 2$} & $48$ {\tiny $\pm 1$} & $50$ {\tiny $\pm 2$} & $52$ {\tiny $\pm 1$} & $52$ {\tiny $\pm 1$} \\
\texttt{Visual manipulation ($\mathbf{5}$ tasks)} & - & $42$ {\tiny $\pm 4$} & $60$ {\tiny $\pm 2$} & - & - & - & - & $22$ {\tiny $\pm 2$} & $50$ {\tiny $\pm 5$} & $\mathbf{65}$ {\tiny $\pm 2$} \\



\bottomrule
\end{tabular}
\begin{tablenotes}
\item[1] Due to the high computational cost of pixel-based tasks,
we selectively benchmark $5$ methods that achieve strong performance on state-based OGBench tasks.
\end{tablenotes}
\end{threeparttable}
}
%
\end{table*}




\section{Experiments}
\label{sec:results}

In this section, we empirically evaluate the performance of FQL,
comparing it to previous offline RL and offline-to-online RL approaches
on a variety of challenging tasks.
We also provide extensive analyses and ablations
on policy extraction strategies and FQL's design choices.

\subsection{Experimental Setup}

\textbf{Benchmarks.}
We use the recently proposed \mbox{\textbf{OGBench}} task suite~\citep{ogbench_park2025} as the main benchmark (\Cref{fig:ogbench}).
OGBench provides a number of diverse, challenging tasks across robotic locomotion and manipulation, with both state and pixel observations,
where these tasks are generally more challenging than standard D4RL tasks~\citep{d4rl_fu2020},
which have been saturated as of 2025~\citep{rebrac_tarasov2023, d5rl_rafailov2024, bottleneck_park2024}.
While OGBench was originally designed for benchmarking offline goal-conditioned RL,
we use its reward-based single-task variants (``\texttt{-singletask}'')
to make it compatible with standard reward-maximizing offline RL algorithms.
We employ $5$ locomotion %
and $5$ manipulation environments %
where each environment provides $5$ separate tasks, bringing the total to $\mathbf{50}$ state-based OGBench tasks.
In addition, we consider $5$ diverse OGBench \textbf{visual} manipulation tasks
to challenge the agent's ability to handle $64 \times 64 \times 3$-sized image observations. 
Finally, we also employ relatively challenging $6$ \texttt{antmaze} and $12$ \texttt{adroit} tasks
from the \textbf{D4RL} benchmark.
%
%

\textbf{Methods.}
For our offline RL experiments,
we use the following $9$ recent methods as representative examples of a variety of algorithm types and policy extraction strategies.

\textbf{(1) Gaussian policies.}
For standard offline RL methods that use Gaussian policies,
we consider BC,
%
IQL~\citep{iql_kostrikov2022}, and ReBRAC~\citep{rebrac_tarasov2023}.
In particular, ReBRAC is known to achieve state-of-the-art performance on many D4RL tasks~\citep{corl_tarasov2023},
and is the closest Gaussian baseline to FQL in that both are based on behavior-regularized actor-critic (\Cref{sec:preliminaries}).

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/ogbench.pdf}
    \vspace{-10pt}
    \caption{
    \footnotesize
    \textbf{OGBench tasks.} %
    }
    \label{fig:ogbench}
    \vspace{-5pt}
\end{figure}


\textbf{(2) Diffusion policies.}
For diffusion policy-based offline RL methods,
we consider IDQL~\citep{idql_hansenestruch2023}, SRPO~\citep{srpo_chen2024}, and Consistency-AC (CAC)~\citep{consistencyac_ding2024}.
IDQL is based on rejection sampling, and SRPO and CAC are based on policy distillation, as in FQL.
In particular, CAC is the closest diffusion baseline to FQL,
in that they both train distillation policies within the behavior-regularized actor-critic framework,
although CAC still employs backpropagation through time (but with fewer steps)
and is based on consistency models rather than direct one-step distillation.

\textbf{(3) Flow policies.}
Since there are currently only a few prior methods that explicitly employ flow policies~\citep{qipo_anon2025},
we consider flow variants of previous methods to cover the three main policy extraction schemes
discussed in \Cref{sec:prev_solutions}.
Flow advantage-weighted actor-critic (FAWAC) is a flow variant of AWAC~\citep{awac_nair2020},
which uses AWR (\Cref{eq:flow_awr}) as the policy learning objective, conceptually similar to QIPO~\citep{qipo_anon2025}.
Flow behavior-regularized actor-critic (FBRAC) is the flow counterpart of Diffusion-QL (DQL)~\citep{dql_wang2023}
based on the na\"ive Q loss with backpropagation through time (\Cref{eq:dql_actor}). 
Implicit flow Q-learning (IFQL) is the flow counterpart of IDQL based on rejection sampling (\Cref{eq:rejection}).
Notably, FAWAC and FBRAC are different from our method (FQL) \emph{only} by their policy extraction strategies
while sharing the exact same architectures and implementations,
and thus can provide controlled ablation results on our distillation-based policy extraction scheme.

For offline-to-online RL experiments, we consider three prior offline RL methods (IQL, ReBRAC, and IFQL)
that support fine-tuning and achieve strong performance.
%
Additionally, we consider two performant methods specifically designed for data-driven online RL,
Cal-QL~\citep{calql_nakamoto2023} and RLPD~\citep{rlpd_ball2023}.

\textbf{Evaluation.}
For offline RL, we evaluate the performance of methods after a fixed number of gradient steps;
in particular, we do \emph{not} report the best performance across different evaluation epochs
as it may bias results~\citep{corl_tarasov2023}.
To ensure fair comparisons, we \emph{individually} tune hyperparameters of the baselines with similar amounts of training budget (\Cref{sec:exp_details_methods}),
and use the same network size and discount factor, unless otherwise stated.
We use $\mathbf{8}$ seeds for state-based tasks and $\mathbf{4}$ seeds for pixel-based tasks,
and present standard deviations after ``$\pm$'' in tables
and $95\%$ bootstrap confidence intervals as shaded areas in plots,
unless otherwise mentioned.
In tables, we denote values at or above $95\%$ of the best performance in bold,
following OGBench~\citep{ogbench_park2025}.
We refer to \Cref{sec:exp_details} for the full training and evaluation details.

\subsection{Results and Q\&As}
%

We present our results via the following Q\&As.

\ul{\textbf{Q: How good is FQL for offline RL?}}

%
\textbf{A:} FQL achieves the best or near-best performance on most tasks, especially in complex manipulation environments.

\Cref{table:offline} summarizes the aggregated benchmarking result on a total of $73$ state- or pixel-based offline RL tasks
across robotic locomotion and manipulation.
%
We find that FQL generally achieves better performance than previous methods,
including ones based on Gaussian and diffusion policies.
In particular, FQL leads to consistently better performance than its closest diffusion baseline (CAC),
and often significantly outperforms its closest Gaussian baseline (ReBRAC) especially on manipulation tasks,
which feature highly multimodal distributions.
We also highlight that FQL achieves the best performance of $\mathbf{84}\%$
on one of the hardest tasks in the D4RL benchmark,
\texttt{antmaze-large-play} (\Cref{table:offline_full}).
%
%
%


\ul{\textbf{Q: Can't I just use existing policy extraction schemes?}}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/policy_ext.pdf}
    \vspace{-5pt}
    \caption{
    \footnotesize
    \textbf{Policy extraction is important.}
    The bars above compare the performances of different policy extraction methods averaged over the $50$ state-based OGBench tasks in \Cref{table:offline}.
    %
    %
    }
    \label{fig:policy_ext}
\end{figure}

\textbf{A:} You can, but previous policy extraction schemes generally lead to (often \emph{much}) worse performance.

This can be seen by comparing the performances of FQL and \{FAWAC, FBRAC, IFQL\},
which are the closest flow-based baselines to FQL,
but with different policy extraction mechanisms.
In particular, FBRAC is \textbf{exactly} the same as FQL except that it uses backpropagation through time.
We emphasize again that these baselines are implemented on the same codebase, use the same architecture,
and are individually tuned for each environment (\Cref{table:method_hyp}).
%
\Cref{fig:policy_ext} compares their offline RL performances aggregated over the $50$ state-based OGBench tasks in \Cref{table:offline}.
The results show that policy extraction alone can significantly affect performance,
consistent with findings in Gaussian policies~\citep{bottleneck_park2024}.
The results also indicate that our one-step guidance is the most effective,
significantly outperforming the other previous extraction strategies (\Cref{sec:prev_solutions}).
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

%
%
%
%



%
%
%



%
%
%
%
%

\ul{\textbf{Q: Can FQL be fine-tuned with online rollouts?}}


\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/o2o_agg.pdf}
    \vspace{-20pt}
    \caption{
    \footnotesize
    \textbf{Offline-to-online RL results ($\mathbf{8}$ seeds).}
    Fine-tuning starts at $1$M.
    The D4RL results of Cal-QL, ReBRAC, and IQL are taken from \citet{corl_tarasov2023}.
    See \Cref{fig:o2o_full} for the full plots.
    }
    \label{fig:o2o}
\end{figure}


\textbf{A:}
Yes, FQL can be directly fine-tuned without any modifications,
and often significantly outperforms previous methods.

Specifically, we can fine-tune FQL simply by adding new online transitions to the dataset $\gD$,
while continuing to train all networks using the same objective as in offline training.
To show how effective FQL is for fine-tuning,
we evaluate it on $5$ representative OGBench tasks across different categories (\Cref{table:o2o_full})
as well as the $10$ D4RL \texttt{antmaze} and \texttt{adroit} tasks used by \citet{corl_tarasov2023}.
\Cref{fig:o2o} shows the training curves of FQL and previous approaches on these $15$ tasks,
where online fine-tuning starts at $1$M gradient steps (see \Cref{fig:o2o_full} and \Cref{table:o2o_full} for the full results).
The results show that FQL achieves the best fine-tuning performance
compared to both previous offline RL approaches (including IFQL, the strongest flow-based baseline)
and methods specifically designed for online fine-tuning (Cal-QL and RLPD).



%

%

\ul{\textbf{Q: What are the important hyperparameters of FQL?}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/abl_coef.pdf}
    \vspace{-20pt}
    \caption{
    \footnotesize
    %
    \textbf{The BC coefficient $\alpha$ needs to be tuned.}
    The plots show how different values of $\alpha$ affect offline RL performance.
    }
    \label{fig:abl_coef}
\end{figure}

\textbf{A:}
The most important hyperparameter is the BC coefficient.

\Cref{fig:abl_coef} shows the ablation results of the BC coefficient $\alpha$ on three tasks.
This hyperparameter needs to be tuned for each environment based on the suboptimality of the dataset,
as is typical for most offline RL methods~\citep{corl_tarasov2023, bottleneck_park2024}.
Other than $\alpha$, the default hyperparameters of FQL work well,
although tuning some additional hyperparameters
(\eg, target value aggregation described in \Cref{sec:impl_details})
can slightly boost performance on some tasks.
We provide an extensive ablation study on a total of $4$ factors of FQL in \Cref{sec:abl}.
%
%

\ul{\textbf{Q: Do I need to tune flow-related hyperparameters?}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/abl_t_dist.pdf}
    \vspace{-20pt}
    \caption{
    \footnotesize
    %
    \textbf{You can just use the uniform time distribution.}
    FQL's performance is generally robust to flow-related hyperparameters.
    }
    \label{fig:abl_t_dist}
\end{figure}

\textbf{A:} No, in general.

For example, \Cref{fig:abl_t_dist} shows how the time sampling distribution for flow matching affects performance,
where we consider the uniform distribution, $\mathrm{Unif}([0, 1])$ (default),
the beta distribution used by \citet{pi0_black2024},
and the logit normal distribution used by \citet{sd3_esser2024}.
The results suggest that time distributions matter only marginally,
and the simplest uniform distribution is often sufficient to achieve the best performance.
Similarly, we find that the performance is generally robust to the number of flow steps (the default is $10$),
as long as it is not too small (see \Cref{sec:abl}).

\clearpage

\ul{\textbf{Q: How fast is FQL?}}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.65\linewidth]{figures/speed_states.pdf}
    \vspace{-5pt}
    \caption{
    \footnotesize
    \textbf{Run time comparison on \texttt{cube-double}.}
    }
    \label{fig:speed}
\end{figure}
%

\textbf{A:}
FQL is one of the fastest flow-based offline RL methods.

\Cref{fig:speed} shows that,
in terms of both training and inference costs,
FQL is only slightly slower than Gaussian policy-based offline RL methods,
while being faster than most flow-based baselines.
See \Cref{fig:speed_full} for the detailed comparison results. %

\ul{\textbf{Q: Are flow policies better than diffusion policies?}}

\textbf{A:} Maybe, but we do \textbf{not} make such a claim in this paper.

The main contribution of this paper is our \emph{policy extraction} scheme
(one-step guidance), not just the use of flow matching itself.
Although we show that one-step guidance combined with flow matching (\ie, FQL) achieves better performance than
previous policy extraction schemes for diffusion and flow policies (\Cref{table:offline}),
we believe it is possible to apply our one-step guidance to diffusion policies
with appropriate modifications to convert SDEs to ODEs~\citep{score_song2021} to achieve similar performance,
given the equivalence between the two frameworks~\citep{diffusionflow_gao2024}.
Nevertheless, flow matching has one arguably clear advantage over denoising diffusion:
it is \emph{much} simpler to implement!

\section{Closing Remarks}
%

We presented flow Q-learning (FQL), a simple and performant offline RL method that leverages an expressive flow policy
and reparameterized policy gradients, without suffering from backpropagation through time.
We showed that FQL generally leads to the best performance on challenging tasks across robotic locomotion and manipulation,
offline RL and offline-to-online RL,
as well as state- and pixel-based settings.
FQL, however, is not perfect;
see \Cref{sec:limitations} for the limitations of FQL.

As a closing remark, we would like to reiterate one particularly appealing property of FQL --- \textbf{simplicity}:
%
one small algorithm box (\Cref{alg:fql}) essentially captures the entire training objectives of FQL (modulo minor details),
%
\emph{including} all of flow matching, iterative sampling, and value learning.
Given that offline RL is notoriously sensitive to implementation details in general~\citep{corl_tarasov2023},
we believe proposing a simple yet performant method is a particularly important contribution to the community.
We hope that FQL, with our clean, open-source implementation, spurs future research in scalable offline RL algorithms.


\section*{Acknowledgments}
We thank Chongyi Zheng for noticing an issue in our initial implementation.
This work was partly supported by the Korea Foundation for Advanced Studies (KFAS),
AFOSR FA9550-22-1-0273, and ONR N00014-20-1-2383.
This research used the Savio computational cluster resource provided by the Berkeley Research Computing program at UC Berkeley.
Some figures in this work use Twemoji, an open-source emoji set created by Twitter and licensed under CC BY 4.0.



%

%
%
%

\bibliography{icml2025}
\bibliographystyle{icml2025}


%
%
%
%
%
\newpage
\appendix
\onecolumn

\section{Limitations}
\label{sec:limitations}
One potential limitation of FQL is that it requires numerically solving ODEs during training
to minimize the distillation loss (\Cref{eq:distill}).
While this is not necessarily a significant speed bottleneck on both state- and pixel-based tasks in our experiments (as shown in \cref{fig:speed_full})
since flow matching happens in the relatively low-dimensional \emph{action} space (as opposed to image generation),
we believe this may further be improved by incorporating a more advanced one-step distillation method,
such as shortcut models~\citep{shortcut_frans2025}.
Another limitation is that it does not have a ``built-in'' exploration mechanism for online fine-tuning.
For example, FQL does not achieve the best online fine-tuning on the \texttt{puzzle-4x4} task (\Cref{table:o2o_full}),
in which exploration can help avoid local optima.
While we find that FQL without any additional exploration bonuses
is enough to achieve strong performance on many challenging tasks (\Cref{fig:o2o}),
we believe it can be further improved by combining FQL with a more principled exploration strategy
or additional specialized fine-tuning techniques, leaving them for future work.
%
%
%
%


\section{Implementation Details}
\label{sec:impl_details}

In this section, we describe the full implementation details of FQL.

\textbf{Flow matching.}
As mentioned in \Cref{sec:preliminaries}, we use the simplest flow-matching objective (\Cref{eq:flow_policy})
based on linear paths and uniform time sampling.
We use a step count of $10$ for the Euler method across all tasks,
and for simplicity, we do not use sinusoidal embeddings for the time variable.
See \Cref{fig:appx_abl_t_dist,fig:appx_abl_flow_steps} for ablation studies on these flow-related hyperparameters.
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%

\textbf{Value learning.}
Following standard practice in RL, we train two Q functions to improve stability.
We take the mean of the two Q values
for the Q loss term in the actor objective (\Cref{eq:fql_actor}).
We also use the mean for the target value in the critic objective (\Cref{eq:brac_critic}) by default,
but we use the minimum of the two Q values (which is often referred to as clipped double Q-learning~\citep{td3_fujimoto2018})
for the \texttt{adroit} and OGBench \texttt{antmaze-\{large, giant\}} tasks,
as we find it to be slightly better.
See \Cref{fig:appx_abl_q_agg} for an ablation study on this choice.
%
%
%

\textbf{Online fine-tuning.}
For offline-to-online RL, we simply add online transitions to the dataset,
without distinguishing them from the offline transitions
(\ie, we do not use balanced sampling, unlike \citet{o2o_lee2021, calql_nakamoto2023, rlpd_ball2023}).
We continue to train the components of FQL with the same objective as in offline training (\Cref{alg:fql}).


\textbf{Network architectures.}
For FQL,
we use $[512, 512, 512, 512]$-sized multi-layer perceptions (MLPs) for all neural networks.
We apply layer normalization~\citep{ln_ba2016} to value networks to further stabilize training.
We find that using a large enough network is especially important in navigation environments (\eg, \texttt{antmaze}).

\textbf{Image processing.}
For pixel-based environments, we use a smaller variant of the IMPALA encoder~\citep{impala_espeholt2018}
and apply a random-shift augmentation with a probability of $0.5$,
following the official implementation of \citet{ogbench_park2025}.
In addition, we use frame stacking with three images,
which we find to be important on some pixel-based tasks, such as \texttt{cube} and \texttt{puzzle}.

\textbf{Training and evaluation.}
We train FQL with $1$M gradient steps for state-based OGBench tasks and $500$K steps for D4RL and pixel-based OGBench tasks,
and evaluate the agent every $100$K steps using $50$ episodes.
For OGBench, following the official evaluation scheme~\citep{ogbench_park2025},
we report the average success rates across the last three evaluation epochs
($800$K, $900$K, and $1$M for state-based tasks and $300$K, $400$K, and $500$K for pixel-based tasks).
For D4RL, following \citet{corl_tarasov2023}, we report the performance at the last epoch.
For offline-to-online RL results (\Cref{table:o2o_full}), we report the performances at $1$M and $2$M steps.

\textbf{BC coefficient $\alpha$.}
The most important hyperparameter of FQL is the BC coefficient $\alpha$ in \Cref{eq:fql_actor}.
We perform a hyperparameter search over $\{1000, 3000, 10000, 30000\}$ for \texttt{adroit} tasks
and $\{3, 10, 30, 100, 300, 1000\}$ for the other tasks,
and use the best one for each environment.
We use larger values for \texttt{adroit} tasks
simply because their return scale is significantly larger than that of the other tasks.
We believe normalizing the Q loss as in \citet{td3bc_fujimoto2021}
would lead to more similar $\alpha$ values across different tasks.
While we do not apply this normalization technique in our experiments,
we recommend \textbf{enabling} Q normalization for new tasks (which is available in our official implementation)
and tuning $\alpha$ starting from $\{0.03, 0.1, 0.3, 1, 3, 10\}$.
See \Cref{fig:appx_abl_coef} for an ablation study on the BC coefficient.

\textbf{Hyperparameters.}
We refer to \Cref{table:hyp,table:method_hyp,table:method_hyp_o2o} for the complete list of hyperparameters.


\section{Ablation Study}
\label{sec:abl}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/abl_coef.pdf}
         \vspace{-10pt}
         \caption{
         \footnotesize
         \textbf{Ablation study on the BC coefficient $\alpha$.}
         }
         \label{fig:appx_abl_coef}
         \vspace{10pt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/abl_q_agg.pdf}
         \vspace{-10pt}
         \caption{
         \footnotesize
         \textbf{Ablation study on the target value aggregation method.}
         }
         \label{fig:appx_abl_q_agg}
         \vspace{10pt}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/abl_flow_steps.pdf}
         \vspace{-10pt}
         \caption{
         \footnotesize
         \textbf{Ablation study on the number of flow steps.}
         }
         \label{fig:appx_abl_flow_steps}
         \vspace{10pt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/abl_t_dist.pdf}
         \vspace{-10pt}
         \caption{
         \footnotesize
         \textbf{Ablation study on the flow time distribution.}
         }
         \label{fig:appx_abl_t_dist}
         \vspace{10pt}
    \end{subfigure}
    \vspace{-5pt}
    \caption{
    \footnotesize
    \textbf{Ablation studies.}
    We ablate several components of FQL and study how they affect performance.
    The results are averaged over $8$ seeds.
    }
    \label{fig:appx_abl}
\end{figure}

In this section, we ablate several components of FQL and study how they affect performance.
\Cref{fig:appx_abl} shows our ablation results, where we present training curves of FQL with different hyperparameters
on a representative selection of tasks.

\textbf{BC coefficient $\alpha$.}
As discussed in the main paper, the BC coefficient $\alpha$ is the most important hyperparameter of FQL.
\Cref{fig:appx_abl_coef} demonstrates that $\alpha$ needs to be tuned for each task based on the suboptimality of the dataset,
as is typical for most offline RL methods~\citep{bottleneck_park2024}.

\textbf{Target value aggregation methods.}
As discussed in \Cref{sec:impl_details}, we train two Q functions ($Q_1$ and $Q_2$)
and use their mean, $(Q_1 + Q_2) / 2$, for target values in the critic loss by default,
but we use their minimum, $\min(Q_1, Q_2)$, for some tasks, such as \texttt{adroit}.
We present the ablation results in \Cref{fig:appx_abl_q_agg}
with the BC coefficient $\alpha$ individually tuned for each ablation setting.
The results show that not using clipped double Q-learning often leads to better performance,
which is aligned with recent findings in online RL~\citep{rlpd_ball2023, bro_nauman2024, simba_lee2025}.

\textbf{Flow steps.}
To numerically solve ODEs, we use the Euler method, which requires a pre-specified number of steps.
In this work, we use $10$ steps for all experiments.
\Cref{fig:appx_abl_flow_steps} shows the ablation results,
which suggest that the performance is generally robust to the number of flow steps,
as long as it is not too small.

\textbf{Time distributions for flow matching.}
In this work, we use the uniform distribution, $\mathrm{Unif}([0, 1])$, to sample time steps for flow matching.
Prior works have considered other time distributions as well.
For example, \citet{sd3_esser2024} use the logit normal distribution to emphasize intermediate steps
(\ie, first sample $\tilde t$ from the standard normal distribution, $\tilde t \sim \gN(0, I)$,
and then map it via the sigmoid function, $t \gets 1 / (1 + e^{-\tilde t})$),
and \citet{pi0_black2024} employ a beta distribution, $\mathrm{Beta}(1, 1.5)$, to make the flow model
focus more on the initial steps.
We evaluate these three strategies and report the results in \Cref{fig:appx_abl_t_dist}.
The results suggest that the performance is generally robust to the choice of the time distribution,
and the simplest uniform distribution is often enough to achieve the best performance.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%




\clearpage
\section{Additional Results}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/speed_full.pdf}
    \vspace{-10pt}
    \caption{
    \footnotesize
    \textbf{Run time comparison.}
    FQL is only slightly slower than Gaussian policy-based offline RL methods,
    while being faster than most other flow-based methods in terms of both training and inference speeds.
    The run times are measured on the same machine using a single A5000 GPU, and are averaged over $8$ seeds.
    }
    \label{fig:speed_full}
\end{figure}

\textbf{Run time comparison.}
\Cref{fig:speed_full} compares the training and inference speeds of different methods on \texttt{cube-double} and \texttt{visual-cube-double},
where we consider methods implemented in the same codebase as FQL for a fair comparison.
The results show that FQL achieves the best or near-best speed
in terms of both training and inference among flow-based approaches.
Notably, FQL is faster than FBRAC during training as it does not use potentially costly backpropagation through time,
and is faster than IFQL during inference as it does not use rejection sampling.

\textbf{Full results.}
We present the full per-task offline RL results in \Cref{table:offline_full} and the full offline-to-online RL results in
\Cref{table:o2o_full} and \Cref{fig:o2o_full}.
The results are averaged over $8$ seeds ($4$ seeds for pixel-based tasks),
and we report standard deviations after ``$\pm$'' in tables
and $95\%$ bootstrap confidence intervals as shaded areas in plots.
In tables, we denote values at or above $95\%$ of the best performance in bold, following OGBench~\citep{ogbench_park2025}.
Results without standard deviations or confidence intervals indicate that they are taken from prior work;
the D4RL results of BC, IQL, ReBRAC, and Cal-QL are taken from \citet{corl_tarasov2023},
and the \texttt{antmaze} results of IDQL and SRPO are from \citet{idql_hansenestruch2023} and \citet{srpo_chen2024},
respectively.


\clearpage

\thispagestyle{empty}
\begin{table*}[t!]
\vspace{-30pt}
\caption{
\footnotesize
\textbf{Full offline RL results.}
We present the full results on the $73$ OGBench and D4RL tasks. \texttt{(*)} indicates the default task in each environment.
The results are averaged over $8$ seeds ($4$ seeds for pixel-based tasks) unless otherwise mentioned.
}
\label{table:offline_full}
\centering
\vspace{5pt}
\scalebox{0.69}
{
\begin{threeparttable}
\begin{tabular}{lcccccccccc}
\toprule
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\texttt{Gaussian Policies}} & \multicolumn{3}{c}{\texttt{Diffusion Policies}} & \multicolumn{4}{c}{\texttt{Flow Policies}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-11}
\texttt{Task} & \texttt{BC} & \texttt{IQL} & \texttt{ReBRAC} & \texttt{IDQL} & \texttt{SRPO} & \texttt{CAC} & \texttt{FAWAC} & \texttt{FBRAC} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\midrule


\texttt{antmaze-large-navigate-singletask-task1-v0 (*)} & $0$ {\tiny $\pm 0$} & $48$ {\tiny $\pm 9$} & $\mathbf{91}$ {\tiny $\pm 10$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $42$ {\tiny $\pm 7$} & $1$ {\tiny $\pm 1$} & $70$ {\tiny $\pm 20$} & $24$ {\tiny $\pm 17$} & $80$ {\tiny $\pm 8$} \\
\texttt{antmaze-large-navigate-singletask-task2-v0} & $6$ {\tiny $\pm 3$} & $42$ {\tiny $\pm 6$} & $\mathbf{88}$ {\tiny $\pm 4$} & $14$ {\tiny $\pm 8$} & $4$ {\tiny $\pm 4$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 1$} & $35$ {\tiny $\pm 12$} & $8$ {\tiny $\pm 3$} & $57$ {\tiny $\pm 10$} \\
\texttt{antmaze-large-navigate-singletask-task3-v0} & $29$ {\tiny $\pm 5$} & $72$ {\tiny $\pm 7$} & $51$ {\tiny $\pm 18$} & $26$ {\tiny $\pm 8$} & $3$ {\tiny $\pm 2$} & $49$ {\tiny $\pm 10$} & $12$ {\tiny $\pm 4$} & $83$ {\tiny $\pm 15$} & $52$ {\tiny $\pm 17$} & $\mathbf{93}$ {\tiny $\pm 3$} \\
\texttt{antmaze-large-navigate-singletask-task4-v0} & $8$ {\tiny $\pm 3$} & $51$ {\tiny $\pm 9$} & $\mathbf{84}$ {\tiny $\pm 7$} & $62$ {\tiny $\pm 25$} & $45$ {\tiny $\pm 19$} & $17$ {\tiny $\pm 6$} & $10$ {\tiny $\pm 3$} & $37$ {\tiny $\pm 18$} & $18$ {\tiny $\pm 8$} & $\mathbf{80}$ {\tiny $\pm 4$} \\
\texttt{antmaze-large-navigate-singletask-task5-v0} & $10$ {\tiny $\pm 3$} & $54$ {\tiny $\pm 22$} & $\mathbf{90}$ {\tiny $\pm 2$} & $2$ {\tiny $\pm 2$} & $1$ {\tiny $\pm 1$} & $55$ {\tiny $\pm 6$} & $9$ {\tiny $\pm 5$} & $76$ {\tiny $\pm 8$} & $38$ {\tiny $\pm 18$} & $83$ {\tiny $\pm 4$} \\
\midrule
\texttt{antmaze-giant-navigate-singletask-task1-v0 (*)} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $\mathbf{27}$ {\tiny $\pm 22$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 5$} \\
\texttt{antmaze-giant-navigate-singletask-task2-v0} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $\mathbf{16}$ {\tiny $\pm 17$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 7$} & $0$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 7$} \\
\texttt{antmaze-giant-navigate-singletask-task3-v0} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $\mathbf{34}$ {\tiny $\pm 22$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} \\
\texttt{antmaze-giant-navigate-singletask-task4-v0} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $5$ {\tiny $\pm 12$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 4$} & $0$ {\tiny $\pm 0$} & $\mathbf{14}$ {\tiny $\pm 23$} \\
\texttt{antmaze-giant-navigate-singletask-task5-v0} & $1$ {\tiny $\pm 1$} & $19$ {\tiny $\pm 7$} & $\mathbf{49}$ {\tiny $\pm 22$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $6$ {\tiny $\pm 10$} & $13$ {\tiny $\pm 9$} & $16$ {\tiny $\pm 28$} \\
\midrule
\texttt{humanoidmaze-medium-navigate-singletask-task1-v0 (*)} & $1$ {\tiny $\pm 0$} & $32$ {\tiny $\pm 7$} & $16$ {\tiny $\pm 9$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $38$ {\tiny $\pm 19$} & $6$ {\tiny $\pm 2$} & $25$ {\tiny $\pm 8$} & $\mathbf{69}$ {\tiny $\pm 19$} & $19$ {\tiny $\pm 12$} \\
\texttt{humanoidmaze-medium-navigate-singletask-task2-v0} & $1$ {\tiny $\pm 0$} & $41$ {\tiny $\pm 9$} & $18$ {\tiny $\pm 16$} & $1$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $47$ {\tiny $\pm 35$} & $40$ {\tiny $\pm 2$} & $76$ {\tiny $\pm 10$} & $85$ {\tiny $\pm 11$} & $\mathbf{94}$ {\tiny $\pm 3$} \\
\texttt{humanoidmaze-medium-navigate-singletask-task3-v0} & $6$ {\tiny $\pm 2$} & $25$ {\tiny $\pm 5$} & $36$ {\tiny $\pm 13$} & $0$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $\mathbf{83}$ {\tiny $\pm 18$} & $19$ {\tiny $\pm 2$} & $27$ {\tiny $\pm 11$} & $49$ {\tiny $\pm 49$} & $74$ {\tiny $\pm 18$} \\
\texttt{humanoidmaze-medium-navigate-singletask-task4-v0} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $\mathbf{15}$ {\tiny $\pm 16$} & $1$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $5$ {\tiny $\pm 4$} & $1$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 2$} & $1$ {\tiny $\pm 1$} & $3$ {\tiny $\pm 4$} \\
\texttt{humanoidmaze-medium-navigate-singletask-task5-v0} & $2$ {\tiny $\pm 1$} & $66$ {\tiny $\pm 4$} & $24$ {\tiny $\pm 20$} & $1$ {\tiny $\pm 1$} & $3$ {\tiny $\pm 3$} & $91$ {\tiny $\pm 5$} & $31$ {\tiny $\pm 7$} & $63$ {\tiny $\pm 9$} & $\mathbf{98}$ {\tiny $\pm 2$} & $\mathbf{97}$ {\tiny $\pm 2$} \\
\midrule
\texttt{humanoidmaze-large-navigate-singletask-task1-v0 (*)} & $0$ {\tiny $\pm 0$} & $3$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $6$ {\tiny $\pm 2$} & $\mathbf{7}$ {\tiny $\pm 6$} \\
\texttt{humanoidmaze-large-navigate-singletask-task2-v0} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} \\
\texttt{humanoidmaze-large-navigate-singletask-task3-v0} & $1$ {\tiny $\pm 1$} & $7$ {\tiny $\pm 3$} & $8$ {\tiny $\pm 4$} & $3$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 3$} & $1$ {\tiny $\pm 1$} & $10$ {\tiny $\pm 2$} & $\mathbf{48}$ {\tiny $\pm 10$} & $11$ {\tiny $\pm 7$} \\
\texttt{humanoidmaze-large-navigate-singletask-task4-v0} & $1$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $\mathbf{2}$ {\tiny $\pm 3$} \\
\texttt{humanoidmaze-large-navigate-singletask-task5-v0} & $0$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $\mathbf{2}$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 3$} \\
\midrule
\texttt{antsoccer-arena-navigate-singletask-task1-v0} & $2$ {\tiny $\pm 1$} & $14$ {\tiny $\pm 5$} & $0$ {\tiny $\pm 0$} & $44$ {\tiny $\pm 12$} & $2$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 3$} & $22$ {\tiny $\pm 2$} & $17$ {\tiny $\pm 3$} & $61$ {\tiny $\pm 25$} & $\mathbf{77}$ {\tiny $\pm 4$} \\
\texttt{antsoccer-arena-navigate-singletask-task2-v0} & $2$ {\tiny $\pm 2$} & $17$ {\tiny $\pm 7$} & $0$ {\tiny $\pm 1$} & $15$ {\tiny $\pm 12$} & $3$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $8$ {\tiny $\pm 1$} & $8$ {\tiny $\pm 2$} & $75$ {\tiny $\pm 3$} & $\mathbf{88}$ {\tiny $\pm 3$} \\
\texttt{antsoccer-arena-navigate-singletask-task3-v0} & $0$ {\tiny $\pm 0$} & $6$ {\tiny $\pm 4$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $8$ {\tiny $\pm 19$} & $11$ {\tiny $\pm 5$} & $16$ {\tiny $\pm 3$} & $14$ {\tiny $\pm 22$} & $\mathbf{61}$ {\tiny $\pm 6$} \\
\texttt{antsoccer-arena-navigate-singletask-task4-v0 (*)} & $1$ {\tiny $\pm 0$} & $3$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $12$ {\tiny $\pm 3$} & $24$ {\tiny $\pm 4$} & $16$ {\tiny $\pm 9$} & $\mathbf{39}$ {\tiny $\pm 6$} \\
\texttt{antsoccer-arena-navigate-singletask-task5-v0} & $0$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 2$} & $15$ {\tiny $\pm 4$} & $0$ {\tiny $\pm 1$} & $\mathbf{36}$ {\tiny $\pm 9$} \\
\midrule
\texttt{cube-single-play-singletask-task1-v0} & $10$ {\tiny $\pm 5$} & $88$ {\tiny $\pm 3$} & $89$ {\tiny $\pm 5$} & $\mathbf{95}$ {\tiny $\pm 2$} & $89$ {\tiny $\pm 7$} & $77$ {\tiny $\pm 28$} & $81$ {\tiny $\pm 9$} & $73$ {\tiny $\pm 33$} & $79$ {\tiny $\pm 4$} & $\mathbf{97}$ {\tiny $\pm 2$} \\
\texttt{cube-single-play-singletask-task2-v0 (*)} & $3$ {\tiny $\pm 1$} & $85$ {\tiny $\pm 8$} & $92$ {\tiny $\pm 4$} & $\mathbf{96}$ {\tiny $\pm 2$} & $82$ {\tiny $\pm 16$} & $80$ {\tiny $\pm 30$} & $81$ {\tiny $\pm 9$} & $83$ {\tiny $\pm 13$} & $73$ {\tiny $\pm 3$} & $\mathbf{97}$ {\tiny $\pm 2$} \\
\texttt{cube-single-play-singletask-task3-v0} & $9$ {\tiny $\pm 3$} & $91$ {\tiny $\pm 5$} & $93$ {\tiny $\pm 3$} & $\mathbf{99}$ {\tiny $\pm 1$} & $\mathbf{96}$ {\tiny $\pm 2$} & $\mathbf{98}$ {\tiny $\pm 1$} & $87$ {\tiny $\pm 4$} & $82$ {\tiny $\pm 12$} & $88$ {\tiny $\pm 4$} & $\mathbf{98}$ {\tiny $\pm 2$} \\
\texttt{cube-single-play-singletask-task4-v0} & $2$ {\tiny $\pm 1$} & $73$ {\tiny $\pm 6$} & $\mathbf{92}$ {\tiny $\pm 3$} & $\mathbf{93}$ {\tiny $\pm 4$} & $70$ {\tiny $\pm 18$} & $\mathbf{91}$ {\tiny $\pm 2$} & $79$ {\tiny $\pm 6$} & $79$ {\tiny $\pm 20$} & $79$ {\tiny $\pm 6$} & $\mathbf{94}$ {\tiny $\pm 3$} \\
\texttt{cube-single-play-singletask-task5-v0} & $3$ {\tiny $\pm 3$} & $78$ {\tiny $\pm 9$} & $87$ {\tiny $\pm 8$} & $\mathbf{90}$ {\tiny $\pm 6$} & $61$ {\tiny $\pm 12$} & $80$ {\tiny $\pm 20$} & $78$ {\tiny $\pm 10$} & $76$ {\tiny $\pm 33$} & $77$ {\tiny $\pm 7$} & $\mathbf{93}$ {\tiny $\pm 3$} \\
\midrule
\texttt{cube-double-play-singletask-task1-v0} & $8$ {\tiny $\pm 3$} & $27$ {\tiny $\pm 5$} & $45$ {\tiny $\pm 6$} & $39$ {\tiny $\pm 19$} & $7$ {\tiny $\pm 6$} & $21$ {\tiny $\pm 8$} & $21$ {\tiny $\pm 7$} & $47$ {\tiny $\pm 11$} & $35$ {\tiny $\pm 9$} & $\mathbf{61}$ {\tiny $\pm 9$} \\
\texttt{cube-double-play-singletask-task2-v0 (*)} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $7$ {\tiny $\pm 3$} & $16$ {\tiny $\pm 10$} & $0$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 2$} & $2$ {\tiny $\pm 1$} & $22$ {\tiny $\pm 12$} & $9$ {\tiny $\pm 5$} & $\mathbf{36}$ {\tiny $\pm 6$} \\
\texttt{cube-double-play-singletask-task3-v0} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 1$} & $17$ {\tiny $\pm 8$} & $0$ {\tiny $\pm 1$} & $3$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $4$ {\tiny $\pm 2$} & $8$ {\tiny $\pm 5$} & $\mathbf{22}$ {\tiny $\pm 5$} \\
\texttt{cube-double-play-singletask-task4-v0} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $\mathbf{5}$ {\tiny $\pm 2$} \\
\texttt{cube-double-play-singletask-task5-v0} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 3$} & $4$ {\tiny $\pm 2$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $3$ {\tiny $\pm 2$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 2$} & $17$ {\tiny $\pm 6$} & $\mathbf{19}$ {\tiny $\pm 10$} \\
\midrule
\texttt{scene-play-singletask-task1-v0} & $19$ {\tiny $\pm 6$} & $94$ {\tiny $\pm 3$} & $\mathbf{95}$ {\tiny $\pm 2$} & $\mathbf{100}$ {\tiny $\pm 0$} & $94$ {\tiny $\pm 4$} & $\mathbf{100}$ {\tiny $\pm 1$} & $87$ {\tiny $\pm 8$} & $\mathbf{96}$ {\tiny $\pm 8$} & $\mathbf{98}$ {\tiny $\pm 3$} & $\mathbf{100}$ {\tiny $\pm 0$} \\
\texttt{scene-play-singletask-task2-v0 (*)} & $1$ {\tiny $\pm 1$} & $12$ {\tiny $\pm 3$} & $50$ {\tiny $\pm 13$} & $33$ {\tiny $\pm 14$} & $2$ {\tiny $\pm 2$} & $50$ {\tiny $\pm 40$} & $18$ {\tiny $\pm 8$} & $46$ {\tiny $\pm 10$} & $0$ {\tiny $\pm 0$} & $\mathbf{76}$ {\tiny $\pm 9$} \\
\texttt{scene-play-singletask-task3-v0} & $1$ {\tiny $\pm 1$} & $32$ {\tiny $\pm 7$} & $55$ {\tiny $\pm 16$} & $\mathbf{94}$ {\tiny $\pm 4$} & $4$ {\tiny $\pm 4$} & $49$ {\tiny $\pm 16$} & $38$ {\tiny $\pm 9$} & $78$ {\tiny $\pm 14$} & $54$ {\tiny $\pm 19$} & $\mathbf{98}$ {\tiny $\pm 1$} \\
\texttt{scene-play-singletask-task4-v0} & $2$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 1$} & $3$ {\tiny $\pm 3$} & $4$ {\tiny $\pm 3$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $\mathbf{6}$ {\tiny $\pm 1$} & $4$ {\tiny $\pm 4$} & $0$ {\tiny $\pm 0$} & $5$ {\tiny $\pm 1$} \\
\texttt{scene-play-singletask-task5-v0} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} \\
\midrule
\texttt{puzzle-3x3-play-singletask-task1-v0} & $5$ {\tiny $\pm 2$} & $33$ {\tiny $\pm 6$} & $\mathbf{97}$ {\tiny $\pm 4$} & $52$ {\tiny $\pm 12$} & $89$ {\tiny $\pm 5$} & $\mathbf{97}$ {\tiny $\pm 2$} & $25$ {\tiny $\pm 9$} & $63$ {\tiny $\pm 19$} & $\mathbf{94}$ {\tiny $\pm 3$} & $90$ {\tiny $\pm 4$} \\
\texttt{puzzle-3x3-play-singletask-task2-v0} & $1$ {\tiny $\pm 1$} & $4$ {\tiny $\pm 3$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 2$} & $2$ {\tiny $\pm 2$} & $1$ {\tiny $\pm 2$} & $\mathbf{16}$ {\tiny $\pm 5$} \\
\texttt{puzzle-3x3-play-singletask-task3-v0} & $1$ {\tiny $\pm 1$} & $3$ {\tiny $\pm 2$} & $3$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $\mathbf{10}$ {\tiny $\pm 3$} \\
\texttt{puzzle-3x3-play-singletask-task4-v0 (*)} & $1$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $\mathbf{16}$ {\tiny $\pm 5$} \\
\texttt{puzzle-3x3-play-singletask-task5-v0} & $1$ {\tiny $\pm 0$} & $3$ {\tiny $\pm 2$} & $5$ {\tiny $\pm 3$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $\mathbf{16}$ {\tiny $\pm 3$} \\
\midrule
\texttt{puzzle-4x4-play-singletask-task1-v0} & $1$ {\tiny $\pm 1$} & $12$ {\tiny $\pm 2$} & $26$ {\tiny $\pm 4$} & $\mathbf{48}$ {\tiny $\pm 5$} & $24$ {\tiny $\pm 9$} & $44$ {\tiny $\pm 10$} & $1$ {\tiny $\pm 2$} & $32$ {\tiny $\pm 9$} & $\mathbf{49}$ {\tiny $\pm 9$} & $34$ {\tiny $\pm 8$} \\
\texttt{puzzle-4x4-play-singletask-task2-v0} & $0$ {\tiny $\pm 0$} & $7$ {\tiny $\pm 4$} & $12$ {\tiny $\pm 4$} & $14$ {\tiny $\pm 5$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $5$ {\tiny $\pm 3$} & $4$ {\tiny $\pm 4$} & $\mathbf{16}$ {\tiny $\pm 5$} \\
\texttt{puzzle-4x4-play-singletask-task3-v0} & $0$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 3$} & $15$ {\tiny $\pm 3$} & $34$ {\tiny $\pm 5$} & $21$ {\tiny $\pm 10$} & $29$ {\tiny $\pm 12$} & $1$ {\tiny $\pm 1$} & $20$ {\tiny $\pm 10$} & $\mathbf{50}$ {\tiny $\pm 14$} & $18$ {\tiny $\pm 5$} \\
\texttt{puzzle-4x4-play-singletask-task4-v0 (*)} & $0$ {\tiny $\pm 0$} & $5$ {\tiny $\pm 2$} & $10$ {\tiny $\pm 3$} & $\mathbf{26}$ {\tiny $\pm 6$} & $7$ {\tiny $\pm 4$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $5$ {\tiny $\pm 1$} & $21$ {\tiny $\pm 11$} & $11$ {\tiny $\pm 3$} \\
\texttt{puzzle-4x4-play-singletask-task5-v0} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 1$} & $7$ {\tiny $\pm 3$} & $\mathbf{24}$ {\tiny $\pm 11$} & $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $4$ {\tiny $\pm 3$} & $2$ {\tiny $\pm 2$} & $7$ {\tiny $\pm 3$} \\
\midrule
\texttt{antmaze-umaze-v2} & $55$ & $77$ & $\mathbf{98}$ & $\mathbf{94}$ & $\mathbf{97}$ & $66$ {\tiny $\pm 5$} & $90$ {\tiny $\pm 6$} & $\mathbf{94}$ {\tiny $\pm 3$} & $92$ {\tiny $\pm 6$} & $\mathbf{96}$ {\tiny $\pm 2$} \\
\texttt{antmaze-umaze-diverse-v2} & $47$ & $54$ & $84$ & $80$ & $82$ & $66$ {\tiny $\pm 11$} & $55$ {\tiny $\pm 7$} & $82$ {\tiny $\pm 9$} & $62$ {\tiny $\pm 12$} & $\mathbf{89}$ {\tiny $\pm 5$} \\
\texttt{antmaze-medium-play-v2} & $0$ & $66$ & $\mathbf{90}$ & $84$ & $81$ & $49$ {\tiny $\pm 24$} & $52$ {\tiny $\pm 12$} & $77$ {\tiny $\pm 7$} & $56$ {\tiny $\pm 15$} & $78$ {\tiny $\pm 7$} \\
\texttt{antmaze-medium-diverse-v2} & $1$ & $74$ & $\mathbf{84}$ & $\mathbf{85}$ & $75$ & $0$ {\tiny $\pm 1$} & $44$ {\tiny $\pm 15$} & $77$ {\tiny $\pm 6$} & $60$ {\tiny $\pm 25$} & $71$ {\tiny $\pm 13$} \\
\texttt{antmaze-large-play-v2} & $0$ & $42$ & $52$ & $64$ & $54$ & $0$ {\tiny $\pm 0$} & $10$ {\tiny $\pm 6$} & $32$ {\tiny $\pm 21$} & $55$ {\tiny $\pm 9$} & $\mathbf{84}$ {\tiny $\pm 7$} \\
\texttt{antmaze-large-diverse-v2} & $0$ & $30$ & $64$ & $68$ & $54$ & $0$ {\tiny $\pm 0$} & $16$ {\tiny $\pm 10$} & $20$ {\tiny $\pm 17$} & $64$ {\tiny $\pm 8$} & $\mathbf{83}$ {\tiny $\pm 4$} \\
\midrule
\texttt{pen-human-v1} & $71$ & $78$ & $\mathbf{103}$ & $76$ {\tiny $\pm 10$} & $69$ {\tiny $\pm 7$} & $64$ {\tiny $\pm 8$} & $67$ {\tiny $\pm 5$} & $77$ {\tiny $\pm 7$} & $71$ {\tiny $\pm 12$} & $53$ {\tiny $\pm 6$} \\
\texttt{pen-cloned-v1} & $52$ & $83$ & $\mathbf{103}$ & $64$ {\tiny $\pm 7$} & $61$ {\tiny $\pm 7$} & $56$ {\tiny $\pm 10$} & $62$ {\tiny $\pm 10$} & $67$ {\tiny $\pm 9$} & $80$ {\tiny $\pm 11$} & $74$ {\tiny $\pm 11$} \\
\texttt{pen-expert-v1} & $110$ & $128$ & $\mathbf{152}$ & $140$ {\tiny $\pm 6$} & $134$ {\tiny $\pm 4$} & $103$ {\tiny $\pm 9$} & $118$ {\tiny $\pm 6$} & $119$ {\tiny $\pm 7$} & $139$ {\tiny $\pm 5$} & $142$ {\tiny $\pm 6$} \\
\texttt{door-human-v1} & $2$ & $3$ & $-0$ & $6$ {\tiny $\pm 2$} & $3$ {\tiny $\pm 3$} & $5$ {\tiny $\pm 2$} & $2$ {\tiny $\pm 1$} & $4$ {\tiny $\pm 2$} & $\mathbf{7}$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} \\
\texttt{door-cloned-v1} & $-0$ & $\mathbf{3}$ & $0$ & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 2$} & $2$ {\tiny $\pm 1$} \\
\texttt{door-expert-v1} & $\mathbf{105}$ & $\mathbf{107}$ & $\mathbf{106}$ & $\mathbf{105}$ {\tiny $\pm 1$} & $\mathbf{105}$ {\tiny $\pm 0$} & $98$ {\tiny $\pm 3$} & $\mathbf{103}$ {\tiny $\pm 1$} & $\mathbf{104}$ {\tiny $\pm 1$} & $\mathbf{104}$ {\tiny $\pm 2$} & $\mathbf{104}$ {\tiny $\pm 1$} \\
\texttt{hammer-human-v1} & $\mathbf{3}$ & $2$ & $0$ & $2$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $\mathbf{3}$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} \\
\texttt{hammer-cloned-v1} & $1$ & $2$ & $5$ & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $\mathbf{11}$ {\tiny $\pm 9$} \\
\texttt{hammer-expert-v1} & $127$ & $\mathbf{129}$ & $\mathbf{134}$ & $125$ {\tiny $\pm 4$} & $127$ {\tiny $\pm 0$} & $92$ {\tiny $\pm 11$} & $118$ {\tiny $\pm 3$} & $119$ {\tiny $\pm 9$} & $117$ {\tiny $\pm 9$} & $125$ {\tiny $\pm 3$} \\
\texttt{relocate-human-v1} & $\mathbf{0}$ & $\mathbf{0}$ & $\mathbf{0}$ & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} & $\mathbf{0}$ {\tiny $\pm 0$} \\
\texttt{relocate-cloned-v1} & $-0$ & $0$ & $\mathbf{2}$ & $-0$ {\tiny $\pm 0$} & $-0$ {\tiny $\pm 0$} & $-0$ {\tiny $\pm 0$} & $-0$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $-0$ {\tiny $\pm 0$} & $-0$ {\tiny $\pm 0$} \\
\texttt{relocate-expert-v1} & $\mathbf{108}$ & $\mathbf{106}$ & $\mathbf{108}$ & $\mathbf{107}$ {\tiny $\pm 1$} & $\mathbf{106}$ {\tiny $\pm 2$} & $93$ {\tiny $\pm 6$} & $\mathbf{105}$ {\tiny $\pm 3$} & $\mathbf{105}$ {\tiny $\pm 2$} & $\mathbf{104}$ {\tiny $\pm 3$} & $\mathbf{107}$ {\tiny $\pm 1$} \\
\midrule
\texttt{visual-cube-single-play-singletask-task1-v0}\tnote{1} & - & $70$ {\tiny $\pm 12$} & $\mathbf{83}$ {\tiny $\pm 6$} & - & - & - & - & $55$ {\tiny $\pm 8$} & $49$ {\tiny $\pm 7$} & $\mathbf{81}$ {\tiny $\pm 12$} \\
\texttt{visual-cube-double-play-singletask-task1-v0}\tnote{1} & - & $\mathbf{34}$ {\tiny $\pm 23$} & $4$ {\tiny $\pm 4$} & - & - & - & - & $6$ {\tiny $\pm 2$} & $8$ {\tiny $\pm 6$} & $21$ {\tiny $\pm 11$} \\
\texttt{visual-scene-play-singletask-task1-v0}\tnote{1} & - & $\mathbf{97}$ {\tiny $\pm 2$} & $\mathbf{98}$ {\tiny $\pm 4$} & - & - & - & - & $46$ {\tiny $\pm 4$} & $86$ {\tiny $\pm 10$} & $\mathbf{98}$ {\tiny $\pm 3$} \\
\texttt{visual-puzzle-3x3-play-singletask-task1-v0}\tnote{1} & - & $7$ {\tiny $\pm 15$} & $88$ {\tiny $\pm 4$} & - & - & - & - & $7$ {\tiny $\pm 2$} & $\mathbf{100}$ {\tiny $\pm 0$} & $94$ {\tiny $\pm 1$} \\
\texttt{visual-puzzle-4x4-play-singletask-task1-v0}\tnote{1} & - & $0$ {\tiny $\pm 0$} & $26$ {\tiny $\pm 6$} & - & - & - & - & $0$ {\tiny $\pm 0$} & $8$ {\tiny $\pm 15$} & $\mathbf{33}$ {\tiny $\pm 6$} \\



\bottomrule
\end{tabular}
\begin{tablenotes}
\item[1] Due to the high computational cost of pixel-based tasks,
we selectively benchmark $5$ methods that achieve strong performance on state-based OGBench tasks.
\end{tablenotes}
\end{threeparttable}
}
\vspace{-10pt}
\end{table*}


\clearpage

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/o2o_full.pdf}
    \vspace{-20pt}
    \caption{
    \footnotesize
    \textbf{Offline-to-online RL results.}
    Online fine-tuning starts at $1$M steps.
    The results are averaged over $8$ seeds unless otherwise mentioned.
    }
    \label{fig:o2o_full}
\end{figure*}


\begin{table*}[t!]
\vspace{-10pt}
\caption{
\footnotesize
\textbf{Offline-to-online RL results.}
The results are averaged over $8$ seeds unless otherwise mentioned.
}
\label{table:o2o_full}
\centering
\vspace{5pt}
\scalebox{0.69}
{
\begin{tabular}{lcccccc}

\toprule
\texttt{Task} & \texttt{IQL} & \texttt{ReBRAC} & \texttt{Cal-QL} & \texttt{RLPD} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\midrule



\texttt{humanoidmaze-medium-navigate-singletask-v0} & $21$ {\tiny $\pm 13$} $\to$ $16$ {\tiny $\pm 8$} & $16$ {\tiny $\pm 20$} $\to$ $1$ {\tiny $\pm 1$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} $\to$ $8$ {\tiny $\pm 10$} & $56$ {\tiny $\pm 35$} $\to$ $\mathbf{82}$ {\tiny $\pm 20$} & $12$ {\tiny $\pm 7$} $\to$ $22$ {\tiny $\pm 12$} \\
\texttt{antsoccer-arena-navigate-singletask-v0} & $2$ {\tiny $\pm 1$} $\to$ $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $26$ {\tiny $\pm 15$} $\to$ $39$ {\tiny $\pm 10$} & $28$ {\tiny $\pm 8$} $\to$ $\mathbf{86}$ {\tiny $\pm 5$} \\
\texttt{cube-double-play-singletask-v0} & $0$ {\tiny $\pm 1$} $\to$ $0$ {\tiny $\pm 0$} & $6$ {\tiny $\pm 5$} $\to$ $28$ {\tiny $\pm 28$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $12$ {\tiny $\pm 9$} $\to$ $40$ {\tiny $\pm 5$} & $40$ {\tiny $\pm 11$} $\to$ $\mathbf{92}$ {\tiny $\pm 3$} \\
\texttt{scene-play-singletask-v0} & $14$ {\tiny $\pm 11$} $\to$ $10$ {\tiny $\pm 9$} & $55$ {\tiny $\pm 10$} $\to$ $\mathbf{100}$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 2$} $\to$ $50$ {\tiny $\pm 53$} & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{100}$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 1$} $\to$ $60$ {\tiny $\pm 39$} & $82$ {\tiny $\pm 11$} $\to$ $\mathbf{100}$ {\tiny $\pm 1$} \\
\texttt{puzzle-4x4-play-singletask-v0} & $5$ {\tiny $\pm 2$} $\to$ $1$ {\tiny $\pm 1$} & $8$ {\tiny $\pm 4$} $\to$ $14$ {\tiny $\pm 35$} & $0$ {\tiny $\pm 0$} $\to$ $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{100}$ {\tiny $\pm 1$} & $23$ {\tiny $\pm 6$} $\to$ $19$ {\tiny $\pm 33$} & $8$ {\tiny $\pm 3$} $\to$ $38$ {\tiny $\pm 52$} \\
\midrule
\texttt{antmaze-umaze-v2} & $77$ $\to$ $\mathbf{96}$ & $98$ $\to$ $75$ & $77$ $\to$ $\mathbf{100}$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{98}$ {\tiny $\pm 3$} & $94$ {\tiny $\pm 5$} $\to$ $\mathbf{96}$ {\tiny $\pm 2$} & $97$ {\tiny $\pm 2$} $\to$ $\mathbf{99}$ {\tiny $\pm 1$} \\
\texttt{antmaze-umaze-diverse-v2} & $60$ $\to$ $64$ & $74$ $\to$ $\mathbf{98}$ & $32$ $\to$ $\mathbf{98}$ & $0$ {\tiny $\pm 0$} $\to$ $94$ {\tiny $\pm 5$} & $69$ {\tiny $\pm 20$} $\to$ $93$ {\tiny $\pm 5$} & $79$ {\tiny $\pm 16$} $\to$ $\mathbf{100}$ {\tiny $\pm 1$} \\
\texttt{antmaze-medium-play-v2} & $72$ $\to$ $90$ & $88$ $\to$ $\mathbf{98}$ & $72$ $\to$ $\mathbf{99}$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{98}$ {\tiny $\pm 2$} & $52$ {\tiny $\pm 19$} $\to$ $93$ {\tiny $\pm 2$} & $77$ {\tiny $\pm 7$} $\to$ $\mathbf{97}$ {\tiny $\pm 2$} \\
\texttt{antmaze-medium-diverse-v2} & $64$ $\to$ $92$ & $85$ $\to$ $\mathbf{99}$ & $62$ $\to$ $\mathbf{98}$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{97}$ {\tiny $\pm 2$} & $44$ {\tiny $\pm 26$} $\to$ $89$ {\tiny $\pm 4$} & $55$ {\tiny $\pm 19$} $\to$ $\mathbf{97}$ {\tiny $\pm 3$} \\
\texttt{antmaze-large-play-v2} & $38$ $\to$ $64$ & $68$ $\to$ $32$ & $32$ $\to$ $\mathbf{97}$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{93}$ {\tiny $\pm 5$} & $64$ {\tiny $\pm 14$} $\to$ $80$ {\tiny $\pm 5$} & $66$ {\tiny $\pm 40$} $\to$ $84$ {\tiny $\pm 30$} \\
\texttt{antmaze-large-diverse-v2} & $27$ $\to$ $64$ & $67$ $\to$ $72$ & $44$ $\to$ $\mathbf{92}$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{94}$ {\tiny $\pm 3$} & $69$ {\tiny $\pm 6$} $\to$ $86$ {\tiny $\pm 5$} & $75$ {\tiny $\pm 24$} $\to$ $\mathbf{94}$ {\tiny $\pm 3$} \\
\midrule
\texttt{pen-cloned-v1} & $84$ $\to$ $102$ & $74$ $\to$ $138$ & $-3$ $\to$ $-3$ & $3$ {\tiny $\pm 2$} $\to$ $120$ {\tiny $\pm 10$} & $77$ {\tiny $\pm 7$} $\to$ $107$ {\tiny $\pm 10$} & $53$ {\tiny $\pm 14$} $\to$ $\mathbf{149}$ {\tiny $\pm 6$} \\
\texttt{door-cloned-v1} & $1$ $\to$ $20$ & $0$ $\to$ $\mathbf{102}$ & $-0$ $\to$ $-0$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{102}$ {\tiny $\pm 7$} & $3$ {\tiny $\pm 2$} $\to$ $50$ {\tiny $\pm 15$} & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{102}$ {\tiny $\pm 5$} \\
\texttt{hammer-cloned-v1} & $1$ $\to$ $57$ & $7$ $\to$ $\mathbf{125}$ & $0$ $\to$ $0$ & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{128}$ {\tiny $\pm 29$} & $4$ {\tiny $\pm 2$} $\to$ $60$ {\tiny $\pm 14$} & $0$ {\tiny $\pm 0$} $\to$ $\mathbf{127}$ {\tiny $\pm 17$} \\
\texttt{relocate-cloned-v1} & $0$ $\to$ $0$ & $1$ $\to$ $7$ & $-0$ $\to$ $-0$ & $0$ {\tiny $\pm 0$} $\to$ $2$ {\tiny $\pm 2$} & $-0$ {\tiny $\pm 0$} $\to$ $5$ {\tiny $\pm 3$} & $0$ {\tiny $\pm 1$} $\to$ $\mathbf{62}$ {\tiny $\pm 8$} \\




\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{table*}





\section{Experimental Details}
\label{sec:exp_details}

We implement FQL and many of the baselines in JAX~\citep{jax_bradbury2018}
on top of OGBench's reference implementations~\citep{ogbench_park2025}.
We provide our full implementation and exact commands to reproduce the main results of FQL
at \url{https://github.com/seohongpark/fql}.
%

\subsection{Environments, Tasks, and Datasets}

\textbf{OGBench~\citep{ogbench_park2025}.}
OGBench is our main benchmark, and we use $10$ environments, $50$ state-based tasks, and $5$ pixel-based tasks from OGBench.
Since OGBench was originally designed for offline goal-conditioned RL,
we use the single-task variants (``\texttt{-singletask}'') of OGBench tasks to benchmark standard reward-maximizing offline RL methods,
which are available from the \texttt{1.1.0a1} version of OGBench.
Each OGBench environment provides five evaluation goals,
each of which defines a different task (\texttt{-singletask-task1} to \texttt{-singletask-task5}),
and one of them is set to be a default task (\texttt{-singletask} without a suffix).
Given an evaluation goal,
the corresponding \texttt{singletask} variant labels the transitions in the dataset
with a semi-sparse reward function. %
The semi-sparse reward function (for the fixed task) is defined as the negative of the number of remaining subtasks at a given state.
Locomotion tasks have only one subtask (``reach the goal''), and rewards are always $-1$ or $0$.
Manipulation tasks usually involve more than one subtasks
(\eg, ``open the drawer'', ``turn the first button's color blue'', etc.),
and rewards are bounded by $-n_\mathrm{task}$ and $0$, where $n_\mathrm{task}$ is the number of subtasks,
up to $16$ in the set of environments we use.
The episode ends when the agent achieves the goal.

In our experiments, we use the following $10$ state-based and $5$ pixel-based datasets (each dataset provides $5$ different tasks).
\begin{itemize}[noitemsep, topsep=0pt]
\item State-based datasets
\begin{itemize}[noitemsep, topsep=0pt]
\item \texttt{antmaze-large-navigate-v0}
\item \texttt{antmaze-giant-navigate-v0}
\item \texttt{humanoidmaze-medium-navigate-v0}
\item \texttt{humanoidmaze-large-navigate-v0}
\item \texttt{antsoccer-arena-navigate-v0}
\item \texttt{cube-single-play-v0}
\item \texttt{cube-double-play-v0}
\item \texttt{scene-play-v0}
\item \texttt{puzzle-3x3-play-v0}
\item \texttt{puzzle-4x4-play-v0}
\end{itemize}
\item Pixel-based datasets
\begin{itemize}[noitemsep, topsep=0pt]
\item \texttt{visual-cube-single-play-v0}
\item \texttt{visual-cube-double-play-v0}
\item \texttt{visual-scene-play-v0}
\item \texttt{visual-puzzle-3x3-play-v0}
\item \texttt{visual-puzzle-4x4-play-v0}
\end{itemize}
\end{itemize}
We choose these environments to cover diverse types of challenges.
\texttt{antmaze} and \texttt{humanoidmaze} require controlling
either a quadrupedal agent (with $8$ degrees of freedom) or a humanoid agent (with $21$ degrees of freedom)
to reach a goal position in a given maze.
\texttt{antsoccer} requires controlling a quadrupedal agent to dribble a ball to a desired location.
\texttt{cube}, \texttt{scene}, and \texttt{puzzle} require manipulating diverse objects with a robot arm,
where \texttt{scene} involves long-horizon control of multiple objects (up to $8$ subtasks)
and \texttt{puzzle} requires combinatorial generalization.
The tasks with the \texttt{visual-} prefix require pixel-based control solely from $64 \times 64 \times 3$-sized images.
For dataset types, we employ the standard ones (\texttt{navigate} for locomotion and \texttt{play} for manipulation).
These datasets feature high suboptimality since they consist of trajectories performing \emph{random} tasks
(\eg, reaching random goals or manipulating random objects in the scene),
and thus require a high degree of ``stitching'' capabilities.
We use all of the five tasks for each state-based environment,
but we use only the first task (the one labeled as \texttt{singletask-task1}) for each pixel-based environment due to high computational cost.
For evaluation, we consider binary task success rates (in percentage), following the original evaluation criterion.

\textbf{D4RL~\citep{d4rl_fu2020}.}
To enable direct comparisons with previously reported results, we additionally employ $18$ relatively hard D4RL tasks in our experiments.
We use the following $6$ \texttt{antmaze} and $12$ \texttt{adroit} tasks.
\begin{itemize}[noitemsep, topsep=0pt]
\item \texttt{antmaze-umaze-v2}
\item \texttt{antmaze-umaze-diverse-v2}
\item \texttt{antmaze-medium-play-v2}
\item \texttt{antmaze-medium-diverse-v2}
\item \texttt{antmaze-large-play-v2}
\item \texttt{antmaze-large-diverse-v2}
\item \texttt{pen-human-v1}
\item \texttt{pen-cloned-v1}
\item \texttt{pen-expert-v1}
\item \texttt{door-human-v1}
\item \texttt{door-cloned-v1}
\item \texttt{door-expert-v1}
\item \texttt{hammer-human-v1}
\item \texttt{hammer-cloned-v1}
\item \texttt{hammer-expert-v1}
\item \texttt{relocate-human-v1}
\item \texttt{relocate-cloned-v1}
\item \texttt{relocate-expert-v1}
\end{itemize}
D4RL \texttt{antmaze} has the same high-level objective as OGBench \texttt{antmaze},
but with different (relatively less challenging) maze layouts, datasets, and evaluation goals.
%
%
\texttt{adroit} tasks (\texttt{pen}, \texttt{door}, \texttt{hammer}, and \texttt{relocate})
require dexterous manipulation with a high-dimensional ($24$-D) action space.
We measure binary task success rates (in percentage) for \texttt{antmaze} and normalized returns for \texttt{adroit},
following the original evaluation scheme~\citep{d4rl_fu2020}.

\subsection{Methods and Hyperparameters}
\label{sec:exp_details_methods}


In this work, we consider a total of $11$ previous offline RL and offline-to-online RL approaches.
%
We use the same default hyperparameters, architecture, and codebase for previous methods, unless otherwise mentioned.
Also, we \emph{individually} tune the method-specific hyperparameters of prior approaches for each environment,
as described in detail below.
For OGBench tasks, we tune each method on the \emph{default} task of each environment
(\ie, the task corresponding to the ``\texttt{-singletask}'' without a task ID),
and use the best hyperparameters for the other four tasks from the same environment.


\textbf{BC.}
For behavioral cloning, we train a Gaussian policy with a unit standard deviation.
We consider $[256, 256, 256, 256]$- and $[512, 512, 512, 512]$-sized MLPs
and use the latter (which is also our default network size) for all environments.

\textbf{IQL~\citep{iql_kostrikov2022}.}
We re-implement IQL on top of the same codebase as FQL.
We perform a hyperparameter search over expectile values in $\{0.7, 0.9\}$ and AWR inverse temperatures in $\{0.3, 1, 3, 10\}$.
We use a fixed expectile value of $0.9$ for all environments,
while the AWR inverse temperature $\alpha$ is individually tuned for each environment (\Cref{table:method_hyp,table:method_hyp_o2o}).
We find that IQL tends to overfit on state-based OGBench manipulation tasks,
and thus use smaller $[256, 256, 256, 256]$-sized MLPs for these state-based tasks (but not for pixel-based tasks), which we find perform better.

\textbf{ReBRAC~\citep{rebrac_tarasov2023}.}
We re-implement ReBRAC on the same codebase as FQL.
ReBRAC has two major hyperparameters: the actor and critic BC coefficients.
We consider $\{0.003, 0.01, 0.03, 0.1, 0.3, 1\}$ for the actor BC coefficient $\alpha_1$
and $\{0, 0.001, 0.01, 0.1\}$ for the critic BC coefficient $\alpha_2$.
Since actor regularization is generally (far) more important than critic regularization~\citep{rebrac_tarasov2023},
we first perform a sweep over actor BC coefficients without critic regularization,
and perform a second sweep over critic BC coefficients with the best actor BC coefficient.
We report the individually tuned hyperparameters in \Cref{table:method_hyp,table:method_hyp_o2o}.
We use the default values for the other hyperparameters (\eg, noise standard deviation, noise clipping threshold, etc.),
and normalize Q values only in the actor loss, following the official implementation~\citep{corl_tarasov2023}.

\textbf{IDQL~\citep{idql_hansenestruch2023}.}
We use the official open-source implementation of IDQL.
For network architectures,
we use the default residual multilayer perception (MLP) (three blocks of $[256, 1024, 256]$-sized residual layers) for the behavioral diffusion policy
and consider $\{[256, 256], [256, 256, 256, 256], [512, 512], [512, 512, 512, 512]\}$ for the size of the value network.
We find that using $4$-layer value networks in this codebase leads to unstable training,
and thus choose $[512, 512]$ for OGBench locomotion tasks and $[256, 256]$ for OGBench manipulation tasks.
We consider $\{0.7, 0.9\}$ for the IQL expectile value,
and $\{32, 64, 128\}$ for the number of test-time action samples.
%
%
We individually tune the number of action samples ($N$) for each task (\Cref{table:method_hyp}),
and use an IQL expectile of $0.7$ for OGBench locomotion and \texttt{adroit} tasks and $0.9$ for OGBench manipulation tasks.
We use the default values for the other hyperparameters.
Following the original training scheme, we train the agent for $3$M steps ($1.5$M for value functions),
three times longer than FQL's training epochs.
For compatibility with our evaluation scheme,
we report the average performance over $2.5$M, $2.75$M, and $3$M steps for OGBench tasks,
and the final performance for D4RL tasks.

\textbf{SRPO~\citep{srpo_chen2024}.}
For SRPO, we first used its official implementation to obtain OGBench results
but were unable to achieve reasonable performance, despite initial hyperparameter sweeps.
Hence, we re-implement SRPO on top of the codebase of IDQL (the closest method to SRPO),
which we find to lead to better performance.
We use the same tuned hyperparameters as IDQL for value learning and behavioral policy learning.
For the Q coefficient ($\beta$ in \citet{srpo_chen2024}),
we perform a hyperparameter search over $\{0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3\}$
and use the best one for each environment (\Cref{table:method_hyp}).

\textbf{Consistency-AC (CAC)~\citep{consistencyac_ding2024}.}
We use the official open-source implementation of Consistency-AC.
We consider $\{0.003, 0.01, 0.03, 0.1, 0.3, 1\}$ for the Q loss coefficient ($\eta$ in \citet{consistencyac_ding2024})
and use the best one for each environment (\Cref{table:method_hyp}).
For other hyperparameters for OGBench tasks,
we mostly follow the default ones for D4RL \texttt{antmaze} tasks,
as these are closest to OGBench tasks in that they both use sparse rewards and involve goal-reaching.
Namely, we do not normalize Q values, scale the consistency loss, and apply maximum Q backup.
For D4RL \texttt{antmaze}, we re-evaluate its performances on the \texttt{-v2} tasks
(the original paper uses \texttt{-v0} tasks)
with the hyperparameters provided in the official implementation.
For D4RL \texttt{adroit} tasks, we mainly use the default hyperparameters tuned for \texttt{adroit}
but perform an additional hyperparameter sweep over Q loss coefficients in $\{0.003, 0.01, 0.03\}$
for the other tasks not used in the original paper (\Cref{table:method_hyp}).
For all tasks, we apply gradient clipping with a threshold of $5$ and do not use online model selection
to ensure a fair comparison.

\textbf{FAWR, FBRAC, and IFQL.}
FAWR, FBRAC, and IFQL are implemented on top of the same codebase as FQL, sharing the same flow-matching implementation.
To enable apples-to-apples comparisons,
we use the same default hyperparameters as IQL for FIQL,
and the same default ones as FQL for FAWR and FBRAC.
However, we individually tune the policy extraction-related hyperparameters for each environment.
For the inverse temperature $\alpha$ in FAWR (\Cref{eq:flow_awr}), we consider $\{0.3, 1, 3, 10\}$.
For the number of test-time action samples $N$ in IFQL (\Cref{eq:rejection}), we consider $\{32, 64, 128\}$.
For the BC coefficient $\alpha$ in FBRAC (\Cref{eq:dql_actor}),
we consider $\{1000, 3000, 10000, 30000\}$ for \texttt{adroit} tasks
and $\{1, 3, 10, 30, 100, 300\}$ for the other tasks.
We present the task-specific hyperparameters in \Cref{table:method_hyp,table:method_hyp_o2o}.

\textbf{Cal-QL~\citep{calql_nakamoto2023}.}
We use the official implementation of Cal-QL.
For the CQL regularizer coefficient $\alpha$, we consider $\{0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\}$
as well as its Lagrange dual variant with target action gaps $\beta$ of $\{0.2, 0.5, 0.8\}$.
We use individually tuned values of these hyperparameters for different tasks (\Cref{table:method_hyp_o2o}).
For the network size, we consider both $[256, 256, 256, 256]$- and $[512, 512, 512, 512]$-sized MLPs,
and use $[512, 512, 512, 512]$ for OGBench locomotion tasks and $[256, 256, 256, 256]$ for OGBench manipulation tasks.
We also consider scaling rewards by $\{1, 3, 10\}$,
and use a value of $10$ to scale rewards for all tasks.
We use the default values for the other hyperparameters
(\eg, using a mixing ratio of $0.5$, taking the maximum over $10$ actions when computing target values,
using importance sampling for the CQL regularizer, etc.).

\textbf{RLPD~\citep{rlpd_ball2023}.}
We re-implement RLPD on top of the same codebase as FQL.
To ensure a fair comparison with other methods,
we use an update-to-data ratio of $1$ and employ two Q functions.
Clipped double Q-learning is only applied to D4RL \texttt{adroit} tasks,
as in FQL.
We do not use entropy backups, as we find it to be better.


\textbf{FQL.} See \Cref{sec:impl_details}.


We provide the complete list of hyperparameters in \Cref{table:hyp}
and task-specific hyperparameters in \Cref{table:method_hyp,table:method_hyp_o2o}.

\clearpage

\begin{table}[t]
\caption{
\footnotesize
\textbf{Hyperparameters for FQL.}
}
\vspace{5pt}
\label{table:hyp}
\begin{center}
\scalebox{0.8}
{
\begin{tabular}{ll}
    \toprule
    \textbf{Hyperparameter} & \textbf{Value} \\
    \midrule
    Learning rate & $0.0003$ \\
    Optimizer & Adam~\citep{adam_kingma2015} \\
    Gradient steps & $1000000$ (default), $500000$ (D4RL, pixel-based OGBench) \\
    Minibatch size & $256$ \\
    MLP dimensions & $[512, 512, 512, 512]$ \\
    Nonlinearity & GELU~\citep{gelu_hendrycks2016} \\
    Target network smoothing coefficient & $0.005$ \\
    Discount factor $\gamma$ & $0.99$ (default), $0.995$ (\texttt{antmaze-giant}, \texttt{humanoidmaze}, \texttt{antsoccer}) \\
    Image augmentation probability & $0.5$ \\
    Flow steps & $10$ \\
    Flow time sampling distribution & $\mathrm{Unif}([0, 1])$ \\
    %
    Clipped double Q-learning & False (default), True (\texttt{adroit}, \texttt{antmaze-\{large, giant\}-navigate}) \\
    BC coefficient $\alpha$ & \Cref{table:method_hyp,table:method_hyp_o2o} \\
    \bottomrule
\end{tabular}
}
\end{center}
\end{table} 

\clearpage

\thispagestyle{empty}
\begin{table*}[t!]
\vspace{-30pt}
\caption{
\footnotesize
\textbf{Task-specific hyperparameters for offline RL.} 
We refer to \Cref{sec:exp_details_methods} for the description for each hyperparameter variable.
We individually tune these hyperparameters for each task,
but in OGBench, we tune them on the default task (denoted by \texttt{(*)})
and use the best hyperparameters for the other four tasks.
``-'' indicates that the corresponding result is taken from the prior work (or does not exist).
}
\label{table:method_hyp}
\centering
\vspace{5pt}
\scalebox{0.69}
{
\newlength{\mylen}
\setlength{\mylen}{19pt}
\begin{tabular}{l@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c}
\toprule
& \texttt{IQL} & \texttt{ReBRAC} & \texttt{IDQL} & \texttt{SRPO} & \texttt{CAC} & \texttt{FAWAC} & \texttt{FBRAC} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\texttt{Task} & $\alpha$ & $(\alpha_1, \alpha_2)$ & $N$ & $\beta$ & $\eta$ & $\alpha$ & $\alpha$ & $N$ & {\color{myblue}$\alpha$} \\
\midrule


\texttt{antmaze-large-navigate-singletask-task1-v0 (*)} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $3$ & $32$ & $10$ \\
\texttt{antmaze-large-navigate-singletask-task2-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $3$ & $32$ & $10$ \\
\texttt{antmaze-large-navigate-singletask-task3-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $3$ & $32$ & $10$ \\
\texttt{antmaze-large-navigate-singletask-task4-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $3$ & $32$ & $10$ \\
\texttt{antmaze-large-navigate-singletask-task5-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $3$ & $32$ & $10$ \\
\midrule
\texttt{antmaze-giant-navigate-singletask-task1-v0 (*)} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-giant-navigate-singletask-task2-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-giant-navigate-singletask-task3-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-giant-navigate-singletask-task4-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-giant-navigate-singletask-task5-v0} & $10$ & $(0.003, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $10$ & $32$ & $10$ \\
\midrule
\texttt{humanoidmaze-medium-navigate-singletask-task1-v0 (*)} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $0.03$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-medium-navigate-singletask-task2-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $0.03$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-medium-navigate-singletask-task3-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $0.03$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-medium-navigate-singletask-task4-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $0.03$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-medium-navigate-singletask-task5-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $0.03$ & $3$ & $30$ & $32$ & $30$ \\
\midrule
\texttt{humanoidmaze-large-navigate-singletask-task1-v0 (*)} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-large-navigate-singletask-task2-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-large-navigate-singletask-task3-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-large-navigate-singletask-task4-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $30$ & $32$ & $30$ \\
\texttt{humanoidmaze-large-navigate-singletask-task5-v0} & $10$ & $(0.01, 0.01)$ & $32$ & $0.3$ & $1$ & $3$ & $30$ & $32$ & $30$ \\
\midrule
\texttt{antsoccer-arena-navigate-singletask-task1-v0} & $1$ & $(0.01, 0.01)$ & $32$ & $0.03$ & $1$ & $10$ & $30$ & $64$ & $10$ \\
\texttt{antsoccer-arena-navigate-singletask-task2-v0} & $1$ & $(0.01, 0.01)$ & $32$ & $0.03$ & $1$ & $10$ & $30$ & $64$ & $10$ \\
\texttt{antsoccer-arena-navigate-singletask-task3-v0} & $1$ & $(0.01, 0.01)$ & $32$ & $0.03$ & $1$ & $10$ & $30$ & $64$ & $10$ \\
\texttt{antsoccer-arena-navigate-singletask-task4-v0 (*)} & $1$ & $(0.01, 0.01)$ & $32$ & $0.03$ & $1$ & $10$ & $30$ & $64$ & $10$ \\
\texttt{antsoccer-arena-navigate-singletask-task5-v0} & $1$ & $(0.01, 0.01)$ & $32$ & $0.03$ & $1$ & $10$ & $30$ & $64$ & $10$ \\
\midrule
\texttt{cube-single-play-singletask-task1-v0} & $1$ & $(1, 0)$ & $32$ & $0.03$ & $0.003$ & $1$ & $100$ & $32$ & $300$ \\
\texttt{cube-single-play-singletask-task2-v0 (*)} & $1$ & $(1, 0)$ & $32$ & $0.03$ & $0.003$ & $1$ & $100$ & $32$ & $300$ \\
\texttt{cube-single-play-singletask-task3-v0} & $1$ & $(1, 0)$ & $32$ & $0.03$ & $0.003$ & $1$ & $100$ & $32$ & $300$ \\
\texttt{cube-single-play-singletask-task4-v0} & $1$ & $(1, 0)$ & $32$ & $0.03$ & $0.003$ & $1$ & $100$ & $32$ & $300$ \\
\texttt{cube-single-play-singletask-task5-v0} & $1$ & $(1, 0)$ & $32$ & $0.03$ & $0.003$ & $1$ & $100$ & $32$ & $300$ \\
\midrule
\texttt{cube-double-play-singletask-task1-v0} & $0.3$ & $(0.1, 0)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{cube-double-play-singletask-task2-v0 (*)} & $0.3$ & $(0.1, 0)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{cube-double-play-singletask-task3-v0} & $0.3$ & $(0.1, 0)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{cube-double-play-singletask-task4-v0} & $0.3$ & $(0.1, 0)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{cube-double-play-singletask-task5-v0} & $0.3$ & $(0.1, 0)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\midrule
\texttt{scene-play-singletask-task1-v0} & $10$ & $(0.1, 0.01)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{scene-play-singletask-task2-v0 (*)} & $10$ & $(0.1, 0.01)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{scene-play-singletask-task3-v0} & $10$ & $(0.1, 0.01)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{scene-play-singletask-task4-v0} & $10$ & $(0.1, 0.01)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\texttt{scene-play-singletask-task5-v0} & $10$ & $(0.1, 0.01)$ & $32$ & $0.1$ & $0.3$ & $0.3$ & $100$ & $32$ & $300$ \\
\midrule
\texttt{puzzle-3x3-play-singletask-task1-v0} & $10$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $100$ & $32$ & $1000$ \\
\texttt{puzzle-3x3-play-singletask-task2-v0} & $10$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $100$ & $32$ & $1000$ \\
\texttt{puzzle-3x3-play-singletask-task3-v0} & $10$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $100$ & $32$ & $1000$ \\
\texttt{puzzle-3x3-play-singletask-task4-v0 (*)} & $10$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $100$ & $32$ & $1000$ \\
\texttt{puzzle-3x3-play-singletask-task5-v0} & $10$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $100$ & $32$ & $1000$ \\
\midrule
\texttt{puzzle-4x4-play-singletask-task1-v0} & $3$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $300$ & $32$ & $1000$ \\
\texttt{puzzle-4x4-play-singletask-task2-v0} & $3$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $300$ & $32$ & $1000$ \\
\texttt{puzzle-4x4-play-singletask-task3-v0} & $3$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $300$ & $32$ & $1000$ \\
\texttt{puzzle-4x4-play-singletask-task4-v0 (*)} & $3$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $300$ & $32$ & $1000$ \\
\texttt{puzzle-4x4-play-singletask-task5-v0} & $3$ & $(0.3, 0.01)$ & $32$ & $0.1$ & $0.01$ & $0.3$ & $300$ & $32$ & $1000$ \\
\midrule
\texttt{antmaze-umaze-v2} & - & - & - & - & $0.01$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-umaze-diverse-v2} & - & - & - & - & $0.01$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-medium-play-v2} & - & - & - & - & $0.01$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-medium-diverse-v2} & - & - & - & - & $0.01$ & $3$ & $10$ & $32$ & $10$ \\
\texttt{antmaze-large-play-v2} & - & - & - & - & $4.5$ & $3$ & $1$ & $32$ & $3$ \\
\texttt{antmaze-large-diverse-v2} & - & - & - & - & $3.5$ & $3$ & $1$ & $32$ & $3$ \\
\midrule
\texttt{pen-human-v1} & - & - & $32$ & $0.03$ & $0.003$ & $0.03$ & $30000$ & $32$ & $10000$ \\
\texttt{pen-cloned-v1} & - & - & $32$ & $0.1$ & $0.003$ & $0.3$ & $10000$ & $32$ & $10000$ \\
\texttt{pen-expert-v1} & - & - & $32$ & $0.1$ & $0.03$ & $0.1$ & $30000$ & $32$ & $3000$ \\
\texttt{door-human-v1} & - & - & $32$ & $0.01$ & $0.03$ & $1$ & $30000$ & $32$ & $30000$ \\
\texttt{door-cloned-v1} & - & - & $32$ & $0.03$ & $0.03$ & $1$ & $10000$ & $128$ & $30000$ \\
\texttt{door-expert-v1} & - & - & $32$ & $0.01$ & $0.03$ & $3$ & $30000$ & $32$ & $30000$ \\
\texttt{hammer-human-v1} & - & - & $128$ & $0.1$ & $0.03$ & $3$ & $30000$ & $32$ & $30000$ \\
\texttt{hammer-cloned-v1} & - & - & $32$ & $0.1$ & $0.003$ & $0.03$ & $10000$ & $32$ & $10000$ \\
\texttt{hammer-expert-v1} & - & - & $32$ & $0.03$ & $0.03$ & $3$ & $30000$ & $32$ & $30000$ \\
\texttt{relocate-human-v1} & - & - & $32$ & $0.03$ & $0.01$ & $0.3$ & $30000$ & $128$ & $10000$ \\
\texttt{relocate-cloned-v1} & - & - & $64$ & $0.03$ & $0.01$ & $0.1$ & $3000$ & $32$ & $30000$ \\
\texttt{relocate-expert-v1} & - & - & $32$ & $0.01$ & $0.003$ & $1$ & $30000$ & $32$ & $30000$ \\
\midrule
\texttt{visual-cube-single-play-singletask-task1-v0} & $1$ & $(1, 0)$ & - & - & - & - & $100$ & $32$ & $300$ \\
\texttt{visual-cube-double-play-singletask-task1-v0} & $0.3$ & $(0.1, 0)$ & - & - & - & - & $100$ & $32$ & $100$ \\
\texttt{visual-scene-play-singletask-task1-v0} & $10$ & $(0.1, 0.01)$ & - & - & - & - & $100$ & $32$ & $100$ \\
\texttt{visual-puzzle-3x3-play-singletask-task1-v0} & $10$ & $(0.3, 0.01)$ & - & - & - & - & $100$ & $32$ & $300$ \\
\texttt{visual-puzzle-4x4-play-singletask-task1-v0} & $3$ & $(0.3, 0.01)$ & - & - & - & - & $300$ & $32$ & $300$ \\



\bottomrule


\end{tabular}
}
\vspace{-10pt}
\end{table*}

\begin{table*}[t!]
\vspace{-30pt}
\caption{
\footnotesize
\textbf{Task-specific hyperparameters for offline-to-online RL.} 
We refer to \Cref{sec:exp_details_methods} for the description for each hyperparameter variable.
We individually tune these hyperparameters for each task, and ``-'' indicates that the corresponding result is taken from the prior work.
}
\label{table:method_hyp_o2o}
\centering
\vspace{5pt}
\scalebox{0.69}
{
%
%
\setlength{\mylen}{60pt}
\begin{tabular}{l@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c@{\hskip \mylen}c}
\toprule
& \texttt{IQL} & \texttt{ReBRAC} & \texttt{Cal-QL} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\texttt{Task} & $\alpha$ & $(\alpha_1, \alpha_2)$ & $(\alpha, \beta)$ & $N$ & {\color{myblue}$\alpha$} \\
\midrule


\texttt{humanoidmaze-medium-navigate-singletask-v0} & $10$ & $(0.01, 0.01)$ & $(-, 0.8)$ & $32$ & $100$ \\
\texttt{antsoccer-arena-navigate-singletask-v0} & $1$ & $(0.01, 0.01)$ & $(-, 0.2)$ & $64$ & $30$ \\
\texttt{cube-double-play-singletask-v0} & $0.3$ & $(0.1, 0)$ & $(0.01, -)$ & $32$ & $300$ \\
\texttt{scene-play-singletask-v0} & $10$ & $(0.1, 0.01)$ & $(0.01, -)$ & $32$ & $300$ \\
\texttt{puzzle-4x4-play-singletask-v0} & $3$ & $(0.3, 0.01)$ & $(0.003, -)$ & $32$ & $1000$ \\
\midrule
\texttt{antmaze-umaze-v2} & - & - & - & $32$ & $10$ \\
\texttt{antmaze-umaze-diverse-v2} & - & - & - & $32$ & $10$ \\
\texttt{antmaze-medium-play-v2} & - & - & - & $32$ & $10$ \\
\texttt{antmaze-medium-diverse-v2} & - & - & - & $32$ & $10$ \\
\texttt{antmaze-large-play-v2} & - & - & - & $32$ & $3$ \\
\texttt{antmaze-large-diverse-v2} & - & - & - & $32$ & $3$ \\
\midrule
\texttt{pen-cloned-v1} & - & - & - & $128$ & $1000$ \\
\texttt{door-cloned-v1} & - & - & - & $128$ & $1000$ \\
\texttt{hammer-cloned-v1} & - & - & - & $128$ & $1000$ \\
\texttt{relocate-cloned-v1} & - & - & - & $128$ & $10000$ \\


\bottomrule


\end{tabular}
}
\vspace{-10pt}
\end{table*}


%
%


\end{document}
