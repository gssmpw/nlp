\section{Prior Work}
\label{sec:related}

\textbf{Offline RL and offline-to-online RL.}
%
The goal of offline RL is to train a policy using only previously collected data.
Hundreds of offline RL methods and techniques have been proposed so far,
and many of them are based on a single central idea:
maximizing the return while minimizing a discrepancy measure between
the state-action distribution of the dataset and that of the learned policy____.
Previous works have implemented this high-level objective in diverse ways through
behavioral regularization____,
conservatism____,
in-sample maximization____,
out-of-distribution detection____,
dual RL____,
and generative modeling____.
After finishing offline RL training, we can further fine-tune the policy with additional online rollouts.
This setting is often referred to as offline-to-online RL,
for which several techniques have been proposed____.
Our method, FQL, is mainly designed for offline RL,
but we show that it can also be directly fine-tuned with online rollouts without any algorithmic changes.
%
%
%

\textbf{RL with diffusion and flow models.}
Motivated by the recent successes of iterative generative modeling techniques,
such as denoising diffusion____
and flow matching____,
researchers have developed diverse ways to integrate them into RL.
Previous works have applied iterative generative models to
planning and hierarchical learning____,
world modeling and data augmentation____,
and policy modeling (\Cref{sec:prev_solutions}).
Our method belongs to the third category,
where we model a policy with an expressive flow network
to capture the arbitrarily complex distribution of the behavioral policy.
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{How Have Previous Works Trained Diffusion and Flow Policies with RL?}
\label{sec:prev_solutions}

%


%
%
%
%
%

Various approaches have been proposed for training diffusion or flow policies with RL.
In this section, we provide an in-depth review of these methods, discuss their advantages and limitations,
and explain how FQL relates to prior work.
Prior methods can be categorized into several groups based on their \emph{policy extraction} strategies____.

%
%
%
%
%
%

%
%
%
%
%
%
%
%

%

\textbf{(1) Weighted behavioral cloning.}
One straightforward approach to modulating a diffusion or flow policy is
to assign \emph{weights} to transition samples based on the corresponding learned values. 
The most basic form uses advantage-weighted regression (AWR)____
with the following objective:
\begin{align}
    %
    \max_\theta \ \ \E_{s, a \sim \gD}\left[e^{\alpha(Q(s, a) - V(s))} \gL_\mathrm{Flow}(\theta)\right],
    \label{eq:flow_awr}
\end{align}
where $\alpha$ is an inverse temperature hyperparameter,
and $Q(\pl{s}, \pl{a}): \gS \times \gA \to \sR$ and $V(\pl{s}): \gS \to \sR$ are
state-action and state value functions, respectively____.
For diffusion policies, $\gL_\mathrm{Flow}(\theta)$ is replaced with a diffusion loss.
Intuitively, this objective makes the policy selectively clone transitions with high advantages.
Among previous works, 
QGPO____,
EDP____,
QVPO____,
and QIPO____ are mainly based on weighted behavioral cloning.

Weighted behavioral cloning is simple and easy to implement.
However, it is known to be one of the least effective policy extraction methods____,
due to the small number of effective samples and limited expressivity.%
%
\footnote{See ____ for further discussions.}
In our experiments, we empirically show that weighted behavioral cloning
generally leads to subpar performance, especially on complex tasks.

%
%
%


\textbf{(2) Reparameterized policy gradient.}
Another popular approach to guide an iterative generative model is
to directly maximize the value function $Q(\pl{s}, \pl{a})$ with reparameterized gradients,
while regularizing it with a flow or diffusion loss,
as in \Cref{eq:dql_actor}.
Among previous approaches,
Diffusion-QL____,
DiffCPS____,
Consistency-AC____,
SRDP____,
and EQL____
implement this scheme with backpropagation through time.

Reparameterized policy gradient is known to be one of the most effective policy extraction methods for Gaussian policies____.
However, when na\"ively applied to iterative generative models,
it requires backpropagation through time (\Cref{eq:fql_actor}),
which often incurs stability issues %
and leads to suboptimal performance (\Cref{sec:results}).

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\textbf{(3) Rejection sampling.}
The third category is rejection sampling.
Instead of adjusting the parameter of the generative model,
we can sample $N$ actions from a \emph{fixed} BC policy, and select the action that has the highest value.
In other words, we treat the following formula as a policy:
\begin{align}
    \argmax_{\substack{a_1, \dots, a_N \text{$:$ } a_i \sim \pi^\beta}} \ \ Q(s, a_i), \label{eq:rejection}
\end{align}
where $\pi^\beta$ is a BC policy trained by a flow or diffusion objective.
Among previous works, 
SfBC____,
IDQL____,
and AlignIQL____
are based on (variants of) rejection sampling.

Rejection sampling is simple and stable.
However, it requires querying the policy and value function $N$ times
at \emph{every} environment step during inference (and possibly during training as well, depending on the method).
This can be prohibitive with larger models or a larger number of samples.
%

%

%
%
%
%
%
%
%

\textbf{(4) Others.}
Besides these three major categories,
other techniques have also been proposed to guide a diffusion policy to maximize the learned value function,
based on some combination of the above strategies____,
action gradients____,
bi-level MDPs____,
value alignment____,
and implicit Q-learning____.

\textbf{Contextualizing FQL in prior work.}
Our approach, FQL, falls into the second category, reparameterized policy gradient,
which is known to be one of the most effective policy extraction schemes____.
However, unlike the previous methods discussed above in the same category, which use backpropagation through time,
we entirely bypass recursive backpropagation
by only steering the one-step policy to maximize values (\Cref{eq:fql_actor}),
while training the flow policy solely with the BC loss.
%
%
%
%
Among previous works,
Consistency-AC____, SRPO____, and DTQL____
also employ distillation,
and in particular, Consistency-AC____ shares a conceptually similar high-level objective to our method
(but with consistency models instead of direct one-step distillation).
However, they either still use backpropagation through time____
or are based on implicit Q-learning____,
which is known to be less effective than actor-critic learning____.
In contrast, we train a \emph{one-step} policy within a more effective actor-critic framework,
with no backpropagation through time.
In our experiments,
we empirically show that our approach leads to significantly better performance
than previous distillation-based methods (Consistency-AC and SRPO) as well as other policy extraction schemes.
%

\begin{table*}[t!]
\vspace{-10pt}
\caption{
\footnotesize
\textbf{Offline RL results.}
FQL achieves the best or near-best performance on most of the $\mathbf{73}$ diverse, challenging benchmark tasks.
%
%
The performances are averaged over $\mathbf{8}$ seeds ($\mathbf{4}$ seeds for pixel-based tasks),
but the cells without the ``$\pm$'' sign indicate that the numbers are taken
from prior works____.
See \Cref{table:offline_full} for the full results.
}
\label{table:offline}
\centering
\vspace{5pt}
\scalebox{0.76}
{
%
\begin{threeparttable}
\begin{tabular}{lcccccccccc}
\toprule
%
%
%
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\texttt{Gaussian Policies}} & \multicolumn{3}{c}{\texttt{Diffusion Policies}} & \multicolumn{4}{c}{\texttt{Flow Policies}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-11}
\texttt{Task Category} & \texttt{BC} & \texttt{IQL} & \texttt{ReBRAC} & \texttt{IDQL} & \texttt{SRPO} & \texttt{CAC} & \texttt{FAWAC} & \texttt{FBRAC} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\midrule



\texttt{OGBench antmaze-large-singletask ($\mathbf{5}$ tasks)} & $11$ {\tiny $\pm 1$} & $53$ {\tiny $\pm 3$} & $\mathbf{81}$ {\tiny $\pm 5$} & $21$ {\tiny $\pm 5$} & $11$ {\tiny $\pm 4$} & $33$ {\tiny $\pm 4$} & $6$ {\tiny $\pm 1$} & $60$ {\tiny $\pm 6$} & $28$ {\tiny $\pm 5$} & $\mathbf{79}$ {\tiny $\pm 3$} \\
\texttt{OGBench antmaze-giant-singletask ($\mathbf{5}$ tasks)} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 1$} & $\mathbf{26}$ {\tiny $\pm 8$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 4$} & $3$ {\tiny $\pm 2$} & $9$ {\tiny $\pm 6$} \\
\texttt{OGBench humanoidmaze-medium-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 1$} & $33$ {\tiny $\pm 2$} & $22$ {\tiny $\pm 8$} & $1$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $53$ {\tiny $\pm 8$} & $19$ {\tiny $\pm 1$} & $38$ {\tiny $\pm 5$} & $\mathbf{60}$ {\tiny $\pm 14$} & $\mathbf{58}$ {\tiny $\pm 5$} \\
\texttt{OGBench humanoidmaze-large-singletask ($\mathbf{5}$ tasks)} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 0$} & $\mathbf{11}$ {\tiny $\pm 2$} & $4$ {\tiny $\pm 2$} \\
\texttt{OGBench antsoccer-arena-singletask ($\mathbf{5}$ tasks)} & $1$ {\tiny $\pm 0$} & $8$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $12$ {\tiny $\pm 4$} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 4$} & $12$ {\tiny $\pm 0$} & $16$ {\tiny $\pm 1$} & $33$ {\tiny $\pm 6$} & $\mathbf{60}$ {\tiny $\pm 2$} \\
\texttt{OGBench cube-single-singletask ($\mathbf{5}$ tasks)} & $5$ {\tiny $\pm 1$} & $83$ {\tiny $\pm 3$} & $91$ {\tiny $\pm 2$} & $\mathbf{95}$ {\tiny $\pm 2$} & $80$ {\tiny $\pm 5$} & $85$ {\tiny $\pm 9$} & $81$ {\tiny $\pm 4$} & $79$ {\tiny $\pm 7$} & $79$ {\tiny $\pm 2$} & $\mathbf{96}$ {\tiny $\pm 1$} \\
\texttt{OGBench cube-double-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 1$} & $7$ {\tiny $\pm 1$} & $12$ {\tiny $\pm 1$} & $15$ {\tiny $\pm 6$} & $2$ {\tiny $\pm 1$} & $6$ {\tiny $\pm 2$} & $5$ {\tiny $\pm 2$} & $15$ {\tiny $\pm 3$} & $14$ {\tiny $\pm 3$} & $\mathbf{29}$ {\tiny $\pm 2$} \\
\texttt{OGBench scene-singletask ($\mathbf{5}$ tasks)} & $5$ {\tiny $\pm 1$} & $28$ {\tiny $\pm 1$} & $41$ {\tiny $\pm 3$} & $46$ {\tiny $\pm 3$} & $20$ {\tiny $\pm 1$} & $40$ {\tiny $\pm 7$} & $30$ {\tiny $\pm 3$} & $45$ {\tiny $\pm 5$} & $30$ {\tiny $\pm 3$} & $\mathbf{56}$ {\tiny $\pm 2$} \\
\texttt{OGBench puzzle-3x3-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 1$} & $21$ {\tiny $\pm 1$} & $10$ {\tiny $\pm 2$} & $18$ {\tiny $\pm 1$} & $19$ {\tiny $\pm 0$} & $6$ {\tiny $\pm 2$} & $14$ {\tiny $\pm 4$} & $19$ {\tiny $\pm 1$} & $\mathbf{30}$ {\tiny $\pm 1$} \\
\texttt{OGBench puzzle-4x4-singletask ($\mathbf{5}$ tasks)} & $0$ {\tiny $\pm 0$} & $7$ {\tiny $\pm 1$} & $14$ {\tiny $\pm 1$} & $\mathbf{29}$ {\tiny $\pm 3$} & $10$ {\tiny $\pm 3$} & $15$ {\tiny $\pm 3$} & $1$ {\tiny $\pm 0$} & $13$ {\tiny $\pm 1$} & $25$ {\tiny $\pm 5$} & $17$ {\tiny $\pm 2$} \\
\texttt{D4RL antmaze ($\mathbf{6}$ tasks)} & $17$ & $57$ & $78$ & $79$ & $74$ & $30$ {\tiny $\pm 3$} & $44$ {\tiny $\pm 3$} & $64$ {\tiny $\pm 7$} & $65$ {\tiny $\pm 7$} & $\mathbf{84}$ {\tiny $\pm 3$} \\
\texttt{D4RL adroit ($\mathbf{12}$ tasks)} & $48$ & $53$ & $\mathbf{59}$ & $52$ {\tiny $\pm 1$} & $51$ {\tiny $\pm 1$} & $43$ {\tiny $\pm 2$} & $48$ {\tiny $\pm 1$} & $50$ {\tiny $\pm 2$} & $52$ {\tiny $\pm 1$} & $52$ {\tiny $\pm 1$} \\
\texttt{Visual manipulation ($\mathbf{5}$ tasks)} & - & $42$ {\tiny $\pm 4$} & $60$ {\tiny $\pm 2$} & - & - & - & - & $22$ {\tiny $\pm 2$} & $50$ {\tiny $\pm 5$} & $\mathbf{65}$ {\tiny $\pm 2$} \\



\bottomrule
\end{tabular}
\begin{tablenotes}
\item[1] Due to the high computational cost of pixel-based tasks,
we selectively benchmark $5$ methods that achieve strong performance on state-based OGBench tasks.
\end{tablenotes}
\end{threeparttable}
}
%
\end{table*}