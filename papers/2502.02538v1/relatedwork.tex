\section{Prior Work}
\label{sec:related}

\textbf{Offline RL and offline-to-online RL.}
%
The goal of offline RL is to train a policy using only previously collected data.
Hundreds of offline RL methods and techniques have been proposed so far,
and many of them are based on a single central idea:
maximizing the return while minimizing a discrepancy measure between
the state-action distribution of the dataset and that of the learned policy~\citep{offline_levine2020, dualrl_sikchi2024}.
Previous works have implemented this high-level objective in diverse ways through
behavioral regularization~\citep{awac_nair2020, td3bc_fujimoto2021, rebrac_tarasov2023},
conservatism~\citep{cql_kumar2020},
in-sample maximization~\citep{iql_kostrikov2022, sql_xu2023, xql_garg2023},
out-of-distribution detection~\citep{mopo_yu2020, morel_kidambi2020, edac_an2021, sacrnd_nikulin2023},
dual RL~\citep{optidice_lee2021, dualrl_sikchi2024},
and generative modeling~\citep{dt_chen2021, tt_janner2021, diffuser_janner2022}.
After finishing offline RL training, we can further fine-tune the policy with additional online rollouts.
This setting is often referred to as offline-to-online RL,
for which several techniques have been proposed~\citep{o2o_lee2021, hybridrl_song2023, calql_nakamoto2023, rlpd_ball2023, aca_yu2023}.
Our method, FQL, is mainly designed for offline RL,
but we show that it can also be directly fine-tuned with online rollouts without any algorithmic changes.
%
%
%

\textbf{RL with diffusion and flow models.}
Motivated by the recent successes of iterative generative modeling techniques,
such as denoising diffusion~\citep{diffusion_sohl2015, ddpm_ho2020, cg_dhariwal2021}
and flow matching~\citep{flow_lipman2023, sd3_esser2024},
researchers have developed diverse ways to integrate them into RL.
Previous works have applied iterative generative models to
planning and hierarchical learning~\citep{diffuser_janner2022, dd_ajay2023, guidedflow_zheng2023, adaptdiffuser_liang2023, hdmi_li2023, sgp_suh2023, ldcq_venkatraman2024, hd_chen2024},
world modeling and data augmentation~\citep{synther_lu2023, dwm_ding2024, pgd_jackson2024, diamond_alonso2024},
and policy modeling (\Cref{sec:prev_solutions}).
Our method belongs to the third category,
where we model a policy with an expressive flow network
to capture the arbitrarily complex distribution of the behavioral policy.
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{How Have Previous Works Trained Diffusion and Flow Policies with RL?}
\label{sec:prev_solutions}

%


%
%
%
%
%

Various approaches have been proposed for training diffusion or flow policies with RL.
In this section, we provide an in-depth review of these methods, discuss their advantages and limitations,
and explain how FQL relates to prior work.
Prior methods can be categorized into several groups based on their \emph{policy extraction} strategies~\citep{bottleneck_park2024}.

%
%
%
%
%
%

%
%
%
%
%
%
%
%

%

\textbf{(1) Weighted behavioral cloning.}
One straightforward approach to modulating a diffusion or flow policy is
to assign \emph{weights} to transition samples based on the corresponding learned values. 
The most basic form uses advantage-weighted regression (AWR)~\citep{rwr_peters2007, awr_peng2019, awac_nair2020}
with the following objective:
\begin{align}
    %
    \max_\theta \ \ \E_{s, a \sim \gD}\left[e^{\alpha(Q(s, a) - V(s))} \gL_\mathrm{Flow}(\theta)\right],
    \label{eq:flow_awr}
\end{align}
where $\alpha$ is an inverse temperature hyperparameter,
and $Q(\pl{s}, \pl{a}): \gS \times \gA \to \sR$ and $V(\pl{s}): \gS \to \sR$ are
state-action and state value functions, respectively~\citep{rl_sutton2005}.
For diffusion policies, $\gL_\mathrm{Flow}(\theta)$ is replaced with a diffusion loss.
Intuitively, this objective makes the policy selectively clone transitions with high advantages.
Among previous works, 
QGPO~\citep{qgpo_lu2023},
EDP~\citep{edp_kang2023},
QVPO~\citep{qvpo_ding2024},
and QIPO~\citep{qipo_anon2025} are mainly based on weighted behavioral cloning.

Weighted behavioral cloning is simple and easy to implement.
However, it is known to be one of the least effective policy extraction methods~\citep{closer_fu2022, bottleneck_park2024},
due to the small number of effective samples and limited expressivity.%
%
\footnote{See \citet{bottleneck_park2024} for further discussions.}
In our experiments, we empirically show that weighted behavioral cloning
generally leads to subpar performance, especially on complex tasks.

%
%
%


\textbf{(2) Reparameterized policy gradient.}
Another popular approach to guide an iterative generative model is
to directly maximize the value function $Q(\pl{s}, \pl{a})$ with reparameterized gradients,
while regularizing it with a flow or diffusion loss,
as in \Cref{eq:dql_actor}.
Among previous approaches,
Diffusion-QL~\citep{dql_wang2023},
DiffCPS~\citep{diffcps_he2023},
Consistency-AC~\citep{consistencyac_ding2024},
SRDP~\citep{srdp_ada2024},
and EQL~\citep{entropydql_zhang2024}
implement this scheme with backpropagation through time.

Reparameterized policy gradient is known to be one of the most effective policy extraction methods for Gaussian policies~\citep{bottleneck_park2024}.
However, when na\"ively applied to iterative generative models,
it requires backpropagation through time (\Cref{eq:fql_actor}),
which often incurs stability issues %
and leads to suboptimal performance (\Cref{sec:results}).

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\textbf{(3) Rejection sampling.}
The third category is rejection sampling.
Instead of adjusting the parameter of the generative model,
we can sample $N$ actions from a \emph{fixed} BC policy, and select the action that has the highest value.
In other words, we treat the following formula as a policy:
\begin{align}
    \argmax_{\substack{a_1, \dots, a_N \text{$:$ } a_i \sim \pi^\beta}} \ \ Q(s, a_i), \label{eq:rejection}
\end{align}
where $\pi^\beta$ is a BC policy trained by a flow or diffusion objective.
Among previous works, 
SfBC~\citep{sfbc_chen2023},
IDQL~\citep{idql_hansenestruch2023},
and AlignIQL~\citep{aligniql_he2024}
are based on (variants of) rejection sampling.

Rejection sampling is simple and stable.
However, it requires querying the policy and value function $N$ times
at \emph{every} environment step during inference (and possibly during training as well, depending on the method).
This can be prohibitive with larger models or a larger number of samples.
%

%

%
%
%
%
%
%
%

\textbf{(4) Others.}
Besides these three major categories,
other techniques have also been proposed to guide a diffusion policy to maximize the learned value function,
based on some combination of the above strategies~\citep{diffusiondice_mao2024},
action gradients~\citep{dipo_yang2023, ddiffpg_li2024, parl_mark2024, dac_anonymous2025},
bi-level MDPs~\citep{dppo_ren2025},
value alignment~\citep{eda_chen2024},
and implicit Q-learning~\citep{srpo_chen2024, dtql_chen2024}.

\textbf{Contextualizing FQL in prior work.}
Our approach, FQL, falls into the second category, reparameterized policy gradient,
which is known to be one of the most effective policy extraction schemes~\citep{bottleneck_park2024}.
However, unlike the previous methods discussed above in the same category, which use backpropagation through time,
we entirely bypass recursive backpropagation
by only steering the one-step policy to maximize values (\Cref{eq:fql_actor}),
while training the flow policy solely with the BC loss.
%
%
%
%
Among previous works,
Consistency-AC~\citep{consistencyac_ding2024}, SRPO~\citep{srpo_chen2024}, and DTQL~\citep{dtql_chen2024}
also employ distillation,
and in particular, Consistency-AC~\citep{consistencyac_ding2024} shares a conceptually similar high-level objective to our method
(but with consistency models instead of direct one-step distillation).
However, they either still use backpropagation through time~\citep{consistencyac_ding2024}
or are based on implicit Q-learning~\citep{iql_kostrikov2022},
which is known to be less effective than actor-critic learning~\citep{rebrac_tarasov2023}.
In contrast, we train a \emph{one-step} policy within a more effective actor-critic framework,
with no backpropagation through time.
In our experiments,
we empirically show that our approach leads to significantly better performance
than previous distillation-based methods (Consistency-AC and SRPO) as well as other policy extraction schemes.
%

\begin{table*}[t!]
\vspace{-10pt}
\caption{
\footnotesize
\textbf{Offline RL results.}
FQL achieves the best or near-best performance on most of the $\mathbf{73}$ diverse, challenging benchmark tasks.
%
%
The performances are averaged over $\mathbf{8}$ seeds ($\mathbf{4}$ seeds for pixel-based tasks),
but the cells without the ``$\pm$'' sign indicate that the numbers are taken
from prior works~\citep{corl_tarasov2023, idql_hansenestruch2023, srpo_chen2024}.
See \Cref{table:offline_full} for the full results.
}
\label{table:offline}
\centering
\vspace{5pt}
\scalebox{0.76}
{
%
\begin{threeparttable}
\begin{tabular}{lcccccccccc}
\toprule
%
%
%
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\texttt{Gaussian Policies}} & \multicolumn{3}{c}{\texttt{Diffusion Policies}} & \multicolumn{4}{c}{\texttt{Flow Policies}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-11}
\texttt{Task Category} & \texttt{BC} & \texttt{IQL} & \texttt{ReBRAC} & \texttt{IDQL} & \texttt{SRPO} & \texttt{CAC} & \texttt{FAWAC} & \texttt{FBRAC} & \texttt{IFQL} & \texttt{\color{myblue}FQL} \\
\midrule



\texttt{OGBench antmaze-large-singletask ($\mathbf{5}$ tasks)} & $11$ {\tiny $\pm 1$} & $53$ {\tiny $\pm 3$} & $\mathbf{81}$ {\tiny $\pm 5$} & $21$ {\tiny $\pm 5$} & $11$ {\tiny $\pm 4$} & $33$ {\tiny $\pm 4$} & $6$ {\tiny $\pm 1$} & $60$ {\tiny $\pm 6$} & $28$ {\tiny $\pm 5$} & $\mathbf{79}$ {\tiny $\pm 3$} \\
\texttt{OGBench antmaze-giant-singletask ($\mathbf{5}$ tasks)} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 1$} & $\mathbf{26}$ {\tiny $\pm 8$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $4$ {\tiny $\pm 4$} & $3$ {\tiny $\pm 2$} & $9$ {\tiny $\pm 6$} \\
\texttt{OGBench humanoidmaze-medium-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 1$} & $33$ {\tiny $\pm 2$} & $22$ {\tiny $\pm 8$} & $1$ {\tiny $\pm 0$} & $1$ {\tiny $\pm 1$} & $53$ {\tiny $\pm 8$} & $19$ {\tiny $\pm 1$} & $38$ {\tiny $\pm 5$} & $\mathbf{60}$ {\tiny $\pm 14$} & $\mathbf{58}$ {\tiny $\pm 5$} \\
\texttt{OGBench humanoidmaze-large-singletask ($\mathbf{5}$ tasks)} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 1$} & $2$ {\tiny $\pm 1$} & $1$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $0$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 0$} & $\mathbf{11}$ {\tiny $\pm 2$} & $4$ {\tiny $\pm 2$} \\
\texttt{OGBench antsoccer-arena-singletask ($\mathbf{5}$ tasks)} & $1$ {\tiny $\pm 0$} & $8$ {\tiny $\pm 2$} & $0$ {\tiny $\pm 0$} & $12$ {\tiny $\pm 4$} & $1$ {\tiny $\pm 0$} & $2$ {\tiny $\pm 4$} & $12$ {\tiny $\pm 0$} & $16$ {\tiny $\pm 1$} & $33$ {\tiny $\pm 6$} & $\mathbf{60}$ {\tiny $\pm 2$} \\
\texttt{OGBench cube-single-singletask ($\mathbf{5}$ tasks)} & $5$ {\tiny $\pm 1$} & $83$ {\tiny $\pm 3$} & $91$ {\tiny $\pm 2$} & $\mathbf{95}$ {\tiny $\pm 2$} & $80$ {\tiny $\pm 5$} & $85$ {\tiny $\pm 9$} & $81$ {\tiny $\pm 4$} & $79$ {\tiny $\pm 7$} & $79$ {\tiny $\pm 2$} & $\mathbf{96}$ {\tiny $\pm 1$} \\
\texttt{OGBench cube-double-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 1$} & $7$ {\tiny $\pm 1$} & $12$ {\tiny $\pm 1$} & $15$ {\tiny $\pm 6$} & $2$ {\tiny $\pm 1$} & $6$ {\tiny $\pm 2$} & $5$ {\tiny $\pm 2$} & $15$ {\tiny $\pm 3$} & $14$ {\tiny $\pm 3$} & $\mathbf{29}$ {\tiny $\pm 2$} \\
\texttt{OGBench scene-singletask ($\mathbf{5}$ tasks)} & $5$ {\tiny $\pm 1$} & $28$ {\tiny $\pm 1$} & $41$ {\tiny $\pm 3$} & $46$ {\tiny $\pm 3$} & $20$ {\tiny $\pm 1$} & $40$ {\tiny $\pm 7$} & $30$ {\tiny $\pm 3$} & $45$ {\tiny $\pm 5$} & $30$ {\tiny $\pm 3$} & $\mathbf{56}$ {\tiny $\pm 2$} \\
\texttt{OGBench puzzle-3x3-singletask ($\mathbf{5}$ tasks)} & $2$ {\tiny $\pm 0$} & $9$ {\tiny $\pm 1$} & $21$ {\tiny $\pm 1$} & $10$ {\tiny $\pm 2$} & $18$ {\tiny $\pm 1$} & $19$ {\tiny $\pm 0$} & $6$ {\tiny $\pm 2$} & $14$ {\tiny $\pm 4$} & $19$ {\tiny $\pm 1$} & $\mathbf{30}$ {\tiny $\pm 1$} \\
\texttt{OGBench puzzle-4x4-singletask ($\mathbf{5}$ tasks)} & $0$ {\tiny $\pm 0$} & $7$ {\tiny $\pm 1$} & $14$ {\tiny $\pm 1$} & $\mathbf{29}$ {\tiny $\pm 3$} & $10$ {\tiny $\pm 3$} & $15$ {\tiny $\pm 3$} & $1$ {\tiny $\pm 0$} & $13$ {\tiny $\pm 1$} & $25$ {\tiny $\pm 5$} & $17$ {\tiny $\pm 2$} \\
\texttt{D4RL antmaze ($\mathbf{6}$ tasks)} & $17$ & $57$ & $78$ & $79$ & $74$ & $30$ {\tiny $\pm 3$} & $44$ {\tiny $\pm 3$} & $64$ {\tiny $\pm 7$} & $65$ {\tiny $\pm 7$} & $\mathbf{84}$ {\tiny $\pm 3$} \\
\texttt{D4RL adroit ($\mathbf{12}$ tasks)} & $48$ & $53$ & $\mathbf{59}$ & $52$ {\tiny $\pm 1$} & $51$ {\tiny $\pm 1$} & $43$ {\tiny $\pm 2$} & $48$ {\tiny $\pm 1$} & $50$ {\tiny $\pm 2$} & $52$ {\tiny $\pm 1$} & $52$ {\tiny $\pm 1$} \\
\texttt{Visual manipulation ($\mathbf{5}$ tasks)} & - & $42$ {\tiny $\pm 4$} & $60$ {\tiny $\pm 2$} & - & - & - & - & $22$ {\tiny $\pm 2$} & $50$ {\tiny $\pm 5$} & $\mathbf{65}$ {\tiny $\pm 2$} \\



\bottomrule
\end{tabular}
\begin{tablenotes}
\item[1] Due to the high computational cost of pixel-based tasks,
we selectively benchmark $5$ methods that achieve strong performance on state-based OGBench tasks.
\end{tablenotes}
\end{threeparttable}
}
%
\end{table*}