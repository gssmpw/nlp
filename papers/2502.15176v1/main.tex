%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,screen,nonacm]{acmart} %remove nonacm to print "Submitted to ACM" & add 'review' to have numbering

\usepackage{amsmath}
%\usepackage{amssymb} % Add this package for \checkmark
\usepackage{booktabs}
\usepackage{soul}


\newcommand{\cmark}{\checkmark} % Checkmark
\newcommand{\xmark}{\text{\sffamily X}}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmlicensed}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{XXXXXXX.XXXXXXX}
%



%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Methods and Trends in Detecting Generated Images: A Comprehensive Review  }

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Arpan Mahara}
\email{amaha038@fiu.edu}
\orcid{0009-0003-5831-3552} % Add ORCID if available

\author{Naphtali Rishe}
\email{rishen@cs.fiu.edu}
\orcid{0000-0002-1611-4067}

\affiliation{%
  \institution{Knight Foundation School of Computing and Information Sciences, Florida International University}
  %\streetaddress{11200 SW 8th St.}
  \city{Miami}
  \state{Florida}
  %\postcode{33199}
  \country{USA}
}
%% Optionally, you can add acknowledgments or author notes if needed, as follows:
% \authornote{Both authors contributed equally to this research.}
% \authornotemark[1]


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Mahara and Rishe}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
The proliferation of generative models, such as Generative Adversarial Networks (GANs), Diffusion Models, and Variational Autoencoders (VAEs), has enabled the synthesis of high-quality multimedia data. However, these advancements have also raised significant concerns regarding adversarial attacks, unethical usage, and societal harm. Recognizing these challenges, researchers have increasingly focused on developing methodologies to detect synthesized data effectively, aiming to mitigate potential risks. Prior reviews have primarily focused on deepfake detection and often lack coverage of recent advancements in synthetic image detection, particularly methods leveraging multimodal frameworks for improved forensic analysis. To address this gap, the present survey provides a comprehensive review of state-of-the-art methods for detecting and classifying synthetic images generated by advanced generative AI models. This review systematically examines core detection methodologies, identifies commonalities among approaches, and categorizes them into meaningful taxonomies. Furthermore, given the crucial role of large-scale datasets in this field, we present an overview of publicly available datasets that facilitate further research and benchmarking in synthetic data detection.
\end{abstract}


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Forensic Image Detection, Fingerprint Analysis, Generative Adversarial Networks (GANs), Diffusion Models, Frequency Domain Analysis, Vision-Language Models (VLMs), Patch-Based Detection, Cross-Model Generalization, Cross-Domain Detection, Cross-Scene Analysis, Synthetic Image Identification}


%\received{}
%\received[revised]{}
%\received[accepted]{}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
The advent of advanced generative models has enabled the creation of highly realistic synthetic images. These images are synthesized through various approaches, including conditional methods such as image-to-image translation and text-to-image translation, as well as unconditional generation. The present survey focuses on the detection of fully synthesized images, distinguishing them from subtle manipulations or partial modifications. While prior survey have addressed the detection of image manipulations achieved through traditional computer-based techniques \cite{sencar2013digital}, image editing tools \cite{zheng2019survey}, and deepfake methodologies \cite{wang2024deepfake}, our focus diverges from these areas. Specifically, we examine detection methods designed to identify images synthesized by state-of-the-art generative models \cite{nichol2021glide, ramesh2021zero, rombach2022high, Wukong}, including recent approaches that incorporate multimodal techniques \cite{radford2021learning} for enhanced detection accuracy. Given the increasing relevance of such methods in forensic image analysis, it is crucial to explore their long-term applicability and effectiveness in generative image detection.

The detection of images generated by both conditional and unconditional generative methods has emerged as a rapidly growing research area. However, despite this increasing interest, a comprehensive review consolidating and systematically categorizing existing detection methodologies remains lacking. This survey addresses this gap by providing a structured analysis of detection approaches, classifying them based on their underlying techniques and contributions. 
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/Different_Generative_Models_Family.png}
    \caption{Illustration of Simplified Architecture of GAN, Diffusion, VAE and Autoregressive Generative Models.}
    \Description{Diagram illustrating the architectures of various generative models}
    \label{fig:detection_categories}
\end{figure*}

\paragraph{Generative Adversarial Networks (GANs)}
GANs were first proposed by Goodfellow et al. \cite{goodfellow2014generative}. The architecture consists of two neural networks, a generator \( G \) and a discriminator \( D \), that compete in an adversarial game, optimizing the adversarial loss:
\begin{equation}
\min_G \max_D \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log(1 - D(G(z)))].
\label{eq:adversarial_loss}
\end{equation}
This innovative approach introduced adversarial training, boosting the quality of generated data. Despite their success, GANs face challenges such as mode collapse and instability during training.

\paragraph{Diffusion Models}
Diffusion models were first introduced by Sohl-Dickstein et al.~\cite{sohl2015deep} and later demonstrated in practice by Ho et al.~\cite{ho2020denoising}. These models progressively convert an initial Gaussian noise distribution into a structured data distribution via a bidirectional diffusion process. The reverse process is guided by the following equation:
\begin{equation}
\mathbf{x}_{t-1} = \mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\alpha_t}} \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) + \sqrt{\beta_t} \mathbf{z}_t,
\label{eq:reverse_process}
\end{equation}
where \(\beta_t\) and \(\alpha_t\) are variance scaling parameters. Diffusion models have achieved state-of-the-art results in image generation and have played a pivotal role in advancing text-to-image generation. However, they remain computationally intensive due to the iterative denoising process.

\paragraph{Variational Autoencoders (VAEs)}
VAEs were proposed by Kingma and Welling~\cite{kingma2013auto}. VAEs learn a probabilistic mapping between data and a latent space by maximizing the Evidence Lower Bound (ELBO):
\begin{equation}
\mathcal{L} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z)).
\label{eq:vae_elbo}
\end{equation}
This framework enables efficient data sampling (encoding) and reconstruction (decoding). Despite their innovative probabilistic approach, VAEs often generate blurry images compared to GANs due to their reliance on pixel-wise reconstruction loss.

\paragraph{Autoregressive Models}
Autoregressive models for image generation were first introduced in PixelRNN \cite{van2016pixel} and PixelCNN \cite{van2016conditional} by Oord et al. These models factorize the joint probability distribution of an image as a product of conditional probabilities, generating pixels sequentially: \begin{equation} p(\mathbf{x}) = \prod_{i=1}^{N} p(x_i | x_{<i}), \label{eq:autoregressive} \end{equation} where each pixel \(x_i\) is predicted based on previously generated pixels. PixelRNN uses recurrent layers to model spatial dependencies, while PixelCNN employs masked convolutions to improve parallelization. Recently, Transformer-based autoregressive models\cite{parmar2018image} have further improved scalability by leveraging self-attention mechanisms. Autoregressive models achieve high-quality image synthesis with explicit likelihood estimation but suffer from slow generation due to their sequential nature.

While these four generative model families, GANs, Diffusion Models, VAEs, and Autoregressive, have dominated the landscape of image generation, other methods such as Normalizing Flows~\cite{rezende2015variational}, have also made significant contributions and deserve attention. With these advancements, several state-of-the-art generative models have been made publicly available alongside commercially accessible tools such as Adobe Firefly \cite{adobe_firefly}, MidJourney \cite{midjourney}, DALL·E 3 \cite{dalle3}, and Imagen 3 \cite{imagen3}.

Although generative models have enabled significant advancements in image synthesis, their accessibility introduces critical concerns related to misinformation, privacy, and security. The ability to generate highly realistic AI-synthesized images raises ethical and security risks, including deceptive media \cite{sandrini2023generative}, identity fraud \cite{zhang2025ai}, and geopolitical manipulation \cite{mylrea2025generative}. Consequently, forensic detection has become an essential area of research, aiming to reliably distinguish AI-generated imagery from real-world photographs. Despite significant progress, existing detection techniques face challenges in generalizability, robustness, and scalability across evolving generative architectures. This survey provides a comprehensive review of image detection methodologies, summarizing key advancements, identifying existing limitations, and outlining future directions. By examining the evolving landscape of adversarial threats in generative AI, this study highlights critical challenges and opportunities, reinforcing the need for continued research in forensic detection.


\section{Reviews}
In the present survey of detection methods for AI-generated images, we categorize them into six distinct groups, with the final category comprising commercial methods. For each group, we discuss the core proposal of each method in ascending order based on publication date. At the end of each subsection, we present a table summarizing whether the experimental evaluations of the methods satisfy three key criteria, described as follows:

1. Cross-Family Generators: This criterion evaluates whether a detection method, trained on images from one type of generative model (e.g., GAN), was tested on and demonstrated effectiveness in detecting images generated by a different type of generative model (e.g., Diffusion models). A method that evaluates images from multiple generative model types satisfies the cross-family criterion.

2. Cross-Category: This criterion examines whether a method was trained and tested on images belonging to different classes. For example, a detector trained on human face images and evaluated on its ability to detect generated images of animals satisfies the cross-category requirement.

3. Cross-Scene: This criterion assesses whether a method’s performance was tested across datasets with distinct data distributions. For instance, a detector trained on images from the ImageNet \cite{russakovsky2015imagenet} dataset and evaluated on images from the LSUN-Bedroom \cite{yu2015lsun} dataset satisfies the cross-scene requirement. Importantly, methods that meet the cross-scene criterion typically also satisfy the cross-category requirement, although the converse is not necessarily true.

It is worth mentioning that these criteria are not extreme cases and do not imply that such evaluations are impossible for any given method. Instead, they are based on the descriptions and experimental setups reported in the original papers. The goal is to assist future work in considering these criteria to demonstrate generalizability and provide real-world evidence.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/ACM_Detection_Categories.png}
    \caption{Categorization of Detection Methods Based on Core Architecture and Methodological Proposals. The figure illustrates the division of detection methods into categories. Each category is highlighted using blue-colored rectangles. Some methods are connected to multiple categories, shown using light-red highlights and arrows pointing to the respective sub-categories.}
    \Description{Diagram illustrating the categories.}
    \label{fig:detection_categories}
\end{figure*}

\subsection{Spatial Domain Analysis / Spatial Feature Analysis Methods}
Spatial domain analysis methods focus on detecting generated images by analyzing intrinsic features within the pixel-level spatial representation. These methods leverage spatial patterns such as texture irregularities, unnatural edge formations, and color inconsistencies, which are common artifacts introduced during the image synthesis process. By examining spatial features like intensity gradients, local pixel dependencies, and edge sharpness, these approaches can effectively uncover subtle distortions. Techniques often employed include convolutional neural networks (CNNs) for automated feature extraction, statistical analysis of pixel intensity distributions, and handcrafted feature-based classifiers. Spatial domain methods excel in capturing localized anomalies and abrupt visual transitions, providing a robust and interpretable approach for image forgery detection.

\subsubsection{Co-occurrence Matrices On Detecting GAN Generated Images by Nataraj et al.: 2019}
Nataraj et al. \cite{nataraj2019detecting} proposed a GAN-generated image detection method using co-occurrence matrices extracted from RGB channels. Unlike forensic techniques that used such matrices, their approach bypasses residual computations and directly processes pixel co-occurrence matrices using a deep convolutional neural network (CNN). The method computes co-occurrence matrices on the red, green, and blue channels of an image, capturing statistical anomalies introduced by GAN-generated images. These matrices are structured into tensors, which are subsequently processed through a custom CNN. The network consists of convolutional layers with ReLU activations, max pooling operations, and fully connected layers, concluding with a sigmoid activation for binary classification. The evaluation was conducted on two diverse datasets: CycleGAN \cite{zhu2017unpaired} (unpaired image-to-image translations) and StarGAN \cite{choi2018stargan} (facial attribute manipulations).

\subsubsection{CNN-Generated Images Are Surprisingly Easy to Spot: Wang et al. (2020)}
Wang et al. \cite{wang2020cnn} conducted a pivotal study demonstrating that a detection method trained on images generated by a single generative model, particularly a GAN, can generalize to detect synthetic images from a variety of unseen CNN-based generative models. This finding challenges the previously established view that cross-model generalization is inherently difficult for forensic classifiers. To evaluate this, the authors trained ProGAN \cite{karras2017progressive} on the LSUN dataset \cite{yu2015lsun}, producing a dataset of 720K images for training and 4K for validation, with an equal split between real and generated images. They employed a ResNet-50 \cite{he2016deep}, pretrained on ImageNet, as a binary classifier. Robust feature learning was facilitated through various data augmentation strategies, including: (a) no augmentation, (b) Gaussian blur, (c) JPEG compression, and (d) combinations of both with varying probabilities (50\% and 10\%).
The trained classifier demonstrated strong generalization capabilities, successfully detecting images synthesized by other prominent generative models such as StyleGAN \cite{karras2019style}, BigGAN \cite{brock2018big}, CycleGAN \cite{zhu2017unpaired}, StarGAN \cite{choi2018stargan}, and GauGAN \cite{park2019semantic}. This work highlights the existence of common artifacts across CNN-generated images, suggesting that classifiers can leverage these shared patterns for generalizable detection across different architectures and tasks.

\subsubsection{Detection and Attribution of GAN Images: Goebel et al. (2020)}
Goebel et al. \cite{goebel2020detection} introduced a comprehensive framework to detect, attribute, and localize GAN-generated images through co-occurrence matrix analysis and deep learning. This method builds on insights from steganalysis, leveraging pixel-level co-occurrence features to identify artifacts introduced during image synthesis. The approach begins by computing co-occurrence matrices from the RGB channels of the input image in four orientations: horizontal, vertical, diagonal, and anti-diagonal. Each co-occurrence matrix captures a 256×256 histogram of pixel value pairs, normalized and stacked into a tensor of size \(256 \times 256 \times 12\). The matrices are defined as:
\[
C_{i,j} = \sum_{m,n} 
\begin{cases} 
1, & \text{if } I[m,n] = i \text{ and } I[m+1,n] = j \\ 
0, & \text{otherwise}.
\end{cases}
\]

These features are then processed using a modified XceptionNet \cite{chollet2017xception}, designed for three key tasks:
1. Binary Detection: Classifying images as real or GAN-generated.
2. Multi-Class Attribution: Identifying the GAN architecture (e.g., ProGAN, CycleGAN).
3. Localization: Generating heatmaps to identify manipulated regions through patch-based analysis.

Extensive experiments conducted on over 2.76 million images demonstrated the effectiveness of this method across various GAN models, including ProGAN, StyleGAN, CycleGAN, StarGAN, and SPADE/GauGAN. Additionally, t-SNE \cite{van2008visualizing} visualizations showed a clear separation between real and GAN-generated images, reinforcing the model's interpretability and robustness against varying JPEG compression levels and patch sizes. This framework advances GAN forensics by integrating detection, attribution, and localization into a unified pipeline.

\subsubsection{Estimating Artifact Similarity with Representation Learning: Li et al. (2022), GASE-Net}
Li et al. \cite{li2022ArtifactSimilarity} introduced GASE-Net, a framework designed to detect GAN-generated images by estimating artifact similarity. This method addresses challenges in cross-domain generalization and robustness against post-processing, using a two-stage approach: representation learning and representation comparison. In the representation learning stage, a ResNet-50 backbone is modified with instance normalization (IN) applied to the shallow layers to enhance feature extraction by filtering out instance-specific biases. This ensures that the learned representations remain invariant across different domains while retaining category-level distinctions. Feature maps from reference images are aggregated element-wise to form domain prototypes, which serve as robust representations of GAN or pristine image domains.

In the representation comparison stage, the feature map of a consult (suspicious) image is concatenated with the domain prototypes along the channel dimension. A shallow CNN processes the concatenated tensor to output similarity scores. The network is optimized using a Category and Domain-Aware (CDA) loss, which maximizes inter-class separation and minimizes intra-class variation by leveraging both domain and category information. The ground truth similarity scores \( v_{\text{true}} \) for optimization are defined as:
\[
\hat{s}_n =
\begin{cases}
1, & \text{if } y^* = y_n, \\
0, & \text{if } y^* \neq y_n,
\end{cases}
\]
where \( y^* \) and \( y_n \) denote the category labels of the consult image and the \( n \)-th domain prototype, respectively. The predicted similarity scores \( v_{\text{pred}} \) are optimized against \( v_{\text{true}} \) using Mean Square Error (MSE) loss.

During inference, similarity scores are averaged across GAN-generated and pristine domains. An image is classified as GAN-generated if the average fake score exceeds the pristine score. Extensive experiments demonstrated that GASE-Net outperforms state-of-the-art methods in cross-domain scenarios, preserving resilience against various post-processing techniques, including JPEG compression, Gaussian blur, and resizing.

\subsubsection{Local Intrinsic Dimensionality Analysis: Lorenz et al. (2023), AdaptedMultiLID}
Lorenz et al. \cite{lorenz2023detecting} proposed a framework utilizing the Multi-Local Intrinsic Dimensionality (multiLID) method for detecting diffusion-generated images. This approach builds upon the earlier work \cite{lorenz2022unfolding} and demonstrates strong performance in distinguishing both diffusion and GAN-generated images. The method starts by extracting low-dimensional feature maps using an untrained ResNet18 model. MultiLID is then computed to capture local growth rates of feature densities within the latent space. The multiLID feature vector for each sample \( x \) is defined as:
\[
\text{multiLID}_d(x)[i] = -\log\left(\frac{d_i(x)}{d_k(x)}\right),
\]
where \( d_i(x) \) and \( d_k(x) \) represent the Euclidean distances to the \( i^{\text{th}} \) and \( k^{\text{th}} \) nearest neighbors, respectively. A random forest classifier is trained on the labeled multiLID scores to perform image classification. Extensive experiments validate the effectiveness of this method, with high detection accuracy achieved for both diffusion and GAN-generated images across multiple datasets. The framework is also resilient to post-processing operations such as JPEG compression and Gaussian blur, making it suitable for real-world detection scenarios.

\subsubsection{AI-Generated Image Detection: Baraheem et al. (2023)}
Baraheem et al. \cite{baraheem2023ai} proposed a framework to detect GAN-generated images using transfer learning on pretrained classifiers. The authors compiled a diverse dataset called Real and Synthetic Images (RSI), consisting of 48,000 images synthesized by 12 GAN architectures across tasks such as image-to-image, sketch-to-image, and text-to-image generation. EfficientNetB4 \cite{tan2019efficientnet} achieved the best detection performance after fine-tuning on the RSI dataset. The model's architecture was modified by replacing the classifier head with layers for global average pooling, batch normalization, ReLU activation, dropout, and a sigmoid output. The training utilized the Adam optimizer with a batch size of 64, an initial learning rate of 0.001, and data augmentation techniques such as horizontal flips. To facilitate model explainability, the authors incorporated four Class Activation Map (CAM) methods, GradCAM \cite{selvaraju2017gradcam}, AblationCAM \cite{ramaswamy2020ablationcam}, LayerCAM \cite{jiang2021layercam}, and Faster ScoreCAM \cite{wang2020scorecam}, to visualize the discriminative regions influencing classification decisions. 

\subsubsection{Diffusion Reconstruction Error (DIRE): Wang et al. (2023)}
Wang et al. \cite{wang2023dire} proposed DIffusion Reconstruction Error (DIRE), a novel method to detect diffusion-generated images by leveraging reconstruction errors from pre-trained diffusion models. The method addresses the limitations of previous detectors, which struggled to generalize across different diffusion models. The key idea is that diffusion-generated images can be reconstructed more accurately by the pre-trained DDIM model \cite{song2020denoising} than real images. DIRE is defined as the \(L_1\)-norm of the difference between the input image \( x_0 \) and its reconstructed counterpart \( x'_0 \):
\[
\text{DIRE}(x_0) = \|x_0 - x'_0\|_1.
\]

The process involves applying forward noise to the input image and then performing a reverse denoising process to generate the reconstruction. A ResNet-50 classifier is trained using binary cross-entropy loss on these DIRE representations to distinguish between real and generated images. The authors demonstrated that DIRE achieves state-of-the-art performance on their proposed DiffusionForensics dataset, which includes images generated by various diffusion models across multiple domains (e.g., LSUN-Bedroom \cite{yu2015lsun}, ImageNet \cite{russakovsky2015imagenet}, and CelebA-HQ \cite{karras2017progressive}). Extensive experiments showed that DIRE not only excels in detecting diffusion-generated images but also generalizes well to unseen models and maintains robustness under perturbations like Gaussian blur and JPEG compression.

\subsubsection{Classification and Explainable Identification: Bird and Lotfi (2024)}
Bird and Lotfi \cite{bird2024cifake} introduced a framework for detecting AI-generated images using a large-scale dataset, CIFAKE, which is detailed in the datasets section. Their classification approach employs a Convolutional Neural Network (CNN) that processes images through stacked convolutional and pooling layers, followed by fully connected layers with a final sigmoid activation for binary classification. As seen in \cite{baraheem2023ai}, the study emphasizes explainability by implementing Gradient Class Activation Mapping (Grad-CAM) \cite{selvaraju2017gradcam}, which generates heatmaps to highlight regions influencing the model’s decisions. Grad-CAM computes importance weights \(\alpha_k\) for each feature map \(A_k\), producing a visual explanation as:
\[
L_c^{\text{Grad-CAM}} = \text{ReLU} \left( \sum_k \alpha_k A_k \right), \quad \alpha_k = \frac{1}{Z} \sum_{i,j} \frac{\partial y_c}{\partial A_k^{i,j}},
\]
where \(Z\) is the spatial size of the feature map. The heatmaps reveal that the model focuses on subtle imperfections, often in the image background, to differentiate between real and synthetic images. This approach improves trust in AI-generated image detection by combining high classification performance with visual interpretability, making it a valuable contribution to computer vision and data authenticity research.

\subsubsection{Self-Contrastive Learning on ImagiNet: Boychev et al. (2024)}
Boychev et al. \cite{boychev2024imaginet} introduce a large-scale dataset, ImagiNet, consisting of 200,000 real and synthetic images, as detailed in the datasets section. The detection framework consists of two stages: pretraining with Self-Contrastive Learning (SelfCon) and a calibration step. During the pretraining phase, the ResNet-50 \cite{he2016deep} backbone, initialized with ImageNet weights, is paired with a sub-network that projects intermediate feature maps into a shared latent space. This setup produces two output embeddings per input image, facilitating contrastive learning. The SelfCon loss is defined as:
\[
L_{\text{SelfCon}} = \sum_{i \in A, \omega \in \Omega} -\frac{1}{|P(i)||\Omega|} \sum_{p \in P(i), \omega' \in \Omega} \log \frac{\exp(\omega(x_i) \cdot \omega'(x_p) / \tau)}{\sum_{l \in Q(i)} \exp(\omega(x_i) \cdot \omega'(x_l) / \tau)},
\]
where \(A\) represents the set of anchor images in a batch, \(P(i)\) denotes positive samples for anchor \(x_i\), \(Q(i)\) contains negative samples, and \(\omega(x)\) is the normalized embedding. The method balances feature similarities and differences using a temperature parameter \(\tau\).

In the calibration step, the sub-network and projection heads are removed, and a multilayer perceptron (MLP) classifier is trained using cross-entropy loss on an equal number of real and synthetic images. This stage enhances robustness by fine-tuning the learned features for both detection and model identification tasks. Experimental results indicate that the framework achieves up to 0.99 AUC and 95\% balanced accuracy, demonstrating robust zero-shot performance on various synthetic image benchmarks. 


\begin{table*}
  \caption{Evaluation of Cross-Family Generators, Cross-Category, and Cross-Scene Generalization in Detection Models from Spatial Domain Analysis Category}
  \label{tab:cross_evaluation_spatial}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Models} & \textbf{Cross-Family Generators} & \textbf{Cross-Category} & \textbf{Cross-Scene} \\
    \midrule
    Nataraj et al. \cite{nataraj2019detecting} & \xmark & \xmark & \xmark \\
    \midrule
    Wang et al. \cite{wang2020cnn} & \xmark & \cmark & \xmark \\
    \midrule
    Goebel et al. \cite{goebel2020detection} & \xmark & \cmark & \xmark \\
    \midrule
    Li et al. \cite{li2022ArtifactSimilarity} & \xmark & \xmark & \xmark \\
    \midrule
    Lorenz et al. \cite{lorenz2023detecting} & \cmark & \cmark & \xmark \\
    \midrule
    Baraheem et al. \cite{baraheem2023ai} & \xmark & \cmark & \xmark \\
    \midrule
    Wang et al. \cite{wang2023dire} & \cmark & \cmark & \cmark \\ %Here they mention Cross-dataset similar to cross-scene
    \midrule
    Bird \& Lotfi \cite{bird2024cifake} & \xmark & \cmark & \xmark \\
    \midrule
    Boychev et al. \cite{boychev2024imaginet} & \cmark & \cmark & \xmark \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/Types_of_VisionLanguageMethods.png}
    \caption{Taxonomy of Vision-Language Models (VLMs) categorized by learning paradigms.}
    \Description{Diagram illustrating the types of VLMs}
    \label{fig:vision_language_models}
\end{figure*}


\subsection{Multimodal Vision-Language Methods}
Multimodal vision-language methods leverage vision-language models (VLMs) (see Fig. \ref{fig:vision_language_models} for the types of VLMs) trained on large-scale image-text datasets to detect generated images by aligning visual and textual features. These approaches detect inconsistencies in synthetic images through cross-modal embeddings, enabling effective detection across a wide range of generative methods. Models such as CLIP \cite{radford2021learning} which belongs to the contrastive learning \cite{chen2020simplecontrastive} category (see Fig. \ref{fig:vision_language_models} for categorization) are commonly employed in transfer learning setups, with novel adaptations to distinguish real images from generated ones. These methods demonstrate robust generalization and adaptability to various generative techniques and contexts, as will be discussed in the following paragraphs.

\subsubsection{Universal Fake Image Detector by Ojha (2023)}
Ojha et al. \cite{ojha2023towards} identified that existing fake image detectors struggle with generalization, often misclassifying images from unseen generative models as real. This limitation arises from classifiers being asymmetrically tuned to detect artifacts specific to training data. To address this, the authors propose constructing a meaningful feature space using CLIP:ViT \cite{dosovitskiy2020image, radford2021learning}, trained on 400 million image-text pairs. They employ two approaches: (a) mapping training images to CLIP's final layer to create feature representations, then during inference, classifying images based on cosine distance to the nearest neighbor in real and fake feature spaces, and (b) augmenting CLIP with a linear layer for binary classification. Both methods demonstrated generalization, effectively detecting synthetic images from state-of-the-art generative models.


\subsubsection{Language-Guided Synthetic Image Detection by Wu (2023)}
Wu et al. \cite{wu2023generalizable} propose a language-guided approach for detecting synthetic images by integrating image-text contrastive learning in a VLM. They reformulate the detection task as an identification problem, determining whether a query image aligns with an anchor set of text-labeled images. This method enhances generalization by aligning visual features with carefully designed textual labels such as "Real Photo," "Real Painting," "Synthetic Photo," and "Synthetic Painting." The authors found these labels more effective than simpler "real" or "fake" categories, as they account for differences in image types, such as camera-captured versus digitally created content. The proposed LASTED framework encodes images using a ResNet-50x64 vision encoder and text labels using a transformer-based text encoder. During training, both encoders generate visual and textual representations, \(e_v\) and \(e_t\), respectively. A contrastive loss aligns these features, ensuring that matched pairs have higher similarity scores than unmatched ones:
\[
L_I = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp(e_{v,i} \cdot e_{t,i} / \tau)}{\sum_{j=1}^N \exp(e_{v,i} \cdot e_{t,j} / \tau)},
\]
where \(\tau\) is a temperature parameter.

During testing, the text encoder is discarded. A mean representation vector is computed from an anchor set of known images. The input image's representation is compared to this anchor vector using cosine similarity, which determines its category based on a predefined threshold. This approach allows the model to generalize across diverse generative models and contexts. While LASTED model has achieved better performance on the selected dataset, as seen in the paper text-labels are influential factors, selecting only four labels with photo and paintings might limit towards generalizability when the images are from very different domain such as medical images and the satellite imagery. The researchers should design appropriate labels for their specific tasks related images.

\subsubsection{GenDet: Good Generalizations by Zhu (2023)}
Zhu et al. \cite{zhu2023gendet} address the challenge of detecting synthetic images from unseen generators, which existing methods struggle to classify due to limited generalization. The authors propose \textbf{GenDet}, a detection model composed of two key components: \textit{Teacher-Student Discrepancy-Aware Learning} and \textit{Generalized Feature Augmentation}. These components are trained through an adversarial framework to improve generalization to both seen and unseen generators. The model employs a feature extractor \(E\), based on CLIP:ViT \cite{radford2021learning}, to extract features from input images. The \textit{Teacher-Student Discrepancy-Aware Learning} is designed to: a) Reduce the difference in output between a teacher network (\(N_t\)) and a student network (\(N_s\)) for real images, b) Amplify this difference for fake images to enhance detection.
The discrepancy losses are defined as:
\[
\mathcal{L}_{\text{min}} = \frac{1}{b} \sum_{i=1}^b \|N_t(f_i) - N_s(f_i)\|_2^2, \quad \mathcal{L}_{\text{max}} = -\mathcal{L}_{\text{min}},
\]
where \(f_i\) represents features from real or fake images, and \(b\) is the batch size.

To further enhance generalization, the \textit{Generalized Feature Augmenter} adversarially generates augmented features. This augmenter facilitates the depletion of the difference between the teacher and student networks for augmented fake features, encouraging the model to detect unseen synthetic images by maintaining a large discrepancy during inference. Finally, a binary classifier \(N_c\) is trained on the variation between the teacher and student outputs to classify images as real or fake. Experiments demonstrate that GenDet achieves state-of-the-art performance on the UniversalFakeDetect and GenImage datasets, surpassing prior methods in both accuracy and mean average precision (mAP).


\subsubsection{Mixture of Low-Rank Experts: Liu 2024}
Liu et al. \cite{liu2024mixture} propose a transferable AI-generated image detection model utilizing CLIP-ViT as the backbone with parameter-efficient fine-tuning. The method modifies the MLP layers of the last three ViT-B/32 blocks through a Mixture of Low-Rank Experts (MoLE), integrating both shared and separate Low-Rank Adapters (LoRAs). Shared LoRAs capture common feature representations across datasets, while separate LoRAs specialize in diverse generative patterns. A trainable gating mechanism dynamically assigns input tokens to appropriate experts, with a load-balancing loss ensuring uniform expert utilization. The model freezes most CLIP parameters, adapting only LoRA modules and a new MLP classification head with a sigmoid activation. The forward operation in each MLP block is expressed as:
\[
\Delta Wx = \frac{\alpha}{r} B A x + \sum_{i=1}^{N} G_i(x) \frac{\alpha_i}{r_i} B_i A_i x,
\]
where $G_i(x)$ is the gating function, and $A$, $B$, $A_i$, and $B_i$ are low-rank matrices.

The approach achieves state-of-the-art generalization across unseen diffusion and autoregressive models, with superior robustness to post-processing perturbations like Gaussian blur and JPEG compression. Experimental results on benchmarks such as UnivFD and GenImage show that the proposed method surpasses existing detectors, including UnivFD and DIRE, by up to +12.72\% in classification accuracy.


\subsubsection{MERGING A MIXTURE OF HYPER LORAS: HYPERDET by Cao 2024}
Cao et al. \cite{cao2024hyperdet} propose a generalizable synthetic image detection framework, HyperDet, leveraging the large pretrained multimodal model, CLIP: ViT-L/14, as the backbone, similar to Liu et al. \cite{liu2024mixture}, but with few notable innovations. Unlike Liu et al., the authors introduce a novel grouping of Spatial Rich Model (SRM) filters into five distinct groups to generate multiple filtered views of input images, capturing varying levels of pixel artifacts and features. Along this, HyperDet employs Hyper LoRAs, a hypernetwork-based approach that generates Low-Rank Adaptation (LoRA) weights for fine-tuning the CLIP model. These LoRA weights are computed using three types of embeddings: task embeddings, layer embeddings, and position embeddings. The outputs of these LoRA experts are merged to form a unified representation for classification, effectively integrating shared and specific knowledge for generalizable feature extraction. During training, HyperDet fine-tunes the last eight fully connected layers of the CLIP: ViT-L/14 model, along with the newly introduced Hyper LoRAs modules. To address imbalanced optimization, the framework employs a composite binary cross-entropy loss function, incorporating both original and filtered views of the images. This design obtains robust performance in detecting synthetic images across diverse generative models and datasets.

\subsubsection{Forgery-Aware Adaptive Transformer (FatFormer) by Liu: 2024}
Liu et al. \cite{liu2024fatformer} introduce FatFormer, a generalizable synthetic image detection framework utilizing the pretrained CLIP model inspired by the work of Ojha et al. \cite{ojha2023towards}. The authors address the limitations of freezing CLIP's layers, which hinders the generalization of forgery detection. FatFormer integrates two modules: the Forgery-Aware Adapter (FAA) and Language-Guided Alignment (LGA), for effective adaptation of CLIP's features. The FAA module extracts forgery artifacts from both image and frequency domains. The Image Forgery Extractor applies lightweight convolution layers to capture low-level artifacts, while the Frequency Forgery Extractor employs Discrete Wavelet Transform (DWT) and grouped attention mechanisms to dynamically aggregate multi-band frequency clues. The final adapted feature representation at each ViT stage is defined as:
\[
g^{(j)} = g^{(j)}_{\text{img}} + \lambda \cdot g^{(j)}_{\text{freq}},
\]
where \(\lambda\) balances image and frequency contributions.

The LGA module enhances text prompts using a Patch-Based Enhancer (PBE) and aligns image patch tokens with text embeddings through the Text-Guided Interactor (TGI). Contrastive loss is applied on the cosine similarities between image and text embeddings:
\[
S^{(i)} = \cos(f^{(0)}_{\text{img}}, f^{(i)}_{\text{text}}), \quad S'^{(i)} = \frac{1}{N} \sum_{t=1}^N \cos(f^{(t)}_{\text{img}}, f^{(i)}_{\text{text}}),
\]
where \(N\) is the number of patches.

\subsubsection{Raising the Bar with CLIP: By Cozzolino 2024}
Cozzolino et al. \cite{cozzolino2024raising} implement the CLIP: ViT-L/14 pretrained VLM, similar to the approaches in \cite{cao2024hyperdet, liu2024mixture, koutlis2025leveraging}, for detecting synthetic images with a straightforward yet impactful adjustment. The authors propose generating synthetic images by feeding real-image captions to text-to-image models and then extracting feature vectors for both real and synthetic images using CLIP's image encoder. Specifically, feature vectors are obtained from the second-to-last layer of the ViT module:

\[
r_i = \text{CLIP}(R_i), \quad f_i = \text{CLIP}(F_i),
\]

where \( r_i \) and \( f_i \) represent feature vectors for real image \( R_i \) and synthetic image \( F_i \), respectively.

For classification, the authors employ a simple linear Support Vector Machine (SVM). Notably, the analysis presented suggests that the CLIP-based detector does not rely on the same low-level traces exploited by most existing detectors, making it potentially more robust against adversarial attacks that target low-level features. The authors report using 32,000 images generated by GANs, diffusion models, and commercial text-to-image models for training and evaluation, demonstrating the good performance of their proposed approach across diverse datasets.

\subsubsection{Representation from Encoder-Decoder for Image Detection by Koutlis: 2025}
Koutlis and Papadopoulos \cite{koutlis2025leveraging} propose Representations from Intermediate Encoder-Blocks (RINE) to improve synthetic image detection by extracting low-level features from multiple layers of CLIP’s Vision Transformer (ViT). The method captures both low- and high-level visual semantics by concatenating CLS tokens from each intermediate transformer block into a comprehensive feature representation. The extracted CLS tokens from each block \( Z_l \) are aggregated:
\[
K = \bigoplus_{l=1}^{n} Z_l^{[0]} \in \mathbb{R}^{b \times n \times d},
\]
where \( Z_l^{[0]} \) denotes the CLS token from the \( l \)-th transformer block. To improve feature selection, a Trainable Importance Estimator (TIE) dynamically assigns weights to these representations:
\[
\widetilde{K}_{ik} = \sum_{l=1}^{n} S(A_{lk}) \cdot K_{ilk},
\]
where \( S(A_{lk}) \) represents softmax-activated importance scores for each block.

The features are processed through a projection network, then passed to a classification head with ReLU-activated dense layers and a final sigmoid output for binary classification. The framework optimizes performance using Binary Cross-Entropy (BCE) and Supervised Contrastive Loss:
\[
L = L_{CE} + \xi L_{Cont.},
\]
where \( \xi \) balances the contributions of both objectives. The authors demonstrate that RINE surpasses state-of-the-art methods on 20 test datasets, achieving a +10.6\% accuracy improvement, with training requiring only one epoch (approximately 8 minutes). Additionally, the model is robust to image perturbations, maintaining strong performance across GAN, diffusion, and other synthetic image types.



\begin{table*}
  \caption{Evaluation of Detection Models from Pretrained Vision-Language Methods Category on Cross-Family Generators, Cross-Category, and Cross-Scene Generalization}
  \label{tab:cross_evaluation_visionlanguage}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Models} & \textbf{Cross-Family Generators} & \textbf{Cross-Category} & \textbf{Cross-Scene} \\
    \midrule
    Ojha et al. \cite{ojha2023towards} & \cmark & \cmark & \xmark \\
    \midrule
    Wu et al. \cite{wu2023generalizable} & \cmark & \cmark & \xmark \\
    \midrule
    Zhu et al. \cite{zhu2023gendet} & \cmark & \cmark & \xmark \\
    \midrule
    Liu et al. \cite{liu2024mixture} & \cmark & \cmark & \xmark \\
    \midrule
    Cao et al. \cite{cao2024hyperdet} & \cmark & \cmark & \xmark \\
    \midrule
    Liu et al. \cite{liu2024fatformer} & \cmark & \cmark & \xmark \\
    \midrule
    Cozzolino et al. \cite{cozzolino2024raising} & \cmark & \cmark & \xmark \\
    \midrule
    Koutlis et al. \cite{koutlis2025leveraging} & \cmark & \cmark & \xmark \\
    \bottomrule
  \end{tabular}
\end{table*}



\begin{table*}[ht]
\centering
\caption{Evaluation of multimodal vision-language methods on GAN and diffusion-generated images using the UnivFD dataset \cite{ojha2023towards}. Results are reported in classification accuracy (\%). Methods from \cite{cozzolino2024raising} and \cite{koutlis2025leveraging} were trained on only four classes of ProGAN-generated images, while others were trained on the full ProGAN dataset from \cite{wang2020cnn}.}
\label{tab:detection-results-visionlangugage}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccccccccccc}
\hline
Method           & \multicolumn{7}{c}{Generative adversarial networks} & \multicolumn{2}{c}{Low-level vision} & \multicolumn{2}{c}{Perceptual loss} & \multicolumn{8}{c}{Diffusion models} & Total \\ \cline{2-7} \cline{9-10} \cline{11-12}   \cline{13-20}
                           & Pro-    & Cycle-  & Big-    & Style-  & Gau-    & Star-   & Deep-   & SITD   & SAN    & CRN    & IMLE   & Guided  & \multicolumn{3}{c}{LDM}                   & \multicolumn{3}{c}{GLIDE}  & DALL-E \\  
                           & GAN     & GAN     & GAN     & GAN     & GAN     & GAN     & fakes   &        &        &        &        &         & 200    & 200s    & 100    & 100   &  50 & 100       \\  
                           &         &         &         &         &         &         &         &        &        &        &        &         & steps  & w/CFG  &    steps & 27        & 27        & 10 \\  
                           &         &         &         &         &         &         &         &        &        &        &        &         &         &         &         &          &         & \\ \hline

Ojha et al. \cite{ojha2023towards} & 100.0 & 98.50 & 94.50 & 82.00 & 99.50 & 97.00 & 66.60 & 63.00 & 57.50 & 59.5 & 72.00 & 70.03 & 94.19 & 73.76 & 94.36 & 79.07 & 79.85 & 78.14 & 86.78 & 81.38 \\
Zhu et al. \cite{zhu2023gendet} & 99.00 & 99.50 & 99.30 & 99.05 & 99.00 & 96.75 & 88.20 & 63.50 & 67.50 & 93.90 & 98.75 & 98.70 & 98.80 & 98.60 & 98.75 & 98.75 & 98.75 & 98.75 & 98.45 & 94.42 \\
Liu et al. \cite{liu2024mixture} & 100.0 & 99.33 & 99.67 & 99.46 & 99.83 & 97.07 & 77.53 & 81.11 & 65.50 & 82.32 & 96.79 & 90.70 & 98.30 & 95.90 & 98.75 & 92.40 & 93.95 & 93.00 & 94.90 & 92.45 \\
Cao et al. \cite{cao2024hyperdet} & 100.0 & 97.40 & 97.50 & 97.50 & 96.20 & 98.65 & 73.85 & 93.00 & 75.00 & 92.75 & 93.20 & 77.35 & 98.70 & 96.60 & 98.80 & 87.75 & 89.95 & 88.70 & 97.00 & 92.10 \\
Liu et al. \cite{liu2024fatformer} &99.90 & 99.30 & 99.50  & 97.20 &99.40  & 99.80 & 93.20 &\_ & \_ & \_ & \_  &76.10 & 98.60 & 94.90   & 98.70  & 94.40  & 94.70 & 94.20 & 98.80 & \_  \\
Koutlis et al. \cite{koutlis2025leveraging} & 100.0 & 99.30 & 99.60 & 88.90 & 99.80 & 99.50& 80.60 & 90.60 & 68.30 & 89.20 & 90.60 & 76.10 & 98.30 & 88.20 & 98.60 & 88.90 &92.60 &90.70 & 95.00 &  \_   \\

\hline
\end{tabular}
}
\end{table*}

\subsection{Frequency Domain Analysis Methods}
Frequency domain analysis transforms image data into the spectral domain, facilitating the detection of periodic artifacts, noise distributions, and variations in frequency components often associated with synthetic image generation. Techniques such as the Discrete Fourier Transform (DFT), Discrete Wavelet Transform (DWT), and Discrete Cosine Transform (DCT) are commonly used to extract these spectral features. The Fourier Transform, proposed by Fourier \cite{baron2003analytical}, decomposes signals into their constituent frequencies and is effective in identifying global periodic patterns, including checkerboard artifacts. The two-dimensional Discrete Fourier Transform (DFT) of an image \( f(x, y) \) of size \( M \times N \) is given by:

\begin{equation}
F(u, v) = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x, y) e^{-j 2\pi \left( \frac{ux}{M} + \frac{vy}{N} \right)}
\label{eq:DFT}
\end{equation}

where \( F(u, v) \) represents the frequency domain coefficients, and the inverse DFT (IDFT) reconstructs the image as:

\begin{equation}
f(x, y) = \frac{1}{MN} \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u, v) e^{j 2\pi \left( \frac{ux}{M} + \frac{vy}{N} \right)}
\end{equation}

To improve computational efficiency, the Fast Fourier Transform (FFT), introduced by Cooley and Tukey \cite{cooley1965algorithm}, accelerates the DFT process, enabling the rapid detection of periodic patterns and aliasing artifacts commonly found in synthetic images. The Wavelet Transform, introduced by Haar \cite{haar1911theorie}, provides both spatial and frequency localization, making it suitable for detecting transient and localized artifacts. Unlike the Fourier Transform, which represents signals in terms of sinusoidal waves, the Discrete Wavelet Transform (DWT) uses wavelet basis functions to analyze variations in different frequency bands. The one-level decomposition of an image using DWT is given by:

\begin{equation}
f(x, y) = \sum_{m} \sum_{n} c_{m,n} \phi_{m,n}(x, y) + \sum_{m} \sum_{n} d_{m,n} \psi_{m,n}(x, y)
\label{eq:DWT}
\end{equation}

where \( \phi_{m,n}(x, y) \) represents the approximation coefficients (low-frequency components), and \( \psi_{m,n}(x, y) \) captures the detailed coefficients (high-frequency components) at different scales. The Discrete Cosine Transform (DCT), widely applied in image compression (e.g., JPEG), is particularly effective for analyzing energy distribution in smooth regions and identifying compression-induced anomalies \cite{ahmed1974discrete}. The 2D DCT of an image block \( f(x, y) \) of size \( N \times N \) is computed as:

\begin{equation}
F(u, v) = \alpha(u) \alpha(v) \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} f(x, y) 
\cos \left[ \frac{(2x+1)u\pi}{2N} \right] 
\cos \left[ \frac{(2y+1)v\pi}{2N} \right]
\label{eq:DCT}
\end{equation}
where \( \alpha(u), \alpha(v) \) are normalization factors:

\begin{equation}
\alpha(u) =
\begin{cases}
\frac{1}{\sqrt{N}}, & \text{if } u = 0 \\
\frac{\sqrt{2}}{\sqrt{N}}, & \text{if } u > 0
\end{cases}
\end{equation}

The Inverse DCT (IDCT) reconstructs the image block from its frequency coefficients. Since DCT concentrates most of the signal energy in a few low-frequency components, it is useful for detecting compression artifacts and synthetic image inconsistencies.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{images/Independent_Frequency_Analysis.png}
    \caption{Comparison of frequency domain transformations for real and generated images. The first column presents the input images, followed by their respective DFT, DWT, and DCT representations. The images were obtained from ForenSynth \cite{wang2020cnn}, and the generated image was produced using CycleGAN \cite{zhu2017unpaired}. These images are reproduced under the Attribution-NonCommercial-ShareAlike 4.0 International license.}
    \Description{Diagram illustrating different frequency transformation methods.}
    \label{fig:application_frequency_methods}
\end{figure*}

\subsubsection{Simulation Artifacts and Detection in Frequency Domain by Zhang: 2019}
Zhang et al. \cite{zhang2019detecting} proposed a detection framework leveraging frequency-domain analysis to identify GAN-generated images. Their key contributions include the introduction of AutoGAN, a GAN simulator that mimics artifacts found in diverse GAN architectures, and the use of frequency spectrum inputs instead of spatial features for classification. AutoGAN generates synthetic images by simulating the up-sampling artifacts present in GAN-generated images, independent of specific GAN architectures. The generator applies adversarial loss as shown in Table \ref{eq:adversarial_loss} and an $l_1$-norm loss function to minimize differences between generated and real images. This allows training without requiring access to actual GAN-generated images. To extract meaningful features in the frequency domain, the framework applies a 2D Discrete Fourier Transform (DFT), as given by equation~\eqref{eq:DFT}, to RGB images, generating three-channel frequency representations. These representations undergo logarithmic transformation and normalization within the range  before being used for classification. For classification, a ResNet-34 model, pre-trained on ImageNet \cite{russakovsky2015imagenet}, processes the transformed frequency features.

\subsubsection{Fourier Spectrum Discrepancies: Dzanic 2020}
Dzanic et al. \cite{dzanic2020fourier} presented a method for analyzing high-frequency Fourier models to highlight the limitations of generative models, such as GANs and VAEs, in reconstructing high-frequency components. Their approach involves applying the Fourier Transform to images to obtain a reduced spectrum, which is then modeled using two decay parameters: \(b_1\), representing the magnitude of high-frequency content, and \(b_2\), representing the decay rate. These parameters were used to train a KNN classifier capable of distinguishing synthetic images from real ones, achieving 99.2\% accuracy on uncompressed high-resolution images. The process involves normalizing the Fourier transform by the DC gain, converting the data to normalized polar coordinates, binning and averaging Fourier coefficients to create a reduced spectrum, and fitting the decay parameters above a threshold wavenumber. This comprehensive method underscores the effectiveness of frequency domain features in identifying the discrepancies characteristic of synthetic image generation.

\subsubsection{Liu's Detection Method Derived from Analysis on Real Images: 2022}
Liu et al. \cite{liu2022detecting} proposed a novel approach to detecting synthetic images by focusing on the inherent noise patterns of real images, deviating from existing methods that analyze artifacts in generated images. They introduced the concept of Learned Noise Patterns (LNP), a high-dimensional spatial mapping derived from neural networks, to characterize the noise properties of real images. By comparing these learned patterns with the noise present in synthetic images, the method identifies discrepancies that indicate image generation. Leveraging both spatial and frequency domain representations, this approach demonstrated improved accuracy in detecting synthetic images across multiple domains.

\subsubsection{Two-Stream Convolutional Network for Fake Content Detection by Yousaf: 2022}
Yousaf et al. \cite{yousaf2022fake} proposed TwoStreamNet, a two-stream convolutional neural network designed to enhance the generalizability of fake visual content detection by jointly analyzing spatial and frequency features. The network comprises two main modules: the Spatial Stream and the Frequency Stream, which independently process input images and fuse their outputs at the classification stage to improve detection accuracy.

The Frequency Stream captures frequency domain artifacts by first converting images to the YCbCr color space to decorrelate color channels. Discrete Fourier Transform (DFT) and Discrete Wavelet Transform (DWT) (see equation~\eqref{eq:DWT}) are applied to each channel. DFT decomposes image signals into real and imaginary components, while DWT captures both low- and high-frequency sub-bands. The resulting frequency features, represented as \(H \times W \times 18\) feature maps, are processed using ResNet-50 \cite{he2016deep} to extract discriminative frequency patterns. The Spatial Stream focuses on spatial domain features by processing original RGB images augmented with Gaussian blur and JPEG compression, similar to the approach in \cite{wang2020cnn}. These augmented images are passed through a ResNet-50 network to extract spatial features. Finally, the outputs of the two streams are fused via probability averaging, ensuring equal contributions from spatial and frequency domains to the final decision. This combined framework highlights the importance of integrating frequency features for robust detection of synthesized visual content.

\subsubsection{Synthbuster by Bammey: 2023}
Bammey \cite{Synth10334046} introduced Synthbuster, a forensic method for detecting synthetic images generated by state-of-the-art diffusion models. The method begins by applying a cross-difference filter, originally defined by Chen et al. \cite{chen2008image}, to highlight periodic frequency artifacts in synthetic images. The filter acts as a high-pass filter, generating residual images using the operation:

\[
C(x, y) = |I(x, y) + I(x+1, y+1) - I(x+1, y) - I(x, y+1)|,
\]

where \( I(x, y) \) represents pixel intensity. 

The Fast Fourier Transform (FFT) is then applied to the residual image, extracting spectral components corresponding to periodicities \(0, 2, 4,\) and \(8\) in both vertical and horizontal directions. The resulting features form a 135-dimensional magnitude peak vector for the RGB channels. These features are classified using a histogram-based gradient boosting tree classifier (HBGB) \cite{ke2017lightgbm}, trained specifically to distinguish synthetic images from real ones.

Synthbuster adapts techniques traditionally used for detecting JPEG compression artifacts to address artifacts from diffusion models, highlighting potential overlaps between the two. The method was validated on JPEG-compressed images to mitigate misclassification risks. Despite its simplicity, Synthbuster outperformed state-of-the-art methods for diffusion image detection. Additionally, the authors have publicly released their dataset, as detailed in the datasets section.


\subsubsection{Meng's Artifact Feature Purification: 2024}
Meng et al. \cite{meng2024artifact} addressed two key limitations in existing detection methods: poor generalization across generative models and limited effectiveness on images from diverse large-scale datasets. To overcome these challenges, they proposed the Artifact Purification Network (APN), which extracts generalizable artifact features through explicit and implicit purification processes.
Explicit purification isolates artifact features in spatial and frequency domains by employing feature decomposition and frequency-band proposals to detect suspicious patterns. Implicit purification, guided by a classifier, further refines these features using mutual information estimation, enhancing the robustness of detection across various generators and datasets. This dual purification approach significantly improves generalization and detection accuracy.


\subsubsection{An Image Transformation Perspective by Li: 2024}
Li et al. \cite{li2024improving} identified biases in existing detection methods, including weakened and overfitted artifact features, which limit their generalization capability. To address these challenges, the authors proposed the SAFE (Simple Preserved and Augmented Features) framework, designed to improve generalizable artifact detection by integrating effective transformations with preprocessing and augmentations. The framework employs several key strategies: (1) replacing conventional down-sampling with a crop operator (RandomCrop for training and CenterCrop for inference) to preserve local correlations and prevent artifact distortion; (2) applying invariant augmentations, such as ColorJitter and RandomRotation, to mitigate color discrepancies and irrelevant rotation-related features; (3) using a patch-based random masking strategy to enhance sensitivity to local image regions and subtle artifacts; and (4) incorporating Discrete Wavelet Transform (DWT) to extract high-frequency features critical for distinguishing synthetic from real images. The processed data was used to train a ResNet model \cite{he2016deep} with 1.44M parameters, achieving strong detection accuracy during testing. This approach demonstrated the effectiveness of integrating diverse transformations to improve detection robustness and generalization.

\subsubsection{Yan's AIDE Method for Synthetic Image Detection (2024)}
Yan et al. \cite{yan2024sanity} introduced the Chameleon dataset, a collection of AI-generated images designed to closely resemble real-world scenes, highlighting the limitations of existing detectors, which frequently misclassify these images as real. To address these challenges, the authors proposed AIDE (AI-generated Image Detector with Hybrid Features), a method that combines low-level frequency-based features with high-level semantic embeddings from CLIP. AIDE processes input images by dividing them into patches and applying discrete cosine transform (DCT) (see equation~\eqref{eq:DCT}) to extract frequency-domain features. Patches are sorted based on computed scores, with the highest- and lowest-frequency patches selected, resized, and processed through SRM \cite{fridrich2012richSRM} to capture noise patterns. These frequency-based features are embedded using a ResNet-50 network, while high-level semantic features are extracted via ConvNeXt-based OpenCLIP. The outputs from both networks are concatenated and passed through a multi-layer perceptron (MLP) for final classification. This hybrid approach demonstrated better performance across benchmarks, including improvements on \textit{AIGCDetectBenchmark} and GenImage \cite{zhu2024genimage}, while achieving competitive results on the challenging Chameleon dataset.



\begin{table*}
  \caption{Evaluation of Detection Models from Frequency Domain Analysis Category on Cross-Family Generators, Cross-Category, and Cross-Scene Generalization}
  \label{tab:cross_evaluation_frequency}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Models} & \textbf{Cross-Family Generators} & \textbf{Cross-Category} & \textbf{Cross-Scene} \\
    \midrule
    Zhang et al. \cite{zhang2019detecting}  & \xmark & \cmark & \xmark \\
    \midrule
    Dzanic et al. \cite{dzanic2020fourier} & \xmark & \xmark & \xmark \\
    \midrule
    Liu et al. \cite{liu2022detecting} & \cmark & \cmark & \cmark \\
    \midrule
    Yousaf et al. \cite{yousaf2022fake} & \cmark & \cmark & \xmark \\
    \midrule
    Bammey \cite{Synth10334046} & \cmark & \cmark & \xmark \\
    \midrule
    Meng et al. \cite{meng2024artifact} & \cmark & \cmark & \cmark \\
    \midrule
    Li et al. \cite{li2024improving} & \cmark & \cmark & \cmark \\
    \midrule
    Yan et al. \cite{yan2024sanity} & \cmark & \cmark & \cmark \\
    \bottomrule
  \end{tabular}
\end{table*}


\subsection{Fingerprint Analysis Methods}
Fingerprint analysis in digital forensics refers to techniques that detect unique, traceable features left behind by devices or processes during image generation. Traditional approaches focused on detecting handcrafted features, including device fingerprints and postprocessing fingerprints. Device fingerprints, such as the photo-response nonuniformity (PRNU) pattern \cite{lukas2006digital}, arise from manufacturing imperfections in imaging sensors, leaving a unique and stable mark on each captured image. Postprocessing fingerprints, on the other hand, originate from in-camera processing pipelines, including operations like demosaicking and compression, which embed specific patterns into images \cite{cozzolino2019noiseprint}. While fingerprint analysis often overlaps with frequency domain methods, its primary focus is on extracting distinct fingerprints specific to generative models, such as GANs and diffusion models, rather than general spectral artifacts. The following methods are categorized under fingerprint analysis, as they aim to identify the unique traces of generative techniques.

\subsubsection{GAN Fingerprint Analysis by Marra et al. (2019)}
Marra et al. \cite{marra2019gans}, building on the concept of PRNU \cite{lukas2006digital}, introduced the idea that GANs, like digital cameras, imprint unique and stable fingerprints on their outputs, enabling source attribution and forensic analysis. The study focused on extracting GAN fingerprints using two models: CycleGAN \cite{zhu2017unpaired} and ProGAN \cite{karras2017progressive}. To estimate the fingerprint \(F\), the residual signal \(R_i\) between a GAN-generated image \(X_i\) and its denoised version \(f(X_i)\) is computed:
\[
R_i = X_i - f(X_i),
\]
where \(f(\cdot)\) is a denoising filter. The fingerprint is then estimated as the average residual over \(N\) samples:
\[
\hat{F} = \frac{1}{N} \sum_{i=1}^{N} R_i.
\]
As \(N\) increases, random noise diminishes, yielding a stable fingerprint.

To attribute an image to its GAN source, the correlation index between the image's residual and the computed GAN fingerprint is evaluated. Experiments demonstrated that GAN fingerprints are distinct, with histogram analysis showing clear separations between fingerprints of different GANs. Additionally, training the same GAN architecture on different datasets produces unique fingerprints, highlighting the method's generalizability for source attribution. This foundational work established GAN fingerprints as a robust tool for source attribution but noted the challenge of detecting cross-generator images without explicit training on all potential generator outputs.

\subsubsection{GAN Fingerprint Attribution by Yu et al. (2019)}
Yu et al. \cite{yu2019attributing} proposed a framework for attributing GAN-generated images to their source by learning GAN fingerprints. This approach extends the work of Marra et al. \cite{marra2019gans} by systematically separating image fingerprints and model fingerprints instead of relying on hand-crafted features.

The image fingerprint \(F_I\) is extracted using a reconstruction autoencoder, defined as the residual signal:
\[
F_I = I - R(I),
\]
where \(R(I)\) is the reconstructed image generated by the autoencoder. The autoencoder is trained with a combination of pixel-wise \(L_1\) and adversarial losses to ensure accurate reconstruction and artifact retention. The model fingerprint \(F_y\) for each GAN \(y \in Y\) is learned as a latent feature vector during training, encapsulating the architectural, dataset, and hyperparameter-specific traits unique to the GAN. Attribution is performed by computing the correlation between the image fingerprint \(F_I\) and model fingerprints \(F_y\) and maximizing this correlation over all potential sources. The process is supervised using a cross-entropy loss that encourages accurate attribution.
This framework demonstrated improved accuracy in attributing images to their source GAN by effectively capturing both image-specific and model-specific traits, advancing the generalizability and robustness of GAN attribution methods.

\subsubsection{Disentangling GAN Fingerprints by Yang et al. (2021)}
Yang et al. \cite{yang2021learning} proposed the GAN Fingerprint Disentangling Network (GFD-Net), advancing previous methods by focusing on disentangling content-irrelevant and GAN-specific fingerprints in synthetic images. Unlike earlier approaches that employ classification setups, GFD-Net integrates a generator, a discriminator, and an auxiliary classifier within an extended GAN framework to explicitly learn and separate these fingerprints. The generator adopts a U-Net architecture with an encoder-decoder structure. A classifier is added to the encoder's output to predict the GAN source, enhancing the generator's feature learning capability. The generator outputs a fingerprint \( f \), which is added to a real image \( x_{\text{real}} \) to produce a fingerprinted synthetic image \( x_{\text{fp}} \):
\[
x_{\text{fp}} = x_{\text{real}} + f.
\]

The discriminator utilizes a PatchGAN architecture to classify images by evaluating smaller patches and averaging their authenticity scores, encouraging the generator to focus on GAN-specific features. Consequently, a ResNet-50 classifier is employed to differentiate fingerprints from different GANs. The training process alternates between:
1. Training the generator (\(G\)) with fixed discriminator (\(D\)) and classifier (\(C\)).
2. Training the discriminator (\(D\)) and classifier (\(C\)) with a fixed generator (\(G\)). The generator is trained using a combination of loss functions:
\[
L_G = \omega_1 L_{z_G} + \omega_2 L_{\text{adv}_G} + \omega_3 L_{\text{cls}_G} + \omega_4 L_{\text{percept}_G},
\]
where \(L_{z_G}\) ensures the generator correctly predicts the image source, \(L_{\text{adv}_G}\) enforces realistic fingerprinted images, \(L_{\text{cls}_G}\) ensures the fingerprint represents the GAN source, and \(L_{\text{percept}_G}\) minimizes perceptual differences between \(x_{\text{fp}}\) and \(x_{\text{real}}\). The discriminator and classifier are trained with two loss functions:
\[
L_{\text{adv}_D} = \mathbb{E}[\log(1 - D(x_{\text{fp}}))] + \mathbb{E}[\log(D(x_{\text{real}}))],
\]
\[
L_{\text{cls}_C} = \text{L}_{\text{CE}}(C(x_{\text{real}}), y) + \text{L}_{\text{CE}}(C(x_{\text{fp}}), y),
\]
where \(L_{\text{adv}_D}\) ensures the discriminator distinguishes real and fingerprinted images, and \(L_{\text{cls}_C}\) trains the classifier for accurate source attribution. By explicitly disentangling GAN-specific fingerprints from image content, GFD-Net demonstrated improved robustness and generalizability compared to earlier methods. Its design builds on prior GAN fingerprinting approaches by incorporating both architectural and training innovations, addressing limitations in source attribution across diverse GANs.


\subsubsection{FingerprintNet: Synthesized Fingerprints for Generalized GAN Detection by Jeong: 2022}
Jeong et al. \cite{jeong2022fingerprintnet} proposed FingerprintNet, a framework aimed at generalizing GAN-generated image detection by synthesizing diverse GAN fingerprints. This approach addresses the challenge of detecting images from unseen GAN architectures without relying on GAN-specific training datasets. FingerprintNet employs an autoencoder-based fingerprint generator, incorporating random layer selection, multi-kernel deconvolution, and feature blending modules to create diverse and robust fingerprints. The generator is trained using a combination of reconstruction loss and similarity loss, ensuring that synthesized fingerprints accurately represent the characteristics of GAN-generated images.

For detection, FingerprintNet applies a Fast Fourier Transform (FFT) to the generated images to extract 2D spectra, where GAN fingerprints are more evident. A ResNet-50 \cite{he2016deep} classifier is then used to distinguish real from generated images based on these fingerprint-highlighted spectra. To address dataset imbalance, the generator creates three fake images for each real image, and a mixed-batch strategy is applied during training to maintain balanced mini-batches. By synthesizing fingerprints and avoiding reliance on specific GAN datasets, FingerprintNet demonstrates improved generalization for detecting images from unseen GAN architectures, advancing robustness in GAN detection tasks.

\subsubsection{Learning on Gradients by Tan: 2023}
Tan et al. \cite{tan2023learning} proposed a novel detection framework, Learning on Gradients (LGrad), which leverages gradient-based representations as generalized artifacts. The framework begins by employing a transformative model, \(T\), a pretrained CNN, to convert real and generated images into feature vectors. The gradient of \( \text{sum}(l) \) with respect to the input image is then computed to capture generalized artifacts:
\[
G = \frac{\partial \text{sum}(l)}{\partial I_i},
\]
where \( l \) represents the feature vector output by \(T\) for an input image \( I_i \).

Following this, a classification model, ResNet-50 \cite{he2016deep}, pretrained on the ImageNet dataset \cite{russakovsky2015imagenet}, is trained on the computed gradients to learn underlying artifacts. Notably, \(T\) remains fixed during gradient computation and is reused during inference to first extract gradients, which are then classified by the trained model. The authors highlight that training the classifier on computed gradients enables generalized learning of artifacts common across GAN models. The paper also compares the performance of various transformative models, including pretrained classifiers and GAN discriminators, showcasing their impact on detection performance. Readers are encouraged to refer to the main paper for a detailed analysis.

\subsubsection{Data Augmentation in Fingerprint Domain By Wang: 2023 (Scaling \& Mixup)}
Wang et al. \cite{wang2023fingerprintAug} proposed a framework to enhance the generalizability of GAN-generated image detectors by augmenting synthetic data in the fingerprint domain. The method involves two key contributions: (1) extraction of fingerprints from synthetic images across different scenes using an encoder trained with Mean Squared Error (MSE) and adversarial losses, and (2) improved cross-GAN generalization through fingerprint perturbation. The framework utilizes an autoencoder trained on real images to extract residual fingerprints from GAN-generated images. These fingerprints, which represent GAN-specific artifacts, are made generalizable across scenes by incorporating a Gradient Reversal Layer (GRL) alongside MSE loss.

To address architecture dependency and simulate fingerprints of unseen GANs, two augmentation strategies were introduced: scaling, which applies a random scaling factor \(\alpha\) to modify fingerprint intensity, and Mixup, which combines fingerprints from multiple samples using weighted sums to generate diverse synthetic fingerprints. After augmentation, perturbed fingerprints are added back to the reconstructed images to create augmented fake images. These augmented samples are used to train a binary classifier with cross-entropy loss, improving detection accuracy across unseen GAN architectures. This approach demonstrates the effectiveness of fingerprint augmentation in enhancing the generalization of GAN detectors, addressing the variability of GAN fingerprints across architectures and scenes.

\subsubsection{Corvi's Analysis on Trending Detection Methods to Detect Generated Images by Diffusion Models: 2023}
Corvi et al. \cite{corvi2023detection} examined the effectiveness of current forensic detection methods in identifying synthetic images generated by diffusion models. Building on the methodology of Marra et al. \cite{marra2019gans}, they employed a denoising filter \cite{zhang2017beyond} to isolate noise residuals by subtracting the scene content from images. Averaging the residuals across multiple images produced a synthetic fingerprint, capturing the artifacts specific to the generation process. Additionally, Corvi et al. conducted spectral analysis by applying the Fourier transform to averaged residuals from 1,000 images. Their findings revealed distinct spectral patterns for models like Stable Diffusion and Latent Diffusion, whereas weaker artifacts were observed in ADM and DALL·E 2, posing challenges for detection. 
The study highlighted the limitations of current detectors, particularly in handling resized and compressed images, and emphasized the need for robust methods tailored to diffusion models. Their findings contribute to understanding the unique challenges of detecting diffusion-generated images, with datasets and code available for further research.

\subsubsection{MaskSim (Li, 2024)}
Li et al. \cite{li2024masksim} proposed MaskSim, a forensic framework for detecting synthetic images generated by diffusion models by focusing on extraction of artifacts. The method leverages traceable artifacts in the Fourier Transformed Spectrum, selectively amplifies these artifacts, and uses a simple linear classifier to achieve competitive detection performance. The framework begins by preprocessing input images with a DnCNN denoiser \cite{zhang2017beyond} to suppress textures and enhance residual synthesis artifacts building on approaches introduced in \cite{marra2019gans} and later adapted in \cite{corvi2023detection} for diffusion-generated images. The residual image undergoes DFT to compute the logarithmic magnitude spectrum, which is then refined using a trainable \(1 \times 1\) convolutional layer and an element-wise masking procedure. A trainable mask is applied to the spectrum, followed by Batch Normalization, and a normalized reference spectrum is computed for comparison.
Detection is performed by computing the cosine similarity between the masked spectrum and the reference spectrum. For synthetic images, cosine similarity values are maximized, while for real images, the absolute cosine values are minimized. A logistic regression classifier, trained using cross-entropy loss, predicts the probability of an image being synthetic. During testing, only regular cosine similarity is used, ensuring robustness and avoiding overfitting. The framework was validated on a dataset of diffusion-generated images \cite{Synth10334046}, achieving strong detection performance and demonstrating its effectiveness in leveraging frequency artifacts for synthetic image detection.

%This paper overlaps with \cite{jeong2022fingerprintnet} 
\begin{comment}
\subsubsection{Jeong's Self-supervised Methodology in GAN generated image detection: 2024}
\textbf{Both: Dominant Fingerprint as it uses GAN training to extract artifact}
Jeong et al. \cite{JEONG2024219} presented a GAN detection framework consisting of two modules: an artificial artifact generator and a GAN detector. The detection process begins with training a generator based on an autoencoder architecture to reconstruct input images. The objective was to extract frequency-level artifacts. Since variations in upsampling layers influence the appearance of these artifacts, the authors implemented three autoencoders with different configurations of deconvolution and upsampling layers. Readers are referred to the main paper for detailed architecture specifications.

Following this, the GAN detector, built on ResNet-50 \cite{he2016deep}, was trained using a mini-batch setup to distinguish reconstructed images from real ones. Prior to classification, input images were converted into 2D spectra using the Fast Fourier Transform (FFT) for frequency-level analysis. A notable aspect of this framework is its self-supervised nature, as the detector is trained exclusively on images reconstructed by the generator, eliminating the need for additional GAN-generated images. 
\end{comment}

\begin{table*}
  \caption{Fingerprint Analysis Methods on Cross-Family Generators, Cross-Category, and Cross-Scene Generalization}
  \label{tab:fingerprint_analysis}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Models} & \textbf{Cross-Family Generators} & \textbf{Cross-Category} & \textbf{Cross-Scene} \\
    \midrule
    Marra et al. \cite{marra2019gans} & \xmark & \xmark & \xmark \\
    \midrule
    Yu et al. \cite{yu2019attributing} & \xmark & \xmark & \xmark \\
    \midrule
    Yang et al. \cite{yang2021learning} & \xmark & \cmark & \cmark \\
    \midrule
    Jeong et al. \cite{jeong2022fingerprintnet} & \cmark & \cmark & \xmark \\
    \midrule
    Corvi et al. \cite{corvi2023detection} & \cmark & \xmark & \xmark \\ 
    \midrule
    Tan et al. \cite{tan2023learning} & \xmark & \cmark & \cmark \\ 
    \midrule
    Wang et al. \cite{wang2023fingerprintAug} & \xmark & \cmark & \xmark \\
    \midrule
    Li et al. \cite{li2024masksim} & \xmark & \cmark & \cmark \\
    %\midrule
    % Jeong et al. \cite{JEONG2024219} & \xmark & \cmark & \xmark \\ Note: This overlaps with \cite{jeong2022fingerprintnet}
    \bottomrule
  \end{tabular}
\end{table*}



\begin{comment}
\begin{table}[h!]
\centering
\caption{Evaluations on the images from Forensynths \cite{wang2020cnn} dataset. The training was done in one kind of GAN generated images and testing on different GAN-generated images}
\setlength{\tabcolsep}{4pt} % Reduce the gap between columns
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccccccc|c}
\toprule
Method & \multicolumn{2}{c}{StyleGAN} & \multicolumn{2}{c}{StyleGAN2} & \multicolumn{2}{c}{BigGAN} & \multicolumn{2}{c}{CycleGAN} & \multicolumn{2}{c}{StarGAN} & \multicolumn{2}{c}{GauGAN} & \multicolumn{2}{c}{Mean} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
 & Acc & AP & Acc & AP & Acc & AP & Acc & AP & Acc & AP & Acc & AP & Acc & AP \\
\midrule
Jeong \cite{jeong2022fingerprintnet} & 74.1 & 85.3 & 89.5 & 96.1 & 85.0 & 94.8 & 71.2 & 96.9 & 99.9 & 100.0 & 75.9 & 90.9 & 82.6 & 94.0 \\
Wang et al. (Scaling) \cite{wang2023fingerprintAug}  & 85.7 & 98.6 & 83.8 & 98.2 & 81.2 & 85.3 & 83.3 & 93.9 & 99.1 & 100.0 & 75.1 & 81.3 & 84.7 & 92.9 \\
Wang et al. (Mixup) \cite{wang2023fingerprintAug} & 82.2 & 98.7 & 78.0 & 98.1 & 79.1 & 84.8 & 86.4 & 95.6 & 98.8 & 100.0 & 83.4 & 90.3 & 84.7 & 94.6 \\
\bottomrule
\bottomrule
\end{tabular}%
}
\label{tab:cross_gan_evaluations}
\end{table}


\begin{table}[h!]
\centering
\caption{AUC and ACC Evaluations Across Different Diffusion Generated Images as Mentioned in \cite{li2024masksim}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccccccc}
\toprule
Method & \multicolumn{2}{c}{SD-1} & \multicolumn{2}{c}{SD-2} & \multicolumn{2}{c}{SD-XL} & \multicolumn{2}{c}{DALL·E 2} & \multicolumn{2}{c}{DALL·E 3} & \multicolumn{2}{c}{Midjourney} & \multicolumn{2}{c}{Firefly} & \multicolumn{2}{c}{Mean} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
 & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC \\
\midrule
Corvi et al. \cite{corvi2023detection} & 100.0 & 99.6 & 99.5 & 97.2 & 98.9 & 80.4 & 48.8 & 49.9 & 54.9 & 49.7 & 99.8 & 95.0 & 86.2 & 52.4 & 84.0 & 74.9 \\
Li et al. \cite{li2024masksim} & 89.4 & 75.5 & 99.1 & 95.9 & 96.6 & 90.0 & 68.2 & 55.4 & 90.2 & 75.3 & 96.4 & 90.9 & 76.0 & 64.0 & 88.3 & 79.4 \\
\bottomrule
\end{tabular}%
}
\label{tab:auc_acc_evaluations}
\end{table}
\end{comment}

\begin{table}[h!]
\centering
\caption{Evaluation of Fingerprint Analysis Methods on GAN and Diffusion-Generated Images.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccccccc}
\toprule
\multicolumn{17}{c}{\textbf{GAN-Based Evaluations using Forensynths dataset \cite{wang2020cnn})}. Accuracy (Acc) and Average Precision (AP) metrics are reported.} \\
\multicolumn{17}{c}{\cite{tan2023learning} was trained on class Bedroom from ProGAN-generated image, while remaining were trained on class Horse.} \\
\midrule
Method & \multicolumn{2}{c}{StyleGAN} & \multicolumn{2}{c}{StyleGAN2} & \multicolumn{2}{c}{BigGAN} & \multicolumn{2}{c}{CycleGAN} & \multicolumn{2}{c}{StarGAN} & \multicolumn{2}{c}{GauGAN} & \multicolumn{2}{c}{Mean} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15}
 & Acc & AP & Acc & AP & Acc & AP & Acc & AP & Acc & AP & Acc & AP & Acc & AP \\
\midrule
Jeong \cite{jeong2022fingerprintnet} & 74.1 & 85.3 & 89.5 & 96.1 & 85.0 & 94.8 & 71.2 & 96.9 & 99.9 & 100.0 & 75.9 & 90.9 & 82.6 & 94.0 \\
Tan et al. \cite{tan2023learning} & 82.60 & 95.60 & 83.30 & 98.40 & 76.20 & 81.80 & 82.30 & 90.60 & 99.70 & 100.0 & 71.70 & 75.00 & 80.90 & 87.40 \\
Wang et al. (Scaling) \cite{wang2023fingerprintAug} & 85.7 & 98.6 & 83.8 & 98.2 & 81.2 & 85.3 & 83.3 & 93.9 & 99.1 & 100.0 & 75.1 & 81.3 & 84.7 & 92.9 \\
Wang et al. (Mixup) \cite{wang2023fingerprintAug} & 82.2 & 98.7 & 78.0 & 98.1 & 79.1 & 84.8 & 86.4 & 95.6 & 98.8 & 100.0 & 83.4 & 90.3 & 84.7 & 94.6 \\
\midrule
\multicolumn{17}{c}{\textbf{Diffusion-Based Evaluations on the dataset as mentioned in (\cite{li2024masksim})}. Area Under the Curve (AUC) and Accuracy (Acc) metrics are reported.} \\
\midrule
Method & \multicolumn{2}{c}{SD-1} & \multicolumn{2}{c}{SD-2} & \multicolumn{2}{c}{SD-XL} & \multicolumn{2}{c}{DALL·E 2} & \multicolumn{2}{c}{DALL·E 3} & \multicolumn{2}{c}{Midjourney} & \multicolumn{2}{c}{Firefly} & \multicolumn{2}{c}{Mean} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
 & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC & AUC & ACC \\
\midrule
Corvi et al. \cite{corvi2023detection} & 100.0 & 99.6 & 99.5 & 97.2 & 98.9 & 80.4 & 48.8 & 49.9 & 54.9 & 49.7 & 99.8 & 95.0 & 86.2 & 52.4 & 84.0 & 74.9 \\
Li et al. \cite{li2024masksim} & 89.4 & 75.5 & 99.1 & 95.9 & 96.6 & 90.0 & 68.2 & 55.4 & 90.2 & 75.3 & 96.4 & 90.9 & 76.0 & 64.0 & 88.3 & 79.4 \\
\bottomrule
\end{tabular}%
}
\label{tab:combined_evaluations_fingerprint}
\end{table}



\subsection{Patch-Based Analysis Methods}
Patch-Based Analysis methods focus on identifying synthetic images by analyzing localized patches rather than the entire image. These methods exploit inconsistencies or artifacts that may appear within smaller regions, enabling the detection of subtle generative patterns. By dividing an image into patches and examining features such as texture, edge coherence, or pixel-level anomalies, these approaches enhance detection granularity. Patch-based strategies are particularly effective in scenarios where global analysis may miss fine-grained artifacts introduced by generative models.


\subsubsection{Patch-Based Classification by Chai et al.: 2020}
Chai et al. \cite{chai2020makes} proposed a patch-based classification framework using truncated ResNet and Xception backbones to classify localized patches of an image as real or fake. By limiting the model's receptive field, the framework focuses on local artifacts rather than global image structure, enhancing generalization across datasets and generative model types. Each patch is processed independently using a 1×1 convolution layer appended to the truncated backbone, and cross-entropy loss is applied. The final output is obtained by averaging the softmax predictions across patches. This increases the data-to-parameter ratio, improving generalization to unseen data. The framework includes a visualization mechanism that generates heatmaps, highlighting regions contributing to the classifier’s predictions. These heatmaps reveal that complex textures, such as hair and facial boundaries, are key to distinguishing real from generated images.

\subsubsection{Orthogonal Training in Detecting GAN-Generated Images: 2022}
Mandelli et al. \cite{mandelli2022detecting} proposed a compact detection framework designed to generalize to unseen GAN architectures. This method employs orthogonal training, where multiple CNNs with EfficientNet-B4 \cite{tan2019efficientnet} backbones are trained on datasets differing in semantic content, GAN models, and post-processing operations. The framework divides input images into 128×128 RGB patches, which each CNN analyzes independently. These patch-level scores are aggregated to classify the entire image. Synthetic images receive the highest patch scores, while real images receive the lowest. The final decision is derived from the average of all CNNs' image-level scores. Experimental results confirmed the method's strong performance in detecting StyleGAN3-generated images without prior exposure during training

\subsubsection{Fusing Global and Local Information by Ju: 2022}
Ju et al. \cite{ju2022fusing} proposed a detection method that combines global and local features to improve generalization for synthetic image detection. The framework uses a ResNet-50 backbone to extract global feature maps from input images. These global features are complemented by a Patch Selection Module (PSM), which identifies and processes the most informative patches to capture subtle, localized artifacts. The PSM selects patches by sliding windows of sizes $3 \times 3$ and $2 \times 2$ over the activation maps, scoring each patch based on its aggregated activation. The top patches (\( k = 6 \)) are mapped back to the original image and reprocessed through ResNet-50 for local feature extraction. Subsequently, an Attention-based Feature Fusion Module (AFFM) combines the global and local features through multi-head attention, generating a unified representation for classification. The authors evaluated their method using a dataset synthesized by 19 different models, including GANs and autoencoder-based DeepFakes. Experimental results showed improved performance over prior methods, particularly under diverse post-processing conditions such as Gaussian blur and JPEG compression.

\subsubsection{Zhong's PatchCraft: 2024}
Zhong et al. \cite{zhong2024patchcraft} introduced PatchCraft, a novel framework emphasizing texture patches over global semantic features for detecting synthetic images. The key innovation is a preprocessing technique, Smash and Reconstruction, which segments an image into patches, ranks them by texture diversity—measured as the sum of pixel differences in horizontal, vertical, and diagonal directions—and reconstructs two images: one enriched with rich-texture patches and another with poor-texture patches. Both reconstructed images are processed through Spatial Rich Model (SRM) filters \cite{fridrich2012richSRM} to extract high-frequency noise patterns, which are further analyzed using a learnable convolutional block. The residual noise patterns between rich and poor texture regions capture inter-pixel correlations, forming a fingerprint for generative models. This fingerprint is leveraged by a convolutional neural network (CNN) classifier, trained with cross-entropy loss, to distinguish between real and synthetic images. During inference, the same pipeline extracts the fingerprint, enabling an accurate classification of input images across diverse generative models.

\subsubsection{Chen's Single Patch Method for AI-Generated Image Detection (2024)}
Chen et al. \cite{chen2024single} proposed the Single Simple Patch (SSP) network, designed to identify whether an image is real or AI-generated by analyzing a single low-texture patch. The selected patch, with dimensions \( M \times M \), is determined by having the lowest texture diversity among all image patches, calculated using the method in \cite{zhong2024patchcraft}. The selected patch is resized to match the original image dimensions and processed using the SRM \cite{fridrich2012richSRM} to extract high-frequency noise patterns. These noise patterns are then fed into a ResNet-50 \cite{he2016deep} classifier, which is trained with binary cross-entropy loss to distinguish real images from generated ones.
To address performance challenges with low-quality images (e.g., those affected by blur or compression artifacts), Chen et al. introduced two additional modules: an enhancement module and a perception module. The perception module, a lightweight three-class classifier, identifies whether a patch is blurry, compressed, or intact. Its predictions guide the enhancement module, which uses a U-Net architecture to improve the patch quality by performing deblurring, decompression, or reconstruction. The improved patch is then processed by the SSP network. 

\subsubsection{Inter-Patch Dependencies for AI-Generated Image Detection by Chen: 2024 (IPD-Net)}
Chen et al. \cite{chen2024ipd} introduced IPD-Net, a detection framework that leverages inter-patch dependencies to improve generalization for AI-generated image detection. This work builds on previous research by Zhong et al. \cite{zhong2024patchcraft}, which demonstrated that inconsistencies in interpixel relations between rich and poor texture regions can serve as key features for detection. IPD-Net extends this by modeling dependencies between patches using a self-attention mechanism. The framework consists of two main modules: Inter-Patch Dependencies Extraction and Inter-Patch Dependencies Classification. In the extraction module, input images are preprocessed with operations such as Gaussian blur and JPEG compression before being passed through SRM filters \cite{fridrich2012richSRM} to extract noise patterns. A non-trained ResNet-50 backbone generates feature maps, and patch dependencies are computed using dot-product similarity across all patches. In the classification module, the dependency matrix undergoes two-dimensional average pooling to reduce dimensionality. A linear classification layer with sigmoid activation then predicts whether an image is real or synthetic. The model is trained using binary cross-entropy loss, and the architecture supports end-to-end training.
Experimental evaluations on the Forensynths \cite{wang2020cnn} and GenImage \cite{zhu2024genimage} datasets demonstrated that IPD-Net outperforms state-of-the-art baseline models in both in-dataset and cross-dataset evaluations, showcasing strong generalization capabilities.

\begin{table*}
  \caption{Evaluation of Patch-Based Analysis Methods on Cross-Family Generators, Cross-Category, and Cross-Scene Generalization}
  \label{tab:patch_based_analysis}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Models} & \textbf{Cross-Family Generators} & \textbf{Cross-Category} & \textbf{Cross-Scene} \\
    \midrule
    Chai et al. \cite{chai2020makes} & \xmark & \xmark & \xmark \\
    \midrule
    Mandelli et al. \cite{mandelli2022detecting} & \xmark & \cmark & \xmark \\
    \midrule
    Ju et al. \cite{ju2022fusing} & \cmark & \cmark & \xmark \\
    \midrule
    Zhong et al. \cite{zhong2024patchcraft} & \cmark & \cmark & \xmark \\
    \midrule
    Chen et al. \cite{chen2024single} & \cmark & \cmark & \xmark \\
    \midrule
    Chen et al. \cite{chen2024ipd} & \cmark & \cmark & \xmark \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table}[h!]
\centering
\caption{Evaluation of Patch-Based Analysis Methods on GAN and Diffusion-Generated Images.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccccccccccccccc}
\toprule
\multicolumn{17}{c}{\textbf{GAN-Based Evaluations using Forensynths dataset \cite{wang2020cnn}}. Accuracy (Acc) is reported.} \\
\midrule
Method & \multicolumn{1}{c}{ProGAN} & \multicolumn{1}{c}{StyleGAN} & \multicolumn{1}{c}{StyleGAN2} & \multicolumn{1}{c}{BigGAN} & \multicolumn{1}{c}{CycleGAN} & \multicolumn{1}{c}{StarGAN} & \multicolumn{1}{c}{GauGAN} \\
\midrule
Chai et al. \cite{chai2020makes} & 75.03 & 79.16& \_ & \_ & \_ & \_ & \_ \\
Ju et al. \cite{ju2022fusing} & 100.0 & 85.20 & 83.30 & 77.40 & 87.00 & 97.00 & 77.00 \\
Zhong et al. \cite{zhong2024patchcraft}& 100.0 & 92.77 & 89.55 & 95.8& 70.17 & 99.97 & 71.58 \\
Chen et al. \cite{chen2024single} & 97.05 & 96.05 & \_ & 68.65 & 83.25 & 95.00 & 57.85 \\
 Chen et al. \cite{chen2024ipd} & 99.98 & 95.19 & \_ & 81.02 & 86.57 & 99.08 & 68.67 \\
\midrule
\multicolumn{17}{c}{\textbf{Diffusion-Based Evaluations on the GenImage dataset \cite{zhu2024genimage}}. Accuracy (Acc) metrics are reported.} \\
\multicolumn{17}{c}{Chen et al. \cite{chen2024single} and Chai et al. \cite{chai2020makes} were trained on SD V1.4, while all other methods were trained on ProGAN from the ForenSynths dataset.}\\
\multicolumn{17}{c}{The performance metrics for Chai et al. were recorded from the work of Meng et al. \cite{meng2024artifact}} \\

\midrule
Method & \multicolumn{1}{c}{SDv1.4} & \multicolumn{1}{c}{SD-1.5} & \multicolumn{1}{c}{ADM} & \multicolumn{1}{c}{Glide} & \multicolumn{1}{c}{Midjourney} & \multicolumn{1}{c}{VQDM} & \multicolumn{1}{c}{wukong} & \multicolumn{1}{c}{DALLE2} & \multicolumn{1}{c}{SDXL} \\
\midrule
Chai et al. \cite{chai2020makes}& 99.70 & 99.40 & 51.00& 54.10 & 66.20 & 54.10 & 96.70 & \_ & \_ \\
Ju et al. \cite{ju2022fusing}& 51.00 & 51.40 & 49.00 & 57.20 & 52.20 & 55.10 & 51.70 & 52.80 & 55.60 \\
Zhong et al. \cite{zhong2024patchcraft}& 95.38 & 95.30 & 82.17 & 83.79 & 90.12 & 88.91 & 91.07 & 96.60 & 98.43\\
Chen et al. \cite{chen2024single}  & 99.20 & 99.30 & 78.90 & 88.90 & 82.60 & 96.00 & 98.60 & \_ & \_ \\
Chen et al. \cite{chen2024ipd} & 80.03 & 79.70 & 84.38 & 95.05 & 72.19 & 79.37 & 77.19 & \_ & \_ \\
\bottomrule
\end{tabular}%
}
\label{tab:combined_evaluations_patchbased}
\end{table}



\begin{table*}[ht]
\centering
\caption{Comprehensive Evaluation of methods on GAN and diffusion-generated images using the UnivFD dataset \cite{ojha2023towards}. Results are reported in classification accuracy (\%).}
\label{tab:comprehensive-accuracy-detection-results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccccccccccc}
\hline
Method           & \multicolumn{7}{c}{Generative adversarial networks} & \multicolumn{2}{c}{Low-level vision} & \multicolumn{2}{c}{Perceptual loss} & \multicolumn{8}{c}{Diffusion models} & avg. \\ \cline{2-7} \cline{9-10} \cline{11-12}   \cline{13-20}
                           & Pro-    & Cycle-  & Big-    & Style-  & Gau-    & Star-   & Deep-   & SITD   & SAN    & CRN    & IMLE   & Guided  & \multicolumn{3}{c}{LDM}                   & \multicolumn{3}{c}{GLIDE}  & DALL-E \\  
                           & GAN     & GAN     & GAN     & GAN     & GAN     & GAN     & fakes   &        &        &        &        &         & 200    & 200s    & 100    & 100   &  50 & 100       \\  
                           &         &         &         &         &         &         &         &        &        &        &        &         & steps  & w/CFG  &    steps & 27        & 27        & 10 \\  
                           &         &         &         &         &         &         &         &        &        &        &        &         &         &         &         &          &         & \\ \hline
Nataraj et al. \cite{nataraj2019detecting} &97.70 & 63.15 & 53.75 & 92.50 & 51.1 & 54.7 & 57.1 & 63.06 &55.85 &65.65 & 65.80& 60.50& 70.70& 70.55 &71.00 &70.25& 69.60 &69.90 &67.55&66.86\\
Zhang et al. \cite{zhang2019detecting}&49.90 &99.90 &50.50 &49.90 &50.30 &99.70 &50.10 &50.00 &48.00 &50.60 &50.10 &50.90 &50.40 &50.40 &50.30 &51.70 &51.40 &50.40 &50.00 &55.45\\
Wang et al. \cite{wang2020cnn} & 100.0 & 80.49 & 55.77 & 64.14 & 82.23 & 80.97 & 50.66 & 56.11 & 50.00 & 87.73 & 92.85 & 52.30 & 51.20 & 52.20 & 51.40 & 53.45 & 55.35 & 54.30 & 52.60 & 64.41 \\
Chai et al. \cite{chai2020makes} & 68.81 & 53.02 & 55.76 & 59.24 & 52.64 & 77.49 & 55.78 & 59.65 & 48.80 & 65.57 & 61.69 & 52.26 & 58.53 & 60.72 & 58.21 & 55.78 & 56.58 & 55.05 & 61.24 & 58.78 \\
Ojha et al. \cite{ojha2023towards} & 100.0 & 98.50 & 94.50 & 82.00 & 99.50 & 97.00 & 66.60 & 63.00 & 57.50 & 59.5 & 72.00 & 70.03 & 94.19 & 73.76 & 94.36 & 79.07 & 79.85 & 78.14 & 86.78 & 81.38 \\
Wang et al. \cite{wang2023dire} &100.0 & 67.73 & 64.78 & 83.08 & 65.30 & 100.0 & 94.75 & 57.62 & 60.96 & 62.36 & 62.31 & 83.20 & 82.70 & 84.05 & 84.25 & 87.10 & 90.80 & 90.25 & 58.75 & 77.89\\
Tan et al. \cite{tan2023learning}&99.90 & 85.10 & 83.00& 94.80& 72.50 & 99.60& 56.40& 47.80 & 41.10 & 50.60 & 50.70 & 74.20 & 94.20 & 95.90& 95.00& 87.20 & 90.80 & 89.80 & 88.40 & \_\\
Corvi et al. \cite{corvi2023detection}&100.00 & 92.00 & 96.90 & 99.40& 94.80 & 99.50& 54.10& 90.60 & 55.50 & 100.00 & 100.00 & 53.90 & 58.00 & 61.10 & 57.50& 56.90 & 59.60 & 58.80 & 71.70 & \_\\
Zhu et al. \cite{zhu2023gendet} & 99.00 & 99.50 & 99.30 & 99.05 & 99.00 & 96.75 & 88.20 & 63.50 & 67.50 & 93.90 & 98.75 & 98.70 & 98.80 & 98.60 & 98.75 & 98.75 & 98.75 & 98.75 & 98.45 & 94.42 \\
Liu et al. \cite{liu2024mixture} & 100.0 & 99.33 & 99.67 & 99.46 & 99.83 & 97.07 & 77.53 & 81.11 & 65.50 & 82.32 & 96.79 & 90.70 & 98.30 & 95.90 & 98.75 & 92.40 & 93.95 & 93.00 & 94.90 & 92.45 \\
Cao et al. \cite{cao2024hyperdet} & 100.0 & 97.40 & 97.50 & 97.50 & 96.20 & 98.65 & 73.85 & 93.00 & 75.00 & 92.75 & 93.20 & 77.35 & 98.70 & 96.60 & 98.80 & 87.75 & 89.95 & 88.70 & 97.00 & 92.10 \\
Liu et al. \cite{liu2024fatformer} &99.90 & 99.30 & 99.50  & 97.20 &99.40  & 99.80 & 93.20 &\_ & \_ & \_ & \_  &76.10 & 98.60 & 94.90   & 98.70  & 94.40  & 94.70 & 94.20 & 98.80 & \_  \\
Koutlis et al. \cite{koutlis2025leveraging} & 100.0 & 99.30 & 99.60 & 88.90 & 99.80 & 99.50& 80.60 & 90.60 & 68.30 & 89.20 & 90.60 & 76.10 & 98.30 & 88.20 & 98.60 & 88.90 &92.60 &90.70 & 95.00 &  \_   \\
%Tan et al. \cite{tan2023learning}&\_& \_ & \_ & \_& \_ & \_& \_& \_ & \_ & \_ & \_ & \_ & \_ & \_ & \_& \_ & \_ & \_ & \_ & \_\\
\hline
\end{tabular}
}
\end{table*}


\begin{table*}[ht]
\centering
\caption{Comprehensive Evaluation of methods on GAN and diffusion-generated images using the UnivFD dataset \cite{ojha2023towards}. Results are reported in average precision (\%).}
\label{tab:comprehensive-averageprecision-detection-results}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccccccccccccccccccc}
\hline
Method           & \multicolumn{7}{c}{Generative adversarial networks} & \multicolumn{2}{c}{Low-level vision} & \multicolumn{2}{c}{Perceptual loss} & \multicolumn{8}{c}{Diffusion models} & mAP \\ \cline{2-7} \cline{9-10} \cline{11-12}   \cline{13-20}
                           & Pro-    & Cycle-  & Big-    & Style-  & Gau-    & Star-   & Deep-   & SITD   & SAN    & CRN    & IMLE   & Guided  & \multicolumn{3}{c}{LDM}                   & \multicolumn{3}{c}{GLIDE}  & DALL-E \\  
                           & GAN     & GAN     & GAN     & GAN     & GAN     & GAN     & fakes   &        &        &        &        &         & 200    & 200s    & 100    & 100   &  50 & 100       \\  
                           &         &         &         &         &         &         &         &        &        &        &        &         & steps  & w/CFG  &    steps & 27        & 27        & 10 \\  
                           &         &         &         &         &         &         &         &        &        &        &        &         &         &         &         &          &         & \\ \hline
Nataraj et al. \cite{nataraj2019detecting} &99.74 & 80.95 & 50.61 & 98.63 & 53.11 & 67.99 & 59.14 & 68.98 & 60.42 & 73.06 & 87.21 & 70.20 & 91.21 & 89.02 & 92.39 & 89.32 & 88.35 & 82.79 & 80.96 & 78.11\\ %done
Zhang et al. \cite{zhang2019detecting}&55.39 & 100.0 & 75.08 & 55.11 & 66.08 & 100.0 & 45.18 & 47.46 & 57.12 & 53.61 & 50.98 & 57.72 & 77.72 & 77.25 & 76.47 & 68.58 & 64.58 & 61.92 & 67.77 & 66.21\\ %done
Wang et al. \cite{wang2020cnn} &100.0 & 96.36 & 85.34 & 98.10 & 98.48 & 96.97 & 60.33 & 82.95 & 54.22 & 99.61 & 99.81 & 69.93 & 66.17 & 67.68 & 66.13 & 71.18 & 76.37 & 72.13 & 67.66 & 80.50\\
Chai et al. \cite{chai2020makes} & 68.44 & 55.59 & 64.37 & 64.10 & 58.74 & 84.48 & 59.92 & 72.08 & 47.63 & 73.05 & 68.38 & 58.98 & 77.05 & 76.87 & 76.35 & 75.97 & 77.41 & 74.68 & 71.91 & 68.74\\
Ojha et al. \cite{ojha2023towards} & 100.0 & 99.46 & 99.59 & 97.24 & 99.98 & 99.60 & 82.45 & 61.32 & 79.02 & 96.72 & 99.00 & 87.77 & 99.14 & 92.15 & 99.17 & 94.74 & 95.34 & 94.57 & 97.15 & 93.38 \\ %done
Wang et al. \cite{wang2023dire} &100.0 & 76.73 & 72.80 & 97.06 & 68.44 & 100.0 & 98.55 & 54.51 & 65.62 & 97.10 & 93.74 & 94.29 & 95.17 & 95.43 & 95.77 & 96.18 & 97.30 & 97.53 & 68.73 & 87.63\\

Corvi et al. \cite{corvi2023detection} &100.00 & 98.60 & 99.80 & 100.0& 99.80 & 100.0& 94.70& 99.80& 87.70 & 100.00 & 100.00 & 73.00 & 86.80& 89.40   & 87.30& 86.50 & 89.90 & 89.00 & 96.10 & \_\\ %done
Zhu et al. \cite{zhu2023gendet} & 99.95 & 99.95 & 99.92 & 99.92 & 99.92 & 99.25 & 91.38 & 61.23 & 72.66 & 97.90 & 98.88 & 99.30 & 99.85 & 99.51 & 99.85 & 99.50 & 99.46 & 99.19 & 99.47 & 95.64\\
Liu et al. \cite{liu2024mixture} & 100.0 & 99.85 & 99.88 & 99.69 & 100.0 & 99.68 & 87.38 & 88.26 & 84.48 & 98.82 & 99.84 & 93.39 & 99.81 & 96.80 & 99.88 & 98.71 & 98.84 & 98.60 & 98.81 & 96.99\\
Cao et al. \cite{cao2024hyperdet} & 100.0 & 99.96 & 99.89 & 99.73 & 99.93 & 100.0 & 88.38 & 97.12 & 89.22 & 98.82 & 99.98 & 95.31 & 99.86 & 99.14 & 99.90 & 97.20 & 97.99 & 98.02 & 99.65 & 97.90\\
Tan et al. \cite{tan2023learning} &100.0& 94.00 & 90.70 & 99.90& 79.30 & 100.0& 67.90& \_ & \_ & \_ & \_  & 100.0 & 99.10 & 99.20 & 99.20& 93.20 & 95.10 & 94.90 & 97.30 & \_\\
Liu et al. \cite{liu2024fatformer}&100.0& 100.0 & 100.0 & 99.80& 100.0 & 100.0& 98.00& \_ & \_ & \_ & \_  & 92.00 & 99.80 & 99.10 & 99.90& 99.10 & 99.40 & 99.20 & 99.80 & \_\\
Koutlis et al. \cite{koutlis2025leveraging} & 100.0 & 100.00 & 99.90 & 99.40 & 100.0& 100.0& 97.90 & 97.20 & 94.90 & 97.30 & 99.70 & 96.40 & 99.80 & 98.30 & 99.90 & 98.80 &99.30 &98.90 & 99.30 &  \_   \\ %done
%Tan et al. \cite{tan2023learning}&\_& \_ & \_ & \_& \_ & \_& \_& \_ & \_ & \_ & \_ & \_ & \_ & \_ & \_& \_ & \_ & \_ & \_ & \_\\
\hline
\end{tabular}
}
\end{table*}


\subsection{Commercial Detection Methods}
Watermarking by DeepMind \cite{synthid2024deepmind} is a commercial technology that introduce watermarking to the generated image invisible to human eyes, that work as basis for the detection of the synthesized image properly.

\section{Datasets}
\subsubsection{ForenSynths by Wang: 2020}
The dataset used by Wang et al. \cite{wang2020cnn} is publicly available on their GitHub page, as referenced in the main paper. The training set consists of 724,000 images, including 362,000 real images and 362,000 fake images generated by ProGAN \cite{karras2017progressive}, trained on the LSUN dataset \cite{yu2015lsun} across 20 different object categories. The testing dataset, which includes images generated by various other GAN models, is also accessible on the GitHub page.

\subsubsection{Artifact Dataset by Rahman: 2023}
Rahman et al. \cite{rahman2023artifact} introduced the Artifact dataset, containing 2,496,738 images, including 964,989 real images and 1,531,749 synthetic images generated using 25 different models (both GANs and diffusion models). The dataset includes diverse object categories representative of general real-world content.

\subsubsection{SynthBuster by Bammey: 2023}
Bammey et al. \cite{Synth10334046} introduced the SynthBuster dataset, consisting of 9,000 synthetic images, with 1,000 images generated by each of nine different diffusion models, namely: DALL·E 2 \cite{ramesh2022hierarchical}, DALL·E 3 \cite{ramesh2022hierarchical}, Adobe Firefly \cite{adobe_firefly}, Midjourney v5 \cite{midjourney}, Stable Diffusion \cite{rombach2022high} 1.3, 1.4, 2, and XL, and GLIDE \cite{nichol2021glide}. The dataset is publicly available, with access details provided in the main paper. For real images, the authors recommend using the RAISE-1k dataset \cite{dang2015raise}, accessible via the same portal.

\subsubsection{DiffusionForensics Dataset by Wang et al.: 2023}
Wang et al. \cite{wang2023dire} introduced the DiffusionForensics dataset, specifically designed to evaluate forensic detectors for diffusion-generated images. The dataset includes images sourced from LSUN-Bedroom \cite{yu2015lsun}, ImageNet \cite{russakovsky2015imagenet}, and CelebA-HQ \cite{karras2017progressive}, with the following distributions:  
- 42,000 synthetic images generated from LSUN-Bedroom,  
- 50,000 synthetic images generated from ImageNet, and  
- 42,000 synthetic face images generated using Stable Diffusion V2 (SD-V2), paired with 42,000 real images from CelebA-HQ.

\subsubsection{UnivFD Dataset by Ojha et al.: 2023}
Ojha et al. \cite{ojha2023towards} expanded the ForenSynths dataset by integrating images generated from various models, including ProGAN \cite{karras2017progressive}, CycleGAN \cite{zhu2017unpaired}, BigGAN \cite{brock2018large}, StyleGAN \cite{karras2019style}, GauGAN \cite{park2019semantic}, StarGAN \cite{choi2018stargan}, Deepfakes \cite{rossler2019faceforensics++}, SITD \cite{chen2018learning}, SAN \cite{dai2019second}, CRN \cite{chen2017photographic}, and IMLE \cite{li2019diverse}. In addition, diffusion-based models such as the Guided diffusion model (ADM) \cite{dhariwal2021diffusion}, LDM \cite{rombach2022high}, GLIDE \cite{nichol2021glide}, and DALL-E \cite{ramesh2021zero} were included, increasing its diversity for forensic detection studies.

\subsubsection{Community Forensics by Park: 2024}
Many existing datasets contain images generated by diverse generative models; however, they often lack diversity and generalization capability for many-to-many scenarios. These scenarios include testing images generated from multiple diffusion models against those from various GANs, as well as incorporating generative models such as VAEs. Recognizing the need for a large, diverse dataset capable of detecting synthetic images from unseen generative models, Park and Owens \cite{park2024community} introduced the Community Forensics Dataset. This dataset comprises 2.4 million images, generated by 4,803 distinct generative models, alongside an equal number of real images, making it more diverse than previous datasets. To evaluate its effectiveness, the authors trained and tested state-of-the-art models on this dataset, utilizing a binary classification setup with pre-trained models ViT-S \cite{dosovitskiy2020image} and ConvNeXt-S \cite{liu2022convnet} as backbone architectures. Unlike previous methods that freeze backbone layers, the authors fine-tuned the entire backbone end-to-end, achieving better performance compared to existing state-of-the-art methods on both the Community Forensics dataset and other publicly available benchmarks.

\subsubsection{GenImage by Zhu: 2024}
Zhu et al. \cite{zhu2024genimage} introduced a large-scale dataset containing synthetic images generated by advanced state-of-the-art GANs and diffusion models (Wukong \cite{Wukong} and VQDM \cite{gu2022vector}, including commercial models, alongside real images. The dataset comprises approximately 1.3 million synthetic images and 1.3 million real images, covering diverse general image content. The authors also propose two real-world analysis factors to assess detection performance:  
(a) Cross-Generator Image Classification: training detectors on images generated by one model and evaluating them on images from different generators, and (b) Degraded Image Classification: testing detectors on degraded images affected by factors such as low resolution, JPEG compression, and Gaussian blur.

\subsubsection{CIFAKE Dataset by Bird and Lotfi: 2024}
Bird and Lotfi \cite{bird2024cifake} released a dataset comprising 120,000 images, equally divided between 60,000 real images and 60,000 synthetic images. The real images originate from CIFAR-10 \cite{krizhevsky2009learning}, which includes ten object categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The synthetic images were generated using diffusion models, including commercially available ones.

\subsubsection{AIGCDetection Benchmark Dataset by Zhong et al.: 2024}
Zhong et al. \cite{zhong2024patchcraft} introduced a curated benchmark dataset incorporating images generated by 17 different generative models, including GANs and diffusion models. The dataset aggregates samples from ForenSynths \cite{wang2020cnn} and GenImage \cite{zhu2024genimage}, and includes additional images from recent diffusion models, offering a comprehensive benchmark for generative image forensics.

\subsubsection{ImagiNet Dataset by Boychev: 2024}
Boychev et al. \cite{boychev2024imaginet} presented the ImagiNet dataset, comprising 200,000 images equally split between real and synthetic categories. The dataset further categorizes images into photos, paintings, faces, and uncategorized types. The synthetic images are generated using GANs, diffusion models, and proprietary generative models. For evaluation, the dataset is divided into 160,000 training images and 40,000 test images.

\subsubsection{Chameleon Dataset by Yan et al.: 2024}
Yan et al. \cite{yan2024sanity} introduced the Chameleon dataset, which focuses on realistic AI-generated image detection. The dataset consists of 150,000 synthetic images covering content categories such as humans, animals, scenes, and objects, generated using GANs and diffusion models. In addition, it includes 20,000 real images to facilitate forensic evaluation.


\subsection{Discussion}
The performance of synthetic image detection is primarily evaluated using classification accuracy (Acc) and average precision (AP), which are mathematically expressed as follows:

\begin{equation}
    \text{Acc} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\begin{equation}
    \text{AP} = \sum_n (R_n - R_{n-1}) P_n
\end{equation}

where \( TP \), \( TN \), \( FP \), and \( FN \) represent true positives, true negatives, false positives, and false negatives, respectively. The AP is computed as the weighted sum of precision \( P_n \) at different recall levels \( R_n \).

For clarity, we present a comparative analysis of methods across five distinct categories, following a general review of their reported performances in the literature. The results are detailed in Table \ref{tab:detection-results-visionlangugage}, Table \ref{tab:combined_evaluations_fingerprint}, and Table \ref{tab:combined_evaluations_patchbased}. Additionally, a comprehensive performance comparison across all five categories using the UnivFD dataset \cite{ojha2023towards} is provided in Table \ref{tab:comprehensive-accuracy-detection-results} for classification accuracy and Table \ref{tab:comprehensive-averageprecision-detection-results} for average precision. The evaluation results on the UnivFD dataset \cite{ojha2023towards} are selected due to its inclusion of state-of-the-art detection methods, particularly those utilizing Vision-Language approaches, which have demonstrated strong forensic capabilities in synthetic image detection. These methods exhibit significant potential for real-world applications, reinforcing their role in advancing forensic detection.


\section{Conclusion}  
The advancement of generative AI in image synthesis and forensic detection has been an ongoing and increasingly active research area, driven by both the successes of AI and the ethical concerns it introduces. From a practical standpoint, forensic detection must remain ahead to mitigate potential human-related harm. While several reviews exist in the current literature, they do not comprehensively cover detection methods capable of identifying images generated by state-of-the-art generative models, nor do they adequately address the role of multimodal approaches in detection. To bridge this gap, we have conducted a comprehensive survey of detection methodologies. First, we categorize these methods into five primary groups based on their underlying approaches. We then analyze their comparative performance on publicly available datasets and assess whether they satisfy three key criteria for evaluating their generalizability. Our findings indicate that detection methods leveraging multimodal frameworks tend to exhibit greater robustness and adaptability across different generative models.  
While this study primarily focuses on the image domain, the insights gained, specifically regarding frequency-domain analysis, can be extended to forensic detection in other modalities, such as audio and video analysis. Transforming such data into the frequency domain with appropriate operations may reveal anomalies that serve as modality-specific fingerprints for adversarial detection. 

\section*{Acknowledgment}
This material is based in part upon work supported by the National Science Foundation under Grant Nos.  MRI20 CNS-2018611 and MRI CNS-1920182.




\bibliographystyle{ACM-Reference-Format}
%\bibliography{references}
\input{main.bbl}

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
