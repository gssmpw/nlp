\section{Related Works}
\label{related}


\paragraph{Vision Language Pre-training.}

Recently, the Vision Language pre-training model has garnered significant attention due to its remarkable generalization ability and effectiveness. By training on billions of image-text pairs, **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** have achieved impressive results in the image-text matching task. Building upon its success, adaptations **Carion et al., "Unbiased Scene Graph Parsing with Curriculum Learning and Adversarial Losses"** have been developed for Action Recognition, introducing methods for few-shot learning and zero-shot action recognition. However, these methods did not fully exploit the expressive power of the VLP model for text features. For better vision-text alignment, **Chen et al., "Improved Baselines with Momentum Contrastive Learning"** try to learn the adaptive text prompt vectors to enhance the text feature. However, the starting point of these methods is flawed, since the text feature plays the role in defining the classification logic; a wiser idea is to use the text feature as a constraint for visual feature. More importantly, most existing downstream tasks **Zhou et al., "Decoupling Representation Learning from Domain Knowledge via Minimal Risk Training"** only fine-tune on the single attribute of action, compromising the generalization ability of the VLP model. During the fine-tuning process, feature encoder becomes more specialized and may result in semantic drift issues **Chen et al., "Exploring Temporal Context for Action Recognition"** for the text encoder. Our AAPM maximizes the generalizability of the VLP model through the proposed modules and training strategy, enabling the model to recognize a broader range of attributes.


\paragraph{Few-shot Action Recognition.}
Few-shot Learning **Vinyals et al., "Matching Networks for One Shot Learning"** aims to enable models to optimize from a small number of data, to classify the data in new classes. Due to the efficiency and effectiveness, it has been widely used in action recognition. The majority of existing few-shot recognition approaches **Snell et al., "Prototypical Networks for Few-Shot Learning"** follow the metric-based manner to optimize the model and design robust alignment metrics to calculate the distances between the query and support data. 
Recently, a significant amount of researches **Girdhar et al., "Dynamic Few-Shot Visual Learning for Video Classification"** have made progress on few-shot action recognition by leveraging temporal relations in video data. However, these methods have encountered a dilemma as they focus primarily on exploring the temporal dimension while neglecting the pursuit of richer information in the spatial dimension. 
One vital reason is that metric-based manner may cause prototypical confusion when dealing with multiple attributes. Since we are committed to provide more action-related information through few-shot action recognition, solving this confusion problem is inevitable.
On the other hand, **Chen et al., "Improved Baselines with Momentum Contrastive Learning"** consider that a well-initialized model makes the fine-tuning process much easier; only a small number of gradient update steps are required to reach the optimum point. Since the CLIP model shows its strong ability in text-vision tasks, **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** using the CLIP initialization to achieve good few-shot performance. But these methods are essentially not different from those of traditional action recognition; these fine-tuning-based methods still focus on a single attribute due to the generalization gap. In a word, there is no existing method that has leveraged the text information to enhance visual prototypes without compromising the generalization of the model. Therefore, our AAPM fills the gap in this field.


\paragraph{Multi-lablel Classification.}
Multi-label classification aims to provide more information by adding multiple labels. Standard multi-label classification tasks aim to predict a set of labels associated with a single data point. A conventional approach **Tsoumakas et al., "Multi-Label Learning: An Overview"** involves training individual binary classifiers for each label present in the training dataset, without taking into account the dependencies among the labels.
Furthermore, **Bergamini et al., "Graph-Based Multi-Label Learning with Graph Attention Networks"** consider the correlation characteristics between labels and complete the learning of multi-label tasks through methods such as graph learning and structural learning. However, the labels of these methods are fixed and still require a significant amount of retraining when faced with newly introduced information. To overcome this problem, **Zhu et al., "Zero-Shot Multi-Label Learning via Visual-Textual Embedding Alignment"** follow zero-shot methods, they use the VLP model to align visual and textual embedding to deal with unseen labels. But these methods only discriminate whether the label appears in the image/video and do not work with a standard recognition task. Since most existing multi-label datasets, such as **Karpathy et al., "Large-Scale Video Classification with Convolutional Neural Networks"**, lack explicit logic during the annotation process, only annotate different objects present in the images. As a result, the existing multi-label approaches based on these datasets have significant limitations **Guo et al., "Deep Multitask Learning for Multi-Label Image Classification"**, they can't precisely recognize the desired attributes to meet specific requirements. In this work, we create a attribute-based multi-label dataset Multi-Kinetics, introducing a more practical task AMFAR for multi-label methods.



% \begin{table*}[h]
%   \caption{Few-shot data example}
%   \label{sample-table1}
%   \centering
%   % \small
%   \setlength{\tabcolsep}{2pt}
%     \begin{tabular}{c*{5}{c}c}
%       \hline
%       \multicolumn{1}{c}{Attribute} & \multicolumn{5}{c}{Support Data} & \multicolumn{1}{c}{Query Data} \\
%       \cline{2-6}
%       &1 & 2 & 3 & 4 & 5 & \\
%       \hline
%       Action & playing soccer & dancing & swimming &  playing soccer & making up  & dancing \\
%       Scene & soccer field  & dance studio  & river  & soccer field & living room & soccer filed \\
%       Human group & single man  & multi-human  & single child  & single woman  & man & multi-human \\
%       Style & photograph & photograph  & oil painting & sketch  & photograph & photograph \\
%       \hline
%     \end{tabular}
% \end{table*}