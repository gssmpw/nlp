\documentclass[journal]{IEEEtran}

\usepackage{etex}
\reserveinserts{1000}
\setcounter{totalnumber}{50}
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}
\renewcommand{\floatpagefraction}{1.0}
\usepackage{pifont}
\usepackage{bm}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
% \bibliographystyle{IEEEtran}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[english]{babel}
\usepackage[normalem]{ulem}
\usepackage{graphicx,epstopdf}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{latexsym}
\usepackage{eurosym}
\usepackage{mathtools}
\usepackage{lscape}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{optidef}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{subcaption}
\usepackage{enumitem}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{conj}{Conjecture}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{rem}{Remark}
\usepackage{amsthm}
\newcommand{\orag}[1]{\textcolor{orange}{[{#1}}]}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  breaklines=true,
  frame=single,
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{green},
  stringstyle=\color{red},
  showstringspaces=false,
  escapeinside={(*@}{@*)}
}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
% \title{Real-Time Adaptive-Bitrate Video Streaming with FFmpeg and Latent Diffusion Model}
% \title{Latent Diffusion Model-Enabled Semantic Adaptive Video Streaming for Next-Generation Wireless Networks}
\title{Semantic-Aware Adaptive Video Streaming Using Latent Diffusion Models for Wireless Networks}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Zijiang~Yan$^{*}$,~%~\IEEEmembership{Student~Member,~IEEE,}
        Jianhua~Pei$^{*}$,~\IEEEmembership{Student~Member,~IEEE,}
         Hongda~Wu, ~\IEEEmembership{Member,~IEEE,}
    Hina~Tabassum,~\IEEEmembership{Senior~Member,~IEEE,}
    and~Ping~Wang,~\IEEEmembership{Fellow,~IEEE}
    % <-this % stops a space
\thanks{Z. Yan, J. Pei,  H. Wu, H. Tabassum and P. Wang are with the Department
of Electrical Engineering and Computer Science, York University, Canada. Z. Yan and J. Pei contributed equally to this work and are co-first authors.}
% \thanks{Z.Yan is with Bell Media Inc., Toronto, Canada}
% \thanks{J. Pei is with School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, China }
}% <-this % stops a space
% \thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2005; revised August 26, 2015.}}

% The paper headers
% \markboth{IEEE Wireless Communications,~Vol.~1, No.~1, October~2025}%
% {Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}



% make the title area
\maketitle


% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
This paper proposes a novel framework for real-time adaptive-bitrate video streaming by integrating latent diffusion models (LDMs) within the FFmpeg techniques. This solution addresses the challenges of high bandwidth usage, storage inefficiencies, and quality of experience (QoE) degradation associated with traditional constant bitrate streaming (CBS) and adaptive bitrate streaming (ABS). The proposed approach leverages LDMs to compress I-frames into a latent space, offering significant storage and semantic transmission savings without sacrificing high visual quality. While it keeps B-frames and P-frames as adjustment metadata to ensure efficient video reconstruction at the user side, the proposed framework is complemented with the most state-of-the-art denoising and video frame interpolation (VFI) techniques. 
These techniques mitigate semantic ambiguity and restore temporal coherence between frames, even in noisy wireless communication environments. 
Experimental results demonstrate the proposed method achieves high-quality video streaming with optimized bandwidth usage, outperforming state-of-the-art solutions in terms of QoE and resource efficiency. This work opens new possibilities for scalable real-time video streaming in 5G and future post-5G networks.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Video streaming, semantic communication, data compression, latent diffusion model, adaptive-bit-rate, FFmpeg.
\end{IEEEkeywords}


\raggedbottom
\IEEEpeerreviewmaketitle



\section{Introduction}

\IEEEPARstart{I}{n} today's digital age, the demand for streaming services with high-quality video content has surged alongside the proliferation of Internet-enabled devices, bridging the gap between media products and edge users \cite{zhou2022cadm}. Streaming services, which cater to a vast audience with diverse preferences, face the challenge of delivering superior video quality while optimizing wireless network bandwidth and resource usage. Real-time adaptive bit-rate streaming (ABS) has emerged as a vital technology in this landscape, which enables dynamically adjusting video quality based on network conditions to provide a seamless viewing experience and improved quality of experience (QoE) \cite{bentaleb2024bitrate}. 
One significant advancement in ABS is the integration of fast forward moving picture experts group (FFmpeg), advanced video coding (H.264), hypertext transfer protocol (HTTP), and dynamic adaptive streaming over HTTP (DASH). These open-source multimedia frameworks are capable of decoding, encoding, transmission, and streaming digital media files in various formats. In particular, FFmpegâ€™s flexibility and efficiency make it an indispensable tool for real-time video processing and adaptive streaming solutions.


Diffusion models (DMs) \cite{blattmann2023stable}, a class of Artificial Intelligence generative content (AIGC) models, have shown promising results in generating high quality media content. These models can also intelligently refine video streams at the receiver side, significantly improving visual quality while minimizing bandwidth requirements. By integrating latent diffusion models (LDMs) \cite{pei2024latent} into the FFmpeg framework, high-quality real-time video streaming can be achieved while dynamically adapting to varying network conditions and diverse quality of experience (QoE) requirements based on different media content. Along another note, the advent of fifth-generation (5G) and beyond 5G (B5G) networks offer unprecedented speeds and low latency, providing an ideal infrastructure for deploying sophisticated video streaming technologies. Despite their advantages, 5G and B5G networks may introduce more semantic ambiguities due to encoding techniques that are more sensitive to wireless channel noises, large-scale high-quality 4K/6K video streaming delivery, and the increased utilization of machine learning models, all of which must be addressed to the quality of the transmitted video. As demonstrated by the excellent performance of LDM in wireless image transmission and video generation tasks, advanced data compression and channel denoising techniques can also ensure a flawless streaming experience with higher QoE and quality of service (QoS) in wireless video streaming.



To this end, this paper first highlights the key challenges in the domain of conventional video streaming and DM-enabled video streaming. Next, we propose a novel \textit{LDM-enabled semantic-aware adaptive video streaming} framework that enables transfer of high-quality  video contents over wireless channels, considering different frame types~\footnote{To efficiently transmit video within limited bandwidth, video streaming employs frame decomposition into three frame types: I-frames, P-frames, and B-frames \cite{pourreza2021extending}.  The details are given in Section~\ref{subsection:vfi}. 
% We will further explain these frames in Section \ref{subsection:vfi}.
    % I-frames (Intra-coded Frames) are self-contained frames compressed using only their internal information, akin to standalone image files. 
    % P-frames (Predictive Frames) encode only motion differences and residual data. 
    % B-frames (Bidirectional Predictive Frames) utilize both preceding and following frames for prediction, achieving higher compression efficiency.
}. 
{Specifically, the proposed video streaming scheme integrates LDM and FFmpeg for efficient compression,  reducing bandwidth by encoding keyframes as latent features and motion data as metadata. To enhance robustness, LDMs denoise I-frames and regenerate B/P-frames, ensuring better video quality over noisy wireless channels. Moreover, an adaptive bitrate mechanism using CNN-GRU optimizes streaming based on varying network conditions, media content, and user preferences for an improved viewing experience.}
The efficiency and reliability of the proposed system are quantified in terms of \textit{QoE} that incorporates the trade-off between streaming video quality, quality fluctuation, and the risk of rebuffering events\cite{kan2022improving}.  providing concrete evidence of its superiority over the other ABS algorithms such as, BOLA, Comyco, MERINA \cite{kan2022improving}.

The rest of this paper is organized as follows. Section \ref{problemformulation} introduces existing challenges of video streaming and DM-aided semantic video communication. Section \ref{proposedmethod} presents the design and workflow of the proposed LDM-enabled ABS framework, including its key components for compression, transmission, and video reconstruction. Section \ref{casestudy} provides numerical results and performance analysis to validate the effectiveness of the proposed approach in real-time streaming scenarios. Finally, Section \ref{conclusion} concludes the paper and outlines future research directions.


\section{Fundamental Challenges in Semantic Communication for Video Streaming} \label{problemformulation}

This section outlines several fundamental challenges in semantic-aware video streaming, including constant bit-rate streaming (CBS), adaptive bit-rate streaming (ABS), and DM-enabled bit-rate streaming. A comparative analysis of the various bit-rate streaming scheme is presented in \textbf{Fig.~\ref{fig:comparison_cbs_abs}}. {
 In CBS, parallel channels are utilized for transmission within the wireless zone to the user side, requiring extensive wireless resources. In ABS, a single channel is transmitted to the user side, and frames can be downscaled in the computing zone based on the user's request. This approach conserves wireless transmission resources while providing adaptive bitrate options. LD-ABS introduces a novel mechanism where only latent frames are transmitted, eliminating the need for complete frames. This design significantly compresses the transmission, reducing bandwidth and storage requirements, and enabling ultra-efficient streaming.
} 
% in Subsections~\ref{subsection:bandwidth},  \ref{subsection:lowQoE}, \ref{subsection:latency}, and \ref{subsection:vfi}, including visual data compression, latency, channel noise, and the need for real-time bitrate adaptation. 






\subsection{ {High Bandwidth and Storage Utilization for CBS}} \label{subsection:bandwidth}
Multi-bitrate streaming consumes a substantial amount of bandwidth and resources for transmission. To satisfy the diverse customers' demands of streaming video,  traditionally providers adopt constant bitrate streaming (CBS), which involves providing parallel streaming channels to broadcast video in different resolutions and qualities. 

\textbf{(i) Bandwidth Consumption}: CBS requires a fixed amount of bandwidth for each resolution and quality level, irrespective of the content's complexity \cite{wang2023trafada}. 
% \cite{wang2023trafada}
This can lead to inefficient use of available bandwidth, especially during periods of low network traffic or when streaming less complex scenes \cite{su2024reducing}. CBS necessitates maintaining multiple streams for different quality levels, which can overload the content delivery network (CDN) \cite{kan2022improving}. %\cite{li2023metaabr}
In the context of CBS, as illustrated in Fig. \ref{fig:comparison_cbs_abs}, let's assume we have 4 parallel routes to stream videos. We capture video at a 1080P resolution source. Each route streams video at 1080P, 720P, 360P, and 144P resolutions, respectively. FFMpeg decodes the video with different bitrates to their respective reference frames $I_n$ and affiliated predictive frames $P_n$ and $B_n$. Note that the size of predictive frames is significantly smaller than the reference frames \cite{wiegand2003overview}. %\cite{tomar2006converting}.

% \begin{figure*}
% \includegraphics[width=1\linewidth]{images/cbs-abs-ldabs.pdf}

% \caption{\color{red}Comparison of CBS, ABS and proposed LDM enabled ABS (LD-ABS) }
% \label{fig:comparison_cbs_abs}
% \end{figure*}


Users manually select the resolution on the player side. This design requires all frames across all resolutions to be sent to the user side. Broadcasting video in different bitrates consumes considerable traffic and resources, overburdening the CDN. Overall, CBS utilizes the highest storage and resources due to multi-bitrate streaming.

\textbf{(ii) Server Overload}: Handling numerous simultaneous requests for high-quality video streams strain server resources and  place a heavy burden on networking infrastructure and cloud resources, leading to increased latency \cite{wang2023trafada}, buffering issues \cite{su2024reducing}, and server crashes at peak times or major events.

\textbf{(iii) Wireless Channel Impairments}: In wireless communication systems, the variability in signal quality and network conditions makes it difficult to maintain a constant bitrate. High-bandwidth streams can quickly exhaust available wireless resources, leading to degraded QoE for mobile users and increased traffic usage \cite{wang2023trafada}. %\cite{su2024reducing}.
\begin{figure}
\includegraphics[width=1\linewidth]{images/abs-cbs.pdf}

\caption{Comparison of CBS, ABS and proposed LDM enabled ABS (LD-ABS) }
\label{fig:comparison_cbs_abs}
\end{figure}

\subsection{ {Low QoE Multi-Bitrate Adaptive Streaming}} \label{subsection:lowQoE}


{ABS dynamically adjusts video quality based on a userâ€™s network conditions and device capabilities to optimize viewing experiences while efficiently utilizing bandwidth. Unlike CBS, which delivers pre-defined quality streams through parallel channels, ABS continuously monitors real-time factors such as network throughput, buffer occupancy, and device performance to make fine-grained adjustments.}
Achieving high-quality video streaming without excessive bandwidth consumption remains a challenging task. Traditional ABS algorithms often struggle to balance video quality and network efficiency, leading to either compromised video quality or excessive data usage \cite{bentaleb2024bitrate}. Video streaming services require ABS to cater to varying network conditions and device capabilities.

 \textbf{(i) Challenges of diverse wireless conditions: } { Compared to CBS, which does not automatically select chunks, ABS must dynamically adjust the video bitrate in response to varying network conditions. This adaptation introduces potential latency during bitrate switching, especially in dynamic wireless environments. Additionally, ABS aggressively reduces bitrate under poor network conditions, often resulting in compression artifacts. In contrast, CBS maintains a consistent quality, reducing the risk of rebuffering but lacking the flexibility to optimize for fluctuating bandwidth. \cite{kan2022improving}.}
% Adapting the bitrate to the current wireless environment is challenging. High-resolution streaming depends on stable wireless communication conditions. Users with unstable or low-availability wireless networks face difficulties watching high-resolution streaming videos without frequent interruptions and pauses \cite{kan2022improving}. %\cite{li2023metaabr}}


\textbf{(ii) Cache and Buffering Issues: }
Managing cache and buffering of short clips in ABS streaming is complex. %\cite{menon2023emes} 
Storing multiple resolution video clips adds complexity and consumes significant storage and bandwidth\cite{ wang2023trafada}.  %\cite{menon2023emes} 

\textbf{(iii) Downscaling to Ensure Users Watching Smooth and Continuous Video: }
Existing ABS methods attempt to maintain smooth play and playback by frequently switching bitrates. However, excessive bitrate switching negatively impacts QoE and QoS for viewers \cite{wang2023trafada}.
    

\subsection{{VFI and Temporal Consistency}} 
\label{subsection:vfi}
Streaming video is typically decomposed into Intra-coded frames (I-frames), {Predictive-coded frames (P-frames) and Bidirectionally predictive-coded frames  (B-frames) }\cite{pourreza2021extending}, which are defined, respectively, as follows:
\begin{itemize}
    \item \textbf{I-frames} are compressed using only the information within the frame itself. They contain a complete image, similar to a PNG image file.
    \item \textbf{P-frames} use data from the previous I-frame or P-frame to predict and encode only the differences (motion vectors and residual data).
    \item \textbf{B-frames} use data from both preceding and following I-frames or P-frames to predict the frame. They encode differences from both directions, often achieving higher compression.
\end{itemize}

Reconstructing frames based on these reference frames and adjustment frames is difficult \cite{danier2024ldmvfi}. In video compression, I-frames serve as key reference frames containing the full image data, while P- and B-frames store only the motion vectors\footnote{Motion vectors capture the displacement of objects between frames, encoding movement without redundantly storing unchanged visual information already present in the I-frame.} and differences relative to I-frames.   Motion vectors are light compared to frames, allowing for efficient reconstruction while maintaining high compression efficiency. However, estimating motion vectors between frames is complex. Ensuring temporal consistency across frames is challenging, especially when using generative models for frame reconstruction \cite{danier2024ldmvfi}. In addition, accurate motion estimation is crucial for aligning the reconstructed P-frames and B-frames with the original video sequence. Errors in motion estimation can result in misalignment and temporal inconsistencies \cite{danier2024ldmvfi}.

\subsection{ {High Latency and Low Resolution Restoration on Reverse Diffusion Frame Reconstruction }}\label{subsection:latency}

Traditional diffusion models require multiple steps to generate high-resolution frames, making them impractical for real-time video applications, which are restricted to 500ms \cite{pei2024latent}.  

\textbf{(i) Slow reconstruction speed: } The reverse diffusion process involves gradually denoising the corrupted frames by iteratively applying the diffusion model which result in high latency, especially for long video sequences \cite{yu2024efficient,danier2024ldmvfi,yu2023video}.

\textbf{(ii) Computational complexity: }
Reconstructing all the frames across multiple resolutions overburdens VAE. 

\textbf{(iii) Limited frame generalization: } {DMs} are trained on specific datasets and may not generalize well to unseen or diverse content. In addition, streaming video contains semantic errors. These difficulties lead to challenges on restoring low-resolution frames from different sources or with different characteristics \cite{yu2024efficient}.%blattmann2023align


\section{State-of-the-Art: DM-Aided Video Streaming} \label{subsection:relatedwork}
{
Traditional video compression standards, such as H.264, present challenges in terms of bandwidth consumption, storage requirements, and computational overhead \cite{wiegand2003overview}. To achieve real-time performance, dedicated hardware acceleration (e.g., specialized GPUs or ASICs) is often required for multi-channel H.264 encoding to prevent frame drops and buffering delays on the producer side \cite{ho2022video}. Additionally, packet loss in predicted frames (P- or B-frames) can degrade following frames until an I-frame refreshes the stream. To address the aforementioned issues, LDMs and DMs are becoming popular. In this section, we introduce the fundamental principles of LDMs and compare the key differences between DMs and LDMs followed by a review of existing literature in the domain of DM/LDM-enabled video streaming.
%
% Subsequently, we present DMs and LDMs-enabled video streaming applications. 
% Subsequently, we compare the storage and bandwidth occupancy among CBS, ABS, and the proposed Latent Diffusion Adaptive Bitrate Streaming (LD-ABS). Finally, we present the LD-ABS.
}

\subsection{Overview of Latent Diffusion Models}

 LDMs specifically operate in a lower-dimensional latent space rather than pixel space, leading to more efficient computation and better scalability \cite{pei2024latent}. Conditional LDMs (CLDMs) further enhance this approach by conditioning the diffusion process on additional information, such as motion adjustment metadata. Among various implementations, stable diffusion \cite{blattmann2023stable} achieve high-quality outputs with improved efficiency and reduced computational requirements. Stable diffusion's architecture leverages both LDMs and CLDMs, allowing it to generate detailed and coherent frames while maintaining a smaller memory footprint compared to other models. This makes LDMs particularly suitable for compressing streaming video, as they can effectively balance the trade-offs between computational load, memory usage, and output quality, enabling efficient and scalable video compression. The key distinctions of LDMs compared to traditional DMs are:
 
$\bullet$ \textbf{Restoration of original data:} LDMs learn to restore the original data by reversing the noising process through incremental denoising and reconstruction.

$\bullet$ \textbf{Systematic degradation of training data:} Both LDMs and DMs systematically introduce Gaussian noise to degrade the original data in a step-by-step process known as diffusion. This transformation simplifies the data distribution.

$\bullet$ \textbf{Operating in latent space:} Unlike traditional DMs that operate directly in pixel space, LDMs function in a lower-dimensional semantic latent space. This shift leads to more efficient computation and improved scalability.

$\bullet$ \textbf{Conditional diffusion:} CLDMs enhance the standard diffusion process by incorporating additional information, such as motion adjustment metadata, to condition the model denoising processes. This results in higher quality and more relevant generated frames.

$\bullet$ \textbf{Denoising via Neural Network:} LDMs utilize neural networks to capture complex spatiotemporal relationships across different frames in video streaming, facilitating the generation of high-fidelity frames and enhancing video synthesis.


\subsection{Recent Advancements on DM-aided Video Streaming}
\subsubsection{DM-aided Video Streaming}
{
Zhou \textit{et al.} in \cite{zhou2022cadm} presented Codec-aware Diffusion Modeling (CaDM), a novel neural-enhanced video streaming paradigm. CaDM improves compression efficiency by reducing both the resolution and color bit-depth of video frames during encoding. At the decoder side, it employs a denoising diffusion process conditioned on the encoder's settings to restore high-quality frames, achieving significant bit-rate savings while maintaining superior visual quality.
%
Li \textit{et al.} in \cite{li2024extreme} proposed an extreme video compression approach leveraging the predictive power of the forward diffusion process (FDP) with a pre-trained model. However, this method lacks flexibility as it is not adaptable to different video sources. While DMs offer advancements in video streaming, high computational costs and iterative sampling processes can introduce latency, posing limitations for real-time applications. 
% In addition, LDMs can compress video representations into a compact latent space while generating high-quality reconstructions efficiently, significantly reducing the required transmission bandwidth. 

}

\subsubsection{LDM-Aided Video Streaming}
{
Danier \textit{et al.} in \cite{danier2024ldmvfi} presented LDMVFI, leveraging high-fidelity image synthesis capabilities of DMs for video frame interpolation (VFI). LDMVFI generates dynamic frames based on I-frames and incorporates a vector-quantized auto-encoding model, VQ-FIGAN, to enhance VFI performance.
%
Yu \textit{et al.} \cite{yu2024efficient} introduced Content-Motion LDM, which decomposes a video into a content frame (image-like) and a low-dimensional motion latent representation. 
%
Yu \textit{et al.} \cite{yu2023video} proposed the Projected Latent Video Diffusion Model (PVDM), consisting of an autoencoder stage and a diffusion model stage. The autoencoder employs one latent vector to capture common content (e.g., background) while using two additional vectors to encode motion. By utilizing a 2D image-like latent space, PVDM avoids the computational overhead of traditional 3D convolutional networks, relying instead on a 2D convolution-based diffusion model.
%
Ma \textit{et al.} in \cite{ma2025diffusion} presented DiffVC, a diffusion-based perceptual neural video compression framework that effectively integrates a foundational diffusion model with the video conditional coding paradigm. This framework leverages temporal context from previously decoded frames and the reconstructed latent representation of the current frame to guide LDMs in generating high-quality images.
% Authors in \cite{wu2024promptus} propose Promptus, a method that converts frame images into inverse prompts to improve frame generation in  using stable diffusion models. However, it is challenge to generate the reconstruction losand perceptual loss between the generated current frame and real current frame under multi-channel streaming.
%
% Authors in \cite{ma2024latte} propose Latte, a novel LDM-based Transformer. Latte extracts spatio-temporal tokens from input videos and employs Transformer blocks to model video distributions in latent space. To efficiently handle the substantial number of tokens, four optimized variants are introduced, decomposing the spatial and temporal dimensions of input videos.
%
Despite their advancements, these approaches do not consider I-frames and B-frames, relying solely on consecutive frames. This design choice may lead to incorrect I-frame predictions due to improper conditioning, while also reducing training efficiency and increasing bandwidth usage, particularly under complex wireless transmission conditions.
}


\section{Real time Latent Diffusion Adaptive-Bitrate Video Streaming Framework } \label{proposedmethod}

{ In this section, we will first propose a channel-aware bitrate selector that automatically decides the bitrate of a video stream for the next chunk according to the channel conditions, semantic performance requirements, and users' choices in Subsection~\ref{bitrateselector}. Then, we will introduce the remaining compression, denoising, and reconstruction parts of the latent diffusion adaptive bitrate video streaming (LD-ABS) framework with the selected video resolution to transmit the video frame-by-frame in Subsection~ \ref{subframework}.   }


\begin{figure*}
\includegraphics[width=1\linewidth]{images/abr_nn.png}
\caption{Proposed Next Chunk Bitrate Selection for Adaptive Bitrate Streaming Framework}
\label{fig:abr}
\end{figure*}


\begin{figure*}
\includegraphics[width=1\linewidth]{images/Framework.png}
\caption{The diagram illustrates the structure of the proposed framework for LDM-enabled real-time video streaming, where $\bm{z}$ is the semantic latent features of video frames $\bm{x}$ encoded by VAE, $T$ represents the total number of forward steps of DM, $t$ denotes the $t$-th step, $n$ represents the $n$-th frame of the video stream with total $N$ frames, $\bm{\theta}$ is the parameters of the DMâ€™s denoising U-Net neural network \cite{pei2024latent}, $r$ represents the starting point for denoising received key frame semantic latent feature signals in the wireless channel, which is related to the channel state, $s$ represents the starting step for fine-tuning non-key frame semantic latent feature, and $\bm{c}$ is the condition for the conditional LDM, i.e., the small-size meta-data being transmitted. Furthermore, $q(\cdot|\cdot)$ denotes the forward process of the DMs, and $p_{\bm{\theta}}(\cdot|\cdot)$ represents the reverse denoising process. }
% \label{fig:framework}
\vspace{-0.4cm}
\label{fig:architecture}
\end{figure*}



% \subsubsection{Workflow of LDM aid Rate adaptive Video Streaming}

% In this section, we outline the workflow for a LDM aid Rate adaptive video streaming framework. As shown in Fig.1  

\subsection{Channel-Aware Video Streaming Bitrate Selector} \label{bitrateselector}


 

In practical scenarios, an edge user typically sends an HTTP GET request to request a streaming service. Overall, ABS utilizes CNN-GRU model to select the bitrate as the output of neural networks (NN) for the next chunk and the proposed ABS selector diagram is depicted in Fig. \ref{fig:abr}. We divide ABS selector into the following three sub-units, each corresponding to rendering streaming video \cite{kan2022improving}:

\begin{itemize}
\item \textbf{Wireless Channel Resource Estimation:} 
This sub-unit estimates the variations of the noisy physical wireless channel. A channel state detector within the wireless communication network identifies the channel status and forwards this information to the neural network.

\item \textbf{Experience Buffer:} 
Inspired by offline Reinforcement Learning (RL) techniques, we use a buffer to store past expert strategies, allowing the algorithm to sample randomly from the buffer pool during the training process.

\item \textbf{Neural Network:} For each episode $e$, the ABS learning agent determines a suitable bitrate for the next chunk via a neural network (NN). The input features of this NN in $k$-th chunk are ($ S_k = \{ C_k, M_k, F_k \}$) and are detailed as follows: \textbf{\textit{1) Past Wireless Channel Estimate:}} The learning agent uses the past $t$ chunks' channel status vector $C_k = \{ c_{k-t}, \dots, c_k \}$ as input to the NN, where $c_i$ represents the channel throughput for video chunk $i$. \textbf{\textit{2) Latent Content:}} To detect the diversity of video content, the learning agent uses $M_k = \{ N_{k+1}, V_{k+1} \}$, where $N_{k+1}$ and $V_{k+1}$ denote the size of each bitrate of the next chunk $(k+1)$ and the perceptual quality metrics (semantic ambiguities) for each bitrate of the next chunk, respectively. \textbf{\textit{3) Player Rollout Playback:}} Rollout information is collected by the player on the edge user's side and is defined as $F_k = \{v_{k-1}, B_k, D_k, m_k\}$. Here, $v_{k-1}$ represents the video quality of the last video chunk selected, while $B_k$, $D_k$, and $m_k$ represent the buffer utilization,  loading time, and the normalized remaining chunks, respectively, for the past $t$ chunks.
\end{itemize}

Ultimately, the learning agent then selects the next chunk bitrate. The NN architecture includes a 1-dimensional CNN (1D-CNN), a fully connected 128-dimensional layer (FC-128), and GRUs that output 128-dimensional vectors (GRU-128). We use ReLU as the activation function and softmax for the last layer.  Consequently, as a part of LD-ABS, the ABS selector can mitigate the QoE loss challenges discussed in Section \ref{problemformulation} by selecting the optimal bitrate, thereby rationalizing bandwidth and storage consumption based on available resources, channel conditions, and QoS requirements.
 






\subsection{LDM-enabled Compression, Denoising, and Recontruction }
\label{subframework}

For the remainder of LD-ABS, to achieve higher compression of streaming video, we consider using LDM to compress the original reference frames into latent reference I frames. Meanwhile, we only retain the adjustment metadata for the predictive B and P frames. On the user side, latent reference frames are denoised to restore reference frames, and predictive frames are restored using conditional LDM and VFI. Recognizing the challenges faced by LDM-enabled real-time adaptive multi-band streaming, we propose a collaborative end-to-end LDM-enabled streaming framework. This framework aims to deliver high-resolution, low-latency streaming video while conserving communication bandwidth and storage with adaptive time-varying appropriate bitrate. Specifically, the proposed framework involves the following key components:

% \subsubsection{Decode the video into I-frames, P-frames, and B-frames via FFmpeg}
\textbf{(i)} \textbf{Decode the video into I-frames, P-frames, and B-frames via FFmpeg} 

 In the beginning, We utilize FFmpeg to decode the streaming videos into their respective frame types in Fig. \ref{fig:architecture}, Step \scalebox{1.2}{\ding{172}}. It is crucial that each frame maintains accurate timestamps, which ensures that the streaming video can be reconstructed accurately in the final step. To achieve higher compression performance and reduce bandwidth usage, we do not compress each frame individually. Since I-frames are larger compared to other frames, we use the VAE in the LDM to compress I-frames into semantic latent features. B-frames and P-frames are left uncompressed, as compressing them would still consume significant computational resources. Moreover, these frame sizes are relatively small because they only store the differences from the previous or next frame.


\textbf{(ii) Semantic Encoding for I-Frames compression and B/P-Frames motion vector compression} 

In this phase, we utilize the VAE of LDM to compress I-frames, as depicted in Fig. \ref{fig:architecture}, Step \scalebox{1.2}{\ding{173}}. Since I-frames are comparatively larger than B-frames and P-frames, we compress I-frames to reduce bandwidth and storage requirements, thereby reducing the load on the CDN. The process involves using a key-frame VAE to compress I-frames into semantic Latent I-frames (L-I-frames) through the LDM process (Red VAE in Fig. \ref{fig:architecture}. On the other hand, B-frames and P-frames motion vectors and adjustments are encoded via the non-key-frame VAE (Blue VAE in Fig. \ref{fig:architecture}. To this end, it is no longer necessary to transmit the complete B/P-frames; instead, only the compressed L-I-frames and compressed motion vectors need to be transmitted, overcoming the bottleneck of high bandwidth and cache storage requirements discussed in Section \ref{problemformulation}.



\textbf{(iii) Transmitting semantic latent features  through noisy wireless channel}
% Transmitting Frames Through Wireless Channels

As depicted in Fig. \ref{fig:architecture}, streaming video is sent to the edge user side via ultra-reliable low-latency communications (URLLC). The fluctuating wireless environment provides an unstable communication bandwidth and channel degradation without prior awareness. ABS must first provide smooth, non-interruptive, low-latency service to satisfy QoS, and then aim to provide high bitrate streaming to satisfy QoE. With a constant channel-bandwidth-ratio (CBR), the transmitter selects the appropriate bitrate based on the wireless environment, as different bitrate streams require distinct latent spaces to store latent frames. 

Furthermore, the 5G environments may introduce random channel noises and semantic errors during latent feature transmission processes, which can lead to incomplete or corrupted information delivery to the edge user side. As a result, not all frames in a streaming video are successfully or perfectly transmitted. Specifically, the transmission of L-I-frames, as well as the encoding information from B-frames and P-frames, may be affected by channel noises, attenuation and other uncertainties, further impacting the quality and integrity of the received video stream. Base Stations (BSs) and the proposed ABS mechanism process and schedule these requests, ensuring efficient resource allocation and maintaining QoE, considering computation complexity and streaming video quality.



\textbf{(iv) Restoring key frames through LDM denoising processes}

As highlighted in Fig.~\ref{fig:architecture}, Step \scalebox{1.2}{\ding{174}}, Upon receiving the L-I-frames $\bm{z}_r^n$, which contain wireless channel noises and errors, we apply a short reverse process to fine-tune the compressed L-I-frames $\bm{z}_r^n$ to restore the precise reference frames  $\bm{z}_0^n$. The fine-tuning step aims to improve the reconstruction quality by completing the channel denoising task. Finally, we pass $\bm{z}_0^n$ through the Red VAE decoder to reconstruct the I-frames.  Since the starting point $r$ for channel denoising adapts to the variations in channel estimation results, the proposed LD-ABS approach enhances the robustness of video transmission against challenges such as cahnnel noises, gain attenuation, and semantic errors under time-varying wireless network conditions. Furthermore, since the LDM performs the denoising process in a low-dimensional semantic latent space with fewer steps, it achieves a relatively fast reconstruction speed. This helps mitigate the high computational complexity and slow processing typically associated with DMs.

\textbf{(v) Restoring non-key frames through conditional LDM} 

We follow this process to restore non-key frames. After recovering the I-frames $\bm{z}_0^n$, we apply a fine-tuning process based on the recovered latent space for L-I-frames as illustrated in Fig. \ref{fig:architecture}, Step \scalebox{1.2}{\ding{175}}. Specifically, a short forward noise $\bm{n}$ is added to the recovered latent space from the I-frames to determine $\bm{z}^n_s$, which prepares the content for reconstructing the B-frames and P-frames from the I-frames. Next, we apply a reverse diffusion process to denoise and restore the non-key latent features. This process is conditioned on the motion vector encodings of the B-frames and P-frames. Finally, the fine-tuned non-key latent features are decoded using the Red VAE decoder to generate Latent B-frames (L-B-frames) and Latent P-frames (L-P-frames). Since the semantic latent vectors of B/P-frames are fine-tuned based on similar L-I-frames, challenges commonly encountered in FFmpeg technologies, such as frame adjustment and motion estimation, are effectively addressed. Consequently, only the encodings of motion vectors, which serve as the condition $\bm{c}$ for the CLDM, need to be transmitted.


\textbf{(vi) Merging new frames to a new streaming video}

 The latent spaces of I-frames, B-frames, and P-frames are decoded by the key-frame VAE decoder as shown in Fig. \ref{fig:architecture} Step \scalebox{1.2}{\ding{176}}, to reconstruct the corresponding I-frames, B-frames, and P-frames. In the final step, as depicted in Fig. \ref{fig:architecture}, Step \scalebox{1.2}{\ding{177}}, we utilize FFmpeg to merge the newly constructed frames in chronological order to reconstruct the new streaming video. 
 



\section{Numerical Result and Discussion}
\label{casestudy}

\begin{figure}
    \vspace{-1mm}
    \centering
    \begin{tabular}{cc}
        % First row (FCC traces)
        \includegraphics[width=0.45\linewidth]{images/simulation/fcc/avg_QoE3_tf.pdf} &
        \includegraphics[width=0.45\linewidth]{images/simulation/fcc/CDF_QoE_tf.pdf} \\
        (a) & (b) \\
        
        % Second row (3GPP traces)
        \includegraphics[width=0.45\linewidth]{images/simulation/3gp/avg_QoE3_t3g.pdf} &
        \includegraphics[width=0.45\linewidth]{images/simulation/3gp/CDF_QoE_t3g.pdf} \\
        (c) & (d) \\
    
        % Third row (OBOE traces)
        \includegraphics[width=0.45\linewidth]{images/simulation/oboe/avg_QoE3_to.pdf} &
        \includegraphics[width=0.45\linewidth]{images/simulation/oboe/CDF_QoE_to.pdf} \\
        (e) & (f) 
    \end{tabular}
    \vspace{-2mm}
    \caption {Comparison of QoE metrics for different traces: (a)-(b) correspond to FCC traces, (c)-(d) correspond to 3GPP traces, and (e)-(f) correspond to OBOE traces. Specifically, (a),(c),(e) depict trace traffic vs QoE reward, and (b),(d),(f) show the average value of chunk QoE vs CDF.}
    \label{fig:abr_performance}
    \end{figure}
    


\subsection{Experiment setup}
To assess performance of average chunk QoE, consistency, and fast adaptation across a wide range of wireless environments, we evaluate our proposed LD-ABS on a virtual player, which is widely assessing system performance  of video streaming  \cite{kan2022improving}. This player simulates the adaptive video streaming process using real-world network throughput datasets (3G/HSDPA, FCC, OBOE) and allows comparison with other ABS algorithms under various user and network conditions \cite{kan2022improving}.



\subsection{Baseline Algorithms}
We evaluate LD-ABS against the following state-of-the-art ABS algorithms \cite{kan2022improving}:
\begin{itemize}
    \item \textbf{BOLA}: A buffer-based algorithm that employs Lyapunov optimization to select the optimal bitrate version while considering buffer occupancy constraints. %\cite{spiteri2020bola}

    \item \textbf{RobustMPC}: A model-based algorithm that formulates the bitrate selection as an optimization problem over a future horizon of $h$ video chunks using model predictive control. The future throughput is estimated using the harmonic mean of the average throughput measured over the past five downloaded chunks. %\cite{yin2015control}

    \item \textbf{Comyco}: A model-free neural ABS algorithm that leverages neural networks to approximate an offline near-optimal solution through lifelong imitation learning. %\cite{huang2019comyco}

    % \item \textbf{Pensieve}: A DRL-based algorithm utilizing the A3C method to learn an optimal policy for rate adaptation by mapping buffer occupancy, throughput, and chunk size dynamics.

    
\end{itemize}





\subsection{QoE Performance Comparison}

\begin{figure}
\includegraphics[width=1\linewidth]{images/simulation/4algs_compare_bars.pdf}
\caption{Performance evaluation of different ABS algorithms.}
\label{fig:combined_figure}
\end{figure}




 In ABS, the video is temporally divided into $K$ chunks (i.e., segments) of fixed duration $L$. Each chunk is encoded into multiple quality versions at different bitrates, with the set of available bitrates denoted as $\mathcal{A} = \{a_1, \dots, a_M\}$, where $M$ is the total number of bitrate options. $a_k$ represents the bitrate selected for the $k$-th chunk. 
 The Markov Decision Process (MDP) state $s_k \in \mathcal{S}$ for chunk $U_k$ is characterized by six features: (1)~previous chunk average throughput $C_{k-1}$, (2)~corresponding download time $d_{k-1}$, (3)~chunk sizes for all available bitrate versions of the $k$-th chunk, (4)~current buffer occupancy $B_{k-1}$, (5)~selected bitrate $a_{k-1}$ for the previous chunk, and (6)~the remaining number of chunks yet to be downloaded.
{To evaluate user-side QoE, we adopt an objective metric  that balances video quality (bitrate utility), quality fluctuations (smoothness penalty), and playback stalls (rebuffering penalty) through a linear combination \cite{kan2022improving}. The QoE for chunk \( k \) is defined as follows:
\begin{equation}
    \begin{aligned}
        \mathrm{QoE}(s_k, a_k) &= 
        m(a_k) - \alpha | m(a_k) - m(a_{k-1})| \\
        &\quad - \beta \max(0, d_k - B_{k-1})
    \end{aligned}
\end{equation}


where

\begin{itemize}
    \item \textbf{Bitrate Utility}: \( m(a_k) = \log(a_k/\min(\mathcal{A})) \) quantifies video quality perception, where higher bitrates improve visual quality but consume more bandwidth, increasing rebuffering risk.
    \item \textbf{Smoothness Penalty}: \( \alpha | m(a_k) - m(a_{k-1})| \) discourages large fluctuations in video quality between consecutive chunks, ensuring a consistent viewing experience.
    \item \textbf{Rebuffering Penalty}: \( \beta \max(0, d_k - B_{k-1}) \) penalizes playback stalls when the buffer runs out before the next chunk is downloaded.
\end{itemize}

% - **Bitrate Utility** \( m(a_k) = \log(a_k/\min(\mathcal{A})) \) quantifies video quality perception, where higher bitrates improve visual quality but consume more bandwidth, increasing rebuffering risk.
% - **Smoothness Penalty** \( \alpha | m(a_k) - m(a_{k-1})| \) discourages large fluctuations in video quality between consecutive chunks, ensuring a consistent viewing experience.
% - **Rebuffering Penalty** \( \beta \max(0, d_k - B_{k-1}) \) penalizes playback stalls when the buffer runs out before the next chunk is downloaded.
The penalty weights are set as \( \alpha = 1 \) and \( \beta = 2.66 \) to balance smooth transitions and minimize rebuffering delays, following \cite{kan2022improving}.
The bitrate selection \( a_k \) for the \( k \)-th chunk is optimized to maximize the overall QoE based on the tradeoffs above.
% , ensuring a trade-off between high video quality, minimal buffering, and smooth playback.
}
The key observation is that LD-ABS consistently outperforms baseline algorithms in terms of average chunk QoE and bitrate utility. Moreover, the results demonstrate that LD-ABS maintains robust performance across all sessions, with the largest proportion of sessions achieving higher QoE values. 
As shown in Fig.~\ref{fig:abr_performance}(b), which illustrates the cumulative distribution functions (CDFs) of the average QoE across all sessions for different algorithms, at least 95\% of LD-ABS sessions achieve an average QoE greater than zero. 

{ Fig.~\ref{fig:combined_figure} shows that our proposed LD-ABS framework outperforms the testing benchmarks in \cite{kan2022improving}, in terms of chunk QoE, bitrate utility, rebuffering penalty, and smoothness penalty.
 % Fig.~\ref{fig:combined_figure}(b)  demonstrates that the average player throughput remains reasonable across  3 different wireless datasets in terms of different chunk bitrates.
 }
 
 {Fig.~\ref{fig:demo} illustrates a sample streaming simulation within a chunk. This chunk consists of one I-frame followed by a P-frame, another P-frame, and a B-frame. Instead of transmitting the entire frames through a noisy wireless channel, only lightweight adjustment metadata is transmitted. The LDM then reconstructs the respective latent frames based on the recovered latent I frame ($z_o^n$) and the received adjustment metadata, generating the corresponding adjusted frames accordingly.
 }






\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/simulation/diffusion_process_demo_vertical.pdf}
    \caption{Sample streaming video chunk simulation on the proposed LD-ABS framework for frame compression, transmission, and reconstruction in wireless networks.}
    \label{fig:demo}
\end{figure}





\section{Conclusion and Future Direction} \label{conclusion}
%buffereing cache control

%Noise on wireless communication channel 




This paper proposes a novel LDM-aided adaptive bitrate streaming framework to improve the efficiency and quality of real-time video streaming over wireless networks. The LDM-based approach could significantly reduce bandwidth consumption with high perceptual quality by semantic-aware video compression and reconstruction. It combines diffusion-based denoising with VFI to enhance temporal coherence and reduce the impact of noisy and attenuated wireless channels. Our experimental results confirm that the LD-ABS surpasses existing ABS methods in terms of QoE, robustness across sessions, and network adaptability.

While the above results look promising, many challenges are still open for future research. The multi-step reverse diffusion process introduces some latency in reconstructing the video, in particular for longer video sequences. Investigating acceleration methods like knowledge distillation and model quantization is imperative. Second, since current LDMs are only trained within a specific dataset, they might lack the generalization ability across diverse video contents and network conditions. Future work should be devoted to domain adaptation and continual learning to enhance the robustness in real-world streaming scenarios, such as live news broadcasts and sports streaming, as well as vehicular networks.




\bibliographystyle{IEEEtran}
\bibliography{main.bib}
\end{document}


