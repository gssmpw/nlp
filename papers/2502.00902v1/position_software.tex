
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{csquotes}

\usepackage{hyperref}
\usepackage{listings}
\usepackage[frozencache,cachedir=.]{minted}

\usepackage[mode=buildmissing]{standalone} %
\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}


\usepackage{acro}
\acsetup{make-links}


\DeclareAcronym{ml}{
  short = ML,
  long = Machine Learning,
}
\DeclareAcronym{plos}{
  short = PLOS,
  long = Public Library of Science,
}
\DeclareAcronym{icml}{
  short = ICML,
  long = International Conference on Machine Learning,
}
\DeclareAcronym{neurips}{
  short = NeurIPS,
  long = Conference on Neural Information Processing Systems,
}
\DeclareAcronym{pip}{
  short = pip,
  long = package installer for Python,
}
\DeclareAcronym{pypa}{
  short = PyPA,
  long = Python Packaging Authority,
}
\DeclareAcronym{pmlr}{
    short = PMLR,
    long = Proceedings of Machine Learning Research
}
\DeclareAcronym{iclr}{
    short = ICLR,
    long = International Conference on Learning Representations
}
\DeclareAcronym{pep}{
    short = PEP,
    long = Python Enhancement Proposal
}


\definecolor{tabBlue}{HTML}{1f77b4}
\definecolor{tabOrange}{HTML}{ff7f0e}
\definecolor{tabGreen}{HTML}{2ca02c}
\definecolor{tabRed}{HTML}{d62728}
\definecolor{tabPurple}{HTML}{9467bd}
\definecolor{tabBrown}{HTML}{8c564b}
\definecolor{tabPink}{HTML}{e377c2}
\definecolor{tabGray}{HTML}{7f7f7f}
\definecolor{tabOlive}{HTML}{bcbd22}
\definecolor{tabCyan}{HTML}{17becf}

\newcommand{\LV}[1]{{\color{tabBlue} {\bf (LV: #1)}}}
\newcommand{\MW}[1]{{\color{tabGreen} {\bf (MW: #1)}}}



\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{More Rigorous Software Engineering Would Improve
Reproducibility in Machine Learning Research}

\begin{document}

\twocolumn[
\icmltitle{Position: More Rigorous Software Engineering Would Improve
 Reproducibility in Machine Learning Research}



\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Moritz Wolter}{hpca}
\icmlauthor{Lokesh Veeramacheneni}{hpca}
\end{icmlauthorlist}

\icmlaffiliation{hpca}{High-Performance Computing and Analytics Lab, Bonn University, Bonn, Germany}

\icmlcorrespondingauthor{Moritz Wolter}{moritz.wolter@uni-bonn.de}


\icmlkeywords{Machine Learning, Software Engineering, Tests}

\vskip 0.3in
]



\printAffiliationsAndNotice{\icmlEqualContribution} %

\begin{abstract}
Experimental verification and falsification of scholarly work are part of the scientific method's core. To improve the \ac{ml}-communities' ability to verify results from prior work, we argue for more robust software engineering.
We estimate the adoption of common engineering best practices by examining repository links from all recently accepted \acf{icml}, \acf{iclr} and \acf{neurips} papers as well as \acf{icml} papers over time.
Based on the results, we recommend how we, as a community, can improve reproducibility in \acf{ml}-research.
\end{abstract}



\section{Introduction} \label{submission}

Modern science relies on trust, that we partially establish by experimental evidence. Scientists often propose logically coherent hypotheses, then verify them experimentally~\cite{popper2005logic}.
Next to theoretically proven discovery. A large part of \ac{ml}-research rests upon empirical foundations. Consequently, reproducibility is a key concern in \ac{ml} research. \textit{When we fail to enable reproduction by other scientists, this pillar of our scientific structure becomes shaky.} 
This position paper focuses on what we as a community are already doing to allow the reproducibility of our work and where we can still improve.
Since \ac{ml} is a field that is heavily reliant on software,
we draw upon established software engineering best practices,
measure their adoption and argue for more rigorous software engineering in \ac{ml} work.
\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{plots/paper_radar_chart.pdf}
    \caption{The state of software engineering at major 2024 \ac{ml}-conferences. The chart illustrates web-crawled percentages of files and folders which are tied to the adoption of software engineering best practices.}
    \label{fig:radar_chart}
\end{figure}
Figure~\ref{fig:radar_chart} illustrates potential avenues for improvement.
The figure illustrates best practice adoption estimates. We argue that supplementary code facilitates reproduction if it comes with a license, contains a readme with reproduction steps, documents software requirements, and is carefully tested. Figure~\ref{fig:radar_chart} illustrates our community's potential for future growth along many of these dimensions.

\section{Related work}
Previous work has identified missing code or data as an important obstacle
to reproducibility in \ac{ml} \cite{hutson2018artificial}.
More recently, neighbouring computational fields have started to identify software quality
as a second key hindrance \cite{hoyt2023improving}.
In response, the \ac{plos} has proposed a set of guidelines, encouraging its authors to follow 
\cite{Yasset2016TenGit,Andreas2012TenSoftware,Kjetil2013TenReproducible,Lost2017TenUsable}.
Within the machine learning community, the \ac{neurips} has also started to address this issue by proposing its own guidelines \cite{NeuripsCodeguide}.
We will now consider elements of scientific software engineering, with a focus on standardized features, which we can measure.



\subsection{Licensing software}
Providing a link to a public open-source project is not the same as supplying a license. Without a license, the host platform's terms of service apply \cite{opensource2025guide}. On GitHub, for example, the terms allow the public to view and fork the project. Other use is not allowed \cite{opensource2025guide}. 
Therefore, providing a license is a necessary prerequisite. It allows future scientists to build on the existing codebase.\cite{Yasset2016TenGit,opensource2025guide}.

\subsection{Onboarding new users with a README}
Typically, ML repositories do not come with complete project documentation. Providing a README is a first step towards documentation and is considered a good practice.
The README should come with a project description, a quick start guide, and share information on how to contribute \cite{NeuripsCodeguide,Yasset2016TenGit}.
 
Providing LICENCE and README files is a language-agnostic step. We will now focus on the specifics of Python.


\subsection{Python's dominance in \ac{ml}}
Python dominates in the repositories we found in \ac{icml}-papers (Figure~\ref{fig:uses_python}). 
Since major \ac{ml}-frameworks like PyTorch \cite{paszke2017automatic},
Jax \cite{jax2018github} or Tensorflow \cite{tensorflow2015-whitepaper} all provide Python functionality, Python's dominance does not surprise us. Additionally, the \ac{neurips} tips for releasing code \cite{NeuripsCodeguide}, for example, are written with Python in mind.

\subsection{Sharing requirements in Python}
A recommended way to share requirements is the inclusion of a \texttt{requirements.txt} file 
\cite{pip2025docs,NeuripsCodeguide}. The file lists the code library names a project depends on
with optional version numbers. Even though other file names are possible
\cite{pip2025docs}, most code-authors use \texttt{requirements.txt} as the recommended file name. Providing the file should allow others to install all requirements automatically with the \ac{pip}.

\subsection{Freezing conda}
Projects that use conda \cite{conda2025repo} for dependecy management should
provide a \texttt{environment.yml} file \cite{NeuripsCodeguide}.
The file will allow Conda to recreate the software environment in which experiments were conducted automatically. In contrast to \ac{pip}, conda does not rely on a system compiler and delivers binary dependencies directly.

\subsection{Tests in Python}
Automated tests allow users, as well as code authors, to verify that the code works as intended.
This is important for new users to get started. Here, successful test runs indicate that everything is set up correctly \cite{Lost2017TenUsable}.
During development, automated tests facilitate code verification. Primarily, they allow us
to check new ideas before integrating these into the core codebase \cite{Yasset2016TenGit}.
Last but not least, the test documents core development assumptions and the key behaviour of the code.
When other groups or new people in the same group join a project, the tests help them ensure older features remain operational as new functionality is added. 
In the python-world \texttt{pytest} \cite{pytest2025docs} provides a functional and
\texttt{unittests} \cite{unittest2025pythondocs} a more object-oriented approach to testing.
Regarding test organization, the \ac{pypa} recommends a \texttt{tests} folder \cite{packaging2025python},
which contains the test files. The \texttt{tests} folder should be in the root directory of the project.
Within the Jax ecosystem, Chex~\cite{chex2025docs} offers a very elegant way to automatically evaluate individual tests in different \ac{ml}-specific settings.
Its variants decorator automatically tests a function's compatibility with just-in-time compilation, device mapping and parallelization.

\subsection{Packaging Python code}\label{sec:packaging}

Testing ensures quality and reproducibility within the original code-base.
However, most community members are working on their own projects and would reuse code-modules or individual functions from others. Copying code from the original repository is a simple way to reuse code. Unfortunately, this approach leads to code duplication within the community,
with all the problems that come with it \cite{Yasset2016TenGit}. 
A prominent problem, for example, is that once a bug is identified, it must be manually fixed for every duplication. To combat duplication, we require a system that makes it simple to install external dependencies as a library. This is where packaging comes in.
A packaged Python project can be automatically installed via \ac{pip}.
The process makes it available without introducing duplication through Python imports.
The process is straightforward, a packaged project should have \texttt{src} and \texttt{tests}
folders \cite{packaging2025python}, additionally \ac{pip} needs to know about requirements
and other project metadata. Such information is typically provided by a  \texttt{pyproject.toml} file. 
When authors use \texttt{setuptools} \cite{setuptools2025docs} to package their code,
a \texttt{setup.py} or \texttt{setup.cfg} will appear. 

\subsection{Testing packaged code}

Packaging allows easy installation of code. To guarantee that results are reproducible
we can modify the test setup to test our code \textit{and} our dependencies-specification.
We argue that package testing is desirable from a reproducibility point of view, because
it allows authors to test their code package from a user perspective. 
The test process requires a tool that creates a virtual environment, installs the package,
and runs the tests.
Common choices in the python world are tox \cite{tox2025docs} or
nox \cite{nox2025docs}. Using tox requires us to set up a \texttt{tox.toml}, or \texttt{tox.ini} file.
For nox we need a \texttt{noxfile.py}. Both files should appear in the root directory of the project.

\subsection{Fixing seeds}
We require pseudorandom initialization to initialize network matrices before we begin experimenting. Neural network optimization is typically not a convex problem. In other words, there is no guarantee that the optimizer will find the global optimum from any starting point.
In the interest of reproducibility, we must record the seed value we feed into the random number generator. After all, the seed value determines the starting point of the optimization process. Jax \cite{jax2018github} makes the state of the pseudorandom number generator explicit by introducing a unique key object. To generate keys, programmers must provide seed values by default. Their approach solves a lot of reproducibility issues.
PyTorch users can optionally choose to set seed values \cite{PyTorch2024randomness}. In the interest of reproducibility, this position paper argues we should make that a habit in all our scientific work.


Overall, the related work suggests,
use versioning for code and data \cite{Kjetil2013TenReproducible,NeuripsCodeguide},
fix seeds whenever pseudorandom numbers are involved \cite{Kjetil2013TenReproducible},
automate for continuous integration \cite{Yasset2016TenGit}, 
document dependencies \cite{NeuripsCodeguide,Andreas2012TenSoftware},
and to provide a README \cite{NeuripsCodeguide,Yasset2016TenGit} as well as LICENSE file \cite{Yasset2016TenGit}.


\section{An estimate of the software ecosystem state at \acs{icml} from 2014 until 2024}\label{sec:estimate_eng_icml}
We have surveyed the literature and identified a set of engineering best practices, which
could aid us in improving reproducibility in \ac{ml} research. 
This section systematically looks for signs of best practice adoption within the software repositories shared in \ac{icml} papers from 2014 until 2024. 

\begin{table*}
    \centering
    \caption{ICML conference year and the corresponding proceedings number we used for the webcrawl.}
    \label{tab:icml_pmlr}
    \begin{tabular}{l r r r r r r r r r r r } \toprule
    \ac{icml}-year    &2024 & 2023 & 2022 & 2021 & 2020&  2019& 2018&  2017&  2016&  2015&  2014 \\ 
    \acs{pmlr}-volume & 235 &  202 &  162 &  139 &  119&    97&   80&    70&    48&    37&    32  \\ \bottomrule
    \end{tabular}
\end{table*}

Our crawling process starts at the \acf{pmlr}-website. We create links to crawl by combining the 
\ac{pmlr}-processings website with the \ac{icml} volume number for each year as shown in Table~\ref{tab:icml_pmlr}.

Using beautiful-soup \cite{richardson2023soupdocs}, we extract all links to papers and supplementary material,
by filtering for links ending with \texttt{pdf}. In a next step we download the pdfs and extract all 
links to public GitHub pages using pdfx \cite{Hager2021pdfx}. 
After downloading a beautiful-soup representation of all GitHub pages, 
we isolate repository pages by removing all pages without a branch-picker.
\begin{table*}
\centering
\caption{Valid crawled repository links in all accepted papers per year at the \ac{icml} }
\label{tab:active_repository_pages}
\begin{tabular}{l r r r r r r r r r r r }\toprule
\ac{icml}-year          & 2024  & 2023 &  2022 & 2021  & 2020    & 2019  &   2018  & 2017  & 2016 & 2015 & 2014 \\
active-repository-links & 1008  &  699 &   405 &  326 & 399   & 263 &  153  & 56  & 42 & 28 & 12    \\ \bottomrule
\end{tabular}
\end{table*}
Table~\ref{tab:active_repository_pages} lists the number of active repositories per conference year we can crawl in 2025.

Based on the beautiful-soup representations of the repository roots,
we search for files (
\texttt{requirements.txt},
\texttt{noxfile.py},
\texttt{LICENSE},
\texttt{README.md},
\texttt{README.rst},
\texttt{tox.toml},
\texttt{tox.ini},
\texttt{setup.py},
\texttt{setup.cfg},
\texttt{pyproject.toml},
\texttt{environment.yml}
) and folders
(
\texttt{test},
\texttt{tests},
\texttt{.github/workflows}
)
which allow us to estimate the adoption of software engineering best practices.

In the interest of reproducibility, we provide the code we wrote for this analysis at \url{https://github.com/BonnBytes/position_we_need_more_tests_in_ml}.

When looking at these numbers it is important to keep in mind that we are browsing the repositories in their
modern 2025 state. We have no information on the state of the repositories at the time of each conference.
We should, therefore, interpret these numbers with caution. Our methodology works, however, for one-time code releases,
which are very common in the \ac{ml}-community.

\subsection{Licensing and readme files over time.}
\begin{figure}
    \includestandalone{./plots/icml_license_and_readme}
    \caption{\ac{icml} repository link crawl results for \texttt{LICENSE} and \texttt{README} files. The crawl includes accepted \ac{icml}-papers from 2014 (14) until 2024 (24).
    To save legend space, we use two-digit numbers for years.
    We add the counts for \texttt{README.md} and \texttt{README.rst} files, and show both as \texttt{README}.}
    \label{fig:license_and_readme}
\end{figure}
Figure~\ref{fig:license_and_readme} shows the adoption of license and readme files
in software repositories over time. Fortunately, the trend in terms of readme files
is up! In 2024, 95\% of repositories had a readme.
Unfortunately, the share of license files stagnates at around 50\% over time, perpetuating the legal problems described in \cite{opensource2025guide}.


\subsection{The adoption of Python at ICML over time.}
\begin{figure}
    \includestandalone{./plots/icml_uses_python}
    \caption{ICML repository link crawl results for Python adoption.
             We count each repository where python is listed as a langauge
             in GitHub's language-box.}
    \label{fig:uses_python}
\end{figure}
Figure~\ref{fig:uses_python} shows the adoption of Python over time.
The trend is pointing upwards, in 2014 only 58.3\% of repositories used Python,
its share has climbed to 90.4\% in 2024.
We believe this development bolsters the case for more rigorous software engineering.
Since fewer language barriers exist, packaging especially, would help us to
collaborate more effectively as a community.

\subsection{Requirements documentation}
\begin{figure}
    \includestandalone{./plots/icml_requirements}
    \caption{Estimated dependecy documentation over time. Is is also possible to share
             dependencies via \texttt{setup.py} or \texttt{pyproject.toml} files. Therefore
             this plot should be considered jointly with Figure~\ref{fig:packaging_icml},
             which looks at packaging.}
    \label{fig:requirements}
\end{figure}
Figure~\ref{fig:requirements} shows the share of repositories with
\texttt{requirements.txt} and \texttt{environment.yml} at \ac{icml} over time.
The numbers leave much to be desired. In 2024, for example, the
overall share of Python repositories was 90.4\%. 
Unfortunately, the combined share of projects with \texttt{requirements.txt} and \texttt{environment.yml}
files was only 39.2\%. This number implies that the results of many projects will not be reproducible straightforwardly. We see a small boom in 2020, when the \ac{neurips} code guide appeared  \cite{NeuripsCodeguide}, which asks authors to provide these files, more files start to appear. However, far too many repositories are missing requirements documentation. Especially because it is possible to provide both files automatically with a single command each.
Pip users can run
\begin{minted}{bash}
    pip freeze > requirements.txt
\end{minted}
to create the file. Similarly, for conda users providing the file requires typing
\begin{minted}{bash}
    conda env export > environment.yml
\end{minted}
into the terminal. Afterwards both groups can simply commit the files to their code repositories. 
Providing these files is easy! We can do better!

\subsection{Packaging adoption}
\begin{figure}
    \includestandalone{./plots/icml_packaging}
    \caption{Estimated Python packaging adoption over time.}\label{fig:packaging_icml}
\end{figure}
Packaging as described in section~\ref{sec:packaging} is an elegant way to improve code reusability.
This section discusses our estimates of Python code packaging at \ac{icml} over time.
Figure~\ref{fig:packaging_icml} illustrates that the numbers stagnate at around 20\% for 
\texttt{setup.py}-files and 5\% for \texttt{pyproject.toml}-files.
The ratio is somewhat surprising, given that \ac{pep}-518 recommends the use of \texttt{pyproject.toml} files over \texttt{setup.py} files
since 2016 \cite{Cannon2016pep518}. The \ac{pep} also outlines the case against \texttt{setup.py}.
In any event, packaging with \texttt{setup.py} is still better than doing nothing at all,
which is what most of the community is doing. Overall, summing up the numbers for \texttt{setup.py} and \texttt{pyproject.toml}, roughly a quarter of code releases are packaged. These numbers suggest that we have room to grow!


\subsection{Test and workflow use}
\begin{figure}
    \includestandalone{./plots/icml_tests}
    \caption{Systematic test adoption over time. The numbers for tox combine counts of \texttt{tox.ini} and \texttt{tox.toml},
    both of which are valid filenames.}\label{fig:tests_icml}
\end{figure}

Finally we turn our attention to containerized testing and workflow automation.
Figure~\ref{fig:tests_icml} illustrates our estimate of the adoption of both methods
in the \ac{icml} community over time. Since containerized testing requires a correct
specification of dependencies, we expect to see lower numbers.
The numbers from the workflow column at the right of Figure~\ref{fig:tests_icml}, measure
the shape of repositories with a \texttt{.github/workflows} folder.
The small numbers suggest that the community works largely without automated testing and workflow automation, even though most projects are team efforts with multiple authors. Automated testing makes teamwork easier. No one wants to be the one that breaks to build. Similarly, we don't want to ask each other to run the tests constantly.
An automated workflow allows us to delegate these tasks to a machine, which we argue is in everyone's best interest. 


\section{An estimate of the Software Engineering at major 2024-\ac{ml} conferences}
We use the OpenReview API to download all accepted papers from \ac{iclr} and \ac{neurips} in 2024. We then repeat the process described in section~\ref{sec:estimate_eng_icml}. We include 
numbers from \ac{icml}-2024 for comparison. The numbers are based on 1008 code-repositories for \ac{icml}, 898 for \ac{iclr}, and 1568 repos we found in accepted \ac{neurips} papers.


\subsection{Licensing and readme}
\begin{figure}
    \includestandalone{./plots/ml_in_2024_license_and_readme}
    \caption{2024 \ac{ml}-conference paper repository link crawl results for \texttt{LICENSE} and \texttt{README} files.
    We added the counts for \texttt{README.md} and \texttt{README.rst} files.}
    \label{fig:ml24_license_and_readme}
\end{figure}
Figure~\ref{fig:ml24_license_and_readme} shows the adoption of licensing and README files over time.
Surprisingly \ac{neurips} has the lowest share of repositories with a license file, even though
the conference organizers commendably provide the \ac{neurips} code release guidelines \cite{NeuripsCodeguide}, which ask authors to provide a license file.
\ac{iclr} has the best overall numbers.

\subsection{The adoption of Python at major \ac{ml} conferences}
\begin{figure}
    \includestandalone{./plots/ml_in_2024_uses_python}
    \caption{2024 \ac{ml}-conference paper repository link crawl results for Python adoption.
             We count each repository where Python is listed as a language
             in Github's language box.}
    \label{fig:ml24_uses_python}
\end{figure}
Figure~\ref{fig:ml24_uses_python} reveals that around 90\% of the repository links at major conferences in 2024 contain Python code.

\subsection{Requirements documentation}
\begin{figure}
    \includestandalone{./plots/ml_in_2024_requirements}
    \caption{2024 \ac{ml}-conference repository link shares with \texttt{requirements.txt}
    and \texttt{environment.yml} files.
    The NeurIPS call for papers asks authors to provide these files \cite{NeuripsCodeguide}.}
    \label{fig:ml24_requirements}
\end{figure}
Figure~\ref{fig:ml24_requirements} counts \texttt{requirements.txt} and \texttt{envorinment.yml} files. The \ac{neurips} conference organizers have explicitly communicated \textit{tips for releasing research code}~\cite{NeuripsCodeguide} in their call for papers, which includes a call for authors to include both files. Unfortunately, these exemplary efforts do not lead to a higher share of these files than the file counters in code repositories at \ac{icml} or \ac{iclr}. Once more, the \ac{iclr} appears slightly ahead of its peer venues.

\subsection{Packaging adoption}
\begin{figure}
    \includestandalone{./plots/ml_in_2024_packaging}
    \caption{Python-Packaging indicators at major 2024 \ac{ml} conferences.}
    \label{fig:ml2024_packaging}
\end{figure}
We compare the ratio of Python files related to packaging in Figure~\ref{fig:ml2024_packaging}. Again, we see the best numbers at \ac{iclr}. However, the numbers indicate that 80\% of all repository authors still choose not to package their code.

\subsection{Test and workflow use}
\begin{figure}
    \includestandalone{./plots/ml_in_2024_tests}
    \caption{Systematic test adoption over at major 2024 \ac{ml} conferences.}
    \label{fig:ml2024_test_workflow}
\end{figure}
The overall picture repeats itself once more when we consider test-folder counts, automated test-tool adoptions and workflows in Figure~\ref{fig:ml2024_test_workflow}. We see slightly more adoption at \ac{iclr}, but the numbers are low overall.


\section{How can we do better?}
When submitting or reviewing experimental work with code, we could ask ourselves a couple of key questions. A similar procedure has previously been proposed for chemoinformatics \cite{hoyt2023improving}.
\begin{itemize}
    \item Does the code have a license file?
    \item Is a readme included at the repository root?
    \item Does the project facilitate automatic installation by others via packaging? It's never too late to do this! When deadline pressure prevents us from spending the time early, this can still happen between paper acceptance and conference presentation.
    \item Is it possible to run tests automatically? Did we test our external code dependencies by running a containerized set of tests?
    \item Are all pseudorandom number generator seed values fixed?
    \item Did automatic code checkers like flake8 or MyPy (if we are using type annotations) report any problems?
\end{itemize}

Sometimes, there are good reasons for a no, which can be an acceptable answer.
A mindless test crusade risks triggering authors to withhold code altogether. To improve the current situation, we should ask code authors these or similar questions more frequently when reviewing and read the author's answers with an open mindset. If we notice that engineering best practices are not observed, we should share relevant sources from the Python community with the authors.

\section{Alternative views: In artificial neural network development, systematic software engineering adds bureaucracy and slows us down}
Mindlessly enforcing software engineering practices when they don't help authors would be harmful. \textit{A foolish consistency is the hobgoblin of little minds} \cite{pep8,emerson1841self}, we should limit ourselves to cases where systematic tests are efficient and valuable.

We need the ability to accept brilliant new mathematical ideas to conferences, even if there is no accompanying code or the code is imperfect. Good theory does not require code to be valid.

Furthermore, automatic tests become expensive very quickly when we evaluate artificial neural networks as part of the tests. The situation is even trickier for neural network training code, which is costly and often takes long to run.
Testing neural networks is difficult. We often lack clearly defined outputs. At the same time, many \ac{ml}-codes have non-deterministic properties, which makes them hard to test.
Furthermore, automatic tests often become too large for online workflows, and users get feedback too late. After all, speed is crucial since groups compete with each other. Forgoing testing, dependency documentation, and packaging initially saves time, which produces a competitive advantage since skipping systematic testing will allow projects to be finished quicker and papers to appear earlier.



\section{Conclusion}
This section concludes our call to action for more rigorous software engineering in machine learning.
In the previous section, we argued that forgoing testing and dependency documentation,for example, saves time. This is probably
the case for small, short-term projects. After all, there must be a reason why we did not find more test indicators in \ac{ml}-project repository links. We want to use this opportunity to make the case for more rigorous research software engineering. 
For the following two reasons, we firmly believe that software engineering in research saves time and reaps benefits in the long run.
The first reason is that scientists typically continue to work in a similar direction for years.
Research is split into smaller sub-projects, which are published along the way.
The sub-projects build on top of each other. Building a well-tested and packed firm foundation saves time because it reduces the need to work around bugs.  
Systematic testing and packaging act as an early warning and control system. The former allows us to identify problems early on and later keeps code duplication in check. This ensures that a bug, once fixed, is gone for good. Bugs in the evaluation code, for example, can invalidate entire projects. 
Moreover, early testing of our code protects us from embarrassing retractions later. 
As the network runtimes become prohibitively long, we focus our automated testing on the data pre-processing pipeline. After all, in typical conference paper projects, the gradient computation code often comes from large, carefully tested external frameworks. Consequently, we are more likely to find bugs in the preprocessing than in the network. Doing so will save us time because it is improbable to successfully train an artificial neural network on top of a buggy foundation.

Our second reason is the argument that science is a team effort. As team members come and go, new team members must reproduce the numbers from the previous generation. If the last generation chose to skip dependency documentation and testing, it has accumulated technical debt, which comes due at this point. With PyTorch, for example, \enquote{reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms} \cite{PyTorch2024randomness}. To reproduce results, the environment in which PyTorch ran needs to be documented. If that does not happen, our only option is to start guessing version numbers or redesigning prior work. Both activities are an avoidable waste of time and effort.

Theoretical mathematical papers which rest their claim to validity on theoretical proofs are unlikely to come with repository links. It is unlikely that we see links from that group of papers in our data. Therefore, we believe the low numbers for software engineering best practice indicators can and should improve. We, for example, reuse our software all the time. This is why we believe good code quality is in our best interest. This is probably the case for many in the community. Let's make our own lives better by improving our code.

\section*{Impact Statement}
This paper studies the state of adoption of software engineering best practices at major \ac{ml} conferences. We believe our work positively impacts the community by alerting them to methods that allow everyone to grow as a developer. We strongly believe this improves scientific research by prioritizing quality. 

\bibliography{position_software}
\bibliographystyle{icml2025}

\newpage
\appendix
\onecolumn
\section{Appendix}

\printacronyms

\begin{figure*}
    \includestandalone{./plots/icml_plot}
    \caption{ICML paper-link crawl results for converence papers from ICML-2014 
             up until ICML-2024 in a single figure.}
\end{figure*}

\end{document}
