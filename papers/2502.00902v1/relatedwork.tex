\section{Related work}
Previous work has identified missing code or data as an important obstacle
to reproducibility in \ac{ml} \cite{hutson2018artificial}.
More recently, neighbouring computational fields have started to identify software quality
as a second key hindrance \cite{hoyt2023improving}.
In response, the \ac{plos} has proposed a set of guidelines, encouraging its authors to follow 
\cite{Yasset2016TenGit,Andreas2012TenSoftware,Kjetil2013TenReproducible,Lost2017TenUsable}.
Within the machine learning community, the \ac{neurips} has also started to address this issue by proposing its own guidelines \cite{NeuripsCodeguide}.
We will now consider elements of scientific software engineering, with a focus on standardized features, which we can measure.



\subsection{Licensing software}
Providing a link to a public open-source project is not the same as supplying a license. Without a license, the host platform's terms of service apply \cite{opensource2025guide}. On GitHub, for example, the terms allow the public to view and fork the project. Other use is not allowed \cite{opensource2025guide}. 
Therefore, providing a license is a necessary prerequisite. It allows future scientists to build on the existing codebase.\cite{Yasset2016TenGit,opensource2025guide}.

\subsection{Onboarding new users with a README}
Typically, ML repositories do not come with complete project documentation. Providing a README is a first step towards documentation and is considered a good practice.
The README should come with a project description, a quick start guide, and share information on how to contribute \cite{NeuripsCodeguide,Yasset2016TenGit}.
 
Providing LICENCE and README files is a language-agnostic step. We will now focus on the specifics of Python.


\subsection{Python's dominance in \ac{ml}}
Python dominates in the repositories we found in \ac{icml}-papers (Figure~\ref{fig:uses_python}). 
Since major \ac{ml}-frameworks like PyTorch \cite{paszke2017automatic},
Jax \cite{jax2018github} or Tensorflow \cite{tensorflow2015-whitepaper} all provide Python functionality, Python's dominance does not surprise us. Additionally, the \ac{neurips} tips for releasing code \cite{NeuripsCodeguide}, for example, are written with Python in mind.

\subsection{Sharing requirements in Python}
A recommended way to share requirements is the inclusion of a \texttt{requirements.txt} file 
\cite{pip2025docs,NeuripsCodeguide}. The file lists the code library names a project depends on
with optional version numbers. Even though other file names are possible
\cite{pip2025docs}, most code-authors use \texttt{requirements.txt} as the recommended file name. Providing the file should allow others to install all requirements automatically with the \ac{pip}.

\subsection{Freezing conda}
Projects that use conda \cite{conda2025repo} for dependecy management should
provide a \texttt{environment.yml} file \cite{NeuripsCodeguide}.
The file will allow Conda to recreate the software environment in which experiments were conducted automatically. In contrast to \ac{pip}, conda does not rely on a system compiler and delivers binary dependencies directly.

\subsection{Tests in Python}
Automated tests allow users, as well as code authors, to verify that the code works as intended.
This is important for new users to get started. Here, successful test runs indicate that everything is set up correctly \cite{Lost2017TenUsable}.
During development, automated tests facilitate code verification. Primarily, they allow us
to check new ideas before integrating these into the core codebase \cite{Yasset2016TenGit}.
Last but not least, the test documents core development assumptions and the key behaviour of the code.
When other groups or new people in the same group join a project, the tests help them ensure older features remain operational as new functionality is added. 
In the python-world \texttt{pytest} \cite{pytest2025docs} provides a functional and
\texttt{unittests} \cite{unittest2025pythondocs} a more object-oriented approach to testing.
Regarding test organization, the \ac{pypa} recommends a \texttt{tests} folder \cite{packaging2025python},
which contains the test files. The \texttt{tests} folder should be in the root directory of the project.
Within the Jax ecosystem, Chex~\cite{chex2025docs} offers a very elegant way to automatically evaluate individual tests in different \ac{ml}-specific settings.
Its variants decorator automatically tests a function's compatibility with just-in-time compilation, device mapping and parallelization.

\subsection{Packaging Python code}\label{sec:packaging}

Testing ensures quality and reproducibility within the original code-base.
However, most community members are working on their own projects and would reuse code-modules or individual functions from others. Copying code from the original repository is a simple way to reuse code. Unfortunately, this approach leads to code duplication within the community,
with all the problems that come with it \cite{Yasset2016TenGit}. 
A prominent problem, for example, is that once a bug is identified, it must be manually fixed for every duplication. To combat duplication, we require a system that makes it simple to install external dependencies as a library. This is where packaging comes in.
A packaged Python project can be automatically installed via \ac{pip}.
The process makes it available without introducing duplication through Python imports.
The process is straightforward, a packaged project should have \texttt{src} and \texttt{tests}
folders \cite{packaging2025python}, additionally \ac{pip} needs to know about requirements
and other project metadata. Such information is typically provided by a  \texttt{pyproject.toml} file. 
When authors use \texttt{setuptools} \cite{setuptools2025docs} to package their code,
a \texttt{setup.py} or \texttt{setup.cfg} will appear. 

\subsection{Testing packaged code}

Packaging allows easy installation of code. To guarantee that results are reproducible
we can modify the test setup to test our code \textit{and} our dependencies-specification.
We argue that package testing is desirable from a reproducibility point of view, because
it allows authors to test their code package from a user perspective. 
The test process requires a tool that creates a virtual environment, installs the package,
and runs the tests.
Common choices in the python world are tox \cite{tox2025docs} or
nox \cite{nox2025docs}. Using tox requires us to set up a \texttt{tox.toml}, or \texttt{tox.ini} file.
For nox we need a \texttt{noxfile.py}. Both files should appear in the root directory of the project.

\subsection{Fixing seeds}
We require pseudorandom initialization to initialize network matrices before we begin experimenting. Neural network optimization is typically not a convex problem. In other words, there is no guarantee that the optimizer will find the global optimum from any starting point.
In the interest of reproducibility, we must record the seed value we feed into the random number generator. After all, the seed value determines the starting point of the optimization process. Jax \cite{jax2018github} makes the state of the pseudorandom number generator explicit by introducing a unique key object. To generate keys, programmers must provide seed values by default. Their approach solves a lot of reproducibility issues.
PyTorch users can optionally choose to set seed values \cite{PyTorch2024randomness}. In the interest of reproducibility, this position paper argues we should make that a habit in all our scientific work.


Overall, the related work suggests,
use versioning for code and data \cite{Kjetil2013TenReproducible,NeuripsCodeguide},
fix seeds whenever pseudorandom numbers are involved \cite{Kjetil2013TenReproducible},
automate for continuous integration \cite{Yasset2016TenGit}, 
document dependencies \cite{NeuripsCodeguide,Andreas2012TenSoftware},
and to provide a README \cite{NeuripsCodeguide,Yasset2016TenGit} as well as LICENSE file \cite{Yasset2016TenGit}.