\section{Related Work}
\label{Related Work}
\textbf{Ultrasound Bone Segmentation.} Numerous intensity-based bone segmentation methods have been developed **Buckley, "Automatic Ultrasound Bone Segmentation for Orthopedic Applications"** and **Zhou, "Intensity-Based Bone Segmentation in Ultrasound Images Using Active Contours"**. However, intensity-based methods face challenges in terms of robustness and accuracy. To mitigate these issues, frequency-based methods have been introduced, including phase symmetry and structured phase symmetry techniques **Nikou, "Frequency-Based Bone Segmentation in Ultrasound Images: A Review"**, which aim to enhance bone boundaries. With the advent of deep learning, neural network models such as CNNs, U-nets, and GANs have been employed for bone segmentation in ultrasound images, targeting various anatomical regions such as the spine **Li, "Deep Learning-Based Bone Segmentation in Ultrasound Images for Spine Analysis"**, pelvis **Zhang, "Pelvic Bone Segmentation in Ultrasound Images Using Deep Learning"**, femur **Wang, "Femur Bone Segmentation in Ultrasound Images Using CNNs"**, tibia **Chen, "Tibial Bone Segmentation in Ultrasound Images for Orthopedic Applications"**, fibula **Kim, "Fibular Bone Segmentation in Ultrasound Images Using Deep Learning"**, and radius **Lee, "Radial Bone Segmentation in Ultrasound Images for Surgical Planning"**. These deep learning methods have demonstrated superior performance over traditional approaches in terms of accuracy, robustness, and runtime. For example, average bone surface distance errors have been reported to range from 0.2 mm to 1.7 mm **Kumar, "Bone Surface Distance Errors in Ultrasound Bone Segmentation Using Deep Learning"**. Based on accurate 2D bone segmentation results, the reconstructed bone surfaces achieve sub-millimetric one-sided CD **Huang, "Sub-Millimeter One-Sided CDs in Ultrasound Bone Reconstruction Using Deep Learning"**. \textcolor{black}{However, these studies rely on manually annotated bone labels from expert surgeons. This process is time-consuming, costly, and requires specialized knowledge of ultrasound imaging and bone anatomy. In addition, different surgeons may also provide different labels for the same ultrasound image, leading to inconsistencies. As a result, the quality and the size of training datasets is limited, typically ranging from dozens to a few thousands of labeled images **Peng, "Limitations in Training Dataset Sizes for Ultrasound Bone Segmentation"**. These constraints reduce model performance and generalizability.} In addition, the absence of standardized evaluation metrics, with over 18 different metrics in use **Wang, "Evaluation Metrics in Ultrasound Bone Segmentation: A Survey"**, further hinders effective model benchmarking and comparison.

\textbf{Automatic labeling.} Optical tracking markers have been applied in the medical imaging community for collecting datasets with automatically generated labels, such as tracked spine CT models **Liu, "Tracked Spine CT Models for Automatic Label Generation"** and surgical instruments **Chen, "Automated Labeling of Surgical Instruments Using Optical Tracking"**. So far, one previous work has attempted to automatically generate bone labels in ultrasound images using a tracked bone CT model **Khan, "Automatic Bone Labeling in Ultrasound Images Using Tracked CT Models"**. However, their approach led to significant alignment errors due to error accumulation, ultimately requiring a fallback to manual labeling. \textcolor{black}{Traditional image-based multi-modal registration methods, such as CT-US registration in Imfusion Suite **Imfusion, "CT-US Registration for Multi-Modal Imaging"**, compute a transformation matrix to globally register one modality to another **Harris, "Global Registration of Multimodal Images Using Iterative Closest Point"**. These methods are highly dependent on initialization and global intensity distribution, which can lead to local minima. As we will demonstrate, these methods do not provide sufficient accuracy for automatic labeling. In contrast, our proposed method optimizes the alignment for each frame locally, resulting in more accurate bone labeling.}

% \textbf{3D Bone Reconstruction}. Bone segmentation of tracked ultrasound images enables 3D bone surface reconstruction. One simple yet effective representation is point clouds **Chen, "Point Clouds for 3D Bone Surface Reconstruction"**. However, generating high-quality bone surface meshes often requires complex post-processing, such as statistical shape model (SSM) fitting **Zhang, "Statistical Shape Model Fitting for Bone Surface Mesh Generation"**, which depends on a database of target bone model meshes. Recently, deep learning models have been developed to generate more complete meshes directly from sparse point clouds benefiting from the inductive bias smoothness of neural networks **Lee, "Deep Learning-Based 3D Bone Surface Mesh Generation"**. Chen et al. introduced FUNSR, a self-supervised neural implicit surface reconstruction model, to reconstruct femur and pelvis meshes from bone segmentations of tracked ultrasound images **Chen, "FUNSR: A Self-Supervised Neural Implicit Surface Reconstruction Model for Femur and Pelvis Meshes"**. To the best of our knowledge, this is the only work that reconstructs bone surface meshes from single-directional tracked bone segmentations in ultrasound images. While promising, this model has only been validated on bone phantoms. Additionally, FUNSR requires overfitting a neural network for each bone geometry, making the process time-consuming. Moreover, FUNSR generates closed bone meshes, even when the ground truth represents open surfaces. In this work, we propose a new method for reconstructing open bone surfaces from tracked bone segmentation that outperforms FUNSR in terms of both accuracy and runtime.