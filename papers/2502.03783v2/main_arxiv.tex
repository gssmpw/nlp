%% 
%% Copyright 2007-2024 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.3 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.3 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%% $Id: elsarticle-template-num.tex 249 2024-04-06 10:51:24Z rishi $
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{hyperref}
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}
\usepackage{xcolor}
\usepackage{tabularx}

\usepackage{svg}
%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage{bm}
\usepackage{subcaption}
\usepackage{gensymb}
\DeclareUnicodeCharacter{2212}{\textminus}
\DeclareMathOperator*{\argmax}{arg\,max} 




\journal{Computers in Biology and Medicine}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{UltraBones100k: A reliable automated labeling method and large-scale dataset for ultrasound-based bone surface extraction}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[label1]{Luohong Wu\corref{cor1}}
\ead{luohong.wu@balgrist.ch}%% Author name

\author[label1]{Nicola A. Cavalcanti}
\author[label1]{Matthias Seibold} %% Author name
\author[label2]{Giuseppe Loggia}
\author[label2]{Lisa Reissner}
\author[label1,label3]{Jonas Hein}
\author[label2]{Silvan Beeler}
\author[label2]{Arnd Viehöfer}
\author[label2]{Stephan Wirth}
\author[label1]{Lilian Calvet}
\author[label1]{Philipp Fürnstahl}
\cortext[cor1]{Corresponding author: Lengghalde 5, Zurich, 8008, Zurich, Switzerland}


%% Author affiliation
\affiliation[label1]{organization={Research in Orthopedic Computer Science, Balgrist University Hospital, University of Zurich},%Department and Organization
            addressline={Lengghalde 5}, 
            city={Zurich},
            postcode={8008}, 
            state={Zurich},
            country={Switzerland}}

\affiliation[label2]{organization={Department of Orthopaedics, Balgrist University Hospital, University of Zurich},%Department and Organization
            addressline={Forchstrasse 340}, 
            city={Zurich},
            postcode={8008}, 
            state={Zurich},
            country={Switzerland}}

\affiliation[label3]{organization={Computer Vision and Geometry Group, ETH Zurich},%Department and Organization
            addressline={Ramistrasse 101}, 
            city={Zurich},
            postcode={8092}, 
            state={Zurich},
            country={Switzerland}}

%% Abstract
\begin{abstract}
%% Text of abstract
Background: Ultrasound-based bone surface segmentation is crucial in computer-assisted orthopedic surgery. However, ultrasound images have limitations, including a low signal-to-noise ratio, acoustic shadowing, and  speckle noise, which make interpretation difficult. Existing deep learning models for bone segmentation rely primarily on costly manual labeling by experts, limiting dataset size and model generalizability. Additionally, the complexity of ultrasound physics and acoustic shadow makes the images difficult for humans to interpret, leading to incomplete labels in low-intensity and anechoic regions and limiting model performance. To advance the state of the art in ultrasound bone segmentation and establish effective model benchmarks, larger and higher-quality datasets are needed.\\
Methods: We propose a methodology for collecting ex-vivo ultrasound datasets with automatically generated bone labels, including anechoic regions. The proposed labels are derived by accurately superimposing tracked bone Computed Tomography (CT) models onto the tracked ultrasound images. These initial labels are refined to account for ultrasound physics. To clinically evaluate the proposed method, an expert physician from our university hospital specialized on orthopedic sonography assessed the quality of the generated bone labels. A neural network for bone segmentation is trained on the collected dataset and its predictions are compared to expert manual labels, evaluating accuracy, completeness, and F1-score. \\
Results: We collected the largest known dataset of 100k ultrasound images of human lower limbs with bone labels, called UltraBones100k. A Wilcoxon signed-rank test with Bonferroni correction confirmed that the bone alignment after our optimization pipeline significantly improved the quality of bone labeling ($p < 0.001$). The model trained on UltraBones100k consistently outperforms manual labeling in all metrics, particularly in low-intensity regions (320\% improvement in completeness at a distance threshold of 0.5 mm)\\
Conclusion: This work is promising to facilitate research and clinical translation of ultrasound imaging in computer-assisted interventions.
\end{abstract}




%% Keywords
\begin{keyword}
Ultrasound bone segmentation \sep Bone surface segmentation \sep Bone surface reconstruction \sep Ultrasound image analysis \sep Computer-assisted orthopedic surgery 
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section

\section{Introduction}
\label{sec_introduction}
\textcolor{black}{Ultrasound is a well-established imaging technology widely used in the clinical diagnosis of musculoskeletal pathologies, including bone and soft tissue injuries \cite{ultrasound_diagnosis}. It also facilitates dynamic assessments of joint and tendon movements, stability\cite{joint_stability}, and the detection of fracture healing, structural abnormalities, infections, ligament injuries, nerve compression, and mechanical impingement \cite{ligamentous_injury}. As evidenced by a substantial body of literature \cite{3D_ultrasound_interventions,3D_ultrasound_needle_placement,3D_ultrasound_trauma,spine3,pelvis_femur_tibia}, the automated segmentation and 3D reconstruction of bone surfaces from ultrasound images are gaining increasing significance as a powerful tool for interventional Computer-Assisted Orthopedic Surgery (CAOS) systems. Compared to ionizing imaging modalities like fluoroscopy, ultrasound offers advantages in improving guidance and navigation during interventional and surgical procedures\cite{3D_ultrasound_interventions}, including trauma treatment\cite{3D_ultrasound_trauma}, needle placement\cite{3D_ultrasound_needle_placement}, and robotic spine surgery \cite{3D_ultrasound_robotic_spine_surgery}.}

Accurate 3D modeling of bone geometry requires precise 2D identification and segmentation of bone anatomies in ultrasound images, which relies heavily on high-quality labels produced by expert professionals\cite{pcd1,pcd2,pcd3,FUNSR}. Due to the highest acoustic impedance compared to surrounding soft tissues, bones typically appear as high-intensity regions when the ultrasound probe is perpendicular to the bone surface. However, when the probe is positioned at a larger oblique angle to the bone surface, bones can appear as low-intensity regions as most portion of acoustic energy is reflected into surrounding soft tissues instead of back to the probe. These low-intensity regions can only be inferred by surgeons through anatomical knowledge but cannot be accurately labeled. The same applies to fully anechoic regions formed by the complete absorption of the ultrasound signal in the overlying bone layers.

To address these challenges, various medical imaging methods based on handcrafted features have been developed to enhance the interpretation of ultrasound images \cite{bone_shadow_and_phase,phase_symmetry3}. Recently, deep learning-based bone segmentation methods have shown promising improvements in robustness, accuracy, and processing speed \cite{spine1,spine2, spine3,pelvis, pelvis_femur_tibia, femur_fibula_tibia_humerus_radius_ulna, radius_femur_tibia}. \textcolor{black}{However, the performance of such models relies on the quality of the manually labeled training dataset. Manual bone labeling is time-consuming, and requires specialized knowledge of ultrasound imaging and bone anatomy, which cannot be produced on a large scale if relying solely on human experts. The resulting datasets are often small, ranging from dozens to a few thousands labeled slices. This limits the performance and generalizability of the trained models. Additionally, most existing methods train and validate their models on non-public datasets and use non-standardized evaluation metrics, making it difficult to compare and benchmark the performance of different models across studies \cite{review1,review2}. The absence of large-scale, accurately labeled public datasets has been a critical obstacle for the community in developing more advanced models.}



\textcolor{black}{In this work, we propose a methodology that enables the collection of large-scale ultrasound datasets with automatically generated bone labels. Instead of relying on manual labeling, our approach utilizes optical tracking to align a 3D bone model, extracted from a preoperative CT scan, with tracked ultrasound images. The initial alignment is further refined using our proposed intensity-based optimization strategy for each frame, rather than relying on traditional global optimization \cite{CT-US_registration}. This approach enhances the precision and completeness of bone labels in ultrasound images. With this methodology, we collected a dataset of 100k annotated ultrasound images with bone labels from 14 human cadaveric lower legs. A deep learning model for bone segmentation is trained on the resulting UltraBones100k dataset, which significantly outperforms manual segmentation performed by an expert surgeon. }
% To highlight the relevance of our approach for CAOS, we demonstrate accurate 3D bone reconstruction as a downstream application based on both signed (SDF) and unsigned (UDF) distance fields. The reconstructed bone surface achieved an mean Chamfer Distance (CD) of 0.632 mm. 
% and 95\% Hausdorff Distance (HD) of 1.438 mm.% 
% The main contributions of this work include:

% \begin{itemize}
% \item A methodology for collecting an ultrasound dataset with automatically generated complete and accurate CT-derived bone  labels.
% \item A dataset of around 100k ultrasound images of 14 human lower limbs, including fibula, tibia, and calcaneus, with high-quality bone labels. A deep learning model for bone  segmentation is also provided as baseline for future comparison. 
% \end{itemize}
To ensure reproducibility of the results, the dataset and pretrained model weights, have been made publicly available\footnote{Link will be available upon acceptance.}. 


\section{Related Work}\label{Related Work}
\textbf{Ultrasound Bone Segmentation.} Numerous intensity-based bone segmentation methods have been developed \cite{review1,review2}. However, intensity-based methods face challenges in terms of robustness and accuracy. To mitigate these issues, frequency-based methods have been introduced, including phase symmetry and structured phase symmetry techniques \cite{phase_symmetry3,bone_shadow_and_phase}, which aim to enhance bone boundaries. With the advent of deep learning, neural network models such as CNNs, U-nets, and GANs have been employed for bone segmentation in ultrasound images, targeting various anatomical regions such as the spine \cite{spine1,spine2,spine3}, pelvis \cite{pelvis, pelvis_femur_tibia}, femur \cite{femur_fibula_tibia_humerus_radius_ulna,pelvis_femur_tibia,radius_femur_tibia}, tibia \cite{pelvis_femur_tibia,radius_femur_tibia,femur_fibula_tibia_humerus_radius_ulna}, fibula \cite{femur_fibula_tibia_humerus_radius_ulna}, and radius \cite{radius_femur_tibia,femur_fibula_tibia_humerus_radius_ulna}. These deep learning methods have demonstrated superior performance over traditional approaches in terms of accuracy, robustness, and runtime. For example, average bone surface distance errors have been reported to range from 0.2 mm to 1.7 mm \cite{radius_femur_tibia, SSM, spine3,pelvis}. Based on accurate 2D bone segmentation results, the reconstructed bone surfaces achieve sub-millimetric one-sided CD \cite{pcd2}. \textcolor{black}{However, these studies rely on manually annotated bone labels from expert surgeons. This process is time-consuming, costly, and requires specialized knowledge of ultrasound imaging and bone anatomy. In addition, different surgeons may also provide different labels for the same ultrasound image, leading to inconsistencies. As a result, the quality and the size of training datasets is limited, typically ranging from dozens to a few thousands of labeled images \cite{review2}. These constraints reduce model performance and generalizability.} In addition, the absence of standardized evaluation metrics, with over 18 different metrics in use \cite{review1}, further hinders effective model benchmarking and comparison. 

\textbf{Automatic labeling.} Optical tracking markers have been applied in the medical imaging community for collecting datasets with automatically generated labels, such as tracked spine CT models \cite{spineDepth} and surgical instruments \cite{markerless_tool_tracking}. So far, one previous work has attempted to automatically generate bone labels in ultrasound images using a tracked bone CT model \cite{pcd3}. However, their approach led to significant alignment errors due to error accumulation, ultimately requiring a fallback to manual labeling. \textcolor{black}{Traditional image-based multi-modal registration methods, such as CT-US registration in Imfusion Suite \footnote{https://www.imfusion.com/}, compute a transformation matrix to globally register one modality to another \cite{CT-US_registration}. These methods are highly dependent on initialization and global intensity distribution, which can lead to local minima. As we will demonstrate, these methods do not provide sufficient accuracy for automatic labeling. In contrast, our proposed method optimizes the alignment for each frame locally, resulting in more accurate bone labeling.}



% \textbf{3D Bone Reconstruction}. Bone segmentation of tracked ultrasound images enables 3D bone surface reconstruction. One simple yet effective representation is point clouds \cite{pcd1,pcd2,pcd3}. However, generating high-quality bone surface meshes often requires complex post-processing, such as statistical shape model (SSM) fitting \cite{SSM}, which depends on a database of target bone model meshes. Recently, deep learning models have been developed to generate more complete meshes directly from sparse point clouds benefiting from the inductive bias smoothness of neural networks \cite{neural_pull}. Chen et al. introduced FUNSR, a self-supervised neural implicit surface reconstruction model, to reconstruct femur and pelvis meshes from bone segmentations of tracked ultrasound images \cite{FUNSR}. To the best of our knowledge, this is the only work that reconstructs bone surface meshes from single-directional tracked bone segmentations in ultrasound images. While promising, this model has only been validated on bone phantoms. Additionally, FUNSR requires overfitting a neural network for each bone geometry, making the process time-consuming. Moreover, FUNSR generates closed bone meshes, even when the ground truth represents open surfaces. In this work, we propose a new method for reconstructing open bone surfaces from tracked bone segmentation that outperforms FUNSR in terms of both accuracy and runtime.




\section{Method} \label{method}
% In the following sections, we first present our dataset collection methodology in Section~\ref{dataset}. Next, we describe the application to bone surface reconstruction in Section~\ref{3D_bone_reconstruction}.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{figures/collection_setup_annotated.pdf}
\caption{The dataset acquisition setup includes an optical tracking system (A), a human cadaveric specimen (B) with mounted optical markers (C), and an tracked and calibrated ultrasound device (D) .}\label{fig:collection_setup}
\end{figure}
\subsection{Dataset Collection and Automatic Bone labeling} \label{dataset}



\textbf{Acquisition Setup}. Our dataset acquisition setup using human cadaveric specimens is illustrated in Figure~\ref{fig:collection_setup}. \textcolor{black}{Ethics approval has been granted for our study by the Zurich cantonal ethics committee under BASEC Nr. 2023-01681.} Prior to data acquisition, K-wires with a diameter of 2.5 mm and a length of 150 mm (DePuy Synthes, USA), along with custom-made 3D-printed marker mounts equipped with infrared-sensitive fiducial markers, were mounted to each specimen. The K-wires were used to stabilize the bones of interest, including the lower leg and foot, to prevent bone movement during scanning. Optical markers were tracked by an optical tracking system (FusionTrack 500, Atracsys, Switzerland). A CT scan was performed on each specimen with an image resolution of 512 × 512 pixels, an in-plane resolution of 0.839 mm × 0.839 mm, and a slice thickness of 0.6 mm (NAEOTOM Alpha, Siemens, Germany). Bone segmentation was generated using the region growing and thresholding functionalities in Mimics (Materialise, Leuven, Belgium). For data acquisition, each specimen was scanned nine times using a tracked ultrasound device (AixplorerUltimate, SuperSonic Imagine, France) equipped with a linear probe that has an imaging frequency range of 4.0 to 15.0 MHz. The scanning procedure consisted of three forward and backward passes along the longitudinal axis of the tibia, three along the fibula, and three covering the foot (including the bones of the ankle joint such as the calcaneus and talus), aiming to maximize bone surface coverage. Ultrasound images, along with the tracking data of the ultrasound probe and the specimen, were recorded with corresponding timestamps.



\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{figures/transformation_chain.pdf}
\caption{\textcolor{black}{Diagram explaining the transformation chain across all components. Solid black arrows represent known transformations, which are directly measured by the optical tracking system. The dashed-line arrow represents the unknown transformation from the image plane to the ultrasound probe, which will be determined through calibration.}}
\label{fig:transformation_chain}
\end{figure}

\textbf{System Calibration}. \textcolor{black}{To determine the coordinates of the tracking fiducials on the specimen marker within the CT space, virtual spheres are created to match the size of the physical fiducials in MeshLab\cite{MeshLab}. These virtual spheres are initially manually aligned with the corresponding fiducials in the segmented CT model, followed by refinement using the point-to-point Iterative Closest Point algorithm \cite{ICP}. } The resulting registration transformation matrix allows for the accurate calculation of relative poses between the ultrasound probe and the bone CT model. However, converting each pixel of an ultrasound image into its corresponding point in the CT space requires accurate spatial calibration and accurate tracking of the ultrasound probe. In addition, temporal calibration is necessary to synchronize the ultrasound images with the tracking data. Following the convention in existing works \cite{calibration_review,spatial_calibration}, we define the origin of the ultrasound image at the top-left corner. The transformation chain across all components is illustrated in Figure~\ref{fig:transformation_chain}.
\textcolor{black}{The pixels of the ultrasound image at timestamp \( t \) are defined in homogeneous coordinates as \( \mathcal{P}^t = \left\{ \begin{bmatrix} u, v, 0, 1 \end{bmatrix}^T \mid u \in [0, W), v \in [0, H) \right\} \), where \( W \) and \( H \) represent the image width and height, respectively. The corresponding point cloud in CT space, \( \mathcal{U}^t = \left\{ \begin{bmatrix} x_j, y_j, z_j, 1 \end{bmatrix}^T \mid j \in [0, H \times W) \right\} \), is computed as
\begin{equation}\label{eq:pixel_to_points}
\mathcal{U}^t = \mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{OT}}\cdot \mathsf{T}^{t+\Delta t}_{\text{OT} \leftarrow \text{US}} \cdot \mathsf{T}_{\text{US} \leftarrow \text{IP}}\cdot \mathsf{T}_{\text{S}}\cdot \mathcal{P}^t=\mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{US}} \cdot \mathsf{T}_{\text{US} \leftarrow \text{IP}} \cdot\mathsf{T}_{\text{S}} \cdot \mathcal{P}^t
\end{equation}
\textcolor{black}{
with:
    \begin{itemize}
        \item $\mathsf{T}_{\text{S}} = \begin{bmatrix}
        s_{x} & 0 & 0 & 0 \\
        0 & s_{y} & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 1
        \end{bmatrix}$, where $s_x$ is the width of one pixel in mm, and $s_y$ is the height of one pixel in mm. $s_x$ and $s_y$ are known from the ultrasound device.
        \item $\mathsf{T}_{\text{US} \leftarrow \text{IP}}=\begin{bmatrix}
r_{11} & r_{12} & r_{13} & d_x \\
r_{21} & r_{22} & r_{23} & d_y \\
r_{31} & r_{32} & r_{33} & d_z \\
0 & 0 & 0 & 1
\end{bmatrix}$, belonging to the the 3D Special Euclidean group $\text{SE}(3)$, is the transformation matrix that maps 3D points from the coordinate system of the image plane (origin: top-left corner) to the coordinate system of the marker attached to the ultrasound probe. The determination of this matrix refers to the process of spatial ultrasound calibration \cite{spatial_calibration,calibration_review}. Note that a transformation matrix \( \mathsf{T}(\boldsymbol{\theta}, \mathbf{d}) \) can be defined using the Euler rotation angles \( \boldsymbol{\theta} \) and the translation vector \( \mathbf{d} \), which together specify the relationship between two coordinate systems.
        \item $\mathsf{T}^{t+\Delta t}_{\text{OT} \leftarrow \text{US}}\in \text{SE}(3)$ is the transformation matrix at timestamp $t+\Delta t$ that transforms 3D points from the coordinate system of the ultrasound probe marker to the coordinate system of the optical tracking system. As the probe moves during dataset collection, $\mathsf{T}_{O \leftarrow US}$ changes over time instead of a constant matrix. At each timestamp $t $, two types of data were collected: ultrasound image data ($ \mathcal{P}^t $) and tracking data from multiple markers ($ \mathsf{T}^t $). Due to differences in sensor characteristics, such as frame rates and data acquisition timings, $ \mathcal{P}^t $ and $ \mathsf{T}^t $ are not inherently synchronized, resulting in a temporal offset denoted as $ \Delta t $. Determining $ \Delta t $ is thus the process of temporal calibration. In our setup, the tracking data has a higher temporal resolution than the ultrasound data ($ \Delta t < 0 $), as forming a B-mode ultrasound image requires additional time to process the original acoustic signals.
        \item $\mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{OT}}\in \text{SE}(3)$ is the transformation matrix at timestamp $t+\Delta t$ from the coordinate system of the optical tracking system to the coordinate system of the CT scan, which can be determined using the marker attached to the cadaveric specimen.
        \item For simplicity, we denote \( \mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{US}}(\boldsymbol{\theta}, \mathbf{d}) = \mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{OT}}(\boldsymbol{\theta}_1, \mathbf{d}_1) \cdot \mathsf{T}^{t+\Delta t}_{\text{OT} \leftarrow \text{US}}(\boldsymbol{\theta}_2, \mathbf{d}_2) \) as the direct transformation from the ultrasound marker to the CT space.
    \end{itemize}
}
Unlike previous studies that treated spatial and temporal calibration independently \cite{spatial_calibration,calibration_review}, we performed both calibrations simultaneously using a Z-phantom \cite{spatial_calibration}. The temporal offset \( \Delta t \) was sampled over the range \([-300, 0)\) ms with a step size of 0.1 ms. For each \( \Delta t \), \( \mathsf{T}_{\text{US} \leftarrow \text{IP}} \) was computed, and the corresponding 3D localization recorded. The optimal parameters were identified by minimizing the 3D localization error.
Our method achieved a 3D localization error of 1.07 mm, outperforming previously reported errors (\(\approx 1.5\) mm) \cite{spatial_calibration,calibration_review}.
With precise calibration results, 2D pixel coordinates can be accurately mapped to 3D points in the CT space. For simplicity, we denote the pixel corresponding to the point $\mathbf{u}_i^t \in \mathcal{U}^t$ as $\mathbf{p}_i^t \in \mathcal{P}^t$, and define $f(\mathbf{u}_i^t)$ as the intensity value of $\mathbf{p}_i^t$ in the original ultrasound image.}







\textbf{Initial Bone Labeling} Points are sampled uniformly over the bone CT model to generate the CT point cloud $\mathcal{C}$. Both $\mathcal{C}$ and $\mathcal{U}^t$ (with original intensity values) are rendered in Figure~\ref{fig:optimization_flow}. The CT-ultrasound bone surface intersection is defined as $\mathcal{I}^t=\mathcal{C} \cap \mathcal{U}^t = \left\{ \mathbf{u} \in \mathcal{U}^t \mid \min_{\mathbf{c} \in \mathcal{C}} \lVert \mathbf{u} - \mathbf{c} \rVert \leq \gamma \right\}$ where $\gamma$ is the parameter defining the thickness of the intersection ($\gamma=$0.3 mm determined empirically). Ideally, $\mathcal{I}^t$ can be projected back onto the ultrasound image, forming a complete bone label. However, as also reported previously \cite{pcd3}, mismatches \textcolor{black}{(1 to 2 mm)} often occur due to accumulated errors, including tracking error and calibration error (see Figure~\ref{fig:optimization_flow}). Although these mismatches are typically below 2 mm, they are noticeable given the high resolution of ultrasound images, which can reach up to 0.05 mm per pixel. 

\begin{figure*}[t]
        \centering
        \includegraphics[width=0.98\textwidth]{figures/flowchart.pdf}
        \caption{Overview of the automated 2D bone labeling pipeline. By leveraging 3D spatial data, the ultrasound image and CT bone model are overlaid to produce an initial bone alignment (red). Optimization of Eq.~(\ref{eq:optim}) refines the alignment (green). Following the refinement, the bone shadow (blue) and incidence angles (yellow) are computed. Using ultrasound physics, invisible bone surface regions are excluded, resulting in the final bone label.}
        \label{fig:optimization_flow}
\end{figure*}


\textbf{Optimization Step for Refined Alignment.}  Bone contours that are roughly perpendicular to the ultrasound probe typically appear as high-intensity regions in ultrasound images due to the underlying ultrasound physics. Based on this principle, we propose an intensity-based optimization method designed to refine the location of CT-derived labels. \textcolor{black}{The proposed optimization relies on the following assumptions: (i) the main error sources are tracking and calibration errors, as the only undetermined variables in Eq.~(\ref{eq:pixel_to_points}) are the tracking matrices (\(\mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{OT}}\) and \(\mathsf{T}^{t+\Delta t}_{\text{OT} \leftarrow \text{US}}\)) and the calibration matrix (\(\mathsf{T}_{\text{US} \leftarrow \text{IP}}\)); (ii) pixels with high intensity in the vicinity of the non-corrected labels mostly correspond to the true bone contour. Beneath the bone shadow, there are almost uniformly dark regions, as bones possess the highest acoustic impedance compared to the surrounding soft tissues, causing most acoustic waves to be reflected back; (iii) the CT-ultrasound alignment error does not exceed an mean distance of 2 mm, given that the optical tracking system offers sub-millimeter accuracy\footnote{\url{https://atracsys.com/fusiontrack-500/}} and the calibration matrix achieves a 3D localization error of 1.07 mm. Furthermore, a random image sampled from each ultrasound sweep to measure mismatch yielded a mean error of $1.13 \pm 0.606$ mm.} 

The key principle of our optimization method is to add small perturbations ($\boldsymbol{\theta}_\epsilon$ and $\mathbf{d}_\epsilon$) to the ultrasound image poses in order to maximize the intensity of pixels at the intersection between the ultrasound image and bone CT model while minimizing deviations from the recorded tracking data. We define the perturbed point cloud as $\mathcal{U}^t(\boldsymbol{\theta}_\epsilon,\mathbf{d}_\epsilon)=\mathsf{T}^{t+\Delta t}_{\text{CT} \leftarrow \text{US}}(\boldsymbol{\theta}+\boldsymbol{\theta}_\epsilon,\mathbf{d}+\mathbf{d}_\epsilon) \cdot \mathsf{T}_{\text{US} \leftarrow \text{IP}}\cdot \mathsf{T}_{\text{S}} \cdot \mathcal{P}^t$ and the perturbed intersection as $\mathcal{I}^t(\boldsymbol{\theta}_\epsilon,\mathbf{d}_\epsilon)=\mathcal{C} \cap \mathcal{U}^t(\boldsymbol{\theta}_\epsilon,\mathbf{d}_\epsilon) $.  The proposed solution is written as:
\begin{equation}\label{eq:optim}
\argmax_{\boldsymbol{\theta}_{\epsilon} ,\mathbf{d}_{\epsilon} \in [-1, 1]^3} {\frac{\sum_{ \mathbf{u} \in \mathcal{I}^t(\boldsymbol{\theta}_\epsilon,\mathbf{d}_\epsilon)}{f(\mathbf{u})}}{|\mathcal{I}^t(\boldsymbol{\theta}_\epsilon,\mathbf{d}_\epsilon)|}
-\lambda \frac{\sum_{i}{||\mathsf{T}(\boldsymbol{\theta},\mathbf{d}) \cdot \mathbf{F}_i-\mathsf{T}(\boldsymbol{\theta}+\boldsymbol{\theta}_\epsilon,\mathbf{d}+\mathbf{d}_\epsilon) \cdot \mathbf{F}_i||_2}}{N}}
\end{equation}
$\mathbf{F}_i$ denotes the homogeneous coordinate vector of the $i$-th tracking fiducial of the ultrasound probe marker, with $N$ fiducials in total.
The first term of equation~(\ref{eq:optim}) represents the mean intensity values of points in $\mathcal{I}^t(\boldsymbol{\theta}_\epsilon,\mathbf{d}_\epsilon)$ under the corrected image pose.
The second term represents the mean deviation of the tracking fiducials induced by the pose correction in terms of Euclidean distance.
By leveraging the bounded set and regularization term, our approach ensures the solution remains close to the initial tracking data. The weighting parameter $\lambda$ is empirically determined from the range [0.01, 0.1, 1, 10, 100,1000]. The optimization problem is solved using the differential evolution method \cite{differential_evoution}. Each $\lambda$ value was tested on randomly sampled frames, and the results were evaluated quantitatively to select the optimal $\lambda$. To identify failed optimizations, a validation step is performed. Specifically, ultrasound images with a mean fiducial location correction greater than 1 mm after optimization are discarded, as the tracking device provides sub-millimetric accuracy. After the validation step, approximately 18.7\% frames are discarded, with the mean fiducial location correction of the remaining frames being 0.481 mm. An example of a full bone  label refined using the proposed optimization is shown in Figure~\ref{fig:optimization_flow}.
To account the bone shadow effect, bone regions within the shadow (blue region in Figure~\ref{fig:optimization_flow}) are discarded. In addition, we project the normal vectors from the bone CT model onto the ultrasound images and compute the incidence angles $\alpha$ between the projected normal vectors and the vertical scan lines, as shown in Figure~\ref{fig:optimization_flow}. According to ultrasound imaging physics, the intensity of bone pixels decreases as $\alpha$ increases, with \(\alpha > 90\degree\) being a critical boundary. In this study, we keep bone pixels with \(\alpha \leq 85\degree\) to avoid boundary cases. The final bone label is shown in Figure~\ref{fig:optimization_flow}. More examples of bone labels before and after the optimization are shown in Figure~\ref{fig:before_and_after_optimization}.

\begin{figure*}[t]
\centering
\includegraphics[width=0.95\textwidth]{figures/before_and_after_optimization.pdf} % Adjust the image file name
\caption{Comparison results of bone annotation before and after optimization. \textcolor{red}{Red} regions show the initial alignment, \textcolor{green}{green} regions show the refined alignment, \textcolor{yellow}{yellow} regions are the overlap.}
\label{fig:before_and_after_optimization}
\end{figure*}



\textbf{Clinical Evaluation.} \textcolor{black}{To clinically evaluate the quality of bone annotations following our optimization pipeline, an expert physician specialized on orthopedic sonography assessed the quality of both the initial and optimized bone annotations. For comparison with state-of-the-art, we also employed the traditional "ultrasound to CT registration" algorithm available in the ImFusion Suite\footnote{https://www.imfusion.com/} (ImFusion, Munich, Germany) to compute a global transformation matrix for registering the ultrasound data to the CT volume (the default parameter setting yielded the best results in our experiments), \textcolor{black}{rather than optimizing the alignment for each frame as our method does}. In our experiment, one frame was randomly sampled from each ultrasound sweep, resulting in a total of 119 frames. Bone annotations were performed using three alignment methods: the initial alignment, the optimized alignment derived from our method, and the bone alignment obtained after the CT-US registration step.
The resulting bone alignment outputs from these three methods were presented to an expert physician specialized on orthopedic sonography in random order. The expert evaluated the quality of each bone alignment using the following criteria: \begin{itemize} \item \textbf{0:} Poor alignment, with large sections showing visible errors exceeding 3 mm. \item \textbf{1:} Moderate alignment, characterized by some sections with visible errors between 1–3 mm. \item \textbf{2:} Good alignment, exhibiting only minor visible errors below 1 mm. \item \textbf{3:} Excellent alignment, with no visible errors. \end{itemize}
} \textcolor{black}{A screenshot of the rating software interface is shown in Figure~\ref{rating_app}. A scale indicating 1, 2, and 3 mm is displayed in the top-left corner of each image.
}

\begin{figure}[h!]
        \centering
        \includegraphics[width=0.99\linewidth]{figures/example_rating.png}
        \caption{\textcolor{black}{A screenshot of the bone alignment rating software.}}
        \label{rating_app}
\end{figure}

\subsection{Bone Segmentation}
\label{bone_segmentation}
\textbf{Segmentation Mode.}  \textcolor{black}{In our study, 14 cadavers were scanned, resulting a dataset of over 100k ultrasound images with bone labels.} We split the dataset into 10 training specimens, 1 validation specimen, and 3 test specimens (around 25k ultrasound images). The training and validation sets are further split using cross-validation. A U-net with ResNet encoders is trained on UltraBones100k to automatically segment bones in ultrasound images\cite{Unet,Segmentation_models_pytorch}.  The loss function is a weighted combination of Dice Loss (weight=1) and Binary Cross Entropy Loss (weight=1). We trained the model using the Adam optimizer with $\beta=(0.9, 0.999)$, starting with an initial learning rate of 1e-5, which decayed by 0.9 every 10 epochs. The training process ran for 100 epochs on an NVIDIA V100 GPU, with a batch size of 32. The code and pretrained model will be publicly available. 

\textbf{Manual annotation.} \textcolor{black}{To compare the automatic bone labeling by the trained model with manual labeling, 700 ultrasound images were randomly sampled from the test set and manually labeled by an expert surgeon. The surgeon identified the hyperechoic bone surface layer and its corresponding acoustic shadow based on the following knowledge about ultrasound imaging and anatomical structures. Bone surfaces in ultrasound images are identified as a hyperechoic (bright) layer resulting from the significant reflection of ultrasound waves at the bone-tissue interface. This bright layer is typically accompanied by a characteristic acoustic shadow extending posteriorly, as the underlying bone attenuates or reflects the majority of the ultrasound signal. The anatomical region being scanned significantly influences the clarity and confidence of bone surface identification. Larger bones (e.g., $>$1 cm in diameter) with smooth, superficial surfaces are more readily identified due to their distinct echogenicity and reduced interference from surrounding tissue artifacts or acoustic noise. In contrast, smaller or deeper bones, or those with irregular surfaces, may present more ambiguous features.  Experienced operators rely on a combination of real-time image feedback and their knowledge of anatomical landmarks to delineate tissue structures, including bone surfaces. This process underscores the operator-dependent nature of ultrasound-based diagnostics in clinical practice. For labeling purposes, the ImFusion Label software\footnote{https://www.imfusion.com/} (ImFusion, Munich, Germany) was employed to annotate the bone surface layer on a pixel-wise basis within the acquired images. This software allows precise manual delineation of the bone surface, facilitating the creation of ground-truth labels for subsequent analysis."}

\textbf{Evaluation Metrics.} In previous studies \cite{review1,review2}, over 18 different metrics were used for evaluating bone segmentation in ultrasound images, including intersection-based (e.g., Dice score) and distance-based metrics (e.g., one-sided CD), which complicates fair benchmarking of models. Bone labels in ultrasound are thin, even small shifts can drastically reduce intersection-based scores. While CD is easy to interpret, it struggles with false positives. To address this, we propose using distance-thresholded accuracy, completeness, and F1-score, which balance intersection-based and distance-based evaluations. Accuracy represents the fraction of correct predictions out of all predictions at a certain distance threshold, while completeness measures the fraction of correct predictions relative to the ground truth. Particularly, our evaluation is based on the CT-derived labels, allowing to compare the performance between automatic segmentation and manual labeling. \textcolor{black}{Specifically, for a pixel in the predicted bone label $\hat{\mathbf{x}} \in \mathcal{P}$, its distance to the ground truth $\mathcal{G}$ is defined as follows:
\begin{equation}
    d_{\hat{\mathbf{x}}\rightarrow \mathcal{G}} = \underset{\mathbf{x}\in \mathcal{G}}{\operatorname{min}}
\| \hat{\mathbf{x}} - \mathbf{x} \|,
\end{equation}
The accuracy, completeness, and F1 score with a distance threshold $\sigma$ are defined as follows:
\begin{equation}
    Acc(\sigma) = \frac{1}{|\mathcal{P}|} \sum_{\hat{\mathbf{x}} \in \mathcal{P}}
    [ d_{\hat{\mathbf{x}}\rightarrow \mathcal{G}} < \sigma ]
\end{equation}
\begin{equation}
    Com(\sigma) = \frac{1}{|\mathcal{G}|} \sum_{\mathbf{x} \in \mathcal{G}}
    [ d_{\mathbf{x}\rightarrow \mathcal{P}} < \sigma ]
\end{equation}
\begin{equation}
    F(\sigma) = \frac{2 Acc(\sigma) \cdot Com(\sigma)}{Acc(\sigma) + Com(\sigma)},
\end{equation}
where $[.]$ is the Iverson bracket.
} 

To particularly assess the performance of our method in segmenting lower-intensity bone regions, we quantitatively divide the labels into high-intensity and low-intensity regions. The intensity values of all bone pixels in the collected ultrasound images are recorded, and the Otsu method is applied to determine an intensity threshold separating the bone pixels into two classes: high-intensity and low-intensity \cite{otsuThreshold}. The threshold is chosen to minimize intraclass variance and maximize interclass variance. Exemplary results are shown in Figure~\ref{fig:qualitative_AI_vs_Human}.



\section{Results} \label{results}
% \begin{itemize}
% \item trained deep learning model vs manual annotation for both high-intensity bone and low-intensity bone. + discussion
% \item our reconstruction vs FUNSR + discussion
% \item limitation + future work
% \end{itemize}


\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.60\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/qualitative_AI_vs_Human.pdf}
  \caption{}
  \label{fig:annotation_image}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.38\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/quantitative_4.pdf}
  \caption{}
  \label{fig:hist}
  \end{subfigure}
  \caption{(a) Qualitative comparison between manual labels and predicted ones. The first column shows the input images, the second column shows the CT-derived ground truth labels, the third column shows the manual labels, and the last column shows the bone labels predicted by the model trained on our labels. The bone labels are divided into high-intensity regions (\textcolor{green}{green}) and low-intensity regions (\textcolor{blue}{blue}). (b) Distribution of pixel intensities over labels for manually labeled images. The red line represents the threshold determined by the Otsu method. While manual labels are mostly located in high-intensity regions, the proposed CT-derived labels allows for predictions in both high-intensity and low-intensity regions.}
  \label{fig:qualitative_AI_vs_Human}
\end{figure}






\textcolor{black}{\textbf{Dataset Variability.} With our methodology, we have collected a dataset of 100k annotated ultrasound images with
bone labels from 14 human cadaveric lower legs from different donors. The donors cover different physical characteristics, with heights ranging from 155 to 185 cm, weights from 59 to 99 kg, and BMI values between 19 and 30. The age range spans from 41 to 76 years, and the dataset includes both male and female specimens. Furthermore, arthritis status is documented, with some specimens having a history of falls or related conditions.}

\begin{figure*}[h!]
        \centering
        \includegraphics[width=0.9\linewidth]{figures/rating_result.png}
        \caption{Results of the rating experiment.}
        \label{rating_result}
\end{figure*}
\textbf{Clinical Evaluation.} \textcolor{black}{The results of the bone annotation rating experiment is shown in Figure~\ref{rating_result}. The mean ratings are 1.45 for the initial alignment, 0.87 for CT-US registration, and 2.39 for our optimized alignment. A Wilcoxon signed-rank test with Bonferroni correction demonstrated a significant improvement in bone labeling quality following our optimization pipeline ($p < 0.001$).}



\textbf{Automatic vs Manual Labeling.} Qualitative results between the manual bone labels and the model-predicted labels are shown in Figure~\ref{fig:qualitative_AI_vs_Human}. Quantitative results, including accuracy, completeness, and F1 score, are presented in Figure~\ref{fig:quantitative_AI_vs_Human}. On average, at a distance threshold of 0.5 mm, the trained model achieves an accuracy of 0.900, completeness of 0.895, and an F1 score of 0.893 across all bone regions. Particularly, for low-intensity regions, the accuracy of the trained model (0.789) is 27.4\% higher than that of manual labeling (0.619), the completeness (0.801) is 320\% higher compared to manual labeling (0.190), and the F1 score (0.778) is 197\% higher than the manual labeling (0.261). 

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/quantitative_1.pdf}
  \caption{}
  \label{fig:quantitative_acc}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[b]{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/quantitative_2.pdf}
  \caption{}
  \label{fig:quantitative_com}
  \end{subfigure}
  % \hfill
  \begin{subfigure}[b]{0.32\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figures/quantitative_3.pdf}
  \caption{}
  \label{fig:quantitative_F1}
  \end{subfigure}
  \caption{Quantitative comparison between manual and predicted bone labels.}
  \label{fig:quantitative_AI_vs_Human}
\end{figure}


% Bone surface reconstruction results are shown in Figure~\ref{fig:qualitative_fibula}. Both SDF and UDF-based methods successfully reconstructs the bone surface. The one-sided CDs from the SDF-based reconstruction to the bone CT model, considered as the 3D ground truth, are 1.748 mm, 0.731 mm, and 0.828 mm for the fibula, calcaneus, and tibia, respectively. The CDs for the UDF-based reconstruction are 0.566 mm, 0.680 mm, and 0.652 mm, respectively. The low values of one-sided CD %and 95\% HD%
% from the reconstruction to the bone CT model demonstrate accurate bone predictions. The SDF-based FUNSR method tends to produce some false positives on our dataset by closing the reconstructed surfaces in  regions where bone contours are invisible in the ultrasound images (especially for the fibula). This results in lower accuracy compared to the UDF-based method which is more suitable for reconstructing partly visible bone surfaces.

% \begin{figure}[t]
% \centering
% \includesvg[width=0.925\textwidth]{figures/qualitative_all.svg}
% \caption{SDF and UDF-based bone surface reconstruction results for the fibula, calcaneous and tibia.
% }
% \label{fig:qualitative_fibula}
% \end{figure}

% \begin{figure}[h!t]
% \centering
% \includesvg[width=0.99\textwidth]{figures/qualitative_all.svg}
% \caption{SDF and UDF-based bone surface reconstruction results for the fibula, calcaneous and tibia. The mean one-sided CDs from the reconstruction to the bone CT model are 1.102 mm for the SDF-based method and 0.632 mm for the UDF-based method. The mean 95\% HDs are 2.165 mm and 1.438 mm, respectively.}
% \label{fig:qualitative_fibula}
% \end{figure}



% \begin{table}[h!t]
% \caption{Mean results of surface reconstruction performance.}\label{tab:reconstruction}
% \centering
% \begin{tabularx}{\textwidth}{@{\extracolsep\fill}l*{6}{X}} % Use tabularx to fit within the page width
% \toprule
%  & \multicolumn{2}{c}{Fibula} & \multicolumn{2}{c}{Tibia} & \multicolumn{2}{c}{Foot bones} \\
% \cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7} 
% Metric &CD(mm)&95\%HD(mm)&CD(mm)&95\%HD(mm)&CD(mm)& 95\%HD(mm)\\
% \midrule
% Poisson (9) \footnotemark[1]  & 1.178 & 3.118 & 0.928 & 2.189 & 0.873 & 2.180 \\
% Poisson (15)  & 0.758 & 1.884 & 0.751 & 1.763 & 0.819 & 2.028 \\
% FUNSR  & 1.748 & 2.765 & 0.828 & 1.920 & 0.731 & 1.811 \\
% Ours & \textbf{0.566} & \textbf{1.265} & \textbf{0.652} & \textbf{1.496} & \textbf{0.680} & \textbf{1.552} \\
% \botrule
% \end{tabularx}
% \footnotetext{The input bone segmentations are same across all methods in each experiment.}
% \footnotetext[1]{Poisson (9) refers to Poisson mesh reconstruction with a depth parameter of 9.}
% \end{table}
\textbf{Generalizability.} Our dataset consists of images collected from the lower extremities of human subjects. To qualitatively assess the generalizability of the trained model to new anatomies and specimens, we apply it to ultrasound images of the pelvis and femur collected from new specimens.Qualitative results of the trained model's generalizability on new anatomies are shown in Figure~\ref{fig:hip_femur_results}
\begin{figure}[t]
            \centering
            \includegraphics[width=0.95\textwidth]{figures/hip_femur_results.pdf}
            \caption{Examples of ultrasound images depicting the pelvis and femur, along with the corresponding predicted bone labels. }
            \label{fig:hip_femur_results}
\end{figure}

\section{Discussion}
\label{sec:discussion}
The results highlight our methodology as a promising solution for collecting large-scale ultrasound datasets with automatically generated bone labels. Clinical evaluation results confirm the quality of these labels. Furthermore, the pelvis and hip experiment results demonstrate the effectiveness of the collected dataset in training deep learning models with strong generalizability.

\textbf{Comparison with SOTA.} \textcolor{black}{Previous work \cite{pcd3} aimed to align a tracked CT bone model with tracked ultrasound images. However, they reported that the mismatch of 1–2 mm was too large to allow for accurate automatic bone labeling in ultrasound images, and manual labeling was adopted in the end. In contrast, the results show that our optimization pipeline enhances CT-ultrasound alignment, enabling automatic accurate bone label generation. In addition, unlike the traditional CT-US registration method \cite{CT-US_registration}, which calculates a single global transformation matrix for the entire ultrasound sweep to maximize overlapping voxel intensities, our approach computes a transformation matrix for each individual ultrasound frame. According to the results of the clinical evaluation, our method achieves significantly improved bone annotations. This improvement is statistically validated by the Wilcoxon signed-rank test with Bonferroni correction ($p < 0.001$).}

\textbf{Automatic vs Manual Labeling.} \textcolor{black}{ Qualitative results in Figure~\ref{fig:qualitative_AI_vs_Human} indicate that the model trained on UltraBones100k outperforms the surgeon in segmenting bones in ultrasound images. Manual labels mostly include only high-intensity bone regions. In contrast, predicted bone labels include both high-intensity and low-intensity regions. Quantitative results in Figure~\ref{fig:quantitative_AI_vs_Human} demonstrate that the trained model consistently outperforms surgeon's manual labeling, regardless of the distance threshold used. These qualitative and quantitative results in combination with the intensity distribution of labels illustrated in Figure~\ref{fig:hist} further confirm that the expert surgeon faces difficulty in correctly labeling bone contours in low-intensity regions. These findings underscore the superiority of our approach over the manual labeling.} 

\textbf{Challenges in Dataset Availability.} \textcolor{black}{ Unlike natural images, manually labeling bones in ultrasound images requires specialized expertise in ultrasound imaging and bone anatomy, making it financially and temporally expensive. This constraint is a key factor limiting the size of training datasets in existing studies \cite{review2}, which typically range from only dozens to a few thousand samples. The small dataset size restricts the performance and generalizability of trained models. Moreover, most existing datasets are not publicly available. As highlighted in prior reviews \cite{review1,review2}, the lack of a large-scale, publicly accessible dataset remains a major challenge in the ultrasound vision community. Our dataset collection methodology represents a solution to mitigate the dataset challenge. Furthermore, we make the proposed method, as well as the generated dataset publicly available. UltraBones100k is the largest known dataset with high quality bone labels, aiming to help advance future developments in intraoperative ultrasound for computer assisted surgery.}

\textbf{Generalizability.} \textcolor{black}{Although our dataset is the largest among existing works and includes specimens with diverse body conditions, its size remains relatively small compared to datasets in other domains. However, the fundamental principles of ultrasound imaging remain consistent across individuals, and the bone-soft tissue structure exhibits relatively uniform characteristics across different bodies. As a result, the appearance of bones in ultrasound images remains consistent. In the human body, bones are surrounded by soft tissues, and due to their high acoustic impedance, they consistently appear as bright regions in ultrasound images, regardless of anatomical differences. Additionally, since 2D ultrasound images capture only partial bone segments, they are often unaffected by variations in overall anatomical shape. The presence of shared structural features across bones—such as those in the arms and legs—further contributes to the model’s generalizability. Qualitative results in Figure~\ref{fig:hip_femur_results} indicate the model’s generalizability on ultrasound images of the pelvis and femur, even though it was trained on data collected from the lower extremities.}

\textbf{Limitations.} \textcolor{black}{Although the experiment results demonstrate the generalizability of the model trained on UltraBones100k, its performance may decline in anatomies with significantly different bone-soft tissue structures and shapes, such as the spine and skull. Depending on the required level of accuracy, additional data collection may be necessary for specific anatomies and pathological conditions. In some cases, this may involve the collection of in vivo data. Conducting data collection studies with a limited number of patients is a common and justifiable approach in such scenarios. Furthermore, the publicly available model pretrained on UltraBones100k can be fine-tuned with newly collected data to enhance its performance for these specific cases.} 


\section{Conclusion}
Our methodology presents a promising solution to create large-scale and highly accurate training data for bone surface extraction in ultrasound images without relying on expert labeling. Serving as a benchmark, the segmentation model trained on UltraBones100k outperformed an expert surgeon with significantly more complete bone labels, demonstrating its potential for downstream tasks, such as 3D bone surface reconstruction and registration, and clinical application.
% We proposed a method for collecting ultrasound datasets with automated bone annotation and a dataset., but its applicability to other anatomies requires further validation. Although the model outperformed the surgeon in annotation, improvements are needed in low-intensity regions. Advanced models could target these regions, and testing on images from different devices and anatomies is necessary to assess generalizability. 

% In conclusion, our methodology for collecting orthopedic ultrasound datasets presents a promising solution to the shortage of publicly available datasets in the ultrasound vision community. The deep learning model trained on our dataset outperformed an expert surgeon with significantly more complete bone annotation, demonstrating its potential for downstream tasks and clinical application. We believe this work will help drive meaningful advancements in the ultrasound based surgical navigation and robotic surgery, and inspire further exploration in this evolving field.






\section*{CRediT authorship contribution statement}
\textcolor{black}{\textbf{Luohong Wu}: Conceptualization, Data curation, Formal analysis, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing.  \textbf{Nicola Cavalcant}: Conceptualization, Data curation, investigation, Methodology, Resources, Writing – review and editing. \textbf{Matthias Seibold}: Conceptualization, Investigation, Methodology, Project administration, Supervision, Writing – original draft, Writing – review and editing. \textbf{Giuseppe Loggia}: Conceptualization, Methodology, Validation, Resources. \textbf{Lisa Reissner}: Methodology, Investigation, Validation, Resources. \textbf{Jonas Hein}: Methodology,  Writing – review and editing. \textbf{Silvan Beeler}: Project administration, Resources. \textbf{Arnd Viehöfer}: Conceptualization, Methodology, Resources, Project administration. \textbf{Stephan Wirth}: Project administration, Resources. \textbf{Lilian Calvet}: Methodology, Supervision, Writing – original draft, Writing – review and editing. \textbf{Philipp Fürnstah}: Conceptualization, Funding acquisition, Methodology, Resources, Project administration, Supervision, Writing – review and editing.}

\section*{Acknowledgement}
This research has been funded by the Innosuisse Flagship project PROFICIENCY No. PFFS-21-19. This work has also been supported by the OR-X, a Swiss national research infrastructure for translational surgery, and associated funding by the University of Zurich and University Hospital Balgrist.

\section*{Declaration of Generative AI and AI-assisted Technologies in the Writing Process}
During the preparation of this work the authors used ChatGPT in order to improve the readability and language of the manuscript. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the published article.

\bibliographystyle{elsarticle-num} 
\bibliography{reference}




\end{document}

\endinput
%%
%% End of file `elsarticle-template-num.tex'.
