




\paragraph{Evaluating LLM-Human Alignment}
Recent research has increasingly focused on probing LLMs political opinions. Most approaches typically follow a two-stage process: (1) assessing an LLM's political stance on specific topics, and (2) measuring how closely its responses align with human opinions.
A common strategy for evaluating LLM opinions involves using political orientation tests (e.g., Political Compass Test\footnote{www.politicalcompass.org/test}, as in \citealt{rottger-etal-2024-political,feng-etal-2023-pretraining}) or survey questionnaires (e.g., PewResearch ATP\footnote{www.pewresearch.org/writing-survey-questions/}, as in \citealt{santurkar2023whose}). To quantify the alignment between human and LLM responses, prior work typically measures the similarity of their opinion distributions using either (a) distance-based metrics—such as the 1-Wasserstein distance \cite{santurkar2023whose,sanders2023demonstrations} and Jensen–Shannon divergence \cite{durmus2024towards}—or (b) statistical analyses, including Cohen's Kappa \cite{Argyle_Busby_Fulda_Gubler_Rytting_Wingate_2023,hwang-etal-2023-aligning} and Pearson correlation coefficients \cite{movva-etal-2024-annotation}. 
% \ye{between what and what? you talk about the metrics, but what are these metrics comparing?}. 
We refer to \citealt{ma-etal-2024-potential} for an extensive survey of methods in this area. In this work, we use SCOPE to probe LLMs' political opinions because it offers several advantages over the above-mentioned political surveys used in previous studies: The cases in SCOPE are selected by experts, ensuring that they address the most important and publicly salient legal topics. Experts carefully word the questions and response options to be understandable to the general public. Moreover, political experts have annotated each case with thoughtfully chosen keywords, which facilitate our retrieval of relevant documents from large pretraining corpora, as detailed in \autoref{sec:pd_corpora}.
% \ye{the part on statistical significance seems out of place here. it's not the story you're trying to make, and it takes away from the actual content of the background which is the source of gauging human opinions and political stances}
% \ye{the whole part on the statistical test is out of place. you talk about part of the method that tests for statistical significance. it shouldn't be part of the background, but part of the method/results section}
% % \paragraph{Meta Evaluation Strategies}
% % Ma et al., Findings 2024  surveyed different evaluation approaches to validate the alignment of LLMs’ behavior with human. 
% Taking inspiration from NLG \ye{indicate the accronym the first time you introduce them} tasks such as machine translation and summarization, we categorize the alignment metrics into two broad meta evaluation strategies: instance-level vs. system-level. A group is regarded \textbf{The Instance-level alignment} metrics are often based on distance-based measures (JSD, WD, XXX, cite), which define the (mis)alignment as the distance between the opinion distributions of two groups on each individual instance \ye{this sentence is very complex. it has some grammatical issue as well. break it down. what's instance-level about it?}. The aggregated score is then used to evaluate the alignment of  two groups’ opinion distributions on the questions of a certain collection. On the other hand, \textbf{The system-level alignment} metrics involve correlation and statistical analysis (pearson correlation, tau, etc cite XXX). \ye{the distinction between the two is unclear..} These metrics evaluates how aligned are the opinion distributions of two groups across a collection of questions. However, we notice that significance tests are generally not used when comparing the llm-opinion alignment. Thus it is possible that the greater similarity to a certain demographic group (such libertarian) over another group (such as conservative) is attributable to chance rather than a systematic improvement. In this work, we focus on the analysis of system-level alignment, and introduce using \ye{grammar.} Williams test as significance test for comparing the difference of LLM's alignment with different groups. \ye{what is this test? what does it test for? cite?}





\paragraph{LLMs and their pretraining corpra}
Existing studies have explored the impact of biases in training corpora on LLM behavior, primarily through second-stage controlled training setups such as continual pretraining \cite{feng-etal-2023-pretraining, chalkidis-brandl-2024-llama}. While continual pretraining can offer valuable insights into the causal links between training data and model outputs, these studies rarely applied to study LLMs' behavior based on initial pretraining phase, where biases are fundamentally embedded. Additionally, it is also computationally expensive to conduct such extensive pre-training experiments on initial phase. %\ye{you're making two very distinct arguments. and i don't see what's the relevance on the first one? since data is typically shuffled, why does it matter where continual training phase happens?}
 % conducted continual training on texts reflecting diverse political perspectives to quantify how political bias in pretraining data influences fairness in high-stakes social-oriented tasks. 
% Moreover, this approach entails comparing two LLMs—a standard off-the-shelf model and another that is continually trained on a curated, much smaller dataset \ye{that's fine, what's the problem with that?}.
An alternative strategy involves investigating the correlation between biases in training corpora and those in model outputs \cite{seshadri-etal-2024-bias}. Previously this approach has been under exploited, primarily due to the limited accessibility of large-scale pretraining datasets.
% \ye{another relevant paper: https://arxiv.org/abs/2308.00755 (we studied biases in text-to-image models, but w/o continual training, just observations)}. 
Many commercial LLM providers (e.g., GPT-4 by \citealt{openai2023gpt4} and Claude by \citealt{anthropic2023claude}) disclose minimal information about their training sets, not even corpus size or data source. With the growing call in the academic community for transparency and accessibility of LLM pretraining data \cite{10.1145/3593013.3594002,10.1145/3594737}, several organizations have begun to make large-scale pretraining datasets publicly available, including \textit{RedPajama} \citep{weber2024redpajama} and \textit{Dolma} \cite{soldaini-etal-2024-dolma}. These initiatives are complemented by the development of APIs and analytical tooling platforms, such as WIMDB \cite{elazar2024whats}, which facilitate comprehensive analysis of the corpora. 
In this paper, we leverage WIMDB to analyze the political leanings in five publicly accessible corpora and subsequently evaluate how these leanings correlate with the outputs generated by various LLMs. 
% (see \autoref{sec:llms_corpora}).

% \ye{this section/subsec ends abruptly. be more explicit about how we make use of these advancements/technology to answer the questions we're interested in.}

% Previous research demonstrates that LLMs reflect the political leanings embedded in their training corpora by continual training on corpora with diverse political perspectives (cite Yulia, partisan twitter) \ye{why do you have to continue training? if you only show the effect of the model compared to the data it was trained on at the end, it doesn't really show that the rest of the data was important}. While continual training can help uncover causal links between the political leanings in training data and the outputs of LLMs, this approach is computationally intensive and typically out of reach for smaller academic labs \ye{wasn't this study done in an academic lab? how much data are we talking about?}. Further, the lack of transparency of the training data is another hindrance to investigate how the opinions in the training data influce the llm's behavior \ye{how does this point help your case? you can't do your analysis as well if you don't have access to the training data}. Due to the increasingly competitive nature of the field, many commercial LLM developers like GPT-4 (OpenAI, 2023) and Claude (Anthropic, 2023) have been disclosing minimal information of their training data. Recently there are some calls for better analyses and transparency to understand of LLMs’ training data, and subsequently to better understand the capabilities and drawbacks of LLMs. For example, the Allen Institute for AI offers open access to its pretraining dataset, Dolma \cite{soldaini-etal-2024-dolma}, providing a valuable resource for such analyses \ye{there are other open data/models. worth mentioning them all together, don't focus on ai2}. AI2 has also introduced WIMBD \cite{elazar2024whats} \ye{it's not just ai2.. don't use this company/institution glorification, it's strange ;) just use normal citation}, an API and tooling platform designed to facilitate large-scale corpus analysis. In this work, leveraging WIMBD, we explore the the political leanings in five public available corpora, and evaluate their correlation with different LLMs. 







