The increased adoption of Large Language Models (LLMs) and their potential to shape public opinion have sparked interest in assessing these models' political leanings. Building on previous research that compared LLMs and human opinions and observed political bias in system responses, we take a step further to investigate the underlying causes of such biases by empirically examining how the values and biases embedded in training corpora shape model outputs.
% \ye{but we have a bunch of models w/o their data. is the focus now the data?}. 
Specifically, we propose a method to quantitatively evaluate political leanings embedded in the large pretraining corpora. Subsequently we investigate to whom are the LLMs' political leanings more aligned with, their pretrainig corpora or the surveyed human opinions. As a case study, we focus on probing the political leanings of LLMs in 32 U.S. Supreme Court cases, addressing contentious topics such as abortion and voting rights. Our findings reveal that LLMs strongly reflect the political leanings in their training data, and no strong correlation is observed with their alignment to human opinions as expressed in surveys.
% \ye{i don't really understand this sentence. why would it? it's such a different sample.}. 
These results underscore the importance of responsible curation of training data and the need for robust evaluation metrics to ensure LLMs' alignment with human-centered values.
% \ye{I'm not sure what's the point of this last sentence. The first part is about the importance of curation, but it's not directly evident from what you said earlier. there's something missing in the thought chain that should be stated explicitly. the second part isn't clear, because you propose such a method, allegedly, so why is it important to have such robust evaluations? do we need more of these? is yours not good enough? what's missing?}


% (++Updated++) While previous research compared the LLMs and human opinions and observed political bias in LLMs responses,  we take a step further to investigate their underlying causes, empirically examining how the values and biases embedded in training corpora shape model outputs. Leveraging survey data from political science, this paper probes the political preferences of LLMs in U.S. Supreme Court cases, which often address contentious topics such as abortion and voting rights. We pose a central question: The court, the public, or the training data—whose political views do LLMs most closely align with? We postulate that LLMs inherently mirror the values they encounter in their training data, and we conduct an empirical investigation to test this claim. By analyzing the political leanings embedded in pretraining data, we compare LLMs’ alignment with the political stances found within these datasets against both public survey responses and official judicial votes. Our findings reveal that, although LLMs strongly reflect the political leanings of their training data, yet this effect is not observed in alignment with human opinions. These results underscore the importance of responsible curation of training data and the need for robust evaluation metrics to ensure LLMs’ alignment with human-centered values.


% This paper investigates the political preferences of large language models (LLMs) in the context of U.S. Supreme Court cases, which often address contentious political issues such as abortion and voting rights. We pose the central question: The court, the public, or the training data - whose political views are LLMs most aligned with? 
% % \ye{why is this sentence capitalized?} 
% While prior research focuses primarily on LLM alignment with human demographic groups \ye{what does it mean?}, it often overlooks the fact that LLMs learns from training corpora, which incorporate multiple standpoints instead of adhering to just a single viewpoint.
% % \ye{just one -> a single}  
% We postulate that LLMs inherently reflect the values they encounter in their training data and conduct an empirical investigation to validate this effect. We carry out an empirical investigation to examine the political leanings embedded in pretraining data and compare their alignment with LLMs to both with human respondents (public opinions) and with the Court (offical institution) viewpoints. 
% % We employ alignment metrics \ye{what's that?} and introduce statistical significance tests \ye{we don't introduce them. this is part of the details methodology. i wouldn't mention it in the abstract} to evaluate the extent of this alignment. 
% Our findings reveal that LLMs show a strong and statistically significant alignment with their training corpora, yet this effect is not observed in alignment with human opinions \ye{but you said earlier that it was simply not tested. i'm confused. why would we expect that if it wasn't tested?}. These results highlight the importance of responsible training data curation and underscore the need for more robust evaluation metrics to ensure LLMs’ alignment with human perspectives.

% \ye{earlier in the abstract you said that the training data contain multiple opinions, so how does it align with what you say about the alignment of the model with specific views?}


% \san{Is this your main argument  or Did i understand something wrong  -  Ensuring that LLMs produce politically impartial responses is increasingly critical to preventing information bubbles, promoting fair representation, and mitigating confirmation bias. While prior works have examined the political leanings of LLMs, we take a step further to investigate their underlying causes, empirically examining how the values, biases, and norms embedded in training corpora shape model outputs. }







% Large language models (LLMs), such as ChatGPT, are increasingly integrated into applications ranging from legal support to search engines, where they hold the potential to influence public opinion and societal views. 

% Their human-like responses and powerful capabilities have sparked research interest in understanding their opinions and values compared to Humans.  However, LLMs learn exclusively from their training data, meaning that any biases in the data are inevitably reflected in their behavior.
% In this work, we evaluate the political preferences of eight LLMs on key policy issues addressed by the U.S. Supreme Court. By comparing their outputs with human survey data and the Court’s rulings, our analysis confirms prior observations about the left-leaning tendencies of                                       LLMs (cite). To uncover the origins of this bias, we put forth a quantitative framework to investigate the political leanings reflected in the LLMs training data. Our analysis on five open-source pretraining corpora reveals that the political leanings of various pretraining data are strongly correlated to each other, but not well aligned with human respondents. Further, we observe that while misaligned with human populations, the ideological tendencies reflected in the LLMs’ outputs have a strong correlation with those reflected in the training data. These findings emphasize the need for greater transparency in curating LLM training data and highlight the importance of advancing methods to mitigate biases in the models.