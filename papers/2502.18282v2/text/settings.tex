% \paragraph{LLMs and Corpora}

% \label{app:model_card}
% \autoref{tab:model_card} contains the summary statistics of LLMs tested in this work, along with the corpora which they are trained on. 
\subsection{Evaluated LLMs} We evaluate eight models that have been fine-tuned for instruction following and conversational abilities. This includes seven open-source models: Gemma-7b-it \cite{team2024gemma}, Llama-3-8B-Instruct, Llama-70B-Instruct \cite{dubey2024llama3herdmodels}, OLMo-7B-Instruct, OLMo-7B-SFT \cite{groeneveld-etal-2024-olmo}, BLOOMZ \cite{muennighoff2022crosslingual}, and T0 \cite{sanh2021multitask}, as well as one closed-source model, GPT-4o \cite{openai2023gpt4}. Details about these models can be found in \autoref{tab:model_card}. Further implementation details are discussed in \autoref{app:implementation}.

% Our research centers on open-weight, instruction-tuned language models. Specifically, we examine the gemma-7b-it \cite{team2024gemma}, two variants of Llama3 in distinct sizes (8B and 70B) \cite{dubey2024llama3herdmodels} \ye{so far you only talked about the fact we want to compare to the data. but then here you start by describing open-weights, but close-data models. this is confusing. at least start with the fully open source models. and you should also mention earlier (intro, at least) that we also experiment with other kinds of models}, and two OLMo models of the same size but differing in their finetuning methodologies (SFT and Instruct) \cite{groeneveld-etal-2024-olmo}. Additionally, we include the multilingual model BLOOMZ \cite{muennighoff2022crosslingual} and the T0 model \cite{sanh2021multitask}. 
% Our focus on open-weight models stems from our interest in evaluating how these models respond to user instructions, as well as our commitment to upholding the principles of open science and reproducibility \ye{sounds weak}. 
\subsection{Pretraining Corpora}
\autoref{tab:model_card} lists the corresponding pretraining corpora (when available) of the LLMs we investigated in this work. It is important to note that among the various LLM-corpora pairs we consider, only the OLMo-SFT and OLMo-Instruct models were trained directly on the pretraining corpus \textit{Dolma} \cite{soldaini-etal-2024-dolma}. While for all other pairs, the LLMs may not have been trained exactly on the versions of the corpora we consider, due to factors such as filtering, or inclusion of additional data \cite{elazar2024whats}. Despite these discrepancies, we treat the documented corpora as reasonable proxies for analysis, as they represent the closest publicly available approximations of the actual training data for these models\footnote{All models examined in this paper have undergone post-training, such as Supervised or Instruction Fine-Tuning, which may also influence the opinions in models outputs. However, prior research \cite{feng-etal-2023-pretraining} suggests that the shift introduced by post-training is relatively small. We also explored the correlation between LLMs' political leanings and that in their post-training data, but did not observe any significant correlation. Further discussions can be found in \autoref{app:post-training}.}.
% \ye{what about the instruction tuning data? we need to acknowledge them. do we look into these datasets as well?}
% \ye{why is gemma not capitalized? at least in the table, the model short name should be capitalized imo}

% we know the data that was perfectly used for training, while for others - 
% % \ye{this last bit is very confusing. the whole story so far was you want to compare to the training data. but now you say that only 2 models were trained on known data? what about t0 btw?}
% \sx{
% We need to point out, as stated in section 4.X, we don't the exact pretraining data for most of these models, except for DOLMA and OLMo models.....the rest....}
% Extracting the court's voting the public's survey respondence from the Jesee 2022 dataset, as well as probing LLMs and detecting the stances in training corpora, provide us with a set of preference distributions D of different respondent groups. We now evaluate the LLMs' political preference alignment with respect to different groups, including the court, three demographic public groups (overall, democrats and republican), as well as five different pretraining corpora.
% due to the lack of details in most of the training data curation,


% \begin{figure*}[h]
% \centering
% \resizebox{0.99\linewidth}{!}{
% \includegraphics[width=\linewidth]{fig/williams.png}
% }
% \caption{P-value of Williams significance tests, in each subfig, where a colored cell in row i (named on y-axis), col j (named on x-axis) indicates that the llm m correlates significantly higher with group i than group j
% }
% \label{fig:williams}
% \end{figure*}
