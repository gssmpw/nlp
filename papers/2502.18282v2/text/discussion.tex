% \section{Implications and Future Directions}
% Ensuring that large language models (LLMs) remain politically impartial is crucial to preventing information bubbles, promoting fair representation, and mitigating confirmation bias.
% Our analysis reveals that LLMs, rather than aligning with human values, tend to reflect the political leanings in their training data, often exhibiting moderate-to-liberal tendencies. Our empirical results does not only directly underscores the necessity for further research in evaluation metric to validate the alignment of LLMs. It has also broader practical implications in two folds.  (++ Shorten the text+++)
% \subsection{Robust Evaluation Metrics}
% \begin{itemize}
%     \item signicifance test
%     \item Learn from the meta evaluation of machine translation and summarization
% \end{itemize}

\paragraph{Training Data Curation}
% Our empirical results show that LLM highly closely their training data's political leanings \ye{there's something off in this sentence}.
% While pretraining data forms the foundation of LLMs, their curation is an opaque process \ye{so? what's the point this sentence is trying to make?}.
% The selection of training data not only determines the knowledge base of an LLM but also shapes how it contextualizes information. 
Our empirical results indicate that LLMs closely reflect the political leanings present in their training data, raising concerns given the lack of transparency and accountability in the data curation process. Historians \cite{harari2024nexus} compare this process to the canonization of religious text, in which a group of religious authorities decides which works to include or exclude, subsequently shaping the evolution of beliefs and societal norms. Similarly, a small group of AI engineers determine which sources are deemed ``trustworthy'' and which are classified as ``harmful'', 
% (e.g., extremist content, conspiracy theories), 
ultimately shaping the epistemic landscape of AI-generated knowledge.
% Such centralized control can raise concerns about transparency, accountability, and diversity in the training data process \cite{10.1145/3531146.3533088}. 
To mitigate these issues, 
% collaboration between AI developers and policymakers is essential. 
the AI community can adopt ``datasheets''\cite{gebru2021datasheets}, which is widely used in the community benchmark datasets. The datasheets should document key metadata, including data sources, filtering methodologies, and known biases or limitations. Policymakers, in turn, should establish legal frameworks mandating independent audits and risk assessments of training data curation.
% Ultimately, embracing more transparent and collaborative approaches—similar to peer review and open-source methodologies in academia-may help address the risks associated with unchecked gatekeeping in AI development.

\paragraph{Public Discourse Framework}
Our research reveals that most LLMs exhibit alignment with their training corpora, yet not necessarily with the surveyed human opinions. Nonetheless, in the public discourse framework, attributing human characteristics to AI - known as anthropomorphizing — seems to be quite natural. 
% due to LLMs' fluent responses and advanced capabilities. 
This tendency may lead to an over-reliance on AI, as users might confuse AI-generated responses for human beings, leading to excessive trust \cite{google_pair_2019}. Further, anthropomorphism can obscure accountability, shifting the responsibility away from developers and onto the LLM itself. Recent studies \cite{mccoy2024embers} suggest moving away from anthropomorphic and advocate for a reframing of public dialogue in alternative conceptual frameworks, such as viewing LLMs as simulation systems of the integration of diverse perspectives in their training data \cite{shanahan2023role}. In conclusion, fostering a clear public understanding of the distinctions between AI and human beings is essential for a more responsible engagement with AI technologies.
% \ye{this paragraph can be shorten considerably.}