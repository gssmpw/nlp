% Large language models (LLMs) such as ChatGPT \cite{openai2023gpt4} are widely used in various applications. %, often generating opinionated language on subjective topics. 
Values and opinions expressed by large language models (LLMs) such as ChatGPT \cite{openai2023gpt4} have the potential to influence public perceptions and shape societal views. As such, it is essential to evaluate the political leanings of LLMs. Previous research (e.g., \citealt{perez-etal-2023-discovering,hartmann2023political}) found a misalignment between LLM outputs and human opinions, and several identified a left-leaning bias of LLMs.
\begin{figure}[t]
\centering
\includegraphics[width=0.97\linewidth]{fig/fig1.pdf}
\caption{Assessing the political leanings of LLMs, and comparing it with that in their training data, and of human respondents.}
% \ye{the figure isn't very clear. what are you trying to show? perhaps it's worth adding a y column on top? Also, the caption isn't very descriptive. perhaps something like: ``Assessing and comparing the political leanings of LLM, their training data, and human respondents.''}
\label{fig:fig1}
\end{figure}
LLMs derive their knowledge primarily from their pre-training data, which are typically composed of internet text. However, the structure of online content regularly skews toward overrepresenting specific viewpoints \cite{10.1093/pnasnexus/pgae474}, often introducing biases into these models. Such biases can then propagate through model outputs, reinforcing existing societal biases \cite{feng-etal-2023-pretraining}. Despite increasing awareness of these issues, to the best of our knowledge, no prior work has shown how to quantitatively analyze political bias in the content of large pre-training corpora and subsequently assess how well LLM outputs align with human opinions versus the biases in training data. To address this gap, we propose a pipeline to retrieve relevant documents from the pretraining corpora, then evaluate the political leanings expressed in these documents, and subsequently assess the alignment of  political leanings in pretraining corpora with the responses generated by the LLM. 

As a case study, we focus on US Supreme Court cases, which frequently address contentious and politically charged issues, such as death penalty, abortion, same-sex marriage, and voting rights, making them strong indicators of political leanings. 
% Political scientists \cite{doi:10.1073/pnas.2120284119,doi:10.1126/sciadv.adk9590} have shown that the Court has shifted to a more conservative stance relative to the general US public since 2020. 
% LLM pre-training data are drawn from both public discourse (e.g., forum discussions, blog posts) and legal sources and literature (e.g., legal opinions; \citealt{niklaus-etal-2024-multilegalpile}). 
Leveraging the \textsc{SCOPE}~\cite{doi:10.1073/pnas.2120284119}\footnote{\citealt{doi:10.1073/pnas.2120284119} has not named the dataset. Hereafter, we refer to the dataset as \textsc{SCOPE}: \textbf{S}upreme \textbf{CO}urt Case \textbf{P}olitical \textbf{E}valuation.} survey data on US Supreme Court cases from political studies, 
% allows us to systematically assess LLMs' alignment with three different groups: the public, the Court, and their pre-training corpora
this paper examines the political leanings of eight LLMs and five open-source pre-training corpora, comparing them to human survey responses and Supreme Court rulings\footnote{We will release the code and data upon publication.}. The main contributions of our work are threefold:
\begin{itemize}
    \item We conduct a quantitative analysis of political bias in large pre-training corpora by examining the political stance of the documents in the corpora.
    \item We compare LLMs' alignment with both surveyed human opinions and with their pre-training corpora (as illustrated in \autoref{fig:fig1}).
    \item Our empirical findings indicate that LLMs exhibit significant alignment with their training corpora, yet we do not find strong alignment with human opinions. This highlights the critical need for responsible data curation and rigorous evaluation of LLM alignment.
\end{itemize}
% Large language models (LLMs) like ChatGPT are widely used in various applications, often generating opinionated language on subjective topics. The fact that values and opinions\footnote{AOV, In this paper, we use the terms attitude, opinion and value interchangeably. Further "While we use the term “LM opinions” for brevity, we do not view LMs as having their own opinions, but instead as reflecting those of humans involved in their design process." (Santukar)} presented by LLMs have the potential to influence public perceptions and shape societal views, motivates researchers to evaluate the political leanings of LLMs. Further, LLM’s fluent responses and powerful capabilities creates a human-like impression, and it is natural to use anthropomorphism language to describe LLMs that we use to describe human behaviour. For example, papers often evaluate the `opinions' of LLMs and subsequently compared the LLMs and human opinions and observed political bias in LLMs responses. For instance., [MODEL X]has a mid-liberal political leaning (e.g., Perez et al. (2022b) and Hartmann et al. (2023) \ye{use proper citation}) .




% However, LLMs are only capable of learning from the data they were trained on \ye{idk what only means, or how to quantify it. i'd change the argument here to say that llms' main source of knowledge is their training data, and thus...}, which is a great amount of textual data from billions of humans- humans with different perspectives. That means that when prompted, LLMs might sometimes generate responses that incorporate these varied perspectives rather than a single viewpoint. 
% This motivates us to inverstigate LLM’s opinion alignment with not only human respondent groups, but also with their pretraining corpora. We introduce a framework \ye{?} to quantitative evaluate the political leanings in the pretraining corpora. 

% In this work, we investigate the political preferences of LLMs leveraging the survey data from political science in the context of U.S. Supreme Court cases, which often address contentious issues such as abortion, gun control, and voting rights. Political scientists revealed that the Court has become more conservative than the US public \ye{cite?}.  Given that the pretraining data of LLMs are scraped from the internet, which includes both the public opinions (forum discussions, blog posts etc.) and the courts’ opinion (official legal documents; \cite{niklaus-etal-2024-multilegalpile})
% \ye{do we know that?}. The special design of the Jesse2022 \ye{fix citation} allows to study whether LLMs align more with institutional or Public Opinion \ye{don't capitalize. but also, what does that mean? how do you quantify it?}, or a mix of it (the training data)?


% We conduct a systematic evaluation of how well the LLMs align with different groups\footnote{the public, the court and the corpora}. \san{put them explicitly rather than groups}\ye{+1. this is important, don't use a footnote for this information}
% Further,  we also conduct significance test to asses whether the better alignment with a certain group are attributable to chance rather than a systematic improvement.

% We evaluate the political preferences of eight LLMs and five open-source pretraining corpora on key policy issues addressed by the U.S. 
% Supreme Court in Jesses \san{fix name} Dataset. By comparing with human survey data and the Court’s rulings using various alignment metrics, our results reveal that LLMs are significantly \ye{?} aligned \ye{with} the \ye{their} training corpora; but such significant alignment is not found to human \ye{which alignment? to the training data? ambiguous}. Our empirical results have practical implications in two folds \ye{strange phrasing}. For researchers in the AI community, we advocate for the further research \ye{fix grammar}  in evaluation metric to validate the alignment LLMs responses \ye{vague. say what you mean more explicitly}. For policy makers, we advocate for the regulation for the curation of training data \ye{fix grammar}, to ensure the promote transparency and accountability \ye{grammar}. For public communication discourse, we suggest moving away from framing LLMs as if they were humans \ye{this has nothing to do with the paper. it shouldn't appear in it, especially not in the intro imo.
% in general, in papers, you want to have one message, and make it super clear and sharp. the moment you have more than one, it becomes complex, and it dilutes the point you're trying to make. also relevant for the following sentence.}. Instead, we should educate the public that LLMs should be understood as simulation systems that model human behaviors based on the distributions present in training data.

% Our main contributions are three-fold:

% \begin{enumerate}
%     \item  We propose to quantitatively investigate the political preference in the LLMs pretraining corpora to understand the underlying causes of the political bias in LLMs .
%     \item We oberserved that LLMs align more strong with their training corpora than human. Our further significanse test confirm the.....
%     We conduct significance tests and provide an analysis of different alignment evaluation metrics. \ye{using significance tests is not a contribution.} Our results underscores the necessity for more studies in the evaluation metrics of human-LLM alignment.
%     \item Our Empirical experiments on the  LLMs' political preferences on cases from the U.S. Supreme Court reveals that LLMs, rather than aligning with human values, tend to reflect the political leanings in their training data.
% \end{enumerate}

