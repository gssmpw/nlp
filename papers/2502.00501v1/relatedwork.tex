\section{Literature Review}
%%%%%%Section 2%%%%%%%

In this section, we focus on the feature selection of both traditional and state-of-the-art causal inference literature. Ideally, exact matching would be the best way to reach an unbiased estimation of causal quantity since it reaches the exact covariate balance. However, strict exact matching may result in no matched pairs when dealing with high-dimensional data. Therefore, various distance metrics are developed for matching process. For instance, Mahalanobis Distance Matching (MDM) defines Mahalanobis distance \cite{rubin1979using}, while Coarsened Exact Matching (CEM) \cite{iacus2012causal} introduces binning methods. However, calculating Mahalanobis distance is time-consuming \citep{stuart2010matching}, while the strata of CEM required for matching would increase exponentially \citep{roberts2015matching}. Propensity score matching (PSM), introduced by \cite{rosenbaum1983central}, is more popular than previous distance measures due to its efficiency and ability to handle high-dimensional data. PSM uses a propensity score defined as the probability of the units being treated as the distance measure. It maps all the covariates into a one-dimensional scalar, thus making all the points from the treatment and control groups comparable. However, some researchers argue that PSM creates a fully randomized design instead of a fully blocked randomized experimental design \citep{king2019propensity}, and may provide non-robust matched pairs.

The unscalable and non-robustness properties of traditional matching algorithms drive researchers to consider feature selection in causal inference. By reducing the dimension of the observational data, the subsequent matching algorithms can reach a lower bias and variance. \cite{wang2012bayesian} proposed the Bayesian Effect Estimation (BAC), which leverages a two-stage framework to select the features. It incorporates both prior knowledge about the confounding factors and the observed data and uses the estimation of the probability distribution of the exposure effect on the outcome model to adjust for the confounding factors. \cite{talbot2015bayesian} proposed the Bayesian Causal Effect Estimation Algorithm (BCEE), which first includes potential confounders and modify the effect of the treatment, then estimates the model parameters by using Markov Chain Monte Carlo (MCMC) methods to generate posterior distributions of the treatment effect. Although powerful for reaching unbiased estimation, BCEE is time-consuming and is not scalable to high-dimensional data. \cite{ertefaie2018variable} proposed the penalized modified objective function estimators (PMOE) method, which defines a penalized objective function that combines both least squares and maximum likelihood estimates of the parameters in the exposure and outcome model and performed slightly better than BAC.

Inheriting the two-stage structure model, \cite{shortreed2017outcome} proposed the Outcome-Adaptive Lasso (OAL) method, which leverages the principles of the adaptive lasso (ADL) initially proposed by \cite{zou2005regularization} and aligns the exposure model with the prior knowledge indicated by the coefficients of the outcome model. Following the augmented form of elastic net estimator \citep{zou2006adaptive}, which is more stable than the adaptive lasso on highly correlated datasets, \cite{balde2022reader} proposed a generalized OAL (GOAL) model for causal inference and presents a smaller bias on high-dimensional data compared to the OAL. Then, \cite{islam2021feature} proposed the outcome adaptive elastic net (OAENet) method, presenting the adaptive elastic net estimator and demonstrated through testing on both synthetic and real-world datasets, particularly showcasing its effectiveness in handling highly correlated structure datasets. However, OAENet struggles to maintain the oracle property.

In addition to the two-stage framework, several other recently developed methods based on causal graph theory have also demonstrated promising results in feature selection for causal inference. For instance, \cite{gruber2010application} proposed the collaborative targeted maximum likelihood estimation (CTMLE) method, which employs a targeted semi-parametric double robust plug-in estimator to estimate the average treatment effect (ATE). \cite{kursa2010boruta} introduced the Boruta method, which utilizes random forest for classification. In Boruta, the treatment indicator (T) and the outcome variable (Y) are separately synthesized in a random forest classifier, and the Z score is considered to minimize mean accuracy loss. \cite{de2011covariate} proposed the DWR (De Luna, Waernbaum, and Richardson) causal-graph framework, which performs non-parametric graph-based covariate selection.

Besides the feature selection methods discussed above, some researchers have also attempted to utilize machine learning based feature selection methods. For example, \cite{wang2021flame} proposed the Fast Large-scale Almost Matching Exactly Approach (FLAME) method, which leverages two factors (prediction error (PE) and balance factor (BF)) and uses a penalty hyperparameter to trade off the two standards. Then FLAME performs a backward feature selection process, aiming to choose exactly the set of covariates that can predict the outcomes. Although powerful and time-efficient, FLAME is not recommended to be used when there are continuous covariates unless the assumption that binning the covariates wouldn't affect causal estimation is met \citep{wang2021flame}.

In this paper, we propose a novel feature selection framework for causal inference that addresses the limitations of existing models. Designed to minimize selection bias and variance, the framework integrates an SVM estimator for the exposure model and a penalty smoothing function to enhance performance and achieve the oracle property. This robust three-stage approach demonstrates superior capability in identifying confounders and pure outcome predictors.

Our key contributions can be summarized as follows:

1. \textbf{Mitigate Selection Bias}: Selection bias in Causal Inference is a key challenge \citep{vanderweele2019principles, wooldridge2016should, lu2020feature}. Our proposed feature selection method is designed to overcome the selection bias. Experimental results using state-of-the-art algorithms leveraging widely used synthetic datasets \citep{shortreed2017outcome, islam2021feature, wang2012bayesian} where the ground truth (i.e., the true treatment effect) is known show that applying our proposed feature selection algorithm in the design stage helps to mitigate the selection bias of treatment estimation.
\\ \hspace*{1em} 2. \textbf{Practical Application}: Interpretability is one of the key challenges for any causal inference method. Practitioners, such as medical experts, policymakers and scientists, do not accept findings unless they are interpretable. By allowing feature selection in the design stage using the proposed technique, we enhance the robustness of causal conclusions with fewer features. The smaller set of selected features improves the interpretability of these conclusions, making them more accessible to practitioners, including policymakers and medical experts. Moreover, our proposed feature selection method selects very few features from the NSDUH dataset to estimate ATT. The insight gained from the feature selection could guide researchers to focus on fewer features while collecting data (only the selected features) for future studies of similar problems, thereby reducing time and cost in scientific research and real-world applications. 
\\ \hspace*{1em}3. \textbf{Stability and Consistency}: Real-world observational data, such as healthcare data, often suffers from class imbalance issues (where the treatment group is much smaller than the control group). Our proposed algorithm is stable and robust, especially when large number of bootstrap iterations are required to address the class imbalance issue. Experiments with a real-world healthcare dataset (NSDUH dataset) demonstrate that our proposed feature selection technique outperforms other state-of-the-art methods, such as OAENet, in maintaining consistency (oracle property) when selecting features. Our analysis illustrates the robustness of our framework, showing that the selected variables remain largely consistent across different instances drawn from the same dataset.
\\ \hspace*{1em} 4. \textbf{CPU Time}: Our feature selection algorithm improves the robustness of conclusions by reducing bias in the estimation of the average treatment effect with selected features, while also reducing computation time. This shows the scalability and promise of our proposed method for unbiased estimation of ATT leveraging real-world high-dimensional observational data.   


%%%%%%%%%%%%%%Section 3%%%%%%%%%%%%%%