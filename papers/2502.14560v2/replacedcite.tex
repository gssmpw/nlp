\section{Related Work}
\label{sec:rel}

\textbf{Preference Learning Algorithms.}Reinforcement Learning from Human Feedback %also known as dueling RL %____ or preference-based %RL ____, 
has become a crucial component of recent Large Language Models (LLMs) such as ChatGPT ____. While the classical RLHF pipeline traditionally uses Proximal Policy Optimization, several alternative approaches have been proposed. These include but not limited other RL-based training algorithms ____, rejection sampling ____, conditional supervised fine-tuning ____, and Direct Preference Optimization ____. Among these alternatives, DPO has gained significant attention due to its simplicity and robust performance. Following the introduction of DPO, numerous works ____ have attempted to improve its performance by modifying the DPO objective.

\textbf{Data Selection in LLM Fine-tuning.} Data selection is crucial in LLM post-training ____, primarily due to two key observations: LLM training typically converges rapidly, and excessive data can degrade model performance through overfitting or exposure to toxic content ____. Recent research has focused on enhancing instruction tuning efficiency by identifying high-quality subsets from large instruction datasets ____, often adapting active learning query strategies ____ to assess sample uncertainty and diversity. However, data efficiency in preference learning remains relatively unexplored. While some studies studied annotation cost reduction in preference dataset creation ____, our work firstly provides clear criteria for identifying informative samples while filtering toxic ones, thereby improving both DPO's efficiency and performance. Furthermore, our method extends to iterative DPO ____ and its variants ____, where training data is generated online.