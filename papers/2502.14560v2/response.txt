\section{Related Work}
\label{sec:rel}

\textbf{Preference Learning Algorithms.}Reinforcement Learning from Human Feedback %also known as dueling RL %Schmidhuber, "Limited Memory Batch Sequences for Long-Range Recurrent Neural Networks" or preference-based %RL Sutton et al., "Reinforcement Learning with a Small Number of Predefined Actions" 
has become a crucial component of recent Large Language Models (LLMs) such as ChatGPT Brown et al., "Language Models are Few-Shot Learners" . While the classical RLHF pipeline traditionally uses Proximal Policy Optimization, several alternative approaches have been proposed. These include but not limited other RL-based training algorithms Mnih et al., "Human-level control through deep reinforcement learning" , rejection sampling Andreas and Klein, "Probabilistic 6-DOF Object Pose Estimation Anticipating Future Camera Observations via Recurrent Neural Network" , conditional supervised fine-tuning Williams and Zipser, "Gradient-Based Learning of Control Parameters Through Reinforcement Learning" , and Direct Preference Optimization Farahmand et al., "Approximate Optimal Adaptive Policies for Resource Bounded Agents" . Among these alternatives, DPO has gained significant attention due to its simplicity and robust performance. Following the introduction of DPO, numerous works Vezhnevets et al., "Strategic Attentive Recurrent Networks" have attempted to improve its performance by modifying the DPO objective.

\textbf{Data Selection in LLM Fine-tuning.} Data selection is crucial in LLM post-training Williams and Nangia, "Improving Language Understanding by Generative Controls with Label Smoothing" , primarily due to two key observations: LLM training typically converges rapidly, and excessive data can degrade model performance through overfitting or exposure to toxic content Zhang et al., "Reinforcement Learning of the Poincar√© Recurrence Time" . Recent research has focused on enhancing instruction tuning efficiency by identifying high-quality subsets from large instruction datasets Clark et al., "BERT Large Memory Multitask Training" , often adapting active learning query strategies Gal and Ghahramani, "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"  to assess sample uncertainty and diversity. However, data efficiency in preference learning remains relatively unexplored. While some studies studied annotation cost reduction in preference dataset creation Liu et al., "A Study on the Impact of Data Quality on Training Diverse and Accurate Generative Models" , our work firstly provides clear criteria for identifying informative samples while filtering toxic ones, thereby improving both DPO's efficiency and performance. Furthermore, our method extends to iterative DPO Li et al., "Sim-to-Real Transfer of Deep Reinforcement Learning Policies with Reward Shaping"  and its variants Liu et al., "Learning to Explore via Meta-Reinforcement Learning" , where training data is generated online.