\section{Methodology}
\label{sec:method}


We follow the model from \citet{zhu2023principled} to illustrate why data selection can improve model performance. We assume that reward model $r(x,y)=\langle \phi(x,y),\omega^*\rangle$ with some feature function $\phi(\cdot,\cdot)$. 
For reward learning, our reward model can be an explicit $r(x,y)$~\citep{ouyang2022training}, while for DPO, $\beta\log\frac{\pi_\theta(y|x)}{\pi_{\mathrm{ref}}(y|x)}$ plays the role of reward model implicitly~\citep{rafailov2024direct}.
Based on observations in previous literature, we
can derive such features by 
removing the last layer of the pre-trained model. However, both humans and other large models may use inaccurate reward functions to generate labels, where the labels represent the ranking of two responses. We say the preference between $y_w$ and $y_l$ is generated by $r(x,y_w)-r(x,y_l)+\zeta$ where $\zeta$ is an exogenous error. 
We use $\Delta\phi(x)$ to denote $\phi(x,y_w)-\phi(x,y_l)$ for simplicity.

\textbf{Parameter Shrinkage.} Here, we hope to find $\omega$ to minimize
\begin{align}\label{eq:est_beta}
\mathcal{L}_{\mathrm{RM}}(\omega)&=-\mathbb{E}_{x,\zeta} \Big[\frac{1}{1+e^{-\langle\Delta\phi(x),\omega^*\rangle-\zeta}}\log(\frac{1}{1+e^{-\langle\Delta\phi(x),\omega\rangle}})\nonumber\\
&+\frac{1}{1+e^{\langle\Delta\phi(x),\omega^*\rangle+\zeta}}\log(\frac{1}{1+e^{\langle\Delta\phi(x),\omega\rangle}})\Big].
\end{align}
It holds that the first-order condition is 
\begin{align}
   & \mathbb{E}_{x,\zeta}\Big[\frac{1}{1+e^{\langle\Delta\phi(x),\omega^*\rangle+\zeta}}\frac{e^{\langle\Delta\phi(x),\omega\rangle}\Delta\phi(x)}{1+e^{\langle\Delta\phi(x),\omega\rangle}}\Big]\nonumber\\
    =& \mathbb{E}_{x,\zeta}\Big[\frac{1}{1+e^{-\langle\Delta\phi(x),\omega^*\rangle-\zeta}}\frac{e^{-\langle\Delta\phi(x),\omega\rangle}\Delta\phi(x)}{1+e^{-\langle\Delta\phi(x),\omega\rangle}}\Big].
    \label{eq:FOC}
\end{align}
Since we know that $\langle \Delta\phi(x),\omega^*\rangle$ is positive, when $\zeta$ is small comparing to the margin, it holds that $\frac{1}{1+e^{\langle\Delta\phi(x),\omega^*\rangle+\zeta}}$ is convex with respect to $\zeta$. Due to Jensen's inequality, it holds that 
\begin{align*}
    &\mathbb{E}_{x,\zeta}\Big[\frac{1}{1+e^{\langle\Delta\phi(x),\omega^*\rangle+\zeta}}\frac{e^{\langle\Delta\phi(x),\omega\rangle}\Delta\phi(x)}{1+e^{\langle\Delta\phi(x),\omega\rangle}}\Big]
    \\
    \ge& \mathbb{E}_{x}\Big[\frac{1}{1+e^{\langle\Delta\phi(x),\omega^*\rangle}}\frac{e^{\langle\Delta\phi(x),\omega\rangle}\Delta\phi(x)}{1+e^{\langle\Delta\phi(x),\omega\rangle}}\Big].
\end{align*}
Similarly, we have 
\begin{align*}
    &\mathbb{E}_{x,\zeta}\Big[\frac{1}{1+e^{-\langle\Delta\phi(x),\omega^*\rangle-\zeta}}\frac{e^{-\langle\Delta\phi(x),\omega\rangle}\Delta\phi(x)}{1+e^{-\langle\Delta\phi(x),\omega\rangle}}\Big]
    \\
    \le& \mathbb{E}_{x}\Big[\frac{1}{1+e^{-\langle\Delta\phi(x),\omega^*\rangle}}\frac{e^{-\langle\Delta\phi(x),\omega\rangle}\Delta\phi(x)}{1+e^{-\langle\Delta\phi(x),\omega\rangle}}\Big].
\end{align*}
Since the optimal $\omega$ is $\omega^*$ without $\zeta$, plugging $\omega^*$ in \Cref{eq:FOC} will cause the left-hand side to be greater than the right-hand side. Therefore,
the optimal $\omega$ with the existence of $\zeta$ intends to shrink to the original point compared to $\omega^*$ so that the first-order condition is still satisfied.  

We provide the underlying intuition with an extreme example. If $\mathbb{V}(\zeta)$ goes to infinity, the preference between $y_w$ and $y_l$ mainly depends on $\zeta$,
approaching a Rademacher distribution,
then $\omega=0$ could be a good solution to \Cref{eq:est_beta}. 
In other words, $\zeta$ offsets part of the information provided by the reward model, causing the model's parameters to shrink toward zero. 
Thus, data selection is essential for acquiring policies with good performance.
Finally, we remark that $\zeta$ can come from multiple resources, including human classification errors, different embeddings or reward models from other LLMs and so on.

\textbf{Parameter Inflation.}
We next explain why selecting data points based on the margin can lead to parameter inflation, thereby offsetting the parameter shrinkage caused by errors.

First, when the margin is large, namely, $\langle \Delta\phi(x),\omega^*\rangle+\zeta$ is large, from the S-shaped graph of $\sigma(\cdot)$, we know that the slope is very small in this area. As a result, the probability of preference reversal caused by $\zeta$ is low, which means the likelihood of incorrect samples is also low.

Secondly, given prompt $x$, as we select data with large $\langle \Delta\phi(x),\omega^*\rangle+\zeta$, the posterior distribution of $\zeta$ is skewed toward the positive side. Therefore, the preferences corresponding to this kind of data are more pronounced, leading to inflated estimates of $\omega$ in \Cref{eq:est_beta}.

Finally, we point out that if realized $y_w$ and $y_l$ are all separable, proportional scaling of $\omega$ can reduce the value of \Cref{eq:est_beta} continuously. Hence, some techniques like regularization or early stopping when training are indispensable.

In summary, inaccuracies in the reward model can cause the parameters of LLMs to shrink toward zero. By selecting data with larger margins, we can compensate for the performance degradation caused by this shrinkage. The balance between parameter shrinkage and inflation offers the potential to enhance the performance of LLMs.


\subsection{Dual-Margin guided Data Selection for Preference Learning}

In this section, we present the Dual-Margin (DM) guided efficient preference data selection that enhances the performance of direct preference optimization while significantly reducing the training cost. Driven by the theoretical result, our main idea is to let the model reach an overall high margin during preference learning. We realize this by providing a robust estimation of reward margins for the entire dataset ahead of full-set training, which then allows efficient high-margin data filtering. DM operates in two stages: (1) computing external and implicit rewards, and (2) fusing these rewards to suppress noise and achieve more reliable margin estimates.

\paragraph{Initialization.} We aim select a subset $\mathcal{D}_{\mathrm{train}}$ from $\mathcal{D}=\{(x^i,y_w^i,y_l^i)\}_{i=1}^{N}$, such that the model $\pi_{\theta}$ trained on the $\mathcal{D}_{\mathrm{train}}$ achieves performance comparable or better than that trained on $\mathcal{D}$. To guide this selection, we leverage an external reward model $r_{\mathrm{ex}}(x,y)$ and a small seed dataset $\mathcal{D}_0$ to learn an implicit reward model $r_{\mathrm{im}}(x,y)$. 

\paragraph{Step 1: Reward calculation.} First, we calculate $r_{\mathrm{ex}}(x^i,y_w^i)$ and $r_{\mathrm{ex}}(x^i,y_l^i)$ for each datum in $\mathcal{D}$. 
\citet{rafailov2024direct} proves that we can calculate implicit reward as 
\begin{align*}
   r_{\mathrm{im}}(x,y)=\log \frac{\pi_\theta (y|x)}{{\pi_{\mathrm{ref}}}(y|x)}=\log \frac{\prod_j \pi_{\theta}(y_j|x,y_{1,\cdot,j-1})}{\prod_j \pi_{\mathrm{ref}}(y_j|x,y_{1,\cdot,j-1})}.
\end{align*}
As this reward margin between chosen and rejected samples calculated by different models shows a strong correlation (refer to Figure~\ref{fig:visual}), we propose disentangling its calculation from the target model $\pi_\theta$. In particular, we introduce a small model $\pi_s$, and fine-tune it on $\mathcal{D}_0$ to get a weakly aligned model $\pi_s^1$. These two models are then utilized for low-cost implicit rewards calculation on $\mathcal{D}$.

\paragraph{Step 2: Margin fusion.} After calculating the margins $m_{\mathrm{ex}}=r_{\mathrm{ex}}(x,y_w)-r_{\mathrm{ex}}(x,y_l)$ and $m_{\mathrm{im}}=r_{\mathrm{im}}(x,y_w)-r_{\mathrm{im}}(x,y_l)$, we consider appropriate fusion strategies for combining these two signal sources to reduce noise in the reward modeling process. The simplest approach, which we call \textbf{DM-ADD}, directly sums the signals. This strategy reflects a lenient selection criterion where samples are considered valuable if they demonstrate a high margin from either reward source.

A strict fusion approach considers that samples should receive low priority if they show a low margin in either signal. We first transform margin values to margin-guided probability through a simple linear transformation
$$
\mathbb{P}(m) = \frac{\mathrm{clip}(m, M_1, M_2) - M_1}{M_2 - M_1}, \quad \text{ for } m \in \{m_{\mathrm{ex}}, m_{\mathrm{im}}\},
$$
where $\mathrm{clip}(m) = \min(\max(m, M_1), M_2))$ and $(M_1, M_2)$ are tuning parameters. Following the fusion principle in multi-view probabilistic clustering \citep{deng2024a3s}, we further obtain 
\begin{align}
   &\mathbb{P}(y_w\ge y_l | m_\mathrm{ex},m_\mathrm{im})  \notag\\
   =&\frac{\mathbb{P}(m_\mathrm{ex})\mathbb{P}(m_\mathrm{im})}{\mathbb{P}(m_\mathrm{ex})\mathbb{P}(m_\mathrm{im})+(1-\mathbb{P}(m_\mathrm{ex}))\cdot(1-\mathbb{P}(m_\mathrm{im}))}.
    \label{eq:mul}
\end{align}
This adaptive approach mitigates the adverse effects of outlier samples with unusually high margin values. See more implementation details in Appendix~\ref{app:imp}. Overall, this fusion approach is referred to as \textbf{DM-MUL}.

\paragraph{Sample Selection.} We select the samples with the highest fused margins to construct $\mathcal{D}_{\mathrm{train}}$. After selection, the subset can be used for DPO training on any target model.

\subsection{Data Efficient Online Iterative DPO} 
\label{methd:online}

Online RLHF \citep{xiong2024iterative} has gained significant attention for its effectiveness in aligning models with human preferences. The method iteratively generates multiple responses for given prompts and employs an external reward model to identify Best-of-N (BoN) and Worst-of-N (WoN) responses, forming preference pairs. This `self-reward maximization' approach effectively sharpens the model's response space by reducing the probability of generating low-quality responses \citep{huang2024self}. However, the online generation process still produces ambiguous samples, particularly when prompts either have highly deterministic answer patterns or are exceptionally challenging for models to address. Such on-policy data pairs may not effectively guide model improvement. To address this problem, we propose a straightforward solution called DPO-DM: incorporating DM into each round of the on-policy data collection process and conducting preference optimization exclusively on selected high-margin samples. This approach can substantially reduce training costs, particularly when dealing with large prompt spaces.
