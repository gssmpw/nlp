
\section{Datasets and Evaluation Details}
\label{app:data}

\paragraph{Data information.} The detailed information about the datasets used in the experiments is shown in Table~\ref{tab:data_detail}. The test sets of \textbf{TL;DR} and \textbf{HH} are sampled from their original large testing pool, and we utilize prompts in AlpacaEval as the test sets for all models trained on \textbf{UltraFeedback} and its variants. In particular, results in Table~\ref{tab:3b-test} utilize Text-Davinci-003 generated answers as reference response as it can better reflect how models' ability varies with different training data, and results in other tables all utilize GPT4-1106-preview generated answer as reference response (i.e., AlpacaEval 2.0).

\begin{table}[h]
    % \vspace{-0.3cm}
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0cm}
    \vspace{-15pt}
    \setlength{\tabcolsep}{40pt} 
    \caption{Statistical information about the training and evaluation datasets used in the experiments.}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
        Dataset & $|\mathcal{D}_{train}|$ & $|\mathcal{D}_{test}|$ & Type\\
        \midrule
        TL;DR & 92858 & 400 & Summarization \\
        HH & 86372 & 400 & Helpful \& Harmless \\
        UltraFeedback & 61135 & - & Hybrid \\
        Llama-UltraFeedback & 59876 & - & Hybrid \\
        Mistral-UltraFeedback & 59720 & - & Hybrid \\
        AlpacaEval & - & 805 & Chat  \\
        \bottomrule
    \end{tabular}}
    \label{tab:data_detail}
    \vspace{-15pt}
\end{table}

\paragraph{Evaluation details.} Win rate judgments serve as a key evaluation metric across our experiments. For the \textbf{TL;DR} and \textbf{HH}, we slightly modify the evaluation prompts suggested by \citet{rafailov2024direct} for the win rates judge. We use the original prompt designed for AlpacaEval and AlpacaEval 2.0 benchmark in all testing.

{\small \begin{spverbatim}
**TL;DR win rate prompt**: Which of the following summaries does a better job of summarizing the most important points in the given forum post, without including unimportant or irrelevant details? A good summary is both precise and concise.

Post:
{Post}

Summary A:
{Summary_A}

Summary B:
{Summary_B}

FIRST provide a one-sentence comparison of the two summaries, explaining which you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your choice. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
Preferred: <"A" or "B">
\end{spverbatim}}

{\small \begin{spverbatim}
**HH win rate prompt**: For the following query to a chatbot, which response is more helpful or harmless? A helpful response provides useful information, while a harmless response avoids causing/suggesting harm or discomfort.

Query: {query}

Response A:
{res_a}

Response B:
{res_b}

FIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful or harmless. SECOND, on a new line, state only "A" or "B" to indicate which response is more helpful or harmless. Your response should use the format:
Comparison: <one-sentence comparison and explanation>
More helpful or harmless: <"A" or "B">
\end{spverbatim}}


\section{More Implementation Details}
\label{app:imp}
We present the implementation details of our baseline methods: TOP, MID, and BOT using margins calculated from three metrics, Conditional Perplexity (CPPL), External (Ex), and Implicit (Im) rewards. Subsequently, we describe the implementation of DM-MUL in our experimental setup. 

The baseline strategies are implemented as follows: first, we eliminate outlier samples with extreme margin values (both positively high and negatively low) for CPPL, Ex, and Im metrics. For the TOP and BOT strategies, we select samples based on their ranking positions at the upper and lower ends of the distribution, respectively. The MID strategy involves random sampling from the subset of samples whose margin values fall within the interval $[-\tau,\tau]$, where $\tau$ is set to 0.1 for CPPL and 1.0 for Ex/Im metrics.

For DM-MUL, we set $M_1 = -2$ as the lower bound for both external and implicit reward margins. The upper bound $M_2$ is determined dynamically based on two conditions: (1) The number of samples with margin values in the interval $[M_2, \text{max margin}]$ is less than 30, or (2) The number of samples in $[M_2, \text{max margin}]$ is less than $\text{max margin} - M_2$.

\section{Visualization of Margin Distributions}

\begin{figure*}[ht]
	\centering
        \vspace{-5pt}
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_dpo_2k_reward_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_dpo_2k_reward_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[UltraFeedback]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_dpo_2k_reward_distribution.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Distribution of implicit reward margins on \textbf{TL;DR}, \textbf{HH}, and \textbf{UltraFeedback} datasets. The reward is calculated using the Llama-3.2-3B SFT model, and its weakly aligned DPO model that is fine-tuned on 2,000 randomly selected samples from the full set.
	}
	\label{fig:im-margin}
	\vspace{-15pt}
\end{figure*}

\begin{figure*}[ht]
	\centering
        \vspace{-5pt}
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_skywork_reward_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_skywork_reward_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[UltraFeedback]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_skywork_reward_distribution.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Distribution of external rewards on \textbf{TL;DR}, \textbf{HH}, and \textbf{UltraFeedback} datasets. The reward is calculated using Skywork-Reward-Llama-3.1-8B-v0.2.
	}
	\label{fig:ex-margin}
	\vspace{-15pt}
\end{figure*}

\begin{figure*}[ht]
	\centering
        \vspace{-5pt}
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_sft_ppl_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_sft_ppl_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[UltraFeedback]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_sft_ppl_distribution.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Distribution of conditional perplexity (also named instruction following difficulty) margins on \textbf{TL;DR}, \textbf{HH}, and \textbf{UltraFeedback} datasets. The perplexity is calculated with the Llama-3.2-3B SFT model.
	}
	\label{fig:cppl-margin}
	\vspace{-15pt}
\end{figure*}

\subsection{Singular Margin Distribution} 
\label{app:singular}
The margin distributions calculated using CPPL margin, External and Implicit DPO reward margins, as illustrated in Figures~\ref{fig:cppl-margin},\ref{fig:ex-margin},\ref{fig:im-margin}, reveal a notable concentration of sample margins around zero. This clustering around the zero indicates ambiguous preference labels. It leads to the challenge in preference learning, as evidenced by the substantially slower decrease in training loss (and slower increase in training margin) compared to samples with larger margins, as shown in Section~\ref{exp:analysis}.


\subsection{Joint Margin Distribution} 
\label{app:joint}
To complement the left and middle subfigures in Figure~\ref{fig:visual}, we present additional results showing the joint margin distributions of samples on the other datasets in Figure~\ref{fig:visual-extra}. Our analysis reveals that external and implicit margins exhibit minimal correlation across all four datasets, while implicit margins calculated by different models maintain a high correlation. These further enhance the rationality of our design detail of DM: fusion of both margins and disentangling implicit margin from the target model (if the target model is a bit large and we want to accelerate the enumeration process of the full-set.)

\begin{figure*}[ht]
        \vspace{-10pt}
	\centering
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_DM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_DM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[Llama-UltraFeedback]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/llama_uf_DM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[Mistral-UltraFeedback]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/mistral_uf_DM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_DIM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_DIM_distribution.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Subfigure (a)-(d): scatter plot showing the joint distribution of samples across external and implicit reward margin values on four datasets. Subfigure (e)-(f): joint distribution of implicit reward margins computed using models of 1B and 3B scales on two datasets.
	}
	\label{fig:visual-extra}
	\vspace{-10pt}
\end{figure*}

\section{More Experimental Results}

\subsection{Train Loss and Margin Curves - 3B}
\label{app:loss}
To complement the right subfigure in Figure~\ref{fig:visual}, we present additional results showing the progression of training loss and margins throughout the DPO training process. The results are shown in Figure~\ref{fig:loss-margin}. All strategies demonstrated consistent patterns in loss reduction: both TOP margin-oriented and BOT strategies achieved rapid decreases in training loss, while the MID strategy exhibited slower convergence and remained at significantly higher final loss values. Regarding training margins, TOP strategies achieved higher levels compared to BOT and MID approaches. Notably, our proposed DM-ADD and DM-MUL strategies demonstrated even larger margins than the Implicit Margin-TOP strategy.


\begin{figure*}[ht]
	\centering
        \vspace{-5pt}
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_Loss.pdf}\\
        \end{minipage}%
        }
        \subfigure[TL;DR]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/tldr_Margin.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_Loss.pdf}\\
        \end{minipage}%
        }
        \subfigure[HH]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/hh_Margin.pdf}\\
        \end{minipage}%
        }
        \subfigure[UltraFeedback]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_Loss.pdf}\\
        \end{minipage}%
        }
        \subfigure[UltraFeedback]{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_Margin.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{DPO train loss and margin on \textbf{TL;DR}, \textbf{HH}, and \textbf{UltraFeedback} datasets. The training was implemented with Llama-3.2-3B SFT version on different subsets selected by five strategies.
	}
	\label{fig:loss-margin}
	\vspace{-15pt}
\end{figure*}

\subsection{Train Loss and Margin Curves - 8B}
\label{app:loss-8b}
Figure~\ref{fig:loss-margin-8b} presents the progression of training loss and margins during DPO training of the 8B model. We show the fine-tuned version of the Llama-3-8B (base) model trained on UltraFeedback as a representative case since all models exhibited similar trends, as in Figure~\ref{fig:loss-margin}.

\begin{figure*}[ht]
	\centering
        \vspace{-5pt}
        \subfigure{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_Loss_8b_sft.pdf}\\
        \end{minipage}%
        }
        \subfigure{
        \begin{minipage}[t]{0.4\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_Margin_8b_sft.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{DPO train loss and margin on \textbf{UltraFeedback} datasets. The training was implemented with Llama-3-8B SFT version on different subsets selected by six strategies.
	}
	\label{fig:loss-margin-8b}
	\vspace{-15pt}
\end{figure*}

\subsection{Resources and computation cost}
\label{app:compute}
For all experiments, we utilized 8 A100 GPUs. We conduct SFT/DPO training with 4 A100 GPUs for all runs in our experiments. For both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) training, we allocated 4 A100 GPUs per run. Training 8B parameter models on the \textbf{UltraFeedback} dataset for two epochs required approximately 9 hours of computation time. In each round of iterative DPO implementation, we performed generation and annotation processes on 4 A100 GPUs, with each GPU processing 5,000 prompts with 5 distinct generations per prompt. The overall generation that utilizes vLLM \citep{kwon2023efficient} for acceleration takes about 1.5 hours, and the corresponding reward annotation takes about 2 hours.

\subsection{More Results for Ablation Study on the DPO Variants}
\label{app:dpo-variant}

As a complementary study to the results shown in Figure \ref{fig:abl_po}, we conducted experiments using the Llama-3-8B-Instruct model while maintaining all other experimental parameters. The results, presented in Figure \ref{fig:abl_po_ins}, demonstrate that models trained on subsets selected by DM-MUL achieved significantly higher win rates across most evaluation scenarios.

\begin{figure*}[h]
	\centering
        \subfigure[IPO]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/IPO-instruct.pdf}\\
        \end{minipage}%
        }
        \subfigure[KTO]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/KTO-instruct.pdf}\\
        \end{minipage}%
        }
        \subfigure[SLiC]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/SLIC-instruct.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Ablation study on variants of DPO: win rate comparison on IPO, KTO, and SLiC algorithms. The experiments utilize the \textbf{UltraFeedback} dataset for preference optimization, with the fine-tuned Llama-3-8B (instruct) model as the initial model. Random and DM select 6,000 samples (10\% of the full set) for subset training.
	}
	\label{fig:abl_po_ins}
	\vspace{-15pt}
\end{figure*}

\subsection{Ablation Study on Hyper-parameters}
\label{app:lr-abl}

We investigate the impact of hyperparameters $\beta$ and training epochs on DPO performance. Specifically, we train the \textbf{Llama-UltraFeedback} model using DPO with $\beta=0.01$ for both one and two epochs. The model's performance is evaluated using AlpacaEval 2.0 win rates, with results shown in Table~\ref{tab:abl_beta}. Our experimental results demonstrate that DM-MUL selected subsets significantly enhance the performance of vanilla DPO on these different hyperparameter configurations. Notably, DPO-DM-MUL achieves superior results compared to state-of-the-art preference learning algorithms such as SimPO \citep{meng2024simpo}, despite the latter utilizing the complete dataset for DPO training.

\begin{table}[h]
    % \vspace{-0.3cm}
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0cm}
    \setlength{\tabcolsep}{9pt} 
    \caption{Ablation study on $\beta$ ($\beta=0.01$): performance comparison of methods trained on Llama-Ultrafeedback dataset, benchmarked against state-of-the-art algorithms trained on the complete dataset.}
    \label{tab:abl_beta}
    \tiny
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
         Method & LC (\%) & WR (\%) & Epochs \\
        \midrule
         DPO & 38.65 & 39.57 & 2 \\
         DPO & 40.30 & 37.90 & 1 \\
         \textbf{DPO-DM-MUL} & 43.66 & \textbf{42.52} & 2 \\
         \textbf{DPO-DM-MUL} & \textbf{45.17} & 41.86 & 1 \\
         KTO & 33.10 & 31.80 & 1 \\
         IPO & 35.60 & 35.60 & 1 \\
         SimPO & 43.72 & 39.88 & 1 \\
        \bottomrule
    \end{tabular}}
    \vspace{-10pt}
\end{table}

\textbf{However, we observe that selecting a small $\beta$ value is not consistently a good choice for DPO training.} Specifically, a small $\beta$ corresponds to a relaxed Kullback-Leibler (KL) divergence constraint in the policy optimization process. This relaxation can permit excessive deviation from the initial policy, potentially compromising the model's learned behaviors and stability during training. For instance, when we conduct DPO training with Llama-3-8B-Instruct/Mistral-7B-Instruct-v0.2 on the Ex/Im-TOP selected 6,000 subsets from \textbf{UltraFeedback}, with $\beta=0.01$ and two epochs update, we find that although the model could respond normally to most questions, it sometimes outputs repeated or chaotic tokens, as shown in Figure~\ref{fig:case}. And their win rates on AlpacaEval 2.0 dropped by more than 10 points as a consequence. 

Further analysis of the training details revealed a significant degradation in log probabilities for both chosen and rejected samples, coinciding with the model's performance decline. Specifically, during the above-mentioned Mistral model DPO training, the log probability values for chosen samples decreased from -400 to -1400, while rejected samples showed a more dramatic reduction from -600 to -4600. Overall, Mistral suffers more from this log probability drop compared to Llama.

Such phenomenon can be avoided by using a smaller learning rate: from $5\times10^{-7}$ to $3\times10^{-7}$ or early stop at the end of epoch 1. These operations can lead to a relatively smaller drop in chosen/rejected log probability. However, we still do not recommend using small $\beta$, as more potential sacrifices in other tasks such as reasoning may also appear in this case.

\begin{figure*}[h]
  \centering
  \includegraphics[width=\textwidth]{fig/case.pdf}
  \vspace{-15pt}
  \caption{The model breaking pattern when conducting DPO training with small $\beta$ ($\beta=0.01$) for Llama-3-8B-Instruct and Mistral-7B-Instruct-V0.2. We select two examples of abnormal responses given by each model.}
  \label{fig:case}
  \vspace{-5pt}
\end{figure*}