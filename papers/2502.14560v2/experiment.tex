\section{Experiments}
\label{sec:experiment}

We organize the experiments as follows: we explain the experimental setup in Section~\ref{exp:setting}; we compare DM with various sample selection baselines on diverse preference tasks and present the detailed results in Section~\ref{exp:off-3b}; then we focus on the important chat task, and explore the effectiveness of DM in enhancing comprehensive dialogue ability in Section~\ref{exp:alpaca}; next, we explore the online setting in Section~\ref{exp:online}.
lastly, we perform an ablation study for the offline preference data selection in Section~\ref{exp:ablation}.

\subsection{Experimental Setup}
\label{exp:setting}

\begin{table*}[t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0cm}
    \setlength{\tabcolsep}{5pt} 
    \renewcommand{\arraystretch}{1.3} 
    \caption{GPT4 win rates vs. chosen answers on three tasks. DPO is implemented on subsets selected using different filtering strategies. The supervised fintuned model of Llama-3.2-3B-base is used for initialization. Each strategy selected 2,000 samples for DPO training across \textbf{TL;DR}, \textbf{HH}, and \textbf{Ultrafeedback}. Bold and underlined numbers indicate the best and runner-up performances, respectively.}
    \label{tab:3b-test}
    \small
    \begin{tabular}{l|cc|ccc|ccc|ccc|cc|c}
    \toprule
    Filter & Init & Rand & \multicolumn{3}{c|}{External Margin} & \multicolumn{3}{c|}{Implicit Margin} & \multicolumn{3}{c|}{IFD Margin} & \multicolumn{2}{c|}{DM} & Full  \\
    \midrule
    Strategy&  &  & TOP & MID & BOT & TOP & MID & BOT & TOP & MID & BOT & ADD & MUL & \\
    \midrule
    \textbf{TL;DR} & 34.00 & 46.50 & \underline{66.25} & 42.00 & 22.00 & 30.75 & 43.00 & 19.75 & 1.75 & 41.25 & 55.25 & \underline{63.75} & \textbf{83.25} & 36.75 \\
    \textbf{HH} & 70.25 & 84.25 & 82.25 & 76.50 & 69.75 & \textbf{92.25} & 81.25 & 32.00 & 11.25 & \underline{90.00} & 64.50 & 85.25 & 87.25 & \underline{92.00} \\
    \textbf{Ultrafeedback} & 61.49 & 82.86 & \underline{91.18} & 73.29 & 25.84 & 89.81 & 77.14 & 37.02 & 72.05 & 83.60 & 54.53 & \textbf{91.55} & \underline{90.68} & 80.99\\
    
         \bottomrule
    \end{tabular}
\end{table*}

\begin{figure*}[t]
        \vspace{-10pt}
	\centering
        \subfigure{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_DM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/uf_DIM_distribution.pdf}\\
        \end{minipage}%
        }
        \subfigure{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/DPO_Train_Loss.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Visualization on \textbf{UltraFeedback}. (Left) Scatter plot showing the joint distribution of samples across external and implicit reward margin values. (Middle) Joint distribution of implicit reward margins computed using models of 1B and 3B scales. (Right) DPO training loss curves of Llama-3.2-3B on different subsets selected by the external or implicit margin.
	}
	\label{fig:visual}
	\vspace{-10pt}
\end{figure*}

\paragraph{Preference Datasets.} We evaluate our approach using three established preference datasets: (1) Reddit \textbf{TL;DR} summarization dataset \citep{volske2017tl,stiennon2020learning} that contains human-written summaries and human-rated results, (2) Anthropic Helpful and Harmless dialogue dataset (\textbf{HH}) \citep{bai2022training}, and (3) \textbf{UltraFeedback} \citep{cui2023ultrafeedback}, which comprises quality-scored model responses across diverse prompts from multiple sources. To explore how models react to on-policy data, we leverage two modified versions of the \textbf{UltraFeedback} dataset, \textbf{Llama-UltraFeedback} and \textbf{Mistral-UltraFeedback} \citep{meng2024simpo}. In the variants, the original chosen and rejected responses are replaced with the highest and lowest scored responses, respectively, sampled from five candidates generated by the corresponding Instruct model.  The scores are given by the \href{https://huggingface.co/llm-blender/PairRM}{PairRM} \citep{jiang2023llm} reward model. Statistics about these datasets are in Appendix~\ref{app:data}. 

\paragraph{Models.} We perform sample selection and DPO training experiments across three model architectures: Llama-3.2-3B \citep{llama32}, Llama-3-8B \citep{dubey2024llama}, and Mistral-7B \citep{jiang2023mistral}, under Base and Instruct setup. For the base model (\href{https://huggingface.co/meta-llama/Llama-3.2-3B}{Llama-3.2-3B} and \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B}{Llama-3-8B}), we first establish fundamental instruction-following capabilities through supervised fine-tuning on the \href{https://huggingface.co/datasets/RLHFlow/SFT-OpenHermes-2.5-Standard}{RLHFlow/SFT-OpenHermes-2.5-Standard} datasets. For the instruct setup, we directly use \href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{Llama-3-8B-Instruct} or \href{https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2}{Mistral-7B-Instruct-v0.2} as the start of DPO training. Concerning external reward model for margin calculation, we adopt the recent \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork-Reward-Llama-3.1-8B-v0.2} \citep{liu2024skywork} which is the best reward model at this scale according to the \href{https://huggingface.co/spaces/allenai/reward-bench}{RewardBench} leadboard.

\paragraph{Evaluation.} Following \citet{rafailov2024direct}, we evaluate the models using 400 randomly sampled test sets from the validation/test pools of the \textbf{TL;DR} and \textbf{HH} datasets, separately. For models trained on \textbf{UltraFeedback}, we employ AlpacaEval and AlpacaEval 2.0 \citep{li2023alpacaeval} as our evaluation benchmark, which consists of 805 diverse questions. {\footnote{They use the same set of questions, and differ in their reference response generation: AlpacaEval uses Text-Davinci-003 \citep{ye2023comprehensive}, whereas AlpacaEval 2.0 employs GPT4-1106-preview.} As the ground truth oracle is unavailable, we assess model performance through win rates against reference responses (written by humans or LLMs like GPT4). 
Specifically, we employ GPT-4 as a proxy for human judgment across three distinct settings: summarization, helpful or harmless completion, and single-turn dialogue. 
We utilized a fixed decoding temperature ($T=0.7$) for all model generation in the experiments. More details are presented in Appendix \ref{app:data}. 

\paragraph{Baselines.} For the offline data selection setting, we compare our method with three types of methods: (1) \textbf{Random}, a simple yet effective strategy in many domains (e.g., Instruction tuning \citep{xia2024less}), (2) \textbf{Conditional Perplexity (IFD)} \citep{li2023quantity}, which utilizes token probability to measure informativeness. We use the difference in IFD scores between chosen and rejected responses for preference data selection. (3) \textbf{External/Implicit Margin (M-Ex/Im)} computes the gap between chosen and rejected responses using either external reward models or implicit DPO rewards. For (2) and (3), We segment the data into TOP (highest positive difference), MID (near-zero difference), and BOT (lowest negative difference) subsets. 
Specifically, previous work \citep{wu2024beta} argues that low-gap preference pairs are more informative (i.e., chosen and rejected answers are similar), here we use the IFD-MID strategy to quantify this scheme and call it \textbf{Low-Gap}. For the iterative DPO setting, we compare our approach against the standard online iterative DPO baseline established by \citet{xiong2024iterative,dong2024rlhf} and run for three rounds, each using 20k prompts sampled from \textbf{UltraFeedback}.

\paragraph{Implementation Details.} The SFT training of the base model is carried out for two epochs with a learning rate of $2\times10^{-5}$. 
Sample packing \citep{tunstall2023zephyr} is employed to accelerate the training, and we use a block size of 4096. 
As for DPO training, we follow \citet{rafailov2024direct} and used a fixed value of $\beta=0.1$, except for \textbf{TL;DR} where $\beta=0.5$. The DPO training process runs for two epochs using the AdamW optimizer. The learning rate follows a cosine schedule with a 0.1 warm-up ratio, and the global batch size is set to 32. For implicit reward calculation, we employ a Llama-3.2-3B SFT model and its DPO-tuned model, which is fine-tuned on 2,000 randomly selected samples from the complete dataset. More implementation details are presented in Appendix \ref{app:imp}.

\begin{table*}[t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0cm}
    \setlength{\tabcolsep}{12pt} 
    \caption{Performance comparison on AlpacaEval 2.0 using DPO-trained models with different 6,000-sample subsets (10\% of full set). Both SFT and Instruct variants of Llama-3-8B were evaluated. LC and WR denote length-controlled and raw win rate, respectively. \textbf{Bold} number denotes the best-performing selected subset.}
    \label{tab:offline}
    \small
    \begin{tabular}{l|cccc|cccc}
    \toprule
    Dataset & \multicolumn{4}{c|}{\textbf{UltraFeedback}} & \multicolumn{4}{c}{\textbf{Llama-UltraFeedback}} \\
    Model & \multicolumn{2}{c}{\textbf{Llama-3-Base (8B)}} & \multicolumn{2}{c|}{\textbf{Llama-3-Instruct (8B)}} & \multicolumn{2}{c}{\textbf{Llama-3-Base (8B)}} & \multicolumn{2}{c}{\textbf{Llama-3-Instruct (8B)}}\\
    \midrule
    Metric & LC (\%) & WR (\%) & LC (\%) & WR (\%) &LC (\%) & WR (\%) & LC (\%) & WR (\%) \\
    \midrule
    Init & 9.61 & 6.69 & 22.92 & 22.57 & 9.61 & 6.69 & 22.92 & 22.57 \\
    \midrule
    Full & 17.32 & 15.30 & 28.64 &26.54 & 19.92 & 16.45 & 32.31 & 32.44\\
    Random & 12.33 & 10.96 & 22.74 & 24.59& 11.58 & 9.51 & 31.51 & 31.92\\
    Low-Gap & 13.93 & 11.40 & 28.19 & 27.95& 11.12 & 7.87 & 34.95 & 34.25\\
    M-Ex & 16.61 & 14.81 & 26.28& 25.24& 21.11 & 18.63 & 35.10 & 34.80\\
    M-Im & 19.33 & 17.80 & 29.71&29.44 & 18.88 & 16.25 & 33.71 & 32.92\\
    \textbf{DM-ADD}  & 18.64 & 18.46 & \textbf{29.75} & \textbf{29.62}& 20.98 & 18.29 & 35.97 & 35.79\\
    \textbf{DM-MUL} & \textbf{19.53} & \textbf{19.09} & 28.30 & 28.39& \textbf{21.67} & \textbf{20.01} & \textbf{36.36} & \textbf{36.47}\\
         \bottomrule
    \end{tabular}
    \vspace{-8pt}
\end{table*}

\subsection{Win Rate Comparison with Baselines on Classic Preference Datasets}
\label{exp:off-3b}

Firstly, we compare DM and baseline strategies on three widely-used preference datasets: \textbf{TL;DR}, \textbf{HH}, and \textbf{UltraFeedback}. Using a Llama-3.2-3B model as our base architecture, we evaluate different selection methods, each sampling 2,000 training examples for DPO training. We use AlpacaEval as the test sets of \textbf{UltraFeedback} as it better reveals the degree of improvement. The results, measured by GPT4 win rates, are presented in Table~\ref{tab:3b-test}. We summarize the findings below.
\begin{itemize}[leftmargin=*]
    \item \textbf{DM consistently achieves (near) optimal win rates across all three tasks, while all baselines show weak results on at least one task}. This demonstrates that our method is more robust to useless or toxic samples in diverse domains. For example, DM-MUL achieves 17\% higher win rates to second most effective baselines on \textbf{TL;DR} task, this is because the DM-MUL assigns low selection priority so long as a low margin is observed from one source of reward.
    
    \item \textbf{More data in DPO training does not always yield better results} - using just 2-5\% of carefully selected data can surpass the performance achieved with the full dataset. This finding suggests the presence of ineffective samples in these tasks, as evidenced by the performance patterns of X-MID and X-BOT strategies (where X represents implicit margin). The results in Table~\ref{tab:3b-test} reveal that external, implicit, and IFD margins sometimes can identify toxic samples, as models trained on such data perform significantly worse than the original SFT model. This phenomenon underscores the critical importance of data filtering in DPO training.
    highlighting that training data should undergo a thorough quality assessment before being used in the training process.

    \item Among all methods, \textbf{only implicit margin-BOT consistently identifies toxic samples, emphasizing the value of incorporating DPO implicit rewards into the DM strategy design}. Despite its strong performance in RewardBench, the Skywork reward model's margin signals prove less effective than random selection on \textbf{HH}, highlighting the Out-of-Distribution challenge external reward models face when evaluating unfamiliar behavioral patterns/preferences. As for the IFD margin metric, it exhibits notable inconsistency across different datasets, rendering it unreliable for evaluating new datasets where prior preference patterns are unknown.
    
\end{itemize}


\subsubsection{Analysis}
\label{exp:analysis}
We analyze DM from two perspectives: the joint distribution of different sources of reward margins and the training dynamics for subsets with different margin properties. We utilize the \textbf{UltraFeedback} datasets for this analysis.

The left and middle panels of Figure~\ref{fig:visual} illustrate the sample distribution across external-implicit and implicit-implicit margin dimensions on \textbf{UltraFeedback}. Several key observations emerge: (1) The correlation between implicit and external reward margins is notably weak. In particular, samples exhibiting high positive implicit margins span a broad range of external margins, including strongly negative values, and vice versa. This pattern is consistent across other datasets (see Appendix~\ref{app:joint}). This divergence highlights the distinct preference patterns captured by these two reward types. Supporting this observation, Table~\ref{tab:3b-test} demonstrates that neither margin type consistently outperforms the other in subset selection: external margins prove more effective for \textbf{TL;DR}, implicit margins show superior performance on \textbf{HH}, while both perform comparably on \textbf{UltraFeedback}. These findings underscore the importance of combining both reward types in an ensemble approach. (2) In contrast, we observe a strong correlation between implicit reward margins calculated by models of different sizes (Llama-3.2 3B and 1B), and the robust correlation is observed on other datasets as well (see Appendix~\ref{app:joint}). 
This validates our design of using a separate small model for implicit reward margin calculations.

The right panel of Figure~\ref{fig:visual} shows DPO training loss curves across different subset selection strategies. TOP/BOT selections converge quickly to low loss values, while MID selections maintain steady, higher loss levels. Surprisingly, BOT-selected samples learn as quickly as TOP-selected ones, suggesting that `bad' preferences are also easy to grasp for LLM. The pattern is consistent across different datasets and models (see Appendix~\ref{app:loss} for more results concerning loss and margin curves on other datasets). While this observation might explain proposals that use absolute margin values for selection \citep{muldrew2024active}, Table~\ref{tab:3b-test} reveals that TOP and BOT samples ultimately produce opposing effects despite similar training dynamics. 
However, we observe a difference between BOT area samples selected by external and implicit margins, where Im-BOT exhibits training dynamics that are more similar to Im-TOP area samples in terms of loss reduction speed.


\subsection{AlpacaEval 2.0 Win Rate Comparison}
\label{exp:alpaca}
In this section, we conduct a detailed examination of the chat task to understand how data filtering enhances both DPO training efficiency and models' versatile conversational abilities compared to GPT-4-turbo, representing a key application area for preference learning. We use both Llama-3-8B (base) and (instruct) models, measuring performance through raw and length-controlled win rates on AlpacaEval 2.0, a benchmark widely adopted in preference learning studies. The results of DM and various baselines are shown in Table~\ref{tab:offline}.

\begin{table}[t]
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0cm}
    % \setlength{\tabcolsep}{2pt}
    \caption{Iterative DPO: AlpacaEval 2.0 benchmark results across three iterations of DPO training with \textbf{UltraFeedback} prompts.}
    \label{tab:onine}
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|cccccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Llama-3-Base (8B)}} & \multicolumn{3}{c}{\textbf{Llama-3-Instruct (8B)}} \\
    \cmidrule{2-7}
    & LC (\%) & WR (\%) & Len  & LC (\%) & WR (\%) & Len \\
         \midrule
    DPO (r1) & 17.64 & 13.36 & 1496 & 40.51  & 43.90 & 2173\\
    DPO (r2) & 23.06 & 22.45 & 1897 & 42.51 & 49.23 & 2366\\
    DPO (r3) & 29.03 & 30.86 & 2736 & 44.51 & 53.12 & 2860\\
    \midrule 
    \midrule
    DPO-DM (r1) & 16.35 & 13.09 & 1624 & 42.20 & 45.74 & 2091 \\
    DPO-DM (r2) & 23.79 & 24.17 & 1901 & 46.40 & 50.60 & 2316\\
    DPO-DM (r3) & \textbf{32.31}& \textbf{33.91} & 2565& \textbf{48.49} & \textbf{54.99} & 2774\\
    \bottomrule
    \end{tabular}}
    \vspace{-10pt}
\end{table}

\textbf{DM-ADD and DM-MUL consistently outperform full dataset DPO training.} Table~\ref{tab:offline} shows models trained on DM-selected subsets (using only 10\% of data) achieve 2-4\% higher raw and LC win rates compared to full dataset training across different models (base and instruct) and datasets (offline and on-policy). These consistent improvements demonstrate DM's robustness and effectiveness, validating the value of data filtering for DPO training. In contrast, all baseline methods demonstrate inconsistent performance across different datasets and models. We attribute this instability to ambiguous samples whose margins vary significantly across different reward models, suggesting that proper fusion of reward signals is necessary to mitigate such noise. Appendix \ref{app:loss-8b} presents additional training loss and margin curves, illustrating distinct training dynamics across different subsets, and the trend is similar to that on the 3B model as in Appendix \ref{app:loss}.

\textbf{On-policy data primarily benefits its corresponding on-policy model.} Llama-3-8B-Instruct shows approximately 8\% higher win rates when trained on the on-policy \textbf{Llama-UltraFeedback} data compared to the off-policy \textbf{UltraFeedback} data, while Llama-3-8B-base demonstrates notably smaller improvements between these datasets. 
This phenomenon persists in both full and subset DPO training.

\begin{figure*}[t]
	\centering
        \subfigure[IPO]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/IPO.pdf}\\
        \end{minipage}%
        }
        \subfigure[KTO]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/KTO.pdf}\\
        \end{minipage}%
        }
        \subfigure[SLiC]{
        \begin{minipage}[t]{0.32\linewidth}
            \centering
            \includegraphics[width=1\textwidth]{fig/SLIC.pdf}\\
        \end{minipage}%
        }
        \vspace{-10pt}
	\caption{Ablation study on variants of DPO: win rate comparison on IPO, KTO, and SLiC algorithms. The experiments utilize the \textbf{UltraFeedback} dataset for preference optimization, with the fine-tuned Llama-3-8B (base) model as the initial model. Random and DM select 6,000 samples (10\% of the full set) for subset training.
	}
	\label{fig:abl_po}
	\vspace{-5pt}
\end{figure*}


\subsection{Online Iterative DPO}
\label{exp:online}
We explore the benefits of data selection in iterative DPO training using prompts from \textbf{UltraFeedback} as in \citet{xiong2024iterative}. Our methodology employs DM-MUL to filter 5,000 samples from a pool of 20,000 generated samples per iteration. We evaluate both the base and instruct versions of Llama-3-8B, conducting single-epoch training rounds with hyperparameters $\beta=0.1$ and learning rate $5\times10^{-7}$. Table~\ref{tab:onine} presents our experimental findings, from which we draw two key observations:

While Best-of-N and Worst-of-N preference pairs from a strong reward model are informative, data filtering is still important to DPO training. DPO-DM performs better for both base and instruct settings using only 25\% of the data. This can be attributed to the presence of numerous ambiguous, low-margin samples in the constructed online preference dataset. 
% Furthermore, we find that more high-margin preference pairs are generated when models are iteratively trained on high-margin samples (see Appendix~\ref{}); 
Furthermore, standard iterative DPO shows increasing output length across training rounds, but data filtering mitigates this issue. By round three, DPO-DM demonstrates both superior performance and better output length control.

\begin{table}[t]
    % \vspace{-0.3cm}
    \centering
    \setlength{\abovecaptionskip}{0.1cm}
    \setlength{\belowcaptionskip}{0cm}
    \setlength{\tabcolsep}{9pt} 
    \caption{Ablation study on model choice (\textbf{Mistral-7B-Instruct-v-0.2}). Each strategy selects a 6,000-sample subset.}
    \label{tab:abl_model}
    \tiny
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{c|ccc}
        \toprule
         Method & LC (\%) & WR (\%) & Length \\
        \midrule
        Initial & 17.11 & 14.72 & 1676 \\
         Full & 18.00 & 18.77 & 4189 \\
         Random & 19.42 & 16.64 & 1621 \\
         Low-Gap & 13.31 & 14.35 & 2525 \\
         \textbf{DM-ADD} & 23.67 & 18.21 & 1527 \\
         \textbf{DM-MUL} & \textbf{26.04} & \textbf{20.53} & 1568 \\
        \bottomrule
    \end{tabular}}
    \vspace{-5pt}
\end{table}

\subsection{Ablation Study}
\label{exp:ablation}

\looseness=-1 A critical aspect of dataset filtering methods is their generalization capability - specifically, how well the method (selected subset) transfers to new models, similar optimization algorithms, etc. From this perspective, we examine DM's robustness and effectiveness across several dimensions.



\textbf{Data filtering remains effective for new architecture and datasets.} While our initial experiments focused on the Llama architecture and tokenizers, we investigated DM's effectiveness when applied to fundamentally different model architectures. We evaluate the Mistral-7B-Instruct model using its corresponding \textbf{Mistral-UltraFeedback} dataset, maintaining consistent experimental parameters with those described in Section~\ref{exp:alpaca}. The AlpacaEval 2.0 evaluation results are presented in Table~\ref{tab:abl_model}. Our experiments show that DM-MUL significantly outperforms baseline methods while maintaining output lengths (i.e. around 1600) comparable to the initial instruct model. We observe that models trained on the full dataset tend to generate notably longer responses (i.e., 4189), primarily due to their learned behavior of incorporating numerous hyperlinks (e.g., https://www.ibdb.com/) in an attempt to enhance response credibility. Similarly, models trained on the Low-Gap selected subset exhibit increased response lengths (i.e., 2525) and follow a comparable pattern of including hyperlinks. In contrast, models trained on DM-selected subsets do not display this behavior. We consider the inclusion of hyperlinks without proper RAG (Retrieval-Augmented Generation) \citep{lewis2020retrieval} or website verification mechanisms to be problematic, as such references can be unreliable.


\textbf{DM selected subsets are effective across other preference learning algorithms.} We examine the effectiveness of our selected subsets across different DPO variants and compare the raw/LC win rates of models trained on full or selected subsets by IPO \citep{azar2024general}, KTO \citep{ethayarajh2024kto} and SLiC \citep{zhao2023slic}. The results are presented in Figure~\ref{fig:abl_po}. Our results demonstrate that DM-M consistently outperforms full-set training across all three algorithms, achieving particularly notable improvements in raw/LC win rates - over 12\% - for the IPO algorithm. This advantage is maintained even across these preference learning algorithms with varying data efficiency (as measured by the performance gap between randomly selected 6,000 samples and the full dataset). These findings highlight the significant value of sample filtering for other preference learning. Additional results related to instruct model training can be found in Appendix~\ref{app:dpo-variant}.

\textbf{Additional hyperparameter ablation studies.} We conducted experiments to investigate the impact of DM data selection across different values of $\beta$ and learning rates. The results demonstrate that our data selection strategy consistently enhances DPO training performance on different hyperparameter configurations. Notably, we observed that DPO training becomes unstable when $\beta$ approaches very small values, occasionally resulting in repetitive token outputs (e.g., ``\verb|\r|'', ``based on'', ``--''). This instability can be alleviated by selecting a smaller learning rate and early stopping (e.g., training for only one epoch). These modifications effectively constrain the decrease in the log probability of chosen tokens, thereby maintaining training stability. Detailed results are presented in Appendix~\ref{app:lr-abl}.
