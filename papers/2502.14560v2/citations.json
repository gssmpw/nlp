[
  {
    "index": 0,
    "papers": [
      {
        "key": "pacchiano2021dueling",
        "author": "Pacchiano, Aldo and Saha, Aadirupa and Lee, Jonathan",
        "title": "Dueling rl: reinforcement learning with trajectory preferences"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "chen2022human",
        "author": "Chen, Xiaoyu and Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Wang, Liwei",
        "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "li2023remax",
        "author": "Li, Ziniu and Xu, Tian and Zhang, Yushun and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan",
        "title": "Remax: A simple, effective, and efficient method for aligning large language models"
      },
      {
        "key": "zhong2024dpo",
        "author": "Zhong, Han and Feng, Guhao and Xiong, Wei and Cheng, Xinle and Zhao, Li and He, Di and Bian, Jiang and Wang, Liwei",
        "title": "Dpo meets ppo: Reinforced token optimization for rlhf"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "dong2023raft",
        "author": "Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Zhang, Yihan and Chow, Winnie and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong",
        "title": "Raft: Reward ranked finetuning for generative foundation model alignment"
      },
      {
        "key": "gulcehre2023reinforced",
        "author": "Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others",
        "title": "Reinforced self-training (rest) for language modeling"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lu2022quark",
        "author": "Lu, Ximing and Welleck, Sean and Hessel, Jack and Jiang, Liwei and Qin, Lianhui and West, Peter and Ammanabrolu, Prithviraj and Choi, Yejin",
        "title": "Quark: Controllable text generation with reinforced unlearning"
      },
      {
        "key": "yang2024rewards",
        "author": "Yang, Rui and Pan, Xiaoman and Luo, Feng and Qiu, Shuang and Zhong, Han and Yu, Dong and Chen, Jianshu",
        "title": "Rewards-in-context: Multi-objective alignment of foundation models with dynamic preference adjustment"
      },
      {
        "key": "zhang2024reward",
        "author": "Zhang, Shenao and Liu, Zhihan and Liu, Boyi and Zhang, Yufeng and Yang, Yingxiang and Liu, Yongfei and Chen, Liyu and Sun, Tao and Wang, Zhaoran",
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rafailov2024direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhao2023slic",
        "author": "Zhao, Yao and Joshi, Rishabh and Liu, Tianqi and Khalman, Misha and Saleh, Mohammad and Liu, Peter J",
        "title": "Slic-hf: Sequence likelihood calibration with human feedback"
      },
      {
        "key": "azar2024general",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      },
      {
        "key": "ethayarajh2024kto",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Kto: Model alignment as prospect theoretic optimization"
      },
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "Simpo: Simple preference optimization with a reference-free reward"
      },
      {
        "key": "tang2024generalized",
        "author": "Tang, Yunhao and Guo, Zhaohan Daniel and Zheng, Zeyu and Calandriello, Daniele and Munos, R{\\'e}mi and Rowland, Mark and Richemond, Pierre Harvey and Valko, Michal and Pires, Bernardo {\\'A}vila and Piot, Bilal",
        "title": "Generalized preference optimization: A unified approach to offline alignment"
      },
      {
        "key": "han2024f",
        "author": "Han, Jiaqi and Jiang, Mingjian and Song, Yuxuan and Leskovec, Jure and Ermon, Stefano and Xu, Minkai",
        "title": "$ f $-PO: Generalizing Preference Optimization with $ f $-divergence Minimization"
      },
      {
        "key": "xu2024contrastive",
        "author": "Xu, Haoran and Sharaf, Amr and Chen, Yunmo and Tan, Weiting and Shen, Lingfeng and Van Durme, Benjamin and Murray, Kenton and Kim, Young Jin",
        "title": "Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation"
      },
      {
        "key": "hong2024orpo",
        "author": "Hong, Jiwoo and Lee, Noah and Thorne, James",
        "title": "Orpo: Monolithic preference optimization without reference model"
      },
      {
        "key": "park2024disentangling",
        "author": "Park, Ryan and Rafailov, Rafael and Ermon, Stefano and Finn, Chelsea",
        "title": "Disentangling length from quality in direct preference optimization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2024survey",
        "author": "Wang, Jiahao and Zhang, Bolin and Du, Qianlong and Zhang, Jiajun and Chu, Dianhui",
        "title": "A Survey on Data Selection for LLM Instruction Tuning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "swayamdipta2020dataset",
        "author": "Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A and Choi, Yejin",
        "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics"
      },
      {
        "key": "deng2023counterfactual",
        "author": "Deng, Xun and Wang, Wenjie and Feng, Fuli and Zhang, Hanwang and He, Xiangnan and Liao, Yong",
        "title": "Counterfactual active learning for out-of-distribution generalization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "cao2023instruction",
        "author": "Cao, Yihan and Kang, Yanbin and Wang, Chi and Sun, Lichao",
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models"
      },
      {
        "key": "li2024superfiltering",
        "author": "Li, Ming and Zhang, Yong and He, Shwai and Li, Zhitao and Zhao, Hongyu and Wang, Jianzong and Cheng, Ning and Zhou, Tianyi",
        "title": "Superfiltering: Weak-to-strong data filtering for fast instruction-tuning"
      },
      {
        "key": "xia2024less",
        "author": "Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi",
        "title": "Less: Selecting influential data for targeted instruction tuning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ren2021survey",
        "author": "Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B and Chen, Xiaojiang and Wang, Xin",
        "title": "A survey of deep active learning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "muldrew2024active",
        "author": "Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David",
        "title": "Active Preference Learning for Large Language Models"
      },
      {
        "key": "yasunaga2024alma",
        "author": "Yasunaga, Michihiro and Shamis, Leonid and Zhou, Chunting and Cohen, Andrew and Weston, Jason and Zettlemoyer, Luke and Ghazvininejad, Marjan",
        "title": "ALMA: Alignment with Minimal Annotation"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xiong2024iterative",
        "author": "Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong",
        "title": "Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhang2024self",
        "author": "Zhang, Shenao and Yu, Donghan and Sharma, Hiteshi and Zhong, Han and Liu, Zhihan and Yang, Ziyi and Wang, Shuohang and Hassan, Hany and Wang, Zhaoran",
        "title": "Self-exploring language models: Active preference elicitation for online alignment"
      }
    ]
  }
]