\section{Introduction}
\label{sec:intro}

Reinforcement Learning from Human Feedback \citep[RLHF;][]{christiano2017deep,ziegler2019fine} has emerged as a crucial technique for aligning Large Language Models (LLMs) with human preferences and values. Traditional RLHF implementations involve a two-stage process: reward model training based on preference data followed by reinforcement learning optimization. However, this approach presents significant computational challenges, requiring loading multiple model instances and extensive hyperparameter tuning.



As an alternative, \citet{rafailov2024direct} introduced Direct Preference Optimization (DPO), which streamlines the alignment process by directly optimizing the LLM policy from preference data. DPO has demonstrated comparable effectiveness while substantially reducing computational requirements compared to classical RLHF. Following DPO's introduction, numerous studies have proposed improvements through modified learning objectives \citep{zhao2023slic,azar2024general,ethayarajh2024kto}
and iterative learning schemes \citep{xiong2024iterative}. 
While these \textbf{algorithmic} advances have shown promise, there remains a critical gap in our understanding of the \textbf{data-centric} aspects of preference learning: \emph{what characteristics of preference data contribute most to model alignment?}


This work thoroughly studies the impact of preference data quality on DPO training, which is crucial for developing more efficient training strategies. In particular, we achieve both \emph{improved performance} and \emph{reduced computational costs} through strategic data selection. Our research makes three primary contributions:

(1) We prove in theory the necessity of data selection in the presence of exogenous noise. Specifically, the inherent noise in the reward model may flip the preference between response pairs, leading to the emergence of the \emph{parameter shrinkage} issue. Furthermore, we demonstrate that margin-based selection criteria can effectively address this issue by inducing \emph{parameter inflation}.


(2) Building on this insight, we propose a margin-maximization principle for dataset curation that incorporates signals from the ensemble of external rewards and DPO implicit rewards. Through extensive experiments across diverse datasets and base models, we show that this selection strategy shows two consistent advantages: it substantially reduces computational overhead via efficient data selection and improves model performance compared to training on the full dataset. In particular, on the Ultrafeedback dataset and its variants, our method identifies a $10\%$ data subset for DPO training on LLama and Mistral series models, consistently achieving 3\% to 8\% point improvements on the AlpacaEval 2.0 benchmark relative to training on the complete dataset.
    
(3) Finally, we extend our data selection framework to iterative DPO settings, showing that selectively sampling online data can simultaneously lower computational costs and improve performance. In particular, we achieve 48.49\% win rate and 54.99\% length-control win rate on the AlpacaEval 2.0 benchmark using only $25\%$ of the online data for training.


Our findings provide both theoretical insights into the dynamics of preference learning and practical guidelines for more efficient DPO implementations. This work bridges an important gap between algorithmic innovation and data quality considerations in the context of LLM alignment.