\section{Background}


Reinforcement Learning from Human Feedback (RLHF) has emerged as a key method for aligning LLMs with human preferences. It leverages training data of the form $\mathcal{D} = \{x, y_w, y_l\}$, where $x$ represents the input prompt, and $y_w$ and $y_l$ denote the preferred and dispreferred responses, respectively. The RLHF pipeline typically involves two stages: reward learning and policy optimization.

\textbf{Reward Learning.} In the reward learning stage, a reward model is trained to approximate human preferences based on preference data. By adopting the Bradley-Terry model \citep{bradley1952rank} to capture human preference, reward training involves minimizing the loss:
$$
\mathcal{L}_{\mathrm{RM}}(r) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\big[ \log \sigma\big(r(x, y_w) - r(x, y_l)\big) \big],
$$
where $\sigma(\cdot)$ is the sigmoid function. 

\textbf{Policy Optimization with Reinforcement Learning.} Once the reward model $r$ is trained, it is used to guide the optimization of a policy $\pi_\theta(y|x)$, where $\theta$ denotes the parameters of the model. This stage often employs reinforcement learning techniques such as Proximal Policy Optimization \citep[PPO;][]{schulman2017proximal} to optimize the policy by maximizing the expected reward. 
$$
\max_{\pi_\theta} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot|x)} \left[r(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{\mathrm{ref}}(y|x)} \right],
$$
where $\beta > 0$ is the regularization parameter.
However, this RL approach can be computationally expensive, sensitive to reward misspecification and require careful hyperparameter tuning.

Recently, as an alternative to the RL-based policy optimization in RLHF, \emph{Direct Preference Optimization} \citep[DPO;][]{rafailov2024direct} has been proposed. DPO simplifies the reward alignment process by directly incorporating human preference data into supervised training. Instead of defining and optimizing a reward function explicitly, DPO minimizes
$$
\mathcal{L}_{\mathrm{DPO}}(\theta) = - \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\mathrm{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\mathrm{ref}}(y_l|x)} \right).
$$
By bypassing the intermediate step of reinforcement learning, DPO offers a more stable and computationally efficient alternative to standard RLHF, while still aligning models effectively with human feedback.



