\section{Related Work}
\label{sec:rel}

\textbf{Preference Learning Algorithms.}Reinforcement Learning from Human Feedback %also known as dueling RL %\citep{pacchiano2021dueling} or preference-based %RL \citep{chen2022human}, 
has become a crucial component of recent Large Language Models (LLMs) such as ChatGPT \citep{ouyang2022training}. While the classical RLHF pipeline traditionally uses Proximal Policy Optimization, several alternative approaches have been proposed. These include but not limited other RL-based training algorithms \citep{li2023remax,zhong2024dpo}, rejection sampling \citep{dong2023raft,gulcehre2023reinforced}, conditional supervised fine-tuning \citep{lu2022quark,yang2024rewards,zhang2024reward}, and Direct Preference Optimization \citep{rafailov2024direct}. Among these alternatives, DPO has gained significant attention due to its simplicity and robust performance. Following the introduction of DPO, numerous works \citep{zhao2023slic,azar2024general,ethayarajh2024kto,meng2024simpo,tang2024generalized,han2024f,xu2024contrastive,hong2024orpo,park2024disentangling} have attempted to improve its performance by modifying the DPO objective.

\textbf{Data Selection in LLM Fine-tuning.} Data selection is crucial in LLM post-training \citep{wang2024survey}, primarily due to two key observations: LLM training typically converges rapidly, and excessive data can degrade model performance through overfitting or exposure to toxic content \citep{swayamdipta2020dataset,deng2023counterfactual}. Recent research has focused on enhancing instruction tuning efficiency by identifying high-quality subsets from large instruction datasets \citep{cao2023instruction,li2024superfiltering,xia2024less}, often adapting active learning query strategies \citep{ren2021survey} to assess sample uncertainty and diversity. However, data efficiency in preference learning remains relatively unexplored. While some studies studied annotation cost reduction in preference dataset creation \citep{muldrew2024active, yasunaga2024alma}, our work firstly provides clear criteria for identifying informative samples while filtering toxic ones, thereby improving both DPO's efficiency and performance. Furthermore, our method extends to iterative DPO \citep{xiong2024iterative} and its variants \citep{zhang2024self}, where training data is generated online.