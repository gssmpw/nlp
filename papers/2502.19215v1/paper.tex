\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{orcidlink}
\usepackage{glossaries}
\usepackage{listings}
\usepackage{stfloats} % For double column figures
\usepackage{subcaption} % For subfigures
\usepackage[outputdir=build,frozencache=true]{minted}
\definecolor{bg}{rgb}{0.95,0.95,0.95}
\usepackage{lipsum}

%% Control spacing around captions
\usepackage[font=footnotesize]{caption}

%% Prevent lifted tilde in minted environment
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\input{glossary.tex}
\input{res/metrics.tex}

%% Configure listings
\lstset{
language=C,                        % Code language
basicstyle=\ttfamily\footnotesize, % Code font, Examples: \footnotesize, \ttfamily
literate={~}{{\fontfamily{ptm}\selectfont \textasciitilde}}1
}
\setminted[]{
    fontsize=\footnotesize,
    bgcolor=bg,
    style=tango,
    breaklines
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Multicast-Capable AXI Crossbar\\ for Many-core Machine Learning Accelerators}

\ifdefined\blindreview
\else
\author{\IEEEauthorblockN{Luca Colagrande}
\IEEEauthorblockA{\textit{Integrated
Systems Laboratory (IIS)} \\
\textit{ETH Zurich}\\
Zurich, Switzerland \\
colluca@iis.ee.ethz.ch
\orcidlink{0000-0002-7986-1975}}
\and
\IEEEauthorblockN{Luca Benini}
\IEEEauthorblockA{\textit{Integrated
Systems Laboratory (IIS)} \\
\textit{ETH Zurich}\\
Zurich, Switzerland \\
lbenini@iis.ee.ethz.ch
\orcidlink{0000-0001-8068-3806}}
}
\fi

\maketitle

\begin{abstract}
To keep up with the growing computational requirements of machine learning workloads, many-core accelerators integrate an ever-increasing number of processing elements, putting the efficiency of memory and interconnect subsystems to the test. In this work, we present the design of a multicast-capable AXI crossbar, with the goal of enhancing data movement efficiency in massively parallel machine learning accelerators. We propose a lightweight, yet flexible, multicast implementation, with a modest area and timing overhead (12\,\% and 6\,\% respectively) even on the largest physically-implementable 16-to-16 AXI crossbar. To demonstrate the flexibility and end-to-end benefits of our design, we integrate our extension into an open-source 288-core accelerator. We report tangible performance improvements on a key computational kernel for machine learning workloads, matrix multiplication, measuring a 29\,\% speedup on our reference system.
\end{abstract}

\begin{IEEEkeywords}
AI accelerators, on-chip networks, multicast communication, AXI
\end{IEEEkeywords}

\section{Introduction}

In recent years, a wide range of many-core general-purpose accelerators have emerged to keep up with the computational requirements of modern \gls{ml} workloads \cite{peng2024}.
Aiming for higher peak performance figures, these accelerators integrate an ever-increasing number of \glspl{pe}:
%% First option would be to compare peak performance of AI accelerators taken from 2021 (reuther2021) vs. 2023 (reuther2023) surveys.
%% Second option is to compare number of CUDA cores in GPUs in 2020 (A100) vs. 2022 (H100).
the number of CUDA cores in Nvidia's leading \glspl{gpu} increased by more than $2\,\times$ in only two years, rising from 6912 in the A100 \cite{choquette2021} to 16896 in the H100 \cite{choquette2023}.

To translate peak performance into actual performance, it is critical to keep all \glspl{pe} busy for a significant fraction of the operating time.
This poses a significant challenge on the memory and interconnect subsystems, which must be able to sustain the bandwidth required to feed the \glspl{pe} with data.
% Exacerbating this problem, is the fact that peak performance has been scaling at a rate of $3\times$ every 2 years, outpacing the growth of DRAM bandwidth, which has only scaled at $1.6\times$ every 2 years \cite{gholami2024}.
Pressure on main memory can be relieved by reusing data on-chip.
To this end, most accelerators present a \gls{llc}; a notable example is Nvidia's H100 \gls{gpu} with its 50~MB L2 cache \cite{choquette2023}.

To further multiply on-chip bandwidth, most accelerators feature additional levels of memory, e.g. shared memory in \gls{gpu} \glspl{sm}, and on-chip networks to provide shorter and parallel communication paths between \glspl{pe}.
Emblematically, Nvidia also recently introduced direct \gls{sm}-to-\gls{sm} communication within \glspl{gpc} in their Hopper-architecture \glspl{gpu} \cite{choquette2023}, where \glspl{sm} in a \gls{gpc} are interconnected together by a dedicated on-chip \gls{sm}-to-\gls{sm} network.
%% Removed for brevity.
% Unfortunately, detailed implementation and performance specifics remain undisclosed due to the architecture's proprietary nature.
% While attempts towards dissecting and benchmarking this feature have been made, a detailed microarchitectural-level understanding of the implementation and its performance implications have yet to be developed \cite{luo2024}.

% On-chip networks are a scalable solution to augment available on-chip bandwidth.
% The effective bandwidth utilized by an application depends on the particular communication pattern it exhibits and on specific network characteristics, such as the injection bandwidth at each node.
These parallel communication paths can be exploited by taking advantage of a computation's data reuse patterns.
In the case of matrix multiplication $\textbf{C}=\textbf{A}\times\textbf{B}$, a key kernel for \gls{ml} workloads, blocks of rows of matrix \textbf{A} are loaded into distinct clusters, while blocks of columns of matrix \textbf{B} have to be broadcast to all clusters (as detailed in section \ref{sec:performance}).
% a key kernel for \gls{ml} workloads, rows of the first matrix are reused with all columns of the second.
In this setting, multicast communication is extremely beneficial; as such, many recent commercial platforms \cite{prabhakar2024, vasiljevic2024, maddury2024, makino2024} integrate multicast-capable on-chip networks, but their implementations remain undisclosed and, to the best of our knowledge, there are no detailed performance analyses of these designs in the open literature.
% Multicast allows a message to be sent to multiple destinations in parallel over multiple links.
% , effectively multiplying the injection bandwidth at the source node by the message's replication factor.
% In section \ref{sec:results} we will show how this can benefit \gls{ml} applications, as we demonstrate the effect of multicast communication on a key computational kernel such as GEMM.

Most works in the literature focus on the design of multicast-capable networks for cache-coherent shared-memory systems \cite{jerger2008, abad2009, krishna2011, konstantinou2020}, employing multicast in the coherency protocol implementation. For area and energy efficiency reasons, massively parallel \gls{ml} accelerators do not typically implement cache-coherency, relying on software-managed \glspl{spm} instead; \glspl{gpu} being a prominent example with their \glspl{sm}' shared memories.
Other works either assume a mesh topology \cite{krishna2011, ouyang2021, ouyang2023}, implement destination encodings which are not scalable to the massive parallelism in \gls{ml} accelerators \cite{abad2009, kim2010, zuckerman2024}, or both \cite{wang2009, samman2008, ma2012}.

This work presents the design of a scalable multicast-capable \gls{xbar}, suited for the implementation of on-chip networks for massively parallel \gls{ml} accelerators.
It further differentiates from previous works in that it is fully AXI-compliant and open-source, and thus readily available for integration with standard IPs.
Finally, to the best of our knowledge, this is the first work to evaluate the benefits of multicast communication on a key \gls{ml} kernel. To summarize our contributions, we:
\begin{enumerate}
    \item Design and implement a multicast-capable AXI \gls{xbar}, releasing it as open-source hardware.
\ifdefined\blindreview
    \footnote{www.hidden-for-double-blind-review.com}
\else
    \footnote{https://github.com/colluca/axi/tree/multicast}
\fi
    \item Extend Occamy \cite{occamy}, an existing open-source many-core \gls{ml} accelerator, with multicast capabilities.
\ifdefined\blindreview
    \footnote{www.hidden-for-double-blind-review.com}
\else
    \footnote{https://github.com/colluca/occamy/tree/multicast}
\fi
    \item Evaluate the \gls{xbar}'s area and timing characteristics and the overhead to support multicast.
    \item Evaluate the benefits of multicast communication on a key computational kernel for \gls{ml} workloads.
\end{enumerate}

\noindent
We elaborate on the first two contributions in section \ref{sec:implementation}.
The latter two are discussed in section \ref{sec:results}.

\section{Implementation}
\label{sec:implementation}

\subsection{Multicast-Capable AXI Crossbar}
\label{sec:xbar}

We develop our contributions on the open-source AXI \gls{xbar} design by Kurth et al. \cite{kurth2022}.
Its architecture is shown in figure \ref{fig:xbar}.
Masters and slaves are connected through an array of demuxes and muxes.
The \gls{xbar} is associated with an address map: a set of address rules, each mapping an address interval to a slave of the \gls{xbar}.
When a master sends a write request, the address is compared with every rule in the address map (by the address decoder), and the request is routed to the slave associated with the matching rule.
The destination address is propagated, unmodified, in the output request.
% We implement multicast in the form of multicast write transactions.
% As such, we will ignore AXI's AR and R channels in the following discussion, as the hardware handling these channels is not affected by our extensions.
%% Shortened to:
As multicast only involves write transactions (AW, W and B channels), we ignore AXI's AR and R channels in the following discussion \cite{axi}.

To define a multicast transaction, a write request must carry multiple destination addresses.
Various multi-address encodings have been proposed in the networking field \cite{chiang1994}, to address multiple nodes in a network.
While our method presents some similarities with the ``multiple region mask'' encoding \cite{chiang1994}, we target the representation of multiple addresses in the global memory space of a system, rather than subcubes of a k-ary n-cube network.

We extend the AXI protocol, without compromising backward compatibility, by passing a mask in the \lstinline{aw_user} signal.
If a bit in the mask is set to 1, the corresponding bit in the address is interpreted as a don't care (X), encoding both logic 0 and 1.
By masking $n$ bits in the address we can represent $2^n$ addresses.
For direct correspondence between address and mask bits, we take the mask to be as wide as the address, although this is not mandatory.

Figure \ref{fig:multiaddr} presents two example address sets that can be represented with our encoding.
While not all possible address sets can be represented, our encoding is suited for massively parallel accelerators, as the encoding size scales logarithmically with the total size of the address space and is independent of the address set size.
Conversely, the ``all destination'' encoding \cite{chiang1994}, which can represent any address set, scales linearly with the address set size.
% As we will show in section \ref{sec:occamy}, the flexibility provided by our encoding is sufficient to implement multicast in Occamy.

We extend the address decoder to support multi-address encodings.
The output is a mask (\lstinline{aw_select}) indicating which slaves contain at least one of the destination addresses, together with the subset falling within each slave.

We require every multicast-targetable region, defined by a ``multicast rule'', to 1) be a power-of-two in size and 2) be aligned to an integer multiple of its size.
Any rule satisfying these constraints can be converted from the interval-form encoding (IFE) to the mask-form encoding (MFE), using the following formulas:

\begin{minted}{python}
  mfe.addr = ife.start_addr
  mfe.mask = ife.end_addr - ife.start_addr - 1
\end{minted}

We integrate logic to convert all multicast rules to mask form.
Calculating \lstinline{aw_select} then boils down to:
\begin{minted}{python}
  masked_bits         = req.mask | rule.mask
  match_bits          = ~(req.addr ^ rule.addr)
  aw_select[rule.idx] = &(masked_bits | match_bits)
\end{minted}
% That is, if all bits in the request address match all bits in the rule address, with the exception of bits that are masked in the request or in the rule.
The intersection between the request's and a rule's address sets can be found by resolving the masked bits as:
\begin{minted}{python}
  out.mask = req.mask & rule.mask
  out.addr = (~req.mask & req.addr) | (req.mask & rule.addr)
\end{minted}

\begin{figure}[t]
  \captionsetup{belowskip=-1em}
  \centering
  \includegraphics[width=\columnwidth]{fig/multiaddr}
  \caption{Examples of contiguous (left) and strided (right) address sets representable with our encoding, as paths in the binary number tree. Mask bits selectively fork the path of the original address (blue).}
  \label{fig:multiaddr}
\end{figure}

The \gls{xbar} logic is implemented in the \lstinline{axi_demux} and \lstinline{axi_mux} submodules.
The prior demultiplexes AW and W channel transactions from a master to the addressed slaves, and multiplexes B channel transactions in the opposite direction.
As B responses from different slaves can arrive out-of-order, the demux blocks AW transactions with the same AXI ID as any outstanding transaction, unless directed to the same slave.
To evaluate this condition, it maintains a table of slaves occupied by outstanding transactions, indexed by AXI ID.

Upon a multicast, multiple B responses are expected from different slaves.
Processing multicast transactions out-of-order would require expensive buffering and deadlock-avoidance logic.
We thus disallow multicast transactions until all outstanding unicast transactions have completed and vice versa.
Multiple outstanding multicast transactions are allowed if directed to the same master ports, within a configurable maximum number.

\begin{figure*}[htb]
  \centering

  % Row 1
  \begin{subfigure}{0.355\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/xbar}
    \caption{}
    \label{fig:xbar}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.34\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/mux}
    \caption{}
    \label{fig:mux}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.26\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/occamy}
    \caption{}
    \label{fig:occamy}
  \end{subfigure}

  % Add some space between the rows
  \vspace{0.4em}

  % Row 2
  \begin{subfigure}{0.84\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/demux}
    \caption{}
    \label{fig:demux}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.15\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/deadlock}
    \caption{}
    \label{fig:deadlock}
  \end{subfigure}

  % Combined caption
  \caption{(a) Block diagram of a 4-to-4 AXI \gls{xbar}, (b) AXI mux submodule (unicast datapath is highlighted in blue, multicast datapath in green, and the logic arbitrating the two in orange), (c) Occamy SoC and (d) AXI demux submodule (multicast stall logic is highlighted in orange, logic controlling AW channel forking in blue, and B channel joining in green); (e) Scenario creating the deadlock condition.}
\end{figure*}

Figure \ref{fig:demux} shows a high-level block diagram of the multicast logic in the \lstinline{axi_demux} submodule.
The green region highlights the logic responsible for joining B responses from different master ports.
The \lstinline{stream_join_dynamic} module ensures that a B handshake is propagated only after receiving a response from every slave.
All responses carry the same ID; we arbitrarily propagate the ID from the first addressed slave using a priority encoder.
On the other hand, the \lstinline{resp} fields may differ and must be properly joined.
As the AXI specification does not cover this scenario, we choose to return a \lstinline{SLVERR} response if any of the responses are either \lstinline{SLVERR} or \lstinline{DECERR}.
We further disallow exclusive multicast transactions, excluding \lstinline{EXOKAY} responses, so the logic boils down to a simple OR-reduction.

Figure \ref{fig:mux} shows a block diagram of the \lstinline{axi_mux} submodule.
Highlighted in green is the logic required to handle multicast transactions.
Two additional 1-bit signals are generated in every demux and routed to every mux in the \gls{xbar}: \lstinline{aw.is_mcast} and \lstinline{aw.commit}.
The prior is used to select between unicast and multicast datapaths; multicast transactions are prioritized, as they have stricter ordering requirements.
The latter is required to prevent deadlocks.

Consider the scenario represented in figure \ref{fig:deadlock}.
Slave 0 receives the AW0 transaction before AW1. According to the AXI specification \cite{axi}, it must thus receive all W0x transactions before any W1x transaction.
On the other hand, slave 1 expects W transactions in the opposite order.
As we cannot buffer all W transactions, we must stall a transaction until all destinations are ready to receive it.
This condition leads to a deadlock, as master 0 waits on \lstinline{w_ready} from slave 1, and master 1 from slave 0.

To prevent this, we force a master to ``acquire'' all slaves at once, breaking Coffman's ``wait for'' condition \cite{coffman1971}.
This is achieved by using a priority-encoder (\lstinline{lzc} module), to ensure consistent master selections across muxes.
When all addressed muxes are ready, the demux asserts the \lstinline{aw.commit} signal, ``releasing'' the muxes in the following cycle.

\subsection{Multicast-Capable \gls{ml} Accelerator}
\label{sec:occamy}

A block diagram of the Occamy \gls{soc} \cite{occamy} is presented in figure \ref{fig:occamy}.
Occamy integrates a configurable number of Snitch clusters \cite{zaruba2021}, each equipped with a 128\,KiB L1 memory and \gls{dma} engine.
Clusters are interconnected through two networks: a narrow 64-bit network for synchronization and control packets issued by the cores' \glspl{lsu}, and a wide 512-bit network shared by the instruction cache and \gls{dma} subsystems.
Both networks are implemented by a two-level hierarchy of \glspl{xbar}.
At the top level, a configurable-size \gls{llc} is connected to the wide network.

Clusters are mapped to consecutive address intervals of size \lstinline{0x40000} starting from address \lstinline{0x01000000}, satisfying the constraints imposed for the definition of multicast targets.

We integrate our extension in every \gls{xbar} of the two networks.
We further extend the Snitch cluster's \gls{lsu} and \gls{dma} engine, to respectively issue multicast interrupts on the narrow network, accelerating synchronization, and data transfers on the wide network, enhancing data movement efficiency, as we will see in section \ref{sec:performance}.

\section{Results}
\label{sec:results}

\subsection{Area and Timing Analysis}

We synthesize the design using Synopsys' Fusion Compiler 2021.06 under
worst-case conditions at 0.72\,V and 125\,\textdegree C in GLOBALFOUNDRIES’ 12LP+ technology, with a 1\,ns clock constraint.

% Figure \ref{fig:area} shows the area of an N-to-N \gls{xbar}, with and without multicast support.
% On an 8-to-8 \gls{xbar}, our extensions introduce an overhead of \ResultEightByEightCrossbarOverheadkGE\,kGE, \ResultEightByEightCrossbarOverheadPercent\,\% of the original area.
% All configurations meet the target 1\,GHz operating frequency, with the exception of the 16-to-16 \gls{xbar} which incurs a \ResultSixteenBySixteenCrossbarFrequencyOverheadPercent\,\% frequency degradation.

Figure \ref{fig:area} shows the area of an N-to-N \gls{xbar}, with and without multicast support.
On 8-to-8 and 16-to-16 \glspl{xbar}, our extensions introduce overheads of \ResultEightByEightCrossbarOverheadkGE\,kGE and \ResultSixteenBySixteenCrossbarOverheadkGE\,kGE (\ResultEightByEightCrossbarOverheadPercent\,\% and \ResultSixteenBySixteenCrossbarOverheadPercent\,\% of the baseline \gls{xbar}), respectively.
% The area scales quadratically with N, as the number of demuxes (muxes) grows with the number of masters (slaves), and the complexity of each demux (mux) grows with the number of slaves (masters).
As the area scales quadratically with N, 16-to-16 is typically at the upper limit for \glspl{xbar} that can be implemented at the physical level, and interconnect scale-up is obtained by going multi-stage in a hierarchy of \glspl{xbar} \cite{kurth2022}.
% Extrapolating the data, we derive an asymptotic upper bound on the area overhead introduced by the multicast extension of \ResultAsymptoticOverheadPercent\,\%.

All configurations meet the target 1\,GHz operating frequency, with the exception of the 16-to-16 \gls{xbar} which incurs a very modest \ResultSixteenBySixteenCrossbarFrequencyOverheadPercent\,\% frequency degradation.

\begin{figure*}[t]
  \centering
  % Subfigure 1
  \begin{subfigure}{0.22\textwidth}
    \centering
    \includegraphics[width=\textwidth]{res/plot1}
    \caption{}
    \label{fig:area}
  \end{subfigure}
  \hfill
  % Subfigure 2
  \begin{subfigure}{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{res/plot2}
    \caption{}
    \label{fig:microbenchmark}
  \end{subfigure}
  \hfill
  % Subfigure 3
  \begin{subfigure}{0.28\textwidth}
    \centering
    \includegraphics[width=\textwidth]{res/plot3}
    \caption{}
    \label{fig:gemm-results}
  \end{subfigure}
  % Subfigure 4
  \begin{subfigure}{0.18\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/gemm}
    \caption{}
    \label{fig:gemm-schedule}
  \end{subfigure}
  \caption{(a) Area of the original and multicast-capable \glspl{xbar} (numbers on top of the bars report the area increase); (b) Speedup on the microbenchmark with our extensions (numbers on top of the bars report the equivalent parallel fraction according to Amdahl's law for the 32\,KiB data points); (c) Performance of the matmul kernel; (d) Parallelization and scheduling of the matmul kernel.}
\end{figure*}

\subsection{Performance Evaluation}
\label{sec:performance}

We conduct the performance evaluation through cycle-accurate RTL simulations of the Occamy \gls{soc} using QuestaSim 2023.4, with a 1\,GHz clock frequency. We assume an Occamy system with 32 clusters, organized into 8 groups of 4 clusters each, and a 4\,MiB \gls{llc}.

We first evaluate our extension on a microbenchmark, which consists in one cluster sending the same data to all other clusters using its \gls{dma} engine.
We compare the runtime of the multicast \gls{dma} transfer using our extensions to a multiple-unicast approach, where unicast \gls{dma} transfers are issued to every destination cluster.
For transfers to more than one group (i.e. 8, 16 and 32 clusters), we also compare to a hierarchical software-based multicast approach, where the source cluster sends the data to one cluster in every other group, which in turn forwards the data to the other clusters in its group.
The distribution within groups can thus proceed in parallel.

The colored bars in figure \ref{fig:microbenchmark} show the speedup of the multicast transfer over the multiple-unicast baseline.
The speedup increases with the number of clusters, and approaches the ideal parallel speedup, with the equivalent parallel fraction per Amdahl's law reaching \ResultThirtyTwoClusterEightKiBParallelFraction\,\% on 32 clusters.
This is due to constant sequential overheads, such as the round-trip latency, being amortized over multiple transfers.
Similarly, we observe a small increase in speedup with growing transfer sizes, ranging from \ResultThirtyTwoClusterTwoKiBSpeedup$\times$ to \ResultThirtyTwoClusterThirtyTwoKiBSpeedup$\times$ on a 32-cluster transfer.
The white overlays represent the speedup of the hierarchical software-based multicast approach over the baseline.
As we can see, hardware-supported multicast still gives significant speedups over the software-based approach, with a geometric mean speedup of \ResultThirtyTwoClusterGeometricMeanSpeedup$\times$ on the 32-cluster transfers. 

Finally, we evaluate how multicast support translates to tangible performance improvements on a key computational kernel for \gls{ml} applications, i.e. matrix multiplication (matmul).
We execute the largest square double-precision matrix multiplication tile which fits in Occamy's \gls{llc}: $256\times256$ matrices, accounting for double buffering.
As illustrated in figure \ref{fig:gemm-schedule}, every cluster computes a distinct $8\times256$ row block of the product matrix \textbf{C}, calculating an $8\times16$ tile of the row block at a time.
The corresponding tile of \textbf{A} need only be loaded once into L1, and can be reused in every successive (steady-state) iteration.
The cluster \glspl{dma} are used to move data between \gls{llc} and cluster L1 memories in a double-buffered fashion.

Figure \ref{fig:gemm-results} displays the attained performance in a roofline plot of the Occamy architecture.
The baseline kernel features a low steady-state \gls{oi} of \ResultBaselineTileNOperationalIntensity\,FLOPS/byte, as all clusters have to load the \textbf{B} matrix tile from the \gls{llc}.
This places the kernel in the memory-bound region, achieving \ResultBaselineTileNPerformanceGFLOPS\,GFLOPS, or \ResultBaselineTileNPerformancePercentage\,\% of the maximum theoretical performance with this specific \gls{oi}.

By exploiting multicast, we can load the \textbf{B} matrix tile once and broadcast it to all clusters in parallel. The total number of bytes read from the \gls{llc} is reduced, resulting in \ResultHybridTileNOperationalIntensityIncrease$\times$ and \ResultMulticastTileNOperationalIntensityIncrease$\times$ higher \glspl{oi} respectively with software-based and hardware-supported multicast.
These respectively translate to \ResultHybridTileNPerformanceIncrease$\times$ and \ResultMulticastTileNPerformanceIncrease$\times$ performance improvements, reaching \ResultMulticastTileNPerformanceGFLOPS\,GFLOPS with hardware-supported multicast. This result shows how \gls{ml} applications can benefit from multicast support, making it a viable solution to enhance the on-chip bandwidth utilization of many-core \gls{ml} accelerators.

\section{Conclusion}

In this work, we presented the design of a multicast-capable AXI crossbar, leveraging a scalable, yet flexible, multi-address encoding scheme.
We analyzed the area and timing characteristics of the design, showing how multicast support can be achieved with a modest area and timing overhead (\ResultSixteenBySixteenCrossbarOverheadPercent\,\% and \ResultSixteenBySixteenCrossbarFrequencyOverheadPercent\,\% respectively) even on the largest physically-implementable 16-to-16 AXI crossbar.
We integrated our design into an open-source 288-core \gls{ml} accelerator, demonstrating its flexibility on an actual system.
Finally, we evaluated the performance impact on a key computational kernel for machine learning workloads, matrix multiplication, measuring a \ResultMulticastTileNPerformanceIncreaseOverHybridPercentage\,\% improvement with our solution, proving that multicast can provide a low-cost solution to enhance the on-chip bandwidth utilization of massively parallel \gls{ml} accelerators.

%% Just doesn't fit after all the additions made during L's review.
% \section*{Acknowledgment}
% This work has been supported in part by ‘The European Pilot’ project under grant agreement No 101034126 that receives funding from EuroHPC-JU as part of the EU Horizon 2020 research and innovation programme.

% References section

\bibliography{paper}
\bibliographystyle{IEEEtran}

\end{document}
