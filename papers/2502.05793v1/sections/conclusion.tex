\section{Conclusion}
\label{sec:conclusion}

This paper studies the impact of the reference determinacy assumption in the NLI dataset creation process. We release the \datasetname benchmark, and investigate the trickle-down effect of reference ambiguity in NLI on both the human annotators and subsequently on the NLI model training process. We hope that future NLI researchers and practitioners pay attention to this problem, especially when trying to apply NLI models in downstream use cases.

\section*{Limitations}
Our study focuses on understanding the implication of reference (in-)determinacy and its impact from a data perspective. Our modeling experiments use one fixed architecture with different mixtures of NLI datasets for training. Although it is mostly due to the fact that we want to understand the impact of using different types of NLI datasets for training, experimenting with more models could potentially eliminate model architecture as the confounder in our results. 
Although not the focus of our study, but the study could be extended and strengthened with experiments with large language models to understand the models react and respond to ambiguities in the input with the NLI task format. As we discussed at the end of \cref{sec:related}, we leave the two parts for future exploration.

\section*{Ethical Considerations}
To the best of our knowledge, our study does not introduce ethical concerns.