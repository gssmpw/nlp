\section{Related Work}
\label{sec:related}
% \paragraph{Ambiguity in language inference. } 
As ambiguity is an indispensable element in how we interpret and express language, many language understanding tasks require models to be able to recognize the resolve the ambiguity that exists in an user query \cite{xu-etal-2019-asking,zamani2020mimics,stelmakh-etal-2022-asqa,feng-etal-2023-generic,zhao2024beyond}. For instance,  \citet{min-etal-2020-ambigqa} observe that ambiguous questions might lead to different answers depending on what the user intent is, and this would lead to annotation ambiguities when raters are asked to provide a single answer for an ambiguous question. With NLI, previous studies \cite{pavlick-kwiatkowski-2019-inherent,nie-etal-2020-learn} have found that inherent human disagreements exist in NLI labels, and the disagreement usually follows instance-dependent pattern.   
This work explores the understudied problem of explaining and understanding the cause of disagreements. Being able to understand the disagreements can potentially lead to the development of better NLI systems, as 
\citet{zhou-etal-2022-distributed} and \citet{zhang-de-marneffe-2021-identifying} show the merit of modeling the uncertainty distribution of NLI labels.

Our work tries to understand the impact of annotation artifacts \cite{gururangan-etal-2018-annotation, bowman-etal-2020-new} on the downstream applicability of NLI tasks and models. In practice, researchers have found that NLI models would exploit such artifacts  \cite{poliak-etal-2018-hypothesis, mccoy-etal-2019-right}, which potentially hurts the downstream applicability. Our work is motivated by the use case of using NLI for verifying text and factual consistency \cite{schuster-etal-2021-get, schuster-etal-2022-stretching, honovich-etal-2022-true, gao-etal-2023-rarr}, and we seek to understand the limitation of NLI models in such use cases. To this end, a series of recent studies \cite{chen-etal-2023-propsegment, chen-etal-2024-sub, havaldar2025entailed} investigate alternative NLI task formulation and model architecture that incorporate additional context into NLI decisions. 

% \paragraph{Potential Broader Impact.}
Beyond NLI and its downstream applciations, it remains to be seen whether the reference or context ambiguity problem exists in other tasks and datasets as well.  Along this line,  \citet{liu-etal-2023-afraid} designs a suite of tests that show current instruction-tuned language models often fail to respond to input ambiguity. \citet{malaviya2024contextualized} study how under-specification of context can lead to lower agreement and unreliable evaluation conclusions when doing model evaluations.  
From these findings, We conjecture that this could be due to the inherent reference ambiguity in other tasks during the instruction-tuning stage of these models. We hope to explore this thread in future work. 

% \subsection{Inherent Disagreements in NLI}
% \subsection{Inductive Biases in NLI Models}