% \section{Motivation and Design Desiderata}
% \section{Preliminaries}
% \label{sec:prelim}
% \subsection{Textual Entailment and Contradiction}
% \textit{Textual Entailment} \cite{DaganGl04} is defined as a directional relation between a pair of text expressions, namely a premise and a hypothesis. We say that the premise entails the hypothesis if humans would typically infer that the hypothesis is most likely true in the context of the premise. 
% Similarly,
% we say that the premise contradicts the hypothesis if the hypothesis is highly unlikely to be true given the information described in premise \citep{de-marneffe-etal-2008-finding}. 

% The task of NLI or RTE is usually formatted as a three-way classification of a premise and a hypothesis into \emph{entailment}, \emph{contradiction} or \emph{neutral}, where \emph{neutral} indicates that the premise neither entails or contradicts the claims in full. NLI has widely been adopted as a general task format in the NLP community \cite{wang-etal-2018-glue, wang2019superglue}, and many datasets have since been created to facilitate the evaluation of model's language learning capabilities \cite{poliak-2020-survey}.

\section{The Reference Determinacy Assumption}
\label{ssec:prelim-rd}
When we create and label NLI examples, \textit{reference determinacy} (RD) is a practical assumption for guaranteeing the correctness and consistency of annotated labels. For instance, suppose a hypothesis and premise pair both mention \textit{John Doe}, the perceived entailment or contradiction relation could change based on whether we believe the two ``John Doe''s are a single real-world person. 

\paragraph{The creation processes of most NLI datasets assume reference determinacy.} 

For example, in SNLI \cite{bowman-etal-2015-large} and MNLI \cite{williams-etal-2018-broad}, annotators were asked to write novel hypotheses that are either true/false/neutral in the context of a given premise. During  labeling, the hypothesis is interpreted in the context of the premise, where entities and events in the two are assumed to be co-refer between the hypothesis and premise
% \alex{Nit: worth defining "entity and event coreference" crisply, since the term is not universally known} are assumed between the two. 
As a result, we see examples like in \autoref{tab:lead}, where majority of the annotators would agree on the contradiction or entailment label, when the premise and hypothesis likely refer to different events without the RD assumption.  

Following
% \hagait{Optional: "Following" and then replace "follow" with "use"} 
MNLI and SNLI, large-scale NLI datasets, e.g. \citet{marelli-etal-2014-sick, Khot2018SciTaiLAT, conneau-etal-2018-xnli}, among others,
% \hagait{Should we cite some?}
typically use similar processes to create and label hypotheses from given premises. 
Here, we study models trained on MNLI, SNLI, plus other notable datasets including ANLI \cite{nie-etal-2020-adversarial} and VitaminC \cite{schuster-etal-2021-get}. We aim to understand the behavior of models trained on these datasets at recognizing relevance between hypothesis and premise pairs.

% \paragraph{NLI datasets are typically created with the reference determiancy assumption.} 

% \subsection{NLI datasets and Annotation Processes}
% \label{ssec:nli-annotation}
% \subsection{Human disagreements on contradictions}


% \subsection{Models perform worse on contradictions}






