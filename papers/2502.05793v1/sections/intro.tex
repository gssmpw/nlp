
\section{Introduction}
Natural Language Inference (NLI), or Recognizing Textual Entailment (RTE), provides a general task format for evaluating the semantic relation between two pieces of text, where a system is expected to predict if a hypothesis statement can be inferred from a given premise.
% Textual entailment describes the directional relationship between two text expressions, where one can be inferred true by human readers given the other. 
For the past few decades, NLI has been the centerpiece for the development and evaluation of language understanding systems \cite{dagan2005pascal, bowman-etal-2015-large, williams-etal-2018-broad, nie-etal-2020-adversarial}.  

As the use of NLI now spreads across a wider variety of downstream applications, such as text classification \cite{yin-etal-2019-benchmarking}, fact verification \cite{schuster-etal-2021-get}, hallucination detection \cite{kryscinski-etal-2020-evaluating}, text attribution \cite{gao-etal-2023-rarr}, etc., 
% it becomes important to understand how \textit{applicable} the NLI task definition, datasets and models are to the downstream use cases. 
it is important to understand how the \textit{definitions} and \textit{assumptions} made for collection of previous NLI datasets and models trained on them affect their usefulness in downstream use cases.
% Specifically, we question whether there exist discrepancies between the NLI task setups versus how NLI is used in downstream applications. 

\input{tables/lead_table}

In this paper, we revisit and study the effect of \textit{reference determinacy} (RD), a common assumption formed in the labeling of NLI datasets.
With RD, the NLI label between a pair of premise and hypothesis is annotated under the assumption that the pair refer to the same context \cite{bowman-etal-2015-large}.
We illustrate the idea behind RD through an example in \autoref{tab:lead}, where the premise and the hypothesis describe two different events. 
The premise \textit{contradicts} the hypothesis (i.e.,\ premise $\rightarrow \neg$ hypothesis) only when we opt to assume that the two events happen on the same road at the same time. Otherwise, the pair would be labeled \textit{neutral}, as the two events are most likely unrelated. 

RD is a practical assumption for the NLI label definition. Without the RD assumption, the entailment and contradiction relations would only exist when the hypothesis and premise describe functional relations that are universally true or false \cite{ritter-etal-2008-contradiction}, e.g. factual knowledge about an entity. For this reason, most large-scale NLI benchmarks follow the RD assumption during their annotation processes (\cref{ssec:prelim-rd}). However, if we train NLI models exclusively on hypothesis-premise pairs created with the RD assumption, this could lead to the resulting models having limited ability to recognize if a hypothesis is relevant to a premise.

We demonstrate the trickle-down effects of such NLI model behavior in downstream tasks such as fact verification.
Specifically, we sample claims from FEVER \cite{thorne-etal-2018-fever} and VitaminC \cite{schuster-etal-2021-get} and study how NLI models behave when used to verify against evidence retrieved from the web. From the sampled claims, we construct the \datasetname benchmark (\cref{sec:benchmark}), which features 1,143 NLI pairs with expert judgements for whether the premise and hypothesis refer to the same context, as well as the correct NLI label.

With \datasetname, we observe that both finetuned NLI models as well as LLMs few-shot prompted to classify 3-way NLI labels often fail to recognize context mismatches, which leads to many false entailment and contradiction predictions. On five popular NLI datasets (\cref{sec:results}), we demonstrate that different combinations of training datasets result in similar type of reference (in-)determinacy problem in the finetuned model. This indicates the existence of a reference determinacy bias in all five datasets, which we discuss in the context of how each of the five datasets are created. We propose strategies to filter out entailment or contradiction examples labeled only due to the reference determinacy assumption, and show this can mitigate the reference determinacy bias of finetuned NLI models at inference time.

Reference determinacy, we discover, can also partly explain part the distribution of human disagreements of NLI labels, a problem known to be widespread in popular NLI datasets  \cite{pavlick-kwiatkowski-2019-inherent,nie-etal-2020-learn}. Our analysis shows that human typically disagree more on examples where reference determinacy cannot be safely assumed, and disagreements happen when annotators are instructed to do so regardless.
% We demonstrate that most NLI datasets exhibit. We show that, 

In summary, our contributions in the paper are: 
\begin{itemize}[leftmargin=*, itemsep=0em]
    \item We introduce the \datasetname benchmark, a dataset featuring 1,143 examples for studying the the effect of reference determinacy in NLI, a common assumption in the creation processes of NLI datasets.  
    \item With \datasetname, we investigate the downstream impact of the reference determinacy assumption of NLI dataset creation process. We show that finetuned NLI models and LLMs exhibit reference determinacy bias and often fail to recognize context mismatches.
    % \hagait{optional: I feel like this item should come first as it is more concrete. \alex{I'll throw in a weak vote for keeping Sihao's order here; I agree that 2nd bullet is more concrete, but the narrative flows better as is IMHO}}
    % We introduce the \datasetname benchmark featuring 1,143 examples with expert-labeled reference determinacy judgements and the NLI labels. 
    \item We discover and study the connection of the reference determinacy assumption to the inherent human disagreement on NLI labels.  
    % \hagait{I wonder if this item is too long. Maybe we can split into (1) show models have RD bias. (2) Suggest ways to mitigate the issue. (3) discuss the connection with human disagreement, which can be merged with the very first item ("We investigate and discuss the effect of...")} 
    % We discuss the implication of this and its connection to human disagreement on NLI, as well as a few options to mitigate the issue.  
\end{itemize}
% we introduce the \datasetname benchmark. The hypothesis and premise pairs in \datasetname are created to emulate the fact verification setting \cite{thorne-etal-2018-fever}, where the premise is retrieved from Wikipedia, but is not necessarily relevant to the hypothesis. We evaluate ... 



% NLI tasks and datasets has been widely adopted for evaluating the progress of language understanding systems \cite{bowman-etal-2015-large, }. To date, many tasks \cite{dagan2005pascal}


% Story of the paper
% \begin{itemize}
%     \item Motivation: NLI models have a hard time distinguishing between neutral and contradictions.
%     \item Observation: Models are super bad at classifying whether premise and hypothesis are relevant.
%     \item RefNLI Benchmark: A hard, adversarial benchmark for neutral vs. contradiction + neutral vs. entailment classification
%     \item Human disagreements on NLI can only in part be attributed to Reference problems. X$\%$ of examples in MNLI and Y$\%$ of examples in SNLI 
%     \item (Optional but better to have) Benefit of modeling contradiction -- Better results for contradiction detection in a downstream task? 
% \end{itemize}

% t5-3B
% (array([0.93069307, 0.93913829, 0.87037623]), array([0.9462095 , 0.90584133, 0.88978668]), array([0.93838715, 0.92218935, 0.87997443]), array([3179, 3441, 3094]))
% t5-base
% (array([0.87088989, 0.92017599, 0.80789555]), array([0.90814722, 0.85091543, 0.84001293]), array([0.88912843, 0.88419145, 0.82364126]), array([3179, 3441, 3094]))
% t5-small
% (array([0.81875   , 0.87667393, 0.77410118]), array([0.86536647, 0.81807614, 0.7863607 ]), array([0.84141306, 0.846362  , 0.78018278]), array([3179, 3441, 3094]))

% Label mapping is not 
% bert-large
% (array([0.91332511, 0.7957308 , 0.893125  ]), array([0.85110664, 0.85943004, 0.88951136]), array([0.88111888, 0.82635468, 0.89131452]), array([3479, 31230, 3213]))
% bert-base
% (array([0.89445828, 0.77009456, 0.85771979]), array([0.82581201, 0.83445405, 0.85932151]), array([0.85876551, 0.80098356, 0.8585199 ]), array([3479, 3123, 3213]))
% bert-tiny
% (array([0.75566389, 0.67387896, 0.72634597]), array([0.75740155, 0.6881204 , 0.70961718]), array([0.75653173, 0.68092522, 0.71788413]), array([3479, 3123, 3213]))

