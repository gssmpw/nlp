\section*{Limitations}
Since the \datasetname dataset feature entailment labels for \textit{all} propositions in a document, the label distribution are naturally imbalanced, which would potentially pose challenge for modeling. We observe low presence of contradiction examples in our dataset construction process, which could be a limiting factor for the utility of the dataset.
Unlike previous NLI datasets~\cite{bowman-etal-2015-large, williams-etal-2018-broad}, we speculate that reference determinacy, i.e. whether the hypothesis and premise refer to the same scenario at the same time, cannot be certainly guaranteed and safely assumed in our case, which in part leads to low presence of contradictions during annotation.
We offer a detailed discussion on the implications of reference determinacy and contradictions in Appendix~\ref{appendix:contradictions}. We leave the exploration on \emph{natural} contradictions for future work. 

As the annotation complexity and cost scales quadratically w.r.t. the number of propositions in a document, we truncate the documents in \datasetname to the first ten sentences of the original document. 

\section*{Ethical Considerations}
In the proposition-level entailment task ($T_2$), the inference of the entailment relation between a premise document and a hypothesis proposition uses the \emph{assumption} that the premise document is true. The assumption is common to NLI datasets \cite{dagan2005pascal, bowman-etal-2015-large, williams-etal-2018-broad}, and is necessary for the task's structure. With the documents in \datasetname, we make the assumption only for the experimental purpose of $T_2$, and make no claim about the actual veracity of the premise documents.

The full dataset will be released upon publication.