\begin{table*}[t]
    \centering
% \resizebox{\textwidth}{!}{%
\small
\begin{tabular}{p{0.2\linewidth}p{0.45\linewidth}cc}
\toprule
\multicolumn{1}{c}{\textbf{Hypothesis}} &
\multicolumn{1}{c}{\textbf{Premise}} &
\multicolumn{1}{c}{\textbf{RefNLI Label}} &
\multicolumn{1}{c}{\textbf{Model Pred.}}

\\ \midrule
Sabbir Khan made his directorial debut in 2001. &
In 2009 he made his directorial debut with the film ``Kambakkht Ishq'' (2009) that starred Akshay Kumar and Kareena Kapoor. &
\textcolor{orange}{Ambiguous} & 
\textcolor{magenta}{Contradiction}
\\ \midrule
\multicolumn{4}{p{0.95\linewidth}}{\textcolor{gray}{\textbf{Explanation}: The premise contains the ambiguous reference ``he'' as the director that made the debut. However, there exists an assignment of the pronoun "he" such that the hypothesis can be contradicted. In this case, without resolving the pronoun reference, the NLI label can not be determined. Therefore, the label here is ``Ambiguous''. }}
\\ \midrule
Wales has a large region rich in coal deposits. &
Recent explorations have revealed prospective deposits of rare-earth elements, a company is proposing further analysis of these mineral deposits. &
Neutral & 
\textcolor{magenta}{Contradiction}
\\ \midrule
\multicolumn{4}{p{0.95\linewidth}}{\textcolor{gray}{\textbf{Explanation}: The premise does not specify the location of the deposits of rare-earth elements. However, as coal is not a type of rare-earth element, we know for sure that whichever location the premise is referring to, the premise here cannot be used to support or contradict the hypothesis. Therefore, the label is ``Neutral''. }}
\\ \midrule
Same Old Love is a work of music. &
``Same Old Love'' was also performed on ``The Ellen DeGeneres Show'', ``The Tonight Show Starring Jimmy Fallon'', 2015 American Music Awards, and at the 2015 Billboard Women in Music. &
\textcolor{teal}{Entailment} & 
\textcolor{magenta}{Contradiction}
\\ \midrule
\multicolumn{4}{p{0.95\linewidth}}{\textcolor{gray}{\textbf{Explanation}: The annotators agree that it's reasonable to assume that ``Same Old Love'' refers to the same thing without ambiguity here. Therefore, the label here is ``Entailment''. }}
\\ \midrule
Buffy the Vampire Slayer is exclusively a Japanese television series. &
``Buffy the Vampire Slayer'' comics refer to comic books based on the television series ``Buffy the Vampire Slayer'' & \textcolor{magenta}{Contradiction} &
\textcolor{teal}{Entailment}
\\ \midrule
\multicolumn{4}{p{0.95\linewidth}}{\textcolor{gray}{\textbf{Explanation}: Even though there could be a Japanese television series named Buffy the Vampire Slayer, the premise would refute the hypothesis that it is exclusively a Japanese television series. Therefore, the label here is ``Contradiction''. }}

\\ \bottomrule
\end{tabular}%
% }
\caption{
Examples from our study and the \datasetname benchmark. Compared to the usual three-way NLI label set, i.e. \emph{entailment}, \emph{neutral} and \emph{contradiction}, we explicitly distinguish the \emph{ambiguous} cases, where reference determinacy between the hypothesis and premise is meaningful yet cannot be established. ``Model Pred.'' shows predictions made by the RoBERTa-based NLI model \citet{nie-etal-2020-adversarial} under three-way classification. 
% The gray text is not included in the retrieval but is provided only for reader to understand the context.
}
\label{tab:refnli-examples}
\end{table*}

\section{A Case Study of Reference (In-)Determinacy}
\label{sec:benchmark}
NLI models are typically finetuned exclusively on examples created with the reference determinacy assumption. We first study the effect of the RD assumption when we use such NLI models to solve downstream tasks.
Specifically, we aim to understand how an NLI model would behave in a realistic scenario where the premise can be irrelevant to the hypothesis.
In such cases, if there exists enough information in the evidence to establish reference determinacy, i.e. humans would be able to determine whether the evidence is related to the claim or not, an ideal NLI model should be able to correctly derive the NLI label. 

Motivated by this, we study the use of NLI for the task of fact verification. 
We construct the \datasetname benchmark, which features 1,143 pairs of claim and retrieved Wikipedia evidence sentence, with human-labeled reference determinacy and entailment relations.
% We study the behavior of NLI models when they are applied to verify claims against such retrieved evidence where reference determinacy cannot be safely assumed. 
 
% To understand the effect of training a NLI model exclusively with examples created with the reference determinacy assumption, we study 

% we conduct a case study and introduce the \datasetname benchmark. The examples in \datasetname are sampled to emulate the fact verificaiton setting, where a hyasdpothesis needs to be verified against a corpus of textual knowledge source, e.g. Wikipedia. 

% Match Title Count: 
%         {'neutral': 6983, 'contradiction': 1078, 'entailment': 244}
%         {'neutral': 0.8408187838651415, 'contradiction': 0.12980132450331125, 'entailment': 0.02937989163154726}
% Match Title Count: 
%         {'neutral': 176606, 'contradiction': 64019, 'entailment': 5841}
%         {'neutral': 0.71655319597835, 'contradiction': 0.2597477948276841, 'entailment': 0.023699009193965902}

\subsection{Sampling Claims and Evidence}
% To construct the \datasetname benchmark for our study, 
We start by sampling claims from the validation and test splits of FEVER \cite{thorne-etal-2018-fever} and VitaminC \cite{schuster-etal-2021-get}. With each claim, we use BM25 to retrieve the top-10 passages from an English Wikipedia dump from 2018-07-01 with \texttt{pyserini} \cite{Lin_etal_SIGIR2021_Pyserini}  .\footnote{\url{https://github.com/castorini/pyserini}} Note that most of the retrieved passages would not be related to the entity or event described in the claim.  
Next, given each claim and each sentence in the top-10 retrieved passages, 
we classify their relation with a widely-used, pretrained RoBERTa model \cite{liu2019roberta} finetuned on a mixture of NLI datasets from \citet{nie-etal-2020-adversarial}.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/claims_categorization.png}
    \caption{The distribution of label predictions by RoBERTa NLI mixture model from  \citet{nie-etal-2020-adversarial} when used to verify claims against retrieved evidence sentences from the correct vs. (most likely) irrelevant Wikipedia pages.}
    \label{fig:fever-dist}
\end{figure}


\paragraph{NLI model predicts many false contradictions.} 
On the development set of FEVER, we compare how the model behaves when a claim is verified against evidence sentences from the ``correct'' Wikipedia page, as labeled in FEVER, compared to sentences from other Wikipedia pages, which are likely to be irrelevant to the claim. In the later case, we expect the NLI model to discover very little \textit{supporting} or \textit{contradicting} evidence, as the page is unlikely to be relevant to the claim. 


\autoref{fig:fever-dist} shows the distribution of the NLI model's label predictions when used to verify claims labeled as \textit{supported (True)} or \textit{refuted (False)} by Wikipedia in FEVER. 
We observe that apart from the case where true claims are verified against sentences from the correct Wikipedia page, NLI models make contradiction predictions much more frequently than entailments in all the other three cases. 
While finding contradictions of false claim in the \textit{correct}  Wikipedia page where the refuting evidence comes from is what we want to see, interestingly we observe that the NLI model predict much more contradictions against \emph{irrelevant} Wikipedia pages, i.e. pages about a different entity. 
% \alex{But the 3 cases here are different in terms of how they each fit into our story, right? IMHO worth expanding on this explicitly: "contradiction of false claim in correct page" is a good thing, while "...of true claim in irrelevant page" shows the capacity off OTS NLI models to get wrong answers and "...of false claim in irrelevant page" shows their capacity to wrongly attribute real contradictions}. 
In cases where the sentence comes from such irrelevant Wikipedia pages, the pattern of potential ``false contradictions'' from the model is largely visible. 
The finding here echoes our initial hypothesis, suggesting the NLI model seems to be lacking the ability to recognize whether an evidence sentence refers to the same context as the claim.

% and keep all claim-sentence pairs where the model predicts entailment or contradiction. Here, we focus only on examples of entailment and contradiction following our initial observation that, in scenarios where reference determinacy matters, NLI mistakes are most typically false contradictions and entailments that become neutral if we don't assume reference determinacy. We will demonstrate how stronger NLI models and LLMs behave compared to the RoBERTa NLI model on the set of examples. 

\subsection{The \datasetname Benchmark}
\label{ssec:benchmark}
To further validate our hypothesis and understand why NLI models behave this way, we design a human study and analyze the example predictions made by NLI models in this setting.

From the set of examples where the RoBERTa NLI model predicts entailment or contradictions, and the evidence does not come from the correct Wikipedia page, we sample a subset for human annotation uniformly at random. The authors of the paper then annotate each claim and evidence sentence pair with one of the following four labels. The label set here follows what is expected from a fact verification system, when asked to verify a given claim in the context of the evidence.

\begin{itemize}[leftmargin=*, itemsep=0em]
    % \item \hagait{Maybe move that to be the last item? Basically neutral's definition is "not ambiguous" AND "not contradict" AND "not entail", so maybe it is clearer to have that after all of the rest.}\alex{+1 to Hagai, but this should be 3rd, since "ambiguous" is defined in terms of the other 3}
    \item \emph{Entailment}: if the human annotator thinks that the evidence and claim likely refer to the same context, and the evidence is sufficient to fully support the claim.
    \item \emph{Contradiction}: if the human annotator thinks that the evidence and claim likely refer to the same context, and claim is unlikely to be true given the evidence.
    \item \emph{Ambiguous}: if it is unclear whether the claim and the evidence refer to the same context (e.g. contain ambiguous reference), and there exist multiple possible assignments or interpretations of references that could make the example fall into at least 2 of the other 3 labels.
    \item \emph{Neutral}: if it is clear that the evidence cannot support or contradict the claim in any way, i.e. there exists no interpretation or assignment of references of the evidence where it can support or contradict the claim.
\end{itemize}

Compared to the usual 3-way NLI labels, the label set here is designed to distinguish where reference determinacy cannot be safely established between a hypothesis and a premise. 
When there exist ambiguous references, a fact verification system should not make any assumption about the reference and conclude its entailment relation with the evidence. 
Note that even if there exist ambiguous references, as long as the premise is unrelated to the hypothesis, no matter how the ambiguous reference is interpreted, the system could still deem the claim as \emph{neutral}, as there is no way that the claim can be supported or refuted by the evidence. This follows the intuition that ambiguity in reference determinacy only matters when there exists an interpretation where the evidence could be related to the claim.
To help understand the motivation behind the label set design, we include one example of each label in \autoref{tab:refnli-examples}. For instance, the first example is labeled as ambiguous, as there exist an possible assignment of the pronoun \emph{he} $\rightarrow$ \emph{Sabbir Khan}, such that a fact verification should not conclude that the hypothesis is irrelevant to the evidence. On the other hand, in the second example of ``Wales coal deposit'', as Wales coal deposit is not a type of rare-earth element, the premise is always going to be irrelevant to the claim, no matter what the assignments of the ambiguous references in the premise are. 
% it is not clear who \emph{he} refers to in the premise, unless we assume the hypothesis and premise must be talking about the same entity. 
% \hagait{nit: should we add explanations for the labels (either in the table or outside of it)or we assume they're very very obvious to the readers?}. 
We include a more detailed description of the annotation guidelines and discussion of corner cases in Appendix \ref{appendix:rater-guidelines}.

\paragraph{The difference between \textit{neutral} vs. \textit{ambiguous}.}  From the NLI task's perspective, the notable difference is that \emph{neutral} hypothesis-premise pairs themselves contain enough information for humans to judge that the premise is irrelevant to the claim. In such cases, it is reasonable to expect a good NLI model to make the correct prediction, whereas for \emph{ambiguous} examples, the correct label cannot be determined without the RD assumption. In our study, we do not expect NLI models to work well for ambiguous examples. NLI models' behavior with respect to ambiguity is investigated in greater detail in a recent study from \citet{liu-etal-2023-afraid}. 

\input{tables/refnli}

\paragraph{Annotation process.}
The authors went through a total of 1,143 example pairs, where one author produced the initial label and another author verified and adjudicated the label.  
On a sub-sample of 102 claims, we ask three authors to produce the label individually and we observe 0.83 Fleiss' $\kappa$ under 4-way classification, suggesting a good inter-rater agreement under the setting. In the rest of the paper, we denote the annotated set of examples as the \datasetname benchmark. 

\paragraph{Statistics.}
In \datasetname, the authors went through a total of 1,143 pairs of claim and evidence sentences, with 905 \emph{neutrals}, 66 \emph{contradictions}, 37 \emph{entailments}, and 135 \emph{ambiguous} cases.
% \hagait{nit: Do we think statistics of the FEVER label are also interesting?}
% In cases where the claim contains ambiguous reference, annotators were instructed to skip the example altogether. The case of ambiguity in claim is studied by  

% \subsection{Experimental Settings}
