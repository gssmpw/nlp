

\section{Can Reference (In-)determinacy Explain Human Disagreements?}
% \subsection{Can human disagreements in NLI be attributed to RD?}
\label{ssec:human-disagree}

Next, we study whether inherent human disagreements \cite{pavlick-kwiatkowski-2019-inherent} on NLI labels can potentially be attributed, at least in part, by the reference ambiguity between the hypothesis and premise. We conduct an experiment with the ChaosNLI dataset \cite{nie-etal-2020-learn}. ChaosNLI contains samples of the original SNLI and MNLI datasets, where each example is re-labeled by 100 different crowdsource workers. 
% Here we focus on studying the label distribution by the 100 annotators. 
ChaosNLI presents an interesting case for our purpose, as the human raters were not given explicit instructions to assume reference determinacy, which was instead deferred to their own judgement. 
To understand whether and how reference ambiguity might lead to human disagreements, the authors went through 500 random samples respectively from SNLI and MNLI split of ChaosNLI, and labeled whether ambiguity exists between the hypothesis and premise, following the same annotation protocol as in \cref{ssec:benchmark}.

We compute the Pearson correlation between the number of votes each label received for each NLI example. Here, a higher correlation value between two labels (e.g., $\rightarrow 1$) indicates that humans disagree and confound the two labels more often, and vice versa. \autoref{tab:human-corr} shows the our results. 


\paragraph{Humans disagree more between contradiction and neutral labels.}
Overall, we observe that human raters tend to split votes between the neutral and contradiction labels more frequently than other combinations.
% Across all instances in ChaosNLI, 
% As \autoref{tab:human-corr} shows,
Notably, on SNLI, we see a much weaker negative correlation ($r=-0.08$) between contradiction and neutral, compared to the relatively strong negative correlation between the other two label pairs. On MNLI, we observe a similar pattern, yet the gap is much smaller ($r=-0.37$ between contradiction and neutral). 
When we compare the ChaosNLI annotations against the original labels from MNLI and SNLI's five-way annotation, we observe that the change in majority label happens more often between entailment vs. neutral and contradiction vs. neutral, as shown in \autoref{fig:mnli-snli-cm} in Appendix~\ref{app:disagree}.


% We additionally look at how the majority label flips with ChaosNLI's re-annotation compared to the original annotation from MNLI and SNLI. 
% As shown in \autoref{fig:mnli-snli-cm}, we observe that the majority change happens between entailment vs. neutral and contradiction vs. neutral. 
% For our purpose, ChaosNLI presents an interesting case for studying the effect of reference determinacy, because the annotation process of ChaosNLI does not explicitly enforce the reference determiancy assumption, unlike the annotatonMNLI and SNLI. 





\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/percentage.png}
    \caption{Distribution of the majority labels from the MNLI and SNLI split of ChaosNLI, when the reference between the hypothesis and premise is \textit{ambigious} vs. \textit{unambiguous}.}
    \label{fig:percentage}
\end{figure}

\paragraph{Human disagreements can in part be attributed to reference ambiguity.} 
To estimate the percentage of examples that exhibit disagreements due to reference determinacy, we look at how the correlation between votes on different labels changes with respect to whether reference ambiguity exists in the data. From \autoref{tab:human-corr}, we see that in both MNLI and SNLI, a large fraction of the examples exhibit the problem of reference ambiguity ($\tilde~47\%$ in SNLI, $\tilde~46\%$ in SNLI). When we compare the case between ambiguous vs. unambiguous examples, we see that on both datasets, the rater agreement between contradiction and neutral improves when we go from ambiguous to unambiguous cases, while we observe the vice versa between entailment and neutral labels. 
We observe that the change in agreement patterns are mostly due to whether the rater can safely establish reference determinacy between the hypothesis and premise. If so, then whether raters would agree on the hypothesis is contradicted by the premise is less likely to be impacted by the additional judgement of whether the two statements refer to the same context. 

In \autoref{fig:percentage}, we see how the majority label distribution shifts according to whether ambiguity exists in NLI examples. We observe that in ambiguous cases, the annotators are more likely to label an example as neutral, while in the unambiguous case, raters are more likely to judge the hypothesis as entailed or contradicted by the premise. 

The findings here echo our hypothesis that the existence of reference ambiguity in NLI examples would lead to more disagreements among annotators. This potentially suggests that human disagreement can at least in part be attributed to the reference (in-)determinacy problem, and the annotation process would have more disagreement especially when raters are not explicitly instructed to assume RD during the annotation process.

% We see that the disagreement between contradiction and neutral, as well as entailment and neutral go down for both SNLI and MNLI. 
% manually inspect a random sample of 300 examples from each of the MNLI and SNLI sample subset
% \footnote{Due to time constraints of the submission, we only managed to inspect a subsample of 300. We plan to do a more comprehensive analysis on all of ChaosNLI in next version.}\hagait{nit: I think we can skip this footnote}
% in ChaosNLI. We count how many examples within the sample are cases where humans cannot confidently establish reference determinacy without assuming so. 
% On SNLI, we estimate a high percentage ($\sim68\%$) examples to have reference in-determinacy problem. On MNLI, the percentage is much smaller, but we observe that it is generally more difficult to make such judgements on MNLI.  

% \autoref{tab:human-corr} shows the correlation value on the subset of examples where reference cannot be determined by human. 




% \section{Broader Impact and Best Practices}
% \label{sec:broader}