% T5 large - all
% 
% STD      = [0.03626749 0.07923668 0.00927901] [0.06732767 0.10754811 0.03355734] [0.05361514 0.09868326 0.02832482]
% P, R, F1 = [0.15762274 0.2578125  0.98985801] [0.92424242 0.89189189 0.53922652] [0.26931567 0.4  0.6981402 ] [ 66  37 905]
%
% Current system: mnli_only_predictions.jsonl	 Num of predictions: 1077
% [2.80 17.36 1.21] [4.32 18.72 3.50] [4.49 16.18 3.46]
% [11.42 47.22 98.40] [96.88 73.91 42.56] [20.43 57.63 59.42] [ 64  23 867]
% Current system: nlimix_all_predictions.jsonl	 Num of predictions: 1143
% [3.57 7.51 0.93] [6.43 11.17 3.39] [5.29 9.60 2.89]
% [15.76 25.78 98.99] [92.42 89.19 53.92] [26.93 40.00 69.81] [ 66  37 905]
% Current system: serve_all_mix.jsonl	 Num of predictions: 1143
% [3.61 7.94 1.04] [6.29 9.29 3.16] [5.34 9.92 2.67]
% [15.76 25.78 98.99] [92.42 89.19 53.92] [26.93 40.00 69.81] [ 66  37 905]
% Current system: serve_all_wo_anli.jsonl	 Num of predictions: 1143
% [4.77 12.31 0.97] [7.27 12.38 2.87] [6.49 11.51 2.03]
% [19.87 50.00 98.46] [89.39 83.78 70.61] [32.51 62.63 82.24] [ 66  37 905]
% Current system: serve_all_wo_fever.jsonl	 Num of predictions: 1143
% [4.01 10.87 0.97] [6.61 12.18 2.95] [5.92 11.07 2.36]
% [15.71 37.97 98.35] [90.91 81.08 59.45] [26.79 51.72 74.10] [ 66  37 905]
% Current system: serve_all_wo_mnli.jsonl	 Num of predictions: 1143
% [6.00 11.55 0.79] [6.62 13.44 2.89] [7.86 12.01 1.95]
% [22.30 38.27 98.94] [90.91 83.78 71.93] [35.82 52.54 83.30] [ 66  37 905]
% Current system: serve_all_wo_snli.jsonl	 Num of predictions: 1143
% [4.39 8.87 0.94] [6.96 9.69 3.00] [6.30 8.77 2.25]
% [17.39 36.71 98.80] [90.91 78.38 63.76] [29.20 50.00 77.50] [ 66  37 905]
% Current system: serve_all_wo_vitaminc.jsonl	 Num of predictions: 1143
% [3.01 6.85 1.04] [6.62 11.60 3.28] [4.59 8.68 3.08]
% [14.06 23.57 98.85] [92.42 89.19 47.40] [24.40 37.29 64.08] [ 66  37 905]
% Current system: serve_anli_only.jsonl	 Num of predictions: 1143
% [5.09 12.25 0.86] [7.66 13.23 3.19] [7.02 12.21 2.26]
% [19.05 38.75 98.86] [90.91 83.78 66.96] [31.50 52.99 79.84] [ 66  37 905]
% Current system: serve_fever_only.jsonl	 Num of predictions: 1143
% [4.08 5.21 2.46] [8.90 14.63 3.58] [5.41 7.69 2.62]
% [6.29 12.69 90.42] [13.64 67.57 66.74] [8.61 21.37 76.80] [ 66  37 905]
% Current system: serve_mnli_only.jsonl	 Num of predictions: 1143
% [2.47 14.78 1.35] [6.07 10.24 3.55] [4.02 11.49 3.55]
% [10.92 62.75 98.20] [93.94 86.49 42.21] [19.56 72.73 59.04] [ 66  37 905]
% Current system: serve_sma_nli.jsonl	 Num of predictions: 1143
% [3.54 12.51 1.39] [8.25 13.73 3.25] [5.51 12.17 2.77]
% [13.56 44.78 97.83] [89.39 81.08 54.70] [23.55 57.69 70.16] [ 66  37 905]
% Current system: serve_snli_only.jsonl	 Num of predictions: 1143
% [1.83 12.56 2.55] [6.48 10.57 2.17] [3.12 10.91 2.89]
% [8.40 50.77 97.07] [93.94 89.19 21.99] [15.42 64.71 35.86] [ 66  37 905]
% Current system: serve_vitaminc_only.jsonl	 Num of predictions: 1143
% [3.80 8.21 1.05] [9.59 12.12 3.06] [5.25 9.72 2.17]
% [19.64 28.97 98.23] [83.33 83.78 67.40] [31.79 43.06 79.95] [ 66  37 905]

% Gemini XL 3 way
% Class entailment: Precision: 0.36792452830188677, Recall: 0.5909090909090909, F1: 0.45348837209302323
% Class contradiction: Precision: 0.5660377358490566, Recall: 0.8108108108108109, F1: 0.6666666666666667
% Class neutral: Precision: 0.964622641509434, Recall: 0.9048672566371682, F1: 0.9337899543378996

% T5 base (non)
% Class entailment: Precision: 0.09233176838810642, Recall: 0.8939393939393939, F1: 0.1673758865248227
% Class contradiction: Precision: 0.6571428571428571, Recall: 0.6216216216216216, F1: 0.6388888888888888
% Class neutral: Precision: 0.9640718562874252, Recall: 0.3558011049723757, F1: 0.5197740112994351

% Class entailment: Precision: 0.10387323943661972, Recall: 0.8939393939393939, F1: 0.18611987381703468
% Class contradiction: Precision: 0.6571428571428571, Recall: 0.6216216216216216, F1: 0.6388888888888888
% Class neutral: Precision: 0.9703703703703703, Recall: 0.43425414364640885, F1: 0.6



\section{Evaluating Model's Reference Determinacy Biases}
\label{sec:results}
With \datasetname, we try to understand the effect of training datasets on the resulting NLI models' capabilities of recognizing reference determinacy. For this, we finetune a T5-large \cite{raffel2020exploring} model on different combinations of NLI datasets, and study their behaviour on \datasetname. 

\subsection{Experimental Settings}
\paragraph{Datasets.} We study a mixture of five large-scale NLI datasets: SNLI \cite{bowman-etal-2015-large} MNLI, \cite{williams-etal-2018-broad},
ANLI, \cite{nie-etal-2020-adversarial} and VitaminC \cite{schuster-etal-2021-get} and the processed NLI sentence-pair style of FEVER used in VitaminC. 

\paragraph{Training.} We initialize the model with pretrained T5-large 1.1 checkpoint using the T5x library \cite{roberts2022t5x}. We finetune the model with different combinations of the datasets, as shown in \autoref{tab:refnli}. The label set across dataset is unified to match the three-way classification on MNLI and SNLI, where each label is represented as a single token in the T5 output vocabulary space. For variations of training dataset (mixtures), we use a learning rate of $1e-4$ with the Adam optimizer \cite{Kingma2014AdamAM} and batch size of 128 during finetuning. 
% Hyperparameter settings and additional details can be found in Appendix TODO 

\paragraph{Evaluation.} We evaluate each finetuned model on all examples in \datasetname. We report the per-label precision and recall of predicted label, which is computed by the output label token with the highest softmax probability. To account for the effect of using different classification thresholds for each label in label imbalanced setting, we additionally report the per-label area under ROC (AUROC) score over the output label probability distribution under one-label-vs-rest setting. 
% \hagait{I think AUROC is also problematic in label-imbalanced scenarios} 

We additionally evaluate Gemini$_{\texttt{ultra}}$ with 8-shot in context learning \cite{team2023gemini} as a point of comparison to contrast the behavior of finetuned NLI models with an instruction tuned large langauge model. 
\subsection{Results}
\autoref{tab:refnli} shows the classification results. We generally observe that models exhibit low precision and high recall on both contradiction and entailment predictions, suggesting the presence of many false positive predictions made on the two labels. In terms of AUROC, it's more visibly clear that models perform generally worse on recognizing contradictions compared to recognizing entailments, which echoes our observations in \cref{sec:benchmark}.

\paragraph{All training datasets show similar patterns of false contradictions and entailments.} Across all combinations of training datasets, we observe similar patterns of many false contradiction and entailment predictions, with slight variations across datasets. With respect to entailment predictions, we see almost all training configurations lead to high AUROC score (i.e. $>0.85$). However, with respect to contradictions, we observe a larger discrepency across different datasets. We observe that including SNLI and Fever(NLI) in the training mix would lead to worst performance in terms of contradiction detection.  In both leave-one-out and single dataset training settings, we observe ANLI to be the most useful dataset to include during training, especially for contradiction detection. Interestingly, ANLI (arguably) happens to be the one dataset where the reference determinacy assumption is least enforced during the annotation process, yet no definitive conclusion can ever be drawn here due to the existence of many other confounders.

On Gemini$_{\texttt{ultra}}$, we observe a much lower rate of false contradiction and entailment compared to all of the finetuned NLI models. That said, there still exists a gap between the performance on contradictions vs. entailments. For Gemini, we do not report the AUROC score as we do not have access to the output token probabilities during inference. 

% \hagait{I don't remeber if we discussed this, but I think that seeing the prediction distribution over the ambiguous examples is also interesting, i.e., out of ambiguous examples, how many (or \%) examples were predicted as entailment/contradiction/nuetral}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{c|ccc}
    \toprule
    \multirow{ 2}{*}{Model} & \multicolumn{3}{c}{$F_1$ score w.r.t each label}  \\
    & \emph{Entails} & \emph{Neutral} & \emph{Contradicts} \\
    
    \midrule
    T5-Small & 84.14 & 84.64 & 78.02 \\
    T5-Base & 88.91 & 88.42 & 82.36 \\
    T5-3B & 93.79 & 92.19 & 87.95 \\
    BERT-Tiny & 71.78 & 75.65 & 68.09 \\ 
    BERT-Base & 85.85 & 85.88 & 80.10 \\
    BERT-Large & 89.13 & 88.11 & 82.63 \\
     \bottomrule
    \end{tabular}
    \caption{Per-Label F$_1$ score of different models finetuned on MNLI and tested on MNLI validation set. We observe that model generally perform worse on contradictions compared to the other two labels.}
 \label{tab:model-perform-worse}
\end{table}

\begin{table}[t]
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \toprule
    \multirow{ 2}{*}{Label} & \multicolumn{3}{c}{Metric}  \\
    & \emph{Precision}~$\uparrow$ & \emph{Recall}~$\uparrow$ & \emph{AUROC}~$\uparrow$ \\
    
    \midrule
    Entail. & 15.76 $\rightarrow$ \textbf{32.26} & \textbf{89.18} $\rightarrow$ 84.85 & 90.91 $\rightarrow$ \textbf{94.57} \\
    Neutral & \textbf{98.99} $\rightarrow$ 97.81 & 53.92 $\rightarrow$ \textbf{69.09} & 87.49 $\rightarrow$ \textbf{88.49} \\
    Contra. & 15.76 $\rightarrow$ \textbf{20.29} & \textbf{92.42} $\rightarrow$ 84.85 & 90.91 $\rightarrow$ \textbf{91.18} \\
     \bottomrule
    \end{tabular}
}
    \caption{Per-Label precision recall and AUROC of T5-large trained on the mixture of five datasets before $\rightarrow$ after training set filtering described in \cref{ssec:mitigating}}
 \label{tab:mitigating}
\end{table}

\subsection{Are Contradictions More Difficult to Learn?}
In the previous section, we observe a wide performance gap when finetuned NLI models are applied to recognize contradictions in settings where reference determinacy cannot be assumed. An additional factor here is that contradiction might be inherently a more difficult problem to learn from the training data distribution. \autoref{tab:model-perform-worse} shows an experiment where we finetune different variants of BERT \cite{devlin-etal-2019-bert} and T5 on the MNLI training set. When we evaluate the models on the MNLI dev set, we observe that the model consistently perform worse on contradiction examples. 
Here we hypothesize that the low validation performance of contradictions might be attributed to the inherent human disagreement \cite{pavlick-kwiatkowski-2019-inherent}, where the human raters tend to have more disagreements on contradictions compared to the other labels. We show and discuss evidence of this, as well as how this can be connected to the reference determinacy assumption later in \cref{ssec:human-disagree}.   


\begin{table*}[t]
    \centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
    \multirow{ 2}{*}{Dataset} & \multirow{ 2}{*}{Ambiguous Reference?} & \multicolumn{3}{c}{Correlation Between Human Votes ($\downarrow$)}  \\
    & & \emph{Ent.} $\leftrightarrow$ \emph{Neu.} & \emph{Ent.} $\leftrightarrow$ \emph{Con.} & \emph{Con.} $\leftrightarrow$ \emph{Neu.} \\
    
    \midrule
    \multirow{ 3}{*}{SNLI \cite{bowman-etal-2015-large}} & \textit{All} & -0.63$^{**}$ & -0.73$^{**}$ & \textbf{-0.08}$^{*}\ \ $ \\ \cmidrule{2-5}
    & No ($\tilde~53\%$) & -0.74$^{**}$ & -0.48$^{**}$ & \textbf{-0.23}$^{**}$ \\ 
    & Yes ($\tilde~47\%$) & \textbf{-0.36}$^{**}$ & -0.51$^{**}$ & -0.61$^{**}$
    \\
    \midrule
    \multirow{ 3}{*}{MNLI \cite{williams-etal-2018-broad}} & \textit{All} & -0.62$^{**}$ & -0.50$^{**}$ & \textbf{-0.37}$^{**}$ \\ \cmidrule{2-5}
    & No ($\tilde~54\%$) & -0.64$^{**}$ & -0.74$^{**}$ & \textbf{-0.03}$\ \ \ \ $
 \\
    & Yes ($\tilde~46\%$) & -0.52$^{**}$ & -0.70$^{**}$ & \textbf{-0.25}$^{**}$ \\
     \bottomrule
    \end{tabular}
    }
    \caption{To understand how reference ambiguity affects human agreement in NLI, we compute the Pearson correlation among 100 human votes per example provided in ChaosNLI \cite{nie-etal-2020-learn}. Correlation of $-1$ indicates perfect agreement among raters on the distinction between two labels, and vice versa. We randomly sample 500 examples respectively from SNLI and MNLI split of ChaosNLI and annotated whether each example contains ambiguous reference or not. (* denotes $p < 0.05$, ** denotes $p < 0.01$ for the correlation coeffecient.) }
    % \alex{p-values for which hypothesis? that "yes ambiguous" bucket has a different probability of agreement for a pair of raters versus the "not ambiguous" bucket? and under which statistical test?}}
    % The ``wo/ RD'' row shows an estimate correlation on examples involving reference (in-)determinacy within a 300 random sample.}
    \label{tab:human-corr}

\end{table*}

\subsection{Mitigating the Effect of Reference Determinacy}
\label{ssec:mitigating}
To further validate that the reference determinacy assumption in the training data has an impact on downstream performance, we demonstrate that filtering out examples where reference determinacy cannot be easily determined improves the resulting model's performance on \datasetname. 

With the mixture of five training datasets, we check whether a contradiction or entailment example is likely to be affected by the reference determinacy assumption, by the simple heuristics of lexical overlap. If a hypothesis and the premise share a token-level Jaccard similarity less than or equal to $0.15$, we would discard this example from the training set, as we conjecture that it is more likely
% \alex{I'm somewhat queasy about making this claim as written, since we make no direct effort to validate this heuristic. Given the timeline, perhaps let's just hedge harder on the wording? e.g. "as it is likely that" $\to$ "as we conjecture that it's likelier that..."}
that the example is only labeled as contradiction or entailment due to the RD assumption. We filter out such examples from the training mix, and perform a rebalance of the label distribution by random re-sampling neutral examples to match the number of contradiction or entailment examples left in the dataset.  

The evaluation results are shown in \autoref{tab:mitigating}. We see that the method generally improves the precision of entailment and contradiction predictions. We also see minor improvements across all labels in terms of AUROC. The findings here further validate our hypothesis that training with examples created with the RD assumption has a trickle-down effect on the performance of NLI models in real-world settings.