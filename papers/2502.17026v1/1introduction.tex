
\section{Introduction}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=0.9\linewidth]{figs/demoFig.pdf}
%     \caption{A demonstration that different reasoning paths lead to the same answer.}
%     \label{fig:demo}
% \end{figure}

Deep learning models have long been criticized for their lack of trustworthiness due to their complex network structures and opaque decision-making processes~\cite{li2022deeplearninginterpretable,doshi2017interpretable,samek2021explainingdl}. This has motivated researchers to investigate methods for understanding and quantifying the uncertainty associated with these models~\cite{abdar2021dluncertaintyreview,loquercio2020generaluncertainty,maddox2019simpleuncertainty}. Recently, Large Language Models (LLMs) have demonstrated significant advancements over traditional deep learning approaches across a variety of tasks~\cite{zhao2023surveyllm,naveed2023comprehensivellm}. However, concerns about their reliability persist. LLMs often produce outputs that are difficult to verify, particularly in scenarios requiring complex reasoning~\cite{shi2024llmreasoning}. This introduces risks in critical applications, such as healthcare or legal domains~\cite{cascella2023llm-hearcare,jayakumar2023llm-legal}, where incorrect or unreliable reasoning can have severe consequences. Properly quantifying uncertainty in the reasoning processes of LLMs is therefore crucial for ensuring their safe and effective deployment.

Existing research on Uncertainty Quantification (UQ) for LLMs primarily focuses on analyzing semantic uncertainty~\cite{kuhn2023semantic,lin2023generating, qiu2024semanticuq, da2024llm}, which involves examining patterns in the meaning and phrasing of multiple responses generated for a given question. Although this approach provides insights into output-level variability, it neglects the logical reasoning steps that lead to these answers. As a result, it fails to address foundational issues in the reasoning process that could be used to debug or optimize model outputs. For instance, when asked by the the same question, different reasoning paths may converge on the same final answer (an example shown in Appendix Figure~\ref{fig:newnew}), yet some paths may involve inconsistencies or logical leaps. By quantifying uncertainty at the level of reasoning steps, we can better identify such inconsistencies, support human-in-the-loop systems for validating outputs in sensitive applications, and uncover weaknesses in a model's reasoning process. This highlights the importance of incorporating reasoning-based uncertainty quantification.

In this paper, we address the problem of uncertainty quantification for logical reasoning steps by explicitly modeling reasoning processes as logical topologies. Existing work often treats the reasoning process generated by single Chain-of-Thought (CoT) sequences as a single "long answer"~\cite{wei2022CoT,wang2023CoT_cue}, calculating semantic consistencies directly. While this approach captures some aspects of reasoning paths, it oversimplifies real-world reasoning processes, which often involve hierarchical dependencies and parallel sub-tasks. To overcome these limitations, we propose a novel formalism that explicitly models reasoning as a logical graph. In this representation, nodes correspond to individual reasoning steps, while edges capture logical dependencies between them. This structure enables more granular and interpretable analyses of uncertainty.

Based on the structural representation, we introduce a graph-based measure for assessing uncertainty, by first encoding the node and edge descriptions into the semantic embeddings, and then performing a graph-edit-distance comparison, our framework captures the uncertainty from both semantic and topology aspects. Besides, we also propose a redundancy measure for the valid reasoning path, which helps understand the LLM's reasoning effectiveness. %~\hua{did not see entropy-based measures in the method section}
Extensive experiments on diverse datasets and LLMs demonstrate the utility of the proposed quantification methods.
In summary, the contributions of this paper are:
\begin{itemize}
\item We identify limitations in existing UQ approaches for LLMs and propose a novel framework that explicitly models the reasoning process from explanations as a logical topology. 
\item We introduce multiple measures, including graph-based uncertainty measures and redundancy metrics, to provide granular reasoning variance and interpretable assessments of redundancy. 
\item We demonstrate the effectiveness of our framework through extensive experiments across multiple datasets and LLMs. Our results highlight its ability to identify inconsistencies in reasoning paths, improving trustworthiness, and interpretability. Additionally, we discover three widely adopted reasoning patterns in LLMs and show the chance of improvement under the redundancy measure.
\end{itemize}


%%%%%%%%%%%
