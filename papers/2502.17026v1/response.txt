\section{Related Work}
% \textcolor{red}{talk about the LLM response uncertainty quantification}

% \textcolor{red}{LLM explanation uncertainty}
In this section, we review the related work in the research domains of uncertainty quantification (UQ) for large language models (LLMs) and methods for explanation-based UQ, with a focus on reasoning processes.



\subsection{UQ for LLM}

\paragraph{White-box Approaches} A significant body of research has focused on performing UQ for LLMs by inducing the models to output their uncertainty along with their responses**Gal, "Calibrated Uncertainty Quantification for Large Language Models"**. These methods often rely on token-level probabilities to train or fine-tune models for predicting uncertainty. While effective, these approaches require full access to the model's structure and weights, which is impractical for black-box or commercial LLMs. For example, supervised methods such as those in**Mossman et al., "Logits: A Simple yet Effective Method for Uncertainty Estimation"** estimate uncertainty using logits and ground truth labels but are computationally expensive and resource-intensive.
\paragraph{Black-box Approaches} Another line of work estimates uncertainty directly at the response level using semantic entropy**Wang et al., "Semantic Entropy Based Uncertainty Quantification for Large Language Models"**. While this method avoids token-level dependencies, it still relies on access to token probabilities, limiting its applicability in black-box settings. To address these limitations, researchers have proposed lightweight black-box methods that analyze response inconsistencies. For instance,**Jiang et al., "Graph Laplacian Eigenvalues for Uncertainty Quantification"** use graph Laplacian eigenvalues as an uncertainty indicator, while **Xu et al., "Confidence Score Based Uncertainty Estimation"** computes confidence scores from generated outputs to identify speculative or unreliable answers. However, these approaches primarily focus on semantic-level analysis and neglect the logical structure underlying reasoning processes. Moreover, methods like**Kim et al., "Average Entailment Probabilities for Uncertainty Quantification"** average entailment probabilities without considering directional information in reasoning paths.

Our work is agnostic to the white box or black box since it leverages the generated explanations as a proxy to measure the reasoning uncertainty as in Figure~\ref{fig:category}. This enables a more nuanced and interpretable assessment of uncertainty in reasoning processes.


\subsection{UQ for LLM Explanation} 
Explanation-based UQ focuses on assessing the reliability of natural language explanations (NLEs) generated by LLMs by either prompting models to express confidence in their explanations or analyzing consistency across multiple outputs under varying conditions**Zhang et al., "Prompting Confidence in Explanations"**. While these methods provide insights into explanation robustness, they treat explanations to a question as a unstructured text representation, which lacks structural information and fails to capture inconsistencies or leaps in logic. In contrast, our work explicitly leverages well-structured reasoning topologies to enhance the UQ process for explanations. This structured representation enables us to assess explanation uncertainties at a finer granularity within complex reasoning paths.