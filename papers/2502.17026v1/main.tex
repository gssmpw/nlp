%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%%
% \documentclass[sigconf,anonymous,review]{acmart}
\documentclass[sigconf]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{tikz}
\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}    
\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{makeidx}
\usepackage{multicol}
\usepackage{balance}
\usepackage{enumitem}
\usepackage{array}
\usepackage{comment}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{amsthm}
\usepackage{mwe}
\usepackage{tcolorbox}
\usetikzlibrary{shapes.geometric, positioning}

\definecolor{darkpink}{rgb}{0.8, 0.2, 0.5}  % This is a custom dark pink color
\definecolor{lightblue}{rgb}{0.4, 0.529, 0.639}
\usepackage{setspace} % for adjusting line spacing

\definecolor{grey}{gray}{0.5}

\definecolor{custompurple}{HTML}{d0d7e2}

\newtheorem{definition}{Definition}


\input{math_commands.tex}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}

\usepackage{adjustbox}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{soul}

\usepackage{multirow}
\usepackage[textsize=tiny]{todonotes}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{colortbl}
\definecolor{blue}{HTML}{daebfa}
% \usepackage{subfigure}
\usepackage{bbm}
\usepackage{bbold}
\usepackage{listings}
% \usepackage{algpseudocode} 
\usepackage{xspace}
\def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}
\usepackage{multirow}
\usepackage{multicol}
\renewcommand{\b}[1]{\mathbf{#1}}%. 

\definecolor{lightpink}{HTML}{900e96}

\newcommand{\hua}[1]{\textcolor{blue}{hua:#1}}


% \usepackage{algpseudocode}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{$^\dag$Longchao Da, $^\dag$Xiaoou Liu, $^\dag$Jiaxin Dai, \\$^\ddag$Lu Cheng, $^\S$Yaqing Wang, $^\dag$Hua Wei}
\affiliation{%
  \institution{$^\dag$Arizona State University, $^\ddag$University of Illinois Chicago, $^\S$Purdue University.}
  \country{}
}
\email{hua.wei@asu.edu}
% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification. 
  The response data and code will be released upon publication.
  % For better reproducibility, the code and dataset are available at the anonymous \underline{repository}\footnote{\url{https://anonymous.4open.science/r/UQ-topology-1EAB/README.md}}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010147.10010178.10010224.10010245</concept_id>
  <concept_desc>Computing methodologies~Artificial intelligence</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010224.10010245.10010246</concept_id>
  <concept_desc>Computing methodologies~Natural language processing</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010224.10010245.10010250</concept_id>
  <concept_desc>Computing methodologies~Uncertainty quantification</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010224.10010238</concept_id>
  <concept_desc>Computing methodologies~Machine learning algorithms</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010224.10010245.10010249</concept_id>
  <concept_desc>Computing methodologies~Explainable AI</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Artificial intelligence}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[300]{Computing methodologies~Uncertainty quantification}
\ccsdesc[300]{Computing methodologies~Machine learning algorithms}
\ccsdesc[300]{Computing methodologies~Explainable AI}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Uncertainty Quantification, Natural Language Explanation, Large Language Models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\input{1introduction}
\input{2related}



\section{Preliminaries}
% \textcolor{red}{talk about the LLM explanation evaluation and common settings: given one response, generate explanations, and then use the early answering measures faithfulness.}
In this section, we provide the foundational concepts for our study, including the definition of uncertainty in LLMs and the quantification of uncertainty in natural language explanations (NLE).

\subsection{Uncertainty of LLMs}

Uncertainty quantification (UQ) has been a critical topic in classical machine learning tasks such as classification and regression~\cite{lakshminarayanan2017simple, gal2016dropout, hernandez2015probabilistic, abdar2021dluncertaintyreview}. However, natural language generation (NLG) poses unique challenges for uncertainty quantification due to its high-dimensional output space, the need to account for semantic equivalence across distinct token sequences, and the limited accessibility of internal model outputs in black-box LLMs~\cite{lin2023generating}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/category.pdf}
    \caption{The category of the LLM uncertainty measure. It can be divided into two types, direct measure and proxy-based measure (use the explanations of each answer).}
    \label{fig:category}
\end{figure}

Given these challenges, most UQ methods for LLMs focus on analyzing uncertainty directly from model-generated responses as shown in the left part of Figure.~\ref{fig:category}. The problem could be defined as follows:

\begin{problem} [Uncertainty Quantification for LLMs]
\label{prob:uq}
When an LLM $M$ is provided with an input $x$, either a query or prompt, the goal of an uncertainty function $U_{x}$ is to map the generated outputs to a scalar score that determines the uncertainty of the LLM $M$, i.e., 
\begin{equation}\label{eq:uq}
    U_{x} = \mathcal{U}\left(\{M(x_i)\}_{i=1}^n\right)
\end{equation}
\end{problem}

% \textcolor{green}{$U_x$ in equation~\ref{eq:uqall} represents  overall uncertatiny score. Maybe it's more reasonable. $U_x$ is the uncertainty score, and then $\mathcal{U}$ is the uncertainty function. }
Here, $\{M(x_i)\}_{i=1}^n$ denotes a set of $n$ responses generated by the model $M$, and $\mathcal{U}$ aggregates uncertainty across multiple responses. Note that while noticed that some work models have uncertainty describing the confidence of a specific output given the input~\cite{tanneru2024quantifying}, in this paper $U_{x}$ only depends on $x$ and is a property of the predicted distribution, which is estimated by $\mathcal{U}$ that aggregates uncertainty across multiple responses. 

Depending on the specific UQ method used, each $x_i$ may correspond to either repeated inputs or rephrased variations of the input prompt $x$. For black-box methods that analyze response variability or semantic consistency~\cite{lin2023generating}, multiple outputs ($n > 1$) are typically required. In contrast, white-box methods that rely on internal model information such as logits~\cite{kadavath2022language} may only require a single output ($n = 1$).
However, both black-box and white-box methods have limitations when applied to reasoning tasks. Black-box methods often focus on semantic output variability without capturing deeper uncertainties in reasoning steps. White-box methods are frequently inaccessible due to API restrictions or computational constraints. 




% There is uncertainty in LLMs, several ways to quantify it, such as direct evaluation (direct outputs) [black box, white box], and generating proxy explanations [black box, more suitable for reasoning tasks], then quantify the explanations uncertainty. 


\subsection{Uncertainty of LLM Explanations}
% Introduce the problem formulation, what is the goal in equations. 
%In this section, we will provide a formal definition of the Natural Language Explanation (NLE) and the uncertainty quantification in the explanations~\cite{tanneru2024quantifying}.
To address these issues, researchers try to understand uncertainties in reasoning processes through the \textit{natural language explanations (NLEs)}~\cite{camburu2018snli,tanneru2024quantifying} as a proxy. As shown in the right part of Figure.~\ref{fig:category}, An NLE is a textual reasoning sequence generated by a language model $ M $, typically derived to justify or explain the answer $ a $ for a given input question $ x^q $. We formally define it as follows:

\begin{definition}[Natural Language Explanation]
Given a model $ M $ and an input prompt $ x^q $, an NLE can be represented as:
\begin{equation}
    M(x^q + x^e) = a + a^e
\end{equation}
where \( x^e \) is an explanation-specific prompt, \( a \) is the model's answer to the query \( x^q \), and \( a^e \) is the generated explanation accompanying the answer.
\end{definition}

The explanation \( a^e \) contains a sequence of reasoning steps, represented as \( a^e = \{s_1, s_2, ..., s_m\} \), which capture the reasoning process or justification for \( a \).

To quantify uncertainty in explanations, we extend Problem~\ref{prob:uq} to consider \( n \) explanations generated for the same query \( x^q \). Each explanation \( a^e_i \) (\( i \in \{1, 2, \dots, n\} \)) corresponds to a set of reasoning steps derived from the same query. Each explanation consists of \( m \) reasoning steps, represented as \( a^e_i = \{s_{i,1}, s_{i,2}, ..., s_{i,m}\} \).
The overall uncertainty across all \( n \) explanations is captured by aggregating reasoning-level uncertainties for each explanation. This can be formally defined as follows:

\begin{problem}[Uncertainty Quantification for LLM Explanations]
\label{prob:uq-exp}
Given an input prompt $ x^q $ and an explanation-specific prompt \( x^e \), the model $ M $ generates a set of answers \( a_i \) to the query \( x^q \), along with accompanying explanations \( a_i^e \). The uncertainty for the query \( x^q \) is then defined as:
    \begin{equation}\label{eq:uqall}
    U_{x^q} = \mathcal{U}\left(\bigcup_{i=1}^n a^e_i\right) = \mathcal{U}\left(\bigcup_{i=1}^n \{s_{i,1}, s_{i,2}, ..., s_{i,m}\}\right)
    \end{equation}
\end{problem}
Here, \( U_{x^q} \) represents the overall uncertainty for the query \( x^q \), and \( \mathcal{U} \) aggregates uncertainties across all reasoning steps from all \( n \) explanations.

Unlike prior methods that focus on token-level or semantic variability~\cite{tanneru2024quantifying}, in this paper, we explicitly models reasoning structures within explanations. By leveraging logical topologies, we aim to capture nuanced uncertainties at both the explanation level and individual reasoning step level.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figs/newhaha.pdf}
    \caption{The illustration of how the topology is constructed. }
    \label{fig:mainfigr}
\end{figure}

\section{Method}
% \textcolor{red}{an image showcases the problem of simple CoT, it is only a chain, not capturing the summarization, analysis, etc., and shows our topology can reflect multiple styles of the thinking path.}

In this section, we propose a novel framework for reasoning uncertainty quantification in large language models (LLMs). To capture the complexity of reasoning paths, the proposed framework consists of two main steps: (1) \textit{Reasoning Topology Elicitation}, which constructs a structured reasoning graph from LLM-generated explanations, and (2) \textit{Topology-enabled Reasoning Quantification}, which leverages the constructed graph to perform uncertainty quantification using measures such as graph edit distance and reasoning path redundancy analysis. These steps work together as a comprehensive framework for analyzing reasoning uncertainty in LLMs.


% \subsection{Reasoning Topology Elicition}
% \begin{figure*}
%     \centering
%     \includegraphics[width=0.97\textwidth]{figs/reasoningTopology.pdf}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure*}
\subsection{Reasoning Topology Elicitation}
The objective of this step is to construct a structured reasoning topology that captures the complexity of reasoning paths generated by large language models (LLMs). Existing approaches~\cite{tanneru2024quantifying} represent reasoning explanation as linear sequences of steps from Chain-of-Thought (CoT) prompting~\cite{wei2023chainofthoughtpromptingelicitsreasoning}. While linear text sequences provide basic interpretability, they fail to capture complex logical transitions required for tasks like comparative reasoning or multi-faceted conclusions. This lack of structural richness limits the ability to analyze and quantify uncertainty in LLM-generated reasoning processes.

To address these limitations, we propose to elicit reasoning topologies from a question $x_q$ to an answer $a$ as a directed graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ is the set of nodes corresponding to knowledge points or intermediate steps, and $\mathcal{E}$ is the set of edges capturing logical dependencies between them. For example, given the query \( x^q \): ``What are the causes of climate change?", the knowledge points \( K^q \) might include sub-questions such as \( k_1 \): ``What is the role of greenhouse gases?", \( k_2 \): ``How does deforestation contribute?", and \( k_3 \): ``What is the impact of industrial activities?". The corresponding answers \( A = \{a_1, a_2, a_3\} \) provide detailed explanations for each sub-question towards the final answer $a_3$. These knowledge-answer pairs are then connected based on their logical dependencies to form the reasoning topology graph \( \mathcal{G}^q \).
This graph-based representation enables a more comprehensive understanding of the reasoning process and provides a foundation for richer uncertainty quantification.


Specifically, the construction of $\mathcal{G}$ consists of three modules: (1) \textit{Knowledge Point Reflection}, where the LLM identifies sub-questions or knowledge points required to address the query; (2) \textit{Self-Answering Module}, where the LLM generates answers for each identified knowledge point; and (3) \textit{Reasoning Topology Construction}, where the knowledge-answer pairs are organized into a directed graph that reflects the overall reasoning path.


\subsubsection{Knowledge Point Reflection Module} The first module, the knowledge point reflection module, involves eliciting sufficient information that can be used to support the conclusion drawing toward the input $x^q$. The input to this module is the input query \( x^q \) along with the prompt template \( T_1 \) to encourage the LLM to reflect `What knowledge basis (or sub-questions) it should know to draw a final conclusion?'. And the output of this module is the set of knowledge points \( K^q \) extracted as a series of sub-questions, i.e., $K^q = \{ k_1, k_2, \dots, k_n \}$. Specifically, we design a prompt template \( T_1 \) to guide the model in reflecting on the sub-questions or knowledge points required for solving \( x^q \):

\begin{tcolorbox}[colback=gray!10, colframe=black, width=8.5cm, arc=1mm, auto outer arc, boxrule=0.05pt, fontupper=\small]
\textbf{Template $T_1$:} Given a question \{$x^q$\}, reflect and generate the knowledge points or sub-questions necessary to solve this query. Ensure that the output is both sufficient and concise.
\end{tcolorbox}


% \begin{tcolorbox}[colback=gray!10, % gray background
%                   colframe=black, % black frame color
%                   width=8.5cm, % Box width
%                   arc=1mm, auto outer arc,
%                   boxrule=0.05pt,
%                   fontupper=\small] % Small font for text

% \textbf{Template $T_1$: Given a question \{$x^q$\}, you should reflect and come up with the knowledge point as a sub-question that you need to solve this query. Two standards: sufficient and concise.}
% \end{tcolorbox}

The model generates a set of knowledge points \( K^q =M(x_q, T_1) = \{k_1, k_2, \dots, k_n\} \), where each \( k_i \) corresponds to a specific sub-question or piece of information identified as necessary to address the query $ x^q $ under the guidance of prompt $ T_1 $. To ensure traceability, we assign unique identifiers to each knowledge point using a tagging function \( f(\cdot) \):
\begin{equation}
    K^q_{\text{tag}} = \{\text{id}_1:k_1, \text{id}_2:k_2, \dots, \text{id}_n:k_n\}.
\end{equation}

In the later sections of this paper, we assume the $k^q$ always carries its identifier while performing computing ($K_q \Leftrightarrow  K^q_{tag}$).

\subsubsection{Self-answering Module} To provide answers for the knowledge points $ K^q = \{k_1, k_2, \dots, k_n\} $ elicited in the previous module, we design a self-answering module to generate precise answers. For each sub-question $ k_i \in K^q $, the model $ M $ generates an answer $ a_i $ using the following prompt $ T_2 $, ensuring coherence and sufficiency in addressing each of the knowledge points:

\begin{tcolorbox}[colback=gray!10, colframe=black, width=8.5cm, arc=1mm, auto outer arc, boxrule=0.05pt, fontupper=\small]
\textbf{Template $T_2$:} Given a sub-question \{$k_i$\}, provide a precise answer that directly addresses the query without further discussion.
\end{tcolorbox}



% \begin{tcolorbox}[colback=gray!10, % gray background
%                   colframe=black, % black frame color
%                   width=8.5cm, % Box width
%                   arc=1mm, auto outer arc,
%                   boxrule=0.05pt,
%                   fontupper=\small] % Small font for text

% \textbf{Template $T_2$: Given a sub-question \{$k_i$\}, provide a precise answer that directly addresses the query without further discussion.} \\

% \begin{equation}
%     A = M\{K^q, T_2\} = \{M\{k_1, T_2\}, M\{k_2, T_2\}, \dots, M\{k_n, T_2\}\}
% \end{equation}
% \end{tcolorbox}

The model generates answers \( A = \{a_1, a_2, \dots, a_n\} \), where each answer \( a_i = M(k_i, T_2) \). So we have:

\begin{equation}
    A = \{a_1, a_2, \dots, a_n\} = \{M\{k_1, T_2\}, M\{k_2, T_2\}, \dots, M\{k_n, T_2\}\}
\end{equation}

This formulation explicitly links each sub-question $ k_i $ to its corresponding answer $ a_i $, ensuring clarity in the relationship between the elicited knowledge points and their responses. The resulting set of knowledge-answer pairs forms the basis for constructing the reasoning topology:
\begin{equation}
    \mathcal{D}_m = \{(k_1, a_1), (k_2, a_2), \dots, (k_n, a_n)\}
\end{equation}

% Here, $ A = \{a_1, a_2, \dots, a_n\} $ represents the set of answers generated for the corresponding knowledge points $ K^q $. Each answer $ a_i $ is generated as:
% \begin{equation}
%     a_i = M\{k_i, T_2\}, \quad \forall k_i \in K^q
% \end{equation}

% Expanding the result in equation (5), we see that $ A $ explicitly combines the answers $ a_i $ for all sub-questions $ k_i $, so we have:
% \[
% A = \{a_1, a_2, \dots, a_n\} = \{M\{k_1, T_2\}, M\{k_2, T_2\}, \dots, M\{k_n, T_2\}\}
% \]

 % By doing so, it actually constructed a set $\mathcal{D}_m$ of meta-elements, i.e., ($k$, $a$) pairs used in later topology structure construction, which will be introduced in the next paragraph.


\subsubsection{Reasoning Topology Construction Module}
% Since the goal of our task is to derive a topology that can reflect the overall reasoning path, a critical step would be to connect the ($k$, $a$) pairs in a certain format, and this connection should be done by the explanation model itself. Based on this requirement, we leverage the few-shot learning ability of LLM and guide it in connecting the basis ($k$, $a$) pairs following its reasoning procedure. By sampling several examples from a demo set $E$ and feeding to the model, the LLM learns to depict the reasoning path in a structured way for this simple task. Then the structure in $\mathcal{D}_m$ is changed by following: 
% \begin{equation}
%     \mathcal{D}_m = \{(k_1, a_1), (k_2, a_2)...(k_n, a_n)\} \Rightarrow \{(\textcolor{blue}{a^j} ,k_1, a_1), (\textcolor{blue}{a^{j-\delta}},k_2, a_2)...(\textcolor{blue}{a^{j+\delta}},k_n, a_n)\}
% \end{equation}
% which shows that a certain $a^j$  is anchored for each of the ($k$, $a$) pairs during this process, thus coming to a closed-form structure. 
To construct the reasoning topology graph \( \mathcal{G}^q = (\mathcal{V}, \mathcal{E}) \), a critical step would be to connect the ($k$, $a$) pairs in a structured format based on their logical dependencies. Since we are quantifying the uncertainty of LLM explanations, this connection should be determined by the model itself to explain.
Therefore, in this module, we leverage the few-shot learning ability of LLMs and guide them in connecting the basis ($k$, $a$) pairs following their reasoning procedure. By sampling $\mathbf{\textit{F}}$ amount of $e^{F}$ as few-shot \underline{e}xamples from a demonstration set $\mathcal{F}$ and feeding them to the model, the LLM learns to depict the reasoning path in a structured way for this task\footnote{Please find details of few-shot learning in Appendix.}:
$\mathcal{\hat{D}}_m = M(\mathcal{D}_m, e^{F})$, the transformation from $\mathcal{D}_m$ to $\mathcal{\hat{D}}_m$ follows:
\begin{align}
    % \mathcal{D}_m &= \{(k_1, a_1), (k_2, a_2), \dots, (k_n, a_n)\}  \Rightarrow  \notag \\
    \mathcal{\hat{D}}_m &= M(\mathcal{D}_m, e^F) =\{(a^{p_1}, k_1, a_1), (a^{p_2}, k_2, a_2), \dots, (a^{p_n}, k_n, a_n)\}
\end{align}
where each $ a^{p_i} $ is an answer node that connects to the corresponding knowledge-answer pair ($ k_i, a_i $).

To ensure that the reasoning path forms a structured yet flexible topology that adapts to the complexity of real-world cases, the specific ordering of $ p_i $ is not predetermined and depends on the actual reasoning structure generated by the model. Then for better illustration, we switch the order in the tuple as below, by applying graph concepts, we have the first two as the `node' positions and the last as the `edge' position: 
\begin{equation}\label{eq:meta}
 (a^{p_1}, k_1, a_1) \Rightarrow  (\underbrace{a^{p_1},a_1}_{{nodes}},\underbrace{k_1}_{edge})
\end{equation}
where the order of two nodes is defined by the reasoning LLM. 

Now we can write a basic reasoning step as:
\begin{equation}\label{eq:stepv1}
\mathcal{\text{Step}}_{ij} = [\text{node}_i, \text{node}_j, \text{edge}_{ij}]    
\end{equation}
where $ \text{node}_i $ is the starting node representing either a question, a sub-question, or an intermediate response, $ \text{node}_j $ is the resulting node from $ \text{node}_i $, and connected by $ \text{edge}_{ij} $, which serves as the reasoning operation or sub-question. 

Specifically, for the initial input query $x_q$, we denote the node as \texttt{nodeRaw}; for the final answer $a$, we denote as  \texttt{nodeResult}. All other steps in the middle are the reasoning process, with a clearly defined structure. 
The final graph structure includes all reasoning steps from query $x^q$ to the final answer $a$ as nodes (\(v_i\)) and their dependencies as edges (\(e_{ij}\)):
The reasoning process from query $q$ to the final answer $a$ can be finalized as a directed graph structure 
\begin{equation}\label{eq:graph} 
\mathcal{G}^q = (\mathcal{V}, \mathcal{E}), 
\end{equation}
where
\begin{equation}\label{eq:node}
    \mathcal{V} = \{\texttt{nodeRaw}, \text{node}_1, \dots, \texttt{nodeResult}\} = \{v_0, v_1,  ...\}
\end{equation}
and the edges are expressed as:
\begin{equation}\label{eq:edge}
    \mathcal{E} = \{e_{ij (1)}, e_{ij (2)} ...\} 
\end{equation}
where $e$ stands for edge and $\{e_{ij} \mid \text{edge}_{ij} : \text{node}_i \to \text{node}_j\}$, $e_{ij}$ represents reasoning operations or dependencies between nodes. (start with index `1' since we assume `0' is the \texttt{nodeRaw}). The graph-based structure captures the full reasoning topology, including branching, dependencies, and multi-step interactions, which allows for better reflection of the relationships between intermediate steps.

Now from a graph concept, the reasoning steps combined with Eq.~\ref{eq:stepv1} are formalized as below:
\begin{equation}\label{eq:step}
S = \{ \text{Step}_{ij} \mid \text{Step}_{ij}= [v_{i}, v_{j}, e_{ij} ] v_{i}, v_{j} \in \mathcal{V}, e_j \in \mathcal{E} \}
\end{equation}
where each triplet represents a logical transition between reasoning steps. Note that for complex reasoning, the final answer does not necessarily rely on all of the reasoning steps. For example, when being asked about "if currently is summer in Australia, what season is it in Canada?", in the reasoning chain, some of the LLM might delve into `what causes the season differences', which is redundant steps in a concise reasoning.


\subsection{Topology-enabled Reasoning Quantification}
Our framework enables multidimensional uncertainty analysis through structural and semantic examination of reasoning topologies. Given a query $x^q$, the model $M$ will be asked $L$ times for explanation elicitation, on which $L$ reasoning topologies $\{\mathcal{G}^q_i\}_{i=1}^L$ will be generated from the previous step. We can measure the consistencies of $\{\mathcal{G}^q_i\}_{i=1}^L$, where both graph structure for reasoning topology, and the embeddings of node and edge sentences for semantics will be considered.

\subsubsection{LLM Reasoning Uncertainty Based on Graph Edit Distance}
We quantify structural uncertainty through comparative analysis of multiple reasoning topologies $\{\mathcal{G}_i\}_{i=1}^L$ generated for the same query $x^q$. 
Traditional graph comparison method~\cite{bai2020learning} focuses on matching `sets' of node embeddings in a broad sense (e.g., across various structured data domains like social networks~\cite{huang2024community} and chemistry~\cite{guzman2024deep}), but we would expect to quantify based on the reasoning steps, which requires a more fine-grained design of distance measure. To tackle the above issue, we first use context-aware embeddings for semantic encoding, and then design a fine-grained, reasoning-step based Graph Edit Distance (GED). 
Specifically, we compare the reasoning structure by jointly considering semantic similarity and structural alignment in a three-step process: 

\vspace{2mm}
\textbf{Step1: Semantic Embedding.} In order to measure semantic meanings of reasoning steps, for each graph $\mathcal{G} \in \{\mathcal{G}^q_i\}_{i=1}^L$ we employ an embedding function $\mathcal{L}$ to encode the representation of nodes and edges in graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$. Since each node $ v \in \mathcal{V}$ and edge $ e \in \mathcal{E}$ serves as a textual description, we can derive contextual embeddings:
\begin{equation}
        \mathbf{h}_v = \mathcal{L}(v), \quad \mathbf{h}_e = \mathcal{L}(e), \quad \forall v \in \mathcal{V}, e \in \mathcal{E}
\end{equation}
In this paper, we use BERT as our embedding function $\mathcal{L}$ but other embeddings could also be used for different domain contexts, e.g,~\cite{rasmy2021med} for medical text. This step encodes the semantics of nodes and edges while preserving the logical structure of the reasoning process. 

% The adjacency structure of $ G $ is then defined as:
% \begin{equation}
% A(i_1, i_2) =
% \begin{cases} 
% \mathcal{L}(e_j), & \text{if } (v_{i_1}, v_{i_2}, e_j) \in S \\
% 0, & \text{otherwise}
% \end{cases}
% \end{equation}
% where an edge exists only if a reasoning step transitions to another.

\vspace{2mm}
\textbf{Step2: Reasoning Topology Distance.} In our setting, we have $L$ reasoning structures $\{\mathcal{G}^q_i\}_{i=1}^L$ generated. To measure the pairwise distance of two reasoning structures $ \mathcal{G}_1 $ and $ \mathcal{G}_2 $, inspired by the concept of graph edit distance~\cite{gao2010survey}, we use the minimum transformations required to align the two graphs to quantify their pairwise distance. 

\textit{\underline{A. Substitution Costs: }} 
For two corresponding nodes in different reasoning topology graphs $ \mathcal{G}_1 = (\mathcal{V}_1, \mathcal{E}_1) $ and $ \mathcal{G}_2 = (\mathcal{V}_2, \mathcal{E}_2) $ we define the semantic substitution cost based on $ v_i \in \mathcal{V}_1 $ and $ e_k \in \mathcal{E}_1 $:
% \begin{equation}
% c_{\text{node}}(v_i, v_j) = 1 - \cos(\mathcal{L}(v_i), \mathcal{L}(v_j))
% \end{equation}
% where cosine similarity measures the semantic alignment of reasoning steps. Similarly, the edge substitution cost is:
% \begin{equation}
% c_{\text{edge}}(e_k, e_m) = 1 - \cos(\mathcal{L}(e_k), \mathcal{L}(e_m))
% \end{equation}
% capturing the similarity between reasoning transitions.
\begin{equation}\label{eq:costsubsi}
c(v_i, v_j, e_k, e_m) =
\begin{cases} 
1 - \cos(\mathbf{h}_i^v, \mathbf{h}_j^v), & \text{node substitute} \\
1 - \cos(\mathbf{h}_k^e, \mathbf{h}_m^e), & \text{edge substitute}
\end{cases}
\end{equation}
where cosine similarity measures the semantic alignment of reasoning steps and elicitation questions, this can capture the difference from direct meaning: either given the similar elicitation (edge sub-question), the sub-response (nodes) are different - there might exist an incorrect answer, or different edges that lead to the similar response - there might be a jumping step.

\textit{\underline{B. Deletion/Insertion Costs: }} 
The cost of deleting a node or an edge is computed based on its average similarity to other nodes or edges within the same reasoning topology $\mathcal{G}$~\cite{serratosa2021redefining, bai2018graph}. For the two reasoning graphs, we compute the deletion cost for a node $ v_i \in \mathcal{V}_1 $ or an edge $ e_k \in \mathcal{E}_1 $ with respect to the graph $ \mathcal{G}_1 $, as follows:

\begin{equation}
c_{\text{del.}}(v_i, v_j, e_k, e_m) =
\begin{cases} 
1 - \frac{1}{|\mathcal{V}_1| - 1} \sum\limits_{\substack{v_j \in \mathcal{V}_1, \\ v_j \neq v_i}} \cos(\mathbf{h}_i^v, \mathbf{h}_j^v), & \text{node delete} , \\[6pt]
1 - \frac{1}{|\mathcal{E}_1| - 1} \sum\limits_{\substack{e_m \in \mathcal{E}_1, \\ e_m \neq e_k}} \cos(\mathbf{h}_k^e, \mathbf{h}_m^e), & \text{edge delete}.
\end{cases}
\end{equation}
where $(\cdot)$ is the same as in Eq.~\ref{eq:costsubsi}, $\sum\limits_{\substack{v_j \in \mathcal{V}_1, v_j \neq v_i}} \cos(\mathbf{h}_i^v, \mathbf{h}_j^v)$ shows the semantic connectivity from node $v_i$ to other nodes in the topology, if the embedding meaning is closer to other nodes, the value is larger, and then the value is normalized by the number of remaining nodes $|\mathcal{V}_1| - 1$, and subtracted from 1 to compute the deletion cost. %\hua{why this function is in this form? where the inspiration comes from?} 
Highly similar nodes or edges in $ \mathcal{G}_1 $ (e.g., redundant sub-questions) will have lower deletion costs, as their removal minimally impacts the reasoning flow. Conversely, unique or critical nodes and edges (e.g., important conclusions or key transitions) will incur higher deletion costs due to their significant role in maintaining reasoning integrity and structural coherence.

% \begin{equation}
% c_{\text{del}}(v_i) = 1 - \frac{1}{|\mathcal{V}_1| - 1} \sum_{v_j \in \mathcal{V}_1, v_j \neq v_i} \cos(\mathcal{L}(v_i), \mathcal{L}(v_j))
% \end{equation}

% Similarly, the cost of edge deletion/insertion is:

% \begin{equation}
% c_{\text{del}}(e_k) = 1 - \frac{1}{|\mathcal{E}_1| - 1} \sum_{e_m \in \mathcal{E}_1, e_m \neq e_k} \cos(\mathcal{L}(e_k), \mathcal{L}(e_m))
% \end{equation}

% ensuring that missing or added reasoning steps are properly penalized.

\vspace{2mm}
\textbf{Step3: Graph Distance for Reasoning Uncertainty.}
Based on the above two steps, we can derive the overall graph edit cost in joint consideration of semantic meaning and topology variance as:
\begin{equation}
\text{GED}(\mathcal{G}_1, \mathcal{G}_2) =
C_{\text{sub.}}(\mathcal{P}) + C_{\text{del.}}(\mathcal{V}_1, \mathcal{E}_1, \mathcal{P})
\end{equation}
where $ \mathcal{P} $ represents the optimal matchings for sub-questions (edges $ \mathcal{P}_e $) and sub-responses (nodes $ \mathcal{P}_v $), computed using the Hungarian algorithm~\cite{mills2007dynamic}. The term $ C_{\text{sub.}}(\mathcal{P}) $ accounts for the total substitution costs over the two graphs, and  $ C_{\text{del.}}(\mathcal{V}_1, \mathcal{E}_1, \mathcal{P}) $ captures the total deletion costs for nodes and edges over the two graphs, details are explained in the Appendix.~\ref{sec:equationdetail1}. So we can calculate the minimal total cost of transformations by finding:
\begin{equation}
    \text{GED}_m(\mathcal{G}_1, \mathcal{G}_2) = \min_{\mathcal{P}} \text{GED}(\mathcal{G}_1, \mathcal{G}_2)
\end{equation}
 A higher GED implies a higher difference in the reasoning phase by considering both embedding and structures.

We use this computed reasoning distance to construct a distance matrix across multiple reasoning structures. Given a set of reasoning topologies $ \{\mathcal{G}_1, \mathcal{G}_2, \dots, \mathcal{G}_n\} $, we compute pairwise distances using the GED-based similarity measure: $d_{ij} = \text{GED}_m(\mathcal{G}_i, \mathcal{G}_j)$,  which then forms the overall distance matrix between $k$ reasoning topologies

\begin{equation}
    D^{\mathcal{G}_n} = [d_{ij}] = [\text{GED}_m(\mathcal{G}_i, \mathcal{G}_j)]_{n\times n}
\end{equation}

where each entry $ d_{ij} $ quantifies the structural and semantic difference between the reasoning processes in $ \mathcal{G}_i $ and $ \mathcal{G}_j $. 
Here, we now can resort to the UQ measure $\mathcal{U}(\cdot)$ to the variance of the distances in $ D^{\mathcal{G}_n} $, which reflects the inconsistency or stability of the model’s reasoning behavior. Combining Eq.~\ref{eq:uqall}, we have the uncertainty score over a query $x^q$ as:
\begin{equation}
    \mathcal{U}_{\text{struct}}(x^q) = \text{Var}(D^{\mathcal{G}_n}),
\end{equation}
where $ D^{\mathcal{G}_n} $ is the pairwise distance matrix over the set of reasoning topologies $ \{\mathcal{G}_1, \mathcal{G}_2, \dots, \mathcal{G}_n\} $. The function $\mathcal{U}(\cdot)$ = $ \text{Var}(\cdot) $ computes the overall variance of all pairwise distances. A higher variance indicates greater inconsistency in the model’s reasoning, suggesting that the LLM generates significantly different structures across multiple responses to the same query.

\subsubsection{LLM Reasoning Redundancy Measure}
% \textcolor{red}{talk about the redundancy in the reasoning graph, and provide some insights here.}
It is known that the LLM's reasoning efficiency varies based on the problem type and model weights~\cite{plaat2024reasoninglargelanguagemodels}, and the reasoning topology provides a good reference to understand the efficiency by analyzing the detailed steps. We find that LLMs do not necessarily rely on all of the nodes from its reasoning topology for the final conclusion drawing, which means that, some of the sub-steps do not contribute to solving a problem and it causes the efficiency decrease. Here, we propose a way of measurement named `Reasoning Redundancy'.  Reflect the reasoning steps in Eq.~\ref{eq:step}: $S = \{ [ v_i, v_j, e_{ij} ] \mid v_i, v_j \in \mathcal{V}, e_{ij} \in \mathcal{E} \}$, we aim to measure the redundancy based on the valid path constructed by the steps.

\begin{definition}[Redundant Node]
A node $ v_k \in \mathcal{V} $ is redundant if it does not contribute to the reasoning path from $\text{nodeRaw}$ to $\text{nodeResult}$. Formally, a redundant node satisfies:
\[
v_k \notin \bigcup_{[v_i, v_j, e_{ij}] \in \mathcal{P}_{\text{valid}}} \{v_i, v_j\},
\]
where $ \mathcal{P}_{\text{valid}} $ represents the set of all valid paths contributing to the final conclusion.
\end{definition}


We have designed detailed criteria for efficient searching as shown in Appendix~\ref{sec:redun}, then we perform the searching for $\mathcal{P}_{valid}$ valid paths and `Redundancy Rate' using traversal algorithm (DFS), then we have the redundancy rate of the reasoning process for $a^e_i$  as:
\begin{equation}\label{eq:redund}
    r_{\text{redun.}} (a^e_i) = \frac{|\mathcal{V}_{\text{redundant}}|}{|\mathcal{V}|}
\end{equation}
where the $ |\mathcal{V}| $ is the total number of nodes in the reasoning topology, and $ |\mathcal{V}_{\text{redundant}}| $ is the number of redundant nodes. 

% \hua{I did not see the relationship of $r_{\text{redun.}} (a^e_i)$ with uncertainty. How uncertainty come out of this? Need an explanation of this value, the higher the better? why?}

% \hua{in the introduction, you mentioned there is entropy based evaluation, where is that in the method section?}


\subsubsection{A Recipe in Practice}
The above-mentioned quantification methods have different properties in potential usage. The \textbf{Topology-UQ} is mainly proposed to evaluate the trustworthiness of LLM responses, make pair-wise reasoning path comparisons, or conduct completeness checks like what is strengthened in the AI4education domain~\cite{ng2024artificial}, where the teaching is focusing on the logic steps rather than final answers. The reasoning \textbf{Redundancy} measure can be used to conduct a reasoning-efficiency check. It helps to identify the non-necessary discussions for a problem and guide the LLM for improvement by probing and marking a more concise but correct path because high-quality answering should not only focus on correctness but also involve solving efficiency. 
\begin{table}[h!]
  
  \label{tab:commands}
  \begin{tabular}{cl}
    \toprule
    Methods & Application \\
    \midrule
    Topology-UQ &  \ding{172}Trustworthy, \ding{173}Path similarity \ding{174}Completeness \\
    Redundancy &   \ding{172}Efficiency check, \ding{173}Guide improvement \\
    \bottomrule
  \end{tabular}
  \caption{The usage of two quantification measures.}
  \vspace{-3mm}
\end{table}


\begin{table*}[h]
    \centering
    \begin{tabular}{lccccccccccccccc}
        \toprule
        \multirow{2}{*}{Methods} & \multicolumn{3}{c}{GPT4o-mini} & \multicolumn{3}{c}{DeepSeek-R1} & \multicolumn{3}{c}{Llama-3.3-70B}  & \multicolumn{3}{c}{Llama3-8b} & \multicolumn{3}{c}{Phi4}\\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
                    & PCC & SRC & KR & PCC  & SRC &  KR & PCC & SRC &  KR & PCC & SRC &  KR & PCC & SRC &  KR \\
        \midrule
        \multicolumn{16}{c}{\textbf{Dataset: \textbf{BoolQ}}} \\
        \midrule
        CoTA        & 0.03 &0.05 &0.03           & \underline{-0.22}& \underline{-0.20} & \underline{-0.13}    &-0.00&0.05&0.03        &-0.07 &-0.06&-0.04           &0.08 &0.07 &0.05      \\
        Embed-UQ    & 0.16 & 0.18 &0.12           &0.56 &0.56 &0.39      &0.09&0.12&0.08         & 0.04&0.06&0.04      &0.02 &0.02 &0.01   \\
        Entail-UQ   &\textbf{-0.12} &\textbf{-0.13 } &\textbf{-0.09}       &0.48 &0.46 &0.32     &\underline{-0.04} &\underline{-0.08}&\underline{-0.05}              &\underline{-0.08}&\underline{-0.08}&\underline{-0.05}       & \textbf{-0.09}&\textbf{-0.09} &\textbf{-0.06}      \\
        NLI-logit   &0.19 &0.18 &0.12         &0.56 &0.56 &0.39     &0.07&0.10&0.07       &0.03&0.03&0.02       & \underline{-0.01} &\underline{-0.01} &\underline{-0.01}  \\
        Ours        &\cellcolor{blue}{\underline{-0.03}} &\cellcolor{blue}{\underline{-0.05}} &\cellcolor{blue}{\underline{-0.03}}         & \cellcolor{blue}{\textbf{-0.29}} & \cellcolor{blue}{\textbf{-0.26}} & \cellcolor{blue}{\textbf{-0.17}}       &\cellcolor{blue}{\textbf{-0.14}} &\cellcolor{blue}{\textbf{-0.10}} &\cellcolor{blue}{\textbf{-0.07}}      &\cellcolor{blue}{\textbf{-0.24}} &\cellcolor{blue}{\textbf{-0.23}} &\cellcolor{blue}{\textbf{-0.15}}      & 0.01&0.01 &0.01       \\
        \midrule
        \multicolumn{16}{c}{\textbf{Dataset: \textbf{GSM8K}}} \\
        \midrule
        CoTA        & \underline{-0.12} &\underline{-0.10} &\underline{-0.07}       &\textbf{-0.40} &\textbf{-0.39} &\textbf{-0.26}   &\underline{-0.13} &\underline{-0.13} &\underline{-0.09}         & -0.04&-0.03&-0.02      & \textbf{-0.14} &\textbf{-0.13} &\textbf{-0.09}        \\
        Embed-UQ    &\underline{-0.12}  &\underline{-0.10}  &\underline{-0.07}         & 0.09& 0.10&0.07     &0.15&0.14&0.10           &0.23&0.23&0.15        & 0.28&0.28 &0.19 \\
        Entail-UQ   &0.14  &0.14 &0.09       & 0.68& 0.66&0.47      &0.15&0.13&0.09              &\underline{-0.08} &\underline{-0.07} &\underline{-0.05}     & \underline{0.07} &\underline{0.07}  &\underline{0.05}    \\
        NLI-logit   & 0.00 &0.01 & 0.01       &0.10 &0.11 &0.07      &0.13&0.12&0.08          & 0.21&0.19&0.13      & 0.29&0.29 &0.20\\
        Ours        &\cellcolor{blue}{\textbf{-0.35}} &\cellcolor{blue}{\textbf{-0.34}} &\cellcolor{blue}{\textbf{-0.23}}        &\cellcolor{blue}{\underline{-0.22}}  &\cellcolor{blue}{\underline{-0.20}} &\cellcolor{blue}{\underline{-0.14}}        &\cellcolor{blue}{\textbf{-0.43}} &\cellcolor{blue}{\textbf{-0.41}} &\cellcolor{blue}{\textbf{-0.28}}             & \cellcolor{blue}{\textbf{-0.14}} &\cellcolor{blue}{\textbf{-0.13}} &\cellcolor{blue}{\textbf{-0.08} }           &0.12 &0.10 &0.06 \\
        % \midrule
        % \multicolumn{16}{c}{\textbf{Dataset: \textbf{GeoQA}}} \\
        % \midrule
        % CoTA & & & & & & & & & & \\
        % Embed-UQ & & & & & & & & & & \\
        % Entail-UQ & & & & & & & & & & \\
        % NLI-logit & & & & & & & & & & \\
        % Ours & & & & & & & & & & \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of our methods with different baselines on various datasets and large language models. The results show that our results consistently outperform the baseline methods. The value reflects the correlation between the uncertainty and the ground truth faithfulness, the more faithful an LLM performs, the less uncertainty it should be, so by the UQ methods, we would expect a negative correlation for a well-performed UQ method.}
    \label{tab:mainresults}
    
\end{table*}


\section{Experimental Study}
In this section, we conducted extensive experiments covering 5 LLMs: GPT4o-mini, DeepSeek-R1 (distilled version on llama3-70b), Phi4, Llama3-8b, Llama3-70b (v3.3), 3 challenging datasets and using a total of 5 methods. We use the proposed method for topology-based LLM explanation analysis, then we try to answer the following research questions.
\begin{itemize}
    \item RQ11: Can the proposed method reveal the actual uncertainty in LLM's reasoning? 
    % \textcolor{red}{finding2: our method outperforms others in the correlation of faithfulness, others show a slight correlation, but ours better reveals the actual faithfulness.}
    \item RQ2: How do LLMs perform in the proposed redundancy measure? 
     \item RQ3: From the proposed topology reasoning elicitation, do LLMs share certain patterns commonly?
     % \item Other discussion (Appendix): how much does LLM's ability of instruction influence our method, and the parameter set of total response amount.
    
\end{itemize}



\subsection{Experiment Settings}
In this section, we introduce the experiment settings including the dataset adopted for analysis and baselines used for comparison. Besides, we also introduce the evaluation metrics for measuring the UQ methods' performance on \texttt{Natural Language Explanation} tasks.

\subsubsection{Dataset}
In this study, to align with the research community~\cite{mirzadeh2024gsm, toshniwal2024openmathinstruct, zhang2024careful}, we utilized widely adopted datasets, including GSM8k~\cite{gsm8k} and BoolQ~\cite{boolq}, which require complex reasoning rather than simple question-answering. These benchmarks assess LLM's ability to perform multi-step inference and logical reasoning. Besides, we also develop a new dataset, GeoQA, especially for condition-based reasoning tasks. We will explain the details of the GeoQA dataset in Appendix~\ref{sec:geoqa} and provide brief introduction of all datasets in Appendix~\ref{sec:data}


\subsubsection{Baselines} 
To the best of our knowledge, there are only few works focusing on the uncertainty quantification of NLE, thus, we not only included the existing method (1) Chain-of-Thought Agreement (CoTA)~\cite{tanneru2024quantifying}, but also analyze (2) Embedding distance-based UQ for NLE (Embed-UQ), (3) Entailment probability-based (Entail-UQ), and (4) NLI-logit based UQ, as our extra baselines to understand their interpretability of LLM explanations. We provide a brief introduction here, and for detailed explanation of baselines is in Appendix.~\ref{sec:baseline1}. 
 \begin{itemize}
    \item CoTA: This is designed to quantify uncertainty in LLM explanations by measuring the agreement between two Chain-of-Thought explanations. It assesses whether intermediate reasoning steps in one CoT explanation are aligned with corresponding steps in another by calculating their entailment scores.
    \item Embed-UQ: It measures the uncertainty of NLE by embedding distances. Given a query $ x^q $, collected $ k $ responses and, their associated explanations $ \{a^e_1, a^e_2, \dots, a^e_k\} $. Each explanation $ a^e_i $ is embedded into a high-dimensional space, and the pairwise distances between embeddings are computed. %The variance of these pairwise distances quantifies the uncertainty of the explanations.

    \item Entail-UQ: It extends Embed-UQ by replacing the embedding-based distance metric with an entailment-based similarity measure. Given explanations such as $a^e_k$, we compute an entailment similarity matrix, where the similarity between two explanations is calculated by entailment models~\footnote{off-the-shelf DeBERTa-large model}. %The uncertainty is quantified as the variance of $ 1 - \text{similarity} $ values, capturing the logical consistency across explanations. 

    % \item NLI-logit: It passes the input to the NLI model (microsoft-deberta-large-mnli) and extracts the model's embedding logit to calculate the cosine distances based on the explanations.

\end{itemize}
\subsubsection{Evaluation and Metrics}
To evaluate the performance of uncertainty quantification methods in LLM explanation tasks, we follow the standard practice that compares uncertainty results with actual faithfulness~\cite{tanneru2024quantifying}. The ground truth faithfulness score reveals how much the model relies on its complete reasoning process, which is calculated through a strategy named `Early Answering' as proposed by~\cite{lanham2023measuring}, we provide details on how the faithfulness score is derived in Appendix~\ref{sec:faith}. \textbf{Ideally, a UQ method is good if, for a higher faithful set, it generates lower uncertainty, and vise versa}~\cite{tanneru2024quantifying}. Hence, we employ three robust statistical metrics to quantify the correlation between the derived uncertainty and faithfulness. First, we use the commonly adopted metric - Pearson Correlation Coefficient (PCC), which is to measure the linear correlation between two variables. And given the relative small amount of each bootstrap sample, we employ two extra metrics Spearman Rank Correlation (SRC) and Kendall Rank Correlation (KR), the calculation of metrics is also in Appendix.

For fair evaluation and to avoid bias in single answers, we conduct bootstrap for a given dataset $D_{test}$ and measure the correlation in each sub-set $D'$ level between uncertainty with the same level of faithfulness score. The sub-set is cut as 20 questions with 10 responses for each question = 200 $\cdot a^e$ and bootstrap is conducted 1000 times on each dataset.

\begin{figure*}
    \centering
    \includegraphics[width=0.88\linewidth]{figs/patterns.pdf}
    \caption{The examples of three types of reasoning patterns. As shown in the image, the first type directly reflects the correct answer in its first reasoning step, and now actually performing the reasoning, the second type is like humans, try to recall the knowledge points, and narrow down the range, and find the possible answer in a thought chain, while the third one is a combination of two, it has the steps to ask for characteristics, but it also try to reflect if it has seen this question before at the edge1, which is a direct attempt to use answer to match the question, the last step is to confirm if the memory of answer matches the requirement in the question.}
    \label{fig:patterns}
\end{figure*}


\subsection{Quantitative Evaluation (RQ1)} 
In order to understand how our proposed method works in the reasoning uncertainty measure tasks for LLM explanations, we perform experiments on GSM8K, BoolQ, and GeoQa datasets. Due to the page limit, GSM8K, and BoolQ are shown in the Table.~\ref{tab:mainresults}, our method reveals a stronger negative correlation between the derived UQ results and the groundtruth faithfulness across different statistic metrics, the results on GPT4o-mini, Llama3-70b, and DeepSeek-R1 are more convincing because they have a more stable performance on the Topology elicitation task as in Figure.~\ref{fig:topologytask}, which, is a key step for the proposed UQ method, the performance on Phi4 and Llama3-8b not promising as ranked in the last two positions in Appendix Figure.~\ref{fig:topologytask}. This research question result shows that our method is effective in revealing the LLM's real faithfulness, yet it is more suitable for LLMs with good instruct-following abilities. 







\subsection{Redundancy Measure of LLMs (RQ2)} 
Benefit from the topology structure, we are able to extract the reasoning path $\mathcal{P}_{\text{valid}}$ that successfully connects from \texttt{nodeRaw} to \texttt{nodeResult}. Then we can effectively compute the node that is not contributing to the final answer, and this serves as a sign of redundancy in the LLM's reasoning process. Following the Eq.~\ref{eq:redund}, we analyze the redundancy rate for each of the LLMs including GPT-4o-mini, Llama3-8b, Phi4, DeepSeek-R1(distilled), and Llama2-70b on both the node and edge redundancy. We find that, surprisingly, the GPT-4o-mini shows a significantly high redundancy rate in both the nodes and edges, it might reveal the high accuracy of the model comes from a border searching space (or generating length) when conducting reasoning and proposing solutions. However, this also reflects there would be a great potential to optimize the reasoning process for the GPT-wise models. Comparatively, the DeepSeek (distilled llama version) is relatively low, which might indicate the model's training was conducted with a special design to encourage the `valid' reasoning which eventually contributes to the final result. We show more results in the appendix for reference. 
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\linewidth]{figs/redundancy.pdf}
    \caption{The quantification of the LLM's reasoning redundancy on three datasets. }
    \label{fig:redundancy}
    \vspace{-5mm}
\end{figure}



\subsection{Findings on the Reasoning Patterns from LLMs (RQ3)} Based on the topology representations, we conducted further analysis from both -structural and -content sides. First, we conducted clustering on the extracted reasoning topologies from LLMs (for convenience, we did not consider different models respectively), and we found there are two dominant reasoning topology patterns: `single chain' and `node$\rightarrow$branches$\rightarrow$discuss$\rightarrow$merge' to a conclusion. Then, on the content side, we found three answering modes from the LLMs: \ding{172} Completion based: the model has seen the data in the training phase, and completes it as a `cloze question'.
\ding{173} Forward reasoning (actual reasoning): The model is actually performing reasoning based on the analysis procedure like the human beings, given a question, it first recalls relevant concepts and knowledge that contain the necessary information. Then, through a step-by-step narrowing-down process, it refines its understanding, gradually converging on the most precise answer. \ding{174} Combination of two $+$ verify: the model uses two methods and double-checks to verify the answer. Three modes are shown in Figure.~\ref{fig:patterns}. We hope this could serve as an inspiration for the potential future study.


% \textcolor{red}{finding1: the ability of instruction following, varies, and the GPT, Deepseek, performs the best, while others vary.   find5: the correlation between faithfulness and correctness (in appendix).}

% Normalized Mutual Information (NMI)
% Pearson Correlation Coefficient (PCC-r)
% Spearman Rank Correlation (SRC)





% \begin{verbatim}
%   \title[short title]{full title}
% \end{verbatim}



\section{Conclusion}
In this paper, we highlight the critical importance of understanding uncertainty in LLM explanations by revealing the reasoning consistency. By introducing a novel framework grounded in a reasoning topology perspective, the paper provides a structured method to decompose explanations into knowledge and reasoning dimensions. This allows for precise quantification of uncertainty, assessment of knowledge redundancy, and deeper insights into the model’s reasoning structure. Then, the paper based on the proposed formal structural construction, it propose a graph-edit distance based uncertainty measure, empirical study shows a better performance in revealing the true faithfulness of the natural language models. After that, a redundancy-based method is introduced to quantify the redundancy of the LLMs. The approaches not only enhance interpretability but also serve as a guide for improving robustness and faithfulness in LLM-generated explanations. 
% \section{CCS Concepts and User-Defined Keywords}

% Two elements of the ``acmart'' document class provide powerful
% taxonomic tools for you to help readers find your work in an online
% search.

% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.

% User-defined keywords are a comma-separated list of words and phrases
% of the authors' choosing, providing a more flexible way of describing
% the research being presented.

% CCS concepts and user-defined keywords are required for for all
% articles over two pages in length, and are optional for one- and
% two-page articles (or abstracts).

% \section{Sectioning Commands}

% Your work should use standard \LaTeX\ sectioning commands:
% \verb|\section|, \verb|\subsection|, \verb|\subsubsection|,
% \verb|\paragraph|, and \verb|\subparagraph|. The sectioning levels up to
% \verb|\subsusection| should be numbered; do not remove the numbering
% from the commands.

% Simulating a sectioning command by setting the first word or words of
% a paragraph in boldface or italicized text is {\bfseries not allowed.}

% Below are examples of sectioning commands.

% \subsection{Subsection}
% \label{sec:subsection}

% This is a subsection.

% \subsubsection{Subsubsection}
% \label{sec:subsubsection}

% This is a subsubsection.

% \paragraph{Paragraph}

% This is a paragraph.

% \subparagraph{Subparagraph}

% This is a subparagraph.

% \section{Tables}

% The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
% package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
% high-quality tables.

% Table captions are placed {\itshape above} the table.

% Because tables cannot be split across pages, the best placement for
% them is typically the top of the page nearest their initial cite.  To
% ensure this proper ``floating'' placement of tables, use the
% environment \textbf{table} to enclose the table's contents and the
% table caption.  The contents of the table itself must go in the
% \textbf{tabular} environment, to be aligned properly in rows and
% columns, with the desired horizontal and vertical rules.  Again,
% detailed instructions on \textbf{tabular} material are found in the
% \textit{\LaTeX\ User's Guide}.

% Immediately following this sentence is the point at which
% Table~\ref{tab:freq} is included in the input file; compare the
% placement of the table here with the table in the printed output of
% this document.


% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

\clearpage
% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|appendix|'' command:
% \begin{verbatim}
%   \appendix
% \end{verbatim}
% and note that in the appendix, sections are lettered, not
% numbered. This document has two appendices, demonstrating the section
% and subsection identification method.

% \section{Multi-language papers}

% Papers may be written in languages other than English or include
% titles, subtitles, keywords and abstracts in different languages (as a
% rule, a paper in a language other than English should include an
% English title and an English abstract).  Use \verb|language=...| for
% every language used in the paper.  The last language indicated is the
% main language of the paper.  For example, a French paper with
% additional titles and abstracts in English and German may start with
% the following command
% \begin{verbatim}
% \documentclass[sigconf, language=english, language=german,
%                language=french]{acmart}
% \end{verbatim}

% The title, subtitle, keywords and abstract will be typeset in the main
% language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
% begin title, subtitle and keywords, can be used to set these elements
% in the other languages.  The environment \verb|translatedabstract| is
% used to set the translation of the abstract.  These commands and
% environment have a mandatory first argument: the language of the
% second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
% of their usage.

% \section{SIGCHI Extended Abstracts}

% The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
% not in Word) produces a landscape-orientation formatted article, with
% a wide left margin. Three environments are available for use with the
% ``\verb|sigchi-a|'' template style, and produce formatted output in
% the margin:
% \begin{description}
% \item[\texttt{sidebar}:]  Place formatted text in the margin.
% \item[\texttt{marginfigure}:] Place a figure in the margin.
% \item[\texttt{margintable}:] Place a table in the margin.
% \end{description}







%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

\clearpage


\section{Notation Table}
In this section, we provide a comprehensive notation table as shown in the Table.~\ref{tab:notation}.
\begin{table}[h!]
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{cc}
    \toprule
    Notation & Explanation \\
    \midrule
    $U_{x}$ &  Uncertainty function \\
    $M$ & Large language model \\
    $x$ & Input(query or prompt) \\
    $\mathcal{U}$& Aggregation\\
    $x^q$ & Input prompt \\
    $x^e$ & Explanation-specific prompt \\
    $a$ & LLMs answer \\
    $a^e$ & Generated explanation accompanying the answer $a$ \\
    $s_i$ & Reasoning step \\
    $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ & Directed reasoning graph \\
    $\mathcal{V}$ & Set of nodes\\
    $|\mathcal{V}|$ &Number of nodes \\
    $\mathcal{E}$ & Set of edges \\
    $|\mathcal{E}|$ & Number of edges \\
    $e_{ij}$ & Edge from node $i$ to node $j$ \\
    $K^q$ & Knowledge points \\
    $T$ & Prompt template \\
    $\mathcal{D}_m$ & Set of knowledge-answer pairs for reasoning topology\\
    $(k_i,a_i)$ & Knowledge-answer pairs\\
    $e^F$ & Sampling $F$ amount of edges \\
    $\mathcal{F}$ & Demonstration set \\
    % $\hat{\mathcal{D}}_m$ & \textcolor{green}{I'm not sure what the $m$ represents?} \\
    $\mathcal{L}(\cdot)$ & Embedding function \\
    $\mathbf{h}$ & Contextual embedding \\
    $cos(\cdot)$ &Cosine similarity \\
    $\mathcal{P}$ &Optimal matching for sub-questions \\
    $d_{ij}$ & Graph edit distance between $\mathcal{G}_i$ and $\mathcal{G}_i$ \\
    $r_{redun.}$ & Redundancy rate \\
    \bottomrule
  \end{tabular}
  }
  \caption{The notations and explanations in this paper.}
  \label{tab:notation}
\end{table}


\section{Details of the GeoQA Dataset}\label{sec:geoqa}
The GeoQA dataset is designed to evaluate the reasoning capabilities of large language models (LLMs) on conditional geographical questions, emphasizing the comparative reasoning topology of their responses. By anchoring specific knowledge within conditional constraints and requiring models to infer results or solutions, GeoQA enables an in-depth analysis of the reasoning paths taken by LLMs. The dataset spans 20 categories, covering diverse geographical topics such as climate, biome, tectonic plates, continental drift, altitude, sea level, desertification, urbanization, demography, population density, ocean currents, river basin, watershed, mountain range, volcano, earthquake, glacier, permafrost, and monsoon. Each question is crafted to test multi-step reasoning, integration of domain-specific knowledge, and the ability to navigate complex cause-effect relationships, making GeoQA a unique and challenging benchmark for geographical reasoning.


\section{Empirical Study on the Choice of Number for Generations}
It is a basis setup that we need to query LLM $\mathcal{M}$ with a query $x^q$ for $k$ times and collect a set of explanations to perform the NLE uncertainty measure. We have conducted a survey on related literature and found there is no standard definition or setting, so we conducted a preliminary study on the number of responses and tried to find the most suitable one (since the larger the response is, the more computationally expensive it will be for later evaluation).



\section{Details of Baseline methods}\label{sec:baseline1}

% \begin{itemize}
%     \item CoTA: This is designed to quantify uncertainty in LLM explanations by measuring the agreement between two Chain-of-Thought explanations. It assesses whether intermediate reasoning steps in one CoT explanation are aligned with corresponding steps in another by calculating their entailment scores.
%     \item Embed-UQ: It measures the uncertainty of NLE by embedding distances. Given a query $ x^q $, collected $ k $ responses and, their associated explanations $ \{a^e_1, a^e_2, \dots, a^e_k\} $. Each explanation $ a^e_i $ is embedded into a high-dimensional space, and the pairwise distances between embeddings are computed. The variance of these pairwise distances quantifies the uncertainty of the explanations.

%     \item Entail-UQ: It extends Embed-UQ by replacing the embedding-based distance metric with an entailment-based similarity measure. Given explanations such as $a^e_k$, we compute an entailment similarity matrix, where the similarity between two explanations is calculated by entailment models~\footnote{off-the-shelf DeBERTa-large model}. The uncertainty is quantified as the variance of $ 1 - \text{similarity} $ values, capturing the logical consistency across explanations. 

%     \item NLI-logit: It passes the input to the NLI model (microsoft-deberta-large-mnli) and extracts the model's embedding logit to calculate the cosine distances based on the explanations.

% \end{itemize}
\subsubsection{CoTA.} Chain-of-Thought Agreement (CoTA) evaluates the agreement between two Chain-of-Thought (CoT) explanations generated for the same query. Each CoT explanation consists of a sequence of reasoning steps, denoted as:
\[
CoT_a = \{s_{a1}, s_{a2}, \dots, s_{aN_a}\}, \quad CoT_b = \{s_{b1}, s_{b2}, \dots, s_{bN_b}\}.
\]
The CoTA metric quantifies agreement between the two CoT explanations by calculating the maximum semantic alignment for each step in $ CoT_a $ with steps in $ CoT_b $, and vice versa. Formally, CoTA is defined as:
\begin{equation}
\begin{aligned}
\text{CoTA}(CoT_a, CoT_b) = &\ \frac{1}{N_a + N_b} \bigg( \sum_{i=1}^{N_a} \max_{j=1,\dots,N_b} E(s_{ai}, s_{bj}) \\
&\ + \sum_{j=1}^{N_b} \max_{i=1,\dots,N_a} E(s_{bj}, s_{ai}) \bigg)
\end{aligned}
\end{equation}


where $ N_a $ and $ N_b $ are the number of steps in $ CoT_a $ and $ CoT_b $, respectively.

The entailment function $ E(s_i, s_j) $ measures the semantic agreement between two reasoning steps using a Natural Language Inference (NLI) model. It is defined as:
\[
E(s_i, s_j) = 
\begin{cases} 
1, & \text{if } s_i \text{ entails } s_j, \\
0, & \text{otherwise.}
\end{cases}
\]

The entailment model employs pre-trained NLI models, such as DeBERTa~\cite{he2020deberta}, fine-tuned for evaluating entailment relationships between statements. This binary scoring avoids dependency on confidence calibration, and we take the threshold as 0.7 to provide the binary cut.

\begin{table*}[t!]
\centering
\caption{Examples of GeoQA dataset.}
\label{tab:Error Type}
% \footnotesize
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Question Type}           & \textbf{Type Explain}  & \textbf{Example Questions} & \textbf{Analysis}   \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\\\Glacier\\\end{tabular}} 
& \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\These questions explore \\glacial movement, erosion,\\ and the impact of climate \\change on ice dynamics.\newline\end{tabular}} 
& \begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{172}} : If glaciers carve striations into bedrock,\\ what do these scratches indicate about past movements?\\\quad\end{tabular}       
& \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\Striations indicate past movement direction, \\requiring cause-and-effect analysis, \\while glacier mass loss demands understanding \\the imbalance between accumulation and ablation.\\\end{tabular}} 
\\ \cline{3-3} & & \begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{173}} : If a glacier loses mass due to melting and sublimation\\ exceeding accumulation, what process is occurring?\\\\\end{tabular} & \\ 
\hline

                              

\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\\\Earthquake\\\quad\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\These questions focus on \\seismic wave behavior,\\ fault activity, and \\earthquake detection.\\\quad\end{tabular}} & 
\begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{172}} : If seismic waves are recorded by a network \\of seismographs, which method is used to pinpoint\\ the origin of the disturbance?\\\quad\end{tabular}       & 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\Triangulation requires reasoning through \\wave arrival times, while P-wave detection relies on \\comparing wave speeds and impact \\to explain early warning systems.\\\quad\end{tabular}} \\ 
\cline{3-3} & & \begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{173}} : If earthquake early warning systems rely on \\detecting the initial P-waves, which characteristic\\ of these waves makes this feasible?\\\quad\end{tabular}    & \\
\hline

\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\\\permafrost\\\quad\end{tabular}}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\These questions examine \\permafrost thawing, \\climate feedback loops, and \\seasonal variations.\\\end{tabular}} & 
\begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{172}} : If permafrost thaws due to rising temperatures, \\releasing trapped methane, \\what global issue does this exacerbate?\\\quad\end{tabular}       & 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\Thawing permafrost and methane release involve \\feedback loops, while active layer variations \\require analyzing environmental factors\\ like temperature and insulation.\\\quad\end{tabular}} \\ 
\cline{3-3} &  &
\begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{173}} : If the active layer above permafrost varies in\\ thickness seasonally, what factors influence its depth?\\\quad\end{tabular}  &  \\ 
\hline

\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\\\Monsoon\\\quad\end{tabular}}  & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\These questions cover \\seasonal wind shifts,\\ monsoon patterns, and\\ storm formation.\\\end{tabular}} & 
\begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{172}} : If the East Asian monsoon affects countries\\ like China and Japan,\\ what two seasons does it primarily influence?\\\quad\end{tabular}       & 
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\The East Asian monsoon’s impact on seasons \\involves reasoning about wind shifts, \\while monsoon depressions require \\linking low-pressure systems to storm formation.\\\quad\end{tabular}} \\ 
\cline{3-3} &   & \begin{tabular}[c]{@{}c@{}}\\\textcolor{black}{\ding{173}} : If monsoon depressions form in the Bay of Bengal, \\ what weather events might they trigger upon landfall?\\\quad\end{tabular}  & \\ 
\hline

\end{tabular}}
\end{table*}

\subsubsection{Embed-UQ.} Embed-UQ measures uncertainty by embedding natural language explanations into a semantic space and computing the variance of pairwise distances. Given a query $ x^q $, let $ \{a^e_1, a^e_2, \dots, a^e_k\} $ represent $ k $ explanations generated by the model. Using an embedding function $ \mathcal{L}(\cdot) $, each explanation $ a^e_i $ is mapped to a high-dimensional embedding:
\[
\mathbf{h}_i = \mathcal{L}(a^e_i).
\]
The pairwise distances between embeddings are computed as:
\[
d_{ij} = \|\mathbf{h}_i - \mathbf{h}_j\|,
\]
where $ d_{ij} $ represents the distance between explanations $ a^e_i $ and $ a^e_j $. The uncertainty is then quantified as the variance of the distance matrix $ D $:
\[
\mathcal{U}(x^q) = \text{Var}(D),
\]
where $ D = \{d_{ij} \mid 1 \leq i, j \leq k\} $.

\subsubsection{Entail-UQ.}
Entail-UQ modifies the distance computation in Embed-UQ by using an entailment-based similarity measure instead of embedding distances. Given the same set of explanations $ \{a^e_1, a^e_2, \dots, a^e_k\} $, an entailment model computes the similarity between two explanations $ a^e_i $ and $ a^e_j $ as:
\[
s_{ij} = E(a^e_i, a^e_j),
\]
where $ E(\cdot, \cdot) $ is the entailment function that outputs a similarity score between 0 and 1. The dissimilarity is then defined as $ 1 - s_{ij} $, and the uncertainty is computed as the variance of the dissimilarity matrix $ S $:
\[
\mathcal{U}(x^q) = \text{Var}(1 - S),
\]
where $ S = \{s_{ij} \mid 1 \leq i, j \leq k\} $. Similarly to CoTA, in our implementation, we adopt DeBERTa~\cite{he2020deberta} as the model to generate entailment logit and pass through a layer of softmax to transform it into probabilities.


\begin{figure}
    \centering
    \includegraphics[width=0.98\linewidth]{figs/success_rate_comparison_centered_groups.png}
    \caption{The model's success rate to generate legitimate reasoning topology. It is calculated by the percentage of LLMs successfully generating the reasoning path from the nodeRaw to nodeResult following the few-shot prompt. It can be witnessed that, generally the GeoQA is a harder one to generate due to the conditional question types, which are rarely seen in the LLM training tasks.}
    \label{fig:topologytask}
\end{figure}



\section{Research Methods}

\subsection{Part One}\label{sec:equationdetail1}
The final full version of the graph edit distance is as below:

\begin{equation}
\text{GED}(\mathcal{G}_1, \mathcal{G}_2) =
C_{\text{sub.}}(\mathcal{P}) + C_{\text{del.}}(\mathcal{V}_1, \mathcal{E}_1, \mathcal{P})
\end{equation}

where $ \mathcal{P} $ represents the optimal matchings for nodes ($ \mathcal{P}_v $) and edges ($ \mathcal{P}_e $), computed using an algorithm such as the Hungarian algorithm~\cite{hamuda2018improved}. The term $ C_{\text{sub.}}(\mathcal{P}) $ accounts for substitution costs, defined as:

\begin{equation}
C_{\text{sub.}}(\mathcal{P}) = \sum_{(v_i, v_j) \in \mathcal{P}_v} c(v_i, v_j) + \sum_{(e_k, e_m) \in \mathcal{P}_e} c(e_k, e_m),
\end{equation}

where $ c(v_i, v_j) $ and $ c(e_k, e_m) $ represent the node and edge substitution costs, respectively. The term $ C_{\text{del.}}(\mathcal{V}_1, \mathcal{E}_1, \mathcal{P}) $ captures the deletion costs for nodes and edges in $ \mathcal{G}_1 $ that are not matched, given by:

\begin{equation}
C_{\text{del.}}(\mathcal{V}_1, \mathcal{E}_1, \mathcal{P}) = 
\sum_{v_i \in \mathcal{V}_1 \setminus \mathcal{P}_v} c_{\text{del.}}(v_i) + 
\sum_{e_k \in \mathcal{E}_1 \setminus \mathcal{P}_e} c_{\text{del.}}(e_k).
\end{equation}

Here, $ c_{\text{del.}}(v_i) $ and $ c_{\text{del.}}(e_k) $ denote the deletion costs of unmatched nodes and edges, respectively. This formulation quantifies the total cost required to align the two reasoning structures by summing the substitution costs of matched components and the deletion costs of unmatched ones.

\subsection{Algorithm for Detection of Redundant Nodes and Dead Branches}\label{sec:redun}
To identify redundant nodes and branches of dead nodes in the reasoning topology $ S = \{ [v_i, v_j, e_{ij}] \mid v_i, v_j \in \mathcal{V}, e_{ij} \in \mathcal{E} \} $, we first define the outgoing edges of a node $ v_k $ as:
$\text{Out}(v_k) = \{ v_j \mid [v_k, v_j, e_{kj}] \in S \}$
Then A node $ v_k $ is considered redundant if it has no outgoing edges and is not the final node:
\begin{equation}
\text{Out}(v_k) = \emptyset \quad \text{and} \quad v_k \neq \text{NodeResult}.    
\end{equation}
Then, we compute the set of valid paths, $ \mathcal{P}_{\text{valid}} $, connecting $\text{NodeRaw}$ to $\text{NodeResult}$ using DFS. A node $ v_k \in \mathcal{V} $ is redundant if it does not appear in any valid path:
\begin{equation}
v_k \notin \bigcup_{[v_i, v_j, e_{ij}] \in \mathcal{P}_{\text{valid}}} \{v_i, v_j\}.    
\end{equation}
In order to more efficiently detect branches of dead nodes (not contributing to the whole reasoning path), let $ v_k $ and its parent $ v_p $ satisfy:
\[
\text{Out}(v_p) = \{ v_k \}, \quad \text{and} \quad v_p, v_k \notin \bigcup_{[v_i, v_j, e_{ij}] \in \mathcal{P}_{\text{valid}}} \{v_i, v_j\}.
\]
In this case, $ v_p $ and $ v_k $ form a dead branch, as neither contributes to any reasoning path leading to $\text{NodeResult}$. And finally, the redundancy rate is computed as:
\[
redun.(a^e_i) = \frac{|\mathcal{V}_{\text{redundant}}|}{|\mathcal{V}|},
\].
It allows to systematically identify nodes and branches that do not contribute to the reasoning process, providing insights into the inefficiencies in the model's reasoning topology and we can analyze to understand where we can improve in the model training or fine-tuning process.


\section{Experimental Details}\label{sec:faith}
\subsection{Details of the dataset}\label{sec:data}
\begin{itemize}
    \item \textbf{GSM8K~\cite{gsm8k}}: This dataset contains 8,000 high-quality math word problems, designed to evaluate LLMs' ability to perform arithmetic reasoning. It is a standard benchmark for testing the numerical and reasoning capabilities of LLMs.
    % \item \textbf{SVAMP~\cite{svamp}}: SVAMP is created to test the robustness of LLMs in solving math word problems under distribution shifts. Focusing on reasoning adaptability and sensitivity to semantic variations. This dataset challenges LLMs' ability to generalize beyond memorized patterns.
    % \item \textbf{ASDiv~\cite{asdiv}}: The ASDiv is a collection of diverse math word problems, covering various types of arithmetic reasoning tasks such as addition, subtraction, multiplication, and division. Unlike GSM8K, ASDiv, it emphasizes linguistic diversity in its problem statements.
    \item \textbf{BoolQ~\cite{boolq}}: BoolQ is a yes/no question-answering dataset derived from naturalistic information-seeking questions. The dataset is used to evaluate the ability of LLMs to reason logically over textual evidence and produce accurate binary answers.
    \item \textbf{GeoQA}: GeoQA is a self-constructed dataset designed to evaluate the reasoning capabilities of LLMs in conditional questions. With 20 categories, including climate and tectonic processes, its tasks require inference of specific results from given conditions. GeoQA emphasizes multi-step reasoning and domain-specific integration, making it a challenging benchmark (details in the Appendix~\ref{sec:geoqa}).

    % \item \textbf{GeoQA}: GeoQA is a self-constructed dataset from the logical reasoning question in a geographical domain. It is designed to evaluate the reasoning capabilities of LLMs across 20 diverse categories, including climate, tectonic processes, and population dynamics. The dataset challenges LLMs with conditional reasoning tasks, requiring inference of specific results based on given conditions. For example, questions like "If glacial periods alternated with interglacial periods during the Pleistocene, what cycles explain these climate changes?" test the ability to understand and apply concepts such as Milankovitch Cycles. GeoQA emphasizes multi-step reasoning and domain-specific knowledge integration, making it a challenging benchmark for geographical reasoning.
\end{itemize}
\subsection{The calculation of the faithfulness core}
In our experiments, we utilize a strategy called \textbf{Early Answering} to measure the faithfulness of the reasoning paths \( a^e = \{s_1, s_2, \dots, s_n\} \), which are generated by the LLM for a given query \( x^q \). This strategy involves truncating the reasoning steps \( a^e \) progressively and prompting the model to answer the query \( x^q \) combined with the partial reasoning path \( \{s_1, s_2, \dots, s_k\} \), where \( k \in \{1, 2, \dots, n\} \). For example, instead of providing the entire reasoning \( x^q + s_1 + s_2 + \dots + s_n \), the model is prompted to answer using only \( x^q + s_1 \), \( x^q + s_1 + s_2 \), and so on, until the full reasoning path is reached.

The Early Answering process evaluates how often the LLM’s responses, derived from the partial reasoning path \( \{s_1, s_2, \dots, s_k\} \), match the final answer \( a \) generated using the complete reasoning \( x^q + s_1 + s_2 + \dots + s_n \). This evaluation reflects the faithfulness of the reasoning path: if the model consistently reaches the correct answer \( a \) with partial reasoning, it may indicate that the reasoning steps are unnecessary (post-hoc). Conversely, a lower match rate suggests that the intermediate reasoning steps are essential for arriving at the correct final answer, thereby indicating greater faithfulness.

We quantify the faithfulness $V_{faith}$ using the following equation:
\begin{equation}
V_{\text{faith}} = 1-  \underbrace{\frac{1}{n} \sum_{k=1}^n \mathbb{I}\left(f(x^q + \{s_1, s_2, \dots, s_k\}) = a\right)}_{\text{un-faithfulness}}
\end{equation}
where \( n \) represents the total number of reasoning steps in \( a^e \), \( f(x^q + \{s_1, s_2, \dots, s_k\}) \) is the LLM’s output when prompted with the query \( x^q \) and the partial reasoning path \( \{s_1, s_2, \dots, s_k\} \), \( a \) denotes the final answer generated using the complete reasoning path, and \( \mathbb{I}(\cdot) \) is an indicator function that equals 1 if the condition is true (i.e., the partial reasoning output matches the final answer), and 0 otherwise. The $V_{\text{faith}}$ is calculated by $1- \text{un-faithfulness}$, and the unfaithfulness means: how much the model’s final answer \( a \)  (Not) depends on the intermediate reasoning steps, since by removing sub-steps, it still reaches same answer. 

A high faithfulness score indicates that the model’s final answer is more dependent on intermediate reasoning steps, suggesting that the reasoning is not post-hoc, and this faithfully reflects that the logical steps are required to derive the answer, by this high faithfulness, the UQ measure should align with it, in other words, a UQ method is good if it derives a lower uncertainty when the faithfulness is high, and vise versa. 


\section{Prompt Template \& Few Shot Examples}
Here we introduce details of the prompt template used in this paper as well as some few-shot examples to guide the LLMs to follow the elicitation process.
% Example begins
\onecolumn
\noindent\rule{\textwidth}{0.4pt} % Top line
\textbf{The Prompt Template to Elicit Knowledge Points.}

\tikz[baseline]{\draw[dashed] (0,0) -- (0.97\textwidth,0);} % Top dashed line

\textbf{System Description:} You are a helpful assistant to do the following: Given a question, you should reflect and come up with the sufficient knowledge that you need to solve this question. Two standards: \textcolor{red}{sufficient} and \textcolor{red}{concise}. And you should respond with numbered points.

\textbf{Task Description:} Given a question: \{ question\_i \}. Please provide a response following system requirements and learning the format from the example: \{ few\_shot\_example \}.

\textbf{Example1:} 

\textcolor{grey}{Question: If it is currently summer in Australia, what season is it in Canada?}

\textcolor{blue}{Expected Response (For required knowledge):}

\textcolor{blue}{%
1. Where is Australia located on Earth? }

\textcolor{blue}{2. Where is Canada located on Earth?}

\textcolor{blue}{3. What is the geographical relationship between Australia and Canada?}

\textcolor{blue}{4. How does the tilt of the Earth affect seasons?}

\textbf{Examples ...} 

\textbf{Output:} \{Placeholder\}

\noindent\rule{\textwidth}{0.4pt} % Bottom line
% \twocolumn

% block begin 2

\noindent\rule{\textwidth}{0.4pt} % Top line
\textbf{The Prompt Template to Express Reasoning Path.}

\tikz[baseline]{\draw[dashed] (0,0) -- (0.97\textwidth,0);} % Top dashed line

\textbf{System Description:} You are a reasoning assistant, you will see some Edge-Node pairs, which stands for the Q-A pairs, try to find a reasoning path based on these Q-A pairs that solves the question. 

\textbf{Task Description:} Given a \{ question \}. Please learn how it is reasoned from the example: Reason\_Path\_Example. Now give the reasoning path for \{q\_a\}.

\textbf{Constraints:}  

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
1. NodeRaw and NodeResult are nominal term,
NodeRaw stands for Question itself and 
NodeResult stands for the End of reasoning. 
;}
}

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
2. When reason to the conclusion, there should be an added: ResultNode and ResultEdge as: [Nodex, NodeResult, ResultEdge];}
}

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
3. [NodeRaw, Node0, Edge0]: indicates  NodeRaw is connected with Node0 by Edge0.
[Nodex, NodeResult, ResultEdge]: indicates  Nodex is connected with NodeResult by ResultEdge.;}
}

\textbf{Example1:} 

\textcolor{grey}{Question: If it is currently summer in Australia, what season is it in Canada?}

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
Edge0: Where is Australia located on Earth?, Node0: Australia in the Southern Hemisphere.;}
}

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
Edge1: Where is Canada located on Earth?, Node1: Canada is located in the Northern Hemisphere.;}
}

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
Edge2: What is the geographical relationship between Australia and Canada?, Node2: Australia and Canada are in the opposite hemisphere.;}
}

{\fontsize{8}{10}\selectfont
\textcolor{grey}{%
Edge3: How does the tilt of the Earth affect seasons?, Node3: Opposite hemispheres experience opposite seasons because of the Earth's tilt.;}
}

\textbf{A Possible Output:}

{\fontsize{8}{10}\selectfont
\textcolor{blue}{%
Structure: {[NodeRaw, Node0, Edge0], [NodeRaw, Node1, Edge1], [Node0, Node2, Edge2], [Node1, Node2, Edge2], [Node2, Node3, Edge3], [Node3, NodeResult, ResultEdge]}; ResultEdge: It is summer in Canada.;}
}

\textbf{Output:} \{Placeholder\}

\noindent\rule{\textwidth}{0.3pt} % Bottom line
\twocolumn
% Example ends

% \begin{figure*}
%     \centering
%     \includegraphics[width=1.0\textwidth]{figs/sensitivity_size.pdf}
%     \caption{We conduct sensitive analysis on the baseline model's performance and on the size of the response set, the size $n$ means the $n \times n$ shape similarity matrix to construct the graph for random walk Laplacian. We can observe that, on the baselines shown above, the $n$ has a slight effect on the performance of the UQ methods, generally, when $n < 10$, the UQ performance is more unstable, but when $n$ is around 10 to 15, the performance tends to be unchanged (within the range of the experiment $[3, 30]$). Thus, empirically, for a stable evaluation and suitable query cost, it is suggested that the setting be 10 in this paper's study.}
%     \label{fig:sensitive}
% \end{figure*}


% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_gpt4omini_dataset_BoolQ_result.png}
%     \caption{The UQ methods on GPT4o-mini's response at BoolQ dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_deepseek_dataset_BoolQ_result.png}
%     \caption{The UQ methods on DeepSeek-R1's response at BoolQ dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_llama3_70_dataset_BoolQ_result.png}
%     \caption{The UQ methods on Llama-3.3-70B's response at BoolQ dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_llama3_8_dataset_BoolQ_result.png}
%     \caption{The UQ methods on Llama3-8b's response at BoolQ dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_phi4_dataset_BoolQ_result.png}
%     \caption{The UQ methods on Phi4's response at BoolQ dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_gpt4omini_dataset_GSM8K_result.png}
%     \caption{The UQ methods on GPT4o-mini's response at GSM8K dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_deepseek_dataset_GSM8K_result.png}
%     \caption{The UQ methods on DeepSeek-R1's response at GSM8K dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_llama3_70_dataset_GSM8K_result.png}
%     \caption{The UQ methods on Llama-3.3-70B's response at GSM8K dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_llama3_8_dataset_GSM8K_result.png}
%     \caption{The UQ methods on Llama3-8b's response at GSM8K dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/experi1/model_phi4_dataset_GSM8K_result.png}
%     \caption{The UQ methods on Phi4's response at GSM8K dataset}
%     \label{fig:enter-label}
% \end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/redundantsp.pdf}
%     \caption{The Example of the redundancy for LLMs (GPT4o-mini)}
%     \label{fig:enter-label}
% \end{figure*}



\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/fullexample.pdf}
    \caption{The Example of of same question but different reasoning path and leading to the same answer: `If it is currently summer in Australia, what season is it in Canada?' .}
    \label{fig:newnew}
\end{figure*}


% \subsection{Sensitive Analysis}

% In this section, we conduct analysis on the parameter (number of answers and explanation) to generate to provide a more stable analysis. Since there is no standard value in the open research community of the uncertainty quantification for LLMs, we conducted the preliminary analysis on the most widely acknowledged work~\cite{lin2023generating} in the domain to explore the best number of answer-generation. The result is as shown in the Figure.~\ref{fig:sensitive}. 




\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
