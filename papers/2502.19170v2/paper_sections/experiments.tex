\section{Experimental Results}
\label{sec:experiments}
Similarly to what was done in \cite{bernstein2018signsgd}, we first test out findings on a toy example, before testing them on the standard MNIST dataset. The setting is a 1000-dimensional quadratic with $\mathcal{N}(0, 1)$ noise added to each gradient component. The results are shown in \ref{fig:toy_example_batch_1_500} and demonstrate the influence of the batch size on the convergence behavior in presence of varying proportion of adversaries. 

\begin{figure}[ht!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/toy_example_batch1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/toy_example_batch500.png} 
    \end{subfigure}
    \caption{Influence of the batch size on the convergence of signSGD with majority vote in the presence of omniscient adversaries, for the toy example with 27 workers and varying numbers of adversaries.}
    \label{fig:toy_example_batch_1_500}
\end{figure}

The advantage of the toy example is that the assumption of Gaussian noise and the assumption of the adversaries knowing the exact gradient are met perfectly. 
The toy example notebook can be found using this \href{https://colab.research.google.com/drive/1vVhphWR9kPUxGH--dJAMoqIxedUTQvDh#scrollTo=cpxNFAP-MdQ0}{\textbf{link}}.

As a more relevant experiment, the MNIST dataset is used, which consists of 60000 training samples and 10000 testing samples. Figures \ref{fig:mnist64}, \ref{fig:mnist256} and \ref{fig:mnist512} show on the left the training loss and on the right the test accuracy over all iterations.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/loss_500_64.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/acc_500_64.png}
    \end{subfigure}
    \caption{Training loss and test accuracy for batch size 64 and 500 iterations shows no convergence for more than 33\% adversaries.}
    \label{fig:mnist64}

    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/loss_200_256.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/acc_200_256.png}
    \end{subfigure}
    \caption{Training loss and test accuracy for batch size 256 and 200 iterations shows no convergence for 48\% adversaries.}
    \label{fig:mnist256}

    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/loss_200_512.png} 
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]
        {figures/acc_200_512.png} 
    \end{subfigure}
    \caption{Training loss and test accuracy for batch size 512 and 200 iterations shows convergence for up to 48\% adversaries.}
    \label{fig:mnist512}
\end{figure}



All of the $\calB$ omnisicent adversaries compute a gradient on their minibatch.
Those $\calB$ gradients are aggregated on an adversary server with a majority vote to estimate the true gradient and the outcome $\gradest$ is communicated to all adversaries.
Every adversary sends then the opposite of $\gradest$ to the main server where the gradients of all the honest and dishonest workers are aggregated by a majority vote. 
The CNN is developed in a PyTorch environment and using Cuda. The MNIST notebook can be found using this \href{https://colab.research.google.com/drive/1vQpUZgRLQjmRx4AQnt2UX_iorjHVj9rH#scrollTo=NmR53BEtMdRP}{\textbf{link}}.

The experiments with different batch sizes show that convergence and the learning of useful models highly depends on the batch size, which correlates to the probability $p$ that the honest workers to get the sign of the gradients right. It can be seen that for big enough batch sizes with up to 48\% adversaries useful models can be learnt. All details concerning the experimental settings can be found in the appendix \ref{experimental_setting}.
