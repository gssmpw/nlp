\section{Model}
\label{sec:model}

\begin{algorithm}[ht!]
    \caption{\textsc{signSGD} with majority vote (Proposed in \cite{bernstein2018signsgd}).}
    \label{singSGD algo}
\begin{algorithmic}
\Require learning rate $\eta > 0$, weight decay $\lambda \geq 0$, mini-batch size $n$, initial point $x_t$ at each of $M$ workers.
\Repeat
    \State \textbf{on} $m^{\text{th}}$ worker
    \State \hskip2em $\tilde{g}_{m,t} \leftarrow \frac{1}{n} \sum_{i=1}^n 
    g_{m,i}(x_t)$
    \State \hskip2em \textbf{push} $\text{sign}(\tilde{g}_{m,t})$ \textbf{to} server 
    \State \textbf{on} server
    \State \hskip2em $O_t \leftarrow \sum_{m=1}^M \text{sign}(\tilde{g}_{m,t})$  
    \State \hskip2em \textbf{push} $\text{sign}(O_t)$ \textbf{to} each worker 
    \State \textbf{on} every worker
    \State \hskip2em $x_{t+1} \leftarrow x_t - \eta (\text{sign}(O_t) + \lambda x_t)$
\Until convergence
\end{algorithmic}
\end{algorithm}

We consider the now standard setting of the signSGD with majority vote algorithm, as proposed in the seminal work of \cite{bernstein2018signsgd}, shown in Algorithm \ref{singSGD algo}. In this context, workers communicate solely the sign for each component of their gradient to the server, thereby improving communication efficiency and robustness against malicious actors. 

The key notation used throughout this work is presented in Table~\ref{tab:notation}, providing clarity on the variables and concepts employed in the analysis.

\begin{table}[ht!]
\caption{Key Notation}
\label{tab:notation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{p{2cm} p{13cm}}
\toprule
Notation & Description  \\
\midrule
$O$ & Output gradient of the server after the majority vote. \\ 
\obj & Objective function, e.g., Loss function. \\
\gradtrue & \emph{True} gradient of the objective function \obj, with respect to the parameters $\theta$. \\ 
\gradest & \emph{Estimate} gradient of the objective function \obj, before taking the majority vote $O = \text{sign} (\gradest)$. \\ 
$\gradtrue_{i}$ & \emph{True} gradient of the objective function at index $i$. \\ 
$\gradest_{i, t, v}$ & \emph{Estimate} gradient at index $i$, at time step $t$ for a worker $v$. \\ 
$M, \calB$ & The honest and adversary workers, respectively. Total number of worker is $Q=M+ \calB$. \\ 
$K$, $K'$ & The sum of contributions of honest $M$ and adversary $\calB$ workers, respectively. It holds $O = \signgradest = \text{sign}(K + K')$. \\ 
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


The Byzantine framework we borrow from the large literature on BFT learning~\cite{lamport, blanchard2017machine, lilisu} and want to emphasize that \emph{worst case} and \emph{Byzantine} attackers are identical situations.

We are going back to the formal definition of Byzantine Faults \cite{lamport} — characterized by omniscient, arbitrarily colluding attackers — and demonstrating how signSGD can withstand such threats. This distinction is essential, as the strategies employed by malicious workers to undermine convergence differ significantly under the Byzantine Fault model. 

\begin{definition} \label{omniscient} \textnormal{(Omniscient Adversaries).} A system consists of $Q$ workers, where a subset of $\alpha Q$ are 	\emph{omniscient adversaries}. These adversaries aim to prevent the convergence of 	\emph{signSGD with majority vote} by leveraging all capabilities allowed for a Byzantine adversary. In particular, 
\begin{itemize}
    \item they coordinate their votes by accessing all $\alpha Q \in \calB$ adversarial gradients $\gradest_{t, v}$ for every adversary $v \in \calB$ before transmitting $\signgradest_{t, v}$ to the server, 
    
    \item they possess knowledge of the true gradient $\gradtrue_t$ at each step $t$, 
    \item they observe the gradients $\gradest_{t, w}$ submitted by all honest workers $w \in M$ at step $t$ and
    \item they know the current and past values of the objective function $\obj_t$, but not its~future~values.
\end{itemize}

\end{definition}