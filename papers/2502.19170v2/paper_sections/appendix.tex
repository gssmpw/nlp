
\section{Proof of Lemma \ref{Lemma 1}: Accuracy of Sign Guessing}
\label{prooflemma}

\begin{proof}

\noindent From Lemma 1 of \cite{bernstein2018signsgd}, we know that:
\begin{equation*}
    1 - p = \Prob[\sign{\gradest_i} \neq \sign{\gradtrue_i}] \leq \begin{cases}
    \frac{2}{9}\frac{1}{S_i^2}, & \text{if } S_i > \frac{2}{\sqrt{3}} \\
    \frac{1}{2} - \frac{S_i}{2\sqrt{3}}, & \text{otherwise.}
    \end{cases}
\end{equation*}


We show that in both cases, this probability can be upper bounded by $\frac{1}{2} - \frac{S_i}{2\sqrt{4 + S_i^2}}$. 



\emph{Case 1}: $S_i > \frac{2}{\sqrt{3}}$.
\begin{equation*}
    1 - p \leq \frac{2}{9}\frac{1}{S_i^2} \overset{!}{\leq} \frac{1}{2} - \frac{S_i}{2\sqrt{4 + S_i^2}} \iff \frac{4\sqrt{4 + S_i^2} + 9S_i^3}{18S_i^2\sqrt{4 + S_i^2}} \overset{!}{\leq} \frac{1}{2}
\end{equation*}
Notice that for $S_i > \frac{2}{\sqrt{3}}$, the left hand-side term monotonically decreases until $S_i \approx 1.52$ and monotonically increases after that value. In the first range $S_i \in \left]\frac{2}{\sqrt{3}}, 1.52\right[$, we have a highest value of $\approx 0.42 \leq \frac{1}{2}$, for  $S_i = \frac{2}{\sqrt{3}}$. In the second range, $S_i \in \left[1.52, \infty\right[$, we notice that for $S_i \to \infty$ the left hand side converges to the value $\frac{1}{2}$. Therefore, the bound in this case is satisfied. \\
\emph{Case 2}: $S_i \leq \frac{2}{\sqrt{3}}$.
\begin{equation*}
    1 - p \leq \frac{1}{2} - \frac{S_i}{2\sqrt{3}} \overset{!}{\leq} \frac{1}{2} - \frac{S_i}{2\sqrt{4 + S_i^2}} \iff  \sqrt{3} \overset{!}{\leq} \sqrt{4 + S_i^2}.
\end{equation*}


For the smallest value $S_i  = 0$, the term $\sqrt{4 + S_i^2}$ sufficiently upper bounds the inequality. Since this term monotonically increases for growing values of $S_i$, the inequality is satisfied also in \textit{Case 2} and the bound is proven.


Thus:
\begin{equation*}
    1 - p = \Prob[\sign{\gradest_i} \neq \sign{\gradtrue_i}] \leq \frac{1}{2} - \frac{S_i}{2\sqrt{4 + S_i^2}}.
\end{equation*}

Moreover, we can show that
\begin{equation*}
    \frac{p(1 - p)}{(p - \frac{1}{2})^2} \leq \frac{4}{S_i^2}
\end{equation*}
Indeed, by rearranging the terms we have:
\begin{align*}
& \quad \frac{1}{2} - \frac{S_i}{2 \sqrt{4 + S_i^2}} \geq 1 - p \\
&\iff  \frac{S_i}{2 \sqrt{4 + S_i^2}}\leq p - \frac{1}{2} \\
&\iff  \frac{1}{2}\leq (p - \frac{1}{2})(\frac{\sqrt{4 + S_i^2}}{S_i}) \\
&\iff \frac{1}{4} \leq (p - \frac{1}{2})^2(\frac{4}{S_i^2} + 1) \\
&\iff \frac{1}{4(p - \frac{1}{2})^2} - 1 \leq \frac{4}{S_i^2} \\
&\iff \frac{p(1 - p)}{(p - \frac{1}{2})^2} \leq \frac{4}{S_i^2} 
\end{align*}
This concludes our proof.



\end{proof}

\newpage

\section{Experimental Setting}
\label{experimental_setting}

\begin{table}[ht!]
    \centering
    \caption{Details for the Toy Example}
    \begin{center}
    \begin{small}
    \begin{tabular}{ll}
        \toprule
        Category & Details \\
        \midrule
        Objective Function & $0.5 \times \text{np.dot}(x, x)$ \\
        Parameter Vector Size & 1000 \\
        Noise Scale ($\sigma$) & 1.0 \\
        Device & CPU \\
        Total Workers & 27 \\
        Batch Size & 1, t (iteration counter) or 500  \\
        Iterations (T) & 500 \\
        Initial Learning Rate & 1.0 \\
        Learning Rate Schedule & $\text{initial\_lr} / \sqrt{t+1}$ \\
        Repeats & 5 (for worker tests), 1 or 3 (for adversary tests) \\
        Optimizer & signSGD with majority vote \\
        Adversaries & 
        \begin{tabular}{l}
            - Blind: Flipping based on individual gradient estimates \\
            - Omniscient: Flipping based on knowledge of true gradients
        \end{tabular} \\
        Evaluation Metrics & Objective value every iteration \\
        Machine & Windows, Intel Core i7 \\
        Computational Time & 
        \begin{tabular}{l}
            - approx. 3 sec for 1 repeat, batch size 1 and 500 iterations \\
            - approx. 3 min 19 sec for 1 repeat, batch size 500 and 500 iterations 
        \end{tabular} \\
        \bottomrule
    \end{tabular}
    \end{small}
    \end{center}
\end{table}


\begin{table}[ht!]
    \centering
    \caption{Details for the MNIST Experiment}
    \begin{center}
    \begin{small}
    \begin{tabular}{ll}
        \toprule
        Category & Details \\
        \midrule
        Dataset & MNIST \\
        Training Set & Split into 27 chunks for workers \\
        Test Set & Standard MNIST test set \\
        Transformations & 
        \begin{tabular}{l}
            - Center crop to 26x26, resize to 28x28 \\
            - Random brightness, contrast, saturation, and hue adjustments \\
            - Random rotation (±10 degrees) and affine transformation (±5 degrees) \\
            - Normalize with mean 0.1307 and std 0.3081
        \end{tabular} \\
        Model & Custom CNN with 2 conv layers and 2 linear layers \\
        Layers & 
        \begin{tabular}{l}
            - conv1: 1 input, 32 output, kernel size 3 \\
            - conv2: 32 input, 64 output, kernel size 3 \\
            - fc1: 9216 input, 128 output \\
            - fc2: 128 input, 10 output \\
            - Dropout: 0.25 and 0.5
        \end{tabular} \\
        Activation Functions & ReLU, log-softmax in output layer \\
        Device & GPU if available, otherwise CPU \\
        Total Workers & 27 \\
        Batch Size & 64, 256 or 512 (specified in title of plots) \\
        Iterations (T) & 200 (for batch size 256 and 512), 500 (for batch size 64) \\
        Learning Rate ($\eta$) & 0.0001 \\
        Weight Decay ($\lambda$) & 0.05 \\
        Momentum ($\beta$) & 0.9 \\
        Repeats & 1 \\
        Optimizer & signSGD with majority vote \\
        Loss Function & Negative Log-Likelihood Loss (F.nll\_loss) \\
        Adversaries & Simulated by flipping the sign of the aggregated adversary gradient \\
        Evaluation Metrics & Training loss and test accuracy every 5 iterations \\
        Computational Time & 
        \begin{tabular}{l}
            - approx. 50 min for 1 repeat, batch size 64 and 500 iterations \\
            - approx. 65 min for 1 repeat, batch size 512 and 200 iterations 
        \end{tabular} \\
        \bottomrule
    \end{tabular}
    \end{small}
    \end{center}
\end{table}
