\section{Introduction}



As AI models grow in size, training them motivates multiple forms of distributed learning, ranging from the fully centralized parameter-server setting~\cite{li2014scaling} to fully decentralized setups~\cite{jungle}. Common to all forms of distributed learning, several challenges have been the subject of intense research over the past decade. A non-exhaustive list of these challenges includes (1) robustness of the distributed learning system to adversarial behavior by some of its members, (2) communication costs among participants in the learning procedure, and (3) preserving the privacy of users' data while learning in a distributed fashion.

Focusing on Stochastic Gradient Descent (SGD), the workhorse of today's most successful AI applications, an important body of work has addressed the first~\cite{blanchard2017machine, el2020robust, lilisu, jungle, alistarh2018byzantine, tournesol, boussetta2021aksel, rouault2022practical}, the second~\cite{qsgd, gandikota2021vqsgd, karimireddy2019error, haddadpour2021federated}, and the third~\cite{ji2014differential, li2020privacy, kairouz2021distributed, fang2021privacy, sun2018private} challenges separately. In this work, we are interested in understanding how well the first and second challenges can be tackled simultaneously. Namely, we ask whether it is possible to reduce the communication cost (2) of distributed learning while simultaneously remaining robust to adversarial behavior within the system (1).

A first partially positive answer to our question was provided in~\cite{bernstein2018signsgd} using signSGD\footnote{For brevity, we refer to "signSGD with majority vote" simply as "signSGD" throughout the rest of this paper.}, a variant of SGD in which only the signs of each component of the minibatch stochastic gradient are transmitted. Specifically, it was shown that the signSGD algorithm is fault tolerant, but only against a weak class of attackers — namely, blind multiplicative adversaries — that cannot communicate or coordinate to carry out a shared (optimal) attack.

The core of our contribution is proving the resilience of the signSGD algorithm against \emph{omniscient} and \emph{colluding} adversaries — entities capable of orchestrating more sophisticated attacks by pooling resources and information with other adversaries. Thanks to the nature of signSGD, we can narrow down the damage that can be caused by \emph{any possible attack on signSGD}, bounding it by the damage of a specific \emph{optimal attack}.

In the parlance of distributed computing~\cite{lamport}, we consider not only the fault tolerance of signSGD but also its \emph{Byzantine Fault Tolerance} (BFT). In the realm of distributed machine learning, the BFT property refers to the algorithm's resilience against omniscient malicious workers attempting to prevent convergence in a distributed setting~\cite{blanchard2017machine, alistarh2018byzantine, mhamdi2018hidden, bernstein2018signsgd}. This represents an extension of the original definition in distributed systems, where BFT refers to the system's ability to continue functioning (and reach consensus) despite the presence of malicious, omnipotent attackers~\cite{lamport, castro1999practical}.

Our objective is to build upon the findings of~\cite{bernstein2018signsgd} by addressing and defining a more powerful class of attackers, namely Byzantine adversaries, who are capable of launching arbitrary attacks — in contrast to the weaker adversaries considered in the original work of~\cite{bernstein2018signsgd}. It is worth noting that this gap was explicitly acknowledged in~\cite{bernstein2018signsgd}, where fault tolerance against blind multiplicative adversaries was demonstrated for signSGD, but BFT was not established and the gap never closed. Furthermore, in light of a concurrent and recent proof presented in~\cite{jin2024sign}, which also argues for the BFT of the signSGD algorithm, we propose a comparative analysis of both proofs. Our analysis focuses on evaluating the theoretical limitations and proof methodologies of the approach proposed by~\cite{jin2024sign} compared to ours. In particular, we do not resort to any unknown constants, and we consider truly Byzantine attackers in the context of the signSGD framework — i.e., adversaries that can be omniscient, know the exact value of the true gradient, and are not captured by probabilities on their behavior.

The structure of the paper is organized as follows: Section~\ref{sec:model} details our model, notation, and useful definitions; Section~\ref{sec:theoretical-contribution} provides our analysis and establishes the BFT of signSGD; Section~\ref{sec:comparative} offers a comparative analysis with~\cite{jin2024sign}, which is, to the best of our knowledge, the only other work arguing for the BFT of signSGD; Section~\ref{sec:experiments} experimentally supports our findings; and finally, Section~\ref{sec:conclusion} concludes the paper and discusses its limitations.