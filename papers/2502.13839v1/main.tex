\documentclass[5p,times]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{Performance optimization of BLAS algorithms with band matrices for RISC-V processors}% \tnoteref{label1}} 
%\tnotetext[label1]{}

%% Article title

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{Anna Pirova}
\author{Anastasia Vodeneeva}
\author{Konstantin Kovalev}
\author{Alexander Ustinov}
\author{Evgeny Kozinov}
\author{Alexey Liniov}
\author{Valentin Volokitin}
\author{Iosif Meyerov \corref{cor1}}%\fnref{label_coresp}}


%\fntext[label_coresp]{Corresponding author.}
\cortext[cor1]{Corresponding author.}
\ead{meerov@vmk.unn.ru}


%% Author affiliation
\affiliation{organization={Department of HPC and System Programming, Lobachevsky State University of Nizhny Novgorod},%Department and Organization
            addressline={23, Prospekt Gagarina}, 
            city={Nizhny Novgorod},
            postcode={603022}, 
%            state={},
            country={Russia}}

%% Abstract
\begin{abstract}
%% Text of abstract
The rapid development of RISC-V instruction set architecture presents new opportunities and challenges for software developers. Is it sufficient to simply recompile high-performance software optimized for x86-64 onto RISC-V CPUs? Are current compilers capable of effectively optimizing C and C++ codes or is it necessary to use intrinsics or assembler? Can we analyze and improve performance without well-developed profiling tools? Do standard optimization techniques work? Are there specific RISC-V features that need to be considered? These and other questions require careful consideration. In this paper, we present our experience optimizing four BLAS algorithms for band matrix operations on RISC-V processors. We demonstrate how RISC-V-optimized implementations of OpenBLAS algorithms can be significantly accelerated through improved vectorization of computationally intensive loops. Experiments on Lichee Pi 4A and Banana Pi BPI-F3 devices using RVV 0.7.1 and RVV 1.0 vector instruction sets respectively, show speedups of 1.5x to 10x depending on the operation compared to the OpenBLAS baseline. In particular, the successful use of vector register grouping with RVV can lead to significant performance improvements.
\end{abstract}

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
High performance computing \sep OpenBLAS \sep RISC-V \sep  Band Matrix \sep Vectorization \sep Performance Optimization
\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%

%% Use \section commands to start a section
\section{Introduction}
\label{sec1}

The development of the RISC-V instruction set architecture (ISA) opens up new opportunities to solve a wide range of problems focusing on high-performance computing systems. Openness, freedom from patent restrictions and unlimited possibilities for further development \cite{c1} attract researchers and engineers around the world. In fact, there are no high-performance server-class systems that are competitive in performance with top x86 architecture solutions. Despite this, the current level of development in this area allows us not only to dream about the future but also to conduct quite successful research and development. This fact is reflected in the emergence of thematic workshops on RISC-V HPC at major conferences on supercomputer technologies, such as, for example, Supercomputing Conference, ISC High Performance, Parallel Programming and Applied Mathematics, High Performance, Edge And Cloud Computing. Apparently, in the coming years we could expect the emergence of multicore vector out-of-order RISC-V processors suitable for building HPC-class systems. The use of such systems requires not only the availability of appropriate equipment, but also the development of the entire ecosystem. The current state of affairs in the readiness of the RISC-V ecosystem area for HPC is described in \cite{c2}. The paper \cite{c3} provides an overview of open source RISC-V architecture processor cores. The work \cite{c4} examines existing and future RISC-V ISA extensions and demonstrates exciting prospects for the creation and use of RISC-V devices for various application areas (IoT, AI, HPC, Communication, Security, Computer Graphics, etc.). 

Further progress is impossible without the development of system software. A systematic review of the achievements in the area of RISC-V system software is presented in \cite{c5}. The work \cite{c6} describes the Vitamin-V project, which is being developed with the support of the Horizon Europe program. The goal of the project is to create a comprehensive open-source software stack for RISC-V that can be used in cloud services \cite{c6}. Great opportunities for the RISC-V community lie at the intersection of hardware and software, in the area of software-hardware co-design. The papers \cite{c7} and \cite{c8} discuss the exceptional importance of predictive modeling of processor architectures and early feedback on proposed technical solutions in the creation of future supercomputers. The works \cite{c9} and \cite{c10} clearly demonstrate the possibilities of co-design using two algorithms: multiplication of sparse matrices by dense vectors and convolution. The future success of this approach is undoubted. A vast body of work has been devoted to analyzing performance and optimizing programs for current and future RISC-V architecture processors. In \cite{c11} and \cite{c12}, a detailed study of the 64-core SG2042 processor performance was carried out -- the first processor with dozens of RISC-V cores claiming to belong to the HPC domain. The authors demonstrated that the processor, although significantly inferior to its x86 counterparts, demonstrated acceptable performance for compute-bound applications while in memory-bound scenarios it required improved memory handling mechanisms. The papers \cite{c13,c14,c15} discussed the results of testing RISC-V devices using tests typical of the HPC area. The paper \cite{c16} provided an overview of the state-of-the-art in vectorization on RISC-V and analyzed the capabilities of Allwinner D1 processors. The papers \cite{c17} and \cite{c18} showed the advantages of using long SIMD vectors on promising RISC-V processors. The paper \cite{c19} studied the capabilities of compilers for automatic vectorization. A series of papers are devoted to testing the performance and optimization of specific scientific software in astrophysics \cite{c20}, machine learning \cite{c21}, computer vision \cite{c22}, graph algorithms \cite{c23}, optimization of Fast Fourier Transform \cite{c24} and Merge sort \cite{c25} algorithms. 

In general, the studies performed allow us to draw the following conclusions. \textit{Firstly}, there is a need for well-developed performance analysis tools, such as Intel VTune and Intel Advisor. \textit{Secondly}, the experience of porting and optimizing both large problem-oriented software frameworks and individual computational kernels shows that the codes are quite easily adapted for RISC-V, and known approaches to speed up computations work as expected. \textit{Thirdly}, there is an obvious need for high-performance mathematical libraries that are optimized deeply for RISC-V processors. In this paper, we focus on optimizing the performance of four algorithms in the OpenBLAS library, one of the most common open implementations for BLAS specifications. These algorithms work with band matrices. They were chosen as targets for optimization based on large-scale performance testing of OpenBLAS on x86 processors. This showed a significant lag in these functions compared to implementations from Intel OneAPI Math Kernel Library (MKL). Therefore, the possibility of further optimization is focused primarily on the RISC-V platform, which involves improving vectorization using RISC-V specifics. The implementation uses RVV 0.7.1 and RVV 1.0 intrinsic functions, and integration with OpenBLAS is planned. The code is publicly available. 

The paper is structured as follows. Section \ref{sec2} provides an overview of publications devoted to optimizing BLAS algorithms for various hardware platforms. Section \ref{sec3} describes the main algorithms and optimizations that we implemented. Section \ref{sec4} presents the results of the computational experiments. Section \ref{sec5} concludes the paper.

\section{Related Work}
\label{sec2}

The BLAS standard was proposed in 1979 \cite{c27} and was updated until the early 2000s
\cite{c28,c29}. Dense linear algebra algorithms are the computational kernels of many libraries used in scientific and engineering applications, so a large number of works are devoted to the issues of high-performance implementation of BLAS for different architectures. There is a lot of attention in the community to optimizing the general matrix multiplication (GEMM) algorithm. This algorithm is one of the main computational kernels in many Computational Science problems. Firstly, other BLAS level 3 functions can be implemented using this algorithm \cite{c48}. Secondly, GEMM, along with other BLAS algorithms, plays an important role in other matrix algorithms in the LAPACK computational linear algebra library \cite{c65}. The most common approach to developing a high-performance GEMM implementation is described in \cite{c46}. The main idea is based on splitting the original matrix into cache-sized blocks and allocating CPU-specific low-level microkernels implemented with intrinsics or assembler. This approach was implemented in GotoBLAS and was then used in many other libraries. Optimization of other BLAS level 3 functions in GotoBLAS is described in \cite{c47}. The specifics of BLAS implementation using isolated microkernels in the BLIS library are described in \cite{c62}. For many years, high-performance implementations of GEMM and other BLAS functions have been developed by Intel in the Intel Math Kernel Library (MKL) \cite{c66}. High-performance GEMM implementations for multicore processors are discussed in \cite{c49}. With the advent of the first GPUs, approaches to GEMM optimization for graphics processors began to actively develop \cite{c51,c52}, where a fairly high degree of utilization of computing resources was also achieved. 

Optimization of vector and matrix operations for RISC-V processors is often discussed in the community in terms of improving the performance of neural network inference. In \cite{c54}, the adaptation of the integer GEMM BLIS algorithm for these purposes is proposed, and its performance on the Greenwave GAP8 processor with hardware support of dot product kernel is studied. In \cite{c55}, vectorized GEMM microkernels for RISC-V processors are generated automatically, and their performance compared to that of OpenBLAS implementations on C906 and C910 processors is evaluated. Few works have been done on optimizing algorithms for band matrices. The optimization of the band matrix-matrix multiplication algorithm has been considered in \cite{c30,c31,c32,c33,c34,c35,c36}. In early works \cite{c30,c31,c32} a matrix storage scheme based on diagonals is proposed, which differs from those used in BLAS and LAPACK, and adapted the band matrix multiplication algorithm for it. An important step towards vector computations for algorithms working with band matrices on the CRAY X-MP computer was made in \cite{c32}. In \cite{c33}, the functions of multiplying a symmetric band matrix by a vector (SBMV) and by a general matrix (SBMM) were considered. The authors proposed to split the original band matrix into dense blocks and reduce the computations to a sequence of operations with dense matrices (symmetric, general and triangular matrix-vector product for SBMV and symmetric, general, and triangular matrix-matrix product for SBMM). This approach is not always effective in the case under consideration, since we optimize the operation of OpenBLAS for matrices with a narrow bandwidth. In \cite{c34}, this approach was adapted for use on GPUs, and the symmetric band matrix storage scheme was modified to reduce the number of function calls from BLAS level 2. In \cite{c35,c36}, a compact diagonal band matrix storage scheme and the corresponding band matrix-matrix multiplication algorithm for CPUs \cite{c35} and GPUs \cite{c36} are proposed. In \cite{c58}, a multithreaded algorithm for band matrix-vector multiplication is presented, which guarantees the same computational accuracy for multiple runs of the application.

With the advent of the new RISC-V architecture, the issue of the basic algorithms implementations efficiency, taking into account the requirements and limitations of the BLAS specifications, is once again becoming relevant. These include algorithms for working with band matrices. To study this, it was necessary to choose one of the BLAS implementations optimized for RISC-V processors with RVV 0.7.1 or RVV 1.0 extensions. Currently, there are many open source and commercial implementations available with low-level optimization for various architectures. Some libraries are optimized for specific types of processors, while others have implementations for a wide range of architectures. Thus, the free software package Intel OneAPI Math Kernel Library \cite{c37} is widely used for Intel processors, the AOCL-BLAS package \cite{c39} optimized for AMD processors and IBM ESSL libraries optimized for IBM processors are also popular. Open packages such as ATLAS \cite{c38}, ARMAS \cite{c43}, BLIS \cite{c62}, Eigen \cite{c41}, and OpenBLAS \cite{c40} are optimized for many different platforms and are widely used in scientific research. One of the newest libraries, BLASFEO \cite{c44}, is optimized for embedded systems and provides a high-performance BLAS implementation for cache-fitting matrices. The Netlib implementation is often used as the baseline reference implementation for BLAS on x86 and some other platforms \cite{c59}. For GPUs, libraries such as cuBLAS, clBLAST, ViennaCL are commonly used. The BLIS, OpenBLAS, and Eigen libraries have low-level computational kernels for a wide range of processors, including ARM, IBM Power, MIPS (OpenBLAS, Eigen), but vectorization for RISC-V processors is only partially supported. A comparison of various BLAS implementations on ARM and RISC-V processors as of 2020 is given in \cite{c26}. The BLIS library implements vectorization for SiFive x280 processors (August 2024). The latest release of the Eigen library does not support RISC-V, but a merge request with support of RVV 1.0 instructions (September 2024) is added. Compared to other libraries, OpenBLAS has the most complete vectorization for RISC-V processors and low-level computational kernels with RVV 0.7.1 and RVV 1.0 support for several types of processors. Therefore, we chose it as a baseline implementation, studied the performance of the developed algorithms and identified functions that had potential for further improvement.



\section{Algorithms and their implementation for RISC-V CPUs}
\label{sec3}

\subsection{Methodology of selecting algorithms for optimization}

As the review of BLAS implementations has shown, the OpenBLAS library is widely used in computations and has the most complete implementation for RISC-V processors. It should be noted that C++ compilers are not yet able to automatically vectorize loops for the RVV 0.7.1 vector instruction set, and generating vectorized code for the RVV 1.0 instruction set does not always lead to efficient utilization of computational resources. The latter consideration is also true for vector extensions available in commodity CPUs. In this regard, developers of high-performance mathematical libraries often resort to a low-level implementation of the main computational kernels using intrinsics or assembler. The OpenBLAS library also contains separate implementations using RVV 0.7.1 and RVV 1.0 intrinsics. However, many codes have potential for additional optimization \cite{c63}, and OpenBLAS is not an exception.

To investigate this issue, it was necessary to determine which of the OpenBLAS algorithms could be improved. Currently, there is no generally accepted reference high-performance BLAS implementation for RISC-V processors. Therefore, to identify algorithms with potential for optimization, we selected Intel Xeon processors with x86-64 architecture as a testing environment and compared the performance of OpenBLAS algorithms and their Intel MKL counterparts, which are highly optimized for these processors. Testing was performed for BLAS level 2 and level 3 functions in double and single precision in single-threaded and multi-threaded modes. Three classes of tests were performed based on the input data size: small, medium, and large. In the ``small'' class, the amount of initial data was chosen so that all the data fit into the L1 cache. In the ``medium'' class, it was chosen so that some data would not fit into the L1 cache but would still be within the L3 cache. In the ``large'' class, the initial data volume exceeded the L3 cache capacity, but the data still completely contained to RAM.

The BLAS-Tester library \cite{c61} was used to measure performance. Testing was repeated several times, and the time taken and the performance in MFlops were recorded. For each function, the lowest 20\% and highest 5\% performance scores were discarded from the results of each class. The remaining measurements were then averaged.

As a result of the testing, it was found that the implementation of four OpenBLAS functions that perform operations with band matrices lags significantly behind their counterparts from Intel MKL. These functions perform band matrix-vector multiplication for general, symmetric, and triangular matrices, and solve a system of linear algebraic equations with a triangular band matrix. The largest lag was observed for matrices with a narrow bandwidth, and these functions were selected for further optimization.

\subsection{Data Structures}

In the OpenBLAS library, a non-transposed band matrix $A$ of size $m \times n$ with $ku$ upper and $kl$ lower diagonals is stored as a one-dimensional array with indexing corresponding to the column-wise storage of a dense rectangular matrix $A’$ of size $(kl + ku + 1) \times lda$. Here $lda$ is the leading dimension of A. If the matrix is non-transposed, then $lda \geq n$, otherwise $lda \geq m$ (Fig. \ref{fig1}). For a triangular band matrix, only the specified triangle elements are stored (Fig. \ref{fig2}).

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\textwidth]{fig_1.png}
\caption{Storage format of a general band matrix in the OpenBLAS library. $A$ is a general band matrix, $A'$ and $A'^T$ are dense matrices, which are the storage structures of matrices $A$ and $A^T$, respectively}\label{fig1}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.7\textwidth]{fig_2.png}
\caption{Storage format of triangular and symmetric band matrices of general form in the OpenBLAS library. $A$ is an original upper triangular band matrix, $A'$ and $A'^T$ are dense matrices, which are the storage structures of matrices $A$ and $A^T$, respectively}\label{fig2}
\end{figure*}

Note that the choice of data structure for storing the matrix significantly affects the performance of key algorithms. Thus, to access the column elements of the original matrix $A$, it is necessary to sequentially access the row elements of the matrix $A'$, which is efficient in terms of cache memory usage and convenient for loading into a vector register. The situation is somewhat worse when traversing the diagonal of the original matrix, since in this case it is necessary to load a column from $A’$. This results in memory jumps across $lda$ elements and requires the use of computationally intensive gather instructions (indexed load in RISC-V terminology). A similar problem arises when loading a row of the matrix $A$, which requires working with diagonals of $A’$ in memory, going from the upper right to the lower left corner. These considerations must be taken into account when implementing the underlying algorithms. Typically, these algorithms have low arithmetic intensity and their performance is highly dependent on the efficiency of memory management and the success of vectorization.

\subsection{GBMV: general band matrix-vector multiply}

\subsubsection{Reference algorithm}

Consider the general band matrix-vector multiplication (GBMV) function. According to the BLAS standard, it performs the operation $y = \alpha \times op(A) \times x + \beta y$, where $A$ is a matrix, $op(A)=A$ or $op(A)=A^T$, $x$, $y$ are vectors, and $\alpha$, $\beta$ are scalars. The pseudocode of the baseline version of the GBMV algorithm is shown in Algorithm \ref{alg:1}.

\begin{algorithm}[ht]
\caption{Pseudocode of the baseline GBMV algorithm in the OpenBLAS library}\label{alg:1}
\begin{algorithmic}[1]

\State void \textbf{GBMV}(INT m, INT n, INT ku, INT kl, FLOAT alpha,
       FLOAT *a, INT lda, FLOAT *X, FLOAT *Y) \{
\State INT offset\_u = ku; INT offset\_l = ku + m;
\For {(i = 0; i < MIN(n, m + ku); i++)}
  \State start   = MAX(offset\_u, 0);
  \State end     = MIN(offset\_l, ku + kl + 1);
  \State length  = end - start;
\EndFor
\If {(not TRANSPOSED)}
\State    \textbf{AXPY}(length, alpha * X[i], a + start, \par
              Y + start - offset\_u); 
\Else
\State    Y[i] += alpha * \textbf{DOT}(length, a + start, \par
              X + start - offset\_u);
\EndIf
\State    offset\_u --; offset\_l --; a += lda;
\State \}
\end{algorithmic}
\end{algorithm}

\begin{figure}[t]
\centering
\includegraphics[width=0.3\textwidth]{fig_3.png}
\caption{Scheme of the baseline algorithm for multiplying a general band matrix by a vector in the OpenBLAS library}\label{fig3}
\end{figure}

The product of $op(A) \times x$ is calculated in a loop over the matrix columns (Algorithm \ref{alg:1}, line 3). For a non-transposed matrix, at each loop iteration, the column of the matrix $A$ is multiplied by the vector $x$ element and the result is added to the corresponding vector $y$ element. For a transposed matrix, the dot product of the matrix $A$ column and the corresponding vector $y$ elements is calculated. In the OpenBLAS library, BLAS level 1 operations are used: AXPY (Algorithm \ref{alg:1}, line 9) and DOT (Algorithm \ref{alg:1}, line 11). These operations are vectorized for various architectures and sets of vector instructions including RVV for RISC-V processors (Fig. \ref{fig3}).

We found that the baseline algorithm performs poorly for narrow-band matrices. When a matrix column has fewer elements than can fit in a vector register, the scalar versions of AXPY and DOT are used, which reduces performance. Otherwise, if the bandwidth is not a multiple of the vector register size, then the inefficiently vectorized tail can be significant for a small bandwidth.

\subsubsection{Optimized algorithm}

For matrices with a narrow bandwidth, a modification of GBMV is proposed that allows for more efficient vectorization by changing the matrix traversal order. To calculate the product $op(A) \times x$ we divide the matrix into vertical blocks, the size of which is equal to the number of elements that fit into the vector register. In each block, a matrix traversal is performed along the diagonals (Fig. \ref{fig4}). Then vector operations of addition and multiplication are performed with the elements of the matrix $A$ diagonal and the corresponding elements of the vectors $x$ and $y$.

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{fig_4.png}
\caption{Scheme of the optimized algorithm for multiplying a general band matrix by a vector in the OpenBLAS library}\label{fig4}
\end{figure}

The pseudocode for the optimized algorithm is given in Algorithm \ref{alg:2}. This algorithm could be implemented with any set of vector instructions that support addition, multiplication, index load in RISC-V terms, and gather in x86 terms.

\begin{algorithm}[ht]
\caption{Pseudocode of the optimized GBMV algorithm}\label{alg:2}
\begin{algorithmic}[1]

\State void \textbf{GBMV\_Optimized}(INT m, INT n, INT ku, INT kl, \par 
         FLOAT alpha, FLOAT *a, INT lda,  FLOAT *X, \par FLOAT *Y) \{
\State   INT start, end;
\State   INT BLOCK\_SIZE = GET\_VECTOR\_LENGTH(); 
\State   start = ku; 
\If {(not TRANSPOSED)}
\State      end = MIN(n, m - kl);
\Else
\State      end = MIN(m, n - kl);
\EndIf
\State end -= (end - start) \% BLOCK\_SIZE; 
\State \textbf{/* baseline Algorithm \ref{alg:1} for the columns with indices from 0 to start - 1 */}
\State ptrdiff\_t stride\_x, stride\_y; 
\If {(not TRANSPOSED)}
\State    stride\_x = 0; stride\_y = - ku + j;
\Else
\State    stride\_x = - ku + j; stride\_y = 0;
\EndIf

\For {(i = start; i + BLOCK\_SIZE < end; \par i += BLOCK\_SIZE)} 
\For {(j = 0; j < kl + ku + 1; j++)} 
\State       x\_copy = LOAD(X + i + stride\_x); 
\State       y\_copy = LOAD(Y + i + stride\_y);
\State       diag\_a = LOAD\_WITH\_STRIDE(a + j, STRIDE);
\State       x\_copy = MULT(x\_copy, factor);
\State       y\_copy = FMA(diag\_a, x\_copy, y\_copy);
\State       STORE(Y + i + stride\_y, y\_copy);
\EndFor 
\State    a += lda * BLOCK\_SIZE;    
\EndFor 

\State \textbf{/* baseline Algorithm \ref{alg:1} for the columns with indices from end to n - 1 */}
\State \}

\end{algorithmic}
\end{algorithm}

In Algorithm \ref{alg:2}, lines 3-10 define the starting and ending indices of the columns of matrix $A$, which will be traversed by the optimized algorithm. Line 11 calls Algorithm \ref{alg:1} to process columns with fewer than $lda$ elements. Lines 13-18 calculate the index shift relative to $i$ for operations with vectors $x$ and $y$. The loop in lines 18-27 performs matrix block multiplication: loading vectors $x$ and $y$ (lines 20-21), loading a diagonal fragment into a vector register (line 22), calculating the product (lines 23-24), and storing the result in $y$ (line 25). Note that loading a diagonal requires using a vector load instruction with non-unit-stride access since these elements are not consecutive in memory. Line 29 calls Algorithm \ref{alg:1} processing last columns with indices from end to $n - 1$.

\subsection{SBMV: symmetric band matrix-vector multiply}

The approach described above is also used to optimize the symmetric band matrix-vector multiplication (SBMV) and triangular band matrix-vector multiplication (TBMV).

SBMV performs the operation $y = \alpha \times A \times x + \beta \times y$, where $x$, $y$ are vectors, $\alpha$, $\beta$ are scalars, $A$ is a symmetric band matrix of $n \times n$ size with $k$ side diagonals. In the baseline version of SBMV, the product $\alpha \times A \times x$ is calculated for one matrix $A$ column at each iteration, AXPY and DOT are called to update the vector $y$, since only one matrix triangle is stored. Algorithm \ref{alg:3} shows an optimized algorithm for this function in case when the matrix is stored as a lower triangle. In case when it is stored as an upper one, the code differs in terms of shifting indexes in arrays $X$, $Y$, a and the calculating column size formula (lines 5, 20, 25). In lines 4-23 of Algorithm \ref{alg:3}, the first $n - k$ columns of the matrix of size $k$ are traversed. The AXPY and DOT calls are separated, because preliminary performance measurements showed that the DOT takes much longer to execute. In lines 4-7 of Algorithm \ref{alg:3}, the AXPY is executed for the first $n - k$ columns, in lines 8-18 of Algorithm \ref{alg:3} vectorized DOT for the same columns is calculated. The matrix is traversed along the diagonals, similar to the optimized GBMV algorithm. DOT for columns that do not fit into vector registers is performed using the baseline algorithm (lines 19-23). The calculations for the matrix last columns with a smaller size are also performed using the baseline algorithm (lines 24-30).

\begin{algorithm}[ht]
\caption{Pseudocode of the optimized SBMV algorithm. The matrix is stored as a lower triangle}\label{alg:3}
\begin{algorithmic}[1]

\State void \textbf{SBMV\_L\_Optimized}(INT n, INT k, FLOAT alpha, FLOAT *a,
                        INT lda, FLOAT *X, FLOAT *Y) \{
\State INT BLOCK\_SIZE = GET\_VECTOR\_LENGTH();
\State INT iend = (n - k) - (n - k) \% BLOCK\_SIZE;
\For {(i = 0; i < n - k; i++)}
\State    length = MIN(k, n - i - 1);
\State    AXPY(length + 1, alpha * X[i], a + lda * i, Y + i);
\EndFor
\For {(i = 0; i < iend; i += BLOCK\_SIZE)}  
\State y\_copy =LOAD(y + i); 
\For {(INT j = 0; j < k; j++)}
\State    x\_copy = LOAD(x + i + 1 + j); 
\State    diag\_a = LOAD\_WITH\_STRIDE(a + 1 + j,\par STRIDE); 
\State    mul = MUL(x\_copy, diag\_a); 
\State    y\_copy = FMA(y\_copy, alpha, mul); 
\EndFor
\State    SAVE(y + i, y\_copy); 
\State    a += BLOCK\_SIZE * lda; 
\EndFor
\For {(; i < n - k; i++)} 
\State length = MIN(n - i - 1, k); 
\State Y[i] += alpha * DOT(length, a + 1, X + i  + 1); 
\State a += lda; 
\EndFor
\For {(; i < n; i++)}
\State    length = MIN(n - i - 1, k);
\State    AXPY(length + 1, alpha * X[i], a + k - length,  
\State         Y + i - length);
\State    Y[i] += alpha * DOT(length, a + 1, X + i  + 1);
\State    a += lda;
\EndFor
\State \} 

\end{algorithmic}
\end{algorithm}

\subsection{TBMV: triangular band matrix-vector multiply}

TBMV performs the operation $x=op(A) \times x$, where $x$, $y$ are vectors, $A$ is a triangular band matrix of $n \times n$ size with $k$ side diagonals, and $op(A)=A$ or $op(A)=A^T$. Similar to GBMV, at each iteration the product of the matrix column $op(A)$ and the corresponding vector $x$ element is calculated. The result is then written into the input vector $x$, and the traversal is performed ``bottom-up'' and ``top-down'' for a lower and an upper triangular matrix, respectively. To optimize this function, the same approach was used as for GBMV. The pseudocode for the optimized algorithm for a lower triangular non-transposed matrix is given in Algorithm \ref{alg:4}. It traverses the matrix ``bottom-up''. For the last columns, calculations are performed using the baseline algorithm (lines 4-14), and for the remaining columns, they are performed by an optimized algorithm that traverses along diagonals (lines 15-31).

\begin{algorithm}[ht]
\caption{Pseudocode of the optimized TBMV algorithm. The matrix is lower triangular and non-transposed}\label{alg:4}
\begin{algorithmic}[1]

\State void \textbf{TBMV\_LN\_Optimized}(INT n, INT k, FLOAT *a,\par                        INT lda, FLOAT *B) \{
\State    INT BLOCK\_SIZE = GET\_VECTOR\_LENGTH();
\State    a += (n - 1) * lda;
\State    INT iend = n - k - (n - k) \% BLOCK\_SIZE;
\For {(i = n - 1; i >= iend; i--)} 
\State       length  = MIN(n - i - 1, k);
\If { (length > 0)}
\State          AXPY(length, B[i], a + 1, 1, B + i + 1);
\EndIf
\If{ (not a unit main diagonal)}
\State          B[i] *= a[0];
\EndIf
\State       a -= lda;
\EndFor
\State    a -= (lda - 1) * BLOCK\_SIZE;
\For {(; i >= 0; i -= BLOCK\_SIZE)}             
\State       length = k;
\State       b\_old = LOAD(B + i);
\If {(not a unit main diagonal)}
\State          diag = LOAD\_WITH\_STRIDE(a, stride\_a);
\State          z = MUL(b\_old, diag);
\State          SAVE(B + i, z);
\EndIf
\For {(j = 1; j < k + 1; j++)}
\State          diag = LOAD\_WITH\_STRIDE (a + j, stride\_a);
\State 	    z = LOAD(B + i + j);
\State 	    z = FMA(z, diag, b\_old);
\State 	    SAVE(B + i + j, z);
\EndFor
\State 	 a -= lda * BLOCK\_SIZE;
\EndFor
\State \}

\end{algorithmic}
\end{algorithm}

\subsection{TBSV: triangular band matrix-vector solve}

\subsubsection{Reference algorithm}

Consider the function of solving SLAE $op(A) \times x=b$ with a triangular band matrix (TBSV). At each iteration, the scalar product of the matrix $A$ non-diagonal elements and the already found values of the vector of unknows $x$ is subtracted from the right-side element $b[i]$. Then the next unknown $x[i]$ is calculated. (Fig. \ref{fig5}).

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth]{fig_5.png}
\caption{Scheme of the solving SLAE algorithm with a TBSV band matrix in the OpenBLAS library.}\label{fig5}
\end{figure}

Depending on stored triangle of the matrix and whether the matrix is transposed or not, this operation is performed using the DOT or AXPY. A pseudocode of the TBSV algorithm for a lower triangular matrix is given below (Algorithm \ref{alg:5}).

\begin{algorithm}[t]
\caption{Pseudocode of the baseline TBSV algorithm in the OpenBLAS library}\label{alg:5}
\begin{algorithmic}[1]
\State void \textbf{TBSV\_L}(INT n,  INT k, FLOAT *a,  INT lda, \par FLOAT *B) \{
\State    INT i, length;
\For{ (i = 0; i < n; i++)}
\If {(TRANSPOSED)}
\State       length  = MIN(i, k);
\If {(length > 0)} 
\State          B[i] -= \textbf{DOT}(length, a + k - length, 
\State                     B + i - length); 
\EndIf
\State       B[i] /= a[k];
\Else
\State       B[i] /= a[0];
\State       length  = MIN(n - i - 1, k);
\If {(length > 0)} 
\State          \textbf{AXPY}(length, -B[i], a + 1, B + i + 1); 
\EndIf
\State       a += lda;
\EndIf
\EndFor
\State \}
\end{algorithmic}
\end{algorithm}

\subsubsection{Optimized algorithm}

The TBSV optimized algorithm is similar to Algorithm \ref{alg:5} at the top level. The optimization consists of a custom implementation of vector operations, DOT and AXPY. Unlike the baseline implementation, we selected the optimal size of the logical vector registers for these functions (see Section \ref{sec:Methodology} for details). The pseudocode of the optimized TBSV algorithm for lower triangular matrices is shown in Algorithm \ref{alg:6}. Lines 5-15 calculate DOT and lines 24-32 calculate AXPY.

\begin{algorithm}[ht!]
\caption{Pseudocode of the optimized TBSV algorithm for a lower triangular matrix}\label{alg:6}
\begin{algorithmic}[1]

\State void \textbf{TBSV\_L\_Optimized}(INT n, INT k, FLOAT *a, \par INT lda, 
                             FLOAT *B) \{
\If{ (TRANSPOSED)}
\State    \textbf{/* baseline Algorithm \ref{alg:5} for the first k columns */}
\For {(i = k; i < n; i++)}
\State          A\_ptr = a;
\State          B\_ptr = B + i - k;
\State          vector\_sum = BROADCAST(0); 
\For {(int j = k; j > 0; )} 
\State             a\_copy = LOAD(A\_ptr);
\State             b\_copy = LOAD(B\_ptr);
\State             vector\_sum = FMA(a\_copy, b\_copy, \par vector\_sum);
\State             step = MIN(j, MAX\_VECTOR\_LENGTH);
\State             A\_ptr += step; B\_ptr += step; j -= step;
\EndFor
\State          dot = SUM(vector\_sum);
\State          B[i] = (B[i] - dot)/ a[k];
\State          A += lda;
\EndFor
\Else
\For {(i = 0; i <= n - k - 1; i++)}
\State          B[i] /= a[0];
\State          A\_ptr = a + 1;
\State          B\_ptr = B + i + 1;
\For {(j = k; j > 0; )}
\State             a\_copy = LOAD(A\_ptr);
\State             b\_copy = LOAD(B\_ptr);
\State             mult = BROADCAST(-B[i]);
\State             b\_copy = FMA(mult, a\_copy, b\_copy);
\State             STORE(B\_ptr, b\_copy);
\State             step = MIN(j, MAX\_VECTOR\_LENGTH);
\State             A\_ptr += step; B\_ptr += step; j -= step;
\EndFor
\State          a += lda;
\EndFor
\State       \textbf{/* baseline Algorithm \ref{alg:5} for the last k columns */}
\EndIf
\State \}

\end{algorithmic}
\end{algorithm}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{fig_6.png}
\caption{Performance of the OpenBLAS reference and our optimized GBMV implementations on 5M-row matrices with different bandwidths: a) double-precision GBMV on Lichee Pi 4A board, b) single-precision GBMV on Lichee Pi 4A board, c) double-precision GBMV on Banana Pi BPI-F3 board, d) single-precision GBMV on Banana Pi BPI-F3 board. All experiments are performed in sequential mode}\label{fig6}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{fig_7.png}
\caption{Performance of the OpenBLAS reference and our optimized SBMV implementations on 5M-row matrices with different bandwidths: a) double-precision SBMV on Lichee Pi 4A board, b) single-precision SBMV on Lichee Pi 4A board, c) double-precision SBMV on Banana Pi BPI-F3 board, d) single-precision SBMV on Banana Pi BPI-F3 board. All experiments are performed in sequential mode. $L$ -- matrix is stored as lower triangle, $U$ -- matrix is stored as upper triangle}\label{fig7}
\end{figure*}


\section{Numerical results}
\label{sec4}

\subsection{Infrastructure}
Computational experiments were conducted on two types of nodes. The first test system, \textit{Lichee Pi 4A} board, has a T-Head TH1520 processor (4x1.85GHz T-Head C910 cores with 12-stage out-of-order multiple issue superscalar pipeline, ISA RV64GC and 128-bit RVV 0.7.1 standard) with 16 GB of RAM and the Debian GNU/Linux 12 (bookworm) operating system. It uses the gcc (Xuantie-900 linux-5.10.4 Toolchain V2.8.1 B-20240115) 10.4.0 cross compiler. The second test system, \textit{Banana Pi BPI-F3} board, has a SpacemiT Keystone K1 processor (8x1.6GHz SpacemiT x60 cores with 8-stage in-order dual-issue pipeline, RVA22 Profile and 256-bit RVV 1.0 standard) with 16 GB of RAM and the Bianbu 1.0.15 operating system. It uses the GCC RISC-V 14.2.0 cross-compiler.



\subsection {Methodology} \label{sec:Methodology}  
The algorithms described in Section \ref{sec3} were implemented for single and double precision for the AVX-512 vector instruction sets (x86 processors), RVV 0.7.1 and RVV 1.0 vector instruction sets (RISC-V processors), and were integrated into the serial and parallel interfaces of the OpenBLAS library. The optimized code is available at \cite{c67}. Testing was performed using the BLAS-Tester \cite{c61}. For each algorithm, the execution time and performance of the baseline and optimized implementations were compared for matrices ranging from 100,000 to 5 million rows and bandwidth ranging from 1 to 32 for matrix-vector multiplication functions and from 1 to 51 for TBSV. Sequential versions of all functions and parallel versions of matrix-vector multiplications were tested. Each launch was repeated several times, and the results were averaged. It was found that the algorithm performance does not depend on the matrix size but depends on its bandwidth. Therefore, the plots of the performance dependence on the bandwidth for fixed size matrices are given below. Since RVV allows combining several registers into one logical register and working with the second ones using the $LMUL$ parameter, we empirically selected the optimal value of this parameter for each test system. For matrix-vector multiplication functions, $LMUL=4$ was selected for Lichee Pi 4A and $LMUL=2$ -- for Banana Pi BPI-F3, which corresponds to 512-bit logical registers. TBSV uses $LMUL=1$ for Lichee Pi 4A and $LMUL=2$ for Banana Pi BPI-F3, which corresponds to 128-bit and 512-bit logical registers. Testing parallel versions of matrix-vector multiplication algorithms showed that in multithreaded mode their running time is reduced only for matrices of about 5 million rows and a bandwidth of at least 20. Since our implementations are optimized for matrices with a narrow bandwidth, using parallelization here is not practical. Additionally, sequential implementations of BLAS functions are in demand when solving many problems using numerical modeling methods, where parallel computations at a higher algorithm level are used. In any case, effective vectorization of basic mathematical functions is a necessary condition for utilization of the processor resources.

For brevity, we denote the versions of the functions intended for the lower triangular non-transposed matrices as $LN$, the lower triangular transposed -- $LT$, the upper triangular non-transposed -- $UN$, the upper triangular transposed -- $UT$. In the plots, the bandwidth is denoted by $lda$.

\subsection{GBMV Algorithm}
At first, we consider GBMV. The results of computational experiments for the sequential version of the function on matrices with 5 million rows and different bandwidth are shown in Figure \ref{fig6}. GBMV was implemented in two versions, for the non-transposed and for the transposed matrices.


On Lichee Pi 4A (RVV 0.7.1), the optimized algorithm for double-precision matrices outperforms the baseline for non- transposed matrices with fewer than 20 diagonals and for transposed ones with any bandwidth. The average speedup is 2.4x and 4.2x, respectively. For single-precision matrices, the optimized version significantly outperforms the baseline for matrices of any size, showing a performance 2.9x and 5.6x greater for non-transposed and transposed matrices, respectively.

For Banana Pi BPI-F3 (RVV 1.0), the optimized version outperforms the baseline when the bandwidth is less than or equal to 8 for double-precision and less than or equals to 14 in single-precision matrices. The average speedup is 2.4x and 3.2x for double-precision and 3.1x and 4.9x for single-precision non-transposed and transposed matrices, respectively.

In general, the optimized version outperforms the baseline one for matrices with less than 14 diagonals in most test configurations on both systems. In all test configurations, a great relative speedup (up to 10x) is obtained on transposed matrices, where DOT was called in the baseline algorithm. The average speedup is slightly greater for single-precision matrices than for double-precision ones.


\subsection{SBMV Algorithm}

The results of computational experiments for the SBMV sequential version are shown in Figure \ref{fig7}. We consider matrices with 5 million rows and different bandwidth. SBMV is implemented in two versions, for transposed and non-transposed matrices.


The plots show that the speedup for non-transposed matrices does not exceed 2.5x and averages 1.8x and 1.3x for Lichee Pi 4A (RVV 0.7.1) and Banana Pi BPI-F3 (RVV 1.0) test systems, respectively. On Lichee Pi 4A the optimized version outperforms the baseline one for any $lda$ value. For Banana Pi BPI-F3, it outperforms up to $lda=14$ for double-precision and up to $lda=20$ for single-precision matrices. For a larger bandwidth, the optimized version slows down compared to the baseline one. Therefore, for RVV 1.0, switching to the reference version depending on the bandwidth was added. In general, for RVV 0.7.1, the optimized version works stably in all test configurations. For RVV 1.0, it works stably for matrices with a bandwidth of less than 14, but the average performance speedup is less than that of the GBMV function.

\subsection{TBMV Algorithm}

Here we consider TBMV. The results of computational experiments for the TBMV sequential version are shown in Figure \ref{fig8}. The plots were constructed for matrices with 1 million rows and different bandwidths.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{fig_8.png}
\caption{Performance of the OpenBLAS reference and our optimized TBMV implementations on 1M-row matrices with different bandwidths. The results on Lichee Pi 4A board are shown in plots a)-d), the results on Banana Pi BPI-F3 board are shown in plots e)-h). All experiments are performed in sequential mode.
a) double-precision TBMV for upper triangular matrices (UN and UT case), b) single-precision TBMV for upper triangular matrices (UN and UT case), c) double-precision TBMV for lower triangular matrices (LN and LT case), d) single-precision TBMV for lower triangular matrices (LN and LT case), e) double-precision TBMV for upper triangular matrices (UN and UT case), f) single-precision TBMV for upper triangular matrices (UN and UT case), g) double-precision TBMV for lower triangular matrices (LN and LT case), h) single-precision TBMV for lower triangular matrices (LN and LT case)
}\label{fig8}
\end{figure*}

This function is implemented in four versions: for upper and lower triangles, transposed and non-transposed matrices (LN, LT, UN, UT). The optimized version replaces DOT in the baseline algorithm in versions UT and LT, and provides the greatest performance speedup. For TBMV on Lichee Pi 4A (RVV 0.7.1) these versions outperform the baseline ones for any bandwidth and provide a greater advantage for the smaller bandwidth. The average advantage is 2.5x with the UT, 5.2x with the LT version for double-precision matrices and 2.6x and 4.5x for single-precision ones, respectively. On Banana Pi BPI-F3 (RVV 1.0) the performance of optimized UT and LT is higher than the baseline one with any bandwidth for single-precision matrices and with narrow bandwidth in double-precision ones. In almost all test cases for bandwidth of 2 and 3, the optimized algorithm significantly outperforms the baseline, by 2x to 13x on RVV 0.7.1 and by 4x to 20x on RVV 1.0. However, the least effect of the optimization is seen for the LN version, where the optimized algorithm replaced AXPY and traversed the matrix ``bottom-up''. On Banana Pi BPI-F3, the optimized algorithm outperforms the baseline one only for matrices with no more than 6 diagonals in single and double precision. On Lichee Pi 4A, LN optimized version works better on narrow bandwidth single-precision matrices, for double-precision ones -- when the bandwidth is greater than 8. For UN versions on Lichee Pi 4A, the optimized algorithm outperforms baseline one for wide double-precision matrices (when bandwith is greater than 8) and narrow (bandwith is less than 5) or wide (bandwith is 20 and more) single-precision matrices. On Banana Pi BPI-F3, the optimized version outperforms the baseline by an average of 2x and 2.8x for any bandwidth double- and single-precision matrices, respectively.

\subsection{TBSV Algorithm}

The results of the computational experiments for TBSV are shown in Figure \ref{fig9}. The plots are given for matrices with 250000 rows and different bandwidths.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{fig_9.png}
\caption{Performance of the OpenBLAS reference and our optimized TBSV implementations on 250 000-row matrices with different bandwidths. The results on Lichee Pi 4A board are shown in plots a)-d), the results on Banana Pi BPI-F3 board are shown in plots e)-h). All experiments are performed in sequential mode.
a) double-precision TBSV for upper triangular matrices (UN and UT case), b) single-precision TBSV for upper triangular matrices (UN and UT case), c) double-precision TBSV for lower triangular matrices (LN and LT case), d) single-precision TBSV for lower triangular matrices (LN and LT case), e) double-precision TBSV for upper triangular matrices (UN and UT case), f) single-precision TBSV for upper triangular matrices (UN and UT case), g) double-precision TBSV for lower triangular matrices (LN and LT case), h) single-precision TBSV for lower triangular matrices (LN and LT case)
}\label{fig9}
\end{figure*}

TBSV is implemented in four variants: for the upper and lower triangles, transposed and non-transposed matrices (LN, LT, UN, UT).

The plots show that the optimized implementation on both test boards outperforms the baseline version for both variants (UT and LT) where the baseline implementation calls DOT. On the Lichee Pi 4A (RVV 0.7.1) board, the average performance speedup of the UT version is 2.8x for double- and 3.0x for single-precision matrices. For the LT version, it is 1.3x faster for both single and double precision. On the Banana Pi BPI-F3 (RVV 1.0) board, the optimized version is 1.5x and 1.8x faster than the baseline one for UT and LT, respectively, with both single and double precision. On this board, LN and UN optimized versions outperform the baseline implementation for all test cases and work on average 1.5x faster. The optimization also improves performance on the Lichee Pi 4a board for these TBSV variants. The optimized LN version outperforms the baseline one for double-precision and single-precision matrices with a bandwidth of at least 9 and 20, respectively. It wins on average by 1.8x in double and 2.2x in single precision compared to the baseline version. The UN version performs better on matrices of average bandwidth. It outperforms the baseline by 13\% in both single and double precision.

We conclude that vectorization performed on RVV 1.0 speeds up all TBSV variants. Vectorization on RVV 0.7.1 allows for speeding up the baseline version for transposed matrices with any bandwidth and non-transposed ones with a certain bandwidth. The speedup is less than for other functions, since the algorithmic capabilities of vectorization are limited here.


\section{Conclusion}
\label{sec5}

In this paper, we optimized four BLAS level 2 functions for RISC-V processors: matrix-vector multiplication for general, triangular, and symmetric band matrices, and solving SLAE with a triangular band matrix. Compared to the OpenBLAS RISC-V baseline implementation, our optimization achieves an advantage by changing the matrix traversal order from element-wise to diagonal, which increases vectorization efficiency. The implementation is based on OpenBLAS and is intended for RISC-V processors that support the vector extensions RVV 0.7.1 and RVV 1.0.

Overall, the results of experiments show that, for all four functions, the optimized implementation performance is better than the baseline for matrices with a certain bandwidths or in all cases considered. For all functions, the optimized version provides the greatest speedup for upper triangular or transposed matrices in cases where DOT is called from the baseline algorithm. The switching thresholds between the base and optimized implementations can be determined empirically depending on the input matrix bandwidth. It is advisable to use the parallel version of OpenBLAS for matrices with a number of rows of at least 5 million and a large bandwidth (more than 20). In other cases, it is more profitable to use parallelism at a higher level.

The developed experimental implementation is publicly available in \cite{c67}. In the future, we plan to integrate it into the OpenBLAS library and continue research on improving the efficiency of cache memory usage, for example, using the approaches proposed in \cite{c43}. Another area of interest is the development of analytical models for automatic selection of algorithm parameters, such as switching thresholds between different implementations, which can be implemented using machine learning algorithms. As specialized instruction sets for sparse matrix operations become available in the RISC-V architecture, a promising area of research is emerging that is important for practical use in Computational Science. Overall, the prospects for using RISC-V processors for high-performance computing seem undeniable and open up a large field for new research and development both in the area of system programming (development of compilers and profilers) and in the area of porting and optimizing specific software frameworks.


\section*{Acknowledgements}
The project is supported by the Lobachevsky University academic excellence program ``Priority-2030''. The authors acknowledge the use of computational resources provided by the University (Lobachevsky Supercomputer).



%Example citation, See 
%\cite{c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c50,c51,c52,c53,c54,c55,c56,c58,c59,c60,c61,c62,c63,c64,c65,c67,c66}

\begin{thebibliography}{59}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{#2}
\providecommand{\path}[1]{#1}
\providecommand{\DOIprefix}{doi:}
\providecommand{\ArXivprefix}{arXiv:}
\providecommand{\URLprefix}{URL: }
\providecommand{\Pubmedprefix}{pmid:}
\providecommand{\doi}[1]{\href{http://dx.doi.org/#1}{\path{#1}}}
\providecommand{\Pubmed}[1]{\href{pmid:#1}{\path{#1}}}
\providecommand{\bibinfo}[2]{#2}
\ifx\xfnm\relax \def\xfnm[#1]{\unskip,\space#1}\fi
%Type = Article
\bibitem[{Adit and Sampson(2022)}]{c19}
\bibinfo{author}{Adit, N.}, \bibinfo{author}{Sampson, A.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{Performance left on the table: An evaluation of compiler autovectorization for {RISC-V}}.
\newblock \bibinfo{journal}{IEEE Micro} \bibinfo{volume}{42}, \bibinfo{pages}{41--48}.
\newblock \DOIprefix\doi{10.1109/MM.2022.3184867}.
%Type = Inproceedings
\bibitem[{Alonso et~al.(2023)Alonso, Andreu, Canal, Di~Carlo, Chenet, Costa, Girones, Gizopoulos, Karakostas, Otero, Papadimitriou, Rodríguez and Savino}]{c6}
\bibinfo{author}{Alonso, M.}, \bibinfo{author}{Andreu, D.}, \bibinfo{author}{Canal, R.}, \bibinfo{author}{Di~Carlo, S.}, \bibinfo{author}{Chenet, C.}, \bibinfo{author}{Costa, J.}, \bibinfo{author}{Girones, A.}, \bibinfo{author}{Gizopoulos, D.}, \bibinfo{author}{Karakostas, V.}, \bibinfo{author}{Otero, B.}, \bibinfo{author}{Papadimitriou, G.}, \bibinfo{author}{Rodríguez, E.}, \bibinfo{author}{Savino, A.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Validation, verification, and testing ({VVT}) of future {RISC-V} powered cloud infrastructures: the {Vitamin-V Horizon Europe Project} perspective}, in: \bibinfo{booktitle}{2023 IEEE European Test Symposium (ETS)}, pp. \bibinfo{pages}{1--6}.
\newblock \DOIprefix\doi{10.1109/ETS56758.2023.10174216}.
%Type = Misc
\bibitem[{Asanovi\'{c} and Patterson(2024)}]{c1}
\bibinfo{author}{Asanovi\'{c}, K.}, \bibinfo{author}{Patterson, D.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Instruction sets should be free: The case for {RISC-V}. {EECS Department, University of California, Berkeley, Tech. Rep.} {UCB/EECS-2014-146}}.
%Type = Inproceedings
\bibitem[{Berger-Vergiat et~al.(2023)Berger-Vergiat, Cardwell, Feinberg, Hammond, Hughes, Levenhagen and Pedretti}]{c14}
\bibinfo{author}{Berger-Vergiat, L.}, \bibinfo{author}{Cardwell, S.G.}, \bibinfo{author}{Feinberg, B.}, \bibinfo{author}{Hammond, S.D.}, \bibinfo{author}{Hughes, C.}, \bibinfo{author}{Levenhagen, M.}, \bibinfo{author}{Pedretti, K.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Evaluation of {HPC} workloads running on open-source {RISC-V} hardware}, in: \bibinfo{editor}{Bienz, A.}, \bibinfo{editor}{Weiland, M.}, \bibinfo{editor}{Baboulin, M.}, \bibinfo{editor}{Kruse, C.} (Eds.), \bibinfo{booktitle}{High Performance Computing}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{538--551}.
%Type = Article
\bibitem[{Blackford et~al.(2002)Blackford, Petitet, Pozo and at~el.}]{c29}
\bibinfo{author}{Blackford, L.S.}, \bibinfo{author}{Petitet, A.}, \bibinfo{author}{Pozo, R.}, \bibinfo{author}{at~el.}, \bibinfo{year}{2002}.
\newblock \bibinfo{title}{An updated set of basic linear algebra subprograms {(BLAS)}}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{28}, \bibinfo{pages}{135–151}.
\newblock \DOIprefix\doi{10.1145/567806.567807}.
%Type = Misc
\bibitem[{Brown(2024)}]{c2}
\bibinfo{author}{Brown, N.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{{RISC-V for HPC}: Where we are and where we need to go}.
\newblock \URLprefix \url{https://arxiv.org/abs/2406.12398}, \href{http://arxiv.org/abs/2406.12398}{{\tt arXiv:2406.12398}}.
%Type = Inproceedings
\bibitem[{Brown and Jamieson(2025)}]{c12}
\bibinfo{author}{Brown, N.}, \bibinfo{author}{Jamieson, M.}, \bibinfo{year}{2025}.
\newblock \bibinfo{title}{Performance characterisation of the {64-Core SG2042 RISC-V CPU for HPC}}, in: \bibinfo{editor}{Weiland, M.}, \bibinfo{editor}{Neuwirth, S.}, \bibinfo{editor}{Kruse, C.}, \bibinfo{editor}{Weinzierl, T.} (Eds.), \bibinfo{booktitle}{High Performance Computing. ISC High Performance 2024 International Workshops}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{354--367}.
%Type = Inproceedings
\bibitem[{Brown et~al.(2023)Brown, Jamieson, Lee and Wang}]{c11}
\bibinfo{author}{Brown, N.}, \bibinfo{author}{Jamieson, M.}, \bibinfo{author}{Lee, J.}, \bibinfo{author}{Wang, P.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Is {RISC-V} ready for {HPC} prime-time: Evaluating the 64-core {Sophon SG2042 RISC-V CPU}}, in: \bibinfo{booktitle}{Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{1566–1574}.
\newblock \DOIprefix\doi{10.1145/3624062.3624234}.
%Type = Inproceedings
\bibitem[{Burylov et~al.(2007)Burylov, Chuvelev, Greer, Henry, Kuznetsov and Sabanin}]{c66}
\bibinfo{author}{Burylov, I.}, \bibinfo{author}{Chuvelev, M.}, \bibinfo{author}{Greer, B.}, \bibinfo{author}{Henry, G.}, \bibinfo{author}{Kuznetsov, S.}, \bibinfo{author}{Sabanin, B.}, \bibinfo{year}{2007}.
\newblock \bibinfo{title}{Intel performance libraries: Multi-core-ready software for numeric-intensive computation}, in: \bibinfo{booktitle}{Intel technology journal}, pp. \bibinfo{pages}{1--7}.
%Type = Article
\bibitem[{{Clint Whaley} et~al.(2001){Clint Whaley}, Petitet and Dongarra}]{c38}
\bibinfo{author}{{Clint Whaley}, R.}, \bibinfo{author}{Petitet, A.}, \bibinfo{author}{Dongarra, J.J.}, \bibinfo{year}{2001}.
\newblock \bibinfo{title}{Automated empirical optimizations of software and the {ATLAS} project}.
\newblock \bibinfo{journal}{Parallel Computing} \bibinfo{volume}{27}, \bibinfo{pages}{3--35}.
\newblock \DOIprefix\doi{10.1016/S0167-8191(00)00087-9}.
%Type = Article
\bibitem[{Cui et~al.(2023)Cui, Li and Wei}]{c4}
\bibinfo{author}{Cui, E.}, \bibinfo{author}{Li, T.}, \bibinfo{author}{Wei, Q.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{{RISC-V} instruction set architecture extensions: A survey}.
\newblock \bibinfo{journal}{IEEE Access} \bibinfo{volume}{11}, \bibinfo{pages}{24696--24711}.
\newblock \DOIprefix\doi{10.1109/ACCESS.2023.3246491}.
%Type = Inproceedings
\bibitem[{Demmel(1989)}]{c65}
\bibinfo{author}{Demmel, J.}, \bibinfo{year}{1989}.
\newblock \bibinfo{title}{{LAPACK}: a portable linear algebra library for supercomputers}, in: \bibinfo{booktitle}{IEEE Control Systems Society Workshop on Computer-Aided Control System Design}, pp. \bibinfo{pages}{1--7}.
\newblock \DOIprefix\doi{10.1109/CACSD.1989.69824}.
%Type = Inproceedings
\bibitem[{Diehl et~al.(2024)Diehl, Syskakis, Dais, Brandt, Kheirkhahan, Singanaboina, Marcello, Taylor, Leidel and Kaiser}]{c20}
\bibinfo{author}{Diehl, P.}, \bibinfo{author}{Syskakis, P.}, \bibinfo{author}{Dais, G.}, \bibinfo{author}{Brandt, S.R.}, \bibinfo{author}{Kheirkhahan, A.}, \bibinfo{author}{Singanaboina, S.Y.}, \bibinfo{author}{Marcello, D.}, \bibinfo{author}{Taylor, C.}, \bibinfo{author}{Leidel, J.}, \bibinfo{author}{Kaiser, H.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{{ Preparing for HPC on RISC-V: Examining Vectorization and Distributed Performance of an Astrophysics Application with HPX and Kokkos }}, in: \bibinfo{booktitle}{SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis}, \bibinfo{publisher}{IEEE Computer Society}, \bibinfo{address}{Los Alamitos, CA, USA}. pp. \bibinfo{pages}{1656--1665}.
\newblock \DOIprefix\doi{10.1109/SCW63240.2024.00207}.
%Type = Article
\bibitem[{Dongarra et~al.(1990)Dongarra, Du~Croz, Hammarling and Duff}]{c28}
\bibinfo{author}{Dongarra, J.J.}, \bibinfo{author}{Du~Croz, J.}, \bibinfo{author}{Hammarling, S.}, \bibinfo{author}{Duff, I.S.}, \bibinfo{year}{1990}.
\newblock \bibinfo{title}{A set of level 3 basic linear algebra subprograms}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{16}, \bibinfo{pages}{1–17}.
\newblock \DOIprefix\doi{10.1145/77626.79170}.
%Type = Inproceedings
\bibitem[{D\"{o}rflinger et~al.(2021)D\"{o}rflinger, Albers, Kleinbeck, Guan, Michalik, Klink, Blochwitz, Nechi and Berekovic}]{c3}
\bibinfo{author}{D\"{o}rflinger, A.}, \bibinfo{author}{Albers, M.}, \bibinfo{author}{Kleinbeck, B.}, \bibinfo{author}{Guan, Y.}, \bibinfo{author}{Michalik, H.}, \bibinfo{author}{Klink, R.}, \bibinfo{author}{Blochwitz, C.}, \bibinfo{author}{Nechi, A.}, \bibinfo{author}{Berekovic, M.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{A comparative survey of open-source application-class {RISC-V} processor implementations}, in: \bibinfo{booktitle}{Proceedings of the 18th ACM International Conference on Computing Frontiers}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{12–20}.
\newblock \DOIprefix\doi{10.1145/3457388.3458657}.
%Type = Inproceedings
\bibitem[{Dufrechou et~al.(2014)Dufrechou, Ezzatti, Quintana-Ort{\'i} and Rem{\'o}n}]{c34}
\bibinfo{author}{Dufrechou, E.}, \bibinfo{author}{Ezzatti, P.}, \bibinfo{author}{Quintana-Ort{\'i}, E.S.}, \bibinfo{author}{Rem{\'o}n, A.}, \bibinfo{year}{2014}.
\newblock \bibinfo{title}{Efficient symmetric band matrix-matrix multiplication on {GPUs}}, in: \bibinfo{editor}{Hern{\'a}ndez, G.}, \bibinfo{editor}{Barrios~Hern{\'a}ndez, C.J.}, \bibinfo{editor}{D{\'i}az, G.}, \bibinfo{editor}{Garc{\'i}a~Garino, C.}, \bibinfo{editor}{Nesmachnow, S.}, \bibinfo{editor}{P{\'e}rez-Acle, T.}, \bibinfo{editor}{Storti, M.}, \bibinfo{editor}{V{\'a}zquez, M.} (Eds.), \bibinfo{booktitle}{High Performance Computing}, \bibinfo{publisher}{Springer Berlin Heidelberg}, \bibinfo{address}{Berlin, Heidelberg}. pp. \bibinfo{pages}{1--12}.
\newblock \DOIprefix\doi{10.1007/978-3-662-45483-1\_1}.
%Type = Inproceedings
\bibitem[{Fatahalian et~al.(2004)Fatahalian, Sugerman and Hanrahan}]{c51}
\bibinfo{author}{Fatahalian, K.}, \bibinfo{author}{Sugerman, J.}, \bibinfo{author}{Hanrahan, P.}, \bibinfo{year}{2004}.
\newblock \bibinfo{title}{Understanding the efficiency of {GPU} algorithms for matrix-matrix multiplication}, in: \bibinfo{booktitle}{Proceedings of the ACM SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{133–137}.
\newblock \DOIprefix\doi{10.1145/1058129.1058148}.
%Type = Inproceedings
\bibitem[{Fibich et~al.(2020)Fibich, Tauner, Rössler and Horauer}]{c26}
\bibinfo{author}{Fibich, C.}, \bibinfo{author}{Tauner, S.}, \bibinfo{author}{Rössler, P.}, \bibinfo{author}{Horauer, M.}, \bibinfo{year}{2020}.
\newblock \bibinfo{title}{Evaluation of open-source linear algebra libraries targeting {ARM and RISC-V} architectures}, in: \bibinfo{booktitle}{2020 15th Conference on Computer Science and Information Systems (FedCSIS)}, pp. \bibinfo{pages}{663--672}.
\newblock \DOIprefix\doi{10.15439/2020F145}.
%Type = Article
\bibitem[{Frison et~al.(2018)Frison, Kouzoupis, Sartor, Zanelli and Diehl}]{c44}
\bibinfo{author}{Frison, G.}, \bibinfo{author}{Kouzoupis, D.}, \bibinfo{author}{Sartor, T.}, \bibinfo{author}{Zanelli, A.}, \bibinfo{author}{Diehl, M.}, \bibinfo{year}{2018}.
\newblock \bibinfo{title}{Blasfeo: Basic linear algebra subroutines for embedded optimization}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{44}.
\newblock \DOIprefix\doi{10.1145/3210754}.
%Type = Article
\bibitem[{G\`{o}mez et~al.(2023)G\`{o}mez, Mantovani, Focht and Casas}]{c18}
\bibinfo{author}{G\`{o}mez, C.}, \bibinfo{author}{Mantovani, F.}, \bibinfo{author}{Focht, E.}, \bibinfo{author}{Casas, M.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{{HPCG} on long-vector architectures: Evaluation and optimization on {NEC SX-Aurora and RISC-V}}.
\newblock \bibinfo{journal}{Future Generation Computer Systems} \bibinfo{volume}{143}, \bibinfo{pages}{152--162}.
\newblock \DOIprefix\doi{10.1016/j.future.2023.01.015}.
%Type = Article
\bibitem[{Goto and Geijn(2008)}]{c46}
\bibinfo{author}{Goto, K.}, \bibinfo{author}{Geijn, R.A.v.d.}, \bibinfo{year}{2008}.
\newblock \bibinfo{title}{Anatomy of high-performance matrix multiplication}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{34}.
\newblock \DOIprefix\doi{10.1145/1356052.1356053}.
%Type = Article
\bibitem[{Goto and Van De~Geijn(2008)}]{c47}
\bibinfo{author}{Goto, K.}, \bibinfo{author}{Van De~Geijn, R.}, \bibinfo{year}{2008}.
\newblock \bibinfo{title}{High-performance implementation of the level-3 {BLAS}}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{35}.
\newblock \DOIprefix\doi{10.1145/1377603.1377607}.
%Type = Inproceedings
\bibitem[{Gupta et~al.(2023)Gupta, Papadopoulou and Peric\`{a}s}]{c10}
\bibinfo{author}{Gupta, S.R.}, \bibinfo{author}{Papadopoulou, N.}, \bibinfo{author}{Peric\`{a}s, M.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Challenges and opportunities in the {Co-design} of convolutions and {RISC-V} vector processors}, in: \bibinfo{booktitle}{Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{1550–1556}.
\newblock \DOIprefix\doi{10.1145/3624062.3624232}.
%Type = Inproceedings
\bibitem[{Haque et~al.(2023)Haque, Choudhury and Hossain}]{c35}
\bibinfo{author}{Haque, S.A.}, \bibinfo{author}{Choudhury, N.}, \bibinfo{author}{Hossain, S.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Matrix multiplication with diagonals: Structured sparse matrices and beyond}, in: \bibinfo{booktitle}{Proceedings of the 2023 7th International Conference on High Performance Compilation, Computing and Communications}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{69–76}.
\newblock \DOIprefix\doi{10.1145/3606043.3606053}.
%Type = Article
\bibitem[{Haque et~al.(2024)Haque, Parvez and Hossain}]{c36}
\bibinfo{author}{Haque, S.A.}, \bibinfo{author}{Parvez, M.T.}, \bibinfo{author}{Hossain, S.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{{GPU} algorithms for structured sparse matrix multiplication with diagonal storage schemes}.
\newblock \bibinfo{journal}{Algorithms} \bibinfo{volume}{17}.
\newblock \DOIprefix\doi{10.3390/a17010031}.
%Type = Inproceedings
\bibitem[{Igual et~al.(2023)Igual, Pi\~{n}uel, Catal\'{a}n, Mart\'{\i}nez, Castell\'{o} and Quintana-Ort\'{\i}}]{c55}
\bibinfo{author}{Igual, F.}, \bibinfo{author}{Pi\~{n}uel, L.}, \bibinfo{author}{Catal\'{a}n, S.}, \bibinfo{author}{Mart\'{\i}nez, H.}, \bibinfo{author}{Castell\'{o}, A.}, \bibinfo{author}{Quintana-Ort\'{\i}, E.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Automatic generation of micro-kernels for performance portability of matrix multiplication on {RISC-V} vector processors}, in: \bibinfo{booktitle}{Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{1523–1532}.
\newblock \DOIprefix\doi{10.1145/3624062.3624229}.
%Type = Misc
\bibitem[{Intel(2025)}]{c37}
\bibinfo{author}{Intel}, \bibinfo{year}{2025}.
\newblock \bibinfo{title}{{Intel oneAPI Math Kernel Library Reference Manual}}.
\newblock \bibinfo{howpublished}{\url{https://www.intel.com/content/ www/us/en/developer/tools/oneapi/onemkl.html}}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Misc
\bibitem[{Jacob and Guennebaud(2021)}]{c41}
\bibinfo{author}{Jacob, B.}, \bibinfo{author}{Guennebaud, G.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{Eigen}.
\newblock \bibinfo{howpublished}{https://eigen.tuxfamily.org/index.php}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Misc
\bibitem[{Kozinov et~al.(2024)Kozinov, Vasiliev, Gorshkov, Kustikova, Maklaev, Volokitin and Meyerov}]{c21}
\bibinfo{author}{Kozinov, E.}, \bibinfo{author}{Vasiliev, E.}, \bibinfo{author}{Gorshkov, A.}, \bibinfo{author}{Kustikova, V.}, \bibinfo{author}{Maklaev, A.}, \bibinfo{author}{Volokitin, V.}, \bibinfo{author}{Meyerov, I.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Vectorization of gradient boosting of decision trees prediction in the {CatBoost} library for {RISC-V} processors}.
\newblock \URLprefix \url{https://arxiv.org/abs/2405.11062}, \href{http://arxiv.org/abs/2405.11062}{{\tt arXiv:2405.11062}}.
%Type = Article
\bibitem[{K\r{a}gstr\"{o}m et~al.(1998)K\r{a}gstr\"{o}m, Ling and van Loan}]{c48}
\bibinfo{author}{K\r{a}gstr\"{o}m, B.}, \bibinfo{author}{Ling, P.}, \bibinfo{author}{van Loan, C.}, \bibinfo{year}{1998}.
\newblock \bibinfo{title}{{GEMM-based level 3 BLAS}: high-performance model implementations and performance evaluation benchmark}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{24}, \bibinfo{pages}{268–302}.
\newblock \DOIprefix\doi{10.1145/292395.292412}.
%Type = Article
\bibitem[{Lawson et~al.(1979)Lawson, Hanson, Kincaid and Krogh}]{c27}
\bibinfo{author}{Lawson, C.L.}, \bibinfo{author}{Hanson, R.J.}, \bibinfo{author}{Kincaid, D.R.}, \bibinfo{author}{Krogh, F.T.}, \bibinfo{year}{1979}.
\newblock \bibinfo{title}{Basic linear algebra subprograms for fortran usage}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{5}, \bibinfo{pages}{308–323}.
\newblock \DOIprefix\doi{10.1145/355841.355847}.
%Type = Inproceedings
\bibitem[{Lee et~al.(2023)Lee, Jamieson, Brown and Jesus}]{c16}
\bibinfo{author}{Lee, J.K.L.}, \bibinfo{author}{Jamieson, M.}, \bibinfo{author}{Brown, N.}, \bibinfo{author}{Jesus, R.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Test-driving {RISC-V} vector hardware for {HPC}}, in: \bibinfo{editor}{Bienz, A.}, \bibinfo{editor}{Weiland, M.}, \bibinfo{editor}{Baboulin, M.}, \bibinfo{editor}{Kruse, C.} (Eds.), \bibinfo{booktitle}{High Performance Computing}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{419--432}.
%Type = Article
\bibitem[{Leiserson et~al.(2020)Leiserson, Thompson, Emer, Kuszmaul, Lampson, Sanchez and Schardl}]{c63}
\bibinfo{author}{Leiserson, C.E.}, \bibinfo{author}{Thompson, N.C.}, \bibinfo{author}{Emer, J.S.}, \bibinfo{author}{Kuszmaul, B.C.}, \bibinfo{author}{Lampson, B.W.}, \bibinfo{author}{Sanchez, D.}, \bibinfo{author}{Schardl, T.B.}, \bibinfo{year}{2020}.
\newblock \bibinfo{title}{There’s plenty of room at the top: What will drive computer performance after {Moore’s} law?}
\newblock \bibinfo{journal}{Science} \bibinfo{volume}{368}, \bibinfo{pages}{eaam9744}.
\newblock \DOIprefix\doi{10.1126/science.aam9744}.
%Type = Article
\bibitem[{Madsen et~al.(1976)Madsen, Rodrigue and Karush}]{c30}
\bibinfo{author}{Madsen, N.K.}, \bibinfo{author}{Rodrigue, G.H.}, \bibinfo{author}{Karush, J.I.}, \bibinfo{year}{1976}.
\newblock \bibinfo{title}{Matrix multiplication by diagonals on a vector/parallel processor}.
\newblock \bibinfo{journal}{Information Processing Letters} \bibinfo{volume}{5}, \bibinfo{pages}{41--45}.
\newblock \DOIprefix\doi{10.1016/0020-0190(76)90077-6}.
%Type = Inproceedings
\bibitem[{Mantovani et~al.(2023)Mantovani, Vizcaino, Banchelli, Garcia-Gasulla, Ferrer, Ieronymakis, Dimou, Papaefstathiou and Labarta}]{c7}
\bibinfo{author}{Mantovani, F.}, \bibinfo{author}{Vizcaino, P.}, \bibinfo{author}{Banchelli, F.}, \bibinfo{author}{Garcia-Gasulla, M.}, \bibinfo{author}{Ferrer, R.}, \bibinfo{author}{Ieronymakis, G.}, \bibinfo{author}{Dimou, N.}, \bibinfo{author}{Papaefstathiou, V.}, \bibinfo{author}{Labarta, J.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Software development vehicles to enable extended and early co-design: A {RISC-V} and {HPC} case of study}, in: \bibinfo{editor}{Bienz, A.}, \bibinfo{editor}{Weiland, M.}, \bibinfo{editor}{Baboulin, M.}, \bibinfo{editor}{Kruse, C.} (Eds.), \bibinfo{booktitle}{High Performance Computing}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{526--537}.
%Type = Article
\bibitem[{Matsumoto et~al.(2011)Matsumoto, Nakasato, Sakai, Yahagi and Sedukhin}]{c52}
\bibinfo{author}{Matsumoto, K.}, \bibinfo{author}{Nakasato, N.}, \bibinfo{author}{Sakai, T.}, \bibinfo{author}{Yahagi, H.}, \bibinfo{author}{Sedukhin, S.G.}, \bibinfo{year}{2011}.
\newblock \bibinfo{title}{Multi-level optimization of matrix multiplication for {GPU}-equipped systems}.
\newblock \bibinfo{journal}{Procedia Computer Science} \bibinfo{volume}{4}, \bibinfo{pages}{342--351}.
\newblock \DOIprefix\doi{10.1016/j.procs.2011.04.036}. \bibinfo{note}{proceedings of the International Conference on Computational Science, ICCS 2011}.
%Type = Article
\bibitem[{Mezger et~al.(2022)Mezger, Santos, Dilillo, Zeferino and Melo}]{c5}
\bibinfo{author}{Mezger, B.W.}, \bibinfo{author}{Santos, D.A.}, \bibinfo{author}{Dilillo, L.}, \bibinfo{author}{Zeferino, C.A.}, \bibinfo{author}{Melo, D.R.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{A survey of the {RISC-V} architecture software support}.
\newblock \bibinfo{journal}{IEEE Access} \bibinfo{volume}{10}, \bibinfo{pages}{51394--51411}.
\newblock \DOIprefix\doi{10.1109/ACCESS.2022.3174125}.
%Type = Misc
\bibitem[{Netlib(2025)}]{c59}
\bibinfo{author}{Netlib}, \bibinfo{year}{2025}.
\newblock \bibinfo{title}{{ Reference BLAS}}.
\newblock \bibinfo{howpublished}{https://www.netlib.org/blas/}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Inproceedings
\bibitem[{Perez et~al.(2021)Perez, Fell and Davis}]{c8}
\bibinfo{author}{Perez, B.}, \bibinfo{author}{Fell, A.}, \bibinfo{author}{Davis, J.D.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{Coyote: An open source simulation tool to enable {RISC-V} in {HPC}}, in: \bibinfo{booktitle}{2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)}, pp. \bibinfo{pages}{130--135}.
\newblock \DOIprefix\doi{10.23919/DATE51398.2021.9474080}.
%Type = Article
\bibitem[{Ram{\'i}rez et~al.(2022)Ram{\'i}rez, Castell{\'o} and Quintana-Ort{\'i}}]{c54}
\bibinfo{author}{Ram{\'i}rez, C.}, \bibinfo{author}{Castell{\'o}, A.}, \bibinfo{author}{Quintana-Ort{\'i}, E.S.}, \bibinfo{year}{2022}.
\newblock \bibinfo{title}{A {BLIS}-like matrix multiplication for machine learning in the {RISC-V ISA-based GAP8} processor}.
\newblock \bibinfo{journal}{The Journal of Supercomputing} \bibinfo{volume}{78}, \bibinfo{pages}{18051--18060}.
\newblock \DOIprefix\doi{10.1007/s11227-022-04581-6}.
%Type = Misc
\bibitem[{Rautila(2021)}]{c43}
\bibinfo{author}{Rautila, H.}, \bibinfo{year}{2021}.
\newblock \bibinfo{title}{Armas}.
\newblock \bibinfo{howpublished}{https://github.com/hrautila/armas}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Inproceedings
\bibitem[{Rem{\'o}n et~al.(2008)Rem{\'o}n, Quintana-Ort{\'i} and Quintana-Ort{\'i}}]{c33}
\bibinfo{author}{Rem{\'o}n, A.}, \bibinfo{author}{Quintana-Ort{\'i}, E.S.}, \bibinfo{author}{Quintana-Ort{\'i}, G.}, \bibinfo{year}{2008}.
\newblock \bibinfo{title}{The implementation of {BLAS} for band matrices}, in: \bibinfo{editor}{Wyrzykowski, R.}, \bibinfo{editor}{Dongarra, J.}, \bibinfo{editor}{Karczewski, K.}, \bibinfo{editor}{Wasniewski, J.} (Eds.), \bibinfo{booktitle}{Parallel Processing and Applied Mathematics}, \bibinfo{publisher}{Springer Berlin Heidelberg}, \bibinfo{address}{Berlin, Heidelberg}. pp. \bibinfo{pages}{668--677}.
%Type = Inproceedings
\bibitem[{Rodrigues et~al.(2023)Rodrigues, Sousa and Ilic}]{c9}
\bibinfo{author}{Rodrigues, A.}, \bibinfo{author}{Sousa, L.}, \bibinfo{author}{Ilic, A.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Performance modelling-driven optimization of {RISC-V} hardware for efficient {SpMV}}, in: \bibinfo{editor}{Bienz, A.}, \bibinfo{editor}{Weiland, M.}, \bibinfo{editor}{Baboulin, M.}, \bibinfo{editor}{Kruse, C.} (Eds.), \bibinfo{booktitle}{High Performance Computing}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{486--499}.
%Type = Misc
\bibitem[{{Science of High-Performance Computing (SHPC) group}(2024)}]{c39}
\bibinfo{author}{{Science of High-Performance Computing (SHPC) group}}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{{AOCL-BLAS}}.
\newblock \bibinfo{howpublished}{https://github.com/amd/blis}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Inproceedings
\bibitem[{Smith et~al.(2014)Smith, Geijn, Smelyanskiy, Hammond and Zee}]{c49}
\bibinfo{author}{Smith, T.M.}, \bibinfo{author}{Geijn, R.v.d.}, \bibinfo{author}{Smelyanskiy, M.}, \bibinfo{author}{Hammond, J.R.}, \bibinfo{author}{Zee, F.G.V.}, \bibinfo{year}{2014}.
\newblock \bibinfo{title}{Anatomy of high-performance many-threaded matrix multiplication}, in: \bibinfo{booktitle}{2014 IEEE 28th International Parallel and Distributed Processing Symposium}, pp. \bibinfo{pages}{1049--1059}.
\newblock \DOIprefix\doi{10.1109/IPDPS.2014.110}.
%Type = Article
\bibitem[{Su{\'a}rez et~al.(2024)Su{\'a}rez, Almeida and Blanco}]{c15}
\bibinfo{author}{Su{\'a}rez, D.}, \bibinfo{author}{Almeida, F.}, \bibinfo{author}{Blanco, V.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Comprehensive analysis of energy efficiency and performance of {ARM} and {RISC-V SoCs}}.
\newblock \bibinfo{journal}{The Journal of Supercomputing} \bibinfo{volume}{80}, \bibinfo{pages}{12771--12789}.
\newblock \DOIprefix\doi{10.1007/s11227-024-05946-9}.
%Type = Article
\bibitem[{Tang et~al.(2024)Tang, Qi, Lu and Jiang}]{c58}
\bibinfo{author}{Tang, T.}, \bibinfo{author}{Qi, H.}, \bibinfo{author}{Lu, Q.}, \bibinfo{author}{Jiang, H.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Multithreaded reproducible banded matrix-vector multiplication}.
\newblock \bibinfo{journal}{Mathematics} \bibinfo{volume}{12}.
\newblock \DOIprefix\doi{10.3390/math12030422}.
%Type = Inproceedings
\bibitem[{Tsao and Turnbull(1993)}]{c31}
\bibinfo{author}{Tsao, A.}, \bibinfo{author}{Turnbull, T.}, \bibinfo{year}{1993}.
\newblock \bibinfo{title}{A comparison of algorithms for banded matrix multiplication}, in: \bibinfo{booktitle}{Supercomputing Research Center.}
\newblock \URLprefix \url{https://api.semanticscholar.org/ CorpusID:15110293}.
%Type = Misc
\bibitem[{{University of Nizhny Novgorod, ITMM}(2025)}]{c67}
\bibinfo{author}{{University of Nizhny Novgorod, ITMM}}, \bibinfo{year}{2025}.
\newblock \bibinfo{title}{Code for optimized {RISC-V} versions}.
\newblock \bibinfo{howpublished}{https://github.com/UNN-ITMM-Software/OpenBLAS/tree/band\_matrix\_RVV\_improving}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Article
\bibitem[{Van~Zee and van~de Geijn(2015)}]{c62}
\bibinfo{author}{Van~Zee, F.G.}, \bibinfo{author}{van~de Geijn, R.A.}, \bibinfo{year}{2015}.
\newblock \bibinfo{title}{{BLIS}: A framework for rapidly instantiating {BLAS} functionality}.
\newblock \bibinfo{journal}{ACM Trans. Math. Softw.} \bibinfo{volume}{41}.
\newblock \DOIprefix\doi{10.1145/2764454}.
%Type = Inproceedings
\bibitem[{Vizcaino et~al.(2023)Vizcaino, Ieronymakis, Dimou, Papaefstathiou, Labarta and Mantovani}]{c17}
\bibinfo{author}{Vizcaino, P.}, \bibinfo{author}{Ieronymakis, G.}, \bibinfo{author}{Dimou, N.}, \bibinfo{author}{Papaefstathiou, V.}, \bibinfo{author}{Labarta, J.}, \bibinfo{author}{Mantovani, F.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Short reasons for long vectors in {HPC CPUs}: A study based on {RISC-V}}, in: \bibinfo{booktitle}{Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis}, \bibinfo{publisher}{Association for Computing Machinery}, \bibinfo{address}{New York, NY, USA}. p. \bibinfo{pages}{1543–1549}.
\newblock \DOIprefix\doi{10.1145/3624062.3624231}.
%Type = Inproceedings
\bibitem[{Vizcaino et~al.(2024)Vizcaino, Labarta and Mantovani}]{c23}
\bibinfo{author}{Vizcaino, P.}, \bibinfo{author}{Labarta, J.}, \bibinfo{author}{Mantovani, F.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Graph computing on long vector architectures {(Yes, It Works!)}}, in: \bibinfo{booktitle}{2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, pp. \bibinfo{pages}{986--995}.
\newblock \DOIprefix\doi{10.1109/IPDPSW63119.2024.00169}.
%Type = Inproceedings
\bibitem[{Volokitin et~al.(2023)Volokitin, Kozinov, Kustikova, Liniov and Meyerov}]{c13}
\bibinfo{author}{Volokitin, V.}, \bibinfo{author}{Kozinov, E.}, \bibinfo{author}{Kustikova, V.}, \bibinfo{author}{Liniov, A.}, \bibinfo{author}{Meyerov, I.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Case study for running memory-bound kernels on {RISC-V CPUs}}, in: \bibinfo{editor}{Malyshkin, V.} (Ed.), \bibinfo{booktitle}{Parallel Computing Technologies}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{51--65}.
%Type = Article
\bibitem[{Volokitin et~al.(2024)Volokitin, Vasiliev, Kozinov, Kustikova, Liniov, Rodimkov, Sysoyev and Meyerov}]{c22}
\bibinfo{author}{Volokitin, V.D.}, \bibinfo{author}{Vasiliev, E.P.}, \bibinfo{author}{Kozinov, E.A.}, \bibinfo{author}{Kustikova, V.D.}, \bibinfo{author}{Liniov, A.V.}, \bibinfo{author}{Rodimkov, Y.A.}, \bibinfo{author}{Sysoyev, A.V.}, \bibinfo{author}{Meyerov, I.B.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Improved vectorization of {OpenCV} algorithms for {RISC-V CPUs}}.
\newblock \bibinfo{journal}{Lobachevskii Journal of Mathematics} \bibinfo{volume}{45}, \bibinfo{pages}{130--142}.
\newblock \DOIprefix\doi{10.1134/S1995080224010530}.
%Type = Misc
\bibitem[{Xianyi(2012)}]{c61}
\bibinfo{author}{Xianyi, Z.}, \bibinfo{year}{2012}.
\newblock \bibinfo{title}{Blas-tester}.
\newblock \bibinfo{howpublished}{https://github.com/xianyi/BLAS-Tester}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Misc
\bibitem[{Xianyi and Kroeker(2025)}]{c40}
\bibinfo{author}{Xianyi, Z.}, \bibinfo{author}{Kroeker, M.}, \bibinfo{year}{2025}.
\newblock \bibinfo{title}{Openblas}.
\newblock \bibinfo{howpublished}{https://github.com/OpenMathLib/OpenBLAS}.
\newblock \bibinfo{note}{Last accessed 2025/02/07}.
%Type = Article
\bibitem[{Zhang et~al.(2024)Zhang, Zhou, Zhang, Ma and Gong}]{c25}
\bibinfo{author}{Zhang, J.}, \bibinfo{author}{Zhou, J.}, \bibinfo{author}{Zhang, X.}, \bibinfo{author}{Ma, D.}, \bibinfo{author}{Gong, C.}, \bibinfo{year}{2024}.
\newblock \bibinfo{title}{Fine-grained vectorized merge sorting on {RISC-V}: from register to cache}.
\newblock \bibinfo{journal}{CCF Transactions on High Performance Computing} \DOIprefix\doi{10.1007/s42514-024-00201-2}.
%Type = Inproceedings
\bibitem[{Zhao et~al.(2023)Zhao, Zhang and Zhang}]{c24}
\bibinfo{author}{Zhao, X.}, \bibinfo{author}{Zhang, X.}, \bibinfo{author}{Zhang, Y.}, \bibinfo{year}{2023}.
\newblock \bibinfo{title}{Optimization of the {FFT} algorithm on {RISC-V CPUs}}, in: \bibinfo{editor}{Bienz, A.}, \bibinfo{editor}{Weiland, M.}, \bibinfo{editor}{Baboulin, M.}, \bibinfo{editor}{Kruse, C.} (Eds.), \bibinfo{booktitle}{High Performance Computing}, \bibinfo{publisher}{Springer Nature Switzerland}, \bibinfo{address}{Cham}. pp. \bibinfo{pages}{515--525}.
%Type = Article
\bibitem[{Zlatev et~al.(1988)Zlatev, Vu, Wasniewski and Schaumburg}]{c32}
\bibinfo{author}{Zlatev, Z.}, \bibinfo{author}{Vu, P.}, \bibinfo{author}{Wasniewski, J.}, \bibinfo{author}{Schaumburg, K.}, \bibinfo{year}{1988}.
\newblock \bibinfo{title}{Computations with symmetric, positive definite and band matrices on a parallel vector processor}.
\newblock \bibinfo{journal}{Parallel Computing} \bibinfo{volume}{8}, \bibinfo{pages}{301--312}.
\newblock \DOIprefix\doi{10.1016/0167-8191(88)90134-2}. \bibinfo{note}{proceedings of the International Conference on Vector and Parallel Processors in Computational Science III}.

\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.


