\section{Related Work}
\label{sec2}

The BLAS standard was proposed in 1979 ____ and was updated until the early 2000s
____. Dense linear algebra algorithms are the computational kernels of many libraries used in scientific and engineering applications, so a large number of works are devoted to the issues of high-performance implementation of BLAS for different architectures. There is a lot of attention in the community to optimizing the general matrix multiplication (GEMM) algorithm. This algorithm is one of the main computational kernels in many Computational Science problems. Firstly, other BLAS level 3 functions can be implemented using this algorithm ____. Secondly, GEMM, along with other BLAS algorithms, plays an important role in other matrix algorithms in the LAPACK computational linear algebra library ____. The most common approach to developing a high-performance GEMM implementation is described in ____. The main idea is based on splitting the original matrix into cache-sized blocks and allocating CPU-specific low-level microkernels implemented with intrinsics or assembler. This approach was implemented in GotoBLAS and was then used in many other libraries. Optimization of other BLAS level 3 functions in GotoBLAS is described in ____. The specifics of BLAS implementation using isolated microkernels in the BLIS library are described in ____. For many years, high-performance implementations of GEMM and other BLAS functions have been developed by Intel in the Intel Math Kernel Library (MKL) ____. High-performance GEMM implementations for multicore processors are discussed in ____. With the advent of the first GPUs, approaches to GEMM optimization for graphics processors began to actively develop ____, where a fairly high degree of utilization of computing resources was also achieved. 

Optimization of vector and matrix operations for RISC-V processors is often discussed in the community in terms of improving the performance of neural network inference. In ____, the adaptation of the integer GEMM BLIS algorithm for these purposes is proposed, and its performance on the Greenwave GAP8 processor with hardware support of dot product kernel is studied. In ____, vectorized GEMM microkernels for RISC-V processors are generated automatically, and their performance compared to that of OpenBLAS implementations on C906 and C910 processors is evaluated. Few works have been done on optimizing algorithms for band matrices. The optimization of the band matrix-matrix multiplication algorithm has been considered in ____. In early works ____ a matrix storage scheme based on diagonals is proposed, which differs from those used in BLAS and LAPACK, and adapted the band matrix multiplication algorithm for it. An important step towards vector computations for algorithms working with band matrices on the CRAY X-MP computer was made in ____. In ____, the functions of multiplying a symmetric band matrix by a vector (SBMV) and by a general matrix (SBMM) were considered. The authors proposed to split the original band matrix into dense blocks and reduce the computations to a sequence of operations with dense matrices (symmetric, general and triangular matrix-vector product for SBMV and symmetric, general, and triangular matrix-matrix product for SBMM). This approach is not always effective in the case under consideration, since we optimize the operation of OpenBLAS for matrices with a narrow bandwidth. In ____, this approach was adapted for use on GPUs, and the symmetric band matrix storage scheme was modified to reduce the number of function calls from BLAS level 2. In ____, a compact diagonal band matrix storage scheme and the corresponding band matrix-matrix multiplication algorithm for CPUs ____ and GPUs ____ are proposed. In ____, a multithreaded algorithm for band matrix-vector multiplication is presented, which guarantees the same computational accuracy for multiple runs of the application.

With the advent of the new RISC-V architecture, the issue of the basic algorithms implementations efficiency, taking into account the requirements and limitations of the BLAS specifications, is once again becoming relevant. These include algorithms for working with band matrices. To study this, it was necessary to choose one of the BLAS implementations optimized for RISC-V processors with RVV 0.7.1 or RVV 1.0 extensions. Currently, there are many open source and commercial implementations available with low-level optimization for various architectures. Some libraries are optimized for specific types of processors, while others have implementations for a wide range of architectures. Thus, the free software package Intel OneAPI Math Kernel Library ____ is widely used for Intel processors, the AOCL-BLAS package ____ optimized for AMD processors and IBM ESSL libraries optimized for IBM processors are also popular. Open packages such as ATLAS ____, ARMAS ____, BLIS ____, Eigen ____, and OpenBLAS ____ are optimized for many different platforms and are widely used in scientific research. One of the newest libraries, BLASFEO ____, is optimized for embedded systems and provides a high-performance BLAS implementation for cache-fitting matrices. The Netlib implementation is often used as the baseline reference implementation for BLAS on x86 and some other platforms ____. For GPUs, libraries such as cuBLAS, clBLAST, ViennaCL are commonly used. The BLIS, OpenBLAS, and Eigen libraries have low-level computational kernels for a wide range of processors, including ARM, IBM Power, MIPS (OpenBLAS, Eigen), but vectorization for RISC-V processors is only partially supported. A comparison of various BLAS implementations on ARM and RISC-V processors as of 2020 is given in ____. The BLIS library implements vectorization for SiFive x280 processors (August 2024). The latest release of the Eigen library does not support RISC-V, but a merge request with support of RVV 1.0 instructions (September 2024) is added. Compared to other libraries, OpenBLAS has the most complete vectorization for RISC-V processors and low-level computational kernels with RVV 0.7.1 and RVV 1.0 support for several types of processors. Therefore, we chose it as a baseline implementation, studied the performance of the developed algorithms and identified functions that had potential for further improvement.