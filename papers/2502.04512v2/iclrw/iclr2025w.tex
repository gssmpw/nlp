
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{newfloat}
\usepackage{wrapfig}

\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{graphicx}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{diagbox}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}         % colors
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{bbm}


\title{Safety is Essential for Responsible Open-Ended Systems}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    AI advancements have been significantly driven by a combination of foundation models and curiosity-driven learning aimed at increasing capability and adaptability. A growing area of interest within this field is Open-Endedness — the ability of AI systems to continuously and autonomously generate novel and diverse artifacts or solutions. This has become relevant for accelerating scientific discovery and enabling continual adaptation in AI agents. This position paper argues that the inherently dynamic and self-propagating nature of Open-Ended AI introduces significant, underexplored risks, including challenges in maintaining alignment, predictability, and control. This paper systematically examines these challenges, proposes mitigation strategies, and calls for action for different stakeholders to support the safe, responsible and successful development of Open-Ended AI. 
\end{abstract}

\section{Introduction}



Artificial Intelligence (AI) has achieved remarkable progress driven by foundation models~\cite{bommasani2021opportunities}. Across various modalities, these models have shown incredible performance in tasks for which they were designed~\cite{ramesh2021zero, rombach2022high, achiam2023gpt, radford2023robust, brooks2024video}. However, they are not yet capable of autonomously and indefinitely producing new creative, interesting and diverse discoveries.
Such open-ended discovery is key to making progress on problems that cannot be solved by simply following a specified objective. Indeed humans use such open-ended processes to accumulate knowledge and solve difficult problems. Thus, it has been argued that open-endedness is a key ingredient for Artificial Superintelligence~\cite{stanley2019open, team2021open, jiang2023general, nisioti2024text, hughes2024open}, which could outperform humans at a wide range of tasks~\cite{morrisposition}.

\begin{wrapfigure}{r}{0.45\linewidth}
\vspace{-3mm}
\centering
    \includegraphics[width=0.44\textwidth]{figures/OE3.pdf}
    \caption{Open-Ended (OE) AI generates novel artifacts over time, potentially co-evolving with environments and societal values to drive creativity and progress. However, this \textit{position paper} argues that its unpredictability, difficulty in control, and cascading misalignment pose catastrophic risks to societal and global stability.} 
    \label{fig:teaser}
    \vspace{-6mm}
\end{wrapfigure} 

Specifically, Open-Ended (OE) AI continuously produces artifacts that are novel and learnable to humans. This enables it to generate new, complex, creative, and adaptive solutions over time~\cite{soros2014identifying,stanley2017open, clune2019ai,sigaud2023definition,lu2024ai,akiba2024evolutionary}. Unlike traditional AI systems that optimize for fixed objectives, OE AI perpetually explores new solutions and adapts to changing circumstances without being given an explicit goal.

There is a large diversity of systems that aim to be open-ended. 
The Paired Open-Ended Trailblazer (POET) \cite{wang2019paired} facilitates OE exploration by co-evolving environments and agents. The environments become increasingly diverse and complex based on the weaknesses of the agent, while the agent develops solutions that may transfer across environments. The Voyager method \cite{wang2023voyager} is an LLM-powered embodied agent for lifelong learning in Minecraft. It utilizes an automatic curriculum for OE exploration, a skill library to store and retrieve complex behaviors, and an iterative prompting mechanism incorporating feedback and self-verification to refine executable actions. 

Historically, it has been a challenge to guide the exploration of OE AI toward artifacts that are novel and interesting to humans, but recently Large Language Models (LLMs) have been applied to accelerate this process. Since LLMs have been trained on large amounts of human data, they have built an understanding of what is interesting and desirable to humans. Recent work has leveraged LLMs as backbones for OE evolution and exploration~\cite{lehman2023evolution,zammit2024map,aki2024llm}. This opens up many beneficial applications for OE AI. LLMs have shown emergent behaviors in OE scientific discovery~\cite{lu2024ai}, navigating novel environments~\cite{wang2023voyager}, and eliciting truthful answers from LLMs~\cite{khan2024debating}. However, with the growing interest and potentially large-scale application of OE AI, we must evaluate and address the risks coming from these systems.

\textbf{While OE AI offers significant potential, it poses unique and substantial risks that must be addressed for a safe and responsible deployment. Its inherent unpredictability and uncontrollability necessitate dedicated research to ensure safety and alignment with societal values.}

While discussions on AI safety are broadly relevant, this paper focuses on the unique safety challenges posed by OE AI. Previously, \citet{hughes2024open} and \citet{ecoffet2020open} have touched on these. However, this paper offers a deeper, more comprehensive, and up-to-date overview of the safety challenges in OE AI and suggests concrete research directions and actions to address them. 


We first define OE AI and argue that its safety depends on our ability to systematically identify, assess, and mitigate risks (Section~\ref{sec:def}).
Building on this definition we identify that issues such as the unpredictability of future artifacts, the trade-off between creativity and control, and the difficulties of aligning OE AI with human values are key safety risks (Section~\ref{sec:risk}). To address these, we suggest research directions to develop continuously adapting oversight, constraints, and safety evaluations for OE AI (Section~\ref{sec:mitigation}). Lastly, we call for actions from various stakeholders - industry, academic researchers, governments, and funding bodies (Section~\ref{sec:call4action}).

\section{What is Open-Endedness} \label{sec:def}
Defining Open-Endedness remains an ongoing challenge, as no single definition fully captures its scope~\cite{stanley2016role, stanley2017open, stanley2015greatness, lehman2011abandoning}. 
One definition frames OE as generating artifacts that are novel, and learnable for an external observer~\cite{hughes2024open}. %, where novelty is judged relative to the observer's perspective. 
This definition introduces subjectivity, as novelty can be evaluated differently depending on the observer, and excludes systems generating unintelligible artifacts (e.g., TV noise). Another view models OE systems via evolutionary principles, prioritizing diversity and incremental complexity in behaviors or solutions~\cite{packard2019overview}. Such systems autonomously create and solve problems without direct human intervention, mimicking the processes of biological evolution. Another perspective views OE as a search problem characterized by continuous exploration across a vast and evolving state space, generating diverse and increasingly complex solutions without explicit end goals~\cite{sigaud2023definition}. We adopt the definition by \citet{hughes2024open}, which frames OE as generating novel and learnable artifacts to an external observer. This is particularly suited for ML contexts and facilitates a structured approach to identifying risks w.r.t. the observer incurred by the evolving nature. 

\paragraph{Definition}

\\

\textit{An open-ended AI system is one that continuously generates artifacts that are novel and learnable for an observer.}


Consider a system $S$ that generates a sequence of artifacts $A_{1:t}$ indexed by time $t$, where each artifact resides within a state space $\mathcal{A}$. The observer $O$ has a model $M_t$ that has observed a sequence of artifacts $A_{1:t}$ up until $t$. $M_t$ is a proxy for the observer's prediction capability. The observer judges the quality of $M_t$ by a loss function $\mathcal{L}(M_t, A_{t^\prime})$, where $A_{t^\prime}$ is an artifact generated in future, $t^\prime > t$. 


Borrowing from \citet{hughes2024open} we consider a system to display \textbf{novelty} if it produces artifacts that become progressively less predictable as time advances. Formally:
\begin{equation}\label{def:novelty}
    \forall t<t^\prime \ \ \exists t^*>t^\prime: \mathbb{E}[\mathcal{L}(M_t,A_{t^\prime})] < \mathbb{E}[\mathcal{L}(M_t,A_{t^*})]
\end{equation}

This means for a static observer there will always be an artifact in the future that is worse at getting predicted. This ensures the system keeps generating outputs that introduce new and less predictable information over time.
 
OE AI is \textbf{learnable} if incorporating a longer history of artifacts improves the observer’s ability to predict future outputs. This is formalized as:
\begin{equation}\label{def:learnability}
    \forall t<t^\prime<t^*:
    \mathbb{E}[\mathcal{L}(M_t,A_{t^*})] > \mathbb{E}[\mathcal{L}(M_{t^\prime},A_{t^*})]
\end{equation}

Here, the loss decreases as the observer integrates more past artifacts, indicating improved understanding over time.

In contrast, we use the term ``traditional'' to refer to all AI systems that are not open-ended. This also includes systems that act autonomously or continually adapt, such as LLM agents or RL algorithms, as long as they are not open-ended.

\paragraph{Applications}
OE AI has been proposed as the pathway for agents to evolve skills and knowledge in diverse, rich task environments across infinite horizons, often as a way to achieve ASI~\cite{team2021open, hughes2024open, nisioti2024text}. Systems like REAL-X~\cite{cartoni2020real, cartoni2023real} demonstrate the potential of OE architectures for sensorimotor skill acquisition, where robots autonomously learn how to interact with their environments and generalize these skills to new tasks. OE learning has been applied to games to create evolving game scenarios~\cite{che2024gamegen}. It can serve as a complementary tool in human-led innovation, augmenting creativity by generating a new environment. Genie~\cite{bruce2024genie} produces an OE array of unique, action-controllable virtual worlds from various prompts. \citet{lu2024ai} demonstrated the potential of using LLMs in an OE setting to follow the scientific discovery paradigm: from hypothesis to paper generation. Finally, there is a stream of work that uses the MAP-Elites framework~\cite{mouret2015illuminating} to generate diverse adversarial prompts to improve model robustness via iterative adversarial fine-tuning~\cite{samvelyan2024rainbow, deep2024ferret, han2024ruby}.  

\paragraph{Safety of Open-Ended AI}
Several definitions of safety exist, originating from domains with a long history of safety research, such as aerospace, healthcare, and critical infrastructure~\cite{suyama2005,kafka2012}. In AI, safety aims to prevent AIs from being used to cause harm or themselves causing harm. Thus safety for AI is often tied to error-based definitions, where safety violations occur due to identifiable faults or deviations from intended behavior. However, applying these definitions to OE AI presents unique challenges. For OE AI, which evolves unpredictably and generates novel outputs, errors cannot be predefined as it operates beyond the boundaries of prior design specifications. As a result, error-based definitions of safety are inapplicable to OE. Instead, we adopt a risk management perspective to define safety for OE AI~\cite{leveson2012}. Here, safety is \textit{the ability to systematically identify, assess, and mitigate risks}, even when the system’s artifacts are novel. This definition implies that under high-stakes scenarios, the absence of risk management itself is a risk.

%

\section{Challenges and Risks}
OE AI exhibits emergent behavior, where outputs may deviate significantly from expectations due to vast input spaces, complex internal dynamics, or adaptation to changing conditions. They may develop unsafe, unethical, or misaligned behaviors. We discuss their inherent unpredictability challenges, trade-offs, difficulty to control, and broader consequential societal factors. 

\label{sec:risk}
% \subsection{Tradeoffs}

\subsection{Unpredictability}
\label{subsec:unpredictability}

OE AI is necessarily unpredictable, due to its propensity for generating novel artifacts. As artifacts become increasingly novel they become even more unpredictable. Imagine an OE system $S$ that produces increasingly novel scientific discoveries $A \in \mathcal{A}$. Some of these artifacts, e.g., the recipe for a novel, dangerous viruses, are unsafe. However, when starting to run this system at time $t$ it will be difficult for us to foresee which discoveries it will produce at a later time $t^\prime$ and predict their safety. 

%Indeed OE systems are necessarily unpredictable, due to their propensity for generating novel artifacts.
Formally, we assume that a lower loss of the model on an artifact corresponds to a higher probability of predicting that artifact: $\mathcal{L}(M_t,A_{t^\prime}) < \mathcal{L}(M_t,A_{t^*})  = P_{M_t}(A_{t^\prime}) > P_{M_t}(A_{t^*})$, with $P_{M_t}(a)$ denoting the probability the model puts on artifact $a$. This assumption holds for loss functions such as Cross-Entropy. 
From this, it becomes clear that the novelty definition (Definition \ref{def:novelty}) implies that there is always a more unpredictable artifact that will be generated in the future: $\forall t<t^\prime \ \ \exists t^*>t^\prime: \mathbb{E}[P_{M_t}(A_{t^\prime})] > \mathbb{E}[P_{M_t}(A_{t^*})]$.

Unpredictability makes it difficult %\mjf{truely "impossible"? strong statement. check.} 
for us to anticipate whether trajectories of future artifacts $\{A_t\}^\infty_{t=n}$ will be safe. This undermines our ability to conduct solid risk management, thus, reducing the trust we can put in such a system to behave safely. 

In traditional Reinforcement Learning (RL) the reward function provides a handle to predict future trajectories. RL agents are trained to create trajectories that achieve high rewards on a clearly defined reward function. From this we can derive that highly rewarded trajectories are more likely to be generated than trajectories with low reward. In contrast, OE AI lacks such an objective. %, thus, removing this angle for predictability.
Additionally, the novelty criteria~\cite{lehman2011abandoning} or evolutionary developments~\cite{lehman2010revising,eb5f2d5b0674d0ca67021d79e638f6758a853ebf} in OE AI encourage divergence, making it more complex to anticipate the safety of future artifacts.

% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% OE AI is unpredictable due to the drive for novelty and a lack of an objective. This undermines our ability to guarantee safety and conduct risk management.
% \end{mdframed}





\subsection{Creativity vs. Control}

OE AI creates a fundamental tension between creativity and control in OE search \cite{ecoffet2020open}.

\textbf{Lack of Explicit Guidance.} OE AI often operates without predefined boundaries, constraints, or clear objectives. This allows it to explore vast and uncharted regions of the state space freely and generate creative solutions that are not reachable by simply specifying the desired state. While this promotes novelty and creativity, it makes it difficult to predict or control the direction of the system to ones we deem valuable and safe.

\textbf{Evolving Model and Environment.} Unlike traditional systems, the agent gains new skills and capabilities, generating new artifacts and adapting over time. The evolving nature of the OE AI requires adapting the guidance given to it since the constraints on objectives given earlier might become outdated as the model and its environment change.

% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% OE AI poses trade-offs between creativity and control. This is due to the lack of clear guidance. It also means that any given guidance needs to be constantly adapted.
% \end{mdframed}




\subsection{Misalignment}
The ability to align AI systems with human values is a grand challenge within the field of AI Safety \cite{hendrycks2022unsolvedproblemsmlsafety,ji2024aialignmentcomprehensivesurvey} that is essential for ensuring the safety and usefulness of AI systems. The aim is to align the goals that an AI system intrinsically values and pursues with those of its human designers. This can include intended objectives, ethical guidelines, or safety requirements.
AI alignment is usually formulated for AI systems that optimize an explicit, human-designed reward function. In such a setting misalignment can occur because the reward function does not precisely match the designers' objective \cite{krakovna2020specification} or because the AI internalizes goals that are different from the explicit incentives \cite{shah2022goal,di2022goal}.

However, OE AI does not optimize an explicitly defined reward function with a focus on diversity. %is not given an objective. 
Instead, the designers may provide implicit incentives by structuring the search process in ways that are likely to lead to artifacts that they value highly. This necessitates a different lens for analyzing the alignment of OE AI \cite{ecoffet2020open}.

The designer might not correctly specify their values in the structure of the OE AI or process. The result would be an OE AI being driven towards an undesired goal. OE AI could still learn to intrinsically pursue goals that are different from those specified in the OE process. For example, humans evolved by evolution, which is an OE process whose structure causes it to optimize for inclusive fitness. However, humans do not value inclusive fitness intrinsically but have intrinsic drives towards sugary foods or protected sex.

\textbf{Alignment of Evolving Systems.} Another difference is that the goals pursued by an OE AI can evolve throughout its lifetime, while the goals pursued by a traditional ML system remain static. This means that tests or guarantees about the alignment of an OE AI at one time become outdated as the system keeps evolving. Additionally, as OE AI explores novel situations, we cannot be sure that alignment training performed initially will generalize to new situations.

% this go summarised into the above
% \textit{State-space misalignment.} The state space in an OE system can be constantly evolving~\cite{wang2019paired, wang2020enhanced}. Even if the system is aligned at initialization ($t=0$), state-space evolution can lead to misalignment due to unanticipated transitions into new or poorly understood regions. This occurs because the alignment criteria is not well defined in the new state-spaces, also was difficult to anticipate it.

\textbf{Alignment of Interactive Components.} 
OE AI systems often include multiple components. This might be an LLM with additional components, multiple agents or an agent in an evolving environment. Even though these individual components might be aligned, their dynamic interactions can result in emergent behaviors that are misaligned. For example, in an OE process with multiple agents who do not want to cause harm, incentives and inter-agent dynamics can force them into equilibria where harming others is necessary. Due to the unpredictable nature of each component, predicting such dynamics is not possible. 
% \vspace{-1mm}
% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% A new framing for the alignment of OE AI with human values is necessary since its objectives are not specified, continuously evolve and emerge from the interaction of multiple components.
% \end{mdframed}

\subsection{Traceability}
Tracking and reproducing an OE AI's processes and outcomes generated is a challenging task. %The lack of well-defined training objectives or clear end goals makes it difficult to trace the system's evolution. 
This could be coupled with a negative cascading effect that small changes in artifacts or system states can trigger, causing the system to diverge from its intended trajectory. %This has several implications; 1) it can rapidly propagate and accelerate negative artifacts, 2) makes the system harder to replicate, and 3) it unbound and exacerbates the effect of small changes. 
% For example After initialization with a hand-designed replicator, co-evolution in Tierra~\cite{ray1991approach} proceeds to create co-evolutionary arm races of parasites and hyperparasites. 


\textbf{Lack of Reproducibility.} Reproducing the evolving OE AI at a certain time is significantly more challenging than traditional AI due to 1) the lack of clear training objectives, and 2) not being able to reproduce the intermediate environmental feedback and states~\cite{flageat2023uncertain, flageat2024exploring}, making it hard to trace and attribute the exploration paths. For example, evolving to images that resemble real objects from random initial images is like ``finding needles in a haystack''~\cite{secretan2008picbreeder} given the astronomically large search space. This can hinder the rigorous scientific progress in this domain which requires transparent, open-source, and auditable technologies. 

\textbf{Difficulties in Attribution.} {A research direction that helps enable oversight, and evaluate and improve the correctness of solutions is self-consistency checks. \citet{wang2023selfconsistency} used a prompting strategy that samples a diverse set of reasoning paths and then selects the most consistent answer. \citet{fluri2024evaluating} proposed a framework to evaluate superhuman models by checking if they follow interpretable human rules, e.g., counterfactuals should flip the predicted decisions. Creating similar tests for OE AI is more difficult. One can change the parameters of the initial state of an OE AI to create a counterfactual environment; however, due to compounded cascading effects, the effects of the changed parameters cannot be easily isolated and are entangled with other novelty-related randomized intermediate states.}
% \vspace{-1mm}
% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% Specific outcomes from OE AI are harder to reproduce. This hinders our understanding of the technology and makes it harder to run consistency-based diagnostics. 
% \end{mdframed}

\subsection{Resource Constraints} \label{sec:resources}
As the OE AI runs longer, it generates increasingly complex artifacts that require more computational and human resources to evaluate. %As such it becomes increasingly difficult for humans to provide sufficient supervision to an OE system. 
Unlike traditional ML models, OE AI requires more continuous evaluation without clear guarantees of utility. OE AI is run for a longer time before producing useful results since it involves much exploration and is not targeted toward specific useful results. 
Furthermore, it is difficult to predict whether an OE AI will produce valuable artifacts. Thus, the significant computational resources might not be justified. 
%. It might not be justifiable to expend significant computational resources to run an OE system that might produce valuable outputs after running for a long time.
These issues are exacerbated in OE AI that employs an LLM as a backbone since their large parameter size makes them expensive to run compared to smaller specialized models. Therefore, developing OE AI with adaptive resource constraints is important. 


% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% OE AI uses significant computational and human resources for speculative returns.
% \end{mdframed}


\subsection{Trade-offs}
\begin{wrapfigure}{r}{0.4\linewidth}
    \vspace{-6mm}
\centering
    \includegraphics[width=0.39\textwidth]{figures/irontriangle.pdf}
        \caption{The Impossible Triangle of OE AI illustrates that safety, speed, and novelty cannot be maximized together.}
    \label{fig:irontriangle}
    \vspace{-6mm}
\end{wrapfigure} 

As the OE AI systems evolve, they must balance competing priorities, often resulting in trade-offs that make the deployment of these systems challenging. As explained in~\autoref{fig:irontriangle}, OE AI inherently faces a trade-off between speed, novelty, and safety, creating a trilemma where optimizing two of these dimensions often compromises the third. Speed refers to the rate at which the system can generate new artifacts. Novelty measures the degree of uniqueness or originality in each newly generated artifact. Safety represents the system’s adherence to predefined constraints, ensuring outputs avoid harmful, unethical, or undesirable outcomes. 

\textbf{Application-Specific Needs.} 
Trade-offs can be difficult to navigate because they can depend on the types of problems we use OE AI for, which may require specific emphasis on one of these dimensions. In safety-critical applications such as drug discovery or medical diagnosis, safety is the foremost concern, often necessitating slower exploration to ensure rigorous validation and prevent harm, limiting novelty and speed. Conversely, in applications like gaming or art, novelty is prioritized to foster creativity, where the associated risks are generally lower, allowing safety to be sacrificed in favor of rapid, diverse output generation. Lastly, in autonomous vehicles or real-time industrial systems, the focus is on quick, reliable responses, with novelty being a secondary concern to ensure the system can operate effectively in dynamic, time-sensitive environments. 


% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% OE AI faces a trade-off between speed, novelty, and safety, requiring application-specific decisions.
% \end{mdframed}


\subsection{Social and Human risks}
It is crucial to consider the societal risks of OE AI. While all new technologies may have negative societal consequences, the unpredictable and evolving nature of OE AI may amplify known AI harms or introduce unanticipated ones. 


\textbf{The Rate of Novelty.} %\iv{removing anticipation line coz it is debatable} 
OE AI generates more novel artifacts than traditional AI and the rate of innovation and disruption is harder to anticipate.
%We cannot reliably anticipate the rate or the extent of disruption of novel artifacts produced by OE AI. 
This might outpace society's ability to adapt, integrate, and understand new developments. History provides examples of the disruptive effects of excessive novelty, such as the Industrial Revolution, which, while transformative, led to widespread social upheaval, labor displacement, and the erosion of traditional ways of life. Purely AI-led innovation can result in a loss of human agency in shaping scientific and societal progress, leaving individuals feeling disconnected from the process of discovery and creation. 


\textbf{Uninteresting Artifacts.}
OE AI should produce results that are interesting and useful to the observer. Quantifying ``interestingly new'' progress has been one of the grand challenges in OE research. Foundation models have been used as a Model-of-Interestingness (MoI)~\cite{zhang2024omni} to denote the human notion of what can be considered ``novel'' and at the same time ``interesting''.
However, OE AI could still produce uninteresting artifacts. This could be because its sense of interestingness might be misaligned with ours or because it may get stuck in a narrow set of artifacts without exploring more widely.
Also, as artifacts can be very complex it can be difficult for humans to determine whether they are truly interesting and useful. This could lead to situations where an OE AI produces useless, uninteresting artifacts, while humans do not recognize this. If such a system is kept running it will be a waste of resources. Furthermore, it might limit human creativity if human ideas are biased by generated artifacts or if humans think there is nothing more to explore. {Such problems are now discussed with LLMs and how they can homogenize individuals' beliefs and lead to a false impression of consensus~\cite{burton2024large}.}


\textbf{Difficulty to Plan.} As discussed in Section~\ref{sec:resources}, it is intractable to foresee, plan, or track the OE AI's progress or whether it would produce valuable solutions. Given limited resources, we may need to prioritize which problems we delegate to OE AI. This has a resemblance to funding decisions for research proposals. Our society needs transparent, fairways of deciding on appropriate allocations.  


\textbf{Reshaping Human Values.} {LLMs may learn to mislead humans as a result of reward hacking~\cite{wen2024language}, wrongly convincing human evaluators that performance has increased. Persuasion, deception, or drifting to rogue goals are examples of the catastrophic risks of AI discussed in the literature with anticipation of becoming more likely when AI is adaptive~\cite{hendrycks2023overview}, such as the case in OE AI. Due to cascading effects, OE AI may generate solutions that are initially, and then increasingly, misaligned, such as inaccurate scientific findings or biased policies. Values within societies may also drift over time, sometimes for the worse~\cite{hendrycks2023overview}. As humans continue to get exposed to these proliferating artifacts, they might get normalized and set harmful precedence, i.e, OE AI may gradually change societal and human values instead of getting OE AI aligned to human values. 


%Extending to OE systems, they may gradually convince humans with inaccurate scientific findings or biased policies~\cite{hendrycks2023overview}.



\textbf{Accountability.}
Assigning accountability for the actions of traditional AI is an ongoing legal and ethical debate. However, it is even more complicated for OE systems, since they act autonomously and inherently behave in ways they were not designed to. This makes it unclear whether developers can be blamed for the wrongdoings of the model. Furthermore, OE AI does not follow traditional procedures for training and data collection, requiring new frameworks for assigning responsibility.

\textbf{Environmental Factors.} 
Current AI models use exuberant amounts of energy. Training GPT-3 consumed 1287 MWh of electricity, resulting in 502 metric tons of carbon emission~\cite{patterson2021carbon}. Data centers use around 2.5 percent of global electricity, rivaling the aviation industry in greenhouse gas emissions~\cite{wired2023}. The current paradigm of OE AI uses LLMs as a backbone; running these models continuously requires a high amount of computation, which can have a significant environmental impact.
\vspace{-1mm}
% \begin{mdframed}[backgroundcolor=yellow!10,shadow=true,shadowsize=2pt,roundcorner=10pt,innerleftmargin=5pt,innerrightmargin=5pt]
% OE AI poses societal risks such as overwhelming humans with novelty, constraining human creativity, challenging legal accountability, or causing environmental damage.
% \end{mdframed}
\section{Technical Mitigations of Risks}
\label{sec:mitigation}

To address the risks and challenges, we explore and suggest research directions that enhance safety against catastrophic risks while responsibly maintaining the benefits of OE exploration.

\subsection{Oversight}
 \label{subsec:oversight}

As it is hard to anticipate the safety of OE processes, it is critical to oversee, either by humans or another system, their behavior during execution. Oversight provides a mechanism to monitor, guide, and correct system behavior, ensuring outputs align with human values and safety expectations. 

%Maintaining oversight of an OE system while operating in unconstrained and unpredictable environments is a fundamental challenge. Oversight provides a mechanism to monitor, guide, and correct system behavior, ensuring outputs align with human values and safety expectations. As it is hard to anticipate the safety of OE processes ahead of time, it is critical to oversee, either by humans or by another system, their behavior during execution. %This can be maintained either by humans or by another system. 

\textbf{Human-in-the-Loop Oversight.}
Ultimately, only humans can define safety and desirable values. Thus it is critical to have a human in the loop when OE AI is run. This could mean that a human actively monitors new artifacts. The human overseer could intervene when unsafe artifacts are generated or filter which artifacts should be propagated to future iterations of the system. Furthermore, a human overseer could provide feedback and guidance that steers the OE process in interesting directions. OE can involve AI and human components working together~\cite{secretan2008picbreeder}.
However, humans are limited in their capacity and might not be able to accurately judge complex artifacts, but should nevertheless set standards to remain in control.

\textbf{Interpretable Decision-Making.} To facilitate humans in providing oversight, future research needs to create interpretable OE AI whose decisions and reasoning traces are transparent to a human observer. Forcing OE AI to reason about its decisions in natural language, makes it inherently interpretable to humans, allowing inspection and failure detection~\cite{hu2024thought,betley2025tell}. 
%Incorporating language during the training of agents was found to improve their causal and relational understanding~\cite{lampinen2022tell,hu2024thought,lampinen2024passive}. 
%This also has safety advantages by providing humans with diagnostic tools to inspect the reasoning of models and detect failures~\cite{hu2024thought,betley2025tell}. 
Systems can be trained to explain their artifacts to a weaker model to simulate a human overseer. Furthermore, interpretability tools can be used to understand which input features~\cite{wang2024gradientbasedfeatureattribution} or inner representations~\cite{alain2018understandingintermediatelayersusing,cunningham2023sparseautoencodershighlyinterpretable} were relevant to a decision.


\textbf{Hierarchical Oversight.} Oversight can be expensive when a human or a large model needs to check every artifact. Hierarchical oversight can structure the supervision into layers, where a less expensive monitoring process oversees every artifact and reports artifacts or behaviors to higher levels with more expensive supervisors.
%where responsibilities are distributed across a hierarchy of supervisors. 
Works such as ~\cite{christiano2018supervising, chavan2024automation} propose mechanisms where higher layers guide or intervene in the functioning of lower layers. By analyzing the system's outputs at multiple levels of abstraction, hierarchical oversight can identify risks before they escalate while being resource efficient.

\textbf{Scalable Oversight.}
Providing effective oversight is difficult for humans when generated artifacts become too complex for them to evaluate accurately. 
Scalable oversight seeks to align AI systems whose outputs surpass human expertise or are too numerous for humans to evaluate properly~\cite{burns2023weak}. Approaches such as Iterated Distillation and Amplification (IDA)~\cite{christiano2018supervising}, Debate~\cite{irving2018aisafetydebate} or Recursive Reward Modeling (RRM)~\cite{NEURIPS2018_8cbe9ce2} could be applied to ensure the safety of OE AI. For example, OE AI could be forced to justify its actions in a debate with another agent, RRM could be used to align an overseer AI that can accurately evaluate new artifacts, or OE AI could be trained via IDA to internalize human notions of safety and interestingness.
Furthermore, self-diagnostic tools such as ~\cite{kamoi2024can, huang2023large} can be applied to OE AI to detect vulnerabilities in the system. %Finally, scalable oversight needs to accommodate the dynamic nature of OE AI by developing adaptable evaluation metrics and uncertainty thresholds. 

%%% orig %%% 
%\textbf{Adaptive oversight.} An overseer to an OE AI needs to be able to generalize to novel, possibly OOD artifacts. As such it is valuable if the oversight mechanism is continually adapted to changes in the OE AI and its environment. One could have another OE AI acting as a co-evolving overseer. Additionally, it could be promising to use causal models to enhance supervision by identifying underlying mechanisms rather than relying solely on correlations. Furthermore, interpretability tools could help to alert an observer to changes in the objectives pursued by an OE AI.

\textbf{OE AI for Adaptive Oversight.} For OE AI, oversight should not only scale to complex artifacts but also accommodate the dynamic nature of OE AI by developing adaptable evaluation and uncertainty thresholds. An overseer needs to be able to generalize to novel, possibly OOD artifacts. OE AI itself can be used to develop new safety-specialized mechanisms that work in tandem with the diversity-driven OE AI. An example is an overseer OE AI that co-evolves and judges safety. 

% Research into adaptive oversight frameworks that evolve alongside the system is essential for ensuring long-term safety and alignment. 

% \iv{does it make sense to talk about continual auditing in the intro paragraph about oversight? and not as a subsection?}
% \textbf{Continual auditing: }

\textbf{OE AI for Risk Extrapolation.} Similarly, a specialized OE AI can be used to anticipate and simulate in advance the future trajectories of artifacts and assess their risks and cascading effects. This OE AI could be optimized to generate novel but specifically harmful artifacts. This would need quantification and uncertainty methods to measure how close the main artifact is to the hypothesized harmful ones, based on this, an abortion or intervention step can follow.

%To enable oversight in cases where humans cannot judge the future trajectory of artifacts, a separate, orthogonal OE AI can be used to forecast and assess the risk of that trajectory. This OE AI could be optimized to generate novel but specifically harmful artifacts. This may simulate \emph{in advance} the cascading effect where artifacts from the main OE AI become more risky. This would need a quantification method to measure how close the main artifact is to the hypothesized harmful scenarios, based on this, an abortion or intervention step can follow. %Given the limited resource budget, we argue that supervision must be prioritized. 

\textbf{Consequential Actions.} As OE AI continues to evolve and explore, it may intervene in its environments. We already observe strong progress in autonomous and embodied agents. However, for risky applications, e.g., scientific experiments, we would need to limit the OE AI from performing catastrophically consequential actions where we cannot yet anticipate their outcomes. An alternative is to build simulations and models that are faithful to our world that would enable sand-boxed artifact generation. Given the challenges posed by novel and emergent artifacts, exploring causal models is a promising direction, as they exhibit greater robustness on novel data~\cite{richens2024robust}.

% \textbf{Human in the Loop.} This can serve as a critical component of maintaining oversight of OE systems. This requires humans to understand and interpret the behavior of OE's current states and interact with them. For instance, LLMs, widely used for novelty and diversity generation, facilitate human communication and feedback~\cite{lampinen2022tell,raad2024scaling,hu2024thought}. However, steering the entire OE system toward a specific goal remains challenging due to its inherent unpredictability and evolving nature. %As demonstrated in works like~\cite{secretan2008picbreeder}, human feedback can be used to guide an OE system by selecting outputs.   

%\textbf{Interpretable Decision-Making and Self-Reporting.} To facilitate humans in providing oversight, future research needs to create interpretable OE systems whose decisions and reasoning traces are transparent to a human observer. For instance, LLMs, widely used for novelty and diversity generation, facilitate human communication and feedback~\cite{lampinen2022tell,raad2024scaling,hu2024thought}. Recent research on situational awareness~\cite{betley2025tell} shows that LLMs can self-describe learned behaviors such as generating secure code despite not being trained on such descriptions. Systems can be trained to explain their artifacts to a weaker model to simulate a human observer. These strategies can support human-in-the-loop safety by creating OE systems that are more interpretable and can report to humans when dangerous artifacts arise~\cite{hu2024thought}.


% \textbf{OOD/Anomaly/Novelty detection and report to human/do more evals}

% \textbf{Self-reporting?}
\vspace{-3mm}
\subsection{Constraints} %\iv{rename as constrains?}
Most existing safety frameworks focus on structured environments with predefined goals. However, building guardrails to prevent the OE AI from exploring unsafe artifacts will be crucial to ensure the safety of these systems.

\textbf{Constrained Exploration.}
Since OE AI often pursues diversity, the exploration process can inadvertently drive the system into unsafe or misaligned state spaces. By constraining exploration to an $\epsilon$-ball, the system can balance novelty with safety, similar to safe exploration in RL~\cite{garcia2015comprehensive}. This requires constrained novelty metrics that evaluate novelty relative to both past behaviors and predefined safety constraints. In simple, discrete domains, such a novelty metric could be formally specified, while LLM-based judges could quantify novelty in more complex domains. Based on the novelty scores of new artifacts, it would be possible to penalize novel behaviors that exceed a probabilistic safety threshold or confidence bound, as modeled using techniques like Gaussian Processes~\cite{sui2015safe,turchetta2016safe} or reachability analysis~\cite{krakovna2018penalizing, fisac2018general}. Furthermore, novelty search can be combined with shielding mechanisms~\cite{dawood2024dynamic} to dynamically reject unsafe actions. Finally, safety constraints also can be introduced in Minimal Criterion Coevolution~\cite{brant2017minimal}. 

\textbf{Artifact Complexity Budget.} Setting a complexity budget might help balance novelty and exploration with the ability of humans to understand, evaluate, and digest new artifacts. %Such a budget could constrain the frequency and complexity of new artifacts. 
This budget serves as a safeguard, preventing excessive unpredictability and mitigating the risk of negative compounding effects that may arise from unrestrained exploration. By dynamically adjusting this budget it is possible to navigate the creativity-control trade-off.

\textbf{Setting Specific Rules.}
While OE AI continuously evolves and faces new challenges, there are rules we never want it to break. Although such rules cannot cover all unsafe behaviors, they can still prevent some failures. 
While constraints do limit the creativity of the OE AI by cutting off some of the search space, the system is still able to openly explore the remaining space, thus retaining its open-endedness.
To take a more abstract and flexible view, rules could be specified as general principles in a constitution~\cite{bai2022constitutional} that can be reinterpreted in new situations, or dynamically created and updated by AI. An LLM guiding the OE AI's decisions can either reason about these rules~\cite{guan2025deliberative} or causally~\cite{kiciman2023causal}. Recent work~\cite{zarembatrading} shows the potential and promise of LLMs, when given enough intermediate reasoning steps, to reason in compliance tasks. 
This also provides an effective framework for overseers to judge new artifacts.
\vspace{-2mm}
\subsection{Adaptive Alignment}
% Adaptive safe guards can be integrated into OE systems to ensure that their behavior remains aligned. Current safe guards for LLMs or RL are currently static, which might not be effective for OE systems.
%Since OE AI keeps evolving and exploring new state spaces, safeguards can become outdated or fail to generalize to new situations. Thus OE AI needs safeguards that adapt to its situation to continually ensure safe behavior.

%\textbf{Adaptive Alignment.} 
Current alignment techniques assume a model and its environment remain static, thus only requiring safety training once. New continual alignment algorithms could allow us to adapt safety as the model and its circumstances change~\cite{zhang2024cppo}. 
While~\citet{moskovitz2023confronting} composite reward weighting dynamically and~\citet{hong2024adaptive} address overoptimization and ambiguity, they lack robust mechanisms for long-term feedback loops. Multi-agent RL for co-evolving alignment dynamics in OE systems can be a promising research direction. Using dynamic reward functions can adjust the reward signals to reflect the evolving human preferences or system performance. Adaptive preference scaling~\cite{fang2024clha, hong2024adaptive}, and distributional preference reward modeling~\cite{li2024aligning} have been used to refine reward functions in RL-based systems by adjusting reward weights in response to shifting human feedback or performance degradation. For OE AI, dynamic reward calibration must go beyond simple reward adjustments to handle the continuous and diverse outputs produced by such systems. 

% Previous work~\cite{moskovitz2023confronting} has attempted to dynamically compose reward weighting and ~\cite{hong2024adaptive} to address overoptimization and ambiguity. However, these lack robust mechanisms for long-term feedback loops. Adaptive preference scaling~\cite{fang2024clha, hong2024adaptive}, and distributional preference reward modeling~\cite{li2024aligning} have been used to refine reward functions in RL-based systems by adjusting reward weights in response to shifting human feedback or performance degradation. However, for OE systems, dynamic reward calibration must go beyond simple reward adjustments to handle novel, OOD outputs produced by such systems. 

% \textbf{Adaptive oversight.} An overseer to an OE AI needs to be able to generalize to novel, possibly OOD artifacts. As such it is valuable if the oversight mechanism is continually adapted to changes in the OE AI and its environment. One could have another OE AI acting as a co-evolving overseer. Additionally, it could be promising to use causal models to enhance supervision by identifying underlying mechanisms rather than relying solely on correlations. Furthermore, interpretability tools could help to alert an observer to changes in the objectives pursued by an OE AI.


% \textbf{Adaptive alignment.} Many of the current alignment RLHF methods rely on static, pre-defined safety constraints (e.g., pretrained reward models). While ~\cite{moskovitz2023confronting} composite reward weighting dynamically and ~\cite{hong2024adaptive} address overoptimization and ambiguity, they lack robust mechanisms for long-term feedback loops. multi-agent RL for co-evolving alignment dynamics and leveraging interpretability tools for proactive safety monitoring in open-ended systems can be a promising research direction. 


% \textbf{Bounding Novelty}
% \rut{The definitions of novelty and learnability in OE systems implicitly bound the degree of novelty. While novelty is defined as increasing unpredictability over time for a static observer, learnability guarantees that adaptive observers can reduce predictive error by integrating a longer history of artifacts. However, there is no explicit method provided to ensure that novelty remains within manageable bounds, and without such mechanisms, the system risks producing artifacts that are too unpredictable for even adaptive observers to handle effectively. To address this, it is essential to implement explicit methods for bounding novelty to ensure the system remains learnable. }

% - too novel -> observer cannot keep up; cannot learn from it

\subsection{Safety Evaluations}
Finally, continuous safety evaluation of OE AI is important for understanding the extent of unsafe behaviors. 

\textbf{Benchmarking OE Safety.} Developing benchmarks specifically for OE AI is crucial for quantifying its risks and evaluating failure modes. Existing benchmarks, such as those on multi-agent risks and unintended consequences~\cite{rivera2020tanksworld}, provide some insights but fail to incorporate the unique characteristics of OE algorithms. A dynamic benchmark explicitly designed for OE AI would need to address its continuous evolution, novelty generation, and dynamic complexity. For example, the difficulty of tests could be adjusted to the OE AI's changing capabilities. 

\textbf{Redteaming OE Systems.} The previously outlined direction of ``extrapolating risks'' is beneficial to anticipate future risks even if the OE system is aligned. On the other hand, targeted red teaming can reveal failures for individual components or the entire system. Red teaming allows us to stress-test OE systems by actively probing their vulnerabilities and finding situations in which they behave unsafely.%This process becomes particularly complex for OE systems because of their intrinsic capacity for generating unbounded novelty, emergent behaviors, and unanticipated outputs. 
This could involve manually or adversarially finding inputs on which the OE system misbehaves.
~\citet{lehman2023evolution, bradley2023quality, liu2024large} uses LLMs to enhance genetic programming by generating diverse, functional artifacts. These outputs could serve as adversarial artifacts to test and evaluate system robustness like in ~\cite{samvelyan2024rainbow}, but here the aim would be to test the entire OE systems.
Further, one could construct an environment in which the OE system is being led to produce unsafe artifacts.




\section{Call for Action}
\label{sec:call4action}
Ensuring the responsible deployment of OE AI requires active engagement from various stakeholders.

\textbf{Funding}
bodies can shape research priorities. 
They could urge OE researchers to consider and address the safety risks of their work. Further, they could dedicate resources toward robust safety mechanisms and evaluations for OE AI.
% \textbf{Research} intersection of safety and open-ended research presents an exciting and underexplored domain. We believe that the progress in OE research must be coupled with safety research. 
% the area of OE safety is fairly underexplored and as mentioned in Section~\ref{sec:risk} and Section~\ref{sec:mitigation}, there are interesting projects. Establishing standardized definitions and frameworks will be essential for fostering effective communication among researchers and stakeholders. 

\textbf{Research} on the intersection of safety and OE research is crucial, impactful and under-explored. We argue that safety should be a critical part of OE research.
This requires general awareness of the risks and dedicated research on safety problems.
Additionally, the AI safety community should dedicate research to the specific risks of OE AI. We hope this paper can provide a bridge to foster exchange and collaboration between these communities.


% \textbf{Opportunities.} Besides prioritizing the safety of OE AI, using OE AI to red-team even traditional models and agentic applications is an underexplored research direction and has a direct benefit of understanding how models can fail in unpredictable ways. Since it can be a sandboxed environment, it does not directly suffer from the risks we identified of OE AI for achieving ASI.  

\textbf{Opportunities} lie in the application of OE AI to AI Safety. Aside from providing adaptive oversight (Section \ref{subsec:oversight}) OE AI can be used to red-team traditional models \cite{samvelyan2024rainbow} and agentic applications, in addition to automating interpretability research. 


\textbf{Policy Makers} should mandate audits of sufficiently capable OE AI to ensure adherence to safety standards and societal values. Comprehensive auditing protocols must account for the dynamic and emergent nature of these systems.


% \textbf{Industry} developing OE systems must prioritize collaborations with academia and policymakers to scale oversight mechanisms and conduct rigorous red-teaming. This would support safe and catastrophic-free deployment of OE systems. 

\textbf{Industry} deploying OE AI must implement and rigorously test oversight mechanisms and guardrails for OE systems. Furthermore, comprehensive evaluation of societal and catastrophic risks should be conducted in collaboration with third-parties, academia and governments.

% \textbf{Public}. The resources to run these OEs are only with a few companies. Hence, the public doesn't really have much say, however if a system were to take in a lot of resources, we hope the general public is consulted and societal well-being is prioritized. 

\textbf{Public}. The ability and resources to run OE AIs are centralized in a few companies. Since deploying them comes with large resource costs and safety risks, the public should be educated and consulted on these decisions to prioritize.




% \section{Alternative Views}
% We argue that safety is essential for the responsible development of OE AI. In contrast, one can argue that an overemphasis on safety and being overly cautious may lead to a lack of progress in the field. Over-constraining these systems through safety mechanisms undermines their capacity for innovation. This especially holds if one thinks that current OE systems are not capable enough yet to cause large harm. Under this view, work on safety should be postponed until these systems pose concrete dangers. Lastly, some might argue that OE systems are inherently uncontrollable, hence, imposing safety mechanisms is futile.

\section{Conclusion}
Open-Ended AI is a promising paradigm for generating novel, adaptive solutions in complex and dynamic environments, driving interest across research and applied domains. However, its open-ended nature introduces specific safety challenges that must be addressed to enable responsible deployment and maximize its societal benefits. We argue that the inherent unpredictability and uncontrollability of OE AI, challenges in ensuring and maintaining alignment, traceability, and societal impacts, as well as trade-offs in resource use and safety. We highlight the critical importance of human and automated oversight over OE AI. Further, we suggest ways of giving adaptive guidelines to OE AI that retain its creativity and co-evolve with it. Lastly, we call for targeted safety evaluations and provide concrete suggestions on how different stakeholders can contribute to the responsible development of OE AI.
Ultimately, we hope this paper will lead the OE and safety communities and other stakeholders to consider safety a priority in the development and deployment of OE AI. %and describe a path on how to achieve this. 

% \begin{itemize}
%     \item Open-endedness for security
%     \item Autonomous driving
%     \item Open-endedness for science: a) with physical experiments b) without physical experiments
%     \item Open-ended agents
%     \item AI-driven evolutionary simulations
%     \item Open-ended AI learning environments
%     \item Gaming
%     \item AI companions
%     \item Robotics and embodied AI
%     \item Education
%     \item Ecosystem Simulations
%     \item Creative AI, AI for art
%     \item open ended life-long personalized agents (: https://arxiv.org/pdf/2412.13103)
% \end{itemize}
\bibliography{main.bib}
\bibliographystyle{iclr2025_conference}

\end{document}


