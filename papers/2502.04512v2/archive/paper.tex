\documentclass{article}
\usepackage[utf8]{inputenc}

% Used in the explanation text
\usepackage{hyperref}
\hypersetup{
    colorlinks = true,
    citecolor = {blue},
    urlcolor = {blue},
}

% Used by the template
\usepackage{setspace}
\usepackage{changepage} % to adjust margins
\usepackage[breakable]{tcolorbox}
\usepackage{float} % for tables inside tcolorbox https://tex.stackexchange.com/a/274342
\usepackage{enumitem}

\newcommand{\mario}[1]{\textbf{\color{blue}Mario: #1}}


\title{Open endedness - Risks and Mitigations}


\begin{document}

\maketitle

\begin{abstract}
    In recent years, advancements in artificial intelligence (AI) have been significantly driven by a combination of foundational models and curiosity-driven learning aimed at increasing efficiency and adaptability. These developments have resulted in impactful applications across domains, such as healthcare. A growing area of interest within this field is open-endedness — the ability of AI systems to autonomously generate novel and diverse artifacts or solutions over time (basically generate problems and solutions autonomously). \footnote{We need to come up with a concrete definition.} This has become particularly relevant(?)  \mario{this sounds weak. you need to make a point that this is a relevant submission.} for accelerating scientific discovery and enabling continual adaptation in AI agents. The existing work has largely concentrated on frameworks to achieve open-endedness, the associated risks are largely underexamined and overlooked. The inherent continual nature of OE systems complicates the alignment, predictability, and control of these models
    This paper systematically examines the risks inherent in open-ended systems and proposes targeted mitigation strategies to support the safe deployment of open-ended AI.
    
\end{abstract}


\section{What is Open-Endedness}
The problem is, I think, \mario{I assume this is a draft - we cannot have first person narrative in the submission} (hi Mario: sorry this is the archive file, main.tex is the main one) that there is no one definition. However, most of the definitions cover some key aspects. It refers to a system continuously producing novel and increasingly complex artifacts, solutions, or insights without predefined endpoints. There are often parallels drawn to evolution. Such a setting allows for the dynamic expansion of an AI system’s capabilities and problem spaces and has significant implications for fields ranging from scientific discovery to autonomous research and development.

Several formal definitions of open-endedness in the literature emphasize distinct aspects of novelty generation and iterative complexity. Will list some of them here:

\begin{enumerate}
    \item OE is the production of novel, useful, and learnable artifacts by an external observer~\cite{}. In this definition, the observer is an important aspect. This is a bit limiting as it will be a subjective definition. Open-ended systems must include mechanisms to detect and preserve novelty and usefulness within generated artifacts. I think the term "novel" can be considered in a less subjective form right? 

    \item OE can be defined from the evolution aspect. OE systems are set up to prioritize diversity and complexity in agent behaviors, favoring incremental innovation and diverse phenotype evolution \cite{}. This approach showed the autonomous problem creation and solution development without a direct human guidance loop. 

    \item OE can be defined as an open-ended search problem, defined by continual exploration without explicit end goals, capable of creating diverse and complex solutions across a vast search space. 
    
\end{enumerate}

As per definition 1, they disregard systems that display novelty but are not informative to humans (eg: TV noise). This definition assumes the human aspect as a judge of novelty. But if you want something to novel (then it may not make sense to humans right? For example: Move 37 in Alpha Go). The absence of an "absolute" definition is detrimental since safety issues can be easily overlooked and ignored. 


\subsection{Examples of Open-Endedness}

\begin{itemize}
    \item Open-endedness for security
    \item Autonomous driving
    \item Open-endedness for science: a) with physical experiments b) without physical experiments
    \item Open-ended agents
    \item AI-driven evolutionary simulations
    \item Open-ended AI learning environments
    \item Gaming
    \item AI companions
    \item Robotics and embodied AI
    \item Education
    \item Ecosystem Simulations
    \item Creative AI, AI for art
    \item open ended life-long personalized agents (saw a recent paper: https://arxiv.org/pdf/2412.13103)
\end{itemize}


\subsection{Relation to scientific discovery}
Open-endedness has significant implications for scientific discovery, as it aligns closely with scientific research's iterative and exploratory nature. In science, progress often involves hypothesizing, testing, and refining models or generating entirely new paradigms. 

\textbf{What is science?} I suppose science and novelty are debatable. At its core, science seeks to uncover patterns, establish causal relationships, and generate predictive models that explain phenomena. However, what constitutes "scientific progress" is not universally agreed upon, particularly when novelty is introduced into the equation. Science is inherently iterative, with progress often hinging on the interplay between established knowledge and new, sometimes counterintuitive, discoveries. 

~\cite{kuhn1997structure} concept of scientific revolutions highlights that science does not always progress linearly but instead undergoes paradigm shifts driven by the introduction of fundamentally new ideas. Paradigms are established frameworks of theories and practices, and novelty arises when anomalies within the paradigm accumulate to the point where a new paradigm is proposed. For example: The transition from Newtonian mechanics to Einstein's theory of relativity\~paradigm shift. 

\subsubsection{Relation btw OE and Scientific discovery}
The relationship between novelty in science and open-endedness is closely related to how novelty drives scientific progress and how open-ended systems can autonomously generate new knowledge, hypotheses, and solutions. The idea of generating novel artifacts is closely related to scientific discovery. \textcolor{blue}{An open-ended AI model for scientific discovery could generate a continuous series of innovations, each building on the previous ones, much like how scientific knowledge progresses through the incremental refinement of theories.} \textcolor{red}{However reproducibility is a core aspect of scientific discovery, which does not necessarily fit within the domain of OE. This challenges the scientific discovery’s framework on controlled experimentation and reproducible results. Management between novelty and rigor in the context of OE4Science.}


\section{Why is it bad}

\subsection{Novelty and OOD}

\textcolor{green}{Correct me: Novelty can refer to the generation of solutions, hypotheses, or artifacts that are outside the scope of what the system has previously encountered or trained on.} This inherently resembles the concept of OOD in machine learning, where the data or problems encountered by the system differ from those seen during training. While novelty is a fundamental feature of open-ended systems, it also brings with it several risks associated with OOD. 

\subsubsection{Misalignment} 
As OE systems explore new solution spaces, they may produce outputs that significantly deviate from the original safe distribution. RLHF may not transfer well in such domains. When the system encounters novel scenarios, the reward structure—may no longer apply. For example, the reward model might have been calibrated based on a particular (safe) distribution of data or tasks, but OE mechanism can create new contexts where the same rewards no longer make sense.

Since OE systems don't rely on explicit goals, solving for alignment becomes difficult. [Classically AI alignment is framed as “we have a goal in mind and the system doesn’t end up following that goal”.]

\subsubsection{Anticipation/Detection}

As OE systems continuously generate new solutions, there is an inherent unpredictability in the outcomes. Novel outputs may be highly varied, making it difficult to anticipate and detect the system's behavior or impact.

\subsubsection{Safety}
Existing safety mechanisms are often designed for systems operating within a predefined problem space. When OE systems generate novel, OOD solutions, these mechanisms can be broken. safety filters can be bypassed.

\subsection{Social and Human aspect}
As these systems generate new artifacts, they impact society in a way that may be difficult to predict or control. 

\subsubsection{Accountability}
As open-ended systems autonomously generate new solutions, it becomes increasingly difficult to assign accountability for their actions or outputs. The current accountability paradigm includes fine-grained responsibility held from AI users to AI developers as accountability, for different stages. This accountability line becomes blurred as models become more autonomous. 

\subsubsection{Catastrophic Risks}
\subsubsection{Unemployment}

\subsubsection{Exploitation/Collapse/Echo-chamber }


\subsection{Risks from OE4Science}
\textcolor{red}{Extend the definition for truth}
\subsubsection{Evaluation}
These models produce highly unpredictable outputs which means that the current scientific paradigm may not be able to handle it. \textcolor{red}{for eg:} . To evaluate the usefulness of such novel innovations may be a wasted effort of humans. The current paradigm 

\subsubsection{no evals more ethical dilemma}


\subsubsection{Resource competition}

\subsubsection{Environmental factor}

\subsubsection{Biases in the data that might lead} a bit weak because the past re



\section{How can you prevent it?}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{archive/Images/irontriangleOEAI.pdf}
    \caption{The "Iron Triangle" of OEIA systems shows, that safety, speed and novelty (complexity) cannot be satisfied simultaneously. One of the qualities has to be sacrificed or capped depending on the domain of application.}
    \label{fig:enter-label}
\end{figure}



Add a new definition or principle of open-endedness as truth for advocating the principles of the system.

\subsection{Evolutionary Limits}

The caps on speed and scale on OEAI's evolutionary capabilities might be required to keep up with the oversight. For example, an AI oversight system could trigger slow down of the OEAI for the evaluation. It could also execute pause in OEAI development, when collaborative human evaluation is needed.

\subsection{Adaptive alignment} ~\cite{zhang2024copr}

\subsection{Dynamic Reward Calibration}

\subsection{Hierarchical Oversight}

\subsection{Scalable Oversight}

\subsection{Constrained Explorations? but that's not open ended}

\subsection{Continual auditing}

\subsection{HUman in loop systems}

\subsection{OOD/Anomaly/Novelty detection and report to human/do more evals}

\subsection{Self-reporting?}

\subsection{Automate pause switches for smth unethical?}


\end{document}
