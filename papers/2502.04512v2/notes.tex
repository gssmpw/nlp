while NS does not explicitly optimize for a problem-specific objective, it optimizes these implicit objectives at the level of the archive. The paper emphasizes that understanding NS as an archive-space optimization process (rather than a solution-space search). https://cdn.aaai.org/ocs/18424/18424-79325-1-PB.pdf

Q) do all OE systems need to have cascading effect? 

An extension of novelty search called minimal criteria novelty search (MCNS) is introduced that brings a new abstraction of natural evolution to EC: evolution as a search
for novel ways of doing the same thing. This extension also suggests a way to constrain
the raw search for novelty that may sometimes be necessary in practice.

Polyworld [174] is also a simulation of an ecological system of competing agents, but gives
these basic agents embodiment in a two-dimensional world, a brain (i.e. a neural network),
and basic actions it may execute (e.g. eating, mating, fighting, moving). The only goal is
survival. Interestingly, behaviors seen in real life organisms, such as flocking, grazing, and
foraging, although not directly specified by the system, emerge from the interactions and
evolution of agents

Comparing the behavior of these approaches in terms of discovering and diversifying feasible game content, results indicate that FINS is able to discover feasible solutions faster in cases where such solutions are rare. FI2NS is likely to evolve more diverse content, but underperforms in highly constrained search spaces as it struggles to discover and maintain a sizable feasible population unless the offspring boost mechanism is used.

which arXiv:2006.07495v1 [cs.NE] 12 Jun 2020 studies the principles driving processes of continual evolutionary innovation, often from the lens of generating intelligent behavior.

open-ended search has been presented to the ML community as a grand challenge

For example, intuitive human-designed fitness functions can be optimized
in undesirable ways (Lehman et al., 2018) and agents can
fail catastrophically when deployed if training does not anticipate gamut of possible real-world scenarios (HadfieldMenell et al., 2017). 

about the predictability of openended systems have been studied in ALife (Wagenaar and
Adami, 2004; Taylor and Hallam, 1998), and likely bear on
the safety of open-ended search. More generally, the extent to which the creativity of open-ended algorithms can be
controlled (Stanley and Lehman, 2015; Lehman et al., 2018)

 there is increasing interest in ML
algorithms that themselves learn to innovate (e.g. to invent
new search algorithms and architectures). As a result, openended search is now being pursued within the paradigm of
statistical ML (Guttenberg, Virgo, and Penn, 2019; Wang et
al., 2019; Akkaya et al., 2019). 

further motivating study of its
safety profile, as real-world applications emerge (Akkaya et
al., 2019

If open-ended systems could be made as directable as individual RL agents, then work defining objectives which
preserve controllability (Hadfield-Menell et al., 2016; 2017;
Carey and Everitt, 2023) might be a promising path towards
more controllable open-ended systems.)


Intrinsic Motivation and Novelty in Reinforcement Learning (RL):
Intrinsically motivated exploration methods, like curiosity-driven RL or goal-conditioned RL (e.g., CURIOUS) seek novelty in tasks or state-action spaces to improve learning efficiency and solve sparse reward problems. While effective in exploring uncharted regions, such methods often disregard safety boundaries, inadvertently causing unsafe or irreversible actions during the search for novel solutions.

