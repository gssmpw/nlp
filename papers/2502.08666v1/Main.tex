\documentclass[10pt]{article}

% packages
% \usepackage[accepted]{arxiv} 

 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{breakurl} 
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{enumerate}
\newtheorem{assumption}{Assumption}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{placeins}
\usepackage{caption}
\usepackage[font=normalsize]{caption}




% personalized commands
\newcommand{\mk}[1]{\textcolor{red}{Michael: #1}}
\newcommand{\mm}[1]{\textcolor{magenta}{Miranda: #1}}

% \icmltitlerunning{Hallucination, Monofacts and Miscalibration: An Empirical Investigation}

% \begin{document}

% \onecolumn
% \icmltitle{Hallucination, Monofacts and Miscalibration: An Empirical Investigation}
% \begin{icmlauthorlist}
% \icmlauthor{Muqing Miao}{upenn}
% \icmlauthor{Michael Kearns}{upenn}
% \end{icmlauthorlist}


% \icmlaffiliation{upenn}{Department of Computer and Information Science, University of Pennsylvania, Philadelphia, Pennsylvania, USA}
% \icmlcorrespondingauthor{Miranda Miao}{mmiao@seas.upenn.edu}
% \icmlcorrespondingauthor{Michael Kearns}{mkearns@cis.upenn.edu}
% \icmlkeywords{Language Models, Model Calibration, Hallucination, Learning Theory, Model Reliability}
% \vskip 0.3in


% \printAffiliationsAndNotice{} 
\title{Hallucination, Monofacts, and Miscalibration: An Empirical Investigation}

\author{
    Muqing Miao\thanks{Department of Computer and Information Science, University of Pennsylvania, Philadelphia, Pennsylvania, USA. Correspondence to: Miranda Muqing Miao \texttt{<miaom@seas.upenn.edu>}, Michael Kearns \texttt{<mkearns@cis.upenn.edu>}.} \and
    Michael Kearns\footnotemark[1]
}

\date{}  

\begin{document}

\maketitle

\begin{abstract}
Recent theoretical work \citep{kalai2024calibrated} proves that a particular notion of hallucination rate in LLMs must be lower bounded by the training data monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration. Through systematic experiments with $n$-gram models and in-context learning with LLMs, we empirically investigate and validate this theory by examining how different underlying data distributions affect the monofact rate and a model's tendency to hallucinate. We then vary model miscalibration through controlled upweighting of training sample while holding monofact rates constant, allowing us to isolate miscalibration's reduction effect on hallucination. These findings suggest that both the distribution of fact frequencies in training data and the calibration-hallucination trade-off are inherent to probabilistic language generation. Our results also suggest that current practices of aggressive deduplication in training data may need to be reconsidered, as selective duplication could serve as a principled mechanism for reducing hallucination.
\end{abstract}

%Our results confirm the theory's predicted trade-offs between hallucination and calibration, with higher miscalibration consistently reducing hallucination rates for any given monofact rate.
% , with direct implications for model training strategies. 

\section{Introduction}
Consider using a language model to write a biography of a living person. The model may confidently state that ``John Smith was born in Seattle in 1982, earned his PhD from Stanford in 2008, and now leads AI research at Tech Corp,'' but these ``facts'' could be completely fabricated. Such hallucinations (plausible but verifiably false statements) are a critical issue for language models. When lawyers have submitted hallucinated legal cases or doctors have received incorrect medical advice, the consequences can be severe \citep{mccoy2024dangerous, shin2023humiliated}.

A natural question is: Why do modern language models hallucinate? While issues like outdated training data or adversarial prompts contribute to hallucination, recent theoretical work demonstrates a more fundamental cause: calibrated language models must hallucinate at a rate related to the prevalence of rare facts in their training data \citep{kalai2024calibrated}. Specifically, they prove that for arbitrary facts whose veracity cannot be systematically determined from training data, the hallucination rate has a statistical lower bound tied to the fraction of facts that appear exactly once in training (the \emph{monofact} rate) minus model miscalibration (a measure of overfitting). Importantly, this theoretical relationship suggests a practical approach for reducing hallucination that does not require knowledge of the true distribution: controlled duplication of training examples can induce beneficial miscalibration while maintaining model performance.

In this paper, we provide the first systematic empirical investigation of this theoretical framework. Using carefully controlled experiments with bigram models and in-context learning with language models, we examine how different distributions over fact frequencies influence monofact and hallucination rates. We then develop an algorithm to induce varying levels of miscalibration while holding monofact constant in order to isolate miscalibration's impact on hallucination. Our study provides three main contributions:

First, we show that training data constructed using Pareto or Zipf distributions create patterns where certain facts appear much more frequently than others. These patterns can produce widely varying monofact rates by adjusting underlying parameters that influence such frequency, and we demonstrate that these variations directly drive corresponding changes in hallucination rates. In contrast, data frequencies constructed using Gaussian and Poisson distributions tend to produce more uniform fact frequencies, resulting in a narrower range of monofact rates and limiting their usefulness for exploring this relationship.

Second, we introduce an upweighting algorithm that induces controlled miscalibration by selectively duplicating transition counts for subsets of training examples in our $n$-gram models. For a chosen number $k$ of training examples, our algorithm incrementally adds one additional count to both their initial state probabilities and bigram transition frequencies, effectively amplifying their contribution to the model. By systematically varying $k$ while maintaining constant monofact rates, we demonstrate that hallucination reduction is associated with increased levels of miscalibration. Our experimental results show that even selective upweighting of a small fraction of training examples can substantially reduce hallucination rates, providing a practical mechanism for controlling the calibration-hallucination trade-off.

Third, we extend these findings to LLMs through in-context learning experiments, showing that the relationship between monofact rates and hallucination persists even in real world LLMs. 

Our results have meaningful implications for addressing hallucination in language models. They demonstrate that two fundamental factors drive a model's tendency to hallucinate: the distribution of fact frequencies in the training data (which directly affects monofact rate) and the model calibration (the model's ability to generalize). 

% This insight could help explain a commonly observed phenomenon --- language models tend to hallucinate less when dealing with systematic facts, which appear repeatedly in consistent formats, compared to arbitrary facts that occur infrequently and in diverse forms.

More importantly, our findings reveal a fundamental tension in language model optimization: while traditional machine learning approaches emphasize maintaining calibration, our results indicate that perfect calibration may actually impede a model's ability to reliably generate factual content. Some degree of miscalibration appears necessary to reduce hallucination rates effectively.

This insight has implications for training data preparation. While current practices emphasize aggressive deduplication of training data \citep{lee2021deduplicating}, our experiments demonstrate that controlled upweighting of training examples can induce beneficial miscalibration and reduce hallucination rates. This provides theoretical justification for selective reintroduction of duplicates during post-training and suggests a new direction for model development: strategically introducing duplication where it most effectively reduces hallucination while maintaining overall model performance. 

\subsection{Related Work}
An important line of theoretical research shows that when training data contains many rare examples, memorization becomes indispensable for achieving low error. \cite{feldman2020does} demonstrates that for such distributions with frequent rare occurrences, any algorithm aiming for small error must memorize a substantial fraction of these uncommon instances. This observation connects directly to the theoretical lower bound on hallucination, which shows that well-calibrated language models must hallucinate at least at the rate of facts that appear only once in training \citep{kalai2024calibrated}. In other words, when many facts occur infrequently, models face a tension between avoiding hallucinations and maintaining calibration: the scarceness of certain facts forces either deeper memorization or an increased risk of fabricated outputs.

Recent empirical work reinforces the link between unfamiliar examples and hallucination. \cite{kang2023unfamiliar} introduce a “familiarity score” that quantifies how closely a model’s training data matches a test example, and they observe that hallucination rates grow almost linearly with unfamiliarity. Their findings echo the notion of “monofacts” by highlighting how rare or singly observed statements pose the greatest challenge for accurate prediction. \cite{gekhman2024does} similarly show that novel facts are learned more slowly by language models, and that integrating these facts tends to elevate hallucination rates in a linear fashion. Both studies underscore the inherent difficulty models face when handling sparse knowledge or atypical data.

Collectively, these works suggest that hallucination arises in part from the basic statistical structure of real-world datasets, especially when they include many low-frequency facts. The more the underlying distribution skews toward these rare observations, the higher the baseline for hallucinations becomes. 

\section{$n$-Gram Model Methodology}

Our experimental framework utilizes $n$-gram language models with a focus on bigrams. We craft a simplified but rigorous setting while closely following the assumptions described by \cite{kalai2024calibrated} in Section 4.1 \emph{Factoid Assumptions}.

\subsection{Experimental Methodology}
We employ bigram models as our primary experimental framework for several reasons. First, bigram models provide a simplified yet theoretically grounded setting. Second, they represent one of the most fundamental statistical approaches to language modeling \citep{shannon1948mathematical}, capturing local word dependencies through conditional probabilities. While modern LLMs are significantly more complex, bigram models share the core characteristic of learning probability distributions over sequences \citep{bengio2003neural, jurafsky2000speech}, making them ideal for studying the fundamental tension between monofact and hallucination.

We have experimented with higher-order $n$-grams and found that increased model capacity generally leads to lower hallucination rates while holding other factors constant, and the fundamental relationships between monofact rate, miscalibration, and hallucination remain consistent across different orders. We chose bigrams as our primary model as they provide sufficient model capacity for our experimental setup which involves just 6-word sequences, while offering the clearest demonstration of these relationships without the added complexity of higher-order dependencies. 

In addition, we design our data to be comma-separated sequences without filler words to test bigram behavior in its purest form. This simplified setting helps isolate the core statistical phenomena, as low-order $n$-grams typically generate gibberish when trained on natural language with unstructured filler words. 

While this controlled setting may appear artificial, we later demonstrate in Section \ref{sec: LLM_method} and \ref{sec: LLM_results} that our key findings about the relationships between monofact rates and hallucination generalize to experiments with real-world LLMs on more complicated statements.

\subsection{Definitions and Main Theorem}
\phantomsection
\label{sec:framework}

To investigate hallucination in a controlled setting, we need to carefully define what constitutes a fact versus a hallucination. Consider the statement ``Timothée Chalamet starred in Dune: Part Two directed by Denis Villeneuve.'' This is a true movie fact. However, if our model generates ``Timothée Chalamet starred in Dune: Part Two directed by Jack Black,'' this would be a hallucination --- a plausible but false statement. Our framework formalizes this distinction and allows us to measure how often such hallucinations occur. 

In our setting, we consider $U$ the universe of all possible comma-separated movie-related statements. Within this universe, some statements are true movie facts (our set $T$), while the remaining statements are false (our set $F$), allowing us to precisely identify when the model generates hallucinations. Following the theoretical framework\footnote{Kalai and Vempala use different notation in their original framework: $Y$ for universe (our $U$), $F$ for true facts (our $T$), $H$ for hallucinations (our $F$), $O$ for training data (our $S$), and $U$ for unobserved data (our $H$).}, we define:

\begin{itemize}
    \item $U$: The universe of all possible statements, consisting of 6-entity comma-separated sequences in the form ``Actor, Costar, Movie, Director, Genre, Year''. For example, any combination of these entities forms a possible statement in our universe
    
    \item $T \subset U$: The set of true movie statements (facts) from our universe --- these are the sequences that represent actual movies with their correct actors, directors, genres and years
    
    \item $F \subset U$: The set of false statements (hallucinations), where $F = U \setminus T$
    
    \item $S \subset T$: The training dataset. A subset of \( T \) that the model is exposed to during training
    
    \item $H = U \setminus S$: The set of unobserved statements that could be true or false

    \item $G$: The set of generated statements by the bigram model post-training

    \item \( p \): The true probability distribution over \( T \), which we vary as a part of our experiment

    \item \( g \): The bigram model's probability distribution over \( T \)

\end{itemize}

The key theoretical result establishes a lower bound on hallucination \citep{kalai2024calibrated}:
\begin{equation}
\label{eq:hallucination_bound}
f_{gen} \geq \hat{\text{MF}} - \text{Mis}(g,p) - \frac{3e^{-m}}{\delta} - \sqrt{\frac{6\ln(6/\delta)}{n}}
\end{equation}
where $f_{gen}$ represents the hallucination rate of the generative model, $\hat{\text{MF}}$ denotes the empirically observed ``monofact'' rate (fraction of facts that appear exactly once in training), $m$ represents the sparsity parameter controlling the ratio of true facts to possible hallucinations, $n$ is the number of training examples, and $\delta$ is the confidence parameter. Since the last two terms become negligible for large training sets and reasonable sparsity parameters, we focus on the relationship between hallucination rate, monofact rate, and miscalibration. Rather than solely verifying this lower bound, our experiments treat it as an approximation, investigating how varying monofact rate and miscalibration affects hallucination in practice.

\subsection{Dataset Construction}
We construct our $T$ dataset through a multi-stage process using the IMDb's non-commercial datasets \citep{imdb2024datasets}. 

\begin{enumerate}
    \item First, we utilize 20,000 unique movie facts from IMDb's datasets to create our base set of true facts. Each fact is a 6-entity comma-separated sequence as defined in \ref{sec:framework}. One example sequence is: ``Timothée Chalamet, Zendaya, Dune: Part Two, Denis Villeneuve, Science Fiction, 2024''
    
    \item We then construct $p$, the distribution over true facts $T$, by controlling how many times each fact appears in our dataset. For each fact, we draw a repetition count from one of the following distributions and replicate the fact accordingly:
    \begin{itemize}
        \item Pareto distribution with probability density function:
        \[ f(x;\gamma,x_m) = \frac{\gamma x_m^\gamma}{x^{\gamma+1}}, x \geq x_m \]
        where $\gamma$ is the shape parameter and $x_m$ is the scale parameter
        
        \item Zipf's distribution with probability mass function:
        \[ p(k;\rho,N) = \frac{1/k^\rho}{\sum_{n=1}^N 1/n^\rho} \]
        where $\rho$ is the skewness parameter and $N$ is the number of elements
        
        \item Gaussian distribution with probability density function:
        \[ f(x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \]
        where $\mu$ is the mean and $\sigma$ is the standard deviation
        
        \item Poisson distribution with probability mass function:
        \[ p(k;\lambda) = \frac{\lambda^k e^{-\lambda}}{k!} \]
        where $\lambda$ is both the mean and variance parameter
    \end{itemize}
    
    \item Finally, we create training set $S$ by sampling with replacement from this expanded dataset, maintaining a fixed size of 5,000 samples.
\end{enumerate}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{images/distributions_grid.png}
    \small
    \caption{Visual comparison of fact repetition frequencies for four probability distributions—Pareto, Zipf, Normal, and Poisson—across varying parameters. Each subplot shows how often a fact appears (x-axis) versus how many facts have that appearance count (y-axis, logarithmic scale).}
    \label{fig:distributions}
\end{figure*}
\FloatBarrier

By varying these distribution parameters, we can systematically control how frequently facts appear in our training data, allowing us to study how different repetition patterns and distributions of facts influence both the monofact rate and, consequently, hallucination.

Figure \ref{fig:distributions} shows the induced fact frequency distributions after dataset construction. It illustrates that with Pareto and Zipf distributions, a small subset of facts dominate by appearing with notably higher frequency than others.

\subsection{Statement Generation Process}
Let $g$ be a distribution over factual statements. For our movie dataset, each statement is a structured 6-tuple where each position can only contain tokens of a specific type (actors in position 1, costars in position 2, etc.). To build our bigram model, we first convert each 6-tuple in our training data into a set of adjacent pairs: (actor, costar), (costar, movie), (movie, director), (director, genre), and (genre, year). This creates a dataset of token pairs that we use to train a single bigram model, effectively treating our structured tuples as a bag of adjacent pairs.
For any such pair of tokens $(t_1, t_2)$, the bigram model learns:
\[ g(t_1, t_2) = g(t_1)g(t_2|t_1) \]
where $g(t_1)$ represents the marginal distribution over first tokens and $g(t_2|t_1)$ represents the conditional distribution over second tokens given the first token.
To generate a new 6-tuple statement, we apply this model sequentially:
\[ g(x) = g(t_1)\prod_{i=1}^{5} g(t_{i+1}|t_i) \]
The generation proceeds as follows:
\begin{enumerate}
\item Sample an initial token $t_1$ according to $g(t_1)$
\item For $i$ from 1 to 5:
\begin{itemize}
\item Sample token $t_{i+1}$ according to $g(t_{i+1}|t_i)$
\end{itemize}
\end{enumerate}
Note that while this approach is similar to standard bigram language modeling, it does not exploit the full structure of our movie data where each position has its own distinct token space (i.e., ``Timothée Chalamet'' can only appear in the \textit{Actor} slot). Instead, we treat all position pairs equivalently in building our transition probabilities. For evaluation, each model is trained on $|S| = 5,000$ statements and generates an equal number of new statements.

\subsection{Hallucination Evaluation}
Our hallucination evaluation process is simple. We define hallucinated generation $F_{gen}$ as any generation that does not belong in $T$. In other words:

\begin{equation}
  F_{gen} \coloneqq G \cap F
\end{equation}

The hallucination rate is simply computed as:

\[ f_{gen} = \frac{|F_{gen}|}{|G|} \]

\subsection{Miscalibration Measurement}

Before diving into technical details, let's understand calibration through a simple example. Consider a weather forecaster who predicts a 30\% chance of rain on multiple days. If they are well-calibrated, it should actually rain on approximately 30\% of the days where they made this prediction. More generally, a model is considered calibrated if among events it assigns a certain probability to, that probability matches their true frequency of occurrence. 

In our context, calibration means that for statements where our bigram model assigns similar probabilities, these probabilities should match the average true probability $p$ across those statements --- a weaker requirement than matching the exact probability of each individual statement.

We employ a logarithmic binning strategy following \cite{kalai2024calibrated}. For a given parameter $\epsilon \in [0,1]$, we created bins with boundaries:

\begin{equation}
B_i = [(1-\epsilon)^{i+1}, (1-\epsilon)^i]
\end{equation}

where $i \geq 0$ indexes the bins. A statement $x$ in $F$ is assigned to bin $B_i$ if $g(x)$ fell within its boundaries. This logarithmic binning ensures appropriate granularity across different orders of magnitude of probabilities. As an additional check, we employ an alternative adaptive binning approach that evenly distributes the probability mass, and find that it yields comparable results.\footnote{Let \( b \in \mathbb{N} \) and define the adaptive partition:
\[
\mathcal{V}_b(g) \coloneqq \{ B_{[0, a_1]}, B_{(a_1, a_2]}, \ldots, B_{(a_{b-1}, a_b]} \}
\]
where
\[
a_i = \sup \left\{ z \in [0, 1] \,\middle|\, \sum_{x : g(x) \leq z} g(x) \leq \frac{i}{b} \right\}.
\]\citep{kalai2024calibrated}}

The experiments in this paper utilize an $\epsilon$ of 0.1 unless otherwise indicated. 

 The total variation distance between $p$ and $g$ for each bin is computed as follows. First, let $\mathcal{B}(g)$ be an arbitrary partition of the distribution $g$, where for each statement $x$, there exists exactly one bin $B \in \mathcal{B}(g)$ such that $x$ belongs to $B$ if and only if $g(x)$ falls within that bin's probability range. For any such partition $\mathcal{B}(g)$, we define $p^{\mathcal{B}(g)}$ to be the $\mathcal{B}(g)$-coarsening of p, where for all $x \in B$:
 
\[ p^{\mathcal{B}(g)}(x) = \frac{p(B)}{|B|} \]

Then, the miscalibration is defined as:
\begin{align}
\text{Mis}(g, p) &\coloneqq
\|p^{\mathcal{B}(g)} - g\|_{\text{TV}}  \\
&= \frac{1}{2} \sum_{B \in \mathcal{B}(g)} \sum_{x \in B} \left| \frac{p(B)}{|B|} - g(x) \right| \\
&= \frac{1}{2} \sum_{B \in \mathcal{B}(g)} \left| p(B) - g(B) \right|
\end{align}

where $p(B_i)$ is the sum of true probabilities for all statements in bin $i$, and $g(B_i)$ is the sum of model-assigned probabilities for the same bin. 

Thus, the final miscalibration score can also be understood as as the sum of total variation distances across all bins:

\begin{equation}
\text{Mis}(g, p) = \sum_{i}^{|B|} \text{TV}_{B_i}(p,g)
\end{equation}

This metric quantifies the degree of miscalibration, with larger values indicating greater discrepancy between the true ($p$) and predicted probability ($g$) distributions. In this paper, miscalibration is strictly defined over $g$ and $p$.


\begin{figure*}[t] % Use figure* to force spanning both columns
    \centering
    \begin{minipage}{0.49\textwidth}
        \begin{algorithm}[H] % Use 'H' from float package to force positioning
            \sloppy
            \caption{Controlled Miscalibration via Sample Upweighting}
            \label{alg:miscal}
            \begin{algorithmic}[1]
                \STATE \textbf{Input:}
                \STATE \quad Original bigram model $M$ with transition counts trained on $S$
                \STATE \quad Training examples ${x_1,\ldots,x_n} \in S$
                \STATE \quad Number of examples to upweight, as indicated by $k$
                \STATE \textbf{Output:} Updated model $M'$ with upweighted transitions
                \STATE Initialize $M'$ as a copy of $M$
                \FOR{$i = 1$ \textbf{to} $k$}
                    \STATE $\text{tokens} \leftarrow \text{Tokenize}(x_i)$
                    \IF{$\text{len(tokens)} \geq \text{order}$}
                        \STATE $\text{start} \leftarrow \text{tokens[1:order]}$
                        \STATE $M'{\text{initial}}[\text{start}] \leftarrow M'{\text{initial}}[\text{start}] + 1$
                        \FOR{$j = 0$ \textbf{to} $\text{len(tokens)} - \text{order}$}
                            \STATE $\text{current} \leftarrow \text{tokens}[j:j+\text{order}]$
                            \STATE $\text{next} \leftarrow \text{tokens}[j+\text{order}]$
                            \STATE $M'{\text{trans}}[\text{current}][\text{next}] \leftarrow M'{\text{trans}}[\text{current}][\text{next}] + 1$
                        \ENDFOR
                    \ENDIF
                \ENDFOR
                \STATE Normalize all transition probabilities and initial state probabilities in $M'$ by calling Algorithm \ref{alg:subroutine}
                \STATE \textbf{Return} $M'$
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \begin{algorithm}[H]
            \sloppy
            \caption{Bigram Model Update Subroutine}
            \label{alg:subroutine}
            \begin{algorithmic}[1]
                \STATE \textbf{Input:}
                \STATE \quad $initial\_probs$: Dictionary of initial state probabilities
                \STATE \quad $transitions$: Dictionary of transition probabilities \{state: \{next\_state: probability, ...\}, ...\}
                \STATE \textbf{Output:} Normalized $initial\_probs$ and $transitions$
                \STATE \textbf{Procedure:}
                \STATE \textbf{1. Normalize Initial Probabilities}
                \STATE \quad $init\_total \gets \sum_{state} initial\_probs[state]$
                \FOR{each $state$ in $initial\_probs$}
                    \STATE $initial\_probs[state] \gets initial\_probs[state] / init\_total$
                \ENDFOR
                \STATE \textbf{2. Normalize Transition Probabilities}
                \FOR{each $state$ in $transitions$}
                    \STATE $transition\_total \gets \sum_{next\_state} transitions[state][next\_state]$
                    \FOR{each $next\_state$ in $transitions[state]$}
                        \STATE $transitions[state][next\_state] \gets transitions[state][next\_state] / transition\_total$
                    \ENDFOR
                \ENDFOR
                \STATE \textbf{Return} $initial\_probs$, $transitions$
            \end{algorithmic}
        \end{algorithm}
    \end{minipage}
\end{figure*}


\subsection{Controlled Miscalibration and Hallucination Reduction}

To systematically investigate how miscalibration affects hallucination while holding monofact rates constant, we introduce a training sample upweighting algorithm. Our approach allows us to induce varying levels of miscalibration by selectively reinforcing observed transitions, without altering the underlying monofact rate of the training data. 

\begin{figure*}[htbp]
   \centering
   % First figure
   \begin{minipage}{\textwidth}
       \centering
       \includegraphics[width=\textwidth]{images/pareto.png}
       \caption{Experiments with Pareto distributions. Left: Relationship between mono-fact rates and hallucination rates. Right: Effect of varying the shape parameter ($\gamma$) on monofact rates.}
       \label{fig:pareto}
   \end{minipage}

   \vspace{1em} % Small space between figures

   \begin{minipage}{\textwidth}
       \centering
       \includegraphics[width=\textwidth]{images/zipf.png}
       \caption{Experiments with Zipf distributions. Left: Relationship between mono-fact rates and hallucination rates. Right: Effect of varying the shape parameter ($\rho$) on monofact rates.}
       \label{fig:zipf}
   \end{minipage}

   \vspace{1em} % Small space between figures

   % Third figure
   \begin{minipage}{\textwidth}
       \centering
       \includegraphics[width=\textwidth]{images/comparison_miscal_mono.png}
       \caption{Relationship between miscalibration and mono-fact rates for Pareto and Zipf distributions, showing that lower mono-fact rates consistently correspond to lower miscalibration scores prior to intervention.}
       \label{fig:miscal}
   \end{minipage}
\end{figure*}
 % \vspace{-2em}
\FloatBarrier

Specifically, after initial model training, we upweight transition probabilities derived from a subset of the training data. For a given size $k$, we select $k$ training examples and amplify their contribution to the model's transition matrix by adding their token count one more time in the bigram model, effectively duplicating them. This process increases the model's confidence in those particular transitions while maintaining the same set of observed facts, allowing us to isolate the effects of miscalibration from changes in the monofact rate.

The procedure is detailed in Algorithm \ref{alg:miscal}. For each selected training example, we increment both the initial state probabilities and the bigram transition counts. After each upweighting session, we call Algorithm \ref{alg:subroutine} to maintain normalization of the bigram model's initial and transition probabilities. 

As we increase the number of upweighted examples, we should observe that upweighting of a subset of the training data leads to higher miscalibration as the model becomes overconfident in certain transitions. As the upweighted subset approaches the full training set, both miscalibration and hallucination rates should return to their baseline levels prior to any interventions, since the effect of double counting all tokens is eliminated through normalization as detailed in Algorithm \ref{alg:subroutine}. 

\begin{figure*}[t]
   \centering
   % First figure
   \begin{minipage}{\textwidth}
       \centering
       \includegraphics[width=\textwidth]{images/normal.png}
       \caption{Experiments with Gaussian distributions. Left: Relationship between mono-fact rates and hallucination rates. Right: Effect of varying the mean ($\mu$) and standard deviation ($\delta$) on monofact rates, where $\delta$ = 1/2 * $\mu$.}
       \label{fig:normal}
   \end{minipage}
   \vspace{1em} % Small space between figures

   % Second figure
   \begin{minipage}{\textwidth}
       \centering
        \includegraphics[width=\textwidth]{images/poisson.png}
        \caption{Experiments with Poisson distributions. Left: Relationship between mono-fact rates and hallucination rates. Right: Effect of varying the parameter ($\lambda$) on monofact rates, where higher $\lambda$ values indicate higher distributional means.}
        \label{fig:poisson}
   \end{minipage}
\end{figure*}

\section{$n$-Gram Model Results}
\label{sec:experiments}

Our empirical results provide support for Kalai and Vempala's theoretical lower bound (Equation \ref{eq:hallucination_bound}) on hallucination rates. Figure~\ref{fig:pareto} shows results from experiments using Pareto distributions, and Figure~\ref{fig:normal} presents findings using Gaussian distributions.

\subsection{Pareto Distribution Results}
The left panel of Figure~\ref{fig:pareto} demonstrates a positive correlation between monofact rates and hallucination rates, with the relationship appearing approximately linear. The hallucination rate increases from approximately 0\% to 45\% as the monofact rate increases from 0\% to 80\%, supporting the theoretical prediction that higher monofact rates is associated with increased hallucination \citep{kalai2024calibrated}.

The right panel of Figure~\ref{fig:pareto} illustrates how the Pareto shape parameter ($\gamma$) influences monofact rates. As $\gamma$ decreases from 4.0 to 1.0, we observe that monofact rates decrease from 80\% to 0\%, with the steepest change occurring between $\gamma$ values of 2.0 and 0.75. This aligns with theoretical expectations, as fact frequencies constructed based on a Pareto distribution with smaller $\gamma$ result in some facts appearing much more frequently than others (as indicated in Figure \ref{fig:distributions}). When this occurs, facts are more likely to appear multiple times rather than just once, leading to a smaller monofact rate, and consequently our bigram model tends to hallucinate less.

This pronounced effect likely stems from how uneven fact frequencies affect the conditional probabilities for certain word combinations. In our bigram model, when some facts appear much more frequently than others, their corresponding word pairs also appear more often, leading to stronger memorization of these specific transitions. This pattern of selective memorization creates a more pronounced relationship between monofact rates and hallucination. We find similar results for the Zipf distribution as shown in Figure~\ref{fig:zipf}. 

In Figure~\ref{fig:miscal}, we also illustrate how monofact rate organically correlates with miscalibration without any intervention. The results show that miscalibration remains low when the monofact rate is low, aligning with the intuitive fact that the lower bound on miscalibration cannot be negative.

\subsection{Gaussian Distribution Results}
The Gaussian distribution experiments (Figure~\ref{fig:normal}) reveal a differing pattern. The left panel shows a much weaker correlation between monofact rates and hallucination rates in a narrow range (between 70\% and 90\%). In fact, it is difficult to induce a wider range of monofact rates using Gaussian distributions. Despite varying the distribution parameters significantly, the monofact rates remain constrained. 

This finding makes sense as the Gaussian distribution is centered around a mean. Even when mean and standard deviation is high, most statements will just be repeated at a similarly high rate in $p$. This means the bigram model will have relatively even conditional distributions of word pairs across transitions. Unlike the Pareto distribution where certain transitions dominate. We find similar results for the Poisson distribution as shown in Figure~\ref{fig:poisson}.

% \begin{figure*}[t]
% \centering
% \includegraphics[width=\textwidth]{images/miscalibration_main_final.png}
% \caption{Relationship between miscalibration (blue line, left y-axis) and hallucination rates (red line, right y-axis) for select Pareto shape parameters $\gamma$ holding monofact constant. Dotted lines indicate metrics prior to any intervetions. Each subplot shows how miscalibration and hallucination evolve as we duplicate token occurence for more and more statement from the training data (size of 5,000).
% }
% \label{fig:miscalibration_main}
% \end{figure*}


\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{images/miscalibration_final.png}
\caption{Relationship between miscalibration (blue line, left y-axis) and hallucination rates (red line, right y-axis) for select Pareto shape parameters $\gamma$ holding monofact constant. Dotted lines indicate metrics prior to any intervetions. Each subplot shows how miscalibration and hallucination evolve as we duplicate token occurence for more and more statements from the training data (size of 5,000).
}
\label{fig:miscalibration}
\end{figure*}

\subsection{Controlled Miscalibration as Hallucination Reduction Mechanism}

Having established how monofact rates naturally arise from the underlying data distribution and drive hallucination rates, we now focus on the third key component of our analysis: model miscalibration. While monofact rates are determined by the training data distribution and thus difficult to control directly, miscalibration offers a potential lever for reducing hallucination without altering the monofact rate. 

Our experimental results demonstrate an inverse relationship between miscalibration and hallucination rates while maintaining fixed monofact rates. Figure \ref{fig:miscalibration} illustrates how miscalibration (blue line, left y-axis) and hallucination rates (red line, right y-axis) evolve as we increase the number of upweighted training examples $k \in {0, 312, ..., |S|}$, where $|S|$ denotes the size of the training set.

The miscalibration metric exhibits a sharp increase when the first subset of 312 training examples is upweighted, and maintains elevated levels as $k$ increases. However, as $k$ approaches $|S|$, the miscalibration gradually returns to baseline levels, consistent with the theoretical intuition that the impact of double counting the entire training set ($k = |S|$) should be zero post bigram normalization.

Hallucination rates show a consistent decrease as miscalibration increases, validating the theoretical prediction about their fundamental trade-off \citep{kalai2024calibrated}. This inverse relationship is particularly pronounced for training sets with higher monofact rates, where we observe hallucination reductions of up to 50\%. This suggests that the benefits of trading off calibration for reduced hallucination are most significant when the underlying data distribution produces more singleton observations.

Notably, our results indicate that even selective upweighting of a small fraction of the training data ($k \ll |S|$) can yield significant reductions in hallucination rates. This finding has important implications for LLM training pipelines. While modern language models are typically trained on deduplicated data \citep{lee2021deduplicating} to prevent memorization and improve generalization, our results suggest there may be theoretical justification for controlled reintroduction of duplicates during post-training. Such selective upweighting could serve as a principled mechanism for reducing hallucination while maintaining desired model behaviors on other tasks.


\section{LLM Methodology}
\label{sec: LLM_method}
To extend our investigation beyond $n$-gram models, we conduct similar $p$ distribution-based experiments using LLMs. We first create a synthetic universe of facts U where we can fully control and verify the true distribution. These experiments are conducted via in-context learning using Gemini \citep{team2024gemini}, selected for its accessibility and context window of 1 million tokens, which enables the input of over 1,000 statements per prompt.

\subsection{Synthetic Statement Construction} 
We generate a synthetic dataset of 1 million statements, each containing 6 distinct entities to ensure the data would be novel to the LLM (not observed during pre-training). Each entity type is carefully designed to be plausible yet previously unseen:

\begin{enumerate}
    \item Entity 1: Random 10-digit numbers
    \item Entity 2: Fictional names generated using phonetic rules
    \item Entity 3: Randomly formed ASCII letter sequences
    \item Entity 4: Procedurally generated city names
    \item Entity 5: Generated language names
    \item Entity 6: Base64-encoded 16-byte random strings
\end{enumerate}


These entities are combined into coherent statements following a fixed template: ``On day [Entity 1], [Entity 2] convened with [Entity 3] to debate the secret message they found in an obscure cave in [Entity 4], while whispering in [Entity 5], which reads [Entity 6].''


\begin{figure*}[t]
   \centering
   % First figure
   \begin{minipage}{\textwidth}
       \centering
       \includegraphics[width=\textwidth]{images/gemini_binned_boxplot_pareto.png}
       \caption{Distribution of hallucination rates as a function of monofact rate. Each box plot shows the distribution of hallucination rates across 100 iterations. The x-axis shows increasing monofact rates (10\% to 90\%). Higher monofact consistently lead to increased hallucination rates.}
       \label{fig:gemini_pareto_boxplot}
   \end{minipage}

   \vspace{1em} % Small space between figures

   % Second figure
   \begin{minipage}{\textwidth}
       \centering
       \includegraphics[width=\textwidth]{images/gemini_binned_boxplot_normal.png}
       \caption{Distribution of hallucination rates as a function of monofact rate and $\mu$ and $\delta$ = 1/2 *  $\mu$. Each box plot shows the distribution of hallucination rates across 100 iterations.}
       \label{fig:gemini_normal_boxplot}
   \end{minipage}
\end{figure*}


\subsection{Distribution Construction and Sampling}
Similar to our bigram experiments, we construct different fact frequency distributions of $p$ over the true facts $T$. This allows us to systematically vary the monofact rate in our sampled training data S. In this setting, we test the Pareto and Gaussian distributions. 

\subsection{Evaluation}
Following the same methodology as our bigram experiments, we define hallucination as any statement not in $T$. This controlled setting allows us to precisely quantify hallucination rates and evaluate how different $p$ distributions affect LLM behavior.

\section{LLM Results}
\phantomsection
\label{sec: LLM_results}
Our LLM experiments provide additional empirical support for Kalai and Vempala's theoretical upper bound on hallucination rates using in-context learning. As illustrated in Figure~\ref{fig:gemini_pareto_boxplot}, we evaluate how monofact rates change as we vary how often facts repeat (controlled by the Pareto shape parameter $\gamma$), and how this influences hallucination across 100 rounds of in-context learning. Two main insights emerge from these results.

We observe a direct, positive relationship between monofact rate and hallucination rate. As monofact rate increases from 10\% to 90\%, hallucination rates consistently rise across all Pareto shape parameters. This trend aligns with the theoretical claim that the monofact rate directly raises the theoretical bound on hallucinations.

When we conduct the same experiment using Gaussian distribution as shown in Figure \ref{fig:gemini_normal_boxplot} the underlying distribution is unable to yield a wide range of monofact rates. For the range of monofact rates  (70\% to 90\%) that we are able to produce, we find no relationship between hallucination and monofact rates.

Overall, these experiments highlight the interplay between monofact rates and repetition patterns in governing the prevalence of hallucinations in the context of LLMs.

% \subsection{Systematic vs.\ Arbitrary Facts}
% Our experiments show that facts appearing frequently in training lead to fewer hallucinations. This helps explain why \emph{systematic facts} (e.g., $1 < 2$''), which appear repeatedly in structured form, are learned more reliably than \emph{arbitrary} facts (e.g., Person X was born on date Y''), which appear infrequently and in varying forms.

\section{Conclusion and Future Work}
\phantomsection
\label{sec:conclusion}

Through systematic experiments with $n$-gram models and LLM in-context learning, we studied how monofact rates, miscalibration, and hallucination interact. Our results validate Kalai and Vempala's lower bound for hallucination, showing that the true data distribution critically influences monofact rates --- patterns where facts appear with varying frequency affect both monofact rates and hallucination. Specifically, when certain facts appear much more frequently than others, we observe lower monofact rates and reduced hallucination. We also found a trade-off between model calibration and factual reliability: controlled miscalibration can lower hallucination at a fixed monofact rate. These findings highlight the importance of fact repetition patterns and strategic calibration management. Notably, our upweighting experiments suggest selective data duplication can manage this trade-off, challenging aggressive deduplication and offering insights for building more reliable language models.


Future research could focus on developing more sophisticated techniques for mitigating hallucination in LLMs. A crucial direction will be to investigate how the observed relationships between monofact rates, hallucination, and miscalibration translate to more complex model architectures. Post-training techniques to strategically introduce duplications to reduce hallucination could be an interesting direction.

\section{Acknowledgements}

We give warm thanks to Adam Kalai and Chris Callison-Burch for helpful discussions.

\iffalse
\section{Impact Statement}

This work has significant implications for improving the reliability and safety of language models. Our empirical validation of the relationship between monofact rates, miscalibration, and hallucination provides concrete insights that could help reduce harmful hallucinations in high-stakes applications such as legal research and medical advice, where factual accuracy is crucial. 

% The findings that selective duplication of training examples can reduce hallucination rates challenges current practices of deduplication in model training pipelines and suggests a principled approach for improving model reliability.

More broadly, this research contributes to the growing body of work on making AI systems more trustworthy and reliable. By providing empirical evidence for fundamental statistical relationships in language model behavior, we advance our understanding of how to build safer AI systems that can be deployed more confidently in critical applications. The controlled experimental framework we develop also provides a foundation for future work on systematic evaluation and mitigation of hallucination in more complex language models.
\fi

\bibliography{main}
\bibliographystyle{plainnat}
% \include{Appendix_arXiv}
\end{document}