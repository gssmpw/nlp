\section{Related Work}
An important line of theoretical research shows that when training data contains many rare examples, memorization becomes indispensable for achieving low error. ____ demonstrates that for such distributions with frequent rare occurrences, any algorithm aiming for small error must memorize a substantial fraction of these uncommon instances. This observation connects directly to the theoretical lower bound on hallucination, which shows that well-calibrated language models must hallucinate at least at the rate of facts that appear only once in training ____. In other words, when many facts occur infrequently, models face a tension between avoiding hallucinations and maintaining calibration: the scarceness of certain facts forces either deeper memorization or an increased risk of fabricated outputs.

Recent empirical work reinforces the link between unfamiliar examples and hallucination. ____ introduce a “familiarity score” that quantifies how closely a model’s training data matches a test example, and they observe that hallucination rates grow almost linearly with unfamiliarity. Their findings echo the notion of “monofacts” by highlighting how rare or singly observed statements pose the greatest challenge for accurate prediction. ____ similarly show that novel facts are learned more slowly by language models, and that integrating these facts tends to elevate hallucination rates in a linear fashion. Both studies underscore the inherent difficulty models face when handling sparse knowledge or atypical data.

Collectively, these works suggest that hallucination arises in part from the basic statistical structure of real-world datasets, especially when they include many low-frequency facts. The more the underlying distribution skews toward these rare observations, the higher the baseline for hallucinations becomes.