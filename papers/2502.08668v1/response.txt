\section{Related Works}
\label{sec:related_works}

\subsection{Styles in Natural Languages}
In linguistics, style has been seen as the unique way individuals or groups engage in conversation, conveying politeness or formality, and able to be controlled and adjusted to suit the intended social context **Biber, "Variation Across Speech and Writing"**. Additionally, style is the set of linguistic features such as tone, punctuation, word choice, and syntactic structure, playing a key role in stylistics and sentiment analysis **Halliday, "Language and Social Semiotics"**. 

Subsequently, research on style transfer in text has considered style as the specific manner in which ideas are expressed in text, distinguishable from the content **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**. In style transfer tasks, style is represented by personal style, formality, politeness, offensiveness, genre, and sentiment **Lample et al., "Monolingual Unsupervised Machine Translation"**. Current studies of styles focus on computational models for style transfer; Cross-Alignment with non-parallel text **Hill et al., "Learning Discourse Parsing from Non-Parallel Text"**, Retrieve-and-Edit approach **Gupta and Lebret, "Diverse Beaming for Improved Language Translation"**, Unsupervised style transfer **Lample and Conneau, "Unsupervised Machine Translation by Cross-Lingual Alignment of Codenames"**, Generative probabilistic model **Rezende et al., "Stochastic Backpropagation through the Guide Distribution"**, and Reinforcement Learning for style transfer **Papineni et al., "Rapid Progress in Question Answering"**.

\subsection{Evaluating Style Transfer in Text}
Evaluation metrics are vital for text style transfer as they provide precise, quantitative assessments of how effectively the generated text adheres to the target stylistic attributes while preserving semantic integrity. However, evaluation can be challenging due to the subjective nature of style. It typically involves automatic evaluation and human evaluation **Kang et al., "Comparison of Human Evaluation and Automatic Evaluation for Text Style Transfer"**.

\subsubsection{Automatic Evaluation}
The automatic evaluation measures how well the meaning of the original sentence was preserved in the output (generated sentence). The following metrics are commonly used:
\begin{itemize}
    \item BLEU: Measures n-gram precision between generated text and references **Papineni et al., "BLEU: A Method for Automatic Evaluation of Machine Translation"**.
    \item ROUGE: Assesses overlap of n-grams, focusing on recall to evaluate content coverage **Lin, "ROUGE: A Package for Automatic Evaluation of Summarization Systems"**.
    \item METEOR: Evaluates translation quality using precision, recall, stemming, and synonymy **Banerjee and Lavie, "METEOR: An Automatically Evaluated Metric for Machine Translation"**.
    \item BERTScore: Utilizes BERT embeddings to measure semantic similarity between generated and reference texts **Zhang et al., "BERTScore: Evaluating Text Generation with BERT"**.
\end{itemize}

\subsubsection{Human Evaluation}
Human judges assess how well the generated text adheres to the desired style and maintains semantic integrity. **Liu et al., "Human Evaluation for Text Style Transfer"** delineates the distinctions between human evaluation and automatic methods, illustrating how human assessment captures nuanced stylistic and semantic subtleties. However, it is costly and lacks the consistency, objectivity, and scalability provided by automatic evaluation methods. Additionally, both methods are limited by their reliance on reference texts, which may not fully capture the breadth of acceptable outputs or the creative potential of the generated text. 

\subsection{Anomaly Detection and VAE}
Anomaly detection has evolved through various methodologies to address the challenge of identifying outliers across different domains **Chandola et al., "Anomaly Detection: A Survey"**. The advent of deep learning introduced Autoencoders **Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders"**, which makes it possible to detect anomalies in high-dimensional data by analyzing reconstruction errors. Further advancements have been made with Variational Autoencoders (VAE), which leverage both probabilistic modeling and latent space representations **Kingma et al., "Variational Dropout and the Local Reparameterization Trick"**. We will employ a VAE model, trained on high-dimensional embedding vectors representing a single stylistic attribute, to identify anomalies by capturing deviations in stylistic characteristics.