\section{Results}\label{sec:results}

\subsection{Training Convergence and Loss Analysis}

For all 36 hyperparameter combinations, both the training loss and test loss decreased and eventually converged, indicating that the models successfully learned from the data and reached a stable state in terms of reconstruction error. Detailed loss curves and analysis are provided in Figure \ref{fig:appendix_loss}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{test_set_loss.png}
    \caption{Test set loss during training. The x-axis represents the number of epochs, and the y-axis represents the mean error. The hyperparameters of each model are as follows: starting from left the 1st, 2nd, and 3rd columns represent feature dimensions of 8, 64, and 256, respectively, and the starting from top 1st, 2nd, and 3rd rows represent 1, 3, and 6 hidden layers, respectively.}
    \label{fig:appendix_loss}
\end{figure}

\subsection{L2 Error Distribution and FLD Analysis}

The L2 error distribution for each model is presented in Figure \ref{fig:appendix_l2_error}. The minimum Fisher’s Linear Discriminant (FLD) between the L2 norm distributions of the reconstructed sentence vectors from the trained dataset (ASV) and the anomaly datasets (NET, ASVS, Coverdale, Geneva, KJV Strongs) across the 36 models is shown in Figure \ref{fig:appendix_fld}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{distribution.jpg}
    \caption{L2 error distribution on ASV, NET, ASVS, Coverdale, Geneva, and KJV Strongs. The x-axis represents the L2 error between the original and reconstructed sentence vector, and the y-axis represents the distribution density. The hyperparameters of each model are as follows: starting from left the 1st, 2nd, and 3rd columns represent feature dimensions of 8, 64, and 256, respectively, and starting from top the 1st, 2nd, and 3rd rows represent 1, 3, and 6 hidden layers, respectively.}
    \label{fig:appendix_l2_error}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{FLD.jpg}
    \caption{(Left) Minimum and (Right) Maximum of FLD between ASV and other 5 anomaly datasets (NET, ASVS, Coverdale, Geneva, and KJV Strongs). A higher minimum FLD indicates better differentiation between ASV and anomaly L2 error distributions.}
    \label{fig:appendix_fld}
\end{figure}

The minimum FLD is more important than the maximum FLD for determining the separation between normal and anomaly data. A high minimum FLD represents the model that has the most differentiation between the ASV original and the anomaly reconstructions, indicating the best-performing model in terms of distinguishing between the original and anomalous styles based on the L2 norm distribution. Figure \ref{fig:appendix_fld} shows that the minimum FLD is maximized in models with 3 hidden layers and a feature dimension size between 32 and 128. Models with too small or too large hidden layers and feature dimensions tend to perform poorly in anomaly differentiation.

Across the 36 models, the anomaly dataset that produced the minimum FLD most frequently was Geneva, appearing 31 times, followed by KJV Strongs, which appeared 5 times. This suggests that the L2 error distribution of the Geneva dataset was generally the closest to that of ASV, making it the hardest to distinguish from ASV. Conversely, the anomaly dataset that consistently produced the maximum FLD in all 36 models was Coverdale, indicating that it was the easiest to distinguish from ASV based on the L2 error distribution. This result highlights the distinctiveness of Coverdale's style compared to ASV, while Geneva's style appears more similar.

\subsection{Impact of Context Subtraction on VAE Performance}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{no_sub_distribution.jpg}
    \caption{L2 error distribution on ASV, NET, ASVS, Coverdale, Geneva, and KJV Strongs, without parallel sentence (KJV) subtraction. The x-axis represents the L2 error between the original and reconstructed sentence vector, and the y-axis represents the distribution density. The hyperparameters of each model are as follows: starting from left the 1st, 2nd, and 3rd columns represent feature dimensions of 8, 64, and 256, respectively, and the starting from top 1st, 2nd, and 3rd rows represent 1, 3, and 6 hidden layers, respectively.}
    \label{fig:appendix_no_sub_l2_error}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{no_sub_FLD.jpg}
    \caption{(Left) Minimum and (Right) Maximum of FLD between ASV and other 5 anomaly datasets (NET, ASVS, Coverdale, Geneva, and KJV Strongs), without parallel sentence (KJV) subtraction. A higher minimum FLD indicates better differentiation between ASV and anomaly L2 error distributions.}
    \label{fig:appendix_no_sub_fld}
\end{figure}

Training the VAE without subtracting context parallel sentence (KJV) vectors demonstrated that both the training loss and test loss decreased and converged, indicating successful learning. However, as shown in Figure \ref{fig:appendix_no_sub_l2_error} in Appendix F, the mean L2 error across all distributions was higher compared to the models trained with parallel sentence subtraction.

When comparing Figures \ref{fig:appendix_no_sub_fld} and \ref{fig:appendix_fld}, the Fisher’s Linear Discriminant (FLD) for the no-subtraction case (from context parallel sentence vectors) is significantly lower than for the subtracted case. Specifically, the mean of the minimum FLD across the 36 models in the subtracted case is 1.111, while the mean for the no-subtraction case is 0.116, making the FLD approximately 9.6 times lower without subtraction.

Furthermore, the highest maximum FLD in the no-subtraction case (1.000) is nearly the same as the lowest minimum FLD in the subtracted case (0.983). This stark difference in FLD highlights that when trained without subtracting the context parallel sentence vectors, the VAE's ability to distinguish anomalies from normal (trained domain) data is significantly diminished. This result reinforces the idea that the subtraction of context helps the VAE better capture stylistic differences, leading to clearer separation between ASV and other translations.

\section{Discussion}\label{sec:discussion}

This study extracted the styles of various Bible translations and utilized a Variational Autoencoder (VAE) model to analyze how these styles differ, particularly in comparison to the American Standard Version (ASV). The results revealed that the styles of each Bible translation followed a normal distribution, and these distributions could be clearly distinguished from that of the ASV. This indicates that there are stylistic differences between the ASV and other translations, and that these differences can be effectively detected using the VAE model.

After optimizing the VAE model’s hyperparameters, the process of distinguishing between the ASV and other translation styles resulted in a Type 1 error of 8.7\% and a Type 2 error of 6.7\%, with a total error rate of 15.3\%. Conversely, the model achieved an accuracy of 84.7\%, demonstrating its ability to effectively differentiate styles. This level of accuracy suggests that the model can clearly recognize the distribution of a specific style and use it as a basis to distinguish between the styles of different translations.

However, the VAE model was optimized for distinguishing a single style. While it was useful for detecting differences between a specific translation style and the ASV, it had limitations when it came to distinguishing multiple styles simultaneously or understanding the relationships between complex, multi-dimensional styles. These limitations stem from the structural characteristics of the VAE, which compresses the data’s features during learning, making it inherently challenging to fully capture the complex characteristics of the data. Therefore, to distinguish multiple styles simultaneously, it may be necessary to use other models or train the VAE model in a more sophisticated manner.

The ability to extract a specific style suggests that the style’s characteristics can be quantified and represented as a probability distribution. This means that AI can utilize this quantified style representation to generate text that adheres to a specific style. For example, in text generation tasks where a particular writing style or tone is required, a 'style metric' could be used as a numerical and comparable indicator to assess and ensure that the generated text conforms to the desired style.

The approach taken in this study opens up the possibility of expanding the research to other parallel text datasets. By applying this methodology to other text domains, researchers can study the stylistic differences and their implications within each domain. For example, the approach could be extended to analyze the styles of different translations of literary works, legal document translations, or works by various authors.

We have demonstrated that the VAE model can distinguish between the original and anomaly data using the reconstruction L2 error. To measure the overall accuracy, False Positive Rate (FPR), and False Negative Rate (FNR) of the model, we created an Accuracy Test Dataset using data not included in the training set. This dataset consisted of 1,000 samples, with 50\% of the samples being from ASV and the remaining 50\% from five anomaly datasets (NET, ASVS, Coverdale, Geneva, and KJV Strongs).

\begin{table}[t]
\caption{Accuracy \& Error Rates of Models 1, 2, and 3 on Anomaly Detection}
\label{tab:accuracy}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\setlength{\tabcolsep}{4pt} % Default value: 6pt. Reduce this to make the table narrower.
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Type I Error} & \textbf{Type II Error} \\
\hline
Model 1 & 83.5\% & 9.8\% & 6.7\% \\
Model 2 & 82.9\% & 10.1\% & 7.0\% \\
Model 3 & 83.4\% & 9.8\% & 6.8\% \\
\hline
\textbf{Average} & \textbf{83.3\%} & \textbf{9.9\%} & \textbf{6.8\%} \\
\hline
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


The binary classification results showed that the lowest overall error rate was achieved when the threshold was set at mean + 0.8 std. The average overall error rate across the three models was 16.8\%. The relatively high FNR, particularly with Geneva, suggests that modern English Bible translations inherently do not exhibit distinct stylistic differences.

The results of the anomaly detection using the VAE in this study also show trends similar to what would be expected when humans classify ASV and other Bible versions. In this study, the L2 error distributions of ASV and Geneva had a significant overlap, making it difficult to classify them with a low error rate using a specific threshold. In contrast, the L2 error distribution of Coverdale barely overlapped with ASV, and the FLD was the highest across all models. More typically, 67.8\% (ASV 34.6\%, KJV Strongs 33.2\%) of type 2 error in anomaly detection is from Geneva and KJV Strongs.

\begin{table}[t]
\caption{Original Sentences of 3 Different Versions: ASV, Geneva, Coverdale}
\label{tab:verses}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabularx}{0.45\textwidth}{lX}
\toprule
\textbf{Verse} & \textbf{Translation} \\
\midrule
\textbf{Gen 1:1} & \textbf{ASV:} In the beginning God created the heavens and the earth. \newline
\textbf{Geneva:} In the beginning God created the heauen and the earth. \newline
\textbf{Coverdale:} In ye begynnynge God created heauen \& earth: \\
\midrule
\textbf{Mat 1:1} & \textbf{ASV:} The book of the generation of Jesus Christ, the son of David, the son of Abraham. \newline
\textbf{Geneva:} The book of the generation of Jesus Christ the son of David, the son of Abraham. \newline
\textbf{Coverdale:} This is the boke of the generacion of Iesus Christ ye sonne of Dauid, the sonne of Abraham. \\
\midrule
\textbf{Joh 3:16} & \textbf{ASV:} For God so loved the world, that he gave his only begotten Son, that whosoever believeth on him should not perish, but have eternal life. \newline
\textbf{Geneva:} For God so loveth the world, that he hath given his only begotten Son, that whosoever believeth in him, should not perish, but have everlasting life. \newline
\textbf{Coverdale:} For God so loued the worlde, that he gaue his onely sonne, that who so euer beleueth in hi, shulde not perishe, but haue euerlastinge life. \\
\bottomrule
\end{tabularx}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Table \ref{tab:verses} illustrates the textual differences between three versions of the Bible (ASV, Geneva, Coverdale), which could influence the VAE's ability to distinguish anomalies. The relatively low accuracy of anomaly detection using the VAE in this study may be attributed to the subtle stylistic differences between the texts. This implies that using sentences with clearer stylistic differences and more varied contexts in future experiments could result in better accuracy.