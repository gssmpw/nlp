\section{Related Works}\label{sec:related_works}

\subsection{Styles in Natural Languages}
In linguistics, style has been seen as the unique way individuals or groups engage in conversation, conveying politeness or formality, and able to be controlled and adjusted to suit the intended social context \cite{Labov1997}. Additionally, style is the set of linguistic features such as tone, punctuation, word choice, and syntactic structure, playing a key role in stylistics and sentiment analysis \cite{pang}. 

Subsequently, research on style transfer in text has considered style as the specific manner in which ideas are expressed in text, distinguishable from the content \cite{shen}. In style transfer tasks, style is represented by personal style, formality, politeness, offensiveness, genre, and sentiment \cite{Toshevska_2022}. Current studies of styles focus on computational models for style transfer; Cross-Alignment with non-parallel text \cite{shen}, Retrieve-and-Edit approach \cite{li-etal-2018-delete}, Unsupervised style transfer \cite{prabhumoye-etal-2018-style}, Generative probabilistic model \cite{he2020probabilisticformulationunsupervisedtext}, and Reinforcement Learning for style transfer \cite{gong2019reinforcementlearningbasedtext}.

\subsection{Evaluating Style Transfer in Text}
Evaluation metrics are vital for text style transfer as they provide precise, quantitative assessments of how effectively the generated text adheres to the target stylistic attributes while preserving semantic integrity. However, evaluation can be challenging due to the subjective nature of style. It typically involves automatic evaluation and human evaluation \cite{jin-etal-2022-deep}.

\subsubsection{Automatic Evaluation}
The automatic evaluation measures how well the meaning of the original sentence was preserved in the output (generated sentence). The following metrics are commonly used:
\begin{itemize}
    \item BLEU: Measures n-gram precision between generated text and references \cite{papineni-etal-2002-bleu}.
    \item ROUGE: Assesses overlap of n-grams, focusing on recall to evaluate content coverage \cite{lin-2004-rouge}.
    \item METEOR: Evaluates translation quality using precision, recall, stemming, and synonymy \cite{banerjee-lavie-2005-meteor}.
    \item BERTScore: Utilizes BERT embeddings to measure semantic similarity between generated and reference texts \cite{zhang2020bertscoreevaluatingtextgeneration}.
\end{itemize}

\subsubsection{Human Evaluation}
Human judges assess how well the generated text adheres to the desired style and maintains semantic integrity. \cite{Yamshchikov_2021} delineates the distinctions between human evaluation and automatic methods, illustrating how human assessment captures nuanced stylistic and semantic subtleties. However, it is costly and lacks the consistency, objectivity, and scalability provided by automatic evaluation methods. Additionally, both methods are limited by their reliance on reference texts, which may not fully capture the breadth of acceptable outputs or the creative potential of the generated text. 

\subsection{Anomaly Detection and VAE}
Anomaly detection has evolved through various methodologies to address the challenge of identifying outliers across different domains \cite{NIPS1999_8725fb77, 4781136}. The advent of deep learning introduced Autoencoders \cite{article}, which makes it possible to detect anomalies in high-dimensional data by analyzing reconstruction errors. Further advancements have been made with Variational Autoencoders (VAE), which leverage both probabilistic modeling and latent space representations \cite{kingma2022autoencodingvariationalbayes}. We will employ a VAE model, trained on high-dimensional embedding vectors representing a single stylistic attribute, to identify anomalies by capturing deviations in stylistic characteristics.
