\section{Methodology}\label{sec:methodology}

\subsection{Data Collection and Preprocessing}
This study utilizes biblical data collected from \textit{Bible SuperSearch} \cite{biblesupersearch}, a platform operating under the GNU GPL open source license. Ten different versions were initially considered: KJV, NET, ASV, ASVS, Coverdale, Geneva, KJV\_Strongs, Bishops, Tyndale, and WEB. However, Bishops, Tyndale, and WEB were excluded due to insufficient parallel data. The remaining versions were selected for their linguistic diversity and historical backgrounds to enhance the depth of our style classification study.

The biblical texts are publicly available under the GNU GPL license, allowing free use for research purposes. Our study adhered to these guidelines without altering the original texts. In the preprocessing phase, we extracted the biblical data in JSON format and encoded all text files using UTF-8 to handle special characters. The initial data quality was high, minimizing the need for extensive text cleaning.

\subsection{Embedding and Model Training}
We employed OpenAI's text-embedding-3-small model to embed each biblical sentence into 1536-dimensional vectors. This model was chosen for its balance between performance and computational efficiency, making it suitable for our research. These high-dimensional vectors capture the nuanced language style of the sentences, providing foundational data for style-based classification.

\subsection{Style Extraction}\label{subsec:style_extraction}
Text embedding is assumed to include both content and style, as represented by the following equation:

\[
\text{text\_embedding} = \text{style\_embedding} + \text{content\_embedding}
\]

Under this assumption, text embedding can be seen as simultaneously containing both the content and stylistic features of the text. In this study, we utilized this assumption to perform an analysis based on Bible data. The Bible data consists of the same verse expressed in multiple translations in a parallel structure, where the content remains the same, but the style varies. This characteristic of Bible data justifies the assumption that each translation’s content embedding is identical. That is, the differences between the translations are primarily due to style, allowing for style analysis to be conducted. The core assumption of this study is that the difference in text embeddings between translations reflects the difference in styles. This can be expressed mathematically as follows:

\begin{multline*}
\text{KJV\_embedding} - \text{Other\_embedding} = \\
\text{KJV\_style\_embedding} - \text{Other\_style\_embedding}
\end{multline*}

Through this relationship, we calculated the difference between the two text embeddings and, based on this, measured the difference in style between the translations. Specifically, the goal of the study was to analyze the text embedding differences between KJV (King James Version) and other translations (e.g., ASV (American Standard Version)) to quantify the stylistic features. To do this, we calculated the difference between embeddings, represented as 1536-dimensional vectors, and used Variational Autoencoder (VAE) as a tool to analyze the distribution of these vectors.

The VAE is an unsupervised learning method that models the distribution of data in a latent space. In this study, we aimed to utilize the VAE to classify the embedding differences between translations and detect stylistic differences through anomaly detection. By compressing the input data and reconstructing it, VAE retains the important features while learning the distribution, allowing for the modeling of stylistic differences between translations.

During the training process of the VAE, we used the distribution differences between \textit{KJV\_embedding} and \textit{ASV\_embedding}. The VAE learned the difference between KJV and ASV embeddings in the latent space and then measured the similarity between the reconstructed distribution and the original distribution. We computed the L2-norm in this reconstruction process to quantitatively evaluate the stylistic similarity or difference between the translations. This allowed us to analyze the stylistic differences between KJV and ASV, as well as conduct comparative analyses with other translations.

\begin{table}[htbp]
\caption{Notation used throughout this article.}
\label{tab:notation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{0.12\textwidth} p{0.25\textwidth}}
\toprule
\textbf{Symbol} & \textbf{Description} \\
\midrule
$\mathbf{k}^{(i)}$ & Embedding of KJV, \newline $i = 1, \cdots, N$ \\
$\mathbf{a}^{(i)}$ & Embedding of ASV \\
$\mathbf{y}^{(i)}_j$ & Embedding of other Bibles,\newline $j = 1, \cdots, 5$ \\
$\mathbf{x}^{(i)}$ & KJV\_style\_embedding \newline $-$ ASV\_style\_embedding \\
$\mathbb{R}^d$ & $d$-dimensional input space \\
$\mathbb{R}^p$ & $p$-dimensional \newline feature space ($p < d$) \\
$\psi:\mathbb{R}^d \rightarrow \mathbb{R}^p$ & Encoder of VAE \\
$\theta:\mathbb{R}^p \rightarrow \mathbb{R}^d$ & Decoder of VAE \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{algorithm}[htbp]
   \caption{VAE Training Process}
   \label{alg:vae_training}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\mathbf{x}^{(i)}$ \quad \COMMENT{Training data}
   \STATE {\bfseries Output:} $w_{\psi}$ (encoder parameters), $w_{\theta}$ (decoder parameters)
   \STATE Initialize parameters $w_{\psi}$, $w_{\theta}$
   \REPEAT
      \FOR{$i=1$ {\bfseries to} $N$}
         \STATE $z^{(i)} = \psi(\mathbf{x}^{(i)}, w_{\psi})$ \quad \COMMENT{Generate feature vectors}
         \STATE $\hat{\mathbf{x}}^{(i)} = \theta(z^{(i)}, w_{\theta})$ \quad \COMMENT{Reconstruct original embedding}
      \ENDFOR
      \STATE $\mathcal{L}_{\text{mse}} = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{x}^{(i)} - \hat{\mathbf{x}}^{(i)})^2$
      \STATE Update $w_{\psi}$ and $w_{\theta}$ using gradients of $\mathcal{L}_{\text{mse}}$
   \UNTIL{parameters $w_{\psi}$ and $w_{\theta}$ converge}
   \STATE \textbf{return} $w_{\psi}$, $w_{\theta}$
\end{algorithmic}
\end{algorithm}



 In conclusion, this study evaluated the stylistic differences between Bible translations using VAE for anomaly detection. Through this process, we effectively quantified the stylistic similarities and differences between various translations. Based on the VAE model, trained on the difference between \textit{KJV\_embedding} and \textit{ASV\_embedding}, we similarly analyzed the stylistic differences between other translations. This methodology enabled sophisticated text analysis that went beyond merely examining content features to include stylistic features. Thus, we provided new insights into how stylistic differences manifest within the embedding space

\subsection{Model Architecture and Training Details}
The VAE model used in this study has an input dimension of 1536, and both encoder and decoder use fully connected (FC) layers. The size of each hidden layer follows a geometric sequence from the input dimension of 1536 to the final feature dimension (rounded to the nearest integer). Batch normalization is applied to all layers except the final output layers of both the encoder and decoder. The activation function used is Leaky ReLU ($\alpha$=1e-2) except for the final output layer of the encoder and decoder. The final output layer of the decoder uses a Sigmoid-based activation function to ensure that the output distribution lies within the range [-1,1].

The hyperparameters are as follows: 6 values for the number of hidden layers (ranging from 1 to 6) and 6 values for the feature dimension (ranging from $2^3$ to $2^8$), resulting in 36 total combinations.

We split 13,823 sentence vectors into training and test sets with a 9:1 ratio, using KJV-ASV differences as training data. The model employs fully connected layers with batch normalization and Leaky ReLU activation, and is trained using the Adam optimizer and MSE loss function. A schematic of the model structure is provided in Figure \ref{fig:model structure}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{VAE_Model_Structure.png}
    \caption{A schematic illustration of the VAE model. The encoder receives a 1,536-dimensional original (sentence embedding) vector as input and outputs a feature vector of the feature dimension. The decoder takes the feature vector of the feature dimension as input and outputs a 1,536-dimensional reconstructed vector.}
    \label{fig:model structure}
\end{figure}

\subsection{Evaluation Metrics}\label{subsec:evaluation_metrics}
According to our hypothesis, the KJV-ASV vector is expected to contain information related to the style of ASV, with KJV as the reference point. If a VAE with a sufficiently small feature dimension can effectively reconstruct this vector, it suggests that the VAE is leveraging specific stylistic features during the encoding-decoding process. On the other hand, if data not included in the model’s training process are reconstructed through the VAE, the reconstruction quality is expected to be poor compared to the original. Based on this characteristic, we aim to perform anomaly detection using the VAE.

We aim to verify whether the VAE, trained using KJV-ASV vectors, has effectively learned the unique style of ASV. To do so, the trained VAE will be applied to six Bible translations (ASV, NET, ASVS, Coverdale, Geneva, and KJV Strongs), and we will examine if the model successfully distinguishes ASV’s unique style compared to other translations. For the test dataset (not used during model training), ASV will serve as the normal data, and the other five translations (NET, ASVS, Coverdale, Geneva, and KJV Strongs) will serve as anomaly data, consisting of sentence embedding vectors corresponding to the same Bible verses as in the test dataset. To remove the context of KJV during ASV training, the VAE was trained on the differences between the sentence vectors of ASV and KJV (KJV-ASV). Similarly, the anomaly data from the other Bible translations will be processed by subtracting the corresponding KJV sentence vectors, following the same procedure.

Among the 36 hyperparameter sets, the model that most clearly differentiates the reconstruction L2 error distribution between the training data and the anomalies will be considered the most effective in detecting the unique style of ASV. We will evaluate how well the original data and anomaly data are distinguished using Fisher’s Linear Discriminant (FLD). FLD increases as the squared difference between the means of the two distributions becomes larger, and the sum of their variances becomes smaller. The formula for FLD \( S \) is as follows:

\[
S = \frac{(\mu_1 - \mu_2)^2}{\sigma_1^2 + \sigma_2^2}
\]

where \( \mu_1 \) and \( \mu_2 \) are the means of the original data and anomaly data distributions, respectively, and \( \sigma_1 \) and \( \sigma_2 \) are the variances of the original data and anomaly data distributions, respectively. This metric will help quantify how well the model separates the 
original data from anomalies based on reconstruction errors.

\begin{algorithm}[htbp]
   \caption{Anomaly Detection by VAE}
   \label{alg:anomaly_detection}
\begin{algorithmic}[1]
   \STATE \textbf{Input:} $\mathbf{a}^{(i)}, \mathbf{y}^{(i)}_j$, Trained parameters by $\mathbf{x}^{(i)}$: $w_{\psi}, w_{\theta}$, $\alpha = 0.1, \ldots, 1.4$
   \STATE \textbf{Output:} Fisher’s Linear Discriminant (FLD): $S_j$
   \FOR{$i = 1$ \textbf{to} $N$}
      \STATE $z^{(i)} = \psi(\mathbf{a}^{(i)}, w_{\psi})$
      \STATE $\hat{\mathbf{a}}^{(i)} = \theta(z^{(i)}, w_{\theta})$
      \FOR{$j = 1$ \textbf{to} $5$}
         \STATE $z^{(i)}_j = \psi(\mathbf{y}^{(i)}_j, w_{\psi})$
         \STATE $\hat{\mathbf{y}}^{(i)}_j = \theta(z^{(i)}_j, w_{\theta})$
      \ENDFOR
   \ENDFOR
   \STATE $\ell_{2,\mathbf{a}} = \|\mathbf{a}^{(i)} - \hat{\mathbf{a}}^{(i)}\|_2$
   \STATE $\ell_{2,\mathbf{y}_j} = \|\mathbf{y}^{(i)}_j - \hat{\mathbf{y}}^{(i)}_j\|_2$
   \STATE $\mu_{\mathbf{a}}, \sigma_{\mathbf{a}}, \mu_{\mathbf{y}_j}, \sigma_{\mathbf{y}_j} \leftarrow$ mean and standard deviation of $\ell_{2,\mathbf{a}}, \ell_{2,\mathbf{y}_j}$
   \STATE Find the threshold minimizing total error: $\gamma = \mu_{\mathbf{a}} + \alpha \sigma_{\mathbf{a}}$
   \STATE $S_j = \frac{(\mu_{\mathbf{a}} - \mu_{\mathbf{y}_j})^2}{\sigma_{\mathbf{a}}^2 + \sigma_{\mathbf{y}_j}^2}$
   \IF{$S_j > \gamma$}
      \STATE $\mathbf{y}_j$ is anomaly
   \ELSE
      \STATE $\mathbf{y}_j$ is not anomaly
   \ENDIF
   \STATE \textbf{return} $S_j$
\end{algorithmic}
\end{algorithm}