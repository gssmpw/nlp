\section{Related works}
\textbf{Multi-Vehicle Cooperative Decision-Making Based on Reinforcement Learning}:
In recent years, the problem of multi-vehicle cooperative decision-making in dynamic traffic environments has garnered significant attention. Rule-based or optimization-based methods often struggle to scale and adapt in complex traffic scenarios. Increasingly mature reinforcement learning algorithms such as MADQN, MADDPG, MAPPO, and Qmix enable agents to learn optimal strategies through interaction with the environment \cite{wang2024research}. These algorithms have significantly improved the quality of multi-vehicle interaction decision-making in scenarios such as unsignalized intersections \cite{zhuang2023cooperative, HADDAD2022105019, li2020deep, 10417752}, highway ramps \cite{10643142, 10159552, zhao2023multi}, and mixed scenarios \cite{louati2024sustainable, 10123696}.

Despite the success of existing multi-vehicle cooperative decision-making methods in certain fixed scenarios, designing efficient reinforcement learning algorithms that account for the dynamic characteristics of traffic flow remains an open research challenge in complex, continuous traffic flow environments.

\textbf{Reward Function Design in RL}:
In reinforcement learning, the reward function is crucial for guiding the learning process of agents. Designing an appropriate reward function can significantly enhance the performance of algorithms \cite{eschmann2021reward, icarte2022reward}, especially in complex multi-vehicle cooperative decision-making problems, where it is essential to incorporate problem-specific features into the reward design.

Reward shaping is a commonly used technique in reinforcement learning research. For instance, \cite{naik2024reward} and \cite{liu2022meta} introduced a bi-level optimization mechanism to improve data efficiency, enabling effective learning of reward functions and policies even with limited human feedback.

Building on the aforementioned research, this study introduces a differential reward mechanism to enhance multi-vehicle cooperative decision-making capabilities. By leveraging reward differentiation, we propose a theoretically grounded approach to improve policy learning and multi-agent coordination. Specifically, in continuous traffic flow environments, this method effectively optimizes state transitions and stabilizes the reward distribution among agents, thereby enhancing decision-making performance.