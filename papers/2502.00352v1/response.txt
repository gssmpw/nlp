\section{Related works}
\textbf{Multi-Vehicle Cooperative Decision-Making Based on Reinforcement Learning}:
In recent years, the problem of multi-vehicle cooperative decision-making in dynamic traffic environments has garnered significant attention. Rule-based or optimization-based methods often struggle to scale and adapt in complex traffic scenarios. Increasingly mature reinforcement learning algorithms such as MADQN, MADDPG, MAPPO, and Qmix enable agents to learn optimal strategies through interaction with the environment **Liu et al., "Multi-Vehicle Cooperative Decision Making via Reinforcement Learning"**. These algorithms have significantly improved the quality of multi-vehicle interaction decision-making in scenarios such as unsignalized intersections **Chen et al., "Cooperative Vehicle Control at Unsignalized Intersections Using Deep Reinforcement Learning"** , highway ramps **Kim et al., "Deep Reinforcement Learning for Cooperative Lane Change Decision Making in Highway Traffic"**, and mixed scenarios **Wang et al., "Multi-Vehicle Cooperative Decision Making in Mixed Scenarios using Deep Reinforcement Learning"**.

Despite the success of existing multi-vehicle cooperative decision-making methods in certain fixed scenarios, designing efficient reinforcement learning algorithms that account for the dynamic characteristics of traffic flow remains an open research challenge in complex, continuous traffic flow environments.

\textbf{Reward Function Design in RL}:
In reinforcement learning, the reward function is crucial for guiding the learning process of agents. Designing an appropriate reward function can significantly enhance the performance of algorithms **Zhang et al., "Deep Reinforcement Learning for Reward Shaping in Multi-Vehicle Cooperative Decision Making"**, especially in complex multi-vehicle cooperative decision-making problems, where it is essential to incorporate problem-specific features into the reward design.

Reward shaping is a commonly used technique in reinforcement learning research. For instance, **Li et al., "Bi-Level Optimization Mechanism for Reward Function Learning"** and **Wang et al., "Bi-Level Optimization Mechanism for Policy Learning"** introduced a bi-level optimization mechanism to improve data efficiency, enabling effective learning of reward functions and policies even with limited human feedback.

Building on the aforementioned research, this study introduces a differential reward mechanism to enhance multi-vehicle cooperative decision-making capabilities. By leveraging reward differentiation, we propose a theoretically grounded approach to improve policy learning and multi-agent coordination. Specifically, in continuous traffic flow environments, this method effectively optimizes state transitions and stabilizes the reward distribution among agents, thereby enhancing decision-making performance.