%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\usepackage{xcolor}
%\definecolor{darkblue}{rgb}{0.0,0.5,0.5}
\usepackage[]{hyperref}
\hypersetup{colorlinks,breaklinks,linkcolor=blue,urlcolor=blue,anchorcolor=blue,citecolor=blue}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{mathtools}
\usepackage{color}
\usepackage{adjustbox}
\usepackage{amssymb}
\usepackage{amsfonts}
% \usepackage{unicode-math}
\usepackage{multirow}
\usepackage{cite}
% \usepackage{amsmath}
\usepackage{bm}
\usepackage[utf8x]{inputenc} 

\def\bfmX{{\mbox{\protect\boldmath$X$}}}
\def\eqdef{\mathbin{:=}}
\def\lebmeas{\mu^{\hbox{\rm \tiny Leb}}}
\def\Prbty{{\cal  M}}
\def\Prob{{\sf P}}
\def\Probsub{{\sf P\! }}
\def\Expect{{\sf E}}
\def\taboo#1{{{}_{#1}P}}
\def\epsy{\varepsilon}
\def\varble{\,\cdot\,}
\def\transpose{{\hbox{\tiny\it T}}}
\def\grad{\nabla}
\def\gradpsi{\nabla \psi}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
A Differentiated Reward Method for Reinforcement Learning based Multi-Vehicle Cooperative Decision-Making Algorithms
}

\author{
Ye Han, Lijun Zhang$^*$, Dejian Meng  % <-this % stops a space
\thanks{Ye Han, Lijun Zhang, Dejian Meng are with the School of Automotive Studies, Tongji University, Shanghai 201804, China.
        {\tt\small \{hanye\_leohancnjs, tjedu\_zhanglijun, mengdejian\}@tongji.edu.cn}}%
\thanks{$^*$Corresponding author: Lijun Zhang}
}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Reinforcement learning (RL) shows great potential for optimizing multi-vehicle cooperative driving strategies through the state-action-reward feedback loop, but it still faces challenges such as low sample efficiency. This paper proposes a differentiated reward method based on steady-state transition systems, which incorporates state transition gradient information into the reward design by analyzing traffic flow characteristics, aiming to optimize action selection and policy learning in multi-vehicle cooperative decision-making. The performance of the proposed method is validated in RL algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle penetration. The results show that the differentiated reward method significantly accelerates training convergence and outperforms centering reward and others in terms of traffic efficiency, safety, and action rationality. Additionally, the method demonstrates strong scalability and environmental adaptability, providing a novel approach for multi-agent cooperative decision-making in complex traffic scenarios.
\end{abstract}

\section{Introduction}

As autonomous driving technology evolves towards networking and collaboration, multi-vehicle cooperative decision-making is expected to become a crucial means of enhancing traffic efficiency and road safety. Research indicates that in typical scenarios such as unsignalized intersections and highway merging zones, traditional single-vehicle decision-making systems, due to their lack of global coordination capabilities, may result in traffic efficiency loss and potential safety hazards \cite{sharma2021assessing}. Multi-vehicle cooperative decision-making systems hold significant research value in constructing the next generation of intelligent transportation systems.

Reinforcement Learning (RL), with its adaptive learning capabilities in dynamic environments, has gradually become one of the mainstream methods for vehicle decision-making\cite{kiran2021deep,folkers2019controlling,wang2024research}. Driven by deep reinforcement learning, vehicle decision systems have achieved good performance improvement in key metrics such as trajectory prediction accuracy and risk avoidance \cite{kiran2021deep}. However, the application of Multi-Agent Reinforcement Learning (MARL) in autonomous driving still faces challenges such as low sample efficiency, the curse of dimensionality, and long-tail problems \cite{10774177}.

Multi-vehicle cooperative decision-making algorithms typically utilize vehicle speed signals, vehicle positions, and interaction events between vehicles (e.g., car-following, negotiating lane changes, collision avoidance, etc.) to design reward mechanisms. These signals help guide the vehicles to make reasonable decisions. Therefore, the design of the reward function is of crucial importance \cite{abouelazm2024review, he2024exploring}. Specifically, from a mesoscopic traffic flow perspective, the state of vehicles is in most time stable and changes gradually over time. This can lead to reinforcement learning algorithms failing to distinguish between actions due to errors. In this paper, we proposed a differentiated reward method based on a steady-state transition system. Experimental results demonstrate that the differentiated reward method significantly accelerates the training convergence speed of reinforcement learning and exhibits more rational behavior in action selection.

The main contributions of this paper can be summarized as follows:
\begin{itemize}
  \item [1)] 
  A differentiated reward method in vehicle decision-making with steady-state transition is formulated from perspective of reinforcement learning theory. By employing differentiated reward, The performance of reinforcement learning algorithms in continuous multi-vehicle cooperative decision-making tasks is enhanced.
  \item [2)]
  Thurough simulation experiments for multi-vehicle cooperative decision-making under different autonomous vehicle penetration rates in a continuous traffic flow environment is conducted, validating the scalability and learning stability of differential rewards in multi-agent traffic scenarios.
\end{itemize}

\section{Related works}

\textbf{Multi-Vehicle Cooperative Decision-Making Based on Reinforcement Learning}:
In recent years, the problem of multi-vehicle cooperative decision-making in dynamic traffic environments has garnered significant attention. Rule-based or optimization-based methods often struggle to scale and adapt in complex traffic scenarios. Increasingly mature reinforcement learning algorithms such as MADQN, MADDPG, MAPPO, and Qmix enable agents to learn optimal strategies through interaction with the environment \cite{wang2024research}. These algorithms have significantly improved the quality of multi-vehicle interaction decision-making in scenarios such as unsignalized intersections \cite{zhuang2023cooperative, HADDAD2022105019, li2020deep, 10417752}, highway ramps \cite{10643142, 10159552, zhao2023multi}, and mixed scenarios \cite{louati2024sustainable, 10123696}.

Despite the success of existing multi-vehicle cooperative decision-making methods in certain fixed scenarios, designing efficient reinforcement learning algorithms that account for the dynamic characteristics of traffic flow remains an open research challenge in complex, continuous traffic flow environments.

\textbf{Reward Function Design in RL}:
In reinforcement learning, the reward function is crucial for guiding the learning process of agents. Designing an appropriate reward function can significantly enhance the performance of algorithms \cite{eschmann2021reward, icarte2022reward}, especially in complex multi-vehicle cooperative decision-making problems, where it is essential to incorporate problem-specific features into the reward design.

Reward shaping is a commonly used technique in reinforcement learning research. For instance, \cite{naik2024reward} and \cite{liu2022meta} introduced a bi-level optimization mechanism to improve data efficiency, enabling effective learning of reward functions and policies even with limited human feedback.

Building on the aforementioned research, this study introduces a differential reward mechanism to enhance multi-vehicle cooperative decision-making capabilities. By leveraging reward differentiation, we propose a theoretically grounded approach to improve policy learning and multi-agent coordination. Specifically, in continuous traffic flow environments, this method effectively optimizes state transitions and stabilizes the reward distribution among agents, thereby enhancing decision-making performance.

\section{Problem Formulation}

\subsection{Markov Decision Process}

We model the interaction between the agent and the environment using a finite Markov Decision Process (MDP) $(\mathcal{S}, \mathcal{A}, \mathcal{R}, p)$, where $\mathcal{S}$, $\mathcal{A}$, and $\mathcal{R}$ represent the state space, action space, and reward space, respectively. The state transition probability is denoted by $p : \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0,1]$. At time step $t$, the agent is in state $S_t \in \mathcal{S}$ and selects an action $A_t \in \mathcal{A}$ using a behavior policy $b:\mathcal{A} \times \mathcal{S} \to [0,1]$. According to the state transition rule $p(s', r \mid s, a) = \Pr(S_{t+1}=s', R_{t+1}=r \mid S_t=s, A_t=a)$, the system transitions to the next state $S_{t+1} \in \mathcal{S}$, and the agent receives a reward $R_{t+1} \in \mathcal{R}$. In the continuous problem we consider, the interaction between the agent and the environment persists indefinitely. The agent's goal is to maximize the average reward obtained over the long term. To achieve this, we aim to estimate the expected discounted sum of rewards for each state, where
$\gamma \in [0,1)$: $v_\pi^\gamma(s) \doteq E [ \sum_{t=0}^\infty \gamma^{t} R_{t+1} \mid S_t=s, A_{t:\infty}\sim\pi ], \forall s$.

\subsection{World Model}

This paper addresses the multi-vehicle cooperative decision-making problem in urban traffic scenarios characterized by continuous traffic flow and mixed autonomy.  

We consider a unidirectional branch of a bidirectional eight-lane road, where vehicles in all four lanes are randomly assigned one of three objectives: going straight, turning left, or turning right. The models for human-driven vehicles (HDVs) in terms of going straight and lane-changing follow the same settings as in our previous work~\cite{han2024value}.  

For connected and autonomous vehicles (CAVs), as illustrated in Fig.~\ref{fig_world_env}, each vehicle has accurate perception of its own position, speed, target lane, and vehicle type, as well as those of vehicles within its observation range. Additionally, CAVs can share their perception information through infrastructure. This means that CAVs within the coordination zone can access and utilize the perception information of all CAVs in the environment.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\columnwidth]{figs/world_env.pdf}
  \caption{Traffic environment for multi-vehicle cooperative decision-making}
  \label{fig_world_env}
\end{figure}

\subsection{Observation/State Space}

For the agent vehicle $i$, its own state vector is defined as:  
$$s_i^{\text{self}} = [p_i^{\text{lon}}, p_i^{\text{lat}}, v_i, \tau_i, g_i, d_i^{\text{left}}, d_i^{\text{front}}, d_i^{\text{right}}]$$  
where $p_i^{\text{lon}} \in \mathbb{R}$ and $p_i^{\text{lat}} \in \mathbb{R}$ represent the longitudinal and lateral positions of the vehicle in the global coordinate system, respectively, $v_i \in \mathbb{R}^+$ denotes the current driving speed, $\tau_i \in \mathbb{Z}^+$ is the discretized vehicle type encoding, and $g_i \in \{0,1\}^k$ represents the driving goal (going straight, turning left, or turning right) using one-hot encoding. $d_i^{\text{left}}, d_i^{\text{front}}, d_i^{\text{right}} \in \mathbb{R}^+$ denote the distances to the nearest vehicles in the left, front, and right lanes, respectively. If no corresponding vehicle exists, the distance is set to a predefined maximum value $d_{\text{max}}$.

For the set of surrounding vehicles $\text{nbr}^i = \{j \mid \|p_i - p_j\|_2 \leq R\}$ (where $R$ is the perception radius), construct the relative state matrix:
\[
M_i^{\text{nbr}} = \underset{j \in \text{nbr}^i}{\text{concat}} \left[ \Delta p_{ij}^{\text{lon}} , \Delta p_{ij}^{\text{lat}} , \Delta v_{ij} , \Delta \tau_{ij} , \Delta g_{ij} \right],
\]
where $\Delta p_{ij}^{\text{lon}} = p_j^{\text{lon}} - p_i^{\text{lon}}$ and $\Delta p_{ij}^{\text{lat}} = p_j^{\text{lat}} - p_i^{\text{lat}}$ represent the relative positional relationships, $\Delta v_{ij} = v_j - v_i$ denotes the speed difference, $\Delta \tau_{ij} = \tau_j - \tau_i$ reflects the difference in vehicle types, and $\Delta g_{ij} = \|g_j - g_i\|_2$ is the Euclidean distance representing the difference in driving goals. When the number of surrounding vehicles $n < N_{max}$, the missing rows are padded with zero vectors.

The observation space is finally represented as:  
\[
o_i = \text{concat} \left[ s_i^{\text{self}}, M_i^{\text{nbr}} \right]
\]

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{figs/reward_comparasion.pdf}
  \caption{A comparative illustration of the general reward function (GR), centering reward function (CR), and differential reward function (DR). In most reinforcement learning studies, the reward function is explicitly expressed based on the new state entered after performing an action. The centralized reward function, on the base of the former, subtracts a baseline value from the reward. In contrast, the differential reward function proposed in this paper derives rewards by comparing the change in states between the current and previous time steps, in conjunction with the actions executed by the agent or system.}
  \label{fig_rew_comp}
\end{figure*}

\subsection{Action space}

The longitudinal control action set is defined as:  
\[
\mathcal{A}_i^{\text{lon}} = \{ a^{\text{acc}}, a^{\text{keep}}, a^{\text{dec}} \}
\]
where \( a^{\text{acc}} \in \mathbb{R}^+ \) represents acceleration, \( a^{\text{keep}} \) denotes maintaining the current speed, and \( a^{\text{dec}} \in \mathbb{R}^- \) represents deceleration. The longitudinal action is converted into actual acceleration execution through a dynamics model, and the vehicle's speed at the next time step is given by:  
\[
\dot{v}_i = \text{clip}(v_i + a^{\text{lon}} \cdot \Delta t, 0, v_{\text{max}})
\]  
where \( \Delta t \) is the decision time step, and the \( \text{clip}(\cdot) \) function ensures that the speed is constrained within the range \([0, v_{\text{max}}]\).

The lateral control action set is defined as:  
\[
\mathcal{A}_i^{\text{lat}} = \{ a^{\text{left}}, a^{\text{hold}}, a^{\text{right}} \}
\]  
where \( a^{\text{left}} \), \( a^{hold} \), and \( a^{right} \) represent changing to the left lane, maintaining the current lane, and changing to the right lane, respectively. The execution of lateral actions satisfies the lane boundary constraints:  
\[
L_i^{t+1} = \begin{cases} 
\max(L_i^t - 1, 1) & \text{if } a^{\text{lat}}=a^{\text{left}} \\
L_i^t & \text{if } a^{\text{lat}}=a^{\text{hold}} \\
\min(L_i^t + 1, N_{lane}) & \text{if } a^{\text{lat}}=a^{\text{right}}
\end{cases}
\]  
where \( L_i^t \in \mathbb{Z}^+ \) denotes the lane number of the vehicle at time \( t \), and \( N_{\text{lane}} \) is the total number of lanes on the road. If the target lane does not exist, the action automatically defaults to \( a^{\text{hold}} \).

The complete action of agent \( i \) at the decision-making moment is the Cartesian product of the longitudinal and lateral actions:  
\[
\mathcal{A}_i = \mathcal{A}_i^{\text{lon}} \times \mathcal{A}_i^{\text{lat}}
\]

The design of the reward function is detailed in Section \ref{rew_design}.

\section{Methodology}

\subsection{Introduction of Reward Centering}

First, we describe the general idea of reward centering\cite{naik2024reward}. Reward centering involves subtracting the empirical mean of the rewards from the observed rewards, thereby achieving a mean-centered effect for the modified rewards. 

For general reinforcement learning algorithms, we can perform a Laurent decomposition on the value function:
The discounted value function can be decomposed into two parts, one of which is a constant that does not depend on the state or action, and thus does not affect the selection of actions. For a policy $\pi$ corresponding to the discount factor $\gamma$, the tabular discounted value function $h_\pi^\gamma: \mathcal{S} \to \mathbb{R}$ can be expressed as
\begin{align}
  \label{eq_laurent_series_expansion}
  h_\pi^\gamma(s) = \frac{r(\pi)}{1-\gamma} + \tilde{h}_\pi(s) + e_\pi^\gamma(s),
\end{align}
where $r(\pi)$ is the state-independent average reward obtained by policy $\pi$, and $\tilde{h}_\pi(s)$ is the differential value of state $s$. For ergodic Markov decision processes, these two terms can be defined as Equation \ref{eq_def_avg_reward_and_value_fn}.
\begin{equation}
  \begin{aligned}
    \vspace{-0.5mm}
    r(\pi) &\doteq \lim_{n\to\infty} \frac{1}{n} \sum_{t=1}^n \mathbb{E} \big[ R_{t} \mid S_0, A_{0:t-1}\sim\pi \big], \\
    \tilde{h}_\pi(s) &\doteq \mathbb{E} \left[ \sum_{k=1}^\infty \big( R_{t+k} - r(\pi) \big) \mid S_t=s, A_{t:\infty}\sim\pi \right], 
    \label{eq_def_avg_reward_and_value_fn}
    \vspace{-0.5mm}
  \end{aligned}
\end{equation}
here, $e_\pi^\gamma(s)$ represents an error term that approaches zero as the discount factor approaches 1. To distinguish the speed $v$, we do not use the common value function notation in reinforcement learning research but instead use $h$ to denote the value function.

In many reinforcement learning problems, the state-independent offset can be quite large. Consider subtracting the constant offset from each state's discounted value, i.e., $h_\pi^\gamma(s) - r(\pi)/(1-\gamma)$, which is referred to as the centered discounted value. The centered discounted value is much smaller in magnitude and changes very little as the discount factor increases. For most reinforcement learning problems (especially long-term ones), when the discount factor approaches 1, the magnitude of the discounted value increases dramatically, while the centered discounted value changes less and approaches the state differential value. The relevant representation of the discounted value function is as follows:
\begin{equation}
  \begin{aligned}
    \label{eq_def_centered_state_value_function}
    \tilde h_\pi^\gamma(s) &\doteq 
    \mathbb{E} \left[ \sum_{t=0}^\infty \gamma^{t} \big( R_{t+1} - r(\pi) \big) \mid S_t=s, A_{t:\infty}\sim\pi \right], \\
    h_\pi^\gamma(s) &= \frac{r(\pi)}{1-\gamma} + {\overbrace{{\color{black}\tilde{h}_\pi(s) + e_\pi^\gamma(s)}}^{ \tilde h_\pi^\gamma(s)}},
  \end{aligned}
\end{equation}
where $\gamma \in [0,1]$. When $\gamma = 1$, the centered discounted value and the differential value are identical, i.e., $\tilde h_\pi^\gamma(s) = \tilde h_\pi(s), \forall s$. More generally, the centered discounted value is the differential value plus the expansion error of the Laurent series, as shown in the second equation of Equation \eqref{eq_def_centered_state_value_function}.

Therefore, reward centering allows reinforcement learning algorithms to capture all information in the discounted value function through two components: (1). the constant average reward and (2). the centered discounted value function. The role of this decomposition is highly intuitive:
\begin{itemize}
    \item[(a)] When $\gamma \to 1$, the discounted value tends to explode, but the centered discounted value remains small and manageable.
    \item[(b)] If the rewards of the reinforcement learning problem are shifted by a constant $c$, the magnitude of the discounted value will increase by $c/(1-\gamma)$, but the centered discounted value remains unchanged because the average reward increases by $c$.
\end{itemize}

\subsection{Reward Differentiation and Its Connection to Reward Centering}
Next, we formulate differentiated reward and analyze its intrinsic connection to reward centering.

Define a Markov chain $\mathcal{S} = \{S(t): t=0,1,2,\ldots\}$, whose state space is $\Re^\ell$, and $\mathcal{S}$ evolves according to a nonlinear state-space model:
\begin{equation}
  S(t+1) = a(S(t),N(t+1)), \quad t \geq 0,
  \label{eq_SP_disc}
\end{equation}
Under these assumptions, for all $t \geq 0$, $S(t+1)$ is a continuous function of the initial condition $S(0)=s_0$.

Under the discount factor $\gamma \in (0,1)$, the discounted value function can be written as:
\begin{equation}
  h_\pi^\gamma(s_0) \eqdef  \sum_{t=0}^{\infty} 
  \gamma^t \mathbb{E} [R(t) \mid R(0) = r_0]\,, \qquad r_0 \in \Re^\ell.
  \label{eq_DCOE_disc}
\end{equation}
The goal of TD learning is to approximate $h^\gamma$ as an element of a function family $\{h_{\pi(\theta)}^\gamma: \theta \in \Re^d\}$. Here, we assume that the discounted value function can be linearized in the following form:
\begin{equation}
  h_{\pi(\theta)}^\gamma = \sum_{j=1}^d \theta_j   \psi_j,
  \label{eq_hLinearPar}
\end{equation}
where \(\theta = (\theta_1, \theta_2, \ldots, \theta_d)^\transpose\), \(\psi = (\psi_1, \psi_2, \ldots, \psi_d)^\transpose\), and the given set of basis functions \(\psi \colon \Re^\ell \to \Re^d\) is assumed to be continuously differentiable. Indeed, most reward function expressions derived from the analytical relationships of vehicle dynamics satisfy this condition.

The goal of TD learning can be expressed as a minimum norm problem:
\begin{equation}
  \begin{aligned}
    \theta^* &= \argmin_\theta \| h_{\pi(\theta)}^\gamma - h^\gamma\|^2_\pi .
  \end{aligned}
  \label{eq_TDgoal}
\end{equation}

Assume that the value function $h^\gamma$ and all its possible approximations $\{h_{\pi(\theta)}^\gamma: \theta\in\Re^d\}$ are continuously differentiable as functions of the state $s$, i.e., for each $\theta \in \Re^d$, $h^\gamma,\, h_{\pi(\theta)}^\gamma \in C^1$. Based on the linear parameterization in \eqref{eq_hLinearPar}, we obtain the following form of the differential value function:
\begin{equation}
  \nabla h_{\pi(\theta)}^\gamma = \sum_{j=1}^d \theta_j \nabla \psi_j\,,
  \label{eq_TDgradgoal_0}
\end{equation}
where the gradient is taken with respect to the state $s$.

At this point, the goal of TD learning changes accordingly to:
\begin{equation}
  \begin{aligned}
    \theta^* &= \argmin_\theta \| \nabla h_{\pi(\theta)}^\gamma - \nabla h^\gamma\|^2_\pi 
  \end{aligned}
  \label{eq_TDgradgoal}
\end{equation}

Paper \cite{devraj2020differential} provides a detailed convergence proof of the problem represented by Equation \eqref{eq_TDgradgoal}. Clearly, in general reinforcement learning algorithms, directly using the differential reward function is essentially equivalent to solving the problem posed by Equation \eqref{eq_TDgradgoal}.

In steady-state traffic flow, if the following conditions hold:
\begin{itemize}
    \item Condition 1: The $\nabla h^\gamma$ is linearly correlated with the state transition direction $\mathbb{E}[S_{t+1} | S_t = s]$;
    \item Condition 2: The reward differential weight $\lambda$ and the centered mean $r(\pi)$ satisfy $\lambda \cdot \nabla h^\gamma \propto \tilde{R}_t$,
\end{itemize}
then reward differentiation and reward centering are equivalent in the value function update, i.e.,
\begin{equation*}
  \mathbb{E}_\pi \left[ \tilde{R}_t + \lambda \cdot r_p^i \right] \approx \mathbb{E}_\pi \left[ \tilde{R}_t \right].
\end{equation*}

At this point, both methods adjust the distribution of reward signals to make the value function focus more on the relative differences between states.

The effectiveness of reward centering has been demonstrated in paper \cite{naik2024reward}. Therefore, in reinforcement learning problems with steady-state transitions, the reward differentiation method is expected to improve performance.

%\begin{itemize}
%    \item \textbf{Directional Guidance Capability:}
%    \begin{itemize}
%        \item Reward Centering: Only reduces global variance by shifting reward signals but cannot directly guide the agent to focus on specific state transition directions.
%        \item Reward Differentiation: Explicitly associates state transition directions through the gradient term $\nabla h^\gamma$. For example, in a lane-changing scenario, when the gradient points toward the target lane, reward differentiation assigns higher rewards to movements in that direction, thereby accelerating policy optimization.
%    \end{itemize}
%
%    \item \textbf{Adaptability to Non-Stationary Reward Distributions:}
%    \begin{itemize}
%        \item Reward Centering: Assumes that the reward mean $\bar{R}_t$ converges to a fixed value $r(\pi)$, which may fail in dynamic traffic flows (e.g., sudden congestion).
%        \item Reward Differentiation: The gradient information $\nabla h^\gamma$ reflects local state changes in real-time. For example, when a vehicle approaches an intersection, the potential field gradient automatically adjusts its direction, dynamically adapting to non-stationary environments.
%    \end{itemize}
%
%    \item \textbf{Coordination of Local Optimization and Global Stability:}
%    \begin{itemize}
%        \item Reward Centering: Globally adjusts the reward baseline, improving the stability of value function estimation but potentially masking local dynamic features.
%        \item Reward Differentiation: Captures local state transition trends (e.g., lateral motion in lane-changing decisions) through the gradient term while retaining the global stability of centering. The combination of both achieves a synergistic effect of "global stability + local optimization."
%    \end{itemize}
%
%    \item \textbf{Mathematical Extensibility:}
%    \begin{itemize}
%        \item Reward Centering: A linear operation that can only handle first-order statistics (mean).
%        \item Reward Differentiation: A nonlinear operation that can incorporate higher-order information (e.g., gradient covariance), for example:
%        \begin{equation*}
%            r_p^i = \mathbf{v}^i \cdot \nabla h^\gamma + \alpha \cdot \text{Tr}(\nabla^2 h^\gamma).
%        \end{equation*}
%    \end{itemize}
%\end{itemize}

\subsection{Differentiated Reward Implementation in Vehicle Decision-Making}
\label{rew_design}

In the field of vehicle decision-making (multi-vehicle collaborative decision-making), the reward function commonly used can be expressed as Equation \eqref{eq_tradi_rewardfun}. \cite{grl-itsc, mvgrl-zju, rewardfun}
\begin{equation}
  \begin{aligned}
    R&=w_{1} R_{\text {speed }}+w_{2} R_{\text {intention }}+w_{3} P_{\text {collision }}+w_{4} P_{L C} \\
    &=\frac{1}{N}\left(w_{1} {\sum_{i=1}^{N} \frac{v_{i}}{v_{\max }}}+w_{2} N_{\text {sat}}+w_{3} N_{\text {col }}+w_{4} N_{L C}\right)
  \end{aligned}
  \label{eq_tradi_rewardfun}
\end{equation}
where $N$ is the number of vehicles in the scene (including HDVs and CAVs), $N_{\text {onramp }}$ is the number of vehicles passing through the intention area at the previous time step and aiming for the ramp, $N_{\text {collision }}$ is the number of collisions, and $N_{L C}$ is the number of frequently lane-changing vehicles.

Based on the ideas proposed in this section, we have improved the reward function in Equation \eqref{eq_tradi_rewardfun} as follows.

Due to the variable number of agents in the setup, this paper cannot directly use the sum of the local rewards of all agents as the reward function. Instead, an index needs to be designed to objectively evaluate the overall traffic quality within the coordinating zone. To this end, we design the following external reward function:  

\begin{equation}
  r_{\text{env}} = \frac{1}{\left | \mathcal{N} _{\text{CAV}}\right | } \textstyle \sum_{i} \left ( \omega_1 r_{a}^i + \omega_2 r_{p}^i \right ) + \omega_3 r_{\text{flow}} + \omega_4 r_{\text{safe}}    
\end{equation}
where \( r_{a}^i \) and \( r_{p}^i \) represent the action reward and position reward for CAV \( i \), respectively, \( r_{\text{flow}} \) is used to evaluate the overall traffic flow speed, and \( r_{\text{safe}} \) is the traffic safety indicator. The parameters \( \omega_{1,2,3,4} \) are the weights assigned to each reward component. Specifically:  
\begin{equation*}
  r_a =
  \begin{cases}
    1 & \text{ if accelerating or keeping highspeed,}\\
    0 & \text{ otherwise.}
  \end{cases}
\end{equation*}

We design a potential field based on the vehicle's longitudinal position and the lane it occupies, to evaluate the value of the current position of vehicle \( i \) relative to its target.
\begin{equation}
  f_p^i\left ( x,y \right )  = \frac{e^{-\frac{(l - x)^2}{2\sigma ^2}}}{\zeta \left |  y_{tar}^i-y \right | + 1}
\end{equation}
Where \( \sigma \) and \( \zeta \) are the longitudinal and lateral decay coefficients, respectively. Based on the concept of reward differentiation, we define the position reward as:  
\begin{equation}
  r_p^i = \mathbf{v}^i \cdot \grad f_p^i\left ( x,y \right )
  \label{eq_pos_rew_def}
\end{equation}
where $\mathbf{v}^i = [v_{x}^i, v_{y}^i]$. In this paper, $v_{y}^i \in \left \{ -1,0,1 \right \} $, so we have the discrete form of Equation \eqref{eq_pos_rew_def},
\begin{equation}
  r_p^i = \left [ v_{x}^i\left ( l-x \right )  +\frac{\zeta v_{y}^i \text{sign}\left ( y -  y_{tar}^i\right ) }{\zeta \left |  y_{tar}^i-y \right | + 1} \right ] \cdot f_p^i\left ( x,y \right ) 
  \label{eq_diff_rew}
\end{equation}
here, we define that when \( y^i = y_{\text{tar}}^i \), \( \text{sign} \left( y - y_{\text{tar}}^i \right) = -v_{y}^i \). It can be observed that the terms \( \omega_1 r_{a}^i \) and \( \omega_2 r_{p}^i \) sometimes have the same reward effect. In this case, we treat them as strengthen of the action reward without making a more detailed distinction.

here is fig of reward function heatmap.

%\begin{figure}[h]
% \centering
% \includegraphics[width=1.0\columnwidth]{figs/reward_vis.pdf} 
% \caption{position reward visualization}
% \vspace{-.5em}
% \label{fig_reward_vis}
%\end{figure}

We use the overall speed of the traffic flow to evaluate the current traffic volume, which is:
\begin{equation*}
  r_{\text{flow}} =  \frac{1}{\left | \mathcal{N}\right | } {\textstyle \sum_{i}}\frac{v^i}{v_{max}} 
\end{equation*}

For the safety indicator, since accidents are rare events and their occurrence typically has a significant impact on overall traffic, we use summation rather than averaging.
\begin{equation*}
  r_{\text{safe}} =  {\textstyle \sum_{i}} \mathbb{I}(i) 
\end{equation*}
Where \( \mathbb{I}(i) \) is the indicator function, which equals 1 if vehicle \( i \) is involved in a collision, and 0 otherwise.


\section{Experiment}

\subsection{Simulation environment and experiment settings}

The experiments are based on the open-source microscopic traffic simulation platform SUMO (Simulation of Urban Mobility), which supports high-precision vehicle dynamics modeling, multi-agent collaborative control, and visual analysis of complex traffic scenarios. \cite{lopez2018microscopic} The simulation scenario is a unidirectional branch of a bidirectional 8-lane highway (4 lanes in the same direction), with a straight road segment length of 250 meters, and a speed limit of 25 m/s (90 km/h). Traffic flow generation is implemented using SUMO's built-in flow module, which injects background vehicles following a Poisson process. The baseline traffic density is set to 250 vehicles per lane per hour, consistent with the traffic characteristics of urban arterial roads during off-peak hours. All vehicles are randomly assigned one of 3 objectives: going straight, turning left, or turning right, the target lane is the middle 2 lanes, the left most lane, and the right most lane, respectively.

\begin{table}[htbp]
\centering
\caption{Key Simulation Parameters}
\label{tab:simulation_parameters}
\begin{tabular}{lc}
\toprule
\textbf{Parameter Category} & \textbf{Parameter Value} \\ \midrule
Road Length & 250 m \\
Number of Lanes & 4 (unidirectional) \\
Traffic Density & 250 /(h $\cdot$ lane) \\
Vehicle Objective & 1/3 each for 3 directions\\
Episode Duration & 18 s \\
Decision Interval & 0.1 s \\
Autonomous Vehicle Penetration Rate & 25\%, 50\%, 75\%, 100\% \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure*}[htbp]
	\centering
	\includegraphics[width=1.0\linewidth]{figs/tk_plot.pdf}
	\caption{Training comparison. To better illustrate the early-stage convergence and later-stage stability of the algorithms, a logarithmic scale was used for the horizontal axis. The decision-making performance of vehicles was evaluated under 3 reinforcement learning algorithms (MADQN, MAPPO, and QMIX) and 4 CAV penetration rates (0.25, 0.50, 0.75, and 1.0) using 3 types of reward functions: differential reward function (green curve), centralized reward function (orange curve), and general reward function (blue curve). In each subplot, the green curve corresponds to the left vertical axis, while the other two curves correspond to the right vertical axis.}
	\label{fig_return_fig}
\end{figure*}

\subsection{Compared Methods}

We compare the training and deployment performance of widely used multi-agent reinforcement learning algorithms, including MADQN \cite{grl-itsc}, MAPPO \cite{chen2020decision}, and QMIX \cite{rashid2020monotonic}, by employing the generally adopted vehicle decision reward function (Equation \eqref{eq_tradi_rewardfun}), the centering reward function (oracle centering in \cite{naik2024reward}), and the differentiated reward function (Equation \eqref{eq_diff_rew}).

All algorithms share the following hyperparameter settings in TABLE \ref{tab_hyperparameters}.
\begin{table}[htbp]
\centering
\caption{Algorithm Hyperparameter Settings}
\label{tab_hyperparameters}
\begin{tabular}{lc}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\ \midrule
Learning Rate & $3 \times 10^{-4}$ \\
Discount Factor ($\gamma$) & 0.98 \\
Replay Buffer Size & $1 \times 10^{5}$ \\
Target Network Update Interval & 100 steps \\
Exploration Rate Decay & 0.99 (linear) \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Metric}

\textbf{Avg. Speed}, reflecting overall traffic efficiency:

$$\bar{v} = \frac{\sum_{t=0}^{\text{END}} \bar{v}_t }{\sum_{t=0}^{\text{END}} 1} ,$$
where $\bar{v}_t$ is the average speed of all vehicles on road at time $t$.

\textbf{Min. Gap}, quantifying potential collision risk:

$$C_{\text{GAP}} = \frac{\sum_{e=0}^{N_{\text{epi}}} g_{\text{min, }e}}{N_{\text{epi}}},$$  
where, $g_{\text{min, }e}$ is the minimum gap of vehicle in test episode $e$, $N_{\text{epi}}$ is the total number of test episodes.

\textbf{Lane Change Frequency}, evaluating the rationality of lane resource utilization:
$$F_{\text{LC}} = \frac{\sum_{t=0}^{\text{END}} n_{\text{LC},t}}{\sum_{t=0}^{\text{END}} n_{\text{CAV},t}} ,$$
where \( n_{\text{LC},t} \) is the number of lane changes action of simulation step $t$, and \( n_{\text{CAV},t} \) is the CAV number of time $t$.

\textbf{Succ. Rate}:
$$\textbf{SR.} = \frac{\text{number of vehicles reach their target}}{\text{number of CAVs}}.$$

\begin{table*}[htbp]
\centering
\caption{Metrics comparison of reward functions in Qmix}
\label{tab_performance_metrics}
\begin{tabular}{ccrrrrr}
\hline
\textbf{Pene. Rate}  & \textbf{Rew. Fn.} & \textbf{Avg. Speed (m/s)} & \textbf{Min. Gap(m)} & \textbf{LC. Rate (time/min)} & \textbf{Succ. Rate} \\ \hline
\multirow{3}{*}{25\%} & GR & 7.95±0.4  & 1.33±0.8 & 0.67±0.03 & 49.20±0.2 \\
                      & CR & 10.95±0.3 & 5.36±0.6 & 0.14±0.02 & 61.45±0.1 \\
                      & DR & \textbf{11.46}±0.2 & \textbf{36.47}±0.4& \textbf{0.07}±0.02 & \textbf{96.49}±0.1 \\ \hline
\multirow{3}{*}{50\%} & GR & 1.78±0.5  & 4.91±1.2 & \textbf{0.01}±0.05 & 36.21±0.3 \\
                      & CR & \textbf{14.18}±0.4 & 5.55±0.9 & 0.10±0.04 & 68.83±0.2 \\
                      & DR & 12.06±0.3 & \textbf{49.04}±0.7& 0.04±0.03 & \textbf{97.01}±0.1 \\ \hline
\multirow{3}{*}{70\%} & GR & 0.16±0.5  & 21.88±1.2& 0.04±0.05 & 40.09±0.3 \\
                      & CR & 0.55±0.4  & 6.01±0.9 & 0.07±0.04 & 70.25±0.2 \\
                      & DR & \textbf{12.01}±0.3 & \textbf{53.56}±0.7& \textbf{0.01}±0.03 & \textbf{94.64}±0.1 \\ \hline
\multirow{3}{*}{100\%}& GR & 0.84±0.5  & 6.07±1.2 & \textbf{0.01}±0.05 & 30.29±0.3 \\
                      & CR & 12.58±0.4 & 5.24±0.9 & 0.15±0.04 & 62.88±0.2 \\
                      & DR & \textbf{16.79}±0.3 & \textbf{32.30}±0.7& 0.10±0.03 & \textbf{88.82}±0.1 \\ \hline
\end{tabular}
\end{table*}

\subsection{Results and Comparison}

Figure \ref{fig_return_fig} illustrates the training process of the algorithms. To better capture the early-stage convergence and later-stage stability, a logarithmic scale was applied to the horizontal axis.  

It can be observed that in MADQN, the training curves for all three reward functions exhibit significant fluctuations, indicating poor stability. Despite tuning the algorithm to its optimal state, the performance of the three reward functions varies across different penetration rates. This instability is likely due to MADQN being based on independent Q-learning, which struggles to adapt to multi-vehicle cooperative decision-making tasks. Similarly, in MAPPO, although the algorithm achieves stable convergence, the performance of the centralized reward function exhibits considerable deviation in the stabilized phase under the same settings. Specifically, for penetration rates of 0.25, 0.5, and 1.0, the orange curve shows substantial deviation after 20,000 episodes. As a result, in Table \ref{tab_performance_metrics}, we only provide quantitative performance metric analyses for QMIX.  

In the experiments with MAPPO and QMIX, it is evident that the convergence speed of the differential reward function is significantly faster than that of the centralized and general reward functions. Additionally, the return values of the centralized reward function are noticeably higher than those of the general reward function, particularly in the later training stages (\textgreater 20,000 episodes). This finding is consistent with the conclusions reported in \cite{naik2024reward}.

Table \ref{tab_performance_metrics} shows the performance metrics of the 3 reward functions under the QMIX algorithm at different penetration rates. It can be observed that, in most cases, the differentiated reward function achieves the best performance in terms of traffic efficiency, safety, action rationality, and task completion rate.

\section{Conclusion and future work}

This paper proposes a differentiated reward method for RL based multi-vehicle cooperative decision-making algorithms. By incorporating state transition gradient information into the reward function design, the method resolves the issue of distinguishing action values in steady-state traffic flow, a challenge commonly encountered with traditional reward mechanisms. Experimental results shows that the differentiated reward method significantly improves the training efficiency and decision quality of multi-agent algorithms, outperforming centering and general reward functions in core metrics such as traffic efficiency, safety, and action rationality. Moreover, the method maintains stable performance across different autonomous driving penetration rates (25\%-100\%), which shows good scalability and learning stability of our method. Future research directions include further optimization of reward function design, exploration of more complex traffic scenarios, and large-scale validation and application in real-world traffic systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\bibliographystyle{IEEEtran.bst}
\bibliography{ref.bib}




\end{document}
