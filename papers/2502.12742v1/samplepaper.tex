% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{longtable}
% \usepackage{tablefootnote}
\usepackage[flushleft]{threeparttable}

\renewcommand\UrlFont{\color{blue}\rmfamily}
\urlstyle{rm}
\hypersetup{
    colorlinks,
    linkcolor={blue},
    citecolor={blue},
    urlcolor={blue}
}

\usepackage{cleveref}
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from Cortical Surfaces}
%
\titlerunning{Cor2Vox}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Anonymous Author}

\author{
Fabian Bongratz\inst{1,2}*
Yitong Li\inst{1,2}*,
Sama Elbaroudy\inst{1},
Christian Wachinger\inst{1,2}}

\institute{Lab for Artificial Intelligence in Medical Imaging, \\ Technical University of Munich (TUM), Germany \and
Munich Center for Machine Learning (MCML), Germany} 
% \\
% \email{fabi.bongratz@tum.de}, \email{yi\_tong.li@tum.de}}

% \inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Bongratz, Y. Li, S. Elbaroudy, C. Wachinger}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%

\def\thefootnote{\normalsize*}\footnotetext{The authors contributed equally to this paper. \\ Email: \texttt{\{fabi.bongratz, yi\_tong.li\}@tum.de}}

\begin{abstract}
% The abstract should briefly summarize the contents of the paper in
% 150--250 words.
Despite recent advances in medical image generation, existing methods struggle to produce anatomically plausible 3D structures. In synthetic brain magnetic resonance images (MRIs), characteristic fissures are often missing, and reconstructed cortical surfaces appear scattered rather than densely convoluted. 
% accurately capture the cortex’s intricate geometry.
% Narrow gyri and sulci often fall below the image resolution and require dedicated approaches to be modeled accurately. 
%There has been considerable progress in controllable generative models for anatomically constrained medial image generation. However, previous approaches cannot cope with the intricate geometry of the human cortex; the narrow gyri and sulci often fall below the image resolution and require dedicated approaches to be modeled accurately. 
To address this issue, we introduce Cor2Vox, the first diffusion model-based method that translates continuous cortical shape priors to synthetic brain MRIs. 
% We represent the cortex using carefully adapted signed distance functions (SDF) and employ a
To achieve this, we leverage a Brownian bridge process which allows for direct structured mapping between shape contours and medical images. Specifically, we adapt the concept of the Brownian bridge diffusion model to 3D and extend it to embrace various complementary shape representations.
% to directly map from the shape to the image domain -- ensuring anatomically-plausible brains.
%We create 3D cortical representations based on modifications of the signed distance function. 
%In this work, we present the first diffusion-based method for brain magnetic resonance imaging (MRI) synthesis that incorporates continuous cortical shape priors during generation. 
%These serve as input to a 3D Brownian Bridge process, enabling a direct transformation from the shape to the image domain. 
% Leveraging this cortical control, Cor2Vox enables detailed assessment through surface distance metrics, which is not possible with alternative methods.
%Thanks to the cortical control, we can go beyond current cortex evaluation metrics and compute the surface distance for detailed assessment. 
Our experiments demonstrate significant improvements in the geometric accuracy of reconstructed structures compared to previous voxel-based approaches. Moreover, Cor2Vox excels in image quality and diversity, yielding high variation in non-target structures like the skull. Finally, we highlight the capability of our approach to simulate cortical atrophy at the sub-voxel level.
% accuracy of the generated target anatomies over previous methods, preserving the model's capability to introduce variation in the rest of the image. We uncover the limitations of mask-based geometric conditioning and propose to incorporate continuous surface-based shape representations. 
Our code is available at~\url{https://github.com/ai-med/Cor2Vox}.

%\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
%
%



\section{Introduction}
Generating synthetic medical images has demonstrated numerous benefits, including improved model robustness~\cite{Gopinath2024synthetic}, enhanced fairness~\cite{burlina2021bias,Ktena2024generativefairness}, reduced manual annotation burdens~\cite{Menten2022simulation,Xanthis2021simulator}, and support in individualized simulations and treatment planning~\cite{zhu2008simulation}. 
Recent progress in diffusion models has enabled the generation of realistic-looking brain MRIs~\cite{Pinaya2022latentmri,Peng2023generatingbrainmri,Kim2024controllable} as in \Cref{fig:motivation}(a). 
%However, when moving beyond 2D views in examining the result and considering 3D cortical surface reconstructions instead, it becomes immediately apparent that in the majority of cases implausible folding patterns are produced, see Fig. 1(b). 
However, examining beyond 2D views to analyze 3D cortical surface reconstructions reveals significant limitations, as most cases exhibit implausible folding patterns as shown in \Cref{fig:motivation}(b). We investigated 20 randomly generated MRIs and reconstructed their corresponding cortical surfaces of the left hemisphere. Severe irregularities were observed in 18 out of 20 reconstructions, including missing central gyri/sulci, unrealistic grooves, and scattered surface structures.
% using a 3D denoising diffusion probabilistic model (DDPM)~\cite{ddpm}.
While initially surprising, this issue aligns with the well-established challenge diffusion models face in generating anatomically accurate hands \cite{narasimhaswamy2024handiffuser},
%hands with the correct number of fingers \cite{narasimhaswamy2024handiffuser} 
a problem that is further amplified given the geometric complexity of the cerebral cortex. %brain due to the cortex's geometric complexity.
%While surprising at first, it relates to the well established  problems of diffusion models in generating hands with correct number of fingers, magnified for the brain with the high geometric complexity of the cortex. 
%Diffusion models on brain MRI achieved impressive results \cite{Peng2023generatingbrainmri} as shown in 2D coronal views in Fig. 1(a). 
%However, when examining 3D cortical surface reconstructions from such generated images, it becomes apparent that in the majority of cases implausible folding patterns are produced, see Fig. 1(b). 
%This issue may not have received wider attention so far, because traditional image generation metrics like SSIM and PSNR are often used to evaluate the quality of synthetic brain MRIs and they fail to capture such structural inconsistencies, focusing instead on visual similarity.
This issue has likely received limited attention because traditional image quality metrics, such as Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise-Ratio
(PSNR), primarily evaluate visual similarity and fail to capture structural inconsistencies and critical neuroanatomical inaccuracies.

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.95\linewidth]{unconditional_flaws_v5.png}
    \includegraphics[width=0.95\linewidth]{unconditional_flaws_new_crop.pdf}
    \caption{Conditioning on realistic cortical surfaces is crucial to avoid anatomical implausibilities in synthetic MRIs. (a) Generated MRIs from an unconditional 3D diffusion model. (b) Anatomical implausibilities; from top to bottom: missing central sulcus, unrealistic wide groove, scattered surface structure. (c) Real MRIs and corresponding cortical surfaces; we highlighted the characteristic central sulcus in red. (d) Our approach (Cor2Vox) incorporates real cortical surfaces to guide the generative process. 
    % We used V2C-Flow~\cite{Bongratz2024v2cflow} for cortical surface reconstruction from the MRIs.
    }
    \label{fig:motivation}
\end{figure}

The problem of anatomical implausibility highlights a core challenge in image generation: achieving effective \emph{control} over the generative process.
While recent methods \cite{Zhang2023controlnet,mou2024t2i,zhao2024uni} have introduced localized conditions like 2D pose skeletons or edge maps to guide image synthesis, they are insufficient for the generation of brain MRIs, as controlling the intricate 3D geometry of the highly folded cortex demands more sophisticated representations.
%ControlNet moved beyond global text conditions and introduced the fine-grained control over image generation using 2D pose skeletons or 2D edge maps. 
%Yet, generating realistic brain MRIs demands a representation capable of capturing the intricate 3D geometry of the tightly folded cerebral cortex. 
Existing continuous shape representations, such as triangular meshes~\cite{Dale1999,MacDonald2000,Patenaude2011shapemodel} and signed distance fields (SDF)~\cite{Cruz2021deepcsr,gopinath2021segrecon,Han2004cruise}, excel at modeling the dense convolutions of the cortex and are instrumental in simulating sub-millimeter-level brain atrophy~\cite{Camara2006atrophy,Rusak2022benchmark}, aiding the benchmarking of segmentation and registration algorithms~\cite{CastellanoSmith2003simulation,Karacali2006simulation,Larson2022synthetic,Sharma2010atrophy}. 
%Using such detailed representations of cortical geometry would have great potential in controlling brain MRI generation.
Incorporating such representations into generative models therefore holds promise for synthesizing anatomically realistic 3D brain scans.

However, integrating geometric representations into standard denoising diffusion probabilistic models (DDPM)~\cite{ddpm,sohl-dickstein-diffusion2015} is suboptimal, as they assume a pure Gaussian noise as the prior, requiring anatomical constraints to be integrated externally. A recent method, Brownian Bridge diffusion models (BBDM)~\cite{li_bbdm_2023}, offers a more flexible approach by modeling the transport between two arbitrary distributions, allowing for a direct mapping between geometric conditions and brain MRIs.
% which avoids cumbersome conditional techniques and enables more efficient and grounded generation. 
Yet, current models~\cite{li_bbdm_2023,Lee2024ebdm} were developed for 2D images, and commonly operate in the latent space after a pre-trained autoencoder, making them unsuited for precisely generating intricate 3D structure of the cerebral cortex.

% We build upon the concept of Brownian bridge diffusion and transfer it to the realm of 3D medical images, incorporating precise anatomical shape priors during the generation. 


% Extending this framework to 3D scans exacerbates the problem, requiring excessively large, redundant network architectures.
% A recent method in computer vision -- Brownian Bridge Diffusion Models (BBDMs)~\cite{li_bbdm_2023} -- offers a more direct and flexible alternative by modeling the transport between two arbitrary distributions, rather than starting from pure noise. BBDMs inherently simplify the generative process, which is particularly advantageous for image translation, where the target distribution is a clean image. 
% It also eliminates the need for cumbersome techniques like guidance or projected sampling, enabling more efficient and grounded generation.

%However, traditional Denoising Diffusion Probabilistic Models (DDPMs) rely on the assumption that the prior distribution of the diffusion process is pure Gaussian noise, requiring cortical constraints to be incorporated as external conditions. 
%When applied to 3D medical images, this approach necessitates an overly large network architecture with significant redundancies.
%% Considering that we are dealing with 3D images, that yields an overly large network architecture with redundancies, since are not interested in the noise. 
%Recently, Brownian bridge diffusion models (BBDM) have been introduced in computer vision as an alternative by directly modeling the transport between two arbitrary probability distributions. This is particularly beneficial for tasks like image translation where the target distribution is already a clean image. 
%By eliminating the need for cumbersome techniques such as guidance or projected sampling, BBDMs enable more efficient and theoretically grounded generative processes.


% \noindent
% \textbf{Contribution.}
Thus, we introduce \emph{Cor2Vox}, the first diffusion model-based method leveraging a 3D shape-to-image Brownian bridge diffusion model for medical image synthesis incorporating continuous shape priors. 
% Technically, we contribute a 3D shape-to-image Brownian bridge diffusion model that operates directly in voxel space with 3D signed distance fields. 
We combine inner white matter and outer pial cortical surfaces deliberately to represent detailed brain structures with 3D SDFs. 
By leveraging both surfaces, Cor2Vox is tailored to model changes in cortical thickness, offering novel possibilities to accurately simulate cortical thinning. 
With the cortical control, we can compute surface distances to precisely quantify the accuracy of the generated tissue boundaries, surpassing the indirect evaluation with Cohen's d~\cite{Wu2024evaluating}. 
%To evaluate the accuracy of the generated tissue boundaries at the subvoxel level, we leverage V2C-Flow~\cite{Bongratz2024v2cflow}, a recent deep learning-based cortical surface reconstruction method. 
We show a significant improvement in the accuracy of the generated images over previous methods, while maintaining the model's ability to introduce variability in other image regions, such as the skull. 
Finally, we used Cor2Vox to simulate cortical atrophy in MRIs, supporting the benchmarking of cortical thickness estimation methods. 

% In this work, we present the first diffusion-based method for medical image synthesis that incorporates continuous shape priors. Technically, we contribute a 3D shape-to-image Brownian bridge diffusion model that operates directly in voxel space~(in contrast to prevalent latent diffusion models) and 3D signed distance fields. We combine inner white matter and outer pial cortical surfaces deliberately to create detailed and accurate representations of the brain’s structure. By leveraging both surfaces, our model is tailored to model changes in cortical thickness, offering applications . To evaluate the accuracy of the generated tissue boundaries at the subvoxel level, we leverage V2C-Flow~\cite{Bongratz2024v2cflow}, a recent deep learning-based cortical surface reconstruction method. We demonstrate a considerable improvement in the accuracy of the generated images over previous methods, preserving the model's capability to introduce variation in the rest of the image, e.g., in the skull. Finally, we show an application \ldots



% \subsection{old}
% The generation of synthetic biomedical images based on anatomical shapes is integral for the optimization of imaging systems and image processing algorithms~\cite{Segars2008CTsimulation}. Specifically, synthetic images have been shown to improve model robustness~\cite{Gopinath2024synthetic} and fairness~\cite{Burlina2021bias,Ktena2024generativefairness} --- two central criteria for computational methods applied in clinical environments. Moreover, tailored synthetic images reduce the manual annotation burden~\cite{Menten2022simulation,Xanthis2021simulator}, and they enable individualized simulations and treatment planning~\cite{zhu2008simulation}.


% Yet, taking \emph{control} over the image-generative process is challenging due to the high dimensionality of images. Moreover, controlled image generation requires a balance between (i)~image quality, (ii)~accuracy regarding the provided condition, and (iii)~variability in the generated output. To this end, sophisticated conditional models were developed in the past, incorporating tabular conditions and/or voxel masks~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene,Kim2024controllable,Lee2024ebdm,park2024shape,Peng2024metadataconditioned,Zhang2023controlnet}. However, the level of detail that can be captured by these methods is limited, see \Cref{fig:motivation} for an illustration. With tabular conditions, we can incorporate demographic variables, e.g., age, or global geometric measures, e.g., ventricular volume, into synthesized medical images~\cite{Peng2024metadataconditioned}. Voxel masks exceed the level of detail to local geometry, enabling control over location and shape of generated tumors in the image for instance~\cite{Dorjsembe2024medddpm,Kim2024controllable}. Yet, the level of detail that can be captured by voxel masks is limited by the voxel size. This represents a problem for intricate geometries like the cerebral cortex.

%, i.e., the thin and highly convoluted sheet of gray matter in the brain






% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.8\linewidth]{motivation.png}
%     \caption{The level of detail that can be achieved by controllable image generation methods based on tabular data and coarse voxel masks is limited. We propose to incorporate continuous brain surfaces to capture fine-grained individual anatomical details and cortical thickness in synthetic brain MRI.}
%     \label{fig:motivation}
% \end{figure}


% shape-describing pixel~(2D) or voxel~(3D) masks into the image synthesis~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene,Kim2024controllable,Lee2024ebdm,park2024shape,Zhang2023controlnet}. 
%~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene} 
%and Canny edge-detected image 
% for the ~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene,Kim2024controllable,Lee2024ebdm,park2024shape,Zhang2023controlnet}.
% However, the resolution of such masks is fundamentally limited by the pixel/voxel size. 

% % The cerebral cortex, i.e., the thin and highly convoluted sheet of gray matter in the brain, 
% To represent the complex sulcal and gyral structures of the cortex,
% % fall below common image resolutions, around one millimeter in magnetic resonance imaging~(MRI) for instance. Moreover, changes in cortical thickness, which is a hallmark of neurodegenerative disease progression and an important biomarker for interventions, are usually in the submillimeter range~\cite{singh2006}.
% % To address the limited image resolution, and to match topological constraints or shape priors,
%  % deformable shape models of the cortex and subcortical structures, i.e.,
% (near-) continuous shape representations in the form of triangular meshes~\cite{Dale1999,MacDonald2000,Patenaude2011shapemodel} and signed distance fields~(SDFs)~\cite{Cruz2021deepcsr,gopinath2021segrecon,Han2004cruise}, are widely used. Compared to voxel masks, these shapes are continuously deformable, enabling fine-grained simulations of brain atrophy in the sub-millimeter range~\cite{Camara2006atrophy,Rusak2022benchmark}. Synthesized images that accurately match the introduced alterations are vital to benchmark segmentation and registration algorithms~\cite{Camara2006atrophy,CastellanoSmith2003simulation,Karacali2006simulation,Larson2022synthetic,Rusak2022benchmark,Sharma2010atrophy}. Moreover, synthetic images conditioned on individual brain anatomy can be used for planning surgical brain interventions~\cite{Hill1999braindeformation,Miga2000braindeformation}.
% % , This issue is generally addressed by representing cortical tissue boundaries with high-resolution triangular meshes~\cite{Dale1999,MacDonald2000}~(explicit representation) or signed distance functions~\cite{Cruz2021deepcsr,gopinath2021segrecon,Han2004cruise}~(implicit representation). 
% % 
% Currently, neither the latest image generation methods
% %~\cite{Dorjsembe2024medddpm,Kim2024controllable,Peng2023generatingbrainmri,Peng2024metadataconditioned,Pinaya2022latentmri} 
% nor evaluation techniques, e.g., based on standard image segmentation~\cite{Wu2024evaluating}, can cope with the challenges arising from the intricate structure of the human brain at the sub-voxel level. 


% deformable models of brain structures Synthesizing matching images from these cortical shapes, which can be deformed with 

% neurological disorders~\cite{Martin1998shapedeformation}

% Converting these cortical shapes back Based on these shapes and congenial deformation models, intra-operative simulations With these shapes from deformed brain shapes are viable

% benchmarking~\cite{Larson2022synthetic,Rusak2022benchmark}, 

% pre-, intra-, and post-operative monitoring of brain surgery~\cite{Hill1999braindeformation,Miga2000braindeformation}.


% geometric conditions, i.e., to \emph{control} the generation, as this improves the quality and utility of the generated data~\cite{Kim2024controllable,Lee2024ebdm,park2019SPADE,Zhang2023controlnet}. Specifically, the controlled generation of medical images based on individual anatomies permits shape-based individualized simulations, e.g., virtual surgery~\cite{zhu2008simulation}, optimization of imaging systems with known anatomy~\cite{Segars2008CTsimulation}, benchmarking reconstruction methods treatment planning~\cite{Palomar2016surface}, augmenting datasets with anatomically rare cases, and the creation of paired/registered data.  CITE SOMETHING (brain tumor paper, etc). Recently, sophisticated conditional models have been developed for the generation of 2D/3D images based on pixel/voxel masks
% %~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene} 
% and Canny edge-detected image contours~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene,Kim2024controllable,Lee2024ebdm,park2024shape,Zhang2023controlnet}.
% However, the shape of the cerebral cortex, i.e., the thin and highly convoluted sheet of gray matter in the brain, cannot be represented adequately at conventional magnetic resonance image~(MRI) resolution due to partial volume effects; instead, its boundaries are commonly defined by high-resolution triangular meshes~\cite{Bongratz2024v2cflow,Dale1999,lebrat2021corticalflow,Ma2023cortexode,MacDonald2000}~(explicit representation) or signed distance functions~\cite{Cruz2021deepcsr,gopinath2021segrecon,Han2004cruise}~(implicit representation). Hence, individualized brain MRI generation requires precise incorporation of such shape priors. Yet, neither the latest brain MRI generation methods~\cite{Dorjsembe2024medddpm,Kim2024controllable,Peng2023generatingbrainmri,Peng2024metadataconditioned,Pinaya2022latentmri} nor evaluation techniques, e.g., based on standard image segmentation~\cite{Wu2024evaluating}, consider the intricate structure of the human cortex at the sub-voxel level~($<$1mm). 
% Difficulty: brain shapes are highly individual -> cite something




% Core contributions:
% first 3D BBDM,
% operating directly in image space,
% conditioning on 3D shapes,
% evaluation with V2C-Flow,


% accurate generation of individualized brain MRI should match the precisely.  representation of the complex cortical morphology in generated images should match these 


% surface representations such as surface meshes by  which has direct implications for brain functionality  

% need the capability to incorporate precise anatomical conditions. 
% There exist sophisticated approaches to tailor image generation to certain queries. Geometry

% synthetic Motivation: address data scarcity in medical imaging, scanner effects, augmenting datasets to reduce bias and improve fairness~\cite{Burlina2021bias,Ktena2024generativefairness}, balancing datasets with respect to disease classes, reducing manual annotation burden

% ~\cite{Gopinath2024synthetic}

% digital twins, individualized predictions and simulations, personalized medicine~\cite{Giuffr2023synthetichealthcare}.

% statistical shape models

% Training with synthetic datasets has yielded good generalization in segmentation~\cite{Billot2023synthseg} and registration~\cite{Hoffmann2022synthmorph} models but there is no way to create synthetic data from cortical surfaces. $\rightarrow$ improving surface-based cortical analysis ~\cite{Bongratz2024v2cflow,gopinath2023reconallclinical}

% Synthetic atrophy for analysis of neurodegeneration and benchmarking~\cite{Larson2022synthetic,Rusak2022benchmark}

% advantage of shapes: morphological changes can be simulated with deformation fields \& physical models, finite elements~\cite{Ferrant2000deformable}

% Simulation of atrophy for benchmarking~\cite{Larson2022synthetic,Rusak2022benchmark}

% shape priors, statistical shape models~\cite{Bastian2023s3m,Iyer2023mesh2ssm}

% Generative models conditioned on masks~\cite{Avrahami2023maskcondition,Dorjsembe2024medddpm,Gafni2022makeascene}, edge maps~\cite{Kim2024controllable,Zhang2023controlnet} and covariates such as age, sex~\cite{Peng2024metadataconditioned} or volumes~\cite{Pinaya2022latentmri}.

% \noindent
% \textbf{Related Work.}
\section{Related Work}
% In the following, we focus on works that are technically related to our method. 
% In the following, we specifically discuss recent image generation methods based on geometric conditions.
% models~\cite{Camara2006atrophy,CastellanoSmith2003simulation,Khanal2016biophysical} and Jacobian transformations~\cite{Karacali2006simulation,Sharma2010atrophy}. However, these methods focus on deforming existing images whereas we aim to generate them from scratch.
% \noindent
% \textbf{Image synthesis with geometric conditions.}
To date, generative adversarial networks (GAN)~\cite{goodfellow2014gan} and 
%Denoising Diffusion Probabilistic Models~
DDPMs~\cite{ddpm,sohl-dickstein-diffusion2015} are among the most prevalent approaches for image generation. 
Both have been extended to incorporate geometric and semantic constraints, such as segmentation masks and edge maps, to achieve more controlled  synthesis~\cite{pix2pix2017,Wang2018conditionalGAN,park2019SPADE,Rombach2022diffusion,Zhang2023controlnet}. 
However, these approaches are typically tailored to 2D images, making their adaptation to 3D medical data inherently challenging. 
Current approaches for 3D medical image generation commonly either
operate in a lower-dimensional latent space~\cite{Kim2024controllable,Peng2024metadataconditioned,Pinaya2022latentmri} or generate 3D volumes slice by slice~\cite{Han2023medgen3d,Peng2023generatingbrainmri,Li2024pasta}. Nonetheless, 
latent diffusion models have inherent limitations in achieving high precision~\cite{Konz2024controllableddpm,Rombach2022diffusion}, while slice-wise generative models introduce inter-slice inconsistencies which demand further post-processing~\cite{Han2023medgen3d,Li2024pasta}.
Recent advancements, such as Med-DDPM~\cite{Dorjsembe2024medddpm}, directly generate 3D medical scans conditioned on voxel segmentation masks, providing control over the location and shape of structures like tumors~\cite{Dorjsembe2024medddpm,Kim2024controllable}. 
Yet, their level of detail is limited by the voxel size, 
% which may be problematic for complex geometries like the cortex.
which poses challenges when generating fine details for complex geometries, such as the cortex.
While this discussion focuses on geometric condition-based image generation, it is worth noting that alternative approaches for synthesizing medical images exist as well, including methods based on Jacobian deformations~\cite{Karacali2006simulation,Sharma2010atrophy} and biophysical models~\cite{Camara2006atrophy,CastellanoSmith2003simulation,Khanal2016biophysical}.

% Finally, a GAN-based method was developed to simulate brain atrophy in 3D brain MRI~\cite{Rusak2022benchmark}. Despite its effectiveness, it is restricted to skull-stripped images.

%Voxel masks exceed the level of detail to local geometry, enabling control over location and shape of generated tumors in the image for instance~\cite{Dorjsembe2024medddpm,Kim2024controllable}. Yet, the level of detail that can be captured by voxel masks is limited by the voxel size. This represents a problem for intricate geometries like the cerebral cortex.

%For both models, GANs~\cite{pix2pix2017,Wang2018conditionalGAN,park2019SPADE} and DDPMs~\cite{Rombach2022diffusion,Zhang2023controlnet}, follow-up extensions exist to incorporate geometric and semantic conditions in the form of segmentation masks and edge maps. However, these models are typically designed for 2D images and are difficult to adapt to 3D medical scans. 

% \noindent
% \textbf{3D biomedical image generation.}
% Nonetheless, several methods exist for generating 3D medical image data. To deal with the additional dimension, which considerably increases the computational and memory complexity, several DDPM-based methods operate in an abstract latent space~\cite{Kim2024controllable,Peng2024metadataconditioned,Pinaya2022latentmri} or generate 3D volumes slice by slice~\cite{Han2023medgen3d,Peng2023generatingbrainmri,Li2024pasta}. However, both approaches are suboptimal. Latent diffusion models suffer from low accuracy on the voxel level due to the mapping to the lower-dimensional latent space~\cite{Konz2024controllableddpm,Rombach2022diffusion}. Slice-wise generative models, on the other hand, introduce problematic inconsistencies between the slices and, therefore, require further post-processing~\cite{Han2023medgen3d}. As a notable exception, Med-DDPM~\cite{Dorjsembe2024medddpm} can directly generate 3D medical scans conditioned on binary tumor segmentation masks. Furthermore, a GAN-based approach was previously proposed to simulate brain atrophy in 3D brain MRI~\cite{Rusak2022benchmark}.

% the generation of images with a latent diffusion model 

% DDIM~\cite{ddim}
% BBDM~\cite{li_bbd\alpha_2023}
% EBDM~\cite{Lee2024ebdm}

% \noindent
% \textbf{Brownian bridge diffusion models.}
% Brownian bridge diffusion models~(BBDMs) were recently proposed for image-to-image translation~\cite{li_bbdm_2023}. Compared to DDPMs, BBDMs map directly between paired images instead of gradually converting images to pure Gaussian noise. However, existing models~\cite{Lee2024ebdm,li_bbdm_2023} were developed for 2D image data, and they are operating in the latent space of an autoencoder trained on 2D natural images. We build upon these works and transfer the idea of Brownian bridge diffusion to the realm of 3D medical images, incorporating precise anatomical shape priors during the generation.

%Hence, these methods are not suited for synthesizing accurate brain images based on 3D anatomical shapes.


% Conditioning:
% Masks

% Segmentation as output, text guidance~\cite{xu2024medsyn}


% Generation of brain MRI:
% Existing methods solely based on DDPM/DDIM

% latent DDIM, age, sex, brain volumes condition, 3D~\cite{Pinaya2022latentmri}

% latent DDIM, text \& canny edge mask (binary), 2D~\cite{Kim2024controllable}

% latent DDPM, age, sex condition, age/sex effects are replicated, FreeSurfer segmentation evaluation~\cite{Peng2024metadataconditioned}

% DDPM, tumor mask Med-DDPM~\cite{Dorjsembe2024medddpm}

% DDPM, segmentation mask~\cite{Konz2024controllableddpm}

% slice-by-slice generation of brain MRI~\cite{Peng2023generatingbrainmri}


% Signed distance functions to represent cortical surfaces~\cite{Cruz2021deepcsr,gopinath2021segrecon}


\section{Methods}



\subsection{Cortical Shape Representations}


Triangular surface meshes are a common \emph{explicit} representation of cortical tissue boundaries~\cite{fischlFreeSurfer2012,Bongratz2024v2cflow}, directly usable for visualization and rendering. 
Alternatively, surfaces can be represented \emph{implicitly} through signed distance fields (SDF). 
Given a surface $\mathcal{S}\subset\mathbb{R}^3$, an SDF is defined as a function: 
\begin{equation} 
g: \Omega \subseteq \mathbb{R}^3 \to \mathbb{R}, 
\end{equation} 
which maps each point $x\in\mathbb{R}^3$ to its orthogonal distance to the surface $\mathcal{S}$. 
The zero-level set of $g$ corresponds to the surface $\mathcal{S}$, and the sign of $g$ indicates whether $x$ lies inside ($g(x)<0$) or outside ($g(x)>0$) the surface.
% Triangular surface meshes are a common representation of cortical tissue boundaries~\cite{fischlFreeSurfer2012,Bongratz2024v2cflow}. This representation is often considered to be \emph{explicit}, as it can be directly used for rendering and visualization. Alternatively, there exists also an \emph{implicit} surface representation in the form of signed distance fields~(SDFs). Namely, a surface $\mathcal{S}\subset\mathbb{R}^3$ is given by the zero-level set of a function $g$, i.e.,
% \begin{equation}
%     g: \Omega \subseteq \mathbb{R}^3 \mapsto \mathbb{R},
% \end{equation}
% which maps each point $x\in\mathbb{R}^3$ to its orthogonal distance to the surface. Moreover, the sign of $g$ indicates whether the point lies inside ($g(x)<0$) or outside ($g(x)>0$) the surface. 
%With algorithms like marching cubes~\cite{lorensen1987mc}, SDFs can be converted to meshes; vice-versa, 
With distance transforms~\cite{Danielsson1980distance}, surface meshes can be converted into SDFs by computing the orthogonal distance from an input point to its closest triangle. Hence, SDFs can be represented as dense voxel grids, making them compatible with 3D image data. Additionally, meshes can be converted into binary edge maps by identifying the occupancy of voxels with mesh faces. 
% However, in contrast to SDFs, these edge maps eliminate information about the surface below the voxel level.



% \subsection{Cortex SDF}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{cortexsdfalg.pdf}
    \caption{Generation of cortex SDF ($\mathcal{S}_c$) and cortical ribbon mask ($\mathcal{R})$.}
    \label{fig:cortexsdf}
\end{figure}



% \begin{algorithm}[t]
% \caption{Generation of Cortex SDF and Cortical Ribbon Mask}
% \label{alg:cortexsdf}
% \begin{algorithmic}[1]
% \Procedure{GetCortex}{$\mathcal{S}_p, \mathcal{S}_w$}
%     % \State $\mathcal{S}_p$: \text{Pial surface SDF of size $(N, H, W, D)$}
%     % \State $\mathcal{S}_w$: \text{White matter surface SDF of size $(N, H, W, D)$}
%     \State $\mathcal{S}_p, \mathcal{S}_w$: \text{Pial and white matter surface SDF of size $(N, H, W, D)$}
%     \State $\mathcal{S}_c \gets \text{initialize cortex SDF as tensor of size } (N, H, W, D) \text{ with entries set to $0$}$
%     \State $\mathcal{R} \gets \text{initialize cortex mask as tensor of size } (N, H, W, D) \text{ with entries set to $0$}$
%     \State $\text{Mask}_{in} \gets (\mathcal{S}_p < 0) \land (\mathcal{S}_w < 0)$ \hfill \(\triangleright\) Identify region under the cortex
%     \State $\mathcal{S}_c ~[\text{Mask}_{in}] \gets \max(\mathcal{S}_p, \mathcal{S}_w)[\text{Mask}_{in}]$ \hfill \(\triangleright\) Assign max SDF for the region
%     \State $\text{Mask}_{out} \gets (\mathcal{S}_p > 0) \land (\mathcal{S}_w > 0)$  \hfill \(\triangleright\) Identify region above the cortex
%     \State $\mathcal{S}_c ~[\text{Mask}_{out}] \gets \min(\mathcal{S}_p, \mathcal{S}_w)[\text{Mask}_{out}]$ \hfill \(\triangleright\) Assign min SDF for the region
%     \State $\text{Mask}_{cortex} \gets \lnot ((\mathcal{S}_p < 0) == (\mathcal{S}_w < 0))$ \hfill \(\triangleright\) Identify cortex region
%     \State $\mathcal{S}_c ~[\text{Mask}_{cortex}] \gets 0$ \hfill \(\triangleright\) Set cortex region to zero
%     \State $\mathcal{R} ~[\text{Mask}_{cortex}] \gets 1$ \hfill \(\triangleright\) Set cortex region to one in the cortex mask
%     \State \Return $\mathcal{S}_c$, $\mathcal{R}$
% \EndProcedure
% \end{algorithmic}
% \end{algorithm}

% As illustrated in Fig. 1(c), the cortex is described by two meshes, the pial surface mesh $\mathcal{M}_p$ and white matter surface mesh $\mathcal{M}_w$. 
As shown in \Cref{fig:motivation}(c), the cerebral cortex has outer and inner surface boundaries, namely the pial and white matter surface, which are commonly represented as separate meshes $\mathcal{M}_p$ and $\mathcal{M}_w$, respectively. 
% surface mesh $\mathcal{M}_p$ and white matter surface mesh $\mathcal{M}_w$. 
We aim to combine both surfaces into a single cortex-describing SDF, $\mathcal{S}_c$, which will be the source input to our framework.
%We propose utilizing cortex SDFs derived from the given brain mesh $\mathcal{M}$ --- comprising pial surface mesh $\mathcal{M}_p$ and white matter surface mesh $\mathcal{M}_w$ --- as the source input to our shape-to-image framework. 
To achieve this, we first convert the cortical meshes into dense SDFs using a distance transform, yielding $\mathcal{S}_p$ and $\mathcal{S}_w$.
% represented as 3D dense voxel grids, denoted as $\mathcal{S}_p = g(\mathcal{M}_p)$ and $\mathcal{S}_w = g(\mathcal{M}_w)$. 
Subsequently, we construct the cortex SDF $\mathcal{S}_c$ by combining $\mathcal{S}_p$ and $\mathcal{S}_w$ 
% through the algorithm detailed 
following the procedure in~\Cref{fig:cortexsdf}. We begin by initializing $\mathcal{S}_c$ as a tensor of the same size as $\mathcal{S}_p$ and $\mathcal{S}_w$, with all entries set to zero. The regions of interest are then identified and processed as follows. 
i) The region outside the cortex is identified where $\mathcal{S}_p$ and $\mathcal{S}_w$ have both positive values; within this region, we assign the lower value, i.e., the distance to the pial surface, to $\mathcal{S}_c$.
% is updated with the maximum values between $\mathcal{S}_p$ and $\mathcal{S}_w$; 
ii) Next, the region inside the cortical ribbon is determined as $\mathcal{S}_p$ and $\mathcal{S}_w$ having both negative values; within this region, we assign the higher value, i.e., the distance to the white matter surface, to $\mathcal{S}_c$. 
% where $\mathcal{S}_c$ is assigned the minimum values between $\mathcal{S}_p$ and $\mathcal{S}_w$; 
iii) Lastly, the cortical ribbon is given by the region where $\mathcal{S}_p$ and $\mathcal{S}_w$ have opposite signs; we set $\mathcal{S}_c$ in this region to zero to ensure a clear cortical boundary. We also extract this cortical ribbon as a separate mask $\mathcal{R}$.
% The resulting cortex SDF $\mathcal{S}_c$ will be used as the input shape domain for our shape-to-image BBDM.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{network_all.pdf}
    \caption{Cor2Vox leverages a shape-to-image Brownian bridge diffusion process to learn a stochastic mapping $f_\theta$ between the shape prior $\mathcal{S}_c$ and the MRI domain $\mathcal{I}$. During the reverse diffusion process, additional shape conditions are incorporated to improve structural alignment in the generated MRIs. $f_\theta$ is modeled using a 3D UNet.}
    \label{fig:architecture}
\end{figure}

\subsection{Shape-to-Image Brownian Bridge Diffusion}
Our objective is to generate an image $\mathcal{I} = f_\theta(\mathcal{S}_c)$ that precisely matches the geometry given by a shape prior $\mathcal{S}_c$.
% train a model $f_\theta(\cdot)$ that, given the shape condition $\mathcal{S}_c$, generates an image $\mathcal{I} = f_\theta(\mathcal{S}_c)$, ensuring that $\mathcal{I}$ is anatomically constrained by the cortex shape specified in $\mathcal{S}_c$.
Inspired by \cite{li_bbdm_2023}, we use a Brownian bridge process and adapt it for directly transforming the shape to the 3D image domain. We show an overview of our method, \emph{Cor2Vox}, in \Cref{fig:architecture}.
Cor2Vox uses paired data $(\mathcal{S}_c^i, ~\mathcal{I}^i)_{i=1}^N$ for training, comprising $N$ pairs of 3D MRIs $\mathcal{I}\in{\mathbb{R}^{H\times W\times D}}$ and corresponding cortical shapes. We assume that shapes are given as dense SDFs $\mathcal{S}_c\in{\mathbb{R}^{H\times W\times D}}$ with the same dimensionality as the MRIs.
% Given the shape domain constructed by $\mathcal{S}_c$ and MRI image domain $\mathcal{I}$, let ($s_c$, $i$) denote the paired data from domains $\mathcal{S}_c$ and $\mathbf{M}$, in which $\mathbf{s}\in{\mathbb{R}^{H\times W\times D}}$ and  $\mathbf{m}\in{\mathbb{R}^{H\times W\times D}}$, with height $H$, width $W$, and depth $D$ respectively. 







\noindent
\textbf{Forward Process.} Unlike DDPM, which perturbs the input until pure Gaussian noise $\varepsilon \sim \mathcal{N}(0, \mathbf{I}$) at timestep $T$, the Brownian bridge 
% Diffusion Model (BBDM)~\cite{li_bbdm_2023} assumes that 
diffusion process maps between 
% both endpoints of the diffusion process as fixed 
data points from the joint distribution of structured  
source and target domains, i.e., $(\boldsymbol{x}_T, \boldsymbol{x}_0) \sim q_{data}(\mathcal{S}_c, ~\mathcal{I})$. Starting from an initial state $\boldsymbol{x}_0$ (i.e., the MRI volume $\mathcal{I}$), the forward process of the Brownian bridge connects it to the destination state $\boldsymbol{x}_T$ (i.e., the shape prior $\mathcal{S}_c$) by:
\begin{equation}
    q(\boldsymbol{x}_t \mid \boldsymbol{x}_0, \boldsymbol{x}_T) = \mathcal{N}\big(\boldsymbol{x}_t; (1 - \alpha_t)\boldsymbol{x}_0 + \alpha_t \boldsymbol{x}_T, \delta_t \mathbf{I}\big), \quad \text{where} \quad \mathcal{S}_c = \boldsymbol{x}_T,
\end{equation}

\noindent
$\alpha_t = t/T$, $T$ the total number of steps in the diffusion process, and $\delta_t = 2(\alpha_t - {\alpha_t}^2)$ the variance term. The transition probability between two consecutive steps can be derived as~\cite{li_bbdm_2023}:
\begin{align}
\label{eq:bbdm_forward}
    q_{\text{BB}}(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_T) &= \mathcal{N}\bigg(\boldsymbol{x}_t; \frac{1 - \alpha_t}{1 - \alpha_{t-1}} \boldsymbol{x}_{t-1} + \left(\alpha_t - \frac{1 - \alpha_t}{1 - \alpha_{t-1}} \alpha_{t-1}\right) \boldsymbol{x}_T, \delta_{t \mid t-1} \mathbf{I}\bigg), \nonumber \\
    \delta_{t \mid t-1} &= \delta_t - \delta_{t-1} \frac{(1 - \alpha_t)^2}{(1 - \alpha_{t-1})^2}.
\end{align}

\begin{algorithm}[t]
\caption{Training Process}
\label{alg:training}
\begin{algorithmic}[1]
\Repeat
    \State Paired data: $\boldsymbol{x}_0 \sim q(\mathcal{I}), \boldsymbol{x}_T \sim q(\mathcal{S}_c), \mathcal{C} = (\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}, \mathcal{R}) \sim q(\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}, \mathcal{R})$
    \State Timestep $t \sim \text{Uniform}(1, \dots, T)$
    \State Gaussian noise $\varepsilon \sim \mathcal{N}(0, \mathbf{I})$
    \State Forward diffusion $\boldsymbol{x}_t = (1 - \alpha_t)\boldsymbol{x}_0 + \alpha_t \boldsymbol{x}_T + \sqrt{\delta_t} \varepsilon$
    \State Take gradient descent step on
    $\nabla_\theta 
    \lVert \alpha_t (\boldsymbol{x}_T - \boldsymbol{x}_0) 
    + \sqrt{\delta_t} \boldsymbol{\varepsilon} - f_\theta(\boldsymbol{x}_t, \mathcal{C}, t) \rVert_1$
\Until{converged}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Sampling Process}
\begin{algorithmic}[1]
\State Sample conditional input $x_T = \mathcal{S}_c \sim q(\mathcal{S}_c)$, $\mathcal{C} = (\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}, \mathcal{R}) \sim q(\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}, \mathcal{R})$
\For{$t = T, \dots, 1$}
    % \State $\varepsilon \sim \mathcal{N}(0, \mathbf{I})$ if $t > 1$, else $\varepsilon = 0$
    % \If{$t > 1$}
    %     \State $\varepsilon \sim \mathcal{N}(0, \mathbf{I})$
    % \Else
    %     \State $\varepsilon = 0$
    % \EndIf
    \State \textbf{if} $t > 1$ \textbf{then} $\varepsilon \sim \mathcal{N}(0, \mathbf{I})$, \textbf{else} $\varepsilon = 0$
    \State $x_{t-1} = c_{xt} \boldsymbol{x}_t + c_{st} \mathcal{S}_c - c_{ft} f_\theta \big(\boldsymbol{x}_t, \mathcal{C}, t \big) + \sqrt{\tilde{\delta}_t} \varepsilon$
\EndFor
\State \Return $x_0$
\end{algorithmic}
\label{alg:sampling}
\end{algorithm}

Thus, at the beginning of the diffusion process, i.e., $t = 0$, we have $\alpha_0 = 0$, starting from the mean value of $\boldsymbol{x}_0 = \mathcal{I}$ with probability 1 and variance $\delta_0 = 0$.
As the diffusion progresses, the variance $\delta_t$ first increases to its maximum value $\delta_{max} = \delta_{T/2} = 1/2$ at the midpoint of the process, and then decreases until it reaches $\delta_{T} = 0$ at $\alpha_T = 1$. At this point, the diffusion concludes in the destination with a mean value equal to $\mathcal{S}_c$. Through this mechanism, the forward diffusion process establishes a stochastic mapping with fixed endpoints from the image domain $\mathcal{I}$ to the shape domain $\mathcal{S}_c$.  \\

\noindent
\textbf{Reverse Process.} The reverse process of BBDM is designed to predict $\boldsymbol{x}_{t-1}$ given $\boldsymbol{x}_t$, starting directly with the conditional input $\boldsymbol{x}_T = \mathcal{S}_c$, different from standard diffusion models that start from the pure Gaussian noise. To improve the structural alignment, we incorporate supplementary shape representations that can be obtained from the cortical meshes. This includes the SDF of the pial surface $\mathcal{S}_p$, the SDF of the white matter surface $\mathcal{S}_w$, a binary edge map $\mathcal{E}$ containing both surfaces, and the cortical ribbon mask $\mathcal{R}$, cf.~\Cref{fig:cortexsdf}. We collectively represent these conditions as $\mathcal{C} = (\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}, \mathcal{R})$, and integrate them at each timestep $t$ to aid the prediction:
\begin{equation}
\label{eq:bbdm_reverse}
    p_\theta(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \mathcal{C}, \mathcal{S}_c) = \mathcal{N}\big(\boldsymbol{x}_{t-1}; \boldsymbol{\mu}_\theta(\boldsymbol{x}_t, \mathcal{C}, t), \tilde{\delta}_t \mathbf{I}\big).
\end{equation}
\noindent Here, $\boldsymbol{\mu}_\theta(\boldsymbol{x}_t, \mathcal{C}, t)$ represents the predicted mean value while $\tilde{\delta}_t$ denotes the variance of noise at each timestep. Following the reparameterization strategy used in DDPM~\cite{ddpm}, we can train a neural network $f_\theta(\cdot)$ to predict solely the noise instead of the mean $\boldsymbol{\mu}_\theta$. Namely, we reformulate $\boldsymbol{\mu}_\theta$ as a linear combination of $\boldsymbol{x}_t$,  $\mathcal{S}_c$, and the estimated part $f_\theta$:
\begin{align}
\label{eq:bbdm_ep}
\boldsymbol{\mu}_\theta(\boldsymbol{x}_t, \mathcal{C}, \mathcal{S}_c, t) &= c_{xt} \boldsymbol{x}_t + c_{st} \mathcal{S}_c + c_{ft} f_\theta(\boldsymbol{x}_t, \mathcal{C}, t), \, \text{where} \\
    c_{xt} &= \frac{\delta_{t-1}}{\delta_t} \frac{1 - \alpha_t}{1 - \alpha_{t-1}} + \frac{\delta_{t|t-1}}{\delta_t} (1 - \alpha_{t-1}), \nonumber  \\
    c_{st} &= \alpha_{t-1} - \alpha_t \frac{1 - \alpha_t}{1 - \alpha_{t-1}} \frac{\delta_{t-1}}{\delta_t}, \nonumber \, \text{and}  \\
    c_{ft} &= (1 - \alpha_{t-1}) \frac{\delta_{t|t-1}}{\delta_t}. \nonumber
\end{align}
The parameters $c_{xt}$, $c_{st}$, and $c_{ft}$ are non-trainable. Instead, they are derived from $\alpha_t$, $\alpha_{t-1}$, $\theta_t$, and $\theta_{t-1}$.
The variance term $\tilde{\delta}_t$ does not need to be learned either; it can be derived in the analytical form as $\tilde{\delta}_t = \frac{\delta_{t|t-1} \cdot \delta_{t-1}}{\delta_t}$. \\


\noindent
\textbf{Training.} We outline the training process in \Cref{alg:training}. The training is designed to minimize the disparity between the joint distribution predicted by the model $f_\theta$ and the training data. This is achieved by optimizing the Evidence Lower Bound (ELBO) defined below:
\begin{align}
\text{ELBO} &= - \mathbb{E}_q \big( \text{D}_{\text{KL}}(q_{\text{BB}}(\boldsymbol{x}_T \vert ~\boldsymbol{x}_0, \mathcal{S}_c) \parallel p(\boldsymbol{x}_T \vert ~\mathcal{C}, \mathcal{S}_c))  \nonumber \\
&\quad + \sum_{t=2}^T \text{D}_{\text{KL}}(q_{\text{BB}}(\boldsymbol{x}_{t-1} \vert ~\boldsymbol{x}_t, \boldsymbol{x}_0, \mathcal{S}_c) \parallel p_\theta(\boldsymbol{x}_{t-1} \vert ~\boldsymbol{x}_t, \mathcal{C}, \mathcal{S}_c)) \nonumber \\
&\quad - \log p_\theta(\boldsymbol{x}_0 \vert ~\boldsymbol{x}_1, \mathcal{C}, \mathcal{S}_c) \big).
\end{align}
By combining the ELBO with~\Cref{eq:bbdm_forward,eq:bbdm_reverse,eq:bbdm_ep}, the training objective is given by:
\begin{equation}
    \mathcal{L}_{\text{Cor2Vox}} =
    \mathbb{E}_{x_0, \mathcal{S}_c, \varepsilon \sim \mathcal{N}(0, \mathbf{I})} \Big[
    \Big\lVert \alpha_t (\mathcal{S}_c - \boldsymbol{x}_0) 
    + \sqrt{\delta_t} \boldsymbol{\varepsilon} - f_\theta(\boldsymbol{x}_t, \mathcal{C}, t) \Big\rVert_1
    \Big].
\end{equation}
\\

\noindent
\textbf{Sampling.} We outline the sampling process in \Cref{alg:sampling}. The sampling process can be derived as:
\begin{equation}
    \boldsymbol{x}_{t-1} = c_{xt} \boldsymbol{x}_t + c_{st} \mathcal{S}_c - c_{ft} f_\theta \big(\boldsymbol{x}_t, \mathcal{C}, t \big) + \sqrt{\tilde{\delta}_t} \varepsilon,
\end{equation}
\noindent where $\varepsilon \sim \mathcal{N}(0, \mathbf{I})$ when $t > 1$, otherwise $\varepsilon = 0$. We accelerate the sampling process using the DDIM~\cite{ddim} strategy, which adopts a non-Markovian process with the same marginal distributions as Markovian inference.


\subsection{Surface-Based Evaluation of Geometric Accuracy}
Since we use continuous brain surfaces, $\mathcal{M}^{\text{Ref}}$, as the input to our model, we can evaluate the geometric accuracy of the generated images via surface-based distance metrics. This is not possible for voxel-based methods, where no such reference is available for comparison. 
% Evaluating the geometric accuracy of generated brain images is particularly challenging in our setting, where the goal is to generate images from continuous surfaces. Traditional evaluation methods fall short in this context, as they are not designed to handle the intricacies of continuous surface generation. 
% Therefore, we propose using a recent cortical surface reconstruction method known as 
Specifically, we propose to use V2C-Flow~\cite{Bongratz2024v2cflow} to obtain cortical surfaces, $\mathcal{M}^{\text{Pred}}$ from the generated images. Based on these reconstructions, we compute the average symmetric surface distance (ASSD):
\begin{equation}
  \mathrm{ASSD}(\mathcal{M}^{\text{Pred}}, \mathcal{M}^{\text{Ref}}) = 
  \frac{\sum_{p \in \mathcal{P}^{\text{Pred}}} d(p,\mathcal{M}^{\text{Ref}}) + \sum_{p \in \mathcal{P}^{\text{Ref}}} d(p,\mathcal{M}^{\text{Pred}})}
  {|\mathcal{P}^{\text{Pred}}| + |\mathcal{P}^{\text{Ref}}|},
\end{equation}
using $|\mathcal{P}^{\text{Pred}}|=|\mathcal{P}^{\text{Ref}}|=$100,000 randomly sampled surface points. The distance $d(p, \mathcal{M})$ measures the orthogonal distance from a point $p\in \mathbb{R}^3$ to its closest triangle in the mesh $\mathcal{M}$.

% compare these reconstructions to the originally provided reference surfaces. We measure the distance between the 

% This method allows us to precisely measure how well the input contours are matched in the generated images, capturing deviations at a sub-voxel level. 
% Such fine-grained evaluation is not feasible with conventional segmentation methods, which have been previously used for assessing synthetic brain MRI~\cite{Wu2024evaluating}. Neither is it practicable to use traditional cortical surface extraction methods like FreeSurfer~\cite{fischlFreeSurfer2012} for large-scale evaluation of generated images due to the long runtime of multiple hours per scan. By employing V2C-Flow, we can achieve a more accurate and detailed evaluation of the geometric fidelity of the generated images within seconds.




\section{Results}

\subsection{Experimental Setting}

\noindent
\textbf{Models and hyperparameters.} We employ a 3D UNet to model $f_\theta$, which is a 3D adaptation of the ADM architecture~\cite{DMbeatsGAN}. 
% It consists of residual layers with down-/up-sampling convolutions, skip connections, and 
We use channels of $[C, 2C, 3C, 4C]$ for each residual stage, where $C = 64$. Global attention is applied at downsampling factor 8, with 4 heads and 64 channels. We use adaptive group normalization to inject the timestep embedding into each residual block.
The model is trained using the Adam optimizer with an initial learning rate of $1 \times 10^{-4}$, reduced by a factor of $0.5$ on the plateau, a batch size of $2$, and an exponential moving average (EMA) rate of $0.995$, for 400 epochs with one NVIDIA H100 GPU. We set 1,000 timesteps for training, while 10 for inference with DDIM~\cite{ddim} sampling strategy for a balance between image quality and computational efficiency.
\\


\begin{figure}[t]
    \centering
    % \includegraphics[width=\linewidth]{vv_error_x_images_v2.png}
    \includegraphics[width=\linewidth]{vv_error_x_images_new_crop.pdf}
    \caption{Generated brain MRIs (top) and vertex-wise errors (bottom, in mm) comparing reconstructed cortical shapes to their original inputs. We show the mean error values on the cortical surface across the test set for the right hemisphere, separately for pial and white matter surfaces.}
    \label{fig:vv-error}
\end{figure}


\noindent
\textbf{Data and preprocessing.}
We used data from the Alzheimer's Disease Neuroimaging Initiative~(ADNI, \url{https://adni.loni.usc.edu}), containing T1-weighted brain MRIs from cognitively normal subjects, subjects diagnosed with mild cognitive impairment~(MCI) and Alzheimer's disease~(AD). We registered all scans to MNI152 space via affine registration, implemented in NiftyReg (v1.5.69). We also processed all data with FreeSurfer (v7.2)~\cite{fischlFreeSurfer2012}. To avoid subject bias, we used only baseline scans, with 1,155/169/323 samples for train/validation/test sets. 
%%%%%% rephrased the following %%%%%%
% We processed all data with FreeSurfer (v7.2)~\cite{fischlFreeSurfer2012} and trained V2C-Flow~\cite{Bongratz2024v2cflow} on the FreeSurfer surfaces, using the same splits as described above. To avoid distortions in the reported results due to discrepancies between FreeSurfer and V2C-Flow, we used the V2C-Flow surfaces as input to Cor2Vox for both training and sampling. 
%%%%%%%%%%%% to %%%%%%%%%%%%%%%%%%%%
We used the pre-trained V2C-Flow~\cite{Bongratz2024v2cflow} model to extract surfaces as input to Cor2Vox for both training and sampling. 
%%%%%%%%%%%%%%%%%%%%%
From the surface meshes, we created white matter and pial SDFs, merging both hemispheres into one SDF, with a KDTree~\cite{Maneewongvatana2002kdtree} implemented in Scipy~(v1.10.0). Internally, our models use a resolution of $128^3$ voxels but we rescaled all outputs to MNI space (1mm isotropic resolution, trilinear interpolation) for evaluation. To assess image quality, we used SynthStrip~\cite{Hoopes2022synthstrip} to skull-strip both generated and original images (\texttt{orig.mgz} files from FreeSurfer), to avoid the scanner noise in the original data to confound the evaluation. Nonetheless, all models were trained to generate the entire MRI, i.e., including the skull and background, as shown in our visualizations.
\\

% and used the \texttt{orig.mgz} files  We balanced all splits according to age, sex, and diagnosis. We mapped all data to the MNI152 standard space, using the affine registration implemented in Niftyreg~\cite{Modat2014niftyregaffine} to MNI152 template with NiftyReg~(v1.5.69), isotropic resolution of 1mm.

% Meshes from V2C-Flow~\cite{Bongratz2024v2cflow}. Trained on the same splits as Cor2Vox. Affine registration~\cite{Modat2014niftyregaffine} to MNI152 template with NiftyReg~(v1.5.69), isotropic resolution of 1mm.

% Data from ADNI~(\url{https://adni.loni.usc.edu}). 1,155 train, 169 val, 323 test, balanced according to age, sex, diagnosis. Baseline only.

% Conversion of mesh to SDF with a KDTree~\cite{Maneewongvatana2002kdtree}, implemented in Scipy~(v1.10.0). 

% We used SynthStrip~\cite{Hoopes2022synthstrip} implemented in FreeSurfer~\cite{fischlFreeSurfer2012}~(v7.4)

\noindent
\textbf{Baselines implementation.}
We compare Cor2Vox to state-of-the-art conditional image generation models, namely Pix2Pix~\cite{pix2pix2017}, Med-DDPM~\cite{Dorjsembe2024medddpm}, and BBDM~\cite{li_bbdm_2023}. 
% Additionally, we extended all models to incorporate our cortex SDF condition ($\mathcal{S}_{c}$). However, it is important to note that this SDF condition differs from the original implementations, which utilize masks and edge maps. 
We adapted Pix2Pix and BBDM for 3D data as both were originally developed for 2D images. 
For BBDM, we applied the Brownian diffusion process in the image space instead of the latent space, in analogy to Med-DDPM and Cor2Vox.  
% Our implementation of Pix2Pix is based on the original repository\footnote{\url{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}}, but adapted it for 3D generation. For Med-DDPM~\cite{Dorjsembe2024medddpm}, a recent 3D diffusion model designed for mask-based brain MRI generation, we also used the original repository as a basis\footnote{\url{https://github.com/mobaidoctor/med-ddpm}}. 
We adapted Med-DDPM by switching the intensity scaling from slice-wise to volumetric and the denoising target from noise to denoised image, which considerably enhanced its performance. 
%as we found them considerably improving its performance.  
We provided the mesh edges ($\mathcal{E}$) and cortical ribbon masks ($\mathcal{R}$) as input conditions to these models, as they closely resemble the masks and edge maps used in the original works.
In addition, we developed a variant of Cor2Vox by replacing the Brownian bridge process with a standard denoising diffusion process with the same input as Cor2Vox; we call this variant Cor2Vox{\scriptsize/DDPM}.





\begin{table}[t]
    \setlength{\tabcolsep}{1.8pt}
    \renewcommand\bfdefault{b}% rather than bx
    \centering
    \caption{Quantitative comparison of implemented methods for 3D brain MRI generation. We report mean \textpm~SD across all samples in our test set. Geometric accuracy of the cortex is in mm.}
    \begin{threeparttable}
    \begin{tabular}{lcccccc}
    \toprule
    &&& \multicolumn{4}{c}{Geometric Accuracy --- ASSD$\downarrow$} \\
    \cmidrule(lr){4-7}
    & \multicolumn{2}{c}{Image Quality} & \multicolumn{2}{c}{Left Hemisphere} & \multicolumn{2}{c}{Right Hemisphere} \\
    \cmidrule(lr){2-3}
    \cmidrule(lr){4-5}
    \cmidrule(lr){6-7}
    Model 
    %& Cond. 
    & SSIM$\uparrow$ & PSNR$\uparrow$ & White & Pial & White & Pial \\

    \midrule
    
    % P2P-3D & Edges
    Pix2Pix$_\mathcal{E}$\cite{pix2pix2017}\tnote{*}
    & 0.891{\tiny\textpm0.011}
    & \textbf{24.76{\tiny\textpm1.96}}
    & 0.784{\tiny\textpm0.190}
    & 0.853{\tiny\textpm0.227}
    & 0.760{\tiny\textpm0.174}
    & 0.804{\tiny\textpm0.195}
    \\

        %DDPM_experiments/whole_brain_edges_128_ts
    % DDPM-3D & Edges
    Med-DDPM$_\mathcal{E}$\cite{Dorjsembe2024medddpm}
    & 0.894{\tiny\textpm0.015}
    & 22.38{\tiny\textpm2.63}
    & 0.506{\tiny\textpm0.042}
    & 0.437{\tiny\textpm0.027}
    & 0.516{\tiny\textpm0.043}
    & 0.436{\tiny\textpm0.026}
    \\

      %Cor2Vox/bbdm_c2v/Result\mathcal{S}_whole_TestSet/Test_samples_meshedges_Ts
    % BBDM-3D & Edges
    BBDM$_\mathcal{E}$\cite{li_bbdm_2023}\tnote{*}
    & 0.898{\tiny\textpm0.020}
    & 19.95{\tiny\textpm2.61}
    & 0.349{\tiny\textpm0.032}
    & 0.307{\tiny\textpm0.021}
    & 0.368{\tiny\textpm0.035}
    & 0.307{\tiny\textpm0.021}
    \\
    
    \midrule

    

    %/mnt/nas/Projects/Cor2Vox/Pix2Pix_experiments/mask_input/
    Pix2Pix$_\mathcal{R}$\cite{pix2pix2017}\tnote{*}
    %  -3D & Mask 
    & 0.885{\tiny\textpm0.013}
    & 23.69{\tiny\textpm2.06}
    & 0.697{\tiny\textpm0.137}
    & 0.691{\tiny\textpm0.155}
    & 0.698{\tiny\textpm0.183}
    & 0.680{\tiny\textpm0.221}
    \\



    % \midrule

    % DDPM_experiments/whole_brain_masks_128_ts
    Med-DDPM$_\mathcal{R}$\cite{Dorjsembe2024medddpm}
    %-3D & Mask
    & 0.899{\tiny\textpm0.016}
    & 22.30{\tiny\textpm2.80}
    & 0.359{\tiny\textpm0.043}
    & 0.347{\tiny\textpm0.034}
    & 0.373{\tiny\textpm0.106}
    & 0.352{\tiny\textpm0.101}
    \\


    % \midrule

    %Cor2Vox/bbdm_c2v/Result\mathcal{S}_whole_TestSet/Test_samples_cortexmask_Ts
    BBDM$_\mathcal{R}$\cite{li_bbdm_2023}\tnote{*}
    %& Mask
    & 0.898{\tiny\textpm0.021}
    & 19.67{\tiny\textpm2.54}
    & 0.328{\tiny\textpm0.037}
    & 0.283{\tiny\textpm0.034}
    & 0.338{\tiny\textpm0.128}
    & 0.287{\tiny\textpm0.121}
    \\

    % \midrule

    

    % P2P-3D & DF
    % Pix2Pix$_{\mathcal{S}_c}$   %\cite{pix2pix2017}\tnote{*}
    % & 0.841{\tiny\textpm0.011}
    % & 22.90{\tiny\textpm1.48}
    % & 1.232{\tiny\textpm0.133}
    % & 1.280{\tiny\textpm0.150}
    % & 1.196{\tiny\textpm0.127}
    % & 1.228{\tiny\textpm0.135}
    % \\

    % %DDPM_experiments/whole_brain_sdf_pial_white_128_ts
    % % DDPM-3D & SDF
    % Med-DDPM$_{\mathcal{S}_{c}}$    %\cite{Dorjsembe2024medddpm}
    % & 0.882{\tiny\textpm0.019}
    % & 20.55{\tiny\textpm2.70}
    % & 0.530{\tiny\textpm0.093}
    % & 0.530{\tiny\textpm0.071}
    % & 0.531{\tiny\textpm0.091}
    % & 0.529{\tiny\textpm0.067}
    %\\

    %/mnt/nas/Projects/Cor2Vox/bbdm_c2v/Results_whole_TestSet/Test_samples_sdfpialwhite_Ts
    % BBDM$_{\mathcal{S}_{p/w}}$\cite{li_bbdm_2023}\tnote{a,b}
    % & 
    % & 
    % & 
    % & 
    % & 
    % & 
    % \\

    % % /mnt/nas/Projects/Cor2Vox/bbdm_c2v/Results_whole_TestSet/Test_samples_cross_Ts/mesh_eval/
    % BBDM-3D & Cross
    % BBDM$_{\mathcal{S}_c}$  %\cite{li_bbdm_2023}\tnote{*}
    % & 0.900{\tiny\textpm0.021}
    % & 20.08{\tiny\textpm2.62}
    % & 0.383{\tiny\textpm0.058}
    % & 0.246{\tiny\textpm0.024}
    % & 0.390{\tiny\textpm0.062}
    % & 0.246{\tiny\textpm0.022}
    % \\

    % %/mnt/nas/Projects/Cor2Vox/bbdm_c2v/Results_whole_TestSet/Test_samples_df_Ts/
    % BBDM-3D & DF
    % & 0.900{\tiny\textpm0.021}
    % & 20.04{\tiny\textpm2.65}
    % & 0.327{\tiny\textpm0.035}
    % & 0.249{\tiny\textpm0.019}
    % & 0.335{\tiny\textpm0.035}
    % & 0.248{\tiny\textpm0.019}
    % \\
    
    % Val results
    % & 0.884{\tiny\textpm0.018}
    % & 20.75{\tiny\textpm2.85}
    % & 0.527{\tiny\textpm0.097}
    % & 0.522{\tiny\textpm0.069}
    % & 0.530{\tiny\textpm0.098} 
    % & 0.527{\tiny\textpm0.069}
    %\\
    \midrule

    % /mnt/data/Cor2Vox/C2V-DDPM/model-2/
    Cor2Vox{\scriptsize/DDPM}
    & 0.902{\tiny\textpm0.015}
    & 23.13{\tiny\textpm2.71}
    & 0.335{\tiny\textpm0.038}
    & 0.300{\tiny\textpm0.033}
    & 0.348{\tiny\textpm0.048}
    & 0.305{\tiny\textpm0.037}
    \\
    
    %/mnt/nas/Projects/Cor2Vox/bbdm_c2v/Results_whole_TestSet/Test_samples_cross+pwmc_New_Ts
    Cor2Vox
    & \textbf{0.906{\tiny\textpm0.018}}
    & 21.10{\tiny\textpm2.74}
    & \textbf{0.283{\tiny\textpm0.029}}
    & \textbf{0.251{\tiny\textpm0.019}}
    & \textbf{0.289{\tiny\textpm0.031}}
    & \textbf{0.251{\tiny\textpm0.017}}
    \\

    \bottomrule
    
    \end{tabular}

    \begin{tablenotes} \scriptsize
            \item[*] Method adapted for 3D image generation.
        \end{tablenotes}
    \end{threeparttable}
    \label{tab:qual-acc}
\end{table}

\subsection{Image Quality and Accuracy}
We visualize the generated brain MRIs from Cor2Vox and implemented baseline methods in \Cref{fig:vv-error}. Additionally, we show local surface-based errors on the FsAverage template that serves as input to V2C-Flow. We report quantitative scores for all methods in \Cref{tab:qual-acc}. Note that the reconstruction errors shown in \Cref{fig:vv-error} are not equivalent to the ASSD; the ASSD is bi-directional and independent of vertices, whereas the surface plots show the average distance of vertices in the reconstructed surfaces to the original cortical boundaries. 

From the qualitative inspection of the generated images, we observe that all methods are capable of generating the same cortical anatomy based on the provided shape condition.
However, the quantitative evaluation in~\Cref{tab:qual-acc} reveals a clear improvement of Cor2Vox over other methods in geometric accuracy, confirmed by surface-based plots in \Cref{fig:vv-error}. 
%with $p$-values $\textless 10^{-7}$ by paired Wilcoxon signed-rank tests. 
%This is also apparent from the surface-based plots. 
White matter surfaces are, on average, by approximately 0.04\,mm ($\sim$10\%) more accurate than those from the 3D BBDM. For the pial surfaces, the gain is similar with a reduced error of around 0.03\,mm. At the same time, Cor2Vox does not sacrifice image quality and achieves the highest SSIM score, albeit only by a slight margin. The 3D Pix2Pix excels in terms of PSNR, however, with an ASSD of more than 0.5\,mm on all surfaces, Pix2Pix is not competitive regarding reconstruction accuracy. Med-DDPM and the Cox2Vox{\scriptsize /DDPM} variant also achieve low errors on both white and pial surfaces but cannot keep up with the best Brownian bridge-based methods. Generally, the surface errors are slightly larger on the white matter than on the pial surfaces, especially in the precentral gyrus. This could be explained by the challenge of precisely synthesizing the subtle contrast between white and gray matter, compared to the more pronounced intensity difference between gray matter and cerebrospinal fluid. Yet, Cor2Vox is the only method achieving an ASSD below 0.3\,mm for the white matter surface.
Finally, paired Wilcoxon signed-rank tests between Cor2Vox and baseline methods indicated highly significant improvements with $p<10^{-7}$ for the geometric accuracy on both surfaces and hemispheres.  


\subsection{Ablation Study}

We evaluate the impact of various shape conditions, whether as the source domain for the generative process or as additional conditioning inputs in Cor2Vox, on the geometric accuracy. As reported in~\Cref{tab:ablation}, we initially tested individual shape conditions --- pial surface SDF $\mathcal{S}_p$, white matter surface SDF $\mathcal{S}_w$, edge map $\mathcal{E}$, cortical ribbon mask $\mathcal{R}$, cortex SDF $\mathcal{S}_c$ --- as single-source domains in the Brownian bridge diffusion process. The pial surface achieves the best accuracy when conditioned solely on $\mathcal{S}_p$, and similarly for the white matter surface with $\mathcal{S}_w$. The cortical ribbon mask ($\mathcal{R}$) helps with the accuracy of the white matter while falling short on the pial surface. Next, we explored using $\mathcal{R}$ as the source domain and its combination with other conditions. This approach performs better than either using no additional conditions or employing two parallel bridge processes for both $\mathcal{S}_p$ and $\mathcal{S}_w$. Finally, we adopt the cortex SDF $\mathcal{S}_c$ as the source domain and experiment with incorporating different sets of shape conditions as additional inputs during the reverse diffusion process. Results show that including more conditions generally improves performance, with the combination of all four shape conditions yielding the best accuracy on the white matter surface and comparable performance on the pial surface across hemispheres. This validates the effectiveness of Cor2Vox with $\mathcal{S}_c$ as the source domain and the importance of including extra shape information via conditioning.


\begin{table}[t]
    \setlength{\tabcolsep}{7pt}
    \renewcommand\bfdefault{b}% rather than bx
    \centering
    \caption{Ablation study of different source domains for the Brownian bridge process and extra input conditions provided to Cor2Vox. We report the mean\textpm SD on the validation set. The geometric accuracy of the cortex is in mm. 
    % Underlined conditions represent the source domain of the Brownian bridge process and non-underlined conditions were provided as additional input to the denoising model. 
    ($\mathcal{S}_p$: Pial SDF, $\mathcal{S}_w$: White matter SDF, $\mathcal{S}_c$: Cortex SDF, $\mathcal{E}$: Edge map, $\mathcal{R}$: Cortical ribbon mask)}
    \begin{threeparttable}
        
    \begin{tabular}{llcccccc}
    \toprule
    && \multicolumn{4}{c}{Geometric Accuracy --- ASSD$\downarrow$} \\
    \cmidrule(lr){3-6}
    % & \multicolumn{2}{c}{Image Quality} 
    && \multicolumn{2}{c}{Left Hemisphere} & \multicolumn{2}{c}{Right Hemisphere} \\
    % \cmidrule(lr){2-3}
    \cmidrule(lr){3-4}
    \cmidrule(lr){5-6}
    Source & Condition 
    % & Cond. & SSIM$\uparrow$ & PSNR$\uparrow$ 
    & White & Pial & White & Pial \\
    \midrule
    %/mnt/nas/Projects/Cor2Vox/bbd\alpha_c2v/size_128_whole/whole_pial+white_Test_samples_128_e400
    % D-BBDM &
  
    %size_128_whole_pial
    % BBDM &
    $\mathcal{S}_p$ & ---
    & 0.375{\tiny\textpm0.066}
    & \textbf{0.233{\tiny\textpm0.021}}
    & 0.386{\tiny\textpm0.068}
    & \textbf{0.236{\tiny\textpm0.022}}
    \\

    % bbd\alpha_c2v/size_128_whole_white
    % BBDM & 
    $\mathcal{S}_w$ & ---
    % & 0.303{\tiny\textpm0.067}
    & \underline{0.289{\tiny\textpm0.066}}
    & 0.375{\tiny\textpm0.031}
    & 0.303{\tiny\textpm0.067}
    & 0.376{\tiny\textpm0.034}
    \\


    % bbd\alpha_c2v/size_128_whole/whole_df_Test_samples_128
    % BBDM & Cortex DF 
    % & 0.902{\tiny\textpm0.020}
    % & 20.10{\tiny\textpm2.70}
    % & 0.333{\tiny\textpm0.039}
    % & 0.250{\tiny\textpm0.022}
    % & 0.342{\tiny\textpm0.040}
    % & 0.249{\tiny\textpm0.021}
    % \\

    % bbd\alpha_c2v/size_128_whole/whole_meshedges_Test_samples_128
    % BBDM & 
    
    $\mathcal{E}$ & ---
    & 0.351{\tiny\textpm0.037}
    & 0.307{\tiny\textpm0.023}
    & 0.369{\tiny\textpm0.042}
    & 0.307{\tiny\textpm0.024}
    \\

    % BBDM & c-new
    $\mathcal{R}$ & ---
    & 0.336{\tiny\textpm0.034}
    & 0.279{\tiny\textpm0.021}
    & 0.335{\tiny\textpm0.037}
    & 0.280{\tiny\textpm0.022}
    \\

    
    % BBDM & W\&P Intersection
    % & 
    % &
    % & 0.361{\tiny\textpm0.087}
    % & 0.365{\tiny\textpm0.033}
    % & 0.373{\tiny\textpm0.088}
    % & 0.366{\tiny\textpm0.033}
    % \\
    
    % BBDM & W\&P Union
    % & 
    % &
    % & 0.416{\tiny\textpm0.083}
    % & 0.242{\tiny\textpm0.025}
    % & 0.429{\tiny\textpm0.087}
    % & 0.244{\tiny\textpm0.026}
    % \\

    % BBDM & Mask
    % & 
    % &
    % & 0.323{\tiny\textpm0.038}
    % & 0.279{\tiny\textpm0.033}
    % & 0.330{\tiny\textpm0.043}
    % & 0.280{\tiny\textpm0.040}
    % \\

    % BBDM & cross SDF
    $\mathcal{S}_c$ & ---
    & 0.381{\tiny\textpm0.066}
    & 0.245{\tiny\textpm0.026}
    & 0.390{\tiny\textpm0.070}
    & 0.247{\tiny\textpm0.026}
    \\
    \midrule

    $\mathcal{S}_p, \mathcal{S}_w$\tnote{*} & ---
    & 0.394{\tiny\textpm0.074}
    & 0.535{\tiny\textpm0.064}
    & 0.418{\tiny\textpm0.079}
    & 0.553{\tiny\textpm0.071}
    \\

    % /mnt/nas/Projects/Cor2Vox/bbdm_c2v/size_128_whole/Test_samples_cortexmask+pw_New
    $\mathcal{R}$ & $\mathcal{S}_p, \mathcal{S}_w$
    & 0.293{\tiny\textpm0.027}
    & 0.262{\tiny\textpm0.018}
    & 0.305{\tiny\textpm0.031}
    & 0.261{\tiny\textpm0.016}
    \\


    % /mnt/nas/Projects/Cor2Vox/bbdm_c2v/size_128_whole/test_samples_cortexmask+pwm_New/
    $\mathcal{R}$  & $\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}$
    & 0.301{\tiny\textpm0.038}
    & 0.263{\tiny\textpm0.021}
    & 0.305{\tiny\textpm0.040}
    & 0.264{\tiny\textpm0.022}
    \\
    \midrule

    %whole_p+w-softunion10_Test_samples_128
    % BBDM & Softunion 10
    % & 
    % &
    % & 0.316{\tiny\textpm0.076}
    % & 0.261{\tiny\textpm0.025}
    % & 0.331{\tiny\textpm0.082}
    % & 0.262{\tiny\textpm0.027}
    % \\

    % whole_p+w-softunion100_Test_samples_128
    % BBDM & Softunion 100
    % & 
    % &
    % & 0.332{\tiny\textpm0.065}
    % & 0.256{\tiny\textpm0.024}
    % & 0.339{\tiny\textpm0.072}
    % & 0.256{\tiny\textpm0.025}
    % \\

    %whole_p+w-cross_grad_Test_samples_128
    % BBDM & cross grad
    % & 
    % &
    % & 0.383{\tiny\textpm0.068}
    % & 0.248{\tiny\textpm0.025}
    % & 0.395{\tiny\textpm0.074}
    % & 0.249{\tiny\textpm0.024}
    % \\

    % Test_samples_df+mc
    % BBDM & df+mc
    % & 
    % &
    % & 0.282{\tiny\textpm0.036}
    % & 0.240{\tiny\textpm0.024}
    % & 0.292{\tiny\textpm0.042}
    % & 0.242{\tiny\textpm0.027}
    % \\

    % bbdm_c2v/size_128_whole/Test_samples_df+pw
    % BBDM & df+pw
    % & 
    % &
    % & 0.310{\tiny\textpm0.058}
    % & 0.231{\tiny\textpm0.015}
    % & 0.326{\tiny\textpm0.063}
    % & 0.234{\tiny\textpm0.015}
    % \\

    % bbdm_c2v/size_128_whole/Test_sample\mathcal{S}_cross+mc
    % BBDM & cross+mc
    % & 
    % &
    % & 0.281{\tiny\textpm0.034}
    % & 0.238{\tiny\textpm0.021}
    % & 0.293{\tiny\textpm0.040}
    % & 0.241{\tiny\textpm0.022}
    % \\

    % %bbdm_c2v/size_128_whole/Test_samples_df+pwmc
    % % BBDM & df+pwmc
    % % & 
    % % &
    % % & 0.271{\tiny\textpm0.030}
    % % & 0.237{\tiny\textpm0.021}
    % % & 0.287{\tiny\textpm0.038}
    % % & 0.235{\tiny\textpm0.022}
    % % \\

    % % bbdm_c2v/size_128_whole/Test_sample\mathcal{S}_cross+pwmc
    % BBDM & cross+pwmc
    % & 
    % &
    % & 0.261{\tiny\textpm0.030}
    % & 0.229{\tiny\textpm0.023}
    % & 0.269{\tiny\textpm0.037}
    % & 0.229{\tiny\textpm0.0254}
    % \\

 
    % bbdm_c2v/size_128_whole/Test_sample\mathcal{S}_cross+c
    % BBDM & cross+c
    % $\underline{\mathcal{S}_c}, \mathcal{R}$
    % & 0.287{\tiny\textpm0.033}
    % & 0.244{\tiny\textpm0.025}
    % & 0.294{\tiny\textpm0.037}
    % & 0.242{\tiny\textpm0.028}
    % \\

    % bbdm_c2v/size_128_whole/Test_samples_df+c
    % BBDM & df+c
    % & 
    % &
    % & 0.305{\tiny\textpm0.037}
    % & 0.268{\tiny\textpm0.034}
    % & 0.316{\tiny\textpm0.040}
    % & 0.267{\tiny\textpm0.038}
    % \\

    %bbdm_c2v/size_128_whole/Test_sample\mathcal{S}_cross+m
    % BBDM & cross+m
    $\mathcal{S}_c$  & $\mathcal{E}$
    & 0.326{\tiny\textpm0.036}
    & 0.241{\tiny\textpm0.021}
    & 0.339{\tiny\textpm0.039}
    & 0.243{\tiny\textpm0.022}
    \\

    
    % /mnt/nas/Projects/Cor2Vox/bbdm_c2v/size_128_whole/Test_samples_cross+c_New
    % BBDM & cross+c-new
    $\mathcal{S}_c$ & $\mathcal{R}$
    & 0.305{\tiny\textpm0.030}
    & 0.261{\tiny\textpm0.021}
    & 0.312{\tiny\textpm0.033}
    & 0.262{\tiny\textpm0.020}
    \\

    %bbdm_c2v/size_128_whole/Test_samples_df+m
    % BBDM & df+m
    % &
    % &
    % & 0.338{\tiny\textpm0.034}
    % & 0.305{\tiny\textpm0.022}
    % & 0.347{\tiny\textpm0.037}
    % & 0.310{\tiny\textpm0.023}
    % \\


        
     % bbdm_c2v/size_128_whole/Test_sample\mathcal{S}_cross+pw
    % BBDM & cross+pw
    $\mathcal{S}_c$ & $\mathcal{S}_p, \mathcal{S}_w$
    & 0.310{\tiny\textpm0.060}
    & \underline{0.236{\tiny\textpm0.020}}
    & 0.321{\tiny\textpm0.064}
    & 0.239{\tiny\textpm0.022}
    \\

 

    %/mnt/nas/Projects/Cor2Vox/bbdm_c2v/size_128_whole/Test_samples_cross+mc_New/
    % BBDM & cross+mc-new    
    $\mathcal{S}_c$ & $\mathcal{E}, \mathcal{R}$
    & \underline{0.289{\tiny\textpm0.031}}
    & 0.252{\tiny\textpm0.016}
    & \underline{0.294{\tiny\textpm0.034}}
    & 0.254{\tiny\textpm0.016}
    \\

    

    %/mnt/nas/Projects/Cor2Vox/bbdm_c2v/size_128_whole/Test_samples_cross+pwm
    % BBDM & cross+pwm
    $\mathcal{S}_c$ & $\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}$
    & 0.294{\tiny\textpm0.043}
    & \underline{0.236{\tiny\textpm0.017}}
    & 0.311{\tiny\textpm0.047}
    & \underline{0.238{\tiny\textpm0.018}}
    \\

     % BBDM & cross+pwc-new
    $\mathcal{S}_c$ & $\mathcal{S}_p, \mathcal{S}_w, \mathcal{R}$
    & 0.305{\tiny\textpm0.036}
    & 0.255{\tiny\textpm0.019}
    & 0.308{\tiny\textpm0.037}
    & 0.255{\tiny\textpm0.019}
    \\

    %/mnt/nas/Projects/Cor2Vox/bbdm_c2v/size_128_whole/Test_samples_cross+pwmc_New/mesh_eval
    % BBDM & cross+pwmc-new
    $\mathcal{S}_c$ & $\mathcal{S}_p, \mathcal{S}_w, \mathcal{E}, \mathcal{R}$
    & \textbf{0.282{\tiny\textpm0.032}}
    & 0.250{\tiny\textpm0.017}
    & \textbf{0.287{\tiny\textpm0.035}}
    & 0.250{\tiny\textpm0.017}
    \\

    
    % &
    % &
    % & 
    % & 
    % & 
    % & 
    % \\


    \bottomrule
    
    \end{tabular}
    \begin{tablenotes}\scriptsize
    \item[*] This model requires two parallel Brownian bridge processes.    
    \end{tablenotes}
    \end{threeparttable}
    
    \label{tab:ablation}
\end{table}


\subsection{Image Variability} \label{sec:variability}

% The goal of Cor2Vox is 
While we aim to generate MRI scans that conform to the shape of given cortical surfaces, it is desirable for applications like dataset augmentation to have high diversity in the remaining image regions. We generated five synthetic MRIs per subject in the test set using different random seeds and calculated the average variance in individual voxels across the synthetic MRIs. Additionally, we computed the mean absolute difference between these synthetic scans and the corresponding real MRIs. As illustrated in \Cref{fig:varibaility}, the largest variation can be expected in the skull regions, whereas the cortical regions exhibit minimal differences to the original data and also comparably low variance. This indicates that Cor2Vox effectively generates realistic MRI scans that accurately align with the cortical surfaces while 
% maintaining variability in other areas, 
promoting diversity in the remaining image parts. % of the images.
%and privacy preservation in medical imaging.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{sample_var+diff_Ts.pdf}
    \caption{We show the mean absolute difference between synthetic and original MRIs, and voxel-wise variance across synthetic MRIs across five random seeds among the whole test set, averaged along three anatomical axes, with 
    darker colors indicating higher variability.
    % deviation from the reference image and, therefore, greater variability of the respective structures.
    }
    \label{fig:varibaility}
\end{figure}

\subsection{Synthesizing Cortical Atrophy}
The deliberate combination of white matter and pial surfaces in Cor2Vox enables the creation of synthetic datasets with simulated cortical thickness changes. Inspired by previous work on cortical atrophy simulation~\cite{Rusak2022benchmark}, we mimicked the process of global cortical thinning in the range of 0.1--0.6 mm by deforming pial surfaces from cognitively normal subjects in our test set ($n=124$) inside towards the white matter boundary. After generating matching MRIs from the altered surfaces (Cor2Vox) and reconstructing respective cortical surfaces (V2C-Flow), we compared the recovered change in cortical thickness (measured bi-directionally~\cite{fischl2000}) to the introduced atrophy, see \Cref{fig:atrophy}. Except for a small occipital area, the introduced changes in the subvoxel range were well recovered, supporting the applicability of Cor2Vox for algorithm benchmarking.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{atrophy_vertex_wise_recovered.pdf}
    \caption{Synthetic global atrophy in the range of 0.1--0.6 mm recovered from MRIs generated by Cor2Vox. We show vertex-wise mean values based on 124 cognitively normal test cases. The closer the recovered values to the introduced atrophy, the better.}
    \label{fig:atrophy}
\end{figure}

% \section{Discussion}
% Critical discussion of data generation: risk of re-identification~\cite{Giuffr2023synthetichealthcare}, brain shape is like a fingerprint~\cite{Wachinger2015brainprint}

% While Cor2Vox is designed for the cortex, our theoretical contributions for 3D shape to image translation are generic for obtaining anatomically plausible synthetic images. 


\section{Conclusion}
We introduced Cor2Vox, a novel method for 3D brain MRI generation based on cortical surfaces. For the first time, we leveraged a shape-to-image Brownian bridge diffusion process for synthesizing anatomically plausible 3D medical scans. We extended the Brownian diffusion process by incorporating multiple complementary shape conditions to enhance anatomical alignment.
By conditioning on realistic 3D shapes, 
Cor2Vox effectively addresses the problem of anatomical implausibilities commonly seen in synthetic brain MRIs. Our results demonstrated state-of-the-art image quality and significant improvements in geometric accuracy compared to existing methods. Moreover, we conducted a comprehensive ablation study and showcased the application of Cor2Vox for simulating individual cortical atrophy. Our findings highlight the critical role of realistic anatomical modeling in synthetic medical image generation and pave the way for advanced data augmentation and algorithm benchmarking. 

% was We demonstrated a considerable improvement compared to  


\begin{credits}
\subsubsection{\ackname} This research was supported by the German Research Foundation (DFG) and the Munich Center for Machine Learning (MCML). We gratefully acknowledge the computational resources provided by the Leibniz Supercomputing Centre (www.lrz.de).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.

\end{credits}
%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.

\bibliographystyle{splncs04}
\bibliography{bibliography}
%

\end{document}
