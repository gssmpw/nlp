To determine whether macronutrients are predictive of user engagement with Reddit food posts, we conduct a binary classification experiment using an XGBoost classifier. We start by extracting four feature sets including nutritional densities, food descriptors, textual features, and a set of control features.

\subsection{Features}
 
\xhdr{Nutritional density}
Our primary focus is the nutritional content of meals, including calories (kCal per $100$ grams) and protein, carbohydrates, and fat, measured in grams as a fraction of $100$ grams total. 

\xhdr{Food descriptors \& categories} 
We categorize each meal based on food descriptors (e.g., taste, texture, preparation methods) and food categories (e.g., main dish, dessert). 
For each descriptor, we start by manually selecting three common examples. 
We extend the list by including two additional examples generated using ChatGPT, chosen for its ability to suggest diverse yet commonly used terms.
A similar approach is applied to food categories, where we manually identify several common examples and expand the dessert list using suggestions from ChatGPT.
A complete list of food categories is provided in Table \ref{table:food_types}.

Using these descriptors and categories, we determine whether the title of a food post contains specific words by performing string matching.
If a match is found, we mark the corresponding descriptor or category as present.
For food descriptors, we use each individual type of descriptor as a new feature. For example, for preparation methods, we check the presence of each method (e.g., ``grilled'').
On the other hand, for food categories, we use the main categories as features, and their types as a way to identify matches. In particular, if ``soup'' or ``salad'' is present in the title, we mark the post as ``Healthy''. 
We use binary features to represent this information, as our primary interest is identifying whether a meal belongs to a specific category.
This approach also works with posts that belong to multiple categories (e.g., a chicken salad can be categorized as both a main dish and healthy).

\begin{table}[b]
\centering
\caption{\textit{Food descriptors \& categories.} Keywords used to classify meals by identifying specific terms in post titles.}
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{ll}
\toprule
%\hline
Food descriptors    & Definition                                                       \\ \hline
Preparation method  & grilled, fried, baked, boiled, steamed                           \\
Taste descriptors   & savory, sweet, spicy, rich, salty                                \\
Texture descriptors & creamy, crispy, tender, juicy, crunchy                           \\ \hline \hline
Food categories     & Definition                                                       \\ \hline
Main dish           & pasta, casserole, roast, chicken, stirfry                        \\
Dessert             & cake, custard, pudding, cookie, pancake, waffle, muffin, biscuit \\
Fast food           & pizza, burger, burrito                                           \\
Healthy             & soup, salad                                                      \\
Plant based         & vegan, vegetarian, veggie                                        \\ 
Pastry              & bread, croissant                                                 \\
\bottomrule
\end{tabular}%
%}
\label{table:food_types}
\end{table}

\begin{figure*}[t]
  \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/rq1_wordcloud.pdf}
        \caption{Words in posts with (red) vs. without engagement (blue)}
        \label{fig:wc1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/high_low_engagement_wordcloud.pdf}
        \caption{Words in resonant (red) vs. non-resonant posts (blue)}
        \label{fig:wc2}
        \end{subfigure}
    \caption{\textit{Engagement discriminators from post titles.} We present discriminative words used significantly differently in engaging and non-engaging posts as word clouds. The red color indicates words more frequently used in posts with (\subref{fig:wc1}) comments, or in resonant (\subref{fig:wc2}) posts. Blue color represents discriminative words more frequently used in posts without (\subref{fig:wc1}) comments, or in non-resonant (\subref{fig:wc2}) posts. The size of each word reflects its frequency within the respective group.}
    \label{fig:worclouds_both}
    \Description{wc rq2}
\end{figure*}



\xhdr{Engagement discriminators}
We identify words that frequently appear in the titles of posts with different engagement levels.
Using a method based on the chi-squared test and contingency tables, we identify words that differ significantly in their usage, hence discriminating strongly between engaging and non-engaging posts. In particular, 
we lemmatize the titles, remove stop words, and split the data into two groups (engaging vs. non-engaging posts).
Next, we identify the $100$ most commonly used words in each group and calculate chi-square ($\chi^2$) values from contingency tables to assess the statistical significance. 
Using this method, we identify $78$ words used with significant variation in engagement and non-engagement posts and $91$ such words in resonant and non-resonant posts.
To ensure the relevance of these discriminative words, we sort identified discriminators by their occurrence frequency and select words that occur in at least $1$\% of posts, which results in five and four most frequent engagement discriminators for engaging vs. non-engaging posts and resonant vs. non-resonant posts, respectively.
Finally, we create two new binary features indicating whether a post title contains a discriminator ($1$ for presence, $0$ for absence) specific to engagement or non-engagement posts.

In Figure \ref{fig:worclouds_both} we depict the engagement discriminators as the word clouds categorized by engagement levels.
For example, in Figure \ref{fig:wc1} we observe that post titles with comments frequently contain words such as ``cheese'', ``chocolate'', ``pizza'', or ``fried''.
On the contrary, post titles without comments often include ``potato'', ``rice'', or ``pork''.
Similarly, in Figure \ref{fig:wc2} we show that resonant posts have titles with ``cheese'', ``pizza'', or ``fried'', while non-resonant posts feature words such as ``pork'' or ``sauce.''
The prominence of words like ``cheese'' and ``pizza'' in posts with high engagement levels suggests that indulgent or popular foods may attract more attention.
Furthermore, words related to preparation methods, such as ``fried'' and ``baked'' seem to play a critical role in capturing user interest in resonant posts.

\xhdr{Control features} 
We define control features as factors that typically influence user engagement in social media but are unrelated to food. In particular, we compute the following features: (i) weekend or weekday indicator, (ii) pre-, during, or post-peak of the COVID-19 pandemic indicator, (iii) user experience indicator (top $5$\% of most active users vs. the remaining users), and (iv) indicator for the first, second, third, or fourth quartile of the day. In addition, we use the post tag as another control variable. We show all possible tags in Table \ref{table:reddit_tags}. 

\begin{table}[b]
\centering
\caption{\textit{r/Food subreddit tags.} Each post should include one of the tags requested by the subreddit moderators.}
%\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
Tag                       & Definition  \\ \midrule
I ate                   & Food you purchased and ate, with no-preparation of your own         \\
Homemade                & Food you, friend, family member etc, made at home         \\
Pro/Chef                 & You work in a food-related industry and made it        \\ \bottomrule
\end{tabular}%
%}
\label{table:reddit_tags}
\end{table}

With control features, we account for various factors that could affect engagement and aim to isolate the effects of nutritional content on user engagement. For example, 
user activity levels vary between weekends and weekdays, influencing post frequency and audience size.
The COVID-19 pandemic altered user behavior on social media, making the timing relative to the pandemic an important factor. 
Posts by experienced users typically receive stronger engagement due to users' familiarity with popular post attributes or their reputation in the subreddit. 
Finally, dividing the day into quartiles (six hours each) ensures balanced analysis across different times of the day.

\subsection{Experimental setup}
\label{sec:prediction}

We conduct two classification experiments: (i) posts with comments vs. posts without comments, and (ii) resonant vs. non-resonant posts. Using our four feature sets we repeat predictions for all combinations of feature sets always including control features. This results in eight different combinations per classification experiment. We divide our datasets in train ($80\%$) and test ($20\%$).
As the evaluation metric, we use ROC-AUC score.

Using our training datasets with control features only, and stratified 5-fold cross-validation we first optimize the hyperparameters for both XGBoost classifiers. To that end, we combine randomized and grid search over these parameter values: $10$, $50$, $100$, $500$, $1,000$ and $50,000$ estimators; maximum tree depth of $1$, $2$, $3$, $4$, $10$, $15$; and learning rate of $0.01$, $0.1$, $0.2$, $0.3$, or $0.4$.
Randomized search allows us to estimate potential parameters quickly by randomly sampling combinations, facilitating a faster initial exploration. After identifying promising parameters, we define a range around these values for each hyperparameter and optimize them through grid search.
Ultimately, our final model for engagement prediction is configured with $26$ estimators, a maximum tree depth of $4$, and a learning rate of $0.3$, and the final model for resonance prediction is configured with $36$ estimators, a maximum tree depth of $2$, and a learning rate of $0.4$.
Using the optimized parameters we then train the final XGBoost models on our training datasets for all different feature set combinations and evaluate these models on the corresponding test datasets with ROC-AUC score. To estimate uncertainty in the test performance we create $1,000$ bootstrap samples from the test datasets. 
Using those bootstrap samples, we calculate $95\%$ confidence intervals for the ROC-AUC score.

Finally, we estimate feature importance using SHAP values. SHAP (SHapley Additive exPlanations) \cite{lundberg_local_2020} values explain individual predictions of machine learning models by revealing each feature's contribution to the final prediction. By aggregating local explanations of each prediction, SHAP values offer an understanding of the global structure of the model, helping us understand each feature's overall impact on the predictions.








