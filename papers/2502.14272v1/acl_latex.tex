\pdfoutput=1

\documentclass[11pt]{article}
\newcommand{\sr}[1]{{\color{magenta} [sirui: #1]}}
\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[thicklines]{cancel}

\title{Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models}








\author{
Yanggan Gu\textsuperscript{1,2}\thanks{This work was completed during the research assistant internship at the Hong Kong University of Science and Technology (Guangzhou).}\quad Junzhuo Li\textsuperscript{1}\quad
Sirui Huang\textsuperscript{1,3}\quad
Xin Zou\textsuperscript{1}\\
\bf Zhenghua Li\textsuperscript{2}$^\dagger$ \quad
\bf Xuming Hu\textsuperscript{1,4}\thanks{Corresponding authors.}\\
\textsuperscript{1}The Hong Kong University of Science and Technology (Guangzhou)\\
\textsuperscript{2}Soochow University\quad
\textsuperscript{3}University of Technology Sydney\\
\textsuperscript{4}The Hong Kong University of Science and Technology \\
\texttt{yanggangu@outlook.com},\quad
\texttt{zhli13@suda.edu.cn},\quad
\texttt{xuminghu@hkust-gz.edu.cn}
}





\newcommand{\mymod}[1]{\textcolor{black}{#1}}
\newcommand{\myadd}[1]{\textcolor{black}{#1}}

\begin{document}
\maketitle

\begin{abstract}
Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs).
However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses.
This limitation \mymod{hinders} student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all \mymod{potential} preferences, thereby providing more nuanced supervisory signals. 
Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences.
Based on this, PAD comprises three key steps: 
(1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference;
and (3) training the student's intrinsic preference distribution to align with the teacher's.
Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches,
achieving over 20\% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences.
Notably, on MT-Bench, using the \textsc{Gemma} model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.

\end{abstract}


\input{sections/intro}
\input{sections/method}
\input{sections/exp}
\input{sections/related_work}
\input{sections/conclusion_limitation}






























\bibliography{custom}

\appendix

\newpage
\input{sections/append}


\end{document}
