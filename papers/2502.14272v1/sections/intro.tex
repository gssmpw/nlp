
\section{Introduction}

Recently, small language models (SLMs) have demonstrated impressive performance across \mymod{a variety of tasks \citep{dubey2024llama3herdmodels, gemmateam2024improvingopen, jiang-2023-mistral7b}. Compared to large language models (LLMs) such as GPT4 \citep{openai-2024-gpt4technicalreport}, SLMs, with their fewer parameters, offer greater efficiency for deployment in diverse applications. 
However, their smaller parameter number limits their capacity to capture the nuances of human preferences, posing challenges in generating responses that align with human values}, such as providing harmless replies to extreme or sensitive questions \citep{tunstall-2024-zephyr}.

\begin{figure}[!t]
    \centering
    \includegraphics[scale=0.335]{figures/comparison_v4.pdf}
    \vspace{-7mm}
    \caption{Comparison of the Teacher-as-Annotator methods and our PAD, where ``A $\succ$ B'' means the LLM prefers response A over B. }
    \label{fig:cmp}
    \vspace{-5mm}
\end{figure}

Unlike SLMs, LLMs exhibit superior alignment with human preferences \citep{openai-2024-gpt4technicalreport, geminiteam2024gemini15unlockingmultimodal}. Consequently, existing works leverage LLMs as teachers to distill preference knowledge into student SLMs \citep{bai-2022-constitutional, cui2023ultrafeedback, tunstall-2024-zephyr, wang-2024-rlvlmf, yuan-2024-selfreward}. 
\mymod{All these works typically encode preference knowledge in teacher LLMs by comparing pairwise responses.}
For example, \citet{bai-2022-constitutional} uses teacher-annotated responses to train a reward model, which guides the student via reinforcement learning. Similarly, \citet{tunstall-2024-zephyr} employs a teacher model for preference annotation but \mymod{instead applies Direct Preference Optimization \citep{rafailov-2023-direct} to optimize the student model. }
 
However, the supervision signals provided by these ``Teacher-as-Annotator'' methods consider only \mymod{the ordering between responses, disregarding the extent to which one response is preferred over another.}
As illustrated in Figure~\ref{fig:cmp}, in Scenario 1, response A is only slightly better than B by providing more informative details; whereas in Scenario 2, response B contains harmful content (in red), making the difference with A more significant. 
Nonetheless, \mymod{both scenarios are uniformly represented as $A \succ B$.}
This simplified treatment overlooks the differences between preference pairs, thereby negatively impacting their generations after preference learning \citep{amini-etal-2024-direct}. 

To address the limitation, we propose a \textbf{P}reference-\textbf{A}ligned \textbf{D}istillation (PAD) framework, in which preference knowledge is encoded as a probability distribution over all potential preferences, providing subtle supervisory signals from the teacher model. 
\mymod{Our insight behind PAD derives from the demonstration that the average log-likelihood of language models can act as reward functions, reflecting their intrinsic preference}.
Based on this insight, our PAD consists of three steps: 
1) Sampling a diverse list of responses from the student model using high temperature; 
2) Calculating rewards for each response using both the teacher and student models, \mymod{and calibrating these rewards using selection probabilities from multiple-choice questions prompting};
3) Enumerating all potential preferences and computing the overall distribution based on the rewards, allowing the student to learn and mimic the teacher's preference distribution. 
As illustrated in Figure~\ref{fig:cmp}, PAD provides more precise signals, distinguishing the subtle difference in Scenario 1 and significantly differentiating safe and harmful responses in Scenario 2. 
To enhance PAD's efficiency, we introduce a Preference Decomposing Strategy, which breaks distillation into multiple rounds to accelerate the training process. Comprehensive experiments across four benchmarks, including Alpaca Eval 2, Arena-Hard, MT-Bench, and GSM8K, with the \textsc{Gemma-2} and \textsc{LLaMA-3} families, demonstrate that PAD consistently outperforms existing approaches, effectively aligning SLMs with human preferences. 

Code is available\footnote{\url{https://github.com/EganGu/PAD}.}, and our main contributions can be summarized as follows:
\begin{itemize}

\item We propose a Preference-Aligned Distillation (PAD) framework, which moves beyond pairwise preference by modeling the full preference distribution, enabling the student to capture the teacher’s nuanced preferences.

\item \mymod{We are the first to demonstrate that the average log-likelihood of language models can directly serve as reward functions, capturing the model’s intrinsic preference.}

\item Experimental results across four benchmarks show that our PAD outperforms existing approaches, suggesting that PAD more precisely captures human preferences.

\end{itemize}
