\section{Related Work}

\paragraph{Traditional Knowledge Distillation}
Knowledge distillation (KD), introduced by \citet{hinton2015distill}, primarily aims at model compression by training a smaller student model to mimic the output behavior of a larger teacher model \citep{kim-rush-2016-sequence, liang-2021-mixkd, zhang-2023-blindly, gu-2024-minillm, agarwal2024onpolicy}.
\citet{kim-rush-2016-sequence} extended KD to machine translation by training students on sequences generated by teachers in order to imitate teacher behavior. More recently, \citet{gu-2024-minillm} advanced KD using reverse KL divergence on student -generated sequence to mitigate exposure bias, improving student model performance. A key feature of these methods is that distillation is performed over the shared vocabulary of both teacher and student models. Our PAD eliminates this limitation, enabling effective distillation with different vocabularies.

\paragraph{Preference Knowledge Distillation}
Motivated by the observation that large models have achieved a high degree of alignment with human values and preferences, many efforts focus on distilling preference knowledge from large models to smaller ones \citep{bai-2022-constitutional, cui2023ultrafeedback, lee-2024-rlaif, yuan-2024-selfreward, tunstall-2024-zephyr, yang2024rlcd}. \citet{bai-2022-constitutional} first introduced this concept, also known as Reinforcement Learning from AI Feedback (RLAIF), where teacher models annotate response pairs from the student to create a preference dataset for training a reward model. \citet{tunstall-2024-zephyr} further utilized teacher-annotated preferences with Direct Preference Optimization (DPO) \citep{rafailov-2023-direct}, streamlining the training of student models.  These approaches follow the "Teacher-as-Annotator" paradigm. The annotated preference datasets generated through this paradigm can be directly employed with methods such as DPO, SimPO \citep{meng-2024-simpo}, and PRO \citep{song-2024-pro}, enabling preference optimization of student models. However, a significant limitation of these methods lies in their reliance on unique ranking, which constrains their ability to model nuanced preferences. In contrast, our PAD treats modeling preference knowledge as a distribution over all possible preferences, enabling nuanced alignment for the student and teacher models.
