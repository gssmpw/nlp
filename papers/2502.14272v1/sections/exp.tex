\input{tables/main_result}

\section{Experiment}

\subsection{Setup}

\paragraph{Models}
We evaluate two model families in our main experiments: 1) \textsc{Gemma-2} Models\footnote{\url{https://ai.google.dev/gemma}} \citep{gemmateam2024improvingopen} include \textsc{Gemma-2-9B-It} as teacher and \textsc{Gemma-2-2B-It} as the student, and 2) \textsc{LLaMA-3} Models\footnote{\url{https://ai.meta.com/blog/meta-llama-3/}} \citep{dubey2024llama3herdmodels} includes \textsc{LLaMA-3.1-8B-Instruct} as teacher and \textsc{LLaMA-3.2-3B-Instruct} as student.

\paragraph{Training}
\mymod{Training data is sourced from \textsc{UltraFeedback}\footnote{\url{https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned}} \citep{cui2023ultrafeedback}, comprising around 60k preference samples across a diverse range of tasks, including mathematical reasoning and open-ended writing. We filter out samples that exceed the models' context length, set the number of sampled responses $n = 4$, and apply a reward calibration ratio $\alpha = 0.8$ to mitigate teacher model bias. Training is conducted for one epoch by default, with further details provided in Appendix \ref{app:train}.}

\paragraph{Evaluation}
We assess our models on four benchmarks: AlpacaEval 2.0 \citep{alpaca-eval}, MT-Bench \citep{zheng-2023-judging}, Arena-Hard \citep{arenahard-2024}, and GSM8K \citep{cobbe-2021-math}, which evaluate a model's conversational versatility across a range of tasks. For AlpacaEval, we report both the raw win rate (WR) and the length-controlled win rate (LC), the latter being robust to verbosity. For Arena-Hard, we report the win rate (WR)\footnote{Please note that for AlpacaEval 2.0 and Arena-Hard, we employ \textsc{LLaMA-3.1-70B-Instruct} as the judge model, which achieved capabilities comparable to GPT-4 Turbo on the judge test of AlpacaEval while being more cost-effective and faster.}. For MT-Bench, we present the average MT-Bench score, evaluated by GPT-4 Turbo. Detailed evaluation settings can be found in Appendix \ref{app:eval}.

\paragraph{Baselines}
We compare PAD with two types of baselines: 
1) Traditional Knowledge Distillation, which aims to learn the teacher's distribution at the logits level, including \textbf{Standard KD} \citep{hinton2015distill}, \textbf{SeqKD}
 \citep{kim-rush-2016-sequence}, and \textbf{MiniLLM} \citep{gu-2024-minillm}; 
2) Preference Knowledge Distillation, which aims to transfer the teacher's preference knowledge to the student model. Under the ``Teacher-as-Annotator'' paradigm, we choose \textbf{DPO} \citep{tunstall-2024-zephyr}, \textbf{SimPO} \citep{meng-2024-simpo}, and \textbf{PRO} \citep{song-2024-pro} as baselines. A detailed description of these baselines can be found in Appendix \ref{app:baseline}.

\subsection{Main Result}
\label{sec:main_result}
\mymod{Table \ref{tab:main} summarizes the experimental results across several benchmarks.}The primary finding is that the student models trained with PAD consistently outperform both their initial counterparts and existing approaches. Specifically, PAD achieves over a 20\% improvement on Alpaca-Eval 2.0 and Arena-Hard, highlighting its ability to more effectively capture the teacher's preferences, thereby aligning more closely with human values.

Preference distillation methods offer significant advantages over traditional KD methods. Traditional KD approaches, such as Standard KD and SeqKD, yield modest improvements (1-3\%) in Alpaca-Eval 2.0 LC scores. In contrast, preference distillation methods like DPO and SimPO show more substantial gains, particularly in aligning with human preferences, as evidenced in Alpaca-Eval 2.0 and Arena-Hard benchmarks. These findings align with the work of \citet{tunstall-2024-zephyr}.

\mymod{Notably, PAD with $\mathcal{L}_\text{PPD}$ outperforms PAD with $\mathcal{L}_\text{VPD}$ across multiple benchmarks, including Alpaca-Eval 2.0 LC (49.62 vs. 46.13) and MT-Bench (7.02 vs. 6.93). For the \textsc{Gemma-2} model family, the student trained with $\mathcal{L}_\text{PPD}$ even surpasses its teacher in MT-Bench, scoring 7.02 compared to the teacher’s 6.99. The key advantage of PPD over VPD lies in its preference modeling approach: by capturing the full preference distribution rather than just a simple ranking, PPD provides more nuanced supervisory signals, thereby better reflecting subtle human preferences—an aspect often overlooked by existing distillation methods.}




\subsection{Analysis}

We analyze the impact of our proposed Preference Decomposing Strategy and Reward Calibration. To assess the generalization capability of PAD, we further investigate its performance when the teacher and student models belong to different families. A more detailed analysis is provided in Appendix \ref{app:additional}.


\paragraph{Effect of Preference Decomposing Strategy}

\input{tables/iteration}


\begin{figure}[!tbp]
    \vspace{-2mm}
    \includegraphics[scale=0.43]{figures/lc_iteration_flipped.pdf}
    \vspace{-4mm}
    \caption{\mymod{Alpaca-Eval LC with different iterations.}}
    \label{fig:lc_iter}
    \vspace{-2mm}
\end{figure}

\mymod{We investigated the impact of the iterative distillation process on performance and training time using the preference decomposing strategy. Table \ref{tab:iter} presents the effects of varying iteration counts and sample sizes. For a sample size of 4, decomposing the sampling process into two iterative steps does not reduce training time, as the low complexity of modeling the distribution with fewer samples renders the effect negligible. However, when the sample size increases to 12, adopting a three-iteration decomposition reduces training time by 20\%, demonstrating that the preference decomposing strategy becomes more advantageous as the sample size grows, thereby accelerating training. Notably, decomposing the sampling into multiple iterations does not result in a significant performance drop, indicating that the strategy effectively balances efficiency with stable performance.}

\mymod{Additionally, we examined the iterative distillation process as a continuous learning method. As shown in Figure \ref{fig:lc_iter}, we compared three high-performing baseline methods—DPO, SimPO, and PRO. The results indicate that the iterative distillation process consistently improves performance across all methods, with PAD achieving the best results, highlighting its superior effectiveness.}


\input{tables/ablation}


\paragraph{Influence of Reward Calibration}
\myadd{We conduct ablation experiments on \textsc{Gemma-2-2B-It} with Reward Calibration, as summarized in Table \ref{tab:reward_ablation}. The results show a significant performance improvement, with an 8.03 LC gain on PAD with $\mathcal{L}_\text{PPD}$.}
\mymod{To further assess the impact of reward calibration, we vary the calibration ratio $\alpha$ and examine its effect on performance, as depicted in Figure \ref{fig:ablation}. Increasing $\alpha$ leads to consistent performance gains, peaking at $\alpha = 0.8$. Beyond this point, further increases in $\alpha$ result in diminishing returns. For applications with lower performance demands, setting $\alpha = 1$ offers a practical trade-off by avoiding the computational cost of calculating the teacher's log-likelihood.}

\begin{figure}[!tbp]
    \centering
    \vspace{-1mm}
    \includegraphics[scale=0.5]{figures/alpha_v2.pdf}
    \vspace{-4mm}
    \caption{\mymod{Alpaca-Eval LC Win Rate with different $\alpha$}}
    \vspace{-1mm}
    \label{fig:ablation}
\end{figure}

\paragraph{Heterogeneous Study} 

Previous experiments typically involve teacher and student models from the same family, sharing a common vocabulary and similar architectures. To evaluate the generalization ability of our PAD method, we extend these experiments by using teacher and student models from different families. As shown in Table \ref{tab:heterogeneous}, our approach consistently outperforms existing methods, demonstrating robust generalization. 
\myadd{Furthermore, a comparison with results in Table \ref{tab:main} reveals that using a stronger teacher model, \textsc{Gemma-9B}, significantly improves the performance of the student model (\textsc{LLaMA-3B}). Specifically, with \textsc{Gemma-9B} as the teacher, the student achieves an LC of 51.96\%, compared to only 33.61\% with \textsc{LLaMA-8B}. This suggests a positive correlation between the teacher's performance and the student's post-distillation performance, aligning with findings from \citet{gu-2024-minillm, lee-2024-rlaif}.}


\input{tables/heterogeneous}

