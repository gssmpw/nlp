\section{Conclusion}

In this paper, we introduced the Preference-Aligned Distillation (PAD) framework, which models the teacher's preference knowledge as a probability distribution over all potential preferences. This supervisory signal enables the student model to capture subtle distinctions between responses. \mymod{Experimental results on the \textsc{Gemma-2} and \textsc{LLaMA-3} model families show that PAD outperforms both traditional knowledge distillation and existing preference distillation methods across four benchmark tasks, highlighting its capacity for learning in-depth human preferences.}

\section*{Limitations}
Our research has several limitations. Firstly, the generalization capability is insufficient as we have not conducted experiments on larger-scale teacher and student models, primarily due to limited computational resources. Secondly, sampling multiple responses consumes more computational overhead. However, because SLMs have relatively smaller parameter sizes, this overhead remains comparatively modest.
Thirdly, our method requires token-level probabilities, which are unavailable in some black-box models. 
