
\section{Background}
This section reviews two topics: 1) Preference modeling in preference learning theory, and 2) The generation process of language models under the reinforcement learning framework. 

\paragraph{Preference Modeling}
Given a prompt $\boldsymbol{x} \in \mathcal{X}$, the language model $\pi$ generates pairs of responses $(\boldsymbol{y_1}, \boldsymbol{y_2}) \sim \pi(\boldsymbol{y} \mid \boldsymbol{x})$. 
A possible preference can be denoted as $\boldsymbol{y_1} \succ \boldsymbol{y_2} \mid \boldsymbol{x}$, where $\boldsymbol{y_1}$ and $\boldsymbol{y_2}$ represent the preferred and dispreferred responses. 
Preferences are assumed to be generated based on a reward model $\boldsymbol{r}(\boldsymbol{y} \mid \boldsymbol{x})$, which assigns a continuous reward $r$ to each response $\boldsymbol{y}$.
For simplicity, we omit $\boldsymbol{x}$ and use $\boldsymbol{r}(\boldsymbol{y})$ to denote $\boldsymbol{r}(\boldsymbol{y} \mid \boldsymbol{x})$.

The pairwise preference probability $p(\boldsymbol{y_1} \succ \boldsymbol{y_2} \mid \boldsymbol{x})$ can be modeled using the Bradley-Terry (BT) framework \citep{bradley-etal-1952-rank} as follows:
\begin{equation}
\label{eq:bt}
    p(\boldsymbol{y_1} \succ \boldsymbol{y_2} \mid \boldsymbol{x}) = \frac{\exp(\boldsymbol{r}(\boldsymbol{y_1}))}{\exp(\boldsymbol{r}(\boldsymbol{y_1})) + \exp(\boldsymbol{r}(\boldsymbol{y_2})))}.
\end{equation}

Now, consider a more generalized scenario with a list of $n$ responses, denoted as $Y_n = \{\boldsymbol{y_i}\}_{i=1}^n$, and the corresponding list of reward $R_n = \{r_i\}_{i=1}^n$. A possible preference ranking $\tau_n = \boldsymbol{y}^{(1)} \succ \cdots \succ \boldsymbol{y}^{(i)} \succ \cdots \succ \boldsymbol{y}^{(n)} \mid \boldsymbol{x}$, where $\boldsymbol{y}^{(i)}$ denotes the response ranked at the $i$-th position. Using the Plackett-Luce ranking model \citep{plackett-1975-permutation, luce-2012-individual}, the preference probability is defined as:
\begin{equation}
\label{eq:pl}
    p(\tau_n) = \prod_{i=1}^{n} \frac{\exp(\boldsymbol{r}(\boldsymbol{y}^{(i)}))}{\sum_{j=i}^{n} \exp(\boldsymbol{r}(\boldsymbol{y}^{(j)}))}.
\end{equation}

\paragraph{Text Generation as a Markov Decision Process (MDP)}
The text generation process can be modeled as an MDP, which is represented by the triple $(\mathcal{S}, \mathcal{V}, u)$\footnote{We omit the transition dynamics $T$ for simplicity. In text generation, these dynamics are deterministic, as each state-action pair uniquely determines the next state.}, where the state space $\mathcal{S}$ represents all possible partially generated sequences, and the action space $\mathcal{V}$ corresponds to the vocabulary in the language model. At each step $t$, an action $y_t \in \mathcal{V}$ (a token) is taken based on the current state $s \in \mathcal{S}$ (the partially generated sequence) and gains a step (token)-level reward $u$. 

\section{\mymod{Self-Derived Log-Likelihood Rewards}}
This section introduces how we derive a reward function from language models without any reference model, providing the theoretical foundation for the framework proposed in the next section.

\paragraph{Inverse Reinforcement Learning (IRL)}
To induce the token-level reward model $u$, we follow the maximum-entropy IRL framework~\citep{ziebart2008maximum, chan2021scalable}, where the Q-value function at step $t$ is defined as:
\begin{align}
    Q(&y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) =\\
    &u(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) + \log \sum_{y_{t+1}} \exp[Q(y_{t+1}\mid \boldsymbol{y}_{\leq t}, \boldsymbol{x})]. \nonumber
\end{align}

Following \citet{hao-2022-teacher}, we parameterize the Q-function as $Q(\cdot) = f_\pi(\cdot)$,
where $f_\pi(\cdot)$ represents the output logits of the language model $\pi$. The reward function $u$ at each step $t$ is then defined as
\begin{align}
    u(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) =& f_\pi(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) \\
    - &\log \sum_{y_{t+1}\in \mathcal{V}} \exp[f_\pi(y_{t+1}\mid \boldsymbol{y}_{\leq t}, \boldsymbol{x})]  \nonumber
\end{align}

We further define $f_t := f_\pi(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x})$ and $Z_t := \sum_{y_t \in \mathcal{V}} \exp\bigl(f_\pi(y_t \mid \boldsymbol{y}_{\leq t-1},  \boldsymbol{x})\bigr)$ for simplicity, which allows us to write that $u(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) = f_t - \log Z_{t+1}$. Please note that at last step, i.e., $t=|y|$, we have $\log Z_{|y|+1} = 0$ according to the definition of the Q-value.

\paragraph{Cumulative Log-Likelihood Reward}
Given the token-level reward function $u$, the sequence-level reward is naturally defined by cumulating the token-level rewards:
\begin{align}
\label{eq:sequence_reward}
    \boldsymbol{r}(\boldsymbol{y} \mid \boldsymbol{x}) &= \sum_{t=1}^{|\boldsymbol{y}|} u(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) = \sum_{t=1}^{|\boldsymbol{y}|} \left(f_t - \log Z_{t+1}\right) \notag \\
    &= \sum_{t=1}^{|\boldsymbol{y}|} \left(f_t - \log Z_t\right) + \log Z_1 - \bcancel{\log Z_{|\boldsymbol{y}|+1}} \notag \\
    &= \sum_{t=1}^{|\boldsymbol{y}|} \log p_\pi(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x}) + \log Z_1 \notag \\
    &= \log p_\pi(\boldsymbol{y} \mid \boldsymbol{x}) + \log Z_1,
\end{align}
where $p_\pi(y_t \mid \boldsymbol{y}_{<t}, \boldsymbol{x})$ is the probability of token $y_t$ given the previous sequences $(\boldsymbol{y}_{<t}, \boldsymbol{x})$. Please note that $\log Z_1$ does not depend on the particular sequence $\boldsymbol{y}$.

\paragraph{Normalized Log-Likelihood Reward}
By combining the Plackett-Luce model in Eq.~\ref{eq:pl} with the cumulative reward in Eq.~\ref{eq:sequence_reward}, the probability for preference $\tau_n$ is given by:
\begin{equation}
\label{eq:pref_modeling}
    p(\tau_n) = \prod_{i=1}^{n} \frac{\exp \left(\log p_\pi(\boldsymbol{y}^{(i)} \mid \boldsymbol{x})\right)}{\sum_{j=i}^{n} \exp \left(\log p_\pi(\boldsymbol{y}^{(j)} \mid \boldsymbol{x}) \right)}.
\end{equation}

When modeling preferences, the term $\log Z_1$ can be eliminated due to the \textit{translation invariance} property of the softmax function. Therefore, the cumulative reward simplifies to:
\begin{equation}
    \boldsymbol{r}(\boldsymbol{y} \mid \boldsymbol{x}) = \frac{1}{|\boldsymbol{y}|} \log p_\pi(\boldsymbol{y} \mid \boldsymbol{x}).
\label{eq:sim-reward}
\end{equation}
where $1/|\boldsymbol{y}|$ is a length-normalized term to avoid bias towards longer sequences \citep{meng-2024-simpo, gu-2024-minillm}.

In other words, the reward of a language model can be formalized as the average log-likelihood, which naturally reflects the inherent preferences of the language model. \mymod{Specifically, the higher the probability the model assigns to generating a response $\boldsymbol{y}$, the greater the associated reward}~\footnote{\myadd{Existing works \citep{rafailov-2023-direct, gu-2024-minillm} also propose constructing reward functions using language models. However, the reward functions we present offer advantages in training efficiency and performance, which are discussed and compared theoretically in Appendix \ref{app:diss-reward}.}}.



\begin{figure*}[ht]
    \centering
    \vspace{-4mm}
    \includegraphics[width=\textwidth, trim=0 160 0 130, clip]{figures/methodology_v2.pdf}
    \vspace{-8mm}
    \caption{The overall process of the PAD contains three critical steps. The initial step involves sampling diverse responses with high temperature (§\ref{sec:phase-1}). Next, rewards for both models are computed, where the rewards of the teacher would be calibrated(§\ref{sec:phase-2}). Finally, the student is trained to mimic the teacher's preference distributions.(§\ref{sec:phase-3})}
    \vspace{-4mm}
    \label{fig:method}
\end{figure*}

\section{PAD: Preference-Aligned Distillation}

\label{sec:method}
This section outlines our PAD, which involves three key training phases (§\ref{sec:phase-1}-\ref{sec:phase-3}), followed by the introduction of a preference decomposition strategy to accelerate the training process (§\ref{sec:phase-4}).

\subsection{Diverse Response Generation}
\label{sec:phase-1}

As the first step, taking prompt $\boldsymbol{x}$ as input, we directly sample $n$ responses $Y_n$ from the student model $\pi^{\text{stu}}$ through repeated sampling. To enhance response diversity, we apply a higher decoding temperature of 0.8. This approach offers two key advantages. First, enabling the generation of higher-quality responses. Existing works have shown that as the number of repeated samples increases, the likelihood of the model generating better answers across various tasks, such as \mymod{mathematics and coding} \citep{wang-2023-selfconsistency, roz-2024-codellama, brown-2024-largelanguagemonkeysscaling}.  Second, mitigating the exposure bias. Exposure bias arises from the mismatch between training and inference, where the model is trained on ground truth contexts but relies on its own predictions during inference, leading to error accumulation. 
\mymod{Following \citet{gu-2024-minillm, agarwal2024onpolicy}, we train the student model on self-generated responses to reduce this bias.}

\subsection{Reward Calculation and Calibration}
\label{sec:phase-2}

Given a prompt $\boldsymbol{x}$ and its corresponding list of responses $Y_n$ from the previous step, we calculate the rewards for both the teacher and student models for each response $\boldsymbol{y}_i \in Y_n$ using Equation (\ref{eq:sim-reward}). These rewards, denoted as $\boldsymbol{r}^{\text{tch}}(\boldsymbol{y}_i)$ and $\boldsymbol{r}^{\text{stu}}(\boldsymbol{y}_i)$, represent the models' average log-likelihood for each response. However, language models often suffer from \textit{miscalibration}, where the assigned likelihoods do not accurately reflect the actual quality of the sequences \citep{zhao-2023-calibrating}. For instance, phrases such as "pros and cons" and "cons and pros" convey the same meaning, but the former may be more frequent in the training data, leading the model to assign it a higher probability. This miscalibration poses a challenge: if the teacher's reward is miscalibrated, aligning the student model to the teacher may propagate this issue.

To address this, we leverage insights from \citet{knowno2023} and \citet{pmlr-v239-ren23a}, who demonstrate that Multiple-Choice Question (MCQ) selection probabilities better capture response quality than sequence likelihoods. We introduce the MCQ selection probability to calibrate the teacher model's reward.\footnote{\myadd{A discussion of other calibration methods is provided in Appendix \ref{app:diss-calibrate}.}} Specifically, each response $\boldsymbol{y}_i \in Y_n$ is randomly mapped to a choice within a predefined set $C_n$ (e.g., $C_3 = \{\text{`A'}, \text{`B'}, \text{`C'}\}$), and we compute the (token-level) probability of selecting each choice:
\begin{equation}
\label{eq:select}
    p_{\text{sel}}(\boldsymbol{y}_i) = p(c_i \mid Y_n, C_n, \boldsymbol{x}),
\end{equation}
where $c_i$ corresponds to the choice associated with response $\boldsymbol{y}_i$.

We then calibrate the reward for each response by combining the normalized log-likelihood reward with the selection probability:
\begin{equation}
    \hat{\boldsymbol{r}}^{\text{tch}}(\boldsymbol{y}) = (1-\alpha) \boldsymbol{r}^{\text{tch}}(\boldsymbol{y}) + \alpha \log p_{\text{sel}}(\boldsymbol{y}),
\end{equation}
where the reward calibration ratio $\alpha \in [0,1]$ is a hyperparameter that balances the influence of the original reward and the MCQ selection probability.

\subsection{Preference Distillation}
\label{sec:phase-3}

Based on different ways of modeling teacher preferences, we employ two losses to distillation: the vanilla preference loss $\mathcal{L}_\text{VPD}$, and the probabilistic preference loss $\mathcal{L}_\text{PPD}$.

\paragraph{Vanilla Preference Distillation (VPD)}
Following \citet{rafailov-2023-direct, song-2024-pro}, the preference is modeled as a unique ranking. Specifically, we obtain ranking $\tau_n$ of the responses $Y_n$ by sorting them according to their rewards $\hat{r}^{\text{tch}}$. The student model is then trained with negative log-likelihood (NLL) loss to maximize the probability of teacher preference using Eq.~\ref{eq:pref_modeling}.
\begin{align}
    \mathcal{L}_\text{VPD} = \sum_{i=1}^n \log \frac{\exp\left( \beta \boldsymbol{r}^{\text{stu}}(\boldsymbol{y}^{(i)}) \right)}{\sum_{j=i}^n \exp\left( \beta\boldsymbol{r}^{\text{stu}}(\boldsymbol{y}^{(j)}) \right)},
\end{align}
where $\beta$ is a hyperparameter that controls the scaling of the reward difference.

\paragraph{Probabilistic Preference Distillation (PPD)}
Inspired by \citet{cao-2007-listnet}, we treat the teacher's rewards as uncertain indicators of preference, which means any preference ranking is assumed to be possible but has a different likelihood. 

The preference distribution over all possible rankings for the teacher is expressed as:
\begin{equation}
    \forall \tau_n \in \mathcal{T},\ p_{\pi^{\text{tch}}}(\tau_n) = \prod_{i=1}^n \frac{\exp\left(\beta\hat{\boldsymbol{r}}^{\text{tch}}(\boldsymbol{y}^{(i)}) \right)}{\sum_{j=i}^n \exp\left(\beta\hat{\boldsymbol{r}}^{\text{tch}}(\boldsymbol{y}^{(j)}) \right)},
\end{equation}
where $\mathcal{T}$ denotes the set of all possible rankings. The student’s preference distribution $p_{\pi^{\text{stu}}}(\tau_n)$ is modeled similarly.

We then employ the Jensen-Shannon divergence (JSD) loss to align the student's and teacher's preference distributions:
\begin{equation}
\label{eq:ppd}
    \mathcal{L}_\text{PPD}=
    \frac{1}{2} \left[\mathcal{D}_\text{KL}(\pi^{\text{tch}} || \pi^\text{mix}) + \mathcal{D}_\text{KL}(\pi^{\text{stu}} || \pi^\text{mix}) \right],    
\end{equation}
where mixed distribution $\pi^\text{mix} = (\pi^{\text{tch}} + \pi^{\text{stu}}) / 2$, and $\mathcal{D}_\text{KL}(\cdot \| \cdot)$ is the Kullback-Leibler divergence (KLD). Specifically, the KLD between the teacher's preference distribution and the mixed distribution is defined as:
\begin{equation*}
\mathcal{D}_\text{KL}(\pi^{\text{tch}} || \pi^\text{mix})= \sum_{\tau_n \in \mathcal{T}} p_{\pi^{\text{tch}}}(\tau_n) \log \frac{p_{\pi^{\text{tch}}}(\tau_n)}{p_{\pi^{\text{mix}}}(\tau_n)}.
\end{equation*}
Similarly, $\mathcal{D}_\text{KL}(\pi^{\text{stu}} || \pi^\text{mix})$ is computed in the same way. By aligning the student's preference distribution with the teacher's, the student model not only learns the correct preference ranking but also captures the teacher’s confidence in these rankings.

\subsection{Preference Decomposing Strategy}
\label{sec:phase-4}

In our PAD, the number of sampled responses, i.e., the sample size $n$, plays a pivotal role. A larger $n$ allows for a more macro comparison among responses, reduces the variance introduced by sampling, and increases the likelihood of generating high-quality responses \citep{brown-2024-largelanguagemonkeysscaling}. However, as $n$ increases, the computational cost of both sampling and forward propagation also rises. Particularly when modeling preference distributions, the complexity grows factorially, making the computation unfeasible when $n$ becomes large.

To reduce the computational cost, we propose a preference decomposition strategy. This strategy breaks down the preference of a large batch of responses into the preferences of multiple smaller batches, allowing the training process to be split into several iterative rounds, thereby reducing the overall computational load.

\paragraph{Decomposing Preference Modeling}
Given a preference ranking $\tau_n$, we define a preference decomposition function $\phi$ to decompose it into $k$ sub-preferences, such that $\phi(\tau_n) = \{\tau_m^{(1)}, \tau_m^{(2)}, \dots, \tau_m^{(k)}\}$.
Assuming that these sub-preferences are independent, we simplify the probability of the complete preference to the probability of the \textit{decomposed preferences} as follows:
\begin{equation}
\label{eq:approx}
p(\tau_n) \xrightarrow{\text{simplify}}  p(\phi(\tau_n)) = \prod_{i=1}^{k} p(\tau_m^{(i)}).
\end{equation}

\begin{figure}[!t]
    \centering
    \vspace{-2mm}
    \includegraphics[scale=0.48]{figures/iterative_distill.pdf}
    \vspace{-7mm}
    \caption{Iterative Distillation Process. }
    \label{fig:iter_distll}
    \vspace{-4mm}
\end{figure}

Hence, we use decomposed preferences as the learning objective, for VPD, its NLL loss for decomposed preferences as
\begin{equation}
\label{eq:log_approx}
\log p(\phi(\tau_n)) = \sum_{i=1}^{k} \log p(\tau_m^{(i)}).
\end{equation}

This shows that the distillation loss for a large batch of responses is equivalent to the sum of losses over multiple smaller batches.\footnote{A similar decomposition can be applied to Preference Propagation Decoding (PPD), with the detailed proof provided in Appendix \ref{app:theoretical}. } Based on this insight, we adopt the \textit{Iterative Distillation Process}, as depicted in Figure \ref{fig:iter_distll}. In this approach, the distillation process is divided into \(k\) iterations, each applied to smaller batches of size \(m\). This reduces the complexity of modeling the preference distribution from \(O(n!)\) to \(O(k \cdot m!)\), thereby significantly lowering the computational cost of training.
