
\clearpage
\section{Decomposing Probabilistic Preference Distillation}
\label{app:theoretical}
Substituting Eq. \ref{eq:approx} into the KLD:
\begin{align*}
     & \sum_{\tau_n} \left( \prod_{i=1}^{k} p_{\pi^{\text{tch}}}(\tau_m^{(i)}) \right) \log \left( \frac{\prod_{i=1}^{k} p_{\pi^{\text{tch}}}(\tau_m^{(i)})}{\prod_{i=1}^{k} p_{\pi^{\text{mix}}}(\tau_m^{(i)})} \right) \\
     =& \sum_{\tau_n} \left( \prod_{i=1}^{k} p_{\pi^{\text{tch}}}(\tau_m^{(i)}) \right) \left( \sum_{i=1}^{k} \log \frac{p_{\pi^{\text{tch}}}(\tau_m^{(i)})}{p_{\pi^{\text{mix}}}(\tau_m^{(i)})} \right)\\
     \intertext{interchange summations,}
     =& \sum_{i=1}^{k} \sum_{\tau_n} \left( \prod_{j=1}^{k} p_{\pi^{\text{tch}}}(\tau_m^{(j)}) \right) \log \frac{p_{\pi^{\text{tch}}}(\tau_m^{(i)})}{p_{\pi^{\text{mix}}}(\tau_m^{(i)})}
\end{align*}
notice that for a fixed $i$, the logarithm term only depends on $\tau_m^{(i)}$, and the product can be separated,
\begin{small}
\begin{align*}
     = \sum_{i=1}^{k} \left( \sum_{\tau_m^{(i)}} p_{\pi^{\text{tch}}}(\tau_m^{(i)}) \log \frac{p_{\pi^{\text{tch}}}(\tau_m^{(i)})}{p_{\pi^{\text{mix}}}(\tau_m^{(i)})} \right) \prod_{j \neq i} \left( \sum_{\tau_m^{(j)}} p_{\pi^{\text{tch}}}(\tau_m^{(j)}) \right)
\end{align*}
\end{small}
\vspace{-6mm}
\begin{flalign*}
     \intertext{based on the independence assumption of sub-preferences, $\sum_{\tau_m^{(j)}} p_{\pi^{\text{tch}}}(\tau_m^{(j)}) = 1$ for each $j$, }
    & = \sum_{i=1}^{k} \sum_{\tau_m^{(i)}} p_{\pi^{\text{tch}}}(\tau_m^{(i)}) \log \frac{p_{\pi^{\text{tch}}}(\tau_m^{(i)})}{p_{\pi^{\text{mix}}}(\tau_m^{(i)})}&
\end{flalign*}

Therefore, the KLD can be decomposed as:
\begin{align*}
   &\mathcal{D}_{\text{KL}}(p_{\pi^{\text{tch}}}(\tau_n) \| p_{\pi^{\text{mix}}}(\tau_n)) =\\ & \quad \quad \quad \quad \sum_{i=1}^{k} \mathcal{D}_{\text{KL}}(p_{\pi^{\text{tch}}}(\tau_m^{(i)}) \| p_{\pi^{\text{mix}}}(\tau_m^{(i)}))
\end{align*}

The JSD Loss (Eq. \ref{eq:ppd}) used in PPD is the average of two KLDs in different directions, making JSD also decomposable.











\section{Implementation Details}
\label{app:inplement}

\subsection{Training}
\label{app:train}

\begin{table}[!tp]
\small
\centering
\setlength{\tabcolsep}{1.3em}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{ccc}
\hline
\textbf{hyperparameter} & \textbf{Value} & \textbf{Searching Space} \\ \hline
$\beta$ & 10 & $[1, 2, 5, 8, 10]$  \\ 
batch size & 128 &  $[32, 64, 128]$  \\ 
warmup ratio & 0.1 &  $[0.05, 0.1]$  \\ 
\hline
\end{tabular}
\caption{The hyperparameter values in PAD training.}
\label{tab:hyper}
\end{table}
We individually search the learning rates for different model families in the range of $ [3e-7, 5e-7, 8e-7, 1e-6, 1e-5]$. As a result, the learning rate for \textsc{Gemma-2} Models is $8e-7$ and for \textsc{Llama-3} Models is $1e-6$. Table \ref{tab:hyper} shows other hyperparameters for training.
All the training experiments in this paper were conducted on 2×A800 GPUs based on the TRL repo\footnote{\url{https://github.com/huggingface/trl/tree/main}}.

\subsection{Evaluation}
\label{app:eval}
\paragraph{Data Statistics}
AlpacaEval 2 consists of 805 questions from five datasets, MT-Bench includes 80 questions across eight categories, and the recently released Arena-Hard is an enhanced version of MT-Bench, comprising 500 challenging questions. Since the training data, ultrafeedback, includes some mathematical reasoning problems, we additionally incorporate the GSM8K test set, which contains approximately 1,300 questions, to evaluate the model's mathematical abilities.

\paragraph{Judge Models} For AlpacaEval 2.0 and Arena-Hard, we employ \textsc{LLaMA-3.1-70B-Instruct} as the judge model. For MT-Bench, we employ GPT-4 Turbo as the judge model. For GSM8K, we report accuracy on the test set. Table \ref{tab:judge} presents the evaluation capability test\footnote{\url{https://github.com/tatsu-lab/alpaca_eval/tree/main/src/alpaca_eval/evaluators_configs}} of these judge models on AlpacaEval. We can see that \textsc{LLaMA-3.1-70B-Instruct} has evaluation capabilities comparable to GPT4-Turbo.

\begin{table}[!tp]
\small
\centering
\setlength{\tabcolsep}{0.6em}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{cc}
\hline
\textbf{Model} & \textbf{Human Agreement} \\ \hline
GPT4 & 69.17  \\ 
\textsc{LLaMA-3.1-70B-Instruct} & 69.10  \\ 
GPT4-Turbo & 68.09 \\
\textsc{LLaMA-3-70B-Instruct} & 67.53  \\ 
\textsc{Qwen2.5-72B-Instruct} & 67.51 \\\hline
Humans & 65.66 \\
\hline
\end{tabular}
\caption{Leaderboard of judge models in AlpacaEval.}
\label{tab:judge}
\end{table}

\subsection{Baselines}
\label{app:baseline}
For traditional knowledge distillation, we consider three baselines: 1) \textbf{Standard KD} \citep{hinton2015distill}: Fine-tunes the student model using the teacher model's logits distribution as a supervision signal, applied to golden responses.
2) \textbf{SeqKD} \citep{kim-rush-2016-sequence}: Directly fine-tunes the student model with cross-entropy loss using responses generated by the teacher model.
3) \textbf{MiniLLM} \citep{gu-2024-minillm}: Employs the teacher model's logits distribution as supervision signal while fine-tuning the student model on its own generated responses.

For preference knowledge distillation, under the "Teacher-as-Annotator" paradigm, we employ three offline preference optimization methods as baselines:
1) \textbf{DPO} \citep{rafailov-2023-direct, tunstall-2024-zephyr}: Treats the student model as a reward model, fine-tuning it based on a reward function derived from a reference model.
2) \textbf{SimPO} \citep{meng-2024-simpo}: Operates similarly to DPO but uses average log-likelihood as the optimization objective.
3) \textbf{RPO} \citep{song-2024-pro}: Extends the above approaches by optimizing with listwise preference information. For a fair comparison, we also use responses sampled from the student model for these baselines. We use the MCQ selection probability introduced in Section \ref{sec:phase-2} as the score to rank the responses. 
For the pairwise preference optimization methods DPO and SimPO, we select the responses with the maximum and minimum rewards to form preference pairs. For the listwise preference optimization method PRO, we directly sort the scores to form the preference ranking. Please kindly note that constructing preference pairs using the maximum and minimum scores of responses is a common practice \citep{cui2023ultrafeedback, meng-2024-simpo}. Moreover, our preliminary experiments indicate that splitting the entire listwise response data into multiple pairwise data and training with DPO/SimPO does not yield significant performance improvements. 

\section{Additional Experiments and Analyses}
\label{app:additional}


\paragraph{Scaling Up} 
\input{tables/scalling_up}
We use \textsc{Gemma-2-27B-It} as the teacher and \textsc{Gemma-2-2B-It} as the student. The overall performance is shown in Table ~\ref{tab:scaling_up}. When employing larger-scale teacher models, our PAD consistently and significantly enhances the ability of small models to align with human preferences. Compared to the main result (§\ref{sec:main_result}), we observe that when using a more capable teacher, the student model achieves greater performance improvements, indicating that the performance gap between the teacher and student is a key factor in determining the extent of the student's enhancement.





\begin{figure}[!tbp]
    \includegraphics[scale=0.23]{figures/sample_v2.pdf}
    \caption{Win Rate with different sample size $n$.}
    \label{fig:sample}
\end{figure}

\paragraph{Effect of Sample Size} We investigate the impact of the number of sampled responses on PPD, and the results can be seen in Figure \ref{fig:sample}. We observe that as the number of sample size $n$ increases, the performance of the student model improves accordingly. This indicates that obtaining more feedback knowledge through extensive sampling from the text generation space facilitates better alignment of the student model with the teacher's preferences.


\section{Comparative Analysis of Reward Functions}
\label{app:diss-reward}

\input{tables/reward_cmp}

\myadd{For the preference distillation, existing works DPO\citep{rafailov-2023-direct} and MiniLLM\citep{gu-2024-minillm} both propose using language models to construct reward functions. 
However, there are significant differences between these methods and our PAD, particularly in the formulation of the reward function, the theoretical foundation, and how the reward is used during training. Below, we elaborate on three key distinctions.}

\subsection{Formulation of Reward Functions}

\paragraph{DPO (Direct Preference Optimization)} \myadd{constructs the reward function using the log ratio of the likelihoods between the "current model" and the "reference model" for the same response:
$$\mathbf{r}(\boldsymbol{y}) = \log \frac{p_\text{current}(\boldsymbol{y})}{p_\text{reference}(\boldsymbol{y})},$$
where $p_\text{current}$ represents the likelihood of response $\boldsymbol{y}$ under the model aligned with human preferences, and $p_\text{reference}$ typically corresponds to the likelihood from a supervised fine-tuning (SFT) model. A higher reward indicates that the current model is more likely to generate $\boldsymbol{y}$ than the reference model.}

\paragraph{MiniLLM} \myadd{is similar in spirit to DPO but uses a “teacher model” and “student model” in a distillation framework. Its reward function is based on the likelihood ratio:
$$\mathbf{r}(\boldsymbol{y}) = \log \frac{p_\text{teacher}(\boldsymbol{y})}{p_\text{student}(\boldsymbol{y})}.$$
Here, a larger reward implies that the teacher model is more inclined to generate the sequence than the student model.}
    
\paragraph{PAD (Ours)} \myadd{defines the reward function as the model’s log-likelihood of the response, averaged by the sequence length for consistency across varying lengths:
$$\mathbf{r}(\boldsymbol{y}) = \frac{1}{|y|} \log p_\text{current}(\boldsymbol{y}).$$
When the model assigns a higher probability to a response, that response receives a higher reward.}


\subsection{Theoretical Foundations and Training Processes}

\paragraph{Different Theoretical Foundations.}
\myadd{DPO is grounded in the Reinforcement Learning from Human Feedback (RLHF) framework, while MiniLLM derives its reward function from the reverse KL-divergence of the policy gradient. In contrast, PAD is based on Inverse Reinforcement Learning (IRL), where we demonstrate that the average log-likelihood can function as a reward, effectively capturing the model’s intrinsic preferences.}

\paragraph{Roles in the Loss Function.}
\myadd{DPO aims to directly maximize the margin between "good" and "bad" responses within its loss function. MiniLLM, on the other hand, treats the reward function as a scaling factor for the SFT loss, where responses with higher rewards experience a more significant increase in likelihood. PAD utilizes the reward function to derive a preference distribution over responses, enabling a student model to emulate the teacher model's preferences. Notably, unlike DPO and PAD, MiniLLM's reward function does not explicitly encode preference.}

\paragraph{Reference Model Requirement.}
\myadd{Both DPO and MiniLLM necessitate the simultaneous use of two models (current/reference or teacher/student) for reward computation. In contrast, PAD requires only the log-likelihood of the current model, eliminating the need for a reference model. This simplification not only streamlines the theoretical framework but also reduces computational cost in practice.}

\paragraph{Alignment with the Generation Stage.}
\myadd{The reward functions in DPO and MiniLLM generally do not align with the probability distribution used during the final text-generation phase. In contrast, PAD’s reward function is directly aligned with the model’s log-likelihood, ensuring consistency between the training and inference.}

\subsection{Empirical Evaluation}

\myadd{Beyond the theoretical distinctions outlined in Table \ref{tab:reward_comparison}, the choice of reward significantly influences performance across various benchmarks. As illustrated in the main results of Table \ref{tab:main}, PAD, which employs average log-likelihood as its reward function, demonstrates superior performance gains.}


\section{Discussion of Reward Calibration}
\label{app:diss-calibrate}

\subsection{Existing Methods}  
\myadd{
Recent studies have shown that large language models (LLMs) are well-calibrated for multiple-choice question-answering and true/false evaluation tasks \cite{kadavath2022languagemodelsmostlyknow, openai-2024-gpt4technicalreport, robinson2023leveraging}, suggesting that these models exhibit better calibration on token-level scores. In addition to the MCQ-based calibration method, we also compare it with the \textit{P(True)} method proposed by \citet{kadavath2022languagemodelsmostlyknow}.
}
\paragraph{P(True)} \myadd{involves asking the model whether a candidate answer is correct. If the answer is correct, the model outputs \texttt{True}, otherwise \texttt{False}, with the probability of \texttt{True} representing the likelihood of the response being correct. Formally, for an input $x$ and a response $y$, this probability is given by:}
$$
p_{\text{true}}(x, y) = p(\text{Yes} \mid x, y).
$$

\myadd{
A variant, \textbf{P(True) with reference}, incorporates all candidate responses:}
$$
p_{\text{true}}(x, y, Y) = p(\text{Yes} \mid x, y, Y).
$$
\myadd{
Notably, the P(True) method is more computationally intensive than the MCQ-based method, as it requires multiple queries to determine the probability values for different responses, whereas MCQ-based calibration can map responses to options and obtain probabilities in a single query.}

\subsection{Experimental Comparison}  
\myadd{We conducted experiments on the \textbf{Alpaca-eval Evaluator} test set \citep{alpaca-eval} to assess the alignment of various calibrated reward functions with human preferences. This test set, consisting of approximately 2.5k data points, is commonly used to evaluate LLM performance as judges, where each data point includes a pair of responses ranked by human preferences. We applied different reward calibration methods to rank these preference pairs and evaluated their alignment with human preferences.}

\myadd{The evaluation metrics include \textit{Human Agreement} and \textit{Prob. prefer longer}.}

\paragraph{Human Agreement:} \myadd{This metric measures the alignment between human and model rankings of response pairs. Given $n$ response pairs ($y_g$ for the good response and $y_b$ for the bad response), the score is calculated as:}
\begin{equation*}
\begin{split}
\text{score} = 1 - \frac{1}{n} \sum_{i=1}^{n} \biggl[ 
    &\left| \text{rank}_i^\text{human}(y_g) - \text{rank}_i^\text{pred}(y_g) \right| \\
    + &\left| \text{rank}_i^\text{human}(y_b) - \text{rank}_i^\text{pred}(y_b) \right| \biggr]
\end{split}
\end{equation*}
\myadd{with a score closer to 1 indicating stronger alignment with human preferences.}

\paragraph{Prob. prefer longer:} \myadd{This metric measures the likelihood that the model's preferred response is longer than the alternative, indicating a potential length bias in the model's preferences.}

\myadd{We evaluated the performance of two models, Gemma-2-9B-It and Llama-3.1-8B-Instruct, with the results presented in Table \ref{tab:calibrate_cmp}. These results show that the MCQ-based calibration achieves the highest alignment with human preferences while exhibiting a relatively lower bias toward longer responses.}

\begin{table}[ht]
\small
\centering
\setlength{\tabcolsep}{1em}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lccccc}
\hline
\multirow{2}{*}{\textbf{Method}} & \textbf{Agreement} & \textbf{Prob. prefer longer} \\ \cline{2-3} 
& (\%) & (\%) \\ \hline
\multicolumn{3}{c}{\textsc{Gemma-2}} \\
MCQ & 68.21 & 65 \\
P(True) & 61.27 & 52 \\
P(True) w/ ref. & 67.75 & 76 \\
\hline
\multicolumn{3}{c}{\textsc{Llama-3}} \\
MCQ & 65.59 & 75 \\
P(True) & 63.12 & 60 \\
P(True) w/ ref. & 65.47 & 76 \\
\hline
\end{tabular}
\caption{Alpaca Evaluator Performance Comparison.}
\label{tab:calibrate_cmp}
\end{table}

\myadd{We further examined the impact of different reward calibration methods on PAD with PPD Loss using two benchmarks, Alpaca-Eval 2.0 and Arena-Hard. The results, shown in Table \ref{tab:calibrate_cmp_eval}, demonstrate that the MCQ-based method consistently outperforms other methods, achieving the highest performance on both benchmarks.
}

\begin{table}[ht]
\centering
\small
\centering
\setlength{\tabcolsep}{1.2em}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcc}
\hline
\multirow{2}{*}{\textbf{Method}} & \textbf{Alpaca-Eval} & \textbf{Arena-Hard} \\ \cline{2-3} 
& LC (\%) & WR (\%) \\ 
\hline
MCQ & \textbf{49.62} & \textbf{59.50} \\
P(True) & 44.09 & 53.96 \\
P(True) w/ ref. & 48.59 & 57.74 \\
\hline
\end{tabular}
\caption{Impact of Different Reward Calibration Methods on PAD with Gemma-2}
\label{tab:calibrate_cmp_eval}
\end{table}

\myadd{These findings highlight the significant impact of the chosen reward calibration method on PAD performance. The MCQ-based method not only aligns more closely with human preferences but also improves the effectiveness of PAD training, leading to better alignment in the distilled model.}
