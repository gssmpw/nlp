\begin{table}[h]
\caption{Training hyper-parameters and settings for dense CLIP used for sparse upcycling and \name}
\label{tab:hyper-parameter}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \setlength{\tabcolsep}{4.0 pt}
\renewcommand{\arraystretch}{1.5}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lc}
\toprule
\multicolumn{2}{c}{\textbf{General}} \\
\midrule
Batch size & 32768 \\
\rowcolor{lightgray} 
Image size & $224 \times 224$ \\
Text tokenizer & T5~\cite{raffel2023exploringlimitstransferlearning}, lowercase \\
\rowcolor{lightgray} 
Text maximum length & 77 tokens \\
% Steps for dense model & 439087 (\textit{i.e.,} $\sim$ 14B examples seen) \\
% \rowcolor{lightgray} 
% Steps for upcycling & 351269 (\textit{i.e.,} $\sim$ 11B examples seen) \\
Optimizer & AdamW ($\beta_1 = 0.9, \beta_2 = 0.98$) \\
% \rowcolor{lightgray} 
% Peak learning rate (LR) & $5e^{-4}$ (dense CLIP), $5e^{-5}$ (\name) \\
\rowcolor{lightgray} 
LR schedule & cosine decays with linear warm-up (first 2k steps) \\
% \rowcolor{lightgray} 
% Weight decay & 0.2 (dense CLIP), 0.05 (\name) \\
Dropout rate & 0.0 \\
\midrule

\multicolumn{2}{c}{\textbf{MoE}} \\
\midrule
% Expert count & 8 (for both text and image seprately), 16 (for shared backbone) \\
Inner structure & Pre-Layer Normalization~\cite{xiong2020layernormalizationtransformerarchitecture} \\
\rowcolor{lightgray} 
Router type & Top-2 routing \\
Expert capacity factor ($C$) & 2.0 (both text and image) \\
\rowcolor{lightgray} 
MoE position & [dense, sparse] (half of MLP layers replaced by MoE layers) \\
Load balance loss weight &0.01 \\
\rowcolor{lightgray} 
Router-z loss weight & 0.0001 \\
\midrule

\multicolumn{2}{c}{\textbf{Dense Model}} \\
\midrule
Steps & 439087 (\textit{i.e.,} $\sim$ 14B examples seen) \\
\rowcolor{lightgray} 
Peak learning rate (LR) & $5e^{-4}$ \\
Weight decay & 0.2 \\
\midrule

\multicolumn{2}{c}{\textbf{\name}} \\
\midrule
Steps & 351269 (\textit{i.e.,} $\sim$ 11B examples seen) \\
\rowcolor{lightgray} 
Peak learning rate (LR) & $5e^{-5}$ \\
Weight decay & 0.05 \\
\rowcolor{lightgray} 
Expert count & 8 (for text and image separately) \\
\midrule

\multicolumn{2}{c}{\textbf{Sparse Model}} \\
\midrule
Steps & 790356 (\textit{i.e.,} $\sim$ 25B examples seen) \\
\rowcolor{lightgray} 
Peak learning rate (LR) & $5e^{-4}$ \\
Weight decay & 0.2 \\
\rowcolor{lightgray} 
Expert count & 8 \\
% \midrule

\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\end{table}