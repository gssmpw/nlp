[
  {
    "index": 0,
    "papers": [
      {
        "key": "shazeer2017outrageouslylargeneuralnetworks",
        "author": "Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean",
        "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer"
      },
      {
        "key": "fedus2022switchtransformersscalingtrillion",
        "author": "William Fedus and Barret Zoph and Noam Shazeer",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "jiang2024mixtralexperts",
        "author": "Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and L\u00e9lio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Th\u00e9ophile Gervet and Thibaut Lavril and Thomas Wang and Timoth\u00e9e Lacroix and William El Sayed",
        "title": "Mixtral of Experts"
      },
      {
        "key": "dai2024deepseekmoeultimateexpertspecialization",
        "author": "Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models"
      },
      {
        "key": "xue2024openmoeearlyeffortopen",
        "author": "Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You",
        "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "mustafa2022multimodalcontrastivelearninglimoe",
        "author": "Basil Mustafa and Carlos Riquelme and Joan Puigcerver and Rodolphe Jenatton and Neil Houlsby",
        "title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "du2024revisitingmoedensespeedaccuracy",
        "author": "Xianzhi Du and Tom Gunter and Xiang Kong and Mark Lee and Zirui Wang and Aonan Zhang and Nan Du and Ruoming Pang",
        "title": "Revisiting MoE and Dense Speed-Accuracy Comparisons for LLM Training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "riquelme2021scalingvisionsparsemixture",
        "author": "Carlos Riquelme and Joan Puigcerver and Basil Mustafa and Maxim Neumann and Rodolphe Jenatton and Andr\u00e9 Susano Pinto and Daniel Keysers and Neil Houlsby",
        "title": "Scaling Vision with Sparse Mixture of Experts"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lepikhin2020gshardscalinggiantmodels",
        "author": "Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen",
        "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "fedus2022switchtransformersscalingtrillion",
        "author": "William Fedus and Barret Zoph and Noam Shazeer",
        "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "komatsuzaki2023sparseupcyclingtrainingmixtureofexperts",
        "author": "Aran Komatsuzaki and Joan Puigcerver and James Lee-Thorp and Carlos Riquelme Ruiz and Basil Mustafa and Joshua Ainslie and Yi Tay and Mostafa Dehghani and Neil Houlsby",
        "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "he2024upcyclinglargelanguagemodels",
        "author": "Ethan He and Abhinav Khattar and Ryan Prenger and Vijay Korthikanti and Zijie Yan and Tong Liu and Shiqing Fan and Ashwath Aithal and Mohammad Shoeybi and Bryan Catanzaro",
        "title": "Upcycling Large Language Models into Mixture of Experts"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "komatsuzaki2023sparseupcyclingtrainingmixtureofexperts",
        "author": "Aran Komatsuzaki and Joan Puigcerver and James Lee-Thorp and Carlos Riquelme Ruiz and Basil Mustafa and Joshua Ainslie and Yi Tay and Mostafa Dehghani and Neil Houlsby",
        "title": "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints"
      },
      {
        "key": "he2024upcyclinglargelanguagemodels",
        "author": "Ethan He and Abhinav Khattar and Ryan Prenger and Vijay Korthikanti and Zijie Yan and Tong Liu and Shiqing Fan and Ashwath Aithal and Mohammad Shoeybi and Bryan Catanzaro",
        "title": "Upcycling Large Language Models into Mixture of Experts"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2024clipmoebuildingmixtureexperts",
        "author": "Jihai Zhang and Xiaoye Qu and Tong Zhu and Yu Cheng",
        "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2024clipmoebuildingmixtureexperts",
        "author": "Jihai Zhang and Xiaoye Qu and Tong Zhu and Yu Cheng",
        "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2024clipmoebuildingmixtureexperts",
        "author": "Jihai Zhang and Xiaoye Qu and Tong Zhu and Yu Cheng",
        "title": "CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling"
      }
    ]
  }
]