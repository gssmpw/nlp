%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{arydshln}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
%\PassOptionsToPackage{table}{xcolor}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{multicol}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\newcommand{\name}{CLIP-UP\xspace}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\name: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe with Sparse Upcycling}
\pagestyle{fancy}
\fancyhead[L]{Preprint}

\begin{document}

\twocolumn[
\icmltitle{\name: A Simple and Efficient Mixture-of-Experts CLIP Training  Recipe with Sparse Upcycling}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
%\icmlauthor{Xinze Wang}{comp}
%\icmlauthor{Chen Chen}{comp}
%\icmlauthor{Yinfei Yang}{comp}
%\icmlauthor{Hong-You Chen}{comp}
%\icmlauthor{Bowen Zhang}{comp}
%\icmlauthor{Aditya Pal}{comp}
%\icmlauthor{Xiangxin Zhu}{comp}
%\icmlauthor{Xianzhi Du}{comp}


\icmlauthor{Xinze Wang}{}
\icmlauthor{Chen Chen}{}
\icmlauthor{Yinfei Yang}{}
\icmlauthor{Hong-You Chen}{}
\icmlauthor{Bowen Zhang}{}

\icmlauthor{Aditya Pal}{}
\icmlauthor{Xiangxin Zhu}{}
\icmlauthor{Xianzhi Du}{}

\setlength{\parskip}{10pt}
Apple

\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Apple, Cupertino, USA}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Xinze Wang}{xinze\_wang@apple.com}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.
\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution}
%\printAffiliationsAndNotice{}% otherwise use the standard text.

\begin{abstract}
Mixture-of-Experts (MoE) models are crucial for scaling model capacity while controlling inference costs. While integrating MoE into multimodal models like CLIP improves performance, training these models is notoriously challenging and expensive. We propose \textbf{CLIP-Up}cycling ( \textbf{\name}), an efficient alternative training strategy that converts a pre-trained dense CLIP model into a sparse MoE architecture. Through extensive experimentation with various settings and auxiliary losses, we demonstrate that \name significantly reduces training complexity and cost. Remarkably, our sparse CLIP B/16 model, trained with \name, outperforms its dense counterpart by 7.2\% and 6.6\% on COCO and Flickr30k text-to-image Recall@1 benchmarks respectively. It even surpasses the larger CLIP L/14 model on this task while using only 30\% of the inference FLOPs. We further demonstrate the generalizability of our training recipe across different scales, establishing sparse upcycling as a practical and scalable approach for building efficient, high-performance CLIP models.

\end{abstract}

\section{Introduction}
\label{submission}

CLIP~\cite{radford2021learningtransferablevisualmodels,jia2021scaling} has proven to be a transformative model across a wide range of domains, including image classification, multimodal retrieval, and AI-driven multimodality content generation~\cite{Zhou_2022,rao2022densecliplanguageguideddenseprediction,gan2022visionlanguagepretrainingbasicsrecent,ramesh2021zeroshottexttoimagegeneration,liu2023visualinstructiontuning}. However, as demands on CLIP grow across these domains, scaling up the model becomes increasingly important to achieve better performance. Recent efforts to scale up CLIP have primarily focused on increasing the size of dense models \cite{Cherti_2023}, which, while effective, result in significant computational overhead and high inference costs.

\begin{figure}[t!]
%\vskip -0.1in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{images/recipe_study.pdf}}
\caption{\textbf{Our proposed Mixture-of-Experts (MoE) CLIP pre-training recipe.} We identify several key ingredients for efficient and effective training including shared vs. separated image-text backbones, training from scratch vs. sparse upcycling from a dense model, use of auxiliary losses, and etc. A detailed analysis is provided in Section~\ref{comparison-methodology}. We conclude a simple strategy that outperforms previous practice~\cite{mustafa2022multimodalcontrastivelearninglimoe} with even less overall training cost.
}
\label{fig:strategy-compare}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure*}[ht]
\vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{images/workflow.pdf}}
\caption{\name overview (Left) and sparse upcycling  initialization (Right). To upcycle from a pre-trained dense checkpoint (e.g., single expert), the MLP layers in some transformer blocks are replaced with MoE layers. During initialization, the MoE MLP parameters are copied from the original dense checkpoint. The MoE router is randomly initialized.}
\label{fig:clip-moe-flow}
\end{center}
\vskip -0.3in
\end{figure*}

Another promising approach to scaling CLIP is through sparse modeling, particularly by introducing Mixture-of-Expert~(MoE) layers~\cite{mustafa2022multimodalcontrastivelearninglimoe}. MoE is a technique that activates only a subset of the model's experts (parameters) per input, which reduces inference cost~\cite{shazeer2017outrageouslylargeneuralnetworks} compared to the dense model with the same number of parameters. MoE models have demonstrated great success in various tasks, including multimodal tasks~\cite{fedus2022switchtransformersscalingtrillion,lepikhin2020gshardscalinggiantmodels}. 
Despite these advances, there remains key problems in prohibitively high training cost of CLIP like models from scratch and the need of complicated auxiliary losses to stabilize the MoE model training. 
\citet{mustafa2022multimodalcontrastivelearninglimoe} investigated training the LIMOE model – an MoE CLIP model with a shared backbone for text and image encoders – from scratch. While it significantly outperforms the dense model, LIMOE demands substantial computational resources and introduces newly defined auxiliary losses for enhanced robustness. For instance, LIMOE with CLIP B/16 requires 1.35 times the training FLOPs of its dense counterpart.

In this work, we explore an alternative training strategy: upcycling a pre-trained dense CLIP model to a sparse MoE architecture. Sparse upcycling~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts} leverages a well-trained dense model to initialize a sparse model with MoE layers. Figure~\ref{fig:clip-moe-flow} illustrates the process in detail. Our experiments start with the CLIP-B/16 architecture from LIMOE, initially trained from scratch with a shared backbone. We then further evaluate the impact of LIMOE auxiliary loss, shared versus separated backbones, and training from scratch versus sparse upcycling. As shown in Figure \ref{fig:strategy-compare}, sparse upcycling with a separated backbone yields the best quality metrics while significantly reducing training costs, lowering ZFLOPS from 4.2 to 3.7 compared to training from scratch. Incorporating LIMOE’s local and global entropy loss ~\cite{mustafa2022multimodalcontrastivelearninglimoe} substantially improves the performance of shared backbone trained from scratch but still lags behind other configurations. Section \ref{comparison-methodology} provides a detailed analysis of training strategies and the impact of LIMOE auxiliary loss under different setups.

% In this work, we explore an alternative training strategy: upcycling a pre-trained dense CLIP model to a sparse MoE architecture. 
% Our experiments, comparing models with shared versus separated backbones and trained via sparse upcycling versus training from scratch, reveal that the separated backbone with sparse upcycling yields the best results. We also observe that inititliazing from a pre-trained dense model greatly stabilizes the training process. In particular, while incorporating the local and global entry loss introduced by LIMOE~\cite{mustafa2022multimodalcontrastivelearninglimoe} substantially improves the performance of the shared backbone trained from scratch, it still lags behind other configurations. Notably, this auxiliary loss is no longer necessary for the other settings, could leading to even stronger models. The key findings are summarized in Figure \ref{fig:strategy-compare}.

In particular, we propose \textbf{\name}, a sparse upcycling approach~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts} for CLIP models with MoE. As illustrated in Figure~\ref{fig:clip-moe-flow}, this method upgrades a pretrained CLIP into a sparsely activated MoE model with relatively low additional training cost. By leveraging the pretrained dense model, \name benefits from a warm start, significantly improving training efficiency. We demonstrate that \name outperforms both continued training of the original dense model and training a sparse model from scratch across different scales.

In summary, our contributions are as follows:
\begin{enumerate}[nosep,topsep=0pt,parsep=0pt,partopsep=0pt, leftmargin=*]
  \item  We introduce \name, a simple yet effective training recipe for Mixture-of-Experts (MoE) CLIP models leverages sparse upcycling from pre-trained weights, eliminating the need for complex auxiliary losses. \name outperforms existing MoE CLIP training methods, regardless of whether the backbone is shared or separated.  
  
  \item  \name achieves substantial performance improvements across various Text-Image retrieval benchmarks compared to the corresponding dense models. 
  Notably, \name with a B/16 backbone surpasses dense CLIP by 7.2\% and 5.5\% on recall@1 of COCO~\cite{lin2014microsoft} and Flickr30K~\cite{flickr30k} text-to-image retrieval tasks respectively.
  
  \item  We show \name is scalable to different model sizes (from B/32 to L/14) and provide a comprehensive analysis of key factors and challenges, providing valuable insights into the design decisions.
\end{enumerate}


\section{Related Work}
\label{related-work}

\textbf{Mixture-of-Experts.} Sparsely-activated MoE models have emerged as a powerful approach for scaling model architectures while maintaining computational efficiency~\cite{shazeer2017outrageouslylargeneuralnetworks,fedus2022switchtransformersscalingtrillion}. By activating only a subset of experts during inference, MoE models significantly reduce the required computation compared to dense models, without sacrificing performance~\cite{jiang2024mixtralexperts,dai2024deepseekmoeultimateexpertspecialization,xue2024openmoeearlyeffortopen}. However, the additional expert layers and the complexity of routing mechanisms required to select active experts during training make MoE models more resource-intensive to train from scratch.

Early pioneer work like LIMOE~\cite{mustafa2022multimodalcontrastivelearninglimoe} applies the MoE framework to CLIP models, using a shared structure for both text and image modalities. This design enables efficient learning of shared representations, leading to significant performance improvements across various multimodal tasks and outperforming compute-matched dense models.

Despite its advantages, LIMOE shares a common limitation with other MoE models: the need for substantial computational resources during training~\cite{du2024revisitingmoedensespeedaccuracy}. 
% For instance, LIMOE with CLIP B/16 requires 1.35 times the training FLOPs compared to its dense counterpart. 
As shown in Figure \ref{fig:strategy-compare}, training with a shared backbone from scratch yields suboptimal results compared to alternative backbone selections and training strategies. While LIMOE auxiliary loss offers some improvement, the overall performance remains inferior, suggesting that the methodology may not be the most effective approach.

% Additionally, it introduces a more complex routing mechanism, Balanced Partition Routing (BPR) (\cite{riquelme2021scalingvisionsparsemixture}), and two more auxiliary losses, to improve training stability. BPR is more intricate than the simpler top-2 routing with "first-come-first-serve" logic commonly used in traditional MoE models (\cite{lepikhin2020gshardscalinggiantmodels}, \cite{fedus2022switchtransformersscalingtrillion}), as well as the more complicated auxiliary losses, increasing implementation complexity and training overhead.

\textbf{Sparse upcycling.} Sparse upcycling~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts} is a technique that effectively reduce training cost of sparse models by reusing existing dense model checkpoints. By leveraging the prior knowledge from a well-trained dense model, the sparse version benefits from accelerated convergence and reduced computational overhead~\cite{he2024upcyclinglargelanguagemodels}.

The current applications of sparse upcycling have been primarily focused on single-modality models~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts,he2024upcyclinglargelanguagemodels}. The effectiveness of this approach in the context of multimodal models remains largely unexplored. The concurrent work CLIP-MoE~\cite{zhang2024clipmoebuildingmixtureexperts} upcycles MoE layers by initializing them with fine-tuned MLP layers via cluster-and-contrast learning. Each expert requires an additional training stage. Scaling this method is challenging, as adding more experts increases complexity and necessitates additional training stages.
% The concurrent work of CLIP-MoE~\cite{zhang2024clipmoebuildingmixtureexperts} employs diversified multiplet upcycling for CLIP using MoE. Its training involves two main stages. First, the MLP layers of the base CLIP model are fine-tuned through a cluster-and-contrast process in Multistage Contrastive Learning. Second, the initialized experts from the MLP layers are used to fine-tune the resulting CLIP-MoE with both contrastive learning loss and router balance loss. However, scaling this method is challenging, as adding more experts increases complexity and necessitates additional training stages.

In this work, we extend sparse upcycling to CLIP with MoE using a single-stage training process. This approach significantly reduces training complexity and cost while achieving superior results, like shown in Figure \ref{fig:strategy-compare}. We also demonstrate its generalizability across shared or separated backbones 
% various architectures 
and model scales. This establishes sparse upcycling as a practical and scalable solution for building efficient, high-performance CLIP models.

% {\color{red} Concurrent work \cite{zhang2024clipmoebuildingmixtureexperts}.}

\section{Sparse Upcycling with Mixture-of-Experts}
\label{method}

Figure \ref{fig:clip-moe-flow} outlines the \name architecture and its upcycle training strategy. The left panel showcases the traditional CLIP contrastive training with expert layers replacing some MLP layers. The right panel demonstrates the innovative upcycle training approach. We'll dive into the specifics of each in this section.


\subsection{CLIP}
Given $n$ pairs of image and text captions $\{(\mathbf{I}_j, \mathbf{T}_j)\}_{j=1}^{n}$, CLIP~\cite{radford2021learningtransferablevisualmodels} learns image representations as $f(\mathbf{I}_j)$ and text representation as $g(\mathbf{T}_j)$ via contrastive learning objective within a training batch. Relevant pairs are positioned closer in the feature space, while irrelevant pairs are pushed farther apart. Given batch size of $B$ and a learned temperature parameter $\theta$, the image to text loss is
\[
\mathcal{L}_{I-T} = - \frac{1}{B} \sum_{j=1}^B \log \frac{e^{\text{sim}(f(\mathbf{I}_j), g(\mathbf{T}_j)) / \theta}}{\sum_{k=1}^{B} e^{\text{sim}(f(\mathbf{I}_j), g(\mathbf{T}_k)) / \theta}}
\tag{1}
\]

and text to image loss is
\[
\mathcal{L}_{T-I} = - \frac{1}{B} \sum_{j=1}^B \log \frac{e^{\text{sim}(f(\mathbf{I}_j), g(\mathbf{T}_j)) / \theta}}{\sum_{k=1}^{B} e^{\text{sim}(f(\mathbf{I}_k), g(\mathbf{T}_j)) / \theta}}
\tag{2}
\]

The contrastive loss is calculated by averaging the above two losses
\[
\mathcal{L}_\text{Contrastive} = \frac{1}{2} (L_{I-T} + L_{T-I})
\tag{3}
\]

\subsection{CLIP with Mixture-of-Experts upcycling} 
In \name, for half of the transformer layers, we replace the dense MLPs with sparsely-gated MoE layers as an alternating dense-sparse-dense structure, similar to Figure~\ref{fig:clip-moe-flow} (Left). This helps the stability of feedforwarding and prevents gradient oscillation, especially during initial training. 


Each MoE layer contains $E$ experts and a router. The experts are MLPs, selectively activated based on the input. The router selects the top-$K$ experts from $E$ total experts, by predicting normalized gating logits for each input token. Let $\mathbf{X}_j \in \mathbb{R}^{D}$ denotes the input of $j$-th token to an MoE layer,  $\mathbf{G}_j \in \mathbb{R}^{D \times E}$ is the gating logis for $E$ experts, $\mathbf{W}_e \in \mathbb{R}^{D}$ is the router weight matrix for expert $e$, the output $MoE(\mathbf{X}_j)$ is computed as follows:
\[
MoE(\mathbf{X}_j) = \mathbf{X}_j + \sum_{e \in \text{Top-K}}g_{e,j}MLP_e(\mathbf{X}_j)
\]
\[
g_{e,j} = 
\begin{cases}
    \mathbf{G}_{e,j}, &\mathbf{G}_{e,j} \in \text{Top-K}({\mathbf{G}_{e,j} | 1 \leq e \leq E}, K), \tag{4} \\
    0, & \text{otherwise},
\end{cases}
\]
\[
\mathbf{G}_{e,j} = Softmax(\mathbf{W}_e^T \mathbf{X}_j)
\]

% \[
% \hspace{-2.0 cm} MoE(\mathbf{X}_j) = \mathbf{X}_j + \sum_{e=1}^{E} g_{e,j}FFN_e(\mathbf{X}_j)
% \]
% \[
% \hspace{0 cm} g_{e,j} = 
% \begin{cases}
%     \mathbf{G}_{e,j}, &\mathbf{G}_{e,j} \in TopK({\mathbf{G}_{e,j} | 1 \leq e \leq E}, K), \tag{2} \\
%     0, & \text{otherwise},
% \end{cases}
% \]
% \[
% \hspace{-2.0 cm} \mathbf{G}_{e,j} = Softmax(\mathbf{W}_e^T \mathbf{X}_j)
% \]
Each expert is assigned a fixed buffer capacity~\cite{fedus2022switchtransformersscalingtrillion}, meaning it can process only a limited number of tokens at a time. With predefined capacity factor $C$, and $B_t$ denotes the number of tokens per batch, the capacity size of each expert $B_e$ is computed as $B_e = (B_t / E) \times C$. This constraint enhances computational efficiency by preventing experts from being overwhelmed, while also ensuring that hardware and memory resources are managed effectively.

Tokens are assigned to experts on a "first-come-first-serve" basis, meaning they are processed in the order they arrive~\cite{fedus2022switchtransformersscalingtrillion}. This simple and effective mechanism handles input streams effectively without introducing additional complexity or overhead from token prioritization or sorting, ensuring that the system remains efficient while distributing tokens across experts.

\textbf{Auxiliary loss.} While the simplified token assignment reduces overhead, it presents the challenge of maintaining a balanced token distribution across all experts. Poor distribution can result in significant token dropping~\cite{zeng2024turnwasteworthrectifying}, which can severely impact performance. Therefore, achieving balance is essential to avoid overloading or underutilizing certain experts. Overuse of some experts may lead to overfitting, while others may not receive enough data for effective training.


To mitigate this, we follow~\cite{zoph2022stmoedesigningstabletransferable} to introduce an auxiliary loss comprising load balance loss and router $z$-loss for each modality, promoting even token distribution. Specifically, the load balance loss encourages the gating mechanism to allocate tokens more uniformly over $E$ total experts. For a sequence of length $S$, the load balance loss is defined as:
\[
\mathcal{L}_\text{Balance} = \sum_{e=1}^E R_e \cdot P_e
\]
\[
R_e = \frac{E}{K \cdot S} \sum_{j=1}^S \mathbbm{1}(\text{Token $j$ dispatched to Expert $e$})
\]
\[
P_e = \frac{1}{S} \sum_{j=1}^S \mathbf{G}_{e,j}
 \tag{5}
\]
% \[
% \hspace{-4.2 cm} \mathcal{L}_{Balance} = \alpha \cdot \sum_{e=1}^E R_e \cdot P_e
% \]
% \[
% \hspace{1.0 cm} R_e = \frac{E}{K \cdot S} \sum_{j=1}^S \mathbbm{1}(\text{Token $j$ dispatched to Expert $e$}) \tag{5}
% \]
% \[
% \hspace{-3.7 cm} P_e = \frac{1}{S} \sum_{j=1}^S \mathbf{G}_{e,j}
% \]
% where $\mathbf{G}_{e,j}$ is the gating logits of Expert $e$ for Token $j$, 
where $R_e$ represents the token assignment ratio to expert $e$, and $P_e$ denotes the average router probability of expert $e$.
% , and $\alpha$ is a hyper-parameter controlling the weight of load balance loss.

The router $z$-loss is to stabilize the gating process by regularizing the output of the router logits, and it ensures the router outputs stay in a reasonable range. It is defined as:
\[
\mathcal{L}_\text{Router} = \frac{1}{S} \sum_{j=1}^{S} \left( \log \sum_{e=1}^{E} e^{\mathbf{G}_{e,j}} \right)^2
\tag{6}
\]
% where $\beta$ is the scaling factor of router-$z$ loss. 

The final training loss is the sum of the contrastive loss and auxiliary loss, with $\alpha$ and $\beta$ as scaling factors,
\[
\label{eq:final_loss}
\mathcal{L} = \mathcal{L}_\text{Contrastive} + \alpha \cdot \mathcal{L}_\text{Balance} + \beta \cdot \mathcal{L}_\text{Router}
\tag{7}
\]


\textbf{LIMOE auxiliary loss.} LIMOE~\cite{mustafa2022multimodalcontrastivelearninglimoe} proposes two new auxiliary losses, the local entropy loss and the global entropy loss, which are applied on a per-modality basis. The local entropy loss is defined as
\[
\mathcal{L}_\text{Local} = \frac{1}{S} \sum_{j=1}^{S} \left( -\sum_{e=1}^{E} \mathbf{G}_{e,j} \log \mathbf{G}_{e,j} \right)
\tag{8}
\]

This encourages concentrated router weights but may reduce the diversity of expert utilization. The global entropy loss is defined as
\[
\mathcal{L}_\text{Global} = -\sum_{e=1}^{E} P_e \log P_e
\tag{9}
\]

which promotes a balanced expert distribution, and enhance diversity in expert usage. To prevent a single modality from overusing too many experts, the global entropy loss is thresholded as:
\[
\mathcal{L}_\text{Global}^{\tau} = \max \{0, \tau + \mathcal{L}_\text{Global} \}
\tag{10}
\]

where $\tau = \log(m)$, ensuring that at least $m$ experts are utilized to minimize the loss. These auxiliary losses stabilize training and improve performance, as demonstrated by~\cite{mustafa2022multimodalcontrastivelearninglimoe}.

According to the best recipe of LIMOE, applying the total training loss becomes:

\begin{equation}
\begin{aligned}
\mathcal{L} = & \mathcal{L}_\text{Contrastive} + \alpha \cdot \mathcal{L}_\text{Balance} +  \beta \cdot \mathcal{L}_\text{Router} \\ 
 & + \gamma \cdot \mathcal{L}^\text{Text}_\text{Local} + \lambda \cdot \mathcal{L}^{\tau, \text{Image}}_\text{Global} + \delta \cdot \mathcal{L}^{\tau, \text{Text}}_\text{Global}
\end{aligned}
\tag{11}
\end{equation}

In our experiments, we replaced the auxiliary loss with the LIMOE training recipe and tuned the hyperparameters for the best perfomrance. While the results demonstrate that the LIMOE auxiliary loss significantly enhances the performance of the shared backbone trained from scratch, its performance still lags behind other configurations and the same recipe doesn't provide similar improvements in those settings. So we chose equation \ref{eq:final_loss} as our final loss. More details will be illustrated in Section \ref{sec:exp}.

% \input{tabels/methodology-compare}
\input{tabels/methodology-compare-no-limoe}


\subsection{Sparse Upcycling Training}

The sparse upcycling process starts from a pre-trained dense CLIP model. The MoE encoders in the upcycled CLIP follow the architecture of the transformer used in~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts} including the number of layers and feature size, similarly for our separated backbone setup.
% including the number of layers and Transformer blocks, 
A selective subset of MLP layers within the Transformer is expanded into MoE layers, with each expert initialized using the same MLP weights from the dense MLP layer of the dense CLIP model, while the router weights are randomly initialized. All other layers, such as attention and embedding layers, are copied directly from the original model. This approach ensures that only a sparse part of the model is modified, while most parameters remain unchanged, preserving the model’s core functionality. After initialization, the upcycled model undergoes additional training steps with the original hyperparameters like batch size, while slightly reducing the learning rate and weight decay to improve performance and stabilize training. The upcycle training process is illustrated in the right panel of figure \ref{fig:clip-moe-flow}.

\begin{figure}[h]  % Create a figure environment
    \centering  % Center the image
    \includegraphics[width=1.0\linewidth]{images/limoe-compare.pdf}
    \vspace{-10pt}
    \caption{Impact of LIMOE auxiliary loss on different training configurations. We observe adding LIMOE loss could sometimes lead to instability issue especially when the backbones are not shared, while our upcycling recipe are more robust.} 
    \label{fig:limoe-compare}  % Add a label for referencing
    \vskip -0.1in
\end{figure}

\section{Experiments}
\label{sec:exp}

\paragraph{Datasets.} For training, we used the same paired image-text dataset for the initial dense CLIP checkpoint training, \name, as well as the baseline dense CLIP model. The training data includes WIT-3000M~\citep{wu2024mofilearningimagerepresentations} and DFN-5B~\citep{fang2023datafilteringnetworks}. For evaluation, we used the standard ImageNet~\citep{deng2009imagenet,pmlr-v119-shankar20c} image classification task and COCO~\citep{lin2014microsoft} and Flickr30K~\citep{flickr30k} image-to-text and text-to-image retrieval tasks. The input image resolution is 224 for all of the datasets. 

\paragraph{Setup.}
We begin by training a dense CLIP model for 440k steps, which serves as the initial stage. Following this, we refine the dense model into an MoE version using the proposed upcycle training, then continuously training it for an additional 350k steps. Both stages use the AdamW optimizer with a batch size of 32k. For the dense model, we apply a learning rate of $5 \times 10^{-4}$ and a weight decay of 0.2. During the upcycle training phase, the learning rate is reduced to $5 \times 10^{-5}$, and the weight decay to 0.05. In the MoE model, half of the MLP layers in the Transformer are replaced with MoE layers, creating an alternating [dense, sparse] structure for both modalities. Each MoE layer includes 8 experts, with the router selecting the top 2 experts for activation. During training, we choose $\alpha = 0.01$ and $\beta = 0.001$ in the router auxiliary loss to ensure balanced expert distribution without dominating the primary cross-entropy objective. Hyper-parameters were fine-tuned based on preliminary experiments to improve performance and ensure training stability. 

\input{tabels/zero_shot_metrics_flops}

Additionally, we train another dense CLIP model for a total of 790k steps to enable a fair comparison. This model is trained with the same configuration as mentioned earlier, using a learning rate of  $5 \times 10^{-4}$ and a weight decay of 0.2 to optimize performance.

\subsection{Recipe Study}
\label{comparison-methodology}
We first investigate the impact of using shared versus separated backbones and the effect of training from scratch compared to employing sparse upcycling. For a fair comparison, the shared backbone configuration uses 16 experts, matching the total number of experts (8 each for text and image) in the separated backbone setup. All experiments are conducted with the CLIP-B/16 model. All results of these models on ImageNet zero-shot classification and COCO/Flickr30k image-text retrieval tasks are illustrated in table \ref{tab:methodology-compare-no-limoe}. 

% \paragraph{Results}
The results show that the separated backbone combined with sparse upcycling delivers the best overall performance, achieving the highest ImageNet Top-1 accuracy of 76.9\% , highest Flickr30K T2I and I2T recall@1 of 80.9\% and 92.3\%, and competitive results in COCO. Separated backbones generally outperform shared ones, likely because of the additional parameters dedicated to each modality. Notably, sparse upcycling provides greater improvements to shared backbones than to separated ones. Compared to shared models trained from scratch, sparse upcycling boosts ImageNet accuracy by 5.5\% and COCO/Flickr retrieval by up to 7.1\%, whereas the separated backbone achieves gains of 2.4\% on ImageNet and up to 4.1\% on retrieval tasks. Sparse upcycling proves effectiveness across both shared and separated architectures, consistently outperforming models trained from scratch. These results highlight the versatility and efficiency of \name across different backbone configurations.

\begin{figure*}[ht]  % Create a figure environment
%\vskip -0.1in
    \centering  % Center the image
    \includegraphics[width=0.95\linewidth]{images/scratch-compare.pdf}
%    \vskip -0.1in
    \caption{Performance comparison along with training EFLOPS of \name and training from scratch on CLIP B/16.}
    \label{fig:scratch-comparison}  % Add a label for referencing
\end{figure*} 

\begin{figure*}[ht]  % Create a figure environment
    \centering  % Center the image
    \vskip -0.1in
    \includegraphics[width=0.95\linewidth]{images/moe-modality.pdf}
%    \vskip -0.1in
    \caption{\name with MoE upcycling for only the text encoder, image encoder, or both. We observe upcycling both the image and text encoders into MoE generally helps, especially for retrieval tasks.} 
    \label{fig:moe-modality}  % Add a label for referencing
    \vskip -0.1in
\end{figure*}

We further explore the impact of adding LIMOE specific auxiliary loss described in Section \ref{method}. Specifically, we set $\tau = 6$ to incentivize at least 6 experts (uniformly) or more (not necessarily in a uniform way)~\cite{mustafa2022multimodalcontrastivelearninglimoe} are used for each modality and fine-tuned the loss weight under our experiment setup. As shown in Figure \ref{fig:limoe-compare}, applying the LIMOE auxiliary loss on the shared backbone trained from scratch significantly improves both ImageNet classification and COCO retrieval metrics, which is aligned with the finding from original LIMOE paper~\cite{mustafa2022multimodalcontrastivelearninglimoe}. However, its performance remains below other configurations without applying LIMOE specific auxiliary losses. 
Adding LIMOE specific loss to best configuration (Separated-Upcycle) achieves a slightly better text-image retrieval metrics but falls short on ImageNet zero-shot classficaition.


In addition, we also find adding these new auxiliary losses make the model hard to train as there are much more hyper-parameters to tune. To our best effort, we cannot make LIMOE specific auxiliary loss work in all configurations, e.g. in Shared-Upcycle and Separated settings. 
Due to the limit of resources, we decide to use the loss in equation \ref{eq:final_loss} as the final training objective to reduce the effort of finding best hyperparameters.

\subsection{Final Model Evaluation and Baselines}

According to the results from the previous section, we select the separted backbone with sparse upcycling as the default recipe and further study its effectiness by training \name models in different model sizes, from B/32 to L/14. Table \ref{zero-shot-metrics-flops-steps} compares the zero-shot image classification and retrieval performance of \name against baseline dense CLIP model trained for an equivalent number of steps across different scales. Extending training steps for dense CLIP from 440k to 790k yields only minor gains. In contrast, \name demonstrates substantial improvements across all metrics. These gains are consistent across model scales, particularly for image-text retrieval tasks. Remarkably, \name B/32, using only 47\% of the inference GFLOPS, outperforms the baseline dense CLIP B/16 by 2.4\% in COCO T2I recall@1 and matches its I2T recall@1. Similarly, \name B/16, requiring just 31\% of the inference GFLOPS, surpasses the baseline dense CLIP L/14 by 1.9\% in COCO T2I recall@1 while matching its I2T recall@1. This highlights the effectiveness of training high-performance CLIP models by leveraging pretrained dense models through sparse upcycling with MoE.

\subsection{Training Efficiency}

To better illustrate the effectiveness of upcycle training, we also show the training process from dense pretraining + upcycling compared to training from scratch in Figure \ref{fig:scratch-comparison}. The initial dense model training provides a strong foundation for subsequent upcycling. In contrast, training from scratch without leveraging the pretrained dense checkpoint, results in much higher computational costs to reach the performance level of \name. This difference is especially noticeable for COCO image-to-text and ImageNet classification tasks. While sparse upcycling introduces an initial quality drop on ImageNet due to model reconfiguration~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts}, \name consistently outperforms the model trained from scratch, highlighting its efficiency in reducing training resources and improving overall performance.

\section{Discussion and More Ablations} \label{ablations}

In this section, we discuss important architecture and training ablations relative to the configuration used during experiment. We use the CLIP B/16 with 8 experts as the default configuration here, expert capacity factor $C = 2.0$, and 6 MoE layers interspersed in the Transformer for both modalities. The hyper-parameters are same as Section \ref{sec:exp}.

% \subsection{Training Efficiency}

% To better illustrate the effectiveness of upcycle training, we also show the training process from dense pretraining + upcycling compared to training from scratch in Figure \ref{fig:scratch-comparison}. The initial dense model training provides a strong foundation for subsequent upcycling. In contrast, training from scratch without leveraging the pretrained dense checkpoint, results in much higher computational costs to reach the performance level of \name. This difference is especially noticeable for COCO image-to-text and ImageNet classification tasks. While sparse upcycling introduces an initial quality drop on ImageNet due to model reconfiguration~\cite{komatsuzaki2023sparseupcyclingtrainingmixtureofexperts}, \name consistently outperforms the model trained from scratch, highlighting its efficiency in reducing training resources and improving overall performance.


% \subsection{Shared architecture across modalities} 

% Table \ref{tab:methodology-compare} demonstrates that \name performs effectively with both shared and separated architectures for image and text modalities. This section further compares \name under shared or separated backbone with dense and sparse model trained from scratch with equivalent steps. 

% \begin{figure}[ht]  % Create a figure environment
%     \centering  % Center the image
%     \includegraphics[width=1.0\linewidth]{images/share-sep-clip-compare.pdf}
%     \caption{Model performance comparison between shared and separated architecture across modalities.} 
%     \label{fig:shared-architecture}  % Add a label for referencing
% \end{figure}

% As shown in Figure \ref{fig:shared-architecture}, the dense and sparse model is trained from scratch with equivalent 790k steps under shared or separated backbones. \name is trained for 350k steps, initialized from a dense checkpoint trained for 440k steps. The separated backbone delivers better results on all metrics for both dense or sparse models trained from scratch, due to the increased model parameters. However, with sparse upcycling, the shared architecture significantly improves, outperforming the separated architecture in COCO image-to-text top-1 retrieval. Moreover, the shared backbone achieves greater improvement over the baseline dense CLIP model trained for the same steps. Compared to the dense CLIP, \name with a shared backbone improves COCO image-to-text top-1 retrieval by 7.82\% and text-to-image by 8.9\%, while the separated backbone achieves 5.54\% and 7.15\% improvements, respectively. These results confirm that \name is particularly beneficial for shared architectures, offering a viable solution for use cases with limited inference resources and memory.

% \name can perform effectively with both shared and separate architectures for image and text modalities as shown in \ref{comparison-methodology}. In this section, we did some detailed analysis of the impact regarding to shared or separated architecture across modalities. In terms of final performance as shown in Figure \ref{fig:shared-architecture}, \name with a shared architecture outperforms the separate architecture in COCO image-to-text top-1 retrieval. However, the separate architecture yields better results in text-to-image retrieval and ImageNet top-1 accuracy. Although the separate architecture generally achieves higher metrics than the shared one, the shared architecture demonstrates greater improvement over the baseline dense CLIP model trained for the same number of steps. Compared to the dense CLIP, \name with a shared backbone improves COCO image-to-text top-1 retrieval by 7.82\% and text-to-image by 8.9\%, while the separate backbone achieves improvements of 5.54\% and 7.15\%, respectively. These results confirm the versatility and effectiveness of \name across both shared and separate architectures.

% \subsection{Impact of Adding LIMOE Specific Auxiliary Loss}

% \begin{figure}[ht]  % Create a figure environment
%     \centering  % Center the image
%     \includegraphics[width=0.8\linewidth]{images/limoe-compare.pdf}
%     \caption{Comparison of LIMOE auxiliary loss impact under different training configurations} 
%     \label{fig:limoe-compare}  % Add a label for referencing
% \end{figure}

% We further explore the impact of adding LIMOE specific auxiliary loss described in Section \ref{method}. Specifically, we set $\tau = 6$ to ensure at least 6 experts are used for each modality and fine-tuned the loss weight under our experiment setup. As shown in Figure \ref{fig:limoe-compare}, applying the LIMOE auxiliary loss on the shared backbone trained from scratch significantly improves both ImageNet classification and MSCOCO retrieval metrics, which is aligned with the finding from original LIMOE paper~\cite{mustafa2022multimodalcontrastivelearninglimoe}. However, its performance remains below other configurations without applying LIMOE specific auxiliary losses. 
% Adding LIMOE specific loss to best configuration (Separated-UpCycle) achieves a slightly better text-image retrieval metrics but falls short on ImageNet zero-shot classficaition.


% In addition, we also find adding these new auxiliary losses make the model hard to train as there are much more hyper-parameters to tune. To our best effort, we cannot make LIMOE specific auxiliary loss work in all configurations.
% % , e.g. in Shared-Upcycle and Separated settings. 
% Due to the limit of resources, we decide to use the loss in equation \ref{eq:final_loss} as the final training objective to reduce the effort of finding best hyperparameters.
% % in the following experiments.

\subsection{MoE added to single or multiple modalities} 

In the \name model, the MoE setup is applied to both the text encoder and image encoder. We also explore the effect of adding MoE layers to only one modality while keeping the other modality fully dense. The COCO retrieval and ImageNet results for these configurations are shown in Figure \ref{fig:moe-modality}, measured across different training steps.

We observe that the initial performance of newly upcycled models tends to decline compared to the starting dense model, regardless of where the MoE setup is applied. However, all configurations recover after approximately 5k training steps. Applying MoE layers to both modalities leads to a more significant initial drop on average. When MoE layers are applied to only one modality (either image or text), the final performance remains comparable. Notably, the initial performance drop is more pronounced when MoE layers are applied to the image modality, suggesting that the model is more sensitive to changes in image representations.

\subsection{Expert capacity factor}

\begin{figure}[h]  % Create a figure environment
    \centering  % Center the image
    \includegraphics[width=1.0\linewidth]{images/capacity-compare.pdf}
    \vskip -0.1in
    \caption{Effects of different expert capacity $C$.} 
    \label{fig:capacity-compare}  % Add a label for referencing
    %\vskip -3pt
\end{figure}

Intuitively, the number of tokens processed by each expert plays a crucial role in determining model quality. In \name, this is controlled by the expert capacity factor, denoted as $C$. A higher $C$ results in less token dropping, thereby reducing the initial quality drop. However, this doesn't necessarily guarantee a higher final model quality. By default, we set the capacity factor to 2.0 for both modalities. As shown in Figure \ref{fig:capacity-compare}, increasing $C_{image}$ to 4.0 significantly boosts the ImageNet zero-shot metrics. However, this adjustment results in a noticeable drop in performance on COCO and Flickr30k retrieval tasks.
% We leave the study of capacity factor for different tasks to a future work.

\begin{figure}[ht]  % Create a figure environment
    %\vskip -0.1in
    \centering  % Center the image
    \begin{subfigure}
        \centering
        \includegraphics[width=1.0\linewidth]{images/coco_dropped_image_token.pdf}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=1.0\linewidth]{images/imagenet_dropped_image_token.pdf}
    \end{subfigure}
    \vskip -0.1in
    \caption{Visualization of image tokens dropped by the router (i.e., not being assigned to any expert due to capacity constraint) on COCO (Top) and ImageNet (Bottom).}
    %\vskip -0.05in
    \label{fig:imagenet-dropped-image-token}  % Add a label for referencing
\end{figure}

Figure \ref{fig:imagenet-dropped-image-token} visualizes the behavior of dropped image tokens under different capacity settings of COCO and Imagenet. The red squares represent the dropped image tokens. With a higher $C_{image}$, fewer image tokens are discarded. This benefits ImageNet performance since the dataset primarily consists of single-object images. Retaining more tokens allows the model to focus on the key features. In contrast, COCO images often depict complex scenes with multiple objects, but not all of which are relevant to the paired captions. Dropping less important image tokens helps the model concentrate on the most important objects, which explains the drop in COCO performance when fewer tokens are discarded.
We leave the further study of capacity factor for different tasks to a future work.

\section{Conclusions}
\label{conclusion}
In this work, we introduce \name, a simple yet efficient training strategy for CLIP that integrates MoE with sparse upcycling. Through extensive experiments on various configurations and auxiliary losses, we show that \name significantly reduces training costs while consistently improving retrieval and classification performance across multiple model scales. Notably, \name outperforms larger dense models in some tasks while drastically cutting inference FLOPs, highlighting sparse upcycling as a practical and scalable approach for building efficient, high-performance CLIP models. Additionally, ablation studies provide comprehensive insights into key design decisions.

% In this work, we introduce \name, a simple and efficient training strategy for CLIP that combines MoE with sparse upcycling. Through extensive experiments across various configurations and auxiliary losses, we show that \name significantly reduces training costs while consistently enhancing performance on retrieval and classification tasks across multiple model scales. Remarkably, \name outperforms larger dense models in certain tasks while drastically reducing inference FLOPs. 

% For instance, the sparse CLIP B/16 backbone surpasses the dense CLIP L/14 by 1.9\% in COCO T2I R@1 retrieval, using only 30\% of the inference FLOPs. Compared to a sparse model with the same architecture trained from scratch, our method achieves superior results with significantly lower training costs. 
% % Ablation studies on different MoE configurations further confirm consistent performance gains across all setups.

\clearpage

\section*{Acknowledgement}
We thank Wentao Wu, Haotian Zhang, and many others for their invaluable help and feedback. 

%\section{Impact Statement}
%
%Our work introduces \name, a simple and efficient strategy to convert dense CLIP models into sparse MoE architectures. \name significantly reduces training complexity and inference costs while simultaneously improving model performance. We have demonstrated its generalizability across different model scales, establishing sparse upcycling as a practical approach for building efficient CLIP models.
%
%This paper advances the field of efficient training strategies for CLIP models. By reducing computational demands, our method contributes to more sustainable pretraining, leading to lower energy consumption and reduced environmental impact. Additionally, by enabling high-performance models to be trained and deployed with fewer resources, our work has the potential to democratize Machine Learning research, making powerful visual foundation models accessible to a wider range of institutions and users with limited computational capabilities.
%
%Overall, our work represents a step forward in sustainable Machine Learning model training, with broad implications for both research and industry adoption of efficient multimodal systems.

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

%\bibliography{example_paper}
%\bibliographystyle{icml2025}

\input{example_paper.bbl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Training Details}
Below, we provide detailed training hyper-parameters and setups for dense CLIP (weights are used for sparse upcycling), sparse CLIP trained from scratch, and \name.

\subsection{Training hyper-parameters}
We primarily follow \cite{radford2021learningtransferablevisualmodels} for hyper-parameter selection, using the WIT-3000M~\cite{wu2024mofilearningimagerepresentations} and DFN-5B~\cite{fang2023datafilteringnetworks} training datasets. Table \ref{tab:hyper-parameter} summarizes the hyper-parameters for all experiments, including MoE-specific configurations and parameters for dense CLIP, sparse CLIP, and \name.

\input{tabels/appendix_hyper_parameters}



\newpage
\section{Ablation study}

\subsection{Normalize gating weights before or after routing.} 
To mitigate the initial quality drop observed when applying sparse upcycling, we experimented with normalizing the router output logits after routing. This ensures the remaining gating weights are normalized to sum to 1, even when some tokens are dropped due to expert capacity constraints. The intuition behind this approach is that in the dense model, each token was previously processed by a single expert MLP.

\begin{figure}[ht]  % Create a figure environment
    \centering  % Center the image
    \includegraphics[width=1.0\linewidth]{images/normalization-compare.pdf}
    \caption{Model performance for gating normalization applied before or after routing} 
    \label{fig:normalization-compare}  % Add a label for referencing
\end{figure}

As shown in Figure \ref{fig:normalization-compare}, normalizing gating weights post-routing helps reduce the initial quality drop. However, in terms of final model performance, this approach shows improved results in image-to-text retrieval, but performs worse in text-to-image retrieval. 

A possible explanation for this discrepancy is that post-routing normalization maintains the magnitude of all remaining tokens, which benefits the image encoder, as most image tokens are informative. In contrast, text encoder often deals with padding tokens, and reducing the magnitude of these tokens can enhance the text encoder's ability to focus on meaningful content. It also aligns with the finding that the initial quality drop is the biggest when adding MoE layers into image modality only.

\newpage
\section{Tabular results}
\subsection{Comparison of model architectures and impact of LIMOE auxiliary loss} 

All results for different model architectures, with and without LIMOE auxiliary loss, as discussed in Section \ref{comparison-methodology}.

\input{tabels/appendix_full_method_compare_with_LIMOE}


\newpage
\subsection{Comparison of MoE added to single modality or both modalities} 

All results from Figure \ref{fig:moe-modality} to compare MoE added into different modalities. 

\input{tabels/appendix_modality_moe}

\newpage
\subsection{Comparison of capacity factor} 

All results from Figure \ref{fig:capacity-compare} to compare the impact of capacity factor $C$.

\input{tabels/appendix_capacity_factor}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
