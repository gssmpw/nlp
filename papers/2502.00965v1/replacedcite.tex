\section{Related Work}
\label{related-work}

\textbf{Mixture-of-Experts.} Sparsely-activated MoE models have emerged as a powerful approach for scaling model architectures while maintaining computational efficiency____. By activating only a subset of experts during inference, MoE models significantly reduce the required computation compared to dense models, without sacrificing performance____. However, the additional expert layers and the complexity of routing mechanisms required to select active experts during training make MoE models more resource-intensive to train from scratch.

Early pioneer work like LIMOE____ applies the MoE framework to CLIP models, using a shared structure for both text and image modalities. This design enables efficient learning of shared representations, leading to significant performance improvements across various multimodal tasks and outperforming compute-matched dense models.

Despite its advantages, LIMOE shares a common limitation with other MoE models: the need for substantial computational resources during training____. 
% For instance, LIMOE with CLIP B/16 requires 1.35 times the training FLOPs compared to its dense counterpart. 
As shown in Figure \ref{fig:strategy-compare}, training with a shared backbone from scratch yields suboptimal results compared to alternative backbone selections and training strategies. While LIMOE auxiliary loss offers some improvement, the overall performance remains inferior, suggesting that the methodology may not be the most effective approach.

% Additionally, it introduces a more complex routing mechanism, Balanced Partition Routing (BPR) (____), and two more auxiliary losses, to improve training stability. BPR is more intricate than the simpler top-2 routing with "first-come-first-serve" logic commonly used in traditional MoE models (____, ____), as well as the more complicated auxiliary losses, increasing implementation complexity and training overhead.

\textbf{Sparse upcycling.} Sparse upcycling____ is a technique that effectively reduce training cost of sparse models by reusing existing dense model checkpoints. By leveraging the prior knowledge from a well-trained dense model, the sparse version benefits from accelerated convergence and reduced computational overhead____.

The current applications of sparse upcycling have been primarily focused on single-modality models____. The effectiveness of this approach in the context of multimodal models remains largely unexplored. The concurrent work CLIP-MoE____ upcycles MoE layers by initializing them with fine-tuned MLP layers via cluster-and-contrast learning. Each expert requires an additional training stage. Scaling this method is challenging, as adding more experts increases complexity and necessitates additional training stages.
% The concurrent work of CLIP-MoE____ employs diversified multiplet upcycling for CLIP using MoE. Its training involves two main stages. First, the MLP layers of the base CLIP model are fine-tuned through a cluster-and-contrast process in Multistage Contrastive Learning. Second, the initialized experts from the MLP layers are used to fine-tune the resulting CLIP-MoE with both contrastive learning loss and router balance loss. However, scaling this method is challenging, as adding more experts increases complexity and necessitates additional training stages.

In this work, we extend sparse upcycling to CLIP with MoE using a single-stage training process. This approach significantly reduces training complexity and cost while achieving superior results, like shown in Figure \ref{fig:strategy-compare}. We also demonstrate its generalizability across shared or separated backbones 
% various architectures 
and model scales. This establishes sparse upcycling as a practical and scalable solution for building efficient, high-performance CLIP models.

% {\color{red} Concurrent work ____.}