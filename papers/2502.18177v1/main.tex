\documentclass[11pt,a4paper,english]{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{babel}
\usepackage{blindtext}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amssymb}
\usepackage{wrapfig}
\usepackage{stfloats}
\usepackage{graphicx}
\usepackage{placeins}  
\usepackage{float}    
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{listings}
\usepackage{multicol}
\usepackage{multirow} 
\usepackage{pgffor}
\usepackage{cleveref}
\usepackage{geometry}
%\usepackage{ulem}

\newcommand{\e}{\mathbb{E}}
\newcommand{\lk}{\left[ }
\newcommand{\rk}{\right] }
\newcommand{\lc}{\left(}
\newcommand{\rc}{\right)}
\newcommand{\Eb}{\mathbb{E}}
\newcommand{\Cb}{\mathbb{C}}
\newcommand{\Rb}{\mathbb{R}}
\newcommand{\Zb}{\mathbb{Z}}
\newcommand{\Pb}{\mathbb{P}}
\newcommand{\Qb}{\mathbb{Q}}
\newcommand{\Nb}{\mathbb{N}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\mc}{\mathcal{m}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\id}{\mathbf{1}}
\newcommand{\veps}{\mathbf{\epsilon}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\zz}{\mathbf{z}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\xx}{\mathbf{x}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\FF}{\mathbf{F}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Lc}{\mathcal{L}}

\newcommand{\myparagraph}[1]{\paragraph{#1}\mbox{}\\}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}


\title{Recurrent Neural Networks for Dynamic VWAP Execution: Adaptive Trading Strategies with Temporal Kolmogorov-Arnold Networks}

\author{%
    Rémi Genet \\
    \small DRM, Université Paris Dauphine - PSL \\
    \small Aplo \\
    \small remi.genet@dauphine.psl.eu \\
}

\begin{document}

\maketitle

\begin{abstract}
The execution of Volume Weighted Average Price (VWAP) orders remains a critical challenge in modern financial markets, particularly as trading volumes and market complexity continue to increase. In my previous work \cite{genet2025staticvwap}, I introduced a novel deep learning approach that demonstrated significant improvements over traditional VWAP execution methods by directly optimizing the execution problem rather than relying on volume curve predictions. However, that model was static because it employed the fully linear approach described in \cite{genet2024tln}, which is not designed for dynamic adjustment. This paper extends that foundation by developing a dynamic neural VWAP framework that adapts to evolving market conditions in real time. We introduce two key innovations: first, the integration of recurrent neural networks to capture complex temporal dependencies in market dynamics, and second, a sophisticated dynamic adjustment mechanism that continuously optimizes execution decisions based on market feedback. The empirical analysis, conducted across five major cryptocurrency markets, demonstrates that this dynamic approach achieves substantial improvements over both traditional methods and our previous static implementation, with execution performance gains of 10–15\% in liquid markets and consistent outperformance across varying conditions. These results suggest that adaptive neural architectures can effectively address the challenges of modern VWAP execution while maintaining computational efficiency suitable for practical deployment.
\end{abstract}

\newpage

\section{Introduction}
The execution of large trading orders in financial markets is a complex and strategic challenge, with the potential to significantly impact both transaction costs and overall market dynamics. In this context, the concept of Volume Weighted Average Price (VWAP) has emerged as a key benchmark and execution strategy, offering market participants a robust framework for minimizing market impact while closely tracking average traded prices over a specified period of time. The practical and theoretical importance of VWAP execution has grown with the rapid evolution of electronic and algorithmic trading platforms. As documented by Mackenzie \cite{Mackenzie}, algorithmic trading now accounts for a substantial majority of institutional order flow. VWAP strategies represent a key component of this automated execution landscape. Despite this practical prominence, however, the academic literature has historically placed greater emphasis on alternative execution benchmarks such as Implementation Shortfall (IS) \cite{perold1988implementation}, opening a door for research in the design and performance of VWAP strategies. Initially, VWAP execution aims to address two fundamental objectives. First, by distributing an order over time and closely tracking the average traded price, VWAP strategies seek to minimize the market impact of large trades - a key component of overall transaction costs as established by Berkowitz et al. \cite{TotalCostOfTransactions}. Second, by targeting a pre-defined benchmark, VWAP provides a transparent and objective measure of the execution quality. This metric is crucial  for institutional investors who must justify their trading performance to stakeholders.

\subsection{VWAP Definition and Discretization}

Following the seminal work of Konishi \cite{Konishi}, the Volume Weighted Average Price (VWAP) over a time period \([0,T]\) is defined as:
\begin{equation}
    \text{VWAP}_{[0,T]} = \frac{\int_0^T P(t)V(t)\,dt}{\int_0^T V(t)\,dt},
\end{equation}
where \(P(t)\) and \(V(t)\) denote the price and volume at time \(t\), respectively. As noted by McCulloch and Kazakov \cite{Culoch2007}, financial markets operate in discrete time intervals, which leads to the discretized form:
\begin{equation}
    \text{VWAP}_{[0,T]} = \frac{\sum_{t=1}^{T} P_t\,V_t}{\sum_{t=1}^{T} V_t},
\end{equation}
where \(P_t\) and \(V_t\) represent the price and volume in the \(t\)-th time interval.

For a trader executing a large order of total size \(Q\), Humphery-Jenner \cite{Humphery} shows that the objective is to minimize the difference between the achieved execution price and the market VWAP. Let \(q_t\) denote the quantity traded in interval \(t\), so that:
\begin{equation}
\sum_{t=1}^{T} q_t = Q.
\end{equation}
The execution price is then given by:
\begin{equation}
P_{\text{exec}} = \frac{\sum_{t=1}^{T} P_t\,q_t}{Q}.
\end{equation}

Following Bialkowski et al. \cite{LeFol2006}, the VWAP execution problem can be formulated as minimizing the slippage:
\begin{equation}
\min_{q_1,\ldots,q_T} \left|\frac{\sum_{t=1}^{T} P_t\,q_t}{Q} - \frac{\sum_{t=1}^{T} P_t\,V_t}{\sum_{t=1}^{T} V_t}\right|.
\end{equation}

For clarity, the normalized order allocation is defined as \(\tilde{q}_t = \frac{q_t}{Q}\) (so that \(\sum_{t=1}^{T} \tilde{q}_t = 1\)) and the normalized market volume profile as \(\tilde{V}_t = \frac{V_t}{\sum_{t=1}^{T} V_t}\) (with \(\sum_{t=1}^{T} \tilde{V}_t = 1\)). With these definitions, the execution price becomes:
\begin{equation}
P_{\text{exec}} = \sum_{t=1}^{T} P_t\,\tilde{q}_t,
\end{equation}
and the market VWAP is:
\begin{equation}
\text{VWAP} = \sum_{t=1}^{T} P_t\,\tilde{V}_t.
\end{equation}

Genet \cite{genet2025staticvwap} reformulates the slippage as a bound:\begin{equation}
    S_T \le \sum_{t=1}^{T} \left| \bigl(P_t-\text{VWAP}_t\bigr)\tilde{q}_t \right| + \sum_{t=1}^{T} \left| \text{VWAP}_t\Bigl(\tilde{q}_t-\tilde{V}_t\Bigr) \right|,
\end{equation}
where \(\text{VWAP}_t\) denotes the market VWAP computed over interval \(t\). The first term quantifies the impact of price deviations weighted by the trader's participation rate, while the second term captures the error due to discrepancies between the trader's normalized allocation \(\tilde{q}_t\) and the market's volume fraction \(\tilde{V}_t\). Under the volume conservation constraint and non-negativity constraints \(\tilde{q}_t \geq 0\), this decomposition separates the overall slippage \(S_T\) into a price deviation component and a volume allocation error component. This optimization problem is particularly challenging because future prices and volumes are unknown at execution time, which necessitates accurate predictions of market dynamics while managing execution risk \cite{frei}. Furthermore, the increasing sophistication of market participants and the rising prominence of transaction cost analysis (TCA) in institutional trading \cite{Madhavan2002} have amplified the importance of VWAP execution strategies. As markets evolve, executing large orders while minimizing market impact becomes increasingly complex, thereby requiring more advanced approaches to VWAP execution.

\subsection{Classical VWAP Approaches}

The theoretical foundation of VWAP execution strategies emerged from a series of seminal works that established the mathematical framework for optimal order execution. Konishi \cite{Konishi} provided one of the first comprehensive analyses of VWAP strategies, demonstrating that in markets where volume and volatility are uncorrelated, the optimal execution curve mirrors the expected relative market volume curve. He further extended his analysis to cover the case where volume and volatility are correlated. This breakthrough provided mathematical validation for practitioners' empirical observations and established a theoretical benchmark for subsequent research in the field. Building on this foundation, McCulloch and Kazakov \cite{Culoch2007} developed a more sophisticated model that incorporated practical constraints and information asymmetries. Their work introduced two crucial elements: constrained trading rates and potential information advantages, acknowledging that traders or brokers might possess sensitive information that could influence their attempts to outperform the VWAP benchmark. Their research also revealed important stylized facts about expected relative volume patterns—most notably, the characteristic S-shape observed in equity markets and the finding that higher-turnover stocks exhibit less variation in their expected relative volume. These patterns echoed the well-documented "U" shape effect in equity market trading activity, thereby providing a crucial link between microstructure theory and practical execution strategies.

\medskip

The relationship between volumes and other market variables has been extensively studied in the literature. Notable contributions include the work of Easley and O'Hara \cite{Easley1987}, Viswanathan and Foster \cite{Foster}, Tauchen and Pitts \cite{Tauchen1983}, and Karpoff \cite{Karpoff1987}, who examined volumes as covariates in analyzing and explaining target variables such as price and volatility, though primarily focusing on low-frequency data rather than intraday patterns. Gourieroux et al. \cite{Gourieroux} made significant contributions to the measurement of market trading activity, providing a theoretical framework for understanding trading volume dynamics. McCulloch and Kazakov \cite{Culoch2012} further extended this line of research by transforming Konishi's fixed model into a continuous dynamic framework. This work established the crucial connection between optimal VWAP trading strategies and accurate intraday volume estimation, demonstrating that successful execution depends fundamentally on the ability to anticipate and adapt to volume patterns throughout the trading day.

\medskip

The evolution of these classical approaches reflects a growing recognition of the complexity inherent in VWAP execution. While these models provided valuable insights and theoretical foundations, they also revealed the limitations of purely static approaches in capturing the dynamic nature of modern markets. This recognition would eventually lead to the development of more sophisticated dynamic approaches and, ultimately, to the application of machine learning techniques in VWAP execution strategies.

\subsection{Dynamic Volume Approaches}

A significant paradigm shift in VWAP execution strategies occurred with the introduction of dynamic volume estimation approaches. Bialkowski et al. \cite{LeFol2006} pioneered this advancement by proposing a novel method for estimating intraday volumes through component decomposition. Their work, later refined in Bialkowski et al. \cite{LeFol2012}, separated volume patterns into two distinct components: one reflecting broader market evolution and another capturing stock-specific patterns. This decomposition enabled more accurate volume predictions by modeling the dynamic component using ARMA and SETAR models, demonstrating substantially improved accuracy compared to traditional static approaches. However, transitioning from simplistic volume modeling to these more advanced methods comes at a cost: such approaches no longer explicitly account for the volume–volatility relationship, as it becomes much more challenging to realistically incorporate both components simultaneously. The shift from static to dynamic approaches was further advanced by Humphery-Jenner \cite{Humphery}, who introduced the concept of Dynamic VWAP (DVWAP) in contrast to the traditional Historical VWAP (HVWAP). Their research highlighted a crucial limitation of historical approaches—their inability to incorporate real-time market information during execution. By developing a framework that adapts to incoming news and market developments, they demonstrated significant improvements over historical methods in both basic VWAP tracking and the management of market dynamics.

\medskip

Alternative theoretical perspectives emerged through the work of Bouchard and Dang \cite{bouchard} and Frei and Westray \cite{frei}, who approached VWAP execution through the lens of stochastic analysis. As Frei and Westray \cite{frei} noted, their derived optimal trading rates depended primarily on volume curves rather than price processes, reflecting the assumption of uncorrelated Brownian motion in price movements. This theoretical framework provided valuable insights into the relationship between volume patterns and execution strategy, even as it highlighted the limitations of purely stochastic approaches. A significant contribution to the practical aspects of VWAP execution came from Carmona and Li \cite{Tianhui}, who examined the strategic considerations at both macro and micro scales. Their research was particularly notable for addressing the practical dilemma faced by brokers in choosing between aggressive and passive orders at the high-frequency level, bringing theoretical insights to bear on practical execution decisions. Guéant and Royer \cite{Gueant} made two crucial contributions that addressed previously understudied aspects of VWAP execution. First, they incorporated a comprehensive market impact model that considered both temporary and permanent effects, addressing a critical concern for institutional investors using VWAP orders to manage large positions. Second, they developed a framework for pricing guaranteed VWAP services using CARA utility functions and indifference pricing. This work represented a significant shift from traditional approaches focused solely on benchmark tracking, introducing a more nuanced understanding of risk-adjusted optimal execution. These dynamic approaches collectively highlighted a crucial insight: while modeling market volumes is important, the assumption of independence between prices and volumes often fails to reflect market reality. This recognition, combined with the increasing availability of computational power and market data, set the stage for the application of more sophisticated analytical techniques, particularly in the domain of machine learning and artificial intelligence.

\subsection{The Rise of Deep Learning in Financial Time Series}
In parallel with these theoretical advances, the field of machine learning has witnessed a rapid development of powerful techniques and architectures, particularly in the domain of deep learning. The field of time series analysis and prediction has been fundamentally transformed by developments in deep learning, particularly in the domain of neural networks. As documented by Sezer et al. \cite{sezer2020financial} in their comprehensive review, deep learning models have increasingly outperformed traditional machine learning approaches across various financial forecasting tasks. The evolution of deep learning architectures for financial applications has been marked by several key innovations. The introduction of Long Short-Term Memory (LSTM) networks by Hochreiter and Schmidhuber \cite{hochreiter1997} addressed the vanishing gradient problem that had limited traditional recurrent neural networks, enabling effective learning of long-term dependencies in sequential data. This was followed by the development of Gated Recurrent Units (GRU) by Cho et al. \cite{cho2014}, offering comparable performance with a more streamlined architecture. A revolutionary step forward came with the introduction of attention mechanisms Bahdanau et al. \cite{bahdanau2014neural}, culminating in the Transformer architecture Vaswani et al. \cite{vaswani2017attention}. While initially developed for natural language processing, these architectures' ability to capture both local and global dependencies in sequential data made them particularly suitable for financial time series analysis.

\medskip

During the last decade, we have seen an explosion of deep learning applications in finance, with researchers tackling increasingly complex challenges. Ackerer et al. \cite{ackerer2020deep} demonstrated the power of neural networks in fitting and predicting implied volatility surfaces, while Horvath et al. \cite{horvath2019deep} showed how deep learning could revolutionize pricing and calibration in volatility models. As highlighted by Zhang et al. \cite{zhang2023deep} in their recent review, deep learning models are gradually replacing traditional statistical and machine learning models as the preferred choice for price forecasting tasks. In the specific domain of trading volume prediction, significant advances have been made through the development of specialized architectures such as Temporal Kolmogorov-Arnold Networks (TKAN) \cite{genet2024tkan}, Signature-Weighted Kolmogorov-Arnold Networks (SigKAN) \cite{inzirillo2024sigkan}, Temporal Kolmogorov-Arnold Transformers (TKAT) \cite{genet2024tkat}, Kolmogorov-Arnold Mixture of Experts (KAMoE) \cite{inzirillo2024kamoe} and Recurrent Neural Networks with Signature-Based Gating Mechanisms (SigGate) \cite{genet2025siggate}. 

\subsection{Deep Learning Approaches to Market Execution}

The application of deep learning to market execution problems has evolved significantly in recent years. Early approaches focused primarily on using neural networks for price prediction or simple trading signals. However, the complexity of VWAP execution, with its intricate relationship between volume patterns, price impact, and timing decisions, presents unique challenges that require more sophisticated approaches. Recent research has begun to explore more advanced applications of deep learning to execution problems. Papanicolaou et al. \cite{papanicolaou2023optimal} demonstrated the effectiveness of using LSTMs for large order execution within the Almgren and Chriss framework, showing how deep learning models could capture cross-sectional relationships between different stocks' execution characteristics. While not specifically focused on VWAP execution, this work highlighted the potential for neural networks to learn complex relationships in market impact and execution timing. One particularly promising development has been the recent introduction of Temporal Kolmogorov-Arnold Networks (TKAN) Genet and Inzirillo \cite{genet2024tkan}. This architecture combines the representational power of Kolmogorov-Arnold Networks with sophisticated temporal processing capabilities, demonstrating exceptional performance specifically in cryptocurrency volume prediction tasks. The success of TKANs in volume prediction suggests their potential applicability to the broader challenge of VWAP execution optimization.

\subsection{From Static to Dynamic Neural VWAP}
My previous research Genet \cite{genet2025staticvwap} established a novel approach to VWAP execution by leveraging deep learning techniques in a fundamentally different way from existing methods. Rather than focusing on volume curve prediction like traditional approaches, I demonstrated that directly optimizing the execution strategy through neural networks could significantly improve performance. Using a Temporal Linear Network (TLN), this static approach showed particular effectiveness in handling market uncertainty and extreme events, consistently outperforming conventional methods across various market conditions. However, the inherent limitations of static approaches become particularly apparent in highly volatile markets such as cryptocurrencies, where market conditions can change dramatically within a single execution window. The success of TKANs in cryptocurrency volume prediction, combined with these limitations of static approaches, suggests a natural evolution toward a more dynamic framework. This observation aligns with earlier findings from Bialkowski et al. \cite{LeFol2012} and Humphery-Jenner \cite{Humphery} about the importance of adapting to changing market conditions, but approaches the problem with the enhanced capabilities offered by modern deep learning architectures.

\medskip

In this paper, I propose a dynamic VWAP execution framework that represents a significant advancement in several key aspects:
First, it maintains the robust foundation of our static approach while incorporating adaptive capabilities through recurrent neural networks. This design choice allows our model to preserve the reliable performance characteristics that made the static approach successful while adding the flexibility to adjust execution strategies based on evolving market conditions. Second, our framework leverages the TKAN architecture, which has demonstrated superior performance in volume prediction tasks Genet and Inzirillo \cite{genet2024tkan}. By applying this architecture to execution strategy rather than just volume prediction, I extend its capabilities to address the full complexity of VWAP execution, including market impact considerations and optimal timing decisions. Third, I address a fundamental limitation of existing approaches by directly incorporating market feedback into our execution decisions. This design creates a more responsive system that can adapt to changing market conditions while maintaining the execution constraints necessary for effective VWAP targeting.

\section{Dynamic VWAP Architecture}

The proposed dynamic VWAP execution framework extends previous work on static neural VWAP optimization. By integrating recurrent neural networks and introducing a novel sequential optimization mechanism, the framework adapts to evolving market conditions while preserving the key principles of VWAP benchmarking. This section provides a detailed overview of the model architecture, including its main components, design considerations, and implementation details.



\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{figures/dynamic_vwap.drawio.jpg}
    \caption{Overview of the dynamic VWAP execution architecture.}
    \label{fig:dynamic_model_overview}
\end{figure}

\subsection{Model Overview}

At its core, the dynamic VWAP framework processes a sequence of market observations to generate optimal execution trajectories in real time. Let \(x_t \in \mathbb{R}^d\) denote the input features at time \(t\), where \(d\) is the feature dimension. Given a lookback period \(l\) and a prediction horizon \(h\), the model operates on input sequences of length \(l + h - 1\), thereby incorporating both historical context and market information received during execution. The architecture consists of three key components: a learnable base volume curve, a recurrent neural network for dynamic adaptation, and a volume adjustment mechanism. These components work together to generate execution trajectories that minimize VWAP slippage while satisfying practical trading constraints.

\subsection{Base Volume Curve}
The foundation of the model is a learnable base volume curve \(v_b \in \mathbb{R}^h\), initialized uniformly as:
\begin{equation}
    v_b^{(0)} = \left\{\frac{1}{h}, \frac{1}{h}, \dots, \frac{1}{h}\right\}.
\end{equation}
This curve is constrained to satisfy two key properties:
\begin{equation}
\sum_{i=1}^h v_b^{(i)} = 1, \quad v_b^{(i)} \geq 0 \quad \forall i \in \{1, \dots, h\}.
\end{equation}

The inclusion of a learnable base curve is motivated by the findings of the static VWAP research Genet \cite{genet2025staticvwap}, which demonstrated the robustness and effectiveness of fixed volume profiles in minimizing execution slippage. By allowing the base curve to adapt during training, the model can potentially discover superior static profiles that serve as the foundation for further dynamic adjustments.

\subsection{Dynamic Volume Adjustment}

The dynamic component begins with a recurrent neural network $\mathcal{R}$ that processes the input sequence to produce hidden states:
\begin{equation}
    h_t = \mathcal{R}(x_t, h_{t-1}),
\end{equation}
where $h_t \in \mathbb{R}^m$ represents the hidden state at time $t$ with dimension $m$.
Note that $h_t$ depends solely on information available up to time $t$, ensuring causality by using both the previous state and the new information at time $t$.

\subsubsection{RNN Architecture Selection}

The recurrent component \(\mathcal{R}\) can be implemented using different RNN variants. Two approaches are considered: Long Short-Term Memory (LSTM) and Temporal Kolmogorov-Arnold Networks (TKAN). LSTM serves as a well-established baseline with proven capability in handling temporal dependencies, while TKAN provides specialized memory management particularly suited for volume forecasting tasks.

\subsection*{LSTM Architecture}

The LSTM unit processes input vector $x_t \in \mathbb{R}^d$ through a series of gates that control information flow. The architecture maintains two types of state: the cell state $c_t$ and the hidden state $h_t$. The key components are the forget gate, which determines what information to discard from the previous state:
\begin{equation}
    f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f),
\end{equation}
The input gate, controlling the integration of new information:
\begin{equation}
    i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i),
\end{equation}
The cell state update, combining previous and new information:
\begin{equation}
    c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t,
\end{equation}
where $\tilde{c}_t = \tanh(W_c x_t + U_c h_{t-1} + b_c)$ represents candidate cell state values.

\subsection*{TKAN Architecture}
The TKAN architecture represents a more specialized approach, developed specifically for volume forecasting tasks. It implements a dual memory mechanism by combining Recurring Kolmogorov-Arnold Networks (RKAN) with additional memory management components. This structure enables sophisticated temporal pattern recognition through hierarchical processing.

At its core, RKAN layers process the input through layer-specific memory states. For each layer $l$, the input transformation is:
\begin{equation}
    s_{l,t}=W_{l,\tilde{x}} x_t + W_{l,\tilde{h}} \tilde{h}_{l,t-1}
\end{equation}
where $W_{l,\tilde{x}} \in \mathbb{R}^{d \times \text{KAN}_{in}}$ and $W_{l,\tilde{h}} \in \mathbb{R}^{\text{KAN}_{out} \times \text{KAN}_{in}}$ transform the current input and the previous sub-state, respectively. The RKAN layer output is computed as:

\begin{equation}
    \tilde{o}_{t} = \phi_{l}(s_{l,t}),
\end{equation}
where $\phi_l$ represents the KAN layer transformation.
The sub-layer memory state is updated according to:
\begin{equation}
    \tilde{h}_{l,t} = W_{hh} \tilde{h}_{l,t-1} + W_{hz} \tilde{o}_{t}.
\end{equation}
The full TKAN combines these RKAN outputs through:
\begin{equation}
    r_t = \text{Concat}[\phi_1(s_{1,t}),\phi_2(s_{2,t}),...,\phi_L(s_{L,t})].
\end{equation}
The output gate then processes this concatenated representation:
\begin{equation}
    o_t = \sigma(W_{o}r_t + b_o),
\end{equation}
where $W_{o} \in \mathbb{R}^{(\text{KAN}_{out} \cdot L,out)}$. The final hidden state update is given by:
\begin{equation}
    h_t = o_t \odot \tanh(c_t).
\end{equation}

This architecture implements two levels of memory management: the RKAN sub-layer memory states ($\tilde{h}_{l,t}$) capture local temporal patterns within each layer, while the global cell state ($c_t$) and hidden state ($h_t$) maintain longer-term dependencies across the entire network. This dual memory mechanism enables our model to effectively capture both fine-grained market dynamics and longer-term trading patterns, providing a rich representation for subsequent volume adjustments.

\subsection{Dynamic Volume Adjustment Network}

The core innovation of the dynamic VWAP framework is the sequential volume adjustment mechanism, which translates temporal patterns from the recurrent network into optimal execution trajectories. For each time step in the prediction horizon, a separate transformation function \(f_i\) (with \(i \in \{1, \dots, h-1\}\)) is employed, implemented as a multi-layer perceptron. For \(t > 0\), the function processes an augmented input that concatenates the RNN hidden state with the history of previous volume decisions:
\begin{equation}
    f_i(h_t, v_{1:t-1}) = W^{(3)}_i \cdot \text{ReLU}(W^{(2)}_i \cdot \text{ReLU}(W^{(1)}_i [h_t; v_{1:t-1}] + b^{(1)}_i) + b^{(2)}_i) + b^{(3)}_i,
\end{equation}
where $[h_t; v_{1:t-1}]$ represents the concatenation of the hidden state $h_t$ with the sequence of previously allocated volumes $v_{1:t-1}$. For the initial timestep ($t=0$), only the hidden state is used as input:
\begin{equation}
    f_0(h_t) = W^{(3)}_0 \cdot \text{ReLU}(W^{(2)}_0 \cdot \text{ReLU}(W^{(1)}_0 h_t + b^{(1)}_0) + b^{(2)}_0) + b^{(3)}_0.
\end{equation}
The output of these functions is then used to compute the volume adjustment factor:
\begin{equation}
    \alpha_i = 1 + \tanh(f_i(h_{l+i-1}, v_{1:i-1})).
\end{equation}
This architectural design enables each adjustment decision to be informed not only by the market conditions (captured in the hidden state) but also by the actual execution trajectory. The use of separate transformation functions for each timestep allows the model to learn time-specific responses that consider both the current market state and the accumulated execution history. This is particularly important as trading decisions often need to balance immediate market conditions with the execution progress achieved so far.

\subsection{Sequential Volume Allocation}

The volume allocation process implements a sequential mechanism that ensures both causality and volume conservation. The process can be broken down into three key steps:
\subsubsection{Base Volume Initialization}
The base volume curve $v_b$ is initialized uniformly and maintained as a learnable parameter with positivity and sum-to-one constraints:
\begin{equation}
    v_b^{(0)} = \{\frac{1}{h}, \frac{1}{h}, ..., \frac{1}{h}\}, \quad \sum_{i=1}^h v_b^{(i)} = 1, \quad v_b^{(i)} \geq 0.
\end{equation}
\subsubsection{Sequential Adjustment Process}
For timesteps $i = 1$ to $h-1$, the allocated volume is computed via a constrained adjustment of the base curve:
\begin{equation}
    v_i = \operatorname{clip}\bigl(\alpha_i\, v_b^{(i)}, 0, 1 - \sum_{j=1}^{i-1} v_j\bigr).
\end{equation}
The clip function enforces three essential constraints:
1. Non-negativity: $v_i \geq 0$
2. Running sum constraint: $\sum_{j=1}^i v_j \leq 1$
3. Volume conservation: ensures sufficient volume remains for future timesteps
Each decision incorporates both the current market state through the RNN hidden state and the execution history through the accumulated volume curve, enabling more informed and context-aware trading decisions.

\subsubsection{Final Timestep Allocation}
For the final timestep $h$, the remaining volume is allocated to ensure complete volume conservation:
\begin{equation}
    v_h = 1 - \sum_{i=1}^{h-1} v_i.
\end{equation}
This construction guarantees that the resulting volume curve satisfies all required constraints:
\begin{equation}
    \sum_{i=1}^h v_i = 1, \quad v_i \geq 0 \quad \forall i \in \{1,...,h\}
\end{equation}
This sequential adjustment mechanism has several important properties. First, it ensures that the executed volume curve always satisfies the VWAP benchmarking constraints. Second, by conditioning each adjustment on the partial volume curve, it allows the model to dynamically adapt to its own prior decisions. Finally, by preserving the base volume curve as a foundation, it strikes a balance between adaptability and robustness, mitigating the risk of extreme adjustments that could lead to poor execution quality.

\subsection{From Training to Real-Time Deployment}

A notable characteristic of the model architecture is the seamless transition from training to real-time deployment. During training, the input sequence includes complete information for the entire period, including future timesteps. This complete sequence is essential for both parameter optimization and loss function computation, as VWAP performance can only be evaluated over the full execution period.

In real-time deployment, the sequential nature of the architecture, where predictions at time $t$ depend solely on information up to that point, permits straightforward adaptation. For deployment, the unknown future portion of the input sequence is padded with zeros:


\begin{equation}
    \tilde{x}_t = \begin{cases}
        x_t & \text{if } t \leq t_{\text{current}} \\
        0 & \text{if } t > t_{\text{current}}
    \end{cases}
\end{equation}

This padding maintains the required input dimensionality while ensuring that future information does not affect current predictions. The causality of the architecture guarantees that these padded values do not influence the predictions for the current timestep, allowing the same model architecture to be used for both training and deployment without structural modifications or performance degradation.

\section{Empirical Results}

This section presents the key findings from the empirical evaluation of the dynamic VWAP framework across a range of market conditions and benchmarks. An overview of the dataset and experimental setup is provided, followed by a detailed analysis of model performance in terms of VWAP slippage, volume curve fit, and execution stability. The approach is then compared to several baseline models, and the implications of the findings for practical VWAP execution are discussed.

\subsection{Experimental Setup}

\subsubsection{Dataset and Preprocessing}

The empirical analysis utilizes hourly trading data from five major cryptocurrencies (BTC, ETH, BNB, ADA, XRP) traded on Binance perpetual futures contracts. The dataset spans from the inception of each contract until July 1, 2024, offering a comprehensive view across different market regimes and conditions. A robust data partitioning strategy is employed to ensure realistic evaluation of model performance. The final 20\% of each cryptocurrency's data is reserved for testing, providing a true out-of-sample evaluation that mirrors real-world deployment scenarios. From the remaining 80\%, the last 20\% is allocated for validation, with the remainder used for training. Due to the temporal nature of the model's lookback window, randomization in dataset splitting is avoided to prevent unwanted overlap between training and validation data.

\subsubsection{Feature Engineering}

The model processes market data using a lookback window of 120 time steps to generate predictions for the subsequent 12 periods. The feature engineering approach emphasizes stationarity and careful handling of temporal dependencies to avoid forward-looking bias. The feature set comprises:

1. Volume Features: Raw volumes are normalized by dividing by a two-week rolling average, with the averaging window shifted by the combined lookback and prediction horizon to prevent information leakage. This normalization helps capture relative volume patterns while maintaining stationarity.

2. Temporal Indicators: Cyclic patterns are incorporated through hour-of-day and day-of-week indicators, enabling the model to learn time-dependent trading patterns characteristic of cryptocurrency markets.

3. Market Metrics: The feature set includes returns computed from bin VWAP prices. Periods without trading activity are assigned zero returns, ensuring continuous representation of market state while accurately reflecting periods of market inactivity.


\subsubsection{Objective}
Given the use of hourly data, the model's objective is to predict trading proportions for each hour. Three different optimization functions are compared. The first two compute deviations from market VWAP using absolute and quadratic terms, respectively. For these calculations, the price of each bin is considered to be its VWAP, allowing for a comparison of the achieved price with the market price by weighting the bin prices using either the predicted volume curve or the market volume curve over the execution period. In addition, the optimization of the quadratic distance to the market volume curve is evaluated, enabling a comparison between direct VWAP optimization and volume curve optimization, which is more common in the literature.


\subsubsection{Model Training Configuration}

A rigorous training protocol is implemented to ensure model stability and convergence. The optimization process employs the Adam optimizer with an initial learning rate of 0.001, along with two essential callback mechanisms:

1. Early Stopping: Training terminates automatically if the validation loss shows no improvement exceeding 0.00001 over a 10-epoch window. This mechanism helps prevent overfitting while ensuring sufficient model convergence.

2. Learning Rate Adaptation: A learning rate reduction callback monitors validation loss and applies a reduction factor of 0.25 after 5 epochs without improvement. The learning rate has a lower bound of 0.000025 to maintain stable optimization.

\medskip

Training proceeds with a batch size of 128 samples and can continue up to 1000 epochs, although early stopping typically results in earlier convergence. To ensure robust evaluation of model performance and stability, 5 independent training runs are conducted for each model configuration, each with different random initializations.

\subsubsection{Implementation Details}

All models are implemented using the Keras 3 framework with a Jax backend. To ensure reproducibility, a global random seed (seed = 1) is set at the framework level. The implementation leverages TKAN package version 0.4.3 and temporal\_linear\_network version 0.1.2 (for the TLN components in the static VWAP implementation). 

\newpage

\subsection{Results and Discussion}

\input{table}

The empirical analysis in table \ref{tab:dynamic_vwap_results} reveals significant improvements in VWAP execution performance through dynamic modeling approaches. The models are evaluated using three key metrics: absolute VWAP loss (measured in $10^{-2}$), quadratic VWAP loss (measured in $10^{-4}$), and the \(R^2\) score for volume curve prediction. The \(R^2\) score is noteworthy as it represents the traditional optimization target for volume-centric methodologies and serves as an important benchmark for comparing the approach with conventional methods.


\subsubsection{Performance Across Model Architectures}

The analysis shows a clear progression of performance improvements as the models evolve from simple linear models to more sophisticated architectures. The original static VWAP implementation using Temporal Linear Networks (TLN) demonstrates significant improvements over naive baselines, with reductions in absolute VWAP loss ranging from 20–25\% across all assets.


\medskip

The transition from TLN to recurrent architectures in the static framework yields substantial additional improvements. Looking at ETH, the static LSTM reduces absolute VWAP loss to 0.139 compared to TLN's 0.144, representing a 3.5\% improvement. The TKAN architecture further enhances these results, with the static implementation achieving 0.136, a 5.6\% improvement over TLN. Similar patterns emerge for BNB, where static LSTM and TKAN achieve losses of 0.137 and 0.134 respectively, compared to TLN's 0.142, representing improvements of 3.5\% and 5.6\%. This consistent pattern suggests that recurrent architectures' ability to capture temporal dependencies provides meaningful advantages even in static implementations. The transition to dynamic architectures marks an even more significant advancement in performance. For ETH, the DynamicVWAP with TKAN achieves an absolute VWAP loss of 0.121, an 11.0\% improvement over its static counterpart (0.136) and a 16.0\% improvement over the static TLN baseline (0.144). In BNB markets, the dynamic TKAN implementation reduces absolute VWAP loss to 0.129, representing improvements of 3.7\% over static TKAN (0.134) and 9.2\% over static TLN (0.142). The dynamic LSTM shows similar patterns but consistently falls slightly behind TKAN, with losses of 0.123 for ETH and 0.131 for BNB. The superiority of TKAN over LSTM becomes particularly pronounced in the dynamic setting. For ADA, dynamic TKAN achieves an absolute loss of 0.178 compared to dynamic LSTM's 0.184, a 3.3\% improvement. This advantage extends to quadratic loss, where dynamic TKAN's 0.127 represents a 4.8\% improvement over dynamic LSTM's 0.133. XRP shows similar patterns, with dynamic TKAN outperforming dynamic LSTM by 3.1\% in absolute loss (0.174 versus 0.179) and 3.2\% in quadratic loss (0.177 versus 0.183).

\medskip

These improvements are particularly noteworthy as they scale with market liquidity. In highly liquid markets such as BTC and ETH, the transition from static to dynamic architectures yields the largest relative improvements, often exceeding 10\%. For less liquid assets such as ADA and XRP, while the absolute improvements are smaller, the relative advantage of dynamic over static implementations remains consistent, indicating that the approach successfully adapts to varying market conditions. The enhanced performance of TKAN in both static and dynamic settings, particularly its superior ability to capture complex temporal patterns in real time, establishes a clear hierarchy of performance. Dynamic TKAN consistently leads, followed by dynamic LSTM, then static TKAN, static LSTM, and finally static TLN, with this ordering remaining stable across all assets and market conditions.

\subsubsection{Impact of Optimization Functions}

In many optimization scenarios, quadratic loss is favored because its gradient decreases with the magnitude of the error, providing smoother parameter updates and facilitating convergence. In contrast, absolute loss maintains a constant gradient regardless of the error size, which may lead to instability in the updates as the same magnitude of adjustment is applied for both small and large errors. Surprisingly, experimental results indicate that models trained with absolute VWAP loss consistently outperform those trained with quadratic loss in out-of-sample testing, for both static and dynamic implementations. For example, in BTC experiments, dynamic TKAN models exhibited a 10.2\% improvement in absolute VWAP loss when trained with absolute loss rather than quadratic loss. Moreover, models trained with absolute loss not only achieved lower absolute VWAP loss but also produced better performance when evaluated using quadratic metrics.

\medskip

Dynamic models optimized for volume curve prediction achieved remarkable improvements in \(R^2\) scores compared to their static counterparts. For instance, with ETH, the dynamic LSTM attained an \(R^2\) of 0.56, compared to 0.13 for the static version—an improvement ranging from 200\% to 400\% across different assets. Despite this enhanced predictive power for the volume curve, such improvement does not directly translate into superior VWAP execution performance. In fact, models that directly optimize for the VWAP execution target, particularly when using absolute loss, yield the best overall execution results—even if their \(R^2\) scores for volume prediction are lower or negative (for example, an \(R^2\) of -0.25 for BTC in the dynamic TKAN model).

\medskip

It is possible that quadratic loss, by emphasizing larger errors, may overweight extreme events during training, leading to an excessive drift in the internal base volume curve and reduced robustness under market stress. In contrast, the constant gradient provided by absolute loss appears to offer a more stable training regime, which is reflected in better generalization to out-of-sample data. In summary, while quadratic loss typically offers smoother optimization dynamics, the robustness achieved by directly targeting absolute execution error appears to be more beneficial in the context of VWAP execution.


\subsubsection{Asset-Specific Performance}

Market structure and liquidity significantly influence the relative advantages of dynamic over static models. More liquid assets like BTC and ETH show the largest relative improvements when moving from static to dynamic approaches, with the performance enhancement being particularly stable across different market conditions. For less liquid assets like ADA and XRP, while the absolute performance remains weaker than for major assets, the relative improvement from static to dynamic models remains consistent, suggesting the proposed approach successfully adapts to varying market conditions.

\subsubsection{Computational Considerations}

The computational requirements of the different architectures warrant careful consideration, particularly with respect to their performance benefits. The transition from static to dynamic models introduces a modest increase in computational overhead, primarily because both approaches already incorporate sequential processing of the full lookback window through recurrent neural network components. For TKAN implementations, dynamic models require 150–220 seconds of training time compared to 115–170 seconds for static counterparts, representing an approximate 30\% increase. Similarly, dynamic LSTM models require 50–85 seconds compared to 35–65 seconds for static versions.

\medskip
A more significant computational distinction is observed when comparing recurrent architectures with the simple linear approach of TLN, which exhibits training times of only 5–8 seconds. This order-of-magnitude difference in computational requirements between TLN and recurrent implementations (both static and dynamic) represents the primary computational trade-off. However, the increased computational cost is justified by substantial performance benefits, as recurrent architectures reduce VWAP tracking errors by 15–25\% compared to TLN approaches.

\medskip

These computational considerations should be evaluated in the context of practical VWAP execution, where even marginal improvements in execution quality can result in significant financial benefits given typical transaction volumes in cryptocurrency markets. Furthermore, the observed training times remain within practical limits for regular model updates, even accounting for the increased complexity of recurrent architectures. The moderate additional overhead introduced by dynamic implementations over static ones appears justified given the resulting improvements in execution quality.

\subsubsection{Visual Analysis of Execution Performance}
Graphical analysis provides additional insights into the comparative performance of different VWAP execution approaches. Figures \ref{fig:dynamic_slippage_full} and \ref{fig:dynamic_slippage_subset} present a comparison of slippage between naive, static, and dynamic implementations across different time horizons. Figures \ref{fig:dynamic_slippage_diff_full} and \ref{fig:dynamic_slippage_diff_subset} highlight the relative performance improvements over the naive baseline.

\medskip

In Figure \ref{fig:dynamic_slippage_full}, examining the full sample period, a notable pattern emerges during periods of sharp price movements. The magnitude of slippage spikes demonstrates a clear hierarchy of performance: dynamic implementations consistently exhibit smaller deviations compared to static models, which in turn show reduced slippage relative to the naive approach. This pattern is particularly evident during significant market events, suggesting that more sophisticated models better maintain execution quality during challenging conditions.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_full_sample_comparison.jpg}
    \caption{Slippage between approaches on the full out-of-sample set}
    \label{fig:dynamic_slippage_full}
\end{figure}
However, the density of information in the full sample analysis can obscure performance differences during more stable market periods. To address this, Figure \ref{fig:dynamic_slippage_subset} provides a more granular view of execution performance. This analysis reveals a consistent pattern in which dynamic implementations not only achieve lower absolute slippage but also maintain this advantage consistently. The visualization clearly demonstrates that improvements in slippage reduction from dynamic models (compared to naive) consistently exceed those achieved by static implementations, providing visual confirmation of the quantitative metrics presented in the earlier tables.

To address potential obscuring of performance differences during more stable market periods, Figure \ref{fig:dynamic_slippage_subset} provides a more granular view of execution performance. This analysis reveals a consistent pattern in which dynamic implementations not only achieve lower absolute slippage but also maintain this advantage consistently. The visualization confirms that improvements in slippage reduction from dynamic models exceed those achieved by static implementations, aligning with the quantitative metrics presented earlier.


\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_2000_sample_comparison.jpg}
    \caption{Slippage between approaches on a subsample of the out-of-sample set}
    \label{fig:dynamic_slippage_subset}
\end{figure}
Figures \ref{fig:dynamic_slippage_diff_full} and \ref{fig:dynamic_slippage_diff_subset} quantify these improvements by showing the difference in absolute slippage versus the naive approach. The predominantly negative values for dynamic implementations (shown in red and blue) confirm systematic outperformance, with the magnitude of improvement often exceeding 0.005 (50 basis points) during challenging market conditions.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_full_sample_slippage.jpg}
    \caption{Difference in absolute slippage versus naive approach on the full out-of-sample set. Negative values indicate improved performance over the naive approach.}
    \label{fig:dynamic_slippage_diff_full}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{figures/vwap_analysis_2000_sample_slippage.jpg}
    \caption{Difference in absolute slippage versus naive approach on the subsample set. Negative values indicate improved performance over the naive approach.}
    \label{fig:dynamic_slippage_diff_subset}
\end{figure}

\subsection{Analysis of Extended Time Horizons}
The appendix Tables \ref{table:dynamic_48_step} and \ref{table:dynamic_6_step} provide insights into model performance across different execution horizons, comparing 48-step and 6-step ahead predictions with the baseline 12-step model. Several key patterns emerge from this analysis. The short-horizon results presented in Table \ref{table:dynamic_6_step} demonstrate notably improved performance across all metrics compared to longer horizons, with absolute VWAP losses typically 30-40\% lower than in the 48-step scenario shown in Table \ref{table:dynamic_48_step}. This suggests that shorter execution windows benefit more significantly from dynamic adaptation. The relative advantage of dynamic over static implementations remains consistent across time horizons, though the absolute magnitude of improvement decreases with longer horizons. This pattern indicates that while dynamic benefits persist, the inherent difficulty of longer-term prediction partially offsets the advantages of adaptive execution. Particularly noteworthy is the stability of model performance, as evidenced by the standard deviations reported in both tables. These remain proportionally consistent across horizons, suggesting that the dynamic implementations maintain reliable execution quality regardless of the prediction timeframe. This stability is particularly important for practical applications where execution horizons may vary based on order characteristics.

\subsubsection{Visual Analysis of Execution Performance}

The graphical analysis reveals several key patterns in the execution behavior across different time horizons and model architectures. For shorter prediction windows (6 and 12 steps ahead), static VWAP implementations using recurrent neural networks demonstrate notable variability around their mean execution curves. This flexibility in predictions appears to contribute to their improved performance compared to traditional approaches. This variability becomes even more pronounced in dynamic implementations, particularly in models optimized for volume curve prediction. The enhanced adaptability manifests as wider prediction bands around the mean execution trajectory, suggesting these models can better respond to changing market conditions while maintaining overall execution objectives.
A striking observation across all architectures is the consistency of average execution curves (shown in red). Despite the significant differences in model complexity and implementation approach, these curves maintain remarkably similar patterns. This convergence suggests that the dynamic approaches preserve the fundamental principles of optimal VWAP execution while enabling better adaptation to market conditions. The analysis of longer-horizon predictions (48 steps ahead) reveals an interesting constraint effect in dynamic models. Significant contraction of prediction bands around the mean can be observed, a direct consequence of the model formulation. The use of scaling factors bounded between 0 and 2 effectively limits the variability of predictions at longer horizons. This observation suggests that while the current model specification performs well for shorter time horizons, modifications may be necessary for optimal long-horizon performance. Specifically, the constraint mechanism might benefit from horizon-dependent scaling to maintain appropriate levels of adaptability across different execution timeframes. These visual patterns provide important insights into both the strengths of the dynamic approach and potential areas for future refinement, particularly in the handling of longer execution horizons. The consistency of mean execution patterns across architectures, combined with the enhanced adaptability of dynamic models at shorter horizons, supports the quantitative findings regarding the advantages of dynamic implementation while highlighting specific areas for potential improvement in model design.

\section{Conclusion}

This paper demonstrates that dynamic neural approaches represent a significant advancement in VWAP execution methodology, particularly in cryptocurrency markets characterized by high volatility and evolving microstructure. While my framework achieves substantial improvements without explicitly modeling market impact, its modular design allows for straightforward integration of impact considerations through modification of the loss function, opening interesting avenues for future research. The empirical results reveal several key insights about the role of recurrent neural architectures in execution problems. While the initial application of RNNs to static VWAP models showed only modest improvements over traditional approaches (3-5\% reduction in tracking error), their true potential emerges through dynamic implementation. By leveraging RNNs' inherent ability to maintain and update state information, the proposed dynamic framework achieves substantial performance gains, reducing absolute VWAP tracking error by 10-15\% in liquid markets and maintaining consistent outperformance across varying market conditions. This dramatic improvement in performance when moving from static to dynamic implementations suggests that the key to successful RNN application lies not in their raw predictive power, but in their ability to adapt to evolving market conditions. The computational considerations reinforce this insight about effective architectural choices. When applied directly, RNNs introduce substantial computational overhead compared to simple linear models like TLN - training times increase by an order of magnitude, from 5-8 seconds to over 150 seconds for TKAN implementations. However, these architectures prove essential as building blocks for our dynamic framework. The additional 30\% computational overhead introduced by the dynamic implementation represents a reasonable trade-off given the resulting 10-15\% improvement in execution quality. This demonstrates that the key to achieving favorable performance-to-cost ratios lies in designing frameworks that effectively leverage the unique capabilities of sophisticated neural architectures.

\medskip

The success of this dynamic framework opens several promising directions for future research. The integration of market impact modeling through modified loss functions could provide more realistic execution strategies for large orders. Additionally, extending the model to incorporate cross-asset dependencies and exploring alternative neural architectures could further enhance performance. Significant improvements could be achieved through more sophisticated feature engineering, moving beyond our intentionally basic approach used to demonstrate the framework's potential. Development of horizon-dependent scaling factors could also enhance the model's effectiveness across different execution timeframes. These results suggest that adaptive neural architectures can effectively address the challenges of modern VWAP execution while maintaining computational efficiency suitable for practical deployment. The framework's ability to balance execution quality with computational constraints makes it particularly valuable for institutional investors managing large orders in cryptocurrency markets. By implementing our model as a standard Keras architecture and making all code publicly available, I provide a foundation that practitioners can readily deploy and researchers can build upon. This research thus contributes to both the theoretical understanding of optimal execution and practical implementation of VWAP strategies, bridging the gap between academic research and industry practice in an increasingly important area of market microstructure.

\section*{Code Availability}
The source code used for all experiments and analyses in this paper is available at \url{https://github.com/remigenet/DeepDynamicVWAP}.

\newpage
\bibliographystyle{IEEEtran}
\bibliography{bib}

\newpage
\begin{appendix}
\input{appendix/main}
\end{appendix}

\end{document}
