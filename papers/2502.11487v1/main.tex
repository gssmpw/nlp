
\documentclass[sigconf]{acmart}

\usepackage{pifont}
\usepackage{tabularx}
\usepackage{enumerate,enumitem}
\usepackage{hyperref}
\usepackage{url}
\usepackage{array}
\usepackage{xcolor}

\begin{document}

\title{Non-Binary LDPC Arithmetic Error Correction For Processing-in-Memory}

\author{Daijing Shi$^{\triangle}$,
	Yihang Zhu$^{\triangle}$,
	Anjunyi Fan$^{\triangle}$,
	Yaoyu Tao$^{\triangle *}$,
	Yuchao Yang$^{\triangle\square*}$,
	and Bonan Yan$^{\triangle *}$
	}
\email{{taoyaoyutyy,yuchaoyang,bonanyan}@pku.edu.cn}
\affiliation{
\institution{$^{\triangle}$Peking University  $^{\square}$Chinese Institute for Brain Research (CIBR)}
\city{Beijing}
\country{China}
}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Shi et al.}

\begin{abstract}
Processing-in-memory (PIM) based on emerging devices such as memristors is more vulnerable to noise than traditional memories, due to the physical non-idealities and complex operations in analog domains. To ensure high reliability, efficient error-correcting code (ECC) is highly desired. However, state-of-the-art ECC schemes for PIM suffer drawbacks including dataflow interruptions, low code rates, and limited error correction patterns. In this work, we propose non-binary low-density parity-check (NB-LDPC) error correction running over the Galois field. Such NB-LDPC scheme with a long word length of 1024 bits can correct up to 8-bit errors with a code rate over 88\%. Nonbinary GF operations can support both memory mode and PIM mode even with multi-level memory cells. We fabricate a 40nm prototype PIM chip equipped with our proposed NB-LDPC scheme for validation purposes. Experiments show that PIM with NB-LDPC error correction demonstrates up to 59.65$\times$ bit error rate (BER) improvement over the original PIM without such error correction. The test chip delivers 2.978$\times$ power efficiency enhancement over prior works.
\end{abstract}

\maketitle
\section{Introduction}\label{sec:intro}

Processing-in-memory (PIM) technologies have by far reshaped the chip architecture design by enabling highly efficient in-/near-memory computations, especially for accelerating multiply-and-accumulation (MAC) kernels dominant in deep neural networks (DNNs)~\cite{zheng2024improving}. 
A handful of PIM designs have been developed based on different kinds of volatile memories such as static random-access memory (SRAM)~\cite{fu2023probabilistic}, dynamic random-access memory (DRAM)~\cite{kao2024ultra}. Recently, emerging nonvolatile memories such as memristors have shown great potential in further improving PIM's computing efficiency, including resistive random-access memory (RRAM)~\cite{wen2024fusion, yue2024scalable}, magnetic random-access memory (MRAM)~\cite{xie2024ed,zhang2024efficient} and phase-change memory (PCM)~\cite{le202364}. With PIM technologies, multiply-and-accumulation (MAC) operations can be executed inside the memory arrays by accumulating voltages~\cite{yoon202240,9731545}, charges~\cite{lee202328,aisy.201900068} or currents~\cite{huo2022computing,xie2024ed,8776485} along bitlines, etc. The overall computing efficiency is therefore boosted by avoiding repetitive data transfer between memories and external processing circuits.

Despite the advantages of PIM, the accumulation along bitlines introduces unavoidable non-idealities regardless of the underlying memory devices. The noise introduced by them is injected probabilistically into data stored in the memory cells and the output computing results~\cite{fan2012analysis,yue2024scalable,he2019noise, zhang2019design,yang2020retransformer,yue2025physical}, such as thermal noise and flickering noise.
In particular, emerging memories may introduce higher device variations and the resulting hard errors further deteriorate the bit error rates (BER) for processing in or near memories, which are easy to induce multi-bit errors (Fig.~\ref{fig:1}(a)).

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{fig/fig1.pdf}
	\caption{ (a) Noise degrades PIM computation accuracy. (b) Correction failures caused by limited supported error correction patterns.}
	\label{fig:1}
	\vspace{-2em}
\end{figure}

To address this unavoidable challenge, error-correcting codes (ECC) are the most commonly used method to attenuate noise corruptions, even though hardware-oriented retraining techniques have also been reported to mitigate the reliability problem of PIM towards DNN applications~\cite{chen2021pruning, shin2021fault, nguyen2021low, putra2021sparkxd, zhang2021efficient}. Integrating ECC with PIM computations requires the ECC to be compatible with PIM's intrinsic arithmetic operations. Existing methods can be roughly divided into two categories depending on whether the ECC needs to interrupt the PIM's computational dataflow. On the one hand, ECC that interrupts PIM computational dataflow usually detects errors and corrects the results by recursively reading the memory row by row~\cite{crafton2022improving}. On the other hand, existing ECCs without interrupting PIM computational dataflow need relatively large look-up tables to enable the decoding and error-correcting process~\cite{crafton2021cim, li202240nm}. Furthermore, existing methods are mainly based on short word lengths with relatively low code rates, which incur excessive area and power overheads due to the high parallelism required to support PIM processing. The error correction capability is by far up to 3-bit errors~\cite{crafton2022improving}. Due to the limited support of error correction patterns, existing methods also cannot support commonly used PIM design techniques, such as two-column differential weight mapping~\cite{xie2024ed, jing2022vsdca}.

In this work, we present a novel non-binary low-density parity-check (NB-LDPC) ECC scheme \cite{park2014fully,declercq2007decoding,2019automated, delima2024fault} optimized for PIM  designs to address the above challenges while achieving high code rate and energy efficiency. The main contributions of this work can be summarized as follows:
\begin{itemize}[leftmargin=*]
	\item We formulate a unified error detection and correction flow for PIM which can work in both conventional read/write mode and PIM mode. The proposed NB-LDPC is compatible with PIM MAC operations.
	\item We develop a novel NB-LDPC long-code-word ECC scheme, which is capable of correcting an arbitrary number of bit errors (depending on the iterative loops) without interrupting PIM computing dataflow.
	\item We tape out an RRAM-based PIM prototype chip monolithically integrated with the proposed NB-LDPC ECC module. To the best of our knowledge, this is the first attempt to realize PIM error correction with LDPC-type ECC. We also release the NB-LDPC decoder Verilog HDL implementation at \url{https://github.com/NoNameSubmission/NBLDPC_PIM}.
	%    \item ECC quantized decoding. In the proposed ECC iterative decoding process, the operations are all quantized into the integer field for power-saving and space-saving.
	%    \item Long word compilation. Since the proposed ECC can detect and correct multi-bit multi-level errors, the length of the codewords can be longer than the conventional ECC for PIM, which gives a higher code rate. In this design, we achieve a code rate of more than 88\% when the word length is 1024.
	%    \item Differential Operations Adaptation. Since the proposed ECC design is based on a special finite field generated by the modulus of a specific prime number, it can tolerate differential operations and multi-level digits, enlarging the tolerable error patterns.
\end{itemize}

In addition to silicon-proven results for NB-LDPC PIM, we further investigate the design space of NB-LDPC with PIM circuitries. Measurements from the fabricated chip and simulations for design space explorations validate the effectiveness of the proposed NB-LDPC scheme. The results show that NB-LDPC ECC design improves the ECC power efficiency for PIM by 2.978$\times$ over existing designs in \cite{crafton2022improving,chang202240nm}. Furthermore, the NB-LDPC ECC is capable of achieving a code rate of more than 88\% with 1024-bit word length and correcting at most 8 error bits.

\section{Preliminaries}\label{sec:background}
\subsection{Processing-In-Memory Design}\label{sec:CIM}
Fig.~\ref{fig:1}(a) illustrates the fundamental PIM macro structure, including input drivers (address controllers), computing units, and analog-to-digital converters (ADCs). 
The PIM computation can be abstracted as:
\begin{equation}
	\mathbf{X}_{1\times n}\cdot\mathbf{W}_{n\times m}=\mathbf{Y}_{1\times m}
	\label{eq:0}
\end{equation}
where $\mathbf{X}$ is the input vector, $\mathbf{W}$ is the weight matrix, and $\mathbf{Y}$ is the output vector, $n$ and $m$ denotes the dimensions.
Before computation, the weight matrix \textbf{W} is binary-encoded and stored in memory arrays. The input vector of VMM operations is fed into PIM through input drivers. With a bit-serial scheme, each bit of the input elements is applied to one WL per clock cycle, controlling the on/off states of the memory cells. The computing results are then accumulated as analog signals on BLs. These signals are sampled and quantized by the ADCs for further operations such as shift-and-add~\cite{lee202328,chang202240nm} to generate the final digital outputs. During this computation process, the output analog signals are prone to noise, leakage current from SRAM cells, non-idealities of RRAM cells, or process variations, etc. (Fig.~\ref{fig:1}(a)), degrading the BER of the computing results. 

\subsection{Existing ECC designs for PIM}

%With the development of Artificial Intelligence (AI), the MAC operations and storage data required by a specific neural network grows rapidly. Considering the large amount of computing, the requirements of computing precision as well as computing accuracy are also raised. Thus, ECC designs focusing on CIM operations are put forward by researchers.
Existing ECC designs can be categorized into two classes, one requiring dataflow interruption while the other one does not. For methods without dataflow interruption, they usually employ lookup tables (LUTs) for error detection and correction. Crafton, \textit{et al.}~\cite{crafton2021cim} propose a revised Hamming code that enables error correction of $\pm1$ bit in specific codeword. Li, \textit{et al.}~\cite{li202240nm} utilizes the modulo operation to compute the encoded word and the output error syndromes. However, large LUTs are difficult to scale up for longer word length or higher code rates, limiting the practical adoptions of these methods. On the other hand, ECC designs for PIM that require dataflow interruptions include ~\cite{crafton2022improving} that detects the error columns of the output words and corrects errors through repeatedly reading the information stored in the memory array. This method achieves good BER performance for RRAM parallel computing but adds extra timing overheads to PIM dataflow.

Fig.~\ref{fig:1}(b) shows the limited error patterns that can be corrected by the existing ECC designs, which usually consider the $\pm1$ error patterns. Thus, these kinds of ECC exhibit efficient error correction performance for a short word length, for example, 32 data bits. However, for a large-scale neural network, large memory capacity brings non-negligible space consumption for short-word-length ECC decoders and check bits. When differential operations are applied, the minimum length of the data bits is 2, covering ``-1'', ``0'', and ``1''. Since the existing methods consider only binary elements, they are difficult to process non-binary codewords. 

\section{Unified ECC For Memory \& PIM Modes}\label{Design}
\subsection{PIM Error Detection With NB-LDPC}\label{sec:Adapt}

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{fig/fig3.pdf}
	\caption{(a) Illustration of ECC operational flow for memories and PIM MAC. (b)The components of the generation matrix $\mathbf{H}_G$ and the hardware deployment of the encoded data bits and check bits (redundancy).}
	\label{fig:3}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=2.0\columnwidth]{fig/fig4.pdf}
	\caption{
		(a) The overall scheme of the NB-LDPC decoder.
		(b) LLV initialization process.
		(c) FBP algorithm.
		(d) Error Correction process in the VNs.
	}
	\label{fig:4}
\end{figure*}

We present an NB-LDPC coding design as a unified ECC scheme for both the memory mode and the PIM mode.
Fig.~\ref{fig:3}(a) shows the decoding flow of the proposed ECC. The NB-LDPC code can be described by a generation matrix $\textbf{H}_G$ and a check matrix $\textbf{H}_C$.
Their elements are all in a predefined Galois field of order $p$, i.e. \textit{GF}($p$).
The name of NB-LDPC originates from \textit{GF}($p$) when $p>2$, where the elements of $\mathbf{H}_G$ and $\mathbf{H}_C$ are not limited to binary elements.
$\textbf{H}_G$ and $\textbf{H}_C$ can be generated by the LDPC construction methods, e.g. PEG~\cite{venkiah2008design} and PCEG~\cite{han2020minimum}. 
$\mathbf{H}_G$ is an $m\times m$ identity matrix concatenated with an $m\times l$ matrix for the check bits generation (Fig.~\ref{fig:3}(b)). $\mathbf{H}_C$ is thereby generated to satisfy:
\begin{equation}
	\mathbf{H}_{G}\cdot\mathbf{H}_{C}^\intercal = \mathbf{0}
	\label{eq:1}
\end{equation}
where $\mathbf{0}$ is the zero matrix. Subsequently, we formulate the workflow of NB-LDPC for PIM designs.

\textbf{\textit{Memory Mode Error Detection}:} When sensing $m$ original data bits and forming a vector\footnote{Here the data bits can be binary bits or non-binary numbers in \textit{GF}(p).} $\textbf{w}^{1\times m}$ (the superscript denotes its dimension), the actual data that are stored in the PIM array is ${\mathbf{w'}}^{1\times l}=\mathbf{w}^{1\times m}\cdot\mathbf{H}_G^{m\times l}$ (Fig.~\ref{fig:3}(b)).
$l$ is the vector length of data bits plus check bits, thus $l>m$.
Note that the code rate is defined as $\frac{m}{l}\le 1$ where $m$ is the number of data bits.
To detect errors, a checking operation is defined as multiplying the codeword under test by the check matrix, i.e. $\left(\mathbf{w'}\cdot\mathbf{H}_C^\intercal\right)$. There are no errors detected if and only if:
\begin{equation}
	{\mathbf{w'}}^{1\times l}\cdot\left(\mathbf{H}_C^{m\times l}\right)^\intercal=\mathbf{0}
	\label{eq:2}
\end{equation}

\textbf{\textit{PIM Mode Error Detection}:} In PIM MAC operations with the NB-LDPC code, both MAC for data bits and check bits are performed. Eq.~\ref{eq:0} is revised to involve the check bits:
\begin{equation}
	\mathbf{X}^{1\times n}\cdot\mathbf{W'}^{n\times l}=\mathbf{Y}'^{1\times l}
	\label{eq:3}
\end{equation}
Thanks to the linearity, $\mathbf{Y}'^{1\times l}$ still satisfies the check matrix operation that yields a zero matrix \textbf{if and only if} there is no error in the PIM MAC results (Eq.~\ref{eq:4}). If the check matrix operation leads to non-zero results, error bits are detected.
\begin{equation}
	\mathbf{Y}'^{1\times l}\cdot(\mathbf{H}_C^{m\times l})^\intercal=\mathbf{X}^{1\times n}\cdot\mathbf{W'}^{n\times l}\cdot(\mathbf{H}_C^{m\times l})^\intercal=\mathbf{0}
	\label{eq:4}
\end{equation}

\subsection{PIM Error Correction With NB-LDPC}\label{sec:IntDecode}
If error bits are detected, the NB-LDPC decoder will then try to correct them.
Fig.~\ref{fig:4}(a) depicts the error correction process of the proposed NB-LDPC scheme.
It consists of variable nodes (VNs), check nodes (CNs), and the interconnections between them that are determined by the check matrix $\mathbf{H}_C$. 
VNs process the received codewords from the PIM array (i.e. $\mathbf{Y}'^{1\times l}$) and determine the final corrected output. CNs calculate the error syndrome and execute the iterative operations to progressively correct and decode the words. The decoding process of the proposed NB-LDPC consists of three steps:

\subsubsection{Logarithmic Likelihood Value Initialization}\label{sec:Init}

When an encoded codeword $\mathbf{Y}'^{1\times l}$ is fed into the NB-LDPC ECC decoder, for each bit, a group of logarithmic likelihood values (LLVs) that stands for the confidence levels of input being each element $k\in$\textit{GF}($p$) is computed inside each VN. The LLV of element $k$ is defined as the logarithmic value of the posterior probability that the correct computing result is $k$.
In this design, we simplify LLV computation as the one-dimensional Manhattan distance from the element $k$ to the original input code bit (Fig.~\ref{fig:4}(b)).
Based on our design for the prototype chip (Sec.~\ref{sec:hi}), this tradeoff incurs slight BER degradation from $3.57\times10^{-6}$ to $4.98\times10^{-6}$ but saves 21.65\% area and 23.69\% power consumption compared with design using full precision computation for LLV computations.

\subsubsection{Forward-Backward Propagation}\label{sec:FBP}

The LLVs generated in Sec.~\ref{sec:Init} are stored in VNs as the prior LLVs to start the iterative decoding process. To start the iteration, the prior LLVs in VNs are treated as the temporal LLVs of the 0$^{\mathrm{th}}$ iteration.
The temporal LLVs are then sent to CNs according to $\mathbf{H}_{C}$ acting as the adjacency matrix.
During the sending process, the LLVs of the VNs are sorted by the element of the traveling paths.
Specifically, the LLV corresponding to element $k \in $\textit{GF}($p$) from the i$^\mathrm{th}$ VN will be stored in the $(k\cdot H_{C_{ij}})^\mathrm{th}$ LLV of the j$^\mathrm{th}$ CN (Eq.~\ref{eq:6}) using the following index function:
\begin{equation}
	\mathbf{LLV}_{j}[k]=\mathbf{LLV}_{i}[k\cdot H_{C_{ij}}]
	\label{eq:6}
\end{equation}
$\mathbf{LLV}$ is the iterated vector of LLVs, $H_{C_{ij}}$ is the ($i$,$j$)$^{\mathrm{th}}$ element of $\mathbf{H}_C$.
In CNs, the received LLVs will be used to detect errors and generate information for temporal LLVs update in VNs.
An algorithm called Forward-Backward Propagation (FBP) is applied in the CNs~\cite{davey1998low,wymeersch2004log}.
FBP is to generate a new LLV group for each VN$_i$ connecting to the same CN, which can tell the specific VN what the other VNs think the correct code bit of it should be.
FBP is done in two steps:

\textbf{Step 1-Internal Propagation:}
In each CN, the LLVs from different VNs are transmitted in two directions to generate forward messages (FM) and backward messages (BM).
The propagation process shown in Fig.~\ref{fig:4}(c) takes two groups of LLVs as input.
In this step of the $i$-th propagation, the input will be \textbf{FM}$_{i-1}$ or \textbf{BM}$_{i-1}$ and \textbf{LLV}$_{i}$ or \textbf{LLV}$_{D_{C}-i+1}$ depending on the directions, respectively.
These two groups of LLVs are first ``added'' in the logarithmic domain to form a new group, corresponding to the multiplication of probabilities.
The addition result of the LLVs for each element $k$ is determined by the maximum LLV summation of the possible choices (Eq.~\ref{eq:8}).
% For example, element 2 can be the summation of “1+1” or “2+0”, corresponding to 3 possible combinations
% LLVs of these combinations are computed while only the largest among them will be kept and selected as the final LLV value.
\begin{equation}
	\mathbf{LLV}_{o}[k]=\max\{j\in \textit{GF}(p)\mid \mathbf{LLV}_{A}[k-j]+\mathbf{LLV}_{B}[j]\}
	\label{eq:8}
\end{equation}
where $\mathbf{LLV}_{A}$, $\mathbf{LLV}_{B}$ are the input of the propagation module, and $\mathbf{LLV}_{o}$ is the output of it. 
The generated LLVs of each element are then normalized to prevent the accumulation effect by subtracting the LLV of element ``0'' from all of the values in the group.
After that, the LLVs are reflected to its reverse element in \textit{GF}($p$), which will be the LLV group of the next FM or BM.
Completing the generation of all FMs and BMs starts the following \emph{Step 2}.

\textbf{Step 2-External Propagation:}
As shown in Fig.~\ref{fig:4}(c), external propagation is based on the previously generated FMs and BMs.
The i$^\mathrm{th}$ LLV$'$ in the CN is propagated from the FM$_{i-1}$ and BM$_{D_C-i}$ for temporal LLV updating in VN$_i$.
As mentioned in \emph{Step 1}, in this operation, the i$^\mathrm{th}$ LLV$'$ is generated by all the VNs connected to the CN except from i$^\mathrm{th}$ VN.
Thus, the final $\mathbf{LLV}'_i$ transmitted back to VN$_i$ eliminates the information from the specific node, preventing the repeated strengthening of the prior LLVs.
Transmission of the LLV$'$s from CNs to VNs is according to Eq.~\ref{eq:6}, which stands for the end of operations in CNs in this iteration.

\subsubsection{Accumulative Error Correction}\label{sec:Correct}

After receiving the LLV$'$s from corresponding CNs, the VNs will start updating the temporal LLV groups determining the final output of the decoder in this iteration.
The prior LLVs generated by the initialization process are added to the LLV$'$s sent back to the VNs by a normal operation instead of that in Sec.~\ref{sec:FBP}.
The element corresponding to the largest LLV in this updated group stands for the final corrected result in \textit{GF}($p$) (Fig.~\ref{fig:4}(d)).

After decoding the codeword in \textit{GF}($p$), the corrected computing results of the PIM units are subsequently interpreted.
The interpreted word is the one that has the shortest 1D Manhattan distance to the determined code bit in this digit.
% For example, if the decoded results out of the NB-LDPC decoder at a certain digit is $1$ and its original input is $2$, the correct output of this digit will be interpreted as $1$.


\subsection{NB-LDPC Compatibility With Various PIM Schemes}
We develop the proposed PIM NB-LDPC ECC scheme in consideration of supporting multi-level cell memory and multi-bit integer-based PIM MAC arithmetic because the proposed NB-LDPC arithmetic code is built on non-binary \textit{GF}($p$) (where $p$ is a prime number). As a specific case, the widely-used differential weight mapping technique~\cite{xie2024ed, jing2022vsdca} for analog-computing PIM can be regarded as a ternary element utilizing the characteristics of modulo operations on negative numbers. Compared with conventional ECC for PIM that only supports binary elements, the proposed design paves the way to implement error correction for general PIM designs. 
%Further, with the development of emerging memory devices, the multi-level compatibility of the proposed ECC can support future reliability improvement of the PIM cores based on multi-level cells.
% --another version of this paragraph:
% Our proposed PIM NB-LDPC ECC scheme is designed specifically to support multi-level cell memory and multi-bit integer arithmetic within PIM architecture. This flexibility stems from the fact that our NB-LDPC arithmetic code utilizes the non-binary Galois field GF($p$), where $p$ is a prime number. As an example, NB-LDPC supports the widely used differential weight mapping technique~\cite{xie2024ed, jing2022vsdca} for analog computing PIM. This technique effectively represents data as ternary elements by leveraging modulo operations on negative numbers.  The proposed NB-LDPC breaks away from conventional PIM ECC designs, which are limited to binary elements. By supporting non-binary arithmetic, NB-LDPC paves the way for error correction in a broader range of PIM architectures.

\section{Circuit Architecture of NB-LDPC}\label{sec:circuit}
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{fig/fig5.pdf}
	\caption{(a) Digital circuit architecture of the proposed NB-LDPC decoder. (b) The PIM memory architectures of multiple PIM cores sharing one ECC decoder. (c) NB-LDPC area overhead in scaled-out PIM memory architectures.}
	\label{fig:5}
\end{figure}

Fig.~\ref{fig:5}(a) shows the circuit architecture of the proposed NB-LDPC arithmetic code. Porting from PIM cores, the proposed NB-LDPC mainly includes an input scheduler, VN and CN processing units, and multiplexers determined by $\mathbf{H}_C$. The design parameters are presented in Table~\ref{tab:0}. 
To conserve chip space, hardware implementations frequently use only partial CNs and VNs, reusing them across time steps.($N_{VI}$, $N_{VA}$) and  ($N_{CI}$, $N_{CA}$) represent the number of VNs and CNs realized in hardware and defined in the NB-LDPC algorithm, respectively. $D_V$ and $D_C$ denote the sparse connectivity of each node in VNs and CNs to each other defined in $\mathbf{H}_C$.

\begin{table}[t]
	\caption{Parameter List for PIM+NB-LDPC}
	\begin{center}
		\begin{tabularx}{1\columnwidth} { 
				>{\centering\hsize=.13\hsize\linewidth=\hsize}X 
				| >{\hsize=.60\hsize\linewidth=\hsize}X 
				| >{\hsize=.14\hsize\linewidth=\hsize}X}
			\hline
			\hline
			Parameter & Meaning & Typical Values\\
			\hline
			$N_P$ & Number of PIM cores sharing 1 ECC decoder & 1$\sim$64\\
			\hline
			$C_P$ & Column parallelism$^\dagger$ of each PIM core & 1$\sim$1024\\
			\hline
			$N_{VI}$ & Number of VNs for iterative decoding & 32$\sim$1024\\
			\hline
			$N_{VA}$ & Number of VNs designed in the algorithm & 32$\sim$1024\\
			\hline
			$N_{CI}$ & Number of CNs for iterative decoding & 1$\sim$16\\
			\hline
			$N_{CA}$ & Number of CNs designed in the algorithm & 4$\sim$512\\
			\hline
			$D_{V}$ & VNs connection degrees & 2$\sim$4\\
			\hline
			$D_{C}$ & CNs connection degrees & 6$\sim$18\\
			\hline
			\hline
			\multicolumn{3}{l}{$^\dagger$Column parallelism is defined as the number of ADCs in a PIM macro.}
		\end{tabularx}
	\end{center}
	\label{tab:0}
\end{table}

The principle above for NB-LDPC error correction forms a datapath as follows: 
\ding{182} The generated codewords from the $N_P$ PIM cores (i.e. PIM MAC computing results) with column parallelism $C_P$ are firstly fed into the input schedulers. The input scheduler buffers and generates the codewords in multiple cycles in a predefined order. They are then sent to the VNs for LLV initialization where the initialized prior LLVs are stored (Sec.~\ref{sec:Init}).  
\ding{183} Transmission between the VNs and CNs is defined by $\mathbf{H}_C$ fixed as hardware connections of multiplexers. According to the ratio of $N_{P}C_{P}/N_{VI}$ and $N_{CI}/N_{CA}$, the LLV initialization cycles and the required cycles of the iterative decoding can be determined, respectively.
\ding{184} The LLVs transmitted to CNs then go through the FBP algorithm. This propagation process is realized by a dedicated computing module embedded inside the CNs to compute both FM or BM and generate LLV$'$s (Sec.~\ref{sec:FBP}). Note that error detection is naturally executed in CNs in that the summation of the codewords is naturally completed during the propagation process. 
\ding{185} At the end of the propagation, the FM or BM is compared with the last or the first LLV group sent by the VNs. Once the maximum element of the 2 groups is the same, elements in this check node pass the error detection. If all the check nodes pass the error detection process, the decoding iteration is over. Otherwise, the LLV$'$s should be sent back to the VNs and start the next iteration. The final codeword generation is carried out in the VNs (Sec.~\ref{sec:Correct}).

\section{Hardware Prototype of PIM With NB-LDPC}\label{sec:hi}

To validate the effectiveness and efficiency of the proposed NB-LDPC, we design and fabricate a prototype chip in a commercial 40nm technology node. It monolithically integrates an RRAM PIM macro with 2.5-bit flash ADCs and a proposed NB-LDPC ECC module. A 256$\times$320 binary 1-RRAM-1-transistor (1T1R) array, namely $N_P=1$ is embedded as the PIM core of the design, with $C_{P}=10$ using Flash ADCs to convert and quantize the analog computing results. For the NB-LDPC realization, we use \textit{GF}(3), resulting in each check bit length of 2 bits. The on-chip NB-LDPC decoder contains 288 VNs ($N_{VI}=288$) and 1 CN ($N_{CI}=1$) to compromise the throughput of the PIM core. Since $N_{VI}=N_{VA}$, LLV initialization and arrangement are controlled by digital logic. The operations of the CNs and transmission between VNs and CNs are then controlled by a finite state machine (FSM) and multiplexers. Besides, the design is also facilitated with a debug port for debug codeword input. We release the proposed PIM NB-LDPC ECC implementation source code in Verilog HDL in \url{https://github.com/NoNameSubmission/NBLDPC_PIM}.

\section{Evaluation}\label{sec:Evaluate}

\subsection{Experiment \& Simulation Setup}\label{sec:setup}

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{fig/fig7.pdf}
	\caption{(a) Our prototype chip: RRAM PIM \& monolithically integrated NB-LDPC decoder @ 40nm technology node and chip test environments. (b) \textbf{Measured} Shmoo plot of the prototype chip. (c) \textbf{Measured} average power consumption of NB-LDPC module solely (excluding RRAM \& ADCs).}
	\label{fig:7}
\end{figure}

Evaluation of the proposed PIM NB-LDPC includes the measurement of the prototype chip and the cycle-accurate simulation for design exploration.
\ding{182} \textbf{Chip Measurement:}
as shown in Fig.~\ref{fig:7}(a), the presented testing chip is embedded into a test board for the experiment.
An FPGA DE0-CV development board is applied as the signal controller of the test chip.
The debug port of the chip is connected to the PC using an I$^2$C /SPI host adapter working in the I$^2$C mode for data transportation.
% Measurement of the test chip is carried out by the Keysight 34470A digit multimeter.
To measure the power consumption of the ECC decoder, we first measure the power consumption of the digital circuits working in the decoding process.
Then, the power consumption of the data transportation port and the digital debug buffer is measured under the same condition.
The power consumption of the proposed ECC design is obtained by subtracting the power consumption of debug circuits from that of the total chip.
\ding{183} \textbf{Design Space Exploration Simulation:}
In addition to the prototype chip as one possible implementation, this work also explores the large design space. To explore the influence of the design parameters in Table~\ref{tab:0}, we synthesize the decoder circuits with different parameters and perform Verilog-based cycle-accurate post-synthesis simulation.
Further, we simulate the influence of the NB-LDPC algorithmic parameters on the BER improvement under various implementations.
This algorithm-level simulation is done using Numpy in Python.
The construction of the check matrix and generator matrix is based on existing coding algorithmic works~\cite{han2020minimum,venkiah2008design}.
The specific value of these non-zero elements of $\mathbf{H}_C$ are randomly picked from the non-zero value in \textit{GF}($p$) where the NB-LDPC ECC for PIM is built.
%The upper limit of the code rate is also determined according to these works.

The BER performance of the NB-LDPC arithmetic code is measured and simulated for different cases.
The hardware-implemented results of the design under different raw BER are measured utilizing the debug port mentioned in Sec.~\ref{sec:hi}.
Other experiments evaluating the influence of code rates and word lengths are demonstrated through simulations.
To show the performance of the proposed ECC design in large-scale neural network applications, a simulation of ResNet-34 on ImageNet dataset~\cite{he2016deep} is carried out with the proposed ECC.
With the help of ~\cite{zhou2016dorefa, liu2020reactnet}, we quantize the DNN weights and activating values of the first and the last layer in the model to 8-bit to fit the PIM scheme, while the other layers are quantized to ternary weight and binary inputs.

\begin{table}[t]
	\caption{Comparison of PIM ECC Designs}
	\centering
	\begin{tabularx}{1\columnwidth}{ 
			>{\centering\hsize=.28\hsize\linewidth=\hsize}X 
			| >{\hsize=.16\hsize\linewidth=\hsize}X 
			| >{\hsize=.08\hsize\linewidth=\hsize}X
			| >{\hsize=.08\hsize\linewidth=\hsize}X
			| >{\hsize=.20\hsize\linewidth=\hsize}X
			| >{\hsize=.16\hsize\linewidth=\hsize}X
		}
		\hline
		\hline
		Work & Row Parallelism & MWL$^\dagger$ (bits) & MTE$^\ddagger$ (bits) & ECC Efficiency (Mbps/W)$^\triangle$ & Efficiency Improvement\\
		\hline
		\textbf{This work} & \textbf{Arbitrary} & \textbf{256} & \textbf{5} & \textbf{1152.00}$^{*}$ & 2.978$\times$\\
		\hline
		DAC'22$^{\text{~\cite{chang202240nm,crafton2022improving}}}$ & 8 & 32 & 3 & 386.82 & 1$\times$\\
		\hline
		ASSCC'21$^{\text{~\cite{crafton2021cim}}}$ & 4 & 32 & 1 & 35.92 & 0.093$\times$\\
		\hline
		ESSCIRC'22$^{\text{~\cite{li202240nm}}}$& 7 & 25 & 1 & 88.47 & 0.229$\times$\\
		\hline
		\hline
		\multicolumn{6}{l}{$^{\dagger}$ MWL means Maximum Word Length}\\
		\multicolumn{6}{l}{$^{\ddagger}$ MTE means Maximum Tolerable Errors in the output word}\\
		\multicolumn{6}{l}{$^{\triangle}$ ECC power efficiency is computed through $\frac{\text{Corrected Bit Rate}}{\text{Power}}$}\\
		\multicolumn{6}{l}{$^{*}$ Power for comparison is measured under row parallelism of 4}\\
	\end{tabularx}
	\label{tab:1}
	
\end{table}

\subsection{Measured Prototype Chip ECC Hardware Performance}\label{sec:measure}
As shown in Fig.~\ref{fig:7}(b), the performance of the proposed design at a word length of 256 bits and 80\% code rate is measured using the prototype chip mentioned in Sec.~\ref{sec:hi}.
The working states are measured from 1V to 1.2V, with the ECC decoder continuously working.
The main clock frequency of the prototype chip varies from 58MHz to 95MHz, validating the feasibility of the proposed NB-LDPC ECC for PIM.
Fig.~\ref{fig:7}(c) shows the mean NB-LDPC power consumption (x-axis, average from 1000 times measurement) under different clock frequencies (y-axis).
The data points in Fig.~\ref{fig:7}(b) correspond to the cases of the highest clock frequency under different supply voltages in the Shmoo plot.
With increasing clock frequency, the power consumption of the ECC decoder shows a growing trend.
The jitter of the measured curve is caused by the measurement method mentioned in Section~\ref{sec:setup}.
Since the power consumption of the debugging module is much larger than the decoder in our prototype chip, the measured results are influenced by the precision of the measurement.

``ECC power efficiency'' is defined as the corrected data bits throughput per unit power consumption~\cite{ferraz2021survey}.
Of our chip, the best ECC power efficiency point is measured at a 1.07V supply and 71MHz working frequency, resulting in 1152.00Mbps/W, as shown in Table~\ref{tab:1}.
For comparison, we provide the simulation or measured results reported by other works. The proposed design successfully improves the ECC efficiency by at most 2.978$\times$ and 2.382$\times$ on average of all cases compared with the best existing ECC for PIM~\cite{crafton2022improving}.
In addition, Table~\ref{tab:1} presents the advantage of the design that it has no requirements for row parallelism (i.e., how many rows are simultaneously turned on for PIM VMM) and supports a longer word length with multi-bit error-correcting ability.

\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{fig/fig9.pdf}
	\caption{
		(a) NB-LDPC code performance with the same code rate but different word lengths.
		(b) NB-LDPC code performance with different code rates but the same word length.
		(c) DNN (ResNet-34) accuracy benchmark on PIM with or without the ECC, compared with the full precision results.
	}
	\label{fig:9}
\end{figure}

As shown in Fig.~\ref{fig:5}(c), sharing the ECC decoder among multiple cores can release the area cost to a large degree.
With 6 PIM macros sharing one decoder, the area penalty of the decoder will fall to lower than 25\%, which is lower than the reported results in ~\cite{crafton2022improving}.

\subsection{Simulated NB-LDPC Arithmetic Code Performance}\label{sec:sim}

Fig.~\ref{fig:9} verifies the error-correcting capability of NB-LDPC under different BERs.
The x-axis and y-axis are the pre- and post-ECC BERs, respectively.
Fig.~\ref{fig:9}(a) shows the impact of different NB-LDPC word lengths on the coding performance.
Besides the prototype chip (256-bit codeword), we generate multiple NB-LDPC codes with word lengths varying from 32 to 1024 (i.e. different $\mathbf{H}_G$ \& $\mathbf{H}_C$ pairs) with a fixed 80\% code rate for comparison.
% to compare with the existing works.
Generally, in our design, shorter codes yield worse error correction performance than the longer ones under the same input BER.
Longer codewords reduce the likelihood of encountering specific dilemmas that can lead to error correction failures, a problem also seen in conventional LDPC designs.
The best BER improvement is achieved with a word length of 1024 which improves the BER by 59.65$\times$ from $1\times10^{-5}$ to $1.676\times 10^{-7}$.
Theoretically, the upper limit of the code rate can be up to 88\% with an information word length of 1024, but it has a trade-off with the BER performance.
Fig.~\ref{fig:9}(b) shows the influence of different NB-LDPC code rates on the coding performance.
The word length is fixed as 512 and the code rate varies from 0.33 to 0.8.
%It is obvious that with a lower code rate, the redundancy of the total code will be higher and the correction will be easier.
The results reveal that lower code rates inherently increase the redundancy of the code, making error correction more effective at the cost of more decoding overhead. 

To evaluate ECC benefits for an end-to-end PIM-based DNN computation, we simulate ResNet-34 computation with PIM executing its MAC.
Fig.~\ref{fig:9}(c) shows the simulated BER improvement using realistic PIM with and without NB-LDPC ECC.
The fault model is simplified and abstracted to a fixed probability of bit flip rate (x-axis) during computation ranging from $10^{-3}$ to $10^{-5}$.
The error might happen in both weights and activations.
The y-axis denotes the final ResNet-34 classification accuracy over the test dataset.
Results reveal that NB-LDPC brings significant improvement when BER is worse than $10^{-4}$.
With a BER at $1\times10^{-3}$, the performance of the neural network decreases by an absolute value of 38.54\%.
NB-LDPC (1024-bit codeword, 80\% code rate) significantly reduces the BER to $4.14\times10^{-4}$ and recovers the DNN classification accuracy by 20.5\%.

\subsection{Design Space Exploration for NB-LDPC Hardware}
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{fig/fig10.pdf}
	\caption{Simulated design space exploration for different parameters: (a) Power efficiency of NB-LDPC decoder, (b) figure of merit of NB-LDPC decoder.}
	\label{fig:10}
\end{figure}

As mentioned in Sec.~\ref{sec:circuit}, $N_{P}C_{P}/N_{VI}$ (implying the processing speed of the VN group) and $N_{CI}/N_{CA}$ (indicating the processing speed of CN group) are the two most important factors influencing the decode speed and power consumption.
Here we take the classical value of $N_{P}=4, C_{P}=10$ for design space exploration.
The code design is fixed and the same as in our silicon-proven prototype described in Sec.~\ref{sec:hi}.
Fig.~\ref{fig:10}(a) presents the influence of $N_{P}C_{P}/N_{VI}$ and $N_{CI}/N_{CA}$ on the ECC power efficiency.
Using the pre-determined parameters in the software algorithm, these two parameters can be changed into $\beta N_{P}C_{P}/N_{VI}$ and $N_{CI}$, where $\beta$ is the ratio of the VNs and the specific partial sums that need to be computed to generate an encoded codeword. In our simulated experiments, $\beta =(N_{VA}+N_{CA}) / (N_{VA}+2N_{CA})$ since each check bit requires 2 bits.
As can be seen from each group of the bar with the same $N_{CI}$, the highest power efficiency of the decoder always occurs when $\beta N_{P}C_{P}/N_{VI}=1$.
This is logical because no hardware will be suspended during the initialization process and thus the highest power efficiency should be achieved. 
Similarly, the highest power efficiency can be achieved when $N_{CI}=16$, namely $N_{CI}/N_{CA}=1$ in the simulation condition.

Further, the influences of these parameters on the overall figure of merit (FoM) of this design are explored.
The selected FoM is the ratio of ECC power efficiency over the NB-LDPC circuits area, which attempts to find the Pareto optimality of ECC power efficiency and the area overhead considering the BER of the whole system will not be affected by these hardware-implemented parameters.
Fig.~\ref{fig:10}(b) presents the simulated FoM results (y-axis) versus different $N_{CI}$ (x-axis).
As can be seen, there lies a sweet point to reach a co-optimal point for efficiency and design (area) overhead to maintain a high error correction capability.
The highest FoM value is achieved when $N_{CI}=8$, $\beta N_{P}C_{P}/N_{VI}=1$.
This is because of the higher cost of the CN processing unit than VN.
The former is 61.83$\times$ larger than the latter according to the synthesis results.

\section{Conclusion}\label{sec:conclude}
In this paper, we develop a novel NB-LDPC ECC for PIM that aims at high power efficiency and high code rate. With silicon-proven architecture and circuit designs, the power efficiency of the proposed NB-LDPC decoder is measured to be 2.978$\times$ better than the existing ECC designs~\cite{crafton2022improving}. Exploration and analysis of the proposed ECC design shows that, with the ability to detect and correct multiple errors in a word, a 1024-bit codeword can achieve a code rate of more than 88\% according to existing mathematical analysis. Simulation results show the proposed design can achieve 59.65$\times$ BER improvement compared with raw results with a word length of 1024 and 80\% code rate.


\bibliographystyle{ACM-Reference-Format}
\bibliography{refs.bib}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
