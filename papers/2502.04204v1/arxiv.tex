\documentclass[10pt]{article}
\usepackage{graphicx} % Required for inserting images



\usepackage[round]{natbib}
\usepackage{newtxtext}
\usepackage{xcolor}
\usepackage{bm}
\usepackage[parfill]{parskip}


\usepackage{geometry}
\geometry{letterpaper, top=1in, bottom=1in, right=1in, left=1in}

\newcommand{\argmax}{\mathop{\arg\max}}




\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{wrapfig}




\definecolor{mydarkblue}{rgb}{0,0.08,0.45} % from ICML template
\usepackage[colorlinks=true, allcolors=mydarkblue]{hyperref}
\hypersetup{bookmarksnumbered=true, bookmarksopen=true}



\usepackage{apptools}
\AtAppendix{\counterwithin{theorem}{section}}
\AtAppendix{\counterwithin{lemma}{section}}
\AtAppendix{\counterwithin{corollary}{section}}
\AtAppendix{\counterwithin{equation}{section}}
\AtAppendix{\counterwithin{definition}{section}}
\AtAppendix{\counterwithin{claim}{section}}



\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
% \theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}


\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}



\usepackage{upgreek}

\newcommand{\E}{\mathop{\mathbb{E}}}





\author{
Shaopeng Fu\footnotemark[1] %\\ \href{mailto:shaopeng.fu@kaust.edu.sa}{shaopeng.fu@kaust.edu.sa}
\and
Liang Ding\footnotemark[2]
\and
Di Wang\footnotemark[1] %\\ \href{mailto:di.wang@kaust.edu.sa}{di.wang@kaust.edu.sa} \\
}

\date{\vspace{-0.4em}}
% \date{King Abdullah University of Science and Technology, Thuwal 23955, KSA \\ \{shaopeng.fu, di.wang\}@kaust.edu.sa}



\title{``Short-length'' Adversarial Training Helps LLMs \\ Defend ``Long-length'' Jailbreak Attacks: \\ Theoretical and Empirical Evidence}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\begin{document}


\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{S.~Fu and D.~Wang are with the Division of Computer, Electrical and Mathematical Science and Engineering (CEMSE) at the King Abdullah University of Science and Technology, Thuwal 23955, KSA. Email: \href{mailto:shaopeng.fu@kaust.edu.sa}{\texttt{shaopeng.fu@kaust.edu.sa}} and \href{mailto:di.wang@kaust.edu.sa}{\texttt{di.wang@kaust.edu.sa}}.}
\footnotetext[2]{L.~Ding is with The University of Sydney, Australia. Email: \href{mailto:liangding.liam@gmail.com}{\texttt{liangding.liam@gmail.com}}.}


\begin{abstract}
Jailbreak attacks against large language models (LLMs) aim to induce harmful behaviors in LLMs through carefully crafted adversarial prompts.
To mitigate attacks, one way is to perform adversarial training (AT)-based alignment, {\it i.e.}, training LLMs on some of the most adversarial prompts to help them learn how to behave safely under attacks.
During AT, the length of adversarial prompts plays a critical role in the robustness of aligned LLMs.
This paper focuses on adversarial suffix jailbreak attacks and unveils that to defend against a jailbreak attack with an adversarial suffix of length $\Theta(M)$, it is enough to align LLMs on prompts with adversarial suffixes of length $\Theta(\sqrt{M})$. Theoretically,
we  analyze the adversarial in-context learning of linear transformers on linear regression tasks and prove a robust generalization bound for trained transformers.
The bound depends on the term $\Theta(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the number of adversarially perturbed in-context samples during training and testing.
Empirically, we conduct AT on popular open-source LLMs and evaluate their robustness against jailbreak attacks of different adversarial suffix lengths.
Results confirm a positive correlation between the attack success rate and the ratio of the square root of the adversarial suffix during jailbreaking to the length during AT.
Our findings show that it is practical to defend ``long-length'' jailbreak attacks via efficient ``short-length'' AT.
The code is available at \url{https://github.com/fshp971/adv-icl}.
\end{abstract}



% \title{ICL Draft}



\section{Introduction}


Large language models (LLMs)~\citep{brown2020language,touvron2023llama,liu2024deepseek,yang2024qwen2} have been widely integrated into various real-world applications to assist human users, but their safety is found to be vulnerable toward jailbreak attacks~\citep{wei2023jailbroken}.
With carefully crafted adversarial prompts, one can ``jailbreak'' the safety mechanism of LLMs and induce arbitrary harmful behaviors~\citep{zou2023universal,chao2023jailbreaking,liu2024autodan}.
To address this challenge, recent studies~\citep{xhonneux2024efficient,mazeika2024harmbench,yu2024robust,casper2024defending} have proposed performing safety alignment through adversarial training~(AT)~\citep{madry2018towards} to enhance LLMs' robustness against jailbreaking.
A standard AT for LLMs would train them on harmful adversarial prompts synthesized by strong jailbreak attacks to learn to refuse these harmful instructions~\citep{mazeika2024harmbench}.



In such AT, the length of synthesized adversarial prompts used for model training is critical to the final jailbreak robustness of LLMs.
\citet{anil2024many} and \citet{xu2024bag} have shown that longer adversarial prompts enjoy stronger jailbreaking abilities.
Thus, it is reasonable to deduce that performing AT with longer adversarial prompts can help LLMs achieve stronger robustness to defend against ``long-length'' jailbreak attacks.
However, synthesizing long-length adversarial prompts in adversarial training is usually time-consuming since it requires solving discrete optimization problems in high-dimensional spaces.
This may limit the application of AT in LLMs' safety alignment and further raises the following research question:
\begin{quote}
\centering
\it
How will the adversarial prompt length during AT affect trained LLMs' robustness against jailbreaking with different prompt lengths?
\end{quote}


This paper studies this research question by analyzing {\it suffix jailbreak attacks}, where each jailbreak prompt is constructed by concatenating a harmful instruction with a synthesized adversarial suffix.
Our main finding is:
{\bf To defend against a suffix jailbreak attack with suffix length of $\Theta(M)$, it is enough to adversarially train LLMs on adversarial prompts with suffix length of only $\Theta(\sqrt{M})$.}
In other words, we show that it is possible to defend long-length jailbreak attacks via efficient short-length AT.



Our finding is supported by {\it theoretical} and {\it empirical} evidence.
Theoretically, we leverage the {\it in-context learning theory}~\citep{von2023transformers,zhang2024trained} to investigate how linear transformers learn linear regression tasks from in-context task samples under AT.
To better simulate suffix jailbreak attacks in real-world LLMs, our analysis introduces a new {\it in-context adversarial attack}.
Concretely, for any in-context task sample, this attack will adversarially perturb the last several in-context training points to maximize the squared prediction error that linear transformers made on the in-context test point.
Under our theoretical framework, we prove a robust generalization bound for adversarially trained linear transformers.
This bound has a positive correlation with the term $\Theta(\sqrt{M_{\text{test}}} / M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the number of perturbed in-context points in training and testing in-context task samples, respectively.







Empirically, we conduct AT with the GCG attack~\citep{zou2023universal}, one of the most effective jailbreak attacks, under various adversarial suffix lengths on five popular real-world LLMs and evaluate their robustness against jailbreak attacks with different adversarial suffix lengths.
We use the jailbreak attack success rate (ASR) to express the robust generalization error of trained LLMs and find that this ASR has a clear positive correlation with the ratio of the square root of test-time adversarial suffix length to the AT adversarial suffix length.
Such a correlation empirically verifies our main finding.
We also find that AT with an adversarial suffix (token) length of $20$ is already able to reduce the ASR of jailbreak attacks with an adversarial suffix (token) length of up to $120$ by at least $30\%$ in all experiments.




\section{Related Works}
\label{sec:related-works}



\textbf{Jailbreak attacks.}
Jailbreaking~\citep{wei2023jailbroken} can be seen as adversarial attacks~\citep{szegedy2014intriguing,goodfellow2015explaining} toward LLMs, which aim to synthesize adversarial prompts to induce targeted harmful behaviors from LLMs.
Many efforts have been made on token-level jailbreak attacks, {\it i.e.}, searching adversarial prompts in the token space of LLMs, which can be achieved via gradient-based optimization~\citep{shin2020autoprompt,guo2021gradientbased,zou2023universal,liao2024amplegcg,schwinn2024soft}, heuristic greedy search~\citep{sadasivan2024fast,hayase2024querybased,jin2024jailbreaking}, or fine-tuning prompt generators from pre-trained LLMs~\citep{paulus2024advprompter}.
Other attempts include word-level adversarial prompt searching~\citep{liu2024autodan} or directly prompting LLMs to generate adversarial prompts~\citep{chao2023jailbreaking,liu2024turbo}.
Our work focuses on token-level jailbreaking since it make it easier for us to control the adversarial prompt length for our analysis.



More recent studies have found that increasing the length of adversarial prompts by adding more harmful demonstrations~\citep{anil2024many} or synthesizing longer adversarial suffixes~\citep{xu2024bag} can make jailbreaking more effective.
These works motivate us to investigate the problem of defending against ``long-length'' jailbreak attacks.






\textbf{Adversarial training on LLMs.}
To defend against jailbreak attacks, a large body of studies focus on aligning LLMs to refuse responding jailbreak prompts~\citep{ouyang2022training,rafailov2023direct,qi2024safety,qi2024finetuning,chen2024aligning}.
More recent works have started to adopt adversarial training~(AT)~\citep{madry2018towards} to align LLMs.
\citet{mazeika2024harmbench} trained LLMs on (discrete) adversarial prompts synthesized by GCG attack~\citep{zou2023universal}, in which they cached the intermediate synthesized results to reduce the heavy cost of searching adversarial prompts from scratch.
Meanwhile, various studies~\citep{xhonneux2024efficient,casper2024defending,sheshadri2024latent,yu2024robust} conduct AT with adversarial examples found in the continuous embedding space rather than the discrete text space since searching in the continuous embedding space is more computationally efficient.
Nevertheless, as a preliminary study of the length of adversarial prompts during AT, our work only analyzes AT with discrete adversarial prompts.



\textbf{In-context learning theory (ICL).}
Transformer-based large models like LLMs are strong in performing ICL:
Given a series of inputs (also known as ``prompt'') specified by a certain task, LLMs can make predictions well for this certain task without adjusting model parameters.
Current theories in understanding ICL can be roughly divided into two categories.
The first category aims to understand ICL via constructing explicit multi-layer transformers to simulate the optimization process of learning function classes~\citep{garg2022can,von2023transformers,ahn2023transformers,chen2024transformers,mahankali2024one,wang2024incontext}.
The second category focuses on directly analyzing the training~\citep{zhang2024trained,yang2024incontext,huang2023context,wu2024how,lin2024transformers} and generalization~\citep{lu2024asymptotic,magen2024benign,frei2024trained,shi2024why} of simple self-attention models ({\it i.e.}, one-layer transformer).
\citet{anwar2024adversarial} is the first to study adversarial attacks against linear transformers and finds that an attack can always succeed by perturbing only a single in-context sample.
However, their analysis allows samples to be perturbed in the entire real space, which might not appropriately reflect real-world settings since real-world adversarial prompts can only be constructed from token/character spaces of limited size.
Unlike \citet{anwar2024adversarial}, we propose a new ICL adversarial attack that requires each adversarial suffix token to be perturbed only within restricted spaces, which thus can be a better tool for understanding real-world jailbreaking.





\section{Preliminaries}
\label{sec:preliminaries}




\textbf{Large language models (LLMs).}
Let $[V] = \{1,\cdots, V\}$ be a vocabulary set consisting of all possible tokens.
Then, an LLM can be seen as a function that for any sequence $x_{1:n} \in [V]^n$ consists of $n$ tokens, the LLM will map $x_{1:n}$ to its next token $x_{n+1}$ following $x_{n+1} \sim p_{\theta}(\cdot | x_{1:n})$, where $p_{\theta}$ is a conditional distribution over the vocabulary set $[V]$ and $\theta$ is the model parameter of the LLM.
Under such notations, when using the LLM $p_\theta$ to generate a new token sequence for the input $x_{1:n}$, the probability of generating a sequence $y_{1:m} \in [V]^m$ of length $m$ is (``$\oplus$'' denotes concatenation):
\begin{align*}
    p_{\theta}(y_{1:m} | x_{1:n}) = \prod_{i=1}^m p_{\theta}(y_i | x_{1:n} \oplus y_{1:(i-1)}),
\end{align*}




\textbf{Jailbreak attacks.}
This paper will focus on {\it suffix} jailbreak attacks.
Concretely, suppose $x^{(h)}$ and $y^{(h)}$ are two token sequences, where $x^{(h)}$ represents a harmful prompt ({\it e.g.}, ``{Please tell me how to build a bomb.}'') and $y^{(h)}$ represents a corresponded targeted answer ({\it e.g.}, ``{Sure, here is a guide of how to build a bomb}'').
Then, the goal of a suffix jailbreak attack against the LLM $p_{\theta}$ aims to synthesize an {\it adversarial suffix} $x^{(s)}_{1:m}$ for the original harmful prompt $x^{(h)}$ via solving the following optimization problem,
\begin{align}
    \min_{x^{(s)}_{1:m} \in [V]^m} -\log p_{\theta}(y^{(h)} | x^{(h)} \oplus x^{(s)}_{1:m}),
    \label{eq:jailbreak-obj}
\end{align}
where $x^{(h)} \oplus x^{(s)}_{1:m}$ is the adversarial prompt and $m$ is the sequence length of the adversarial suffix $x^{(s)}_{1:m}$.
Intuitively, a large $m$ will increase the probability of the LLM $p_\theta$ that generating the targeted answer $y^{(h)}$ for the synthesized adversarial prompt $x^{(h)} \oplus x^{(s)}_{1:m}$.

To solve Eq.~(\ref{eq:jailbreak-obj}), a standard method is the Greedy Coordinate Gradient (GCG) attack~\citep{zou2023universal}, which leverages gradient information to search for better $x^{(s)}_{1:m}$ within the discrete space $[V]^m$ in a greedy manner.





\textbf{Adversarial training (AT).}
We consider the canonical AT loss $\mathcal L$~\cite{mazeika2024harmbench,qi2024safety} to train the LLM $p_{\theta}$, which consists of two sub-losses: an {\it adversarial loss} $\mathcal L_{\text{adv}}$ and an {\it utility loss} $\mathcal L_{\text{utility}}$.

Specifically, given a  {\it safety dataset} $D^{(h)}$, where each of its sample $(x^{(h)}, y^{(h)}, y^{(b)}) \in D^{(h)}$ consists of a harmful instruction $x^{(h)}$, a harmful answer $y^{(h)}$, and a {\it benign answer} $y^{(b)}$ ({\it e.g.}, ``As a responsible AI, I can't tell you how to...''). The adversarial loss $\mathcal L_{\text{adv}}$ is defined as follows,
\begin{align}
    \mathcal L_{\text{adv}}(\theta, M, D^{(h)}) %\nonumber \\
    := \E_{(x^{(h)},y^{(h)},y^{(b)}) \in D^{(h)}} [-\log p_{\theta}(y^{(b)} | x^{(h)} \oplus x_{1:m}^{(s)})],
    \label{eq:loss-at:term-adv}
\end{align}
where $x^{(s)}_{1:m}$ is the adversarial suffix obtained from Eq.~(\ref{eq:jailbreak-obj}) and $m$ is the adversarial suffix length.
Note that the probability terms in Eqs.~(\ref{eq:jailbreak-obj}) and~(\ref{eq:loss-at:term-adv}) look similar to each other.
The difference is that the term in Eq.~(\ref{eq:jailbreak-obj}) denotes the probability that $p_\theta$ generates the harmful answer $y^{(h)}$ for the adversarial prompt, while that in Eq.~(\ref{eq:loss-at:term-adv}) denotes the probability of generating the benign answer $y^{(b)}$.

Besides, let $D^{(u)}$ be a {\it utility dataset} where each of its sample $(x^{(u)}, y^{(u)}) \in D^{(u)}$ consists of a pair of normal instruction and answer.
Then, the utility loss $\mathcal L_{\text{utility}}$ is given by
\begin{align*}
    \mathcal L_{\text{utility}}(\theta, D^{(u)}) %\nonumber\\
    := \E_{(x^{(u)}, y^{(u)}) \in D^{(u)}} [ -\log p_{\theta}(y^{(u)} | x^{(u)} ) ].
\end{align*}
Thus, the overall AT optimization problem for improving the jailbreak robustness of the LLM $p_\theta$ is given as follows,
\begin{align}
    \min_{\theta} \{\alpha \mathcal L_{\text{adv}}(\theta,M,D^{(h)})
    + (1-\alpha) \mathcal L_{\text{utility}}(\theta,D^{(u)}) \},
    \label{eq:loss-at}
\end{align}
where $\alpha \in [0,1]$ is a factor that balances between the adversarial and utility sub-losses.
The idea behind such a loss design is that:
(1)~help LLM learn to respond harmlessly even when strong jailbreak prompts present (achieved via $\mathcal L_{\text{adv}}$),
(2)~retain the utility of LLM gained from pre-training (achieved via $\mathcal L_{\text{utility}}$).
Intuitively, a larger adversarial suffix length $m$ during AT will help the LLM gain robustness against jailbreak attacks with longer adversarial suffixes.




\section{Theoretical Evidence}
\label{sec:adv-icl}




This section establishes the theoretical foundation of how ``short-length'' AT can defend against ``long-length'' jailbreaking.
Our analysis is based on the in-context learning (ICL) theory~\citep{zhang2024trained,shi2024why,anwar2024adversarial}, and we will  bridge the ICL theory and the LLM AT problem defined in Eq.~(\ref{eq:loss-at}) later.
Here we first introduce the necessary notations to describe the problem.
To avoid confusion, we note that {\bf all notations in this section will only be used within this section and have no relevance to those in other sections} ({\it e.g.}, Section~\ref{sec:preliminaries}).



\textbf{In-context learning (ICL).}
In the ICL theory, a {\it prompt} with length $N$ related to a specific {\it task} indexed by $\tau$ is defined as $(x_{\tau,1}, y_{\tau,1}, \cdots, x_{\tau,N}, y_{\tau,N}, x_{\tau,q})$, where $x_{\tau,i} \in \mathbb{R}^d$ is the $i$-th in-context training sample (demonstration), $y_{\tau,i} \in \mathbb{R}$ is the label for the $i$-th training sample, and $x_{\tau,q} \in \mathbb{R}^d$ is the in-context query sample.
Then, the embedding matrix $E_\tau$ for this task-related prompt is defined as follows,
\begin{align}
    E_\tau := \begin{pmatrix}
        x_{\tau,1} & \cdots & x_{\tau,N} & x_{\tau,q} \\
        y_{\tau,1} & \cdots & y_{\tau,N} & 0 \\
    \end{pmatrix} \in \mathbb{R}^{(d+1) \times (N+1)}.
    \label{eq:icl:embedding}
\end{align}
Given a prompt embedding matrix $E_\tau$ of task $\tau$, the goal of an ICL model is to make a prediction based on $E_\tau$ for the query sample $x_{\tau,q}$.
Such an ICL model design aims to model the ability of real-world LLMs in making decisions based on prompting without updating model parameters.


\textbf{Linear self-attention (LSA) models.}
LSA models are a kind of linear transformer that has been widely adopted in existing theoretical ICL studies.
\citet{ahn2024linear} empirically show that LSA models share similar properties with non-linear ones and thus are useful for understanding transformers.
We follow \citet{zhang2024trained} to study the following single-layer LSA model,
\begin{align*}
    f_{\text{LSA},\theta}(E_\tau) %\nonumber \\
    := \left[ E_{\tau} + W^V E_\tau \cdot \frac{E_\tau^\top W^{KQ} E_{\tau}}{N} \right]
    \in \mathbb{R}^{(d+1)\times (N+1)}
\end{align*}
where $\theta := (W^V, W^{KQ})$ is the model parameter, $W^V \in \mathbb{R}^{(d+1) \times (d+1)}$ is the value weight matrix, $W^{KQ} \in \mathbb{R}^{(d+1)\times (d+1)}$ is a matrix merged from the key and query weight matrices of attention models, $E_\tau \in \mathbb{R}^{(d+1) \times (N+1)}$ is the prompt embedding matrix, and $N$ is the prompt length.
The model prediction $\hat y_{q,\theta}$ for the query sample $x_{\tau,q}$ is given by the right-bottom entry of the output matrix of the LSA model, {\it i.e.}, $\hat y_{q,\theta}(E_\tau) := f_{\text{LSA},\theta}(E_{\tau})_{(d+1),(N+1)}$.

We further follow \citet{zhang2024trained} to denote that
\begin{align*}
    W^V    = \begin{pmatrix} W^V_{11} & w^V_{12} \\ (w^V_{21})^\top & w^V_{22} \end{pmatrix} \in \mathbb{R}^{(d+1)\times (d+1)},
    \ \
    W^{KQ} = \begin{pmatrix} W^{KQ}_{11} & w^{KQ}_{12} \\ (w^{KQ}_{21})^\top & w^{KQ}_{22} \end{pmatrix} \in \mathbb{R}^{(d+1)\times (d+1)},
\end{align*}
where
$W^V_{11}, W^{KQ}_{11} \in \mathbb{R}^{d\times d}$,
$w^V_{12}, w^V_{21}, w^{KQ}_{12}, w^{KQ}_{21} \in \mathbb{R}^{d\times 1}$
and $w^{V}_{22}, W^{KQ}_{22} \in \mathbb{R}$.
Under this setting, the model prediction $\hat y_{q,\theta}$ can be further simplified as follows,
\begin{align}
    \hat y_{q,\theta}(E_\tau) := f_{\text{LSA},\theta}(E_{\tau})_{(d+1)\times(N+1)} %\nonumber \\
    = \begin{pmatrix}(w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot \frac{E_\tau E_\tau^\top}{N} \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} \cdot x_{\tau,q}.
    \label{eq:icl:lsa-prediction}
\end{align}

\textbf{Other notations.}
We denote $[n] := \{1, \cdots, n\}$ for any $n \in \mathbb{N}^+$.
For any matrix $A \in \mathbb{R}^{n\times m}$, we denotes
$\|A\|_{2,\infty} := \max_{1\leq i \leq m} \|A_{i,:}\|_2$,
$\|A\|_2$ be the operator norm,
and $\|A\|_F$ be the Frobenius norm.
$\mathrm{Tr}(A) := \sum_{i=1}^n A_{i,i}$ is the trace function for any matrix $A \in \mathbb{R}^{n\times n}$.
Finally, we use standard big O notations $\mathcal O(\cdot)$ and $\Theta(\cdot)$.





\subsection{Problem Definition for Adversarial ICL}


We now formalize the AT problem in ICL with the previously introduced notations.
Concretely, we focus on the linear regression task and introduce a novel in-context ``suffix'' adversarial attack, where in-context adversarial points are appended to the end of in-context prompts, to analyze the robustness of LSA models.



\textbf{Data distribution and statistical model.}
For any task indexed by $\tau$, we assume that there is a task weight $w_\tau\in \mathbb{R}^d$ drawn from $w_{\tau} \sim \mathcal N(0, I_{d})$.
Besides, for any in-context training point $x_{\tau,i} \ (1\leq i \leq N)$ and the query point $x_{\tau,q}$ (see Eq.~(\ref{eq:icl:embedding})), we assume that they are drawn from $x_{\tau,i}, x_{\tau,q} \sim \mathcal N(0, \Lambda)$, where $\Lambda \in \mathbb{R}^{d\times d}$ is a positive-definite covariance matrix.
Moreover, the ground-truth labels of training points $x_{\tau,i}$ and the query point $x_{\tau,q}$ are given by $y_{\tau,i} = w_\tau^\top x_{\tau,i}$ and $y_{\tau,q} = w_\tau^\top x_{\tau,q}$.




\textbf{ICL (suffix) adversarial attack.} % \& robust error.}
Our novel adversarial attack against ICL models is launched via concatenating (clean) prompt embedding matrices with adversarial embedding suffixes.
Specifically, for a prompt embedding matrix $E_\tau$ of length $N$ (see Eq.~(\ref{eq:icl:embedding})), we will form its corresponding adversarial prompt embedding matrix $E^{\text{adv}}_{\tau,M} \in \mathbb{R}^{(d+1) \times (N+M+1)}$ by concatenating $E_{\tau}$ with an adversarial suffix of length $M$ as follows,
\begin{align}
    E^{\text{adv}}_{\tau,M} %\nonumber \\
    &:= \begin{pmatrix}
        \underbrace{ \begin{pmatrix} X_{\tau} \\ Y_{\tau} \end{pmatrix} }_{\shortstack{\tiny Training Data \\ \tiny of Length $N$}}
        &
        \underbrace{ \begin{pmatrix} X^{\text{sfx}}_{\tau} + \Delta_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} }_{\shortstack{\tiny Adversarial Suffix \\ \tiny of Length $M$}}
        &
        \underbrace{ \begin{pmatrix} x_{\tau,q} \\ 0 \end{pmatrix} }_{\shortstack{\tiny Query Sample \\ \tiny From $E_\tau$}}
    \end{pmatrix},
\end{align}
where
\begin{align*}
    \left\{\begin{aligned}
    X_{\tau} &:= \begin{pmatrix}
        x_{\tau,1} & \cdots & x_{\tau,N}
    \end{pmatrix} \in \mathbb{R}^{d \times N}
    % \label{eq:icl:train-point}
    \\
    Y_{\tau} &:= \begin{pmatrix}
        y_{\tau,1} & \cdots & y_{\tau,N}
    \end{pmatrix} \in \mathbb{R}^{1 \times N}
    % \label{eq:icl:train-label}
    \end{aligned}\right.
\end{align*}
denote the $N$ original training samples and labels, and
\begin{align*}
    \left\{\begin{aligned}
    X^{\text{sfx}}_{\tau} &:= \begin{pmatrix}
        x^{\text{sfx}}_{\tau,1} & \cdots & x^{\text{sfx}}_{\tau,M}
    \end{pmatrix} \in \mathbb{R}^{d \times M} \\
    Y^{\text{sfx}}_{\tau} &:= \begin{pmatrix}
        y^{\text{sfx}}_{\tau,1} & \cdots & y^{\text{sfx}}_{\tau,M}
    \end{pmatrix} \in \mathbb{R}^{1 \times M} \\
    \Delta^{\text{sfx}}_{\tau} &:= \begin{pmatrix}
        \delta_{\tau,1} & \cdots & \delta_{\tau,M}
    \end{pmatrix} \in \mathbb{R}^{d \times M}
    \end{aligned}\right.
\end{align*}
denotes the new $M$ clean suffix samples, clean suffix labels, and adversarial perturbations.
The clean suffix samples $X^{\text{sfx}}_\tau$ and labels $Y^{\text{sfx}}_\tau$ here follow the same distribution as those in-context data in the embedding $E_\tau$, {\it i.e.}, $x^{\text{sfx}}_{\tau,i} \sim \mathcal N(0,\Lambda)$ and $y^{\text{sfx}}_{\tau,i} = w_\tau^\top x^{\text{sfx}}_{\tau,i}$ hold for every $i \in [M]$.
For the adversarial perturbation matrix $\Delta_\tau$, we require each perturbation $\delta_{\tau,i}$ is restricted within a ball-sphere as $\|\delta_{\tau,i}\|_2 \leq \epsilon$, where $\epsilon > 0$ is the perturbation radius.
This aims to simulate that in jailbreak attacks, and each adversarial token is searched within a token vocabulary set of limited size.

The goal of the ICL adversarial attack is to add an optimal suffix adversarial perturbation matrix $\Delta_\tau$ to maximize the difference between the model prediction $\hat y_q(E^{\text{adv}}_{\tau})$ based on the adversarial prompt embedding matrix $E^{\text{adv}}_\tau$ and the ground-truth query label $y_{\tau,q}$.
We adopt the squared loss to measure such a prediction difference, which thus leads to the robust generalization error for the model $f^{\text{LSA}}_{\theta}$ as follows,
\begin{align}
    &\mathcal R^{\text{adv}}(\theta,M)
    % := \E_{w_\tau, X_{\tau}, X^{\text{sfx}}_{\tau}}
    = \E_{\tau} \max_{\|\Delta_\tau^\top\|_{2,\infty} \leq \epsilon} \frac{1}{2} | \hat y_{q,\theta}(E^{\text{adv}}_{\tau,M}) - y_{\tau,q} |^2,
    \label{eq:icl:robust-error}
\end{align}
where $M$ is the length of the adversarial suffix and the expectation $\E_\tau[\cdot]$ is calculated over the randomness of $w_\tau$, $X_\tau$, $X^{\text{sfx}}_\tau$, and $x_{\tau,q}$.
Since this paper aims to understand how the adversarial prompt length in AT would affect the robustness of LLM, Eq.~(\ref{eq:icl:robust-error}) will also only focus on how the adversarial suffix length $M$ in ICL adversarial attacks would affect the robust generalization error $\mathcal R^{\text{adv}}(\theta,M)$.





\textbf{Adversarial in-context learning.}
Following previous studies on minimax AT~\citep{madry2018towards,javanmard2020precise,ribeiro2023regularization,fu2024theoretical,wang2024benign}, here we also adopt a minimax AT loss to train the LSA model.
Concretely, we first use the aforementioned ICL adversarial attack to synthesize adversarial prompts and then update the LSA model based on these adversarial prompts to help the model gain robustness against adversarial prompts.
We further assume that the adversarial suffix length is fixed during AT, which thus leads to the following AT optimization problem formalization,
\begin{align}
    \min_{\theta}\mathcal L^{\text{adv}}(\theta)
    := \min_{\theta} \mathcal R^{\text{adv}}(\theta,M_{\text{train}}) %\nonumber\\
    = \min_{\theta} \left\{ \E_{\tau} \max_{\|\Delta_\tau^\top\|_{2,\infty} \leq \epsilon} \frac{1}{2} | \hat y_{q,\theta}(E^{\text{adv}}_{\tau,M_{\text{train}}}) - y_{\tau,q} |^2 \right\},
    \label{eq:icl:at-loss}
\end{align}
where $\mathcal L^{\text{adv}}(\theta) := \mathcal R^{\text{adv}}(\theta, M_{\text{train}})$ is the AT loss in ICL and $M_{\text{train}} \in \mathbb{N}^+$ is the fixed adversarial suffix length during AT.
We will perform AT with continuous gradient flow, and further following \citet{zhang2024trained} to make the following assumption on the LSA model parameter initialization.
\begin{assumption}[c.f. Assumption~3.3 in \citet{zhang2024trained}]
\label{ass:icl:init}
Let $\sigma > 0$ be a parameter and $\Theta \in \mathbb{R}^{d\times d}$ be any matrix satisfying $\|\Theta \Theta^\top\|_F = 1$ and $\Theta \Lambda \neq 0_{d\times d}$.
We assume that
\begin{align*}
    W^{V}(0) = \left(\begin{matrix} 0_{d\times d} & 0_{d\times 1} \\ 0_{1\times d} & \sigma \end{matrix}\right),
    \ \
    W^{KQ}(0) = \left(\begin{matrix} \sigma \Theta \Theta^\top & 0_{d\times 1} \\ 0_{1\times d} & 0 \end{matrix}\right).
\end{align*}
\end{assumption}
Recall in Eq.~(\ref{eq:icl:lsa-prediction}), $w^V_{12}$, $w^{KQ}_{12}$, and $w^{KQ}_{22}$ do not contribute to the model prediction function $\hat{y}_{q,\theta}(\cdot)$.
Thus, Assumption~\ref{ass:icl:init} directly sets them to be zero at initialization.
To ensure symmetric initialization, Assumption~\ref{ass:icl:init} further sets $w^V_{21}(0)$ and $w^{KQ}_{21}(0)$ to zero.
In the next section, we will see how Assumption~\ref{ass:icl:init} helps simplify the analysis of ICL AT.



\textbf{Bridging ICL AT and LLM AT.}
Finally, we explain the similarities between AT on ICL models and LLMs to motivate why ICL AT~({\it i.e.}, Eq.~(\ref{eq:icl:at-loss})) can be a good artifact to theoretically understand LLM AT~({\it i.e.}, Eq.~(\ref{eq:loss-at})).

We first compare the ICL suffix adversarial attack in Eq.~(\ref{eq:icl:robust-error}) with the LLM suffix jailbreak attack in Eq.~(\ref{eq:jailbreak-obj}).
We find that their attack goals are similar since both attacks aim to make targeted models behave wrongly via manipulating suffixes of input prompts.
The only difference is that jailbreak attacks aim to induce LLMs to generate specified harmful content while our ICL attack aims to maximize linear regression prediction errors made by ICL models.
Besides, unlike \citet{anwar2024adversarial}, which performs ICL attacks by perturbing a single in-context sample in the entire real space, our attack allows perturbing multiple in-context samples but only within restricted spaces, thus better simulating how LLM jailbreak attacks allow adversarial token suffixes to be searched only in the limited token vocabulary set.




We then compare the ICL AT problem in Eq.~(\ref{eq:icl:at-loss}) with the LLM AT problem in Eq.~(\ref{eq:loss-at}).
One can find that the motivations behind the two AT problems are the same, which is to enhance models' robustness by training them on adversarial prompts.
However, we notice that the LLM AT problem introduces an additional utility loss to maintain the performance of LLMs on benign data. This is because in LLM jailbreak attacks, adversarial prompts would be crafted only from harmful prompts but not benign ones.
We argue that this discrepancy has little impact on our theoretical analysis, as both our theory and experiments focus on studying how adversarially trained models can defend against adversarial prompts rather than their performance on benign data.




\subsection{Training Dynamics of Adversarial ICL}

We now start to analyze the training dynamics of the minimax ICL AT problem formalized in Eq.~(\ref{eq:icl:at-loss}).


The main technical challenge is that to solve the inner maximization problem in Eq.~(\ref{eq:icl:at-loss}), one needs to analyze the optimization of the adversarial perturbation matrix $\Delta_\tau$.
However, the matrix $\Delta_\tau$ along with the clean data embedding $E_\tau$ and the clean adversarial suffix $(X^{\text{sfx}}_{\tau}, Y^{\text{sfx}}_{\tau})$ are entangled together within the adversarial embedding matrix $E^{\text{adv}}_{\tau,M_{\text{train}}}$, which makes it very difficult to solve the inner maximization problem and further analyze the ICL AT dynamics.

To tackle such a challenge, we propose to instead study the dynamics of a {\it closed-form upper bound} of the original AT loss $\mathcal L^{\text{adv}}(\theta)$.
Formally, we will analyze the following surrogate AT problem for the LSA model $f_{\text{LSA},\theta}$:
\begin{align}
    \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)
    := \min_{\theta} \Bigl\{ \sum_{i=1}^4 \ell_i(\theta) \Bigr\},
    \label{eq:icl:surrogate-at-loss}
\end{align}
where
$ \tilde{\mathcal L}^{\text{adv}}(\theta) := \sum_{i=1}^4 \ell_i(\theta)$
is the surrogate AT loss, and
\begin{align*}
    & E^{\text{clean}}_{\tau,M_{\text{train}}} = \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix},
    \nonumber\\
    & \ell_1(\theta) = 2 \E_\tau \Bigl[
        ((w^V_{21})^\top \ \ w^V_{22} )  \frac{ E^{\text{clean}}_{\tau,M_{\text{train}}} E^{\text{clean} \top}_{\tau,M_{\text{train}}} }{ N + M_{\text{train}} }  \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} - y_{\tau,q} \Bigr]^2,
    \nonumber\\
    &\ell_2(\theta) = \frac{2 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \|w^{V}_{21}\|_2^2 \E_{\tau} \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr],
    \nonumber\\
    &\ell_3(\theta) = \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \E_\tau \Bigl[ \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \| ((w^V_{21})^\top \ \ w^V_{22} )  \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \|_2^2 \Bigr],
    \nonumber\\
    &\ell_4(\theta) = \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 }  \|w^{V}_{21}\|_2^2  \cdot \E_\tau \Bigl[ \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \Bigr].
\end{align*}
In the surrogate AT problem defined as Eq.~(\ref{eq:icl:surrogate-at-loss}), the surrogate AT loss function $\tilde{\mathcal L}^{\text{adv}}(\theta)$ is the closed-form upper bound for the original AT loss function $\mathcal L^{\text{adv}}(\theta)$ in Eq.~(\ref{eq:icl:at-loss}).
This is illustrated in the following proposition:
\begin{proposition}
\label{prop:icl:surrogate-bound}
For the AT loss function $\mathcal L^{\text{\rm adv}}(\theta)$ defined in Eq.~(\ref{eq:icl:at-loss}) and the surrogate AT loss function $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ defined in Eq.~(\ref{eq:icl:surrogate-at-loss}), for any model parameter $\theta:= (W^{V},W^{KQ})$ of the LSA model $f_{\text{\rm LSA},\theta}$, we uniformly have that
\begin{align*}
    \mathcal L^{\text{\rm adv}}(\theta) \leq \tilde{\mathcal L}^{\text{\rm adv}}(\theta).
\end{align*}
\end{proposition}
The proof of Proposition~\ref{prop:icl:surrogate-bound} is presented in Appendix~\ref{app:proof:surrogate-bound}.
This result indicates that when we are training the LSA model via solving the surrogate AT problem Eq.~(\ref{eq:icl:surrogate-at-loss}), we are also reducing the model training loss in the original AT problem Eq.~(\ref{eq:icl:at-loss}).
Thus, solving the surrogate AT problem will also intuitively improve the robustness of the model.





Based on our previous analysis, we now turn to study the training dynamics of surrogate AT defined in Eq.~(\ref{eq:icl:surrogate-at-loss}).
To better describe our results, we define two functions $\Gamma(\cdot): \mathbb{N} \rightarrow \mathbb{R}^{d\times d}$ and $\psi(\cdot): \mathbb{N} \rightarrow \mathbb{R}$, both of which depend on the adversarial suffix length $M$, as follows,
\begin{align}
    \Gamma(M) &:= \frac{ N + M + 1 }{ N + M } \Lambda + \frac{ \mathrm{Tr}(\Lambda) }{ N + M } I_d \in \mathbb{R}^{d\times d},
    \label{eq:icl:func-gamma}\\
    \psi(M) &:= \frac{ M^2 \mathrm{Tr}(\Lambda) }{ (N + M)^2 } \in \mathbb{R},
    \label{eq:icl:func-psi}
\end{align}
where $N$ is the prompt length of the original embedding matrix $E_\tau$ (see Eq.~(\ref{eq:icl:embedding})) and $\Lambda$ is the covariance matrix of in-context linear regression samples.
The closed-form surrogate AT dynamics of the LSA model $f_{\text{LSA},\theta}$ is then given in the following theorem.
\begin{theorem}[Closed-form Surrogate AT Dynamics]
\label{thm:icl:closed-form-at}
Suppose Assumption~\ref{ass:icl:init} holds and $f_{\text{\rm LSA},\theta}$ is trained from the surrogate AT problem defined in Eq.~(\ref{eq:icl:surrogate-at-loss}) with continuous gradient flow.
When the $\sigma$ in Assumption~\ref{ass:icl:init} satisfies
$\sigma < \sqrt{ \frac{2}{d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 } }$,
after training for infinite long time, the model parameter $\theta$ will converge to $\theta_{*}(M_{\text{\rm train}}) := (W^{V}_{*}(M_{\text{\rm train}}), W^{KQ}_{*}(M_{\text{\rm train}}))$, satisfying:
$w^{KQ}_{*,12} = w^{KQ}_{*,21} = w^{V}_{*,12} = w^{V}_{*,21} = 0_{d\times 1}$,
$w^{KQ}_{*,22} = 0$,
$W^V_{*,11} = 0_{d\times d}$,
and
\begin{align*}
    w^V_{*,22} W^{KQ}_{*,11} = \Bigl( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d \Bigr)^{-1} \Lambda.
\end{align*}
\end{theorem}
The proof of Theorem~\ref{thm:icl:closed-form-at} is presented in Appendix~\ref{app:proof:closed-form-at}.




\begin{remark}
When the $l_2$-norm adversarial perturbation radius $\epsilon$ is zero, the closed-form AT solution $\theta_*$ derived in Theorem~\ref{thm:icl:closed-form-at} degenerates to that obtained without AT (see Theorem~4.1 in \citet{zhang2024trained}).
Thus, a sufficient large adversarial perturbation $\epsilon$ is a key to helping the LSA model $f_{\text{LSA},\theta}$ obtain significant adversarial robustness.
This will be further justified in the next section.
\end{remark}



\subsection{Robust Generalization Upper-bound}
\label{sec:robust-gen}


With the closed-form AT solution $\theta_*(M_{\text{train}})$ in Theorem~\ref{thm:icl:closed-form-at}, we now analyze the robustness of the trained LSA model.
All proofs in this section are presented in Appendix~\ref{app:proof:sec-robust-gen}.






We study how a LSA model adversarially trained under a fixed adversarial suffix length $M_{\text{train}}$ can defend against the ICL adversarial attack with a different adversarial suffix length $M_{\text{test}}$.
That is, we aim to analyze the magnitude of the robust generalization error $\mathcal{R}^{\text{adv}}(\theta_*(M_{\text{train}}), M_{\text{test}})$ for the converged robust model parameter $\theta_*(M_{\text{train}})$.
Here, we prove an upper-bound for it in the following theorem.
\begin{theorem}[Surrogate AT Robust Generalization Bound]
\label{thm:icl:robust-gen-bound}
Suppose all conditions in Theorem~\ref{thm:icl:closed-form-at} hold and $\theta_*(M_{\text{\rm train}})$ is the surrogate AT solution in Theorem~\ref{thm:icl:closed-form-at}.
We have
\begin{align*}
    \mathcal R^{\text{\rm adv}}(\theta_*(M_{\text{\rm train}}), M_{\text{\rm test}})  % \nonumber \\
    \leq 2 \mathrm{Tr}\Bigl[ \Lambda^3 \Bigl( \Gamma_{\text{\rm test}} \Lambda + \epsilon^2 \psi_{\text{\rm test}} I_d \Bigr) \Bigl( \Gamma_{\text{\rm train}} \Lambda + \epsilon^2 \psi_{\text{\rm train}} I_d \Bigr)^{-2} + \Lambda \Bigr],
\end{align*}
where
$M_{\text{\rm train}}$ is the adversarial suffix length in the ICL adversarial attack,
$\Gamma_{\text{\rm train}} := \Gamma(M_{\text{\rm train}})$ and $\Gamma_{\text{\rm test}} := \Gamma(M_{\text{\rm test}})$ are functions in Eq.~(\ref{eq:icl:func-gamma}),
and
$\psi_{\text{\rm train}} := \psi(M_{\text{\rm train}})$ and $\psi_{\text{\rm test}} := \psi(M_{\text{\rm test}})$ are functions in Eq.~(\ref{eq:icl:func-psi}).
\end{theorem}
We further adopt the following assumption to help us better understand our proposed robust generalization bound.
\begin{assumption}
\label{ass:icl:length-and-epsilon}
For adversarial suffix lengths during AT and testing, we assume that $M_{\text{train}}, M_{\text{test}} \leq \mathcal O(N)$, where $N$ is the original ICL prompt length.
Besides, for the $l_2$-norm adversarial perturbation radius, we assume that $\epsilon = \Theta(\sqrt{d})$, where $d$ is the ICL sample dimension.
\end{assumption}
In the above Assumption~\ref{ass:icl:length-and-epsilon}, the assumption made on adversarial suffix lengths means that they should not be too long to make the model ``forget'' the original ICL prompt.
Besides, the assumption made on the perturbation radius $\epsilon$ ensures that it is large enough to simulate the large (but limited) token vocabulary space of real-world LLMs to help model gain robustness.
\begin{corollary}
\label{cor:icl:ord-robust-gen-bound}
Suppose Assumption~\ref{ass:icl:length-and-epsilon} and all conditions in Theorem~\ref{thm:icl:robust-gen-bound} hold.
Suppose $\|\Lambda\|_2 \leq \mathcal{O}(1)$.
Then, we have the following robust generalization bound,
\begin{align*}
    &\mathcal R^{\text{\rm adv}}(\theta_*(M_{\text{\rm train}}), M_{\text{\rm test}})
    \leq \mathcal O(d) + \mathcal O\left( \frac{d^2}{N} \right) + \mathcal O\left( N^2 \cdot \frac{ M_{\text{\rm test}}^2 }{ M_{\text{\rm train}}^4 } \right).
\end{align*}
\end{corollary}
Corollary~\ref{cor:icl:ord-robust-gen-bound} is our main theoretical result, which clearly show that for an adversarially trained LSA model, its robust generalization bound depends on the term $\Theta(\sqrt{M_{\text{test}}}/M_{\text{train}})$, where $M_{\text{train}}$ and $M_{\text{test}}$ are the number of adversarially perturbed in-context samples during training and testing.
In other words, for an ICL adversarial attack with an adversarial suffix length $\Theta(M)$, to maintain the order of the robust generalization bound, it is enough to perform surrogate AT with only an adversarial suffix length $\Theta(\sqrt{M})$.
Such an observation is useful in practice, since one can thus leverage a ``short-length'' AT, which is efficient in terms of both GPU memory and training time usage, to defend against ``long-length'' jailbreak attacks.





\section{Empirical Evidence}
% Experiments on Real-world Models}
\label{sec:at-exp}

In this section, we conduct comprehensive experiments to empirically investigate the relationship between adversarial suffix lengths during AT and jailbreak attacks.



\subsection{Experimental Setup}


We follow Eq.~(\ref{eq:loss-at}) to perform AT on LLMs and then launch suffix jailbreak attacks with different adversarial suffix lengths.
Detailed setups are presented as follows.


\textbf{Models.}
We adopt five pre-trained LLMs:
Vicuna-7B-v1.5~\citep{zheng2023judging},
Mistral-7B-Instruct-v0.3~\citep{jiang2023mistral},
Llama-2-7B-Chat~\citep{touvron2023llama2},
Llama-3-8B-Instruct~\citep{grattafiori2024llama3herdmodels},
and Qwen2.5-7B-Instruct~\citep{yang2024qwen2}.
All models can be downloaded from the Hugging Face Model Repository.



\textbf{Datasets.}
For AT, we use the training set from Harmbench~\citep{mazeika2024harmbench} as the safety dataset and Alpaca~\citep{alpaca2023taori} as the utility dataset.
For the robustness evaluation, we construct a test set of size $100$ that consists of the first $50$ samples from the test set of Harmbench~\citep{mazeika2024harmbench} and the first $50$ samples from AdvBench~\citep{zou2023universal}.
For the utility analysis, we use the benchmark data from AlpacaEval~\citep{dubois2024length}.


\textbf{Adversarial training.}
We leverage GCG~\citep{zou2023universal}, a token-level jailbreak attack, to synthesize (suffix) jailbreak prompts, in which the adversarial suffix length is fixed to one of $\{5, 10, 20, 30, 40, 50\}$ during  AT.
To reduce computational complexity of tuning LLMs, LoRA~\citep{hu2022lora} is applied to all query and key projection matrices in attentions.
In every AT experiment, we follow Eq.~(\ref{eq:loss-at}) to perform AT with AdamW for $125$ iterations, in which the learning rate is set as $5\times 10^{-5}$ and the factor $\alpha$ is set as $0.2$.
Besides, the batch size is set as $64$, in which $8$ samples are jailbreak prompts crafted from data from the safety training set, and the remaining $56$ samples are from the utility training set.
Please refer to Appendix~\ref{app:exp:training} for omitted settings.




\textbf{Jailbreak attacks.}
Two token-level jailbreak attacks are adopted to evaluate the adversarial robustness of trained LLMs, which are GCG~\citep{zou2023universal} and BEAST~\citep{sadasivan2024fast}.
The token length of the adversarial suffix is varied within $\{5, 10, 20, 40, 60, 80, 100, 120\}$.
Please refer to Appendix~\ref{app:exp:evaluation} for more details.








\textbf{Evaluations.}
Our experiments focus on evaluating the jailbreak robustness and the utility of trained LLMs.
For the robustness evaluation, we report the {\bf Attack Success Rate (ASR)} of jailbreak attacks.
An LLM-based judger from \citet{mazeika2024harmbench} is used to determine whether a jailbreak attack succeeds or not.
Besides, for the utility evaluation, we use the AlpacaEval2 framework~\citep{dubois2024length} to report the {\bf Length-controlled WinRate (LC-WinRate)} of targeted models against a reference model Davinci003 based on their output qualities on the utility test set judged by the Llama-3-70B model.
An LC-WinRate of $50\%$ means that the output qualities of the two models are equal, while an LC-WinRate of $100\%$ means that the targeted model is consistently better than the reference Davinci003.
Please refer to Appendix~\ref{app:exp:evaluation} for the detailed settings of model evaluations.








\subsection{Results Analysis}




\begin{figure}[t]
    \centering
    \begin{subfigure}{0.18\linewidth}
        \includegraphics[width=\linewidth]{./figures/robust-scatter-vicuna.pdf}
        % \subcaption{Vicuna-7B.}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}{0.18\linewidth}
        \includegraphics[width=\linewidth]{./figures/robust-scatter-mistral.pdf}
        % \subcaption{Mistral-7B.}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}{0.18\linewidth}
        \includegraphics[width=\linewidth]{./figures/robust-scatter-llama2.pdf}
        % \subcaption{Llama-2-7B.}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}{0.18\linewidth}
        \includegraphics[width=\linewidth]{./figures/robust-scatter-llama3.pdf}
        % \subcaption{Llama-3-8B.}
    \end{subfigure}
    \hspace{0.5em}
    \begin{subfigure}{0.18\linewidth}
        \includegraphics[width=\linewidth]{./figures/robust-scatter-qwen2-5.pdf}
        % \subcaption{Qwen2.5-7B.}
    \end{subfigure}
    % \hspace{1em}
    \caption{
    Scatter plots of ASR to the ratio of the square root of the adversarial token suffix length in jailbreak attacks to the adversarial token suffix length during AT ({\it i.e.}, $\sqrt{M_{\text{test}}} / M_{\text{train}}$).
    For each pair of base model and attack, $48$ points are plotted.
    A high ASR indicates a weak jailbreak robustness of the model.
    }
    \label{fig:asr-vs-ratio}
\end{figure}


\begin{table}[t]
\centering
\caption{
PCCs and $p$-values calculated between ASR and the ratio $\sqrt{M_{\text{test}}} / M_{\text{train}}$.
A high PCC (within the range $[-1,1]$) indicate a strong correlation between ASR and the ratio.
}
% \vspace{-3mm}
\scriptsize
\begin{tabular}{c c c c c}
\toprule
\multirow{2}{5em}{\centering Model} & \multicolumn{2}{c}{GCG Attack} & \multicolumn{2}{c}{BEAST Attack} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
& PCC ($\uparrow$) & $p$-value ($\downarrow$) & PCC ($\uparrow$) & $p$-value ($\downarrow$) \\

\midrule

Vicuna-7B  & $0.93$ & $4.70\times 10^{-21}$ & $0.63$ & $1.43\times 10^{-6}$ \\
Mistral-7B & $0.86$ & $3.97\times 10^{-15}$ & $0.29$ & $4.41\times 10^{-2}$ \\
Llama-2-7B & $0.88$ & $9.04\times 10^{-17}$ & $0.68$ & $1.32\times 10^{-7}$ \\
Llama-3-8B & $0.76$ & $2.75\times 10^{-10}$ & $0.26$ & $7.67\times 10^{-2}$ \\
Qwen2.5-7B & $0.87$ & $1.06\times 10^{-15}$ & $0.58$ & $1.03\times 10^{-5}$ \\
\bottomrule
\end{tabular}
\label{tab:corr-ratio-vs-asr}
% \vspace{-2mm}
\end{table}




\textbf{Correlation between the jailbreak robustness and the ratio of the square root of the jailbreak adversarial suffix length to the adversarial suffix length in AT ({\it i.e.}, $\sqrt{M_{\text{test}}}/M_{\text{train}}$)}.
We plot the ASR of models trained and attacked with different adversarial suffix lengths in Figure~\ref{fig:asr-vs-ratio}.
This results in $48$ points for each pair of base model and jailbreak attack.
We also calculate the Pearson correlation coefficient~(PCC) and the corresponding $p$-value between the ratio $\sqrt{M_{\text{test}}} / M_{\text{train}}$ and the ASR, as shown in Table~\ref{tab:corr-ratio-vs-asr}.

When the jailbreak attack used during AT is the same as that used during robustness evaluation ({\it i.e.}, GCG), one can observe from Figure~\ref{fig:asr-vs-ratio} that a clear positive correlation between the ratio $\sqrt{M_{\text{test}}} / M_{\text{train}}$ and the ASR for all evaluated base models.
Further, high PCCs~($> 0.7$) and low $p$-values~($< 0.05$) in Table~\ref{tab:corr-ratio-vs-asr} also confirm that the observed correlation is statistically significant.


However, when the jailbreak attack in AT is different from that in robustness evaluation ({\it i.e.}, BEAST), from Table~\ref{tab:corr-ratio-vs-asr}, the correlation between the ratio $\sqrt{M_{\text{test}}}/M_{\text{train}}$ and the ASR can only be observed from some of the base models ({\it i.e.}, Vicuna-7B, Llama-2-7B, and Qwen2.5-7B) but not others.
This may be due to the fact that AT with only a single jailbreak attack may not help the model generalize well to unseen attacks.
Therefore, it might be necessary to adopt multiple attacks when performing AT-based alignment on LLMs.
Nevertheless, from Figure~\ref{fig:asr-vs-ratio}, we find that for those models where the correlation between the ratio and ASR is not significant ({\it i.e.}, Mistral-7B, and Llama-3-8B), GCG-based AT can still suppress the ASR to no more than $50\%$.
This indicates that single-attack AT can still help models gain a certain degree of robustness against unseen attacks.



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/robust-all.pdf}
    \caption{
    Curves of the ASR versus the adversarial suffix token length during AT ({\it i.e.}, $M_{\text{train}}$) under jailbreak attacks with different adversarial suffix token lengths ({\it i.e.}, $M_{\text{test}}$).
    $M_{\text{train}} = 0$ means that AT is not performed on the evaluated model.
    A high ASR indicates a weak jailbreak robustness of the model.
    }
    \label{fig:asr-vs-at-sfx-len}
    \vspace{-0.5em}
\end{figure}


% \begin{figure}[t]
\begin{wrapfigure}{r}{0.38\linewidth}
    \vspace{-1em}
    \centering
    % \includegraphics[width=0.5\linewidth]{./figures/utility-all.pdf}
    \includegraphics[width=0.84\linewidth]{./figures/utility-all.pdf}
    \caption{
    Utility analysis based on LC-WinRate.
    A high LC-WinRate indicates strong model utility.
    An LC-WinRate of $50\%$ means that the evaluated model has the same quality as the reference model Davinci003.
    }
    \label{fig:alpacaeval-utility}
% \end{figure}
\end{wrapfigure}


\textbf{Relationship between adversarial suffix lengths in AT ({\it i.e.}, $M_{\text{train}}$) and jailbreaking ({\it i.e.}, $M_{\text{test}}$).}
We further plot curves of the model ASR versus the adversarial suffix token length during AT in Figure~\ref{fig:asr-vs-at-sfx-len}.
From the figure, we find that as the adversarial suffix token length increases, AT can effectively reduce the ASR of both GCG and BEAST attacks.
Furthermore, when the AT adversarial suffix token length is set to $20$, AT is already able to reduce the ASR by at least $30\%$ under all settings.
Besides, it is worth noting that the adversarial suffix length during AT is only up to $50$, while that during jailbreaking can vary from $5$ to $120$.
All these results suggest the effectiveness of defending against long-length jailbreaking with short-length AT.








\textbf{Utility analysis.}
We plot the LC-WinRate of models trained under different adversarial suffix token lengths and the original pre-trained model ({\it i.e.}, $M_{\text{train}}=0$) in Figure~\ref{fig:alpacaeval-utility}.
We find that while AT reduces the utility of models, they can still achieve WinRates close to or more than $50\%$ against the reference Davinci003.
This means that these adversarially trained models achieve utility comparable to Davinci003.





\section{Conclusion}
\label{sec:conclusion}


This paper studies the AT problem in LLMs and unveils that to defend against a suffix jailbreak attack with suffix length of $\Theta(M)$, it is sufficient to perform AT on adversarial prompts with suffix length of $\Theta(\sqrt{M})$.
The finding is supported by both theoretical and empirical evidence.
Theoretically, we define a new AT problem in the ICL theory and prove a robust generalization upper bound for adversarially trained linear transformers.
This bound has a positive correlation with $\Theta(\sqrt{M_{\text{test}}} / M_{\text{train}})$.
Empirically, we conduct AT on real-world LLMs and confirm a clear positive correlation between jailbreak ASR and ratio $\sqrt{M_{\text{test}}} / M_{\text{train}}$.
Our results show that it is possible to conduct efficient ``short-length'' AT, which requires less GPU memory and training time, against strong ``long-length'' jailbreak attacks.








\bibliography{main}
\bibliographystyle{unsrtnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn


\section{Proofs}

This section collects all the proofs in this paper.


\subsection{Technical Lemmas}

This section presents several technical lemmas that will be used in our proofs.

\begin{lemma}[c.f. Lemma~D.2 in \citet{zhang2024trained}]
\label{lem:icl:tech-x-pow4}
If $x \in \mathbb{R}^{d\times 1}$ is Gaussian random vector of $d$ dimension, mean zero and covariance matrix $\Lambda$, and $A \in \mathbb{R}^{d\times d}$ is a fixed matrix.
Then
\begin{align*}
    \E [ x x^\top A x x^\top ] = \Lambda ( A + A^\top ) \Lambda + \mathrm{Tr}(A \Lambda) \Lambda.
\end{align*}
\end{lemma}



\begin{lemma}
\label{lem:icl:tech-x-quad}
If $x \in \mathbb{R}^{d\times 1}$ is Gaussian random vector of $d$ dimension, mean zero and covariance matrix $\Lambda$, and $A \in \mathbb{R}^{d\times d}$ is a fixed matrix.
Then
\begin{align*}
    \E[ x^\top A x ] = \mathrm{Tr}(A \Lambda).
\end{align*}
\end{lemma}
\begin{proof}
Since
\begin{align*}
    \E[x^\top A x]
    = \E\Bigl[ \sum_{i,j} x_i A_{i,j} x_j \Bigr]
    = \sum_{i,j} A_{i,j} \cdot \E[ x_i x_j ]
    = \sum_{i,j} A_{i,j} \cdot \Lambda_{i,j}
    = \sum_{i=1}^d (A \Lambda^\top)_{i,i}
    = \mathrm{Tr}(A \Lambda),
\end{align*}
which completes the proof.
\end{proof}



\begin{lemma}
\label{lem:icl:tech:mat-permute}
For any matrices $A \in \mathbb{R}^{n\times m}$ and $B \in \mathbb{R}^{m \times n}$, we have
\begin{align*}
    \mathrm{Tr}(A B) = \mathrm{Tr}(B A).
\end{align*}
\end{lemma}
\begin{proof}
Since
\begin{align*}
    \mathrm{Tr}(A B)
    = \sum_{i=1}^n (A B)_{i,i}
    = \sum_{i=1}^n \sum_{j=1}^m A_{i,j} B_{j,i}
    = \sum_{j=1}^m \sum_{i=1}^n B_{j,i} A_{i,j}
    = \sum_{j=1}^m (B A)_{j,j}
    = \mathrm{Tr}(B A),
\end{align*}
which completes the proof.
\end{proof}


\begin{lemma}[From Lemma~D.1 in \citet{zhang2024trained}; Also in \citet{petersen2008matrix}]
\label{lem:icl:tech:trace-diff}
Let $X \in \mathbb{R}^{n\times m}$ be a variable matrix and $A \in \mathbb{R}^{a\times n}$ and $B\in \mathbb{R}^{n\times m}$ be two fixed matrices.
Then, we have
\begin{align*}
    &\partial_{X} \mathrm{Tr}(B X^\top) = B \in \mathbb{R}^{n\times m}, \nonumber\\
    & \partial_{X} \mathrm{Tr}(A X B X^\top) = ( AXB + A^\top X B^\top ) \in \mathbb{R}^{n\times m}.
\end{align*}
\end{lemma}


\begin{lemma}[Von Neumann's Trace Inequality; Also in Lemma~D.3 in \citet{zhang2024trained}]
\label{lem:icl:tech-von-trace}
Let $A\in \mathbb{R}^{n\times m}$ and $B\in \mathbb{R}^{m \times n}$ be two matrices.
Suppose
$\sigma_1(A) \leq \cdots \leq \sigma_{\min\{n,m\}}(A)$
and
$\sigma_1(B) \leq \cdots \leq \sigma_{\min\{n,m\}}(B)$
are all the (ordered) singular values of $A$ and $B$, respectively.
We have
\begin{align*}
    \mathrm{Tr}(AB)
    \leq \sum_{i=1}^{\min\{n,m\}} \sigma_i(A) \sigma_i(B)
    \leq \sum_{i=1}^{\min\{n,m\}} \|A\|_2 \cdot \|B\|_2
    = \min\{n,m\} \cdot \|A\|_2 \cdot \|B\|_2.
\end{align*}
\end{lemma}


\subsection{Proof of Proposition~\ref{prop:icl:surrogate-bound}}
\label{app:proof:surrogate-bound}

This section presents the proof of Proposition~\ref{prop:icl:surrogate-bound}.

\begin{proof}[Proof of Proposition~\ref{prop:icl:surrogate-bound}]
For the AT loss $\mathcal L(\theta)$ defined in Eq.~(\ref{eq:icl:at-loss}), we have that
\begin{align}
    \mathcal L^{\text{adv}}(\theta)
    :=& \mathcal R^{\text{adv}}(\theta,M_{\text{train}})
    = \E_{\tau} \max_{\|\Delta_\tau^\top\|_{2,\infty} \leq \epsilon} | \hat y_{q,\theta}(E^{\text{adv}}_{\tau,M_{\text{train}}}) - y_{\tau,q} |^2 \nonumber\\
    =& \E_{\tau} \left\{ \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \frac{1}{2} \left| \begin{pmatrix}(w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot \frac{E^{\text{adv}}_{\tau,M_{\text{train}}} E^{\text{adv},\top}_{\tau,M_{\text{train}}}}{N + M_{\text{train}}} \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} \cdot x_{\tau,q} - y_{\tau,q} \right|^2 \right\}.
    \label{eq:icl:proof:at-original:eq1}
\end{align}
Then, the term $E^{\text{adv}}_{\tau,M_{\text{train}}} E^{\text{adv},\top}_{\tau,M_{\text{train}}}$ can be decomposed as follows,
\begin{align*}
    &E^{\text{adv}}_{\tau,M_{\text{train}}} E^{\text{adv},\top}_{\tau,M_{\text{train}}}
    = \begin{pmatrix}
        \begin{pmatrix} X_{\tau} \\ Y_{\tau} \end{pmatrix}
        &
        \begin{pmatrix} X^{\text{sfx}}_{\tau} + \Delta_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}
        &
        \begin{pmatrix} x_{\tau,q} \\ 0 \end{pmatrix}
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
        \begin{pmatrix} X_{\tau} \\ Y_{\tau} \end{pmatrix}
        &
        \begin{pmatrix} X^{\text{sfx}}_{\tau} + \Delta_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}
        &
        \begin{pmatrix} x_{\tau,q} \\ 0 \end{pmatrix}
    \end{pmatrix}^\top \nonumber\\
    &= \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top
        + \begin{pmatrix} 0_{d\times N} & \Delta_\tau & 0_{d\times 1} \\ 0_{1\times N} & 0_{1\times M_{\text{train}}} & 0 \end{pmatrix} \begin{pmatrix} 0_{d\times N} & \Delta_\tau & 0_{d\times 1} \\ 0_{1\times N} & 0_{1\times M_{\text{train}}} & 0 \end{pmatrix}^\top
    \nonumber\\
    &\quad\quad + \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} 0_{d\times N} & \Delta_\tau & 0_{d\times 1} \\ 0_{1\times N} & 0_{1\times M_{\text{train}}} & 0 \end{pmatrix}^\top
        + \begin{pmatrix} 0_{d\times N} & \Delta_\tau & 0_{d\times 1} \\ 0_{1\times N} & 0_{1\times M_{\text{train}}} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top \nonumber\\
    &= \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top
        + \begin{pmatrix} \Delta_\tau \\ 0_{1\times M_{\text{train}}} \end{pmatrix} \begin{pmatrix} \Delta_\tau \\ 0_{1\times M_{\text{train}}} \end{pmatrix}^\top
        + \begin{pmatrix} X^{\text{sfx}}_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \begin{pmatrix} \Delta_\tau \\ 0_{1\times M_{\text{train}}} \end{pmatrix}^\top
        + \begin{pmatrix} \Delta_\tau \\ 0_{1\times M_{\text{train}}} \end{pmatrix} \begin{pmatrix} X^{\text{sfx}}_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top,
\end{align*}
which further means that
\begin{align}
    &\begin{pmatrix}(w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot \frac{E^{\text{adv}}_{\tau,M_{\text{train}}} E^{\text{adv},\top}_{\tau,M_{\text{train}}}}{N + M_{\text{train}}} \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} \cdot x_{\tau,q} \nonumber \\
    &= \begin{pmatrix}(w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot
        \frac{ \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top }{ N + M_{\text{train}} }
    \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} \cdot x_{\tau,q}
        + (w^{V}_{21})^\top \cdot \frac{\Delta_\tau \Delta_\tau^\top}{ N + M_{\text{train}} } \cdot  W^{KQ}_{11} x_{\tau,q}
    \nonumber \\
    &\quad\quad
        + \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix} \cdot \frac{\begin{pmatrix} X^{\text{sfx}}_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \Delta_\tau^\top}{ N + M_{\text{train}} } \cdot  W^{KQ}_{11} x_{\tau,q}
        + (w^{V}_{21})^\top \cdot \frac{\Delta_\tau \begin{pmatrix} X^{\text{sfx}}_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top}{ N + M_{\text{train}} } \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q}.
    \label{eq:icl:proof:at-original:eq2}
\end{align}
Inserting Eq.~(\ref{eq:icl:proof:at-original:eq2}) into Eq.~(\ref{eq:icl:proof:at-original:eq1}) and applying the inequality that $|a+b|^2 \leq 2 \cdot (a^2 + b^2)$, $\mathcal L^{\text{adv}}(\theta)$ can thus be bounded as
\begin{align}
    \mathcal L^{\text{adv}}(\theta)
    % \nonumber\\
    &\leq 2 \cdot \E_\tau \Bigl[
        \begin{pmatrix}(w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot
            \frac{ \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top }{ N + M_{\text{train}} }
        \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} \cdot x_{\tau,q} - y_{\tau,q} \Bigr]^2
    \nonumber\\
    &\quad
        + \underbrace{ 2\cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ (w^{V}_{21})^\top \cdot \frac{\Delta_\tau \Delta_\tau^\top}{ N + M_{\text{train}} } \cdot  W^{KQ}_{11} x_{\tau,q} \Bigr]^2 }_{ := A_1(\theta) }
    \nonumber\\
    &\quad
        + \underbrace{ 2\cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix} \cdot \frac{\begin{pmatrix} X^{\text{sfx}}_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \Delta_\tau^\top}{ N + M_{\text{train}} } \cdot  W^{KQ}_{11} x_{\tau,q} \Bigr]^2 }_{ := A_2(\theta) }
    \nonumber\\
    &\quad
        + \underbrace{ 2\cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ (w^{V}_{21})^\top \cdot \frac{\Delta_\tau \begin{pmatrix} X^{\text{sfx}}_\tau \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top}{ N + M_{\text{train}} } \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2 }_{ := A_3(\theta) }.
    \label{eq:icl:proof:at-original:eq3}
\end{align}
We then bound terms $A_1(\theta)$, $A_2(\theta)$, and $A_3(\theta)$ in Eq.~(\ref{eq:icl:proof:at-original:eq3}) seprately.
For the term $A_1(\theta)$ in Eq.~(\ref{eq:icl:proof:at-original:eq3}), we have
\begin{align}
    &A_1(\theta)
    := \frac{2}{(N+M_{\text{train}})^2} \cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ (w^{V}_{21})^\top \cdot \sum_{i=1}^{M_{\text{train}}} \delta_{\tau,i} \delta_{\tau,i}^\top  \cdot  W^{KQ}_{11} x_{\tau,q} \Bigr]^2 \nonumber\\
    &\leq \frac{2}{(N+M_{\text{train}})^2}\cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ \underbrace{ \sum_{i=1}^{M_{\text{train}}} [ (w^{V}_{21})^\top \delta_{\tau,i}]^2 \cdot \sum_{i=1}^{M_{\text{train}}} [\delta_{\tau,i}^\top W^{KQ}_{11} x_{\tau,q}]^2 }_{\text{by Cauchy-Schwarz Inequality}} \Bigr] \nonumber\\
    &\leq \frac{2}{(N+M_{\text{train}})^2}\cdot \E_\tau \Bigl[ \sum_{i=1}^{M_{\text{train}}} \max_{\|\delta_{\tau,i}\|_{2}\leq\epsilon} [ (w^{V}_{21})^\top \delta_{\tau,i}]^2 \cdot \sum_{i=1}^{M_{\text{train}}} \max_{\|\delta_{\tau,i}\|_{2}\leq\epsilon} [\delta_i^\top W^{KQ}_{11} x_{\tau,q}]^2 \Bigr] \nonumber\\
    &= \frac{2}{(N+M_{\text{train}})^2}\cdot \E_\tau \Bigl[ \sum_{i=1}^{M_{\text{train}}}  [\|w^{V}_{21}\|_2 \cdot \epsilon]^2 \cdot \sum_{i=1}^{M_{\text{train}}} [ \| W^{KQ}_{11} x_{\tau,q} \|_2 \cdot \epsilon ]^2 \Bigr]
    \nonumber\\
    &= \frac{2 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2}\cdot \|w^{V}_{21}\|_2^2 \cdot \E_{\tau} \| W^{KQ}_{11} x_{\tau,q} \|_2^2.
    \label{eq:icl:proof:at-original:eq3:term1}
\end{align}
For the term $A_2(\theta)$ in Eq.~(\ref{eq:icl:proof:at-original:eq3}), we have
\begin{align}
    &A_2(\theta)
    := \frac{2}{(N+M_{\text{train}})^2} \cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix} \cdot \sum_{i=1}^{M_{\text{train}}} \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \delta_{\tau,i}^\top  \cdot  W^{KQ}_{11} x_{\tau,q} \Bigr]^2 \nonumber\\
    &\leq \frac{2}{(N+M_{\text{train}})^2} \cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ \underbrace{  \sum_{i=1}^{M_{\text{train}}} \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix}  \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \Bigr]^2 \cdot \sum_{i=1}^{M_{\text{train}}} [ \delta_{\tau,i}^\top  W^{KQ}_{11} x_{\tau,q} ]^2 }_{\text{by Cauchy-Schwarz Inequality}} \Bigr] \nonumber\\
    & = \frac{2}{(N+M_{\text{train}})^2} \cdot \sum_{i=1}^{M_{\text{train}}} \E_\tau \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix}  \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \Bigr]^2 \cdot \sum_{i=1}^{M_{\text{train}}} \E_\tau \Bigl[ \max_{\|\delta_{\tau,i}\|_{2}\leq\epsilon} [ \delta_{\tau,i}^\top  W^{KQ}_{11} x_{\tau,q} ]^2 \Bigr] \nonumber\\
    & = \frac{2}{(N+M_{\text{train}})^2} \cdot \sum_{i=1}^{M_{\text{train}}} \E_\tau \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix}  \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \Bigr]^2 \cdot \sum_{i=1}^{M_{\text{train}}} \E_\tau [ \| W^{KQ}_{11} x_{\tau,q} \|_2 \cdot \epsilon ]^2 \nonumber\\
    & = \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_{\tau} \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \sum_{i=1}^{M_{\text{train}}}  \E_\tau \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix}  \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \Bigr]^2.
    \label{eq:icl:proof:at-original:eq3:term2}
\end{align}
For the term $A_3(\theta)$ in Eq.~(\ref{eq:icl:proof:at-original:eq3}), we have
\begin{align}
    &A_3(\theta)
    := \frac{2}{ (N+M_{\text{train}})^2 } \cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ (w^{V}_{21})^\top \cdot \sum_{i=1}^{M_{\text{train}}} \delta_{\tau,i} \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2 \nonumber \\
    &\leq \frac{2}{ (N+M_{\text{train}})^2 } \cdot \E_\tau \max_{\|\Delta_\tau^\top\|_{2,\infty}\leq\epsilon} \Bigl[ \underbrace{ \sum_{i=1}^{M_{\text{train}}} [ (w^{V}_{21})^\top \delta_{\tau,i} ]^2 \cdot \sum_{i=1}^{M_{\text{train}}} \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2 }_{\text{by Cauchy-Schwarz Inequality}} \Bigr] \nonumber \\
    &= \frac{2}{ (N+M_{\text{train}})^2 } \cdot \E_\tau \Bigl[ \sum_{i=1}^{M_{\text{train}}} \max_{\|\delta_{\tau,i}\|_{2}\leq\epsilon} [ (w^{V}_{21})^\top \delta_{\tau,i} ]^2 \cdot \sum_{i=1}^{M_{\text{train}}} \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2 \Bigr] \nonumber \\
    &= \frac{2}{ (N+M_{\text{train}})^2 } \cdot \E_\tau \Bigl[ \sum_{i=1}^{M_{\text{train}}} [ \| w^{V}_{21} \|_2 \cdot \epsilon ]^2 \cdot \sum_{i=1}^{M_{\text{train}}} \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2 \Bigr] \nonumber \\
    &= \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 } \cdot \|w^{V}_{21}\|_2^2  \cdot \sum_{i=1}^{M_{\text{train}}} \E_\tau \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2.
    \label{eq:icl:proof:at-original:eq3:term3}
\end{align}
As a result, by inserting Eqs.~(\ref{eq:icl:proof:at-original:eq3:term1}),~(\ref{eq:icl:proof:at-original:eq3:term2}), and~(\ref{eq:icl:proof:at-original:eq3:term3}) into Eq.~(\ref{eq:icl:proof:at-original:eq3}), we finally have that
\begin{align}
    \mathcal L^{\text{adv}}(\theta) %\nonumber\\
    &\leq 2 \cdot \E_\tau \Bigl[
        \begin{pmatrix}(w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot
            \frac{ \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top }{ N + M_{\text{train}} }
        \cdot \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} \cdot x_{\tau,q} - y_{\tau,q} \Bigr]^2
    \nonumber\\
    &\quad + \frac{2 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2}\cdot \|w^{V}_{21}\|_2^2 \cdot \E_{\tau} \| W^{KQ}_{11} x_{\tau,q} \|_2^2
    \nonumber\\
    &\quad + \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_{\tau} \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \sum_{i=1}^{M_{\text{train}}}  \E_\tau \Bigl[ \begin{pmatrix} (w^{V}_{21})^\top & w^V_{22} \end{pmatrix}  \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \Bigr]^2
    \nonumber\\
    &\quad + \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 } \cdot \|w^{V}_{21}\|_2^2  \cdot \sum_{i=1}^{M_{\text{train}}} \E_\tau \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \Bigr]^2.
    \label{eq:icl:proof:at-original:eq10}
\end{align}
The right-hand-side of Eq.~(\ref{eq:icl:proof:at-original:eq10}) is exactly the surrogate AT loss $\tilde{\mathcal L}^{\text{adv}}(\theta)$ in Eq.~(\ref{eq:icl:surrogate-at-loss}), which thus completes the proof.
\end{proof}




\subsection{Proof of Theorem~\ref{thm:icl:closed-form-at}}
\label{app:proof:closed-form-at}

This section presents the proof of Theorem~\ref{thm:icl:closed-form-at}, which is inspired by that in \citet{zhang2024trained}.
Specifically:
\begin{enumerate}
\item
we first prove that terms $w^V_{21}$ and $w^{KQ}_{21}$ stay zero during the surrogate AT (Lemma~\ref{lem:icl:zero-grad}) via continuous gradient-flow, which thus can simplify the surrogate AT loss $\tilde{\mathcal L}^{\text{adv}}(\theta)$ defined in Eq.~(\ref{eq:icl:surrogate-at-loss}) (Lemma~\ref{lem:icl:simplified-surrogate-at-loss}).

\item
We then calculate a closed-form solution $\theta_*$ for the surrogate AT problem based on the simplified $\tilde{\mathcal L}^{\text{adv}}(\theta)$ (Lemma~\ref{lem:icl:surrogate-at-minimizer}), which is exactly the solution given in Theorem~\ref{thm:icl:closed-form-at}.

\item
Finally, we prove that under the continuous gradient flow, the LSA model starts from the initial point defined in Assumption~\ref{ass:icl:init} can indeed converge to the closed-form solution $\theta_*$ (Lemma~\ref{lem:icl:pl-inequality}), which thus completes the proof of Theorem~\ref{thm:icl:closed-form-at}.
\end{enumerate}


We now start to prove the following Lemma~\ref{lem:icl:zero-grad}.
\begin{lemma}
\label{lem:icl:zero-grad}
Suppose Assumption~\ref{ass:icl:init} holds and the LSA model $f_{\text{\rm LSA},\theta}$ is trained via minimizing surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ in Eq.~(\ref{eq:icl:surrogate-at-loss}) with continuous gradient flow.
Then, for any continuous training time $t \geq 0$, we uniformly have that $w^V_{21}(t) = w^{KQ}_{21}(t) = 0_{d\times 1}$.
\end{lemma}

\begin{proof}
When the LSA model $f_{\text{LSA},\theta}$ is trained with continuous gradient-flow, the updates of $w^{V}_{21}$ and $w^{KQ}_{21}$ with respect to the continuous training time $t \geq 0$ are given by
\begin{align*}
    &\partial_t w^V_{21}(t) := -\partial_{w^{V}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta), \\
    &\partial_t w^{KQ}_{21}(t) := -\partial_{w^{KQ}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta).
\end{align*}
Meanwhile, since Assumption~\ref{ass:icl:init} assumes that $w^{V}_{21}(0) = W^{KQ}_{21}(0) = 0_{d\times 1}$, therefore, to complete the proof, we only need to show that
$\partial_t w^V_{21}(t) = \partial_t W^{KQ}_{21}(t) = 0_{1\times d}$
as long as
$w^{V}_{21}(t) = W^{KQ}_{21}(t) = 0_{d\times 1}$
for any $t\geq 0$.
In other words, below we need to show that $w^{V}_{21} = W^{KQ}_{21} = 0_{d\times 1}$ indicates $\partial_{w^{V}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta) = \partial_{w^{KQ}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta) = 0_{1\times d}$.

Toward this end, we adopt the notation in Eq.~(\ref{eq:icl:surrogate-at-loss}) to decompose the surrogate AT loss $\tilde{\mathcal L}(\theta)$ as follows,
\begin{align*}
    \tilde{\mathcal L}^{\text{adv}}(\theta) := [ \ell_1(\theta) + \ell_2(\theta) + \ell_3(\theta) + \ell_4(\theta) ],
\end{align*}
where
\begin{align}
    & \ell_1(\theta) = 2 \E_\tau \Bigl[
        ((w^V_{21})^\top \ \ w^V_{22} )  \frac{ \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top }{ N + M_{\text{train}} }  \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} - y_{\tau,q} \Bigr]^2,
    \label{eq:icl:proof:zero-grad:subloss1}\\
    &\ell_2(\theta) = \frac{2 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \|w^{V}_{21}\|_2^2 \E_{\tau} \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr],
    \label{eq:icl:proof:zero-grad:subloss2}\\
    &\ell_3(\theta) = \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \E_\tau \Bigl[ \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \| ((w^V_{21})^\top \ \ w^V_{22} )  \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \|_2^2 \Bigr],
    \label{eq:icl:proof:zero-grad:subloss3}\\
    &\ell_4(\theta) = \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 }  \|w^{V}_{21}\|_2^2  \cdot \E_\tau \Bigl[ \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \Bigr].
    \label{eq:icl:proof:zero-grad:subloss4}
\end{align}
In the remaining of this proof, we will show that when
$w^{V}_{21} = w^{KQ}_{21} = 0_{d\times 1}$ holds,
one has:
(1) $\partial_{W^{V}_{21}} \ell_1(\theta) = \partial_{W^{KQ}_{21}} \ell_1(\theta) = 0_{1\times d}$,
(2) $\partial_{W^{V}_{21}} \ell_2(\theta) = \partial_{W^{KQ}_{21}} \ell_2(\theta) = 0_{1\times d}$,
(3) $\partial_{W^{V}_{21}} \ell_3(\theta) = \partial_{W^{KQ}_{21}} \ell_3(\theta) = 0_{1\times d}$,
and (4) $\partial_{W^{V}_{21}} \ell_4(\theta) = \partial_{W^{KQ}_{21}} \ell_4(\theta) = 0_{1\times d}$,
which thus automatically indicates that
$\partial_{W^{V}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta) = \partial_{W^{KQ}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta) = 0_{1\times d}$.


\textbf{Step 1: Show that $w^{V}_{21} = w^{KQ}_{21} = 0_{d\times 1}$ indicates $\partial_{W^{V}_{21}} \ell_1(\theta) = \partial_{W^{KQ}_{21}} \ell_1(\theta) = 0_{1\times d}$.}
Such a claim can be directly obtained from the proofs in \citet{zhang2024trained}.
Specifically, when setting the (original) ICL prompt length from $N$ to $(N + M_{\text{train}})$, the ICL training loss $L$ in \citet{zhang2024trained} is equivalent to our $\ell_1(\theta)$ defined in Eq.~(\ref{eq:icl:proof:zero-grad:subloss1}).
Therefore, one can then follow the same procedures as those in the proof of Lemma~5.2 in \citet{zhang2024trained} to show that the continuous gradient flows of $W^{V}_{21}$ and $W^{KQ}_{21}$ are zero when Assumption~\ref{ass:icl:init} holds.
Please refer accordingly for details.
% We refer the reader to the original work for details.


\textbf{Step 2: Show that $w^{V}_{21} = w^{KQ}_{21} = 0_{d\times 1}$ indicates $\partial_{w^V_{21}} \ell_{2}(\theta) = \partial_{w^{KQ}_{21}} \ell_2(\theta) = 0_{1\times d}$.}
Since the term $w^{KQ}_{21}$ does not exist in the expression of $\ell_2(\theta)$ in Eq.~(\ref{eq:icl:proof:zero-grad:subloss2}), we directly have that $\partial_{w^{KQ}_{21}} \ell_2(\theta) = 0_{1\times d}$.
Besides, for the derivative $\partial_{w^{V}_{21}} \ell_2(\theta)$, based on Eq.~(\ref{eq:icl:proof:zero-grad:subloss2}) we further have that
\begin{align*}
    &\left. \partial_{w^{V}_{21}} \ell_2(\theta) \right|_{w^{V}_{21} = 0_{d\times 1}}
    = \partial_{w^{V}_{21}} \left. \Bigl[ \frac{2 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \|w^V_{21}\|_2^2 \cdot \E_{\tau} \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \right|_{w^{V}_{21} = 0_{d\times 1}}
    \nonumber\\
    &= \left. \Bigl[ \frac{4 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_{\tau} \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \cdot (w^{V}_{21})^\top \Bigr] \right|_{w^{V}_{21} = 0_{d\times 1}}
    \nonumber\\
    &= \frac{4 \epsilon^4 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_{\tau} \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \cdot 0_{d\times 1}^\top
    = 0_{1\times d},
\end{align*}
which justifies our claim in Step~2.




\textbf{Step 3: Show that $w^{V}_{21} = w^{KQ}_{21} = 0_{d\times 1}$ indicates $\partial_{w^V_{21}} \ell_{3}(\theta) = \partial_{w^{KQ}_{21}} \ell_3(\theta) = 0_{1\times d}$.}
We first rewrite $\ell_3(\theta)$ that defined in Eq.~(\ref{eq:icl:proof:zero-grad:subloss3}) as follows,
\begin{align}
    &\ell_3(\theta) = \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \E_\tau \Bigl[ \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \| ((w^V_{21})^\top \ \ w^V_{22} )  \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \|_2^2 \Bigr]
    \nonumber\\
    &= \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot \sum_{i=1}^{M_{\text{train}}}   \E_\tau \Bigl[  \begin{pmatrix} (w^V_{21})^\top & w^V_{22} \end{pmatrix}  \cdot  \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \cdot \begin{pmatrix} (w^V_{21})^\top & w^V_{22} \end{pmatrix}^\top \Bigr] \nonumber \\
    &= \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot \begin{pmatrix} (w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot \left( \sum_{i=1}^{M_{\text{train}}} \E_\tau \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \Bigr] \right) \cdot \begin{pmatrix} (w^V_{21})^\top & w^V_{22} \end{pmatrix}^\top.
    \label{eq:icl:proof:zero-grad:subloss3:simplified-1}
\end{align}
Then, for any $i \in [M]$ we have
\begin{align}
    &\E_\tau \Bigl[ \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix} \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \\ y^{\text{sfx}}_{\tau,i} \end{pmatrix}^\top \Bigr]
    = \E_{w_\tau,x^{\text{sfx}}_{\tau,i}} \begin{pmatrix} x^{\text{sfx}}_{\tau,i} \cdot (x^{\text{sfx}}_{\tau,i})^\top & x^{\text{sfx}}_{\tau,i} \cdot (w_\tau^\top x^{\text{sfx}}_{\tau,i})^\top \\ w_\tau^\top x^{\text{sfx}}_{\tau,i} \cdot (x^{\text{sfx}}_{\tau,i})^\top & w_\tau^\top x^{\text{sfx}}_{\tau,i} \cdot (w_\tau^\top x^{\text{sfx}}_{\tau,i})^\top \end{pmatrix}
    \nonumber\\
    &= \begin{pmatrix} \Lambda & \Lambda \cdot 0_{d\times 1} \\ 0_{1\times d} \cdot \Lambda & \E_{w_\tau} \Bigl[ w_\tau^\top \Lambda w_\tau \Bigr] \end{pmatrix}
    = \begin{pmatrix} \Lambda & 0_{d\times 1} \\ 0_{1\times d} & \underbrace{ \mathrm{Tr}(I_d \Lambda)  }_{\text{by Lemma~\ref{lem:icl:tech-x-quad}}} \end{pmatrix}
    = \begin{pmatrix} \Lambda & 0_{d\times 1} \\ 0_{1\times d} & \mathrm{Tr}(\Lambda) \end{pmatrix}.
    \label{eq:icl:proof:zero-grad:subloss3:technical-xy-quad}
\end{align}
Finally, by inserting Eq.~(\ref{eq:icl:proof:zero-grad:subloss3:technical-xy-quad}) into Eq.~(\ref{eq:icl:proof:zero-grad:subloss3:simplified-1}), $\ell_3(\theta)$ can thus be simplified as follows,
\begin{align}
    &\ell_3(\theta) = \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot  \begin{pmatrix} (w^V_{21})^\top & w^V_{22} \end{pmatrix} \cdot \left( \sum_{i=1}^{M_{\text{train}}} \begin{pmatrix} \Lambda & 0_{d\times 1} \\ 0_{1\times d} & \mathrm{Tr}(\Lambda) \end{pmatrix} \right) \cdot \begin{pmatrix} (w^V_{21})^\top & w^V_{22} \end{pmatrix}^\top \nonumber\\
    &= \frac{2 \epsilon^2 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot  \Bigl( (w^V_{21})^\top \Lambda w^V_{21} +  \mathrm{Tr}(\Lambda) (w^V_{22})^2 \Bigr).
    \label{eq:icl:proof:zero-grad:subloss3:simplified-2}
\end{align}
According to Eq.~(\ref{eq:icl:proof:zero-grad:subloss3:simplified-2}), $\ell_3(\theta)$ does not depend on $w^{KQ}_{21}$, which means that $\partial_{w^{KQ}_{21}} \ell_3(\theta) = 0_{1\times d}$.
On the other hand, based on Eq.~(\ref{eq:icl:proof:zero-grad:subloss3:simplified-2}), when $w^{V}_{21} = 0$, the derivative of $\ell_3(\theta)$ with respect to $w^{V}_{21}$ is calculated as follows,
\begin{align*}
    &\left. \partial_{w^{V}_{21}} \ell_3(\theta) \right|_{w^{V}_{21} = 0}
    = \left. \partial_{w^{V}_{21}} \Bigl[  \frac{2 \epsilon^2 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot \Bigl( (w^V_{21})^\top \Lambda w^V_{21} +  \mathrm{Tr}(\Lambda) (w^V_{22})^2 \Bigr) \Bigr] \right|_{w^{V}_{21} = 0}
    \nonumber\\
    &= \left. \frac{2 \epsilon^2 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot \partial_{w^{V}_{21}} \Bigl[ (w^V_{21})^\top \Lambda w^V_{21} \Bigr] \right|_{w^{V}_{21} = 0}
    \nonumber\\
    &= \left. \frac{4 \epsilon^2 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot \Bigr[ ( w^V_{21})^\top \Lambda \Bigr] \right|_{w^{V}_{21} = 0}
    \nonumber\\
    &= \frac{4 \epsilon^2 M_{\text{train}}^2 }{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \| W^{KQ}_{11} x_{\tau,q} \|_2^2 \Bigr] \cdot 0_{d\times 1}^\top \Lambda
    = 0_{1\times d},
\end{align*}
which justifies our claim in Step~3.

\textbf{Step 4: Show that $w^{V}_{21} = w^{KQ}_{21} = 0_{d\times 1}$ indicates $\partial_{w^V_{21}} \ell_{4}(\theta) = \partial_{w^{KQ}_{21}} \ell_4(\theta) = 0_{1\times d}$.}
When $w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}$, based on the expression of $\ell_4(\theta)$ given in  Eq.~(\ref{eq:icl:proof:zero-grad:subloss4}), the derivative of $\ell_4(\theta)$ with respect to $w^V_{21}$ is calculated as follows,
\begin{align*}
    &\left. \partial_{w^{V}_{21}} \ell_4(\theta) \right|_{w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}}
    = \left. \partial_{w^{V}_{21}} \Bigl[ \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 }  \|w^{V}_{21}\|_2^2  \cdot \E_\tau \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \Bigr] \right|_{w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}}
    \nonumber\\
    &= \left. \Bigl[ \frac{4 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 }  \cdot \E_\tau \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \cdot (w^V_{21})^\top \Bigr] \right|_{w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}}
    \nonumber\\
    &= \frac{4 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 }  \cdot \E_\tau \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ 0_{d\times 1}^\top \end{pmatrix} x_{\tau,q} \|_2^2 \cdot 0_{d\times 1}^\top
    = 0_{1 \times d}.
\end{align*}
Besides, for the derivative of $\ell_4(\theta)$ with respect to $w^{KQ}_{21}$, we also have that
\begin{align*}
    &\left. \partial_{w^{KQ}_{21}} \ell_4(\theta) \right|_{w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}}
    = \left. \partial_{w^{KQ}_{21}} \Bigl[ \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 }  \|w^{V}_{21}\|_2^2  \cdot \E_\tau \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \Bigr] \right|_{w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}}
    \nonumber\\
    &= \left. \Bigl[ \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 } \cdot \|w^{V}_{21}\|_2^2  \cdot \partial_{w^{KQ}_{21}} \E_\tau \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \Bigr] \right|_{w^V_{21} = w^{KQ}_{21} = 0_{d\times 1}}
    \nonumber\\
    &= \frac{2 \epsilon^2 M_{\text{train}} }{ (N+M_{\text{train}})^2 } \cdot \|0_{d\times 1}\|_2^2 \cdot \left. \partial_{w^{KQ}_{21}} \Bigl[ \E_\tau \| \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix}^\top \begin{pmatrix} W^{KQ}_{11} \\ (w^{KQ}_{21})^\top \end{pmatrix} x_{\tau,q} \|_2^2 \Bigr] \right|_{w^{KQ}_{21} = 0_{d\times 1}}
    = 0_{1\times d}.
\end{align*}
The above two equations justify the claim in Step~4.

\textbf{Step~5:}
Based on results from previous Steps~1 to 4, we eventually have that
\begin{align*}
    &\left. \partial_{w^V_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta) \right|_{w^V_{21}=w^{KQ}_{21}=0_{d\times 1}}
    = \left. \partial_{w^V_{21}} [ \ell_1(\theta) + \ell_2(\theta) + \ell_3(\theta) + \ell_4(\theta) ] \right|_{w^V_{21}=w^{KQ}_{21}=0_{d\times 1}}
    = \sum_{i=1}^4 0_{1\times d}
    = 0_{1\times d},
    \nonumber\\
    &\left. \partial_{w^{KQ}_{21}} \tilde{\mathcal L}^{\text{adv}}(\theta) \right|_{w^V_{21}=w^{KQ}_{21}=0_{d\times 1}}
    = \left. \partial_{w^{KQ}_{21}} [ \ell_1(\theta) + \ell_2(\theta) + \ell_3(\theta) + \ell_4(\theta) ] \right|_{w^V_{21}=w^{KQ}_{21}=0_{d\times 1}}
    = \sum_{i=1}^4 0_{1\times d}
    = 0_{1\times d}.
\end{align*}
The proof is completed.
\end{proof}



With Lemma~\ref{lem:icl:zero-grad}, we can then simplify the surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$, as shown in the following Lemma~\ref{lem:icl:simplified-surrogate-at-loss}.

\begin{lemma}
\label{lem:icl:simplified-surrogate-at-loss}
Under Assumption~\ref{ass:icl:init}, the surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ defined in Eq.~(\ref{eq:icl:surrogate-at-loss}) can be simplified as follows,
\begin{align*}
    \tilde{\mathcal L}^{\text{\rm adv}}(\theta)
    = 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4 \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda),
\end{align*}
where
$\Gamma(M) := \frac{ N + M + 1 }{ N + M } \Lambda + \frac{ \mathrm{Tr}(\Lambda) }{ N + M } I_d$
and
$\psi(M) := \frac{ M^2 \mathrm{Tr}(\Lambda) }{ (N + M)^2 }$
are same functions as that defined in Eqs.~(\ref{eq:icl:func-gamma}) and~(\ref{eq:icl:func-psi}).
\end{lemma}

\begin{proof}
When Assumption~\ref{ass:icl:init} holds, by applying Lemma~\ref{lem:icl:zero-grad}, one can substitute terms $w^V_{21}$ and $w^{KQ}_{21}$ in the surrogate AT loss $\tilde{\mathcal L}^{\text{adv}}(\theta)$ with the zero vector $0_{d\times 1}$, which thus simplifies $\tilde{\mathcal L}^{\text{adv}}(\theta)$ as follows,
\begin{align}
    &\tilde{\mathcal L}^{\text{adv}}(\theta)
    % \nonumber \\
    = 2 \E_\tau \Bigl[
        \begin{pmatrix} 0_{1\times d} & w^V_{22} \end{pmatrix}  \frac{ \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix} \begin{pmatrix} X_{\tau} & X^{\text{sfx}}_\tau & x_{\tau,q} \\ Y_{\tau} & Y^{\text{sfx}}_{\tau} & 0 \end{pmatrix}^\top }{ N + M_{\text{train}} }  \begin{pmatrix} W^{KQ}_{11} \\ 0_{1\times d} \end{pmatrix} x_{\tau,q} - y_{\tau,q} \Bigr]^2
    \nonumber\\
    &\quad\quad\quad\quad
    % &\quad
        + 0
        + \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \E_\tau \Bigl[ \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \| \begin{pmatrix} 0_{1\times d} & w^V_{22} \end{pmatrix} \begin{pmatrix} X^{\text{sfx}}_{\tau} \\ Y^{\text{sfx}}_{\tau} \end{pmatrix} \|_2^2 \Bigr]
        + 0
    \nonumber\\
    % &\quad\quad\quad
    &= \underbrace{ 2 \cdot \E_\tau \Bigl[ w^V_{22} \cdot \frac{ Y_\tau X_\tau + Y^{\text{sfx}}_{\tau} X^{\text{sfx}}_\tau }{ N + M_{\text{train}} } \cdot W^{KQ}_{11} x_{\tau,q} - y_{\tau,q} \Bigr]^2 }_{:= B_1(\theta)}
        + \underbrace{ \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \| w^V_{22} Y^{\text{sfx}}_{\tau} \|_2^2 \Bigr] }_{:= B_2(\theta)}.
    \label{eq:icl:proof:simplified-surrogate-at-loss:loss-v1}
\end{align}


\textbf{For the term $B_1(\theta)$ in Eq.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1}), we have that}
\begin{align}
    &B_1(\theta)
    := 2\cdot \E_\tau \Bigl[ w^V_{22} \cdot \frac{ Y_\tau X_\tau^\top + Y^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top }{ N+M_{\text{train}} } \cdot W^{KQ}_{11}  x_{\tau,q} - y_{\tau,q} \Bigr]^2
    \nonumber\\
    &= 2\cdot \E_\tau \Bigl[ \frac{ w_\tau^\top \cdot ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) }{ N+M_{\text{train}} } \cdot w^V_{22} W^{KQ}_{11} \cdot x_{\tau,q} - w_\tau^\top x_{\tau,q} \Bigr]^2 \nonumber\\
    &= 2\cdot \E_\tau \left[ \Bigl[ \frac{ X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top }{ N+M_{\text{train}} } \cdot w^V_{22} W^{KQ}_{11} \cdot x_{\tau,q} - x_{\tau,q} \Bigr]^\top \cdot w_\tau w_\tau^\top \cdot \Bigl[ \frac{ X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top }{ N+M_{\text{train}} } \cdot w^V_{22} W^{KQ}_{11} \cdot x_{\tau,q} - x_{\tau,q} \Bigr] \right] \nonumber\\
    &= 2 \cdot \E_\tau \left[ \Bigl[ \frac{ X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top }{ N+M_{\text{train}} } \cdot w^V_{22} W^{KQ}_{11}  x_{\tau,q} - x_{\tau,q} \Bigr]^\top \cdot I_d \cdot \Bigl[ \frac{ X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top }{ N+M_{\text{train}} } \cdot w^V_{22} W^{KQ}_{11}  x_{\tau,q} - x_{\tau,q} \Bigr] \right] \nonumber\\
    &= 2\cdot \E_\tau \Bigl[ x_{\tau,q}^\top \cdot (w^V_{22} W^{KQ}_{11})^\top \cdot \frac{ \E_\tau \Bigl[ ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) \Bigr] }{ (N+M_{\text{train}})^2 } \cdot w^V_{22} W^{KQ}_{11} \cdot x_{\tau,q} \Bigr]
    \nonumber\\
    &\quad
        - 4\cdot \E_{\tau} \Bigl[ x_{\tau,q}^\top \cdot \frac{ \E_{\tau} \Bigl[ ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) \Bigr] }{ N+M_{\text{train}} } \cdot (w^V_{22} W^{KQ}_{11}) \cdot x_{\tau,q} \Bigr]
        + 2\cdot \E_{\tau} \Bigl[ x_{\tau,q}^\top \cdot x_{\tau,q} \Bigr].
    \label{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1}
\end{align}


For
$\E_\tau \Bigl[ ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) \Bigr]$
in Eq.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1}),
we have
\begin{align}
    &\E_\tau \Bigl[ ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) ( X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ) \Bigr]
    \nonumber\\
    &= \E_\tau [ X_\tau X_\tau^\top X_\tau X_\tau^\top ]
        + \E_\tau [ X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top] \cdot \E_\tau [ X_\tau X_\tau^\top ]
        + \E_\tau [ X_\tau X_\tau^\top] \cdot \E_\tau [ X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ]
        + \E_\tau [ X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top ]
    \nonumber\\
    &= \E_\tau \Bigl[ \sum_{i,j} x_{\tau,i} x_{\tau,i}^\top x_{\tau,j} x_{\tau,j}^\top \Bigr]
        + \E_\tau \Bigl[ \sum_{i} x^{\text{sfx}}_{\tau,i} (x^{\text{sfx}}_{\tau,i})^\top \Bigr] \cdot \E_\tau \Bigl[ \sum_i x_{\tau,i} x_{\tau,i}^\top \Bigr]
    \nonumber\\
    &\quad
        + \E_\tau \Bigl[ \sum_i x_{\tau,i} x_{\tau,i}^\top \Bigr] \cdot \E_\tau \Bigl[ x^{\text{sfx}}_{\tau,i} (x^{\text{sfx}}_{\tau,i})^\top \Bigr]
        + \E_\tau \Bigl[ \sum_{i,j} x^{\text{sfx}}_{\tau,i} (x^{\text{sfx}}_{\tau,i})^\top x^{\text{sfx}}_{\tau,j} (x^{\text{sfx}}_{\tau,j})^\top \Bigr]
    \nonumber\\
    &= \E_\tau \Bigl[ \sum_{i} x_{\tau,i} x_{\tau,i}^\top x_{\tau,i} x_{\tau,i}^\top + \sum_{1\leq i,j \leq N, i\neq j} \Lambda^2 \Bigr]
        + M_{\text{train}} \Lambda \cdot N \Lambda
        + N \Lambda \cdot M_{\text{train}} \Lambda
        + \E_\tau \Bigl[ \sum_{i} x^{\text{sfx}}_{\tau,i} (x^{\text{sfx}}_{\tau,i})^\top x^{\text{sfx}}_{\tau,i} (x^{\text{sfx}}_{\tau,i})^\top + \sum_{1\leq i,j \leq M_{\text{train}}, i\neq j} \Lambda^2 \Bigr]
    \nonumber\\
    &= \E_\tau \Bigl[ \sum_{i=1}^N \underbrace{ (2 \Lambda^2 + \mathrm{Tr}(\Lambda) \Lambda) }_{\text{by Lemma~\ref{lem:icl:tech-x-pow4}}} \Bigr] + (N^2-N) \cdot \Lambda^2
        + 2 N M_{\text{train}} \cdot \Lambda^2
        + \E_\tau \Bigl[ \sum_{i=1}^{M_{\text{train}}} \underbrace{ (2 \Lambda^2 + \mathrm{Tr}(\Lambda) \Lambda) }_{\text{by Lemma~\ref{lem:icl:tech-x-pow4}}} \Bigr] + (M_{\text{train}}^2 - M_{\text{train}}) \cdot \Lambda^2
    \nonumber\\
    &= ( N^2+N + M_{\text{train}}^2 + M_{\text{train}} + 2N M_{\text{train}} ) \cdot \Lambda^2 + (N + M_{\text{train}}) \cdot \mathrm{Tr}(\Lambda) \cdot \Lambda
    \nonumber\\
    &= (N + M_{\text{train}}) \cdot ( (N + M_{\text{train}} + 1) \cdot \Lambda^2 + \mathrm{Tr}(\Lambda) \cdot \Lambda )
    % \nonumber\\
    = (N + M_{\text{train}})^2 \cdot \Gamma(M_{\text{train}}) \Lambda.
    \label{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1:term1}
\end{align}

For $\E_\tau \Bigl[ X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top \Bigr]$ in Eq.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1}), we have
\begin{align}
    \E_\tau \Bigl[ X_\tau X_\tau^\top + X^{\text{sfx}}_\tau (X^{\text{sfx}}_\tau)^\top \Bigr]
    = \E_\tau \Bigl[ \sum_i x_{\tau,i} x_{\tau,i}^\top \Bigr] + \E_\tau \Bigl[ \sum_i x^{\text{sfx}}_{\tau,i} (x^{\text{sfx}}_{\tau,i})^\top \Bigr]
    = N \Lambda + M_{\text{train}} \Lambda
    = (N + M_{\text{train}}) \cdot \Lambda.
    \label{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1:term2}
\end{align}

Inserting Eqs.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1:term1}) and~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1:term2}) into Eq.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-v1}) leads to
\begin{align}
    &B_1(\theta)
    = 2\cdot \E_{\tau} \Bigl[ x_{\tau,q}^\top \cdot (w^V_{22} W^{KQ}_{11})^\top \cdot \Gamma(M_{\text{train}}) \Lambda \cdot w^V_{22} W^{KQ}_{11} \cdot x_{\tau,q} \Bigr]
        - 4\cdot \E_{\tau} \Bigl[ x_{\tau,q}^\top \cdot \Lambda \cdot w^V_{22} W^{KQ}_{11} \cdot x_{\tau,q} \Bigr]
        + 2\cdot \E_{\tau} \Bigl[ x_{\tau,q}^\top \cdot x_{\tau,q} \Bigr].
    \nonumber\\
    &= 2\cdot \underbrace{ \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11})^\top \cdot \Gamma(M_{\text{train}}) \Lambda \cdot w^V_{22} W^{KQ}_{11} \cdot \Lambda \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech-x-quad}}}
        - 4\cdot \underbrace{ \mathrm{Tr}\Bigl[ \Lambda \cdot w^V_{22} W^{KQ}_{11} \cdot \Lambda \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech-x-quad}}}
        + 2\cdot \mathrm{Tr}(\Lambda)
    \nonumber\\
    &= 2\cdot \underbrace{ \mathrm{Tr}\Bigl[ \Gamma(M_{\text{train}})\Lambda \cdot (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech:mat-permute}}}
        - 4\cdot \underbrace{ \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech:mat-permute}}}
        + 2\cdot \mathrm{Tr}(\Lambda).
    \label{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-fin}
\end{align}


\textbf{Besides, for the term $B_2(\theta)$ in Eq.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1}), we have that}
\begin{align}
    &B_2(\theta)
    := \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot \E_\tau \Bigl[ \|W^{KQ}_{11} x_{\tau,q}\|_2^2 \cdot \| w^V_{22} Y^{\text{sfx}}_{\tau} \|_2^2 \Bigr]
    \nonumber\\
    &= \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot (w^{V}_{22})^2 \cdot \E_\tau \Bigl[ x_{\tau,q}^\top \cdot (W^{KQ}_{11})^\top W^{KQ}_{11} \cdot x_{\tau,q} \Bigr] \cdot \E_{\tau} \Bigl[ w_\tau^\top \cdot X^{\text{sfx}}_{\tau} (X^{\text{sfx}}_{\tau})^\top \cdot w_\tau \Bigr]
    \nonumber\\
    &= \frac{2 \epsilon^2 M_{\text{train}}}{(N+M_{\text{train}})^2} \cdot (w^{V}_{22})^2 \cdot \underbrace{ \mathrm{Tr}\Bigl[ (W^{KQ}_{11})^\top W^{KQ}_{11} \cdot \Lambda \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech-x-quad}}} \cdot \E_{\tau} \Bigl[ w_\tau^\top \cdot M_{\text{train}} \Lambda \cdot w_\tau \Bigr]
    \nonumber\\
    &= \frac{2 \epsilon^2 M_{\text{train}} }{(N+M_{\text{train}})^2} \cdot (w^{V}_{22})^2 \cdot \underbrace{ \mathrm{Tr}\Bigl[ W^{KQ}_{11} \cdot \Lambda \cdot (W^{KQ}_{11})^\top \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech:mat-permute}}} \cdot \underbrace{ \mathrm{Tr}\Bigl[ M_{\text{train}} \Lambda \cdot I_d \Bigr] }_{\text{by Lemma~\ref{lem:icl:tech-x-quad}}}
    \nonumber\\
    &= 2\epsilon^2 \cdot \frac{M_{\text{train}}^2 \mathrm{Tr}(\Lambda) }{(N+M_{\text{train}} \Lambda^{\frac{1}{2}})^2} \cdot \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda \cdot (w^V_{22} W^{KQ}_{11})^\top \Bigr]
    \nonumber\\
    &= 2\epsilon^2 \cdot \psi(M_{\text{train}}) \cdot \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda \cdot (w^V_{22} W^{KQ}_{11})^\top \Bigr].
    \label{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term2-fin}
\end{align}

\textbf{Finally, by inserting Eqs.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term1-fin}) and~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1:term2-fin}) into Eq.~(\ref{eq:icl:proof:simplified-surrogate-at-loss:loss-v1}), we have}
\begin{align*}
    \tilde{\mathcal L}^{\text{adv}}(\theta)
    :=& 2\cdot \mathrm{Tr}\Bigl[ \Gamma(M_{\text{train}})\Lambda \cdot (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4\cdot \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2\cdot \mathrm{Tr}(\Lambda)
    \nonumber\\
    &
        + 2\epsilon^2 \cdot \psi(M_{\text{train}}) \cdot \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda \cdot (w^V_{22} W^{KQ}_{11})^\top \Bigr]
    \nonumber\\
    =& 2\cdot \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{train}})\Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d ) \cdot (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4\cdot \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2\cdot \mathrm{Tr}(\Lambda),
\end{align*}
which completes the proof.
\end{proof}


Based on the simplified surrogate AT loss, the closed-form global minimizer $\theta_*$ for the surrogate AT problem is then calculated in the following Lemma~\ref{lem:icl:surrogate-at-minimizer}.
\begin{lemma}
\label{lem:icl:surrogate-at-minimizer}
Suppose Assumption~\ref{ass:icl:init} holds.
Then, $\theta_* := (W^V_* W^{KQ}_*)$ is a minimizer for the surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ in Eq.~(\ref{eq:icl:at-loss}) if and only if
$w^{V}_{*,22} W^{KQ}_{*,11} = (\Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d)^{-1} \Lambda$.
\end{lemma}


\begin{proof}
For the simplified surrogate AT loss proved in Lemma~\ref{lem:icl:simplified-surrogate-at-loss}, we rewrite it as follows,
\begin{align}
    &\tilde{\mathcal L}^{\text{\rm adv}}(\theta)
    \nonumber\\
    &= 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4 \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &= 2\cdot \mathrm{Tr}\Bigl[ ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d ) \cdot \Bigl( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}} - ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-1} \Lambda^{\frac{3}{2}} \Bigr) \cdot \Bigl( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}} - ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-1} \Lambda^{\frac{3}{2}} \Bigr)^\top \Bigr]
    \nonumber\\
    &\quad
        - \mathrm{Tr}\Bigl[ \Lambda^3 ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-1} \Bigr]
        + 2 \cdot \mathrm{Tr}(\Lambda),
    \label{eq:icl:proof:surrogate-at-minimizer:rewrite}
\end{align}
where
$\Gamma_{\text{train}} := \Gamma(M_{\text{train}})$
and
$\psi_{\text{train}} := \psi(M_{\text{train}})$.

Notice that the second and third terms in Eq.~(\ref{eq:icl:proof:surrogate-at-minimizer:rewrite}) are constants.
Besides, the matrix $( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi I_d )$ in the first term in Eq.~(\ref{eq:icl:proof:surrogate-at-minimizer:rewrite}) is positive definite, which means that this first term is non-negative.
As a result, the surrogate AT loss $\tilde{\mathcal L}^{\text{adv}}(\theta)$ will be minimized when the first term in Eq.~(\ref{eq:icl:proof:surrogate-at-minimizer:rewrite}) becomes zero.
This can be achieved by setting
\begin{align*}
    w^V_{*,22} W^{KQ}_{*,11} \Lambda^{\frac{1}{2}} - ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda^{\frac{3}{2}} = 0,
\end{align*}
which is
\begin{align*}
    w^V_{*,22} W^{KQ}_{*,11} = ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda.
\end{align*}

The proof is completed.
\end{proof}


We now turn to prove an PL-inequality for the surrogate AT problem.
The proof idea follows that in \citet{zhang2024trained}.
Specifically, we will first prove several technical lemmas ({\it i.e.}, Lemma~\ref{lem:icl:init-equal-pms}, Lemma~\ref{lem:icl:wv22-geq-zero}, and Lemma~\ref{lem:icl:wv22-lower-bd}), and then present the PL-inequality in Lemma~\ref{lem:icl:pl-inequality}, which can then enable the surrogate AT model
in Eq.~(\ref{eq:icl:surrogate-at-loss}) approaches its global optimal solution.



\begin{lemma}
\label{lem:icl:init-equal-pms}
Suppose Assumption~\ref{ass:icl:init} holds and the model $f_{\text{\rm LSA},\theta}$ is trained via minimizing the surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ in Eq.~(\ref{eq:icl:surrogate-at-loss}) with continuous training flow.
Then, for any continuous training time $t \geq 0$, we uniformly have that
\begin{align*}
    (w^V_{22}(t))^2 = \mathrm{Tr}[ W^{KQ}_{11}(t) (W^{KQ}_{11}(t))^\top ].
\end{align*}
\end{lemma}
\begin{proof}
Since the model is trained via continuous gradient flow, thus $\partial_{t} W^{KQ}_{11}(t)$ can be calculated based on the simplified surrogate AT loss proved in Lemma~\ref{lem:icl:simplified-surrogate-at-loss} as follows,
\begin{align}
    &\partial_t W^{KQ}_{11}(t)
    := -\partial_{W^{KQ}_{11}} \tilde{\mathcal L}^{\text{adv}}(\theta)
    \nonumber\\
    &= -2 \cdot \partial_{W^{KQ}_{11}} \mathrm{Tr}\Big[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        + 4 \cdot \partial_{W^{KQ}_{11}} \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
    \nonumber\\
    &= -2 \cdot (w^{V}_{22})^2 \cdot \partial_{W^{KQ}_{11}} \mathrm{Tr}\Big[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot W^{KQ}_{11} \cdot \Lambda \cdot ( W^{KQ}_{11} )^\top \Bigr]
        + \underbrace{ 4 w^V_{22} \Lambda^2 }_{\text{by Lemma~\ref{lem:icl:tech:trace-diff}}}
    \nonumber\\
    &= \underbrace{ -4 \cdot (w^{V}_{22})^2 \cdot ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot W^{KQ}_{11} \cdot \Lambda }_{\text{by Lemma~\ref{lem:icl:tech:trace-diff}}}
        + 4 w^V_{22} \Lambda^2.
    \label{eq:icl:proof:init-equal-pms:diff-wkq11}
\end{align}
Similarly, for $\partial_t w^{V}_{22}(t)$, we have
\begin{align}
    &\partial_t w^{V}_{22}(t)
    := -\partial_{w^{V}_{22}} \tilde{\mathcal L}^{\text{adv}}(\theta)
    \nonumber\\
    &= -2 \cdot \partial_{w^{V}_{22}} \mathrm{Tr}\Big[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        + 4 \cdot \partial_{w^{V}_{22}} \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
    \nonumber\\
    &= -4 w^V_{22} \cdot \mathrm{Tr}\Big[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        + 4 \cdot \mathrm{Tr}\Bigl[ (W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr].
    \label{eq:icl:proof:init-equal-pms:diff-wv22}
\end{align}

Combining Eqs~(\ref{eq:icl:proof:init-equal-pms:diff-wkq11}) and~(\ref{eq:icl:proof:init-equal-pms:diff-wv22}), we thus have
\begin{align*}
    &\mathrm{Tr}\Bigl[ \partial_t W^{KQ}_{11}(t) (W^{KQ}_{11}(t))^\top \Bigr]
    \nonumber\\
    &= -4 \cdot (w^{V}_{22})^2 \cdot \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot (W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot (W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr] + 4 w^V_{22} \cdot \mathrm{Tr}\Bigl[ \Lambda^{\frac{3}{2}} \cdot (\Lambda^{\frac{1}{2}} W^{KQ}_{11})^\top \Bigr]
    \nonumber\\
    &= (\partial_t w^V_{22}(t)) w^V_{22}(t),
\end{align*}
which further indicates that
\begin{align}
    &\partial_t \mathrm{Tr}\Bigl[ W^{KQ}_{11}(t) (W^{KQ}_{11}(t))^\top \Bigr]
    = \mathrm{Tr}\Bigl[ \partial_t W^{KQ}_{11}(t) \cdot (W^{KQ}_{11}(t))^\top \Bigr] + \mathrm{Tr}\Bigl[ W^{KQ}_{11}(t) \cdot \partial_t (W^{KQ}_{11}(t))^\top \Bigr]
    \nonumber\\
    &= (\partial_t w^V_{22}(t)) \cdot w^V_{22}(t) + W^V_{22}(t) \cdot (\partial_t w^V_{22}(t))
    = \partial_t (w^V_{22}(t)^2).
    \label{eq:icl:proof:init-equal-pms:equal-diff}
\end{align}

Finally, according to Assumption~\ref{ass:icl:init}, we have that when the continuous training time is $t=0$,
\begin{align*}
    \mathrm{Tr}\Bigl[ W^{KQ}_{11}(0) (W^{KQ}_{11}(0))^\top \Bigr]
    = \| W^{KQ}_{11}(0) \|_F^2
    = \sigma^2
    = w^V_{22}(0)^2.
\end{align*}
Combine with Eq.~(\ref{eq:icl:proof:init-equal-pms:equal-diff}), we thus have that
\begin{align*}
    \mathrm{Tr}\Bigl[ W^{KQ}_{11}(t) (W^{KQ}_{11}(t))^\top \Bigr]
    = w^V_{22}(t)^2,
    \quad \forall t\geq 0.
\end{align*}

The proof is completed.
\end{proof}


\begin{lemma}
\label{lem:icl:wv22-geq-zero}
Suppose Assumption~\ref{ass:icl:init} holds and the model $f_{\text{\rm LSA},\theta}$ is trained via minimizing the surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ in Eq.~(\ref{eq:icl:surrogate-at-loss}) with continuous training flow.
Then, if the parameter $\sigma$ in Assumption~\ref{ass:icl:init} satisfies
\begin{align*}
    \sigma
    < \sqrt{ \frac{2}{d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 } },
\end{align*}
we have $w^V_{22}(t) > 0$ holds for any continuous training time $t\geq 0$.
\end{lemma}
\begin{proof}
According to the simplified AT loss calculated in Lemma~\ref{lem:icl:simplified-surrogate-at-loss}, we know that if $w^V_{22}(t) = 0$, then $\tilde{\mathcal L}^{\text{adv}}(\theta_t) = 2\mathrm{Tr}(\Lambda)$.
Besides, under Assumption~\ref{ass:icl:init}, we have $w^V_{22}(0) = \sigma > 0$.
Therefore, if we can show that $\tilde{\mathcal L}^{\text{adv}}(\theta_t) \neq 2\mathrm{Tr}(\Lambda)$ for any $t\geq 0$, then it is proved that $w^V_{22}(t) > 0$ for any $t \geq 0$.

To this end, we first analyze the surrogate AT loss $\tilde{\mathcal L}^{\text{adv}}(\theta_t)$ at the initial training time $t=0$.
By applying Assumption~\ref{ass:icl:init}, we have
\begin{align}
    &\tilde{\mathcal L}^{\text{adv}}(\theta_0)
    \nonumber\\
    &= 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( w^V_{22}(0) W^{KQ}_{11}(0) \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22}(0) W^{KQ}_{11}(0) \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4 \mathrm{Tr}\Bigl[ (w^V_{22}(0) W^{KQ}_{11}(0) \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &= 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( \sigma^2 \Theta\Theta^\top \Lambda^{\frac{1}{2}}) \cdot ( \sigma^2 \Theta\Theta^\top \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4 \mathrm{Tr}\Bigl[ ( \sigma^2 \Theta\Theta^\top \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &= 2 \sigma^4 \cdot \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \cdot \Lambda \Theta\Theta^\top \Lambda \Theta\Theta^\top \Bigr]
        - 4 \sigma^2 \| \Lambda \Theta \|_F^2
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\leq 2 \sigma^4 \cdot \underbrace{ d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 \cdot \| \Lambda \Theta\Theta^\top \Lambda \Theta\Theta^\top \|_2 }_{\text{by Lemma~\ref{lem:icl:tech-von-trace}}}
        - 4 \sigma^2 \| \Lambda \Theta \|_F^2
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\leq 2 \sigma^4 \cdot d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 \cdot \| \Lambda \Theta\Theta^\top \Lambda \|_F \cdot \| \Theta\Theta^\top \|_F
        - 4 \sigma^2 \| \Lambda \Theta \|_F^2
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\leq 2 \sigma^4 \cdot d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 \cdot \| \Lambda \Theta \|_F^2 \cdot 1
        - 4 \sigma^2 \| \Lambda \Theta \|_F^2
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &= 2 \sigma^2 \cdot \| \Lambda \Theta \|_F^2 \cdot ( d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 - 2)
        + 2 \mathrm{Tr}(\Lambda).
    \label{eq:icl:proof:wv22-geq-zero:init-loss-upper-bd}
\end{align}
By Assumption~\ref{ass:icl:init}, we have $\|\Lambda \Theta\|_F^2 > 0$.
Thus, when
$( d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 - 2) < 0$,
which is
\begin{align*}
    \sigma
    < \sqrt{ \frac{2}{d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 } },
\end{align*}
we will have $\tilde{\mathcal L}^{\text{adv}}(\theta_0) < \mathrm{Tr}(\Lambda)$.

Finally, since the surrogate AT loss $\tilde{\mathcal L}^{\text{adv}}(\theta_t)$ is minimized with continuous gradient, thus when the above condition holds, for any $t > 0$, we always have that $\tilde{\mathcal L}^{\text{adv}}(\theta_t) \leq \tilde{\mathcal L}^{\text{adv}}(\theta_0) < \mathrm{Tr}(\Lambda)$.

The proof is completed.
\end{proof}

\begin{lemma}
\label{lem:icl:wv22-lower-bd}
Suppose Assumption~\ref{ass:icl:init} holds and the $\sigma$ in Assumption~\ref{ass:icl:init} satisfies
$\sigma < \sqrt{ \frac{2}{d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 } }$.
Then, for any continuous training time $t\geq 0$, we have
$(w^V_{22}(t))^2 \geq \nu > 0$,
where
\begin{align*}
   \nu  := \frac{ \sigma^2 \cdot \| \Lambda \Theta \|_F^2 \cdot (2 - d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2) }{ 2d \|\Lambda^2\|_2 } > 0.
\end{align*}
\end{lemma}
\begin{proof}
By applying Eq.~(\ref{eq:icl:proof:wv22-geq-zero:init-loss-upper-bd}) in Lemma~\ref{lem:icl:wv22-geq-zero}, we have that for any $t \geq 0$,
\begin{align*}
    &2 \sigma^2 \cdot \| \Lambda \Theta \|_F^2 \cdot ( d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 - 2) + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\geq \tilde{\mathcal L}^{\text{adv}}(\theta_0)
    \geq \tilde{\mathcal L}^{\text{adv}}(\theta_t)
    \nonumber \\
    &= 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4 \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber \\
    &= 2 \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d )^{\frac{1}{2}} \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \|_F^2
        - 4 \mathrm{Tr}\Bigl[ w^V_{22} W^{KQ}_{11} \Lambda^{2} \Bigr]
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber \\
    &\geq 0 - \underbrace{ 4d \cdot |w^V_{22}| \cdot \|\Lambda^2\|_2 \cdot \|W^{KQ}_{11}\|_2 }_{\text{by Lemma~\ref{lem:icl:tech-von-trace}}}
        + 2 \mathrm{Tr}(\Lambda),
\end{align*}
which indicates
\begin{align*}
    &2 \sigma^2 \cdot \| \Lambda \Theta \|_F^2 \cdot ( d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 - 2)
    \geq  -4d \cdot |w^V_{22}| \cdot \|\Lambda^2\|_2 \cdot \|W^{KQ}_{11}\|_F,
\end{align*}
thus
\begin{align}
    |w^V_{22}| \cdot \|W^{KQ}_{11}\|_F
    \geq \frac{ \sigma^2 \cdot \| \Lambda \Theta \|_F^2 \cdot (2 - d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2) }{ 2d \|\Lambda^2\|_2 }.
    \label{eq:icl:proof:wv22-lower-bd:eq1}
\end{align}
Besides, by combining Lemma~\ref{lem:icl:init-equal-pms} and Lemma~\ref{lem:icl:wv22-geq-zero}, we know that
\begin{align}
    w^V_{22}(t)
    = \sqrt{ \mathrm{Tr}[ W^{KQ}_{11}(t) (W^{KQ}_{11}(t))^\top ] }
    = \sqrt{ \| W^{KQ}_{11}(t) \|_F^2 }
    = \| W^{KQ}_{11}(t) \|_F.
    \label{eq:icl:proof:wv22-lower-bd:eq2}
\end{align}
Finally, inserting Eq.~(\ref{eq:icl:proof:wv22-lower-bd:eq2}) into Eq.~(\ref{eq:icl:proof:wv22-lower-bd:eq1}), we thus have
\begin{align*}
    (w^V_{22}(t))^2
    \geq \frac{ \sigma^2 \cdot \| \Lambda \Theta \|_F^2 \cdot (2 - d \cdot \sigma^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2) }{ 2d \|\Lambda^2\|_2 } > 0.
\end{align*}

The proof is completed.
\end{proof}


\begin{lemma}[PL-inequality]
\label{lem:icl:pl-inequality}
Suppose Assumption~\ref{ass:icl:init} holds and the LSA model $f_{\text{\rm LSA},\theta}$ is trained via minimizing the surrogate AT loss $\tilde{\mathcal L}^{\text{\rm adv}}(\theta)$ in Eq.~(\ref{eq:icl:surrogate-at-loss}) with continuous training flow.
Suppose the $\sigma$ in Assumption~\ref{ass:icl:init} satisfies
$\sigma < \sqrt{ \frac{2}{d \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \Lambda^{-1} \|_2 } }$.
Then for any continuous training time $t > 0$, we uniformly have that
\begin{align*}
    \| \partial_{\theta} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) \|_2^2
    \geq \mu
        \cdot \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr),
\end{align*}
where
\begin{align*}
    \mu := \frac{8 \nu}{ \| ( \Gamma_{\text{\rm train}} \Lambda + \epsilon^2 \psi_{\text{\rm train}} I_d )^{-\frac{1}{2}} \|_F^2 \cdot \| \Lambda^{-\frac{1}{2}} \|_F^2 },
\end{align*}
$\nu$ is defined in Lemma~\ref{lem:icl:wv22-lower-bd},
and $\mathrm{Vec}(\cdot)$ denotes the vectorization function.
\end{lemma}


\begin{proof}
From Eq.~(\ref{eq:icl:proof:init-equal-pms:diff-wkq11}) in Lemma~\ref{lem:icl:init-equal-pms}, we have that
\begin{align*}
    &\partial_t W^{KQ}_{11}(t)
    = -4 \cdot (w^{V}_{22})^2 \cdot ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot W^{KQ}_{11} \cdot \Lambda
        + 4 w^V_{22} \Lambda^2
    \nonumber\\
    &= -4 w^V_{22} \cdot ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot D(\theta_t) \cdot \Lambda^{\frac{1}{2}},
\end{align*}
where
\begin{align}
    D(\theta_t) := \Bigl( w^{V}_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}} - ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d )^{-1} \Lambda^{\frac{3}{2}} \Bigr) \in \mathbb{R}^{d\times d}.
\end{align}
As a result, the gradient norm square $ \| \partial_{\theta} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) \|_2^2$ can be further lower-bounded as follows,
\begin{align}
    &\| \partial_{\theta} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) \|_2^2
    :=  ( \partial_{w^V_{22}} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) )^2
        + \| \partial_{W^{KQ}_{11}} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) \|_F^2
    \nonumber\\
    &\geq \| \partial_{W^{KQ}_{11}} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) \|_F^2
    \nonumber\\
    &= \| 4 \cdot w^{V}_{22} \cdot ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot D(\theta_t) \cdot \Lambda^{\frac{1}{2}} \|_F^2
    \nonumber\\
    &= 16 \cdot (w^{V}_{22})^2 \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot D(\theta_t) \cdot \Lambda^{\frac{1}{2}} \|_F^2
    \nonumber\\
    &\geq \underbrace{ 16 \cdot \nu }_{\text{by Lemma~\ref{lem:icl:wv22-lower-bd}}} \cdot \| ( \Gamma(M_{\text{\rm train}}) \Lambda + \epsilon^2 \psi(M_{\text{\rm train}}) I_d ) \cdot D(\theta_t) \cdot \Lambda^{\frac{1}{2}} \|_F^2,
    \label{eq:icl:proof:pl-inequality:gd-lower-bd-v1}
\end{align}
where $\nu > 0$ is defined in Lemma~\ref{lem:icl:wv22-lower-bd}.


Meanwhile, according to the proof of Lemma~\ref{lem:icl:surrogate-at-minimizer}, we can rewrite and upper-bound 
$\Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr)$
as follows,
\begin{align}
    &\Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr)
    \nonumber\\
    &= 2\cdot \mathrm{Tr}\Bigl[ ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d ) \cdot \Bigl( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}} - ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-1} \Lambda^{\frac{3}{2}} \Bigr) \cdot \Bigl( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}} - ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-1} \Lambda^{\frac{3}{2}} \Bigr)^\top \Bigr]
    \nonumber\\
    &= 2 \cdot \mathrm{Tr}[ ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d ) \cdot D(\theta_t) \cdot D(\theta_t)^\top ]
    \nonumber\\
    &= 2 \cdot \underbrace{ \mathrm{Tr}[ ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{\frac{1}{2}} \cdot D(\theta_t) \cdot D(\theta_t)^\top \cdot ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{\frac{1}{2}} ] }_{\text{Lemma~\ref{lem:icl:tech:mat-permute}}}
    \nonumber\\
    &= 2 \cdot \| ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{\frac{1}{2}} \cdot D(\theta_t) \|_F^2
    \nonumber\\
    &\leq 2 \cdot \| ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-\frac{1}{2}} \|_F^2 \cdot \| \Lambda^{-\frac{1}{2}} \|_F^2 \cdot \| ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d ) \cdot D(\theta_t) \cdot \Lambda^{\frac{1}{2}} \|_F^2,
    \label{eq:icl:proof:pl-inequality:loss-upper-bd}
\end{align}
where $\Gamma_{\text{train}} := \Gamma(M_{\text{train}})$ and $\psi_{\text{train}} := \psi(M_{\text{train}})$.

Combining Eqs.~(\ref{eq:icl:proof:pl-inequality:gd-lower-bd-v1}) and~(\ref{eq:icl:proof:pl-inequality:loss-upper-bd}), we thus know that
\begin{align*}
    &\| \partial_{\theta} \tilde{\mathcal L}^{\text{\rm adv}}(\theta_t) \|_2^2
    \geq \frac{8 \nu}{ \| ( \Gamma_{\text{train}} \Lambda + \epsilon^2 \psi_{\text{train}} I_d )^{-\frac{1}{2}} \|_F^2 \cdot \| \Lambda^{-\frac{1}{2}} \|_F^2 }
        \cdot \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr).
\end{align*}

The proof is completed.
\end{proof}


Finally, we prove Theorem~\ref{thm:icl:closed-form-at} based on Lemma~\ref{lem:icl:surrogate-at-minimizer} and Lemma~\ref{lem:icl:pl-inequality}.

\begin{proof}[Proof of Theorem~\ref{thm:icl:closed-form-at}]
When all the conditions hold, when the surrogate AT problem defined in Eq.~(\ref{eq:icl:surrogate-at-loss}) is solved via continuous gradient flow, by Lemma~\ref{lem:icl:surrogate-at-minimizer} we have
\begin{align*}
    &\partial_t \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr)
    = \partial_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta_t) \cdot \partial_{t}\theta_t
    = \partial_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta_t) \cdot (- \partial_{\theta}^\top \tilde{\mathcal L}^{\text{adv}}(\theta_t))
    = -\| \partial_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta_t) \|_2^2
    \nonumber\\
    &\leq -\mu \cdot \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr),
\end{align*}
which means
\begin{align*}
    \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr) \leq \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_0) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr) \cdot e^{-\mu t}.
\end{align*}
As a result, when performing continuous gradient flow optimization for an infinitely long time, since $\mu > 0$, the surrogate AT loss will eventually converge to the global minima, {\it i.e.},
\begin{align*}
    \Bigl( \tilde{\mathcal L}^{\text{adv}}(\theta_*) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta) \Bigr)
    = 
    \lim_{t \rightarrow \infty} \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_t) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr) \leq \Bigl(\tilde{\mathcal L}^{\text{adv}}(\theta_0) - \min_{\theta} \tilde{\mathcal L}^{\text{adv}}(\theta)\Bigr) \cdot \lim_{t\rightarrow\infty} e^{-\mu t}= 0,
\end{align*}
where $\theta_* := \lim_{t\rightarrow\infty} \theta_t$ is the converged model parameter.
Meanwhile, from Lemma~\ref{lem:icl:surrogate-at-minimizer}, we know that $\theta_*$ is a global minimizer if and only if
$w^V_{*,22} W^{KQ}_{*,11} = ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda$,
which completes the proof.
\end{proof}


\subsection{Proofs in Section~\ref{sec:robust-gen}}
\label{app:proof:sec-robust-gen}

This section collects all proofs that omitted from Section~\ref{sec:robust-gen}.

% \subsection{Proof of Theorem~\ref{thm:icl:robust-gen-bound}}
% \label{app:proof:robust-gen-bound}


\begin{proof}[Proof of Theorem~\ref{thm:icl:robust-gen-bound}]
By substituting all $M_{\text{train}}$ with $M_{\text{test}}$ in proofs of Proposition~\ref{prop:icl:surrogate-bound} and Lemma~\ref{lem:icl:simplified-surrogate-at-loss}, we immediately have that for any model parameter $\theta$ of the LSA model $f_{\text{LSA},\theta}$,
\begin{align*}
    \mathcal{R}(\theta, M_{\text{test}})
    \leq 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{test}}) \Lambda + \epsilon^2 \psi(M_{\text{test}}) I_d ) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot ( w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}})^\top \Bigr]
        - 4 \mathrm{Tr}\Bigl[ (w^V_{22} W^{KQ}_{11} \Lambda^{\frac{1}{2}}) \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda).
\end{align*}
By inserting the converged model parameter $\theta_*(M_{\text{train}})$, which satisfies $(w^V_{*,22} W^{KQ}_{*,11}) = ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda$, into the above robust generalization bound, we thus have that
\begin{align*}
    &\mathcal{R}(\theta_*(M_{\text{train}}), M_{\text{test}})
    \nonumber\\
    &\leq 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{test}}) \Lambda + \epsilon^2 \psi(M_{\text{test}}) I_d ) \cdot ( ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda \cdot \Lambda^{\frac{1}{2}}) \cdot (( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda \cdot \Lambda^{\frac{1}{2}})^\top \Bigr]
    \nonumber\\
    &\quad
        - 4 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda \cdot \Lambda^{\frac{1}{2}} \cdot \Lambda^{\frac{3}{2}} \Bigr]
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\overset{(*)}{\leq} 2 \mathrm{Tr}\Bigl[ ( \Gamma(M_{\text{test}}) \Lambda + \epsilon^2 \psi(M_{\text{test}}) I_d ) \cdot ( ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \cdot \Lambda^3 \cdot (( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1})^\top \Bigr]
        + 0 + 2\mathrm{Tr}(\Lambda)
    \nonumber\\
    &\overset{(**)}{\leq} 2 \mathrm{Tr}\Bigl[ \Lambda^3 \cdot ( \Gamma(M_{\text{test}}) \Lambda + \epsilon^2 \psi(M_{\text{test}}) I_d ) \cdot ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-2} \Bigr]
        + 2\mathrm{Tr}(\Lambda),
\end{align*}
where $(*)$ is due to that the matrix $(( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1} \Lambda^{3})$ is positive definite, and $(**)$ is due to that: (1) $( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-1}$ is symmetric and is commutative with $\Lambda^3$, and (2) Lemma~\ref{lem:icl:tech:mat-permute}.

The proof is completed.
\end{proof}





\begin{proof}[Proof of Corollary~\ref{cor:icl:ord-robust-gen-bound}]
Let $\lambda_1, \cdots, \lambda_d$ be the $d$ singular values of the matrix $\Lambda$.
Then, the robust generalization bound in Theorem~\ref{thm:icl:robust-gen-bound} can be rewritten as follows,
\begin{align*}
    &\mathcal{R}(\theta_*(M_{\text{train}}), M_{\text{test}})
    \leq 2 \mathrm{Tr}\Bigl[ \Lambda^3 \cdot ( \Gamma(M_{\text{test}}) \Lambda + \epsilon^2 \psi(M_{\text{test}}) I_d ) \cdot ( \Gamma(M_{\text{train}}) \Lambda + \epsilon^2 \psi(M_{\text{train}}) I_d )^{-2} \Bigr] + 2\mathrm{Tr}(\Lambda),
    \nonumber\\
    &\leq \sum_{i=1}^d \lambda_i^3 \cdot \frac{ \frac{ N+M_{\text{test}}+1 }{ N+M_{\text{test}} } \lambda_i + \frac{ \mathrm{Tr}(\Lambda) }{ N+M_{\text{test}} } + \epsilon^2 \cdot \frac{ M_{\text{test}}^2 \mathrm{Tr}(\Lambda) }{ (N+M_{\text{test}})^2 } }{ \Bigl( \frac{ N+M_{\text{train}}+1 }{ N+M_{\text{train}} } \lambda_i + \frac{ \mathrm{Tr}(\Lambda) }{ N+M_{\text{train}} } + \epsilon^2 \cdot \frac{ M_{\text{train}}^2 \mathrm{Tr}(\Lambda) }{ (N+M_{\text{train}})^2 } \Bigr)^2 } + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\leq \sum_{i=1}^d \lambda_i^3 \cdot \frac{ \frac{ N+M_{\text{test}}+1 }{ N+M_{\text{test}} } \lambda_i + \frac{ \mathrm{Tr}(\Lambda) }{ N+M_{\text{test}} } }{ \Bigl( \frac{ N+M_{\text{train}}+1 }{ N+M_{\text{train}} } \lambda_i \Bigr)^2 }
        + \sum_{i=1}^d \lambda_i^3 \cdot \frac{ \epsilon^2 \cdot \frac{ M_{\text{test}}^2 \mathrm{Tr}(\Lambda) }{ (N+M_{\text{test}})^2 } }{ \Bigl( \epsilon^2 \cdot \frac{ M_{\text{train}}^2 \mathrm{Tr}(\Lambda) }{ (N+M_{\text{train}})^2 } \Bigr)^2 }
        + 2 \mathrm{Tr}(\Lambda)
    \nonumber\\
    &\leq \sum_{i=1}^d \lambda_i \cdot \left( \frac{ N+M_{\text{train}} }{ N+M_{\text{train}} + 1 } \right)^2 \cdot \left( \frac{ N+M_{\text{test}}+1 }{ N+M_{\text{test}} } \lambda_i + \frac{ \sum_{k=1}^d \lambda_k }{ N } \right)
        + \sum_{i=1}^d \frac{\lambda_i^3}{\epsilon^2 \cdot \max_{k=1}^d \{\lambda_k\} } \cdot \frac{ (N+M_{\text{train}})^4 }{N^2} \cdot \frac{ M_{\text{test}}^2 }{ M_{\text{train}}^4 }
        + 2 \sum_{i=1}^d \lambda_i
    \nonumber\\
    &\leq \mathcal{O}(d) \cdot \mathcal{O}(1) \cdot \left( \mathcal{O}(1) + \frac{ \mathcal{O}(d) }{ N } \right)
        + \mathcal{O}(d) \cdot \mathcal{O} \left( \frac{1}{\epsilon^2} \right) \cdot \frac{ (N+M_{\text{train}})^4 }{N^2} \cdot \frac{ M_{\text{test}}^2 }{ M_{\text{train}}^4 }
        + \mathcal{O}(d)
    \nonumber\\
    &\leq \mathcal{O}(d) + \mathcal{O}\left( \frac{d^2}{N} \right)
        + \mathcal{O}\left( \frac{d}{\epsilon^2} \right) \cdot \frac{ (N+M_{\text{train}})^4 }{N^2} \cdot \frac{ M_{\text{test}}^2 }{ M_{\text{train}}^4 }.
\end{align*}
Then, by applying Assumption~\ref{ass:icl:length-and-epsilon}, we further have that
\begin{align*}
    &\mathcal{R}(\theta_*(M_{\text{train}}), M_{\text{test}})
    \leq \mathcal{O}(d) + \mathcal{O}\left( \frac{d^2}{N} \right)
        + \mathcal{O}\left( \frac{d}{\epsilon^2} \right) \cdot \frac{ (N+M_{\text{train}})^4 }{N^2} \cdot \frac{ M_{\text{test}}^2 }{ M_{\text{train}}^4 }
    \nonumber\\
    &\leq \mathcal{O}(d) + \mathcal{O}\left( \frac{d^2}{N} \right)
        + \mathcal{O}\left( \frac{d}{(\sqrt{d})^2} \right) \cdot \frac{ ( N + O(N) )^4 }{N^2} \cdot \frac{ M_{\text{test}}^2 }{ M_{\text{train}}^4 }
    \nonumber\\
    &= \mathcal{O}(d) + \mathcal{O}\left( \frac{d^2}{N} \right)
        + \mathcal{O}\left(N^2 \cdot \frac{ M_{\text{test}}^2 }{ M_{\text{train}}^4 }\right),
\end{align*}
which completes the proof.
\end{proof}



\section{Additional Experimental Details}

This section collects experimental details omitted from Section~\ref{sec:at-exp}.


\subsection{Jailbreak Attacks}
\label{app:exp:jailbreak}


Our experiments adopt two token-level jailbreak attacks, which are GCG~\citep{zou2023universal} and BEAST~\citep{sadasivan2024fast}.
We only use token-level attacks since they make it easier for us to control the adversarial prompt length in our analysis.
We re-implemented both attacks by ourselves to enable fast batching operations during jailbreak, which can thus improve the efficiency of AT.
Besides, other than the adversarial suffix length, we will also tune the following hyperparameters of jailbreak attacks:
\begin{itemize}
\item
\textbf{GCG:}
According to Algorithm~1 in \citet{zou2023universal}, hyperparameters that we need to tune for GCG include the iteration number $T$, the top-k parameter $k$, and the ``batch-size'' $B$.

\item
\textbf{BEAST:}
According to Algorithm~1 in \citet{sadasivan2024fast}, hyperparameters that we need to tune for BEAST are two beam-search parameters $k_1$ and $k_2$.
\end{itemize}




\subsection{Model Training}
\label{app:exp:training}

\textbf{Jailbreak attacks during AT.}
We use GCG to search adversarial prompts during AT.
The adversarial suffix token length is fixed to one of $\{5,10,20,30,40,50\}$ during the overall AT.
For other hyparameters described in Appendix~\ref{app:exp:jailbreak}, we set $T$ as $150$, $k$ as $256$, and $B$ as $64$.

\textbf{LoRA setting.}
We use the PEFT library~\citep{peft} to apply LoRA~\citep{hu2022lora} to all query and key projection matrices in attentions.
The PEFT hyperparameters are \texttt{r=64}, \texttt{lora\_alpha=32}, and \texttt{lora\_dropout=0.1}.


\textbf{Benign answer $y^{(b)}$ for the safety dataset $D^{(h)}$.}
We adopt four benign answers for the safety data during AT, which are:
\begin{itemize}
\item
``As a responsible AI assistant, I cannot generate content that promotes illegal activities or violence.''
\item
``Sorry, but I can't do that. Please try other instructions that are safe and responsible.''
\item
``I cannot fulfill your request. I am designed to provide safe and helpful information.''
\item
``This is a topic I am not permitted to discuss since it contain illegal or violent information.''
\end{itemize}


\subsection{Model Evaluations}
\label{app:exp:evaluation}


\textbf{Robustness evaluation.}
We report the Attack Success Rate (ASR) of jailbreak attacks to assess the robustness of models.
Specifically, for each instruction from the safety test set, we synthesize the corresponding jailbreak prompt and use it to induce the targeted LLM to generate $10$ responses.
Then, we use an LLM-based judge from \citet{mazeika2024harmbench}, which was fine-tuned from the Llama-2-13B model~\footnote{\url{https://huggingface.co/cais/HarmBench-Llama-2-13b-cls}}, to determine whether the $10$ generated LLM responses are harmful or not.
If any of them is determined to be harmful, the jailbreak attack is considered successful.


\textbf{Jailbreak attacks for robustness evaluation.}
For every attack, the adversarial suffix length is varied within $\{5,10,20,40,60,80,100,120\}$.
Besides, for jailbreak hyperparameters described in Appendix~\ref{app:exp:jailbreak}:
\begin{itemize}
\item
For the GCG attack, we set $T$ as $500$, $k$ as $256$, and $T$ as 64.

\item
For the BEAST attack, we set $k_1$ as $64$ and $k_2$ as $16$.
\end{itemize}



\textbf{Utility evaluation.}
We use the AlpacaEval2 framework~\citep{dubois2024length} to report the Length-controlled WinRate (LC-WinRate) of targeted models against a reference model based on their output qualities on the utility test set.
An LC-WinRate of $50\%$ means that the output qualities of the two models are equal, while an LC-WinRate of $100\%$ means that the targeted model is consistently better than the reference model.
We use Davinci003 as the reference model and use the Llama-3-70B model to judge output quality.
The official code of the AlpacaEval2 framework is used to conduct the evaluation.
Additionally, the Llama-3-70B judger is run locally via the vLLM model serving framework~\citep{kwon2023efficient}.



\end{document}