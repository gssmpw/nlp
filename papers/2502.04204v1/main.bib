@article{zhang2024trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={49},
  pages={1--55},
  year={2024}
}

@article{javanmard2020precise,
  title={Precise Tradeoffs in Adversarial Training for Linear Regression},
  author={Javanmard, Adel and Soltanolkotabi, Mahdi and Hassani, Hamed},
  journal={arXiv preprint arXiv:2002.10477},
  year={2020}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@inproceedings{liu2024autodan,
  title={{AutoDAN}: {Generating} Stealthy Jailbreak Prompts on Aligned Large Language Models},
  author={Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@inproceedings{hayase2024querybased,
  title={Query-Based Adversarial Prompt Generation},
  author={Jonathan Hayase and Ema Borevkovi{\'c} and Nicholas Carlini and Florian Tram{\`e}r and Milad Nasr},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024},
}

@InProceedings{sadasivan2024fast,
  title={Fast Adversarial Attacks on Language Models In One {GPU} Minute},
  author={Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil},
  booktitle={International Conference on Machine Learning},
  year={2024},
}

@inproceedings{xu2024bag,
  title={Bag of Tricks: {Benchmarking} of Jailbreak Attacks on {LLMs}},
  author={Zhao Xu and Fan Liu and Hao Liu},
  booktitle={Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2024},
}

@inproceedings{rando2024universal,
  title={Universal Jailbreak Backdoors from Poisoned Human Feedback},
  author={Javier Rando and Florian Tram{\`e}r},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@article{liu2024turbo,
  title={AutoDAN-Turbo: {A} lifelong agent for strategy self-exploration to jailbreak {LLMs}},
  author={Liu, Xiaogeng and Li, Peiran and Suh, Edward and Vorobeychik, Yevgeniy and Mao, Zhuoqing and Jha, Somesh and McDaniel, Patrick and Sun, Huan and Li, Bo and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2410.05295},
  year={2024}
}

@article{paulus2024advprompter,
  title={AdvPrompter: {Fast} adaptive adversarial prompting for {LLMs}},
  author={Paulus, Anselm and Zharmagambetov, Arman and Guo, Chuan and Amos, Brandon and Tian, Yuandong},
  journal={arXiv preprint arXiv:2404.16873},
  year={2024}
}

@inproceedings{jin2024jailbreaking,
  title={Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters},
  author={Haibo Jin and Andy Zhou and Joe D. Menke and Haohan Wang},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024},
}

@inproceedings{xhonneux2024efficient,
  title={Efficient Adversarial Training in {LLM}s with Continuous Attacks},
  author={Sophie Xhonneux and Alessandro Sordoni and Stephan G{\"u}nnemann and Gauthier Gidel and Leo Schwinn},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024},
}

@article{chen2024aligning,
  title={Aligning {LLMs} to Be Robust Against Prompt Injection},
  author={Chen, Sizhe and Zharmagambetov, Arman and Mahloujifar, Saeed and Chaudhuri, Kamalika and Guo, Chuan},
  journal={arXiv preprint arXiv:2410.05451},
  year={2024}
}

@article{yu2024robust,
  title={Robust {LLM} safeguarding via refusal feature adversarial training},
  author={Yu, Lei and Do, Virginie and Hambardzumyan, Karen and Cancedda, Nicola},
  journal={arXiv preprint arXiv:2409.20089},
  year={2024}
}

@inproceedings{wei2023jailbroken,
  title={Jailbroken: {How} Does {LLM} Safety Training Fail?},
  author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
  booktitle={Conference on Neural Information Processing Systems},
  year={2023},
}

@inproceedings{qi2024finetuning,
  title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
  author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@article{qi2024safety,
  title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
  author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
  journal={arXiv preprint arXiv:2406.05946},
  year={2024},
}

@article{mazeika2024harmbench,
  title={{HarmBench}: {A} standardized evaluation framework for automated red teaming and robust refusal},
  author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
  journal={arXiv preprint arXiv:2402.04249},
  year={2024}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu},
  booktitle={International Conference on Learning Representations},
  year={2018},
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{ahn2023transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Conference on Neural Information Processing Systems},
  volume={36},
  pages={45614--45650},
  year={2023}
}

@inproceedings{wang2024incontext,
  title={In-context Learning on Function Classes Unveiled for Transformers},
  author={Zhijie Wang and Bo Jiang and Shuai Li},
  booktitle={International Conference on Machine Learning},
  year={2024},
}

@article{pathak2023transformers,
  title={Transformers can optimally learn regression mixture models},
  author={Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu},
  journal={arXiv preprint arXiv:2311.08362},
  year={2023}
}

@inproceedings{mahankali2024one,
  title={One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
  author={Arvind V. Mahankali and Tatsunori Hashimoto and Tengyu Ma},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@article{chen2024transformers,
  title={How transformers utilize multi-head attention in in-context learning? {A} case study on sparse linear regression},
  author={Chen, Xingwu and Zhao, Lei and Zou, Difan},
  journal={arXiv preprint arXiv:2408.04532},
  year={2024}
}

@inproceedings{ahn2024linear,
  title={Linear attention is (maybe) all you need (to understand Transformer optimization)},
  author={Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@inproceedings{guo2024how,
  title={How Do Transformers Learn In-Context Beyond Simple Functions? {A} Case Study on Learning with Representations},
  author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@article{magen2024benign,
  title={Benign overfitting in single-head attention},
  author={Magen, Roey and Shang, Shuning and Xu, Zhiwei and Frei, Spencer and Hu, Wei and Vardi, Gal},
  journal={arXiv preprint arXiv:2410.07746},
  year={2024}
}

@article{frei2024trained,
  title={Trained transformer classifiers generalize and exhibit benign overfitting in-context},
  author={Frei, Spencer and Vardi, Gal},
  journal={arXiv preprint arXiv:2410.01774},
  year={2024}
}

@article{lu2024asymptotic,
  title={Asymptotic theory of in-context learning by linear attention},
  author={Lu, Yue M and Letey, Mary I and Zavatone-Veth, Jacob A and Maiti, Anindita and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2405.11751},
  year={2024}
}

@inproceedings{yang2024incontext,
  title={In-Context Learning with Representations: {Contextual} Generalization of Trained Transformers},
  author={Tong Yang and Yu Huang and Yingbin Liang and Yuejie Chi},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024},
}

@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}

@inproceedings{shi2024why,
  title={Why Larger Language Models Do In-context Learning Differently?},
  author={Zhenmei Shi and Junyi Wei and Zhuoyan Xu and Yingyu Liang},
  booktitle={International Conference on Machine Learning},
  year={2024},
}

@inproceedings{lin2024transformers,
  title={Transformers as Decision Makers: {Provable} In-Context Reinforcement Learning via Supervised Pretraining},
  author={Licong Lin and Yu Bai and Song Mei},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@inproceedings{wu2024how,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Jingfeng Wu and Difan Zou and Zixiang Chen and Vladimir Braverman and Quanquan Gu and Peter Bartlett},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@article{anwar2024adversarial,
  title={Adversarial Robustness of In-Context Learning in Transformers for Linear Regression},
  author={Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer},
  journal={arXiv preprint arXiv:2411.05189},
  year={2024}
}

@article{touvron2023llama2,
  title={Llama 2: {Open} foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{zheng2023judging,
  title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
  author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
  booktitle={Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2023},
}

@article{grattafiori2024llama3herdmodels,
  title={The {Llama 3} Herd of Models}, 
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024},
}

@misc{alpaca2023taori,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  title = {Stanford {Alpaca}: {An} Instruction-following {LLaMA} model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Conference on Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{liu2024deepseek,
  title={DeepSeek-v2: {A} strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2.5 Technical Report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: {A} family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{casper2024defending,
  title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
  author={Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2403.05030},
  year={2024}
}

@article{szegedy2014intriguing,
  title={Intriguing properties of neural networks}, 
  author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
  journal={arXiv preprint arXiv:1312.6199},
  year={2014},
}

@article{goodfellow2015explaining,
  title={Explaining and Harnessing Adversarial Examples}, 
  author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
  journal={arXiv preprint arXiv:1412.6572},
  year={2015},
}

@inproceedings{shin2020autoprompt,
  author={Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh},
  title={{AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts},
  booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
}

@inproceedings{guo2021gradientbased,
  title={Gradient-based Adversarial Attacks against Text Transformers},
  author={Guo, Chuan and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e} and Kiela, Douwe},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@inproceedings{liao2024amplegcg,
  title={Ample{GCG}: {Learning} a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed {LLM}s},
  author={Zeyi Liao and Huan Sun},
  booktitle={Conference on Language Modeling},
  year={2024},
}

@inproceedings{schwinn2024soft,
  title={Soft Prompt Threats: {Attacking} Safety Alignment and Unlearning in Open-Source {LLM}s through the Embedding Space},
  author={Leo Schwinn and David Dobre and Sophie Xhonneux and Gauthier Gidel and Stephan G{\"u}nnemann},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Conference on Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{rafailov2023direct,
  title={Direct preference optimization: {Your} language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Conference on Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{sheshadri2024latent,
  title={Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in {LLMs}},
  author={Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and Casper, Stephen},
  journal={arXiv preprint arXiv:2407.15549},
  year={2024}
}

@inproceedings{arditi2024refusal,
  title={Refusal in Language Models Is Mediated by a Single Direction},
  author={Andy Arditi and Oscar Balcells Obeso and Aaquib Syed and Daniel Paleka and Nina Rimsky and Wes Gurnee and Neel Nanda},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024},
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Conference on Neural Information Processing Systems},
  year={2022}
}

@inproceedings{fu2024theoretical,
  title={Theoretical Analysis of Robust Overfitting for Wide {DNN}s: An {NTK} Approach},
  author={Shaopeng Fu and Di Wang},
  booktitle={International Conference on Learning Representations},
  year={2024},
}

@inproceedings{ribeiro2023regularization,
  title={Regularization properties of adversarially-trained linear regression},
  author={Antonio H. Ribeiro and Dave Zachariah and Francis Bach and Thomas B. Sch{\"o}n},
  booktitle={Conference on Neural Information Processing Systems},
  year={2023},
}

@inproceedings{wang2024benign,
  title={Benign Overfitting in Adversarial Training of Neural Networks},
  author={Yunjuan Wang and Kaibo Zhang and Raman Arora},
  booktitle={International Conference on Machine Learning},
  year={2024},
}

@inproceedings{anil2024many,
  title={Many-shot jailbreaking},
  author={Anil, Cem and Durmus, Esin and Rimsky, Nina and Sharma, Mrinank and Benton, Joe and Kundu, Sandipan and Batson, Joshua and Tong, Meg and Mu, Jesse and Ford, Daniel J and others},
  booktitle={Conference on Neural Information Processing Systems},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral {7B}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{dubois2024length,
  title={Length-Controlled {AlpacaEval}: {A} Simple Way to Debias Automatic Evaluators},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475},
  year={2024}
}

@inproceedings{hu2022lora,
  title={{LoRA}: {Low-Rank} Adaptation of Large Language Models},
  author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle={International Conference on Learning Representations},
  year={2022},
}

@Misc{peft,
  title={{PEFT}: {State-of-the-art} Parameter-Efficient Fine-Tuning methods},
  author={Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished={\url{https://github.com/huggingface/peft}},
  year={2022}
}

@article{petersen2008matrix,
  title={The matrix cookbook},
  author={Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  journal={Technical University of Denmark},
  volume={7},
  number={15},
  pages={510},
  year={2008}
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}