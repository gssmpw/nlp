[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei2023jailbroken",
        "author": "Alexander Wei and Nika Haghtalab and Jacob Steinhardt",
        "title": "Jailbroken: {How} Does {LLM} Safety Training Fail?"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "szegedy2014intriguing",
        "author": "Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus",
        "title": "Intriguing properties of neural networks"
      },
      {
        "key": "goodfellow2015explaining",
        "author": "Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy",
        "title": "Explaining and Harnessing Adversarial Examples"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "shin2020autoprompt",
        "author": "Taylor Shin and Yasaman Razeghi and Robert L. Logan IV and Eric Wallace and Sameer Singh",
        "title": "{AutoPrompt}: Eliciting Knowledge from Language Models with Automatically Generated Prompts"
      },
      {
        "key": "guo2021gradientbased",
        "author": "Guo, Chuan and Sablayrolles, Alexandre and J{\\'e}gou, Herv{\\'e} and Kiela, Douwe",
        "title": "Gradient-based Adversarial Attacks against Text Transformers"
      },
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      },
      {
        "key": "liao2024amplegcg",
        "author": "Zeyi Liao and Huan Sun",
        "title": "Ample{GCG}: {Learning} a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed {LLM}s"
      },
      {
        "key": "schwinn2024soft",
        "author": "Leo Schwinn and David Dobre and Sophie Xhonneux and Gauthier Gidel and Stephan G{\\\"u}nnemann",
        "title": "Soft Prompt Threats: {Attacking} Safety Alignment and Unlearning in Open-Source {LLM}s through the Embedding Space"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "sadasivan2024fast",
        "author": "Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil",
        "title": "Fast Adversarial Attacks on Language Models In One {GPU} Minute"
      },
      {
        "key": "hayase2024querybased",
        "author": "Jonathan Hayase and Ema Borevkovi{\\'c} and Nicholas Carlini and Florian Tram{\\`e}r and Milad Nasr",
        "title": "Query-Based Adversarial Prompt Generation"
      },
      {
        "key": "jin2024jailbreaking",
        "author": "Haibo Jin and Andy Zhou and Joe D. Menke and Haohan Wang",
        "title": "Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "paulus2024advprompter",
        "author": "Paulus, Anselm and Zharmagambetov, Arman and Guo, Chuan and Amos, Brandon and Tian, Yuandong",
        "title": "AdvPrompter: {Fast} adaptive adversarial prompting for {LLMs}"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2024autodan",
        "author": "Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao",
        "title": "{AutoDAN}: {Generating} Stealthy Jailbreak Prompts on Aligned Large Language Models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chao2023jailbreaking",
        "author": "Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric",
        "title": "Jailbreaking black box large language models in twenty queries"
      },
      {
        "key": "liu2024turbo",
        "author": "Liu, Xiaogeng and Li, Peiran and Suh, Edward and Vorobeychik, Yevgeniy and Mao, Zhuoqing and Jha, Somesh and McDaniel, Patrick and Sun, Huan and Li, Bo and Xiao, Chaowei",
        "title": "AutoDAN-Turbo: {A} lifelong agent for strategy self-exploration to jailbreak {LLMs}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "anil2024many",
        "author": "Anil, Cem and Durmus, Esin and Rimsky, Nina and Sharma, Mrinank and Benton, Joe and Kundu, Sandipan and Batson, Joshua and Tong, Meg and Mu, Jesse and Ford, Daniel J and others",
        "title": "Many-shot jailbreaking"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xu2024bag",
        "author": "Zhao Xu and Fan Liu and Hao Liu",
        "title": "Bag of Tricks: {Benchmarking} of Jailbreak Attacks on {LLMs}"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: {Your} language model is secretly a reward model"
      },
      {
        "key": "qi2024safety",
        "author": "Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson",
        "title": "Safety Alignment Should Be Made More Than Just a Few Tokens Deep"
      },
      {
        "key": "qi2024finetuning",
        "author": "Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson",
        "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
      },
      {
        "key": "chen2024aligning",
        "author": "Chen, Sizhe and Zharmagambetov, Arman and Mahloujifar, Saeed and Chaudhuri, Kamalika and Guo, Chuan",
        "title": "Aligning {LLMs} to Be Robust Against Prompt Injection"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "madry2018towards",
        "author": "Aleksander Madry and Aleksandar Makelov and Ludwig Schmidt and Dimitris Tsipras and Adrian Vladu",
        "title": "Towards Deep Learning Models Resistant to Adversarial Attacks"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "mazeika2024harmbench",
        "author": "Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks",
        "title": "{HarmBench}: {A} standardized evaluation framework for automated red teaming and robust refusal"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xhonneux2024efficient",
        "author": "Sophie Xhonneux and Alessandro Sordoni and Stephan G{\\\"u}nnemann and Gauthier Gidel and Leo Schwinn",
        "title": "Efficient Adversarial Training in {LLM}s with Continuous Attacks"
      },
      {
        "key": "casper2024defending",
        "author": "Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan",
        "title": "Defending Against Unforeseen Failure Modes with Latent Adversarial Training"
      },
      {
        "key": "sheshadri2024latent",
        "author": "Sheshadri, Abhay and Ewart, Aidan and Guo, Phillip and Lynch, Aengus and Wu, Cindy and Hebbar, Vivek and Sleight, Henry and Stickland, Asa Cooper and Perez, Ethan and Hadfield-Menell, Dylan and Casper, Stephen",
        "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in {LLMs}"
      },
      {
        "key": "yu2024robust",
        "author": "Yu, Lei and Do, Virginie and Hambardzumyan, Karen and Cancedda, Nicola",
        "title": "Robust {LLM} safeguarding via refusal feature adversarial training"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "garg2022can",
        "author": "Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory",
        "title": "What can transformers learn in-context? a case study of simple function classes"
      },
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "ahn2023transformers",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      },
      {
        "key": "chen2024transformers",
        "author": "Chen, Xingwu and Zhao, Lei and Zou, Difan",
        "title": "How transformers utilize multi-head attention in in-context learning? {A} case study on sparse linear regression"
      },
      {
        "key": "mahankali2024one",
        "author": "Arvind V. Mahankali and Tatsunori Hashimoto and Tengyu Ma",
        "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention"
      },
      {
        "key": "wang2024incontext",
        "author": "Zhijie Wang and Bo Jiang and Shuai Li",
        "title": "In-context Learning on Function Classes Unveiled for Transformers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2024trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained transformers learn linear models in-context"
      },
      {
        "key": "yang2024incontext",
        "author": "Tong Yang and Yu Huang and Yingbin Liang and Yuejie Chi",
        "title": "In-Context Learning with Representations: {Contextual} Generalization of Trained Transformers"
      },
      {
        "key": "huang2023context",
        "author": "Huang, Yu and Cheng, Yuan and Liang, Yingbin",
        "title": "In-context convergence of transformers"
      },
      {
        "key": "wu2024how",
        "author": "Jingfeng Wu and Difan Zou and Zixiang Chen and Vladimir Braverman and Quanquan Gu and Peter Bartlett",
        "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?"
      },
      {
        "key": "lin2024transformers",
        "author": "Licong Lin and Yu Bai and Song Mei",
        "title": "Transformers as Decision Makers: {Provable} In-Context Reinforcement Learning via Supervised Pretraining"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "lu2024asymptotic",
        "author": "Lu, Yue M and Letey, Mary I and Zavatone-Veth, Jacob A and Maiti, Anindita and Pehlevan, Cengiz",
        "title": "Asymptotic theory of in-context learning by linear attention"
      },
      {
        "key": "magen2024benign",
        "author": "Magen, Roey and Shang, Shuning and Xu, Zhiwei and Frei, Spencer and Hu, Wei and Vardi, Gal",
        "title": "Benign overfitting in single-head attention"
      },
      {
        "key": "frei2024trained",
        "author": "Frei, Spencer and Vardi, Gal",
        "title": "Trained transformer classifiers generalize and exhibit benign overfitting in-context"
      },
      {
        "key": "shi2024why",
        "author": "Zhenmei Shi and Junyi Wei and Zhuoyan Xu and Yingyu Liang",
        "title": "Why Larger Language Models Do In-context Learning Differently?"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "anwar2024adversarial",
        "author": "Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer",
        "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "anwar2024adversarial",
        "author": "Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer",
        "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression"
      }
    ]
  }
]