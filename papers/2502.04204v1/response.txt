\section{Related Works}
\label{sec:related-works}

\textbf{Jailbreak attacks.}
Jailbreaking~[1] can be seen as adversarial attacks toward LLMs, which aim to synthesize adversarial prompts to induce targeted harmful behaviors from LLMs.
Many efforts have been made on token-level jailbreak attacks, {\it i.e.}, searching adversarial prompts in the token space of LLMs, which can be achieved via gradient-based optimization~[2], heuristic greedy search~[3], or fine-tuning prompt generators from pre-trained LLMs~[4].
Other attempts include word-level adversarial prompt searching~[5] or directly prompting LLMs to generate adversarial prompts~[6].
Our work focuses on token-level jailbreaking since it make it easier for us to control the adversarial prompt length for our analysis.



More recent studies have found that increasing the length of adversarial prompts by adding more harmful demonstrations~[7] or synthesizing longer adversarial suffixes~[8] can make jailbreaking more effective.
These works motivate us to investigate the problem of defending against ``long-length'' jailbreak attacks.






\textbf{Adversarial training on LLMs.}
To defend against jailbreak attacks, a large body of studies focus on aligning LLMs to refuse responding jailbreak prompts~[9].
More recent works have started to adopt adversarial training~(AT)~[10] to align LLMs.
~[11] trained LLMs on (discrete) adversarial prompts synthesized by GCG attack, in which they cached the intermediate synthesized results to reduce the heavy cost of searching adversarial prompts from scratch.
Meanwhile, various studies~[12] conduct AT with adversarial examples found in the continuous embedding space rather than the discrete text space since searching in the continuous embedding space is more computationally efficient.
Nevertheless, as a preliminary study of the length of adversarial prompts during AT, our work only analyzes AT with discrete adversarial prompts.



\textbf{In-context learning theory (ICL).}
Transformer-based large models like LLMs are strong in performing ICL:
Given a series of inputs (also known as ``prompt'') specified by a certain task, LLMs can make predictions well for this certain task without adjusting model parameters.
Current theories in understanding ICL can be roughly divided into two categories.
The first category aims to understand ICL via constructing explicit multi-layer transformers to simulate the optimization process of learning function classes~[13].
The second category focuses on directly analyzing the training~[14] and generalization~[15] of simple self-attention models ({\it i.e.}, one-layer transformer).
~[16] is the first to study adversarial attacks against linear transformers and finds that an attack can always succeed by perturbing only a single in-context sample.
However, their analysis allows samples to be perturbed in the entire real space, which might not appropriately reflect real-world settings since real-world adversarial prompts can only be constructed from token/character spaces of limited size.
Unlike ~[16], we propose a new ICL adversarial attack that requires each adversarial suffix token to be perturbed only within restricted spaces, which thus can be a better tool for understanding real-world jailbreaking.