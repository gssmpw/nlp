\section{Related Works}
\label{sec:related-works}



\textbf{Jailbreak attacks.}
Jailbreaking____ can be seen as adversarial attacks____ toward LLMs, which aim to synthesize adversarial prompts to induce targeted harmful behaviors from LLMs.
Many efforts have been made on token-level jailbreak attacks, {\it i.e.}, searching adversarial prompts in the token space of LLMs, which can be achieved via gradient-based optimization____, heuristic greedy search____, or fine-tuning prompt generators from pre-trained LLMs____.
Other attempts include word-level adversarial prompt searching____ or directly prompting LLMs to generate adversarial prompts____.
Our work focuses on token-level jailbreaking since it make it easier for us to control the adversarial prompt length for our analysis.



More recent studies have found that increasing the length of adversarial prompts by adding more harmful demonstrations____ or synthesizing longer adversarial suffixes____ can make jailbreaking more effective.
These works motivate us to investigate the problem of defending against ``long-length'' jailbreak attacks.






\textbf{Adversarial training on LLMs.}
To defend against jailbreak attacks, a large body of studies focus on aligning LLMs to refuse responding jailbreak prompts____.
More recent works have started to adopt adversarial training~(AT)____ to align LLMs.
____ trained LLMs on (discrete) adversarial prompts synthesized by GCG attack____, in which they cached the intermediate synthesized results to reduce the heavy cost of searching adversarial prompts from scratch.
Meanwhile, various studies____ conduct AT with adversarial examples found in the continuous embedding space rather than the discrete text space since searching in the continuous embedding space is more computationally efficient.
Nevertheless, as a preliminary study of the length of adversarial prompts during AT, our work only analyzes AT with discrete adversarial prompts.



\textbf{In-context learning theory (ICL).}
Transformer-based large models like LLMs are strong in performing ICL:
Given a series of inputs (also known as ``prompt'') specified by a certain task, LLMs can make predictions well for this certain task without adjusting model parameters.
Current theories in understanding ICL can be roughly divided into two categories.
The first category aims to understand ICL via constructing explicit multi-layer transformers to simulate the optimization process of learning function classes____.
The second category focuses on directly analyzing the training____ and generalization____ of simple self-attention models ({\it i.e.}, one-layer transformer).
____ is the first to study adversarial attacks against linear transformers and finds that an attack can always succeed by perturbing only a single in-context sample.
However, their analysis allows samples to be perturbed in the entire real space, which might not appropriately reflect real-world settings since real-world adversarial prompts can only be constructed from token/character spaces of limited size.
Unlike ____, we propose a new ICL adversarial attack that requires each adversarial suffix token to be perturbed only within restricted spaces, which thus can be a better tool for understanding real-world jailbreaking.