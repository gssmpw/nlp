\section{Related Works}
\label{sec:RelatedWorks}

In our research scope, the ReceiptVQA task is defined as extractive document VQA, wherein given a question written in natural language and a receipt image, a system answers by using verbatim text spans extracted from the image. The following subsections provide a brief overview of VQA in the document domain, receipt understanding, and current VQA research in Vietnamese. 

%----------------------------------------------------------------------------------------

\subsection{Document VQA dataset} \label{sec:rel_docvqa_data}

Many VQA datasets on document images have been introduced in various document types. DocVQA \cite{mathew2021docvqa} encompasses images of industry/business documents, such as letters, forms, tables, and graphics. VisualMRC \cite{tanaka2021visualmrc} contains images that are screenshots of web pages. InfographicVQA \cite{InfographicVQA} and ChartQA \cite{masry2022chartqa} particularly focus on diagrams.

Furthermore, some studies extend the current scale of visual input from single to multiple images. SlideVQA \cite{SlideVQA2023} requires machines to handle a slide deck composed of multiple images, while PDF-VQA \cite{pdfvqa} challenges systems to elicit information from multiple pages of a full PDF document collected from published scientific articles. Also, recently, DUDE \cite{dude} presents as not only a \textit{multi-industry, multi-domain, and multi-page} VQA dataset but also a challenging benchmark with various types of questions and answers, aiming to provide a more real-world evaluation scheme for current and future methodologies.

\subsection{Document VQA methodology} \label{sec:rel_docvqa_med}

Generally speaking, there are two approaches to producing document VQA answers: extractive and generative. The extractive approach (or sequence labeling) produces answers by marking out the start and end indices of a sub-sequence from the context, which is serialized OCR tokens in reading order. This approach has largely been applied via BERT-like layout-aware models, which are pioneered by LayoutLM \cite{LayoutLM}. The introduction of the following pretrained layout models, e.g., LayoutLMv2 \cite{layoutlmv2}, Docformer \cite{docformer}, and LayoutLMv3 \cite{layoutlmv3}, has shown a remarkable enhancement over the past few years.

Nevertheless, as Powalski et al. \cite{tilt} addressed, the extractive approach is not only sensitive to error-prone OCR systems, and output tagging procedures but also inflexible in terms of producing answers. Therefore, the generative approach has been investigated with the addition of a decoder module. With an encoder-decoder backbone (often a T5-based architecture \cite{t5model}), generative models have the ability to auto-regressively generate answers by a \textit{built-in} vocabulary, which is not solely dependent on the OCR token inputs. Therefore, many architectures and pretrained models, such as, TILT \cite{tilt}, UDOP \cite{udop}, and Docformerv2 \cite{docformerv2}, have been presented and achieved state-of-the-art performance on strong benchmarks.

In this paper, we conduct experiments on outstanding baselines based on the extractive approach and generative approach and assess their feasibility on our dataset. Additionally, regarding the experiment scope, we mainly focus on the understanding of document content. Thus, OCR information is first extracted by using an off-the-shelf OCR engine and then used for training the examined models.

%----------------------------------------------------------------------------------------

\subsection{Receipt Understanding} \label{sec:rel_ru}

In recent years, research on receipts has been primarily addressed in the Optical Character Recognition (OCR) field, specifically with the two datasets SROIE \cite{huang2019sroie} and CORD \cite{park2019cord}. SROIE \cite{huang2019sroie}, considered the first public receipt dataset, contains 1,000 scanned receipt images enclosed by special challenges, e.g., poor paper/printing quality, low-resolution scanners, long texts, and small font sizes. Subsequently, CORD \cite{park2019cord} was proposed for the post-OCR parsing task with more than 11,000 Indonesian receipt images. In addition, a receipt dataset for the VQA task, named Receipt AVQA, was introduced for the RECEIPT-AVQA-2023 shared task \cite{ravqa}, encompasses 21,835 English questions annotated from 1,957 receipt images.

Regarding the Vietnamese language, studies on receipt images, likewise, primarily investigate OCR-related tasks. The Mobile Captured Receipt Recognition Challenge (MC-OCR) \cite{mcocr} at the RIVF conference 2021 drew the attention of 105 participants and about 1,285 submission entries. MC-OCR used 2,436 Vietnamese mobile captured receipts to create the dataset that challenges two tasks, \textit{Image Quality Assessment} and \textit{Key Information Extraction}. Recently, in 2024, UIT-MLReceipts dataset was proposed for extracting key information on more than 2,000 receipt images annotated with four classes of bounding box, which are similar to MC-OCR \textit{Key Information Extraction} classes.

%----------------------------------------------------------------------------------------

\subsection{VQA in Vietnamese} \label{sec:rel_vqavi}

Vietnamese VQA has been popularized since the introduction of the ViVQA dataset \cite{vivqa} in 2019. ViVQA dataset, with 10,328 images and 15,000 QA pairs, was conducted in a semi-automatic process, wherein questions and answers from the COCO-QA dataset \cite{cocoqa} were translated to Vietnamese before being revised by crowd-workers. Successively, the first large-scale and manually annotated dataset for VQA in Vietnamese, called OpenViVQA \cite{openvivqa}, was published encompassing 11,199 images of Vietnamese natural scenes and 37,914 open-ended question-answer (QA) pairs. OpenViVQA authors also tackled the scene-text property of the image by classifying each QA pair into \textit{Non-text QA} or \textit{Text QA} label and evaluating strong scene-text VQA baselines as well as their proposed architectures. Besides, the EVJVQA Challenge \cite{Luu_Thuy_Nguyen_2023} of VLSP 2022, handling VQA task in three languages English, Vietnamese, and Japanese, and the VLSP 2023 challenge on Visual Reading Comprehension for Vietnamese\footnote{\url{https://vlsp.org.vn/vlsp2023/eval/vrc}}, tackling the scene text factor in images, have gained attention from Vietnamese research community. Meanwhile, studies on methodologies for VQA in Vietnamese have started to be investigated \cite{phuc2020vican,nguyen2023pat,tran2023bartphobeit}.

To the best of our knowledge, ReceiptVQA is the initial manually-annotated dataset to explore Vietnamese VQA in document images, or receipt images in particular. We hope that our work will encourage and facilitate future extensions and contributions in Vietnamese VQA for document domains. 

%----------------------------------------------------------------------------------------
%	ReceiptVQA Dataset
%----------------------------------------------------------------------------------------