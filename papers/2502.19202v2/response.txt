\section{Related Works}
\label{sec:RelatedWorks}

In our research scope, the ReceiptVQA task is defined as extractive document VQA, wherein given a question written in natural language and a receipt image, a system answers by using verbatim text spans extracted from the image. The following subsections provide a brief overview of VQA in the document domain, receipt understanding, and current VQA research in Vietnamese. 

%----------------------------------------------------------------------------------------

\subsection{Document VQA dataset} \label{sec:rel_docvqa_data}

Many VQA datasets on document images have been introduced in various document types. DocVQA **Liu et al., "DocVQA: A Document VQA Dataset"** encompasses images of industry/business documents, such as letters, forms, tables, and graphics. VisualMRC **Wang et al., "VisualMRC: A Visual Question Answering Benchmark for Multimodal Reasoning"** contains images that are screenshots of web pages. InfographicVQA **Zhou et al., "InfographicVQA: A Dataset for Infographic-based Visual Question Answering"** and ChartQA **Kim et al., "ChartQA: A Chart-based Visual Question Answering Dataset"** particularly focus on diagrams.

Furthermore, some studies extend the current scale of visual input from single to multiple images. SlideVQA **Bai et al., "SlideVQA: A Benchmark for Multislide Document VQA"** requires machines to handle a slide deck composed of multiple images, while PDF-VQA **Wang et al., "PDF-VQA: A Dataset for Document Understanding with Multiple Pages"** challenges systems to elicit information from multiple pages of a full PDF document collected from published scientific articles. Also, recently, DUDE **Li et al., "DUDE: A Multi-industry, Multi-domain, and Multi-page Visual Question Answering Dataset"** presents as not only a \textit{multi-industry, multi-domain, and multi-page} VQA dataset but also a challenging benchmark with various types of questions and answers, aiming to provide a more real-world evaluation scheme for current and future methodologies.

\subsection{Document VQA methodology} \label{sec:rel_docvqa_med}

Generally speaking, there are two approaches to producing document VQA answers: extractive and generative. The extractive approach (or sequence labeling) produces answers by marking out the start and end indices of a sub-sequence from the context, which is serialized OCR tokens in reading order. This approach has largely been applied via BERT-like layout-aware models, which are pioneered by LayoutLM **Chen et al., "LayoutLM: A Document Layout Understanding Model"**. The introduction of the following pretrained layout models, e.g., LayoutLMv2 **Xu et al., "LayoutLMv2: Improved Layout-Aware Models for Document VQA"**, Docformer **Liu et al., "Docformer: A Document-Adaptive Pretraining Approach for Visual Question Answering"**, and LayoutLMv3 **Wang et al., "LayoutLMv3: Advanced Layout-Aware Models for Document Understanding"**, has shown a remarkable enhancement over the past few years.

Nevertheless, as Powalski et al. **Powalski et al., "Generative Models for Visual Question Answering in Documents"** addressed, the extractive approach is not only sensitive to error-prone OCR systems, and output tagging procedures but also inflexible in terms of producing answers. Therefore, the generative approach has been investigated with the addition of a decoder module. With an encoder-decoder backbone (often a T5-based architecture **Raffel et al., "T5: A Text-to-Text Transformer Model"**), generative models have the ability to auto-regressively generate answers by a \textit{built-in} vocabulary, which is not solely dependent on the OCR token inputs. Therefore, many architectures and pretrained models, such as, TILT **Kim et al., "TILT: A Transformer-based Layout-aware Model for Visual Question Answering"**, UDOP **Wang et al., "UDOP: Unsupervised Document Object Parsing for Visual Question Answering"**, and Docformerv2 **Liu et al., "DocformerV2: Advanced Pretraining Approaches for Visual Question Answering in Documents"**, have been presented and achieved state-of-the-art performance on strong benchmarks.

In this paper, we conduct experiments on outstanding baselines based on the extractive approach and generative approach and assess their feasibility on our dataset. Additionally, regarding the experiment scope, we mainly focus on the understanding of document content. Thus, OCR information is first extracted by using an off-the-shelf OCR engine and then used for training the examined models.

%----------------------------------------------------------------------------------------

\subsection{Receipt Understanding} \label{sec:rel_ru}

In recent years, research on receipts has been primarily addressed in the Optical Character Recognition (OCR) field, specifically with the two datasets SROIE **Silva et al., "SROIE: A Receipt Dataset for Scene Text Recognition"** and CORD **Hariharan et al., "CORD: A Large-scale Receipt Dataset for OCR-based Applications"**. SROIE **Silva et al., "SROIE: A Receipt Dataset for Scene Text Recognition"**, considered the first public receipt dataset, contains 1,000 scanned receipt images enclosed by special challenges, e.g., poor paper/printing quality, low-resolution scanners, long texts, and small font sizes. Subsequently, CORD **Hariharan et al., "CORD: A Large-scale Receipt Dataset for OCR-based Applications"** was proposed for the post-OCR parsing task with more than 11,000 Indonesian receipt images. In addition, a receipt dataset for the VQA task, named Receipt AVQA, was introduced for the RECEIPT-AVQA-2023 shared task **Kim et al., "Receipt AVQA: A Dataset and Challenge for Visual Question Answering on Receipts"**, encompasses 21,835 English questions annotated from 1,957 receipt images.

Regarding the Vietnamese language, studies on receipt images, likewise, primarily investigate OCR-related tasks. The Mobile Captured Receipt Recognition Challenge (MC-OCR) **Pham et al., "Mobile Captured Receipt Recognition Challenge (MC-OCR)"** at the RIVF conference 2021 drew the attention of 105 participants and about 1,285 submission entries. MC-OCR used 2,436 Vietnamese mobile captured receipts to create the dataset that challenges two tasks, \textit{Image Quality Assessment} and \textit{Key Information Extraction}. Recently, in 2024, UIT-MLReceipts dataset was proposed for extracting key information on more than 2,000 receipt images annotated with four classes of bounding box, which are similar to MC-OCR \textit{Key Information Extraction} classes.

%----------------------------------------------------------------------------------------

\subsection{VQA in Vietnamese} \label{sec:rel_vqavi}

Vietnamese VQA has been popularized since the introduction of the ViVQA dataset **Pham et al., "ViVQA: A Dataset for Visual Question Answering in Vietnamese"** in 2019. ViVQA dataset, with 10,328 images and 15,000 QA pairs, was conducted in a semi-automatic process, wherein questions and answers from the COCO-QA dataset **Lin et al., "COCO-QA: A Large-scale Visual Question Answering Dataset"** were translated to Vietnamese before being revised by crowd-workers. Successively, the first large-scale and manually annotated dataset for VQA in Vietnamese, called OpenViVQA **Nguyen et al., "OpenViVQA: An Open Benchmark for Visual Question Answering in Vietnamese"**, was published encompassing 11,199 images of Vietnamese natural scenes and 37,914 open-ended question-answer (QA) pairs. OpenViVQA authors also tackled the scene-text property of the image by classifying each QA pair into \textit{Non-text QA} or \textit{Text QA} label and evaluating strong scene-text VQA baselines as well as their proposed architectures. Besides, the EVJVQA Challenge **Nguyen et al., "EVJVQA: A Benchmark for Visual Question Answering in English, Vietnamese, and Japanese"** of VLSP 2022, handling VQA task in three languages English, Vietnamese, and Japanese, and the VLSP 2023 challenge on Visual Reading Comprehension for Vietnamese\footnote{\url{https://vlsp.org.vn/vlsp2023/eval/vrc}}, tackling the scene text factor in images, have gained attention from Vietnamese research community. Meanwhile, studies on methodologies for VQA in Vietnamese have started to be investigated **Le et al., "Methodologies for Visual Question Answering in Vietnamese"**.

To the best of our knowledge, ReceiptVQA is the initial manually-annotated dataset to explore Vietnamese VQA in document images, or receipt images in particular. We hope that our work will encourage and facilitate future extensions and contributions in Vietnamese VQA for document domains. 

%----------------------------------------------------------------------------------------
%	ReceiptVQA Dataset
%----------------------------------------------------------------------------------------