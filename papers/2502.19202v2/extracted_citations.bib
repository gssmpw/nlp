@InProceedings{InfographicVQA,
    author    = {Mathew, Minesh and Bagal, Viraj and Tito, Rub\`en and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, C.V.},
    title     = {InfographicVQA},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {1},
    year      = {2022},
    pages     = {1697-1706}
}

@inproceedings{LayoutLM,
author = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
title = {LayoutLM: Pre-training of Text and Layout for Document Image Understanding},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403172},
doi = {10.1145/3394486.3403172},
abstract = {Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {1192–1200},
numpages = {9},
keywords = {LayoutLM, document image understanding, pre-trained models},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{Luu_Thuy_Nguyen_2023,
   title={EVJVQA CHALLENGE: MULTILINGUAL VISUAL QUESTION ANSWERING},
   ISSN={1813-9663},
   url={http://dx.doi.org/10.15625/1813-9663/18157},
   DOI={10.15625/1813-9663/18157},
   journal={Journal of Computer Science and Cybernetics},
   publisher={Publishing House for Science and Technology, Vietnam Academy of Science and Technology (Publications)},
   author={Luu-Thuy Nguyen, Ngan and Nghia Hieu Nguyen and T.D. Vo, Duong and Tran, Khanh Quoc and Nguyen, Kiet Van},
   year={2023},
   month=sep, pages={237–258} }

@inproceedings{SlideVQA2023,
  author    = {Ryota Tanaka and
               Kyosuke Nishida and
               Kosuke Nishida and
               Taku Hasegawa and
               Itsumi Saito and
               Kuniko Saito},
  title     = {SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images},
  booktitle = {AAAI},
  year      = {2023}
}

@article{cocoqa,
  title={Exploring models and data for image question answering},
  author={Ren, Mengye and Kiros, Ryan and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{docformer,
  title={Docformer: End-to-end transformer for document understanding},
  author={Appalaraju, Srikar and Jasani, Bhavan and Kota, Bhargava Urala and Xie, Yusheng and Manmatha, R},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={993--1003},
  year={2021}
}

@inproceedings{docformerv2,
  title={Docformerv2: Local features for document understanding},
  author={Appalaraju, Srikar and Tang, Peng and Dong, Qi and Sankaran, Nishant and Zhou, Yichu and Manmatha, R},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={2},
  pages={709--718},
  year={2024}
}

@inproceedings{dude,
  title={Document understanding dataset and evaluation (dude)},
  author={Van Landeghem, Jordy and Tito, Rub{\`e}n and Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Joziak, Pawel and Powalski, Rafal and Jurkiewicz, Dawid and Coustaty, Micka{\"e}l and Anckaert, Bertrand and Valveny, Ernest and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={19528--19540},
  year={2023}
}

@inproceedings{huang2019sroie,
  title={Icdar2019 competition on scanned receipt ocr and information extraction},
  author={Huang, Zheng and Chen, Kai and He, Jianhua and Bai, Xiang and Karatzas, Dimosthenis and Lu, Shijian and Jawahar, CV},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1516--1520},
  year={2019},
  organization={IEEE}
}

@inproceedings{layoutlmv2,
    title = "{L}ayout{LM}v2: Multi-modal Pre-training for Visually-rich Document Understanding",
    author = "Xu, Yang  and
      Xu, Yiheng  and
      Lv, Tengchao  and
      Cui, Lei  and
      Wei, Furu  and
      Wang, Guoxin  and
      Lu, Yijuan  and
      Florencio, Dinei  and
      Zhang, Cha  and
      Che, Wanxiang  and
      Zhang, Min  and
      Zhou, Lidong",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.201",
    doi = "10.18653/v1/2021.acl-long.201",
    pages = "2579--2591",
    abstract = "Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672).",
}

@inproceedings{layoutlmv3,
  title={Layoutlmv3: Pre-training for document ai with unified text and image masking},
  author={Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={4083--4091},
  year={2022}
}

@inproceedings{masry2022chartqa,
    title = "{C}hart{QA}: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
    author = "Masry, Ahmed  and
      Do, Xuan Long  and
      Tan, Jia Qing  and
      Joty, Shafiq  and
      Hoque, Enamul",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.177",
    doi = "10.18653/v1/2022.findings-acl.177",
    pages = "2263--2279",
    abstract = "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
}

@InProceedings{mathew2021docvqa,
  author    = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  title     = {Docvqa: A dataset for vqa on document images},
  booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  year      = {2021},
  pages     = {2200--2209},
}

@INPROCEEDINGS{mcocr,
  author={Vu, Xuan-Son and Bui, Quang-Anh and Nguyen, Nhu-Van and Hai Nguyen, Thi Tuyet and Vu, Thanh},
  booktitle={2021 RIVF International Conference on Computing and Communication Technologies (RIVF)}, 
  title={MC-OCR Challenge: Mobile-Captured Image Document Recognition for Vietnamese Receipts}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  keywords={Image recognition;Systematics;Costs;Digital transformation;Organizations;Machine learning;Predictive models},
  doi={10.1109/RIVF51545.2021.9642077}}

@inproceedings{nguyen2023pat,
  title={PAT: Parallel Attention Transformer for Visual Question Answering in Vietnamese},
  author={Nguyen, Nghia Hieu and Van Nguyen, Kiet},
  booktitle={2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@article{openvivqa,
title = {OpenViVQA: Task, dataset, and multimodal fusion models for visual question answering in Vietnamese},
journal = {Information Fusion},
volume = {100},
pages = {101868},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101868},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001847},
author = {Nghia Hieu Nguyen and Duong T.D. Vo and Kiet {Van Nguyen} and Ngan Luu-Thuy Nguyen},
keywords = {Visual question answering, Vision-language understanding, Low-resource languages, Information fusion, Multimodal representation},
abstract = {In recent years, visual question answering (VQA) has attracted attention from the research community because of its highly potential applications (such as virtual assistance on intelligent cars, assistant devices for blind people, or information retrieval from document images using natural language as queries) and challenge. The VQA task requires methods that have the ability to fuse the information from questions and images to produce appropriate answers. Neural visual question answering models have achieved tremendous growth on large-scale datasets which are mostly for resource-rich languages such as English. However, available datasets narrow the VQA task as the answers selection task or answer classification task. We argue that this form of VQA is far from human ability and eliminates the challenge of the answering aspect in the VQA task by just selecting answers rather than generating them. In this paper, we introduce the OpenViVQA (Open-domain Vietnamese Visual Question Answering) dataset, the first large-scale dataset for VQA with open-ended answers in Vietnamese, consists of 11,000+ images associated with 37,000+ question–answer pairs (QAs). Moreover, we proposed FST, QuMLAG, and MLPAG which fuse information from images and questions, then use these fused features to construct answers as humans iteratively. Our proposed methods achieve results that are competitive with SOTA models such as SAAA, MCAN, LORA, and M4C. The dataset11https://github.com/hieunghia-pat/OpenViVQA-dataset. is available to encourage the research community to develop more generalized algorithms including transformers for low-resource languages such as Vietnamese.}
}

@inproceedings{park2019cord,
  title={CORD: a consolidated receipt dataset for post-OCR parsing},
  author={Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  booktitle={Workshop on Document Intelligence at NeurIPS 2019},
  year={2019}
}

@inproceedings{pdfvqa,
  title={VQA: A New Dataset for Real-World VQA on PDF Documents},
  author={Ding, Yihao and Luo, Siwen and Chung, Hyunsuk and Han, Soyeon Caren},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={585--601},
  year={2023},
  organization={Springer}
}

@article{phuc2020vican,
  title={Vican: Co-attention network for vietnamese visual question answering},
  author={Phuc, Nguyen Bao and Nguyen, Tran Hoang and Tho, Quan Thanh},
  journal={Fundamental and Applied Information Technology (FAIR)},
  year={2020}
}

@inproceedings{ravqa,
  title={Receipt-AVQA-2023 Challenge},
  author={Begaev, Artur and Orlov, Evgeny},
  booktitle={Proceedings of the International Conference “Dialogue},
  volume={2023},
  year={2023},
  DOI={10.28995/2075-7182-2023-22-1-11}
}

@article{t5model,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{tanaka2021visualmrc,
  title={Visualmrc: Machine reading comprehension on document images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Yoshida, Sen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={15},
  pages={13878--13888},
  year={2021}
}

@inproceedings{tilt,
  title={Going full-tilt boogie on document understanding with text-image-layout transformer},
  author={Powalski, Rafa{\l} and Borchmann, {\L}ukasz and Jurkiewicz, Dawid and Dwojak, Tomasz and Pietruszka, Micha{\l} and Pa{\l}ka, Gabriela},
  booktitle={Document Analysis and Recognition--ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5--10, 2021, Proceedings, Part II 16},
  pages={732--747},
  year={2021},
  organization={Springer}
}

@inproceedings{tran2023bartphobeit,
  title={BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering},
  author={Tran, Khiem Vinh and Van Nguyen, Kiet and Nguyen, Ngan Luu Thuy},
  booktitle={2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}

@inproceedings{udop,
  title={Unifying vision, text, and layout for universal document processing},
  author={Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={19254--19264},
  year={2023}
}

@inproceedings{vivqa,
    title = "ViVQA: Vietnamese Visual Question Answering",
    author = "Tran, Khanh Quoc  and
      Nguyen, An Trong  and
      Le, An Tran-Hoai  and
      Nguyen, Kiet Van",
    booktitle = "Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation",
    month = "11",
    year = "2021",
    address = "Shanghai, China",
    publisher = "Association for Computational Lingustics",
    url = "https://aclanthology.org/2021.paclic-1.72/",
    pages = "546--554",
}

