\section{Introduction}
Large language models (LLMs) have achieved remarkable success by training on vast datasets that now include a significant portion of the Internet~\citep{achiam2023gpt, gemini2024}. 
Their performance generally scales with training data size~\citep{kaplan2020scaling}, but acquiring new, high-quality data is increasingly challenging, especially for vision-language models (VLMs), which require carefully curated interleaved image and text data.
Recent research~\citep{chen2024selfplay,bai2022constitutional, yuan2024selfrewarding, huang2022large} indicates that self-improvement techniques can use synthetically generated data to overcome this limitation.
We introduce a novel self-improvement method based on goal-oriented play between VLMs.
This approach provides a scalable way to iteratively generate high-quality synthetic data, which can be used to fine-tune the model for further performance improvement.
By carefully designing the game, we can target specific capabilities and domains for improvement, while the goal-oriented nature ensures the quality of the generated data.

We initiate the process with two VLMs which are assigned the roles of "Describer" and "Guesser" in a variant of reference game~\citep{krauss1964changes, das2017visual, de2017guesswhat, hakimov2024usinggameplayinvestigate} which we call "VLM Dialog Game".
Using a set of unlabelled images, we construct a game with one target and several distractor images. 
The Describer answers questions about the target image, while the Guesser poses targeted questions to disambiguate the target from the distractors (Figure~\ref{fig:game-example}). 
While similar games exist, their primary use has been for human-based data collection or VLM evaluation, and our work demonstrates that this game framework also facilitates VLM iterative self-improvement through goal-oriented self-play.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{assets/game-example.pdf}
    \vspace{-8mm}
    \caption{\textbf{Example interaction between a Guesser and a Describer in the VLM Dialog Game.} The Guesser aims to identify the target image from a set of distractors by asking questions, which the Describer answers. Since the Guesser correctly identifies the target image at the end of the game, this dialog is considered successful and included in the fine-tuning data.}
    \vspace{-6mm}
    \label{fig:game-example}
\end{figure}

Thanks to their instruction-following and image-understanding capabilities, the pre-trained VLMs achieve a non-zero success rate in this game.
This inherent ability provides a scalable method for generating interleaved image-text data.
The initial performance is imperfect:
The Describer may provide incorrect answers, and the Guesser may ask irrelevant questions.
However, the game's structure allows us to identify successful game instances where the Guesser correctly selects the target image.
By filtering for these successful dialogs, we automatically obtain a high-quality dataset of interleaved data.
This curated dataset is then used to fine-tune the VLM, improving its proficiency in playing the game and, consequently, its overall image understanding capabilities.
The improved model has a higher success rate in the game, and it can be further used to generate a better dataset of interleaved data, enabling further cycles of improvement.

Our experiments demonstrate that fine-tuning VLMs on the dialog game data yields significant improvements, not just in game performance, but also on related image understanding benchmarks.
Fine-tuning on games based on either OpenImages~\citep{kuznetsova2020openimages} or DOCCI~\citep{OnoeDocci2024} datasets leads to an increase in accuracy on VQAv2~\citep{goyal2017making} benchmark.
The approach is adaptable to specific domains, like robotics, where high-quality data is often scarce.
When the game is designed with frames from robotics episodes we significantly improve the model's ability to detect successful execution in robotics tasks.

The remainder of this paper is organized as follows: Section~\ref{sec:relwork} reviews related literature, Section~\ref{sec:method} describes the VLM dialog game and self-improvement methodology, Section~\ref{sec:exps} presents experimental results in general VQA and robotic success detection, and finally Section~\ref{sec:conclusions} summarizes findings, discusses limitations, and outlines future research.