\documentclass[11pt, a4paper, logo, twocolumn, copyright]{googledeepmind}
\usepackage[authoryear, sort&compress, round]{natbib}
\usepackage{fvextra}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{siunitx}

\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\small, 
    breaklines=true,           
    breakatwhitespace=false     
}

\bibliographystyle{abbrvnat}

% Information about your document.
\title{Vision-Language Model Dialog Games for Self-Improvement}

% Can leave this option out if you do not wish to add a corresponding author.
\correspondingauthor{kksenia@google.com}

% Remove these if they are not needed
\keywords{Vision language models, self-improvement, dialog, games, vision question answering, success detection}

% Can have as many authors and as many affiliations as needed. Best to indicate joint
\author[1]{Ksenia Konyushkova}
\author[1]{Christos Kaplanis}
\author[1]{Serkan Cabi}
\author[1]{Misha Denil}


\affil[1]{Google DeepMind}

\begin{abstract}
The increasing demand for high-quality, diverse training data poses a significant bottleneck in advancing vision-language models (VLMs).
This paper presents VLM Dialog Games, a novel and scalable self-improvement framework for VLMs. 
Our approach leverages self-play between two agents engaged in a goal-oriented play centered around image identification.
By filtering for successful game interactions, we automatically curate a high-quality dataset of interleaved images and text.
We demonstrate that fine-tuning on this synthetic data leads to performance gains on downstream tasks and generalises across datasets.
Moreover, as the improvements in the model lead to better game play, this procedure can be applied iteratively.
This work paves the way for self-improving VLMs, with potential applications in various real-world scenarios especially when the high-quality multimodal data is scarce.
\end{abstract}

\begin{document}

\maketitle

\input{intro}
\input{relwork}
\input{method}
\input{exps}
\input{conclusions}

\section*{Acknowledgements}
We would like to thank Todor Davchev, Thomas Lampe, Murilo Martins and Jingwei Zhang as well as Ira Ktena, Ryutaro Tanno and David Barrett for participating in the project hackathons.
Also, we are thankful to Junkyung Kim and Jason Baldridge for thoughtful feedback on the manuscript.

\newpage
\bibliography{main}

\input{appendix}

\newpage
\end{document}