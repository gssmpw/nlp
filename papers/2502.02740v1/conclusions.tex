\section{Discussion, limitations and conclusion}
\label{sec:conclusions}

This paper introduced VML Dialog Games as a novel self-improvement framework.
Our approach leverages goal-oriented self-play between two agents engaged in a reference-style dialog game. 
By automatically filtering for successful game interactions, we generate a high-quality dataset of interleaved image and text data.
We demonstrated, through experiments on both general visual question answering (using OpenImages, DOCCI and VQAv2) and robotic success detection (using ALOHA), that fine-tuning VLMs on this synthetically generated data leads to significant performance improvements. 
Crucially, our approach requires minimal supervision, demonstrating the potential for scalable and data-efficient VLM training.

Despite promising results, our approach has limitations.
First, effectiveness depends on the initial VLM's instruction-following, as the game can only refine the existing capabilities.
Second, agents might discover trivial or useless "winning" strategies (e.g., querying specific pixel colors or inventing a private vocabulary) without genuine understanding.
While we saw significant improvement in robotics (likely due to its under-representation in pre-training), iterative gains plateaued quickly, and scalability requires further investigation. Finally, some tasks may be solvable via prompt engineering and inference-time computation, potentially avoiding fine-tuning costs.

We believe that the success of our method suggests a promising direction for model development.
While current VLMs continue to improve, the core principle of our approach – learning from successful interactions in a goal-oriented setting – remains applicable. 
Multimodal dialog games offer a general recipe for VLM self-improvement, adaptable to various domains and tasks, particularly those with scarce or specialized data. 
Future work could explore different game designs, prompting strategies, and methods for identifying and utilizing dialog interactions.