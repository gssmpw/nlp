\section{Experiments}
\label{sec:exps}

\subsection{Experimental Setup}
\label{sec:exps-setup}

We evaluate our method using the Gemini 1.5 Flash model \citep{gemini2024} as the base VLM. 
Gemini 1.5 Flash is a powerful, instruction-tuned VLM that can take as input interleaved text and images and it provides a strong base model.
We use standard supervised fine-tuning procedure (see Appendix~\ref{sec:training-details}).
We limit the game length to a maximum of three question-answer turns.
For conciseness, we refer to the self-improvement method of the fine-tuning on synthetically collected dialogs as "VLM Dialog Games".

\subsection{Experiments with General Images in Dialog Games}
\label{sec:exps-docci}

This section details our experiments using the DOCCI~\citep{OnoeDocci2024} and the OpenImages datasets \citep{kuznetsova2020openimages} to evaluate the effectiveness of our self-improvement method for image understanding through VQA tasks.

\subsubsection{Dataset and Game Configuration}
\label{sec:game-config}

\paragraph{DOCCI} dataset contains clusters of images grouped by their category.
We randomly sample \num{1000} image groups, each containing $N = 4$ images from one of \num{149} categories.
Figure~\ref{fig:docci_example} provides an example of a dialog game generated by prompted Gemini using this setup.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{assets/docci-game.pdf}
    \caption{\textbf{An example dialog game using images from the DOCCI dataset}, grouped by clusters.
    The figure shows the Guesser's questions, the Describer's answers, and the Guesser's internal dialog summary.  The Guesser correctly identifies the target image (4) at the end of the dialog.}
    \vspace{-3mm}
    \label{fig:docci_example}
\end{figure}

\paragraph{OpenImages}
We select a subset of \num{1000} random images, forming them into games with $N=4$ images.
As the dataset does not contain clusters, we select the most similar images \citep{jia2021align} as distractors.
An example of a dialog game produced in this scenario is demonstrated in Figure~\ref{fig:open_image_example_dialog}.

\subsubsection{Evaluations Tasks}
\paragraph{Dialog success rate}

Following prior work using dialog games to assess VLM capabilities \citep{hakimov2024usinggameplayinvestigate}, we use the dialog game success rate as one of measures of the model's improvement.
We report the percentage of games where the Guesser correctly identifies the target image across all $N$ tested permutations (as described in Section~\ref{method-filtering}).  

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{assets/dialog4.pdf}
    \caption{\textbf{An example of a dialog game with OpenImages} grouped by the image similarity.
    The figure shows the Guesser's questions, the Describer's answers, and the Guesser's internal dialog summary.  The Guesser correctly identifies the target image (1) at the end of the dialog.}
    \vspace{-3mm}
    \label{fig:open_image_example_dialog}
\end{figure}



\paragraph{Visual question answering (VQA)}

To assess the broader impact of our self-improvement method on general visual understanding, we evaluate the fine-tuned model on a subset of the VQAv2 dataset~\citep{goyal2017making}.
We focus on two specific question types:

\begin{itemize}
    \item \textbf{Binary (yes/no) questions}: Semantically equivalent phrasings (e.g., "No" and "There is no cat") are treated as correct. We report the model accuracy.
    \item \textbf{Object counting questions}: All answers and ground truth labels are converted to numerical form (e.g., "one" becomes "1", "none" becomes "0"). We report a strict exact-match accuracy.
\end{itemize}

\subsubsection{Results}

Table~\ref{tab:docci_captioning} compares the performance of the base Gemini 1.5 Flash model with VLM Dialog Games method.
Fist, results demonstrate that the VLM Dialog Games method with either the DOCCI or OpenImages datasets improves performance within the game with both training and unseen images (e.g., games played on DOCCI by a model trained with OpenImages).
More importantly we also achieve better performance on broader visual understanding tasks as measured by VQA accuracy.
Note that evaluation images for it are drawn from a distinct dataset (VQAv2), demonstrating the generalization of our method.
Specifically, for DOCCI dialog games, the accuracy on the VQAv2 yes/no and counting subsets increased by \num{6.8}\% and \num{2.3}\%, respectively.
For OpenImages dialog games, yes/no question accuracy increases by \num{10.4}\% and remains unchanged for counting questions. 
We hypothesis that different image sources may be better suited for improving specific tasks.
For example, \citet{OnoeDocci2024} note that many DOCCI images contain references to counts, suggesting that this dataset is well-suited for self-improvement on counting task.

\begin{table*}[h]
    \centering
     \caption{\textbf{Comparison of VLM Dialog Games and the initial Gemini 1.5 Flash.} Fine-tuning on dialog game data improves both game success rate and VQA performance (yes/no and counting subsets).  Results demonstrate generalization across training and evaluation datasets.}
    \vspace{5mm}
    \begin{tabular}{l|r|r|r|r}
      \multicolumn{1}{c}{Model} & \multicolumn{2}{c}{game success} & VQA  & VQA \\
       & ~~DOCCI~~ & OpenImages & yes/no & counting \\
      \midrule
      Gemini 1.5 Flash & 20.3\%  & 18.4\% & 73.0\% & 56\% \\
      VLM Dialog Games (DOCCI) & 24.4\% & 21.9\% & 79.8\% (+6.8) & 58.3\% (+2.3) \\
      VLM Dialog Games (OpenImages) & 25.6\% & 23.6\% & 83.4\% (+10.4) & 56\% (+0.0)\\
    \end{tabular}
    \label{tab:docci_captioning}
\end{table*}

\subsection{Ablation Studies}
\label{sec:exps-openimages-ablations}

Next, we investigate the impact of key design choices: the number of images per game and the method of image grouping.
We test the different options on OpenImages dialog games and VQA yes/no question accuracy.

\paragraph{Impact of the number of images per game}
We study the effect of $N$ on the game complexity by varying $N$ from \num{2} to \num{8} (see Appendix~\ref{game-examples-n} for dialog examples). Table~\ref{tab:openimage_n_images} presents the game success rate, the number of question-answer pairs from successful dialogs, and the VQAv2 yes/no accuracy for each $N$.
While fine-tuning with data from any $N$ improves VQAv2 performance compared to the base Gemini 1.5 Flash model, the best result is achieved with $N = 4$ in this study.
With $N = 2$, the game is relatively simple, leading to a high success rate but potentially less informative data, and a higher probability of erroneous data due to the correct guesses by chance.
Conversely, with $N = 8$, the game becomes too difficult, resulting in few successful dialogs for fine-tuning.
These results confirm that balancing game difficulty and the quantity of training data is crucial for generating an optimal dataset for fine-tuning.

\begin{table}[t]
    \centering
    \caption{\textbf{Impact of varying the number of images $N$ per game}: We report the number of successful dialog games (out of \num{1000}), the total number of question-answer pairs extracted, and the VQAv2 yes/no accuracy after fine-tuning. The optimal $N$ in this case appears to be \num{4}, balancing game difficulty and data quantity.}
    \vspace{5mm}
    \begin{tabular}{c|r|r|r}
      $N$ & game  & QA & VQA \\
       & success & pairs & yes/no \\
      \midrule
      2  & 83.7\% & 879 & 81.3\%  (~~+8.3\%) \\
      4  & 18.4\% & 275 & 83.4\% (+10.4\%) \\
      8  & 0.24\% & 34 & 77.1\%  (~~+4.1\%) \\
      \midrule
      \multicolumn{3}{l}{Gemini 1.5 Flash} & \multicolumn{1}{|l}{73\%} \\
    \end{tabular}
    \label{tab:openimage_n_images}
\end{table}

\paragraph{Impact of Image Grouping Strategy}

We investigate how image grouping affects model performance by comparing two strategies: 1) similarity-based grouping (Section~\ref{sec:game-config}), which uses visually and conceptually related distractors to elicit more targeted Guesser questions, and 2) random distractor selection.
Table~\ref{tab:openimage_vqav2_grouping} compares models using these strategies. 
Both strategies improve over the initial Gemini 1.5 Flash checkpoint ($73.0$\%) significantly, therefore, the VLM Dialog Game can be effectively implemented even with random image groupings. 
However, using similar images yields slightly higher accuracy ($83.4$\% vs. $82.6$\%).
While random images produce a larger quantity of successful dialogs ($24.7$\% vs. $18.4$\%), the increased challenge of similar images in a game likely leads to more informative training data.
Thus, we believe that for the best results in fine-tuning, we need to find a right trade off between game difficulty and training data quantity.

\begin{table}[t]
    \centering
    \caption{\textbf{Impact of image grouping strategy:} Both random and semantically similar image groupings lead to significant performance gains compared to the baseline. Although using semantically similar images demonstrates slightly better results, the difference is small, highlighting the robustness of the VLM Dialog Game approach even with random image selection.}
    \vspace{5mm}
    \begin{tabular}{l|r|r}
      Image grouping & game & VQA \\
      strategy & success & yes/no \\
      \midrule
      None (initial) & N/A & 73.0\% \\
      Similar images & 18.4\% & 83.4\% (+10.4\%) \\
      Random images & 24.7\% & 82.6\% (~~+9.6\%) \\
    \end{tabular}
    \label{tab:openimage_vqav2_grouping}
\end{table}

\subsection{Robotics Dialog Games}
\label{sec:exps-robotics}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{assets/robotics_dialog_example_success.pdf}
    \caption{
    \textbf{An example of a dialog game in the robotics domain.} The figure shows the Guesser's questions, the Describer's answers, and the Guesser's internal dialog summary.  The Guesser correctly identifies the target image (1) at the end of the dialog.}
    \vspace{-3mm}
    \label{fig:game-example-robotics-success}
\end{figure}

High-quality interleaved data is scarce in specialized domains, potentially limiting base model performance in applications.  
This section describes our experiments using the VLM Dialog Games on video frames from a robotics manipulation domain where we test VLM success detection in object manipulation tasks.

\subsubsection{Dataset and Game Configuration}
\label{sec:exps-robotics-setup}

We use image frames from videos recorded in the ALOHA setup (A Low-cost Open-source Hardware System for Bimanual Teleoperation)~\citep{zhao2023learning}.
The images feature bimanual robotic arms performing \num{10} object manipulation tasks (e.g., putting objects in containers).
% 1) fold the dress, 2) put the bowl into the drying rack, 3) unbuckle the belt, 4) open the drawer, 5) put the legos into the lego bag, 6) put the cheese in the basket, 7) remove the gears from the board, 8) put banana into the drying rack, 9) close the green trash bin lid, 10) put the giraffe in the rack.
We use images captured from an overhead camera perspective. 
Our dataset comprises \num{20} episodes (both successful and unsuccessful) for each of the \num{10} tasks, totaling \num{200} episodes. 
We limit the game to only two images randomly sampled from the \textit{same task} execution as the success rate drops significantly with more images.
We generate \num{1000} games for each of the \num{10} tasks by sampling different frame combinations.
Figure \ref{fig:game-example-robotics-success} shows a dialog game example.

\subsubsection{Evaluation Task: Success Detection in Robotics}

To evaluate the impact of our method on robotic task understanding, we measure the model's ability to perform success detection. 
Accurate success detection is critical for various robotics applications, including policy training, evaluation, and data curating.
We evaluate success detection on the final frame of video episodes, treating it as a zero-shot VQA task~\citep{du23successvqa}. 
The model is presented with the final frame image and a textual description of the intended task (e.g., "open the drawer") and it is prompted with a question on task completion (e.g., "Is the drawer open?").
We report the accuracy of the model's yes/no responses.

\subsubsection{Baselines}
\label{sec:exps-robotics-baselines}

To isolate the specific contribution of the VLM Dialog Games, we compare our method against the original Gemini 1.5 Flash model and several other baselines.

\paragraph{Description Supervised Fine-Tuning (SFT-Description)}
Since our dialog games design utilizes task descriptions for each robotic episode, we include a baseline fine-tuned directly on image-description pairs.
This baseline "SFT-Description" helps determine if simply exposing the model to paired image and task descriptions from the target domain is sufficient for improvement.

\paragraph{Self-Improving Question Answering (Self-QA)}
This baseline explores an alternative self-improvement approach based on question answering similar to the approach of~\citet{luu2024questioning} (without the image captioning).
The model performs two tasks:
\begin{enumerate}
    \item \textbf{Question generation:} Given an image from the ALOHA dataset, the model generates a question about the scene.
    \item \textbf{Answer generation:} Given an image and a generated question, the model provides an answer.
\end{enumerate}

The prompts used for these tasks are detailed in Appendix~\ref{qa-prompts}.
This baseline tests whether a simpler self-improvement loop without the goal-oriented dialog structure can achieve similar results.

\paragraph{VLM Dialog Games (Answers Only)}
Our fine-tuning data includes both Describer and Guesser perspectives. 
Since the final success detection task closely resembles the Describer's role of answering questions, we include a baseline fine-tuned only on the datapoints from the Describer.
This isolates the contribution of the Guesser's questions to the overall improvement.

\begin{table*}[t]
    \centering
    \caption{\textbf{Success detection accuracy on the ALOHA dataset}, averaged across \num{10} tasks.  Fine-tuning on dialog game data outperforms the initial checkpoint and the other baselines. Iterative refinement further improves performance.}
    \vspace{5mm}
    \begin{tabular}{l|r|r}
      Model   & Game Success & Success Detection Accuracy \\
      \midrule
      Gemini 1.5 Flash & 14.39\% & 56.5\% \\
      VLM Dialog Games (round 1) & 40.15\% (+25.76\%) & 69.5\% (+13.0\%) \\
      VLM Dialog Games (round 2) & 53.74\% (+39.35\%) & 73.0\% (+16.5\%) \\
      \midrule
      SFT-Description & N/A & 65.0\% (~~+8.5\%) \\
      Self-QA & N/A & 67.0\% (+10.5\%) \\
      VLM Dialog Games (answers only) & 17.92\% (+3.53\%) & 68\% (+12.5)\% \\
    \end{tabular}
    \label{tab:robotics_result}
\end{table*}

\paragraph{Multiple Rounds of Self-Improvement}
We expect fine-tuning to improve the model's performance in subsequent games.
Thus, we use the improved model to generates a new, higher-quality dataset of synthetic dialogs. 
These dialogs are filtered and used to fine-tune the next iteration of the model, a process we refer to as "round 1" and "round 2".

In all cases we generate datasets with a size equivalent to the corresponding dialog game dataset and use it to fine-tune the Gemini 1.5 Flash model with the same settings.


\subsubsection{Results}
\label{sec:exps-robotics-results}

Table~\ref{tab:robotics_result} presents the success detection accuracy and game success rates averaged across the $10$ robotic tasks.
The initial Gemini 1.5 Flash model achieves a success detection accuracy of $56.5$\% on this highly specialised domain, only slightly above chance. 
Both the SFT-Description and Self-QA baselines improve upon this, demonstrating the benefit of domain-specific fine-tuning ($65.0$\% and $67.0$\% accuracy, respectively).

However, fine-tuning on a single round of dialog game data (VLM Dialog Games (round 1)) yields a larger improvement, achieving a success detection accuracy of $69.5$\% surpassing the baseline Self-QA by $2.5$\%.
Interestingly, although the VLM received no explicit instructions for success detection, the need to distinguish between frames from the \textit{same} task type lead it to focus on the task progression.
In contrast, the Self-QA method primarily generated object-related questions (see Appendix~\ref{sec:qa-examples} for examples).

Importantly, this initial round of dialog game fine-tuning also substantially increases the game success rate, from $14.39$\% to $40.15$\%, thus enabling further improvement.
We performed a second round of fine-tuning (VLM Dialog Games (round 2)), using data generated by the round 1 model.
This further boosted both the game success rate (to $53.74$\%) and the success detection accuracy (to $73.0$\%), a $16.5$\% absolute improvement over the original base model.

The VLM Dialog Games (answers only) baseline, which uses only the Describer's answers from the dialog games, achieves a success detection accuracy comparable to VLM Dialog Games (round 1). 
However, its game success rate remains comparatively low ($17.92$\%) and does not enable further iterative improvement.
This suggests that while the Describer's answers are sufficient for improving success detection, the Guesser's questions play a crucial role in improving the model's ability to play the dialog game effectively, which is necessary for continued self-improvement.

To conclude, our dialog game framework enables significant adaptation to specialized tasks like robotic success detection, where standard VLM pre-training may be less effective due to the lack of the domain-specific data.
Crucially, this self-improvement is achieved with minimal task-specific supervision, requiring only video episodes to guide the dialog generation.
