\section{\blue{MPAD} Method}
\label{sec:method}

\subsection{Formulation}
In few-shot object detection, the base data is characterized by a large number of base classes $C_{base}$ with an abundance of samples. In contrast, the novel data comprises a few novel classes $C_{novel}$, each with $K$ samples ($K \in \{1, 2, 3, 5, 10\}$ in the PASCAL VOC setting). It is important to note that the base classes and novel classes are disjoint sets (i.e., $C_{base} \cap C_{novel} = \emptyset$). As outlined in previous works~\citep{meta-rcnn, TFA, defrcn}, we define two data sets $D_{s} = \{(I_{s}^i, A_{s}^i)\}_{i=1..N_{s}}$, where $s \in \{base, novel\}$. $I_s$, $A_s$ and $N_{s}$ denote the images, annotations and number of samples in set $s$, respectively. An annotation $A_{s}^{i,j} = (c, b)$ represents a pair consisting of a class name $c \in C_s$ and the bounding box $b$ of the $j$-th object in the $i$-th image.

Typically, FSOD methods involve two stages: base training stage and novel fine-tuning stage. In the base training stage, detectors are trained on $D_{base}$ to acquire extensive knowledge, learn concept features, and build the feature extractor . In the novel fine-tuning stage, the base models are fine-tuned on a balanced set $D_{ft}$ with $K$ samples for each base and novel class to detect both base and novel objects in the image.

\subsection{Foreground-Background Relation-Aware Data Augmentation}
In object detection, an image comprises two main components: the background and the foreground. The foreground highlights the primary objects, while the background provides contextual information that aids in object inference within the images. To augment data with class representativeness, we synthesize both typical and hard samples. For typical foreground samples, ICOS uses input samples to generate novel objects with characteristics pointed out by general knowledge of LLMs. To create hard samples, HPAS mixes prompt embeddings at each time step of the data generation process in the diffusion model. Different from the classification task, the background plays an important role in object detection. Therefore, BAP proposes hard background samples in relation to foreground features. As shown in Figure~\ref{fig:overview}, our overall framework contains three main components: ICOS, HPAS, and BAP, as detailed in the following subsections.  



\subsection{In-Context learning for Object Synthesis}
\textbf{Controllable diffusion.} We utilize PowerPaint~\citep{zhuang2023task} model for the object inpainting task, ensuring that the generated object seamlessly conforms to the specified mask shape. We process the object's bounding box by applying masking and padding, and then use it as the mask input for controllable diffusion. The controllable diffusion $\theta(\cdot)$ takes a prompt embedding $\zeta_c$, a bounding box $b$, and an image $I$ as inputs. The reverse diffusion process is a sequence of denoising steps with time step $t = T,T-1,\ldots,1$.

\begin{equation}
\label{eq:CD}
   z_{t-1} = p(z_{t-1}|\theta(z_t, \zeta_c, b)),
\end{equation}

where $z_T$ is the reference image $I$ and $z_{0}$ is the synthesized image $\hat{I}$.  For each novel class $c \in C_{novel}$, we generate $N$ synthesis samples. We add novel objects to random base images $I_{\text{base}}$, defined as:
\begin{equation}
    \hat{I}_{c} = \theta(I_{base}^{i}, \zeta_{c}, b),
\end{equation}

where $i \sim U(1,N_{base})$, $\zeta_{c} = \mathcal{E}(\texttt{prompt}_c)$ is the prompt embedding and $\mathcal{E}$ is the CLIP text encoder. The bounding box $b$ is randomly chosen from the annotations of $I_{base}^i$ unless otherwise specified. The class label of the selected bounding box $b$ is replaced by $c$.



\begin{figure*}[!t]
    \centering
    \includegraphics[width=\textwidth]{fig/overview.pdf}
    \caption{The overall framework. To exploit the ability of controllable diffusion model for FSOD, we proposed a novel data augmentation method that incorporates various aspects to generate diverse data. Our method includes ICOS, BAP, HPAS. ICOS aims to deeply explore the attributes of novel classes and diversify the prompt for controllable diffusion models. BAP selects hard and typical backgrounds while HPAS generates  hard (mixed) instances}.   
    \label{fig:overview}
\end{figure*}

\textbf{Simple Prompting.} Following previous works~\citep{lin2023explore, fang2024data, wang2024snida}, a simple prompt is created by concatenating the prefix ``\texttt{a}" and the class name, resulting in the simple prompt input $\texttt{prompt}_c = ``\texttt{a photo of a [CLASSNAME]}"$.



\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/chain_of_thought.pdf}
    \caption{\blue{In-context learning} technique for exploring (a) attributes and (b) fine-grained object categories of a novel class given a sample. The input $\texttt{[CLASSNAME]}$ is replaced by class name $c \in C_{novel}$.}
     \label{fig:chain-of-thought}
\end{figure}
\textbf{\blue{In-Context learning} for Object Synthesis (\blue{ICOS}).} The simple prompt mentioned above only outlines a general concept of the object without detailed information, which can result in similar objects within a class and limit diversity. To address this, we propose using in-context learning~\citep{reynolds2021prompt} to  collect and incorporate specific characteristics and class information, enhancing the diversity of the prompts for the diffusion model.

\textit{In-Context learning for Attribute Analysis.} Based on a recent work~\citep{zhu2024llafs}, we explore the attributes of a specific class using LLMs. Specifically, we construct an input and output template to extract appearance information of a class using ChatGPT. Figure \ref{fig:chain-of-thought} (a) demonstrates a in-context learning approach for analyzing parts and attribute values of a class, where the target class name is input for the next inference. We then parse the attributes into a dictionary, with keys and values representing the general appearances and detailed attributes of the class. We randomly select a key-value attributes list $[\texttt{attr}]=\{\texttt{key}_i, \texttt{value}_i\}_{i=1..{n_a}}$ to additional provide information and diversify the prompt. Specifically, we construct the new prompt from $[\texttt{attr}]$ by the template as $\texttt{prompt}_c = ``\texttt{a [CLASS NAME] has $[\texttt{key}_1]$ $[\texttt{value}_1]$, $[\texttt{key}_2]$ $[\texttt{value}_2]$,..., $[\texttt{key}_{n_a}]$ $[\texttt{value}_{n_a}]$}"$.

\textit{In-Context learning  for Fine-Grained Categories.} Fine-grained categories are crucial for assessing the diversity within a class. Several methods~\citep{vu2023instance, wu2024detail} exploit this aspect to improve model generalization. SMS~\citep{vu2023instance} introduces a technique that utilizes fine-grained categories in few-shot instance segmentation by generating hallucinated superclasses from base and novel classes. Inspired by SMS, we leverage LLMs by querying ChatGPT to list the fine-grained categories of class $c$ using the prompt illustrated in Figure \ref{fig:chain-of-thought} (b).

The result of this query is parsed and added to $\texttt{[attr]}$ to generate a diverse set of prompts. The final $\texttt{prompt}_c$ is randomly sampled from the attribute list $\texttt{[attr]}$ and then used for synthesizing novel class samples. See Figure~\ref{fig:ICOS_output2} and Figure~\ref{fig:ICOS_output1} in Appendix~\ref{secA:ICOS-output}  for detailed responses of ICOS.


\subsection{Harmonic Prompt Aggregation Scheduler}

\begin{wrapfigure}{l}{0.5\textwidth}
    \hfill\includegraphics[width=0.5\textwidth]{fig/scheduler.pdf}
    \caption{Visualization of the weighted values of the Harmonic Prompt Aggregation Scheduler across the timesteps of controllable diffusion.}
     \label{fig:weighted-values}
\end{wrapfigure}

In addition to leveraging hard novel samples for the few-shot object detection model, we introduce a mechanism called Harmonic Prompt Aggregation Scheduler (\textbf{HPAS}). The main idea is to mix a base class with a similar novel class to enhance the diversity of the synthetic dataset. This is achieved in the prompt embedding space, step by step, throughout the generation process of the diffusion model. The prompt embedding aggregation scheduler is defined:

\begin{equation}
\label{eq:mix-prompt}
    \gamma_{c,t} = (1-\alpha_t) * \zeta_{c}  + \alpha_t * \zeta_{base},
\end{equation}

where $\alpha_t = m \times \alpha_{(t-1)} + (1-m) \times \left(( w + \frac{t-1}{T-1} \times (1 - w) \right), \, t = 1, \ldots, T$. $\alpha_t$ is the weighted value at $t$-th time step and $w$ is the starting value. By gradually increasing the weight of novel class features and reducing that of the base class, we create a synthetic object that incorporates novel detailed characteristics within the low-level features of the base class. Inspired by~\cite{he2020momentum}, the momentum $m$ is used to retain the main features of the prompt embedding. The weighted values are shown in Figure~\ref{fig:weighted-values}. We substitute $\gamma_{c,t}$ from Eq. (\ref{eq:mix-prompt}) into Eq. (\ref{eq:CD}). In this way, we can create hard samples, as shown in Figure~\ref{fig:mix-up_samples}. The reverse diffusion process  of the diffusion model becomes:

\begin{equation}
\label{eq:mix-CD}
   z_{t-1} = p(z_{t-1}|\theta(z_t, \gamma_{c,t}, b))
\end{equation}


\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/mix-up_samples.pdf}
    \caption{Visualization of the mixed instances of the Harmonic Prompt Aggregation Scheduler during the generation data process in the controllable diffusion model.}
     \label{fig:mix-up_samples}
\end{figure}

\subsection{Background Propoposal}

The background plays an important role in object detection tasks, where the model must distinguish not only between foreground objects, but also between foreground and background. In the base training stage, the model is trained to classify novel classes as background due to the condition $C_{base} \cap C_{novel} = \emptyset$. Therefore, in the novel training stage, we need to guide the model to efficiently distinguish novel classes from new backgrounds by utilizing backgrounds with similar visual features. To address this issue, we introduce the background proposal (\textbf{BAP}), which includes both the hard background proposals and the typical background proposals.

\textbf{Hard background proposal.} Inspired by~\cite{le2019anabranch} where objects concealing in the backgrounds, these camouflaged objects have a foreground that visually resembles the background and it creates difficulties for the model when detecting them. Therefore, we introduce a visual similarity background technique to create hard samples.  

We select backgrounds from base images $I_{base}$ that share similar features with the novel class $c$ by employing cosine similarity. Instead of using textual embeddings, which may not capture essential visual information, we use a pretrained visual encoder $\mathcal{F}(\cdot)$ (e.g., ViT \citep{dosovitskiy2020image}). In FSOD, the number of novel samples is insufficient to represent the general class distribution. Therefore, we use the stable diffusion model~\citep{rombach2022high} $SD(\cdot)$ to synthesize a set of samples for class $c$, denoted by $\hat{I}_{c}$. This synthetic set is used for selecting hard backgrounds. The cosine similarity metric is defined as follows:


\begin{equation}
\label{eq:hard-bg}
    \cos(c, I_{base}^i) = \frac{1}{n}  \sum_{j=0}^n {  \mathcal{F}(I_{base}^i) \cdot \mathcal{F}(\hat{I}^j_{c})\over \| \mathcal{F}(I_{base}^i) \|\| \mathcal{F}(\hat{I}^j_{c}) \|}
\end{equation}

where $\hat{I}_{c} =\left\{\hat{I}_{c}^{j} \mid \hat{I}_{c}^{j}= SD(\texttt{prompt}_c) \right\}_{j=1}^n$ are synthesized images of class $c$. We select the top base backgrounds with the highest similarity scores to the novel class $c$, denoted by $I_{c}^{hard}$.


\textbf{Typical clutter background.} For the typical background, we sample  from $I_{base}$ ones that have clutter features representing scenes with crowded and complex environments (as defined in ~\cite{rosenholtz2007measuring}). 
These samples with noise features force the model to improve its localization ability.

In this paper, we use the entropy score to quantify the clutter level of an image. Specifically, we normalize the feature embedding of the image using the softmax function. Then, we apply the entropy formula as follows:

\begin{equation}
   H_{base}^i = -\sum q_{base}^i\log (q_{base}^i),
\end{equation}

where $q_{base}^i=\text{softmax}(\mathcal{F}(I_{base}^i))$ and $H_{base}^i$ are the feature distribution and information entropy of the background image $I_{base}^i$, respectively. We select the top base backgrounds with the highest entropy score, denoted by $I^{clutter}$.

\subsection{Data generation and model training process}

In summary, for each novel class $c$, we use the proposed backgrounds $I_{c}^{sel} = I_{c}^{hard} \cup I^{clutter}$ to synthesize novel images $\hat{I}_{novel}$. We generate two types of foregrounds: the typical foreground and the hard foreground. For typical foregrounds, we use in-context learning with diverse attributes and fine-grained categories to create $\texttt{prompt}_c$. Then, images with the novel class $c$ are then synthesized through the reverse diffusion process conditioned on prompt embedding $\zeta_c$, as illustrated in Eq.  (\ref{eq:CD}). For hard foregrounds, we use Eq. (\ref{eq:mix-CD}) to generate mixed instances between class $c$ and a selected base class. The annotation $\hat{A}_{novel}^i$ for a synthesized image $\hat{I}_{novel}^i$ is copied from the original annotation data of the selected base image, with the class name within the bounding box $b$ replaced by $c$. The novel synthetic dataset $\hat{D}_{novel} = \{(\hat{I}_{novel}^i, \hat{A}_{novel}^i)\}_{i=1\ldots \hat{N}_{aug}} \cup D_{ft}$ is used to fine-tune detectors during the novel training stage. See Figure~\ref{fig:synthetic-dataset}  in Appendix~\ref{secA:synthesis-vis} for additional visualizations of our synthetic dataset.

