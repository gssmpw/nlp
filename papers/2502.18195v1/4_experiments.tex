\section{Experiments}
\label{sec:experiments}
\subsection{Datasets and Settings}

\begin{table*}[!t]
% \vspace{-4mm}
\centering
\footnotesize
% \addtolength{\tabcolsep}{-3.7pt}
\adjustbox{width=0.99\linewidth}{
\begin{tabular}{l|ccccc|ccccc|ccccc|c}
\toprule
\multirow{2}{*}{Method}  & \multicolumn{5}{c}{Novel Set 1} & \multicolumn{5}{c}{Novel Set 2} & \multicolumn{5}{c|}{Novel Set 3} & \multirow{2}{*}{Mean}\\
 &  1 & 2 & 3 & 5 & 10 & 1 & 2 & 3 & 5 & 10 & 1 & 2 & 3 & 5 & 10 \\\midrule
% FSRW~\cite{yolo-reweighting}& 14.8 & 15.5 & 26.7 & 33.9 & 47.2 & 15.7 & 15.3 & 22.7 & 30.1 & 40.5 & 21.3 & 25.6 & 28.4 & 42.8 & 45.9 & 28.4\\
% MetaDet~\cite{metadet}& 18.9 & 20.6 & 30.2 & 36.8 & 49.6 & 21.8 & 23.1 & 27.8 & 31.7 & 43.0 & 20.6 & 23.9 & 29.4 & 43.9 & 44.1  & 31.0\\
% Meta R-CNN~\cite{meta-rcnn}& 19.9 & 25.5 & 35.0 & 45.7 & 51.5 & 10.4 & 19.4 & 29.6 & 34.8 & 45.4 & 14.3 & 18.2 & 27.5 & 41.2 & 48.1  & 31.1 \\
% TFA w/ fc~\cite{tfa}& 36.8 & 29.1 & 43.6 & 55.7 & 57.0 & 18.2 & 29.0 & 33.4 & 35.5 & 39.0 & 27.7 & 33.6 & 42.5 & 48.7 & 50.2  & 38.7\\
% TFA w/ cos~\cite{tfa}& 39.8 & 36.1 & 44.7 & 55.7 & 56.0 & 23.5 & 26.9 & 34.1 & 35.1 & 39.1 & 30.8 & 34.8 & 42.8 & 49.5 & 49.8  & 39.9\\
% VFA~\cite{han2023few}& 57.7 & 64.6 & 64.7 & 67.2 & 67.4 & 41.4 & 46.2 & 51.1 & 51.8 & 51.6 & 48.9 & 54.8 & 56.6 & 59.0 & 58.9  & 56.1\\
% ICPE~\cite{lu2023breaking}& 54.3 & 59.5 & {62.4} & 65.7 & 66.2 & 33.5 & 40.1 & 48.7 & 51.7 & 52.5 & 50.9 & 53.1 & 55.3 & 60.6 & 60.1 & 54.3\\

% \hline
% DeFRCN* (baseline)~\cite{defrcn}& 53.6 & 57.5 & 61.5 & 64.1 & 60.8 & 30.1 & 38.1 & 47.0 & 53.3 & 47.9 & 48.4 & 50.9 & 52.3 & 54.9 & 57.4 & 51.9 \\
% SFOT* (Ours)& 56.9 & {57.2} & 63.5 & {65.3} & {61.8} & 33.6 & 39.7 & 47.5 & 51.7 & {48.1} & 50.6 & 52.2 & 54.2 & {56.8} & 60.1 & 53.3\\\midrule
TFA w/   fc~\citep{TFA}& 22.9 & 34.5 & 40.4 & 46.7 & 52.0 & 16.9 & 26.4 & 30.5 & 34.6 & 39.7 & 15.7 & 27.2 & 34.7 & 40.8 & 44.6  & 33.8\\
TFA w/ cos~\citep{TFA}& 25.3 & 36.4 & 42.1 & 47.9 & 52.8 & 18.3 & 27.5 & 30.9 & 34.1 & 39.5 & 17.9 & 27.2 & 34.3 & 40.8 & 45.6  & 34.7\\
FSDetView~\citep{xiao2020few}& 24.2 & 35.3 & 42.2 & 49.1 & 57.4 & 21.6 & 24.6 & 31.9 & 37.0 & 45.7 & 21.2 & 30.0 & 37.2 & 43.8 & 49.6  & 36.7\\
% TFA+Halluc~\cite{zhang2021hallucination}& 45.1 & 44.0 & 44.7 & 55.0 & 55.9 & 23.2 & 27.5 & 35.1 & 34.9 & 39.0 & 30.5 & 35.1 & 41.4 & 49.0 & 49.3  & 40.6\\
MPSR~\citep{wu2020multi}& 41.7 & 42.5 & 51.4 & 55.2 & 61.8 & 24.4 & 29.3 & 39.2 & 39.9 & 47.8 & 35.6 & 41.8 & 42.3 & 48.0 & 49.7  & 43.4\\ 
% FSCE~\cite{sun2021fsce}& 44.2 & 43.8 & 51.4 & 61.9 & 63.4 & 27.3 & 29.5 & 43.5 & 44.2 & 50.2 & 37.2 & 41.9 & 47.5 & 54.6 & 58.5  & 46.6\\
FSCE~\citep{sun2021fsce}& 32.9 & 44.0 & 46.8 & 52.9 & 59.7 & 23.7 & 30.6 & 38.4 & 43.0 & 48.5 & 22.6 & 33.4 & 39.5 & 47.3 & 54.0  & 41.2\\
SRR-FSD~\citep{zhu2021semantic}& 47.8 & 50.5 & 51.3 & 55.2 & 56.8 & 32.5 & 35.3 & 39.1 & 40.8 & 43.8 & 40.1 & 41.5 & 44.3 & 46.9 & 46.4  & 44.8\\
DCNet~\citep{hu2021dense}& 33.9 & 37.4 & 43.7 & 51.1 & 59.6 & 23.2 & 24.8 & 30.6 & 36.7 & 46.6 & 32.3 & 34.9 & 39.7 & 42.6 & 50.7  & 39.2 \\
Meta DETR~\citep{meta-detr} & 49.0 & 53.2 & 57.4 & 62.0 & 27.9 & 32.3 & 38.4 & 43.2 & 51.8 & 34.9 & 41.8 & 47.1 & 54.1 & 58.2 & 45.8 & 45.8 \\
Meta F R-CNN~\citep{han2022meta}& 43.0 & 54.5 & 60.6 & {66.1} & {65.4} & 27.7 & 35.5 & 46.1 & 47.8 & {51.4} & 40.6 & 46.4 & 53.4 & {59.9} & {58.6}  & 50.5\\
KFSOD~\citep{zhang2022kernelized}& 44.6 & - & 54.4 & 60.9 & {65.8} & {37.8} & - & 43.1 & 48.1 & 50.4 & 34.8 & - & 44.1 & 52.7 & 53.9  & 49.2\\
DeFRCN~\citep{defrcn}& 40.2 & 53.6 & 58.2 & 63.6 & 66.5 & 29.5 & 39.7 & 43.4 & 48.1 & 52.8 & 35.0 & 38.3 & 52.9 & 57.7 & 60.8  & 49.4\\
MFD~\citep{wu2022multi} & 63.4 & \underline{66.3} & 67.7 & 69.4 & 68.1 & 42.1 & {46.5} & 53.4 & 55.3 & 53.8 & 56.1 & {58.3} & {59.0} & {62.2} & 63.7 & 59.0\\
FCT~\citep{han2022few} &57.1 & 57.9 & 63.2 & 67.1 & 27.6 & 34.5 & 43.7 & 49.2 & 51.2 & 39.5 & 54.7 & 52.3 & 57.0 & 58.7 & 50.9 & 50.0 \\
FS-DETR~\citep{bulat2023fs} & 45.0  & 48.5 & 51.5 & 52.7 & 56.1  & 37.3 & 41.3 & 43.4  & 46.6 & 49.0  & 43.8  & 47.1  & 50.6 & 52.1 & 56.9 & 48.1\\
D\&R~\citep{li2023disentangle}& 41.0 & 51.7 & 55.7 & 61.8 & 65.4 & 30.7 & 39.0 & 42.5 & 46.6 & 51.7 & 37.9 & 47.1 & 51.7 & 56.8 & 59.5  & 49.3\\
VFA~\citep{han2023few}& 47.4 & 54.4 & 58.5 & 64.5 & 66.5 & {33.7} & 38.2 & 43.5 & 48.3 & 52.4 & {43.8} & 48.9 & 53.3 & 58.1 & 60.0  & 51.4\\
FSRN~\citep{guirguis2023towards}& 19.7 & 33.9 & 42.3 & 51.9 & 55.1 & 18.5 & 24.7 & 27.3 & 35.2 & 47.5 & 26.7 & 37.0 & 41.2 & 47.5 & 51.7  & 37.3\\
DiGeo~\citep{ma2023digeo}& 37.9 & 39.4 & 48.5 & 58.6 & 61.5 & 26.6 & 28.9 & 41.9 & 42.1 & 49.1 & 30.4 & 40.1 & 46.9 & 52.7 & 54.7  & 44.0\\
% $\sigma$-ADP Net*~\cite{du2023s}& 35.9 & 40.3 & 49.8 & 56.8 & 65.1 & 25.6 & 30.3 & 41.7 & 41.8 & 50.3 & 33.9 & 35.6 & 43.5 & 47.1 & 55.9  & 43.6\\
NIFF~\citep{guirguis2023niff}& 46.0 & 57.2 & 62.0 & 65.5 & 67.2 & 30.1 & 39.6 & 45.0 & 49.4 & 52.8 & 41.1 & 52.5 & 56.4 & 59.7 & 62.1  & 52.4\\
\hline
\multicolumn{16}{c}{Using deep learning augmentation technique}
\\\hline
TIP~\citep{li2021transformation} & 27.7 & 36.5 & 43.3 & 50.2 & 59.6 & 22.7 & 30.1 & 33.8 & 40.9 & 46.9 & 21.7 & 30.6 & 38.1 & 44.5 & 50.9 & 38.5 \\
Halluc~\citep{zhang2021hallucination}& 47.0 & 44.9 & 46.5 & 54.7 & 54.7 & 26.3 & 31.8 & 37.4 & 37.4 & 41.2 & 40.4 & 42.1 & 43.3 & 51.4 & 49.6  & 43.2\\
\blue{LVC~\citep{kaul2022label}} & 54.5 & 53.2 & 58.8 & 63.2 & 65.7 & 32.8 & 29.2 & 50.7 & 49.8 & 50.6 & 48.4 & 52.7 & 55 & 59.6 & 59.6 & 52.3 \\
\blue{Norm-VAE~\citep{xu2023generating}} & {62.1} & {64.9} & {67.8} & {69.2} & {67.5} & 39.9 & \underline{46.8} & \underline{54.4} & 54.2 & 53.6 & \underline{58.2} & \underline{60.3} & \underline{61.0} & \underline{64.0} & \underline{65.5}  & 59.3\\
SFOT~\citep{vu2023few} & 47.9 & 60.4 & 62.7 & 67.3 & \underline{69.1} & 32.4 & 41.2 & 45.7 & 50.2 & 54.0 & 43.5 & 54.1 & 56.9 & 60.6 & 62.5 & 53.9 \\
Lin et al.~\citep{lin2023explore}$\dagger$ & \underline{67.5} & - & \textbf{69.8} & \textbf{71.1} & \textbf{71.5} & \underline{52.0} & - & 54.3 &  \underline{57.5} & \underline{57.4} & {55.9} & - & 58.6 & 59.6 & {63.9} & \underline{61.6}\\
SNIDA~\citep{wang2024snida} & 59.3 & 60.8 & 64.3 & 65.4 & 65.6 & 35.2 & 40.8 & 50.2 & 54.6 & 50.0 & 51.6 & 52.4 & 55.9 & 58.5 & 62.6 & 55.1 \\
% \rowcolor{Aquamarine!30}
% DeFRCN + MPAD & \textbf{65.2} & \textbf{67.1} & \textbf{67.3} & \textbf{68.3} &   \textbf{68.6} & \textbf{57.5} & \textbf{58.2} & \textbf{59.3} & \textbf{60.7} &  \textbf{62.3} & \\
\rowcolor{Aquamarine!30}
MPAD &\textbf{69.1} & \textbf{69.5} & \underline{69.6} & \underline{69.9} & {68.9} & \textbf{58.4} & \textbf{59.7} & \textbf{61.8} & \textbf{61.8} & \textbf{63.5} & \textbf{70.1} & \textbf{69.8} & \textbf{69.9} & \textbf{70.4} & \textbf{71.4} & \textbf{66.9}\\
% MFD + SNIDA~\cite{wang2024snida} & 64.9 & 67.9 &  69.7 &  71.4 &  70.5 &  42.2 &  47.8 &  54.5 &  56.6 &  54.9 &  58.1 &  61.3 &  60.7 &  63.6 &  66.0 &  60.7 \\
% \rowcolor{Aquamarine!30}
% MFD + MPAD &  67.1 & 67.6 & 68.0 & 69.8 & 69.9 & 47.9 & 47.6 & 48.6 & 50.1 & 51.1 & 60.3 & 60.3 & 60.1 & 61.7 & 63.8 & 59.6 \\
% \hline
% \multicolumn{16}{c}{Using detection post process}
% \\\hline
% DeFRCN + MPAD $\dagger$& 68.7 & 69.3 & 69.6 & 70.2 \\
\bottomrule
\end{tabular}}
\caption{Generalized few-shot object detection performance (nAP50) on the PASCAL VOC dataset. The best and second performances are marked in \textbf{boldface} and \underline{underlined}, respectively. $\dagger$ indicates using post-detection process. 
% . Note that we report the averaged result over 10 runs. *depicts experiments run over multiple random seeds
} 
\label{tab:main_voc}
% \vspace{-7mm}
\end{table*}


\textbf{Dataset settings and evaluation.} Following previous works \citep{meta-rcnn, yolo-reweighting, defrcn}, we assess our MPAD method in the FSOD setting of PASCAL VOC \citep{everingham2010pascal, everingham2015pascal} and MS COCO \citep{lin2014microsoft}. For PASCAL VOC, 20 classes are separated into three sets. In each set, five classes are designated as novel classes $C_{novel}$, and the remaining fifteen classes are used as the base classes $C_{base}$. There are $K$ samples for each novel class ($K\in\{1,2,3,5,10\}$). Regarding MS COCO, the dataset serves as a challenging benchmark for FSOD. 80 classes are split into 60 base classes and 20 novel classes (identical to the 20 PASCAL VOC classes). We select a value of $K$ from the set ($\{1,2,3,5\}$) for each novel and base class to fine-tune detectors. To evaluate the model performance, we follow TFA~\citep{TFA}, DeFRCN~\citep{defrcn} and use the Generalized Few-Shot Object Detection (G-FSOD) which contains both base and novel classes to train and test models in the novel fine-tuning stage. We report AP50 of novel classes (nAP50) on PASCAL VOC dataset and nAP, nAP50, nAP75 metrics for experiments on the COCO dataset. 


\textbf{Implementation details.} Our model adopts DeFRCN~\citep{defrcn}. In both the base training and the fine-tuning stage, we use the same hyper-parameters as DeFRCN~\citep{defrcn}. During the fine-tuning stage, we utilize both real novel data and synthetic data to train models on a single NVIDIA GeForce RTX 2080 Ti GPU. We employ Powerpaint~\citep{zhuang2023task} for the conditional diffusion model $\theta(\cdot)$, and the CLIP text encoder~\citep{radford2021learning}  for $\mathcal{E}(\cdot)$. In our experiments, we aim to generate an equal number of synthetic instances for each novel class. The image feature extractor $\mathcal{F}(\cdot)$ is a pre-trained ViT model~\citep{dosovitskiy2020image} on ImageNet~\citep{deng2009imagenet}. We set $w=0.7$, $m=0.8$ and \blue{$\hat{N}_{aug}=300$}. The number of inference steps is fixed at $T=80$. Several methods show that training with multi-scale objects is crucial in FSOD. Therefore, we implement a fundamental method to increase the diversity under this aspect. In particular, we scale the selected bounding box in the data generation process with a weight. We randomly select weight value in $\{1.25, 1.5, 1.75, 2\}$ in our settings.
% We generate 50 and 350 synthetic images for ablation studies and main experiments, respectively.
% $\gamma_{fine-grained}=2$.
% More ablation studies about $\gamma$ and $N$ are provided in \red{Sec.~\ref{sec:results}}. 


\subsection{Results and Discussion}


\begin{table*}[!t]
% \vspace{-4mm}
\centering
% \footnotesize
% \addtolength{\tabcolsep}{-2.5pt}
\adjustbox{width=0.99\linewidth}{
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
\multirow{2}{*}{Method}
&\multicolumn{3}{c|}{1-shot} & \multicolumn{3}{c|}{2-shot} & \multicolumn{3}{c|}{3-shot} & \multicolumn{3}{c}{5-shot} \\
& nAP & \blue{nAP50} & \blue{nAP75} & \blue{nAP} & nAP50 & nAP75 & nAP & nAP50 & nAP75 & nAP & nAP50 & nAP75 \\ \midrule
TFA w/ fc~\citep{TFA} &  1.6 & 3.4 & 1.3 & 3.8 & 7.8 & 3.2 & 5.0 & 9.9 & 4.6 & 6.9 & 13.4 & 6.3  \\
TFA w/ cos~\citep{TFA} &  1.9 & 3.8 & 1.7 & 3.9 & 7.8 & 3.6 & 5.1 & 9.9 & 4.8 & 7.0 & 13.3 & 6.5 \\
MPSR~\citep{wu2020multi} &  2.3 & 4.1 & 2.3 & 3.5 & 6.3 & 3.4 & 5.2 & - & - & 6.7 & - & - \\
FSDetView~\citep{xiao2020few} &  3.2 & 8.9 & 1.4 & 4.9 & 13.3 & 2.3 & 6.7 & 18.6 & 2.9 & 8.1 & 20.1 & 4.4 \\
DeFRCN~\citep{defrcn} &  4.8 & {9.5} & 4.4 & 8.5 & 16.3 & 7.8 & 10.7 & 20.0 & 10.3 & 13.5 & 24.7 & 13.0  \\ 
Meta-DETR~\citep{meta-detr} & 7.5 & 12.5 & 7.7  & -    & -     & -    & 13.5 & 21.7  & 14   & 15.4  & 25    & 15.8  \\
FCT~\citep{han2022few}       & 5.6 & -    & -    & 7.9  & -     & -    & 11.1 & -     & -    & 14    & -     & -     \\
AirDet~\citep{li2022airdet}    & 6.1 & 11.4 & 6.0 & 8.7 & 16.2 & 8.4 & 10.0 & 19.4 & 9.1 & 10.8 & 20.8 & 10.3 \\
Meta F   R-CNN~\citep{han2022meta} &  5.1 & 10.7 & 4.3 & 7.6 & 16.3 & 6.2 & 9.8 & 20.2 & 8.2 & 10.8 & {22.1} & 9.2 \\

D\&R~\citep{li2023disentangle} &  6.1 & - & - & 9.5 & - & -  & 11.5 & - & - & 13.9 & - & -  \\
FSRN~\citep{guirguis2023towards} &  - & - & - & - & - & -  & - & - & - & 8.7 & 16.1 & 8.2 \\

FS-DETR~\citep{bulat2023fs}  & 7.0   & \underline{13.6} & 7.5  & 8.9  & 17.5  & 9.0    & 10.0   & 18.8  & 10.0   & 10.9  & 20.7  & 10.8 \\
% $\sigma$-ADP Net*~\cite{du2023s} &  - & - & - & - & - & - & - & - & - & - & - & - & 20.3 & - & 20.8  \\
% NIFF*~\cite{guirguis2023niff} &  - & - & - & - & - & - &  - & - & - & 14.6 & - & - \\\hline

\hline
\multicolumn{13}{c}{Using deep learning augmentation technique}
\\\hline
Halluc~\citep{zhang2021hallucination} &  4.4 & 7.5 & 4.9 & 5.6 & 9.9 & 5.9 & 7.2 & 13.3 & 7.4 & - & - & - \\
SFOT~\citep{vu2023few} &  6.7 & 13.2 & 6.0 & 10.5 & \underline{20.3} & 9.7 & 12.5  &  \underline{23.6} & 11.8 & 14.9 & \underline{27.8} & 14.2 \\
Norm-VAE~\citep{xu2023generating} &  \underline{9.5} & - & \underline{8.8} & \underline{13.7} & - & \underline{13.7} & {14.3} & - & \underline{14.4} & {15.9} & - & \underline{15.3} \\
% \cite{lin2023explore} &  18.1 & - & 19.5 & - &- &- & 19.2 & - & 20.2 & 19.8 & - & 20.4  \\ % 20.7 & 21.3 & 23.1 & 23.9
SNIDA\citep{wang2024snida} &   9.3 & - & - & 12.9 & - &- & \underline{14.8} & - & - & \underline{16.1} & - & -  \\ % 20.7 & 21.3 & 23.1 & 23.9
% \rowcolor{Aquamarine!30}
% DeFRCN + MPAD* & \textbf{16.9 }& \textbf{31.8} & \textbf{15.8 }& \textbf{16.8} & \textbf{31.7} & \textbf{16.2} &  &  &  & \textbf{17.5} & \textbf{32.5} & \textbf{16.9}  \\
\rowcolor{Aquamarine!30} MPAD &
\textbf{18.3} & \textbf{31.2} & \textbf{18.8} & \textbf{18.5} & \textbf{31.6} & \textbf{18.9} & \textbf{18.8} & \textbf{31.8} & \textbf{19.1} & \textbf{18.9} & \textbf{32.4} & \textbf{19.3} \\
% \hline
% \multicolumn{13}{c}{Using detection post process}
% \\\hline
\bottomrule
\end{tabular}
}
\caption{Generalized few-shot object detection performance on 1, 2, 3, 5-shot of MS COCO dataset.  The best and second performances are marked in \textbf{boldface} and \underline{underlined}, respectively.} 
%  Note that we report the averaged result over 10 runs. *depicts experiments run over multiple random seeds.
\label{tab:main_coco}
\end{table*}

\subsubsection{Main results}
We conduct G-FSOD experiments on PASCAL VOC~\citep{everingham2010pascal, everingham2015pascal} and MS COCO~\citep{lin2014microsoft} and report the results in Table~\ref{tab:main_voc} and Table~\ref{tab:main_coco}, respectively. These numbers indicate that our method MPAD generally outperforms the baseline and other state-of-the-art methods on FSOD benchmarks by a large margin. 

% \textbf{Results on PASCAL VOC.} Table~\ref{tab:main_voc} shows the results from the three novel sets of PASCAL VOC, comparing our approach with baselines and state-of-the-art methods. Our MPAD method consistently outperforms the baselines across all splits and shots. Notably, our method, based on DeFRCN~\cite{defrcn}, achieves the highest performance of 66.9\%, exceeding the baseline by an average margin of 17.5\%. In extremely low-shot scenarios, our method delivers significantly larger performance gains, with an average increase of +31.0\% nAP50 in the 1-shot setting. Compared to recent data augementation method for FSODLin et al.~\cite{lin2023explore}, our method surpasses their approaches without using the post detection process. 
\textbf{Results on PASCAL VOC.} Table~\ref{tab:main_voc} shows the results from the three novel sets of PASCAL VOC, comparing our approach with baselines and state-of-the-art methods. Our MPAD method consistently outperforms the baselines across all splits and shots. Notably, our method, based on DeFRCN~\citep{defrcn}, achieves the highest performance of 66.9\%, exceeding the baseline by an average margin of 17.5\%. In extremely low-shot scenarios, our method delivers significantly larger performance gains, with an average increase of +31.0\% nAP50 in the 1-shot setting. Considerably, our MPAD surpasses previous works~\citep{wang2024snida, lin2023explore, kaul2022label, li2023disentangle, zhu2021semantic, xu2023generating} that use pretrained CLIP, ViT, diffusion models, language models, or post-processing in detection. Meanwhile, methods~\citep{wang2024snida, lin2023explore} are state-of-the-art data augmentation methods in FSOD. Overall, our approach demonstrates superior performance compared to most existing methods across various splits and shots, highlighting the robustness and generalization capabilities of our method.

\textbf{Results on MS COCO.} We present the experimental results for MS COCO in Table~\ref{tab:main_coco}. By using our method, baseline DeFRCN improves by about 11.5\% on average, particularly in extremely few-shot settings (1 and 2-shot). Specifically, our method enhances nAP, nAP50, and nAP75 by over 13\%, 21\%, and 14\%, respectively, in the 1-shot setting. These results highlight the promising approach to improving FSOD performance by employing controllable diffusion model. 




\begin{table}[bt]
\centering
\begin{tabular}{l|cccccc}
\toprule
 & 1-shot & 2-shot & 3-shot & 5-shot & 10-shot & Mean \\\midrule
Cutout & 54.7   & 57.2   & \underline{62.3}   & \underline{64.0}   & 62.5    & 60.1 \\
GridMask & 54.3   & \underline{58.0}   & 62.0   & 63.7   & 63.2    & 60.2 \\
AutoAugment & 51.3 & 54.4 & 59.6 & 62.1 & 61.0 & 57.7    \\
CutMix & \underline{55.5}   & 57.6   & 61.4   & 63.9   & \underline{63.5}    & \underline{60.4} \\
\rowcolor{Aquamarine!30} MPAD        & \textbf{69.1} & \textbf{69.5} & \textbf{69.6} & \textbf{69.9} & \textbf{68.9} & \textbf{69.4} \\\bottomrule
\end{tabular}
\caption{Few-shot object detection performance (nAP50) of other augmentation methods on Novel Set 1 of PASCAL VOC dataset. The best and second performances are marked in \textbf{boldface} and \underline{underlined}, respectively.}
\label{tab:augment-method}
\end{table}

\textbf{Comparison with different augmentation methods.} Following~\cite{wang2024snida}, we also show few-shot object detection results on PASCAL VOC Novel Set 1 of other augmentation methods Cutout~\citep{devries2017improved}, GridMask~\citep{chen2020gridmask}, AutoAugment~\citep{zoph2020learning}, and CutMix~\citep{yun2019cutmix}
in Table~\ref{tab:augment-method}. The nAP50 results show that our method consistently outperforms these augmentation methods by a large margin (+9\%). This evidence demonstrates the effectiveness of our approach in the context of few-shot object detection. \orange{Detailed ablations on the number of generated images and  training schemes are provided in Appendix~\ref{secA:no_images} and  Appendix~\ref{secA:training_schemes}, respectively.}


\begin{wraptable}{r}{75mm}
\footnotesize
\centering
\adjustbox{width=0.99\linewidth}{
\begin{tabular}{c|c|c|ccc}
\toprule
\multicolumn{2}{c|}{\blue{ICOS}} & \multirow{2}{*}{HPAS} \\
Attributes & Fine-grained. & & nAP & nAP50 & nAP75 \\\midrule
 % & & & 34.6 & 62.1 & 34.4 \\
   &    & & 34.6 & 62.1 & 34.4 \\ %5 seeds
% \checkmark&  &    & & 37.9 & 62.6  & 38.9    \\ % 181
   \checkmark &  & & 38.7 & 65.8  & 39.0 \\ % 180
  \checkmark  & \checkmark & &  41.2 & 68.5 & 42.6\\
\rowcolor{YellowOrange!30}
  \checkmark  & \checkmark &\checkmark  &  42.8 & 69.1 & 45.1 \\
\bottomrule
\end{tabular}
} % end adjust box
\caption{Foreground-foreground ablation studies about ICOS and HPAS. nAP, nAP50, nAP75 metrics on Novel Set 1 of PASCAL VOC are reported to evaluate the importance of each modules.}
\label{tab:abs_foreground}
\end{wraptable}



\subsubsection{Is the diversity of a class necessary?}

We investigate the importance of class diversity in Table~\ref{tab:abs_foreground}. The table indicates that applying different augmentation techniques, which create typical and hard foregrounds, improves the performance of detectors. Specifically, controllable diffusion using \blue{ICOS} in the third row diversifies prompts, enhancing the diversity of the synthetic dataset and increasing detector performance by approximately $6\%$ nAP50 compared to not using \blue{ICOS} (\blue{i.e., directly using PowerPaint with simple prompting, as shown in the first row}). Additionally, by using HPAS, our method generates hard samples for FSOD, which boosts performance to $42.8\%$/$69.1\%$/$45.1\%$ in nAP/nAP50/nAP75. These results demonstrate that both typical and hard foregrounds are crucial for data augmentation, especially in FSOD. Detailed ablation studies on the foreground-foreground approach are provided in Table~\ref{tab:abs_fine-grained}, Figure~\ref{fig:ab_fg_sim}, and Figure~\ref{fig:ab_m_w_hpas} in Appendix~\ref{secA:ab_foreground-foreground}.

\begin{wraptable}{r}{75mm}
\footnotesize
\centering
\adjustbox{width=0.99\linewidth}{
    \begin{tabular}{c|c|c|ccc}
\toprule
 Random &  Typical & Hard & nAP & nAP50 & nAP75 \\\midrule
\checkmark &   &   & 34.6 & 62.1 & 34.4 \\
  & \checkmark &   & 37.0 & 64.8 & 38.3 \\
  &   & \checkmark & 35.2 & 61.9 & 36.0 \\
\checkmark &   & \checkmark & 36.8 & 64.4 & 37.5 \\
\rowcolor{YellowOrange!30}
  & \checkmark & \checkmark & 37.4 & 64.1 & 39.6 \\
\checkmark & \checkmark &   & 37.0 & 64.5 & 37.7 \\
\checkmark & \checkmark & \checkmark & 36.6 & 63.6 & 38.2 \\
\bottomrule
\end{tabular}}
\caption{Foreground-background ablation study about BAP. Metrics on Novel Set 1 of PASCAL VOC are reported to evaluate the importance of each selection.}
\label{tab:abs_background}
\end{wraptable}

\subsubsection{The impact of background selection}

In addition to studying the foreground, we also conduct ablation experiments on background selection, a crucial but often overlooked component. Table~\ref{tab:abs_background} demonstrates the effectiveness of background selection. With random selection, nAP, nAP50, and nAP75 metrics achieve only $34.6\%$, $62.1\%$, and $34.4\%$, respectively, which are lower than other background proposal strategies. By using both our typical and hard background proposal technique, the detector can be improved to $37.4\%$/$64.1.4\%$/$39.6\%$ in nAP/nAP50/nAP75. These results highlight the importance of foreground-background relations and the effectiveness of our BAP method. Therefore, we hope these types of relations can be explored more in few-shot object detection.

\subsection{Limitation}
\label{secA:limitation}
There are several issues with diffusion models. The hallucinations still occur in the generated images. These circumstances can lead to parts or the entire generated object being unrelated to the prompt or resulting in low-quality synthetic images, as shown in the last two rows of Figure~\ref{fig:synthetic-dataset}. \blue{There are several potential ways to reduce the number of hallucinations in generated data. We can apply a filter as a post-process for data generation, which can filter out objects that significantly deviate from the general characteristics. Additionally, we can apply LoRA in PEFT~\citep{peft} to fine-tune the diffusion model on the few-shot data, which could generate synthetic samples with greater similarity to the current dataset and reduce hallucinations in the synthetic data}. Another issue relates to the starting value $w$. This value is fixed, which may not be suitable for all novel classes.  