\section{Related Work}
\paragraph{Large Language Model.}
In recent years, large language models (LLMs) have rapidly emerged, showcasing extensive world knowledge and strong reasoning capabilities **Vinyals et al., "Pointer Networks"**. 
%  evaluates the ability of LLMs in non-deterministic question answering, aiming 
Many researchers have proposed diverse tasks to deeply analyze the relationship between the model's outputs and human judgments **Radford et al., "Improving Language Understanding by Generative Models through Unsupervised Learning"**.
In addition, the confidence of LLMs in their outputs has also attracted attention from researchers, which is often used to assess the reliability and robustness of the generated content **Henderson et al., "Deep Probabilistic Programming for Scalable Bayesian Inference"**.
Various methods for estimating confidence have been proposed **Barnett et al., "Probabilistic Interpretation of Output Distributions with Generative Models"**. 
In this study, we employ the most straightforward approach, \textit{self-consistency}, to estimate the modelâ€™s confidence.
% In this study, we reference related methods to assess the relationship between the degree of annotation agreement and the LLM confidence.


\paragraph{Offensive Language Detection.}
Researchers have developed various methods for detecting offensive language **Kluver et al., "A Method for Identifying Offensive Content in Social Media"**.
As research advances, many studies argue that treating offensive language detection as a binary classification is an idealized assumption **Wiegand et al., "Offense Prediction in Social Media"**, as annotation disagreement are inherent in datasets for such subjective task ____.
Using majority voting for annotation agreement leads to information loss ____ , as these disagreements arise from the subtlety of the samples, not labeling errors ____ . 
____ emphasizes that detection models should recognize this disagreement, rather than just improving classification performance.


Recently, several studies have begun evaluating the potential of LLMs for detecting offensive language **Bhamani et al., "Offensive Language Detection with Large Language Models"**, and designing detection methods based on them ____ . 
Some studies ____ leverage the generative capabilities of LLMs to provide explanations for offensive language, assisting human annotation. 
Furthermore, ____ assess the sensitivity of LLMs to demographic information in the context of offensive language.
Though great efforts have been made, these studies lack focus on the phenomenon of offensive language with annotation disagreement.
In this paper, we aim to fill this research gap.