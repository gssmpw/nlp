\section{Related Work}
\paragraph{Large Language Model.}
In recent years, large language models (LLMs) have rapidly emerged, showcasing extensive world knowledge and strong reasoning capabilities \cite{DBLP:conf/nips/KojimaGRMI22, DBLP:conf/nips/Ouyang0JAWMZASR22, DBLP:journals/corr/abs-2303-08774}. 
%  evaluates the ability of LLMs in non-deterministic question answering, aiming 
Many researchers have proposed diverse tasks to deeply analyze the relationship between the model's outputs and human judgments \cite{DBLP:journals/corr/abs-2408-01419, DBLP:journals/corr/abs-2404-05264}.
In addition, the confidence of LLMs in their outputs has also attracted attention from researchers, which is often used to assess the reliability and robustness of the generated content \cite{DBLP:journals/tacl/JiangADN21}.
Various methods for estimating confidence have been proposed \cite{DBLP:conf/icml/ZhangKH20, DBLP:conf/iclr/0002WSLCNCZ23, DBLP:conf/emnlp/TianMZSRYFM23, DBLP:journals/tmlr/LinHE22}. 
In this study, we employ the most straightforward approach, \textit{self-consistency}, to estimate the modelâ€™s confidence.
% In this study, we reference related methods to assess the relationship between the degree of annotation agreement and the LLM confidence.


\paragraph{Offensive Language Detection.}
Researchers have developed various methods for detecting offensive language \cite{DBLP:conf/icwsm/FountaDCLBSVSK18, DBLP:conf/icwsm/DavidsonWMW17, DBLP:conf/aaai/MathewSYBG021}.
As research advances, many studies argue that treating offensive language detection as a binary classification is an idealized assumption \cite{basile-etal-2021-need, DBLP:conf/aiia/Basile20, plank-2022-problem}, as annotation disagreement are inherent in datasets for such subjective task \cite{DBLP:journals/tacl/PavlickK19, DBLP:journals/jair/UmaFHPPP21}. 
Using majority voting for annotation agreement leads to information loss \cite{DBLP:journals/tacl/DavaniDP22}, as these disagreements arise from the subtlety of the samples, not labeling errors \cite{DBLP:journals/frai/UmaAP22}. 
\citet{DBLP:conf/semeval/LeonardelliAABF23} emphasizes that detection models should recognize this disagreement, rather than just improving classification performance.


Recently, several studies have begun evaluating the potential of LLMs for detecting offensive language \cite{DBLP:conf/icwsm/0006AD24, DBLP:conf/emnlp/RoyH0S23}, and designing detection methods based on them \cite{DBLP:conf/emnlp/ParkKJPH24, DBLP:conf/emnlp/WenKSZLBH23}. 
Some studies \cite{DBLP:conf/ijcai/WangHACL23, DBLP:conf/www/HuangKA23a} leverage the generative capabilities of LLMs to provide explanations for offensive language, assisting human annotation. 
Furthermore, \citet{DBLP:journals/corr/abs-2410-07991, DBLP:conf/acl/ZhangHJL24} assess the sensitivity of LLMs to demographic information in the context of offensive language.
Though great efforts have been made, these studies lack focus on the phenomenon of offensive language with annotation disagreement.
In this paper, we aim to fill this research gap.