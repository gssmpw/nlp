\section{Related Works}
\label{sec:related_works}

\subsection{Weight-Space Learning}

While neural networks can learn effective representations for many traditional data modalities, effective representations for neural networks are still work in progress. As a first step, \citet{statnn} proposed to observe simple statistics of weights, and  \citep{ke2017lightgbm} on them. Others proposed to encode the weights by modeling the connections between the neurons \citep{dws,inr2vec,sane,hyper_repr,eilertsen2020classifying,lim_lol,prem_neural_functionals,tran2024equivariant,functa,probex}. Recent methods \citep{neural_graphs,neural_functional_transformers,graph_meta,kalogeropoulos2024scale} model a network as a graph where every neuron is a node, and train permutation-equivariant architectures \citep{gilmer2017neural,kipf2016semi,attentionisallyouneed,relationalattention} on these graphs.
Probing is an alternative paradigm that encodes the network by observing its outputs to a fixed set of inputs (probes)  \citep{probegen,non_interactive,carlini2024stealing,tahan2024label,choshen2022start,neural_graphs,huang2024lg,dravid2023rosetta,bau2017network}. Differently from these approaches, we propose a probing-based method for zero-shot classification model search.

\subsection{Other Applications of Model Weights}

Learning on model weights has found many applications. Several approaches demonstrated advanced generation abilities using the weights \citep{dravid2024interpreting,erkocc2023hyperdiffusion,dravid2024interpreting,shah2023ziplora}, and others proposed to compress the weights to a smaller, more compact representation \citep{ha2016hypernetworks,nern,peebles2022learning}. A different line of research explored the relations between the weights for recovering the model graph \citep{mother,yax2025phylolm} or for merging \citep{yadav2024ties,gueta2023knowledge,izmailov2018averaging,wortsman2022model,rame2023model}. Recently, a few works proposed to recover the exact black-boxed weights \citep{spectral_detuning,carlini2024stealing} by having access to their fine-tuned versions or to an API. Finally, some relevant works search for new adapters for generative models \citep{hugginggpt,stylus,contentsearch}, however these approaches either rely on available metadata or tailored for generative models. Here, we propose an approach to search for new discriminative models which are capable of detecting a specific concept among other unrelated concepts seen in training time.