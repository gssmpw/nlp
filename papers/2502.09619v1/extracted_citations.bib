@article{attentionisallyouneed,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{bau2017network,
  title={Network dissection: Quantifying interpretability of deep visual representations},
  author={Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6541--6549},
  year={2017}
}

@article{carlini2024stealing,
  title={Stealing part of a production language model},
  author={Carlini, Nicholas and Paleka, Daniel and Dvijotham, Krishnamurthy Dj and Steinke, Thomas and Hayase, Jonathan and Cooper, A Feder and Lee, Katherine and Jagielski, Matthew and Nasr, Milad and Conmy, Arthur and others},
  journal={arXiv preprint arXiv:2403.06634},
  year={2024}
}

@article{choshen2022start,
  title={Where to start? analyzing the potential value of intermediate models},
  author={Choshen, Leshem and Venezian, Elad and Don-Yehia, Shachar and Slonim, Noam and Katz, Yoav},
  journal={arXiv preprint arXiv:2211.00107},
  year={2022}
}

@inproceedings{contentsearch,
  title={Content-based search for deep generative models},
  author={Lu, Daohan and Wang, Sheng-Yu and Kumari, Nupur and Agarwal, Rohan and Tang, Mia and Bau, David and Zhu, Jun-Yan},
  booktitle={SIGGRAPH Asia 2023 Conference Papers},
  pages={1--12},
  year={2023}
}

@inproceedings{dravid2023rosetta,
  title={Rosetta neurons: Mining the common units in a model zoo},
  author={Dravid, Amil and Gandelsman, Yossi and Efros, Alexei A and Shocher, Assaf},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1934--1943},
  year={2023}
}

@article{dravid2024interpreting,
  title={Interpreting the Weight Space of Customized Diffusion Models},
  author={Dravid, Amil and Gandelsman, Yossi and Wang, Kuan-Chieh and Abdal, Rameen and Wetzstein, Gordon and Efros, Alexei A and Aberman, Kfir},
  journal={arXiv preprint arXiv:2406.09413},
  year={2024}
}

@inproceedings{dws,
  title={Equivariant architectures for learning in deep weight spaces},
  author={Navon, Aviv and Shamsian, Aviv and Achituve, Idan and Fetaya, Ethan and Chechik, Gal and Maron, Haggai},
  booktitle={International Conference on Machine Learning},
  pages={25790--25816},
  year={2023},
  organization={PMLR}
}

@incollection{eilertsen2020classifying,
  title={Classifying the classifier: dissecting the weight space of neural networks},
  author={Eilertsen, Gabriel and J{\"o}nsson, Daniel and Ropinski, Timo and Unger, Jonas and Ynnerman, Anders},
  booktitle={ECAI 2020},
  pages={1119--1126},
  year={2020},
  publisher={IOS Press}
}

@inproceedings{erkocc2023hyperdiffusion,
  title={Hyperdiffusion: Generating implicit neural fields with weight-space diffusion},
  author={Erko{\c{c}}, Ziya and Ma, Fangchang and Shan, Qi and Nie{\ss}ner, Matthias and Dai, Angela},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={14300--14310},
  year={2023}
}

@article{functa,
  title={From data to functa: Your data point is a function and you can treat it like one},
  author={Dupont, Emilien and Kim, Hyunjik and Eslami, SM and Rezende, Danilo and Rosenbaum, Dan},
  journal={arXiv preprint arXiv:2201.12204},
  year={2022}
}

@inproceedings{gilmer2017neural,
  title={Neural message passing for quantum chemistry},
  author={Gilmer, Justin and Schoenholz, Samuel S and Riley, Patrick F and Vinyals, Oriol and Dahl, George E},
  booktitle={International conference on machine learning},
  pages={1263--1272},
  year={2017},
  organization={PMLR}
}

@article{graph_meta,
  title={Graph metanetworks for processing diverse neural architectures},
  author={Lim, Derek and Maron, Haggai and Law, Marc T and Lorraine, Jonathan and Lucas, James},
  journal={arXiv preprint arXiv:2312.04501},
  year={2023}
}

@article{gueta2023knowledge,
  title={Knowledge is a region in weight space for fine-tuned language models},
  author={Gueta, Almog and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
  journal={arXiv preprint arXiv:2302.04863},
  year={2023}
}

@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

@article{huang2024lg,
  title={LG-CAV: Train Any Concept Activation Vector with Language Guidance},
  author={Huang, Qihan and Song, Jie and Xue, Mengqi and Zhang, Haofei and Hu, Bingde and Wang, Huiqiong and Jiang, Hao and Wang, Xingen and Song, Mingli},
  journal={arXiv preprint arXiv:2410.10308},
  year={2024}
}

@article{hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{hyper_repr,
  title={Self-supervised representation learning on neural network weights for model characteristic prediction},
  author={Sch{\"u}rholt, Konstantin and Kostadinov, Dimche and Borth, Damian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16481--16493},
  year={2021}
}

@article{inr2vec,
  title={Deep learning on implicit neural representations of shapes},
  author={De Luigi, Luca and Cardace, Adriano and Spezialetti, Riccardo and Ramirez, Pierluigi Zama and Salti, Samuele and Di Stefano, Luigi},
  journal={arXiv preprint arXiv:2302.05438},
  year={2023}
}

@article{izmailov2018averaging,
  title={Averaging weights leads to wider optima and better generalization},
  author={Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1803.05407},
  year={2018}
}

@article{kalogeropoulos2024scale,
  title={Scale Equivariant Graph Metanetworks},
  author={Kalogeropoulos, Ioannis and Bouritsas, Giorgos and Panagakis, Yannis},
  journal={arXiv preprint arXiv:2406.10685},
  year={2024}
}

@article{ke2017lightgbm,
  title={Lightgbm: A highly efficient gradient boosting decision tree},
  author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{lim_lol,
  title={Learning on LoRAs: GL-Equivariant Processing of Low-Rank Weight Spaces for Large Finetuned Models},
  author={Lim, Derek and Gelberg, Yoav and Jegelka, Stefanie and Maron, Haggai and others},
  journal={arXiv preprint arXiv:2410.04207},
  year={2024}
}

@article{mother,
  title={On the Origin of Llamas: Model Tree Heritage Recovery},
  author={Horwitz, Eliahu and Shul, Asaf and Hoshen, Yedid},
  journal={arXiv preprint arXiv:2405.18432},
  year={2024}
}

@article{nern,
  title={NeRN--Learning Neural Representations for Neural Networks},
  author={Ashkenazi, Maor and Rimon, Zohar and Vainshtein, Ron and Levi, Shir and Richardson, Elad and Mintz, Pinchas and Treister, Eran},
  journal={arXiv preprint arXiv:2212.13554},
  year={2022}
}

@article{neural_functional_transformers,
  title={Neural functional transformers},
  author={Zhou, Allan and Yang, Kaien and Jiang, Yiding and Burns, Kaylee and Xu, Winnie and Sokota, Samuel and Kolter, J Zico and Finn, Chelsea},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{neural_graphs,
  title={Graph neural networks for learning equivariant representations of neural networks},
  author={Kofinas, Miltiadis and Knyazev, Boris and Zhang, Yan and Chen, Yunlu and Burghouts, Gertjan J and Gavves, Efstratios and Snoek, Cees GM and Zhang, David W},
  journal={arXiv preprint arXiv:2403.12143},
  year={2024}
}

@article{non_interactive,
  title={Learning useful representations of recurrent neural network weight matrices},
  author={Herrmann, Vincent and Faccio, Francesco and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:2403.11998},
  year={2024}
}

@article{peebles2022learning,
  title={Learning to learn with generative models of neural network checkpoints},
  author={Peebles, William and Radosavovic, Ilija and Brooks, Tim and Efros, Alexei A and Malik, Jitendra},
  journal={arXiv preprint arXiv:2209.12892},
  year={2022}
}

@article{prem_neural_functionals,
  title={Permutation equivariant neural functionals},
  author={Zhou, Allan and Yang, Kaien and Burns, Kaylee and Cardace, Adriano and Jiang, Yiding and Sokota, Samuel and Kolter, J Zico and Finn, Chelsea},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{probegen,
  title={Deep Linear Probe Generators for Weight Space Learning},
  author={Kahana, Jonathan and Horwitz, Eliahu and Shuval, Imri and Hoshen, Yedid},
  journal={arXiv preprint arXiv:2410.10811},
  year={2024}
}

@article{probex,
  title={Representing Model Weights with Language using Tree Experts},
  author={Horwitz, Eliahu and Cavia, Bar and Kahana, Jonathan and Hoshen, Yedid},
  journal={arXiv preprint arXiv:2410.13569},
  year={2024}
}

@inproceedings{rame2023model,
  title={Model ratatouille: Recycling diverse models for out-of-distribution generalization},
  author={Ram{\'e}, Alexandre and Ahuja, Kartik and Zhang, Jianyu and Cord, Matthieu and Bottou, L{\'e}on and Lopez-Paz, David},
  booktitle={International Conference on Machine Learning},
  pages={28656--28679},
  year={2023},
  organization={PMLR}
}

@article{relationalattention,
  title={Relational attention: Generalizing transformers for graph-structured tasks},
  author={Diao, Cameron and Loynd, Ricky},
  journal={arXiv preprint arXiv:2210.05062},
  year={2022}
}

@article{sane,
  title={Towards Scalable and Versatile Weight Space Learning},
  author={Sch{\"u}rholt, Konstantin and Mahoney, Michael W and Borth, Damian},
  journal={arXiv preprint arXiv:2406.09997},
  year={2024}
}

@article{shah2023ziplora,
  title={Ziplora: Any subject in any style by effectively merging loras},
  author={Shah, Viraj and Ruiz, Nataniel and Cole, Forrester and Lu, Erika and Lazebnik, Svetlana and Li, Yuanzhen and Jampani, Varun},
  journal={arXiv preprint arXiv:2311.13600},
  year={2023}
}

@inproceedings{spectral_detuning,
  author={Eliahu Horwitz and Jonathan Kahana and Yedid Hoshen},
  title={Recovering the Pre-Fine-Tuning Weights of Generative Models},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=761UxjOTHB},
  booktitle={ICML}
}

@article{statnn,
  title={Predicting neural network accuracy from weights},
  author={Unterthiner, Thomas and Keysers, Daniel and Gelly, Sylvain and Bousquet, Olivier and Tolstikhin, Ilya},
  journal={arXiv preprint arXiv:2002.11448},
  year={2020}
}

@article{stylus,
  title={Stylus: Automatic Adapter Selection for Diffusion Models},
  author={Luo, Michael and Wong, Justin and Trabucco, Brandon and Huang, Yanping and Gonzalez, Joseph E and Chen, Zhifeng and Salakhutdinov, Ruslan and Stoica, Ion},
  journal={arXiv preprint arXiv:2404.18928},
  year={2024}
}

@inproceedings{tahan2024label,
  title={Label-efficient model selection for text generation},
  author={Tahan, Shir Ashury and Gera, Ariel and Sznajder, Benjamin and Choshen, Leshem and Dor, Liat Ein and Shnarch, Eyal},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8384--8402},
  year={2024}
}

@article{tran2024equivariant,
  title={Equivariant Neural Functional Networks for Transformers},
  author={Tran, Viet-Hoang and Vo, Thieu N and The, An Nguyen and Huu, Tho Tran and Nguyen-Nhat, Minh-Khoi and Tran, Thanh and Pham, Duy-Tung and Nguyen, Tan Minh},
  journal={arXiv preprint arXiv:2410.04209},
  year={2024}
}

@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{yax2025phylolm,
  title={Phylolm: Inferring the phylogeny of large language models and predicting their performances in benchmarks},
  author={Yax, Nicolas and Oudeyer, Pierre-Yves and Palminteri, Stefano},
  year={2025}
}

