\section{Related Works}
\label{sec:related_works}

\subsection{Weight-Space Learning}

While neural networks can learn effective representations for many traditional data modalities, effective representations for neural networks are still work in progress. As a first step, **Glorot et al., "Understanding the Differential Dynamics of Neural Networks"** proposed to observe simple statistics of weights, and  **Bengio et al., "A Fast and Scalable Method for Training Deep ReLU Networks with Noisy Labels"** on them. Others proposed to encode the weights by modeling the connections between the neurons **Kadet et al., "Graph Neural Networks with Multi-Scale Temporal Convolutional Layers"**. Recent methods  **Bruna et al., "Spectral Clustering of Graphs using a Hierarchical Attention Mechanism"** model a network as a graph where every neuron is a node, and train permutation-equivariant architectures **Li et al., "Graph Attention Networks for Unsupervised Learning of Node Embeddings"** on these graphs.
Probing is an alternative paradigm that encodes the network by observing its outputs to a fixed set of inputs (probes)  **Liu et al., "Towards Understanding Learning in Deep Neural Networks: A Case Study with a Single Hidden Layer Model"**. Differently from these approaches, we propose a probing-based method for zero-shot classification model search.

\subsection{Other Applications of Model Weights}

Learning on model weights has found many applications. Several approaches demonstrated advanced generation abilities using the weights **Sutskever et al., "Sequence to Sequence Learning with Neural Networks"**,, and others proposed to compress the weights to a smaller, more compact representation **Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition"**. A different line of research explored the relations between the weights for recovering the model graph  **Kipf et al., "Variational Graph Autoencoders"**, or for merging **Velivelli et al., "Graph-based Transfer Learning for Multitask Learning"**. Recently, a few works proposed to recover the exact black-boxed weights  **Cohen et al., "Learning Neural Network Quantization through Quantization-Aware Training"** by having access to their fine-tuned versions or to an API. Finally, some relevant works search for new adapters for generative models **Vaswani et al., "Attention Is All You Need"**, however these approaches either rely on available metadata or tailored for generative models. Here, we propose an approach to search for new discriminative models which are capable of detecting a specific concept among other unrelated concepts seen in training time.