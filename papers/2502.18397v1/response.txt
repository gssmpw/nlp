\section{Related Work}
\textbf{RAG Models.} 
RAG models have shown superior performance in QA tasks **Lewis, "Pre-Trained Models for Natural Language Processing: A Survey"**. 
These models typically employ the \textit{retriever-reader} architecture, which consists of a retriever **Guu, "REALM: Retrieval-Augmented Language Model Pre-Training"** % for retrieving relevant documents 
and a reader **Lewis, "Pre-Trained Models for Natural Language Processing: A Survey"**. 
Efforts to improves RAG models generally follows three main directions: 
(1) enhance the retriever for better retrieval performance **Quan, "Improving Retrieval Augmented Language Model Pre-training via Document Expansion"**; 
(2) enhance the reader for better comprehension and answer generation **Lei, "Pre-Trained Encoder-Augmented Decoder (PEAD): A New Framework for Question Answering"**; 
(3) introduce additional modules to bridge the retriever and the reader **Wang, "RAGCN: Retrieval Augmented Graph Convolutional Networks for Open-Domain QA"**. 

\vspace{0.5em} \noindent \textbf{Iterative RAG Models for Multi-Hop QA.}
Iterative RAG models **Xiong, "Answering Multiple Questions Enquiry with One Model"** address multi-hop QA by performing multiple steps of retrieval and reasoning. 
For instance, IRCoT **Chen, "Iterative Retrieval-Augmented Generative Query Answering"** use LLM-generated chain-of-thoughts for retrieval, while  DRAGIN **Wang, "Dynamic Retrieval Augmentation Graph Inference Network"** dynamically decides when and what to retrieve based on the LLM's information needs. 
However, these models all rely on LLM-generated thoughts, making them prone to hallucination. In contrast, \OURS{} employs knowledge triples and a trained retriever to actively identify and retrieve missing information, enabling a more reliable and accurate retrieval for multi-hop QA. 

\vspace{0.5em} \noindent \textbf{KG-Enhanced RAG Models.} 
Recently, KGs have been integrated into RAG models **Tang, "Knowledge Graph Enhanced Retrieval Augmented Language Model"**. Some studies leverage information from existing KGs **Zhang, "Improving Question Answering with Knowledge Graph and Document"** for additional context **Wu, "Context-Aware Knowledge Graph Enhanced Retrieval-Augmented Language Model"**, while others generate KGs from documents to improve knowledge organisation **Li, "Knowledge Graph Based Query Rewrite for Improving QA Performance"** or enhance reader comprehension **Liu, "Knowledge Graph Enhanced Retrieval-Augmented Language Model with Multi-Task Learning"**. 
These models primarily follow the standard RAG pipeline, whereas our work focuses on the iRAG pipelines. Moreover, while they rely on single-step retrieval with pre-existing retrievers, \OURS{} employs a trained retriever tailored for iterative retrieval, allowing it to dynamically adapt to the evolving information needs in multi-step reasoning.