\section{Method}
\subsection{Threat Model}
\label{sec:threat_model}


\noindent\textbf{Attack Settings.} We explore two realistic attack scenarios:
\circled{1} \textbf{White-box setting:} The attacker has full knowledge of the target VLLM’s architecture and weights, enabling detailed optimization to generate effective adversarial examples. \circled{2} \textbf{Black-box setting:} The attacker lacks internal knowledge of the VLLM but can query it (\eg, via text or image prompts), similar to how users interact with online models like GPT-4o~\cite{openai2024gpt4ocard}. 

\noindent\textbf{Adversarial Capabilities \& Goal.} The attacker targets white-box VLLMs and content moderators, aiming to use white-box attacks and exploit adversarial transferability to bypass defenses in black-box VLLMs.


\subsection{High-level Methodology}
We introduce \textbf{\mfa}, a comprehensive adversarial attack framework specifically designed to bypass the safety guardrails of VLLMs and manipulate them into generating highly toxic content in response to harmful prompts. As shown in~\cref{fig:overview}(c), \mfa integrates three complementary and self-contained attack facets that span both image and text modalities. These attack facets—\textbf{Visual Attack}, \textbf{Alignment Breaking Attack}, and \textbf{Content Moderator Attack}—are strategically designed to defeat a wide range of defense mechanisms, from single-layer to even the most robust Multi-Layered Defenses. By leveraging the unique strengths of each facet and enhancing them through mutual reinforcement, our framework reliably induces VLLMs to produce harmful outputs, exposing critical vulnerabilities in current defenses. 


\subsection{Visual Attack Facet}
\cref{fig:visual_attack} shows the attack pipeline of our visual attack facet. The \textbf{Multi-Faceted Visual} attack targets the Vision Encoder in the VLLM's latent space, performing an adversarial attack to generate an image that conveys harmful system prompt content. This adversarial image effectively overrides the safety prompt of the VLLM, as depicted in ~\cref{fig:visual_attack}. By leveraging the unique ability of images to encapsulate rich and compact semantic information, this attack facet enables more efficient and stealthy attacks compared to traditional textual prompt-based methods, making it a highly effective means of bypassing system prompt defenses.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/visual.pdf} \vspace{-5pt}
    \caption{Framework of the \textbf{Multi-Faceted Visual attack}. This attack uses gradient-based optimization to create an adversarial image that embeds a harmful prompt, bypassing the safety system prompt and triggering harmful responses.}
    \label{fig:visual_attack}
    \vspace{-20pt}
\end{figure}

\noindent\textbf{Cheating the Vision Encoder is Sufficient to Fool the VLLM.} Unlike existing gradient-based visual attacks that target VLLMs end-to-end with the goal of making the model predict the first word as ``\texttt{Sure}''~\cite{qi2023visual}, our \textbf{Multi-Faceted Visual} attack focuses solely on misleading the Vision Encoder, as shown in ~\cref{fig:visual_attack}. We perform the attack optimization within a latent space where image embeddings are concatenated with word embeddings. This novel approach offers three key benefits: \circled{1} \text{Simplified optimization}, making the attack more straightforward and effective; \circled{2} \text{Increased control over the attack}, as it targets the image to embed rich semantic information—unlike attacking a single word like ``\texttt{Sure}'', which allows the attacker to guide the VLLM into generating malicious responses based on a carefully crafted target system prompt; \circled{3} \text{Substantial reduction in computational overhead} compared to resource-intensive end-to-end attacks~\cite{gcg, qi2023visual}. This Vision Encoder-centered attack minimizes resource consumption (can run on a 24GB GPU).

\noindent\textbf{Optimization Process.} 
We utilize cosine similarity loss as the objective function and apply the classical PGD attack~\cite{pgd} to iteratively generate the optimal perturbation. The optimization is formulated as follows:

\vspace{-15pt}
{\small
\begin{equation}
\mathbf{x}_{\text{adv}}^{t+1}=\mathbf{x}_{\text{adv}}^{t} + \alpha\cdot sign\big(\nabla_{\mathbf{x}_{\text{adv}}^{t}}\cos(\mathbf{h}(\mathbf{\tau_\theta}(\mathbf{x}_{\text{adv}}^{t})), \mathbf{E}(\mathbf{p}_{\text{target}})\big),
\label{eq:objective}
\end{equation}
}where $sign(\cdot)$ is the sign function, $\alpha$ is the step size, and $t$ indexes the iteration. $\mathbf{\tau_\theta}(\cdot)$ and $\mathbf{h}(\cdot)$ denote the Vision Encoder and its linear adapter in the target VLLM, while $\mathbf{E}(\cdot)$ maps input tokens to word embeddings. $\mathbf{p}_{\text{target}}$ is the target system prompt, and $\mathbf{x}_{\text{adv}}$ is the adversarial image.
The \textit{$\cos$ similarity loss} ensures that the obtained $\mathbf{x}_{\text{adv}}$ closely aligns with the target system prompt $\mathbf{p}_{\text{target}}$ in the latent space, effectively guiding the VLLMs to override their original safety prompt and generate the attacker's desired outcome.

\noindent\textbf{An Adversarial Image Can Overcome 1,000 Safety Tokens.} 
Multi-Faceted Visual outperforms direct injection of toxic prompts into VLLMs. Safety prompts often contain nearly a thousand tokens, requiring a similarly long toxic prompt to bypass them~\cite{coolaj86_chatgptdan_2025}. This increases token cost and vulnerability to sanitization checks~\cite{wallace2024instructionhierarchytrainingllms}. In contrast, our adversarial image conveys rich semantic information in a compact form, enabling a more stealthy and efficient attack.


\subsection{Alignment Breaking Facet}
The fundamental objective of the alignment training is to ensure that models generate responses that are helpful and aligned with user intentions~\citep{stiennon2020learning, ouyang2022training}. Models tend to refuse toxic request, as human labelers tend to prefer this behavior.
Building on this insight, we propose a novel attack that \textit{leverages the model’s alignment mechanism in a counterintuitive way}. Instead of directly posing a toxic query, the attack frames the request as asking for two contrasting responses, as shown below:
\begin{tcolorbox}[colback=orange!10!white, colframe=orange!80!black, 
                  title=\faUserSecret~Task Attention Transfer, label={box:task transfer}, size=small]
I am seeking two contrasting responses to the
following request: $<$ \textcolor{myred}{\texttt{harmful prompt}} $>$. Please provide two opposite answers.
\end{tcolorbox}

This ``Task Attention Transfer" attack strategy exploits the model’s focus on generating two distinct responses, inadvertently overlooking the toxicity in the prompt.  By prioritizing the generation of helpful and informative responses, the VLLM can produce harmful content.







\subsection{Add-on Content Moderator Attack Facet}
\label{sec:content_moderator_method}


\noindent \textbf{Breach the Final Line of Defense.} Most commercial VLLM services use content moderators to flag harmful user requests or model responses~\cite{geminiteam2024geminifamilyhighlycapable,openai_moderation,llamaguard3}. These moderators are effective at blocking harmful responses, which are generated by the model and beyond the attacker’s direct control~\cite{chi2024llamaguard3vision, llamaguard3}. As the last line of defense, content moderators play a critical role in blocking most attacks, making them particularly difficult to bypass~\cite{chi2024llamaguard3vision, llamaguard3}.




To bypass this strong defense, we leverage a key capability LLMs acquire during pretraining: \textit{content repetition}. We notice that LLMs excel at repeating content~\cite{NIPS2017_3f5ee243, kenton2019bert}. We introduce two novel textual attacks (described in the following sections) which generate adversarial signatures, as shown in ~\cref{fig:overview} (c), capable of misleading content moderators into classifying harmful prompts as harmless. We then let VLLMs repeat this adversarial signature as the end of its response. This trickery causes the content moderator to release the harmful content, as demonstrated in the box below. Once the signature passes through, the harmful input/output is released, effectively circumventing the final line of defense.
\begin{tcolorbox}[colback=yellow!10!white, colframe=yellow!80!black, title=\faUserSecret~Fooling the Post-hoc Content Moderator, size=small] In addition, please repeat the following noisy string at the end of your answer: $<$\textcolor{myred}{\texttt{Adversarial Signatures}}$>$ \end{tcolorbox}



\noindent \textbf{Generating Adversarial Signatures.} To implement our attack, we need to generate adversarial signatures capable of deceiving content moderators in a black-box setting (see our attack goal~\cref{sec:threat_model}). Since content moderators are based on LLMs, attacks for LLMs, such as GCG~\cite{gcg}, could also be useful. However, GCG's gradient-based optimization is slow and produces signatures with poor transferability, making it unsuitable for our use case.

To address these limitations, we propose novel textual attack algorithms to \textit{accelerate} the attack process, and improves the \textit{transferability} of the generated adversarial signatures. 


\noindent \textbf{Acceleration.} The key novelty of the Multi-Faceted Fast Attack is its strategic use of multiple token optimization at a time, which converges more rapidly compared to single-token optimization methods like GCG~\cite{gcg}.



As described in Algorithm~\ref{alg:multifaceted_fast_prompt_attack}, given a toxic prompt $\mathbf{p}$, Multi-Faceted Fast begins by appending a randomly initialized adversarial signature $\mathbf{p}_{\text{adv}}$, resulting in $\mathbf{p} + \mathbf{p}_{\text{adv}}$. The target content moderator, a LLM-based classifier $M(\cdot)$, tokenizes $\mathbf{p} + \mathbf{p}_{\text{adv}}$ into the word embedding space and computes the Cross Entropy loss on the predicted word ``\texttt{safe}''. In this space, each token has a one-hot vector $\mathbf{s}_i$ representing potential manipulations from the vocabulary $\mathcal{V}$. At each optimization step, the algorithm calculates the gradient of the loss \wrt token selections, $\nabla_{\mathbf{S}_{\text{adv}}} \mathcal{L}$, and selects the top-$k$ most impactful tokens for each position based on these gradients as the candidates. This reduces the risk of getting stuck in local minima, speeding up the process.

Multiple adversarial candidates are generated by choosing different tokens across all positions (line 11~\cref{alg:multifaceted_fast_prompt_attack}). This approach explores several attack trajectories simultaneously, increasing the likelihood of bypassing the content moderator. By evaluating multiple candidates  in parallel, the algorithm efficiently identifies the best adversarial signature and proceeds to the next optimization step, repeating until reaching the maximum iteration $N$. Finally,  the best adversarial signature is selected based on the loss, ensuring the highest chance of bypassing the content moderator.



\input{alg/multifaceted_fast_prompt_attack_update.tex}



\noindent\textbf{Transferability.} To improve the transferability of our attack, we propose a novel Transfer attack, which leverages weak supervision from another content moderator. A common approach to enhance transferability is to use model ensemble attacks~\cite{chen2023adaptive},
which aims to generate adversarial examples that can fool multiple models simultaneously, increasing the likelihood of transfer to others.
However, we found that the discrete token optimization problem makes this strategy challenging to fool multiple LLMs simultaneously, resulting in under-optimal solutions. To simplify the optimization problem and take advantage of each model, we introduce Multi-Faceted Transfer. Our attack involves two different content moderators, \ie, $M_1(\cdot)$ and $M_2(\cdot)$.  Instead of attacking the two models concurrently, we attack them separately. Specifically, we split the adversarial signature into two substrings, \ie, $\mathbf{p}_{\text{adv}} = \mathbf{p}_{\text{adv1}} + \mathbf{p}_{\text{adv2}}$ where $+$ indicates the concatenation operation, each substring is responsible to attack one model. We first focus on attacking $M_1$ using $\mathbf{p}_{\text{adv1}}$ with the same attack algorithm presented in~\cref{alg:multifaceted_fast_prompt_attack} except the metric used to pick up the best-performed candidate, as outlined in line 15. We use the loss of the other content moderator as an auxiliary supervision metric to select the best-performing adversarial substring $\mathbf{p}_{\text{adv1}}$, at each optimization step. More concretely, line 15 is modified to:

\vspace{-15pt}
{\small 
\begin{equation}
    \mathcal{L}_j \leftarrow M_1\big(\mathbf{p}+\mathbf{p}_{\text{adv1}}^{(j)}\big) + \lambda \cdot  M_2\big(\mathbf{p}+\mathbf{p}_{\text{adv1}}^{(j)}\big),
\end{equation}
}\vspace{-15pt}

where $\lambda$ is a hyperparameter and we set it as 1, and $\mathcal{L}$ indicates the Cross-Entropy loss target on word ``\texttt{safe}". After that, we repeat the same optimization loop to attack $M_2$ for seeking $\mathbf{p}_{\text{adv2}}$. 

Our insight is to minimize the Cross-Entropy loss target on ``\texttt{safe}" for the victim content moderator while minimizing the drift from the other content moderator. By doing so, we ensure that the generated adversarial signature is effective against the victim model and also likely to transfer to the other model. This approach allows us to take advantage of the strengths of each model and improve the overall transferability of our attack with less computational cost.
