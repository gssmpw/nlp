\input{tab/table_main_result}
\vspace{-3pt}
\section{Experiments}
\label{sec: experiments}

\subsection{Experimental Settings}
\noindent \textbf{Datasets.}
To evaluate the effectiveness of attacks, we use human-crafted toxic requests, \ie HEHS dataset, following prior work~\cite{qi2023visual}. The full dataset includes 40 manually designed toxic prompts that violate VLLMs' policies and can be flagged as ``\texttt{unsafe}'' by content moderators (details in ~\cref{sec:content_moderator}). Additionally, the proposed Adversarial Signature, a general attack method, is further evaluated using the StrongREJECT~\cite{sr}.


\vspace{-2pt}
\noindent \textbf{Metrics.}
Following prior work ~\cite{qi2023visual}, we employ human evaluators to assess whether the generated responses are genuinely harmful. 
\textit{An attack is considered successful if the response aligns with the harmful instruction and generates harmful content.} Non-refused, irrelevant responses are deemed failures. For the evaluation of Adversarial Signatures, we define attack success as the release of a toxic prompt that was initially flagged by the content moderator.


\vspace{-2pt}
\noindent\textbf{Models.} We primarily evaluate on popular open-sourced and commercial VLLMs as listed in ~\cref{tab:main_results}, the same as that of content moderators as outlined in~\cref{tab:total_moderator}. Refer to ~\cref{sec:vllms_details} and ~\ref{sec:content_moderator} for more details of these models.

\vspace{-2pt}
\noindent \textbf{Baselines.}
For comparison, we use the latest attacks targeting VLLMs, including Visual-AE~\cite{qi2023visual}, FigStep~\cite{gong2023figstep}, and HIMRD~\cite{teng2025heuristicinducedmultimodalriskdistribution}. Additionally, we use GCG~\cite{gcg} as a baseline for attacking LLM-based content moderators.





\vspace{-2pt}
\noindent \textbf{Implementation Details.}
To accommodate varying image size requirements for different VLLMs, we generate two types of adversarial images: 224px (perturbation $128/225$) and 448px (perturbation $64/225$), on MiniGPT-4/InternVL-Chat-V1.5~\cite{chen2024far, zhu2023minigpt}. For open-source VLLMs that require 448px images (\eg, NVLM and Llama-3.2-Vision-Instruct), we use 448px; for others, we use 224px. For commercial VLLMs, both image sizes are evaluated respectively, and the best result is reported. To ensure \mfa is applicable to real-world scenarios, we disable the visual attack on GPT-4o and Mistral-Large, where textual attacks alone are sufficient to bypass defenses with good performance.  Additionally, we perform white-box attacks on the LlamaGuard series~\cite{llamaguard1, llamaguard2, llamaguard3} to generate Adversarial Signatures.
For Visual-AE~\cite{qi2023visual}, we use the officially released most powerful unconstrained adversarial image, \ie perturbation $225/225$, generated on MiniGPT-4.
For FigStep~\cite{gong2023figstep}, we apply the released code to convert harmful prompts into images, as described in their paper.
For HIMRD~\cite{teng2025heuristicinducedmultimodalriskdistribution}, we use its code base, first segmenting harmful instructions across multiple modalities, then performing a text prompt heuristic search with Gemini-1.0-Pro.

  
\vspace{-5pt}
\subsection{Quantitative Results}\vspace{-3pt}
\cref{tab:total_vlm} presents the attack success rates (ASR) for \mfa and baseline methods across both open-source and commercial VLLMs. It also summarizes the defense layers adopted by each VLLM. The table provides a comparative evaluation of attack effectiveness and transferability, demonstrating that \mfa significantly outperforms baselines across various attack scenarios.

\vspace{-3pt}
\textbf{Results on Open-Source VLLMs.}
For the white-box attack on MiniGPT, both \mfa and Visual-AE~\cite{qi2023visual} achieve high ASRs, with \mfa achieving 100\%, 15\% higher than Visual-AE. In black-box transfer-based attacks, \mfa achieves a high ASR of 92.5\% on ShareGPT4V and 85\% on mPLUG-Owl2. In comparison, HIMRD consistently achieves an ASR near 50\%, while Visual-AE and FigStep show ASRs below 40\%. \mfa shows significant ASR reduction on Qwen-VL-Chat, which can be attributed to Qwen-VL-Chat's relatively weak understanding and instruction-following capabilities where failed responses are unrelated to the requests (see~\cref{sec: failure_HIMRD} for failure cases). The decreased ASR on Llama-3.2-Vision-Instruct may result from its adapter-based design for vision-text feature fusion, which is notably different from others. 

\vspace{-3pt}
\noindent \textbf{Results on Commercial VLLMs.} \cref{tab:total_vlm} demonstrates the strong transferability of \mfa in attacking black-box commercial VLLMs, even those protected by Multi-Layered defense mechanisms. Our method consistently achieves an average ASR of 61.56\%, outperforming the second-best attack, FigStep, by 42.18\%. 
\clearpage
\begin{strip}
    \centering
    \begin{minipage}{\textwidth}
        \centering
        \vspace{-15pt}\includegraphics[width=\linewidth]{figs/ADV_EG_V1.0.pdf}  
        \vspace{-20pt} 
        \captionof{figure}{
            \textbf{Qualitative results of \mfa} with baselines on commercial VLLMs, including GPT-4V (purple), GPT-4o (green), Gemini-2.0-Pro (red), Gemini-1.0-Pro (blue), Mistral-Large (orange), and Llama-3.2-11B-Vision-Instruct. \textcolor{BdazzledBlue}{Blue} indicates rejection, \textcolor{myred}{red} denotes harmful responses, and \textcolor{orange}{orange} represents unrelated responses. The bottom section gives more examples. Further detailed examples are available in the~\cref{sec: multi-facetd_egs}.
        }
     \label{fig:qualitative_results}
        \vspace{-5pt}
    \end{minipage}
\end{strip}





Additionally, the modular nature of each facet enables attackers to customize their approach for VLLMs with varying defense strategies.
In contrast, attacks targeting individual defense layers, \eg, alignment training, see substantial performance drops. For instance, Visual-AE experiences a significant ASR reduction, especially against models like Gemini-Pro and OpenAI series, which employ comprehensive Multi-Layered defenses. We observe that HIMRD can bypass guardrails to elicit responses from VLLMs, it struggles to generate genuinely harmful content aligned with  attacker's original instructions leading to failure attacks.





\subsection{Qualitative Results}\vspace{-3pt}
As shown in~\cref{fig:qualitative_results}, our Multi-Faceted Attack effectively induces a diverse range of VLLMs to generate explicitly harmful responses on various topics \footnote{Due to space limitation, see~\cref{sec: multi-facetd_egs} for more examples.}. In contrast, heuristic-based attacks like FigStep and HIMRD often lead to indirect or contextually irrelevant harmful responses. A representative example of HIMRDâ€™s limitations is shown in \cref{fig:failure_HIMRD}. Moreover, Visual-AE consistently fails to trigger harmful content generation, especially when applied to black-box commercial VLLMs, \eg, Gemini-Pro 2.0. These results highlight the superior effectiveness of our Multi-Faceted Attack in eliciting direct and harmful responses from VLLMs, outperforming existing attack methods.


\input{tab/table_moderator_small_size}


\subsection{Ablation Study}\vspace{-3pt}


\noindent \textbf{Adversarial Signature.} As reported in \cref{tab:ablation_moderator}, we conducted experiments under two settings, both using LlamaGuard2 to generate the adversarial font substring $\mathbf{p}_{\text{adv1}}$, with LlamaGuard and LlamaGuard3 serving as weak supervision models respectively, as detailed in \cref{sec:content_moderator_method}. The results show that both the Multi-Faceted Fast and Transfer methods outperform GCG, highlighting the benefits of \textit{optimizing multiple adversarial tokens in parallel} and leveraging \textit{weak supervision} from additional content moderators to \textit{improve effectiveness and transferability}. Specifically, Multi-Faceted Transfer surpasses GCG by an average of 35.34\% and 25.34\% in two settings. 


\noindent \textbf{Attack Facets.} \cref{tab:ablation} investigates the contributions of the three attack facets proposed in \mfa.~\footnote{The ablation studies are conducted on open-sourced models due to their controllability. For commercial models, we lack the necessary access to ablate the employed defense mechanisms.} The results demonstrate that all three attack facets contribute to the final ASR when compared to toxic prompts without attack. Open-sourced VLLMs typically rely on alignment training as their defense, but adding \textit{Visual Attack} and \textit{Adversarial Signature} contributes to bypassing these defenses. \textit{Visual Attack} injects a harmful system prompt within an image, coercing the VLLMs to follow toxic instructions. On the other hand, the \textit{Adversarial Signature} is crafted to deceive LLM-based content moderators into perceiving the toxic prompt as ``\texttt{safe}'' through its strong transferability. Since VLLMs are built upon LLM architectures, the \textit{Adversarial Signature} transfers the implicit ``\texttt{safe}'' semantic to VLLMs, convincing them that the harmful input is acceptable to respond to. When all three facets are combined, the attack performance reaches 75.71\%, demonstrating a synergistic effect. \textit{Each facet exploits a different vulnerability, complementing one another to maximize attack success.}


\input{tab/table_ablation_aug}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/combined_plots.pdf}\vspace{-15pt}
    \label{fig:computational_cost}
    \caption{Comparison of computational costs: (a) Parameters and computations during the attack for Multi-Faceted Attack and Visual-AE. (b) Average success attack time on LlamaGuard.}\label{fig:computational_cost}\vspace{-10pt}
\end{figure}