\section{Introduction}
\label{sec:intro}
Vision-Language Large Models (VLLMs), such as GPT-4o~\cite{openai2024gpt4ocard}, Gemini-Pro~\cite{gemini}, are reshaping user interactions and boosting productivity by enabling Large Language Models (LLMs) to process and understand visual information. However, as these models become increasingly capable, they also pose heightened risks of misuse, including generating harmful, unethical, or unsafe response~\cite{zhao_evaluating_2023, qi2023visual, gong2023figstep, teng2025heuristicinducedmultimodalriskdistribution, gou2024eyes, zhang2024benchmarking}.
\begin{figure*}[tbp]
    \centering
    \includegraphics[width=1.0\linewidth]{figs/Overview_final.pdf}
    \vspace{-20pt}
    \caption{Overview of \textbf{\mfa}. \textbf{(a)} Multi-Layered Defense strategies employed in VLLMs to enhance safety. \textbf{(b)} Existing attacks (\eg, textual attack~\cite{gcg} and visual jailbreaking image~\cite{qi2023visual}) can breach a single defense layer but fail against multi-layered defenses. \textbf{(c)} Our three attack facets work together break the guardrails and contribute to each others successfully, generating high-quality and genuinely harmful responses.}\vspace{-10pt}
    \label{fig:overview}
\end{figure*}
To mitigate risks posed by VLLMs, companies like OpenAI, Google, and Meta have implemented Multi-Layered safety guardrails~\cite{geminiteam2024geminifamilyhighlycapable, openai2024gpt4ocard, grattafiori2024llama3herdmodels}, as demonstrated in ~\cref{fig:overview} (a). These include \textit{alignment training} using RLHF~\citep{stiennon2020learning, ouyang2022training} or RLAIF~\citep{bai2022constitutional}, which aligns models with human values to generate helpful and safe responses. \textit{Safety system prompts} are designed to proactively guide models toward safe responses by setting predefined safety instructions~\citep{LLAMA2, LLAMA}. Additionally, \textit{add-on content moderators} act as external safeguards, flagging toxic user inputs or model responses~\citep{safety_checker, ShieldGemma, llamaguard1, llamaguard2, llamaguard3, openai_moderation}. 

While the above \textit{Multi-Layered Defense} strategy adopted by most commercial VLLMs is effective against existing attacks~\cite{gcg, qi2023visual} that target on jailbreaking the alignment defense, as illustrated in ~\cref{fig:overview} (b), their resilience against sophisticated adversarial attacks remains largely underexplored. The existing attack approaches rely on white-box access with limited black-box transferability~\cite{qi2023visual}, struggle against advanced Multi-Layered Defenses~\cite{gcg, qi2023visual, gong2023figstep}, or produce tangential responses~\cite{teng2025heuristicinducedmultimodalriskdistribution}. Therefore, developing effective attack methods that can evaluate Multi-Layered Defenses accurately and reliably is of profound importance.


To achieve this,
we propose \textbf{\mfa}, a novel adversarial attack framework designed to bypass Multi-Layered Defense and induce VLLMs to generate high-quality toxic content in response to harmful prompts. 
As illustrated in~\cref{fig:overview} (c), \mfa consists of three complementary attack facets. First, \emph{Visual Attack} exploits the rich representability of images to inject a toxic system prompt, causing VLLMs to obey the attacker's instruction without safety concerns and defeating the safety system prompt. Second, \emph{Alignment Breaking Attack} manipulates the alignment mechanism of VLLMs in a counterintuitive way, deceiving the model into prioritizing the generation of two contrasting responses. This misdirection causes the model to focus on completing the primary task while overlooking the toxicity in the prompt. Third, to bypass content moderators that typically prevent harmful outputs, we introduce an \emph{Adversarial Signature}—an attack strategy that misleads content moderators at the end of the response. These self-contained attack facets are modular and can be flexibly employed across various real-world scenarios. Together, they exhibit a synergistic effect, allowing \mfa to bypass single, double, and even the most robust Multi-Layered Defenses in a \textit{black-box} setting. 


Experimental results show that \mfa achieves a remarkable \textit{61.56\% black-box attack success rate} across eight popular commercial VLLMs, including Gemini-2.0-Pro~\cite{google2024gemini}, GPT-4o~\cite{openai2024gpt4ocard}, GPT-4V~\cite{gpt4v}, \etc. This represents a significant improvement of at least 42.18\% over the state-of-the-art (SOTA) attacks, underscoring the effectiveness of our attack. Moreover, our findings reveal critical vulnerabilities in current VLLM safety mechanisms, highlighting the urgent need for more robust defense strategies. 

Our main contributions are as follows: 
\begin{itemize}[leftmargin=*, itemsep=1pt]
\vspace{-5pt}
\item We propose a novel multi-faceted adversarial attack framework, specifically designed to systematically bypass Multi-Layered defenses in VLLMs. This work addresses a significant research gap, providing a comprehensive solution to evade SOTA defenses in VLLMs.

\item \mfa combines three complementary attack strategies—Visual Attack, Alignment Breaking Attack, and Adversarial Signature—to generate high-quality toxic content, overcoming the limitations of existing attacks that either fail to evade Muliti-Layered Defense or produce tangential responses. 

\item \mfa operates effectively in a black-box setting, achieving a 61.56\% attack success rate on commercial VLLMs. This is made possible by the synergistic effect of its three attack facets and their strong transferability, significantly advancing the SOTA in black-box attack performance.
\vspace{-5pt}

\end{itemize}



























% To bridge the current research gap and thoroughly assess the safety of modern VLLMs, we propose \mfa, a novel multimodal adversarial attack framework. Unlike existing approaches, which often focus on single-domain attacks~\cite{}, rely on white-box settings~\cite{}, or falter against advanced multi-layer defenses~\cite{}, \mfa attacks VLLMs in tow separate regimes: vision and text domain. 
% To be specific, \mfa crafts visual adversarial examples, augmented with add-on adversarial signatures and attention-shifting attack strategies. Each has its own advantages, and they offer complementary perspectives. This multifaceted attack strategy enables \mfa to successfully deceive black-box commercial VLLMs, demonstrating transferability and evading defensive mechanisms, as illustrated in~\cref{fig:intro}.
%  As illustrated in~\cref{}, \mfa's capabilities pose an effective stress test to the safety of modern VLLMs.
% However, the enhanced visual comprehension capabilities of VLLMs introduce novel misuse risks. Prior research has demonstrated that malicious actors can increase the likelihood of unsafe responses by incorporating jailbreaking images into the visual input, compared to text-only interactions with LLMs~\cite{}.
% While VLLMs demonstrate remarkable capabilities 
% across a variety of tasks, the presence of harmful and biased content within their training datasets poses a significant challenge. This can lead to the generation of toxic and undesirable outputs. To mitigate this issue,
% Despite the outstanding capabilities of VLLMs on diverse tasks, the detrimental and biased contents in the training datasets may still mislead VLLM to generate toxic and undersirable content. Reinforcement Learning from Human Feedback (RLHF) is introduced to further align LLM to human values~\cite{bai2022training, ouyang2022training}.
% Multi-Faceted Attack propose to attack the vision encoder within VLLM directly, instead of targeting the entire VLLM as adopted by most exiting works. For the textual attack, we propose a novel adversarial token selection algorithm. By doing so, \mfa provides a more fast and transferable approach than existing jailbreaking attacks. We show that our novel textual-modality transfer attack algorithm leads to 30\% improvement in the efficiency of attack, and 2.0 $\times$ effectiveness then existing works.








% From the technique contribution point of view, \mfa introduces a novel attack algorithm by directly targeting the vision encoder within the VLLM, unlike existing methods that typically attack the entire model~\cite{}. This targeted strategy enhances both speed and transferability compared to current jailbreaking attacks. In addition, \mfa propose a novel adversarial token selection algorithm to achieve a fast and strong transferability prompt attack to cheat popular content moderators effectively. Our experiments demonstrate that this combined approach results in a 30\% improvement in attack efficiency and a 2.0 $\times$ increase in effectiveness compared to existing techniques. Specifically, our novel textual-modality transfer attack algorithm contributes significantly to this performance gain.


% Through extensive experiments on multiple popular LLMs, we demonstrate the effectiveness of this attack in uncovering and exploiting vulnerabilities stemming from the visual grounding capacities of MLLMs. Our results highlight the need for further research into securing LLMs against such covert adversarial influence. Our contributions can be summarized as follows:

% \begin{itemize}
%     \item We propose Adversarial Visual Trigger, which leverages the generative capabilities of diffusion models to produce visual adversarial examples that are natural in appearance. This approach avoids the common pitfalls of traditional adversarial methods, such as overfitting and obvious perturbations, making the attacks more covert and difficult to detect.
    
%     \item We introduce a novel attack strategy, the Dual Prompt-Latent Diffusion Optimization (DPLDO), which synergistically refines both visual and textual inputs to craft adversarial visual triggers. These triggers are designed to be effective against black-box Multimodal Large Language Models (MLLMs).
    
%     \item Through extensive experimentation, we demonstrate the vulnerabilities in popular MLLMs, particularly in the context of visual grounding. Our results demonstrate the importance of further research in this area to develop more robust defense mechanisms against such sophisticated adversarial attacks.
% \end{itemize}






% by targeting the vision encoder directly, rather than the entire VLLM. This efficiency makes our method practical for evaluating even very large VLLMs, such as NVLM and Llama 3.2-Vision-Instruct, which are often intractable for existing attack methods.

% resharding It also effectively decouples
% ... and ... allowing independent optimization of each model without changing the code of other models in the dataflow. 




% We release our full Multifaceted Attack dataset, Multifaceted-Threat1K, to promote related research. 


% checkable answers 

% We attack VLLMs in tow separate regimes: vision and text domain. Each has its own advantages, and they offer complimentary perspectives. For vision domain, we

% These models are therefore not ideal for making an apples-to-apples comparison of outcome and process supervision. This setup enables us to conduct several important ablations, that would otherwise be infeasible. 

% separating out signals of helpfulness and harmlessness. We also focus on improving model safety, but focus on fast and scalable automated methods that leverage AI feedback. 

% Most related to our work. 

% proprietary models


% Our experiments find the add-on adversarial signatures to be comparatively lackluster to VLLMs while effectively fooling moderation models. We hypothesize that  

% To the best of our knowledge, this is the first work that 


%-------------------------------------------------------------------------
