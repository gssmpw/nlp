\section{Discussion}
\noindent\textbf{Computational Cost Analysis}
We compare the computational cost of our visual facet attack with that of the Visual-AE, which uses a gradient-based, end-to-end approach for adversarial image optimization. While Visual-AE utilizes the entire VLLM, our method eliminates all transformer layers in the language model, retaining only the vision encoder, its associated linear adapter, and the word embedding layer. This design significantly reduces computational overhead and resource requirements.
As shown in \cref{fig:computational_cost} (a), our image-facet attack uses just one-tenth of the parameters and GMACs for attack the same
MiniGPT4 model. In terms of attack time, as depicted in ~\cref{fig:computational_cost} (b) our proposed Fast attack achieves success on HEHS \cite{qi2023visual} in an average of 17.00 seconds, while GCG~\cite{gcg} takes 43.66 seconds (tested on A800 with the same configuration). 

\noindent \textbf{Failure Case Analysis.} 
The main cause of ours failures is the limited ability of VLLMs to generate contrasting responses as instructed. For example, Qwen-VL-Chat often produces identical responses, while ShareGPT may return a vague answer like “\texttt{Yes and No}” without generating harmful content, leading to unsuccessful attacks.~\footnote{Due to space constraints, detailed representative failure cases are provided in ~\cref{sec:failure_case_analysis}.}  