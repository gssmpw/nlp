\section{Related Work}
\textbf{Vision-Language Large Models}. 
VLLMs are designed to integrate visual and textual information, enabling them to perform tasks that require understanding both types of input~\cite{alayrac2022flamingo,li2023blip2,google2024gemini, openai2024gpt4ocard}. These models typically consist of two main components: a vision encoder (\eg, Vision Transformer) that processes images, and a language model (\eg, GPT or Llama) that processes text ~\cite{zhu2023minigpt, liu2023visual, su2023pandagpt, bert, GPT}. 
By training these components jointly, VLLMs learn the relationships between visual and textual representations, allowing them to handle tasks like image captioning, visual question answering, and chatting~\cite{zhu2023minigpt, liu2023visual, su2023pandagpt, bert, GPT}.
The architecture involves a shared embedding space where both visual and textual features are mapped and aligned, enabling the model to reason across modalities. 

\noindent \textbf{Adversarial Attacks in VLLMs}. 
Existing adversarial attacks on VLLMs face challenges in both effectiveness and transferability. Qi \etal~\citep{qi2023visual} proposed a gradient-based attack that generates adversarial images, aiming to trigger toxic responses by prompting the model to start with the word ``\texttt{Sure}''. However, this method requires full model access, limiting its applicability in black-box settings. Heuristic-based attacks, such as FigStep~\cite{gong2023figstep} and HIMRD~\cite{teng2025heuristicinducedmultimodalriskdistribution}, have also been explored. FigStep embeds malicious prompts within images featuring a white background and appended numerals, guiding the VLLM toward a step-by-step response to the harmful query~\cite{gong2023figstep}. HIMRD splits harmful instructions between image and text, searching heuristically for text prompts that increase the likelihood of affirmative responses~\cite{teng2025heuristicinducedmultimodalriskdistribution}. While these attacks can elicit non-refusal responses, they often result in off-topic or less harmful outputs. Most existing research has focused on open-source models, neglecting the challenge of transferring visual adversarial attacks to commercial models with advanced safety measures. In contrast, our approach improves transferability in black-box settings, effectively bypassing commercial safety guardrails.


\noindent \textbf{Defensive Methods}.
To ensure the safety of VLLMs, various defensive methods have been employed, including safety system prompts, post-training alignment, and content moderation. Safety system prompts provide manual or auto-refined textual guidance to steer model outputs toward safety~\cite{gong2023figstep, geminiteam2024geminifamilyhighlycapable, Jiang2024MixtralOE}, while post-training alignment techniques, such as RLHF and RLAIF, mimic human preferences for both helpful and safe responses~\citep{stiennon2020learning, ouyang2022training,bai2022constitutional}. Additionally, content moderation techniques fine-tune language models to flag prompts and responses that may be unsafe~\cite{safety_checker, ShieldGemma, llamaguard1, llamaguard2, llamaguard3, openai_moderation}.
To further enhance safety, commercial models like GPT-4V~\cite{2023GPT4VisionSC}, Gemini~\cite{geminiteam2024geminifamilyhighlycapable}, and DALLÂ·E 3~\cite{openai2023dalle3} have adopted Multi-Layered Defense mechanisms. These mechanisms combine multiple defensive techniques to make it more challenging for adversarial attacks to bypass the safety measures.
