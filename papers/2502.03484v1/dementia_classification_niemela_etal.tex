%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%% 
%%
%% $Id: elsarticle-template-num.tex 190 2020-11-23 11:12:32Z rishi $
%%
%%
\documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage[utf8]{inputenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{array}
\usepackage{diffcoeff}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{array}

\def\R{\mathbb{R}}
\def\cJ{\mathcal{J}}
\def\cc{\mathbf{c}}
\def\m{\mathbf{m}}
\def\x{\mathbf{x}}
\def\y{\mathbf{y}}
\def\C{\mathbf{C}}


%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Computer Methods and Programs in Biomedicine}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}

\title{Dementia Classification Using Acoustic Speech and Feature Selection}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author[inst1,inst2]{Marko Niemelä\corref{cor1}}
\ead{marko.p.niemela@jyu.fi}
\cortext[cor1]{\ Corresponding author}

\affiliation[inst1]{organization={Faculty of Information Technology, University of Jyväskylä},
            %addressline={PO Box 35}, 
            city={Jyväskylä},
            % postcode={FI-40014}, 
            % state={State One},
            country={Finland}}

\author[inst2,inst3]{Mikaela von Bonsdorff}

\affiliation[inst2]{organization={Faculty of Sport and Health Sciences and Gerontology Research Center}, %Department and Organization
            % addressline={Address Two}, 
            city={Jyväskylä},
            % postcode={FI-40014}, 
            % state={State Two},
            country={Finland}}
            
\affiliation[inst3]{organization={Public Health Programme, Folkhälsan Research Center}, %Department and Organization
            % addressline={Address Two}, 
            city={Helsinki},
            % postcode={FI-40014}, 
            % state={State Two},
            country={Finland}}

\author[inst1,inst4]{Sami Äyrämö}

\affiliation[inst4]{organization={Wellbeing Services County of Central Finland}, %Department and Organization
            % addressline={Address Two}, 
            city={Jyväskylä},
            % postcode={FI-40014}, 
            % state={State Two},
            country={Finland}}

\author[inst1]{Tommi Kärkkäinen}

            


\begin{abstract}
%% Text of abstract
\scriptsize{
\textbf{Background:}  Dementia is a general term for a group of syndromes that affect cognitive functions such as memory, thinking, reasoning, and the ability to perform daily tasks. The number of dementia patients is increasing as the population ages, and it is estimated that over 10 million people develop dementia each year. Dementia progresses gradually, and the sooner a patient receives help and support, the better their chances of maintaining their functional abilities. For this reason, early diagnosis of dementia is important. Diagnosing dementia is often challenging because it requires comprehensive evaluation and the exclusion of other diseases that cause similar symptoms. In recent years, machine learning models based on naturally spoken language have been developed for the early diagnosis of dementia. These methods have proven to be user-friendly, cost-effective, scalable, and capable of providing extremely fast diagnoses. \\
\textbf{Objective and Methods:} 
There are relatively few speech databases available for detecting cognitive decline compared to the scope of research questions. This study utilizes the well-known ADReSS challenge dataset for classifying healthy controls and Alzheimer’s patients. The dataset contains speech recordings from a picture description task featuring a kitchen scene, collected from both healthy controls and dementia patients. Unlike most studies, this research does not segment the audio recordings into active speech segments; instead, acoustic features are extracted from entire recordings using the open-source openSMILE library. Feature extraction from full audio recordings provides better computational performance, allowing for the identification of an optimal number of features.The study employs Ridge linear regression, Extreme Minimal Learning Machine, and Linear Support Vector Machine machine learning models to compute feature importance scores based on model outputs. These importance scores provide valuable insights into which aspects of speech production have the greatest impact on dementia diagnosis. \\
\textbf{Results and Conclusion:} The Ridge model performed best in Leave-One-Subject-Out cross-validation, achieving a classification accuracy of 87.8\%. The EMLM model, proved to be effective in both cross-validation and the classification of a separate test dataset, with accuracies of 85.3\% and 79.2\%, respectively. The study's results rank among the top compared to other studies using the same dataset and acoustic feature extraction for dementia diagnosis. The highest classification accuracies were achieved with fewer than 200 acoustic features. The most important features were frequency-domain spectrogram features and MFCC features, which are known to be associated with Alzheimer's disease. MFCC features provide an objective way to measure speech rate, vocal intensity, articulation clarity, and spectral changes in the voice. \\
\textbf{Keywords:} Dementia, Spontaneous speech, Acoustic features, Machine learning, Feature selection 
}
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%\includegraphics{grabs}
%\end{graphicalabstract}

%%Research highlights
%\begin{highlights}
%\item Research highlight 1
%\item Research highlight 2
%\end{highlights}

%\begin{keyword}
%%% keywords here, in the form: keyword \sep keyword
%keyword one \sep keyword two
%%% PACS codes here, in the form: \PACS code \sep code
%%\PACS 0000 \sep 1111
%%% MSC codes here, in the form: \MSC code \sep code
%%% or \MSC[2008] code \sep code (2000 is the default)
%%\MSC 0000 \sep 1111
%\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text
\section{Introduction}
\label{sec:Intro}

Dementia is a syndrome associated with the impairment of cognitive functions of the brain, such as memory, thinking, reasoning, and learning. It affects a person's ability to manage daily activities and may also cause changes in behavior, mood, and emotions. Dementia is not an independent disease but rather a consequence of underlying diseases or conditions, the most common of which is Alzheimer's disease (accounting for 60--70\% of all cases) \cite{world2017global}. The risk of dementia is increased by age, genetic predisposition, and chronic diseases.

Dementia gradually impairs cognitive abilities, with each stage characterized by specific cognitive and functional changes \cite{world2017global,wang2019cognitive}. In the early stage of dementia, individuals experience short-term memory decline, difficulties in learning new things, and reduced concentration. However, they are generally still able to manage daily activities. In the middle stage, symptoms include a shrinking vocabulary, confusion regarding time and place, and difficulties in decision-making and planning. In the late stage, typical symptoms include long-term memory loss, inability to recognize loved ones, and comprehensive cognitive and functional decline, where the need for assistance becomes so significant that the person is no longer able to perform daily routines independently. The number of people living with dementia is increasing globally as the population ages. According to the World Health Organization (WHO), more than 55 million people currently live with dementia, and this number is expected to nearly triple by 2050 \cite{world2017global}. Early recognition of cognitive symptoms is extremely important to ensure that individuals receive the necessary support at an early stage to mitigate symptoms. This can have a significant impact on the overall societal costs of dementia care \cite{world2017global}.

The progression of dementia can be slowed down with appropriate medication and rehabilitation. Physical and cognitive training, adherence to nutritional guidelines, and managing risk factors for cardiovascular diseases have been shown to prevent cognitive decline \cite{ngandu20152}. The difference was observed in all assessed areas: executive function, processing speed, and complex memory tasks \cite{ngandu20152}. The risk of memory-related diseases could be reduced by up to 30\% \cite{ngandu20152,kulmala2019effect}. World-Wide FINGERS is an international research network aimed at reducing the risk of memory disorders globally, regardless of cultural differences \cite{kivipelto2020world}.

Neuroimaging (particularly MRI, PET, and CT scans), biomarker studies (such as cerebrospinal fluid analysis and blood tests), and anamnesis (including the patient's medical history and background information) have been utilized in the early-stage screening and clinical assessment of dementia \cite{jack2018nia}. MRI studies have shown that in Alzheimer's disease, brain changes are particularly concentrated in memory-related areas, such as the hippocampus and its surrounding regions in the temporal lobe. These changes lead to memory impairment in the early stages of the disease \cite{hanyu2000magnetization}. The challenges of using biomarkers and neuroimaging primarily relate to the costs, availability, standardization, and accuracy of early diagnosis \cite{chiu,young2020imaging}. Machine learning models, particularly deep learning-based convolutional neural networks (CNNs), have been applied in neuroimaging-related diagnostics. However, a significant challenge so far has been the limited availability of data \cite{ebrahimighahnavieh2020deep}.

A more common approach is to use paper-based tests, such as the standardized Mini-Mental State Examination (MMSE) \cite{folstein1975mini} and the Montreal Cognitive Assessment (MoCA) \cite{nasreddine2005montreal}, which evaluate memory, attention, and problem-solving skills. The Consortium to Establish a Registry for Alzheimer's Disease (CERAD) test is a comprehensive neuropsychological test battery designed to assess multiple cognitive domains, including memory (episodic memory, delayed recall), language skills (naming, verbal fluency), and visuospatial ability (copying tasks). This test battery reached its widely used current form mainly in the early 1990s \cite{morris1989consortium}. The CERAD test is detailed and sensitive to mild cognitive impairments, making it particularly suitable for in-depth assessments, long-term follow-ups, and research purposes. With the increasing use of mobile devices, mobile applications have been developed for cognitive testing, such as the Mobile Screening Test (MCS), whose results have been found to correlate with those of the MoCA test \cite{zorluoglu2015mobile}.


The analysis of naturally spoken language provides a cost-effective and accessible approach to identifying dementia. In Alzheimer's disease and other dementias, difficulties with semantic memory, word retrieval, and sentence construction may appear in the early stages \cite{reilly2011anomia,ivanova,fraser2015linguistic}. Technologies related to speech analysis have advanced significantly over the past decade. As a result, artificial intelligence models based on machine learning and speech and language processing have been developed. These models aim to utilize acoustic and text-based features of speech to detect early signs or progression of cognitive impairments \cite{de2020artificial}. Speech analysis offers a non-invasive and natural approach to the early assessment of dementia \cite{beltrami2018speech}.


There are few available speech databases for detecting cognitive decline compared to the broad scope of research questions \cite{de2020artificial}. While analyses are often comprehensive, experiments have typically been conducted with a relatively small number of participants \cite{lopez2015feature,tanaka2017detecting}, which poses a challenge as the results may not generalize well to larger populations. Another common issue is the imbalance between the number of healthy control participants and dementia patients, as well as differences in age and gender distributions within groups. These factors can lead to misleading classification model results. Additionally, in some cases, multiple speech recordings from the same individuals have been used for both training and testing AI models, which may cause classification results to rely on speaker-specific characteristics rather than the symptoms of cognitive impairment. Some studies have also categorized patients based on the severity of dementia, making direct comparisons of results more difficult. Furthermore, many studies do not provide open access to their datasets. The main goal of the INTERSPEECH 2020 Alzheimer's Dementia Recognition through Spontaneous Speech (ADReSS) challenge\footnote{https://dementia.talkbank.org/ADReSS-2020/} was to provide a standardized and openly available speech database to enable the evaluation and comparison of different machine learning models for detecting cognitive decline \cite{luz2021alzheimer}. The competition proposed two evaluation approaches for assessing model performance: Leave-One-Subject-Out (LOSO) cross-validation and a separate test dataset comprising approximately one-third of the total data.


This study utilizes the ADReSS challenge dataset to classify healthy control participants and Alzheimer's patients. Acoustic features are extracted from the audio recordings using the open-source openSMILE library\footnote{https://www.audeering.com/research/opensmile/}. The results indicate that using the Ridge Linear Regression (Ridge), Extreme Minimal Learning Machine (EMLM), and Linear Support Vector Machine (L-SVM) machine learning models, it is possible to diagnose individuals accurately and extremely quickly. On a separate test dataset, classification accuracies exceed 70\%, while LOSO validation achieves over 80\% accuracy. These results rank among the top-performing studies that have used the same dataset and relied exclusively on acoustic (non-verbal) feature extraction for patient diagnosis. Additionally, the experiments run efficiently because, unlike in the original competition, recordings are not segmented into multiple active speech segments, but rather classified based on features extracted from entire recordings. As a result, the total number of feature vectors remains small, corresponding to the number of individuals in the dataset ($N=156$). Thanks to the computational efficiency of the models, this study has been able to determine the optimal number of features that yield the highest classification accuracies. The selection of key features and the optimal feature set is performed by leveraging the models' ability to assign weight values to features based on their output responses.

\section{Background}
\label{sec:Background}

Acoustic features can be analyzed directly from audio files without converting speech into text, making the analysis fast, resource-efficient, and cost-effective. In dementia detection, acoustic methods are particularly useful because the models are independent of language, educational background, and cultural differences.


Acoustic models reflect changes in speech production and voice characteristics. In dementia, variations in fundamental frequency (F0) may occur, which can manifest as fluctuations in voice jitter and amplitude shimmer \cite{meilan}. The dynamics of voice intensity can indicate changes in emotional expression and motor control \cite{scherer2003vocal}. In monotonic speech, the fundamental frequency remains particularly stable, changing very little over time \cite{martinez}. This lack of significant intonation variation makes speech sound flat and monotonous. A slower speech rate and irregular rhythm may suggest cognitive difficulties \cite{pistono2016pauses}. The number, length, and placement of pauses can reveal challenges in linguistic processing and memory \cite{pappagari2020using}. Additionally, the use of simpler words and the avoidance of complex structures may be indicative of cognitive impairments. Mel-frequency cepstral coefficients (MFCCs) \cite{meghanani2021exploration,pappagari2020using} and formant frequencies (F1 and F2) \cite{morris2023second} are commonly used in speech recognition to represent overall voice quality and clarity. In dementia, hoarseness, breathiness, and voice maintenance issues may be prevalent \cite{parlak2023voice}.

The study \cite{eyben2015geneva} introduces a standardized minimal acoustic feature set, the Extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS), which consists of 88 features. This feature set includes prosodic speech features (F0 and voice intensity), spectral features (MFCCs), articulatory features (formant frequencies, jitter, and shimmer), and temporal speech features (number and duration of pauses, as well as speech rate). Other well-known acoustic feature sets used for dementia detection include the Computational Paralinguistics Challenge (ComParE) feature set, which consists of thousands of features \cite{schuller2013interspeech}, the IS10-Paralinguistic set, which is a subset of 1582 paralinguistic features from ComParE \cite{schuller2013interspeech}, the Emotion Base (EmoBase) feature set \cite{eyben}, and the Collaborative Voice Analysis Repository for Speech Processing (COVAREP) \cite{degottex}.

Deep learning models have been utilized in acoustic feature extraction. The x-vector, based on a deep neural network \cite{snyder2018x}, is generated by feeding the network short-time segmented speech features. In study ´\cite{pappagari2020using}, the final layer of the neural network produced a fixed-dimensional x-vector from MFCC features, with the dimension determined by the network's architecture. Principal Component Analysis (PCA) \cite{jolliffe2002principal} was then applied to compute coefficients, forming the final feature vectors. Additionally, statistical measures derived from silent speech segments were incorporated into the feature vectors to improve diagnostic accuracy. In study \cite{cummins2020comparison}, raw segmented audio recordings and zero-frequency filtered (ZFF) signals \cite{gangamohan2017robust} were used as inputs for a deep convolutional neural network (CNN) \cite{li2021survey}, which produced an output representing the probability of Alzheimer's disease. VGGish is a convolutional neural network that processes log-Mel spectrogram features (often in matrix format) as input, compressing them into multidimensional representations for further processing or prediction \cite{gemmeke2017audio}. In study \cite{koo2020exploiting}, VGGish feature extraction was directly applied to dementia diagnosis.

Typically, a set of low-level feature vectors (LLDs) is extracted from speech signals (e.g., from 10-–20 ms speech segments). In feature aggregation, LLD features are combined into global feature vectors. In statistical aggregation, metrics such as mean, median, minimum, maximum, variance, or standard deviation are computed from LLD features \cite{schuller2013interspeech}. The i-vector is a statistical model and an extension of the Gaussian Mixture Model (GMM) \cite{bishop}, which compresses the most relevant characteristics of speech data into a low-dimensional vector representation that can be expressed within a single feature space \cite{dehak2010front}. This method is known for its computational efficiency. In study \cite{pompili2020inesc}, i-vectors were generated using MFCC feature extraction from speech recordings and a Universal Background Model (UBM) \cite{reynolds2000speaker}, which was trained on MFCC features from the VoxCeleb speech database \cite{nagrani2017voxceleb}.

In study \cite{cummins2020comparison}, the Bag-of-Audio-Words (BoAW) method \cite{schmitt2017openxbow} was used to create a codebook of acoustic words. Feature vectors (MFCC, Log-Mel, and ComParE features) were quantized, and a global histogram representation was formed from the quantized feature vectors. Additionally, in the same study, Log-Mel spectrogram-based features were aggregated using a Siamese network \cite{chopra2005learning}, which generated global representations of feature vectors based on mathematical distance metrics. Study \cite{syed2020automated} introduced a new approach to dementia classification called Active Data Representation (ADR). This method clustered eGeMAPS, ComParE, EmoBase, and Multi-Resolution Cochleagram (MRCG) features \cite{chen2014feature} using a self-organizing map (SOM) network \cite{kohonen1990self} and created global histogram representations based on feature extractions and segment durations. The Fisher Vector (FV) model \cite{tian2014speaker} is an efficient method for summarizing speech data. It models the distribution of feature vectors using a Gaussian Mixture Model (GMM), obtaining distribution parameters (centroids, covariance matrices, and weights), which are optimized using the Expectation-Maximization (EM) algorithm \cite{dempster1977maximum}. For each speech signal, gradients are computed with respect to the optimized parameters and are then combined into a single vector. These normalized vectors are used for classification. In study \cite{syed2020automated}, the FV model was applied separately to articulatory features, spectral features, paralinguistic features, ComParE-level features, and VGGish features to aggregate them effectively.

\subsection{Multimodal approaches}

In multimodal approaches, feature fusion or method integration is used to develop more accurate machine learning models. In study \cite{martinc2020tackling}, cross-validation and grid search were applied to find the best feature combination from a total of sixteen sets of lexical and acoustic features. Studies \cite{balagopalan2020bert} and \cite{pappagari2020using} used cross-validation and gradient boosting regression (GBR) \cite{friedman2001greedy} to fuse the outputs of acoustic and lexical prediction models, aiming to minimize prediction errors. Studies \cite{cummins2020comparison}, \cite{edwards2020multiscale}, \cite{koo2020exploiting}, \cite{syed2020automated}, and \cite{haider2019assessment} employed a majority voting classification mechanism to combine the most effective lexical and acoustic models. In study \cite{rohanian2021multi}, parallel recurrent neural networks (RNNs) \cite{rumelhart} with a gating mechanism \cite{arevalo2017gated} were used to integrate GloVe-embedded text features with COVAREP-extracted speech features. Additionally, speech uncertainty markers (such as fillers, repetitions, and corrections) were encoded into POS tags for speech transcription analysis.

\section{Methods}
\subsection{Dataset}

The Pitt Corpus database was collected between 1983 and 1988 as part of Alzheimer's research \cite{becker1994natural}. A total of 319 participants were selected, including 102 healthy control individuals and 204 Alzheimer's patients. The participants' diagnoses were based on clinical assessments and long-term follow-up. The eligibility criteria for participation included a minimum age of 44 years, at least seven years of education, the ability to read and write before the onset of dementia, no central nervous system disorders or medications, an MMSE score of at least 10/30, an understanding of the study's nature, the requirement of a caregiver, and informed consent from both the participant and their caregiver. Participants were tasked with describing events in as much detail as possible based on a kitchen scene picture, which is part of the Boston Diagnostic Aphasia Examination (BDAE) test battery \cite{fong2019factor}. A caregiver guided the participant at the beginning of the picture description task. During free-form conversation, the caregiver had the opportunity to ask follow-up questions or introduce topics to steer the discussion. All participants' responses were recorded as audio files for analysis. Additionally, the recordings were transcribed into the CHAT format, including verbal outputs from the speakers, as well as annotations for pauses, repetitions, and errors.

The ADReSS 2020 challenge dataset is a 156 people subset of the Pitt Corpus, containing one recording per individual. The participants were selected so that half are healthy control individuals and half are dementia patients. Additionally, the dataset is divided into a training set of 108 individuals (54 healthy controls and 54 with Alzheimer's disease) and a test set of 48 individuals (24 healthy controls and 24 with Alzheimer's disease). Table \ref{Table1} illustrates the distribution, taking into account the age and gender distribution of the participants.

\begin{table}[ht!]
\centering
\footnotesize 
\bigskip     
\begin{tabular}{lllllllll}
\hline\noalign{\smallskip}
& \multicolumn{2}{c}{Train-AD} & \multicolumn{2}{c}{Test-AD} & \multicolumn{2}{c}{Train-Non-AD} & \multicolumn{2}{c}{Test-Non-AD} \\
Age & M  & F & M & F & M & F & M & F \\
\noalign{\smallskip}\hline\noalign{\smallskip}
{[}50, 55) & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 \\
{[}55, 60) & 5 & 4 & 2 & 2 & 5 & 4 & 2 & 2 \\
{[}60, 65) & 3 & 6 & 1 & 3 & 3 & 6 & 1 & 3 \\
{[}65, 70) & 6 & 10 & 3 & 4 & 6 & 10 & 3 & 4 \\
{[}70, 75) & 6 & 8 & 3 & 3 & 6 & 8 & 3 & 3 \\
{[}75, 80) & 3 & 2 & 1 & 1 & 3 & 2 & 1 & 1 \\
\noalign{\smallskip}\hline
Total & 24 & 30 & 11 & 13 & 24 & 30 & 11 & 13 \\
\noalign{\smallskip}\hline
\end{tabular}
\caption{ADReSS 2020 dataset (M=male, F=female, AD=Alzheimer's disease).} \label{Table1}  
\end{table}

\subsection{Data preprocessing}

The ADReSS dataset contained audio recordings segmented into active speech segments. However, in this study, entire recordings were used instead of speech segments, as they were available in the DementiaBank database\footnote{https://dementia.talkbank.org/access/English/Pitt.html} from the TalkBank project. The recordings contained significant background noise and extraneous sounds, such as caregiver speech, buzzer sounds, traffic noise, phone ringing, and sudden noises. Noise removal was performed using the adaptive noise reduction filter in Adobe Audition, which did not require manual adjustments. Additionally, unwanted sounds were manually removed, ensuring that the final recordings contained only participant speech or silent pauses during moments of thought. Paralinguistic sounds that reflected emotions and attitudes, such as laughter or sighs, were preserved. The recordings were originally sampled at 44 kHz but were downsampled to 16 kHz, allowing reliable signal reproduction in the 0-–8 kHz frequency range. Since most speech frequency components are below 8 kHz, a 16 kHz sampling rate is sufficient for speech recognition and analysis. Finally, the recordings were normalized according to the EBU R 128 standard \cite{ebu2011loudness} to ensure consistent volume levels and were stored in the WAV file format. The purpose of normalization was to minimize variations in recording conditions, such as the microphone position effect. The average duration of the control participants' recordings was 51.9 seconds (SD: 24.9 seconds), while for Alzheimer's patients, it was 58.7 seconds (SD: 33.0 seconds).

\subsection{Feature extraction}

For acoustic feature extraction, the openSMILE library \cite{eyben} was used to extract 88 eGeMAPS, 988 EmoBase, and 6373 ComParE features from the audio recordings. These features were combined into a comprehensive acoustic feature set, totaling 7449 features. From this combined feature set, low-variance features (157 in total) and duplicate features formed during the fusion of EmoBase and ComParE features (369 in total) were removed. The final feature set contained 6923 features, most of which represented low-level spectral descriptors, delta coefficients that capture momentary spectral changes, or statistical descriptors derived from spectral features and delta coefficients \cite{eyben2015real}. In addition to traditional statistical descriptors such as mean, standard deviation, minimum, and maximum values, other calculated values included percentiles, interquartile ranges, skewness, kurtosis, slope of the linear regression line, linear error, quadratic regression coefficients, quadratic error, temporal statistical characteristics of speech, spectral variance, spectral entropy, spectral centroid, and peak amplitude occurrences.

\subsection{Statistical feature selection}

\subsubsection{Ridge regression}

Ridge regression (Ridge) is an extension of linear regression (LR) \cite{hoerl1970ridge}. The model retains the basic structure of the LR model, which is based on the assumption that the response variable $\mathbf{y}$ can be modeled as a linear combination of explanatory variables $\mathbf{X}$. Ridge regression is designed to improve the stability and predictive accuracy of the LR model in situations where there is multicollinearity among variables, the sample size is small relative to the number of features, or the model is at risk of overfitting. The objective of traditional LR models is to find the coefficients of the linear model by minimizing the least squares error function. Ridge regression adds a regularization term to the error function, which prevents the coefficients from becoming excessively large. The expanded form of the Ridge regression error function can be expressed as follows:


\begin{equation*}
\cJ(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) + \lambda \boldsymbol{\beta}^\top \boldsymbol{\beta},
\end{equation*}
where $\mathbf{y} \in \R^{N}$ is the response variable, $N$ represents the number of observations, $\mathbf{X} \in \R^{N\times n}$ is an $n$-dimensional observation matrix, and $\boldsymbol{\beta} \in \R^n$ represents the regression coefficients to be optimized. By expanding and differentiating with respect to $\boldsymbol{\beta}$, the equation can be expressed as: 
\[
\nabla \cJ(\boldsymbol{\beta}) = -2 \mathbf{X}^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) + 2 \lambda \boldsymbol{\beta}
\]
The problem can be solved directly by setting the gradient to zero. However, when dealing with large datasets, an iterative solution is more efficient:
\[
\boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} - \eta \nabla \cJ(\boldsymbol{\beta}),
\]
where $\eta$  is the learning rate, and $t$ represents the iteration number.

In Ridge regression, the importance values of features can be directly interpreted from the regression coefficients. The absolute values of the regression coefficients represent the absolute impact of the features on the response variables.

\subsubsection{Extreme Minimal Learning Machine}

The Extreme Minimal Learning Machine (EMLM) is a distance-based supervised machine learning model. It combines the Ridge regression method, used for training the weight coefficients of a feedforward neural network, with the kernel function approach of the Minimal Learning Machine (MLM), which is based on distance calculations \cite{karkkainen2019extreme,hamalainen2020minimal}. Distances are computed for a selected set of observation points, referred to as reference points. The number of reference points, $m$, is the only adjustable parameter in the model.

The weight coefficients for distance-based classification, $\mathbf{W} \in \R^{k\times m}$, where $k$ is the number of classes, can be calculated by solving the following linear equation:
\[
\mathbf{W}\left(\mathbf{H}\mathbf{H}^T + \frac{\alpha N}{m}\mathbf{I}\right) = \mathbf{Y}\mathbf{H}^T,
\]
where $\mathbf{H} \in \R^{m\times N}$ is the distance matrix between the reference points $\{\mathbf{r}_i\}_{i=1}^m,\ \mathbf{r}_i \in \mathbb{R}^n$, and observation points $\{\mathbf{x}_j\}_{j=1}^N, \mathbf{x}_j \in \mathbb{R}^n$, with $\left(\mathbf{H}\right)_{ij}=\lVert\mathbf{r}_i-\mathbf{x}_j\rVert_2$. For the full EMLM model considered here, all $N$ observations themselves are used as reference points. The columns of the matrix $\mathbf{Y} \in \mathbb{R}^{k\times N}$ contain the 1-of-$k$ encoded class information. The matrix $\mathbf{I} \in \mathbb{R}^{m \times m}$ is the identity matrix, whose coefficient $\alpha=\sqrt{\epsilon}$ contains the regularization parameter (the square root of the machine epsilon). The predicted output $\mathbf{y}^*$  of the trained model for the input $\mathbf{x}^*$ is given by $\mathbf{y}^*=\mathbf{WH^*}$, where $(H^*)_{i1} = \| r_i - x^* \|_2, i = 1, \dots, m.$

For the input feature vectors in the EMLM model's training dataset, it is possible to compute the average absolute sensitivity values, which can be used as feature importance scores \cite{karkkainen2015assessment}. These sensitivity values are based on the impact of features on the model's output and can be calculated as follows:
\[
\mathcal{FI} = \frac{1}{N} \sum_{j=1}^N\left|\diffp{\mathcal{M}}{{\mathbf{x}_j}}\right| ,
\]
where $\mathcal{M}$ represents the output of the distance-based model. The gradient of the output can be expressed in a well-defined form \cite{karkkainen2005computation} as follows:
\begin{equation*}
\diffp{\mathcal{M}}{{\mathbf{x}_j}} = \mathbf{W}\mathbf{D}^T,\ \mathbf{d}_i = \frac{\mathbf{r}_i-\mathbf{x}_j}{\max\left(\epsilon, |\mathbf{r}_i-\mathbf{x}_j|\right)},\ \ i=1,\ldots,m.
\end{equation*}
In the equation, $\mathbf{d}_i$ refers to column $i$ in the matrix $\mathbf{D}$.

\subsubsection{Support vector machine}


The error function of a Linear Support Vector Machine (L-SVM) is based on maximizing the margin and minimizing classification errors \cite{cortes1995support}. The goal is to find the largest possible margin around the hyperplane that separates the classes while penalizing data points that fall on the wrong side of the margin or are misclassified. The error function, based on hinge loss, optimizes the normal of the hyperplane, i.e., the weight vector $\mathbf{w}$ and the offset from the origin $b$. The objective is to minimize the following error function:

\begin{equation*}
\mathcal{J}(\mathbf{w}, b) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^N \max(0, 1 - y_i (\mathbf{w} \cdot \mathbf{x}_i + b)),
\end{equation*}
where \( C \) is the regularization parameter, which controls the balance between hinge loss and regularization. The gradient of the hinge loss depends on whether a data point violates the margin. The gradient is 0 if  $y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1$
meaning it does not contribute to the loss.  Assuming there exists a set of data points  $S = \{i \mid 1 - y_i (\mathbf{w}^T \mathbf{x}_i + b) > 0\} $ that violate the margin, the gradient of the error function can then be expressed as follows:

\begin{align*} 
\nabla_{\mathbf{w}} \mathcal{J}(\mathbf{w}, b) &= \mathbf{w} - C \sum_{i \in S} y_i \mathbf{x}_i, \\
\nabla_b \mathcal{J}(\mathbf{w}, b) &= -C \sum_{i \in S} y_i.
\end{align*} 
The gradient is used iteratively to update the parameters:
\begin{align*} 
\mathbf{w} &\leftarrow \mathbf{w} - \eta \nabla_{\mathbf{w}} \mathcal{L}(\mathbf{w}, b), \\
b &\leftarrow b - \eta \nabla_b \mathcal{L}(\mathbf{w}, b).
\end{align*} 
The importance values are obtained from the components of the normal vector $\mathbf{w}$. 

\subsection{Feature selection using permutation testing}

In this study, feature selection was performed using permutation testing, which was based on calculating feature importance values from the training dataset (108 individuals in total) and from the permuted training dataset. The importance values were computed using five-fold cross-validation. In five-fold cross-validation, the training data was divided into five subsets, and in each iteration, four subsets (80\% of the training data) were used for model training and feature importance calculation, while the remaining subset was used for validation.

In feature selection, the following process is performed a total of 100 times: 

Randomly split the dataset five times using cross-validation into a training subset (80\% of the data) and a test subset (20\% of the data). For each split, perform the following steps:
\begin{enumerate}
\item Normalize the training subset using Min-Max normalization \cite{ruppert2004elements} within the range $[0,1]$.
\item Train the machine learning model using the normalized training data and its corresponding class labels.
\item Compute the feature importance values for each feature using the trained model.
\item Retrain the model using the same training data but with permuted class labels, and then recalculate the feature importance values.
\item Store both the original feature importance values and the permutation-based feature importance values.
\item After completing five splits, compute the median importance values for each feature based on the five computed values.
\end{enumerate}
After completing the process, each feature will have a total of 100 median importance values and 100 permutation-based median importance values.

The features were ranked from most to least important based on the average importance values. The distributions of importance values were compared using the Wilcoxon signed-rank test \cite{wolfowitz1949non}. The null hypothesis of the test assumes that the distributions originate from the same population with a median of zero. The first feature in the ranked list that did not reject the null hypothesis (at a 5\% significance level) was considered an excess feature, and all preceding features were included in the selected feature set.

\subsection{Comparison of classification methods}


This study employed two evaluation approaches for assessing method performance, as presented in \cite{luz2021alzheimer}: Leave-One-Subject-Out (LOSO) cross-validation and a separate test dataset. In LOSO cross-validation, each individual is iteratively left out as the test set, while the remaining individuals form the training set. This process is repeated for all individuals, and the model's performance is evaluated by calculating the average result across all iterations. Additionally, a separate test dataset was used, accounting for approximately one-third of the total dataset (48 out of 156 individuals). In both LOSO validation and test set experiments, the training data was normalized using Min-Max normalization, and the same normalization coefficients were applied to the test data.

The study compared the performance of three machine learning models: Ridge \cite{hoerl1970ridge}, L-SVM \cite{cortes1995support}, and EMLM \cite{karkkainen2019extreme,hamalainen2020minimal}. First, feature importance weights were calculated for each feature, after which the most important features (those with the highest weights) were used for classification. Before the experiments, the features were ranked from most to least important based on their importance values.

The regularization parameters for the Ridge and L-SVM models were selected using five-fold cross-validation and grid search. The parameters were searched within a logarithmically distributed range from 
$1e-3$ to $1e+3$. For the EMLM model, all training dataset samples were used as reference points. The Ridge model's error function optimization was based on the coordinate descent method, implemented using the Glmnet library\footnote{https://hastie.su.domains/glmnet\_matlab/}, which was ported to MATLAB (V. 2024B, 64-BIT \cite{hastie2014glmnet}. The L-SVM model implementation was based on Python (V. 3.12.2) scikit-learn library\footnote{https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html} (V. 1.6.1), where the SMO algorithm (Sequential Minimal Optimization) \cite{platt1998sequential} was used for minimizing the support vector machine's error function. The EMLM implementation was based on source code available on GitLab\footnote{https://gitlab.jyu.fi/hnpai-public/extreme-minimal-learning-machine}. Experiments were also conducted using the Random Forest model \cite{breiman2001random}, which is commonly used for feature selection and classification. However, Random Forest overfitted the dataset due to the small sample size and large number of features, despite the application of strong regularization ($MinLeafSize = 10$ and $MaxNumSplits = 5$). For this reason, the method was not included in the final experiments.

\section{Results}

Figure \ref{Fig:Fig1} illustrates the classification accuracies of the Ridge, EMLM, and L-SVM models in LOSO validation and separate test dataset classification using the 2500 most important features based on their weight values. Table \ref{tab:2} lists the best classification accuracies along with the number of features required to achieve these accuracies. The Ridge model achieved the highest performance (87.8\%) in LOSO validation. The best classification accuracies for the EMLM and L-SVM models were the same (85.3\%), but the EMLM model required significantly fewer features (186 vs. 1000 features) to achieve this result. Moreover, the EMLM model proved to be clearly the best model for classifying the separate test dataset, achieving an accuracy of 79.2\%. Classifying the test dataset was more challenging for all models compared to LOSO validation. For the L-SVM model, the optimal number of features differed significantly between LOSO validation (1000 features) and test dataset classification (81 features). The experiments aimed to determine the optimal number of features based on the Wilcoxon signed-rank test (Table \ref{tab:3}). In LOSO validation, the models retained high accuracy (82.1\%--84.0\%) using the statistically determined number of features. However, in test dataset classification, the results for Ridge and L-SVM models deteriorated significantly, with accuracies dropping to 66.7\% and 64.6\%, respectively. Figure \ref{Fig:Fig2} presents the confusion matrix illustrating the best dementia classification results obtained with the Ridge model in LOSO validation. \ref{sec:appendix1} lists the 145 most important features used for the diagnosis.


\begin{figure}[ht!]
%\captionsetup[subfigure]{labelformat=empty}
  \centering
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{LOSO.png} 
    %\caption{O200 data set}
  \end{subfigure}
  ~
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{Test.png}
    %\caption{ADS with O200 data}
  \end{subfigure}
  
        \caption{Classification accuracies up to the 2500 most important features in LOSO validation (left) and on the separate test dataset (right).}
  \label{Fig:Fig1}
\end{figure}


\begin{figure}[ht!]
%\captionsetup[subfigure]{labelformat=empty}
  \centering
  \begin{subfigure}{0.50\textwidth}
    \includegraphics[width=\textwidth]{Confusion.png} 
    %\caption{O200 data set}
  \end{subfigure}
        \caption{Confusion matrix for dementia diagnosis based on Ridge regression.}
  \label{Fig:Fig2}
\end{figure}


\begin{table}[ht!]
\centering 
\caption{Best classification accuracies in LOSO validation. The corresponding number of features is shown in parentheses.}
\label{tab:2}       
\begin{tabular}{l|lll|lll}
\hline
Method & LOSO Acc. & F$_1$ & Features & Test Acc. & F$_1$ & Features  \\
\hline
Ridge &  87.8\% & 87.4\% & (145) & 72.9\% & 73.5\% & (131)   \\
EMLM &  85.3\%  & 85.2\% & (186) & 79.2\% & 79.2\% & (152)   \\
L-SVM &  85.3\%  & 85.2\% & (1000)  & 75.0\% & 76.9\% & (81)   \\
\hline
\end{tabular}
\end{table}

\begin{table}[ht!]
\centering 
\caption{Results using the number of features obtained with the Wilcoxon signed-rank test.}
\label{tab:3}       
\begin{tabular}{l|lll|lll}
\hline
Method & LOSO Acc. & F1 score & Features  & Test Acc. & F1 score & Features \\
\hline
Ridge & 84.0\% & 83.4\% & (986) & 66.7\% & 69.2\% & (986)   \\
EMLM & 82.7\%  & 82.6\% & (141) & 77.1\% & 77.6\% & (141)   \\
L-SVM & 82.1\% & 81.8\% & (800) & 64.6\% & 62.2\% & (800)   \\
\hline
\end{tabular}
\end{table}

\section{Discussion}
\label{sec:Discussion}

This study performed dementia diagnosis based on naturally spoken language audio recordings from the ADReSS 2020 benchmark dataset, which is a subset of the larger Pitt Corpus speech database. The study utilized open-source openSMILE feature extraction, extracting acoustic eGeMAPS, EmoBase, and ComParE features, which were fused into a single large feature set. Features with no variation and high correlation were removed, resulting in a total of 6923 features.Unlike in study \cite{luz2021alzheimer}, the audio recordings were not segmented into active speech segments, but instead, feature extraction was performed on entire recordings. In previous literature, the same dataset and acoustic feature extraction have been used in studies \cite{balagopalan2020bert}, \cite{pappagari2020using}, \cite{cummins2020comparison}, \cite{edwards2020multiscale}, \cite{koo2020exploiting}, and \cite{syed2020automated}, with reported classification accuracies ranging from 56.5\% to 85.3\%. In study \cite{syed2020automated}, the best results were obtained using IS10-Paralinguistic and VGGish deep learning-based feature extraction on speech segments, which were aggregated into global feature representations using BoAW and FV methods. The highest accuracy (76.9\%) was achieved with BoAW aggregation, where IS10-Paralinguistic feature vectors were quantized into word-histogram feature vectors. Deep learning-based convolutional neural networks (CNNs) have also yielded strong results. In study \cite{koo2020exploiting}, VGGish feature extraction was used as input to a CNN for patient classification. The classification accuracies obtained in five-fold cross-validation on the training dataset and on the separate test dataset were 85.3\% and 72.9\%, respectively.

In this study, feature selection was based on feature importance values computed using the Ridge, EMLM, and L-SVM models with both true and permuted class labels. The distributions of importance values were compared using the Wilcoxon signed-rank test, applying a 5\% statistical significance level. Statistical testing has also been performed in other studies where dementia patients were identified based on naturally spoken language. In study \cite{koo2020exploiting}, one-way ANOVA (assuming normal distribution) was used to select only those ComParE features that were statistically significant between target classes at a 5\% significance level ($p \leq 0.05$). A total of 393 features were selected, which accounted for approximately 6.2\% of the 6373 available features. In study \cite{edwards2020multiscale}, dimensionality reduction was performed using the Correlation Feature Selection (CFS) method \cite{hall1999correlation}, followed by Recursive Feature Elimination with Cross-Validation (RFECV) \cite{guyon2002gene} in LOSO validation. In the first stage, CFS selected 3593 ComParE features that had the least correlation among each other ($r < 0.85$). Then, in the RFECV-based second stage, a machine learning model was trained on the remaining features, importance values were calculated, and the least important feature was removed iteratively. This process continued until only one feature remained. After the second stage, only 54 ComParE features were selected, achieving 74.0\% accuracy in LOSO validation on the training dataset.

In this study, the best results were primarily based on spectrogram features at the frequency level and MFCC features. These features are known to be associated with Alzheimer's disease \cite{ahn2023deep,huang2024automatic,mahajan2021acoustic}. In Alzheimer's patients, speech is often characterized by imprecise articulation, monotony, increased pauses, and a decline in voice quality. These changes can be measured using spectral centroid, Harmonics-to-Noise Ratio (HNR), spectral dispersion, as well as variations in MFCC and delta-MFCC features.

In this study, LOSO validation yielded significantly better results compared to classification on the separate test dataset. However, the test dataset consisted of only 48 patients. In future research, it may be possible to increase the dataset size by including a larger group of individuals, which could improve the reliability of the results. The DementiaBank database from the TalkBank project contains several corpora, such as the Wisconsin Longitudinal Study (WLS) corpus, which is a large longitudinal dataset \cite{herd2014cohort}. The dataset includes the Cookie Theft picture description task (1370 recordings in total) as well as multiple scored linguistic tasks that assess word retrieval speed (verbal fluency), short-term memory, semantic memory, and linguistic reasoning skills. Future research could also explore a multimodal approach, where both acoustic and lexical features are utilized. In the literature, multimodal methods have generally achieved the highest classification accuracies when classifying the test dataset of the ADReSS benchmark \cite{rohanian2021multi,edwards2020multiscale,pompili2020inesc,koo2020exploiting}.

\section{Conclusion}
\label{sec:Conclusion}

This study focused on identifying Alzheimer's disease based on acoustic speech features, utilizing machine learning models and feature selection. Classifying acoustic speech features presents a promising method for detecting Alzheimer's disease without the need for expensive or invasive diagnostic tests. The Ridge model achieved the best performance in LOSO validation, with a classification accuracy of 87.8\%. The EMLM model was effective in both LOSO validation and test dataset classification, achieving 85.3\% and 79.2\% accuracy, respectively. These classification results rank among the top-performing approaches compared to other studies that used the same dataset and only acoustic feature extraction for dementia diagnosis. However, multimodal methods, which incorporate both acoustic and lexical features, have reported even better results. The experiments included statistical testing using permutation testing and the Wilcoxon signed-rank test to determine the optimal number of features, and the results were promising for LOSO validation. Based on feature importance values, frequency-level MFCC features were particularly effective in distinguishing healthy control individuals from dementia patients.In future research, the goal is to achieve even better results by utilizing larger datasets and exploring multimodal approaches for dementia classification.



\section*{Acknowledgments}
\label{sec:Acknowledgments}

The work of the first Author (MN) was supported by the Finnish Cultural Foundation (Grant Number 30242323).

% The Appendices part is started with the command \appendix;
% appendix sections are then done as normal sections
\clearpage
\appendix
%
\section{Acoustic features}
\label{sec:appendix1}

\begin{table*}[ht!]
\label{tab:4}   
\caption{The 145 most important acoustic features based on Ridge regression.}
\footnotesize
\centering 
\begin{tabular}{rll}
\hline
 Rank &                                            Feature &                    Category \\
\hline
    1 &                                   mfcc\_sma[7]\_lpc2 &                        MFCC \\
    2 &              audSpec\_Rfilt\_sma\_de[8]\_upleveltime50 &        Auditory Spectrogram \\
    3 &            pcm\_fftMag\_spectralEntropy\_sma\_centroid &            Spectral Entropy \\
    4 &                                   mfcc\_sma[7]\_lpc3 &                        MFCC \\
    5 &             pcm\_fftMag\_spectralEntropy\_sma\_de\_lpc4 &            Spectral Entropy \\
    6 &            audSpec\_Rfilt\_sma[1]\_stddevFallingSlope &        Auditory Spectrogram \\
    7 &         pcm\_fftMag\_spectralEntropy\_sma\_de\_skewness &            Spectral Entropy \\
    8 &                  audSpec\_Rfilt\_sma\_de[3]\_minSegLen &        Auditory Spectrogram \\
    9 &                 audSpec\_Rfilt\_sma\_de[3]\_meanSegLen &        Auditory Spectrogram \\
   10 &                               F0final\_sma\_centroid &                  F0 (Pitch) \\
   11 &           pcm\_fftMag\_spectralVariance\_sma\_centroid &           Spectral Variance \\
   12 &                               mfcc\_sma[2]\_linregc1 &                        MFCC \\
   13 &    pcm\_fftMag\_spectralEntropy\_sma\_de\_upleveltime25 &            Spectral Entropy \\
   14 &        pcm\_fftMag\_spectralRollOff90,0\_sma\_linregc1 &           Spectral Roll-Off \\
   15 &        pcm\_fftMag\_spectralRollOff75,0\_sma\_linregc1 &           Spectral Roll-Off \\
   16 &              audSpec\_Rfilt\_sma\_de[3]\_upleveltime50 &        Auditory Spectrogram \\
   17 &              audSpec\_Rfilt\_sma\_de[2]\_upleveltime50 &        Auditory Spectrogram \\
   18 &              audSpec\_Rfilt\_sma\_de[9]\_upleveltime50 &        Auditory Spectrogram \\
   19 &             pcm\_fftMag\_spectralEntropy\_sma\_de\_lpc3 &            Spectral Entropy \\
   20 &        pcm\_fftMag\_spectralRollOff90,0\_sma\_centroid &           Spectral Roll-Off \\
   21 &                           pcm\_zcr\_sma\_de\_quartile2 & Spectral Zero-Crossing Rate \\
   22 &        pcm\_fftMag\_spectralHarmonicity\_sma\_centroid &        Spectral Harmonicity \\
   23 &                                 mfcc\_sma[6]\_qregc1 &                        MFCC \\
   24 &                   jitterLocal\_sma\_de\_upleveltime25 &                      Jitter \\
   25 &        pcm\_fftMag\_spectralRollOff75,0\_sma\_centroid &           Spectral Roll-Off \\
   26 &                audSpec\_Rfilt\_sma\_de[4]\_minRangeRel &        Auditory Spectrogram \\
   27 &                         mfcc\_sma[14]\_upleveltime25 &                        MFCC \\
   28 &                           F0final\_sma\_de\_quartile3 &                  F0 (Pitch) \\
   29 &                            mfcc\_sma\_de[5]\_linregc2 &                        MFCC \\
   30 &                         mfcc\_sma[10]\_upleveltime75 &                        MFCC \\
   31 &                               mfcc\_sma\_de[13]\_lpc3 &                        MFCC \\
   32 &                              mfcc\_sma[12]\_linregc1 &                        MFCC \\
   33 &        pcm\_fftMag\_spectralRollOff50,0\_sma\_linregc1 &           Spectral Roll-Off \\
   34 &                            mfcc\_sma\_de[5]\_linregc1 &                        MFCC \\
   35 &           pcm\_fftMag\_spectralVariance\_sma\_linregc1 &           Spectral Variance \\ 
\hline
\end{tabular}
\end{table*}  

\clearpage

\begin{table}[ht!]
\footnotesize
%\caption{Hellloooooooooooooooo.................}
\centering 
\begin{tabular}{rll}
\hline
 Rank &                                            Feature &                    Category \\
   36 &    pcm\_fftMag\_spectralRollOff75,0\_sma\_peakRangeRel &                        MFCC \\
   37 &                audSpec\_Rfilt\_sma[11]\_upleveltime90 &        Auditory Spectrogram \\
   38 &                        mfcc\_sma\_de[2]\_peakRangeRel &            Spectral Entropy \\
   39 &                            F0final\_sma\_de\_kurtosis &                        MFCC \\
   40 &   pcm\_fftMag\_spectralSlope\_sma\_de\_peakMeanMeanDist &            Spectral Entropy \\ 
   41 &        pcm\_fftMag\_spectralSlope\_sma\_de\_peakMeanAbs &        Auditory Spectrogram \\
   42 &                              F0final\_sma\_de\_iqr2-3 &            Spectral Entropy \\
   43 &         audSpec\_Rfilt\_sma\_de[15]\_stddevRisingSlope &        Auditory Spectrogram \\
   44 &                            mfcc\_sma[7]\_peakMeanRel &        Auditory Spectrogram \\
   45 &           pcm\_fftMag\_spectralCentroid\_sma\_linregc1 &                  F0 (Pitch) \\   
   46 &           pcm\_fftMag\_spectralCentroid\_sma\_centroid &           Spectral Variance \\
   47 &                       pcm\_loudness\_sma\_de\_linregc1 &                        MFCC \\
   48 &       pcm\_fftMag\_fband250-650\_sma\_de\_upleveltime25 &            Spectral Entropy \\
   49 &                       audSpec\_Rfilt\_sma\_de[0]\_lpc0 &           Spectral Roll-Off \\
   50 &                   logRelF0-H1-H2\_sma3nz\_stddevNorm &           Spectral Roll-Off \\
   51 &                              mfcc3\_sma3\_stddevNorm &        Auditory Spectrogram \\
   52 &          pcm\_fftMag\_spectralSlope\_sma\_meanPeakDist &        Auditory Spectrogram \\
   53 &                    pcm\_RMSenergy\_sma\_de\_meanSegLen &        Auditory Spectrogram \\
   54 &               pcm\_fftMag\_psySharpness\_sma\_linregc2 &            Spectral Entropy \\
   55 &               pcm\_fftMag\_psySharpness\_sma\_centroid &           Spectral Roll-Off \\
   56 &               pcm\_fftMag\_fband250-650\_sma\_centroid & Spectral Zero-Crossing Rate \\
   57 &                       mfcc\_sma\_de[11]\_peakRangeAbs &        Spectral Harmonicity \\
   58 &        pcm\_fftMag\_spectralRollOff50,0\_sma\_linregc2 &                        MFCC \\
   59 &               audSpec\_Rfilt\_sma\_de[4]\_peakRangeRel &                      Jitter \\
   60 &            pcm\_fftMag\_spectralRollOff90,0\_sma\_lpc3 &           Spectral Roll-Off \\
   61 &                          shimmerLocal\_sma\_linregc1 &        Auditory Spectrogram \\
   62 &                                   mfcc\_sma[6]\_lpc4 &                        MFCC \\
   63 &              pcm\_fftMag\_spectralSlope\_sma\_kurtosis &                  F0 (Pitch) \\
   64 &                      F0final\_sma\_de\_percentile99,0 &                        MFCC \\
   65 &            audSpec\_Rfilt\_sma\_de[13]\_peakDistStddev &                        MFCC \\
   66 &              pcm\_fftMag\_spectralSlope\_sma\_centroid &                        MFCC \\
   67 & pcm\_fftMag\_spectralKurtosis\_sma\_stddevFallingSlope &                        MFCC \\
   68 &            pcm\_fftMag\_fband1000-4000\_sma\_quartile1 &           Spectral Roll-Off \\
   69 &                     voicingFinalUnclipped\_sma\_lpc3 &                        MFCC \\
   70 &                          mfcc\_sma\_de[4]\_meanSegLen &           Spectral Variance \\
   71 &       pcm\_fftMag\_spectralVariance\_sma\_de\_leftctime &                        MFCC \\
   72 &              audSpec\_Rfilt\_sma\_de[3]\_upleveltime90 &        Auditory Spectrogram \\
   73 &                            mfcc\_sma\_de[7]\_linregc2 &            Spectral Entropy \\
   74 &                       audSpec\_Rfilt\_sma\_de[0]\_lpc1 &                        MFCC \\
   75 &    pcm\_fftMag\_spectralSlope\_sma\_de\_meanRisingSlope &            Spectral Entropy \\
  
\hline
\end{tabular}

\end{table}

\clearpage   
\begin{table}[ht!]
\footnotesize
%\caption{Hellloooooooooooooooo.................}
\centering 
\begin{tabular}{rll}
\hline
 Rank &                                            Feature &                    Category \\  
   76 &                         pcm\_intensity\_sma\_linregc2 &        Auditory Spectrogram \\
   77 &           audSpec\_Rfilt\_sma\_de[15]\_meanRisingSlope &            Spectral Entropy \\
   78 &                           mfcc3V\_sma3nz\_stddevNorm &        Auditory Spectrogram \\
   79 &               pcm\_fftMag\_spectralVariance\_sma\_lpc3 &        Auditory Spectrogram \\
   80 &                               mfcc\_sma\_de[13]\_lpc4 &                  F0 (Pitch) \\   
   81 &                         mfcc\_sma[10]\_upleveltime90 &           Spectral Variance \\
   82 &                audSpec\_Rfilt\_sma[10]\_upleveltime90 &                        MFCC \\
   83 &                                 mfcc\_sma[7]\_qregc1 &            Spectral Entropy \\
   84 &                       audSpec\_Rfilt\_sma\_de[1]\_lpc1 &           Spectral Roll-Off \\
   85 &                               mfcc\_sma\_de[11]\_lpc4 &           Spectral Roll-Off \\
   86 &                       audSpec\_Rfilt\_sma\_de[1]\_lpc0 &        Auditory Spectrogram \\
   87 &               pcm\_fftMag\_spectralVariance\_sma\_lpc1 &        Auditory Spectrogram \\
   88 &           pcm\_fftMag\_spectralEntropy\_sma\_minSegLen &        Auditory Spectrogram \\
   89 &               pcm\_fftMag\_psySharpness\_sma\_linregc1 &            Spectral Entropy \\
   90 &                       audSpec\_Rfilt\_sma\_de[1]\_lpc2 &           Spectral Roll-Off \\  
   91 &                  pcm\_fftMag\_spectralSlope\_sma\_lpc0 & Spectral Zero-Crossing Rate \\
   92 &                           lspFreq\_sma[2]\_quartile3 &        Spectral Harmonicity \\
   93 &            audSpec\_Rfilt\_sma[14]\_stddevRisingSlope &                        MFCC \\
   94 &                            F0final\_sma\_de\_posamean &                      Jitter \\
   95 &             pcm\_fftMag\_spectralEntropy\_sma\_de\_lpc0 &           Spectral Roll-Off \\   
   96 &                              lspFreq\_sma[0]\_minPos &        Auditory Spectrogram \\
   97 &            pcm\_fftMag\_spectralSlope\_sma\_meanSegLen &                        MFCC \\
   98 &            audSpec\_Rfilt\_sma[15]\_stddevRisingSlope &                  F0 (Pitch) \\
   99 &        pcm\_fftMag\_spectralRollOff25,0\_sma\_linregc2 &                        MFCC \\
  100 &                              mfcc\_sma\_de[12]\_amean &                        MFCC \\
  101 &                                 mfcc\_sma[6]\_qregc2 &                        MFCC \\
  102 &                        lspFreq\_sma\_de[4]\_quartile2 &                        MFCC \\
  103 &        audSpec\_Rfilt\_sma\_de[13]\_stddevFallingSlope &           Spectral Roll-Off \\
  104 &                          mfcc\_sma[1]\_upleveltime90 &                        MFCC \\
  105 &          audSpec\_Rfilt\_sma\_de[13]\_meanFallingSlope &           Spectral Variance \\
  106 &                                  logHNR\_sma\_qregc1 &                        MFCC \\
  107 &                                mfcc\_sma[13]\_qregc1 &        Auditory Spectrogram \\
  108 &               pcm\_fftMag\_spectralVariance\_sma\_lpc0 &            Spectral Entropy \\
  109 &                       audSpec\_Rfilt\_sma\_de[1]\_lpc3 &                        MFCC \\
  110 &              audSpec\_Rfilt\_sma\_de[8]\_upleveltime75 &            Spectral Entropy \\
  111 &                            mfcc\_sma\_de[7]\_linregc1 &        Auditory Spectrogram \\
  112 &                                 mfcc\_sma[1]\_qregc1 &            Spectral Entropy \\
  113 &         pcm\_fftMag\_spectralFlux\_sma\_de\_minRangeRel &        Auditory Spectrogram \\
  114 &                                 jitterDDP\_sma\_lpc4 &        Auditory Spectrogram \\
  115 &                      mfcc\_sma\_de[10]\_upleveltime25 &                  F0 (Pitch) \\
\hline
\end{tabular}

\end{table}

\clearpage   
\begin{table}[ht!]
\footnotesize
\centering 
\begin{tabular}{rll}
\hline
 Rank &                                            Feature &                    Category \\  
  116 &                              F0final\_sma\_de\_rqmean &           Spectral Variance \\
  117 &                       audSpec\_Rfilt\_sma\_de[0]\_lpc2 &                        MFCC \\
  118 &                              F0final\_sma\_de\_stddev &            Spectral Entropy \\
  119 &             audSpec\_Rfilt\_sma[4]\_stddevRisingSlope &           Spectral Roll-Off \\
  120 &        pcm\_fftMag\_spectralRollOff50,0\_sma\_centroid &           Spectral Roll-Off \\ 
  121 &             pcm\_fftMag\_fband250-650\_sma\_meanSegLen &        Auditory Spectrogram \\
  122 &          audSpec\_Rfilt\_sma\_de[2]\_stddevRisingSlope &        Auditory Spectrogram \\
  123 &              audSpec\_Rfilt\_sma[1]\_meanFallingSlope &        Auditory Spectrogram \\
  124 &        pcm\_fftMag\_fband250-650\_sma\_de\_segLenStddev &            Spectral Entropy \\
  125 &                 pcm\_fftMag\_fband250-650\_sma\_qregc1 &           Spectral Roll-Off \\
  126 &                       mfcc\_sma\_de[2]\_upleveltime25 & Spectral Zero-Crossing Rate \\
  127 &        pcm\_fftMag\_spectralSlope\_sma\_peakDistStddev &        Spectral Harmonicity \\
  128 &             pcm\_fftMag\_spectralSlope\_sma\_minSegLen &                        MFCC \\
  129 &            audSpec\_Rfilt\_sma\_de[20]\_peakDistStddev &                      Jitter \\
  130 &       pcm\_fftMag\_spectralSlope\_sma\_de\_peakRangeAbs &           Spectral Roll-Off \\
  131 &                               voiceProb\_sma\_maxPos &        Auditory Spectrogram \\
  132 &                            mfcc\_sma\_de[8]\_linregc1 &                        MFCC \\
  133 &       pcm\_fftMag\_spectralSlope\_sma\_meanRisingSlope &                  F0 (Pitch) \\
  134 &                       audSpec\_Rfilt\_sma\_de[1]\_lpc4 &                        MFCC \\
  135 &                         pcm\_RMSenergy\_sma\_centroid &                        MFCC \\
  136 &        pcm\_fftMag\_fband1000-4000\_sma\_upleveltime50 &                        MFCC \\
  137 &               audSpec\_Rfilt\_sma[4]\_meanRisingSlope &                        MFCC \\
  138 &                                   mfcc\_sma[10]\_max &           Spectral Roll-Off \\
  139 &            audSpec\_Rfilt\_sma[2]\_stddevFallingSlope &                        MFCC \\
  140 &     pcm\_fftMag\_spectralEntropy\_sma\_de\_segLenStddev &           Spectral Variance \\
  141 &         pcm\_fftMag\_spectralEntropy\_sma\_de\_risetime &                        MFCC \\
  142 &                     pcm\_RMSenergy\_sma\_de\_maxSegLen &        Auditory Spectrogram \\
  143 &                     voicingFinalUnclipped\_sma\_lpc2 &            Spectral Entropy \\
  144 &     pcm\_fftMag\_spectralEntropy\_sma\_de\_meanPeakDist &                        MFCC \\
  145 &                          mfcc\_sma\_de[14]\_quartile2 &            Spectral Entropy \\
\hline
\end{tabular}

\end{table}




%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
 \bibliographystyle{elsarticle-num} 
 \bibliography{mybib}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

% %% \bibitem{label}
% %% Text of bibliographic item

% \bibitem{}

% \end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
