\subsection{Empirical Mean Error}
In this section, we discuss the error introduced by using a finite empirical mean in \eqref{eq:mean_fully}, \eqref{eq:mean_efim_likd}, and \eqref{eq:bprior_mean} instead of the true expectation. We start by assuming that the \hvhedit{output of the }learned score model follows a sub-Gaussian distribution. \hvhedit{Meaning it have finite sub-Gaussian norm which is defined as:}
\begin{definition}[sub-Gaussian Norm]
    Let $\randomvec{A}\in\mathbb{R}$ be a random scalar, then the sub-Gaussian Norm is givn by:
    $$\norm{\randomvec{A}}_{\psi_2}\triangleq\inf\set{c>0: \expectation{\exp{\brackets{\frac{\randomvec{A}^2}{c^2}}}}{\randomvec{A}} \leq 2}.$$
    In case $\randomvec{A}\in\mathbb{R}^n$ be a random vector then:
$$\norm{\randomvec{A}}_{\psi_2}\triangleq\sup\limits_{\vectorsym{v}\in\mathcal{S}^{n-1}}\inf\set{c>0: \expectation{\exp{\brackets{\frac{\brackets{\vectorsym{v}^T\randomvec{A}}^2}{c^2}}}}{\randomvec{A}} \leq 2},$$
   where $\mathcal{S}^{n-1}$ is the unit sphere in $\matsym{R}^n$.
    
\end{definition}

\hvhdelete{The sub-Gaussian norm for a random scalar $\randomvec{A}\in\mathbb{R}$ is given by $\norm{\randomvec{A}}_{\psi_2}\triangleq\inf\set{c>0: \expectation{\exp{\brackets{\frac{\randomvec{A}^2}{c^2}}}}{\randomvec{A}} \leq 2}$. The sub-Gaussian norm for a random vector $\randomvec{B}\in\matsym{R}^n$ is given by $\norm{\randomvec{B}}_{\psi_2}\triangleq\sup\limits_{\vectorsym{v}\in\mathcal{S}^{n-1}}\norm{\vectorsym{v}^T\randomvec{B}}_{\psi_2} $ where $\mathcal{S}^{n-1}$ s the unit sphere in $\matsym{R}^n$.}
% \todo[inline,color=green]{YB: 1. While all the other notations are standard, the subGaussian norm is not, and would take some effort by many readers to decipher. However, it is only needed in Section V.B. I suggest you only introduce this definition where first needed - In Section V.B. 
% }

% \todo[inline,color=green]{Move here the definition of the subGaussian}
\begin{assumption}[sub-Gaussian]\label{ass:subg} The score vector of all the score neural networks is sub-Gaussian, namely:
$\norm{\postscore{\p}{\X}}_{\psi_2}<\infty$, $\norm{\lscore{\X}{\p}}_{\psi_2}<\infty$ and $\norm{\priorscore{\p}}_{\psi_2}<\infty$.
%     \begin{equation}
%         \norm{\postscore{\p}{\X}}_{\psi_2}<\infty 
%     \end{equation}
% \begin{subequations}
%     \begin{align}
%         &\norm{\lscore{\X}{\p}}_{\psi_2}<\infty\\
%         &\norm{\priorscore{\p}}_{\psi_2}<\infty
%     \end{align}
% \end{subequations}
\end{assumption}
\todoin{YB: The application of the $\psi_2$ norm is not clear here. What is random in $\postscore{\p}{\X}$ and in $\lscore{\X}{\p}$? $\X$, $\p$, or both?\\
HVH: I assume that both $\X$ and $\p$  are random but in scope of the subGaussian norm, I think that we can look at the score function output as a random vector does this make sense? \\
YB: Not quite. The definition of the subGaussian norm involves taking the expectation wrt the random vector $A$. In all  cases that we consider the random vector is the score vector, of dimension $d_{\p}$.
However, with respect to what distribution should the expected value be taken in each case, and why? I think that this deserves some elaboration.
}
Note that there are several examples that %hold to 
satisfy this assumption as presented in \cite{crafts2023bayesian}. For example, since the score neural network is a continuous function then any distribution \hvhedit{with bounded support}
\todo[inline,color=orange]{YB: "Any distribution" of what? Of $\p$, or $\X$, or $\X|\p$? or $\p|\X$? Why? } 
will result in a sub-Gaussian score. Using assumption ~\ref{ass:subg} we provide the following two Theorems:
\begin{theorem}\label{thm:sample_post} Assume that assumptions~\ref{assum:bcrb_reg}, ~\ref{ass:score_cond_reg} and ~\ref{ass:subg} hold, then for any $u>0$ with probability $1-\exp{\brackets{-u}}$:
    \begin{align}\label{eq:error_post_lrn_mean}
        &\norm{\lbfimbs-\fb}_2\leq \bfle+\bfse,
    \end{align}
    where,
    \begin{equation}\label{eq:emprical_mean_bound}
        \bfse=C\norm{\postscore{\p}{\X}}_{\psi_2}u\brackets{\sqrt{\frac{\np+u}{\nds}}},
    \end{equation}
    \hvhedit{is the bound on empirical mean error, $u\brackets{x}=\min\brackets{x,x^2}$ and C is a universal constant. }
     % where 
\end{theorem}
\todoin{Symbol $u$ is used to denote different things in the same equation \eqref{eq:emprical_mean_bound}}
\begin{theorem}\label{thm:sample_lik_prior}
    Assume that assumptions~\ref{assum:bcrb_reg}, ~\ref{ass:score_reg_prior}, ~\ref{ass:score_reg} and ~\ref{ass:subg} hold, then for any $u>0$ with probability $1-\exp{\brackets{-u}}$:
    \begin{align}\label{eq:error_lik_prior_lrn_mean}
        &\norm{\lbfimlps-\fb}_2\leq \mfle+\pfle+\pfse+\mfse,
    \end{align}
    where
    \begin{subequations}
        \begin{align}
            &\mfse=C\norm{\lscore{\X}{\p}}_{\psi_2}u\brackets{\sqrt{\frac{\np+u}{\nds}}}, \nonumber\\
        &\pfse=C\norm{\priorscore{\p}}_{\psi_2}u\brackets{\sqrt{\frac{\np+u}{\nds}}},\nonumber    
        \end{align}
    \end{subequations}
    are the empirical mean error of the $\lmfim$ and $\lpfim$, respectively. 
\end{theorem}
\todoin{Symbol $u$ is used to denote different things in the same equation \eqref{eq:error_lik_prior_lrn_mean}}
\todoin{You have combined the learning error with the finite average error in these theorems. I suggest just showing the finite average error here. You will have an opportunity to combine the various errors (rather trivially, using the triangle inequality) in the final results about the relative error. }
In Theorems ~\ref{thm:sample_post} and ~\ref{thm:sample_lik_prior} provide the bound on \name{} errors due to learning and mean errors for both approaches. 
% \todo[inline,color=green]{You are using the subscript $s$ to denote the mean error - perhaps having in mind "Sampling error".
% But you did not explain the mnemonic, and "sampling error" may not be the right term - it usually means "error due to selecting a sample not representative of the distribution".\\
% HVH: what do you think about $\eta^{(e)}$ which is stand for emprical mean error ?\\
% YB: looks good to me.}
From this result, we observe that as the number of samples $\nds$ increases, the mean error is reduced. In addition, we observe the effect of parameter dimensions $\np$ that as we have more parameters to estimate, we require more samples.
% \todo[inline,color=green]{YB: Given the effort in deriving them, can you provide more insight into these results? \\
% 1. Argue that in many applications, the  regime of interest is $\nds > \np+u $, in which case the bounds depend on $\frac{\np+u}{\nds}$ linearly, implying rapid convergence with increasing number of samples $\nds$. \\
% 2. You say that the bounds show the effect of $\np$. This is true, if the respective subGaussian norms do not depend on $\np$. Is this the case? Why?\\
% 3. The bounds are hard to interpret quantitatively, because or the subGaussian norms that cannot  be computed, and the unspecified constant $C$. Are there simpler and more interpretable (possibly less sharp) bounds or estimates on the error due to finite sample averaging - e.g., based on the central limit Thm, or even the Chebychev inequality?\\
% HVH: I think there are several options to replace this bound:\\
% 1. One is the Matrix Bernstein. For this we need to assume that\\
% $$\frac{1}{n}\norm{\vectorsym{s}_k\vectorsym{s}^T_k-\expectation{\vectorsym{s}\vectorsym{s}^T}{\X,\p}}\leq L$$ is bounded where $\vectorsym{s}=\lscore{\X}{\p}+\priorscore{\p}$ and $\vectorsym{s}_k=\lscore{\x_k}{\p_k}+\priorscore{\p_k}$.

% 2.A different type of bound can be utilized, If assume that the distribution of the learned score is bounded, namly:
% $$\norm{\lscore{\X}{\p}+\priorscore{\p}}\leq L$$,
% then we can use the same bound as in CRB paper. \\
% For some reason I think that the condition of the  Matrix Bernstein is the most reasonable, since: similar to the score matching condition we can claim that with a right choose of neural network this condition can be satisfied.
% \\
% YB: What are $s_k$ and $s$ above? Also are both assumptions 1 and 2 above needed for the Matrix Bernstein?\\
% HVH: As dissuced we will go with the Matrix Bernstein with Intrinsic Dimension.}

% \todo[inline,color=cyan]{HVH: A suggestion of replacing the Sub-Gaussian assumption for sampling error using the Matrix Bernstein Bound.

% \begin{assumption}[Bounded Score]
    
% \end{assumption}

% \begin{theorem}[Sampling Error] Assume that assumptions~\ref{assum:bcrb_reg}, ~\ref{ass:score_cond_reg} hold. In addition assume that
%         $$\norm{\postscore{\p}{\x}\postscore{\p}{\x}^T -\lbfimb}_2\leq B \quad \forall \x,\p\in\ds$$
%         hold, then for any $u>0$ with probability $1-\exp{\brackets{-u}}$:
        
% \end{theorem}
% \begin{proof}
    
% \end{proof}

% }