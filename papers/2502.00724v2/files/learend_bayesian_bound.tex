\section{{Derivation of the \name{}}}\label{sec:lbcrb}
{We turn now to a precise problem statement and derivation of the two proposed methods for the \name{}.}
\begin{tcolorbox}
\begin{problem}\label{problem_one}
 %Assume 
 Suppose that $\probt{\x|\p}{\X|\pr}$ and $\probt{\p}{\pr}$ satisfy Assumptions \ref{assum:bcrb_reg} and %some 
 score matching Assumptions \ref{ass:score_cond_reg} or \ref{ass:score_reg} and \ref{ass:score_reg_prior}. 
 {Let $\bcrb$ be the}  %a
Bayesian Cram\'er-Rao lower bound (BCRB) \eqref{eq:bcrb} on the estimation error of parameter $\pr\in\Ps$ from a %the
measurement {$\xset = \set{{\X}_{i}}_{i=1}^{\niideval}$ containing $\niideval$ i.i.d. measurements ${\X}_i \sim  \probt{\X|\pr_n}{\X|\pr}$.}
{Assume that $\probt{\x|\p}{\X|\pr}$ and $\probt{\p}{\pr}$ are completely or partially unknown.}
Given a data set 
        %
        {\begin{equation}\label{eq:dataset}
          \ds=\set{\pr_n, \mathcal{\X}_n=\set{{\X}_{n,j}}_{j=1}^{\niiddata}   }_{n=1}^{\nds} 
        \end{equation}}
        % \todo[inline,color=green]{Same comments as before about $\ds$.}
\noindent 
of  {parameter-measurement} samples that are independent and identically-distributed (i.i.d) 
as    %
{${\X}_{n,j} \sim  \probt{\X|\pr_n}{\X|\pr}, \pr_n\sim \probt{\pr}{\pr}$, }
obtain {a learned approximation  \name{} $\hat{\bcrb}(\ds)$ to  $\bcrb$} satisfying:
\begin{equation}
     {\hat{\bcrb}}\brackets{\ds}
     \xrightarrow{\nds \rightarrow \infty}\bcrb\quad\text{%
     {a.s}}.
\end{equation}

\end{problem}
\end{tcolorbox}
% \todo[inline,color=green]{YB: Note the technical and language changes in the problem statement. Pls check that you agree.\\
% \textbf{HVH}: What do you think about emphasizing that the PDF are unknown ?, please suggest changes in the text. }
\begin{remark}
    The data set in \eqref{eq:dataset_rel} represents an instance {(realization)} of the data set defined in Problem~\ref{problem_one} (\eqref{eq:dataset}). For theoretical analysis concerning the stochastic nature of the \name{}, we employ the random data set specified in \eqref{eq:dataset}.
\end{remark}

To address Problem~\ref{problem_one}, we propose two approaches: 
(A) the Posterior Approach, which 
%utilized a 
uses conditional score matching to learn the posterior score;  %Second, 
and (B) the Measurement-Prior Approach, which learns two score functions -- one for the prior, and another for the measurement distribution.
 The Measurement-Prior Approach facilitates the incorporation of domain knowledge into the score neural network {improving the representation and learning of the true score}. 
Similar to previous works \cite{habi2023learned,habi2023learning, crafts2023bayesian}, both approaches comprise two stages - of learning, and evaluation. {However, as discussed in the Introduction, they differ from the previous works in most other key aspects.}   
%An overview of 
The two approaches %is shown 
are illustrated in Figure~\ref{fig:main}.

\subsection{Posterior Approach}\label{sec:post_learn}
In the Posterior Approach {( Fig.~\ref{fig:main_post}),} we use the BFIM in \eqref{eq:bcrb}, which only requires the posterior score $\nabla_{\p}\log\probt{\p|\xsetr}{\p|\xset}$, %which is 
a conditional score of $\p$ given a measurement $\X$. Then we use the learned conditional score to evaluate the \name{} by replacing the expectation with an empirical mean. 
\input{files/tikz/apporch_compare} 
\subsubsection{Score Learning}\label{sec:score_learning_post}
 To learn $\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}$, we suggest to use conditional score matching \cite{hyvarinen2005estimation,song2019generative,song2020score}. The goal is to {to fit} a model $\postscore{\p}{\xsetr; \Omega}$ parameterized by $\Omega$ (usually implemented as a neural network) to %obtain 
 the true score function $\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}$ {by minimizing w.r.t. $\Omega$ the discrepancy between the two expressed by} the objective
\begin{equation}
\label{eq:score_matching_objective}
    \lossbs\brackets{\Omega}=%\expectation{
    \mathbb{E}_{\pr, \xset} \norm{\postscore{\pr}{\xset;\Omega}-\nabla_{\pr}\log\probt{\pr|\xset}{\pr|\xset}}_2^2
    %}{\pr,\xset}
\end{equation}
Since we do not have direct access to $\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}$, only a set of i.i.d. samples $\ds$, we cannot directly minimize the objective %in
\eqref{eq:score_matching_objective}. 
%In a 
Similar %way as 
to standard score matching, an %alternative 
equivalent objective function is used {that does not require direct access to $\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}$:
\begin{align}\label{eq:conditional_score_matching}
&\lossbst\brackets{\Omega}=\expectation{\ell_B\brackets{\X,\pr;\Omega}}{\X,\pr}=\\
&\expectation{\norm{\postscore{\pr}{\xset;\Omega}}_2^2}{\pr,\xset}+2\trace{\expectation{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}{\xset,\pr}}.\nonumber
\end{align}

Objective \eqref{eq:conditional_score_matching} is equivalent to \eqref{eq:score_matching_objective} for the purpose of finding the minimizer $\Omega^*$,
in the sense that the two only differ by a constant $C$ that is independent of $\Omega$, i.e., $\lossbs\brackets{\Omega}=\lossbst\brackets{\Omega}+C$, provided that the following conditions hold.
%utilized that requires the  following 
First,} the boundary condition
\begin{equation}\label{eq:boundary_conditions_post}
    \lim\limits_{\p\rightarrow \Psb}\postscore{\p}{\xsetr}\probt{\p|\xsetr}{\pr|\xset}=0,\quad\forall\xsetr,
\end{equation}
% \end{assumption}
{%If the boundary conditions in~\eqref{eq:boundary_conditions_post} hold along some 
Second, the following} regularity conditions. 
\begin{assumption}\label{ass:score_cond_reg}$\,$

\begin{enumerate}[label={\ref*{ass:score_cond_reg}}.\arabic*,labelsep=*, leftmargin=*]
    % \setcounter{enumi}{9}
\item \label{assum:diff_prob_post} The {log-posterior} $\log\probt{\p|\xsetr}{\pr|\xset}$ is differentiable w.r.t. $\p$ {at all $\xsetr$ and $\p\in\Ps$  where $\probt{\p,\xsetr}{\pr,\xset} > 0$}. 
% \todo[inline,color=green]{Shouldn't the condition be
% $\probt{\p,\xsetr}{\pr,\xset} > 0$ (joint PDF, rather conditional), because the expectations are wrt the joint PDF - so this is where we need the gradient to exist?\\
% HVH: Right and done.}

\item The expectation $\expectation{\norm{\nabla\log\probt{\pr|\xset}{\pr|\xset}}_2^2}{\pr,\xset}$ is finite. \label{assum:bound_expection_post}
\item The score neural network $\postscore{\p}{\xsetr}$ is differentiable w.r.t. $\p$. \label{assum:net_cond_score}
%\todo[inline,color=green]{What score network?}
\item The expectation $\expectation{\norm{\postscore{\pr}{\xset}}_2^2}{\pr,\xset}$  is finite. \label{assum:expected_cond_score}

\end{enumerate}
\end{assumption}
% \ybdelete{Then an alternative loss function can be derived that does not require direct access to $\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}${, by using the following relation:}
% % \begin{equation}
%     $\lossbs\brackets{\Omega}=\lossbst\brackets{\Omega}+C,$
% % \end{equation}
% where $C$ is a constant independent of $\Omega$,
% \begin{align}\label{eq:conditional_score_matching}
% &\lossbst\brackets{\Omega}=\expectation{\ell_B\brackets{\X,\pr;\Omega}}{\X,\pr}=\\
% &\expectation{\norm{\postscore{\pr}{\xset;\Omega}}_2^2}{\pr,\xset}+2\trace{\expectation{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}{\xset,\pr}},\nonumber
% \end{align}
% is the score matching alternative objective} \hvhdelete{and $\matsym{J}_{B}\brackets{\Omega}\triangleq\expectation{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}{\xset,\pr}$ is the expected Jacobian matrix of $\postscore{\p}{\xset;\Omega}$ w.r.t. $\pr$.}
\begin{remark}\label{remark:condtion_hold_post}
    %
    {Assumptions~\ref{assum:diff_prob_post} and \ref{assum:bound_expection_post} 
%is 
are implied by the regularity Assumptions~\ref{sas:derivative} and \ref{assume:non_singular} of the BCRB, %Assumptions~\ref{sas:derivative} and~\ref{assume:non_singular}, 
respectively.}
{On the other hand,}
Assumptions~\ref{assum:net_cond_score}, \ref{assum:expected_cond_score} and the boundary condition \eqref{eq:boundary_conditions_post} can be inherently satisfied by selecting an appropriate neural network architecture and non-linear activation function.
\end{remark}
The objective \eqref{eq:conditional_score_matching} enables 
{ to determine the optimum parameters $\Omega^*$ by minimizing a sample average version \eqref{eq:score_post_mean} using} 
a dataset $\ds$ of i.i.d. measurements.
{ 
This yields} an approximation of the Bayesian score $\postscores{\p}{\xsetr} \triangleq \postscore{\p}{\xsetr;\Omega^*}\approx\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}$. 
\subsubsection{Evaluation of the LBCRB}\label{sec:lbcrb_eval_post}
We replace the Bayesian score in \eqref{eq:bfim} with the learned one.%This results learned Bayesian Fisher Information Matrix
\footnote{%
{ By Assumption~\ref{ass:score_expection} we have $\expectation{\nabla_{\pr}\log\probt{{\X},\pr}{{\X},\pr}}{\X,\pr}=0$.  This suggests that the mean of the learned score function can be subtracted to eliminate any bias introduced by it. However, in our numerical experiments this mean subtraction had a negligible effect 
%on the results 
for moderate-size training sets (e.g., $\nds=600$.), and was not used. %
{%However, 
It may prove useful though} for situations with small $\nds$.     }} 
This results  in the learned Bayesian Fisher Information Matrix% which is given by:
\begin{equation}\label{eq:f_score}
\lbfimb\triangleq\expectation{\postscores{\pr}{\xset}\postscores{\pr}{\xset}^T}{\xset,\pr}.
\end{equation}
Since we cannot evaluate the expectation {in \eqref{eq:f_score}}, we replace the expectation with an empirical mean over the entire dataset $\mathcal{D}$ as shown in \eqref{eq:mean_fully}, %that results in 
producing $\lbfimbs$.
Finally, to obtain the LBCRB, we invert $\lbfimbs$, which results in ${\bcrb}\approx\lbcrbbs\triangleq\lbfimbs^{-1}$.

{The Posterior Approach addresses Problem~\ref{problem_one} in a simple way, requiring the training of only one model network for the posterior conditional score. However, this simplicity comes at the cost of two main drawbacks.} 
{First, because
%Given that
}
the \emph{Posterior} Approach involves directly learning of $\nabla_{\p}\log\probt{\p|\xsetr}{\pr|\xset}$, it focuses only on the score of $\xsetr$ without considering that $\xsetr$ may consist of a set of i.i.d. measurements.  This %oversight necessitates 
requires learning  {a score model with a  conditioning input $\xsetr$ of high dimension $\niiddata$, increasing the complexity of the neural network and the sample complexity for training it}. {Second}, if one has {some domain}  knowledge about the problem, it is unclear how to incorporate it into the score function model. In Section~\ref{sec:lik_learn}, we propose the Measurement-Prior Approach, which
{ overcomes both of these limitations, while providing additional advantages.}
\subsection{Measurement-Prior Approach}\label{sec:lik_learn}
% \ybdelete{Now, we present the \emph{Measurement-Prior} Approach which allows incorporating additional domain knowledge.}

% \hvhdelete{
% Here we represent the Bayesian score as a decomposition of the prior score $\nabla_{\p}\log\probt{\p}{\pr}$ and the fisher score $\nabla_{\p}\log\probt{\x|\p}{\x|\pr}$ which is given by:
% \begin{align}\label{eq:lik_prior_decompistion}
%     \nabla_{\p}\log\probt{\hvhreplace{\x}{\mathcal{\X}},\p}{\X,\pr}&=\nabla_{\p}\log\probt{\x|\p}{\X|\pr}\nonumber\\
%     &+\nabla_{\p}\log\probt{\p}{\pr}.
% \end{align}
% }

%
{%Here, 
In this approach, illustrated
%presented 
in Fig.~\ref{fig:main_lik_prior},
}
we employ the decomposition of the BFIM 
%as given 
in \eqref{eq:bfim_decomposition_base} %, dividing it 
into two components: the prior Fisher Information Matrix (FIM), which requires only the score $\nabla_{\p}\log\probt{\p}{\pr}$, and the measurement FIM, which relies on the Fisher score $\nabla_{\p}\log\probt{{\x}|\p}{{\X}|\pr}$.
To learn of the prior score we use the standard score matching presented in Section~\ref{sec:background}, whereas for learning the measurement score, we derive a new Fisher Score Matching (FSM) objective.
% \ybdelete{The entire score learning process is described in Section~\ref{sec:score_learning_lik}.}

{%By dividing 
The separation of the Bayesian score into two learned components %and allowing their learning,
provides several important} 
%we outline and highlight the 
advantages, which we highlight here, {and discuss in detail later in this section.

}
%The first advantage is that 
First, it enables to determine
%the establishment of 
bounds 
{for a measurement with} 
an arbitrary number of 
independently and identically distributed 
i.i.d samples without requiring additional data or training.

{Second, it enables to learn and use a model for the Fisher score for only a \emph{single measurement sample}, reducing the complexity of the model and facilitating its training. 
%The second advantage is the ability 
Third, it enables to}
to incorporate domain knowledge about the estimation problem %within 
into the score neural network{, further reducing its complexity and facilitating its training.
%, which is discussed inSection~\ref{sec:model_informed}. 
}


\subsubsection{Score learning}\label{sec:score_learning_lik}
We  learn two distinct score functions, parametrized by $\paramf$ and $\paramp$, respectively: the Fisher score function $\lscore{\x}{\p; \paramf}$ for a single measurement sample;  and the prior score function $\priorscore{\p;\paramp}$.
%each associated with their respective set of parameters $\paramf$ and $\paramp$.
% \hvhdelete{Substituting them into \eqref{eq:lik_prior_decompistion} yields the following:
% % \begin{equation}
% $\postscore{\p}{\hvhreplace{\x}{\mathcal{X}}}={\sum_{\tilde{\x}\in\mathcal{X}}}\lscore{\hvhreplace{\x}{\tilde{\x}}}{\p}+\priorscore{\p},$
% % \end{equation}
% where now $\Omega=\{\paramf,\paramp\}$ is the combine set of learnable weights.} 
% \ybdelete{To learn an informative fisher and prior score function, we divided the learning into two parts.}  
We begin by learning the prior score $\priorscore{\p {;\paramp}}\approx\nabla_{\p}\log\probt{\p}{\pr}$ using the conventional score matching method detailed in Section~\ref{sec:score_over_view}. To apply the objective %function 
in \eqref{eq:loss_prior_sm}, 
% {note that Assumption~\ref{ass:score_reg_prior} is satisfied by the regularity Assumption~\ref{sas:derivative_prior} of the BCRB.} 
we assume that 
the boundary condition %described in
\eqref{eq:boundary_conditions} 
and Assumptions~\ref{ass:score_reg_prior} hold  
for %similar 
reasons analogous to {those given in Remark~\ref{remark:condtion_hold_post}} for the {posterior} score matching. %assumption mentioned 
% \ybdelete{in Remark\hvhreplace{~\ref{remark:fisher_score}}{~\ref{remark:condtion_hold_post}} .}
% \todo[inline,color=green]{YB: 
% 1. The forward pointer to Remark~\ref{remark:fisher_score}, which in turn points back to Remark~\ref{remark:condtion_hold_post} is confusing. I think that it will be better to justify the assumptions here explicitly, perhaps pointing back to Remark~\ref{remark:condtion_hold_post}.
% \\
% 2. Remark~\ref{remark:fisher_score} is currently incorrect, as commented there. Also,  differentiability of $\log\probt{\p}{\pr}$ does not follow from \ref{sas:derivative}, as commented after \eqref{eq:PriorFim}. We need to add an extra assumption there.\\
% \textbf{HVH:} 1. I think it is more correcrt just point to remark 5.\\
% 2.  Once we clear the base assumption of the BCRB I will correct remake 6 -> Cleared.

% }

Next we address the learning of the Fisher score function. We wish to minimize the mismatch between the true and model scores in the objective
% \hvhreplace{
% \begin{align}
%         &\lossfs\brackets{\paramf}\nonumber\\
%         &=\expectation{\norm{\lscore{\X}{\p;\paramf}-\nabla_{\pr}\log\probt{\X|\pr}{\X|\pr}}_2^2}{\X,\pr}.
% \end{align}}
{
\begin{align}
        &\lossfs\brackets{\paramf}\nonumber\\
        &=\expectation{\norm{\lscore{{\X}}{\p;\paramf}-\nabla_{\pr}\log\probt{{\X}|\pr}{{\X}|\pr}}_2^2}{{\X},\pr}.
\end{align}
}
However, in this case, we cannot use the standard {(conditional)} score matching {technique, because unlike the latter, where the gradient is with respect to the conditioned variable, the gradient in }
%on %\hvhreplace{$\nabla_{\p}\log\probt{\x|\p}{\X|\pr}$}
{$\nabla_{\p}\log\probt{{\x}|\p}{{\X}|\pr}$} {is with respect to the conditioning variable $\p$.} 
To {handle this fundamentally different scenario}, we 
{derive a new Fisher score matching objective. It requires the boundary condition \begin{equation}\label{eq:boundary_condtions_gen_direct}
    \lim\limits_{\p\rightarrow\Psb}  \lscore{{\x}}{\p}\probt{{\x},\p}{{\X},\p} =0,\quad \forall {\x}\in\widetilde{\Upsilon}
\end{equation} and the following regularity conditions.
}
\begin{assumption}[Fisher Score Matching Regularity]\label{ass:score_reg}$\,$
    \begin{enumerate}[label=\text{\ref{ass:score_reg}}.\arabic*,labelsep=*, leftmargin=*]
    \item The %\ybreplace{p.d.f.}
    {measurement {log-likelihood}} %\hvhreplace{$\probt{\x|\p}{\X|\pr}$}
    {$\log\probt{{\x}|\p}{{\X}|\pr}$} is differentiable w.r.t. $\p$ {at all $\x\in\Upsilon$ and $\p\in\Ps$  where $\probt{{\x},\p}{{\X},\pr} > 0$} \label{assum:diff_prob}. 

    \item\label{ass:bound_expection} The expectation %\hvhreplace{$\expectation{\norm{\nabla_{\pr}\log\probt{\x|\pr}{\X|\pr}}_2^2}{\X,\pr}$}
    {$\expectation{\norm{\nabla_{\pr}\log\probt{{\X}|\pr}{{\X}|\pr}}_2^2}{{\X},\pr}$} is finite. 
    \item The score neural network %\hvhreplace{$\lscore{\x}{\p}$}
    {$\lscore{{\x}}{\p}$} is differentiable w.r.t. $\p$. \label{assum:diff_net} 
    \item\label{ass:bound_expection_v2} The expectation   %\hvhreplace{$\expectation{\norm{\lscore{\X}{\pr}}_2^2}{\X,\pr}$}
    {$\expectation{\norm{\lscore{{\X}}{\pr}}_2^2}{{\X},\pr}$} is finite. 
\end{enumerate}
\end{assumption}

\begin{remark}\label{remark:fisher_score}
Assumptions  \ref{assum:diff_net}, \ref{ass:bound_expection_v2}, and the boundary conditions  \eqref{eq:boundary_condtions_gen_direct} hold by the arguments %as present 
in Remark~\ref{remark:condtion_hold_post}.  {As for Assumption~\ref{assum:diff_prob} it implied by Assumptions~\ref{sas:derivative} and \ref{sas:derivative_prior}.}

Similarly, by \eqref{eq:bfim_decomposition_base} and \eqref{eq:non-BayesFim}, 
Assumption~\ref{ass:bound_expection} is implied by the non-degeneracy BCRB regularity Assumption~\ref{assume:non_singular}.

\end{remark}
\begin{theorem}[Fisher Score Matching]\label{thm:liklihood} 
{Suppose that the boundary condition \eqref{eq:boundary_condtions_gen_direct} and the
%Assume that some 
}
regularity conditions Assumptions~\ref{ass:score_reg}) hold.
% \ybdelete{Also, assume that the following boundary condition holds:
% % \hvhreplace{
% % \begin{equation}\label{eq:boundary_condtions_gen_direct}
% %     \lim\limits_{\p\rightarrow\Psb}  \lscore{\x}{\p}\probt{\vectorsym{x},\p}{\X,\p} =0,\quad \forall \x\in\Upsilon
% % \end{equation}}
% {\begin{equation}\label{eq:boundary_condtions_gen_direct}
%     \lim\limits_{\p\rightarrow\Psb}  \lscore{{\x}}{\p}\probt{{\x},\p}{{\X},\p} =0,\quad \forall {\x}\in\widetilde{\Upsilon}
% \end{equation}}
% Then $\lossfs=\lossfst+C$, where $C$ is a constant independent of $\paramf$,
% }
% \hvhreplace{\begin{align}\label{eq:score_match_param_gen_direct}
%     \lossfst\brackets{\paramf}&=\expectation{\norm{\lscore{\X}{\pr;\paramf}}_2^2}{\randomvec{X},\pr}+2\trace{\matsym{J}_{F}\brackets{\paramf}}\nonumber \\ &+2\expectation{\lscore{\X}{\pr;\paramf}^T\nabla_{\p}\log\probt{\pr}{\pr}}{\randomvec{X},\pr},
% \end{align}}
{Define}\begin{align}\label{eq:score_match_param_gen_direct}
    &\lossfst\brackets{\paramf}=2\expectation{\lscore{{\X}}{\pr;\paramf}^T\nabla_{\p}\log\probt{\pr}{\pr}}{{\X},\pr} +\\
    &2\trace{\expectation{\frac{\partial \lscore{\x}{\pr;\paramf}}{\partial\pr}}{\X,\pr}}+\expectation{\norm{\lscore{{\X}}{\pr;\paramf}}_2^2}{{\X},\pr}.\nonumber 
\end{align}
{Then $\lossfs=\lossfst+C$, where $C$ is a constant independent of $\paramf$.}
% \ybdelete{
% is the alternative objective function }\hvhdelete{and  \hvhreplace{$\matsym{J}_{F}\brackets{\paramf}\triangleq\expectation{\frac{\partial \lscore{\x}{\pr;\paramf}}{\partial\pr}}{\X,\pr}$}{$\matsym{J}_{F}\brackets{\paramf}\triangleq\expectation{\frac{\partial \lscore{{\X}}{\pr;\paramf}}{\partial\pr}}{{\X},\pr}$} is the expected Jacobian matrix of \hvhreplace{$\lscore{\x}{\p;\paramf}$}{$\lscore{{\x}}{\p;\paramf}$} w.r.t. $\p$}.
\end{theorem}
%The proof of 
Theorem~\ref{thm:liklihood} is proved %shown 
in Appendix~\ref{sec:lik_score_proof}. {Comparing the new Fisher Score Matching objective \eqref{eq:score_match_param_gen_direct}
with the standard conditional score matching objective (e.g., \eqref{eq:conditional_score_matching}), we notice that the difference is the added new first term in \eqref{eq:score_match_param_gen_direct}. This term involves the expectation of the inner product of the model conditional score of the conditioned variable $\X$ (i.e., the Fisher score $\lscore{{\X}}{\pr}$) with the true score of the conditioning variable $\pr$ (i.e, the true score of the prior, $\nabla_{\p}\log\probt{\p}{\pr}$).}
%{
% \ybdelete{\begin{remark}\label{remark:fisher_score}
% %Regarding the satisfaction of the assumptions outlined in Assumption~\ref{ass:score_reg}: 
% %(i) ~
% \hvhdelete{
% \ref{assum:diff_prob} 
% is implied by Assumption~\ref{sas:derivative} 
% -- one of the BCRB's regularity conditions; 
% %(ii) 
% Assumptions ~\ref{assum:diff_net} and ~\ref{ass:bound_expection_v2} can be inherently satisfied by selecting an appropriate neural network architecture and non-linear activation function;
% } %(iii) 
% %{
% Assumptions \ref{assum:diff_prob}, \ref{assum:diff_net}, \ref{ass:bound_expection_v2}, and the boundary conditions  \eqref{eq:boundary_condtions_gen_direct} hold by the arguments %as present 
% in Remark ~\ref{remark:condtion_hold_post}.  
% \todoin{YB: A noted in the comment after \eqref{eq:PriorFim}, it is not true that Assumption~\ref{assum:diff_prob} follows from Assumption~\ref{sas:derivative}. It will, if we assume differentiability of the log prior. }
% %
% {%While 
% Similarly, by \eqref{eq:bfim_decomposition_base} and \eqref{eq:non-BayesFim}, 
% Assumption ~\ref{ass:bound_expection} is implied by the non-degeneracy BCRB regularity Assumption~\ref{assume:non_singular}.
% }
% \end{remark}
% }

% \hvhdelete{To apply the objective function in \eqref{eq:score_match_param_gen_direct}, we assume that the boundary condition described in \eqref{eq:boundary_condtions_gen_direct} is satisfied. To ensure this, we adopt the assumption~\ref{assume:boundary_prior_lim} and ~\ref{assume:boundary_post_lim} together with the assumption that $\norm{\lscore{\x}{\p}}_2\leq \infty\quad \forall\p\in\Ps\quad\mathrm{and}\quad \x\in\Upsilon$.}

To employ Theorem~\ref{thm:liklihood}, %it is necessary to know 
requires the true prior score function. %However, 
Instead, we substitute the prior score function with the learned version $\priorscore{\p}\approx \nabla_{\p}\log\probt{\p}{\pr}$. This allows us to leverage a dataset of i.i.d. measurements and parameter pairs to learn the Fisher score %through 
using the objective %shown in 
\eqref{eq:score_lik_mean}.
\subsubsection{\pe{} Score Neural Network}\label{sec:model_informed}
{The separation of the modeling and learning of the prior score from that of the Fisher (measurement) score in the Measurement-Prior Approach opens a new opportunity to} integrate domain-knowledge (e.g., knowledge of the physics of the measurement process) into the {Fisher score model. This reduces the NN model complexity, and improves sample complexity.}  




{Specifically, } let us assume that we know some deterministic physical model $\mathcal{M}\brackets{\p}$ that relates the parameter vector $\p$ to the measurement $\X$ %using %\hvhreplace{second order statistics}
{in the following manner}:
\begin{equation}\label{eq:physical_model_score}
    \probt{{\x}|\p}{{\X}|\pr}=\probt{{\x}|\mathcal{M}\brackets{\p}}{{\X}|\vectorsym{\tau}},
\end{equation}
where $\probt{{\x}|\vectorsym{\tau}}{{\X}|\vectorsym{\tau}}$ is an unknown PDF %distribution 
parameterized by the known physical model $\mathcal{M}$. %
{%Note that 
Such a representation can fit several signal processing problems as demonstrated in {Sec.~\ref{subsubsec:MoISNN} and further} in Sec.~\ref{sec:example_models}. }
The score function of the model presented in \eqref{eq:physical_model_score} is given by:
% \begin{equation*}
    $\nabla_{\p}\log\probt{{\x}|\pr}{{\X}|\p}=\divc{\mathcal{M}\brackets{\p}}{\p}^T\at{\nabla_{\vectorsym{\tau}}\log\probt{{\x}|\vectorsym{\tau}}{{\X}|\vectorsym{\tau}}}{\vectorsym{\tau}=\mathcal{M}\brackets{\p}}.$
% \end{equation*}
Since $\probt{{\x}|\vectorsym{\tau}}{{\X}|\vectorsym{\tau}}$ is unknown we replace it %via 
by a neural network model $\iscore{\x}{\vectorsym{\tau}}$.  {The combined model for the Fisher score then becomes} \eqref{eq:model_base_score}.
% \hvhdelete{
% \begin{equation}\label{eq:model_base_score_without_iid}
% \vectorsym{s}_{F}\brackets{\tilde{\x}|\p;\paramf}=\divc{\mathcal{M}\brackets{\p}}{\p}^T\at{\iscore{\tilde{x}}{\vectorsym{\tau};\paramf}}{\vectorsym{\tau}=\mathcal{M}\brackets{\p}}.
% \end{equation}
% Finally, combining \eqref{eq:model_base_score_without_iid} and \eqref{eq:iid_score} we obtain \eqref{eq:model_base_score}.}
%An overview of 
The \pe{} Score 
Neural Network is illustrated %shown 
in Figure~\ref{fig:model_inforamed}.
%{
\begin{remark}\label{remark:opt_mi}
    To optimize the parameters %
    {$\paramf$ defining} $\vectorsym{s}_{I}$, we substitute \eqref{eq:model_base_score} into \eqref{eq:score_lik_mean}, implying that the optimization process relies on a dataset consisting of %\ybreplace{measurement ($\x$)-parameter ($\p$) pairs rather than $\vectorsym{\tau}$}
    {measurement-parameter ($\x,\p$) rather than $(\x,\vectorsym{\tau})$ pairs}. 
    %It is important to note that
    Importantly, $\vectorsym{\tau}=\mathcal{M}\brackets{\p}$ is calculated during the optimization for every given value of $\p$.
\end{remark}
%}
\subsubsection{Evaluation of the LBCRB}\label{sec:lbcrb_eval_lik}
%Now we will present the use of 
The Prior Score and Fisher Score {models} learned in Section~\ref{sec:lbcrb_eval_lik} 
%by showing 
are used in the evaluation phase of the \emph{Measurement-Prior} Approach. %
{Specifically, let $\lscores{\x}{\p}=\lscore{\x}{\p;\paramf^*}\approx\nabla_{\p}\log\probt{\x|\p}{\X|\pr}$ and $\priorscores{\x}=\priorscore{\p;\paramp^*}\approx\nabla_{\p}\log\probt{\p}{\pr}$ be the learned Fisher and prior scores, where $\paramf^*$ and $\paramp$ denote the minimizer's of \eqref{eq:score_lik_mean} and \eqref{eq:score_prior_mean}, respectively.  }
We use the learned Prior and Fisher scores to {express} their FIM's 
\begin{subequations}\label{eq:split_fim_exp}
\begin{equation}
    \tlmfim\triangleq\expectation{\lscores{{\X}}{\pr}\lscores{{\X}}{\pr}^T}{{\X},\pr},
\end{equation}
\begin{equation}
\tlpfim\triangleq\expectation{\priorscores{\pr}\priorscores{\pr}^T}{\pr}.
\end{equation}
\end{subequations}
%Then we utilized 
Substituting into the decomposition of the Bayesian FIM in \eqref{eq:bfim_decomposition_base} 
%to obtain:
yields {an approximation for the Bayesian FIM for a measurement contatining $\niideval$ i.i.d samples,}
\begin{equation}\label{eq:f_score_decomposition}
    \fb\approx\lbfimlp\brackets{\niideval}\triangleq\niideval\cdot\tlmfim+\tlpfim.
\end{equation}
To compute \eqref{eq:split_fim_exp}, we replace the expectation with an empirical mean over the training dataset  $\ds$, %which results 
in \eqref{eq:mean_efim_likd} and \eqref{eq:bprior_mean}. 
{Combining the two learned FIMs as in \eqref{eq:f_score_decomposition} yields
the final learned Bayesian FIM (LBFIM) \eqref{eq:bfim_apx_final} of the Measurement-Prior Approach,
with the corresponding LBCRB $\lbcrblps(\niideval)=\lbfimlps^{-1}(\niideval).$}
 
We would like to emphasize that \eqref{eq:f_score_decomposition} and \eqref{eq:bfim_apx_final} enable us to compute a {learned} BCRB for a measurement containing any desired number $\niideval$ of i.i.d. samples, {regardless of the number $\niiddata$ of i.i.d measurement samples available in the training data set $\ds$ for each value of $\p$.} 

{The explicit use of the statistical independence of the $\niideval$ measurement samples in the Measurement-Prior Approach provides two additional advantages:} (1) 
%we can generalize 
the bound generalizes to any number $\niideval$ of i.i.d. samples {in the measurement,} without  training a new model; and (2) {
the model $\lscore{\x}{\p;\paramf}$ for the score function for a single measurement sample is far simpler than that for multiple samples, making training easier and reducing the number of training samples required.}
 
% \ybdelete{To compute \eqref{eq:split_fim_exp}, we replace the expectation with an empirical mean over the training dataset  $\ds$, which results in \eqref{eq:mean_efim_likd} and \eqref{eq:bprior_mean}. To provide a bound for an arbitrary number of i.i.d. samples $\niideval$, we utilized the FIM decomposition in \eqref{eq:bfim_decomposition_base} and computed the LBFIM (denoted by $\lbfimlps$) as shown in \eqref{eq:bfim_apx_final}. Finally, to obtain the LBCRB we invert \eqref{eq:bfim_apx_final} which results in $\lbcrblps=\lbfimlps^{-1}.$ .}

% \todo[inline,color=green]{Broken sentence and missing equation?}