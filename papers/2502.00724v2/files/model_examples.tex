\section{MEASUREMENTS MODEL EXAMPLES}\label{sec:example_models}
We present three measurement models to illustrate the benefits of the \name.

\subsection{Linear Observation Model}\label{sec:linear_model} 
Let $\pr\in\mathbb{R}^{\np}$ be the parameter {we wish to estimate} and $\matsym{A}\in\mathbb{R}^{{\nx}\times \np}$  a fixed mixing matrix, %then 
and define the  ${n}^{th}$ snapshot measurement ${\X}_{n}\in\mathbb{R}^{{\nx}}$ %is defined as follows:
\begin{equation}\label{eq:linear-model}
    {\randomvec{X}}_{{n}}=\matsym{A}\pr+\randomvec{W}_{n},
\end{equation}
% \todo[inline,color=green]{Remove the tildes above - they are no longer relevant?}
where $\randomvec{W}_{{n}}\sim\normaldis{0}{\matsym{\Sigma}}$ is  additive Gaussian noise with zero mean and covariance matrix $\matsym{\Sigma}$. 
We are given a measurement set $\xset=\set{{\X}_{n}}_{{n}=1}^{\niideval}$,
that consists of $\niideval$ snapshots of the measurement in \eqref{eq:linear-model}.  %{
For the prior of $\p$, we assume a Gaussian distribution,  $\pr\sim\normaldis{0}{\matsym{I}\sigma_P^2}.$  The linear model in \eqref{eq:linear-model}  with the Gaussian prior enables a detailed {study} of the suggested methods and also ensures that Assumption ~\ref{assume:realizable} %
{is satisfied by simple neural network realizations of the score functions}. 

The BCRB for %
the estimation of $\p$ from the measurement {set $\xset$} in this example is given by:
\begin{align}\label{eq:bcrb_linear}
\bcrb=\brackets{{\niideval} \cdot\matsym{A}^T\Sigma^{-1}\matsym{A}+\frac{1}{\sigma_P^2}}^{-1},
\end{align}
and %
{(as expected for the linear Gaussian observation model)} coincides with the covariance of LMMSE. In addition, the {\emph{true}} score vectors of the measurement model  \eqref{eq:linear-model} are given by:
\begin{equation}\label{eq:score_opt_linear}
\vectorsym{s}^{0}_F\brackets{{\x}|\p}=  \matsym{A}^T\Sigma^{-1}\brackets{{\x}-\matsym{A}\p}\textbf{ and } \vectorsym{s}^{0}_P\brackets{\p}=-\frac{\p}{\sigma_p^2}.
\end{equation}
% \todo[inline,color=green]{Say something about $\vectorsym{s}^{0}_B \brackets{{\p}|\x}$, which I assume you use in computing the \name{} using the Posterior Approach? \\
% HVH: Yes and Done.}
{The true posterior score is given by $\vectorsym{s}^{0}_B\brackets{\p|\xsetr}=\sum_{\x\in\xsetr}\vectorsym{s}^{0}_F\brackets{{\x}|\p}+\vectorsym{s}^{0}_P$, where $\xsetr$ represents the set of $\niideval$ i.i.d. measurements. }

{The goal is to compute the LBCRB (a close approximation to the BCRB), when the matrices $\matsym{A}$ and $\matsym{\Sigma}$ are \emph{unknown}, by using a training data set 
$\ds=\set{\xset_n=\set{{\x}_m}_{m=1}^{\niiddata},\p_n}_{n=1}^{\nds}$. 
 It is important to observe that the number $\niiddata$ of i.i.d. samples present in the dataset for each value of $\p$ may differ from the desired number $\niideval$ for obtaining the LBCRB,
 i.e., $\niideval \neq \niiddata$.



\subsection{Frequency Estimation }\label{sec:freq_est}
A simple frequency estimation problem  illustrates the benefits of the \name{}. 
The observation model is  
\begin{equation}\label{eq:freq_est_model}
    \squareb{\X}_n=\cos{\brackets{\pr n}}+\squareb{\randomvec{W}}_n \quad\forall n \in [0,N-1],
\end{equation}
% \todo[inline,color=green]{Why not simply $\cos (\theta n)$?}
where $\p$ is the digital  frequency  parameter to be estimated, and $\randomvec{W}$ is an additive noise. 
%In this example, 
We consider two types of noise: 1)  Gaussian noise $\randomvec{W}\sim \mathcal{N}\brackets{0,\Sigma}$ to validate the \name{} using closed-form results from \cite{van2007bayesian} (Example 3); and 2) a real-world underwater ambient noise taken from \cite{msg0-ag12-22} to simulate frequency estimation with  non-Gaussian noise. 
We consider a four-parameter Beta prior  $\p\sim\mathrm{Beta}\brackets{\alpha_{\p},\alpha_{\p},0,\pi}$. 
% \todo[color=green]{Only one?}
Recall that a random variable $\randomvec{z}\sim\mathrm{Beta}\brackets{\alpha,\beta,l,u}$ has PDF
\begin{equation}
    \probt{z}{\randomvec{z}}\triangleq \frac{\brackets{z-l}^{\alpha-1}\brackets{u-z}^{\beta-1}}{\brackets{u-l}^{\alpha+\beta+1}B\brackets{\alpha,\beta}},
\end{equation}
where  $l<u$ are the lower and upper support parameters, respectively, $\alpha, \beta >0$ are the shape parameters, and $B\brackets{\alpha,\beta}=\frac{\Gamma\brackets{\alpha}\Gamma\brackets{\beta}}{\Gamma\brackets{\beta+\alpha}}$ is the normalization constant, where $\Gamma\brackets{\alpha}$ is the Gamma function. 


In the case of Gaussian noise, the observation model in \eqref{eq:freq_est_model} has the following true score functions:
\begin{subequations}
\begin{equation}\label{eq:score_opt_freq_lik}
\vectorsym{s}^{0}_F\brackets{\x|\p}=\matsym{\frac{\partial \mathcal{M}\brackets{\p} }{\partial \p}}^{T}\Sigma^{-1}\brackets{\x-\mathcal{M}\brackets{\p}},
\end{equation}
\begin{equation}
\vectorsym{s}^{0}_{P}\brackets{\p}=\frac{\alpha_{{\p}}-1}{{\p}}-\frac{\alpha_{{\p}}-1}{\pi-{\p}}
\end{equation}
\end{subequations}
where $\squareb{\mathcal{M}\brackets{\p}}_n=\cos{\brackets{\p n}}$. 
%is sinusoidal function of the parameter vector $\p$.
Here, we show the use of a \pe{} score neural network and its effect on the structure of the neural network. 
%In case of frequency estimation, 
We assume that $\squareb{\mathcal{M}\brackets{\p}}_n=\cos{\brackets{\p n}}$ 
%
{has the known form of a sinusoidal function of the unknown parameter $\p$,}
% \todo[inline,color=green]{So what is there to estimate?\\
% HVH: The noise distribution or more specifically the score of $\randomvec{W}$. } 
whereas the distribution of $\matsym{W}$ is unknown. 

Applying $\squareb{\mathcal{M}\brackets{\p}}_n=\cos{\brackets{\p n}}$ to \eqref{eq:model_base_score} 
% with $\squareb{\mathcal{M}\brackets{\p}}_n=\cos{\brackets{\p n}}$ 
and comparing with \eqref{eq:score_opt_freq_lik}, we find that the  
 $\vectorsym{s}_I$ %
 {that provides the true score}
% \todo[inline,color=green]{Replace the term "optimal score" by "true score throughout, and also use the notation with supersrict $0$ rather than $*$ - check throughout.\\
% HVH:Done} 
is given by $\Sigma^{-1}\brackets{\x-\vectorsym{\tau}}$. %
{In this example, the score neural network only 
%require to obtain 
needs to learn (using $\ds$ and score matching) the score function of the noise distribution, 
%using score matching and $\ds$, 
since the modeling of the dependence of the measurement on the frequency parameter is known. }
{Furthermore, $\vectorsym{s}_I$ } can be represented by a single layer of a fully connected neural network that needs to learn only the inverse of the covariance matrix. %
{%It is important to highlight that, 
Importantly, as explained in Remark~\ref{remark:opt_mi}, $\vectorsym{s}_{I}$ is only indirectly {optimized} through the application of $\mathcal{M}$ using \eqref{eq:model_base_score}.%, rather than being optimized directly. 
}
% \todo[inline,color=green]{1. See comments on linear model.\\
% 2. In this case the derivative  $\partial^T M(\p)/\partial \p$ depends on $\p$, which is the parameter to be estimated, and therefore assumed unknown. Also, it is random with some distribution - so what value of $\p$ should be used to define $S_F$ for the actual computation of the LBCRB? Is this value needed for the training of $S_I$? Elaborate (also in the introduction of this approach in Section IV-B) to avoid confusion.\\
% HVH: 
% 1.  Done.
% 2. Add remark ~\ref{remark:opt_mi} and refernced here.
% } 

In contrast, without leveraging the model information, we %would 
have to learn the cosine function for the score -- a rather complicated nonlinear function. This 
%would 
leads to a significantly more complex neural network that %results with necessitating 
requires more training samples  to train properly, %
{as demonstrated later in Figure~\ref{fig:analysis_mi}}. 
% \todo[inline,color=green]{Is this something you will demonstrate in experiments, or just making an unsubstantiated claim here?\\
% HVH: shown in the experiment in Figure~\ref{fig:analysis_m} and comment to the figure in the text.}

\subsection{%
{One Bit }Quantization}\label{sec:quantization_example}
% \todo[inline]{Maybe change to generic function.}
%Here, we present a 
Consider the linear observation model with quantization. %
{Specifically, let Let $\pr\in\mathbb{R}^{\np}$ be the parameter %
{we wish to estimate} and $\matsym{A}\in\mathbb{R}^{{\nx}\times \np}$  a fixed mixing matrix, %then 
and define the  ${n}^{th}$ snapshot measurement ${\X}_{n}\in\mathbb{R}^{{\nx}}$:} % which is given by:
\begin{equation}\label{eq:lin_q}
{\X}_{n}=\mathrm{sign}\brackets{\matsym{A}\pr+\randomvec{W}_{n}+\boldsymbol{1}_{\nx}\cdot b },
\end{equation}
where %
{$$\squareb{\mathrm{sign}\brackets{\vectorsym{x}}}_i=\begin{cases}
    1 & \squareb{\vectorsym{x}}_i\geq 0\\
    -1 & \squareb{\vectorsym{x}}_i< 0
\end{cases}$$ is an element-wise sign function,} 
%
{ $\pr \in \mathbb{R}^{\np}$,}
$\randomvec{W}_{n}\sim\normaldis{0}{\matsym{\Sigma}}$ is additive zero-mean Gaussian noise  with covariance %matrix
$\matsym{\Sigma}$, $\boldsymbol{1}_{\nx} \in \mathbb{R}^{\nx}$ is a vector of ones, % of size $n$, 
and $b$ is a constant shift. Finally, for the prior
%we assume a  prior Gaussian distribution of 
we assume $\pr\sim\normaldis{0}{\matsym{I}\sigma_p^2}$. 
%It is important to highlight that 
Importantly, even for this simple observation model
%, as depicted in 
\eqref{eq:lin_q}, there is no closed-form solution for the BCRB. However, in 
%simpler 
 the special case %where 
of a diagonal 
$\matsym{\Sigma}$, 
%is a diagonal matrix, 
the %\hvhreplace{Bayesian}
{true} {Fisher} score 
% \todo[inline,color=green]{Do you also need the two other score functions for the experiments?}
has a closed-form solution (derived in Appendix~\ref{apx:derivation_q_score})
%{
\begin{align}\label{eq:score_quantization}
    \vectorsym{s}^{0}_{F}\brackets{\x|\p}&=\frac{\partial\vectorsym{u}\brackets{\p}}{\partial\p}^T\vectorsym{\rho}\brackets{\x,\p}, \\
    \text{where } \squareb{\vectorsym{\rho}\brackets{\vectorsym{x},\p}}_i&=
    \textstyle{\frac{\squareb{\x}_i\exp\brackets{-\frac{\squareb{\vectorsym{u}\brackets{\p}}_i^2}{2\sigma_i^2}}}{\sqrt{2\pi\sigma^2_i}\Phi\brackets{\squareb{\x}_i\frac{\squareb{\vectorsym{u}\brackets{\p}}_i}{\sigma_i} }} } , \nonumber
\end{align}
$\vectorsym{u}\brackets{\p}=\matsym{A}\p+\boldsymbol{1}_{\nx}\cdot b$ is the un-quantized observation function, $\sigma_i^2=\squareb{\Sigma}_{i,i}$ is the $i^{th}$ element in the diagonal matrix $\Sigma$, and
$\Phi$ is the 
%cumulative distribution function (CDF) of
standard Gaussian CDF. %
{Note that the true prior and posterior scores are 
given by the same expressions as for the linear observation model in section~\ref{sec:linear_model}.}

Using \eqref{eq:score_quantization} in \eqref{eq:mean_fully}, we can determine, {for known $\matsym{A}, \matsym{\Sigma}$ and $b$}, the BCRB numerically.
However, this is a very limited special case, and a more general bound cannot be obtained since the CDF of the multivariate Gaussian distribution does not have a close form\cite{genz2009computation}, preventing  the computation of the PMF
%probability mass function (PMF) 
$\pmft{\X|\p}{\x|\pr}$. %
{Needless to say, for unknown $\matsym{\Sigma}$, the BCRB cannot be determined conventionally at all.}

 %
 {Similar to the linear problem of Section~\ref{sec:linear_model}, our goal is to compute the LBCRB (a close approximation to the BCRB), when $\matsym{A}$, $\matsym{\Sigma}$ and $b$ are \emph{unknown}, by using a training data set $\ds=\set{\xset_n=\set{{\x}_m}_{m=1}^{\niiddata},\p_n}_{n=1}^{\nds}$. 

