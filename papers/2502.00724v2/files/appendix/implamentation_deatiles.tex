\subsection{Implementation Details}
{%Here, we provide details of the implantation. 
We 
%begin with the description of the 
use a conditional NN for the score NN
%neural network and the utilization of 
and (similar to \cite{habi2024learning}) train a single NN  on multiple S-s, which are used as a conditioning variable. % (this is similar to \cite{habi2024learning})
}
% \subsubsection{Conditional Score Neural Network}\label{apx:cond_score}
% In many cases, we have a known condition variable, such as the signal-to-noise ratio, sensor parameter, etc. that affects the measurement distribution. Instead of training a multiple score that matches one for each condition, we suggest learning a single score model. Specifically,  we denote this condition by $\vectorsym{c}$, then the score model is given by:
% \begin{equation}\label{eq:model_cond}
% \postscore{\p}{\x,\vectorsym{c}}=\lscore{\x}{\p,\vectorsym{c}}+\priorscore{\p}.
% \end{equation}
% Then to optimize the model in \eqref{eq:model_cond} we use \eqref{eq:score_lik_mean} while the data set also includes the condition vector, namely $\ds=\{\x_i,\p_i,\vectorsym{c}_i\}_{i=1}^{\nds}$ and the objectives change into:
% \begin{subequations}
% \begin{equation}
%     \lossbsm\brackets{\Omega}=\frac{1}{2\nds}\sum_{\p,\x,\vectorsym{c}\in\ds}\norm{\postscore{\p}{\x,\vectorsym{c};\Omega}}_2^2+\trace{\overline{\matsym{J}}_B},
% \end{equation}
% \begin{align}
%     \lossfsm\brackets{\paramf; \paramp}&=\frac{1}{2\nds}\sum_{\x,\p,\vectorsym{c}\in\mathcal{D}}\norm{\lscore{\x}{\p,\vectorsym{c};\paramf}}_2^2+\trace{\overline{\matsym{J}}_{F}}\nonumber \\
%     &+\frac{1}{\nds}\sum_{\x,\p,\vectorsym{c}\in\mathcal{D}}\lscore{\x}{\p,\vectorsym{c};\paramf}^T\priorscore{\p;\paramp},
% \end{align}
% \end{subequations}
% where $\overline{\matsym{J}}_{F}\brackets{\paramf}\triangleq\frac{1}{\nds}\sum_{\x,\p,\vectorsym{c}\in\mathcal{D}}{\frac{\partial \lscore{\x}{\p,\vectorsym{c};\paramf}}{\partial\p}}$ and $\overline{\matsym{J}}_B\brackets{\Omega}\triangleq\frac{1}{\nds}\sum_{\p,\x,\vectorsym{c}\in\ds}{\frac{\partial \postscore{\p}{\x,\vectorsym{c};\Omega}}{\partial\p}}$ are the average Jacobin matrix. 
\subsubsection{Neural Network Structure}
% \ybdelete{In this paper} We use a neural network architecture similar to a multilayer perceptron that incorporates conditional variables \cite{ho2021classifier,karras2020analyzing}. The MLP consists of a sequence of $N$ \todo[inline,color=green]{What $L$ is used for each example? \\
% HVH:L and N are the same this is a mistake on my part, fixed.} 
basic blocks {%that operate as
illustrated in Figure~\ref{fig:basic_block}, each performing the following sequence of operations:}
(i) parameter injection, %is performed,
where the input is scaled and shifted based on the parameter vector $\p$; 
(ii) a fully connected operation; 
%is executed; 
(iii) condition injection (not used in the prior score model $\priorscore{\p}$ ) involving the application of a scale factor to the features; and finally (iv) %a non-linear operation is applied, 
point-wise Swish \cite{ramachandran2017searching} non-linearity.
%for which we utilize Swish \cite{ramachandran2017searching}.
{The parameters of the NN structures used for the various examples are listed in Table~\ref{tab:net-param}.}
\begin{figure}
    \centering
    \input{files/tikz/inject_mlp}
    \caption{Basic NN Block}
    \label{fig:basic_block}
\end{figure}
\subsubsection{Hyper-parameters {and Training}}\label{apx:param}
In all experiments, we use the following hyperparameters. We train the score neural network for  200 epochs using the AdamW \cite{loshchilov2018decoupled} optimizer with learning rate 4e-4 and weight decay $1e-4$, %. Using 
and a data set of $60k$ samples for each S- condition.% (S-). 
If a smaller dataset is used, then the number of epochs is adjusted to keep the same
%such that the 
number of gradient updates.
%will remain the same. 
The batch size is set to 512. We employ exponentially decayed averages (EMA) of the weights with a decay rate of 0.999 as suggested in \cite{song2020improved}. At the end of the training we take the weights from the last EMA update. 
\begin{table}[]
\caption{Neural Network Configuration and Parameters for Each Example. The Score Model specifies the type of score function approximated by the NN.
%the neural network approximates. 
"N Blocks" indicates the number of basic blocks. 
% \todo[inline,color=green]{In the text you say "$L$ basic blocks". What is the connection between blocks and layers?} 
"Non-linearity" {indicates the presence/absence of a nonlinearity in the NN.} {"True Score Realizable" indicates whether the true score function} can be represented by the NN. %neural network.
{{"\pe{}"} {indicates whether the NN is \pe{}.} }. 
}
\label{tab:net-param}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ccccccc}
\hline
Example                                                                                          & \begin{tabular}[c]{@{}c@{}}Score\\ Model\end{tabular} & \begin{tabular}[c]{@{}c@{}}N\\ Blocks\end{tabular} & Width & \begin{tabular}[c]{@{}c@{}}Non\\ Linearity\end{tabular} & \begin{tabular}[c]{@{}c@{}}True \\ Score \\ Realizable\end{tabular} & \begin{tabular}[c]{@{}c@{}}Physics \\ Encoded\end{tabular} \\ \hline
Linear                                                                                           & Posterior                                             & 1                                                  & -    & -                                                      & $\cmark$                                                           & -                                                \\
\multirow{2}{*}{Linear}                                                                          & Fisher                                            & 1                                                  & -    & -                                                      & $\cmark$                                                           & -                                                 \\
                                                                                                 & Prior                                                 & 1                                                  & -    & -                                                      & $\cmark$                                                           & -                                                 \\
\multirow{2}{*}{Quantization}                                                                    & Fisher                                            & 3                                                  & 96    & $\cmark$                                                        & -                                                         & -                                              \\
                                                                                                 & Prior                                                 & 1                                                  & -    & -                                                      & $\cmark$                                                           & -                                                 \\
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Frequency Estimation\\ Gaussian Noise \end{tabular}}   & Fisher                                            & 3                                                  & 96    & $\cmark$                                                        & -                                                          & -                                                 \\
                                                                                                 & Prior                                                 & 3                                                  & 96    &$\cmark$                                                        & -                                                          & -                                                  \\
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Frequency Estimation\\ Gaussian Noise\end{tabular}}   & Fisher                                            & 1                                                  & -    & -                                                      & $\cmark$                                                           & $\cmark$                                                  \\
                                                                                                 & Prior                                                 & 3                                                  & 96    & $\cmark$                                                        & -                                                          &-                                                \\
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Frequency Estimation\\ Underwater Noise\end{tabular}} & Fisher                                            & 2                                                  & 96    & $\cmark$                                                        & -                                                          & $\cmark$                                                  \\
                                                                                                 & Prior                                                 & 3                                                  & 96    & $\cmark$                                                       & -                                                          & -                                              \\ \hline
\end{tabular}%
}
\end{table}
\subsubsection{Measurement Models {Setup}}\label{apx:init_sp}
%In the cases of a 
The linear measurement model \eqref{eq:linear-model} or the quantized linear measurement model \eqref{eq:lin_q} are {set up as follows.} Each element in $\matsym{A}$ is generated by $\squareb{\matsym{A}}_{ij}\sim\normaldis{0}{1}$. For the covariance matrix, we first generate $\matsym{U}\in\mathbb{R}^{d_x\times d_x}$   as %in the following meaner
$\squareb{\matsym{U}}_{ij}\sim\normaldis{0}{1}$ and then $\matsym{\Sigma}=\frac{\matsym{U}\matsym{U}^T}{\trace{\matsym{U}\matsym{U}^T}}$. 