\section{Theoretical Results}\label{sec:theory}
Here, we %theoretically 
investigate the errors of the \name{} due to learning error. %
{Following the standard approach in } learning theory \cite[Chapter 5]{shalev2014understanding}, we divide the learning error into two components: the \emph{approximation error} and the \emph{empirical-mean error}\footnote{In {learning theory} \cite[Chapter 5]{shalev2014understanding}, the empirical mean error is %termed 
known as the "estimation error". However, to avoid confusion with the estimator error %as defined 
in \eqref{eq:bcrb}, we %denote 
refer to it here as the "empirical-mean error".} . We then examine each of these errors individually and their collective impact.
 % \ybdelete{Initially, we evaluate the approximation error of both approaches, demonstrating that the relative score matching objective provides an upper bound on the Fisher Information Matrix (FIM) errors.  Subsequently, we explore the effect of employing the empirical mean in place of the expected value and the constraints imposed by a finite dataset.} %
 % \ybdelete{Afterwards, we present the combined error and the condition when \name{} is invertible and has relation to the score matching objective.}
 In the last part, we demonstrate that the {learned score neural network models are strongly consistent approximations of}  %with 
 the true scores, and that \name{} converges {with probability 1 to the BCRB as the size of the training data set $\ds$ increases,} providing conditions to achieve an accurate approximation. 



% \hvhdelete{Finally, we present the combined error and show the condition under which we obtain a good approximation. Following the %
% {approximation error}, we present the condition when \name{} provides an invertible and there is a relation to the score matching objective. }

{The results} in this section
%, we utilized 
use the  concept of \emph{intrinsic dimension of a matrix}\cite{tropp2015introduction,ipsen2024stable} as a measure of its {\emph{effective}} dimension: %here we prove its determination: 
\begin{definition}[Intrinsic Dimension of a Matrix]
    Let $\matsym{A}\in\mathbb{R}^{d\times d}$ be a non-zero positive semi-definite square matrix. Then its intrinsic dimension is defined as:
    \begin{equation*}
        \mathrm{intdim}\brackets{\matsym{A}}\triangleq\frac{\trace{\matsym{A}}}{\norm{\matsym{A}}_2}.
    \end{equation*}
\end{definition}
The intrinsic dimension of a matrix can be understood as quantifying the number of dimensions
%. Importantly, %The Intrinsic Dimension Of a Matrix 
%it considers 
{by accounting for} the spectral intensity over all dimensions. 
% \ybdelete{%To further explore its relationship with various quantities, refer to the following inequality:
% % \begin{equation*}
%     $1\leq \mathrm{intdim}\brackets{\matsym{A}} \leq \mathrm{rank}\brackets{\matsym{A}}\leq d.$
% % \end{equation*}
% }
From 
%the inequality and 
the definition of intrinsic dimension and the inequality $1\leq \mathrm{intdim}\brackets{\matsym{A}} \leq \mathrm{rank}\brackets{\matsym{A}}\leq d,$
we observe that $\mathrm{intdim}\brackets{\matsym{A}}=d$ when all %signaler 
eigenvalues are identical and nonzero, whereas $\mathrm{intdim}\brackets{\matsym{A}}=1$ when $\matsym{A}$ has rank one. In addition, the intrinsic dimension is more robust to small perturbations than the matrix rank \cite{ipsen2024stable}. 

\subsection{Approximation error}
{We evaluate the approximation error of both approaches, demonstrating that the %relative 
}
score matching objective {that expresses the score mismatch} provides an upper bound on the {learned} Fisher Information Matrix (FIM) errors.
\begin{theorem}[\name{} %
{Approximation Error:} %\ybreplace{Direct}
{Posterior} Approach]\label{thm:lrn:direct}
    Let $\mathrm{sRE}_B\triangleq\frac{\lossbs}{\trace{\fb}}$ be the %
    {relative %
    {approximation error} of the} Bayesian score 
    %
    {and let $\db=\mathrm{intdim}\brackets{\F_B}$.} Suppose that Assumptions~\ref{assum:bcrb_reg}, \ref{ass:score_cond_reg} hold and $\db{\cdot \mathrm{sRE}_B}\leq {0.16}$. Then:
    % \begin{subequations}
    \begin{align}\label{eq:error_learn_post_approch}
        %
        {\mathrm{RE}_{B}^{(a)}}=\frac{\norm{\lbfimb-\fb}_2}{\norm{\fb}_2} &\leq\bfle \triangleq {2.4}\sqrt{\db\cdot\mathrm{sRE}_B}.
                                                % &=\lossbs+2\sqrt{\norm{\widetilde{\mathrm{LBFIM}}}_2\cdot \lossbs}
    \end{align}
   % \hvhdelete{ where $\mathrm{sRE}_B\triangleq\frac{\lossbs}{\trace{\fb}}$ is the {relative {approximation error} of the} Bayesian score.}
    % \end{subequations}
\end{theorem}

\begin{theorem}[\name{} %
{Approximation Error}: Measurement-Prior Approach]\label{thm:lrn:lik_prior}
    Let $\mathrm{sRE}_M\triangleq\frac{\lossfs}{\trace{\fm}}$ and $\mathrm{sRE}_P\triangleq\frac{\lossps}{\trace{\fp}}$ be the {relative errors in learning the Fisher and prior scores}, respectively and let $\dm=\mathrm{intdim}\brackets{\fm}$ and $\intdp=\mathrm{intdim}\brackets{\fm}$. %\ybdelete{be the intrinsic dimension of measurement FIM $\fm$ and Prior FIM $\fp$ , respectively.}
    %Assume 
    Suppose that Assumptions~\ref{assum:bcrb_reg}, \ref{ass:score_reg_prior}, and \ref{ass:score_reg} hold and ${\dm\cdot \mathrm{sRE}_M}\leq {0.16}$, ${\intdp\cdot \mathrm{sRE}_P}\leq {0.16}$. 
    Then:
    \begin{align}\label{eq:error_learn_lik_prior_approch}
        &%
        {\mathrm{RE}_{MP}^{(a)}}=\frac{\norm{\lbfimlp-\fb}_2}{\norm{\fb}_2} \leq  \mpfle\\
    &\mpfle\triangleq2.4\brackets{\frac{\norm{%
    {\niideval}\fm}_2}{\norm{\fb}_2}\sqrt{\dm\cdot\mathrm{sRE}_M}+\frac{\norm{\fp}_2}{\norm{\fb}_2}\sqrt{ \intdp\cdot\mathrm{sRE}_P}}\nonumber
    \end{align}
    % \hvhdelete{where
    % % \begin{equation*}
    %     $\mfle=2.4\sqrt{\dm\cdot\mathrm{sRE}_M}$
    % % \end{equation*}
    % % \begin{equation*}
    % and 
    %     $\pfle=2.4\sqrt{ \intdp\cdot\mathrm{sRE}_P}$
    % % \end{equation*}
    % % \begin{equation}
    % % \end{equation}
    % are the upper bounds on {approximation error} of the measurement FIM and the prior FIM, respectively. }
\end{theorem}

Theorems~\ref{thm:lrn:direct} and \ref{thm:lrn:lik_prior} %
{(proved in Appendix C)} %\ybreplace{provide a useful}
{quantify the} relationship between the %
{score matching} optimization objective, %
{(or equivalently, the relative error in %
{approximating} the score vector),} and the  %
{approximation error} %ybedit
{of the corresponding FIM}. They %\ybreplace{showing that}
{show how a} reduction in the objective (smaller score %
{approximation error}) %\ybreplace{will provide}
{translates to} a smaller learning error %
{for the FIM}.  %{
Moreover, the tightest bound for a selected neural network architecture is achieved when %
{$\Omega
%$ is equal to $
=\Omega^*$ --  the NN parameters that minimize the objective $\mathcal{L}$.} 
 %{
Finally, the assumptions $\mathrm{intdim}\brackets{\fb}\frac{\lossbs}{\norm{\fb}_2}\leq 0.16$, $\mathrm{intdim}\brackets{\fm}\frac{\lossfs}{\norm{\fm}_2}\leq 0.16$ and $\mathrm{intdim}\brackets{\fp}\frac{\lossps}{\trace{\fp}}\leq 0.16$ 
% \todo[inline,color=green]{YB: update to match the actual assumptions in the Thms.}
introduced in Theorems~\ref{thm:lrn:direct} and \ref{thm:lrn:lik_prior} can be satisfied  by minimizing the score %\ybreplace{loss functions}
{objectives} $\lossbs, \lossfs, \text{and } \lossps$.


%
{Comparing between the theoretical results on the {approximation error} of the two approaches reveals 
%we observe 
that the Measurement-Prior Approach will %benefit 
have an advantage in cases where the measurement FIM is the dominant part, and some additional information can be used in the Fisher (Measurement) score model,
%-- the neural network has 
%some additional information, 
e.g. the reuse of i.i.d. samples,  or a model-informed score neural network. This is also demonstrated numerically in Fig.~\ref{fig:k_study} and Fig.~\ref{fig:analysis_mi}.    }

\subsection{Empirical Mean Error}\label{sec:empirical_mean_error}
In this section, we discuss the error introduced by using a finite empirical mean in \eqref{eq:mean_fully}, \eqref{eq:mean_efim_likd}, and \eqref{eq:bprior_mean} instead of the true expectation. We 
%start by 
assume that the %{output of the }
learned score function is bounded for any sample in the dataset $\ds$, namely:
\begin{assumption}[%\ybreplace{Bounded Outer Product Assumption}
{Score Function Bounds}]\label{assume:bounded_score}
For $\Omega^*$, $\paramp^*$ and $\paramf^*$ we assume that:
    % The FIM are bounded for every sample in $\ds$:
\begin{enumerate}[label=\text{\ref{assume:bounded_score}}.\arabic*,labelsep=*, leftmargin=*]
    \item\label{assume:bound_post} $\norm{\postscores{\p_n}{\xsetr_n}}_2^2\leq \cb \quad \forall \xsetr_n,\p_n \in \ds$.
    \item \label{assume:bound_fisher}$\norm{\frac{1}{\niiddata}\sum_{\x\in\xsetr_n}\lscores{\x}{\p_n}\lscores{\x}{\p_n}^T}_2\leq \cm \quad \forall  \xsetr_n,\p_n  \in \ds $ .
\item\label{assume:bound_prior} $\norm{\priorscores{\p_n}}_2^2\leq \cp \quad \forall  \p_n  \in \ds$.
\end{enumerate}
\end{assumption}

{\begin{remark}
The bounds $\cb, \cm$, and $\cp$ are determined for the specific realization in $\ds$, \emph{after it has been observed.} {Hence these bounds are functions of $\ds$, $\cb=\cb(\ds), \cm=\cm(\ds), \cp=\cp(\ds)$, but this dependence will not be shown explicitly.}
\end{remark}
}

Assumptions~\ref{assume:bounded_score} can be inherently satisfied by selecting an appropriate neural network architecture and nonlinear activation function and by the assumption that all samples in finite size set $\ds$ are bounded as well. {Subject to these assumption, we obtain the following results.}

% \todo[inline,color=green]{YB: I think that the results of the paper are correct and complete at this point, and I started a new pass on the technical parts. \\
% 1. I accepted all the previous edits from the beginning through Thm. V.3, so that new edits are clearer.\\
% 2. Edited Thm. V.3 for a cleaner presentation. \\
% 3. @Hai, can you make the corresponding adjustments in the proof of Thm V.3 in the Appendix? Also, can you make similar edits in Thm. V.4?\\
% HVH?: Yes and done. Also note that 
% }
\begin{theorem}[LBCRB Empirical Mean Error: Posterior Approach]\label{thm:sampling_post}
%
{Suppose that Assumptions~\ref{assum:bcrb_reg}, \ref{ass:score_cond_reg}, and the score bound %score  
condition~\ref{assume:bound_post} hold.} 
    Let $\dbb=\mathrm{intdim}\brackets{\lbfimb}$,
    % \ybdelete{be the intrinsic dimension of matrix $\lbfimb$. Assume that assumptions~\ref{assum:bcrb_reg}, ~\ref{ass:score_cond_reg}, the bound score  condition ~\ref{assume:bound_post}} \hvhdelete{and $\nds>>\dbb\frac{C_B^2}{\trace{\lbfimb}}$} 
    %
    {and define 
    $\nbe\triangleq\frac{4}{3}\brackets{u+\log\brackets{8 \dbb}}\brackets{\frac{\cb}{\trace{\lbfimb}}\cdot \dbb+1}$. %hold, 
    }
    Then for any $u>0$ 
    % \ybreplace{
    % {and $\nds\geq \frac{4}{3}\nbe\triangleq\frac{4}{3}\brackets{u+\log\brackets{8 \dbb}}\brackets{\frac{\cb}{\trace{\lbfimb}}\cdot \dbb+1}$}}
    {and $\nds \geq \nbe$} the following bound holds with probability %
    {of at least } $1-\exp{\brackets{-u}}$:
    \begin{align}\label{eq:re_error_post_mean}
        %
        {\mathrm{RE}_{B}^{(e)}}=&\tfrac{\norm{\lbfimbs-\lbfimb}_2}{\norm{\lbfimb}_2}\leq \bfse\triangleq 
        % \ybreplace{\sqrt{3\frac{\nbe}{\nds}}}
        {1.5\sqrt{\frac{\nbe}{\nds}}}.
    \end{align}
    % \hvhdelete{where,
    % % \begin{align}
    % $\mathrm{Q}\brackets{u,d,r}={u+\log\brackets{8d}}\brackets{r\cdot d+1}$
    % {is the function of empirical mean error bound, $\dbb=\mathrm{intdim}\brackets{\lbfimb}$ is the intrinsic dimension of matrix $\lbfimb$. }} 
    Furthermore, %
    {define
    $\nbet=\brackets{\log\brackets{1+2\dbb} +{0.52}} \cdot\brackets{\dbb\frac{\cb}{\trace{\lbfimb}}+1}$. Then for any $\nds\geq\nbet$ the relative empirical mean error is bounded in expectation by
    }
     \begin{align}\label{eq:emprical_mean_error_post_expection}
        \expectation{\mathrm{RE}_{B}^{(e)}}{\ds}\leq
        {\frac{6+1.5\sqrt{2\log\brackets{1+2\dbb}}}{\sqrt{\log\brackets{1+2\dbb} +{0.52}}
        }}\sqrt{\frac{\nbet}{\nds}}.
    \end{align}
    %{ where $\nbet=\frac{\cb}{\norm{\lbfimb}_2}+1$.}
\end{theorem}

\begin{theorem}[LBCRB Empirical Mean Error: Measurement-Prior Approach]\label{thm:sampling_mp}
    Suppose that  Assumptions~\ref{assum:bcrb_reg}, \ref{ass:score_reg_prior}, \ref{ass:score_reg} and the bounded score conditions~\ref{assume:bound_fisher} and \ref{assume:bound_prior} hold.
    Let %
    { $\dmpb=\mathrm{intdim}\brackets{\lbfimlp}$, and define $\nmpe\triangleq \frac{4}{3}\brackets{u+\log\brackets{8 \dmpb}}\brackets{\frac{ \cm+\frac{\cp}{\niideval}}{\trace{\tlmfim}+\frac{\trace{\tlpfim}}{\niideval}}\cdot \dmpb+1}$. }
     Then for any $u>0$ and $\nds\geq \nmpe$ the following bound holds with probability {of at least } $1-\exp{\brackets{-u}}$:
    \begin{align}\label{eq:re_error_mp_mean}
        %
        {\mathrm{RE}_{MP}^{(e)}}=&\frac{\norm{\lbfimlps-\lbfimlp}_2}{\norm{\lbfimlp}_2}\leq \mpfse\triangleq {1.5}\sqrt{\frac{\nmpe}{\nds}}. 
    \end{align}
    Furthermore, define $\nmpet\triangleq\brackets{\log\brackets{1+2\dmpb} +{0.52}}\cdot\brackets{\dmpb\frac{ \cm+\frac{\cp}{\niideval}}{\trace{\tlmfim+\frac{\tlpfim}{\niideval}}}+1}$. Then %
    {for any $\nds\geq \nmpet$ the relative empirical mean error is bounded in expectation by}
     \begin{align}\label{eq:emprical_mean_error_mp_expection}
        &\expectation{\mathrm{RE}_{MP}^{(e)}}{\ds}\leq \frac{6+1.5\sqrt{2\log\brackets{1+2\dmpb}}}{\sqrt{\log\brackets{1+2\dmpb} +{0.52}}}\sqrt{\frac{\nmpet}{\nds}}.
    \end{align}
\end{theorem}
% \todo[color=green,inline]{YB: Moved here (and elaborated) the interpretation of Theorems \ref{thm:sampling_post} and \ref{thm:sampling_mp}, because this is about the finite $\nds$ case, not the case addressed by the corollaries. }
{Theorems \ref{thm:sampling_post} and \ref{thm:sampling_mp}, proved in Appendix~\ref{apx:proof_sample_error}, quantify, with explicit constants (and for sufficiently large $u$, with overwhelming probability) the effect of the size $\nds$ of the training set $\ds$ on the error in the estimated FIMs due to replacing expected values by finite means over $\ds$.  
We also observe
the effect of the intrinsic dimension of the respective FIM (which is upper bounded by $\np$):  as the effective number of 
%we have more
parameters to estimate grows, %we require 
more training samples $\nds$ are required 
to obtain the same FIM error, with the dependence being (up to log factors ) roughly linear in $\dbb$ or $\dmpb$. 


%{
{
%When examining the sampling error, 
Recall from Sec.~\ref{subsec:MP} that an advantage of the Measurement-Prior Approach 
over the Posterior Approach is that it is able to determine the LBCRB for any number $\niideval$ of i.i.d. samples using the same $\ds$, without any additional effort or data.
Another advantage of the Measurement-Prior Approach is revealed by a close comparison
of Thm.~\ref{thm:sampling_post} with Thm.~\ref{thm:sampling_mp}:
% (i) it allows  determination of the LBCRB for any number of i.i.d. samples, denoted as $\niideval$; (ii) 
 % it exhibits 
the Measurement-Prior Approach enjoys
a reduced 
%sampling 
finite mean}
error for datasets $\ds$ consisting of measurements with multiple $\niiddata > 1$ i.i.d. samples per parameter value.
%such that $\niiddata > 1$ . 
}

{The reason for this behavior is that, as we argue below, with high probability, $\cb\geq \niiddata \cm +\cp$. Because $\lbfimlp\approx \fb \approx \lbfimb $, we also have $\dbb \approx \dmpb$. Toghether, this implies that $\nbe \geq \niiddata \nmpe$, and in turn
$\bfse \geq \sqrt{\niiddata}\mpfse$, i.e, the upper bound in \eqref{eq:re_error_mp_mean} on the empirical-mean error for the Measurement-Posterior Approach, is, with high probability, smaller by a factor of $\sqrt{\niiddata}$ than the corresponding bound in \eqref{eq:re_error_post_mean} for the Posterior Approach. 

To justify the claim that with high probability, $\cb\geq \niiddata \cm +\cp$, 
recall the definitions of $\cb, \cm$, and $\cp$ in Assumption~\ref{assume:bounded_score} as the maxima over $\ds$ of the expressions on the left hand sides of the respective definitions. To compare these quantities, we first compare their expected values by} proving in Appendix~\ref{apx:remark_c_relation_proof} the following relation between the true score functions.
\begin{prop} \label{prop:expected_score_ineq}
    \begin{align}
&\expectation{\norm{\nabla_{\p}\log\probt{\p|\xsetr}{\p|\xset}}_2^2}{}\geq\expectation{\norm{\nabla_{\p}\log\probt{\p}{\pr}}_2^2}{}+\nonumber\\
    &\niiddata \expectation{\norm{\frac{1}{\niiddata}\sum_{i=1}^{\niiddata}\nabla_{\p}\log\probt{\x_i|\p}{\X|\pr}\nabla_{\p}\log\probt{\x_i|\p}{\X|\pr}^T}_2}{}.\nonumber
\end{align}
\end{prop}
{Now, assuming that the learning of the score functions was successful and the score mismatches $\mathrm{sRE}_B, \mathrm{sRE}_M$ and $\mathrm{sRE}_P$ defined in Theorem~\ref{thm:lrn:direct} and Theorem~\ref{thm:lrn:lik_prior} are small, the same inequality as in Proposition~\ref{prop:expected_score_ineq} will apply to the respective learned scores $\postscores{\p_n}{\xsetr_n},\lscores{\x}{\p_n}$ and $\priorscores{\p_n}$. For $\niideval>1$, the large gap between the expected values suggests also a similar large gap between the maxima, implying the claim that $\cb\geq \niiddata \cm +\cp$ with high probability. 

This analysis is supported by the empirical results in Fig.~\ref{fig:sample_error_vs_n_samples}, which show a consistently lower empirical-mean errors for the Measurement-Prior Approach than for the Posterior Approach, but in terms of the bounds, and the actual errors.
 }


{In %the consistency 
Section~\ref{subsec:consistency} we %present 
show, using Theorems~\ref{thm:sampling_post} and \ref{thm:sampling_mp},
that the empirical mean error vanishes as the size $\nds$ of the training data set grows.
%convergence  to zero base on  Theorems ~\ref{thm:sampling_post} and ~\ref{thm:sampling_mp}. 



\subsection{{\name{}} Learning Error% and Invertibility
}
\label{subsec:Conv_Invert}
{
}
% \ybreplace{Following Theorems ~\ref{thm:sampling_post} and ~\ref{thm:sampling_mp} we provide two theoretical insights into the \ybreplace{\name}{learned BCRBs using the two approaches}: 1)  a condition under which the learned BFIM is invertible {(i.e., the \name \   exists) }; and 2) a bound on the {relative error in} the \name \  due to the {Approximation error} and  empirical mean error.}
{
To obtain bounds on the deviation $\hat{\bcrb} -\bcrb $ of the \name{} from the BCRB due to learning error, we combine the effects of the approximation error (Theorems \ref{thm:lrn:direct} and \ref{thm:lrn:lik_prior}) and the empirical-mean error (Theorems \ref{thm:sampling_post} and \ref{thm:sampling_mp}, respectively) on the learned FIM, and propagate them through the inverse relation $\lbcrbbs = \lbfimbs^{-1} $ and $\lbcrblps = \lbfimlps^{-1}$. For the latter, we include conditions for the learned FIMs $\lbfimbs$ and $\lbfimlps$ to remain positive definite in the presence of learning errors, to ensure that its inverse, the \name, exists. This is expressed in the following corollaries, proved in Appendix~\ref{apx:proof_inv_re}.
}
\begin{corollary}\label{corr:bound_inv_post}
Suppose that the assumptions of Theorems  {\ref{thm:lrn:direct} and} \ref{thm:sampling_post}  hold {and 
\begin{equation}\label{eq:inv_cond_post}
    \frac{\norm{\lbfimb}_2}{\norm{\fb}_2}\bfse+\bfle <\frac{1}{\kappa\brackets{\fb}}.
\end{equation}
%if 
Then }
{$\lbfimbs\succ 0$} {(i.e., %the learned BCRB,
$\lbcrbbs$ exists)}, 
{and} the following holds with probability {of at least} $1-\exp\brackets{-u}$:
\begin{align}\label{eq:inv_bound_post}
    \mathrm{RE}_{B} {\triangleq
    \frac{\norm{\lbcrbbs-\bcrb}_2}{\norm{
    \bcrb
    }_2}}
\leq\kappa\brackets{\lbfimbs}\brackets{\frac{\norm{\lbfimb}_2}{\norm{\lbfimbs}_2}\bfse+\frac{\norm{\fb}_2}{\norm{\lbfimbs}_2}\bfle}
\end{align}
\end{corollary}

% \todo[inline]{Write proof of the corollary below.}
\begin{corollary}\label{corr:bound_inv_split}
Suppose that %all previous 
the assumptions of Theorems {\ref{thm:lrn:lik_prior} and} \ref{thm:sampling_mp} hold,
{and 
\begin{equation}\label{eq:inv_cond_mp}
    \frac{\norm{\lbfimlp}_2}{\norm{\fb}_2} \mpfse +\mpfle<\frac{1}{\kappa\brackets{\fb}}.
\end{equation}
Then,  
%if 
}
{$\lmfim\succ 0$ and $\lpfim\succ 0$} {(i.e., %the learned BCRB, 
$\lbcrblps$ exists)}, {and} the following
holds with probability {of at least} $1-\exp\brackets{-u}$:
\begin{align}\label{eq:inv_bound_mp}
\mathrm{RE}_{MP} & \triangleq\frac{\norm{\lbcrblps-\bcrb}_2}{\norm{\bcrb}_2} \nonumber \\
&\leq\kappa\brackets{\lbfimlps}\brackets{\frac{\norm{\lbfimlp}_2}{\norm{\lbfimlps}_2} \mpfse+\frac{\norm{\fb}_2}{\norm{\lbfimlps}_2}\mpfle}.
\end{align}
\end{corollary}
% \ybdelete{The proofs of 
% Corollaries ~\ref{corr:bound_inv_post} and ~\ref{corr:bound_inv_split} 
% can be found 
% in Appendix~\ref{apx:proof_inv_re}. }
%\todo[inline,color=green]{
% YB: what principle are you using to number corollaries? why not use the same numbering scheme as for Theorems, Lemmas, etc, so that the above corollaries will be numbered V.5 and V.6? The general recommendation is to use a common running counter for all results, so that seeing a result referenced somewhere in the paper, it is easy to know where to look for it. \\
% HVH: Done, change to running index in the main paper.\\
%
% \todo[inline,color=green]{YB: what do you think, Hai, about the simplified interpretation below?\\
% HVH:It is nice and make the results clearer. By a question? What does the symbol $\tau$ mean or should it be $\eta$?
% }
{Corollaries \ref{corr:bound_inv_post} and \ref{corr:bound_inv_split} admit a simplified interpretation to first order in $\eta^{(e)}$ and $\eta^{(a)}$. Assuming small learning errors, we have $\lbfimlps \approx \lbfimlp \approx\fb \approx\lbfimb  \approx  \lbfimbs$. The bounds in \eqref{eq:inv_bound_post} and \eqref{eq:inv_bound_mp} then simplify, respectively, to
\begin{equation}
   \mathrm{RE}_{B} \leq\kappa\brackets{\bcrb}\brackets{
    \bfse+\bfle} 
\end{equation}
\begin{equation}
   \mathrm{RE}_{MP} \leq\kappa\brackets{\bcrb}\brackets{
    \mpfse+\mpfle}, 
\end{equation}
showing that for both methods, the respective relative empirical-mean and approximation errors essentially add up, with scaling by the condition number $\kappa\brackets{\bcrb} = \kappa\brackets{\fb}$ of the BCRB. 
}

{Conditions \eqref{eq:inv_cond_post} and \eqref{eq:inv_cond_mp} for positive-definiteness of the 
learned FIMs in Corollaries~\ref{corr:bound_inv_post} and \ref{corr:bound_inv_split} are sufficient conditions, but they are the weakest such conditions: if violated, then there may exist learning errors such that the learned FIM is not invertible. In this sense, they are necessary to be able to guarantee the existence of the \name{}.}

%
{These conditions %for invertibility 
can be further detailed into %more 
specific criteria concerning the empirical-mean error and the 
%
approximation error}.
{Assuming small approximation errors, $\lbfimb \approx \fb \approx \lbfimlp$}
%Based on 
\eqref{eq:inv_cond_post} and \eqref{eq:inv_cond_mp} imply that the minimum required sample size for the Posterior Approach is :
{
\begin{equation*}
   % \sqrt
    {\nds}>  2.25\kappa^2\brackets{\fb}
    %
% \frac{\norm{\lbfimb}_2}{\norm{\fb}_2}
    %
    %\sqrt
    %3
    \nbe,
\end{equation*}
}
}
and for the Measurement-Prior Approach:
\begin{equation*}
    \nds>   2.25\kappa^2\brackets{\fb}\nmpe.
\end{equation*}
% \todo[inline,color=green]{YB: Check my change in the condition on $\nds$ above, and make a similar change below.\\
% HVH:Done}
{
%These expression offer a necessary condition regarding the minimal sample size; 
}
Note that both requirements on $\nds$ are quite sensitive to the condition number of the BCRB $\bcrb$: larger condition number will require more training samples.
If these conditions on the training set size are not met, $\lbfimb$ and $\lbfimlp$ 
{may} 
be non-invertible. 
{Furthermore, by the discussion comparing Theorems Theorems \ref{thm:sampling_post} and \ref{thm:sampling_mp} in Section~\ref{sec:empirical_mean_error}, we  typically have $\nbe \geq \niiddata \nmpe$. Combining with the conditions above implies that the {Measurement-Prior} Approach will require smaller $\nds$ when $\niiddata>1$.}

{Turning to the approximation error, Conditions \eqref{eq:inv_cond_post} and \eqref{eq:inv_cond_mp} imply
$\bfle, \mpfle\leq \frac{1}{\kappa\brackets{\fb}} = \frac{1}{\kappa\brackets{\bcrb}}$,
i.e., that the relative approximation errors must be kept below a threshold determined by $\kappa\brackets{\bcrb}$. 
}
% \ybdelete{Moreover, if the minimal sample size condition is hold, we still require an additional condition over the {approximation error} which is given by:
% \begin{equation*}
%     \bfle\leq {\frac{1}{\kappa\brackets{\fb}}-\frac{\norm{\lbfimb}_2}{\norm{\fb}_2}\bfse},
% \end{equation*}
% and for the Measurement-Prior Approach:
% \begin{equation*}
%     \mpfle\leq {\frac{1}{\kappa\brackets{\fb}}-\frac{\norm{\lbfimlp}_2}{\norm{\fb}_2} \mpfse}.
% \end{equation*}


% {From the result of observe that the minimal number of sample scale with $\fb$ condition number. Moreover, to ensure invertibility, the \name{} {approximation error} should be smaller than of both approaches should be at less small than $\frac{1}{\kappa\brackets{\fb}}$.  }}

% \hvhdelete{Specifically, when the true BFIM has large eigenvalues, it is more easily invertible, implying that at higher loss values, the learned FIMs will also be invertible. Moreover, the \name{} error scales with the loss function in both methods, indicating that smaller loss values result in better trained FIMs.}






\subsection{ Consistency \& Convergence}
\label{subsec:consistency}
%
{We consider}  
the convergence of \name{} to the BCRB as number {$\nds$ of training} samples goes to infinity. 
%
{%To show the convergence of \name{}, we 
First we}
show that the empirical-mean error converges to zero as {$\nds \rightarrow \infty$.} Then we prove that a neural network trained using score matching is a consistent estimator of %with 
the true score function. Finally, 
%\ybreplace{based on the consistency and}
{combining these results with} Corollaries \ref{corr:bound_inv_post} and \ref{corr:bound_inv_split}, we show the convergence of the \name{} . 


% \todo[inline,color=green]{Say something about the proof of these corollaries? Do they follow directly from something, or is their proof presented somewhere?\\
% HVH: Done\\
% YB: I think that I was not clear enough. Looks like you thought I was asking about the proof of Corollaries \ref{corr:bound_inv_post} and \ref{corr:bound_inv_split}. I now  deleted your explanations here about the proof of those corollaries, because the proofs are actually available, pointed to after these corollaries.\\
% Instead, I am asking about the proof of Corollaries \ref{corr:sampling_post} and \ref{corr:sampling_mp}. Where is it?\\
% HVH: We have sad it is easy to proof so we obit it from the text. Now I have added a text to say what the proof is base one.}

%
{The convergence of the empirical-mean error follows} directly from
Theorems~\ref{thm:sampling_post} and \ref{thm:sampling_mp} subject to the additional assumption $\frac{\cb}{\nds}, \frac{\cm}{\nds}, \frac{\cp}{\nds}\xrightarrow{\nds\rightarrow\infty}0
{\quad \text{a.s.}}$, as summarized in the following results.

\begin{corollary}[LBCRB Empirical-Mean Error Convergence: Posterior Approach]\label{corr:sampling_post} 
Suppose
%Assume 
that the assumptions of Theorem~\ref{thm:sampling_post} hold and that $\frac{\cb}{\nds}\xrightarrow{\nds\rightarrow\infty}0 
{\quad \text{a.s.}}$, then:
\begin{equation}
\mathrm{RE}_B^{(e)}\xrightarrow{\nds\rightarrow\infty}0 \quad\text{a.s.}
\end{equation}
    
\end{corollary}

\begin{corollary}[LBCRB Empirical-Mean Error Convergence: Measurement-Prior Approach]\label{corr:sampling_mp} %Assume 
Suppose that the assumptions of Theorem~\ref{thm:sampling_mp} hold and that
$%\ybreplace{\frac{\cm+\frac{\cp}{\niideval}}{\nds}}
{\frac{\cm}{\nds}, \frac{\cp}{\nds}}\xrightarrow{\nds\rightarrow\infty}0 %
{\quad \text{a.s.}}$, then:
\begin{equation}
\mathrm{RE}_{MP}^{(e)}\xrightarrow{\nds\rightarrow\infty}0 \quad\text{a.s.}
\end{equation}
    
\end{corollary}





From Corollaries~\ref{corr:sampling_post} and \ref{corr:sampling_mp}, we observe that as the number of samples $\nds$
increases, the %
{empirical}-mean error is reduced%
{, vanishing with probability 1 in the limit of infinite training data}.
%{
 \begin{remark}\label{remark:mean_error_convergance}
    The assumption  $\frac{\cb}{\nds}\xrightarrow{\nds\rightarrow\infty}0 {\quad \text{a.s.}}$ in Corrolary~\ref{corr:sampling_post} is 
    {rather mild. 
    It means that the maximum over the data set $\ds$ of the squared norm of the learned score grows at a slower rate than $|\ds|=\nds$. For example, 
    {as we show in Appendix~\ref{apx:remark_convergance_proof}}, it holds if $\postscores{\pr_n}{\xsetr_n}$ has finite moments up to 6th order. In particular, this requirement is less demanding than the assumption made in \cite{crafts2023bayesian} that the score vector is
        Sub-Gaussian. Indeed, a Sub-Gaussian r.v. has finite moments of any order, rather than just up to 6th order.} Similar comments apply to the assumption about $\cp$ and $\cm$ in Corollary~\ref{corr:sampling_mp}.
\end{remark}

{Next we address the approximation error. We will need the following assumptions about the score neural networks. The first set of assumptions, of realizability of the true score functions by the chosen NN model architectures, is the more fundamental.}
\begin{assumption}[{NN-}Realizable Score Function]\label{assume:realizable}
Let {$\mathcal{S}_B$, $\mathcal{S}_F$ and $\mathcal{S}_P$} be the sets of all score functions representable by the  architectures of the neural networks chosen for the Bayesian, Fisher and prior scores, respectively. {Define the shorthand notation for the true score functions $\vectorsym{s}^{0}_{B}\brackets{\p|\xsetr} \triangleq\nabla_{\p}\log\probt{\p|\xsetr}{ \p | \xset}$,}  $\vectorsym{s}^{0}_{F}\brackets{\x|\p} \triangleq\nabla_{\p}\log\probt{\x|\p}{\X|\p}$ and $\vectorsym{s}^{0}_{P}\brackets{\p}\triangleq\nabla_{\p}\log\probt{\p}{\p}$. Then assume that {all three true scores are realizable {by the chosen NN architectures,} i.e.}:
{$\vectorsym{s}^{0}_\alpha \in \mathcal{S}_\alpha$, $\alpha= B, F, P$.}
\end{assumption}

We argue that Assumption~\ref{assume:realizable} holds {(or can be approximated arbitrarily well)} in the regime of a neural networks with sufficient capacity. {In terms of our parametrized score neural network notation, this means, e.g., for the Bayesian score, that $\vectorsym{s}_{B}\brackets{\p|\xsetr; \Omega} = \vectorsym{s}^{0}_{B}\brackets{\p|\xsetr}$ for some $\Omega=\Omega^{*}$. (Unlike in \cite{hyvarinen2005estimation}, the presence of multiple such alternative $\Omega^*$ is not a concern in our application, and may in fact facilitate the optimization step of score matching.)} 

{The second set of assumptions, of Lipschitz continuity, are more technical regularity assumptions.}
They can be readily satisfied by choice of neural network architecture and training strategy (e.g. $\ell_2$ regularization).

\begin{assumption}[Lipschitz Continuity]\label{assume:lip_cont}
%
For each of the following three alternative settings, 
\begin{enumerate}[label=\text{\ref{assume:lip_cont}}.\arabic*,labelsep=*, leftmargin=*]
    \item $\alpha=B$, $\, f\brackets{\z;\Omega}=\postscore{\p}{\x;\Omega}$, $\, \z=[\p,\x]$, %and% 
    $\, \randomvec{Z}=[\pr,\X]$, \label{assume:lip_cont_post}
    
    \item  $\alpha=P$, $\, f\brackets{\z;\Omega}=\priorscore{\p;\Omega}$, $\, \z=\p$, %and 
    $\, \randomvec{Z}=\pr$, \label{assume:lip_cont_prior}
    
    \item  $\alpha=F$, $f\brackets{\z;\Omega}=\lscore{\x}{\p;\Omega}$, $\z=[\p,\x]$, %and
    $\randomvec{Z}=[\pr,\X]$.\label{assume:lip_cont_fisher}
    
\end{enumerate}

assume the following holds.\\
% \todo[inline,color=orange]{YB: $\nabla_{\p}$ below is applied to vector function $f_\alpha$, so it should be replaced by the Jacobian matrix notation.}
% }
Functions $f_{\alpha}\brackets{\z;\Omega_{}}$, $\divc{f_{\alpha}\brackets{\z;\Omega_{}}}{\p}$ and $f_{\alpha}\brackets{\z;\Omega_{}}f_{\alpha}\brackets{\z;\Omega_{}}^T$ are Lipschitz continuous {with Lipschitz constants finite in expectation,} 
% \todo[inline,color=green]{In 1) and 3) the norms are applied to vectors, so shouldn't the norm be just the 2-norm? The Frobenius norm is only relevant in 2), so why not use the appropriat subscripts on $\norm{\cdot}$?}
namely, for any {$\Omega_{1}, \Omega_{2} \in \mathcal{S}_{\alpha}$ {with $\Delta\Omega=\norm{\Omega_{1}-\Omega_{2}}_2$}:} \\
% \todo[inline,color=green]{\textbf{YB:} Why superscripts on $\Omega$ rather than simple subscripts?}
   1) 
  $\norm{f_{\alpha}\brackets{\z;\Omega_{1}}-f_{\alpha}\brackets{\z;\Omega_{2}}}_2\leq \xi_{\alpha}\brackets{\z}\Delta\Omega$,\\
    2) $\norm{\divc{f_{\alpha}\brackets{\z;\Omega_{1}}}{\p}-\divc{f_{\alpha}\brackets{\z;\Omega_{2}}}{\p}}_2\leq \tau_{\alpha}\brackets{\z}\Delta\Omega$,\\
    3) $\norm{f_{\alpha}\brackets{\z;\Omega_{1}}f_{\alpha}\brackets{\z;\Omega_{1}}^T-f_{\alpha}\brackets{\z;\Omega_{2}}f_{\alpha}\brackets{\z;\Omega_{2}}^T}_F\leq \zeta_{\alpha}\brackets{\z}\Delta\Omega$.\\
    % \todo[inline,color=green]{Typo in the first outer product: should have $\Omega^{(1)}$ in both factors.}
{with} $\expectation{\xi_\alpha\brackets{\randomvec{Z}}{^2}}{\randomvec{Z}},
%<\infty$, 
\expectation{\tau_\alpha\brackets{\randomvec{Z}}}{\randomvec{Z}},
%<\infty$ and $
\expectation{ \zeta_{\alpha}\brackets{\randomvec{Z}}}{\randomvec{Z}}<\infty$. 
% \todo[inline,color=green]{Writing $\leq$ above is meaningless. Did you mean $<$?}


\end{assumption}

% \todo[inline,color=green]{YB: Say that these Lipschitz assumptions are reasonable - can be satisfied by the design of the NN?}

Then, {we obtain the following results.  }

\begin{theorem}[Posterior Score Consistency] \label{thm:post_consist}
    %Assume 
    Suppose that Assumptions~\ref{assum:bcrb_reg}, \ref{ass:score_cond_reg}, {\ref{assume:lip_cont},} and {\ref{assume:realizable}}~ hold, {and that the minimization algorithm applied to the objective in \eqref{eq:score_post_mean} succeeds in finding a global minimizer $\Omega^*={\arg\min\limits_{\Omega}\lossbsm\brackets{\Omega}}$.
    }  Then,
    \begin{equation*}
        \lossbs\brackets{\Omega^{*}}\xrightarrow{a.s} 0\quad\text{as} \quad\abs{\ds}=\nds\rightarrow \infty.
    \end{equation*}
\end{theorem}

\begin{theorem}[Measurement and Prior Score Consistency]\label{thm:mp_consist}
    %Assume 
    Suppose that Assumptions~\ref{assum:bcrb_reg}, \ref{ass:score_reg_prior}, \ref{ass:score_reg}, {\ref{assume:lip_cont},} {and \ref{assume:realizable}} hold, 
    {and that the minimization algorithms applied to the objectives in \eqref{eq:score_prior_mean} and \eqref{eq:score_lik_mean} succeed in finding respective global minimizers {$\paramf^{*}=\arg\min\limits_{\Omega}\lossfsm\brackets{\Omega}$ and $\paramp^{*}=\arg\min\limits_{\Omega}\losspsm\brackets{\Omega}$.}
    }
    Then,
        \begin{equation*}
        \lossfs\brackets{\paramf^{*}}\xrightarrow{a.s} 0\mkern9mu \text{and}\mkern9mu\lossps\brackets{\paramp^{*}}\xrightarrow{a.s} 0, \mkern9mu\text{as} \mkern9mu \abs{\ds}=\nds\rightarrow \infty. 
    \end{equation*}
\end{theorem}


Theorems~\ref{thm:post_consist} and \ref{thm:mp_consist} (proved in Appendix~\ref{apx:proof:consistency_post}) demonstrate that {the score neural networks optimized via score matching are strongly consistent estimators of the true score. }

% \ybdelete{In cases of neural networks, they are realizable when the architecture is sufficiently complex with a sufficiently large dataset.}    

% \ybdelete{Moreover, these consistency results are crucial for ensuring the convergence of LBCRB, as shown in the following Corollaries.}

%hvhedit
{From a practical point of view, {the dependence of}
Theorems~\ref{thm:post_consist} and \ref{thm:mp_consist} {on the  realizability Assumption~\ref{assume:realizable}}
highlights the advantages of \peac{} that uses domain knowledge {in the Measurement-Prior Approach.}  \peac{} enables {the realization and learning of accurate score functions models}: (i) using less complex neural networks (fewer parameters, nonlinearity, etc.); (ii)  {using} less training data; and (iii) {with more reliable training %identifying its parameters
(%optimization 
minimization of the loss function).% can be done more reliably.
}



% \hvhdelete{score matching optimization
% losses are consistent when the score function is realizable with a sufficiently large dataset.}
% \todo[inline,color=green]{YB: I don't think that one can say that "losses are consistent" in our context. Usually one talks about consistency of an estimator.} 
%  \hvhdelete{This also highlight 
% the advantages of MoISNN, which enables a realizable neural network with domain knowledge more easily (see the frequency estimation example in Section~\ref{sec:freq_est}).}

% \todo[inline,color=green]{How does one see this in these corollaries?\\

% The issue is "realizing the score function by a combination of a known function and a neural network of sufficiently low complexity that (1) it can be learned from the available training data; and (2) identifying its parameters (optimizing the loss function) can be done more reliably. Then appeal to the frequency estimation example.\\
% % Is this what you meant? This can be explained along these lines, but this explanation should be separated from the precise statements, so that it is clear that it is a somewhat loose qualitative statement.\\
% HVH: Please see updated text.
% } 



%\todo[inline,color=green]{Interpret these theorems?}
 }

 

% \todo[inline,color=green]{YB: state the converge results in the corollaries for the \name{} error itself, as done at the end of Sec.III-A, rather than the relative error. \\
% HVH: What do you think about the following way I wrote it? I think it address the problem statment?}

{Having established the a.s convergence of the empirical-mean  and the strong consistency of score matching, we are ready for our final results for the \name{} for the two methods.}

\begin{corollary}[Posterior Approach Convergence] \label{corr:re_zero}
Suppose that {the assumptions in Theorem~\ref{thm:post_consist}} {and Corollary~\ref{corr:sampling_post}} hold. Then,
%\begin{equation*}
    $\lim\limits_{\nds\rightarrow \infty } {\lbcrbbs =\bcrb}. \quad\text{a.s}$
%\end{equation*}
\end{corollary}

\begin{corollary}[Measurement-Prior Approach Convergence] \label{corr:re_zero_mp}
Suppose that {the assumptions in Theorem~\ref{thm:mp_consist}} {and Corollary~\ref{corr:sampling_mp}} hold. Then,
%\begin{equation*}
  $  \lim\limits_{\nds\rightarrow \infty } {\lbcrblps=\bcrb}. \quad\text{a.s}$
%\end{equation*}
\end{corollary}
% \todo[inline,color=green]{YB: Good, these are the results we want. But these corollaries need an argument/proof to justify them.}
{
We prove Corollary~\ref{corr:re_zero_mp}. The proof of Corollary~\ref{corr:re_zero} is exactly analogous, with  Corollary~\ref{corr:sampling_post} and Theorems~\ref{thm:lrn:direct} and \ref{thm:post_consist}  playing the role of  Corollary~\ref{corr:sampling_mp} and Theorems
\ref{thm:lrn:lik_prior}, and \ref{thm:mp_consist}, respectively. 
}

{\begin{proof}
   Observe that $\expectation{\lbfimlps}{\X,\pr}=\lbfimlp$.
   %and $\expectation{\lbfimbs}{\X,\pr}=\lbfimb$. 
   Using  Corollary~\ref{corr:sampling_mp} %~\ref{corr:sampling_post} and
   we have that, $\lbfimlps$ 
   %and $\lbfimbs$ 
   from %\eqref{eq:mean_fully} and
   \eqref{eq:bfim_apx_final} converges almost surely to 
   its %their 
   expected value,%s, such that
   i.e., $\lim\limits_{\nds\rightarrow\infty}\lbfimlps=\lbfimlp$ a.s.  
   % and $\lim\limits_{\nds\rightarrow\infty}\lbfimbs=\lbfimb$ a.s. 
   Furthermore, combining
 %  by integrating 
   Theorem~\ref{thm:lrn:lik_prior} 
   %and \ref{thm:lrn:direct} 
   with Theorem~\ref{thm:mp_consist}
   %and \ref{thm:post_consist}, 
   , it follows that $\lim\limits_{\nds\rightarrow\infty} \lbfimlp=\fb$ a.s.
   %and $\lim\limits_{\nds\rightarrow\infty} \lbfimb=\fb$ a.s. 
 Hence, $\lim\limits_{\nds\rightarrow\infty}\lbfimlps=\fb$ a.s.  
 Inverting yields the result. 
\end{proof}
}

{In summary, } as stated in Corollarys~\ref{corr:re_zero} and \ref{corr:re_zero_mp}, with sufficient training data and a properly trained neural networks, the\name{} {will provide the BCRB accurately}. 