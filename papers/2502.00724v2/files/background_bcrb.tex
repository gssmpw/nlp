\subsection{Bayesian Cram\'er Rao Bound}
Let $\X\in\Upsilon\subseteq\mathbb{R}^\nx$ be a random observation vector {that depends on a random parameter vector} $\pr\in\Ps\subseteq\mathbb{R}^\np$, %be a random parameter vector 
and let $\probt{\x,\p}{\X,\pr}$ be their joint {probability density function (PDF).}\footnote{To simplify  notation we write the expressions for continues random vector $\X$;  for discrete or mixed $\X$ they can be written in terms of the joint CDF, or for $\X$ discrete $\probt{\x,\p}{\X,\pr}$ can be replaced by $P_{\X}(\x)\probt{\p | \x}{\pr|\X}$ or by $\probt{\p}{\pr} P_{\X|\pr}(\x|\p)$, where $P_{\X}(\x)$ and $P_{\X|\pr}(\x|\p)$ are the probability mass function (PMF), and conditional PMF of $\X$, respectively.  
}
%of $\X$ and $\p$. 
For convenience we model the parameter set $\Ps$ as a closed set (in the Euclidean metric) with no isolated points. Assume
the following conditions\cite[pages 33-35]{van2007bayesian}, \cite{weinstein1988general}. 
\begin{assumption}[BCRB Regularity Conditions]
    \label{assum:bcrb_reg}
$\probt{\x,\p}{\X,\pr}$ 
satisfies the following conditions:  
\begin{enumerate}[label={\ref*{assum:bcrb_reg}}.\arabic*,labelsep=*, leftmargin=*]

\item\label{sas:derivative} The gradient $\nabla_{\p}\log\probt{\x,\p}{\X,\pr}$ with respect to $\p$  exists and {each of its elements} is absolutely integrable w.r.t $\x$ and $\p$ {on $\Upsilon \times \Ps$}.
\item\label{sas:derivative_prior} {The gradient $\nabla_{\p}\log\probt{\p}{\pr}$ with respect to $\p$  exists.}
\item \label{assume:non_singular}  The matrix  $$\expectation{\nabla_{\pr}\log\probt{\x,\pr}{\X,\pr}\nabla_{\pr}\log\probt{\x,\pr}{\X,\pr}^T}{\X,\pr} 
$$  is {positive-definite} and finite. 
\item The limits $\lim\limits_{\p\rightarrow\partial\Ps{^+}}\p\probt{\x,\p}{\X,\pr}=0$ hold, where $\partial\Ps{^+}$ is the boundary of the set $\Ps$ augmented by the points  of $\Ps$ at infinity  in case $\Ps$ is unbounded. \label{ass:lim}
\item  The conditional expectation of the score  {%equal zero 
vanishes for all $\x$,} $\expectation{\nabla_{\pr}\log\probt{\pr,\x}{\pr,\X}}{\pr|\X}=0$ .\label{ass:score_expection}
\item For all $\p\in\Ps$, the densities $\probt{\x,\p}{\X,\pr}$ have a common support {$\{\x:\probt{\x,\p}{\X,\pr}>0\}{\subseteq \Upsilon}$}  w.r.t. {$\x$} that is independent of $\p$. \label{assume:common_support}
\end{enumerate}
{
\begin{remark}
   Assumption~\ref{assume:non_singular}, although sometimes not stated explicitly (e.g., \cite[pages 33-35]{van2007bayesian}), is, in view of \eqref{eq:bfim}, a natural non-degeneracy assumption for the BCRB.
   %necessary for for the BCRB to be non-degenerate. 
\end{remark}
}

\begin{remark}
When $\X$ is a discrete random variable, a slightly altered set of
regularity conditions {can be used} \cite{zeitler2012bayesian}.
\end{remark}
  \begin{remark}
      In situations where {one or more of assumptions} ~\ref{ass:lim} %or 
      and ~\ref{ass:score_expection} is not satisfied, {both can be substituted by} the bias condition $\lim\limits_{\p\rightarrow\partial\Ps{^+}}\int_{\x}\brackets{\hat{\p}\brackets{\x}-\p}\probt{\x,\p}{\X,\pr}d \x =0$, {where $\hat{\p}\brackets{\x}$ is the estimator of $\p$ that is being considered.}
  \end{remark}
\end{assumption}
{Let $\hat{\p}\brackets{{\xset}}$ be an arbitrary estimator of $\p$  using %\hvhedit
{a set of i.i.d observations $\xset\triangleq\set{\X_i}_{i=1}^{\niideval}$ of $\X$}, with estimator error $\e=\p-\hat{\p}\brackets{%\hvhedit
{\xset}}$. Then, subject to Assumptions~\ref{assum:bcrb_reg},}
the following lower bound on the mean square error (MSE) matrix holds\cite{van2007bayesian}:
\begin{equation}\label{eq:bcrb}
\mathrm{MSE}\brackets{\e}\triangleq\expectation{\e\e^T}{\X,\pr}\succeq\bcrb\triangleq \fb^{-1},
\end{equation}
where $\bcrb$ is the Bayesian Cram\'er Rao Bound  (BCRB), and
\begin{align}\label{eq:bfim} &\fb\triangleq\expectation{\nabla_{\pr}\log\probt{\xset,\pr}{\xset,\pr}\nabla_{\pr}\log\probt{\xset,\pr}{\xset,\pr}^T}{\xset,\pr},\nonumber\\
        &=\expectation{\nabla_{\pr}\log\probt{\pr|\xset}{\pr|\xset}\nabla_{\pr}\log\probt{\pr|\xset}{\pr|\xset}^T}{\xset,\pr}
\end{align}
is the \emph{Bayesian Fisher Information Matrix} {(Bayesian FIM, or BFIM).}
{The second line} in \eqref{eq:bfim} {follows by the} product rule of probability and 
{the vanishing of} $\nabla_{\p}\log\probt{\x}{\X}=0$.
A different representation of the BFIM decomposes   it \cite{van2007bayesian} into the sum of two parts, the \emph{{measurement} {FIM}} {$\fm$} %\hvhedit
{of a \emph{single} observation vector $\X$} and the \emph{prior {FIM}} {$\fp$}:
\begin{equation}\label{eq:bfim_decomposition_base}
\fb=\niideval\cdot\fm+\fp=\niideval\cdot\expectation{\F\brackets{\pr}}{\pr}+\fp,
\end{equation}
where $\F\brackets{\p}$ is the \emph{non Bayesian {FIM}} {(whose inverse is the non-Bayesian CRB)} %\ybedit
{of a single observation vector $\X$} 
\begin{align} \label{eq:non-BayesFim}
\F\brackets{\p}\triangleq&\expectation{\nabla_{\p}\log\probt{\X|\p}{\X|\pr}\nabla^T_{\p}\log\probt{\X|\p}{\X|\pr}}{\X|\p}
\end{align}
\begin{equation} \label{eq:PriorFim}
 \text{and} \qquad  \fp\triangleq\expectation{\nabla_{\pr}\log\probt{\pr}{\pr}\nabla_{\pr}\log\probt{\X|\p}{\X|\pr}^T}{\pr} .
\end{equation}
% \todo[inline,color=green]{YB: Looks like we need an additional assumption here - either that $\log\probt{\x|\p}{\X|\pr}$ is differentiable w.r.t $\p$, or that $\log\probt{\p}{\pr}$ is differentiable w.r.t $\p$. If one of these is assumed, then the other follows from Assumption~\ref{sas:derivative}.\\
% \textbf{HVH:} What do you think about splitting Assumption~\ref{sas:derivative} into two ? one for the prior and one for the measurement? \\
% \textbf{YB:} This appears like a  more natural set of assumptions. Indeed, the existence of the gradients of the logs of the two PDF will imply the existence of the gradient $\nabla_{\p}\log\probt{\x,\p}{\X,\pr}$.\\

% However, with this split, it appears impossible to satisfy the assumption that  each of the elements of $\nabla_{\p}\log\probt{\x,\p}{\X,\pr}$ is absolutely integrable w.r.t $\x$ and $\p$ on $\Upsilon \times \Ps$ without listing this as a separate assumption. \\

% The reason is that we cannot require that each of the elements of $\nabla_{\p}\log\probt{\p}{\pr}$ be absolutely integrable w.r.t $\x$ and $\p$ on $\Upsilon \times \Ps$ when $\Upsilon$ is unbounded, because if the integral w.r.t $\p$ is  nonzero, then the integral w.r.t $\x$ will be unbounded.\\
% If I am right, then it will be simpler to just add the assumption that the log prior is differentiable. \\
% HVH: Added
%  }
We use the term \emph{Posterior Approach} to describe the computation of the BFIM according to \eqref{eq:bfim}, and the term \emph{Measurement-Prior Approach} to describe the computation 
%based on 
according to \eqref{eq:bfim_decomposition_base}.


% \hvhdelete{In many signal processing applications, the observation consists
% of \ybedit{a set $ \mathcal{\X}=\set{\tilde{\X}_{n}}_{n=1}^{\niideval}$ of} $\niideval$ independent and identically distributed (i.i.d.) samples \ybedit{
% % . To be more precise, consider the observation vector $\X=\tilde{\X}^{(1)}\oplus \hdots \oplus\tilde{\X}^{(\niiddata)}$, which is composed of $\niiddata$ i.i.d. samples
% $\tilde{\X}_{i} \hvhedit{\in\widetilde{\Upsilon}\subseteq} \mathbb{R}^\hvhedit{\tilde{\nx}}$} 
% %\hvhedit{where 
% of dimension $\tilde{\nx}=\frac{\nx}{\niideval}$, }

%\todo[inline,color=green]{
% This models the vector $\X$ as composed of multiple iid samples, and decomposes the FIM into the sum of respective FIMs. But in most signal processing application the situation is the \emph{opposite}: there is a model for a single sample, and then one considers the acquisition of a set of multiple such iid samples (e.g., by repeated acquisition). \\
% In the case of the CRB, the standard treatment is to multiply the (Fisher) FIM by number of iid samples.\\
% For the Measurement-Prior Approach you do the same in \eqref{eq:bfim_decomposition}. which makes perfect sense.\\
% I therefore think that in the Posterior Approach, where treatment of iid measurements individually is not accessible, the correct formulation is to consider the set of iid measurements $\mathcal{X}$ as an \emph{aggregate} of measurements, and to condition on $\mathcal{X}$. \\
% In short, rather than decomposing $\Xr$ into iid measurements, for the Posterior Approach we aggregate the iid measurements into $\mathcal{X}$. This eliminates the need for $\tilde{\Xr}$ and $\tilde{d}_x$, and for the associated PDFs and FIMs; the iid samples are simple $\Xr_n \in \mathbb{R}^{d_x}$.\\
% What do you think?\\
% HVH: In general I think it better presentation. 
% I will reorder this section and let see how this does. \\
% YB: good, you can fork-off a new version and keep the current one in case we decide we like it better.\\
% HVH: I think it simplify this section,  please let me know what you think? \\
% Yes, looks much cleaner.\\
% HVH: Thanks, I will go over the rest of the text now. 

% }
%is the dimension of $\tilde{\X}^{(i)}$ }, 
%\todo[inline,color=green]{YB: see my edit. Is this what you had in mind?\\
% HVH: A little bit different, please see change in the text.}
% \hvhdelete{each distributed as $\tilde{\X}^{(i)}\sim\probt{\tilde{\x}|\p}{\tilde{\X}|\pr}\quad\forall i$. This allows us to reformulate the BFIM \ybedit{for the estimation of $\p$ from $\mathcal{\X}$} as
% %in the manner described below:
% \begin{equation}\label{eq:bfim_decomposition}
% \fb=\niideval\cdot\fmt+\fp=\niideval\cdot \expectation{\tilde{\F}\brackets{\pr}}{\pr}+\fp,
% \end{equation}
% where $$\tilde{\F}\brackets{\p}\triangleq\expectation{\nabla_{\p}\log\probt{\tilde{\X}|\p}{\tilde{\X}|\p}\nabla_{\p}\log\probt{\tilde{\X}|\p}{\tilde{\X}|\p}^T}{\tilde{\X}|\p}$$ is the {non-Bayesian FIM} of a single sample.}