\section{{Experiments}}\label{sec:experimental}
\subsection{{Overview}}
We provide experimental results to show the benefits and {evaluate} the suggested \name{}. 
% \todo[inline,color=green]{I think that the correct terminology is "evaluate", or "study" -- "analyze the \name{}" $\rightarrow $ "evaluate the \name{}", "analyze the errors" $\rightarrow $ "evaluate the errors", "error analysis" $\rightarrow $ "study of the error"  etc.  The (theoretical)  "analysis" was done in Sec. V. This needs to be changed throughout. } 
We begin with a %
{study of the \name{} errors}  to show the effects of different errors on the learned bound quality using the simple linear Gaussian measurement model %that is specified in 
of Section~\ref{sec:linear_model}. This is followed by an {evaluation} of the \pe{} score neural network, where we present its benefits in terms of sample complexity, usability, and accuracy. Finally, we use the \name{} on two applications: 1) linear observation model with quantization; 2) the frequency estimation problem in underwater acoustics.  In these two problems, we obtain a learned BCRB that could not be obtained otherwise, showing the main benefit of the \name{}.    

\subsection{Setup}

%

{In all experiment we follow the same process. {To represent a general case,} we  begin the experiment by randomly generating once the static parameters of the measurement model (such as the mixing matrix $\matsym{A}$ and the covariance matrix $\matsym{\Sigma}$) as described in Appendix~\ref{apx:init_sp}. {These parameters of the measurement model are then kept fixed for all experiments.} 

With the measurement model in place, {we simulate the acquisition of a training data set $\ds$ by} randomly drawing $\nds$ 
%pairs of 
independent and identically distributed measurement set-parameter pairs for each SNR. Specifically, 
%
for each SNR, data generation involves initially sampling $\p_i$ from the prior distribution $\probt{\p}{\pr}$. Then, $\niiddata$ independent snapshots ${\x}^{(i)}$ are drawn from 
$\probt{\x|\p_i}{\X{|}\pr}$ for every $i \in [1,\niiddata]$ using the measurement model. This is done $\nds$ times to create the data for a specific SNR $\mathcal{D}$. Combining the data for all {$n_c$} SNR values %result in 
produces the dataset $\mathcal{D}_C=\set{\mathcal{D}_i,\mathrm{SNR}_i}_{i=1}^{n_c}$
of size %
{$N_T=n_c \cdot \nds$. 

}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includesvg[width=1.0\textwidth]{files/results/linear_v2/sampling_error_analysis_different_m_fim_high_snr.svg}
        \caption{Different $\np$}\label{fig:m_re_hist}
    \end{subfigure}%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includesvg[width=1.0\textwidth]{files/results/linear_v2/sampling_error_analysis_different_snr.svg}
        \caption{Different SNR}\label{fig:snr_re_hist}
    \end{subfigure}
    \caption{Histograms %
    {(using 1000 Monte-Carlo trials)} %Probability Density 
    of the \name{} relative errors % [\%]
    $\mathrm{RE}_{B}$ 
    \eqref{eq:inv_bound_post} and $\mathrm{RE}_{MP}$ \eqref{eq:inv_bound_mp} for a linear Gaussian example. }
    \label{fig:hist_sample}
\end{figure*}
{Instead of training a single score NN for each SNR, we use {the SNR as an additional conditioning variable.} Conditioning variables are used extensively  in various generative models \cite{mirza2014conditional,abdelhamed2019noise,liu2019conditional,ho2021classifier} and enable the training of a single generative model to operate in several conditions. In this study, we apply the SNR as a conditioning variable in the following manner.  }
During the training phase, %
{(similar to \cite{habi2024learning})} we optimize a single score
%-based neural network 
NN across $n_c$ signal-to-noise ratios, utilizing the SNR as a conditioning variable. 
%
{In inference, we evaluate the LBCRB for each SNR individually, {providing the value of the SNR as a conditioning input to the trained score NN.} This approach enables to improve the sample complexity {of learning the score NN}, since {the score functions for} different SNRs can be very similar.  } 


Denoting this conditioning variable as $\vectorsym{c}$,  the score models are expressed as $\postscore{\p}{\xsetr,\vectorsym{c}}$ and $\lscore{\x}{\p,\vectorsym{c}}$.
% \begin{equation}\label{eq:model_cond}
% \postscore{\p}{\x,\vectorsym{c}}=\lscore{\x}{\p,\vectorsym{c}}+\priorscore{\p}.
% \end{equation}
Note that the condition $\vectorsym{c}$ does not effect the prior score neural network. %{
In this study, {we use SNR as the conditioning variable $\vectorsym{c}$,
%is defined as the signal-to-noise ratio, 
but it can vary depending on the specific problem. }
Then, to {learn} the score models with the conditional variable, we use 
{the data set $\ds_C$ that also includes the conditioning variables, 
%, namely \hvhreplace{$\ds=\{\xsetr_n,\p_n,\vectorsym{c}_n\}_{n=1}^{\nds}$}{$\ds_C$}
with the objective functions \eqref{eq:score_post_mean} and} \eqref{eq:score_lik_mean} replaced, respectively, by the following.
}
% \todo[inline,color=green]{YB: here you have $\nds$ different values of $c$. I thought that you have only $n_c$ different values of $c$. Also, is the $\nds$ that is used here given by $\nds=n_c \cdot \nds^{c}$? I find all of this quite confusing. \\
% HVH: We have $n_c$ different SNR values (e.g. $-20dB,-18,..,30dB$, ) 
% For each $SNR$ value we generate $\nds^c$ measurement-parameter pairs
% Then we combine all the generated data to $\nds$ which train the score over all SNR values. 
% HVH: Please see update in the text.
% }



% \begin{subequations}
\begin{equation*}
    \lossbsm\brackets{\Omega}=\frac{1}{\nds}\sum_{\p,\xsetr,\vectorsym{c}\in\ds}\norm{\postscore{\p}{\xsetr,\vectorsym{c};\Omega}}_2^2+2\trace{\overline{\matsym{J}}_B},
\end{equation*}
\begin{align*}
    &\lossfsm\brackets{\paramf; \paramp}=\frac{1}{\nds\cdot \niiddata}\sum_{\xsetr,\p,\vectorsym{c}\in\mathcal{D}}\sum_{\x\in\xsetr}\norm{\lscore{\x}{\p,\vectorsym{c};\paramf}}_2^2\nonumber \\
    &+2\sum_{\xsetr,\p,\vectorsym{c}\in\mathcal{D}}\sum_{\x\in\xsetr}\frac{\lscore{\x}{\p,\vectorsym{c};\paramf}^T\priorscore{\p;\paramp}}{\nds\cdot \niiddata}+2\trace{\overline{\matsym{J}}_{F}},
\end{align*}
% \end{subequations}
where $\overline{\matsym{J}}_{F}\brackets{\paramf}\triangleq\frac{1}{\nds\cdot \niiddata}\sum_{\xsetr,\p,\vectorsym{c}\in\mathcal{D}}\sum_{\x\in\xsetr}{\frac{\partial \lscore{\x}{\p,\vectorsym{c};\paramf}}{\partial\p}}$ 
% \todoing{broken typesetting of the formula above}
and $\overline{\matsym{J}}_B\brackets{\Omega}\triangleq\frac{1}{\nds}\sum_{\p,\xsetr,\vectorsym{c}\in\ds}{\frac{\partial \postscore{\p}{\xsetr,\vectorsym{c};\Omega}}{\partial\p}}$ are the average Jacobian matrices. 

%After the 
Once training is completed, we evaluate 
% \todo[inline,color=green]{You mean that you perform the Evaluation Step of the BCRB - of computing $\hat{F}_\alpha$? Clarify, and give the equations used, because now the learned score functions include the conditioning variable $c$, which was not present in the previous parts of the paper. Also, as mentioned in the other comment, you need to clarify that this is done separately at each SNR, and what data is used for this at each SNR, and what is the setting of $c$\\
% HVH: Please see update text below}
the \name{} %
{at the $i^{th}$ SNR as follows}
\begin{equation*}
    \lbcrbbs^{(i)}=\brackets{\frac{1}{\nds}\sum_{\p,\xsetr\in\mathcal{D}_i}\postscores{\p}{\xsetr,\mathrm{SNR}_i}\postscores{\p}{\xsetr,\mathrm{SNR}_i}^T}^{-1}
\end{equation*}
\begin{equation}
    \lbcrblps^{(i)}{(\niideval)} =\brackets{\niideval\cdot  \lmfim^{(i)}+ \lpfim}^{-1},
\end{equation}
where 
{
        \begin{equation*}
     \lmfim^{(i)}\triangleq     \tfrac{1}{\nds\cdot \niiddata} \sum_{\p,\xsetr\in\ds_i} \sum_{\x\in\xsetr}\lscores{\x}{\p, \mathrm{SNR}_i}\lscores{\x}{\p, \mathrm{SNR}_i}^T   
        \end{equation*} 
        }
is the LBFIM of the $i^{th}$ SNR. It is important to note that we evaluate the \name{} under the same SNR conditions seen during training %
{i.e., we don't interpolate to unseen SNR values}. 

In all experiments, except where otherwise specified, we %consider 
assume that the only information available about the observation model is the data set $\ds$, used during the training and evaluation phases. %
{In addition, in all experiments, unless otherwise specified, we use the same set of hyperparameters. % unless otherwise specified. 
% \todo[inline,color=green]{What do you mean by "parameters"? Parameters of the NN? Or of the measurement model? Or the hyperparameters for training?\\
% HVH: Hyperparameters.}
In all experiments, the score neural networks are trained on 20 different SNR values with $\nds=60K$ samples 
% \todo[inline,color=green]{Not clear. $\nds= 60K$? Or $\nds \cdot \niiddata = 60 K$?} 
per SNR.  } The %reset of 
neural network architectures and
hyperparameters %and neural network architectures 
are detailed in Appendix~\ref{apx:param}.


% \hvhdelete{The detailed of the  conditional training provided in Appendix~\ref{apx:cond_score} . }



We %analysis 
evaluate the performance of the \name{} method in Sections~\ref{sec:error_anaylsis_exp} and \ref{sec:model_score_analysis}, where we study the  relative errors % 
{$\mathrm{RE}_{B}$ and $\mathrm{RE}_{MP}$ defined in \eqref{eq:inv_bound_post} and \eqref{eq:inv_bound_mp}} under various conditions such as SNR, parameter dimension, etc., and the mean ($\mathrm{mRE}$) relative error %which is given by:
% \todo[inline,color=green]{YB: you seem to describe two different quantities: (i) "mean relative error under different conditions" \textbf{and}  (ii) mean ($\mathrm{mRE}$) relative error (for which you give a definition below. If this is the case, what is the quantity in (i)?\\
% HVH: there is a was I typo in the text and I have added reference to the RE. }
\begin{equation}\label{eq:mean_re}
    \mathrm{mRE}\triangleq\frac{1}{n_c}\sum_{j=1}^{n_c}\mathrm{RE}\brackets{\mathrm{SNR}_j}.
\end{equation}

% \todo[inline,color=green]{If one subsection addresses one measurement model, then name this model in the subsection title, to improve readability and orientation} 


\subsection{{%Study of
Empirical-Mean Error with Linear Gaussian Observations}}\label{sec:error_anaylsis_exp}
We begin by examining the empirical-mean error %
{and the corresponding theoretical results Theorem~\ref{thm:sampling_post} and \ref{thm:sampling_mp}.}
% \hvhdelete{, derived by substituting the expectation in \eqref{eq:f_score} and \eqref{eq:split_fim_exp} with an empirical mean, as illustrated in \eqref{eq:mean_fully}, \eqref{eq:mean_efim_likd}, and \eqref{eq:bprior_mean}.} \todo[inline,color=green]{Not clear. Are you explaining the derivation here? Why? Don't you already have a definition of the "empirical mean error" in a single equation, to which you can refer?} 
To eliminate the {approximation error} in this experiment, we employ the Gaussian observation model 
% \todo[inline,color=green]{I did not find "the learned Gaussian model" in Section~\ref{sec:linear_model} } 
of Section~\ref{sec:linear_model} (with $\nx=16$ and a prior variance $\sigma_p=2.5$) and use the {true}
% \todo[color=green]{???} 
scores ($\vectorsym{s}_{F}^{0}$, $\vectorsym{s}_{P}^{0}$) in \eqref{eq:score_opt_linear}.  

We evaluate the \name{} with a dataset size of $\nds^{{c}}=64k$ with $\niiddata=\niideval=10$ 
% \todo[inline,color=green]{$\niiddata=??$, $\niideval=??$} 
and compute the relative errors 
$\mathrm{RE}_{B}$ \eqref{eq:inv_bound_post} and $\mathrm{RE}_{MP}$ \eqref{eq:inv_bound_mp}
% \todo[inline,color=green]{insert math symbol for it} 
 compared to the BFIM expression. This procedure is repeated 1000 times, and the {histogram} is shown in Figure~\ref{fig:hist_sample} 
 % \todo[inline,color=green]{Broken figure reference} 
 for various sizes of the parameter vector $\np$ (at SNR $=0dB$) and different SNR values (with $\np=4$).

From Figure~\ref{fig:m_re_hist}, we observe that a large $\np$ leads to {increased} error, {in agreement with the predictions} 
% \todoin{YB: changed "which supports the results in Theorems ..." to "in agreement with the predictions in Theorems ...", because theorems, which are a mathemtical truth, do not need "support". } 
in Theorem~\ref{thm:sampling_mp} and Theorem~\ref{thm:sampling_post}. In the case of SNR, Figure~\ref{fig:snr_re_hist} shows that a low SNR results in a high empirical-mean error. 
% \todo[inline,color=green]{"sampling error" $\rightarrow$ "empirical-mean error", change throughout the paper.}
% \todo[inline,color=green]{Capitalize: "Measurement-Prior Approach", check throughout.}
% \todo[inline,color=green]{Capitalize: "Posterior Approach", check throughout.}
Furthermore, both Figures~\ref{fig:m_re_hist} and \ref{fig:snr_re_hist} demonstrate that the Measurement-Prior
Approach yields smaller errors than
%compared to 
the Posterior Approach. 

% \hvhreplace{This is attributed to the cross term between the prior and Fisher score being zero in the measurement-prior method, whereas in the Posterior Approach, this term approaches zero as the sample size increases.}{This is due to the Measurement-Prior Approach capability to use additional i.i.d $\niiddata>1$, as discussed in Section~\ref{sec:empirical_mean_error}. } \todo[inline,color=green]{Replace the explanation in terms of "cross-terms" by a pointer to the explanation/prediction provided after Thm.~\ref{thm:sampling_mp}? }

To further validate the theoretical results on the empirical-mean error, %theoretical, 
we evaluated the \name{} with various data set sizes from {$\nds=$}32k to  {$\nds=$}256k using the following parameters: $\np=4$, $\nx=16$, {$\niiddata=\niideval=10$} and $SNR=20dB$. We repeat this process 1000 times for each size of the data set and present the mean and standard divination in Figure~\ref{fig:sample_error_vs_n_samples}. In addition to the empirical standard deviation, we 
{plot the} theoretical bounds %presented 
in \eqref{eq:re_error_post_mean} and \eqref{eq:re_error_mp_mean} {(which can be computed in this case\footnote{ When analytical expression {are not available} the bound can be approximated by replacing the expectation with an empirical mean.})}.
\begin{figure}
    \centering
    \includesvg[width=0.8\linewidth]{files/results/linear_v2/sampling_error_analysis_vs_n_samples_fim_high_snr.svg}
    \caption{{%Analysis of the sampling 
    Empirical-mean errors  %\eqref{eq:re_error_post_mean} 
    $\mathrm{RE}_{B}^{(e)}$ and $\mathrm{RE}_{MP}^{(e)}$ {vs. $\nds$}
    for the linear Gaussian estimation problem {with $\niiddata=10$}. 
    %{The x-axis represent the number of samples and the y-axis represent the relative error. }%{
    Upper part: %of this Figure, 
    %the 
    theoretical bounds \eqref{eq:re_error_post_mean} and \eqref{eq:re_error_mp_mean}. % on the sampling error .
    %are shown. While the lower 
    Bottom part: % of this Figure, 
    average and standard deviation of the actual empirical-mean error over 1000 Monte-Carlo trials. 
    }
    }
    
    \label{fig:sample_error_vs_n_samples}
\end{figure}

\begin{figure}
    \centering
    \includesvg[width=0.8\linewidth]{files/results/linear_v2/sampling_error_analysis_vs_m_iid_high_snr.svg}
    \caption{
    % \todo[inline,color=green]{YB: edit caption similar to that of Fig. 4} 
    Empirical-mean errors $\mathrm{RE}_{B}^{(e)}$ and $\mathrm{RE}_{MP}^{(e)}$ vs $\niiddata$
    for the linear Gaussian estimation problem. 
    Upper part: 
    theoretical bounds \eqref{eq:re_error_post_mean} and \eqref{eq:re_error_mp_mean}. 
    Bottom part: 
    average and standard deviation of the actual empirical-mean error over 1000 Monte-Carlo trials. 
    % \todo[inline,color=green]{HVH:Update caption and text\\
    % HVH: Please see the next text related to this result.}
    }
    \label{fig:sample_error_vs_m_iid_samples}
\end{figure}

From Figure~\ref{fig:sample_error_vs_n_samples} we observe that the error decreases with increasing number of samples $\nds$. In addition, comparing to the theoretical bound, %behavior, 
we observe that both the theoretical and empirical results decrease with the same rate. %
{Additionally, this result shows that the Measurement-Prior Approach surpasses the Posterior Approach  in terms empirical-mean error in these scenarios. 
%For further 
To study this further, we performed a similar experiment with 
%changing of 
$\nds^{{c}}=64k$ 
%and 
using various values of $\niiddata=\niideval$, %and the
with results %is 
shown in Figure~\ref{fig:sample_error_vs_m_iid_samples}. 

Figure~\ref{fig:sample_error_vs_m_iid_samples} reveals that, as $\niiddata$ grows, the empirical-mean error in the Measurement-Prior Approach diminishes, whereas in the Posterior Approach the error remains unchanged. This observation 
{is predicted by} the bounds in Theorems~\ref{thm:sampling_post} and \ref{thm:sampling_mp}, and the discussion that followed them. {With reference to Theorems~\ref{thm:sampling_post} and \ref{thm:sampling_mp}, Figure~\ref{fig:sample_error_vs_m_iid_samples} shows that $\frac{\cb}{\trace{\fb}}$ stays constant as $\niiddata$ increases, whereas $\frac{\niiddata\cb +\cp}{\niiddata\trace{\fm}+\trace{\fp}}$ decreases.  } 
Moreover, this highlights an additional benefit of using the Measurement-Prior Approach to %improve 
reduce the empirical-mean error.  


{Finally, we explore the effect of different $\niideval$ on the empirical-mean error, further validating the theoretical results. To this end,
%achive this goal 
we evaluate the LBCRB with various of $\niideval$ values using the following parameters 
(
$\nds=64k$, $\np=4$, $\nx=16$, %
{$\niiddata=10$} and $SNR=-50dB$\footnote{
Note that at  higher SNRs $\niideval$ has little  effect on the empirical-mean error, 
{%since the dominant part is the measurement FIM. %
as predicted by
%this can be observed in 
} Theorem~\ref{thm:sampling_mp}, 
where the upper bound on {$\mathrm{RE}_{MP}^{(e)}$ depends on %is proportional to  %$c_m+\frac{c_p}{\niideval}$ 
 $(\niideval c_m+c_p)\Big/ \left(\niideval\mathrm{Tr}(\tlmfim)+\mathrm{Tr}(\tlpfim) \right) $. Now, $\cm$ and $\mathrm{Tr}(\tlmfim)$ both scale up identically with SNR, so that at high SNR they dominate, and the effect of $\niideval$ %cancels %between
 in the numerator and denominator cancels.
} 
% and by selection low SNR $c_m\approx 0$ then the effect $\niideval$ is dominate. 
}  
). 
%
% \todo[inline,color=green]{YB: This footnote is unclear. As $\niideval$ multiplies the measurement FIM, it would seem that the effect of $\niideval$ on the BCRB is larger at high SNR. Explain how one sees that the opposite effect on the empirical-mean error should be true.\\
% HVH: Please see the reference to Theorem ~\ref{thm:sampling_mp}}
We repeat this process 1000 times for each size of the data set and present the mean and standard divination in Figure~\ref{fig:sample_error_vs_n_iid}. In addition to the empirical standard deviation, we plot
% \todo[inline,color=green] {YB: "we present a line of" is more compactly said as "we plot". Change throughout.\\
% HVH: I didn't find any additional point} the theoretical bound %presented in 
\eqref{eq:re_error_mp_mean}.  }
\begin{figure}
    \centering
    \includesvg[width=0.8\linewidth]{files/results/linear_v2/sampling_error_analysis_vs_n_iid_-50.0.svg}
    \caption{
    %\todo[inline,color=green]{Increase the fonts in the figure?}
    {%Analysis of the sampling 
    Empirical-mean \name{} error $\mathrm{RE}_{MP}^{(e)}$ \eqref{eq:re_error_mp_mean} 
    %on a 
     for the linear Gaussian estimation problem,  vs. the assumed number $\niideval$ of i.i.d measurement samples.
        The Measurement-Prior Approach is able to compute the \name{} for 
    any  $\niideval$ %of i.i.d samples 
    without more data or retraining.   
    %. The x-axis represent the number i,i.d sample to evaluate $\niideval$ and  the 
    Left y-axis:  theoretical bound (red). % and the
    Right y-axis:  actual $\mathrm{RE}_{MP}^{(e)}$ %error 
    (blue); average and standard deviation over 1000 Monte-Carlo trials.% are shown.
    }
    }
    \label{fig:sample_error_vs_n_iid}
\end{figure}

The results in Figures~\ref{fig:hist_sample}, \ref{fig:sample_error_vs_n_samples}, \ref{fig:sample_error_vs_m_iid_samples} and  \ref{fig:sample_error_vs_n_iid}
% \todo[inline,color=green]{YB: why are only figures 3 \& 4 mentioned in this summary statement? What about Figs. 5 and 6?}
{(which do not include approximation error)} validate the theoretical findings in Theorems~\ref{thm:sampling_post} and \ref{thm:sampling_mp}. 
% \ybdelete{without {approximation error}.}

\subsection{%
{%Study of 
Approximation Error}}
%
{This section examines the approximation error in two scenarios: (i) %when the 
{multiple i.i.d samples $\niideval>1$ (to compare between the two approaches we require that dataset $\ds$ includes multiple i.i.d. samples  $\niiddata=\niideval$); } 
% \todo[inline,color=green]{YB: How can you separate here the effect of $\niiddata>1$ on the approximation error from its effect on the empirical-mean error, which we discussed earlier? \\
% HVH: I think that, if it has just the effect of the approximation error the Posterior method should have constant performance as shown in Figure.5 while it actual increases. So it does shown the benefit of MP compare to Posterior no?\\ \textbf{ YB:} Good point, so explain this to the reader? Also, in the discussion of Fig. 5, you can explain that $\cb$ does not increase with $\niiddata$ - although one might conclude this from the relation $\cb > \niiddata \cm + \cp$ \\
% HVH:
% 1. Please see text below that explain this point. \\
% 2. About $\cb$, I think that only $\frac{\cb}{\fb}$ is constant and not $\cb$ by it self. Do you agree?  I add some text but I am not sure about the part of measurement-prior approach is true for any case
% }
and (ii) with the use of a \pe{} score NN.
%neural network.  
}

\subsubsection{%
{%Study of multiple I.I.D Samples with a 
Linear Gaussian Observations %Model 
with $\niideval>1$}}
%
% \ybdelete{Here, we study the advantages %benefits 
% of the Measurement-Prior Approach when $\niidevala>1$.}
% \todo[inline,color=green]{YB: you only show in the experiment the total error of the BCRB, which combines the approximation error and the empirical-mean error. Hence, it is impossible to know which is actually reduced by $\niiddata>1$. But because you know the true score functions for this case, can you compute the score mismatch error itself, and thus determine the approximation error itself? \\
% HVH: Does my addition text below is enough ? }
To illustrate the advantages %benefits 
of 
%benefits of 
{the Measurement-Prior Approach when the LBCRB is computed for} {$\niideval>1$} i.i.d. samples, %
%{$\niiddata>1$}
%
% \todoing{YB: is the issue here $\niiddata>1$, or $\niideval>1$? As this study concerns the approximation error, what matters is the dimension of the input to the Posterior score NN, which is determined by $\niideval$. As we stated before, even if $\niiddata>1$, we could compute the LBCRB for $\niideval=1$, and then we would not see the advantage of the MP-Approach.\\
% Hence, how about modifying the discussion to $\niideval>1$, and modifying the above sentence to:
% "To illustrate the advantages of the Measurement-Prior Approach when $\niideval>1$ ... " \\
% HVH: Yes I agree, please see updated text.} 
we performed the following experiment on the linear Gaussian observation model of Section~\ref{sec:linear_model} (%
{with parameters $\np=4$, $\nx=10$,  and SNR values uniformly spaced between -20dB to 20dB}). We trained two different score models: one employing the Posterior Approach {with $|\xset|=\niideval=\niiddata$,  %(excluding knowledge regarding i.i.d. samples) 
}
and the other {employing} 
% \todo[inline, color=green]{when describing the same concept twice, use the same word to clarify that this is the same concept- don't switch to a synonym}
the Measurement-Prior Approach 
{that computes the measurement score and FIM for a single sample, but evaluates the BCRB for $\niideval=\niiddata$. This was repeated} 
%(including knowledge regarding i.i.d. samples) 
with various numbers (1, 2, 3, 4, and 5) of i.i.d. samples %$\niiddata$
$\niideval$, with
%The 
results 
%are 
presented in Figure~\ref{fig:k_study}. 

\begin{figure}
    \centering
    \includesvg[width=0.8\linewidth]{files/results/linear_v2/k_study.svg}
    \caption{Mean Relative Error 
    {%[\%]
    $\mathrm{mRE}$} 
    \eqref{eq:mean_re} 
    {%over various numbers of 
    vs.} number $\niideval$ of i.i.d. samples { per $\p$ value in the training data set $\ds$.
    }
    } \label{fig:k_study}
\end{figure}

The results indicate that as $\niideval$  increases, the mean relative error %
{$\mathrm{mRE}$} 
%\todo[inline,color=green]{Give math symbol} %
{defined in \eqref{eq:mean_re}} {mostly} decreases for the Measurement-Prior Approach, 
% \todoing{YB: Added "mostly", to account for the increase for $\niiddata=5$. As a matter of curiosity --  do you know why this relative increase? By Fig. 5 the empirical-mean error of the MP-Approach decreases with increasing $\niiddata$, so looks like the approximation error increased at $\niiddata=5$? \\
% HVH: I think it related to hyper-parameter tuning of training, but it is a very small error (around 1\%) .  
% }
whereas it increases for the Posterior Approach. %
{We emphasize that the empirical-mean error does not influence this outcome in Figure~\ref{fig:k_study}. 
We can conclude this from %Since
Figure~\ref{fig:sample_error_vs_m_iid_samples}, which shows that as $\niiddata$ increases, the Posterior Approach empirical-mean error is essentially constant,
%remains stable, 
whereas Figure~\ref{fig:k_study} shows an increase in combined error.  }

The cause of this behavior of {the approximation error} lies in the Posterior Approach requirement to incorporate {all measurement samples, whether %additional 
i.i.d. or not, %samples 
jointly as a combined measurement $\xset$ }through a score NN
%neural network (NN) 
with a larger input dimension. This increases the NN's capacity, %
and as a consequence, the sample complexity for its training increases too. {(This is trivially evident in the example of this experiment, where} the score NN is a fully-connected linear layer, with number of parameters determined by the input and output dimensions.)
% \todo[inline, color=green]{Edited to first state the general observation, then the special case in parenthesis.}


% \todo[inline,color=green]{YB: again, how can you claim that this is the reason for the observed results, when the alternative explanation is the reduction of the empirical-mean error for the MP-Approach? \\
% HVH: I an not sure what is the claim you are referring to?\\
% \textbf{YB:} claim = "\emph{The cause} of this behavior lies in the Posterior Approach requirement to incorporate ..." \\
% HVH: Figure 7 shows that as $\niiddata$ grows, the Postier method's error rises. Conversely, Figure 5 illustrates a constant error with increasing $\niiddata$ when there is no approximation error. Additionally, in this straightforward linear model, the number of parameters rises as more i.i.d. samples are added, meaning that the sample complexity increase as well. Do this make sense? \\
% \textbf{YB:} yes, but as in my previous  comment on the same subject, this needs to be explained to the reader. (once would be enough).
% \textbf{HVH:} Please see add text.
% } 
Conversely, in the Measurement-Prior Approach, the NN maintains a constant input dimension and operates on each i.i.d. sample independently, ensuring that the model capacity remains unchanged. A further advantage of the Measurement-Prior Approach is that the NN does not need to adapt to the presence of $\niideval$ i.i.d. samples, while in the Posterior Approach it must infer {the fact that they are i.i.d.} from the dataset. Consequently, in the Measurement-Prior Approach, increasing $\niideval$ has a similar effect to increasing the size of the dataset $\nds$.

\subsubsection{% {Study of 
\pe{}
Score NN
%{Neural Network 
with 
the Frequency Estimation Model}\label{sec:model_score_analysis}
\begin{figure}
    \centering
    \includesvg[width=0.4\textwidth]{files/results/freq_est_v4/error_ablation_samples_1.svg}
    \caption{\name{} Mean relative error $\mathrm{mRE}$ \eqref{eq:mean_re} vs.  dataset set size $\nds$ with $\niiddata=\niideval=1$  using the Measurement-Prior Approach with a score NN with (solid line) and without (dashed line) {physics encoding}.    }
    \label{fig:analysis_mi}
\end{figure}
To %understand 
study the benefits of the \pe{} score neural network for the \name{}, {we use the Measurement-Prior Approach} on the frequency estimation problem model of Section~\ref{sec:freq_est} (with parameters $\nx=16$, $\niiddata=1$ and $\alpha_\omega=100$).
{For the Fisher Score we} train, for comparison,  two different score neural networks: 
% \todo[inline,color=green]{Wrong section reference. Also, the frequency estimation model does not have such parameters. I guess you have yet to update the frequency estimation example.\\
% HVH:Corrctted the notation of this example in this part. }
(i) {%One score NN employs a measurement prior only 
a regular NN, without 
%\hvhreplace{model information}{\pe{}} 
Physics-encoding;
and (ii) {%another 
a NN with  
%\hvhreplace{model information}{\pe{}}. 
Physics-encoding.}
{In the context of this example,} by 
\pe{} by we mean that the structure of the problem is known, but the noise covariance is unknown.} Then the Fisher score function has the form of \eqref{eq:model_base_score} with the model function $\squareb{\mathcal{M}\brackets{\p}}_n=\cos{\brackets{\omega n }}$. 


We train the two score NNs
%neural networks 
using datasets of varying sizes $\nds$: 1200K, 120K, and 12K samples. 
The Fisher {score} 
%neural network 
NNs
 {without Physics-encoding} consists of a three-layer multilayer perceptron (MLP) that uses Swish\cite{ramachandran2017searching} activation functions between layers, incorporating conditional inputs such as SNR and $\p$. In contrast, the \pe{} Fisher score 
%neural network 
NN {has} 
% \todo[inline, color=green]{The term "features" in the context of NN can be confusing when used instead of "has", "uses".} 
just one layer of MLP without any nonlinear activation function. 

We perform an {evaluation of}  the \pe{} {score neural network} to highlight its advantages, particularly in terms of sample complexity.
% \todo[inline,color=green]{YB: As only aggregate (mean over SNR) is shown, how does this highlight the impact across different SNRs? Perhaps just change the language - no need for additional experiments or plots.\\ 
% HVH: leftover from Figure I removed, we dont see this in this FIgure.}
The results are shown in Figure ~\ref{fig:analysis_mi}. 
% \todo[inline,color=green]{Update Figures from model-informed to \pe{}-> Figured is updated}
 % \todo[inline,color=green]{Include Math symbol for the various  errors, not just equation reference. Reason: the reader may remember the math notations, and this will save her the need to go back to the defininig eqns.\\
 %    HVH: I removed the two sub-figure because one is inconsistent with the frequency estimation problem defined in this paper.
 %    and the second I am not sure about if it true phenomena or a result of a specific training schema.  } 
    
% \hvhdelete{First, Figure~\ref{fig:snr_re_model_inf} shows the relative error across 20 SNR values from -30dB to 10dB, obtained when training with 1200K samples. The data indicate that the \pe{} approach offers significant advantages at high SNR levels, where the measurement Fisher Information Matrix (FIM) significantly influences the BCRB. Subsequently,} 

Figure~\ref{fig:analysis_mi}  
%illustrate 
shows
the mean relative error $\mathrm{mRE}$ \eqref{eq:mean_re}
% \todo[inline,color=green]{give math symbol and eqn ref?} 
{of the \name{}}  vs. dataset size $\nds$. These results reveal that the \pe{} {score} neural network consistently surpasses the regular model, particularly with the smaller dataset size of $N_T=12K$ (comprising $\nds=600$ samples per SNR value). 

% \hvhdelete{ Furthermore, a comparison of Figures ~\ref{fig:re_vs_ds_k1} and ~\ref{fig:re_vs_ds_k4} indicates that increasing the interpolate number of i.i.d. samples $\niideval$ leads to a substantial increase in error for the uninformed model, while the increase in error for the \pe{} approach is comparatively minimal.}

\subsection{Example Use Cases of the \name{} }
In this section, we consider
%present the results of 
two signal processing problems in which calculating the BCRB is impractical. These examples underscore the primary advantage of \name{}, which %facilitates 
enables the determination of a lower bound on Bayesian estimation when both prior and measurement distributions are unknown or only partially known. 
% \ybdelete{The first problem is one-bit quantized measurement with %alongside 
% correlated noise. Despite its seemingly straightforward nature, the score function cannot be calculated analytically. The second problem is frequency estimation with {ocean} subsurface ambient noise. }

\subsubsection{One-Bit Quantization}
%\todo[inline,color=green]{Break into paragraphs, for readability}
{We consider the observation model of Section~\ref{sec:quantization_example} with correlated noise and one-bit quantization. }Despite its seemingly straightforward nature,  the score function for this observation model \emph{cannot be calculated analytically,} {even when the measurement model (matrices $\matsym{A}$ and $\matsym{\Sigma}$ and quantizer model) are known. Furthermore, we consider this problem when both measurement model and prior on $\p$ are  \emph{completely unknown}.
%Here, we present the result in a quantize observation model where the score function cannot be computed analytically. 
}
This emphasizes the main benefit of the \name{}, which can be used without any knowledge about measurement or prior distributions.  




% \ybdelete{In this experiment, we investigate the quantization observation model (Section ~\ref{sec:quantization_example}) in the case where the noise is correlated with the correlation factor $\rho$. Specifically,}
{For the noise correlation, we set $\matsym{\Sigma}$ in the measurement model such that
%we assume that 
}
the noise components in $\randomvec{W}$ are correlated pairwise with correlation factor $\rho$, namely $\squareb{\matsym{\Sigma}}_{i,j}=\rho \sigma_i\sigma_j \quad\forall i\neq j$ and $\squareb{\matsym{\Sigma}}_{i}= \sigma_i^2\quad \forall i$  . 
% \todo[inline,color=green]{Do you mean that this is for $i \neq j$, and   $\matsym{\Sigma}_{i,i}= \sigma_i^2$? So this is a diagonal noise covariance + a rank-one covariance (which accounts for the correlations)?\\
% HVH: Yes, see change in the text.
% } 
{%In this observation, we compute 
The LBCRB  computed at %various 
two {representative values of $\rho$, $\rho=0$ (uncorrelated noise), and  $\rho=0.9$}
is shown}
% and present the result 
in Figure~\ref{fig:q_res}.

{To assess the learning error of the LBCRB in this (highly) non-linear problem,} we evaluate the LBCRB using the true score functions  $\lscore{\x}{\p}=\vectorsym{s}_F^0$ and $\priorscore{\p}=\vectorsym{s}_P^0$ for uncorrelated noise $\rho=0$. {(Recall that 
 for $\rho=0$ and known model parameters, the true score function $\vectorsym{s}_F^0$ is available analytically.)} 
  % \ybdelete{ First, {to shown the study error of the LBCRB in a non-linear problem}.} 
  It is evident {in Figure~\ref{fig:q_res}} that the LBCRB with the true score {essentially} coincides with the LBCRB with the learned score, indicating {%that 
  the high accuracy of the learned LBCRB. %shows small error.
  }
% \todo[inline,color=green]{The above paragraph appears irrelevant here? Does it belong somewhere else?\\
% HVH: I move here and exted a littel bit. }

%From this result, 
We observe that the bound plateaues at low and high SNRs where the bound is dominated by the prior FIM (marked in Figure~\ref{fig:q_res} as the "no information region}). This is because at very high SNRs {the information conveyed by the quantized measurements is greatly reduced} as shown in \cite{9664619} in non-Bayesian cases. {(Intuitively, the noise serves as dithering before quantization, which is known to help overcome quantization error.)} Here, since we are investigating the Bayesian bound, in this region the prior FIM becomes the dominant part, thus the bound plateaues. By examining the mid SNR range (between -8dB to 8dB) we observe that increasing the correlation factor $\rho$ reduces the bound (this was validated over several $\rho$ values). 

{To gain further insight and demonstrate the effectiveness of the \name{}}, we compare to the performance of an approximation of the MMSE estimator, implemented using a neural network.
%We approximate the MMSE estimator by employing a neural network. 
Specifically, we train a neural network to estimate $\p$ from $\X$ 
{%while optimizing 
using the mean square error as the loss function for training}. We find that {with uncorrelated noise}, the MMSE estimator attains the LBCRB across all SNRs. Conversely, in the presence of correlated noise, the MMSE meets the bound in the no-information region, but in the mid SNR range it does not always achieve the bound.
{This suggests that in the latter case, there is room for improvement over our approximate MMSE estimator.}



\begin{figure}
    \centering
    \includesvg[width=0.8\linewidth]{files/results/qlinear_v2/score_bcrb_vs_snr_over_rho_values_with_mmse.svg}
    \caption{\name{} for the linear Gaussian model with 1-bit quantized measurement in %Examining 
    two case:(i) with correlated noise ($\rho=0.9$); and (ii) uncorrelated noises ($\rho=0$). 
    The two no-information region are marked in red. "True score" stands for evaluating the LBCRB without any {approximation error}.  %\todo[inline,color=green]{change optimal to true} 
    }
    \label{fig:q_res}
\end{figure}
In conclusion, we  highlight the advantages of employing Fisher score matching {in quantized measurement scenarios such as the one considered in this experiment.} Previous research \cite{habi2022generative} %achieves 
obtains a non-Bayesian learned bound for quantized measurement by using normalizing flows. However, this approach requires an extra approximation via dequantization of the measurement distribution. This requirement arises because normalizing flows depend on an invertible mapping between the data distribution and a manageable base distribution, often a standard Gaussian. Instead, by learning the Fisher score function, we eliminate the need for this additional approximation.
In Bayesian contexts, normalizing flows can be employed to learn the posterior distribution and subsequently derive the posterior score. However, as demonstrated  in {the next subsection,} this method does not take advantage of  additional side information on the measurement model.
\subsubsection{Frequency Estimation }
% \todo[inline,color=green]{Break into paragraphs, for readability}
We consider the frequency estimation problem of Section~\ref{sec:freq_est}. We train a \pe{} score neural network on a frequency estimation problem (with parameters $\nx=16$, $\niiddata=1$ and $\alpha=100$)
%, see Section ~\ref{sec:freq_est}) 
with  ocean underwater ambient noise. We assume that the cosine dependence $\squareb{\mathcal{M}\brackets{\p}}_n=\cos\brackets{{\p}\cdot n}$ is known, and the %neural network 
NN only requires to learn the underwater ambient noise. The NN
%neural network 
is a five-layer multilayer perceptron (MLP) that uses Swish\cite{ramachandran2017searching} activation functions between layers, incorporating conditioning inputs such as SNR and $\p$. We compare the \name{} with the underwater ambient noise to the BCRB with white Gaussian noise (WGN) when both noises have the same variance. The various bounds  on the {frequency estimate of $\p$} are shown in Figure~\ref{fig:under_weahter_freq}.


\begin{figure}
    \centering
    \includesvg[width=0.8\linewidth]{files/results/freq_est_v3/frequency_error.svg}
    \caption{{%Results on 
    Bounds on frequency estimation with real ocean underwater noise (UWN) vs. Gaussian noises. The \name{} for white Gaussian noise (WGN) coincides with the analytical BCRB for WGN.
    %while comparing Gaussian noise and underwater ambient noise. 
    }
    % \todoing{In the legend, replace "LBCRB Gaussian" $\rightarrow$ "LBCRB WGN" and "BCRB" $\rightarrow$ "BCRB WGN" }
    % 
    }
    \label{fig:under_weahter_freq}
\end{figure}



We observe that {the \name{}}
with underwater ambient noise {is uniformly lower than the BCRB for 
%Gaussian noise 
WGN of the same variance}. %Those results are 
This is expected, since WGN yields the largest CRB for a given noise variance \cite{stoica2011gaussian}. 

%
%Moreover, 
For further comparison, 
we %introduce 
compute the LBCRB for the same problem,
%in a scenario involving 
but with Gaussian noise having the same covariance matrix as the underwater noise. 
{The resulting bound (denoted in Figure~\ref{fig:under_weahter_freq} by "Gaussian (UWN Covariance)" lies below the BCRB for WGN, but above the \name{} for underwater noise.}  

%
{First, we observe that the LBCRB and BCRB with WGN coincide, %with each other 
%which show the small approximation error
demonstrating the good accuracy of the LBCRB. }
{Moreover, } {the comparison between the \name{} with the Gaussian (UWN Covariance) noise and the \name{} with the underwater noise} demonstrates a situation where the LBCRB {with Underwater noise} 
provides a more accurate bound {than would be obtained under the Gaussian assumption.} This potentially highlights an area for improvement in estimation {algorithms in such scenarios.} 
