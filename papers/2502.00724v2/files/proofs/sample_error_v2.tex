\subsection{Proofs of 
%Sampling 
Empirical-Mean Error}\label{apx:proof_sample_error}
{We will use the following result, which is an easy corollary of} "Matrix Bernstein: Hermitian Case with Intrinsic Dimension" \cite[Chapter 7]{tropp2015introduction}.
% \todo[inline,color=green]{YB: Made this a Proposition - so that it can have a corollary.}
\begin{prop}
    [Matrix Bernstein {for Spectral Norm}: Symmetric Case with Intrinsic Dimension]\label{prop:bernstein}
    Let $\matsym{X}_k$ be a finite set of random symmetric matrices, $\matsym{Y}=\sum_{k}\matsym{X}_k$ be a sum over the finite set,  $$\matsym{V}\succeq \expectation{\matsym{Y}^2}{}=\sum_k\expectation{\matsym{X}_k^2}{},$$ 
    be a semi-definite upper bound, and $\nu=\norm{\matsym{V}}_2$ its {spectral} norm.     Assume that:
    $$\expectation{\matsym{X}_k}{\matsym{X}_k}=0\quad\mathrm{and} \quad \norm{\matsym{X}_k}_2\leq L \quad\forall k,$$
     then for $t\geq \sqrt{\nu}+\frac{L}{3}$,
   \begin{equation}\label{eq:lemma_bernstein_prob}
        \mathbb{P}\brackets{\norm{\matsym{Y}}\geq t}\leq 8 d_i
        \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}},
    \end{equation}
    where $d_i=\mathrm{intdim}\brackets{\matsym{V}}\triangleq\frac{\trace{\matsym{V}}}{\norm{\matsym{V}}_2}$ is the Intrinsic Dimension of matrix $\matsym{V}$. In addition the Expectation Bound is given by:
    \begin{align}\label{eq:lemma_bound_expection}
        \expectation{\norm{\matsym{Y}}_2}{\ds}&\leq \sqrt{\nu}\brackets{4+\sqrt{2\log\brackets{1+2d_i}}} \nonumber\\ 
        &+\frac{2L}{3}\brackets{4+\log\brackets{
        1+2d_i}}
    \end{align}
    
\end{prop}
\begin{proof}
    {The relation} 
    $\norm{\matsym{Y}}_2=\max\brackets{\lambda_{max}\brackets{\matsym{Y}},\lambda_{max}\brackets{-\matsym{Y}}}$ %and 
    together with the union bound {yield}
    \begin{align*}
        \mathbb{P}\brackets{\norm{\matsym{Y}}_2\geq t}\leq\mathbb{P}\brackets{\lambda_{max}\brackets{\matsym{Y}}\geq t}+\mathbb{P}\brackets{\lambda_{max}\brackets{-\matsym{Y}}\geq t}.
    \end{align*}
    {Applying} Matrix Bernstein: Hermitian Case with Intrinsic Dimension from \cite[Chapter 7]{tropp2015introduction} twice {establishes}  Proposition~\ref{prop:bernstein}. 
\end{proof}
{Given some $u>0$,} we wish to bound  $\norm{\matsym{Y}}<t$ with probability at least $1-\exp\brackets{-u}$. {This is provided by the following corollary of Proposition~\ref{prop:bernstein}}.
\begin{corollary} \label{cor:bernstein}
Let
\begin{flalign} %\label{eq:t0_bound}
t_u \triangleq &{\frac{bL}{3}}\brackets{1+\sqrt{\frac{{18}\nu}{bL^2}+1}}, 
\text{ where }  b= {u+\log\brackets{8d_i}}. \nonumber
\end{flalign}
Then, subject to the assumptions of Proposition~\ref{prop:bernstein},
    \begin{equation} \label{eq:t_bound}
    \mathbb{P}\brackets{\norm{\matsym{Y}}\leq t_u} \geq 1-\exp\brackets{-u} %\quad \forall t\geq t_0 
    .
    \end{equation}
\end{corollary}
\begin{proof}
    Consider the event complementary to that in \eqref{eq:t_bound},
%
% \ybreplace{, for any $u>0$, meaning {$\mathbb{P}\brackets{\norm{\matsym{Y}}< t}\geq1-\exp\brackets{-u}$,} and  using the following relation 
% \begin{align}\label{eq:prob_exp_releation}
%     1-\exp\brackets{-u}\leq\mathbb{P}\brackets{\norm{\matsym{Y}}< t}=1-\mathbb{P}\brackets{\norm{\matsym{Y}}\geq t},
% \end{align}
% we have:}{or for the complementary event}
\begin{align}\label{eq:apply_bound_one}
   {\mathbb{P}\brackets{\norm{\matsym{Y}}\geq  t} \leq \exp\brackets{-u}} .
\end{align}
To ensue that \eqref{eq:apply_bound_one} holds, we 
{
%use \eqref{eq:lemma_bernstein_prob} from Lemma~\ref{lemma:bernstein} and 
require 
%the following:
}
% \todoin{How do you explain the following inequality? You need to explain why $\exp\brackets{-u}\geq 8 d_i
%         \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}}$ rather than $\exp\brackets{-u}\leq 8 d_i
%         \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}}$, which is the whole point of the discussion in the red box. }
\begin{align}\label{eq:apply_bound}
        {8 d_i
        \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}} \leq \exp\brackets{-u},}
\end{align}
which, {by \eqref{eq:lemma_bernstein_prob} from Proposition~\ref{prop:bernstein},} implies ~\eqref{eq:apply_bound_one}.
% $$\exp\brackets{-u}\leq 8 d_i
%         \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}}$$
%  
        % \todo[inline,color=red]{YB: 
        % The inequality that you wrote above can be satisfied e.g., for $u=100$, and $t=0$, which makes no sense.\\
        % Shouldn't you have the reverse inequality here? That is: 
        % $$\exp\brackets{-u}\geq 8 d_i
        % \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}}$$\\
        % This is also related to my next comment about reversing the inequality in \eqref{eq:t_bound}. \\
        % HVH: I think I am missing something, I rewrote the way to the inequality and I don't see an error. In \eqref{eq:prob_exp_releation} we have an exact relation and in \eqref{eq:apply_bound} I simply apply the bound. \\
        % YB: the error is not in the algebra, rather in the logic. You have an inequality pointing the wrong way.  In fact, the way you have rewritten the \eqref{eq:prob_exp_releation} and \eqref{eq:apply_bound} the error is more difficult to spot than in the previous version!\\
%         Here is another take on an explanation.\\
%         We want to guarantee that 
%         \begin{equation} \label{eq:exp_u_Guarantee}
%         \mathbb{P}\brackets{\norm{\matsym{Y}}\geq  t} \leq \exp(-u) %\quad \forall w \leq u
%         \end{equation}
%      (Note the direction of the inequality) Why? Because this is what we promise in the theorems. (To emphasize this, I edited (change in bold) to  "with probability \textbf{of at least} $1-\exp(-u)$" to the statement of the theorems.) \\
%      Now, by  Lemma~\ref{lemma:bernstein}, we have that  
%      \begin{align}\label{eq:Berenstein_bound}
%     \mathbb{P}\brackets{\norm{\matsym{Y}}\geq  t}\leq 8 d_i
%         \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}}.
% \end{align}
%      How should we combine \eqref{eq:exp_u_Guarantee} with \eqref{eq:Berenstein_bound}?
% I claim that the correct combination is the following:
% \begin{align}\label{eq:Berenstein_Exp_bound}
%     \mathbb{P}\brackets{\norm{\matsym{Y}}\geq  t}\leq 8 d_i
%         \cdot\exp{\brackets{-\frac{0.5t^2}{\nu+Lt/3}}} \leq \exp(-u)
% \end{align}
% Why? Because then \eqref{eq:Berenstein_bound} will imply \eqref{eq:exp_u_Guarantee}.
% Hence, we need to solve the inequality 
% \begin{align}
%     \exp\brackets{-u} \geq  8 d_i
%         \cdot\exp{\brackets{-\frac{0.5t^2} {\nu+Lt/3}}} 
% \end{align}
% which has the opposite direction of the inequality to that in \eqref{eq:apply_bound}.
% Do you agree?\\
% HVH: Now I understand... 
% Please, see the change in the text.\\
% YB: I don't think your change in the text explains  why the inequality points one way rather than the other way. See my edits and comment.
% }
{
Solving \eqref{eq:apply_bound} for $t\geq 0$ yields $t\geq t_u$.
%

}
Now, any $t\geq t_u$
also satisfies $t\geq \sqrt{\nu}+\frac{L}{3}$ as required by Proposition~\ref{prop:bernstein}, and therefore yields a valid bound in \eqref{eq:apply_bound_one}.
%Note that this ensue that $t\geq \sqrt{\nu}+\frac{L}{3}$.
%      
{Selecting $t=t_u$ yields 
 the tightest bound in \eqref{eq:t_bound}, and establishes the corollary. 
%the smallest %possible 
%value of $t$ { in \eqref{eq:t_bound}
}
\end{proof}

% \todo[inline,color=green]{YB: did you mean $t\geq ...$ rather than $t\leq ...$ in \eqref{eq:t_bound}? \\
% HVH: to solve the inequility, we can select any $t$ between the range of  \eqref{eq:t_bound_lower} and \eqref{eq:t_bound} and since we also required that $t\geq \sqrt{\nu}+\frac{L}{3}$, I have select $t=\frac{1}{3}\brackets{u+\log\brackets{8d_i}}L+\sqrt{\nu}$ which address. Maybe there is exist a tighter value. Do this make sense? \\
% YB: As pointed out in my previous comment, I think that you have an error. }
%
%



% \todo[color=green,inline]{Once you reverse the inequality in \eqref{eq:t_bound}, the condition $t\geq \sqrt{\nu}+\frac{L}{3}$ will be automatically satisfied}
\subsubsection{Posterior Approach Empirical Mean Error {Thm. ~\ref{thm:sampling_post}}}
\begin{proof}
Denote $\vectorsym{s}_k=\postscores{\p_k}{\x_k}$ {and} $\matsym{X}_k=\frac{1}{\nds}\brackets{\vectorsym{s}_k\vectorsym{s}_k^T-\lbfimb}$. 
 First we validate the assumptions of Proposition~\ref{prop:bernstein}.
    \begin{align*}
        \expectation{\matsym{X}_k}{\ds}=\frac{1}{\nds}\brackets{\expectation{\vectorsym{s}_k\vectorsym{s}_k^T}{\ds}-\lbfimb}=0,
    \end{align*}
    and 
    \begin{align}\label{eq:l_value_post}
        \norm{\matsym{X}_k}&\leq\frac{1}{\nds}\brackets{\norm{\vectorsym{s}_k\vectorsym{s}_k^T}+\norm{\lbfimb}}\nonumber\\
        &\leq \frac{1}{\nds}\brackets{\cb+\norm{\lbfimb}_2} \triangleq L.
    \end{align}
    Next, we calculate $\matsym{V}$:
    \begin{align*}
        \expectation{\matsym{X}_k^2}{\ds}&=\frac{1}{\nds^2}\expectation{{\vectorsym{s}_k\vectorsym{s}_k^T}\vectorsym{s}_k\vectorsym{s}_k^T -\lbfimb^2}{\ds}\\
        &\preceq \frac{1}{\nds^2}\brackets{\cb\expectation{\vectorsym{s}_k\vectorsym{s}_k^T}{\ds} -\lbfimb^2}\preceq  \frac{\cb\lbfimb}{\nds^2}.
    \end{align*}
    {The first inequality is obtained by $\vectorsym{s}_k\vectorsym{s}_k^T\preceq \norm{\vectorsym{s}_k\vectorsym{s}_k^T}_2\matsym{I}$}.
    Then
    \begin{equation}\label{eq:v_value_post}
    \matsym{V}=\frac{\cb \lbfimb}{\nds} \quad\mathrm{and} \quad \nu=\frac{\cb}{\nds}\norm{\lbfimb}_2.
    \end{equation}
           {Now, using} \eqref{eq:l_value_post}, \eqref{eq:v_value_post} in Corollary ~\ref{cor:bernstein} we obtain:
            \begin{align}
              &\frac{t_u}{\norm{\lbfimb}_2}=\frac{\nbe}{4\nds}\brackets{1+\sqrt{1+24\frac{\nds}{\nbe}\frac{\cb}{\cb+\norm{\fb}_2}}}\nonumber\\
              &=\frac{\nbe}{4\nds}\brackets{1+\sqrt{1+24\frac{\nds}{\nbe}\frac{1}{1+\frac{\norm{\fb}_2}{\cb}}}}\nonumber\\
              &\leq \frac{\nbe}{4\nds}\brackets{1+\sqrt{1+24\frac{\nds}{\nbe}}} \label{eq:tBe_bound}
            \end{align}
            where $\nbe {\triangleq}\frac{4}{3}\brackets{u+\log\brackets{8\dbb}}\brackets{\frac{\cb}{\norm{\lbfimb}_2}+1}$. Using the 
            %following 
            relation {$\trace{\lbfimb}=\dbb\norm{\lbfimb}_2$} 
provides the alternative form
            $\nbe=\frac{4}{3}\brackets{u+\log\brackets{8\dbb}}\brackets{\dbb\frac{\cb}{\trace{\lbfimb}}+1}$.  {Finally, using the easiliy verified inequality $1+\sqrt{1+x} \leq \sqrt{3x/2}$ {for $x\geq 24$,} in \eqref{eq:tBe_bound} yields }
            \begin{equation}\label{eq:bound_t_post_v2}
                \frac{t_u}{\norm{\lbfimb}_2}\leq { 1.5\sqrt{\frac{\nbe}{\nds}}},
            \end{equation}
            for any {$\nds \geq \nbe$}. {Applying the  upper bound of \eqref{eq:bound_t_post_v2}
 in Corollary~\ref{cor:bernstein} yields \eqref{eq:re_error_post_mean} of Theorem~\ref{thm:sampling_post}.}
 % \todo[inline]{HVH: 
 % $$\nbet=\brackets{\log\brackets{1+2d_B} +{0.52}} \cdot\brackets{\dbb\frac{\cb}{\trace{\lbfimb}}+1}$$
 
 % }
             
             Next, {defining $\phi_B{\triangleq}\frac{\cb}{\norm{\lbfimb}_2}+1$,
             we use \eqref{eq:l_value_post} and  \eqref{eq:v_value_post} in 
        \eqref{eq:lemma_bound_expection} %we 
        to obtain a bound on the normalized expected value 
        %which obtained using \eqref{eq:lemma_bound_expection} and given by:
             }
    \begin{align*}
        &\expectation{\frac{\norm{\lbfimbs-\lbfimb}_2}{\norm{\lbfimb}_2}}{\ds}\leq 
         \sqrt{\frac{\cb}{\norm{\lbfimb}_2\nds}}\brackets{4+\sqrt{2
         {\psi_B}
         }}\\
        &+\frac{2}{3\nds}\phi_B\brackets{4+
       {\psi_B}
        } \text{ where } \psi_B= \log\brackets{1+2\dbb} 
    \end{align*}
      Using  $\frac{\cb}{\norm{\lbfimb}_2}\leq \phi_B$ and defining $\alpha\triangleq \frac{2\brackets{4+\psi_B}}{3\brackets{4+\sqrt{2\psi_B}}}$ yields %results in
    {\begin{align*}
        \expectation{\frac{\norm{\lbfimbs-\lbfimb}_2}{\norm{\lbfimb}_2}}{\ds}\leq \brackets{4+\sqrt{2\psi_B}}\brackets{\alpha\frac{\phi_B}{\nds}+\sqrt{\frac{\phi_B}{\nds}} }.
    \end{align*}
    }
     {Now, it is easily verified that $\alpha x+\sqrt{x}\leq 1.5\sqrt{x}$ holds for any $0\leq x \leq \frac{1}{4\alpha^2}$. It  follows that:}
    {\begin{flalign}
      \label{eq:expection_bound_post}
        \textstyle{\expectation{\frac{\norm{\lbfimbs-\lbfimb}_2}{\norm{\lbfimb}_2}}{\ds}} \leq \textstyle{\brackets{6+1.5\sqrt{2\log\brackets{1+2\dbb}}}}\sqrt{\frac{\phi_B}{\nds}}
    \end{flalign}
    for any $\nds \geq 4\alpha^2 \phi_B$.  Since $\psi_B=\log\brackets{1+2\dbb}\geq 1$ it is easy to verify that $4\alpha^2\leq \psi_B +{0.52}$. Hence the error bound \eqref{eq:expection_bound_post} 
    holds for $\nds \geq \brackets{\psi_B +{0.52}}\phi_B$.  Using the relation $\trace{\lbfimb}=\dbb\norm{\lbfimb}_2$ we define $\nbet\triangleq\brackets{\log\brackets{1+2d_B} +{0.52}} \cdot\brackets{\dbb\frac{\cb}{\trace{\lbfimb}}+1}$, producing the form used in Thm. ~\ref{thm:sampling_post}.
    }
\end{proof}
\subsubsection{%Proof of 
Measurement-Prior Empirical Mean Error {Thm.~\ref{thm:sampling_mp}}}
\begin{proof}
 % For notation simplistic we 
  Denote {$\vectorsym{s}_{k,i}=\lscores{\tilde{\x}_{k,i}}{\p_k}$} as the Fisher score  vector  and $\vectorsym{p}_k=\priorscores{\p_k}$ as the prior score of the $k^{th}$ sample in $\ds$.
    Now, let $$\matsym{X}_k=\frac{1}{\nds}\brackets{\frac{\niideval}{\niiddata}\cdot\sum_{i=1}^{\niiddata} \vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T+ \vectorsym{p}_k\vectorsym{p}_k^T-\lbfimlp}=\frac{1}{\nds}\matsym{R}_k.$$
    First we validate the assumptions of {Proposition~\ref{prop:bernstein}}:
    \begin{align*}
        &\expectation{\matsym{X}_k}{\ds}\nonumber\\
        &=\frac{1}{\nds}\brackets{\niideval\expectation{\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}{\ds}+\expectation{\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}-\lbfimlp}=0,
    \end{align*}
    {for every $i^{th}$ i.i.d sample} and 
    \begin{align}\label{eq:l_value_lik_prior}
        \norm{\matsym{X}_k}&\leq\frac{1}{\nds}\brackets{\frac{\niideval}{\niiddata}\cdot{\norm{\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}}+\norm{\vectorsym{p}_k\vectorsym{p}_k^T}+\norm{\lbfimlp}}\nonumber\\
        &\leq \frac{1}{\nds}\brackets{\niideval \cm+\cp+\norm{\lbfimlp}_2}=L.
    \end{align}
   Next, we calculate $\matsym{V}$:
    \begin{align*}
        &\matsym{R}^2_k={\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}+\vectorsym{p}_k\vectorsym{p}_k^T\vectorsym{p}_k\vectorsym{p}_k^T\nonumber\\
        &+\lbfimlp^2+{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}\vectorsym{p}_k\vectorsym{p}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}\\
        &-\brackets{{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}+\vectorsym{p}_k\vectorsym{p}_k^T}\lbfimlp\\
            &-\lbfimlp \brackets{{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T}+\vectorsym{p}_k\vectorsym{p}_k^T}
            % &\preceq C_F^2\vectorsym{s}_k\vectorsym{s}_k^T+\cp^2\vectorsym{p}_k\vectorsym{p}_k^T+\lbfimlp^2-\brackets{\vectorsym{s}_k\vectorsym{s}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T}\lbfimlp\\
            % &-\lbfimlp \brackets{\vectorsym{s}_k\vectorsym{s}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T}+\vectorsym{s}_k\vectorsym{s}_k^T \cp^2+\vectorsym{p}_k\vectorsym{p}_k^T C_F^2
    \end{align*}
    Taking the expectation and using that $\expectation{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T+\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}=\lbfimlp$ yields
    \begin{align*}
        &\expectation{\matsym{R}_k^2}{\ds}= \frac{\niideval^2}{\niiddata}\cm\expectation{\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T} {\ds}+\cp\expectation{\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}\\
        &+\lbfimlp^2+ \frac{\niideval}{\niiddata}\cp\expectation{\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T} {\ds}+\cm\niideval\expectation{\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}\\
        &-2\lbfimlp^2.
    \end{align*}
    Reordering and using $\expectation{\frac{\niideval}{\niiddata}\sum_{i=1}^{\niiddata}\vectorsym{s}_{k,i}\vectorsym{s}_{k,i}^T+\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}=\lbfimlp$ we have:
    
    % \begin{align*}
    %     &\expectation{\matsym{R}_k^2}{\ds}=\brackets{C_F^2+\cp^2}\expectation{{\vectorsym{s}_k\vectorsym{s}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T}}{\ds}\\
    %     &+\lbfimlp^2 -\expectation{\vectorsym{s}_k\vectorsym{s}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}\lbfimlp\\
    %         &-\lbfimlp \expectation{\vectorsym{s}_k\vectorsym{s}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}
    % \end{align*}
    % By using that $\expectation{\vectorsym{s}_k\vectorsym{s}_k^T+\vectorsym{p}_k\vectorsym{p}_k^T}{\ds}=\lbfimlp$ we have:
    \begin{align*}
        \expectation{\matsym{R}_k^2}{\ds}&=\brackets{\niideval \cm+\cp}\lbfimlp-\lbfimlp^2 \\
        &\preceq\brackets{\niideval \cm+\cp}\lbfimlp.
    \end{align*}
    Then,
    \begin{equation}\label{eq:v_value_mp}
\matsym{V}=\frac{\expectation{\matsym{R}_k^2}{\ds}}{\nds}\preceq\frac{\brackets{\frac{\niideval}{\niiddata}\cm+\cp}\lbfimlp}{\nds} 
    \end{equation}
    Finally, {using}  equations \eqref{eq:l_value_lik_prior}, \eqref{eq:v_value_mp} {and} { Corollary~\ref{cor:bernstein} 
    {and following the same steps as  in the proof of Theorem~\ref{thm:sampling_post}},
    we establish Theorem~\ref{thm:sampling_mp}.}

\end{proof}