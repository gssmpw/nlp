\subsection{Proofs of Sampling Error}
From \cite{vershynin2012close} and \cite{kereta2021estimating} we have the following:
\begin{lemma}\label{lemma:samples}
    Let $\hat{\vectorsym{f}}:\mathbb{R}^m\rightarrow \mathbb{R}^n$, $\{\z_n\}_{n=1}^{\nds}$ is a set of i.i.d. samples from $\randomvec{Z}$ and $\hat{\vectorsym{f}}\brackets{\randomvec{Z}}$ is a sub-Gaussian, then with probability at least $1-\exp{\brackets{-u}}$
    \begin{align}
        &\norm{\frac{1}{\nds}\sum_{n=1}^{\nds}\hat{\vectorsym{f}}\brackets{\z_n}\hat{\vectorsym{f}}\brackets{\z_n}^T - \expectation{\hat{\vectorsym{f}}\brackets{\randomvec{Z}}\hat{\vectorsym{f}}\brackets{\randomvec{Z}}^T}{\randomvec{Z}}}_2\nonumber\\
        &\leq C \norm{\hat{\vectorsym{f}}\brackets{\randomvec{Z}}}_{\psi_2}^2 u\brackets{\sqrt{\frac{n+u}{\nds}}},
    \end{align}
    where $u\brackets{x}=\min\brackets{x,x^2}$, C is a universal constant and  $\norm{\x}_{\psi_2}^2$ is the sub-Gaussian norm of $\x$.
\end{lemma}
By combining Lemma~\ref{lemma:cov_vec_error} and Lemma ~\ref{lemma:samples}, we have the following.
\begin{lemma}\label{lemma:lrn_samples}
    \begin{align}
        &\norm{\frac{1}{\nds}\sum_{n=1}^{\nds}\hat{\vectorsym{f}}\brackets{\z_n}\hat{\vectorsym{f}}\brackets{\z_n}^T - \expectation{\vectorsym{f}\brackets{\randomvec{Z}}\vectorsym{f}\brackets{\randomvec{Z}}^T}{\randomvec{Z}}}_2\nonumber\\
        &\leq C \norm{\hat{\vectorsym{f}}\brackets{\randomvec{Z}}}_{\psi_2}^2 u\brackets{\sqrt{\frac{n+u}{\nds}}}+\mathrm{J}+2\sqrt{\mathrm{J}\norm{\matsym{F}}_2}.
    \end{align}
\end{lemma}
\begin{proof}
\begin{align}
    &\norm{\frac{1}{\nds}\sum_{n=1}^{\nds}\hat{\vectorsym{f}}\brackets{\z_n}\hat{\vectorsym{f}}\brackets{\z_n}^T - \expectation{\vectorsym{f}\brackets{\randomvec{Z}}\vectorsym{f}\brackets{\randomvec{Z}}^T}{\randomvec{Z}}}_2=\nonumber\\\nonumber
    &\norm{\frac{1}{\nds}\sum_{n=1}^{\nds}\hat{\vectorsym{f}}\brackets{\z_n}\hat{\vectorsym{f}}\brackets{\z_n}^T-\expectation{\hat{\vectorsym{f}}\brackets{\z}\hat{\vectorsym{f}}\brackets{\z}^T}{\z}}_2\\
    &+\norm{\expectation{\hat{\vectorsym{f}}\brackets{\z}\hat{\vectorsym{f}}\brackets{\z}^T -\vectorsym{f}\brackets{\z}\vectorsym{f}\brackets{\z}^T}{\z}}_2\nonumber
\end{align}
    In the equation above we utilized the triangular inequality. Finally, to obtain Lemma~\ref{lemma:lrn_samples}, we used Lemma~\ref{lemma:cov_vec_error} and Lemma ~\ref{lemma:samples}.
    \end{proof}
\subsubsection{Proof posterior Learning and Sample Error Proof}\label{proof:post:lrn_sample}
\begin{proof}
    We set $\z=[\x,\p]$, $\hat{f}\brackets{\z}=\postscore{\p}{\x}$ and $f\brackets{\z}=\nabla_{\p}\log\probt{\p|\X}{\p|\x}$, then apply Lemma~\ref{lemma:lrn_samples}, which results in Theorem~\ref{thm:sample_post}.
\end{proof}
\subsubsection{Proof Measurement-Prior Error}\label{proof:lik_prior:lrn_samples}
\begin{proof}
    \begin{align}\label{eq:lik_prior_lrn_sample_error_tri}
        &\norm{\lbfimlps-\fb}_2=\norm{\lmfim+\lpfim-\fm-\fp}_2\nonumber\\
        &\norm{\lmfim-\fm}_2+\norm{\lpfim-\fp}_2
    \end{align}
    Firstly, we applied the BFIM decomposition as described in \eqref{eq:bfim_decomposition} along with the definition of $\overline{\mathrm{LBFIM}}$. In the subsequent step, we used the triangle inequality.
    Now, we apply Lemma~\ref{lemma:lrn_samples} on the two parts of \eqref{eq:lik_prior_lrn_sample_error_tri}:
    \begin{itemize}
        \item For $\norm{\lmfim-\fm}_2$, we set $\z=[\x,\p]$, $\hat{f}\brackets{\z}=h_{\Omega_{h}}\brackets{\X|\p}$ and $f\brackets{\z}=\nabla_{\p}\log\probt{\X|\p}{\x|\p}$, then we apply Lemma~\ref{lemma:lrn_samples}.
        \item For $\norm{\lpfim-\fp}_2$    we set $\z=\p$, $\hat{f}\brackets{\z}=p_{\Omega_p}\brackets{\p}$ and $f\brackets{\z}=\nabla_{\p}\log\probt{\p}{\p}$, then we apply Lemma~\ref{lemma:lrn_samples}.
    \end{itemize}
    Combining the results of applying Lemma~\ref{lemma:lrn_samples} twice with \eqref{eq:lik_prior_lrn_sample_error_tri} results in Theorem~\ref{thm:sample_lik_prior}.
\end{proof}