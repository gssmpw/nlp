

\subsection{Proofs of 
%Theoretical Analysis of 
the \name{} {Approximation Error}}
\newcommand{\vDelta}{\vectorsym{\Delta}}
\newcommand{\deltasvec}[0]{\vectorsym{\Delta}_{s}}
\newcommand{\shat}[0]{\hat{\vectorsym{s}}}
\newcommand{\svec}[0]{\vectorsym{s}}
{We will need the following technical result.}
\begin{lemma}[{From vector Difference to Covariance Difference}]\label{lemma:cov_vec_error}
    Let {$\Xr,\Yr \in \mathbb{R}^n$ be random vectors on a common probability space, and define \begin{equation*}
        \mathcal{J}=\expectation{\norm{\Xr-\Yr}_2^2}{}, \quad \quad  \matsym{F}=\expectation{\Yr\Yr^T}{}.
    \end{equation*}
    % \begin{equation*}
    %     \matsym{F}=\expectation{\Yr\Yr^T}{}.
    % \end{equation*}
    Then:}
    {
\begin{align}\label{eq:vector_error}
\frac{\norm{\expectation{\Xr\Xr^T -\Yr\Yr^T}{}}_2}{\norm{\matsym{F}}_2} \leq \frac{\mathcal{J}}{\norm{\matsym{F}}_2}+2\sqrt{\frac{\mathcal{J}}{\norm{\matsym{F}}_2}}.
\end{align}
Furthermore, if $\frac{\mathcal{J}}{\expectation{\norm{\Yr}_2^2}{}} < {0.16}$, then the RHS of \eqref{eq:vector_error} is upper bounded by
\begin{equation} \label{eq:vector_error-simplified}
%3
2.4\sqrt{\frac{\mathcal{J}}{\norm{\matsym{F}}_2}} =
%3
2.4\sqrt{\intdim\brackets{\F}\frac{\mathcal{J}}{\expectation{\norm{\Yr}_2^2}{}}}.
\end{equation}
    }    
\end{lemma}
\begin{proof}
    {Let $\vDelta \triangleq \Xr-\Yr$. Then
    \begin{align}\label{eq:bound_proof_eq_delta_cov}
 \matsym{P}&=\norm{\expectation{\Xr\Xr^T -\Yr\Yr^T}{}}_2 \nonumber\\
 &=
 \norm{\expectation{\vDelta\vDelta^T +\vDelta\Yr^T + \Yr \vDelta^T}{}}_2
    \end{align}
    }
    %\ybreplace{
    Considering
    %}{Using the convexity of the norm and applying the Jensen inequality to} 
    the first term in \eqref{eq:bound_proof_eq_delta_cov} yields
    {\begin{equation}\label{eq:part_one}
        \norm{\expectation{\vDelta\vDelta^T}{}}_2 \leq 
        \trace{\expectation{{\vDelta\vDelta^T}}{}} =  \expectation{\norm{\vDelta}_2^2}{}=\mathcal{J}
        \end{equation}
    }
    Next, {considering}  the second term in \eqref{eq:bound_proof_eq_delta_cov}, yields:
    \begin{align}\label{eq:part_two}
        &\norm{\expectation{\vDelta\Yr^T+\Yr\vDelta^T}{}}_2 
        {=} 2 \norm{\expectation{\vDelta\Yr^T}{}}_2\nonumber\\
        &\leq 2\sqrt{\norm{\expectation{\vDelta\vDelta^T}{}}_2\norm{\expectation{\Yr\Yr^T}{\z}}_2}\nonumber\\
        &\leq 2\sqrt{\expectation{\norm{\vDelta}_2^2}{}\norm{\expectation{\Yr\Yr^T}{\z}}_2}
    \end{align}
    where the first equality holds by the identity $\norm{\matsym{A} + \matsym{A}^T}_2 = 2\norm{\matsym{A}}_2$, the inequality on the second line follows by Lemma XIII.1 from \cite{habi2023learning}, and the third line by \eqref{eq:part_one}.
    Combining \eqref{eq:part_one}, \eqref{eq:part_two} with \eqref{eq:bound_proof_eq_delta_cov}, {applying the triangle inequality,} and dividing by $\norm{\matsym{F}}_2$ {establishes \eqref{eq:vector_error}}:
    \begin{equation}\label{eq:covar_error_full}
        \frac{\matsym{P}}{\norm{\F}_2}\leq\frac{\mathcal{J}}{\norm{\matsym{F}}_2}+2\sqrt{\frac{\mathcal{J}}{\norm{\matsym{F}}_2}}.
    \end{equation}
   {Now, it is easily verified that $x + 2 \sqrt{x} \leq 2.4 \sqrt{x} $ holds for any $0 \leq x \leq 0.16$. 
   % \todo[inline,color=green]{YB:The calculations leading to finding a tight simplified bound are usually tedious, and not shown in papers. What is shown is a demonstration of the correctness of the simplified form, which in this case is the one liner that I included above.\\
   % HVH: I wrote the the new ones in the simplifed form. }
This leads immediately to the upper bound on the LHS of \eqref{eq:vector_error-simplified}.  
   } 
    Finally, replacing 
    \begin{align}
        \frac{\mathcal{J}}{\norm{\matsym{F}}_2}&=\frac{\trace{\matsym{\F}}\mathcal{J}}{\trace{\matsym{\F}}\norm{\matsym{F}}_2}=\mathrm{intdim}\brackets{\F}\frac{\mathcal{J}}{\trace{\matsym{\F}}}\nonumber\\
                                              &=\mathrm{intdim}\brackets{\F}\frac{\mathcal{J}}{\expectation{\norm{\Yr}_2^2}{}}
    \end{align}
    {yields the alternative form of the upper bound in \eqref{eq:vector_error-simplified}.}
\end{proof}
{With} Lemma~\ref{lemma:cov_vec_error} {in hand,} we {proceed to prove the various bounds on the {Approximation error}}.
\subsubsection{Posterior {FIM } {Approximation Error} {Thm.~\ref{thm:lrn:direct}}}\label{proof:post:lrn}
% \todo[inline,color=green]{YB: Writing style: Having a descriptive subtitle for a proof of a Thm or lemma is fine, but the exact reference to the result that is proven should be included in the subtitle, so that a reader looking for a proof of say Thm.~\ref{thm:lrn:direct} can find it easily, without having to search through the entire text of the appendix. I made this change in a few places already, but there remain a few more instances where this needs to be done.}
\begin{proof}
    We set $\Xr=\postscores{\p}{\xset}$ and $\Yr=\nabla_{\p}\log\probt{\p|\xset}{\p|\xset}$, then we apply Lemma~\ref{lemma:cov_vec_error}, 
    {establishing \eqref{eq:error_learn_post_approch}}. 
\end{proof}

\subsubsection{Measurement-Prior {FIM } {Approximation error} {Thm.~\ref{thm:lrn:lik_prior}}}\label{proof:lik_prior:lrn}
% \hvhreplace{Starting with two Lemma's that analyze the individual learning error of each term: }
\begin{proof}
{
%First, 
We apply Lemma~\ref{lemma:cov_vec_error} twice.
%two times: once for the measurement Fisher Information Matrix (FIM) and once for the prior FIM. First, In the context of 
First, for} the measurement FIM  
we set $\Xr=\lscores{\x}{\p}$ and $\Yr=\nabla_{\p}\log\probt{\X|\p}{\x|\p}$
{in} Lemma~\ref{lemma:cov_vec_error}, which results in
\begin{equation}\label{eq:mfim_error}
    \frac{\norm{\tlmfim-\fm}_2}{\norm{\fm}_2}\leq \mfle\triangleq {2.4}\sqrt{\mathrm{intdim}\brackets{\fm}\cdot \frac{\lossfs}{\trace{\fm}}}.
\end{equation}
{Second, for} the 
prior FIM, we set $\Xr=\priorscores{\p}$ and $\Yr=\nabla_{\p}\log\probt{\p}{\p}$
{in} Lemma~\ref{lemma:cov_vec_error}, yielding
\begin{equation}\label{eq:pfim_error}
    \frac{\norm{\tlpfim-\fp}_2}{\norm{\fp}_2}  \leq\eta_{p}^{(l)}\triangleq {2.4}\sqrt{\mathrm{intdim}\brackets{\fp}\cdot \frac{\lossps}{\trace{\fp}}}.
\end{equation}
% \hvhdelete{\begin{lemma}[Prior FIM Error]\label{lem:prior_error_lern}
%     Assume that assumptions ~\ref{assum:bcrb_reg} and ~\ref{ass:score_reg_prior} hold, then:
%     \begin{align}
%         \frac{\norm{\widetilde{\mathrm{LPFIM}}-\fp}_2}{\norm{\fp}_2}  \leq\eta_{p}^{(l)}\triangleq 3\sqrt{\mathrm{intdim}\brackets{\fp}\cdot \frac{\lossps}{\trace{\fp}}}
%     \end{align}
% \end{lemma}
% \begin{lemma}[Measurement FIM Error]\label{lem:lik_error_lern}
%     Assume that assumptions ~\ref{assum:bcrb_reg} and ~\ref{ass:score_reg} hold, then:
%     \begin{align}
%         \frac{\norm{\tlmfim-\fm}_2}{\norm{\fm}_2} \leq \mfle,
%     \end{align}
%     where $\mfle\triangleq 3\sqrt{\mathrm{intdim}\brackets{\fm}_2\cdot \frac{\lossfs}{\trace{\fm}}}$ is the measurement FIM learning error bound.
% \end{lemma}
% Detailed proofs of Lemma ~\ref{lem:prior_error_lern} and ~\ref{lem:lik_error_lern} are presented in Sections ~\ref{proof:prior:lrn} and ~\ref{proof:lik:lrn}, respectively.}
Finally, we use the BFIM decomposition $\fb=\niideval\cdot\fm+\fp$ with the triangular inequality. Then, we employ equations \eqref{eq:mfim_error} and \eqref{eq:pfim_error}, to obtain Theorem~\ref{thm:lrn:lik_prior}.
\end{proof}


% \hvhdelete{
% By employing equations \eqref{eq:mfim_error} and \eqref{eq:pfim_error}, we establish a bound on the learning error of the measurement prior approach as detailed in Theorem~\ref{thm:lrn:lik_prior}. The proofs of Theorem~\ref{thm:lrn:lik_prior} are constructed on the basis of the BFIM decomposition $\fb=\fm+\fp$ in conjunction with the triangular inequality. We subsequently utilized results from \eqref{eq:mfim_error} and \eqref{eq:pfim_error} result in Theorem~\ref{thm:lrn:lik_prior}. }
% \hvhdelete{
% \subsubsection{Proof Prior Error}\label{proof:prior:lrn}
% \begin{proof}
%     We set $\z=\p$, $\hat{f}\brackets{\z}=\priorscore{\p}$ and $f\brackets{\z}=\nabla_{\p}\log\probt{\p}{\p}$, then we apply Lemma~\ref{lemma:cov_vec_error}, which results in Lemma~\ref{lem:prior_error_lern}.
% \end{proof}

% \subsubsection{Proof measurement Error}\label{proof:lik:lrn}
% \begin{proof}
%     We set $\z=[\x,\p]$, $\hat{f}\brackets{\z}=\lscore{\x}{\p}$ and $f\brackets{\z}=\nabla_{\p}\log\probt{\X|\p}{\x|\p}$, then we apply Lemma~\ref{lemma:cov_vec_error}, which results in Lemma~\ref{lem:lik_error_lern}.
% \end{proof}}


\input{files/proofs/sample_error_v2}
\input{files/proofs/c_relation}
\input{files/proofs/empircal_mean_error_convergance}
\subsection{
LBCRB Relative Error Bound}\label{apx:proof_inv_re}
\subsubsection{%Proof 
%Inversion Conditions 
Relative Error Posterior Approach {Corr.~\ref{corr:bound_inv_post}}}\label{sec:proof_post_inv}
\begin{proof}
    If $\lbfimbs \succ 0$ it is  invertible and
    \begin{align}\label{eq:mat_inv_post}
        \mathrm{RE}_{B}&=\frac{\norm{\lbcrbbs-\bcrb}_2}{\norm{\bcrb}_2}=\frac{\norm{\lbfimbs^{-1}\brackets{\fb-\lbfimbs}\fb^{-1}}_2}{\norm{\bcrb}_2}\nonumber\\
        &\leq\norm{\lbcrbbs}_2\norm{\fb-\lbfimbs}_2\nonumber\\
        &\leq\norm{\lbcrbbs}_2\norm{\lbfimb-\lbfimbs}_2+\norm{\lbcrbbs}_2\norm{\fb-\lbfimb}_2 .
    \end{align}
    By combining \eqref{eq:error_learn_post_approch} and \eqref{eq:re_error_post_mean}  with \eqref{eq:mat_inv_post} we have:
    \begin{align}\label{eq:mat_inv_post_step2}
        \mathrm{RE}_{B}\leq\norm{\lbcrbbs}_2\brackets{\norm{\lbfimb}_2\bfse+\norm{\fb}\bfle}.
    \end{align}    
    Multiplying and dividing\eqref{eq:mat_inv_post_step2}  by $\norm{\lbfimbs}_2$  results in \eqref{eq:inv_bound_post}. Next, we provide the {%invertibility 
    condition %of 
    for $\lbfimbs\succ 0$.} {From \eqref{eq:error_learn_post_approch}, \eqref{eq:re_error_post_mean} and using the triangular  inequality we have
    \begin{align}\label{eq:fimb_bound_joint}
        \norm{\lbfimbs-\fb}_2&\leq\norm{\lbfimbs-\lbfimb}_2+\norm{\lbfimb-\fb}_2\nonumber\\
                              &\leq \norm{\lbfimb}_2\bfse+\norm{\fb}\bfle 
    \end{align}
    
    }
    
    Using Assumption~\ref{assume:non_singular}  we have that $\fb\succ 0$, and by definition we have that $\lbfimb$ {is symmetric}. %and using 
    Now by \eqref{eq:fimb_bound_joint} we have with probability $1-\exp\brackets{-u}$, $\lbfimbs \succ0$ %is invertiable 
    if
        \begin{equation*}
       \norm{\lbfimb}_2\bfse+\norm{\fb}\bfle <\eigmin{\fb},
    \end{equation*}
    % \todoing{How does this follow? You are missing a step of bounding the perturbation  $\norm{\lbfimbs - \fb}_2$. Also, not clear where you are using $\lbfimb\succeq 0$.
    % The matrix perturbation result is simply the following:
    % Suppose $A \succ 0$, and $B$ is symmetric, i.e.,  $B=B^T$. Then $B \succ 0$ if $\norm{A-B}_2 < \lambda_{\min}(A)$. Furthermore, if $\norm{A-B}_2 \geq \lambda_{\min}(A)$, then $\exists B$ s.t. $0 \succeq B$.\\
    % HVH: I was think it is simple :), please see updated text. }
    {where the inequality follows from a standard} matrix perturbation result, cf. \cite[Theorem 2.2]{stewart1977perturbation}.
    Finally, dividing both sides by $\norm{\fb}_2$ %we have that 
    yields \eqref{eq:inv_cond_post}.
\end{proof}
\subsubsection{%Proof 
%Inversion Conditions 
Relative Error Measurement-Prior Approach {Corr.~\ref{corr:bound_inv_split}}}\label{sec:proof_lik_prior_inv}
\begin{proof}
If $\lbfimlps \succ 0$ then it is invertible, %we have that:
and
\begin{align}\label{eq:mat_inv_mp}
    \mathrm{RE}_{MP}&\triangleq\frac{\norm{\lbcrblps-\bcrb}_2}{\norm{\bcrb}_2}\nonumber\\
                    &=\frac{\norm{\lbcrblps\brackets{\fb-\lbfimlps}\bcrb}_2}{\norm{\bcrb}_2}\nonumber\\
                    &\leq \norm{\lbcrblps}_2\norm{\fb-\lbfimlps}_2\nonumber\\
                    &\leq  \norm{\lbcrblps}_2\norm{\lbfimlp-\lbfimlps}_2\nonumber\\
                    &+\norm{\lbcrblps}_2\norm{\lbfimlp-\fb}_2
\end{align}
    By combining \eqref{eq:error_learn_lik_prior_approch} and \eqref{eq:re_error_mp_mean}  with \eqref{eq:mat_inv_mp} we have:
    \begin{align}\label{eq:mat_inv_mp_step2}
        \mathrm{RE}_{MP}&= \norm{\lbcrblps}_2 \brackets{\norm{\fm}_2\mfle+\norm{\fp}_2\pfle}\nonumber\\
                        &+\norm{\lbcrblps}_2\norm{\lbfimlp}_2 \mpfse
    \end{align}
    Multiplying and dividing \eqref{eq:mat_inv_mp_step2}  by $\norm{\lbfimlps}_2$  results in \eqref{eq:inv_bound_mp}.
    Next, we provide the 
    %invertibility 
    condition for
    %of 
    {$\lbfimlps \succ 0$}. {From \eqref{eq:error_learn_lik_prior_approch}, \eqref{eq:re_error_mp_mean} and using the triangular  inequality we have
    \begin{align}\label{eq:fim_mp_bound_joint}
        \norm{\lbfimlps-\fb}_2&\leq\norm{\lbfimlps-\lbfimlp}_2+\norm{\lbfimlp-\fb}_2\nonumber\\
                              &\leq \norm{\lbfimb}_2\bfse+\norm{\fb}\bfle 
    \end{align}}

    
    By definition we have $\lbcrblps\succeq 0$ and $\fb\succ 0$, using \eqref{eq:error_learn_lik_prior_approch} and \eqref{eq:re_error_mp_mean} we have with probability $1-\exp\brackets{-u}$, $\lbfimbs \succ 0$ %is invertiable 
    if:
    \begin{equation*}
       \norm{\fm}_2\mfle+\norm{\fp}_2\pfle+\norm{\lbfimlp}_2 \mpfse <\eigmin{\fb},
    \end{equation*}
    %\todoin{Similar comment to that in the previous proof, in Subsection G.1. }
    {where the inequality is by the matrix perturbation result as above. }
    Dividing both sides by $\norm{\fb}_2$ 
    %we have that 
    yields\eqref{eq:inv_cond_post}.
    \end{proof}
    

    

% By definition we have $\lmfim\succeq 0$ and $\lpfim\succeq 0$,  using \eqref{eq:error_lik_prior_lrn_mean} we have that:
% \begin{align}\label{eq:proof_inv_lik_pos}
%     &\eigmin{\lbfimlps}\\
%     &\geq \eigmin{\fb} -\mfle-\pfle-\mfse-\pfse\nonumber\\
%     &\geq\eigmin{\fp} -\pfle-\pfse +\eigmin{\fm}-\mfle-\mfse.\nonumber
% \end{align}
% In \eqref{eq:proof_inv_lik_pos} we use the decomposition of the Bayesian FIM \eqref{eq:bfim_decomposition} and the lower bound on the sum of matrices. Next, we ensure that each part is related to the prior and the measurement FIM is positive, meaning:
% \begin{subequations}
% \begin{align}
%     \eigmin{\fp} -\pfle-\pfse>0\\
%     \eigmin{\fm}-\mfle-\mfse>0
% \end{align}
% \end{subequations}
% Then we follow the same steps as in the proof of the inversion conditions of the poster approach shown in \ref{sec:proof_post_inv} which results in conditions \eqref{eq:cond_inv_lik} and \eqref{eq:cond_inv_prior}.

% Now following the conditions in \eqref{eq:cond_inv_lik}, \eqref{eq:cond_inv_prior} and $\lmfim\succ 0$, $\overline{\mathrm{LPFIM}}\succ 0$, the sum of the prior and measurement FIMs is invertiable, resulting in:
%     \begin{align}\label{eq:mat_inv_lik_prior}
%         &\norm{\lbfimlps^{-1}-\fb^{-1}}_2\nonumber\\
%         &=\norm{\lbfimlps^{-1}\brackets{\fb-\lbfimlps}\fb^{-1}}_2\nonumber\\
%         &\leq\norm{\lbfimlps^{-1}}_2\norm{\fb^{-1}}_2\norm{\fb-\lbfimlps}_2
%     \end{align}
%     Finally, combining \eqref{eq:error_lik_prior_lrn_mean} with \eqref{eq:mat_inv_lik_prior} we obtain \eqref{eq:nv_bound_lik_prior}.



% \subsubsection{Analysis invertability condition}
% Now we wise that the following term will be strictly positive:
%     \begin{align}\label{eq:con2inv}
%         \eigmin{\fb}-\bfse -\lossbs-2\sqrt{\norm{\fb}_2\cdot \lossbs}>0
%     \end{align}
%     {First, divide both sides of \eqref{eq:con2inv} by $\norm{\fb}_2$, yielding:
    
%     $$\kappa\brackets{\fb^{-1}}-\frac{\bfse}{\norm{\fb}_2} -\frac{\lossbs}{\norm{\fb}_2}-2\sqrt{ \frac{\lossbs}{\norm{\fb}_2}}$$
    
%     }
%     By definition of matrix norm, we have $\norm{\fb}_2=\eigmax{\fb}$ and we set $x=\sqrt{ \frac{\lossbs}{\norm{\fb}_2}}$ and solve the following quadratic equation: 
%     % \begin{equation}
%         $-x^2-2x+\kappa\brackets{\fb^{-1}}-\frac{\bfse}{\norm{\fb}_2}$.
%     % \end{equation}
%     To ensue that a solution exist and since $x\geq 0$, we assume that $\kappa\brackets{\fb^{-1}}-\frac{\bfse}{\norm{\fb}_2}>0$, solutions are given by:
%     \begin{align}\label{eq:cor_p1_proof}
%         x&=-\frac{2\pm \sqrt{4+4\brackets{\kappa\brackets{\fb^{-1}}-\frac{\bfse}{\norm{\fb}_2}}}}{2}\nonumber\\
%         &=-1\pm \sqrt{1+\kappa\brackets{\fb^{-1}}-\frac{\bfse}{\norm{\fb}_2}}.
%     \end{align}
%     Then, since $x=\sqrt{ \frac{\lossbs}{\norm{\fb}_2}}$ is strictly positive, we have $$0\leq\sqrt{ \frac{\lossbs}{\norm{\fb}_2}}< \sqrt{1+\kappa\brackets{\fb^{-1}}-\frac{\bfse}{\norm{\fb}_2}}-1. $$ Now if  \eqref{eq:inv_loss_bound_post} inequality holds,
%     then $\eigmin{\lbfimbs}>0$ and $\lbfimbs$ is invertable matrix. 
%     In the next part, we assume that \eqref{eq:inv_loss_bound_post} inequality holds which results in:

