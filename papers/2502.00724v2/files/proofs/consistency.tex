\subsection{Strong Consistency Proofs, Thm.~\ref{thm:post_consist} and Thm.~\ref{thm:mp_consist}}
{The following theorem is a key tool in our proofs. The version presented is  adapted slightly to the case where the domain $\mathcal{S}$ of the parameters $\Omega$ is a subset of Euclidean space.} 
\begin{theorem}[Strong Uniform Law of Large Numbers \cite{andrews1992generic}]\label{lemma:sulln}
    Let  $\Z_n\in\mathbb{R}^n$ be a sequence of random variables and $f:(\mathbb{R}^n,\mathcal{S})\rightarrow \mathbb{R}$ %be 
    a function.
    %, $\mathcal{S}$ is the set of parameters.
     % \todo[inline,color=green]{There is only one function $f$. Did you mean a sequence of random vectors $\X_n$?\\
     % HVH: Changed the opening of the Theorem.} 
     Assume that:
    \begin{enumerate}[label=\text{\ref{lemma:sulln}}.\arabic*,labelsep=*, leftmargin=*]
    \item $\mathcal{S}$ is compact.\label{assume:sulln_compact} %\todo
    \item  Lipschitz Condition: $\abs{f\brackets{\Z_n,\Omega_1}-f\brackets{\Z_n,\Omega_2}}\leq B\brackets{\Z_n}h\brackets{
    {\norm{\Omega_1-\Omega_2}_2}}\quad\forall\Omega_1, \Omega_2\in\mathcal{S} \qquad \text{a.s}$
    for some measurable function $B$ and some nonrandom function $h$ that  satisfies {$\lim_{x\rightarrow 0}h\brackets{x} =0$}.\label{assume:lip_cond}
    
    \item $\expectation{B\brackets{\Z_n}}{\Z_n}\leq \infty$\label{assume:bound_b}
    
    \item Pointwise strong convergence %assumption 
    $\forall\Omega\in\mathcal{S}$:\label{assume:sulln_conv}
    $$\frac{1}{N}\sum_{n=1}^N \brackets{f\brackets{\Z_n,\Omega}- \expectation{f\brackets{\Z_n,\Omega}}{\Z_n}}\xrightarrow{N\rightarrow\infty}0 \quad \text{a.s.}$$
\end{enumerate}
Then:
\begin{equation*}
    \sup\limits_{\Omega}\abs{\frac{1}{N}\sum_{n=1}^N \brackets{f\brackets{\Z_n,\Omega}- \expectation{f\brackets{\Z_n,\Omega}}{\Z_n}}}\xrightarrow{N\rightarrow\infty}0 \quad \text{a.s.}
\end{equation*}
\end{theorem}



\subsubsection{Posterior Score Consistency Proof (Thm~\ref{thm:post_consist})}
\label{apx:proof:consistency_post}
\begin{lemma}[Posterior Score Loss is Lipschitz continuous]\label{lemma:lip_score}
 Define the per-sample loss of the posterior score matching $\ell_B\brackets{\p,\xsetr;\Omega}\triangleq{\norm{\postscore{\p}{\xsetr;\Omega}}_2^2}+2\trace{\frac{\partial \postscore{\p}{\xsetr;\Omega}{}}{\partial\p}} $, and $\Delta_\Omega \triangleq  \norm{\Omega_{1}-\Omega_{2}}_2$. 
 Assume that:
  \begin{itemize}
      \item {The posterior} score {Jacobian matrix} is Lipschitz continuous 
      $$\norm{\nabla_{\p}\postscore{\p}{\xsetr;\Omega_1}-\nabla_{\p}\postscore{\p}{\xsetr;\Omega_2}}_2\leq \tau_B\brackets{\p,\xsetr}\Delta_\Omega$$
      \item {The score} outer product is Lipschitz continuous
      $$\norm{\matsym{R}\brackets{\p|\xsetr;\Omega_1}-\matsym{R}\brackets{\p|\xsetr;\Omega_2}}_F\leq \zeta_B\brackets{\p,\xsetr}\Delta_\Omega$$
  \end{itemize}
where $\matsym{R}\brackets{\p|\xsetr;\Omega} \triangleq\postscore{\p}{\xsetr;\Omega}\postscore{\p}{\xsetr;\Omega}^T$.  {In addition assume that the following expectations: $\expectation{\tau_B\brackets{\pr,\xset}}{\pr,\xset}<\infty$ and $\expectation{\zeta_B\brackets{\pr,\xset}}{\pr,\xset}<\infty$ are finite.} Then
    \begin{equation}
        \abs{\ell_B\brackets{\p,\xsetr;\Omega_1}-\ell_B\brackets{\p,\xsetr;\Omega_2}}\leq L_B\brackets{\p,\xsetr}\Delta_\Omega,
    \end{equation}
% \todo[inline,color=green]{Typo above - $\omega$ instead of $\Omega$?\\
% HVH: I change it to $\Omega$, Please see the todonote in the main text about the vectorization. }
where 
% $\ell_B\brackets{\p,\xsetr;\Omega}\triangleq{\norm{\postscore{\p}{\xsetr;\Omega}}_2^2}+2\trace{\frac{\partial \postscore{\p}{\xset;\Omega}}{\partial\p}} $  is the sample loss, 
$L_B\brackets{\p,\xsetr}=\sqrt{\np}\brackets{2\tau_B\brackets{\p,\xsetr}+\zeta_B\brackets{\p,\xsetr}}$ is the Liphschitz constant. {Furthermore, $\expectation{L_B\brackets{\pr,\xset}}{\pr,\xset}<\infty$. }

    
\end{lemma}
% \todo[inline,color=green]{YB: Define $\tau_B$ and $\zeta_B$, or point to their definitions? A Lemma is usually expected to be a self-contained technical result, that can be used not only for your application, but also elsewhere. \\}
{By concatenating $\p$ and $\xsetr$ into a single vector $\z$,
the proof of Lemma~\ref{lemma:lip_score} 
% \todo[color=green]{Of what?} 
is similar to that of 
%to the proof of 
Lemma 10 in \cite{crafts2023bayesian}.
%{by treating  the parameter $\p$ and the conditional input set $\xsetr$ as a single vector input $\z$ which is a concatenation of all inputs.}
}
% \todo[inline,color=green]{There is no $\z$ in Lemma~\ref{lemma:lip_score}?\\
% HVH: See update text. Do you think we need to be more formal here? }
% assuming $\z$

% to be a concatenation of the inputs. 

% \todo[inline,color=green]{HVH:I think those assumption maybe too strong and we can have a weaker assumption using that Rademacher complexity convergence to zero as $\nds\rightarrow\infty$. \\
% YB: 1.
% What do you assume is dominated by $d_{B}\brackets{\x,\p}$? The iid terms in the sum defining $\lossbsm$? Then do you need to assume something about the true score function?\\
% 2.With the same assumptions, or with vanishing Rademacher complexity, don't you get a Uniform Strong Law of Large Numbers? Wouldn't that enable showing almost sure convergence? \\
% HVH: \\
% 1) Please see clarifation in the text.\\
% 2) I think that if we use Rademacher complexity we can only proof convergence with probability and  remove assumption A.1.3. If we assume A.1.3 I think we can have almost sure convergence.\\
% YB: I think that A.1.3 may appear more natural to assume, as an even stronger assumption is made on the true score in Assumption II.1.2, and on the score NN  in the score-matching regularity assumptions. \\
% HVH: I found a good reference for almost surely convergence writing now the proofs and changing the results in the paper. Next step is the Measurement-Prior apporch.
% }
{Next using Theorem~\ref{lemma:sulln} and Lemma~\ref{lemma:lip_score} we prove the Consistency of the Posterior Score (Theorem~\ref{thm:post_consist}) }
\begin{proof}
% \todoin{YB: Proof of what?}
{Let $\tilde{\Omega}=\arg\min\limits_{\Omega}\lossbst\brackets{\Omega}$. Then}
    \begin{align}
        &\lossbs\brackets{\Omega^*}-\lossbs\brackets{\tilde{\Omega}}=\lossbst\brackets{\Omega^*}-\lossbst\brackets{\tilde{\Omega}}\nonumber\\
        &=\lossbst\brackets{\Omega^*}-\lossbsm\brackets{\Omega^*}+\lossbsm\brackets{\Omega^*}-\lossbsm\brackets{\tilde{\Omega}} \nonumber\\
        &+\lossbsm\brackets{\tilde{\Omega}}-\lossbst\brackets{\tilde{\Omega}}\nonumber\\
        &{\leq
        \lossbst\brackets{\Omega^*}-\lossbsm\brackets{\Omega^*}
        +\lossbsm\brackets{\tilde{\Omega}}-\lossbst\brackets{\tilde{\Omega}}}\nonumber\\
       % & \ybdelete{\leq\sup\limits_{\Omega}\norm{\lossbsm\brackets{\Omega}-\lossbst\brackets{\Omega}}_2 +\norm{\lossbsm\brackets{\tilde{\Omega}}-\lossbst\brackets{\tilde{\Omega}}}_2}\nonumber\\
        &\leq 
        2\sup\limits_{\Omega}\norm{\lossbsm\brackets{\Omega}-\lossbst\brackets{\Omega}}_2,
    \end{align}
    where {the first inequality holds because %$\tilde{\Omega}=\arg\min\limits_{\Omega}\lossbst\brackets{\Omega}$. {Note that
    }
    $\lossbsm\brackets{\Omega^*}-\lossbsm\brackets{\tilde{\Omega}}\leq0$ by definition of $\Omega^*$ and $\tilde{\Omega}$.
    % \ybdelete{The first step is to remove the constant $C$, in the second step, we add the substrate $\lossbsm\brackets{\tilde{\Omega}}$. In the first step, we take the norm and the norm with supreme. In the last step, we take the supreme of the loss difference.} 
    
    {Next we %utilize 
    apply Theorem ~\ref{lemma:sulln}, by setting 
    {$\Z_n=[\pr_n,\xset_n]$ as the $n^{th}$ sample from the dataset $\ds$ and}
    $f\brackets{\Z_n;\Omega}= {\ell_B\brackets{\p_n,\xsetr_n;\Omega}} $ as the per-sample loss of the posterior score matching {defined in Lemma~\ref{lemma:lip_score}.  
    %$\Z_n=[\pr_n,\xset_n]$ as the $n^{th}$ sample from the dataset $\ds$. 
    }
    As first step we validate that all assumptions in Theorem~\ref{lemma:sulln} hold. Assumptions~\ref{assume:lip_cond} and ~\ref{assume:bound_b} hold using Assumption ~\ref{assume:lip_cont_post} and Lemma~\ref{lemma:lip_score}. Assumption~\ref{assume:sulln_compact} holds by choosing the right the parameter space for the neural network (e.g., using {so-called} weight decay{, i.e., $\ell_2$ regularization}).}

    
    % \todo[inline,color=green]{YB: Explain to what function $f$ and to what sequence of r.v. $\X_n$ you wish to apply it?} 
    % \hvhdelete{first we valid its assumptions holds. Assumption ~\ref{assume:lip_cond} and ~\ref{assume:bound_b} holds using ~\ref{assume:lip_cont_post} and Lemma ~\ref{lemma:lip_score}.}
    Next, we show pointwise strong convergence (Assumption~\ref{assume:sulln_conv} ) hold by showing {that %the following exception
    $\expectation{\ell_B\brackets{\pr,\xset;\Omega}}{\xset,\pr}$ is finite.   }
    {
    \begin{align}\label{eq:conv_post_exp_finie_p1}
        &\expectation{\ell_B\brackets{\pr,\xset;\Omega}}{\xset,\pr}=
        \expectation{\norm{\postscore{\pr}{\xset;\Omega}}_2^2}{\xset,\pr}\nonumber\\
        &+2\expectation{{\trace{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}}}{\xset,\pr}.
    \end{align}
    The first term in \eqref{eq:conv_post_exp_finie_p1} is finite using Assumption~\ref{assum:expected_cond_score}. As for the second term we have:
    \begin{align}\label{eq:conv_post_exp_finie_p2}
        &\expectation{{\trace{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}}}{\xset,\pr}=\nonumber\\
        &\sum_i \expectation{\squareb{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}_{i,i}}{\xset,\pr}
    \end{align}
    % \todo[inline,color=green]{The Jacobian is a square matrix, so to get the trace you need to sum the $i,i$ elements. }
    Assuming Boundary {condition} \eqref{eq:boundary_conditions_post} {and} assumptions~\ref{assum:diff_prob_post} and ~\ref{assum:net_cond_score}{ all hold, allows to use %. Then using 
    }
    Lemma ~\ref{lemma:int_by_parts} by setting $g\brackets{\xset,\pr}=\squareb{\postscore{\pr}{\xset;\Omega}}_i$ and $h\brackets{\xset,\pr}=\probt{\pr|\xset}{\pr|\xset}$ yielding% we have that: 
    \begin{align}\label{eq:conv_post_exp_finie_p3}
       &w_i\triangleq\expectation{\squareb{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}_{i,i}}{\pr,\xset}\nonumber\\
       &=-\int_{\xset,\pr} \squareb{\postscore{\pr}{\xset;\Omega}}_i \squareb{\frac{\partial\probt{\pr|\xset}{\pr|\xset}}{\partial\pr}}_i\ d\xset d\pr\nonumber\\
       &=-\expectation{\squareb{\postscore{\pr}{\xset;\Omega}}_i \squareb{\nabla_{\pr}\log\probt{\pr|\xset}{\pr|\xset}}_i}{\pr,\xset}\nonumber\\
       &\leq \abs{\expectation{\squareb{\postscore{\pr}{\xset;\Omega}}_i \squareb{\nabla_{\pr}\log\probt{\pr|\xset}{\pr|\xset}}_i}{\pr,\xset}}
    \end{align}
    Next using Cauchy–Schwarz inequality we have:
    \begin{align}\label{eq:conv_post_exp_finie_p2_cs}
        &\abs{w_i}\\
        &\leq \sqrt{\expectation{\squareb{\postscore{\pr}{\xset;\Omega}}_i^2}{\pr,\xset}\expectation{\squareb{\nabla_{\pr}\log\probt{\pr|\xset}{\pr|\xset}}_i^2}{\pr,\xset}}\nonumber
    \end{align}
    Combining \eqref{eq:conv_post_exp_finie_p2} and \eqref{eq:conv_post_exp_finie_p3} and \eqref{eq:conv_post_exp_finie_p2_cs} along with Assumption ~\ref{assum:bound_expection_post} and ~\ref{assum:net_cond_score} %resulting in 
    showing that $\expectation{{\trace{\frac{\partial \postscore{\pr}{\xset;\Omega}}{\partial\pr}}}}{\xset,\pr}$ is finite and SLLN holds.
    }    
    Then we have pointwise strong convergence that results in
    \begin{equation}\label{eq:conv_probability}
        \lossbs\brackets{\Omega^*}-\lossbs\brackets{\tilde{\Omega}}\leq 2\sup\limits_{\Omega}\norm{\lossbsm\brackets{\Omega}-\lossbst\brackets{\Omega}}_2\xrightarrow{a.s.} 0,
    \end{equation}
    as $\nds\rightarrow\infty$.  In last part, we show that $\lossbs\brackets{\tilde{\Omega}}=0$:
    \begin{align}\label{eq:loss_zero}
        &\lossbs\brackets{\arg\min\limits_{\Omega}\lossbst\brackets{\Omega}}=\lossbs\brackets{\arg\min\limits_{\Omega}\lossbst\brackets{\Omega}+C}\nonumber\\
        &=\lossbs\brackets{\arg\min\limits_{\Omega}\lossbs\brackets{\Omega}}=0,
    \end{align}
    where $C$ is a constant independent of $\Omega$. The last step follows by the
   % is to use the 
    realizable score function assumption~\ref{assume:realizable}.  Finally, combining \eqref{eq:conv_probability} and \eqref{eq:loss_zero} yields Thm~\ref{thm:post_consist}. 
\end{proof}
\subsubsection{Prior and Fisher Score Consistency Proof (Thm~\ref{thm:mp_consist})}
First we 
establish %provide the 
that the Lipschit continuouity requirements are met. %lemmas.
\begin{lemma}[Fisher Score is Lipschitz continuous]\label{lemma:lip_score_fisher}
        {Define the per-sample loss of the Fisher score matching $\ell_F\brackets{\x,\p;\Omega}\triangleq \norm{\lscore{{\x}}{\p;\paramf}}_2^2
    +2\lscore{{\x}}{\p;\Omega}^T\priorscore{\p;\paramp{^*}}+2\trace{\frac{\partial \lscore{{\x}}{\p;\Omega}}{\partial\p}} $, and $\Delta_\Omega \triangleq  \norm{\Omega_{1}-\Omega_{2}}_2$. }
    Assume that: 
    \begin{itemize}
        \item  The Fisher score is  Lipschitz continuous:
        $\norm{\lscore{\x}{\p;\Omega_1}-\lscore{\x}{\p;\Omega_2}}_2\leq \xi_{F}\brackets{\x,\p}\Delta\Omega$
      \item {The Fisher} score {Jacobian matrix} is Lipschitz continuous 
      $$\norm{\nabla_{\p}\lscore{\x}{\p;\Omega_1}-\nabla_{\p}\lscore{\x}{\p;\Omega_2}}_2\leq \tau_F\brackets{\x,\p}\Delta_\Omega$$
      \item {The score} outer product is Lipschitz continuous
      $$\norm{\matsym{R}\brackets{\x,\p;\Omega_1}-\matsym{R}\brackets{\x,\p;\Omega_2}}_F\leq \zeta_F\brackets{\x,\p}\Delta_\Omega$$
  \end{itemize}
  In addition assume that the following expectations: $\expectation{\xi_F\brackets{\x,\p}^2}{\X,\pr}<\infty$, $\expectation{\tau_F\brackets{\x,\p}}{\X,\pr}<\infty$,  and  $\expectation{\zeta_F\brackets{\x,\p}}{\X,\pr}<\infty$ are finite.
    Then
    \begin{equation}
        \abs{\ell_F\brackets{\x,\p;\Omega_{1}}-\ell_F\brackets{\x,\p;\Omega_{2}}}\leq L_F\brackets{\x,\p}\Delta\Omega,
    \end{equation}
    where $L_F\brackets{\x,\p}=\sqrt{\np}\brackets{2\tau_F\brackets{\x,\p}+\zeta_F\brackets{\x,\p}}+2\xi\brackets{\x,\p}\norm{\priorscore{\p;\paramp{^*}}}_2$ is the Liphschitz constant. {Furthermore, $\expectation{L_F\brackets{\X,\pr}}{\X,\pr}<\infty$. }
\end{lemma}
% \todo[inline,color=green]{YB: similar comments to those for Lemma A.4\\
% HVH: Done and  also, I add that $\expectation{L_F\brackets{\X,\pr}}{\X,\pr}<\infty$ to all case since it is require for the SULLN.}
\begin{proof}
    \begin{align}
        &\abs{\ell_F\brackets{\x,\p;\Omega_1}-\ell_F\brackets{\x,\p;\Omega_2}}\leq \abs{p_1}+2\abs{p_2}+2\abs{p_3}\nonumber
    \end{align}
    where $p_1=\norm{\lscore{{\X}}{\p;\Omega_1}}_2^2-\norm{\lscore{{\X}}{\p;\Omega_2}}_2^2$, $p_2=\trace{\frac{\partial \lscore{{\X;\Omega_1}}{\p}}{\partial\p}}-\trace{\frac{\partial \lscore{{\X;\Omega_2}}{\p}}{\partial\p}}$ and
    \begin{equation*}
        p_3= \brackets{\lscore{{\X}}{\p;\Omega_1}-\lscore{{\X}}{\p;\Omega_2}}^T\priorscore{\p;\paramp{^*}}
    \end{equation*}
    To obtain the bound on $p_1$ and $p_2$, we use the same derivation as of Lemma 10 from \cite{crafts2023bayesian}. Next we left with bounding $p_3$:
    \begin{align*}
        p_3\leq \norm{\lscore{{\X}}{\p;\Omega_1}-\lscore{{\X}}{\p;\Omega_2}}_2\norm{\priorscore{\p;\paramp{^*}}}_2
    \end{align*}
    Using assumption ~\ref{assume:lip_cont} we have:
    \begin{align}\label{eq:p3_final}
        p_3\leq \xi_F\brackets{\x,\p}\norm{\priorscore{\p;\paramp{^*}}}_2\Delta\Omega
    \end{align}
    Combining the results of $p_1$ and $p_2$ with \eqref{eq:p3_final} results in Lemma ~\ref{lemma:lip_score_fisher}.
    Next, we proof that $\expectation{L_F\brackets{\X,\pr}}{\X,\pr}<\infty$. Using the Cauchy–Schwarz inequality we have that:
    \begin{align}
        \expectation{\xi_F\brackets{\X,\pr}\norm{\priorscore{\pr;\paramp{^*}}}_2}{\X,\pr}\leq \nonumber\\
        \sqrt{\expectation{\xi_F\brackets{\X,\pr}^2}{\X,\pr}\expectation{\priorscore{\pr;\paramp{^*}}^2}{\pr}}
    \end{align}
    which is finite using Assumption~\ref{assume:expecte_fine_score_matching} and the Lemma assumption $\expectation{\tau_F\brackets{\x,\p}^2}{\X,\pr}<\infty$. Combining with the rest of the Lemma assumption, we obtain that $\expectation{L_F\brackets{\X,\pr}}{\X,\pr}<\infty$
    
    
\end{proof}

%Next, we provide the prior Score is Lipschitz continuous
\begin{lemma}[Prior Score is Lipschitz continuous]\label{lemma:lip_score_prior}

    {Define the per-sample loss of the prior score matching $\ell_P\brackets{\p;\Omega}\triangleq{\norm{\priorscore{\p;\Omega}}_2^2}+2\trace{\frac{\partial \priorscore{\p;\Omega}{}}{\partial\p}} $, and $\Delta_\Omega \triangleq  \norm{\Omega_{1}-\Omega_{2}}_2$. }
    Assume that: 
    \begin{itemize}
      \item {The prior} score {Jacobian matrix} is Lipschitz continuous 
      $$\norm{\nabla_{\p}\priorscore{\p;\Omega_1}-\nabla_{\p}\priorscore{\p;\Omega_2}}_2\leq \tau_P\brackets{\p}\Delta_\Omega$$
      \item {The score} outer product is Lipschitz continuous
      $$\norm{\matsym{R}\brackets{\p;\Omega_1}-\matsym{R}\brackets{\p;\Omega_2}}_F\leq \zeta_P\brackets{\p}\Delta_\Omega$$
  \end{itemize}
where $\matsym{R}\brackets{\p;\Omega} \triangleq\priorscore{\p;\Omega}\priorscore{\p;\Omega}^T$. {In addition assume that $\expectation{\tau_P\brackets{\pr}}{\pr}<\infty$ and $\expectation{\zeta_P\brackets{\pr}}{\pr}<\infty$.}
    Then
    \begin{equation}
        \abs{\ell_P\brackets{\p;\Omega_{1}}-\ell_P\brackets{\p;\Omega_{2}}}\leq L_P\brackets{\p}\Delta_\Omega,
    \end{equation}
    where $L_P\brackets{\z}=\sqrt{\np}\brackets{2\tau_P\brackets{\x,\p}+\zeta_P\brackets{\x,\p}}$ is the Liphschitz constant. {Furthermore, $\expectation{L_P\brackets{\pr}}{\pr}<\infty$. }
\end{lemma}
The proof of Lemma~\ref{lemma:lip_score_prior} is the same as the proof of Lemma 10 in \cite{crafts2023bayesian}. 
To establish consistency in Thm~\ref{thm:mp_consist}, we employ Lemmas~\ref{lemma:lip_score_fisher} and ~\ref{lemma:lip_score_prior} and proceed with a derivation analogous to that in ~\ref{apx:proof:consistency_post}.


