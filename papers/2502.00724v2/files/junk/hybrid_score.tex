% \onecolumn
% \newpage
% \subsection{Model Informed Score Neural Network Examples}
% \todo[inline]{Should move into the examples...}
% % In this section we present several examples showing the use of Model Informed Score Network (MISN) shown in \eqref{eq:misn_base} and the optimal neural network:
% % \subsubsection{Linear Example}
% % Let assume the following measurement model:
% % \begin{equation}
% %     \X=\matsym{A}\p+\randomvec{W},
% % \end{equation}
% % where $\randomvec{W}$ is an additive Gaussian noise with zero mean and covariance matrix $\matsym{\Sigma}$. In this case, one can define $\mathcal{M}\brackets{\p}=\matsym{A}\p$, which results in:
% % \begin{equation}
% %     \nabla_{\p}\log\probt{\x|\p}{\X|\p}\approx\matsym{A}^T\at{g_{\Omega_g}\brackets{\x;\vectorsym{\tau}}}{\vectorsym{\tau}=\matsym{A}\p}.
% % \end{equation}
% % The optimal score for this linear case is given by:
% % \begin{equation}
% %    \nabla_{\p}\log\probt{\x|\p}{\X|\p}=\matsym{A}^T\Sigma^{-1}\brackets{\X-\matsym{A}\p}.
% % \end{equation}
% % In this case, the optimal $\vectorsym{g}_{\Omega_g}^*\brackets{\x,\vectorsym{\tau}}=\Sigma^{-1}\brackets{\x-\vectorsym{\tau}}$ which is a simple feedforward neural network. 


% \subsubsection{Quantization}
% Here, we present a general quantization example with a non-linear function of the parameter $\vectorsym{\varphi}\brackets{\p}$ which is assume to be known:
% \begin{equation}
%     \X=Q\brackets{\vectorsym{\varphi}\brackets{\p}+\randomvec{W},t,n_b},
% \end{equation}
% where $\randomvec{W}$ is an additive Gaussian noise with zero mean and covariance matrix $\matsym{\Sigma}$, $t$ is the quantization threshold and $n_b$ is the number of bits. Now the probability mass function (PMF) \todo[inline,color=green]{Not "pdf" -- $X$ is a discrete random vector} of $\X$ given $\p$ is:
% \begin{align}
%     &\pmft{\X|\p}{\X|\p}=\int_{\vectorsym{b}_l\brackets{\X_1}}^{\vectorsym{b}_h\brackets{\X_1}}\hdots\int_{\vectorsym{b}_l\brackets{\X_n}}^{\vectorsym{b}_h\brackets{\X_n}}\mathbb{N}\brackets{\vectorsym{s},\varphi\brackets{\p},\Sigma}d\vectorsym{s},
% \end{align}
% \todo[inline,color=green]{Elaborate the notation? What are the limits of the integral? Explain that when $X$ is a vector, the integral is a multiple integral?   }
% where $b_l$ and $b_u$ are functions that provide the upper lower limit of the quantization point
% and $\mathbb{N}\brackets{\vectorsym{s},\vectorsym{\mu},\Sigma}=C\exp\brackets{-\brackets{\vectorsym{s}-\vectorsym{\mu}}^T\Sigma^{-1}\brackets{\vectorsym{s}-\vectorsym{\mu}}}$ is the probability density function of the random Gaussian vector and $C$ is the normalization constant. The corresponding score is given by:
% \begin{align}
%     &\nabla_{\p}\log\probt{\X|\p}{\X|\p}=\\
%     &-\frac{\partial\varphi\brackets{\p}}{\partial\p}^T\frac{2\int_{\vectorsym{b}_l\brackets{\X}}^{\vectorsym{b}_h\brackets{\X}}\Sigma^{-1}\brackets{\vectorsym{s}-\varphi\brackets{\p}}\mathbb{N}\brackets{\vectorsym{s},\varphi\brackets{\p},\Sigma}d\vectorsym{s}}{\int_{\vectorsym{b}_l\brackets{\X}}^{\vectorsym{b}_h\brackets{\X}}\mathbb{N}\brackets{\vectorsym{s},\varphi\brackets{\p},\Sigma}d\vectorsym{s}}.\nonumber
% \end{align}
% In these cases, one can assume that $\mathcal{M}\brackets{\p}=\vectorsym{\varphi}\brackets{\p}$, then the optimal neural network is given by:
% $$\vectorsym{g}_{\Omega_g}^*\brackets{\x,\vectorsym{\tau}}=-\frac{2\int_{\vectorsym{b}_l\brackets{\X}}^{\vectorsym{b}_h\brackets{\X}}\Sigma^{-1}\brackets{\vectorsym{s}-\vectorsym{\tau}}\mathbb{N}\brackets{\vectorsym{s},\vectorsym{\tau},\Sigma}d\vectorsym{s}}{\int_{\vectorsym{b}_l\brackets{\X}}^{\vectorsym{b}_h\brackets{\X}}\mathbb{N}\brackets{\vectorsym{s},\vectorsym{\tau},\Sigma}d\vectorsym{s}}$$

% \subsubsection{DOA with Non Gaussian Noise}




% \subsubsection{Examples}
% Here we show how the physically informed score above helps in learning several examples: 
% \paragraph{Linear Example} Let set $\varphi\brackets{\p}=\matsym{A}\p$ be linear transformation and the identity matrix, this results in following score:
% \begin{equation}
%     \nabla_{\p}\log\probt{\x|\p}{\X|\p}=\matsym{A}^T\at{g_{\Omega_g}\brackets{\vectorsym{\epsilon};\x,\p}}{\vectorsym{\epsilon}=\x-\matsym{A}\p}
% \end{equation}
% The optimal score for this linear case is given by:
% \begin{equation}
%    \matsym{A}^T\Sigma^{-1}\brackets{\X-\matsym{A}\p}.
% \end{equation}
% The PINN only needs to learn $\Sigma^{-1}$ meaning $g\brackets{\vectorsym{\epsilon};\x,\p}=\Sigma^{-1}\vectorsym{\epsilon}$ in these cases.  Several notes on this examples:
% \begin{itemize}
%     \item We don't require $\x,\p$ as input to $g$.
%     \item There is a weight sharing of the matrix $\matsym{A}$ between the input and output projection. 
% \end{itemize}

% \paragraph{General Quantization} Let set $\varphi\brackets{\p}$:
% \begin{equation}
%     \X=Q\brackets{\varphi\brackets{\p}+\matsym{W}},
% \end{equation}
% where $\matsym{W}$ is Gaussian noise with zero mean then:
% \begin{align}
%     &\probt{\X|\p}{\X|\p}\\
%     &=C\int_{\vectorsym{b}_l\brackets{\X}}^{\vectorsym{b}_h\brackets{\X}}\exp\brackets{-\brackets{\vectorsym{s}-\varphi\brackets{\p}}^T\Sigma^{-1}\brackets{\vectorsym{s}-\varphi\brackets{\p}}}d\vectorsym{s}\nonumber\\
%     &=C\int_{\vectorsym{b}_l\brackets{\X}-\varphi\brackets{\p}}^{\vectorsym{b}_h\brackets{\X}-\varphi\brackets{\p}}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}\\
%     &=C\int_{\vectorsym{b}_l\brackets{\X}-\X+\vectorsym{\epsilon}\brackets{\p}}^{\vectorsym{b}_h\brackets{\X}-\X +\vectorsym{\epsilon}\brackets{\p}}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}
% \end{align}
% Let derive the score function
% \begin{align}
%     \nabla_{\p}\log\probt{\X|\p}{\X|\p}=\frac{\partial\vectorsym{\epsilon}\brackets{\p}}{\partial\p}^T\frac{\brackets{\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}-\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}}}{\int_{\vectorsym{b}_l\brackets{\X}-\X+\vectorsym{\epsilon}\brackets{\p}}^{\vectorsym{b}_h\brackets{\X}-\X +\vectorsym{\epsilon}\brackets{\p}}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}}
% \end{align}
% % If $\X$ is a point on the boundary of the set $\mathcal{Q}$ the integral in \eqref{} results on $I_{up}$ or $I_{down}$ while any other point the result is $I_c$. 
% %  \begin{equation}
% %     I_{up}=I_c+C\int_{\frac{\Delta}{2}+\vectorsym{\epsilon}\brackets{\p}}^{\infty}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}
% % \end{equation}
% % \begin{equation}
% %     I_c=C\int_{\frac{-\Delta}{2}+\vectorsym{\epsilon}\brackets{\p}}^{\frac{\Delta}{2} +\vectorsym{\epsilon}\brackets{\p}}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}
% % \end{equation}
% %  \begin{equation}
% %     I_{down}=I_c+C\int_{-\infty}^{-\frac{\Delta}{2}+\vectorsym{\epsilon}\brackets{\p}}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}
% % \end{equation}
% % If we assume that the upper and lower boundary integral are near zero we have that:
% % \begin{equation}
% %     \probt{\X|\p}{\X|\p}=C\int_{\frac{-\Delta}{2}+\vectorsym{\epsilon}\brackets{\p}}^{\frac{\Delta}{2} +\vectorsym{\epsilon}\brackets{\p}}\exp\brackets{-\vectorsym{z}\Sigma^{-1}\vectorsym{z}}d\vectorsym{z}
% % \end{equation}
% \paragraph{Linear Quantized}  Let set $\varphi\brackets{\p}=\matsym{A}\p+1\cdot b$ and $\matsym{W}\brackets{\p}=\matsym{I}$ be linear transformation and the identity matrix, this results in following score:
% \begin{equation}
%     \nabla_{\p}\log\probt{\x|\p}{\X|\p}=\matsym{A}^T\at{g_{\Omega_g}\brackets{\vectorsym{\epsilon};\x,\p}}{\vectorsym{\epsilon}=\x-\matsym{A}\p-1\cdot b}
% \end{equation}
% To match the optimal score in \eqref{eq:score_quant} $g$ should be defined as:
% \begin{equation}
%     g_{\Omega_g}\brackets{\vectorsym{\epsilon};\x,\p}=\frac{1}{\sqrt{2\pi\sigma^2_i}}\frac{\tilde{\Delta \vectorsym{E}}\brackets{\vectorsym{\epsilon};\x}}{\tilde{\Delta\vectorsym{\Phi}}\brackets{\vectorsym{\epsilon};\x}},
% \end{equation}
% where
% \begin{align*}
%     \tilde{\Delta \vectorsym{E}}_i&\triangleq  \exp\brackets{-\frac{\brackets{b_l\brackets{\x_i}-\x_i+\squareb{\vectorsym{\epsilon}}_i}^2}{2\sigma_i^2}}\\
%     &-\exp\brackets{-\frac{\brackets{b_u\brackets{\x_i}-\x_i+\squareb{\vectorsym{\epsilon}}_i}^2}{2\sigma_i^2}},
% \end{align*}
% \begin{equation*}
%     \tilde{\Delta\vectorsym{\Phi}}_i=\Phi\brackets{\frac{b_u\brackets{q_i}-\squareb{\vectorsym{u}\brackets{\p}}_i}{\sigma_i}}-\Phi\brackets{\frac{b_l\brackets{q_i}-\squareb{\vectorsym{u}\brackets{\p}}_i}{\sigma_i}}.
% \end{equation*}
% Several note on this examples:
% \begin{itemize}
%     \item $\varphi\brackets{\p}$ can be easily generalized into any function. 
%     \item In this example, we require $\x$ to define the upper and lower limits of the quantization bin. 
% \end{itemize}

% \paragraph{DOA Estimation}
%  Let set $\varphi\brackets{\p}=0$ and $\matsym{W}\brackets{\p}=\brackets{\matsym{A}^H\matsym{A}+\matsym{I}\sigma^2}^{0.5}$

% % \item  Localization: 
% % \end{itemize}



% % \paragraph{Case \rom{2}}
% % Let us extend case \rom{1} by assuming that the noise $\randomvec{V}$ is dependent on $\p$. This change the PDF of $\X$ to:
% % \begin{equation}
% %     \probt{\x|\p}{\X|\p}=\probt{\x-\varphi\brackets{\p}|\p}{\randomvec{V}|\p},
% % \end{equation}
% % and it corresponding score:
% % \begin{align}
% %     &\divc{\varphi\brackets{\p}}{\p}^T\at{\nabla_{\vectorsym{v}}\log\probt{\vectorsym{v}|\p}{\randomvec{V}|\p}}{\vectorsym{v}=\x-\varphi\brackets{\p}}\nonumber\\
% %     &+\at{\nabla_{\p}\log\probt{\vectorsym{v}|\p}{\randomvec{V}|\p}}{\vectorsym{v}=\x-\varphi\brackets{\p}}
% % \end{align}
% % Meaning that the physically informed score model in this case working on the modeling error instead of the input $\X$
% % \paragraph{Case \rom{3}} Adding  scaling noise to $\varphi$
% % \begin{equation}
% %     \X=\varphi\brackets{\p}\odot\randomvec{S} +\randomvec{V},
% % \end{equation}

% % \begin{equation}
% %     \probt{\x|\p,\randomvec{S}}{\X|\p,\randomvec{S}}=\probt{\x-\varphi\brackets{\p}\odot\randomvec{S}|\p}{\randomvec{V}|\p}
% % \end{equation}

% % \begin{align}
% %     \probt{\x|\p}{\X|\p}=\int_{\uvector}\probt{\x|\uvector,\p}{\X|\U,\p} \probt{\uvector|\p}{\U|\p}d\uvector
% % \end{align}
% % By take the derivative w.r.t. to $\p$ we have:
% % \begin{align}
% %     &\nabla_{\p}\probt{\x|\p}{\X|\p}=\nabla_{\p}\int_{\uvector}\probt{\x|\uvector,\p}{\X|\U,\p} \probt{\uvector|\p}{\U|\p}d\uvector\nonumber\\
% %             &=\int_{\uvector}\probt{\x|\uvector,\p}{\X|\U,\p} \nabla_{\p}\probt{\uvector|\p}{\U|\p}d\uvector\nonumber\\
% %             &+\int_{\uvector}\probt{\uvector|\p}{\U|\p}\nabla_{\p}\probt{\x|\uvector,\p}{\X|\U,\p} d\uvector\nonumber\\
% %             &=\int_{\uvector}\probt{\x|\uvector,\p}{\X|\U,\p} \probt{\uvector|\p}{\U|\p}\nabla_{\p}\log\probt{\uvector|\p}{\U|\p}d\uvector\nonumber\\
% %             &+\int_{\uvector}\probt{\x|\uvector,\p}{\X|\U,\p} \probt{\uvector|\p}{\U|\p}\nabla_{\p}\log\probt{\x|\uvector,\p}{\X|\U,\p} d\uvector\nonumber\\
% %     &=\int_{\uvector}\probt{\x,\uvector|\p}{\X,\U|\p}\nabla_{\p}\log\probt{\x|\uvector,\p}{\X|\U,\p} d\uvector\nonumber \\
% %     &+\int_{\uvector}\probt{\x,\uvector|\p}{\X,\U|\p}\nabla_{\p}\log\probt{\uvector|\p}{\U|\p}d\uvector
% % \end{align}

% % \begin{align}
% %     &\nabla_{\p}\log\probt{\x|\p}{\X|\p}\nonumber\\
% %     &=\int_{\uvector}\frac{\probt{\x,\uvector|\p}{\X,\U|\p}}{\probt{\x|\p}{\X|\p}}\nabla_{\p}\log\probt{\x|\uvector,\p}{\X|\U,\p} d\uvector\nonumber\\
% %     &+\int_{\uvector}\frac{\probt{\x,\uvector|\p}{\X,\U|\p}}{\probt{\x|\p}{\X|\p}}\nabla_{\p}\log\probt{\uvector|\p}{\U|\p}d\uvector
% % \end{align}