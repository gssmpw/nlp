\RequirePackage{xcolor}
\documentclass[journal,twoside,web]{ieeecolor}
% \documentclass[]{article}
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[colorlinks=true]{hyperref} % Load hyperref BEFORE cite (important!)
\usepackage{cite}     % Load cite AFTER hyperref

\usepackage[pdftex]{graphicx}
\usepackage{generic}
% \usepackage{cite}
\usepackage{amssymb,amsfonts}
\usepackage{amsthm}
\setlength{\marginparwidth}{1.3cm}
% \usepackage{hyperref}
% \usepackage[colorlinks]{hyperref}

\usepackage{todonotes} % [disable]
\input{includes/base}
\input{includes/commands}
\input{includes/symbols}
\usepackage{nameref}
% \usepackage{color-edits} % [suppress]
% \addauthor[Yoram]{yb}{red}
% \addauthor[Hai]{hvh}{blue}
% \addauthor[Hagit]{hmy}{green}




\begin{document}
\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}

\pgfdeclarelayer{background}
\pgfdeclarelayer{background1}
\pgfsetlayers{background1,background,main}

\title{Learned Bayesian Cram\'er-Rao Bound \\
for Unknown Measurement Models \\ {Using} Score Neural Networks}
\author{Hai Victor Habi, Hagit~Messer,~Life~Fellow,~IEEE and Yoram~Bresler,~Life~Fellow,~IEEE
\thanks{H.V. Habi and H. Meseer are with the School of Electrical Engineering, Tel Aviv University, Tel Aviv 6139001, Israel (e-mail: haivictorh@mail.tau.ac.il; messer@eng.tau.ac.il).}
\thanks{Y.Bresler is with the Department of Electrical and Computer Engineering and the Coordinated Science Lab, University of Illinois Urbana-Champaign, USA (e-mail: ybresler@illinois.edu).
}}
\date{}

\markboth{February 2025}%
{Habi, Messer, and Bresler: Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models  using Score Neural Networks}

\maketitle

\begin{abstract}
    The Bayesian Cramér-Rao bound (BCRB) is a crucial tool in signal processing for assessing the fundamental limitations of any estimation problem as well as benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed without full knowledge of the prior and the measurement distributions. In this work, we propose a fully learned Bayesian Cramér-Rao bound (LBCRB) that learns both the prior and the measurement distributions. Specifically, we suggest two approaches to obtain the LBCRB: the Posterior Approach and the Measurement-Prior Approach. The Posterior Approach provides a simple method to obtain the LBCRB, whereas the Measurement-Prior Approach enables us to incorporate domain knowledge to improve the sample complexity and {interpretability}. To achieve this, we introduce a Physics-encoded score neural network which enables us to easily incorporate such domain knowledge into a neural network. We {study the learning} errors of the two suggested approaches theoretically, and  validate them numerically. We demonstrate the two approaches on several signal processing examples, including a linear measurement problem with unknown mixing and Gaussian noise covariance matrices, frequency estimation, and quantized measurement. In addition, we test our approach on a nonlinear signal processing problem of frequency estimation with real-world underwater ambient noise.
\end{abstract}
\raggedbottom

\begin{IEEEkeywords}
 Score Matching, Bayesian-CRB, Parameter Estimation, Bayesian Fisher information, \pe{}.
\end{IEEEkeywords}

\section{Introduction}
The Bayesian Cramér-Rao Bound (BCRB) \cite{van2004detection} is a crucial tool in signal processing for {assessing} the fundamental limitations of any estimation problem within a Bayesian framework. For instance, the BCRB has been employed to elucidate various signal processing applications such as localization \cite{10184105,10140073} and MIMO systems \cite{nasir2013phase}, among others \cite{xu2004bayesian,rosentha2024asymptotically,mazor2024limitations}. Besides understanding the intrinsic limits of problems, the BCRB has also been used for system design, including waveform design \cite{huleihel2013optimal,turlapaty2014bayesian,zuo2010conditional,sun2024optimal}. 

However, to obtain the BCRB requires \emph{complete knowledge} of both the prior and the measurement distributions. In addition, in  some cases, even when both prior and measurement distributions are known, the BCRB cannot be computed, because the required integration over the parameter distribution does not have a close-form solution.  

%\todo[inline,color=green]{Break into paragraphs, for readability}
Various methods have been proposed %methods 
to derive a bound from data, {rather than analytically.} Some leverage prior knowledge of the problem combined with a learnable component. For instance, \cite{lutati22_interspeech} presents a non-Bayesian bound for single-channel speech separation. Another category of methods, as those described in \cite{duy2022fisher,6975144}, proposes bounds based on f-divergences. However, these methods require access to a {special} dataset {containing observations where for each parameter value there are also observations with a slightly perturbed parameter value.} 
% \todo[color=green]{What's that? HVH: It should be pertubation} 
This condition is generally viable only when one can generate an observation vector {for any desired value of the parameter vector that one aims to estimate}. 


Recently, thanks to the success of generative models in modeling complex, high-dimensional data distributions\cite{song2019generative,kobyzev2020normalizing}, a new approach has been introduced that suggested using a learned Generative Cram\'er Rao bound \cite{habi2023learning} when the measurement distribution is \emph{completely unknown,} but a dataset of independent and identically distributed  (i.i.d)  measurement-parameter pairs is available. The  Generative Cram\'er Rao bound \cite{habi2023learning} achieves this by first learning the measurement distribution using a generative model, and then utilizing it to obtain the learned Generative Cram\'er Rao bound. The approach of using a generative model to learn a performance estimation bound from data has been extended to several other non-Bayesian bounds, such as the misspecified CRB\cite{habi2023learned} and the Barankin bound \cite{habi2024learning}.  

Those approaches utilized normalizing flows\cite{kobyzev2020normalizing,papamakarios2021normalizing} which enable the computation of the probability density function of the measurements. This comes with a major limitation: that there exist an invertible mapping between the measurements and some base distribution that is analytically tractable (usually standard Gaussian). An example in which such mappings do not exist is when the measurements are quantized\cite{habi2022generative}. 

In %case of a 
the Bayesian setting, recent work \cite{crafts2023bayesian} suggested to learn the prior distribution using score matching \cite{hyvarinen2005estimation}. %\hvhedit
{After learning the prior score ($\nabla_{\p}\log\probt{\p}{\pr}$), obtaining the BCRB using this method requires complete {knowledge} of the Fisher score function ($\nabla_{\p}\log\probt{\x|\p}{\X|\pr}$) and its computation. {Given the known Fisher score function and the learned prior score function,} the BCRB is then approximately calculated using empirical means. A {limitation} of this approach is the need for  full knowledge of the Fisher score and ability to compute it, which can be challenging. For instance, calculating the Fisher score for quantized measurement with correlated noise involves integrating a multidimensional Gaussian with nondiagonal covariance, which must be done numerically. Another example is an application with incomplete knowledge of the Fisher score, such as noise from a physical image sensor \cite{abdelhamed2019noise}, or underwater noise \cite{weiss2023towards,msg0-ag12-22}.}



In this work, we %suggest
introduce the \emph{learned Bayesian Cram'er Rao bound (\name{})}, which learns both the prior and the measurement distributions. Specifically, we suggest two approaches to learn the BCRB. The first, the \emph{Posterior Approach,} is based on the score of the posterior distribution ($\nabla_{\p}\log\probt{\p|\x}{\pr|\X}$). The second, the \emph{Measurement-Prior Approach,} is based on the Fisher score function ($\nabla_{\p}\log\probt{\x|\p}{\X|\pr}$) and the prior score function ($\nabla_{\p}\log\probt{\p}{\pr}$). 

%\todo[inline,color=green]{Break into paragraphs, for readability}
The \emph{Posterior Approach} emphasizes learning a conditional score function of the parameter based on the given observation. This process requires 
{to learn} the posterior score function from a dataset consisting of observation-parameter pairs. Following this, the trained posterior score and the dataset are employed to evaluate the BCRB. The \emph{Posterior} Approach offers the advantage of simplicity in both the learning process and the design of the neural network. However, it is restricted in its capacity to integrate domain knowledge regarding the relationship between the parameter and the observations.  

To utilize such domain knowledge and obtain the benefits of combining with a learnable score (similar to model-based deep learning \cite{shlezinger2022model,shlezinger2023model}, physics-informed neural network \cite{banerjee2024physics}, {physics-encoded neural networks \cite{faroughi2024physics,meinders2024application}}), we propose the \emph{Measurement-Prior} Approach. This technique involves  learning the Fisher and prior score functions independently, and subsequently using them to calculate the learned BCRB. 

%Initially, 
We {learn} the Fisher and prior score functions from a dataset consisting of observation-parameter pairs. Although it is possible to learn the prior score function using traditional score matching \cite{hyvarinen2005estimation}, the same approach, {or existing conditional score matching \cite{hyvarinen2005estimation,liu2022estimating,yu2019generalized,yu2022generalized}},
%\todoing{add citation}, 
do not work for learning the Fisher score $\nabla_{\p}\log\probt{\x|\p}{\X|\pr}$ since the derivative is w.r.t. {the conditioning variable,} the parameter $\p$, {rather than w.r.t the conditioned variable $\x$, as in conditional  score matching.} To address this issue, we introduce a variation of score matching that enables to learn the Fisher score, which we call \emph{Fisher Score Matching (FSM)}. This {new} score matching process involves first learning the prior score function via standard score matching \cite{hyvarinen2005estimation} and then using the obtained prior score function to learn the Fisher score function. 

The primary advantage of employing the {Measurement-Prior} Approach is the integration of domain {knowledge}. {Inspired by \cite{faroughi2024physics,meinders2024application}, we suggest a \emph{\pe{} Score Neural Network,} a new type of score neural network that encodes the physicals of the problem in the structure of the network. This improves the accuracy of the approximation and {reduces} the sample complexity of learning it.} Moreover, this method enables to evaluate the learned BCRB in scenarios with {any desired number of} i.i.d. measurements, without {having to learn} a new score function.

% \todo[inline]{Inspired by \cite{}, we suggest a \pe{} score neural network a new type of score neural network that encodes the physicals of the problem in the structure of the network. This improves the accuracy of the approximation and simplifies the sample complexity.}


% \ybdelete{\hvhedit{In the experiment, neural networks were trained with condition variables\cite{},
% \todoin{missing citation} such as the signal-to-noise ratio, allowing us to obtain a single neural network for all SNR values. }}

We apply the \name{} %across multiple 
to several examples to show its benefits and {study} its behavior. {As a technical aspect in the experiments, neural networks were trained with conditioning variables\cite{mirza2014conditional,abdelhamed2019noise,liu2019conditional,ho2021classifier}, 
%\todoing{Missing citation} 
such as the signal-to-noise ratio, allowing us to obtain a single neural network for all SNR values.}


Our findings on a linear mixing measurement model with Gaussian noise,  and on its {1-bit} quantized variant illustrate the advantages of directly learning the Fisher score function from data. In these scenarios, the BCRB is known analytically or can be calculated numerically,
{enabling us to assess} the  {deviation of the \name{} from the exact BCRB due to learning error.} {Next, testing} 
 our approach on a non-linear signal processing problem of frequency estimation, highlights the advantages of a \pe{} score neural network {for reducing}
 sample complexity, {and enabling highly-accurate approximation to the BCRB with limited data.}
Lastly, we {study} two signal processing problems in which the BCRB cannot be computed with previous methods: %which include 
measurement with correlated noise and quantization; and frequency estimation with underwater ambient noise. {While the first problem is important in many sensor systems, including sensor arrays, the second is important for vessel identification \cite{erbe2019effects}.}  


The {main} contributions of this paper are %summarized 
as follows:
\begin{itemize}
    \item We introduce a fully learned Bayesian Cram'er-Rao bound that learns both prior and measurement distributions using score matching.
    
    \item  We introduce Fisher score matching (FSM) that enables to learn the Fisher score function. %\hvhedit{
    The Fisher score {%has additional 
    may be of independent}
    interest, 
    %usage 
    e.g., for the computation of {a learned} non-Bayesian CRB.
    \item  %\hvhedit
    We theoretically explore the FSM and demonstrate that {it produces a strongly consistent estimate of the true score.}
    \item  We {quantify theoretically the non-asymptotic, finite sample learning} error of the two suggested approaches, and validate {the theoretical predictions.}  numerically. %\hvhedit{
    In addition, we show that the LBCRB is a strongly consistent {estimator of} the true BCRB. 

    % HVH: Done and yes they have this in different way.}
    \item  {We demonstrate the two new approaches} on several signal processing examples, including linear and non-linear estimation {and} a real-world application  with underwater ambient noise. 
    %\todoing{Listing the examples one by one does not read well in a list of main contributions. Say something more general?}
\end{itemize}

In the spirit of reproducible research, we make the code of the Learned Bayesian Cram\'er-Rao bound available online \cite{lbcrb_repo}.

The paper is organized as follows. The background and notation are  in Section~\ref{sec:background}. 
The Learned Bayesian Cram'er-Rao bound method is presented in Sec.~\ref{sec:lbcrb_method}. A detailed derivation of Learned Bayesian Cram'er-Rao bound is present in Sec.~\ref{sec:lbcrb} followed by a {study} of its theoretical properties in Sec.~\ref{sec:theory}.   In Sec.~\ref{sec:example_models} we present a set of parameter estimation examples, including linear and nonlinear estimation problems, and frequency estimation with underwater ambient noise. The experimental results for the \name{} are described in Sec.~\ref{sec:experimental}, with conclusions in
%and 
Sec.~\ref{sec:conclusions}.
%provides discussion and conclusions. 
Proofs of the theoretical results of this paper are  in the appendices, which are included in the online Supplementary Material.



\input{files/background}
\input{files/method_short}
\input{files/learend_bayesian_bound}
\input{files/theortical_results}
\input{files/model_examples}
\input{files/results}

\section{Conclusions}\label{sec:conclusions}
%\todo[inline,color=green]{Break into paragraphs, for readability}
This paper proposes two approaches to determine a learned Bayesian Cramér-Rao Bound (LBCRB): the Posterior Approach, and the Measurement-Prior Approach. Both approaches derive the LBCRB using only a dataset of independent and identically distributed (i.i.d.) measurement-parameter pairs. In the Posterior Approach we learn the score of the parameter conditioned on the measurements and use it to derive the BCRB. For the Measurement-Prior Approach, we introduce a variant of score matching that enables  to learn the Fisher score from i.i.d. samples. In addition, we propose a \pe{} score neural network that integrates domain knowledge into the network. 

We  {studied} both approaches theoreticallyand demonstrated the superiority of the Measurement-Prior Approach  over the Posterior Approach when domain knowledge is available, {or when it is desired to compute the \name{} for a measurement comprising multiple i.i.d. samples}. Furthermore, we showed theoretically that in both approaches the LBCRB converges almost surely to the true BCRB . 

We conduct several numerical experiments that validate our theoretical findings and highlight the advantages of the Measurement-Prior Approach when domain knowledge is present. Finally, we present two scenarios where the BCRB cannot be evaluated but the LBCRB can: one involving correlated quantization measurement and the other involving frequency estimation with real-world underwater noise. These examples highlight the usefulness of LBCRB in real life problems. 

% Questions for future research include quantifying the
% impacts of limited representation power of the score model and a limited training data set on the accuracy of
% the LBCRB.  
% \todoin{This can be confusing for the reader: we did study the effect of limited training data, and the effect of finite representation power - as expressed in the score mismatch. Can you provide a tighter description of this proposed direction to eliminate such confusion - or just drop this suggested direction?}

A future research direction is to ensure that LBCRB is
a valid lower bound (rather than a good appropriation to
it) by utilizing methods for error estimation and model
selection \cite{shalev2014understanding}. On the practical side, it will be interesting
to study some of the many real-world applications that
can benefit from this approach, such as inverse problems with poorly characterized sensors. Finally, an addition direction is to use the Fisher score to obtain a learned maximum likelihood estimator. 




\bibliographystyle{IEEEtran}
\bibliography{ref}

\clearpage
\appendix
\input{files/appendix/implamentation_deatiles}
\input{files/proofs/proofs}
\input{files/appendix/examples}
\input{files/appendix/prior_fim}
% \input{files/appendix/unifrom_rolling_off}


\end{document}
