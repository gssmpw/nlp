%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage{subcaption}
\usepackage{caption}
\captionsetup[lstlisting]{labelformat=empty,labelsep=none,justification=centering, singlelinecheck=false}
\usepackage{enumitem}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfig}
\usepackage{xcolor}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Spatial-RAG: Spatial Retrieval Augmented Generation for  Real-World Spatial Reasoning Questions}

% \usepackage{amsthm}
% \theoremstyle{plain}
% \newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter
% \newtheorem{lem}[thm]{Lemma}
% \newtheorem{co}[thm]{Corollary}

% \theoremstyle{definition}
% \newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
% \newtheorem{exmp}[thm]{Example} % same for example numbers

\begin{document}

\twocolumn[
\icmltitle{Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Dazhou Yu}{equal,Emory}
\icmlauthor{Riyang Bao}{equal,Emory}
\icmlauthor{Gengchen Mai}{ut}
\icmlauthor{Liang Zhao}{Emory}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{Emory}{Department of Computer Science, Emory University, Atlanta, United States}
\icmlaffiliation{ut}{Department of Geography and the Environment, University of Texas at Austin, Austin, United States}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Gengchen Mai}{gengchen.mai@austin.utexas.edu}
\icmlcorrespondingauthor{Liang Zhao}{liang.zhao@emory.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{RAG, spatial, LLM agent, tourism question answering}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Spatial reasoning remains challenging for Large Language Models (LLMs), which struggle with spatial data retrieval and reasoning. We propose Spatial Retrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to spatial tasks by integrating sparse spatial retrieval (spatial databases) and dense semantic retrieval (LLM-based similarity). A multi-objective ranking strategy balances spatial constraints and semantic relevance, while an LLM-guided generator ensures coherent responses. Experiments on a real-world tourism dataset show that Spatial-RAG significantly improves spatial question answering, bridging the gap between LLMs and spatial intelligence.
% Spatial reasoning remains a significant challenge for Large Language Models (LLMs), which are traditionally optimized for text-based tasks and struggle with understanding and processing spatial data. This limitation becomes especially evident in domains requiring precise spatial reasoning and decision-making, such as geographic question answering and navigation. To address this gap, we propose Spatial Retrieval-Augmented Generation (Spatial-RAG), a novel framework designed to extend the capabilities of LLMs to handle complex spatial queries effectively. 
% Spatial-RAG integrates a spatial database within a Retrieval-Augmented Generation (RAG) system, facilitating direct handling of spatial reasoning questions through explicit spatial computation and retrieval. %geometric data processing and retrieval.
% Spatial-RAG bifurcates questions into spatial and semantic components, where spatial databases resolve the former through explicit spatial computations, and LLMs process the latter by generating contextually relevant textual responses. Spatial-RAG's architecture is uniquely positioned to synergize the spatial computation capabilities provided by spatial databases with the rich semantic understanding of LLMs. This paper elaborates on the transformation of spatial reasoning questions into structured spatial queries and introduces a unified approach to manage both spatial and textual data as a constrained multi-objective optimization problem. 

\end{abstract}

\section{Introduction}
Spatial questioning has long been a fundamental domain \cite{obe2021postgis}, which adeptly handles a variety of spatial questions, from identifying closest neighbors to detecting line-polygon intersections. However, traditional spatial questioning systems rely on specialized spatial query languages, which are vastly different from human language, making them inaccessible to layman users.  More critically, these systems lack the ability to infer complex spatial and semantic relationships from the nuanced, context-rich nature of human text, limiting their applicability to real-world question-answering scenarios. 
Recent advancements in Large Language Models (LLMs) have transformed many fields within machine learning (ML), particularly in understanding and generating human-like text. This progress has inspired early efforts to bridge the gap between spatial questioning and natural language by directly eliciting spatial knowledge from LLMs. These efforts span a range of applications, including geographic encyclopedic question answering \citep{mai2021geographic,mai2020se,chen2014parameterized,scheider2021geo,contractor2021joint}, geolocalization \citep{zhou2024img2loc,haas2024pigeon}, and automated High-definition map generation \citep{li2022hdmapnet,liu2023vectormapnet}. Despite these advancements, recent studies indicate that LLMs exhibit significant limitations in spatial reasoning \citep{mai2024opportunities,roberts2023gpt4geo}, and struggle with even basic spatial tasks, such as geoparsing \citep{mai2024opportunities} and understanding relative spatial relationships \citep{majic2024spatial}. This gap becomes particularly evident when handling real-world spatial reasoning tasks, such as the one illustrated in Figure \ref{fig:intro}, where the system must recommend a restaurant along a specific route within a predefined area.

\begin{figure}[t!]
  \centering
\includegraphics[width=0.95\linewidth]{fig/intro.png}
  \caption{An example of a real-world spatial reasoning question with nearby spatial objects. Areas that satisfy the spatial constraint are highlighted in blue. }
  \vspace{-0.3cm}
  \label{fig:intro}
\end{figure}

This paper tackles a novel challenge: augmenting LLMs with spatial reasoning capabilities. Specifically, we extend Retrieval-Augmented Generation (RAG) into spatial information retrieval and reasoning, bridging the gap between structured spatial databases and unstructured textual reasoning. 
RAG has demonstrated its effectiveness in knowledge-intensive tasks, such as question answering (QA) \cite{siriwardhana2023improving}, by retrieving domain-specific documents to enhance LLM responses. 
However, existing RAG systems primarily focus on retrieving and generating textual content and lack the spatial intelligence required for spatial reasoning tasks, especially tasks that involve understanding and computing complex spatial relationships among geometries, including points, polylines, and polygons.  

% Despite the pervasive use of document-based knowledge bases—often represented as vectors \cite{lewis2020retrieval} or graphs \cite{edge2024local} for facilitating retrieval systems, these methods aren't typically optimized for handling spatial data and spatial computation, which inherently relies on geometries. Some contemporary approaches attempt to bridge this gap using multimodal models that integrate visual and linguistic data \cite{xia2024rule}. Nevertheless, these methods typically falter when tasked with complex spatial reasoning, struggling, for instance, to accurately determine the relative positions of objects either depicted in images or described textually. 

% Spatial reasoning questions inherently pose several challenges:
% 1) Spatial information is hard to describe using natural language.
% 2) LLM struggles to reason with spatial information. 
% 3) LLM struggles to integrate spatial data with semantic understanding.
Spatial questioning is intrinsically challenging, requiring a synergistic combination of text-based inference and spatial computations. For example, as illustrated in Figure \ref{fig:intro}, answering the question requires LLM to elicit and formulate the user's textual request into the problem of "finding points near the polyline" and solve it based on a spatial map (database). Then, it also requires inferring user preference to select the spatially and semantically preferred candidates. 
Thus, the system must seamlessly integrate structured spatial retrieval with unstructured text-based reasoning, ensuring both spatial accuracy and contextual understanding.
To achieve this, we introduce \textit{Spatial Retrieval-Augmented Generation (Spatial-RAG)}, a novel framework that unifies text-guided spatial retrieval with spatially aware text generation. Specifically, to identify spatially relevant candidate answers, we propose a novel spatial hybrid retrieval module combining sparse and dense retrievers. 
To rank the candidates and generate the final answers, we propose to fuel the generator with retrieved results on the Pareto frontier based on a spatial and semantic joint ranking strategy.
% We define spatial information as constraints of spatial objects and use an LLM to parse natural language questions to spatial database queries.
% Through this approach, we aim to harness and expand the capabilities of LLMs in managing and responding to spatial reasoning questions, enhancing their practicality in real-world scenarios that demand nuanced spatial understanding.
Our contributions are summarized as follows:
\vspace{-0.3cm}
\begin{itemize}[leftmargin=*,wide]
\setlength\itemsep{-0.2em}
\item \textbf{A generic Spatial RAG framework}: We introduce spatial-RAG, the first framework that extends RAG to spatial question answering, to tackle a broad spectrum of spatial reasoning tasks, such as geographic recommendation, spatially constrained search, and contextual route planning. Our approach seamlessly integrates spatial databases, LLMs, and retrieval-based augmentation, enabling effective handling of complex spatial reasoning questions directly within the familiar operational paradigm of LLMs.

\item \textbf{Sparse-dense spatial hybrid retriever}: We propose a hybrid retrieval mechanism that combines sparse retrieval (SQL-based structured queries) with dense retrieval (LLM-powered semantic matching). This dual approach ensures that retrieved results align both spatially and semantically with the user’s query, significantly improving retrieval accuracy in spatial contexts.

\item \textbf{Multi-objective guided spatial text generator:} To handle both spatial constraints and textual reasoning in the spatial question-answering task, we introduce a multi-objective optimization framework that dynamically balances trade-offs between spatial and semantic relevance. This ensures that generated responses are both geometrically accurate and linguistically coherent.

\item \textbf{Real-World Evaluation:} We evaluated our method on a real-world dataset collected from a Tourism website that has user questions and reviews about different spatial entities. Experiments on this dataset reveal the ability to handle real-world spatial reasoning questions.
\end{itemize}
\vspace{-0.3cm}
Through these innovations, Spatial-RAG significantly enhances the spatial reasoning capabilities of LLMs, bridging the gap between structured spatial databases and natural language question answering.

\section{Related Work}
\subsection{Retrieval Augmented Generation}
Retrieval-Augmented Generation (RAG) is a hybrid approach that integrates retrieval systems and generative models to enhance factual accuracy and contextual relevance in natural language generation \cite{fan2024survey}. Unlike conventional language models that rely solely on parametric memory, RAG dynamically retrieves relevant external knowledge before generating a response.
One of the foundational works in RAG is %the model proposed by 
\citet{lewis2020retrieval}, where a retrieval module fetches relevant passages from a large-scale knowledge corpus (e.g., Wikipedia), which are then fused with the question context to generate a more informed response. This technique has proven particularly effective in open-domain question answering (QA), fact verification, and context-aware text generation.
RAG systems have expanded beyond text and document retrieval to incorporate %various data formats. 
a wide variety of data types \cite{he2024g} — tables, graphs, charts, and diagrams. %, and has been proposed to integrate.
While RAG has been widely explored, its application in spatial reasoning question answering remains an unexplored research area. Existing studies have primarily focused on knowledge-grounded dialogues \cite{yu2024llms} but often struggle with integrating spatial computation into the question-answering process effectively.
\subsection{Spatial Questions}
Spatial questions in domain-specific applications can generally be categorized into two distinct types:
\textbf{1) Textual Knowledge-based Spatial Questions} These are spatial questions that can be answered by traditional QA methods without the need for spatial computation and reasoning \cite{lietard2021language}.
% where both the question and its corresponding answers reside primarily in textual formats. 
For example, the question \textit{"What is the population of Los Angeles city?"} falls under this category. Despite their spatial context, these questions are essentially text-based and, hence, can be effectively addressed using traditional Retrieval-Augmented Generation (RAG) methods \cite{christmann2024rag}.
\textbf{2) Spatial Reasoning Questions} This category encapsulates spatial questions that demand a model's capability to comprehend and reason with spatial data and spatial relationships. A common example is a model being presented with textual information describing the spatial relationships among multiple objects \cite{li2024advancing}. An example question could be, \textit{"What is the position of object A relative to object B?"}, where objects \textit{A} and \textit{B} are locations or entities specified on the map. Resolving such queries requires a profound understanding of spatial concepts and robust reasoning skills, which largely depend on the model’s training to handle spatial data.
Several studies  \citep{mai2024opportunities,roberts2023gpt4geo} have investigated the capacity of LLMs to understand spatial concepts, yet these models often struggle with accurate reasoning even after fine-tuning.   Other research \cite{li2023geolm} has attempted to enhance this ability by converting geolocation coordinates into addresses to enrich the semantic context.   However, these improvements tend to be marginal and are mostly limited to straightforward reasoning tasks like describing positions. Moreover, many existing methods rely on predefined sets of actions tailored to specific tasks.

\section{Problem Formulation}
In this study, our primary focus is %on the complex domain of 
\textbf{Spatial Reasoning Questions}. We formulate the problem as follows:
Given a question $q$, the system aims to generate an answer $y$,
\begin{equation}
\begin{aligned}
y^* &= \arg \max_{y} \ \lambda_s^T f_s(q, y) + \lambda_k^T f_k(q, y)  \\
\text{s.t.} \quad & y \in C_s(q), \quad y \in C_k(q),\\
& \lambda_s \geq 0, \quad \lambda_k \geq 0,\\
& \mathbf{1}^T \lambda_s + \mathbf{1}^T \lambda_k = 1,
\end{aligned}
\label{eq:problem_improved}
\end{equation}
where $f_s\in \mathbb{R}^{d_s}$ is the spatial relevance score vector, $f_k\in \mathbb{R}^{d_k}$ is the semantic relevance score vector, $C_s$ is the spatial candidate set that satisfies the spatial constraints of the question, $C_k$ is the semantic candidate set that satisfies the semantic constraints of the question, $\lambda_s, \lambda_k$ are the spatial weights and semantic weights, respectively,  $y^*$ is the optimal answer, $\mathbf{1}^T\lambda_s + \mathbf{1}^T \lambda_k = 1$ ensures a normalized trade-off. 

To solve this problem, there are three questions to answer: 1) How to acquire the spatial candidate set $C_s$? 2) How to synergize the spatial and texts by evaluating $f_s(q,y)$? 3) How to trade off the spatial and semantic aspects?
% This forms the basis of our approach, wherein various subsets of spatial questions are formulated as a combination of this retrieval step followed by task-specific downstream processing. Examples include {1) Point of Interest (POI) Recommendation Task:} The system directly matches the retrieved locations to the user's requirements.{2) Travel Planning Task:} Based on the retrieved locations, the system plans an optimal travel route.{3) Descriptive Task:} The system provides detailed summaries or contextual descriptions of the retrieved locations.

\section{Methodology}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=0.95\textwidth]{fig/arch.png}
  \vspace{-0.3cm}
  \caption{Illustration of the proposed Spatial-RAG framework.}
  \vspace{-0.1cm}
  \label{fig:arch}
\end{figure*}

% \begin{figure*}[htb]
%   \centering
%   \includegraphics[width=0.5\textwidth]{fig/archc.jpg}
%   \caption{Illustration of the proposed framework.}
%   \label{fig:arch}
% \end{figure*}
\subsection{Overview}
Our proposed framework, Spatial Retrieval-Augmented Generation (Spatial-RAG), is illustrated in Figure \ref{fig:arch}. Spatial-RAG consists of three key stages:
First, to construct the spatial candidate set $C_s$, the system must precisely define spatial constraints 
% by parsing the natural language question into a spatial SQL query, 
and then retrieve spatial objects that satisfy them. As depicted in Figure \ref{fig:arch} (Sparse Spatial Retrieval), we achieve this by parsing the input natural language questions into a spatial SQL query, which will be executed on the spatial database to efficiently retrieve relevant spatial objects from the database. This process is detailed in Section \ref{sec:sql}.
Second, to effectively compute spatial relevance $f_s(q,y)$ while integrating textual information, we propose a hybrid spatial retrieval scheme. As shown in Figure \ref{fig:arch} (Dense Retrieval Module), this method combines sparse spatial relevance scores from the database with dense semantic similarity scores from text embeddings. This enables the system to rank retrieved spatial objects based on their spatial relevance to the input question, as detailed in Section \ref{sec:ranking}.  
Third, given both spatial and semantic constraints, %may conflict, 
we formulate a multi-objective optimization problem to balance these factors. The system computes the Pareto front of candidate answers, and the LLM dynamically trades off among these solutions to generate an optimal response. This step is covered in Section \ref{sec:generation}.

% , intelligently partitions a user question into spatial and semantic components. The spatial component, including geometries and relationships, is processed using a spatial database, which returns a curated set of Points of Interest (POIs) along with their spatial relevance. The semantic component is handled by an LLM that leverages its superior semantic reasoning capabilities to generate a coherent response based on the integrated data. Spatial-RAG's modular design allows for easy extension to various types of spatial queries by incorporating specific downstream task modules.


\subsection{Sparse Spatial Retrieval}\label{sec:sql}
The answer to a spatial reasoning question must meet specific spatial constraints. The spatial candidate set \( C_s(q) \) consists of all possible answers \( y \) that satisfy a set of spatial constraints \( \mathcal{C}_s(q) \). Formally, we define:
\begin{equation}
C_s(q) = \{ y \mid {c}_s(y, q) \leq 0, \forall {c}_s \in \mathcal{C}_s(q) \},
\end{equation}
where 
${c}_s(y,q)$  represents a constraint function that encodes a spatial condition (e.g., topological, directional, or distance-based constraints),
$\mathcal{C}_s(q)$  is the set of all spatial constraints associated with the question $q$.
For example, if the spatial constraint requires \( y \) to be within a distance \( \epsilon \) from a reference location \( l_q \), then a possible constraint function is:
\begin{equation}
c_s(y, q) = \text{d}(y, l_q) - \epsilon \leq 0.
\end{equation}
This formulation ensures that only spatially valid answers are included in \( C_s(q) \).

Addressing spatial constraints requires executing a well-defined spatial SQL query within a spatial database. This process involves identifying the appropriate query function, the reference spatial objects, the target spatial objects, and any necessary numerical parameters. Formally, a spatial SQL query can be expressed as:
\begin{equation}
Q_s = \mathcal{F}_s(G_r, G_t, \epsilon)
\label{eq:sql}
\end{equation}
where
\( \mathcal{F}_s \) is the spatial query function that determines the relationship between objects.
\( G_r \) represents the set of reference objects extracted from the question.
\( G_t \) represents the set of target objects that are potential answers.
\( \epsilon \) is the set of numerical parameters and spatial relationships governing the spatial constraint (e.g., distance threshold, topological relations).

Given the diversity and potentially complex nature of these constraints, Large Language Models (LLMs) often struggle to directly construct a complete and executable spatial query from user input. To bridge this gap, we structure the spatial query incrementally, allowing the LLM to systematically populate the required components.

Our approach follows three key steps:
1) \textbf{Geometry Recognition:} Identify and extract the reference spatial objects \( G_r \) and candidate target spatial objects \( G_t \) from the user’s input and extract their spatial footprints -- geometries.
2) \textbf{Query Function Selection:} Determine the appropriate spatial function \( \mathcal{F}_s \) based on the intended spatial relationship (e.g., containment, proximity).
3) \textbf{Parameter Estimation:} Assign numerical constraints \( \epsilon \) to ensure precise spatial filtering (e.g., buffer radius).

By formalizing this structured process, we enhance the LLM’s ability to generate accurate and executable spatial SQL queries. This, in turn, improves the system’s capability to handle complex spatial reasoning questions effectively. 

\subsubsection{Geometry Recognition}

In spatial reasoning tasks, accurately identifying spatial objects and extracting their spatial footprints (i.e., geometries) are essential for parsing questions to spatial queries. Spatial footprints of spatial objects, denoted as \( g\in\mathcal{G} \), can generally be categorized into three fundamental types: points, polylines, and polygons. Formally, we define these categories as follows:
\vspace{-0.3cm}
\begin{itemize}[leftmargin=*,wide]
\setlength\itemsep{-0.2em}
    \item \textbf{Point:}  
    \(    \mathcal{G}_\text{point} = \{ g \mid g \in \mathbb{R}^2, \dim(g) = 0 \} \)
    This category includes single points and multipoints, representing locations with negligible area. Examples include stop signs, address points, and a user's current location. In spatial databases, these entities are typically represented as the 'Point' geometry type.

    \item \textbf{Polyline:}  
    \(    \mathcal{G}_\text{line} = \{ g \mid g \subseteq \mathbb{R}^2, \dim(g) = 1 \} \)
    Polylines, including multipolylines, represent linear one-dimensional objects with negligible width. Common examples include streets, streams, bus routes, and power lines. In spatial databases, these geometries are abstracted as the 'LineString' type.

    \item \textbf{Polygon:}  
    \(    \mathcal{G}_\text{polygon} = \{ g \mid g \subseteq \mathbb{R}^2, \dim(g) = 2 \}   \)
    Polygons, including multipolygons, represent two-dimensional objects that define enclosed areas. These geometries are essential for depicting regions such as census areas, parcels, counties, neighborhoods, and zoning areas.
\end{itemize}
\vspace{-0.3cm}
The complexity of a spatial query depends on the types of spatial footprints of objects involved. For simpler queries, such as "finding the nearest bus stop from a given location", only point geometries are required, and the spatial candidate set is
\begin{equation}
    C_s =\{ g \vert g \in \mathcal{G}_\text{point}, d(g, g_\text{point}) < \epsilon \}
\end{equation}
where \( g_\text{point} \subseteq \mathcal{G}_\text{point} \) represents a point object (e.g., given location), $\epsilon$ is the distance threshold. 
% \begin{equation}
%     \arg\min_{g \in \mathcal{G}_\text{point}} d(q, g),
% \end{equation}
% where \( q \in \mathbb{R}^2 \) is the query location and \( d(q, g) \) represents the Euclidean or geodesic distance, 
For more complex queries, such as "I will walk from home to the university campus along 7th Street and Jones Street; please recommend a café where I can buy breakfast on my walk.", multiple geometry types must be considered, and the spatial candidate set is
\begin{equation}
    C_s =\{ g \vert g \in \mathcal{G}_\text{point}, g \in B(g_\text{polyline},\epsilon)\cup g_\text{polygon}\}
\end{equation}
where \( g_\text{polyline} \subseteq \mathcal{G}_\text{polygon} \) represents a polyline object (e.g., a route), \( g_\text{polygon} \subseteq \mathcal{G}_\text{polygon} \) represents a polygonal region (e.g., a university campus), $B$ is a buffer around the polyline, $\epsilon$ is the buffer size. 
 
By structuring spatial queries in this way, we ensure precise geometric representation, facilitating robust spatial reasoning and query execution.


\subsubsection{Query Function Recognition and Parameter Estimation}
After recognizing the geometries involved in a spatial query, the subsequent step is to determine the appropriate spatial query functions $\mathcal{F}_s$ required to handle various geometrical interactions. Despite the differing interactions among geometries, these can be uniformly addressed using distance functions $\text{d}(g_r, g_t)$, which calculate the shortest distance between two geometrical entities $g_r, g_t \in \mathcal{G}$.

Formally, given sets of reference geometries $G_r \subseteq \mathcal{G}$ and target geometries $G_t \subseteq \mathcal{G}$ , the spatial candidate set $C_s$ can be defined as:
\begin{equation}
% \begin{array}{ll}
\begin{cases} 
\{g_t \in G_t \mid \exists g_r \in G_r, \text{d}(g_r, g_t) \leq \epsilon \} , &\text{if }\text{d}(g_r, g_t) > 0, \\
\{g_t \in G_t \mid \exists g_r \in G_r, g_r \cap g_t \neq \emptyset \}, &\text{if } \text{d}(g_r, g_t) = 0.
\end{cases}
% \end{array}
\label{eq:distance_query}
\end{equation}


Parameters such as search radius or buffer distance $\epsilon$ are autonomously determined by the LLM, typically grounded in contextual understanding (e.g., estimated walking distance or area of interest). The parameter $\epsilon$ can be represented as:
$\epsilon = \phi(q)$,
where $\phi$ is a function that maps the context of the query $q$ to an appropriate numerical value.

Once the geometries $G_r$, $G_t$, functions $\mathcal{F}_s$, and parameters $\epsilon$ are delineated, the system constructs the precise spatial query $Q_s$. This query can be formally expressed by Equation \ref{eq:sql},
which ensures exact retrievals from the spatial database, maintaining both accuracy and relevance in the results. By leveraging these mathematical formulations, the system effectively translates spatial reasoning tasks into executable queries, facilitating robust spatial intelligence within the LLM framework.

\subsection{Hybrid Spatial Objects Ranking}\label{sec:ranking}
The spatial relevance score $f_s$ consists of two components: a score derived from %explicit spatial database computations 
sparse spatial retrieval from the spatial database
and a score from spatial dense retrieval based on
 text similarity between the question and the spatial descriptions of candidate objects. 
Formally, we define:
\begin{equation}
f_s = \lambda_s f_s^\text{sparse} + \lambda_d f_s^\text{dense},
\end{equation}
where $\lambda_s$ and $\lambda_d$ are weighting coefficients controlling the contribution of each score.
\subsubsection{Sparse Spatial Relevance Scoring}
Sparse spatial relevance is computed directly from the spatial database using explicit spatial relationships. The score is determined by the spatial query function $\mathcal{F}_s$, which computes the distance between reference and target objects. Formally, we define:
\begin{equation}
f_s^\text{sparse} = \begin{cases}
\displaystyle \frac{1}{1 + d(g_r, g_t)}, & \text{if } g_r \cap g_t = \emptyset \, \\
1, & \text{if } g_r \cap g_t \neq \emptyset \,
\end{cases}
\end{equation}
where $g_r$ and $g_t$ are reference and target spatial objects, respectively. $d(g_r, g_t)$ is a distance function measuring proximity in the spatial database. If $g_t$ overlaps with $g_r$, we assign a perfect relevance score of 1.

This ensures that objects within a region are maximally relevant, while those outside the region receive scores that decay with increasing distance.
\subsubsection{Dense Spatial Relevance Scoring}
Unlike sparse scoring, dense spatial relevance is inferred from textual descriptions associated with spatial objects. We leverage an LLM to extract key spatial attributes from user queries and compare them with the descriptions of candidate objects.

\paragraph{Extracting Spatial Requirements}
Given a user query $q$ and a set of text descriptions $d_t$ for spatial objects $G_t$, we extract the relevant spatial content via an attention-based masking function:
\begin{equation}
v_{q,s} = \mathcal{E}(\mathcal{M}_s(q)), \quad v_{t,s} =\mathcal{E}(\mathcal{M}_s(d_t)),
\end{equation}
where $v_{q,s}$ and $v_{t,s}$ are dense vector representations of spatial features, and $\mathcal{M}$ is the extraction function mapping input text to a spatial related text, $\mathcal{E}$ is the text encoder.

\paragraph{Ranking via Cosine Similarity}
The relevance score is computed via cosine similarity:
\begin{equation}
f_s^\text{dense} = \frac{v_{q,s} \cdot v_{t,s}}{ \parallel v_{q,s}\parallel\parallel v_{t,s}\parallel }.
\end{equation}

\subsubsection{Hybrid Ranking as a Generalized Model}

We can demonstrate that hybrid ranking generalizes both sparse and dense ranking approaches:
% \vspace{-0.3cm}
\begin{itemize}[leftmargin=*]
\setlength\itemsep{-0.2em}
\item \textbf{Sparse-Only Case:} If $\lambda_d = 0$ , then
$f_s = \lambda_s f_s^\text{sparse}$,
reducing to a purely distance-based ranking.

\item \textbf{Dense-Only Case:} If $\lambda_s = 0$ , then
$f_s = \lambda_d f_s^\text{dense}$,
reducing to a purely semantic-based ranking.

\item \textbf{Hybrid Case (General):} If both weights are nonzero, hybrid ranking benefits from both explicit spatial constraints and implicit semantic relevance, leading to a more comprehensive ranking mechanism.
\end{itemize}
% \vspace{-0.3cm}

This formulation ensures that hybrid ranking outperforms any single-ranking approach by capturing both spatial proximity and semantic alignment.



% \subsubsection{Integration with Spatial Ranking}
% The final relevance score $f(q, y)$ integrates both spatial and semantic relevance, forming a hybrid ranking function:

% \begin{equation}
% f(q, y) = \lambda_s f_s(q, y) + \lambda_k f_k(q, y),
% \end{equation}

% where $\lambda_s$ and $\lambda_k$ are weights balancing spatial and semantic contributions.

% The optimal answer is thus selected by solving:

% \begin{equation}
% y^* = \arg\max_{y \in C_s \cap C_k} f(q, y).
% \end{equation}

% This ensures that the retrieved spatial objects satisfy both spatial and semantic constraints, resulting in an optimal selection.


\subsection{Multi-objective Generation}\label{sec:generation}
The semantic candidate set $C_k$ and the semantic relevance score $f_k$ are calculated based on dense vector similarity, we put the details in appendix \ref{app:dense_semantic}. After all the scores and candidate sets are acquired, the problem becomes a multi-objective optimization problem since each perspective (spatial and semantic) contributes independently.

\subsubsection{Pareto Front Computation}

Given the spatial and semantic relevance scores, our goal is to identify the Pareto-optimal candidates that achieve the best trade-off between these objectives. A candidate $y$ is Pareto-optimal if no other candidate dominates it in both spatial and semantic relevance.
Formally, the Pareto front $P(q)$ is defined as:
\begin{equation}
\begin{aligned}
P(q) = \{ &y \in C_s \cap C_k \ | \  \nexists \; y' \in C_s \cap C_k, \\
& f_s(q, y') \geq f_s(q, y) \ \text{and} \ f_k(q, y') \geq f_k(q, y), \\
& \text{with at least one strict inequality} \}.
\end{aligned}
\end{equation}
This ensures that each candidate in $P(q)$ is non-dominated, meaning no other candidate is strictly better in both spatial and semantic relevance. 

\subsubsection{LLM-Based Trade-Off Decision}

Once the Pareto front $P(q)$ is determined, we use an LLM to dynamically balance the trade-offs between spatial constraints and semantic preferences based on the context of the user query.
Specifically, the LLM receives the user query, sparse spatial relevance scores, and spatial object descriptions as input:
\begin{equation}
I = \left\{ q, \left( f_s^{\text{sparse}}(q, y) \ , d_y  \right) \ ,\forall y \in P(q)  \right\}.
\end{equation}
A dynamic weighting function $\lambda_s,\lambda_k=h(I)$ based on contextual information is extracted from the input, adjusting the importance of spatial vs. semantic relevance, where $h$ is a learned function capturing query-specific trade-offs.

The top-ranked candidate $y^*$ is selected by LLM: 
\begin{equation}
y^* = \arg\max_{y \in P(q)} \lambda_s^T f_s(q, y) + \lambda_k^T f_k(q, y),
\end{equation}
and the LLM generates a natural language response.


The system adapts to different query contexts instead of using a fixed weighting scheme. By structuring decision-making into discrete steps (candidate filtering $\rightarrow$ Pareto selection $\rightarrow$ trade-off balancing $\rightarrow$ response generation), the LLM avoids generating infeasible or illogical results.
This structured approach maximizes accuracy and usability, ensuring that the system's final response aligns closely with the original user intent.


\section{Experiment}
\subsection{Experiment Setting}
\subsubsection{Datasets}
\textbf{TourismQA} \cite{contractor2019large} dataset contains user questions crawled from TripAdvisor posts from 50 cities around the world. Reviews of restaurants, attractions, and hotels for each city are crawled from travel forums and hotel booking websites. We select two two popular tourist cities, New York City and Miami, to evaluate the performance of different methods. 
For New York City, the original dataset contains information on 9,470 Points of Interest (POIs), with a total of 17,448 QA pairs. For Miami, the dataset includes 2,640 POIs and 133 QA pairs. Our preprocessing steps on the dataset include removing POIs with empty review information and eliminating duplicate QA pairs.
\subsubsection{Evaluation Metrics}
We assess the experimental results from multiple dimensions: 1) Delivery Rate: Following \cite{xietravelplanner}, this metric assesses whether the method can deliver a result successfully. 2) Spatial Sparse Pass Rate: This metric evaluates whether the parsed spatial query is correct. 3) Spatial Dense Pass Rate: This metric evaluates whether the answer satisfies the spatial-related semantic constraints in the question.  4) Semantic Pass Rate: This metric evaluates whether the answer meets the semantic constraints in the question. We use GPT-4o to evaluate the last three metrics.

\subsubsection{Models for comparison}
\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}llcccc@{}}
\toprule
\textbf{City}         & \textbf{Model}         & \textbf{Delivery Rate} & \textbf{Spatial Sparse Pass Rate} & \textbf{Spatial Dense Pass Rate} & \textbf{Semantic Pass Rate} \\ \midrule
\multirow{6}{*}{NYC} 
& SD                   & \textbf{100}                & -                      & 57.1                  & 32.1               \\
& TE                   & 99.6                 & -                      & 48.6                  & \underline{54.6}               \\
& ST                   & \textbf{100}                & -                      & 53.6                  & \textbf{55.9}               \\
& Naive RAG            & \underline{99.8}                 & -                      & 52.0                  & 54.5               \\
& GeoLLM                 & \underline{99.8}                & -                      & \underline{69.5}                  & 42.8               \\
\cmidrule(lr){2-6}
& Spatial-RAG (GPT-3.5-Turbo)         & 87.2                 & \textbf{67.0}                  & 64.4                  & 47.4               \\
& Spatial-RAG (GPT-4-Turbo)          & 86.1                 & \underline{65.0}                  & \textbf{71.6}                  & 50.1               \\
\midrule
\multirow{6}{*}{Miami} 
& SD                   & \textbf{100}                 & -                      & 36.8                  & 22.4               \\
& TE                   & \textbf{100}                 & -                      & 28.9                  & 39.5               \\
& ST                   & \textbf{100}                 & -                      & 26.3                  & 42.1               \\
& Naive RAG            & \textbf{100}                 & -                      & 31.6                  & 40.8               \\
& GeoLLM            &     \textbf{100}             & -                      &     \underline{52.6}              &  36.8              \\
\cmidrule(lr){2-6}
& Spatial-RAG (GPT-3.5-Turbo)         & \underline{86.8}                 & \underline{75.8}                  & \textbf{57.6}                  & \underline{43.9}               \\
& Spatial-RAG (GPT-4-Turbo)           & \underline{86.8}                 & \textbf{81.8}                  & 51.5                  & \textbf{45.5}               \\
\bottomrule
\end{tabular}}
\caption{Performance comparison of models in New York City (NYC) and Miami. The framework is deployed on GPT-4-Turbo and GPT-3.5-Turbo and compared its performance with several baseline models, including SD, TE, ST, Naive RAG, and GeoLLM.}
\label{table:city_comparison}
\end{table*}


\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{ 
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{Delivery Rate} & \textbf{Spatial Sparse Pass Rate} & \textbf{Spatial Dense Pass Rate} & \textbf{Semantic Pass Rate} \\ \midrule
Spatial-RAG (GPT-4-Turbo) & 86.1    &  \underline{65.0}& \underline{71.6} & \underline{50.1} \\
\midrule
w/o sparse spatial  & \textbf{98.9} & - & 53.3 & \textbf{78.4} \\
w/o dense spatial  & \underline{89.3} & 61.5 & 68.4 & 49.6 \\
w/o dense semantic  & 85.9 & \textbf{72.8} & \textbf{75.9} & 34.8 \\
\bottomrule
\end{tabular}}
\caption{Different modules of the proposed framework are removed for ablation study.}
\label{table:ablation_study}
\end{table*}


We used GPT-3.5-Turbo and GPT-4-Turbo as the LLM in our framework to assess the impact of LLM capability on performance and compared it against the following four methods:
\textbf{Sort-by-distance} (SD) \cite{contractor2021jointwww}: This method ranks the candidate spatial objects based on their distance to the reference objects in the spatial question.
\textbf{Text embedding} (TE) \cite{cakaloglu2020text}: This greedy method minimizes the distance between the vector embeddings of the text description of the reference object and the target object.
\textbf{Spatial-text} (ST): This approach computes the embeddings of the user’s question and compares the similarity between the question embedding and the text description embedding of the target object. Additionally, the object’s location is encoded as a distance score. The answer is then determined based on the weighted average of these scores.
\textbf{Naive RAG} \cite{lewis2020retrieval}: This method saves all spatial objects' descriptions in a vector database and retrieves the most relevant objects based on vector similarity.
\textbf{GeoLLM} \cite{manvigeollm}: This method encodes the spatial objects to address and enrich its context by adding spatial information of nearby spatial objects. 

% \subsubsection{Implementation Details}
% We leverage MLPs for the spatial feature extractor $\zeta_{SE}$ and the decoder $SD$. For each dataset, we randomly split it into 60\%, 20\%, and 20\% for training, validation, and testing, respectively. All the Deep learning models are trained for a maximum of 200 epochs using an early stop scheme. Adam optimizer with a learning rate of 0.001 is used. 

\subsection{Main Results}
The main results are summarized in \ref{table:city_comparison}
For the NYC dataset, Spatial-RAG demonstrates minimal differences in Delivery Rate when deployed with different LLMs. 
Approximately 86.1\% of the questions are successfully processed. The failure cases where the framework is unable to generate valid recommendations can be classified into two main categories:  Around 12.9\% of the questions fail to retrieve any spatial objects from the spatial database, either due to polygon recognition errors or because the specified region in the SQL query lacks relevant spatial objects. The remaining 0.9\% of cases stem from the LLM’s inability to correctly re-rank the retrieved results during the reranking process.

Additionally, the performance gap of our framework based on two LLMs in generating spatial SQL queries is insignificant. However, GPT-4-Turbo outperforms GPT-3.5-Turbo by approximately 7\% in Spatial Dense Pass Rate and also demonstrates a slight advantage in Semantic Pass Rate, and both models perform slightly below the optimal values (by 5\% and 8\%, respectively). This is due to the trade-off made when jointly considering both spatial proximity and the user's personal preferences.

Since the SD method returns the nearest spatial object, it achieves a higher spatial dense retrieval score than the TE, ST, and Naive RAG methods. However, its exclusive reliance on distance results in inferior performance on other metrics. For GeoLLM, as it considers only the names of spatial objects and their distances, it also achieves competitive performance in Spatial Dense Pass Rate. In contrast, both the TE and ST methods consider semantic contexts, leading to superior performance in this aspect compared to other models. The Naive RAG and ST methods jointly optimize for both spatial dense retrieval and user semantic dense retrieval, resulting in comparable performance between the two models.

For Miami, which differs significantly from NYC in terms of area and geography, Spatial-RAG also demonstrates strong performance. The performance of the baseline models almost remains consistent with the patterns observed in the previous analysis.


\begin{figure*}[t!]
  \centering
  % \includegraphics[width=\textwidth]{fig/arch_e.png}
  \includegraphics[width=\textwidth]{fig/arch_e.jpg}
  \vspace{-0.5cm}
  \caption{An example of how Spatial-RAG operates: Given a question, 1) \textbf{Sparse Spatial Retrieval:} an LLM parses the natural language question into a spatial SQL query for spatial database, retrieving spatial objects that meet the spatial constraints and the sparse spatial relevance scores. 2) \textbf{Question Decomposition and Dense Retrieval:}  In parallel, Spatial-RAG decomposes the question into spatial and semantic components and compares them with the descriptions of spatial objects to perform dense retrieval, filtering out irrelevant ones. 3)  \textbf{LLM Reranking:} A language agent balances the spatial and semantic aspects to rerank the candidate and generate the final answer. }
  \vspace{-0.5cm}
  \label{fig:arch_e}
\end{figure*}
\subsection{Ablation Study}
We conducted three ablation studies to assess the importance of different modules in our framework. We remove the sparse spatial module, dense spatial module, and dense semantic module, respectively, and denote the ablation versions as w/o sparse spatial, w/o dense spatial, and w/o dense semantic.
% \textbf{w/o sparse spatial module}: We remove the sparse spatial module and compare the parsed question with the database’s spatial information and semantic information using vector similarity.
% \textbf{w/o dense spatial module}: We remove the dense spatial module and consider the spatial relevance score based on the user’s location, as well as the semantic similarity.
% \textbf{w/o dense semantic module}: We removed the dense semantic module and considered only the spatial score and the user's spatial component.
The results are summarized in Table \ref{table:ablation_study}. It's found that there is a significant increase in Delivery Rate after removing the sparse spatial module, as this eliminates the need to determine the geometry type. In this case, the model relies solely on semantic information, resulting in a lower spatial score but an increased emphasis on semantic relevance. Similarly, when the dense semantic module is removed, the Spatial Dense Pass Rate reaches its highest value, as the model depends entirely on spatial modules for retrieval. However, this also leads to a lower Semantic Pass Rate, as semantic information is no longer incorporated into the ranking process.
Overall, our spatial-RAG framework integrates both spatial and semantic information, leveraging multi-objective optimization to provide more effective and well-balanced spatial object recommendations.


\subsection{Case Study}
An example of polyline search is given in Figure \ref{fig:arch_e}. Spatial-RAG effectively identifies the user’s intent within a noisy query, detecting that the user needs to travel between two locations (\textbf{Yankee Stadium} and \textbf{Hilton New York}) and is specifically looking for dinner recommendations ("any recos for dinner that evening - something simple"). Based on this, Spatial-RAG classifies the spatial operation as a "point within polyline" query and applies a 500m buffer distance. Within this region, it filters restaurant spatial objects (shown in Fig. \ref{fig:route}). In contrast, traditional methods, such as distance-based approaches, typically generate a buffer zone around a single point, which may not necessarily include POIs along the user’s travel route. Our framework provides a more context-aware understanding of the user’s spatial intent, enabling more precise and relevant recommendations based on both location and user preferences.

\begin{figure}[!hbt]
    \centering
\includegraphics[width=0.45\textwidth]{fig/route.png}
  \vspace{-0.5cm}
    \caption{An example of a "point within polyline" query. The user intends to travel between Yankee Stadium and Hilton New York, seeking dinner recommendations along the route. The buffered region (500m) is highlighted, and restaurants are selected.}
      \vspace{-0.5cm}
    \label{fig:route}
\end{figure}

\section{Conclusion}
Spatial-RAG enhances LLMs' spatial reasoning by integrating structured spatial retrieval with natural language understanding, bridging the gap between spatial databases and AI-driven question answering. Our framework improves spatial intelligence, enabling applications in urban planning, tourism, navigation, and geographic QA. Extensive evaluations show that Spatial-RAG outperforms existing methods, highlighting its potential to advance spatial analysis, autonomous navigation, and augmented reality. Beyond improving spatial reasoning, Spatial-RAG lays the foundation for future multimodal models that 


% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

\section*{Impact Statement}
This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Dense Semantic Retrieval and Ranking}\label{app:dense_semantic}
In the previous section, we derived the spatial candidate set $C_s$ and the spatial relevance score $f_s$. Now, we focus on obtaining the semantic candidate set $C_k$ and the semantic relevance score $f_k$ .

Given a query $q$ , we define the semantic candidate set $C_k(q)$ as:
\begin{equation}
C_k(q) = \{y \mid c_k(y, q) \leq 0, \forall c_k \in \mathcal{C}_k(q)\} ,
\end{equation}
where:
\begin{itemize}[leftmargin=*]
\item $c_k(y, q)$ is a constraint function that filters out spatial objects not satisfying the semantic intent of the query.
\item $\mathcal{C}_k(q)$ is the set of all semantic constraints (e.g., topic matching, category relevance).
\end{itemize}

Each spatial object is associated with textual descriptions, including names, reviews, and additional metadata. However, these descriptions often contain irrelevant or verbose details that may obscure meaningful information. To address this, we use an LLM-based masking function $\mathcal{M}_k$ to remove spatially redundant information and retain only semantically relevant content. The resulting texts are then encoded into a dense embedding space by a text encoder $\mathcal{E}$. Specifically, given a spatial object text description $d_t$,user query $q$, the filtered text representation is:
\begin{equation}
v_{t,k} = \mathcal{E}(\mathcal{M}_k(d_t))\quad v_{q,k} = \mathcal{E}(\mathcal{M}_k(q)).
\end{equation}

The semantic relevance score is then computed using cosine similarity:
\begin{equation}
f_k = \frac{v_{q,k} \cdot v_{t,k}}{ \parallel v_{q,k}\parallel\parallel v_{t,k}\parallel }.
\end{equation}
This score quantifies how well the spatial object aligns with the query's semantic intent, irrespective of spatial factors.


\section{Implementation Details}
\subsection{Semantic Parsing for Spatial Database Query}
For the geometry objects referenced in user queries, Spatial-RAG initially interacts with the spatial database to locate and match the described objects, such as specific points (e.g., a restaurant), roads, or defined areas and subsequently retrieves the pertinent geometrical data. In scenarios where the specified geometrical object does not exist pre-mapped in the database, Spatial-RAG is designed to construct a temporary geometric object. This temporary object serves as a stand-in to facilitate spatial queries based on the user's descriptive input.
This approach allows Spatial-RAG to handle dynamic spatial inquiries efficiently, even when direct matches are not immediately found within the existing database entries. By creating temporary geometrical representations, Spatial-RAG ensures that all spatial queries are processed accurately, maintaining the integrity and effectiveness of the system in delivering precise spatial information and responses.

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.

Functionally, the same outcome might be achieved through different means, for example, searching for a restaurant near a street could involve searching within a buffered polyline or creating a polygon enclosing the polyline and searching within it. Such flexibility in the system implies various methods to achieve the same goal.
This flexibility, however, poses a challenge if the LLM is tasked with generating a complete query directly, as it might lead to the production of hallucinatory, incorrect, or inexecutable code due to confusion or excessive complexity in interpreting spatial data. By structuring the process such that the LLM first identifies the geometry, then determines the function in a step-by-step manner, we mitigate the risks associated with generating errant queries. 
\subsection{Semantic Retrieval}
While spatial databases address spatial constraints based on the query and spatial database, the actual scenario may be complex, for instance, a hotel may be far from the airport on the map but provide a shuttle, which makes it spatially more convenient than a hotel closer but do not provide a shuttle. 

Each spatial object is accompanied by textual descriptions, such as names and reviews. However, the text often contains verbose and irrelevant details that hinder effective decision-making. 
Moreover, for areas with a high density of POIs that meet spatial requirements, it becomes impractical to input all the text information into an LLM (Large Language Model). To manage the data volume and improve relevance to specific queries, these descriptions are summarized across two perspectives:  spatial factors and user preferences. 
We utilize an LLM to preprocess and summarize spatial objects' reviews offline, storing the results in the database for future comparison.
Similarly, the user preferences in the query are dynamically extracted during the online processing stage. The textual aspects of the user preferences and reviews are compared using cosine similarity scores.  

\section{Other Case Studies}
Fig. \ref{fig:other-case} presents additional case studies, illustrating two spatial object selection strategies: filtering spatial objects within a buffered radius around the user and retrieving spatial objects contained within a specific polygon region.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{fig/point_polygon.png}
\caption{(a)  An example of a "point within a $\epsilon$-mile radius" query (b) An example of a "point within polygon" query}
\label{fig:other-case}
\end{figure}

\section{Prompt list}
\lstset{
  basicstyle=\ttfamily\small,   % 等宽字体，小号
  numbers=left,                 % 在左侧显示行号
  numberstyle=\tiny\color{gray},% 行号样式
  frame=single,                 % 代码框类型
  breaklines=true,              % 自动换行
  columns=fullflexible,         % 更好地处理中文或长行
  keepspaces=true,              % 保持空格
  xleftmargin=2em,              % 代码左边距
  framexleftmargin=2em,         % 框与左边距对齐
  % aboveskip=1.5em,                % 代码块上方距离
  % belowskip=1.5em                 % 代码块下方距离
}


\subsection{Prompt: Spatial Information Extraction}
Extract spatial information from user queries, including object geometry type (Point, Polyline (Route), or Polygon (Region)), region name, distance, and buffer distance.
\begin{lstlisting}
Analyze the following user query and extract spatial information: "{user_query}"

Current location context:
- Number of location points: {location_count}
- Multiple points detected: {is_multi_point}

First, determine the spatial query type based on these rules:
1. For single location point ({location_count == 1}):
   - Use Region-based if query explicitly mentions a region
   - Otherwise, use Point-based
   
2. For exactly two points ({location_count == 2}):
   - Use Route-based if query suggests path/route between points
   - Otherwise, fall back to Point/Region based rules
   
3. For multiple points ({location_count > 2}):
   - Only use Point-based or Region-based

Query types:
1. Point-based:
   - For "nearby" or "close": 1km in dense areas
   - For "walking distance": 2km
   - For "not too far": 3km
   
2. Route-based:
   - ONLY available with exactly 2 points
   - For walking routes: 1000m buffer
   - For general routes: 2000m buffer
   - For scenic/exploration: 3000m buffer
   - Consider terms: "route", "path", "between", "from...to", "along"
   
3. Region-based:
   - ONLY if query explicitly mentions these regions:
   Community/Sub-region names: {', '.join(region_names['nta_names'])}
   Borough names: {', '.join(region_names['boro_names'])}
   - Do NOT infer regions from landmarks

Return in strict JSON format:
{
    "query_type": "point" | "route" | "region",
    "region": "matched region name or null",
    "distance_km": number or null,
    "buffer_distance": number or null,
}
\end{lstlisting}

\subsection{Prompt: Semantic Intent Extraction}
Extract semantic intent from user queries, including spatial and nonspatial preference.
\begin{lstlisting}
Analyze the following user query and extract constraints: "{user_query}"

First, determine the main purpose of the query by identifying key terms and context:

Restaurant (R) keywords and contexts:
- Direct terms: "restaurant", "food", "eat", "dining", "meal", "cuisine"
- Food types: "Chinese", "Thai", "Mexican", "Italian", "sushi", etc.
- Meal times: "breakfast", "lunch", "dinner", "brunch"
- Dining related: "menu", "dishes", "chef", "reservation"
- Even if staying at a hotel, if asking about food/dining, it's Restaurant (R)

Hotel (H) keywords and contexts:
- Must be explicitly looking for accommodation
- Direct terms: "hotel", "stay", "accommodation", "room", "book"
- Price per night (e.g., "$200/night")
- Hotel names (e.g., "Hyatt", "Marriott")
- Mentioning a hotel as location reference is NOT H type

Attraction (A) keywords and contexts:
- Direct terms: "visit", "see", "tour", "explore"
- Places: "museum", "park", "gallery", "theater"
- Activities: "sightseeing", "show", "performance"

Important rules:
1. Focus on what the user is ASKING FOR, not what they mention
2. If user mentions staying at a hotel but asks about restaurants, type is R
3. If query is about food/dining/restaurants, type must be R
4. Location references (e.g., "near Hotel X") don't determine type

For each constraint type, extract complete sentences that describe the requirements:


1. Spatial constraints: Where they want to go
   Example: "near Times Square" or "in the Upper West Side area"

2. User constraints: What specific requirements or preferences they have
   Example: "family-friendly restaurant with reasonable prices around $30 per person"

Please return strict JSON format without any comments:
{
    "type": "R/H/A",
    "spatial_constraints": "complete sentence describing location requirements or null",
    "user_constraints": "complete sentence describing user preferences and requirements or null"
}
\end{lstlisting}

\subsection{Prompt: Result Reranking}
Rerank retrieved location results based on user query constraints.
\begin{lstlisting}
As a local recommendation expert, please rank the following places based on user query constraints.

User Query Constraints:
- Spatial Constraints: {query_constraints['spatial_constraints']}
- User Preferences: {query_constraints['user_constraints']}

Candidate Places:
{json.dumps(places, ensure_ascii=False, indent=2)}

Please analyze how well each place matches the user constraints and return a sorted list of places.
Return format should be a JSON array containing sorted indices.
Only return the index array, e.g., [2,0,1,3] means the 3rd place is the best match, followed by 1st, 2nd, and 4th places.
Note: Must return indices for all places, array length should equal input place count ({len(places)}).
\end{lstlisting}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
