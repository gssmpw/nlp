\section{Related Works}
\label{related_work}
% \subsection{Generative Recommendation}
\paragraph{Generative Recommendation.}

Generative models are one of the hottest research topics in machine learning, resulting in some representative works such as Variational AutoEncoders (VAEs) ____, Generative Adversarial Networks (GANs) ____ and Diffusion models ____. Generally, generative models aim to learn the distribution of the training data $\mathbb{P}(\mathbf{x})$and generate new samples $\mathbf{z}\sim \mathbb{P}(\mathbf{x})$. These generative models have also been applied to recommendation, resulting in many remarkable works of VAE-based ____, GAN-based ____ and diffusion-based ____ recommendation.

Recently, Transformer-based PLMs such as LLaMA ____ and GPT ____ have also shown promising capabilities in language generation. With the help of such powerful generative PLMs, some PLM-based recommendation methods have also been proposed. Some early works, such as P5 ____ and M6-Rec ____, attempt to transform recommendation into a language generation task by designing prompts to bridge the gap between the downstream task and the pretraining task of PLMs. Some works focus on leveraging the prior knowledge in PLMs for recommendation by various tuning techniques such as parameter-efficient fine-tuning (PEFT) ____ and instruction tuning ____.

One of the most important tasks in PLM-based recommendation is how to assign an unique sequence of tokens to each item as its ID. Early works ____ directly use the original name of the item or randomly assign an integer for each item, which have weak transferability and are sometimes unintelligible to PLMs. SEATER ____ constructs tree-structured item IDs from a pretrained SASRec ____ model. P5-ID ____ investigates the effect of different item IDs on recommendation. ColaRec ____ captures the collaborative signals between items to construct generative item IDs. Notably, TIGER ____ is the first attempt to use RQ-VAE to construct item IDs by quantizing the item embeddings.

% \subsection{Multi-modal Recommendation}
\paragraph{Multi-modal Recommendation.}
Multi-modal side information of items, such as descriptive text and images, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Early works such as VBPR ____ extract visual features by matrix factorization to achieve more personalized ranking. Some works ____ leverage various types of graph neural network (GNN) to fuse the multi-modal features. For example, LATTICE ____ designs a modality-aware learning layer to learn item-item structures for each modality and aggregates them to obtain latent item graphs. DualGNN ____ proposes a multi-modal representation learning module to model the user attentions across modalities and inductively learn the user preference. MVGAE ____ uses a modality-specific variational graph autoencoder to fuse the modality-specific node embeddings.

Recently, with the profound development of foundation models in different modalities ____, some recent works attempt to leverage pretrained foundation models as feature encoders to encode the multi-modal side information. Following P5 ____, VIP5 ____ extends it into a multi-modal version which encodes the item images by a pretrained CLIP image encoder. MMGRec ____ utilizes a Graph RQ-VAE to construct item IDs from both multi-modal and collaborative information. Moreover, IISAN ____ propose a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.

% \paragraph{Multi-modal codebook.}