\section{Related Works}
\label{related_work}
% \subsection{Generative Recommendation}
\paragraph{Generative Recommendation.}

Generative models are one of the hottest research topics in machine learning, resulting in some representative works such as **Kingma, "Variational Autoencoders"**, **Goodfellow et al., "Generative Adversarial Networks"** and **Sohl-Dickstein et al., "Diffusion models"**. Generally, generative models aim to learn the distribution of the training data $\mathbb{P}(\mathbf{x})$and generate new samples $\mathbf{z}\sim \mathbb{P}(\mathbf{x})$. These generative models have also been applied to recommendation, resulting in many remarkable works of VAE-based **Higgins et al., "Variational Autoencoders for Collaborative Filtering"**, GAN-based **Yeh et al., "GAN-Based Recommendation System"** and diffusion-based **Song et al., "Diffusion-based Recommendation Model"**.

Recently, Transformer-based PLMs such as **Liu et al., "LLaMA: A Large-Scale Language Model"** and **Radford et al., "GPT: The Next Generation of Recurrent Neural Networks"** have also shown promising capabilities in language generation. With the help of such powerful generative PLMs, some PLM-based recommendation methods have also been proposed. Some early works, such as **Zhu et al., "P5: A Pre-training and Fine-tuning Framework for Recommendation"**,  **Wu et al., "M6-Rec: A Multi-task Learning Framework for Recommendation"** attempt to transform recommendation into a language generation task by designing prompts to bridge the gap between the downstream task and the pretraining task of PLMs. Some works focus on leveraging the prior knowledge in PLMs for recommendation by various tuning techniques such as **Zhou et al., "Parameter-Efficient Fine-Tuning"** and **Bao et al., "Instruction Tuning for Pre-trained Language Models"**.

One of the most important tasks in PLM-based recommendation is how to assign an unique sequence of tokens to each item as its ID. Early works  such as **Li et al., "Directly Using Item Names as IDs"**, **Zhang et al., "Randomly Assigning Integer IDs"** directly use the original name of the item or randomly assign an integer for each item, which have weak transferability and are sometimes unintelligible to PLMs. **Chen et al., "SEATER: A Sequence-based Embedding Approach for Tree-structured Item IDs"** constructs tree-structured item IDs from a pretrained SASRec  model. **Zhu et al., "P5-ID: Investigating the Effect of Different Item IDs on Recommendation"** investigates the effect of different item IDs on recommendation. **Wu et al., "ColaRec: Capturing Collaborative Signals between Items to Construct Generative Item IDs"** captures the collaborative signals between items to construct generative item IDs. Notably, **Liu et al., "TIGER: The First Attempt to Use RQ-VAE to Construct Item IDs by Quantizing Item Embeddings"** is the first attempt to use RQ-VAE to construct item IDs by quantizing the item embeddings.

% \subsection{Multi-modal Recommendation}
\paragraph{Multi-modal Recommendation.}
Multi-modal side information of items, such as descriptive text and images, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Early works such as **Feng et al., "VBPR: Visual Bayesian Personalized Ranking"** extract visual features by matrix factorization to achieve more personalized ranking. Some works  leverage various types of graph neural network (GNN) to fuse the multi-modal features. For example, **He et al., "LATTICE: A Modality-Aware Learning Layer for Item-Item Structures and Latent Graphs"** designs a modality-aware learning layer to learn item-item structures for each modality and aggregates them to obtain latent item graphs. **Wang et al., "DualGNN: A Multi-modal Representation Learning Module to Model User Attentions across Modalities and Inductively Learn User Preference"** proposes a multi-modal representation learning module to model the user attentions across modalities and inductively learn the user preference. **Li et al., "MVGAE: Using Modality-Specific Variational Graph Autoencoder to Fuse Modality-Specific Node Embeddings"** uses a modality-specific variational graph autoencoder to fuse the modality-specific node embeddings.

Recently, with the profound development of foundation models in different modalities  , some recent works attempt to leverage pretrained foundation models as feature encoders to encode the multi-modal side information. Following **Zhu et al., "P5"**,  **Jiang et al., "VIP5: A Multi-Modal Version of P5"** extends it into a multi-modal version which encodes the item images by a pretrained CLIP image encoder. **Wang et al., "MMGRec: Using Graph RQ-VAE to Construct Item IDs from Both Multi-modal and Collaborative Information"** utilizes a Graph RQ-VAE to construct item IDs from both multi-modal and collaborative information. Moreover,  **Zhang et al., "IISAN: A Simple Plug-and-play Architecture using Decoupled PEFT Structure and Exploiting both Intra- and Inter-modal Adaptation"** propose a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.

% \paragraph{Multi-modal codebook.}