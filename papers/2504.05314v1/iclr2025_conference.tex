
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage[normalem]{ulem}
\usepackage{enumitem}

% \usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors


\title{Multimodal Quantitative Language for Generative Recommendation}
% \title{Multimodal Quantitative Language for Generative Sequential Recommendation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

\author{
 \textbf{Jianyang Zhai\textsuperscript{1,2}},
 \textbf{Zi-Feng Mai\textsuperscript{1,3}},
 \textbf{Chang-Dong Wang\textsuperscript{1,3}\thanks{Corresponding authors.}}, 
 \textbf{Feidiao Yang\textsuperscript{2}\textsuperscript{\textasteriskcentered}}, \\
 \textbf{Xiawu Zheng\textsuperscript{2,4}}, 
 \textbf{Hui Li\textsuperscript{4}},
 \textbf{Yonghong Tian\textsuperscript{2,5}}
\\
 \textsuperscript{1}Sun Yat-sen University,
 \textsuperscript{2}Pengcheng Laboratory, \\
 \textsuperscript{3}Guangdong Key Laboratory of Big Data Analysis and Processing, \\
 \textsuperscript{4}Xiamen University, 
 \textsuperscript{5}Peking University
\\
\texttt{\{zhaijy01, yangfd\}@pcl.ac.cn, changdongwang@hotmail.com}
}

% \author{
%  \textbf{Jianyang Zhai\textsuperscript{1,2}},
%  \textbf{Zi-Feng Mai\textsuperscript{1,3}},
%  \textbf{Chang-Dong Wang\textsuperscript{1,3}\thanks{Corresponding authors.}}, 
%  \textbf{Feidiao Yang\textsuperscript{2}\textsuperscript{\textasteriskcentered}}, \\
%  \textbf{Xiawu Zheng\textsuperscript{4}}, 
%  \textbf{Hui Li\textsuperscript{4}},
%  \textbf{Yonghong Tian\textsuperscript{2,5}}
% \\
%  \textsuperscript{1}Sun Yat-sen University,
%  \textsuperscript{2}Pengcheng Laboratory, \\
%  \textsuperscript{3}Guangxi Key Laboratory of Digital Infrastructure, Guangxi Zhuang Autonomous Region Information Center
%  \textsuperscript{3}Guangdong Key Laboratory of Big Data Analysis and Processing, \\
%  \textsuperscript{4}School of Informatics, Xiamen University, 
%  \textsuperscript{5}Peking University
% \\
% \texttt{\{zhaijy01, yangfd\}@pcl.ac.cn, changdongwang@hotmail.com}
% \normalsize
% \texttt{\{zhaijy01, yangfd\}@pcl.ac.cn, \{maizf3, zhengdy23\}@mail2.sysu.edu.cn} \\
% \normalsize
% \texttt{changdongwang@hotmail.com, \{zhengxiawu, hui\}@xmu.edu.cn, yhtian@pku.edu.cn} \\
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\iclrfinalcopy
\begin{document}


\maketitle

\begin{abstract}
Generative recommendation has emerged as a promising paradigm aiming at directly generating the identifiers of the target candidates.
Most existing methods attempt to leverage prior knowledge embedded in Pre-trained Language Models (PLMs) to improve the recommendation performance.
However, they often fail to accommodate the differences between the general linguistic knowledge of PLMs and the specific needs of recommendation systems. Moreover, they rarely consider the complementary knowledge between the multimodal information of items, which represents the multi-faceted preferences of users. 
To facilitate efficient recommendation knowledge transfer, we propose a novel approach called Multimodal Quantitative Language for Generative Recommendation (MQL4GRec).
Our key idea is to transform items from different domains and modalities into a unified language, which can serve as a bridge for transferring recommendation knowledge.
Specifically, we first introduce quantitative translators to convert the text and image content of items from various domains into a new and concise language, known as quantitative language, with all items sharing the same vocabulary.
Then, we design a series of quantitative language generation tasks to enrich quantitative language with semantic information and prior knowledge. 
Finally, we achieve the transfer of recommendation knowledge from different domains and modalities to the recommendation task through pre-training and fine-tuning.
We evaluate the effectiveness of MQL4GRec through extensive experiments and comparisons with existing methods, achieving improvements over the baseline by 11.18\%, 14.82\%, and 7.95\% on the NDCG metric across three different datasets, respectively. 
\footnote{Our implementation is available at: \href{https://github.com/zhaijianyang/MQL4GRec}{\textcolor{blue}{https://github.com/zhaijianyang/MQL4GRec}.}}
% Our implementation is available at: \href{https://github.com/zhaijianyang/MQL4GRec}{\textcolor{blue}{https://github.com/zhaijianyang/MQL4GRec}.}
% Our implementation is available at: \href{https://anonymous.4open.science/r/QL4GRec-ED65/}{\textcolor{blue}{https://anonymous.4open.science/r/MQL4GRec-ED65/}.}
\end{abstract}

\section{Introduction}

\begin{wrapfigure}[12]{r}{0.55\textwidth}
\vspace{-2em}
  \centering
  \includegraphics[width=0.55\textwidth]{figures/intro.pdf}
  \caption{Illustration of our MQL4GRec. We translate items from different domains and modalities into a new unified language, which can then serve as a bridge for transferring recommendation knowledge. }
  \label{fig:introduction}
\end{wrapfigure}

Recommendation systems (RS) aim to recommend items to users that they may be interested in, and are widely used on many online platforms, such as e-commerce and social networking \citep{chaves2022efficient, covington2016deep}. 
For a long time, recommendation models that represent users and items using their unique IDs (known as IDRec) have been dominant in the field of RS \citep{kang2018self, Sun2019BERT4RecSR, zhang2024ninerec}. 
However, IDRec may encounter cold start and knowledge transferability issues due to its inherent properties. 
To address these limitations, some literature \citep{unisrec, sun2023universal} employs modal encoders \citep{devlin2018bert, he2016deep} to learn universal representations of items or sequences. While promising, these modal encoders are typically not specifically designed for recommendation tasks, resulting in suboptimal performance.

Recently, generative recommendation has emerged as a promising paradigm, which employs an end-to-end generative model to directly predict identifiers of target candidates \citep{p5, tiger}. 
Due to the success of PLMs in natural language generation (NLG) \citep{2020t5, gpt, llama}, most existing methods attempt to leverage the prior knowledge of PLMs to improve the recommendation performance \citep{tallrec, instructrec, zheng2023adapting}. 
They formalize the recommendation task as a sequence-to-sequence generation process, where the input sequence contains data of items interacted with users, and the output sequence represent identifiers of target items. 
Then they enable PLMs to perform recommendation tasks by adding instructions or prompts.
% They attempt to reduce the gap between PLMs and recommender by designing prompts or instructions, enabling PLMs to perform recommendation tasks.
Despite achieving decent performance, they suffer from the following limitations: 
1) There are significant task differences between PLMs and RS, which may lead to inconsistencies between the general linguistic knowledge of PLMs and the specific requirements of RS;
2) They often overlook the complementary knowledge between the multimodal information of items, which is crucial for capturing the multi-faceted preferences of users.

To address these limitations, it is crucial to bridge the gaps between different domains and modalities, leveraging their recommendation knowledge to enhance the performance of the target domains.
Inspired by significant advancements in NLG, such as pretraining-finetuning \citep{devlin2018bert, t5} and prompt-tuning \citep{gpt, llama}, we propose the idea of transforming items from various domains and modalities into a new and unified language.
A key factor contributing to these significant advances is the use of a shared vocabulary, where tokens are endowed with rich semantic information and prior knowledge across various tasks, which can then be effectively transferred to downstream tasks.
Thus, we aspire for this new language to encompass a vocabulary in which tokens can represent items from various domains and modalities, as depicted in Figure \ref{fig:introduction}. 
Specifically, this language not only serves as a bridge for knowledge transfer but also as identifiers of items, and should be more concise than the original modalities (text and image) to avoid issues in generation \citep{p5id}.

To this end, we propose a novel approach known as Quantitative Language for Multimodal Generative Recommendation (MQL4GRec). 
Specifically, we first introduce quantitative translators to convert the content of items (text and images) into the quantitative language.
We train a separate quantitative translator for each modality of the item, each consisting of a modal encoder and a vector quantizer. 
Together, the codebooks of the two quantitative translators constitute the vocabulary. 
Then, we design a series of quantitative language generation tasks aiming at endowing quantitative language with rich semantic information and prior knowledge, and these tasks can be viewed as microcosms of NLG tasks.
Specifically, we additionally incorporate some special tokens as task prompts.
Finally, we transfer the source domain and multimodal recommendation knowledge to the recommendation tasks through pre-training and fine-tuning.
To evaluate the effectiveness of our proposed MQL4GRec, we conduct extensive experiments and comparisons with existing methods. 
Relative to the baseline, we observe improvements of 11.18\%, 14.82\%, and 7.95\% on the NDCG metric across three datasets, respectively. 
In summary, our proposed MQL4GRec achieves the transfer of recommendation knowledge by breaking down barriers between items across different domains and modalities, demonstrating strong scalability and potential.
Our main contributions can be summarized as follows:
\setlist[itemize]{leftmargin=1.5em}
\begin{itemize}
    \item We propose MQL4GRec, a novel approach that translates items from various domains and modalities into a unified quantitative language, thereby breaking down the barriers between them and facilitating the transfer of recommendation knowledge.
    
    \item We design a series of quantitative language generation tasks that endow quantitative language with rich semantic information and prior knowledge, and enhance the performance of recommendation tasks through pre-training and fine-tuning.
    
    \item We conduct extensive experiments and analyses on three public datasets, and the results validate the effectiveness of our proposed method.
    
\end{itemize}

\section{Related Works}
\label{related_work}
% \subsection{Generative Recommendation}
\paragraph{Generative Recommendation.}

Generative models are one of the hottest research topics in machine learning, resulting in some representative works such as Variational AutoEncoders (VAEs) \citep{vae}, Generative Adversarial Networks (GANs) \citep{gan} and Diffusion models \citep{ddpm}. Generally, generative models aim to learn the distribution of the training data $\mathbb{P}(\mathbf{x})$and generate new samples $\mathbf{z}\sim \mathbb{P}(\mathbf{x})$. These generative models have also been applied to recommendation, resulting in many remarkable works of VAE-based \citep{pevae,recvae}, GAN-based \citep{apr,ipgan,dasp} and diffusion-based \citep{diffkg,diffrec} recommendation.

Recently, Transformer-based PLMs such as LLaMA \citep{llama} and GPT \citep{gpt} have also shown promising capabilities in language generation. With the help of such powerful generative PLMs, some PLM-based recommendation methods have also been proposed. Some early works, such as P5 \citep{p5} and M6-Rec \citep{m6rec}, attempt to transform recommendation into a language generation task by designing prompts to bridge the gap between the downstream task and the pretraining task of PLMs. Some works focus on leveraging the prior knowledge in PLMs for recommendation by various tuning techniques such as parameter-efficient fine-tuning (PEFT) \citep{tallrec} and instruction tuning \citep{instructrec}.

One of the most important tasks in PLM-based recommendation is how to assign an unique sequence of tokens to each item as its ID. Early works \citep{p5,m6rec} directly use the original name of the item or randomly assign an integer for each item, which have weak transferability and are sometimes unintelligible to PLMs. SEATER \citep{seater} constructs tree-structured item IDs from a pretrained SASRec \citep{sasrec} model. P5-ID \citep{p5id} investigates the effect of different item IDs on recommendation. ColaRec \citep{colarec} captures the collaborative signals between items to construct generative item IDs. Notably, TIGER \citep{tiger} is the first attempt to use RQ-VAE to construct item IDs by quantizing the item embeddings.

% \subsection{Multi-modal Recommendation}
\paragraph{Multi-modal Recommendation.}
Multi-modal side information of items, such as descriptive text and images, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Early works such as VBPR \citep{vbpr} extract visual features by matrix factorization to achieve more personalized ranking. Some works \citep{mmgcn,mkgat,grcn} leverage various types of graph neural network (GNN) to fuse the multi-modal features. For example, LATTICE \citep{lattice} designs a modality-aware learning layer to learn item-item structures for each modality and aggregates them to obtain latent item graphs. DualGNN \citep{dualgnn} proposes a multi-modal representation learning module to model the user attentions across modalities and inductively learn the user preference. MVGAE \citep{mvgae} uses a modality-specific variational graph autoencoder to fuse the modality-specific node embeddings.

Recently, with the profound development of foundation models in different modalities \citep{clip,gpt,t5}, some recent works attempt to leverage pretrained foundation models as feature encoders to encode the multi-modal side information. Following P5 \citep{p5}, VIP5 \citep{vip5} extends it into a multi-modal version which encodes the item images by a pretrained CLIP image encoder. MMGRec \citep{mmgrec} utilizes a Graph RQ-VAE to construct item IDs from both multi-modal and collaborative information. Moreover, IISAN \citep{fu2024iisan} propose a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.

% \paragraph{Multi-modal codebook.}


\section{Method}
\label{method}

In this section, we elaborate on the proposed MQL4GRec, a novel approach of transferring recommendation knowledge across different domains and modalities.
We first translate item content into a unified quantitative language, which bridge the gaps between different domains and modalities.
Then, we design a series of quantitative language generation tasks, and achieve the transfer of recommendation knowledge through pre-training and fine-tuning.
The overall framework of the method is illustrated in Figure \ref{fig:framework}.

\subsection{Quantitative Language}

The original modal content of items is complex, which can affect the efficiency and performance of recommendations \citep{p5id}. Therefore, we translate item content from various domains and modalities into a concise and unified quantitative language.
In this subsection, we introduce a quantitative translator to accomplish the aforementioned conversion.


\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{figures/framework.pdf}
  \caption{The overall framework of MQL4GRec. We regard the quantizer as a translator, converting item content from different domains and modalities into a unified quantitative language, thus bridging the gap between them (left). Subsequently, we design a series of quantitative language generation tasks to facilitate the transfer of recommendation knowledge through pre-training and fine-tuning (right).}
  \label{fig:framework}
\end{figure}

% \subsubsection{Quantitative Translator}
\paragraph{Quantitative Translator.}

Vector Quantization (VQ) is an information compression technique widely utilized across various domains \citep{vqvae, rqvae}, which maps high-dimensional data onto a finite set of discrete vectors, known as the codebook.
In this paper, we treat the quantizer as a translator that converts complex item content into a concise quantitative language. Here, the codebook serves as the vocabulary of the quantitative language.

To obtain a unified quantitative language, we first employ a frozen modal encoder (LLaMA or ViT \citep{vit}) to encode item content (text or image), and to obtain the item representation.
Further, we take the item representation as input, and train a Residual-Quantized Variational AutoEncoder (RQ-VAE) \citep{rqvae} for generating item tokens.
RQ-VAE is a multi-level vector quantizer that applies quantization on residuals to generate a tuple of codewords (\textit{i.e.}, item tokens).
As shown in Figure \ref{fig:framework} (left), for an item representation $\boldsymbol{h}$, RQ-VAE first encodes it into a latent representation $\boldsymbol{z}$. At each level $\boldsymbol{l}$, we have a codebook $\mathcal{C}^l=\left\{\boldsymbol{v}_k^l\right\}_{k=1}^K$, where each codebook vector is a learnable cluster center. The residual quantization process can be represented as:
\begin{equation}
c_i=\underset{k}{\arg \min }\left\|\boldsymbol{r}_i-\boldsymbol{v}_k^i\right\|_2^2,
\end{equation}
\begin{equation}
\boldsymbol{r}_{i+1}=\boldsymbol{r}_i-\boldsymbol{v}_{c_i}^i,
\end{equation}
where $c_i$ is the codeword of the $i$-th level, $\boldsymbol{r}_i$ is the residual vector of the $i$-th level, and $\boldsymbol{r}_1 = \boldsymbol{z}$. 
Assuming we have L-level codebooks, the quantization representation of $\boldsymbol{z}$ can be obtained according to $\hat{\boldsymbol{z}}=\sum_{i=1}^L \boldsymbol{v}_{c_i}^i$.
Then $\hat{\boldsymbol{z}}$ will be used as decoder input to reconstruct the item representation $\boldsymbol{h}$. 
The loss function can be represented as:
\begin{equation}
\mathcal{L}_{\mathrm{recon}}=\|\boldsymbol{h}-\hat{\boldsymbol{h}}\|_2^2,
\end{equation}
\begin{equation}
\mathcal{L}_{\mathrm{rqvae}}=\sum_{i=1}^H\left\|\operatorname{sg}\left[\boldsymbol{r}_i\right]-\boldsymbol{v}_{c_i}^i\right\|_2^2+\beta\left\|\boldsymbol{r}_i-\operatorname{sg}\left[\boldsymbol{v}_{c_i}^i\right]\right\|_2^2,
\end{equation}
\begin{equation}
\mathcal{L}(h)=\mathcal{L}_{\mathrm{recon}}+\mathcal{L}_{\mathrm{rqvae}},
\end{equation}
where $\hat{\boldsymbol{h}}$ is the output of the decoder, $\operatorname{sg[*]}$ represents the stop-gradient operator, and $\beta$ is a loss coefficient. The overall loss is divided into two parts, $\mathcal{L}_{\mathrm{recon}}$ is the reconstruction loss, and $\mathcal{L}_{\mathrm{rqvae}}$ is the RQ loss used to minimize the distance between codebook vectors and residual vectors.

Items typically encompass content from multiple modalities, representing various aspects of user preferences. 
In our setup, each item comprises two modalities: text and image. 
We train a quantitative translator for each modality, then add prefixes to the codewords from each of the two codebooks to form a dictionary.
Specifically, for the text quantitative translator, we prepend lowercase letter prefixes to the codewords to obtain $V_t = \{a\_1, b\_2, \ldots, d\_K\}$; for the image quantitative translator, we prepend uppercase letter prefixes to the codewords to obtain $V_v = \{A\_1, B\_2, \ldots, D\_K\}$. Here, \( a/A \) represents the $1$-th level codebook, \( d/D \) represents the $4$-th level codebook, etc. Subsequently, the dictionary can be represented as $V = \{V_t, V_v\}$. 
With each quantitative translator having $LK$ codewords, the size of our dictionary is $2LK$, enabling us to represent a total of \( K^L \) items.

Once the quantitative translators are trained, we can directly use them to translate new items into quantitative language. 
For example, for the item text \textit{"Sengoku Basara: The Last Party"}, after encoding it through the text encoder and RQ-VAE, we obtain a set of codewords (2, 3, 1, 6). Then, by appending lowercase letters before each number, we can get the text quantitative language of the item as \textit{<a\_2><b\_3><c\_1><d\_6>}. Similarly, for the item's image, we can obtain its image quantitative language as \textit{<A\_1><B\_4><C\_2><D\_6>}.


\paragraph{Handling Collisions.}

Translating item content into quantitative language may lead to item collisions, where multiple items possess the same tokens. 
To address this issue, some methods \citep{tiger, p5id} append an additional identifier after the item indices, which may introduce semantically unrelated distributions.
LC-Rec \citep{zheng2023adapting} introduces a uniform distribution constraint to prevent multiple items from clustering in the same leaf node. 
However, this method does not completely resolve collisions, such as when items have the same modality information or when the number of collisions exceeds the size of the last level codebook, which can lead to inflated performance metrics.
(More discussion in Appendix \ref{app:handling}.)

To address the above issue, we reallocate tokens for colliding items based on the distance from the residual vector to the code vectors.
Specifically, for $N$ colliding items, we first calculate the distances $\boldsymbol{D}\in\mathbb{R}^{N \times L \times K}$ between the residual vectors and the code vectors for each level based on $\boldsymbol{d}_k^i=\left\|\boldsymbol{r}_i-\boldsymbol{v}_k^i\right\|_2^2$, and sort the distances to obtain the indices $\boldsymbol{I} = \operatorname{argsort}(\boldsymbol{D}, axis=2) \in\mathbb{R}^{N \times L \times K}$.
Then, we sort the colliding items based on their minimum distance to the code vectors of the last level, i.e., $(item_1, item_2, \ldots, item_N) = \operatorname{sort}_{\operatorname{min}(\boldsymbol{d}^L)}(\textit{colliding items})$.
Finally, we reallocate tokens for the sorted colliding items based on $\boldsymbol{I}$, following these principles: 1) Start from the last level to assign the nearest token to each item. If collisions occur, assign the next nearest token. 2) If there are insufficient tokens in the last level, for the remaining colliding items, reallocate tokens from the second last level based on distance, and then reallocate tokens from the last level. We repeat this process until all colliding items are handled. 



\subsection{Quantitative Language Generation Tasks}

In this subsection, we design several quantitative language generation tasks with the aim of imbuing quantitative language with more semantic information, thereby transferring prior knowledge to the target task, as illustrated in Figure \ref{fig:framework} (right).
Specifically, we additionally include some special tokens in the dictionary, which can serve as prompts to differentiate the types of tasks.

\paragraph{Next Item Generation.}
\label{NIG}

Since our primary goal is to predict the next item, the next item generation task is our main optimization objective.
Specifically, each item contains both text and image modalities, so we have two subtasks: 1) Next Text Item Generation; 2) Next Image Item Generation.
In this context, the input sequence is the item tokens sequence from the user interaction history, and the output sequence is the target item tokens corresponding to the respective modality.
Different modal sequences reflect different aspects of user preferences.

\paragraph{Asymmetric Item Generation.}
\label{AIG}

In the next item generation task, the input and output are tokens of the same modality, and we refer to this task as symmetric.
To facilitate the interaction of recommendation knowledge between two modalities, we introduce asymmetric item generation tasks.
Here, there are two subtasks: 1) Asymmetric Text Item Generation, where the input is the image tokens of the interaction history items, and the output is the text tokens of the target item; 2) Asymmetric Image Item Generation, where the input is the text tokens of the interaction history items, and the output is the image tokens of the target item. For example, for the input sequence \textit{"<*\_6><*\_7><*\_8><a\_2><b\_3><c\_1><d\_6><a\_4><b\_3><c\_8><d\_6>"}, in human-understandable language, it can be described as follows: \textit{"Based on the user's text interaction sequence, please predict the next item's image quantitative language: <a\_2><b\_3><c\_1><d\_6>, <a\_4><b\_3><c\_8><d\_6>"}.

\paragraph{Quantitative Language Alignment}
\label{QLA}

Asymmetric item generation tasks enable the interaction of knowledge between two modalities, but they fall under the category of implicit alignment of the two modalities.
We further introduce explicit Quantitative Language Alignment tasks to directly achieve alignment between the text and image quantitative languages of items.
Here, we also have two subtasks: 1) Text-to-Image Alignment; 2) Image-to-Text Alignment. 
For example, for the input sequence \textit{"<*\_12><*\_13><*\_14><a\_2><b\_3><c\_1><d\_6>"}, in human-understandable language, it can be described as follows: \textit{"Please provide the image quantitative language for the following item: <a\_2><b\_3><c\_1><d\_6>"}.

\subsection{Training and Recommendation}

\paragraph{Training.}

Quantitative language can be viewed as a microcosm of natural language. We employ a two-stage paradigm of pre-training and fine-tuning to optimize the model, which is similar to NLG tasks.
For \textbf{pre-training}, we utilize the source domain datasets, where the pre-training task consists of two sub-tasks for next item generation.
The purpose is to transfer recommendation knowledge from the source domains to the target domains.
For \textbf{fine-tuning}, we conduct it on the target domain dataset, with tasks encompassing all quantitative language generation tasks. 
The aim is to leverage recommendation knowledge from different modalities to explore users' multifaceted preferences.
The tasks mentioned above are conditional language generation tasks performed in a sequence-to-sequence manner.
We optimize the negative log-likelihood of the generation target as follows:
\begin{equation}
    \mathcal{L}_\theta=-\sum_{j=1}^{|\mathbf{Y}|} \log P_\theta\left(\mathbf{Y}_j \mid \mathbf{Y}_{<j}, \mathbf{X}\right),
\end{equation}
where $\theta$ is the model parameters, $\mathbf{X}$ is the input sequence of encoder, and $\mathbf{Y_j}$ is the $j$-th token of $\mathbf{Y}$.

\paragraph{Re-ranking for recommendation.}
There are two sub-tasks in the next item generation task, representing different user preferences. 
Although fine-tuning tasks can facilitate the transfer of recommendation knowledge between them, there might be some information loss.
Therefore, we re-rank items by utilizing the recommendation lists generated from the two sub-tasks. The basic idea is that items appearing in both lists should be ranked higher.
Specifically, we first obtain recommendation lists $R_t$ and $R_v$ for each sub-task through beam search, which include scores for each item.
Then, the new score for each item can be formalized as:
\begin{equation}
\label{equ:score}
s(x) = \begin{cases} 
(s_t(x) + s_v(x)) / 2 + 1 & x \in R_t, x \in R_v \\
s_t(x) & x \in R_t \\
s_v(x) & x \in R_v
\end{cases},
\end{equation}
where $s_i(x)$ is the score of item $x$ in the list $R_i$, and $i \in \{t, v\}$.

\section{Experiments}

\subsection{Experimental Settings}

\paragraph{Datasets.}
We evaluate the proposed approach on three public real-world benchmarks from the Amazon Product Reviews dataset \citep{ni2019justifying}, containing user reviews and item metadata from May 1996 to October 2018.
In particular, we use six categories for pre-training, including \textit{“Pet Supplies”}, \textit{"Cell Phones and Accessories"}, \textit{“Automotive”}, \textit{“Tools and Home Improvement”}, \textit{“Toys and Games”}, \textit{“Sports and Outdoors”}, and three categories for sequential recommendation tasks, including \textit{“Musical Instruments”}, \textit{“Arts Crafts and Sewing”}, \textit{“Video Games”}.
We discuss the dataset statistics and pre-processing in Appendix \ref{dataset}.

\paragraph{Evaluation Metrics.}
We use top-k Recall (Recall@K) and Normalized Discounted Cumulative Gain (NDCG@K) with K = 1, 5, 10 to evaluate the recommendation performance.
Following previous works \citep{p5, p5id}, we employ the \textit{leave-one-out} strategy for evaluation.
We perform full ranking evaluation over the entire item set instead of sample-based evaluation. For the generative methods based on beam search, the beam size is uniformly set to 20.


\subsection{Overall Performance}
In this section, we compare our proposed approach for generative recommendation with the following sequential recommendation methods (which are described briefly in Appendix \ref{baseline}): GRU4Rec \citep{hidasi2015session}, BERT4Rec \citep{Sun2019BERT4RecSR}, SASRec \citep{sasrec}, FDSA \citep{fdsa}, S$^3$-Rec \citep{s3rec}, VQ-Rec \citep{hou2023learning}, MISSRec\citep{wang2023missrec}, P5-CID \citep{p5id}, VIP5 \citep{geng2023vip5}, and TIGER \citep{tiger}. 
Results are shown in Table \ref{tab:results}. Based on these results, we can find:

% For non-generative recommendation methods, FDSA often achieves better performance in most cases, demonstrating that introducing item content as auxiliary information can enhance recommendation performance. 
% For all the baseline methods, TIGER performs well on the Instruments and Arts datasets but does not exhibit superiority on the Games dataset. This may be due to TIGER's lack of auxiliary content information. 
% In contrast, our proposed method introduces recommendation knowledge from different domains and modalities.

For non-generative recommendation methods, MISSRec often achieves better performance in most cases, demonstrating that introducing multimodal information of items can enhance recommendation performance. 
For generative baseline methods, VIP5 with image information does not achieve good results, which may be due to the modal differences between PLMs and image information.
Furthermore, TIGER performs well on the Instruments and Arts datasets but does not exhibit superiority on the Games dataset. This may be due to TIGER's lack of auxiliary content information.
In contrast, our proposed method introduces recommendation knowledge from different domains and modalities.

Compared to baseline methods, our proposed MQL4GRec achieves the best performance in most cases, especially with significant improvements on the NDCG metric.
This superior performance can be attributed to two factors: 1) We translate item content from different domains and modalities into a unified quantitative language, breaking down barriers between them; 2) The series of QLG tasks we designed enable the transfer of recommendation knowledge to target tasks through pre-training and fine-tuning methods.


% \begin{table}
% \centering
% \caption{Performance comparison of different methods on the three datasets. The best and second-best performances are indicated in bold and underlined font, respectively. $^*$ indicates that, in the t-test, our method significantly outperforms the runner-up with p < 0.05.}
% \label{tab:results}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{l|l|ccccccc|cccccl} 
% \toprule
% Dataset                      & Metrics & GRU4Rec & BERT4Rec & SASRec & FDSA           & S$^3$-Rec & VQ-Rec          & MISSRec         & P5-CID & VIP5   & TIGER          & MQL4GRec        & Improv.  & $p$-value   \\ 
% \midrule
% \multirow{5}{*}{Instruments} & HR@1    & 0.0566  & 0.0450   & 0.0318 & 0.0530         & 0.0339    & 0.0502          & 0.0723          & 0.0512 & 0.0737 & \uline{0.0754} & \textbf{0.0833} & +10.48\% & 1.74e-4$^*$   \\
%                              & HR@5    & 0.0975  & 0.0856   & 0.0946 & 0.0987         & 0.0937    & 0.1062          & \uline{0.1089}  & 0.0839 & 0.0892 & 0.1007         & \textbf{0.1115} & +2.39\%  & 2.79e-1   \\
%                              & HR@10   & 0.1207  & 0.1081   & 0.1233 & 0.1249         & 0.1123    & 0.1357          & \uline{0.1361}  & 0.1119 & 0.1071 & 0.1221         & \textbf{0.1375} & +1.03\%  & 5.93e-1   \\
%                              & NDCG@5  & 0.0783  & 0.0667   & 0.0654 & 0.0775         & 0.0693    & 0.0796          & 0.0797          & 0.0678 & 0.0815 & \uline{0.0882} & \textbf{0.0977} & +10.77\% & 1.32e-5$^*$   \\
%                              & NDCG@10 & 0.0857  & 0.0739   & 0.0746 & 0.0859         & 0.0743    & 0.0891          & 0.0880          & 0.0704 & 0.0872 & \uline{0.0950} & \textbf{0.1060} & +11.58\% & 5.04e-7$^*$   \\ 
% \midrule
% \multirow{5}{*}{Arts}        & HR@1    & 0.0365  & 0.0289   & 0.0212 & 0.0380         & 0.0172    & 0.0408          & 0.0479          & 0.0421 & 0.0474 & \uline{0.0532} & \textbf{0.0672} & +26.32\% & 8.55e-17$^*$  \\
%                              & HR@5    & 0.0817  & 0.0697   & 0.0951 & 0.0832         & 0.0739    & \textbf{0.1038} & 0.1021          & 0.0713 & 0.0704 & 0.0894         & \uline{0.1037}  & -        & 9.42e-1   \\
%                              & HR@10   & 0.1088  & 0.0922   & 0.1250 & 0.1190         & 0.1030    & \textbf{0.1386} & 0.1321          & 0.0994 & 0.0859 & 0.1167         & \uline{0.1327}  & -        & 1.02e-2   \\
%                              & NDCG@5  & 0.0602  & 0.0502   & 0.0610 & 0.0583         & 0.0511    & \uline{0.0732}  & 0.0699          & 0.0607 & 0.0586 & 0.0718         & \textbf{0.0857} & +17.08\% & 2.08e-12$^*$  \\
%                              & NDCG@10 & 0.0690  & 0.0575   & 0.0706 & 0.0695         & 0.0630    & \uline{0.0844}  & 0.0815          & 0.0662 & 0.0635 & 0.0806         & \textbf{0.0950} & +12.56\% & 3.15e-9$^*$   \\ 
% \midrule
% \multirow{5}{*}{Games}       & HR@1    & 0.0140  & 0.0115   & 0.0069 & 0.0163         & 0.0136    & 0.0075          & \uline{0.0201}  & 0.0169 & 0.0173 & 0.0166         & \textbf{0.0203} & +1.00\%  & 7.40e-1   \\
%                              & HR@5    & 0.0544  & 0.0426   & 0.0587 & 0.0614         & 0.0527    & 0.0408          & \textbf{0.0674} & 0.0532 & 0.0480 & 0.0523         & \uline{0.0637}  & -        & 1.74e-3   \\
%                              & HR@10   & 0.0895  & 0.0725   & 0.0985 & 0.0988         & 0.0903    & 0.0679          & \textbf{0.1048} & 0.0824 & 0.0758 & 0.0857         & \uline{0.1033}  & -        & 3.09e-1   \\
%                              & NDCG@5  & 0.0341  & 0.0270   & 0.0333 & \uline{0.0389} & 0.0351    & 0.0242          & 0.0385          & 0.0331 & 0.0328 & 0.0345         & \textbf{0.0421} & +8.23\%  & 1.48e-4$^*$   \\
%                              & NDCG@10 & 0.0453  & 0.0366   & 0.0461 & \uline{0.0509} & 0.0468    & 0.0329          & 0.0499          & 0.0454 & 0.0418 & 0.0453         & \textbf{0.0548} & +7.66\%  & 8.80e-6$^*$   \\
% \bottomrule
% \end{tabular}
% }
% \end{table}


\begin{table}
\centering
\caption{Performance comparison of different methods on the three datasets. The best and second-best performances are indicated in bold and underlined font, respectively.}
\label{tab:results}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|ccccccc|ccccc} 
\toprule
Dataset                      & Metrics & GRU4Rec & BERT4Rec & SASRec & FDSA           & S$^3$-Rec & VQ-Rec          & MISSRec         & P5-CID & VIP5   & TIGER          & MQL4GRec        & Improv.   \\ 
\midrule
\multirow{5}{*}{Instruments} & HR@1    & 0.0566  & 0.0450   & 0.0318 & 0.0530         & 0.0339    & 0.0502          & 0.0723          & 0.0512 & 0.0737 & \uline{0.0754} & \textbf{0.0833} & +10.48\%  \\
                             & HR@5    & 0.0975  & 0.0856   & 0.0946 & 0.0987         & 0.0937    & 0.1062          & \uline{0.1089}  & 0.0839 & 0.0892 & 0.1007         & \textbf{0.1115} & +2.39\%   \\
                             & HR@10   & 0.1207  & 0.1081   & 0.1233 & 0.1249         & 0.1123    & 0.1357          & \uline{0.1361}  & 0.1119 & 0.1071 & 0.1221         & \textbf{0.1375} & +1.03\%   \\
                             & NDCG@5  & 0.0783  & 0.0667   & 0.0654 & 0.0775         & 0.0693    & 0.0796          & 0.0797          & 0.0678 & 0.0815 & \uline{0.0882} & \textbf{0.0977} & +10.77\%  \\
                             & NDCG@10 & 0.0857  & 0.0739   & 0.0746 & 0.0859         & 0.0743    & 0.0891          & 0.0880          & 0.0704 & 0.0872 & \uline{0.0950} & \textbf{0.1060} & +11.58\%  \\ 
\midrule
\multirow{5}{*}{Arts}        & HR@1    & 0.0365  & 0.0289   & 0.0212 & 0.0380         & 0.0172    & 0.0408          & 0.0479          & 0.0421 & 0.0474 & \uline{0.0532} & \textbf{0.0672} & +26.32\%  \\
                             & HR@5    & 0.0817  & 0.0697   & 0.0951 & 0.0832         & 0.0739    & \textbf{0.1038} & 0.1021          & 0.0713 & 0.0704 & 0.0894         & \uline{0.1037}  & -         \\
                             & HR@10   & 0.1088  & 0.0922   & 0.1250 & 0.1190         & 0.1030    & \textbf{0.1386} & 0.1321          & 0.0994 & 0.0859 & 0.1167         & \uline{0.1327}  & -         \\
                             & NDCG@5  & 0.0602  & 0.0502   & 0.0610 & 0.0583         & 0.0511    & \uline{0.0732}  & 0.0699          & 0.0607 & 0.0586 & 0.0718         & \textbf{0.0857} & +17.08\%  \\
                             & NDCG@10 & 0.0690  & 0.0575   & 0.0706 & 0.0695         & 0.0630    & \uline{0.0844}  & 0.0815          & 0.0662 & 0.0635 & 0.0806         & \textbf{0.0950} & +12.56\%  \\ 
\midrule
\multirow{5}{*}{Games}       & HR@1    & 0.0140  & 0.0115   & 0.0069 & 0.0163         & 0.0136    & 0.0075          & \uline{0.0201}  & 0.0169 & 0.0173 & 0.0166         & \textbf{0.0203} & +1.00\%   \\
                             & HR@5    & 0.0544  & 0.0426   & 0.0587 & 0.0614         & 0.0527    & 0.0408          & \textbf{0.0674} & 0.0532 & 0.0480 & 0.0523         & \uline{0.0637}  & -         \\
                             & HR@10   & 0.0895  & 0.0725   & 0.0985 & 0.0988         & 0.0903    & 0.0679          & \textbf{0.1048} & 0.0824 & 0.0758 & 0.0857         & \uline{0.1033}  & -         \\
                             & NDCG@5  & 0.0341  & 0.0270   & 0.0333 & \uline{0.0389} & 0.0351    & 0.0242          & 0.0385          & 0.0331 & 0.0328 & 0.0345         & \textbf{0.0421} & +8.23\%   \\
                             & NDCG@10 & 0.0453  & 0.0366   & 0.0461 & \uline{0.0509} & 0.0468    & 0.0329          & 0.0499          & 0.0454 & 0.0418 & 0.0453         & \textbf{0.0548} & +7.66\%   \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Ablation Study}

\begin{table}
\centering
\caption{Ablation study of handling collisions.}
\label{tab:handling}
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{lcccccc} 
\toprule
\multirow{2}{*}{Methods} & \multicolumn{2}{c}{Instruments}   & \multicolumn{2}{c}{Arts}          & \multicolumn{2}{c}{Games}          \\ 
\cmidrule(l){2-7}
                         & HR@10           & NDCG@10         & HR@10           & NDCG@10         & HR@10           & NDCG@10          \\ 
\midrule
TIGER                    & 0.1221          & 0.0950          & \textbf{0.1167} & 0.0806          & 0.0857          & 0.0453           \\
TIGER w/o user           & 0.1216          & 0.0958          & 0.1159          & 0.0810          & 0.0863          & 0.0464           \\
Handling Collisions      & \textbf{0.1277} & \textbf{0.0987} & 0.1163          & \textbf{0.0844} & \textbf{0.0885} & \textbf{0.0473}  \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Handling Collisions.}
\label{Handling Collisions}

We propose a method based on the distance between the residual vector and the codeword vector to resolve item collisions. To validate the effectiveness of our method, we compare it with the collision resolution approach in TIGER, which directly adds an item index layer to resolve item collisions, thereby introducing a semantically unrelated distribution.
The experimental results are shown in Table \ref{tab:handling}. "TIGER w/o user" refers to the removal of the user ID token from the input sequence, which is also done to facilitate a fairer comparison. From the experimental results, it can be seen that our method of handling collisions is more rational and effective.
Furthermore, results indicate that including a user ID token in the input degrades model performance, particularly on the Games dataset. We attribute this to TIGER representing tens of thousands of users with only 2000 tokens, leading to numerous user ID collisions.

% NIG\_1 shows the results of using our proposed method for resolving item collisions. It is evident that our method for resolving item collisions performs better.
% Experimental results indicate that including a user ID token in the input degrades model performance, particularly on the Games dataset. We attribute this to TIGER representing tens of thousands of users with only 2000 tokens, leading to numerous user ID collisions.

\paragraph{Quantitative language generation tasks.}
\label{ablation1}

We initially assess the effect of different QLG tasks on performance without the use of pre-training, and the results are shown in Table \ref{tab:QLG}. (For more detailed results, please refer to Appendix \ref{more_QLG}.)
Various tasks include: (1) NIG: the next item generation task introduced in Section \ref{NIG}; (2) AIG: the asymmetric item generation task; (3) QLA: the quantitative language alignment task.
In this list, tasks without subscripts indicate that two subtasks are used simultaneously.
"Text" denotes evaluating performance by utilizing the next text item generation subtask (i.e., $\text{NIG}_1$); "Image" signifies evaluating performance by utilizing the next image item generation subtask (i.e., $\text{NIG}_2$).

The results indicate that several quantitative language generation tasks designed by us can significantly improve performance. 
Specifically, as the number of tasks increases, the performance of both $\text{NIG}_1$ and $\text{NIG}_2$ improves. 
This indicates that these tasks can enrich the quantitative language by incorporating semantic information and knowledge across different modalities.
In summary, converting the multimodal content of items into a unified quantitative language effectively facilitates the transfer of recommendation knowledge.


\begin{table}
\small
\centering
\caption{Ablation study of various quantitative language generation tasks without pre-training.}
\label{tab:QLG}
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{llcccccc} 
\toprule
\multirow{2}{*}{Modal} & \multirow{2}{*}{Tasks} & \multicolumn{2}{c}{Instruments}   & \multicolumn{2}{c}{Arts}                               & \multicolumn{2}{c}{Games}          \\ 
\cmidrule(lr){3-4}\cmidrule(lr){5-6}\cmidrule(lr){7-8}
                       &                          & HR@10           & NDCG@10         & HR@10           & NDCG@10                              & HR@10           & NDCG@10          \\ 
\midrule
\multirow{4}{*}{Text}  & $\text{NIG}_1$           & 0.1277          & 0.0987          & 0.1163          & 0.0844                               & 0.0885          & 0.0473           \\
                       & NIG                      & 0.1275          & 0.0986          & 0.1205          & 0.0877                               & 0.0928          & 0.0493           \\
                       & + AIG                    & 0.1279          & 0.0987          & 0.1249          & 0.0895                               & 0.1002          & 0.0529           \\
                       & + QLA                    & \textbf{0.1282} & \textbf{0.0993} & \textbf{0.1293} & \textbf{0.0913}                      & \textbf{0.1010} & \textbf{0.0531}  \\ 
\midrule\midrule
\multirow{4}{*}{Image} & $\text{NIG}_2$           & 0.1243          & 0.0968          & 0.1117          & 0.0812                               & 0.0881          & 0.0478           \\
                       & NIG                      & 0.1262          & 0.0986          & 0.1158          & 0.0848                               & 0.0899          & 0.0487           \\
                       & + AIG                    & \textbf{0.1299} & 0.0998          & 0.1218          & 0.0878                               & 0.1002          & 0.0534           \\
                       & + QLA                    & 0.1280          & \textbf{0.1001} & \textbf{0.1259} & \textbf{0.0901}                      & \textbf{0.1017} & \textbf{0.0540}  \\
\bottomrule
\end{tabular}
}
\end{table}


\paragraph{Pre-training.}

We transfer recommendation knowledge from the source domain datasets to the target dataset through pre-training.
Here, we employ $\text{NIG}_1$ to evaluate the recommendation performance, with the results shown in Table \ref{tab:pretraining}. (Additional results can be found in Appendix \ref{more_pre-training}.)
QLG represents the quantitative language generation tasks without pre-training.
Specifically, the pre-training task labeled \textit{"$\text{NIG}_1$ w/ pre-training"} employs only $\text{NIG}_1$ from the source domain datasets. 
On the other hand, the \textit{"QLG w/ pre-training"} task uses both $\text{NIG}_1$ and $\text{NIG}_2$.

The results indicate that, under a single modality, pre-training enhances the performance across three downstream datasets, demonstrating that prior knowledge from the source domain can be effectively transferred to downstream tasks. 
Under dual modalities, pre-training significantly improves performance on the Instruments and Arts datasets; however, it does not yield a notable improvement for the Games dataset, potentially due to overfitting. 
A more intuitive analysis of this phenomenon is provided in Section \ref{pre_dataset}. Finally, we re-rank the items according to Equation (\ref{equ:score}) to generate the final recommendation list.


\begin{table}
\centering
\caption{Ablation study of pre-training and quantitative language generation tasks.}
\label{tab:pretraining}
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{lcccccc} 
\toprule
\multirow{2}{*}{Methods}                  & \multicolumn{2}{c}{Instruments}   & \multicolumn{2}{c}{Arts}          & \multicolumn{2}{c}{Games}          \\ 
\cmidrule(l){2-7}
                                          & HR@10           & NDCG@10         & HR@10           & NDCG@10         & HR@10           & NDCG@10          \\ 
\midrule
$\text{(0) NIG}_1$                        & 0.1277          & 0.0987          & 0.1163          & 0.0844          & 0.0885          & 0.0473           \\
(1) QLG~                                  & 0.1282          & 0.0993          & 0.1293          & 0.0913          & \uline{0.1010}  & \uline{0.0531}   \\
$\text{(2) NIG}_1 \text{w/ pre-training}$ & 0.1334          & 0.1043          & 0.1305          & \textbf{0.0959} & 0.0950          & 0.0508           \\
(3) QLG~~w/ pre-training                  & \uline{0.1362}  & \uline{0.1051}  & \uline{0.1314}  & 0.0944          & 0.0995          & 0.0521           \\
(4) MQL4GRec ((3) + re-ranking)            & \textbf{0.1375} & \textbf{0.1060} & \textbf{0.1327} & \uline{0.0950}  & \textbf{0.1033} & \textbf{0.0548}  \\
\bottomrule
\end{tabular}
}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\columnwidth]{figures/datasets.pdf}
  \caption{The impact of varying amounts of pre-training datasets on recommendation performance.}
  \label{fig:pre_dataset}
\end{figure}

\subsection{Further Analysis}

\paragraph{Pre-training datasets.}
\label{pre_dataset}

In this subsection, we investigate the impact of varying amounts of pre-training datasets on downstream tasks, and the results are shown in Figure \ref{fig:pre_dataset}.
From the results, it can be observed that:
1) Exclusively in the context of text quantitative language, as the number of pre-training datasets increases, the performance of downstream tasks also gradually improves. This suggests that larger numbers in pre-training datasets provide more transferable recommendation knowledge.
2) Following pre-training with quantitative language under two modalities, fine-tuning shows varying trends across different downstream datasets. Specifically, while increasing pre-training datasets enhances performance on the Instruments and Arts datasets, it leads to a gradual decline in performance on the Games dataset. This could indicate either overfitting or significant domain differences between the Games dataset and the source domain datasets.

\paragraph{Pre-training epochs}

In this subsection, we investigate the impact of varying the number of pre-training epochs on downstream tasks, with the results displayed in Figure \ref{fig:epochs}. 
From the figure, it can be observed that:
1) When pre-training is performed solely with text quantitative language, the performance of downstream tasks gradually increases with the number of pre-training epochs and stabilizes around 25 epochs.
2) When pre-training involves both text and image quantitative languages, the Instruments and Arts datasets reach peak performance early, and further training may impair the transfer of recommendation knowledge. In contrast, for the Games dataset, performance deteriorates as the number of pre-training epochs increases. This suggests that for the Games dataset, recommendation knowledge from different modalities might be more crucial than that from the source domain dataset, and there may be conflicts between the two.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\columnwidth]{figures/epochs.pdf}
  \caption{The impact of different pre-training epochs on recommendation performance.}
  \label{fig:epochs}
\end{figure}

\begin{table}[h]
\centering
\caption{Zero-shot capabilities under different number of pre-training datasets.}
\label{tab:zero-shot}
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{ccccccc} 
\toprule
\multirow{2}{*}{Number} & \multicolumn{2}{c}{Instruments} & \multicolumn{2}{c}{Arts} & \multicolumn{2}{c}{Games}  \\ 
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
                        & HR@10            & NDCG@10          & HR@10            & NDCG@10          & HR@10            & NDCG@10           \\ 
\midrule
0                       & 0.00099          & 0.00046          & 0.00113          & 0.00052          & 0.00066          & 0.00031           \\
2                       & 0.00240          & 0.00137          & 0.00140          & 0.00063          & 0.00109          & \textbf{0.00058}  \\
4                       & 0.00310          & 0.00170          & 0.00298          & 0.00132          & 0.00054          & 0.00027           \\
6                       & \textbf{0.00345} & \textbf{0.00171} & \textbf{0.00311} & \textbf{0.00138} & \textbf{0.00116} & 0.00047           \\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Zero-shot capability}

% 表现出了初步的zero-shot的能力，但是还很弱。模型很小，
We investigate whether models pre-trained on the source domain dataset have zero-shot capabilities, as shown in Table \ref{tab:zero-shot}. Here, "Number" represents the number of pre-training datasets, with "0" indicating model parameters randomly initialized. We use $\text{NIG}_1$ to evaluate performance. The results demonstrate that pre-trained models exhibit preliminary zero-shot capabilities on the Instruments and Arts datasets, although they are still weak. However, this capability is not evident on the Games dataset. We attribute this primarily to the scarcity of pre-training data and the limited parameters of the model, resulting in insufficient generalization. In the future, we aim to delve deeper into this phenomenon.

\section{Conclusion}

In this paper, we propose a novel approach named MQL4GRec, which transforms item content from different domains and modalities into a unified quantitative language to facilitate the effective transfer of recommendation knowledge. 
We first train a quantitative translator for each modality, converting items into the quantitative language and breaking down the barriers between them. 
Then, we design a series of quantitative language generation tasks aiming at endowing quantitative language with rich semantic information and prior knowledge.
Finally, we transfer the source domain and multimodal recommendation knowledge to the recommendation tasks through pre-training and fine-tuning.
Our proposed MQL4GRec achieves superior performance compared to the baseline method.
Moreover, MQL4GRec possesses strong scalability and potential as it does not rely on traditional item IDs and bridges the gap between different domains and modalities. We believe this represents a significant step towards universal recommendation models.


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

\subsubsection*{Acknowledgments}
This work was supported by National Key Research and Development Program of China (2021YFF1201200), NSFC (62276277), Guangdong Basic and Applied Basic Research Foundation (2022B1515120059).

% \newpage

\input{iclr2025_conference.bbl}
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Dataset Statistics}
\label{dataset}

\begin{table}[h]
\small
\centering
\caption{Statistics of the preprocessed datasets. “\textbf{Avg}. \textit{len}” represents the average length of item sequences.}
\label{tab:preprocess}
\resizebox{0.65\columnwidth}{!}{
\begin{tabular}{lrrrrr} 
\toprule
\textbf{Datasets} & \textbf{\#Users} & \textbf{\#Items} & \textbf{\#Interactions} & \textbf{Sparsity} & \textbf{Avg.}\textit{ len}  \\ 
\midrule
Pet               & 183697           & 31986            & 1571284                 & 99.97\%           & 8.55                        \\
Cell              & 123885           & 38298            & 873966                  & 99.98\%           & 7.05                        \\
Automotive        & 105490           & 39537            & 845454                  & 99.98\%           & 8.01                        \\
Tools             & 144326           & 41482            & 1153959                 & 99.98\%           & 8.00                        \\
Toys              & 135748           & 47520            & 1158602                 & 99.98\%           & 8.53                        \\
Sports            & 191920           & 56395            & 1504646                 & 99.99\%           & 7.84                        \\ 
\midrule
Instruments       & 17112            & 6250             & 136226                  & 99.87\%           & 7.96                        \\
Arts              & 22171            & 9416             & 174079                  & 99.92\%           & 7.85                        \\
Games             & 42259            & 13839            & 373514                  & 99.94\%           & 8.84                        \\
\bottomrule
\end{tabular}
}
\end{table}

To evaluate the performance of the proposed approach, we conduct experiments in the pre-trained source and target domain settings.
We use six categories from from the Amazon Product Reviews dataset \citep{ni2019justifying} for pre-training, including \textit{“Pet Supplies”}, \textit{"Cell Phones and Accessories"}, \textit{“Automotive”}, \textit{“Tools and Home Improvement”}, \textit{“Toys and Games”}, \textit{“Sports and Outdoors”}, and three categories for sequential recommendation tasks, including \textit{“Musical Instruments”}, \textit{“Arts Crafts and Sewing”}, \textit{“Video Games”}.

Each item in the dataset is associated with a title, a description, and an image. Following previous work \citep{tiger}, we first filter out unpopular users and items with less than five interactions. Then, we create user behavior sequences based on the chronological order. The maximum item sequence length is uniformly set to 20 to meet all baseline requirements. The statistics of our preprocessed datasets are shown in Table \ref{tab:preprocess}.

\section{Baselines}
\label{baseline}

We compare the proposed approach with the following baseline methods: 

\setlist[itemize]{leftmargin=1.5em}
\begin{itemize}
    \item \textbf{GRU4Rec}~\citep{hidasi2015session} introduces Gating Recurrent Unit (GRU) to model user action sequences for session-based recommendations.
    \item \textbf{SASRec}~\citep{sasrec} uses a directional self-attentive model to capture item correlations within a sequence.
    \item \textbf{BERT4Rec}~\citep{Sun2019BERT4RecSR} employs a bi-directional self-attentive model with the cloze objective for modeling user behavior sequences.
    \item \textbf{FDSA}~\citep{fdsa} uses a self-attentive model to capture item and feature transition patterns.
    \item \textbf{S}$^3$\textbf{-Rec}~\citep{s3rec} pre-trains sequential models with mutual information maximization to learn the correlations among attributes, items, subsequences, and sequences.
    \item \textbf{VQ-Rec} \citep{hou2023learning} learns vector-quantized item representations for transferable sequential recommenders.
    \item \textbf{MISSRec} \citep{wang2023missrec} is a multi-modal pre-training and transfer learning framework for sequential recommendation.
    \item \textbf{P5-CID} \citep{p5, p5id} organizes multiple recommendation tasks in a text-to-text format and models different tasks uniformly using the T5 model. Here, we employ P5 with collaborative indexing as the baseline.
    \item \textbf{VIP5} \citep{geng2023vip5} is a multimodal foundation model considering visual, textual, and personalization modalities under the P5 recommendation paradigm, to unify various modalities and recommendation tasks.
    \item \textbf{TIGER} \citep{tiger} adopts the generative retrieval paradigm for sequential recommendation and introduces a semantic ID to uniquely identify items.
\end{itemize}

\section{Implementation Details.}
\label{detail}
To obtain textual representations, we employ LLaMA to encode the title and description of the item as its embedding and use mean pooling to aggregate multiple representations. To obtain visual representations, we utilize CLIP's \citep{clip} image branch as an encoder to encode the images of items, and we employ ViT-L/14 as the backbone. 
Both the encoder and decoder of RQ-VAE are implemented as Multi-Layer Perceptrons (MLPs) with ReLU activation functions.
The level of codebooks is set to 4, with each level consisting of 256 codebook vectors, and each vector has a dimension of 32.  
The model is optimized using the AdamW optimizer, employing a learning rate of 0.001 and a batch size of 1024.

Following previous work \citep{tiger}, we use the T5 \citep{t5} framework to implement our transformer based encoder-decoder architecture. We use 4 layers each for the transformer-based encoder and decoder models with 6 self-attention heads of dimension 64 in each layer. The MLP and the input dimension was set as 1024 and 128, respectively. 
The number of prompt tokens for every task is set to 4.
We employ the AdamW \citep{adamw} optimizer for model optimization, setting the weight decay to 0.01. 
During pre-training, we utilize a batch size of 4096 with a learning rate set to 0.001.
For alignment tuning, we employ a batch size of 512 with a maximum learning rate of 5e-4, and utilize a cosine scheduler with warm-up to adjust the learning rate. 
% More implementation details can be found in Appendix \ref{more_detail}.

Our experiments utilize the Tesla V100 GPU. For pretraining, we use four cards, and for fine-tuning, we use two cards. Since the model has around 13 million parameters, there is still a substantial amount of GPU memory remaining.

% \section{More Implementation Details}
% \label{more_detail}

% To obtain textual representations, we employ LLaMA to encode the title and description of the item as its embedding and use mean pooling to aggregate multiple representations. To obtain visual representations, we utilize CLIP's \citep{clip} image branch as an encoder to encode the images of items, and we employ ViT-L/14 as the backbone. 
% Both the encoder and decoder of RQ-VAE are implemented as Multi-Layer Perceptrons (MLPs) with ReLU activation functions.
% The level of codebooks is set to 4, with each level consisting of 256 codebook vectors, and each vector has a dimension of 32.  
% The model is optimized using the AdamW optimizer, employing a learning rate of 0.001 and a batch size of 1024.

% Our experiments utilize the Tesla V100 GPU. For pretraining, we use four cards, and for fine-tuning, we use two cards. Since the model has around 13 million parameters, there is still a substantial amount of GPU memory remaining.


\section{More Ablation Studies}
\label{more_ablation}

\subsection{Quantitative language generation tasks.}
\label{more_QLG}

We have supplemented Table \ref{tab:more_QLG} with more detailed ablation experiments of quantitative language generation tasks without pre-training. 
We study the impact of each task on recommendation performance through a combination of different tasks. We find that: 1) the AIG task always results in a significant performance improvement; 2) the QLA task needs to be paired with the AIG task in order to achieve better results.
This suggests that quantitative language serves as a bridge for knowledge transfer in recommendations, but we need to design appropriate quantitative language tasks.


\begin{table}
\small
\centering
\caption{Detailed ablation study of various quantitative language generation tasks without pre-training}
\label{tab:more_QLG}
\resizebox{0.85\columnwidth}{!}{
\begin{tabular}{llcccccc} 
\toprule
\multirow{2}{*}{Modal} & \multirow{2}{*}{Tasks}      & \multicolumn{2}{c}{Instruments}   & \multicolumn{2}{c}{Arts}          & \multicolumn{2}{c}{Games}          \\ 
\cmidrule(l){3-8}
                       &                             & HR@10           & NDCG@10         & HR@10           & NDCG@10         & HR@10           & NDCG@10          \\ 
\midrule
\multirow{6}{*}{Text}  & $\text{NIG}_1$              & 0.1277          & 0.0987          & 0.1163          & 0.0844          & 0.0885          & 0.0473           \\
                       & $\text{NIG}_1 \text{ + QLA}$ & 0.1275          & 0.0986          & 0.1166          & 0.0831          & 0.0871          & 0.0465           \\
                       & NIG                         & 0.1275          & 0.0986          & 0.1205          & 0.0877          & 0.0928          & 0.0493           \\
                       & NIG + QLA                   & 0.1263          & 0.0983          & 0.1204          & 0.0867          & 0.0919          & 0.0492           \\
                       & NIG + AIG                   & \uline{0.1279}  & \uline{0.0987}  & \uline{0.1249}  & \uline{0.0895}  & \uline{0.1002}  & \uline{0.0529}   \\
                       & NIG + AIG + QLA             & \textbf{0.1282} & \textbf{0.0993} & \textbf{0.1293} & \textbf{0.0913} & \textbf{0.1010} & \textbf{0.0531}  \\ 
\midrule\midrule
\multirow{6}{*}{Image} & $\text{NIG}_2$              & 0.1243          & 0.0968          & 0.1117          & 0.0812          & 0.0881          & 0.0478           \\
                       & $\text{NIG}_2 \text{ + QLA}$ & 0.1237          & 0.0978          & 0.1143          & 0.0826          & 0.0877          & 0.0468           \\
                       & NIG                         & 0.1262          & 0.0986          & 0.1158          & 0.0848          & 0.0899          & 0.0487           \\
                       & NIG + QLA                   & 0.1265          & 0.0988          & 0.1164          & 0.0849          & 0.0945          & 0.0505           \\
                       & NIG + AIG                   & \textbf{0.1299} & \uline{0.0998}  & \uline{0.1218}  & \uline{0.0878}  & \uline{0.1002}  & \uline{0.0534}   \\
                       & NIG + AIG + QLA             & \uline{0.1280}  & \textbf{0.1001} & \textbf{0.1259} & \textbf{0.0901} & \textbf{0.1017} & \textbf{0.0540}  \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{Pre-training.}
\label{more_pre-training}

We further provide the results of using $\text{NIG}_2$ to evaluate the performance of recommendations in Table \ref{tab:more_pretraining}.
From the results, it can be seen that:
1) On the Instruments and Arts datasets, both pre-training and Quantitative Language Generation (QLG) tasks are useful for improving recommendation performance. This indicates that quantitative language can migrate recommendation knowledge from the source domain and other modalities to the target task.
2) On the Games dataset, QLG with pre-training impairs performance, which might be due to some conflict of recommendation knowledge between the source domain and another modality. In the future, we will further explore this phenomenon.

\begin{table}
\small
\centering
\caption{Detailed ablation study of pre-training and quantitative language generation tasks.}
\label{tab:more_pretraining}
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{llcccccc} 
\toprule
\multirow{2}{*}{Modal} & \multirow{2}{*}{Methods}                  & \multicolumn{2}{c}{Instruments}                     & \multicolumn{2}{c}{Arts}                   & \multicolumn{2}{c}{Games}                            \\ 
\cmidrule(l){3-8}
                       &                                           & HR@10                    & NDCG@10                  & HR@10                    & NDCG@10         & HR@10                    & NDCG@10                   \\ 
\midrule
\multirow{4}{*}{Text}  & $\text{(0) NIG}_1$                        & 0.1277                   & 0.0987                   & 0.1163                   & 0.0844          & 0.0885                   & 0.0473                    \\
                       & (1) QLG~                                  & 0.1282                   & 0.0993                   & 0.1293                   & 0.0913          & 0.1010                   & 0.0531                    \\
                       & $\text{(2) NIG}_1 \text{w/ pre-training}$ & 0.1334                   & 0.1043                   & 0.1305                   & \textbf{0.0959} & 0.0950                   & 0.0508                    \\
                       & (3) QLG~~w/ pre-training                  & \uline{0.1362}           & \uline{0.1051}           & \uline{0.1314}           & 0.0944          & 0.0995                   & 0.0521                    \\ 
\midrule
\multirow{3}{*}{Image} & $\text{(4) NIG}_2$                        & 0.1243                   & 0.0968                   & 0.1117                   & 0.0812          & 0.0881                   & 0.0478                    \\
                       & (5) QLG~ ~                                & 0.1280                   & 0.1001                   & 0.1259                   & 0.0901          & \uline{0.1017}           & \uline{0.0540}            \\
                       & (6) QLG~~w/ pre-training    & 0.1322                   & 0.1029                   & 0.1265                   & 0.0914          & 0.0987                   & 0.0526                    \\ 
\midrule
All                    & (7) MQL4GRec                  & \textbf{\textbf{0.1375}} & \textbf{\textbf{0.1060}} & \textbf{\textbf{0.1327}} & \uline{0.0950}  & \textbf{\textbf{0.1033}} & \textbf{\textbf{0.0548}}  \\
\bottomrule
\end{tabular}
}
\end{table}

\section{Discussion}
\label{discussion}

\subsection{Handling Collisions.}
\label{app:handling}

To address the issue of item collisions, some methods \citep{tiger, p5id} append an additional identifier to the item indices, which may introduce semantically unrelated distributions.
LC-Rec \citep{zheng2023adapting} introduces a uniform distribution constraint to prevent multiple items from clustering in the same leaf node. 
Although the LC-Rec achieves better performance in handling collisions compared to previous approaches, it has an inherent problem: it cannot completely resolve item collisions when item modal content is identical or when the number of collisions exceeds the size of the last level's codebook. This leads to another issue: \textbf{multiple items sharing the same indices results in an unfair comparison of performance}.

In contrast, our method of dealing with collisions is more rational and can essentially solve the aforementioned problems. From the experimental results, our approach achieved similar outcomes to LC-Rec, hence we did not use a dedicated table to list the experimental results.

\subsection{Limitations.}
\label{Limitations}

Although our method achieves state-of-the-art performance, there are still some inherent limitations. For example: 1) The inference time is longer compared to traditional recommendation methods. This is an inherent flaw of generative recommendation systems, as such methods typically employ beam search and auto-regressive techniques to generate the next token.
2) Our method requires item content information, and the scenario where item content is missing has not yet been studied in the paper. This is an issue that we need to further analyze in our next steps.


\end{document}
