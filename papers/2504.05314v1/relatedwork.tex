\section{Related Works}
\label{related_work}
% \subsection{Generative Recommendation}
\paragraph{Generative Recommendation.}

Generative models are one of the hottest research topics in machine learning, resulting in some representative works such as Variational AutoEncoders (VAEs) \citep{vae}, Generative Adversarial Networks (GANs) \citep{gan} and Diffusion models \citep{ddpm}. Generally, generative models aim to learn the distribution of the training data $\mathbb{P}(\mathbf{x})$and generate new samples $\mathbf{z}\sim \mathbb{P}(\mathbf{x})$. These generative models have also been applied to recommendation, resulting in many remarkable works of VAE-based \citep{pevae,recvae}, GAN-based \citep{apr,ipgan,dasp} and diffusion-based \citep{diffkg,diffrec} recommendation.

Recently, Transformer-based PLMs such as LLaMA \citep{llama} and GPT \citep{gpt} have also shown promising capabilities in language generation. With the help of such powerful generative PLMs, some PLM-based recommendation methods have also been proposed. Some early works, such as P5 \citep{p5} and M6-Rec \citep{m6rec}, attempt to transform recommendation into a language generation task by designing prompts to bridge the gap between the downstream task and the pretraining task of PLMs. Some works focus on leveraging the prior knowledge in PLMs for recommendation by various tuning techniques such as parameter-efficient fine-tuning (PEFT) \citep{tallrec} and instruction tuning \citep{instructrec}.

One of the most important tasks in PLM-based recommendation is how to assign an unique sequence of tokens to each item as its ID. Early works \citep{p5,m6rec} directly use the original name of the item or randomly assign an integer for each item, which have weak transferability and are sometimes unintelligible to PLMs. SEATER \citep{seater} constructs tree-structured item IDs from a pretrained SASRec \citep{sasrec} model. P5-ID \citep{p5id} investigates the effect of different item IDs on recommendation. ColaRec \citep{colarec} captures the collaborative signals between items to construct generative item IDs. Notably, TIGER \citep{tiger} is the first attempt to use RQ-VAE to construct item IDs by quantizing the item embeddings.

% \subsection{Multi-modal Recommendation}
\paragraph{Multi-modal Recommendation.}
Multi-modal side information of items, such as descriptive text and images, has been shown to be effective in improving recommendations by providing richer contexts for interactions. Early works such as VBPR \citep{vbpr} extract visual features by matrix factorization to achieve more personalized ranking. Some works \citep{mmgcn,mkgat,grcn} leverage various types of graph neural network (GNN) to fuse the multi-modal features. For example, LATTICE \citep{lattice} designs a modality-aware learning layer to learn item-item structures for each modality and aggregates them to obtain latent item graphs. DualGNN \citep{dualgnn} proposes a multi-modal representation learning module to model the user attentions across modalities and inductively learn the user preference. MVGAE \citep{mvgae} uses a modality-specific variational graph autoencoder to fuse the modality-specific node embeddings.

Recently, with the profound development of foundation models in different modalities \citep{clip,gpt,t5}, some recent works attempt to leverage pretrained foundation models as feature encoders to encode the multi-modal side information. Following P5 \citep{p5}, VIP5 \citep{vip5} extends it into a multi-modal version which encodes the item images by a pretrained CLIP image encoder. MMGRec \citep{mmgrec} utilizes a Graph RQ-VAE to construct item IDs from both multi-modal and collaborative information. Moreover, IISAN \citep{fu2024iisan} propose a simple plug-and-play architecture using a Decoupled PEFT structure and exploiting both intra- and inter-modal adaptation.

% \paragraph{Multi-modal codebook.}