\section{Related Works}
\subsection{Surpass the Context Window Limitation}
The quadratic time complexity \(O(N^{2})\) of self-attention in transformer **Vaswani et al., "Attention Is All You Need"** poses limitations on training larger context window ____ . A wide range of approaches has been explored to extend the context window of these models. One approach, exemplified by Longformer and its subsequent iterations ____, aims to handle much longer contexts by modifying the self-attention mechanism. Another strategy involves training models on shorter sequences and then modifying the Transformer's structure to accommodate longer sequences during inference ____ . Lastly, some researchers are exploring methods such as chunking, leveraging discourse structures, and extracting key content to effectively reduce the context window required by LLMs, thereby facilitating the summarization of extensive documents ____ . Our paper will focus on the extraction method.

\subsection{RAG \& Long Document Summary}
To give LLM the capability of utilizing unknown data, RAG is developed to retrieve relevant materials and utilize them for downstream tasks ____ . Several downstream approaches enhance RAG from various perspectives including graphRAG ____, correctiveRAG ____, hierarchical RAG ____, modular RAG ____ and more ____ .

Long document summary includes extractive based method and abstractive based method. Initially, extractive methods, such as unsupervised Tf-Idf **Robertson et al., "SimpleBM25"** and supervised BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** , were widely adopted until the advent of more potent decoder-based models. Subsequently, abstractive approaches gained prominence, exemplified by the end-to-end method ____, and hybrid extractive-abstractive strategies____ .

Recently, Hypothetical Document Embedding (HyDE) applied LLM reranking to enhance diversity in Q\&A tasks, claiming their method outperforms MMR, which provides nearly no benefit on downstream tasks ____ . However, utilizing LLM for reranking can be expensive, slow and costly. In addition, these studies did not discuss the recall of relevant documents prior to LLM generation and explore various hyperparameters within MMR. This motivates us to explore more on MMR and FPS to enhance diversity, thereby improving the performance of LLMs on downstream tasks.

\subsection{Representative Sentences Extraction}
In the realm of extraction-based summarization, the criterion for choosing key sentences is crucial. Tf-Idf selected sentences based on the frequency of keyword ____ . PEGASUS selected sentences that utilizes structural information and ROUGE scores ____ . BERT selected representative sentences by distance in embedding space ____ . SentenceBERT **Reed et al., "Bert has a memory: Long term sequential Memory for Task-Agnostic Reasoning"** , DeBERTa **He et al., "Deberta: Pre-training of cross-modal transformers for natural language generation and question answering"** , E5 ____ and DPR ____ have been trained on larger corpora using more powerful computational resources. What is more, these methods only focus on selecting sentences that are related with core idea of the document, and it will cause problem for lacking the diversity of selected sentences. This research primarily focuses on leveraging smaller encoder-only models while incorporating diversity as a key consideration.