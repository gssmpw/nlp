\section{Related Works}
\subsection{Surpass the Context Window Limitation}
The quadratic time complexity \(O(N^{2})\) of self-attention in transformer \cite{vaswani2023attention} poses limitations on training larger context window \cite{ouyang2022training,openai2024gpt4}. A wide range of approaches has been explored to extend the context window of these models. One approach, exemplified by Longformer and its subsequent iterations \cite{beltagy2020longformer}, aims to handle much longer contexts by modifying the self-attention mechanism. Another strategy involves training models on shorter sequences and then modifying the Transformer's structure to accommodate longer sequences during inference \cite{press2022train}. Lastly, some researchers are exploring methods such as chunking, leveraging discourse structures, and extracting key content to effectively reduce the context window required by LLMs, thereby facilitating the summarization of extensive documents \cite{Koh_2022}. Our paper will focus on the extraction method.

\subsection{RAG \& Long Document Summary}
To give LLM the capability of utilizing unknown data, RAG is developed to retrieve relevant materials and utilize them for downstream tasks \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}. Several downstream approaches enhance RAG from various perspectives including graphRAG \cite{edge2025localglobalgraphrag}, correctiveRAG \cite{yan2024correctiveretrievalaugmentedgeneration}, hierarchical RAG \cite{sarthi2024raptorrecursiveabstractiveprocessing}, modular RAG \cite{gao2024modularragtransformingrag} and more \cite{10.2308/ISYS-2023-047}.

Long document summary includes extractive based method and abstractive based method. Initially, extractive methods, such as unsupervised Tf-Idf \cite{liang-etal-2021-improving} and supervised BERT \cite{devlin2019bert}, were widely adopted until the advent of more potent decoder-based models. Subsequently, abstractive approaches gained prominence, exemplified by the end-to-end method \cite{lewis2019bart} and hybrid extractive-abstractive strategies\cite{zhang2020pegasus, Yuan_Wang_Cao_Li_2023, xie2022gretel, lim-song-2023-improving}.

Recently, Hypothetical Document Embedding (HyDE) applied LLM reranking to enhance diversity in Q\&A tasks, claiming their method outperforms MMR, which provides nearly no benefit on downstream tasks \cite{eibich2024aragogadvancedragoutput, pickett2024betterragusingrelevant}. However, utilizing LLM for reranking can be expensive, slow and costly. In addition, these studies did not discuss the recall of relevant documents prior to LLM generation and explore various hyperparameters within MMR. This motivates us to explore more on MMR and FPS to enhance diversity, thereby improving the performance of LLMs on downstream tasks.

\subsection{Representative Sentences Extraction}
In the realm of extraction-based summarization, the criterion for choosing key sentences is crucial. Tf-Idf selected sentences based on the frequency of keyword \cite{liang-etal-2021-improving}. PEGASUS selected sentences that utilizes structural information and ROUGE scores \cite{zhang2020pegasus}. BERT selected representative sentences by distance in embedding space \cite{devlin2019bert}. SentenceBERT \cite{reimers2019sentencebert}, DeBERTa \cite{he2021debertadecodingenhancedbertdisentangled}, E5 \cite{wang2024textembeddingsweaklysupervisedcontrastive} and DPR \cite{karpukhin2020densepassageretrievalopendomain} have been trained on larger corpora using more powerful computational resources. What is more, these methods only focus on selecting sentences that are related with core idea of the document, and it will cause problem for lacking the diversity of selected sentences. This research primarily focuses on leveraging smaller encoder-only models while incorporating diversity as a key consideration.