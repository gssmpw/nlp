\section{Related Work}
\label{sec:related_works}

% CASH problemを解くための最も単純な手法として，ランダムサーチやグリッドサーチ**Suganuma, T., "A Simple and Scalable Method for Reducing the Search Space of Hyperparameter Optimization"**__**Wistuba, M., et al., "Grid Search Can Be Nearly Optimal for Some Problems"**がある．
% しかし，CASH problemの探索空間は通常のハイパーパラメータ最適化と比べて大規模であるため，ランダムサーチやグリッドサーチでは予測精度が高くなるハイパーパラメータと機械学習アルゴリズムの組み合わせの発見には通常のハイパーパラメータ最適化よりも膨大な時間を必要とする．

% CASH problemを解くために，ハイパーパラメータ最適化にも利用されるベイズ最適化**Shahriari, B., et al., "Taking the Human Out of the Loop: A Review of Bayesian Optimization"**や遺伝的アルゴリズム**Deb, K., "Multi-Objective Optimization Using Evolutionary Algorithms"**を利用した手法が開発されてきた．
For the CASH problem, BO based approaches **Snoek, J., et al., "Practical Bayesian Optimization of Machine Learning Algorithms"** and genetic algorithm based approaches **Huang, B., et al., "Evolutionary Computation for Hyperparameter Tuning"** have been studied.
%
% ベイズ最適化や遺伝的アルゴリズムを用いてCASH problemを解く方法の一つとして，
% 異なる機械学習アルゴリズムのハイパーパラメータ空間を以下のように一つの探索空間$\*{\Lambda}$として定式化する方法が活用されている．**Bergstra, J., et al., "Algorithms for Hyper-Parameter Tuning"**
A well-known approach to dealing with different HP spaces is to create the concatenated space 
**Falkner, S., et al., "BOHB: Robust and Efficient Hyperparameter Optimization"**:
\begin{align*}
 \*{\Lambda} = \*{\Lambda}^{(1)} \cup \*{\Lambda}^{(2)} \cdots \cup \*{\Lambda}^{(M)} \cup \left\{\lambda_r\right\}, 
 % \label{eq:SMAC_Search_Space}
\end{align*}
%ただし，$\lambda_r$は候補となる機械学習アルゴリズム$A^{(1)}, \ldots, A^{(L)}$を識別するパラメータである．
where 
$\lambda_r$
is a parameter indicating which candidate ML algorithm $A^{(1)}, \ldots, A^{(M)}$ is now selected (active).
%
% この方法により構成された探索空間上で表現されるブラックボックス関数を1つの確率モデルで予測を行うことで，適切な機械学習アルゴリズムとハイパーパラメータの組み合わせの発見を行う．
% このハイパーパラメータ空間を結合する方法でベイズ最適化を活用する場合，conditional hyperparameter**Hutter, F., et al., "AutoML: Approaches, Challenges and Open Problems"**
to扱うことができるSMAC**Eggensperger, K., et al., "Sequential Model-Based Optimization for General-Task Objectives"**を活用した手法がよく用いられる．
%
% SMACでは構成された探索空間において，機械学習アルゴリズム$A^{(m)}$が持たないハイパーパラメータにはdefault valueという固定値を与える．
The surrogate model in SMAC needs some default value for an HP that an ML algorithm $A^{(m)}$ does not have **Falkner, S., et al., "BOHB: Robust and Efficient Hyperparameter Optimization"**.
%
% 従って，与えられたタスクに対して最も高い予測精度を記録する機械学習アルゴリズムとハイパーパラメータの組み合わせが候補に含まれず，最適解を必ず得ることができない可能性がある．