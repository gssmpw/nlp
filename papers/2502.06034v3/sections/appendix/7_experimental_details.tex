\subsection{Experimental Details}

This section provides details on the training and evaluation procedures for the models presented in this paper. The full code for reproducing results and visualizations from the main text is available at: \url{https://github.com/anonymous123-user/Traveling_Waves_Integrate}.

Each model is trained for 300 epochs on the MNIST, Tetrominoes, and Multi-MNIST datasets. We evaluate the validation loss at the end of every epoch and retain the model with the lowest validation loss throughout training. The training process employs the Adam optimizer \citep{kingma_adam_2017} with a learning rate of 0.001 and a batch size of 64.

For dataset partitioning, we use 51,000 images for training, 9,000 for validation, and 10,000 for testing in MNIST. The Tetrominoes dataset consists of 10,000 images for training, 1,000 for validation, and 1,000 for testing. Similarly, the Multi-MNIST dataset comprises 10,000 images for training, 1,000 for validation, and 1,000 for testing. Table \ref{tab:merged_appendix} reports pixel-wise accuracy, IoU, and loss for both foreground and background.

Each model is trained using multiple random seeds. For MNIST and Tetrominoes, we train each model using 10 different random seeds, while for Multi-MNIST, we use 12 seeds. For example, the NWM with a linear readout is trained on MNIST 10 times, each with a different random seed. After training, we evaluate each model individually and present the aggregated results, including the mean and standard deviation, in Tables \ref{tab:merged}, \ref{tab:multi-mnist}, and \ref{tab:merged_appendix}. In total, we train 150 models on MNIST, 150 models on Tetrominoes, and 60 models on Multi-MNIST, leading to a total of 360 models.

We train the Conv-LSTMs for 20 timesteps. The NWM model is trained for 100 timesteps on the MNIST, Tetrominoes, and Multi-MNIST datasets, while for the polygons dataset, the NWM runs for 500 timesteps. In the FFT readout, we use the real component of the discrete Fourier transform. This results in 50 bins for the NWM on MNIST, Tetrominoes, and Multi-MNIST, and 250 bins for the polygons dataset. The LSTM model outputs 10 Fourier bins for MNIST, Tetrominoes, and Multi-MNIST.

All convolutional models are trained with 16 channels. To ensure a fair comparison with the NWM, a linear layer operating channelwise outputs 100 channels. The readout MLP used for MNIST and Tetrominoes consists of four layers. Its input size is given by the number of Fourier bins multiplied by 2, followed by two hidden layers of 256 neurons each, with ReLU activation between layers, and a final output layer producing logits for classification. On Multi-MNIST, the NWM employs a six-layer readout with 32 neurons in each hidden layer.

For the U-Net architecture, the Arch parameter in Tables \ref{tab:multi-mnist} and \ref{tab:merged_appendix} refers to the number of feature maps output by the first convolutional layer. Each U-Net begins with stacked convolutions that maintain spatial resolution, producing $Arch$ feature maps (e.g., 3). The following four layers apply multiple convolutional operations per layer, each reducing spatial resolution by half while doubling the number of feature maps. If the initial layer outputs 3 feature maps, the subsequent layers modify the number as follows:
\[
3 \rightarrow 6 \rightarrow 12 \rightarrow 24 \rightarrow 48.
\]
The final number of feature maps, determined by $Arch$, follows:
\[
2 \rightarrow 32, \quad 3 \rightarrow 48, \quad 4 \rightarrow 64, \quad 5 \rightarrow 80.
\]

The U-Net decoder progressively upsamples the spatial resolution over four layers while simultaneously reducing the number of feature maps by half. By the fourth layer, it restores the feature map count to the value specified by $Arch$. Finally, 1Ã—1 convolutions are used to project the feature maps into logits for pixel-wise classification.

\newpage

\subsection{Extended Semantic Segmentation Results}
Below we include the accuracies, IoU, and Loss values computed over the full set of classes (including the background class) for all models presented in the main text. While these numbers are artificially inflated from the inclusion of the background class (which dominates the majority of pixels) we include them here for competeness. 
\input{tables/tab1_appendix}
\input{tables/tab2_appendix}

Below, we include the minimum, maximum, and median accuracies, IoU, and loss values for MNIST and Tetrominoes.
\input{tables/tab1_appendix_median_min_max}


\begin{figure}[h!] % 'htbp' specifies how the figure should be placed
    \centering
    \includegraphics[width=\linewidth]{figures/fft_mulitpolygon.png} % Width can be adjusted as needed
    % \vspace{-5mm}
    \caption{Visualization of all frequency bins for an example of the Polygons dataset. We see that the background and different shapes appear in separate frequency bins, allowing the model to easily segment the shapes semantically in frequency space. }
    \label{fig:all_fft}
\end{figure}



\begin{figure}[h!] % 'htbp' specifies how the figure should be placed
    \centering
    \includegraphics[width=0.7\linewidth]{figures/shape_combinations.png} % Width can be adjusted as needed
    % \vspace{-5mm}
    \caption{Visualization of the impact of different combinations of shapes in the same image on the frequency space representation of the other shape for the Polygons dataset. We see that while there is a minor impact on the frequency space representation of each shape when another shape appears nearby, the overall frequency spectrum is relatively invariant. This implies that each neuron indeed has global information about all shapes present in the image, but mainly represents the shape which it is currently `located within'.}
    \label{fig:shape_combo}
\end{figure}