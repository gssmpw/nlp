
\section{Introduction}

The propagation of traveling waves of neural activity has been measured on the surface of the brain from the earliest neural recordings \citep{adrian1934, goldman1949, lilly1949method, mickle1953}. Such waves have been measured to travel both locally and globally across cortical regions with a range of velocities \citep{auditory_waves, Muller2016, Zhang2018}. Stimulus-evoked traveling waves have been directly measured in visual cortex \citep{cowey1964} with increasingly sophisticated methodology from penetrating electrodes \citep{EBERSOLE1981160} to voltage-sensitive dye imaging \citep{Muller2014}, including in awake behaving primates \citep{Davis2020}.  Driven by these observations, many theoretical arguments have been put forth to explain the functional roles of these dynamics. Examples include that they are relevant to predictive coding \citep{pc_waves}, the representation of symmetries \citep{keller2024spacetimeperspectivedynamicalcomputation}, the consolidation of long-term memories \citep{Muller2018}, and the encoding of motion \citep{motionwaves}. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/teaser.png} 
    % \vspace{-8mm}
    \caption{\textbf{Overview of traveling wave-based spatial information integration}. An input stimulus triggers an initial condition and sets the response properties of a lattice of neurons with both local input receptive fields and recurrent connectivity. This initial condition evolves over time under the recurrent wave dynamics, and the resulting timeseries at each neuron becomes a globally integrated representation of the visual stimuli.}
    \label{fig:overview}    
    \vspace{-3mm}
\end{figure}

Most relevant to this study, one often hypothesized role is that traveling waves serve as a mechanism for integration and transfer of information over long distances -- a mechanism that is believed to play an important role specifically within visual cortex \citep{Sato2012}. For example, \cite{Kitano_Niiyama_Kasamatsu_Sutter_Norcia_1994} demonstrated early on that local-field potential (LFP) responses of neurons in primary visual cortex could be elicited by stimuli far outside their classic retinotopic receptive fields with increased latency as a function of distance, implying a long-range distance-delayed integration of information. These findings were later reinforced by the intracellular subthreshold membrane potential recordings of \cite{yves1999}, denoting this extended receptive field the `visually evoked synaptic integration field'. However, the hypothesized role of information transfer and integration extends beyond visual stimuli. For example, \cite{motor_waves} found that beta frequency oscillations propagated spatially across the motor cortices of monkeys in preparation for movement, and that information about the visual target was directly encoded in these waves. Similarly, \cite{info_transfer} used direction-specific causal information transfer metrics to demonstrate that traveling waves in the gamma frequency band are correlated with information transfer between different cortical regions; while \cite{direction_waves} showed that waves change direction during information retrieval and processing in a working memory task.

Despite these promising observations however, it remains challenging to investigate these ideas computationally due to a lack of task-trainable artificial neural network models which exhibit traveling wave dynamics. In modern artificial neural networks, information is integrated and transmitted over spatial distances of an input (e.g. an image) or between `tokens' of a sequence either via extremely deep convolutional neural networks \citep{resnet}, bottleneck/pooling layers \citep{ronneberger_u-net_2015}, or all-to-all connectivity as-in Transformers \citep{vaswani2023attentionneed}. Each of these approaches comes with its own computational complexity and expressivity limitations, and it is therefore of great interest to explore alternative methods to integrate disparate information in neural systems.

\looseness=-1
In this paper, we aim to make progress towards understanding the causal role of wave dynamics in the transfer of information by filling this modeling gap, and exploring the computational potential of wave-based models in task-relevant settings. To begin, we take inspiration from the famous mathematical question ``\emph{Can one hear the shape of a drum}", and explore if the techniques underlying this problem, namely the representation of global information through stationary solutions to wave-based dynamical systems, can be equivalently applied to extract global information from locally-connected recurrent neural network hidden states over time. In first part of this paper, we begin with a review of this problem and the associated formalism, outlining how we may construct trainable recurrent neural networks to leverage these ideas as a computational principle. On toy tasks, we demonstrate that these simple models do indeed match theoretical predictions (Figure \ref{fig:theory}), and that they generate wave dynamics which enable the disentanglement of simple shapes in frequency space (Figures \ref{fig:polygons_vid} \& \ref{fig:polygons_fft}). 
In the second part of this paper, we use this intuition to build a suite of more computationally capable convolutional recurrent neural networks (conv-RNNs), with inherently limited receptive field sizes in both their initial encoders and recurrent connections, and test them on more complex global-information processing (semantic segmentation) tasks. We demonstrate how by using the timeseries of each neuron's recurrent neural activity as our primary neural representation during training (schematized in Figure \ref{fig:overview}), such models are able to outperform other locally constrained models, and even rival the performance of some deeper convolutional models such as U-Nets, while often converging more consistently to favorable solutions -- ultimately suggesting that traveling wave-based information integration may be an efficient and stable alternative to existing deep neural network spatial integration techniques.  