\section{Related Work}

% Reaction Diffusion
\paragraph{Wave-based Computing}
While prior work on wave-based computing in trainable task-oriented neural networks remains scarce, there is a rich history of using wave-like or other spatiotemporal field dynamics generally for computation.  
Early work studied the ability for waves to perform simple logical operations and thereby compute in a distributed manner \citep{pwc, wave_compute}, while other work has studied the ability for physical water waves to act as literal instantiations of classic `reservoir computers' \citep{maksymov2023analoguephysicalreservoircomputing}. Classically, the domain of `Neural Field Theory' has studied the role of spatiotemporal field dynamics in neural computation from a rigorous mathematical standpoint, although to-date these models have not been adapted to deep-neural network task-oriented performance. We refer readers to \cite{nft} for a thorough review of such models. 

More recently, \cite{hughes2019wave} have noted the analogy between the wave equation and recurrent neural networks, as we have done here, and used this to suggest that wave-based RNNs with learnable wave speeds may perform a type of analog computation. The authors use this to perform acoustic signal classification in a simplified setting, similar to our study in spirit, but differing in how waves are used and their computational purpose. Most related to the present study, \cite{BALKENHOL20244288} use an architecture similar to ours, with a Laplacian recurrent operator, damping, and gating, to show that when provided with an audio signal at a specific spatial location of the network, neurons at more distant locations can perfectly reconstruct the signal. The authors also show that this network is able to reproduce electrical recordings from macaque monkeys in response to simple grating stimuli, hypothesizing that their detection of high frequency waves is highly related to the transfer of information over large cortical distances.  

\looseness=-1
In terms of task-oriented wave-based models, recent work by \cite{felix} extensively studies the computational abilities of oscillatory neural networks, and specifically notes the emergence of traveling waves in these models in response to visual stimuli. Similarly, work by \cite{nwm, wrnn} studies wave-based RNNs for sequence processing and prediction. Our work fundamentally differs from these in the precise study of how these waves may be utilized for the spatial integration of visual information, as is hypothesized to happen in the visual cortex. Furthermore, our work uniquely demonstrates that a timeseries based readout is crucial for performing this type of integration, inspired by Kac's question, opening the door for future novel applications of these models. 

\vspace{-4mm}
\paragraph{Recurrence vs. Depth}
Another relevant line of research concerns the ability to trade off depth for recurrence in CNNs. 
Early work in this area was performed by \citet{liao2020bridginggapsresiduallearning}, with a more extensive recent study performed by \citet{schwarzschild2022the}. The authors demonstrate how iterating a single convolutional layer in a deep CNN yields similar performance to equivalently deep fully untied CNNs. Our work differs from these in that we demonstrate the advantage of a timeseries readout mechanism, inspired by Kac's question, whereas prior work can be seen as using the 'last' hidden state mechanism, that we see underperforms in this work. Interestingly, our findings thus suggest a potential novel method to improve the performance of these recurrent alternatives to deep networks through the use of our readout, a direction we intend to study in future work. Other more machine learning focused work has studied the impact of various weight-sharing schemes in deep convolutional networks \citep{eigen2014understandingdeeparchitecturesusing, jastrzębski2018residualconnectionsencourageiterative, boulch2017sharesnetreducingresidualnetwork}, however these share the same distinction with the present study in terms of their readout mechanism, while our proposed timeseries readouts appear to be uniquely linked to the wave dynamics that emerge in our models. 


\subsubsection{Binding By Synchrony}
Finally, we believe our work shares an interesting connection with the ``binding by synchrony'' concept \citep{Singer:2007} from early neuroscience research. Specifically, while our model's `binding' of parts into wholes does not rely on precise zero-lag synchrony—where oscillators within an object are perfectly in phase, as in the original framework; our method does rely on traveling waves of activity within objects that can be interpreted as a type of phase-lag synchrony. The ``binding operation'' then involves a transformation of the time signal using a suitable linear projection (our proposed timeseries readout). We believe this connection is valuable precisely since it enables a connection with the extensive historical literature on this concept, while simultaneously forming novel predictions on how such phenomena might manifest in natural neural systems. 
On the machine learning side of this concept, our work shares a strong connection with a class of object-centric learning methods which leverage a notion of synchrony of neural activations to define `bound' visual units for computational purposes. This includes models such as complex autoencoders \citep{lowe_complex-valued_2022, lowe_rotating_2024, stanic_contrastive_2024, gopalakrishnan_recurrent_2024} and recent Artificial Kuramoto Oscillatory Neurons (AKOrN) \cite{miyato_artificial_2024}. 
Unlike our method, the waves in the AKOrN model are not used directly as a representation themselves, but instead are neglected through the use of the `last hidden state' readout method. Perhaps most related to our work, \cite{liboni_image_2023} use a complex-valued recurrent neural network designed to generate traveling waves for image segmentation, with binding information encoded in the temporal phase sequence of these waves. This method can indeed be seen as using traveling waves to integrate information spatially, but contains no trainable components, offering a more theoretical exposition to the problem, as opposed to the task-oriented empirical study presented here. 