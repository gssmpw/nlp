
\section{Experiments: Semantic Segmentation}
In the following, we take the intuition gathered from the previous theoretical motivation and apply it to study if traveling waves can be used in locally constrained recurrent neural network architectures to solve a task requiring global information integration. In particular, on all experiments, the task is semantic segmentation, where each pixel of the original image must be classified as either background, or one of the classes from the dataset, and models are trained to minimize a pixel-wise cross-entropy loss. Crucially, all locally restricted models make use of shallow convolutional encoders (with $3\times3$ kernels), convolutional recurrent connections, and pixel-local decoders, ensuring that the spatial receptive field of each neuron in a single feed-forward pass is limited to be significancy less than the inherent length scale of features necessary to identify class labels in each dataset -- meaning that if the network solves the task, it must be integrating global information through recurrent connections. As baselines, we compare with CNN models of various depths, from 2 to 32 layers, which thereby have receptive fields which span from local to global with respect to the image. On the final more complex datasets, we additionally compare with a more advanced U-Net architecture \citep{ronneberger_u-net_2015} which uses simultaneous depth and a spatial bottleneck to transmit information globally.
We refer readers to the supplementary material for full training details. The full code and video visualizations for the results in this paper are available at: \url{https://github.com/anonymous123-user/Traveling_Waves_Integrate}

\subsection{Datasets}
To validate the core idea that traveling waves can be used to transmit information over large spatial distances, and that this global information can be decoded in a task-relevant manner, we primarily employ four datasets:

\textbf{Polygons:} First, we consider a simple dataset composed of white polygons on black backgrounds, where the classes are given by the number of sides of the polygons. The examples are synthetic $75\times75$ pixel grayscale images with 1 to 2 polygons, each with 3 to 6 edge roughly circumscribed within circles with radii of 15 to 20 pixels. On this dataset, the angle of the corners of the shape are sufficient to correctly classify those patches, but this information must then be transferred to the center of the shape for correct segmentation of the interior.

\textbf{Tetrominoes:} As a second slightly more complex dataset  that has been employed in prior segmentation work \citep{miyato_artificial_2024}, we employ own re-implementation of the Tetrominoes dataset \citep{multiobjectdatasets19}, where each image is composed of 1 to 5 `Tetris' like blocks of varying shapes and colors arranged on a black background. In detail, there are 6 distinct classes of objects from 14-28 pixels long. The increased complexity of shapes and number of objects per image increases the difficulty of this dataset over Polygons.  

\textbf{MNIST:} We use the MNIST dataset \citep{lecun1998mnist} but increase the spatial dimensions to $56\times56$ through interpolation. The pixels are binarized at a threshold of 0.5, and are assigned the associated class label of the digit in the image or `background'. This task is significantly harder than those above since the shapes now differ between instances (i.e. each hand-written 3 is unique) and thus the model must learn these sets of invariances when processing the dynamic representations. 

\textbf{Multi-MNIST:} Finally, we introduce a variant of the Multi-MNIST dataset \citep{sabour2017dynamic}, where each image contains between 1 and 4 digits placed at random, non-overlapping locations on a 128×128 grid. To construct these images, we first upscale each 28×28 MNIST digit to 42×42 using interpolation, and then position them on the larger canvas. The pixels are binarized at a threshold of 0.5, with each pixel labeled according to the digit it belongs to or as `background'. This task is significantly more challenging than previous ones due to the increased spatial dimensions (and thereby greater spatial integration distances), combined with the combinatorial variation in the number and placement of digits.


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/polygons_fft_seq_smaller.png} 
    \vspace{-5mm}
    \caption{\textbf{Wave-based models learn to separate distinct shapes in frequency space.} (Left) Plot of predicted semantic segmentation and select set of frequency bins for each pixel of a given test image. (Right) The full frequency spectrum for each shape in the dataset, averaged over all pixels containing that class label in the dataset. We see that different shapes have qualitatively different frequency spectra, allowing for $>99\%$ pixel-wise classification accuracy on a test set.}
    \label{fig:polygons_fft}
\end{figure*}


\subsection{Models}
In the following we detail the local recurrent models and the associated global baselines that we use in this study. For all models, given an input image $\mathbf{x} \in \mathbb{R}^{C \times H \times W}$, the target output is $\mathbf{y} \in \mathbb{R}^{N \times H \times W}$, a set of N class logits for each pixel.

\paragraph{Locally Coupled Oscillatory RNN (NWM)} As a model which most closely follows the motivational drum analogy introduced earlier, we implement a recurrent neural network parameterized as a network of locally coupled oscillators. In prior work, this model has been referred to as the Neural Wave Machine (NWM) \citep{nwm}, based on the coRNN \citep{cornn}, and is known to be biased towards traveling wave dynamics. In fact, it is known that in the continuum limit of the number of neurons, such networks of coupled oscillators reduce exactly to wave dynamics of Equation \ref{eq:wave} \citep{harvard_lecture_4}. Explicitly, the dynamics of this model are given as:
\begin{equation}
\label{eqn:nwm}
\frac{\partial^2 \mathbf{h}}{\partial t^2} = \sigma\left(\mathbf{w}_{h} \star \mathbf{h}\right) - \gamma_{\theta}(\mathbf{x)} \odot \mathbf{h} - \alpha_{\theta}(\mathbf{x}) \odot \frac{\partial \mathbf{h}}{\partial t}.
\end{equation}
where $\sigma = \tanh$ is the hyperbolic tangent function. In this work, in order to make the recurrent dynamics a function of the input without explicitly clamping hidden states as done in the theoretical section, we modify the original NWM such that the natural frequencies of each oscillator $\gamma$, and the damping term $\alpha$ are a function of the input image, computed through shallow 3-layer CNN models ($\gamma_{\theta}(\mathbf{x)}, \alpha_{\theta}(\mathbf{x)}$). This crucially allows the way in which waves propagate over the hidden state (and therefore the resulting time-dynamics) to be dictated by the input image, and specifically enables the network to emulate soft boundary conditions via large differences in natural frequencies (as seen in Figure \ref{fig:polygons_vid}). The initial state of the model is also set by the shallow 4-layer CNN encoder $\mathbf{h}_0 = f_{\theta}(\mathbf{x})$, with ($3 \times 3$) kernels. Crucially, this yields a receptive field size of ($9 \times 9$) in the final layer, significantly smaller than the spatial dimensions of all shapes in the datasets listed above.

We initialize the recurrent convolutional kernel $\mathbf{w}_h$ to the finite difference approximation of the Laplacian operator from Equation \ref{eq:fd_laplacian} to bias the model towards wave propagation, and initialize the natural frequency encoder $\gamma_{\theta}(\mathbf{x)}$ to the identity to encourage soft boundaries. Finally, we numerically integrate the second order ODE above using the Implicit-Explicit integration scheme from \citep{cornn} with a timestep size of $0.1$ for a fixed amount of time (100 timesteps). All recurrent convolutions in this paper are performed with circular padding to avoid boundary effects. We use 2 channels in the recurrent hidden state on MNIST \& Tetrominoes, and 16 on Multi-MNIST.

\paragraph{Convolutional LSTM} To explore how other locally-constrained recurrent architectures may solve these tasks when not explicitly biased towards wave-dynamics \emph{a prioi}, we implement a convolutional variant of the LSTM \citep{lstm}, where all previous dense connections are now replaced with local convolutions over the spatial dimensions of the hidden state. Explicitly:
\begin{align}
\mathbf{x}_0 &= f_{\theta}(\mathbf{x}), \quad
\mathbf{i}_t = \sigma\!\bigl(\mathbf{w}_{i} \star \mathbf{x}_{t-1} + \mathbf{b}_i\bigr), \\
\mathbf{f}_t &= \sigma\!\bigl(\mathbf{w}_{f} \star \mathbf{x}_{t-1} + \mathbf{b}_f\bigr), \quad 
\tilde{\mathbf{C}}_t = \tanh\!\bigl(\mathbf{w}_{c} \star \mathbf{x}_{t-1} + \mathbf{b}_c\bigr),\\
\mathbf{C}_t &= \mathbf{f}_t \odot \mathbf{C}_{t-1} \;+\; \mathbf{i}_t \odot \tilde{\mathbf{C}}_t, \quad
\mathbf{o}_t = \sigma\!\bigl(\mathbf{w}_{o} \star \mathbf{x}_{t-1} + \mathbf{b}_o\bigr), \\
\mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{C}_t), \quad
\mathbf{x}_t = \mathbf{w}_{o2} \star \sigma(\mathbf{w}_{o1} \star \mathbf{h}_t).
\end{align}

Where $\sigma$ is a sigmoidal activation function, and $\odot$ denotes the Hadamard product. For this model, we do no special initialization, yet as we show in Figure \ref{fig:states}, these models still learn to produce waves in their hidden state in order to integrate spatial information and solve the  task. We use two hidden channels for the LSTM, matching other models in the study. All LSTMs are trained with 20 timesteps as this was found to be optimal for small datasets. Although 100 timesteps may be beneficial in more complex settings, we note that it takes significantly longer to train than other models in this study, and thus we default to the NWM for our larger scale experiments.

\looseness=-1
\paragraph{Recurrent Readout} For the two recurrent models listed above, we must decide how to read out the class label from the sequence of hidden states. The simplest option is to feed the hidden state at the final timestep to a pixel-wise `readout' network for classification. We denote these models \emph{Last} in Table \ref{tab:merged}. Alternatively, we can take some function of the hidden states over a greater extent of the time series, and then feed this output into the readout network. The options for such a time-projection function include taking the maximum or mean hidden state values over time (\emph{Max} and \emph{Mean} respectively), computing the Fourier coefficient amplitudes of the time series (denoted \emph{FFT}), or computing a learnable linear projection of the the full timeseries (denoted \emph{Linear}). In all cases, we parameterize the readout module as a 4-layer MLP for Polygons, MNIST, and Tetrominoes, and a 6-layer MLP for Multi-MNIST. 

\paragraph{Feed-Forward CNN Baselines}
The CNN baselines are composed of a number (L) of convolutional layers, each with ($3\times3$) convolutional kernels, and 16 channels. This baseline is intended to demonstrate the inability of neurons with restricted local receptive fields to perform tasks which require global information (for small L), and the ability of global receptive fields to improve this performance (for large L). Explicitly, with $\mathbf{w}^l$ denoting layer $l$'s convolutional kernel and $\sigma$ denoting a $ReLU$ \citep{nair2010rectified} activation function,  
{\fontsize{9}{12}\selectfont
\begin{equation}
    \mathbf{\hat{y}} =  \sigma(\mathbf{w}^L \star \sigma(\mathbf{w}^{L-1} \star  \ldots \sigma(\mathbf{w}^1 \star \mathbf{x}+ \mathbf{b}^1)\ldots + \mathbf{b}^{L-1}) +  \mathbf{b}^{L})),
\end{equation}}
where $\mathbf{\hat{y}}$ is fed channelwise to a linear layer that outputs 100 channels, equivalent to the time-projected output of the RNN models above. The output of this linear layer is then similarly fed into a `readout' MLP which operates on each pixel's channels individually to produce the class logits.

\paragraph{Non-local U-Net Baseline} Finally, as a competitive non-local effective upper bound on the local models' performance, we implement a simple U-Net model \citep{ronneberger_u-net_2015} to perform segmentation. Succinctly, these models contain 4 encoder layers that decrease spatial resolution and 4 decoder layers that increase spatial resolution, starting with $c_{in}$ channels, and reaching $c_{in} \cdot 2^4$ channels in the bottleneck. Crucially because of the spatial bottleneck, the receptive fields of pixels in the output layer of this model cover the entire image, allowing for simple solutions to the semantic segmentation task. For the U-Net models, we simply take the final output as logits since the decoder network can be seen as type of 'readout'. We evaluate U-Net on the most challenging dataset: Multi-MNIST.