\section{The Price of Robustness}\label{sec:characterizations}

We start by providing a characterization of how the value $\textnormal{OPT}(\delta)$ of an optimal $\delta$-robust contract varies as a function of the parameter $\delta \in (0,1)$, which controls the suboptimality of the agent's best response.
%
The goal of our analysis is to quantify how much is the \emph{price (in terms of utility) that the principal incurs for being robust} to agent's approximate best responses. 
%
Indeed, the parameter $\delta$ can be interpreted as a measure of the robustness level of the principal's contract, with higher $\delta$ values indicating higher levels of robustness.

We first establish upper and lower bounds that identify a suitable region in which the values of $\textnormal{OPT}(\delta)$ are contained.
%
Such bounds only depend on the parameter $\delta \in (0,1)$, the value of an optimal non-robust contract $\textnormal{OPT}$, and the social welfare $\textnormal{SW}$ achievable with non-robust contracts.
%
%\textcolor{red}{OPEN PROBLEM: $\textnormal{OPT}(\delta)\le \max\left(0, \, \textnormal{OPT} - \delta \right) ?$  }
\begin{restatable}[Upper and lower bounds]{proposition}{RealtionNonRobust}\label{pro:properties}
	Given an instance of hidden-action principal-agent problem:
	\begin{enumerate}
		\item For every $\delta \in (0,1)$, it holds:
		\begin{equation*}
			\textnormal{OPT}(\delta) \ge \textnormal{OPT}_{\textnormal{LB}}(\delta) \coloneqq \textnormal{OPT} - 2 \sqrt{\delta} + \delta .
		\end{equation*}
		\item For every $\delta \in (0,1)$, it holds:
		\begin{equation*}
			\textnormal{OPT}(\delta)\le \textnormal{OPT}_{\textnormal{UB}}(\delta) \coloneqq \max\left\{0, \, \textnormal{SW} - \delta \right\}. 
		\end{equation*}
	\end{enumerate}
\end{restatable}
%
In order to prove the first point in Proposition~\ref{pro:properties}, we show that it is always possible to convert a non-robust contract into a robust one by suitably moving it towards the direction of the principal's reward vector.
%
In order to prove the second point of Proposition~\ref{pro:properties}, we observe that, if an optimal $\delta$-robust contract $p^\star$ provides the principal with an expected utility strictly larger than zero, then the opt-out action does \emph{not} belong to the set of $\delta$-best responses for such a contract.
%
Therefore, in an optimal $\delta$-robust contract that provides the principal with an expected utility strictly larger than zero, the agent's utility is at least $\delta > 0$, and thus the principal's expected utility is at most $\textnormal{SW} - \delta$.
%
%We also note that if all the agent's actions with zero cost provide the principal with an expected reward strictly larger than zero, the second point in Proposition~\ref{pro:properties} may no longer hold.
%
%However, this scenario is quite unrealistic, as it corresponds to a situation where the principal, even when committing to provide the agent a null payment in each outcome, achieves a utility strictly greater than zero.

The following proposition shows that the upper and lower bounds in Proposition~\ref{pro:properties} are tight. Formally:
%
\begin{restatable}{proposition}{RealtionNonRobustTwo}\label{pro:tightness}
	Given $\delta \in (0,1)$, for every integer $n > n(\delta)$, there is an instance of hidden-action principal-agent problem (parametrized by $\delta$) with $2n + 1$ actions, where:
	\begin{equation*}
		\textnormal{OPT}(\delta) -\textnormal{OPT}_{\textnormal{LB}}(\delta) \le \mathcal{O}\left(\frac{1}{n}\right).
	\end{equation*}
	%
	% where $ \textnormal{OPT}_{\textnormal{LB}}(\delta) :=  \textnormal{OPT} - 2 \sqrt{\delta} + \delta$. 
	%
	Furthermore, for any $\delta\in(0,1)$, there exists an instance of hidden-action principal-agent problem in which it holds: $$\textnormal{OPT}(\delta)= \textnormal{OPT}_{\textnormal{UB}}(\delta).$$
	%the following holds: 
	%$$\textnormal{OPT}(\delta)= \textnormal{OPT}_{\textnormal{UB}}(\delta)= \textnormal{SW}-\delta.$$
\end{restatable}



% 
%It is important to outline that both the instances considered in Proposition~\ref{pro:tightness} are characterized by only two outcomes.
%We observe that the negative result presented in Proposition~\ref{pro:tightness} can be circumvented by introducing an additional set of assumptions.
%
%Specifically,~\citet{lin2024generalizedprincipalagentproblemlearning} proved that it is possible to achieve an $\textnormal{OPT} - \mathcal{O}(\delta)$ lower bound on the value of $\textnormal{OPT}(\delta)$ when the parameter $\delta>0$ is sufficiently small and there are no dominated actions.
%
Proposition~\ref{pro:tightness} shows that our upper bound $\textnormal{OPT}_{\textnormal{UB}}(\delta)$ is (strictly) tight, while our lower bound $\textnormal{OPT}_{\textnormal{LB}}(\delta)$ is tight up to an additive term that linearly goes to zero as the number of agent's actions $n$ increases.


Finally, thanks to the previous propositions, we can show the following property about the value of an optimal $\delta$-robust contracts as a function of $\delta \in (0,1)$.
%
%
\begin{restatable}{proposition}{RealtionNonRobustThree}\label{pro:properties2}
	The function $(0,1) \ni \delta \mapsto \textnormal{OPT}(\delta)$ is continuous and non-increasing in $\delta$.
	%
	Moreover, $\lim_{\delta \to 0^+} \textnormal{OPT}(\delta)=\textnormal{OPT}$ and $\lim_{\delta \to 1^-} \textnormal{OPT}(\delta)=0$.
	%
	%\albo{Is the first zero correct?}
	%
	% $\textnormal{OPT}(\delta)$, as a function of $\delta \in (0,1)$, is a continuous non-increasing function satisfying the following conditions $\textnormal{OPT}(0^+)=0$ and $\textnormal{OPT}(1)=0$.
	%   \begin{equation*}
		%       \lim_{\delta \to 0^+}\textnormal{OPT}(\delta)=\textnormal{OPT} \quad \textnormal{and}\quad \lim_{\delta \to 1^-}\textnormal{OPT}(\delta)=0.
		%  \end{equation*}  
\end{restatable}
%
Figure~\ref{fig:jumps} shows an example of the region in which the value of $\textnormal{OPT}(\delta)$ is bounded, as defined by the lower and upper bounds in Propositions~\ref{pro:properties}~and~\ref{pro:properties2} as functions of $\delta$.
%
%of the region in which the value of $\textnormal{OPT}(\delta)$ is contained \jiarui{bounded? or "contained between a lower and an upper bound as functions of $\delta$"?} 
%\jiarui{Ah, maybe just say: Figure~\ref{fig:jumps} shows an example of the region defined by the lower and upper bounds as functions of $\delta$.}
%as a function of $\delta \in (0,1)$, $\textnormal{OPT}$, and $\textnormal{SW}$, according to Propositions~\ref{pro:properties}~and~\ref{pro:properties2}.
%
%

\begin{figure}[!t]
	\centering
	%\vspace{2mm}
	\resizebox{0.55\linewidth}{!}{\input{plot1.tikz}}
	%\input{plot1.tikz}
	%\vspace{-2mm}
	\caption{The blue area corresponds to the region in which $\textnormal{OPT}(\delta)$ is bounded as a function of $\delta \in (0,1)$ in an instance of hidden-action principal-agent problem with $\textnormal{SW}=0.9$ and $\textnormal{OPT}=0.7$. 
		% \jiarui{Also, do you mean to put the label $OPT(\delta)$ inside the blue region?}
	}
	\label{fig:jumps}
\end{figure}

%
\paragraph{Comparison with Stackelberg games}
%
The characterization results derived in this section exhibit some crucial differences compared to analogous results derived for Stackelberg games by~\citet{gan2024robust}.
%
Indeed, in such settings, \citet{gan2024robust}~show that the value of an optimal $\delta$-robust commitment crucially depends on a parameter $\Delta > 0$ that represents the \emph{inducibility gap} of the problem instance.
%
Intuitively, the inducibility gap encodes how easy it is for the leader to induce the follower to play \emph{any} action; see~\citep{gan2024robust} for a formal definition.
%
Specifically, in Stackelberg games, the leader's expected utility $\textnormal{OPT}(\delta)$ in an optimal $\delta$-robust commitment is a Lipschitz function lower bounded by $\textnormal{OPT} - \delta / \Delta$ if $\delta < \Delta$, whereas $\textnormal{OPT}(\delta)$ may \emph{not} be even a continuous function if $\delta > \Delta$.
%
In contrast, in hidden-action principal-agent problems, the value of an optimal $\delta$-robust contract is a continuous function with respect to $\delta \in (0,1)$, regardless of the inducibility gap of the instance.
%
Furthermore, the value of $\textnormal{OPT}(\delta)$ is either zero or it is upper bounded by $\textnormal{SW} - \delta$, showing that for large values of $\delta$, the maximum utility that the principal can achieve may be particularly small.
% 
Intuitively, this is because the principal must provide the agent with a large expected payment to induce them to take desirable actions rather than the opt-out one.
%
This upper bound does \emph{not} generally hold in Stackelberg games in which the principal may achieve a large utility even for large values of $\delta$.


