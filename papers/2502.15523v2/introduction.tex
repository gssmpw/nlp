\section{Introduction}\label{sec:introduction}

In \emph{hidden-action principal-agent} problems, a principal tries to steer the behavior of a self-interested agent toward favorable outcomes.
%
The agent has to take a costly action that stochastically determines an outcome resulting in a reward for the principal.
%
The main challenge is that the agent's action is \emph{hidden} to the principal, who can only observe the realized outcome.
%
Thus, the principal influences the agent's behavior by \emph{committing to a contract},
%, which is a payment scheme that specifies a monetary transfer from the principal to the agent for every possible outcome.
%
which is an outcome-dependent payment scheme whose aim is to induce the agent to take a high-cost action leading to high principal's rewards.
%
The principal's goal is to design an \emph{optimal} contract, namely one maximizing their utility, \emph{i.e.}, rewards minus payments.


Nowadays, principal-agent problems find application in a terrific number of real-world settings, such as, \emph{e.g.}, crowdsourcing~\citep{ho2015adaptive}, online labor platforms~\citep{kaynar2023estimating}, blockchain~\citep{cong2019blockchain}, delegation of machine learning tasks~\citep{cai2015optimum}, and pay-for-performance healthcare~\citep{bastani2016analysis,bastani2019evidence}.
%
Moreover, algorithmic contract design is playing a crucial role in today's world, which increasingly relies on AI agents to perform complex tasks (see, \emph{e.g.},~\citep{hadfield2019incomplete,saig2024incentivizing}).


Previous works on algorithmic contract design assume that the agent always plays a \emph{best-response action} to the principal's contract.
%
However, if the agent actually responds (even slightly) suboptimally to the principal, then the principal's utility may substantially deteriorate.
%
This may be the case in most of the real-world applications of interest, for several different reasons.
%
For instance, the principal  may \emph{not} perfectly know agent's features and account for the wrong best response, the agent may \emph{not} be powerful enough to compute an (exact) best-response action, or they may inaccurately interpret the principal's contract. 


In this paper, we initiate the study of hidden-action principal-agent problems \emph{under approximate agent's best responses}.
%
Specifically, we consider settings in which the agent may take actions that are up to $\delta \in (0,1)$ suboptimal under the principal's contract.
%\bac{for coherence with below it should be $\delta \in (0,1)$} suboptimal.
%under the principal's contract.
%
We do \emph{not} make any assumption on the specific $\delta$-best response selected by the agent, but we allow them to take \emph{any} of such actions.
%
Thus, we take a \emph{worst-case approach} and consider the problem of designing contracts under the assumption that the agent selects the worst $\delta$-best response for the principal.
%
Contracts designed in such a way are said to be \emph{robust}, as they guard the principal against any possible (conceivable) suboptimal behavior of the agent.

\subsection{Results and Techniques}

In this paper, we provide an extensive treatment of the computational and learnability aspects of the design of robust contracts.
%
The results presented in the paper are organized into three main parts as summarized below.


\paragraph{The Price of Robustness}
%
In the first part of the paper, as a preliminary analysis, we provide a characterization of the maximum utility that the principal can achieve by means of robust contracts. 
%
Specifically, we provide upper and lower bounds on this utility, as a function of a parameter $\delta \in (0,1)$ quantifying the agent's best response suboptimality.
%
These bounds give insights on the \emph{price (in terms of utility) that the principal incurs for being robust}, as the parameter $\delta$ can be seen as a measure of the robustness level of the principal's contracts.
%
Interestingly, our results show that, differently from what happens in general Stackelberg games (see, \emph{e.g.},~\citep{gan2024robust}), upper/lower bounds do \emph{not} depend on the inducibility gap $\Delta>0$ characterizing the problem instance.
%
In order to derive the bounds, we prove that it is possible to convert any non-robust contract into a robust one by properly moving its payments in the direction of principal’s rewards, and that, in an optimal robust contract, which provides the principal with a utility greater than zero, the agent’s utility should be at least $\delta$.




\paragraph{Computing Robust Contracts}
%
The second part of the paper addresses the problem of computing a robust contract that is optimal for the principal.
%
Our main contribution is a polynomial-time algorithm for this problem.
%
% the design of an algorithm that computes an optimal robust contract in polynomial time. 
%
This is perhaps surprising, since analogous problems in Stackelberg and Bayesian persuasion settings are known to be computationally intractable~\citep{gan2024robust,yang2024computational},
% \jiarui{added this:}
despite having more amenable solution spaces ($\Delta_m$ in these problems vs. $\mathbb{R}_+^m$ in contract design).
%
% Unlike in Stackelberg and Bayesian persuasion settings, where this problem is known to be computationally intractable (see~\citep{gan2024robust,yang2024computational}), we show that, in contract design, an optimal robust commitment can be efficiently computed.
%
At a high level, our algorithm cleverly exploits a particular structure that we discover in the robustness constraints (which ensure that the agent plays the worst approximate best response for the principal).
%
%the core idea exploited by our algorithm is that a constraint ensuring that the agent plays an approximate best response can be effectively reformulated.
%
% this is made possible by the fact that, differently from Stackelberg games, the constraint ensuring that the agent plays an approximate best response can be effectively reformulated.
%
Intuitively, we show that, when the agent's best response and their (worst) approximate best response are fixed to two arbitrarily-selected actions, the problem of computing a utility-maximizing robust contract can be formulated by means of a union of $n+1$ different \emph{linear programs} (LPs).
% 
Therefore, by taking the maximum over all these LPs, one can compute a utility-maximizing robust contract once the agent’s best response and their (worst) approximate best response are fixed.
% 
By further iterating this procedure for all the possible choices of the action pair, we then determine an optimal robust contract.



\paragraph{Learning Robust Contracts} 
%
In the third and last part of the paper, we investigate the learnability of robust contracts.
%
Specifically, we study an online learning framework similar to the one analyzed by~\citet{Zhu2023Sample}, in which the features of agent's actions, \emph{i.e.}, costs and probabilities over outcomes, depend on an agent's \emph{type} that is sampled at each round from some (fixed) unknown probability distribution.
%
%Next we study settings in which the principal and the agent interact over multiple rounds, with each round involving a repetition of the same instance of the problem. 
%
% In this scenario, the principal has no prior knowledge of the agent's features. 
%
At each round, after committing to some contract, the principal only observes the outcome (and its associated reward) realized as an effect of the approximate best response played by the agent.
%
Our main result within this online learning framework is the design of an algorithm that achieves sublinear regret with respect to always playing an optimal robust contract.
%
Additionally, we show that when the parameter $\delta \in (0,1)$ measuring the suboptimality of agent's actions is sufficiently small with respect to the time horizon, our algorithm also achieves sublinear regret with respect to always playing an optimal \emph{non-robust} contract. 
%
Our approach presents some advantages compared to the state-of-the-art proposed by~\citet{Zhu2023Sample} for the non-robust version of the problem, while achieving similar regret guarantees.
%
Indeed, our algorithm employs a simpler discretization of the set of contracts employed by the principal, which does \emph{not} require that the principal knows its rewards beforehand.
%
This is made possible by the fact that we rely on a novel ``continuity'' argument for the principal’s expected utility (see Lemma~\ref{lem:epsilonconvert}),  which is different from the one originally proposed by~\citet{Zhu2023Sample}.






