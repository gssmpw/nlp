
\section{Learning Robust Contracts}\label{sec:learning_problem}


The algorithm in Section~\ref{sec:computational_problem} works under the assumption that the principal has full knowledge of all the payoff-relevant information about the agent and, thus, they can compute an optimal $\delta$-robust contract.
%
% , we assumed that the principal has full knowledge of all payoff-relevant information of the agent and can therefore compute an optimal robust contract.
%
In this section, we address the case in which the principal has no such knowledge, and, thus, they have to learn an optimal $\delta$-robust contract in an online fashion, by repeatedly interacting with the agent.
%over multiple rounds.
%
% assume that the principal has no prior knowledge of this information and can only interact with the agent over multiple rounds.


\subsection{Learning Interaction}\label{sec:learning_interaction}


We consider an online learning framework similar to the one studied by~\citet{Zhu2023Sample}, in which the features of agent's actions, \emph{i.e.}, costs and probabilities over outcomes, depend on an agent's \emph{type} that is sampled at each round from some (fixed) unknown probability distribution.
%
Before formally defining the online learning framework, we introduce some notation that is needed to deal with hidden-action principal-agent problems in which the agent can be of different types.


% we consider hidden-action principal-agent problems in which the features of the agent's actions, \emph{i.e.}, costs and probabilities over outcomes, depend on an agent's \emph{type} that is stochastically determined at each round according to some (fixed) unknown probability distribution.  
%
% $\theta \in \Theta$, which is sampled according to some probability distribution $\lambda \in \Delta_{\Theta}$.


\paragraph{Hidden-action principal-agent problems with types}
%
We let $\Theta$ be the finite set of possible agent's {types}, while $F_{\theta,a}$ and $c_{\theta,a}$ denote the probability distribution over outcomes and the cost, respectively, of action $a \in A$, when the agent has type $\theta \in \Theta$.
%
The agent's type is drawn from an unknown probability distribution $\lambda \in \Delta_\Theta$, with $\lambda_\theta \in [0,1]$ being the probability of type $\theta \in \Theta$.
%
We extend all the notation introduced in Section~\ref{sec:preliminaries} to account for agent's types.
%
Specifically, given $\delta \in (0,1)$, we let $A^{\theta,\delta}(p) \subseteq \mathcal{A}$ be the set of $\delta$-best responses to a contract $p \in \mathbb{R}_+^m$ for type $\theta \in \Theta$.
%
Moreover, we denote by $a^{\theta,\delta}(p)$ the $\delta$-best response that is played by the agent under a $\delta$-robust contract, namely the worst one for the principal.
%
Formally, $a^{\theta,\delta}(p) \in \arg\min_{a \in A^\delta (p)} F_{\theta,a} \cdot (r-p)$.
%
Similarly, we use $A^{\theta}(p)$ and $a^{\theta}(p)$ for (exact) best responses.
%
Finally, the principal's expected utility when committing to a contract $p \in \Rset$ against an agent of type $\theta \in \Theta$ playing an action $a \in \mathcal{A}$ is $u^\sfP(p,a,\theta) \coloneqq F_{\theta, a} \cdot (r - p)$.

We study an online learning framework in which the principal and the agent repeatedly interact over $T > 0$ rounds.
%
Each round involves a repetition of the same instance of hidden-action principal-agent problem, with only the agent's type changing from round to round. 
%
Specifically, at each round $t \in [T]$, the principal-agent interaction is as follows:
%
\begin{enumerate}
	%[noitemsep]
	\item The agent's type $\theta^t \in \Theta$ is sampled from the distribution $\lambda$. Notice that $\theta^t$ is \emph{not} observed by the principal.
	%
	\item The principal commits to a contract $p^t \in \mathcal{C} \coloneqq [0,1]^m$.
	%
	\item After observing the contract $p^t$, the agent plays a $\delta$-best response $a^t\coloneqq a^{\theta^t,\delta}(p^t)$.
	%, which is \emph{not} oserved by the principal. 
	Notice that the action $a^{t}$ is \emph{not} observed by the principal. 
	%
	%action $a^{t}(p^t) \in \A^\delta(p^t)$, which is \emph{not} observed by the principal.
	%
	%Notice that the action $a^{t}(p^t)$ does not necessarily coincide with $a^\delta(p^t)$.
	%
	\item The principal observes the outcome $\omega^t \sim {F}_{a^{t}}$ that is realized as an effect of the agent's action $a^t$.
	%, and they perceive a utility of $u^\sfP(p^t, a^{t})$.
\end{enumerate}
%
We remark that, in the interaction described above, the principal's contract design space is assumed to be limited to the hypercube $\mathcal{C} \coloneqq [0,1]^m$.
%
Restricting the contract space to a bounded set is standard in the literature on online learning in contract design (see, \emph{e.g.},~\cite{ho2015adaptive,Zhu2023Sample,chen2024bounded}) and it is motivated by the negative result proved in Theorem~1 in~\citep{bacchiocchilearning}.
%
Therefore, we need to introduce a formal definition of the principal's expected utility when committing to an optimal $\delta$-robust contract restricted to such a space.
%
Formally:
%
$$
%\textnormal{OPT}(\mathcal{C}) := \max_{p \in \mathcal{C}} u^\sfP(p, a(p)) \quad \textnormal{and} \quad
%
\textnormal{OPT}(\mathcal{C}, \delta) := \max_{p \in \mathcal{C}} \sum_{\theta \in \Theta} \, \lambda_\theta u^\sfP(p, a^{\theta,\delta}(p), \theta).
$$



%The principalâ€™s behavior over the $T$ rounds is modeled by a \emph{policy} $\pi \coloneqq (\pi_t)_{t \in [T]}$, where each $\pi_t$ is a randomized mapping from an history of observations up to round $t-1$, namely ${H}_{t-1} \coloneqq (p^1,\omega^1, \ldots, p^{t-1},\omega^{t-1}) \in \mathcal{H}_{t-1}$, to a contract $p^{t} \in \mathcal{C}$ to be employed at round $t$. 
%
% which is defined, for every $t\in [T]$, as a randomized map from the history of observations up to round $t-1$, \emph{i.e.}, ${H}_{t-1} \coloneqq (p^1,\omega^1, \ldots, p^{t-1},\omega^{t-1}) \in \mathcal{H}_{t-1}$, to a contract $p^{t} \in \mathcal{C}$, where $\mathcal{H}_{t-1}$ is the set comprising all the possible histories up to round $t-1$.
%
%
The goal of the principal is to minimize the \emph{(cumulative) pseudo-regret}, or simply \emph{regret}, which can be defined as:
%
\begin{align*}
	R_T(\mathcal{C},\delta) \coloneqq  T \, \textnormal{OPT}(\mathcal{C},{\delta})-   \mathbb{E} \left[\hspace{-0.2mm}\sum_{t \in [T],\theta \in \Theta} \lambda_\theta u^\sfP(p^t, a^{\theta,\delta}(p^t),\theta)  \right] ,
\end{align*}
%
where the expectation is over the randomness of the learning algorithm.
%to the randomness of the environment that generates the feedback received
%by the learner at each round
%
%From here on, we sometimes omit the dependence on the policy $\pi$ in the regret formulation, referring to $R^{\pi}_T(\mathcal{C}, \delta)$ as $R_T(\mathcal{C}, \delta)$, whenever the policy is clear from the context.
%
Our goal is to design a \emph{no-regret} algorithm for the principal, namely one achieving $R_T(\mathcal{C}, \delta) = o(T)$.

%\albo{Watch out to the $\pi$ here and there.}

%\albo{add types to expectation in the regret definition.}

\subsection{A No-Regret Algorithm}\label{sec:no_regret}

Before introducing our no-regret learning algorithm (Algorithm~\ref{alg:no_regret}), we need to prove a key lemma about the ``continuity'' of the principal's expected utility over the hypercube.
%
In particular, given a $\delta$-robust contract $p \in \Rset$, we show that it is possible to build another contract $p'\in  \Rset$ that is $(\delta + \epsilon)$-robust and provides the principal with utility at most $2\sqrt{\epsilon}$ worse than the one of contract $p$. Formally:
%
\begin{restatable}{lemma}{epsilonconvert}\label{lem:epsilonconvert}
	Given any $\delta,\epsilon \in(0,1)$ and a contract $p \in \Rset$, the following holds for every $\theta \in \Theta$:
	%
	$$ u^\sfP(p', a^{\theta, \delta+\epsilon}(p'),\theta) \ge u^\sfP(p,a^{\theta,\delta} (p),\theta ) - 2 \sqrt{\epsilon},$$
	%
	where $p' \coloneqq (1-\sqrt \epsilon)p + \sqrt \epsilon r$.
\end{restatable}
%
%
%We define $\mathcal{C}:=[0,1]^m$ and we let:
%$$
%\textnormal{OPT}(\mathcal{C}) := \max_{p \in \mathcal{C}} u^\sfP(p, a(p)) \quad \textnormal{and} \quad
%\textnormal{OPT}(\mathcal{C}, \delta) := \max_{p \in \mathcal{C}} u^\sfP(p, a^{\delta}(p)).
%$$
%
%Furthermore, we also define the cumulative regret as follows:
%\begin{align*}
%R_T &\coloneqq T \cdot \textnormal{OPT}(\mathcal{C}) - \mathbb{E}\Big[ \sum_{t \in [T]} u^p(p^t, a^{\delta, t}(p^t)) \Big],\\
%R_T(\delta) &\coloneqq T \cdot \textnormal{OPT}(\mathcal{C}, \delta) - \mathbb{E}\Big[ \sum_{t \in [T]} u^p(p^t, a^{\delta, t}(p^t)) \Big],
%\end{align*}
%
%where the expectation is taken both over the randomness of the algorithm and the stochasticity of the ourcme .
%
%\begin{restatable}{theorem}{NoRegretThmLB}
%	For any algorithm, there exists an instance such that:
%	\begin{equation*}
	%		R_T \ge {\Omega} \left(\sqrt{\delta}T \right).
	%	\end{equation*}
%\end{restatable}
%
%
%
We observe that the idea of shifting payments towards the principal's reward vector, as in Lemma~\ref{lem:epsilonconvert}, was first adopted by~\citet{dutting2021complexity} in non-robust settings, in order to deal with approximately incentive-compatible contracts.

Thanks to Lemma~\ref{lem:epsilonconvert}, it is possible to show that there always exists a contract $p \in \mathcal{B}_{\epsilon}$ that satisfies the following condition, where $\mathcal{B}_{\epsilon} \subseteq \mathcal{C}$ is a uniform grid of the hypercube $\mathcal{C}$, built with step size $\epsilon > 0$ (see also Algorithm~\ref{alg:no_regret}).
%
\[ \sum_{\theta \in \Theta} \lambda_\theta  u^{\textnormal{P}}(p,a^{\theta,\delta}(p),\theta) \ge \textnormal{OPT}(\mathcal{C},\delta) - \mathcal{O}( \sqrt{\epsilon}).\]
%
Consequently, by suitably choosing $\epsilon > 0$ as a function of the time horizon $T$, and by instantiating a no-regret algorithm with set of arms $\mathcal{B}_{\epsilon}$, we can upper bound the regret suffered by Algorithm~\ref{alg:no_regret} as follows.
%
\begin{restatable}{theorem}{NoRegretThm}\label{thm:no_regret}
	The regret suffered by Algorithm~\ref{alg:no_regret} can be upper bounded as follows:
	\begin{align*}
		R_T (\mathcal{C},\delta) \le \mathcal{\widetilde{O}} \left(   T^{1- \frac{1}{2(m+1)}}  \right).
	\end{align*}
\end{restatable}
%
We remark that the regret lower bound introduced by~\citet{Zhu2023Sample} also holds in our setting. 
%
This is because, when the parameter $\delta$ is arbitrarily small, if we consider the same instances used by~\citet{Zhu2023Sample} in their lower bound, the principal is still required to enumerate an exponential number of regions, thus suffering $\Omega(T^{1-1/(m+2)})$ regret.
%
This confirms that an exponential dependence on the number of outcomes is unavoidable in the regret suffered by any algorithm.
%
Furthermore, Theorem~\ref{thm:no_regret} shows regret guarantees similar to those obtained by~\citet{Zhu2023Sample} in the non-robust version of the problem.
%
Indeed, they achieve an upper bound of the order of $\widetilde{\mathcal{O}}(\sqrt{m} \cdot T^{1 - 1/(2m+1)})$ on the regret.
%
%
%
%
\begin{algorithm}[!htp]
	\caption{Regret minimizer for $\delta$-robust contracts}
	\label{alg:no_regret}
	\begin{algorithmic}[1]
		\Require $T >0 $
		\State Set $\epsilon\gets T^{-\frac{1}{m+1}}$
		\State $\mathcal{B}_{\epsilon} \hspace{-0.1mm} \gets \hspace{-0.2mm} \left\{ p \in [0,1]^m \, | \, p_\omega \in \{0,\epsilon, 2\epsilon, ..., 1 \} \, \forall \omega \in \Omega \right\} $ 
		\State Run \texttt{UCB1} with $\mathcal{B}_\epsilon $ as set of arms 
	\end{algorithmic}
\end{algorithm}


Finally, we also show that, when the parameter $\delta \in (0,1)$ is sufficiently small (with respect to the time horizon $T>0$), Algorithm~\ref{alg:no_regret} achieves sublinear regret with respect to an optimal non-robust contract within $\mathcal{C}$.
%
Formally, we define the value of such a contract as follows:
%
$$
\textnormal{OPT}(\mathcal{C}) := \max_{p \in \mathcal{C}}  \sum_{\theta \in \Theta}\lambda_\theta u^\sfP(p, a^\theta(p),\theta).
$$
Furthermore, when $\textnormal{OPT}(\mathcal{C})$ is chosen as baseline, the regret definition becomes the following:
%
%\bac{now $\cdot$ cdot means something else}
\begin{align*}
	%
	R_T(\mathcal{C}) \hspace{-0.5mm}&\coloneqq \hspace{-0.5mm} T \, \textnormal{OPT}(\mathcal{C})\hspace{-0.3mm}-  \hspace{-0.3mm}\hspace{-0.3mm} \mathbb{E}\left[\sum_{t \in [T],\theta \in \Theta} \hspace{-3.2mm}\lambda_\theta u^\sfP(p^t, a^{\theta,\delta}(p^t),\theta) \hspace{-0.3mm} \right] \hspace{-0.5mm},
\end{align*}
%
where the expectation is over the randomness of the learning algorithm.
%
We remark that the regret definition above coincides with the one introduced by~\citet{Zhu2023Sample}.
%
Then, we can prove that the following corollary of Theorem~\ref{thm:no_regret} holds.
%
\begin{restatable}{corollary}{NoRegretCor}\label{cor:no_regret}
	%Let $\mathcal{C} \subseteq [0,1]^m$ be a convex set such that $r \in \mathcal{C}$, then 
	The regret suffered by Algorithm~\ref{alg:no_regret} can be upper bounded as follows:
	\begin{align*}
		R_T (\mathcal{C})  \le \mathcal{\widetilde{O}} \left(   T^{1- \frac{1}{2(m+1)}}  \right) +  2 \sqrt{\delta} T.
	\end{align*}
\end{restatable}
%
We remark that, if we set $\delta = \nicefrac{1}{T^\alpha}$ with $\alpha>0$ in Corollary~\ref{cor:no_regret}, then the regret with respect to an optimal non-robust contract within the hypercube $\mathcal{C}$ is sublinear.
%\albo{Sentence is not finished.}

\begin{remark}
	Our algorithm can be extended to deal with settings in which the agent can play \emph{any} $\delta$-best response within the set $A^{\theta^t,\delta}(p^t)$. In such a setting, the principal's utility is \emph{not} fully stochastic. However, our Algorithm~\ref{alg:no_regret} can be easily extended by instantiating an adversarial no-regret algorithm instead of \texttt{UCB1}.
\end{remark}
%\mat{}
%and it does \emph{not} necessarily coincide with the worst $\delta$-best response for the principal, namely $a^{\theta^t,\delta}(p^t)$.
%We observe that achieving the regret bound presented in Theorem~\ref{thm:no_regret} requires Algorithm~\ref{alg:no_regret} to instantiate an adversarial no-regret algorithm as subprocedure.This is because, when the principal commits to the same contract $p \in \mathcal{C}$, the agent may select different $\delta$-best responses at different rounds $t \in [T]$.
%
%Thus, the principal's expected utility in a contract $p \in \mathcal{C}$ may \emph{not} be a constant independent of $t$.%\footnote{\bac{In the rest of the paper, we assume that the action selected at time $t \in [T]$ by the agent's type $\theta \in \Theta$ in a given contract $p \in \mathbb{R}$ is prescribed by a function $a^{\theta,t}(p)$. This function returns the action that, at time $t \in [T]$, the agent of type $\theta \in \Theta$ would select in a contract $p \in \mathbb{R}$; thus, $a^{\theta,t}(p) \in A^{\theta,\delta}(p)$.}}


\paragraph{Comparison with~\citep{Zhu2023Sample}}
%
We observe that our algorithm provides some advantages compared to the state-of-the-art algorithm proposed by~\citet{Zhu2023Sample}.
%
First, our approach employs as set of contracts a simple discretization of the hypercube, while the approach by~\citet{Zhu2023Sample} requires determining the minimum set of contracts $\mathcal{V}_{\epsilon}(\mathcal{C})$---for some suitable $\epsilon>0$---such that, for every $p \in \mathcal{C}$, there exists a $p' \in \mathcal{V}_{\epsilon}(\mathcal{C})$ that satisfies $(r - p) \cdot p' \ge \cos(\epsilon)$.
%
However, the need of designing such a set of contracts makes the approach by~\citet{Zhu2023Sample} challenging to be employed in practice compared to ours.
%
Second, our algorithm does \emph{not} require \emph{apriori} knowledge of the principal's reward, which is instead required by the algorithm of~\citet{Zhu2023Sample}.
%
% Third, Algorithm~\ref{alg:no_regret} achieves similar perdormances to the one achied by~\cite{Zhu2023Sample} is robust to the agent selecting actions that are possibly $\delta$-suboptimal compared to the actual best responses, this holds when the parameter $\delta>0$ is sufficiently small relative to $T>0$.



%\albo{$r \in \mathcal{C}$?}


% \clearpage
