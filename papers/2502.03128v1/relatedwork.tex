\section{Related Work}
\paragraph{Masked Generative Models for Speech} Masked generative models (MGMs) are a family of generative models that typically employ non-autoregressive transformers~\cite{vaswani2017attention}. These models have achieved significant success, demonstrating performance comparable to or even surpassing autoregressive and diffusion models in image~\cite{chang2022maskgit, chang2023muse, xie2024show} and video~\cite{yu2023magvit, yu2023language} generation, while offering a better balance between quality and speed.
In the speech domain, SoundStorm~\cite{borsos2023soundstorm} uses the semantic tokens from AudioLM~\cite{borsos2023audiolm} and employs MGMs to generate acoustic tokens from a neural audio codec~\cite{zeghidour2021soundstream}, enabling applications like TTS and voice conversion. NaturalSpeech 3~\cite{ju2024naturalspeech} adopts MGMs to generate disentangled speech tokens. MaskGCT~\cite{wang2024maskgct} further leverages MGMs for zero-shot generation, eliminating the need for explicit text-speech alignment or phone-level duration prediction in non-autoregressive TTS models. MaskSR~\cite{li2024masksr} applies MGMs to speech enhancement tasks. In this work, we propose a unified speech generation framework based on MGMs.


\paragraph{Unified Speech Generation} Developing a unified framework capable of handling various tasks is a key research objective in artificial intelligence. In the field of speech generation, UniAudio~\cite{yang2023uniaudio} employs an LLM for next-token prediction to generate multiple types of audio. Similarly, SpeechX~\cite{wang2024speechx} leverages an LLM for unified zero-shot tasks such as TTS, noise suppression, and target speaker extraction. Both models achieve this by concatenating the condition and target speech tokens, followed by AR modeling.
However, these models require large amounts of paired training data for each task, failing to leverage the vast amount of unlabeled speech data effectively. VoiceBox~\cite{le2024voicebox} employs flow matching to unify tasks such as zero-shot TTS, speech editing, and speech enhancement. However, it has notable limitations, such as requiring text and clean speech as references for speech enhancement and relying on phone durations during training for zero-shot TTS. Its successor, AudioBox~\cite{vyas2023audiobox}, extends VoiceBox to unified audio generation with natural language prompt control. Our work is partly inspired by SpeechFlow~\cite{liu2023generative}, which uses flow matching~\cite{lipman2022flow} to learn infilling during pre-training and fine-tunes with task-specific conditions for various speech generation tasks, such as zero-shot TTS and speech separation. However, it is limited to frame-level conditions, such as requiring a frame-level phoneme sequence for TTS. Additionally, predicting mel-spectrograms directly during pre-training may be suboptimal due to the need to predict extensive acoustic details.

Speech discrete representation is also highly relevant to our work, and we have included this part in Appendix~\ref{appendix:speech_rep}.