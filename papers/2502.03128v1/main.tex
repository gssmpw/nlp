
%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{newtxmath}
\usepackage{threeparttable}
\usepackage{colortbl}
\usepackage{tablefootnote}
\usepackage{diagbox}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tikz}

\def\myname{Metis}

% for debug
\newcommand{\blue}[1]{{\color{blue} #1}}
\newcommand{\green}[1]{{\color{green} #1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\orange}[1]{{\color{orange} #1}}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\textit{\myname{}}: A Foundation Speech Generation Model with Masked Generative Pre-training}

\begin{document}

\twocolumn[
\icmltitle{\textit{\myname}: A Foundation Speech Generation Model \\ with Masked Generative Pre-training}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yuancheng Wang}{cuhk}
\icmlauthor{Jiachen Zheng}{cuhk}
\icmlauthor{Junan Zhang}{cuhk}
\icmlauthor{Xueyao Zhang}{cuhk}
\icmlauthor{Huan Liao}{cuhk}
\icmlauthor{Zhizheng Wu}{cuhk}


\end{icmlauthorlist}

\icmlaffiliation{cuhk}{The Chinese University of Hong Kong, Shenzhen}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Speech Generation, Generative Pre-training}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We introduce \textbf{\textit{\myname{}}}, a foundation model for unified speech generation.
Unlike previous task-specific or multi-task models, \myname{} follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks.
Specifically,
1) \myname{} utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms.
2) \myname{} performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition.
3) Through fine-tuning with task-specific conditions, \myname{} achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters.
Experiments demonstrate that \myname{} can serve as a foundation model for unified speech generation: \myname{} outperforms state-of-the-art task-specific or multi-task systems
across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data.
Audio samples are are available at \url{https://metis-demo.github.io/}.
We will release the code and model checkpoints at \url{https://github.com/open-mmlab/Amphion}.
\end{abstract}

\section{Introduction}

% 在导言区添加 \usepackage{tikz}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/speech_gen.png}
        \caption{Two-stage generation.}
        \vspace{-1mm}
        \label{fig:overview}
    \end{subfigure}%
    % 插入虚线（高度自适应图片，默认 2cm）
    \hspace{0.01\textwidth}%
    \raisebox{0.5\height}{\tikz \draw[dashed, line width=0.4pt] (0,0) -- (0,2.5cm);}%
    \hspace{0.01\textwidth}%
    \begin{subfigure}[b]{0.29\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/pre-train.png}
        \caption{Pre-training.}
        \vspace{-1mm}
        \label{fig:pretrain}
    \end{subfigure}%
    % 插入第二条虚线
    \hspace{0.01\textwidth}%
    \raisebox{0.5\height}{\tikz \draw[dashed, line width=0.4pt] (0,0) -- (0,2.5cm);}%
    \hspace{0.01\textwidth}%
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/finetune.png}
        \caption{Fine-tuning.}
        \vspace{-1mm}
        \label{fig:ft}
    \end{subfigure}
    
    \caption{An illustration of Metis. (a) provides an overview of the two-stage speech generation framework, which consists of task-specific (yellow block) and task-independent (light blue block) processes. In this work, we focus on developing a pre-training model for the first stage, as illustrated in (b).  (c) demonstrates the fine-tuning process, where the pre-trained model is adapted to specific tasks.}
    \label{fig:pretrain-ft}
\end{figure*}


Advancing a unified framework capable of addressing diverse tasks is a central research objective within the domain of artificial intelligence. In natural language processing~\cite{radford2019language, devlin2018bert, achiam2023gpt} and computer vision~\cite{he2022masked, bao2021beit, kirillov2023segment}, foundation models leveraging large-scale self-supervised pre-training have demonstrated remarkable adaptability across a wide spectrum of downstream tasks. However, in the domain of speech generation, this potential remains underexplored. A unified speech can integrate various speech generation technologies, such as text-to-speech~\cite{ren2020fastspeech, wang2023neural, shen2023naturalspeech, ju2024naturalspeech, le2024voicebox, wang2024maskgct}, voice conversion~\cite{mohammadi2017overview, qian2019autovc, li2023freevc, choi2023diff}, and speech enhancement~\cite{pascual2017segan, fu2021metricgan+, liu2021voicefixer}. This integration reduces redundant development, facilitates broader applications across diverse domains, and enhances the efficiency of human-machine interaction.

Previous speech generation models are mostly expert models that require extensive task-specific designs~\cite{ren2019fastspeech, ren2020fastspeech, lee2023hierspeech++, wang2018voicefilter, wang2024wesep, wang2023tf, kim2023lip}. UniAudio~\cite{yang2023uniaudio} and SpeechX~\cite{wang2024speechx} are pioneering works that attempt to use autoregressive language models for multiple speech generation tasks. However, \textit{they require a large amount of paired training data for each task and face challenges in extending to new tasks based on pre-trained models}. In addition, the autoregressive approach leads to suboptimal results in certain tasks and is relatively inefficient.

In this paper, we address the research question of \textbf{\textit{how to design a unified speech generation framework that leverages large-scale unlabeled speech data for pre-training and efficiently adapts to diverse speech generation tasks through fine-tuning.}} Inspired by previous two-stage speech generation models~\cite{borsos2023audiolm, kharitonov2023speak, borsos2023soundstorm, anastassiou2024seed, du2024cosyvoice},
we revisit the speech generation process and observe a common structure underlying most tasks: generating intermediate representations from task-specific conditions and synthesizing acoustic representations from these intermediates. The intermediate representations are typically discrete units quantized from self-supervised speech features~\cite{hsu2021hubert, chung2021w2v, zhang2023google, chiu2022self, chen2022wavlm} that encode semantic and prosodic information, while acoustic representations are easily used to reconstruct high-quality waveforms.

Building on this observation, we propose a unified framework that modularizes speech generation tasks as a two-step process, as illustrated in Figure~\ref{fig:overview}: 1) \textbf{Task-Specific Process}: Generating SSL tokens conditioned on task-specific conditions (e.g., text for text-to-speech, noisy speech for speech enhancement, or visual features for lip-to-speech). 2) \textbf{Task-Independent Process}: Generating acoustic representations from the SSL tokens. This formulation allows diverse tasks to be addressed by varying input conditions while maintaining a consistent generation mechanism. Previous studies~\cite{borsos2023audiolm, borsos2023soundstorm, wang2024maskgct, anastassiou2024seed, du2024cosyvoice, guo2024fireredtts, liu2024audioldm, liu2024semanticodec} have established that the second stage of acoustic refinement can be achieved effectively using fully self-supervised diffusion models~\cite{anastassiou2024seed, du2024cosyvoice, guo2024fireredtts, liu2024audioldm, liu2024semanticodec} or language models~\cite{borsos2023audiolm, borsos2023soundstorm, wang2024maskgct}.
Therefore, \textit{our objective is simplified to designing a generative pre-training mechanism for the first stage, enabling the pre-trained model to effectively generalize across various speech generation tasks}. In this work, we adopt masked generative models~\cite{chang2022maskgit} for unconditional pre-training. Masked generative models are trained using a straightforward masked token prediction approach and employ an iterative sampling strategy to generate outputs from a fully masked input. We provide more details about masked generative models in Section~\ref{method:masked}. Once pre-trained, the model can be fine-tuned with limited data and parameters by incorporating task-specific conditions as additional inputs to adapt to various speech generation tasks.

Based on the above discussion, we propose \textit{\textbf{\myname{}}}, a foundation model for unified speech generation. Specifically, Metis has the following key features:

\begin{itemize}

    \item \textbf{Masked Generative Pre-Training:} Metis performs masked generative pre-training on SSL tokens using large-scale unlabeled speech data without any task-specific condition, as illustrated in Figure~\ref{fig:pretrain}. This pre-training phase establishes a strong foundation, allowing Metis to efficiently adapt to various downstream tasks through fine-tuning with minimal task-specific data.

    \item \textbf{Efficient Adaption to Various Speech Generation Tasks:} Metis can be efficiently adapted to a wide range of speech generation tasks, including zero-shot text-to-speech, voice conversion, speech enhancement, target speaker extraction, and lip-to-speech, as illustrated in Figure~\ref{fig:ft}. The fine-tuned models achieve state-of-the-art results, even when using fewer than 20M trainable parameters or significantly less training data.

    \item \textbf{Support for Multimodal Conditional Inputs:} Metis supports multimodal conditional inputs during the fine-tuning phase, including text, audio, and video. This capability solidifies \myname{} as a foundation model for supporting various speech generation tasks.
    We also explore multi-task fine-tuning, showing that the pre-trained model can be efficiently adapted into a powerful multi-task model with minimal modification, enabling novel applications such as text-guided target speaker extraction with task combinations.

\end{itemize}

\section{Related Work}

\paragraph{Masked Generative Models for Speech} Masked generative models (MGMs) are a family of generative models that typically employ non-autoregressive transformers~\cite{vaswani2017attention}. These models have achieved significant success, demonstrating performance comparable to or even surpassing autoregressive and diffusion models in image~\cite{chang2022maskgit, chang2023muse, xie2024show} and video~\cite{yu2023magvit, yu2023language} generation, while offering a better balance between quality and speed.
In the speech domain, SoundStorm~\cite{borsos2023soundstorm} uses the semantic tokens from AudioLM~\cite{borsos2023audiolm} and employs MGMs to generate acoustic tokens from a neural audio codec~\cite{zeghidour2021soundstream}, enabling applications like TTS and voice conversion. NaturalSpeech 3~\cite{ju2024naturalspeech} adopts MGMs to generate disentangled speech tokens. MaskGCT~\cite{wang2024maskgct} further leverages MGMs for zero-shot generation, eliminating the need for explicit text-speech alignment or phone-level duration prediction in non-autoregressive TTS models. MaskSR~\cite{li2024masksr} applies MGMs to speech enhancement tasks. In this work, we propose a unified speech generation framework based on MGMs.


\paragraph{Unified Speech Generation} Developing a unified framework capable of handling various tasks is a key research objective in artificial intelligence. In the field of speech generation, UniAudio~\cite{yang2023uniaudio} employs an LLM for next-token prediction to generate multiple types of audio. Similarly, SpeechX~\cite{wang2024speechx} leverages an LLM for unified zero-shot tasks such as TTS, noise suppression, and target speaker extraction. Both models achieve this by concatenating the condition and target speech tokens, followed by AR modeling.
However, these models require large amounts of paired training data for each task, failing to leverage the vast amount of unlabeled speech data effectively. VoiceBox~\cite{le2024voicebox} employs flow matching to unify tasks such as zero-shot TTS, speech editing, and speech enhancement. However, it has notable limitations, such as requiring text and clean speech as references for speech enhancement and relying on phone durations during training for zero-shot TTS. Its successor, AudioBox~\cite{vyas2023audiobox}, extends VoiceBox to unified audio generation with natural language prompt control. Our work is partly inspired by SpeechFlow~\cite{liu2023generative}, which uses flow matching~\cite{lipman2022flow} to learn infilling during pre-training and fine-tunes with task-specific conditions for various speech generation tasks, such as zero-shot TTS and speech separation. However, it is limited to frame-level conditions, such as requiring a frame-level phoneme sequence for TTS. Additionally, predicting mel-spectrograms directly during pre-training may be suboptimal due to the need to predict extensive acoustic details.

Speech discrete representation is also highly relevant to our work, and we have included this part in Appendix~\ref{appendix:speech_rep}.


\section{Method}

% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[b]{0.52\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/speech_gen.png}
%         \caption{Speech Generation.}
%         \vspace{-1mm}
%         \label{fig:gen}
%     \end{subfigure}
%     \hspace{50pt}
%     \begin{subfigure}[b]{0.27\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/speech_tokenizer.png}
%         \caption{Speech Representation.}
%         \vspace{-1mm}
%         \label{fig:tokenizer}
%     \end{subfigure}
%     \caption{An illustration of the two-stage speech generation process (Fig. (a)) and speech representation learning (Fig. (b)). The unified speech generation framework first generates SSL tokens from any input conditions with the task-specific module, and then passes the SSL tokens to the task-independent module to generate acoustic tokens for high-quality audio. The SSL tokens and acoustic tokens are from self-supervised speech features and quantization from waveforms, respectively as illustrated in Fig. (b).}
%     \vspace{-5mm}
%     \label{fig:tokenizer-gen}
% \end{figure*}


\subsection{Background: Masked Generative Models}
\label{method:masked}

In this section, we provide a brief introduction to masked generative models~\cite{chang2022maskgit, yu2023language, wang2024maskgct}. Consider a discrete sequence $\boldsymbol{x} = [y_1, y_2, \ldots, y_n]$, where $n$ denotes the length of the sequence. We define $\boldsymbol{x}_t = \boldsymbol{x} \odot \boldsymbol{m}_t$ as the operation of masking a subset of tokens in $\boldsymbol{x}$ using the corresponding binary mask $\boldsymbol{m}_t = [m_{t,1}, m_{t,2}, \ldots, m_{t,n}]$. Specifically, this operation involves replacing $x_i$ with a special $\texttt{[MASK]}$ token if $m_{t,i}=1$, and otherwise leaving $x_i$ unmasked if $m_{t,i}=0$. Here, each $m_{t,i}$ is independently and identically distributed according to a Bernoulli distribution with parameter $\gamma(t)$, where $\gamma(t) \in (0,1]$ represents a mask schedule function (for example, $\gamma(t) = \sin(\frac{\pi t}{2T}), t \in (0,T]$). We denote $\boldsymbol{x} = \boldsymbol{x}_0$. The masked generative models are trained to predict the complete sequence (masked tokens) based on the observed tokens (unmasked tokens) and the condition $\boldsymbol{c}$, which can be modeled as \( p_\theta(\boldsymbol{x}_0 \mid \boldsymbol{x}_t, \boldsymbol{c}) \), and the model parameters \(\theta\) are trained to optimize the sum of the marginal cross-entropy for each unobserved token:
\begin{equation}
    \begin{aligned}
        \mathcal{L}_{\text{mask}} &= - \mathbb{E}_{\boldsymbol{x}, t, \boldsymbol{m}_t} \sum_{i=1}^{n} m_{t,i} \cdot \log p_{\theta}(y_i \mid \boldsymbol{x}_t, \boldsymbol{c})
    \end{aligned}
\end{equation}
Note that $\boldsymbol{c}$ may be empty, for example, during the unconditional pre-training stage of our model. At the inference stage, masked generative models generate tokens in parallel through iterative decoding. The process begins with a fully masked sequence $\boldsymbol{x}_T$. Assuming the total number of decoding steps is $S$, for each step $j$ from $1$ to $S$, we first sample $\boldsymbol{\hat{x}}_0$ from $p_{\theta}(\boldsymbol{x}_0 \mid \boldsymbol{x}_{T - (j-1) \cdot \frac{T}{S}}, \boldsymbol{c})$. Then we sample $\lfloor n \cdot \gamma(T - j \cdot \frac{T}{S}) \rfloor$ tokens based on the confidence score to remask, resulting in $\boldsymbol{x}_{T - j \cdot \frac{T}{S}}$, where $n$ is the sequence length of $\boldsymbol{x}$. The confidence score for $\hat{y}_i$ in $\boldsymbol{\hat{x}}_0$ is assigned to $p_{\theta}(y_i \mid \boldsymbol{x}_{T - (j-1) \cdot \frac{T}{S}}, \boldsymbol{c})$ if $y_{T - (j-1) \cdot \frac{T}{S}, i}$ is a $\texttt{[MASK]}$ token; otherwise, we set the confidence score of $\hat{y}_i$ to 1, indicating that tokens already unmasked in $\boldsymbol{x}_{T - (j-1) \cdot \frac{T}{S}}$ will not be remasked. In particular, we choose $\lfloor n \cdot \gamma(T - j \cdot \frac{T}{S}) \rfloor$ tokens with the lowest confidence scores to be masked. Note that the method for calculating the confidence score is not unique. For example, \citet{lezama2022improved} introduces Token-Critic, training a critic model to compute confidence scores, aiding the sampling process. Additionally, \citet{lezama2022improved, xie2024show} suggest that masked generative modeling can be seen as a simplified version of discrete diffusion.

\subsection{Overview of Metis}

Figure~\ref{fig:pretrain-ft} provides an overview of our system, which adopts a two-stage speech generation process for unified speech generation tasks. We first briefly introduce the distinct speech discrete representations used in the two stages in Section~\ref{sec:codec}. Then, we present the pre-training (Section~\ref{sec:pretrain}) and fine-tuning (Section~\ref{sec:fine-tune}) strategies for the first stage, which form the core of our work. Finally, we provide a brief overview of a unified acoustic decoder for all speech generation tasks in Section~\ref{sec:stage-2}.

\subsection{Discrete Representations for Two-Stage Generation}
\label{sec:codec}

\myname{} employs two discrete speech representations for the two-stage generation, as illustrated in Figure~\ref{fig:tokenizer}.
1) \textbf{SSL tokens}: Derived from SSL features of large-scale speech self-supervised learning models~\cite{chung2021w2v, hsu2021hubert, chen2022wavlm, chiu2022self}, SSL tokens encapsulate both semantic and prosodic information, making them well-suited for conditional generation. To minimize information loss, we employ a vector quantization (VQ) model~\cite{van2017neural, esser2021taming} to quantize SSL features into discrete tokens, following~\citet{wang2024maskgct}.
2) \textbf{Acoustic tokens}: Directly obtained from the waveform via vector quantization. The goal is to preserve all the information from the speech to reconstruct a high-quality waveform.
We show more details in Appendix~\ref{appendix:codec}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.72\columnwidth]         {figs/speech_tokenizer.png}
    \caption{Two discrete speech representations for the two-stage speech generation: SSL tokens (left) and acoustic tokens (right).}
    \vspace{-5mm}
    \label{fig:tokenizer}
\end{figure}


\subsection{Masked Generative Pre-training with SSL Tokens}
\label{sec:pretrain}

Based on the previous discussion, most speech generation tasks can be generalized into two stages: \textit{conditions to SSL tokens} and \textit{SSL tokens to acoustic tokens}. The primary distinction among tasks lies in the nature of the conditions. To address this, we propose a unified pre-trained model for the first stage, which can be adapted to various tasks.

We use an unconditional masked generative model on SSL tokens for pre-training. Specifically, we randomly mask tokens in the SSL token sequence \( \boldsymbol{x}^{ssl} \) using the strategy outlined in Section~\ref{method:masked} and predict the masked tokens. We introduce a prompt sequence with the probability \( p \) to further enhance the in-context learning ability of the model. With this probability, a prefix sequence \( \boldsymbol{x}^{ssl}_{prompt} \) from the SSL token sequence is used as a prompt and remains unmasked. This mechanism enables the model to leverage prompt information, thereby improving its adaptability to downstream tasks requiring prompts, such as zero-shot TTS and target speaker extraction. The pre-training objective is to model \( p_\theta (\boldsymbol{x}_0^{ssl} \mid \boldsymbol{x}_t^{ssl}, \boldsymbol{x}_{prompt}^{ssl}) \).
% The overview of the pre-training stage is shown in Figure~\ref{fig:pretrain}.

Our design is motivated by the observation that models trained on extensive data can recover masked SSL tokens from prompts and unmasked tokens, even without task-specific conditions, as shown in other domains~\cite{he2022masked, devlin2018bert}. Empirically, the pre-trained model can generate speech that mimics the prosodic style and timbre of a prompt. However, the generated speech often lacks intelligibility, producing random word concatenations due to the absence of semantic guidance. This highlights the need for task-specific conditions, which can be efficiently incorporated through fine-tuning to adapt the model to various speech generation tasks.

\subsection{Efficient Adaptation to Various Generation Tasks}
\label{sec:fine-tune}

Now, we describe how to efficiently adapt the pre-trained model to various speech generation tasks. We first categorize the conditions for different speech generation tasks into two types: non-frame-level conditions and frame-level conditions. For the former, such as TTS, the condition is a phoneme sequence or a text token sequence. In this case, the model needs to implicitly learn the alignment between the condition and the SSL token sequence. For the latter, such as voice conversion or speech enhancement, the conditions (e.g., the source speech voice conversion or the noisy speech for speech enhancement) can be aligned with the target SSL token sequence at the frame level.
Based on these distinctions, during fine-tuning, for non-frame-level conditions, we simply concatenate the condition with the input sequence along the time dimension.
For frame-level conditions, we apply a simple interpolation to align the condition with the input sequence in the time dimension and then pass it through an MLP-based adapter before adding it to the input. 
% \red{The MLP-based adapter enables the Metis to take multi-modality input (e.g. text, audio, video).}
Then, the fine-tuned model is trained to learn $p_\theta (\boldsymbol{x}_0^{ssl} \mid \boldsymbol{x}_t^{ssl}, \boldsymbol{x}_{prompt}^{ssl}, \boldsymbol{c})$, where \( \boldsymbol{c} \) is the task-specific condition.
% By employing this simple approach, we incorporate task-specific conditions as additional model inputs for fine-tuning the model.
Our experiments demonstrate that the pre-trained model can efficiently adapt to various tasks and achieve remarkable results with minimal data. Additionally, we explored the use of Low-Rank Adaptation (LoRA)~\cite{hu2021lora}, which allows fine-tuning with a small number of trainable parameters. More details are provided in Section~\ref{sec:exp}.



\subsection{Masked Generative Acoustic Decoder}
\label{sec:stage-2}
We train an SSL-to-acoustic model based on masked generative modeling, which serves as a unified acoustic decoder for all speech generation tasks. The model is trained to recover masked tokens from a masked acoustic token sequence \( \boldsymbol{x_t}^a \), conditioned on SSL tokens \( \boldsymbol{x}_{ssl} \) and prompt acoustic tokens \( \boldsymbol{x}_{prompt}^a \). This can be formulated as \( p_\theta (\boldsymbol{x}_0^a \mid \boldsymbol{x}_t^a, \boldsymbol{x}_{prompt}^a, \boldsymbol{x}_{ssl}) \). 
During training, we randomly select a layer for masking in the multi-layered acoustic tokens, while the lower-layer tokens remain unmasked and serve as conditional inputs to the model, following~\cite{borsos2023soundstorm}. During inference, we generate acoustic tokens layer by layer. Finally, we generate the speech by first predicting SSL tokens and then generating acoustic tokens.

\section{Experiments and Results}
\label{sec:exp} 

\subsection{Setup}

\paragraph{Model Architecture} We adopt the same model architecture as described in \citet{wang2024maskgct}, except removing the text embedding during the pre-training phase. The model follows the standard Llama-style architecture~\cite{touvron2023llama, vaswani2017attention}, but replaces causal attention with bidirectional attention.

\paragraph{Dataset} We use a dataset consisting of 300K hours of speech for pre-training, including 100K hours from the Emilia~\cite{he2024emilia} dataset and an additional 200K hours self-collected through the Emilia pipeline, to train our pre-trained model. The dataset contains a diverse range of in-the-wild multilingual speech data, including 87K hours in Chinese, 185K hours in English, 7K hours in German, 8K hours in French, 2.5K hours in Japanese, and 7.5K hours in Korean. The dataset used for fine-tuning is sampled from the pre-trained dataset. 

\paragraph{Training} We pre-train our model on 8 GPUs for a total of 1200K steps. We use the AdamW \cite{loshchilov2017decoupled} optimizer with a learning rate of 1e-4 and 32K warmup steps.
We employ a dynamic batch size, where each batch contains 10K tokens (200 seconds) per GPU.
During training, we randomly select a prefix of the sequence as a prompt that is not masked with a probability \( p = 0.8 \). The length of the prompt is uniformly sampled from the range \( [0\%, 40\%] \) of the total sequence length.

\paragraph{Inference} We follow the inference strategy outlined in~\citet{wang2024maskgct}, with task-specific step adjustments for the fine-tuned model to generate SSL tokens. Additionally, we use classifier-free guidance~\cite{ho2022classifier} for tasks that involve prompts.

\paragraph{Evaluation Metrics} We use multiple evaluation metrics to assess different aspects of the generated speech, including similarity (SIM), intelligibility (WER), and audio quality (DNSMOS, SIG, BAK, OVRL, NISQA).
For the subjective metrics, quality mean option score (CMOS) and similarity mean option score
(SMOS) are used to evaluate speech quality and similarity.
The results of the subjective evaluations are presented in Appendix~\ref{appendix:res_sub}.
Details about these metrics are provided in Appendix~\ref{appendix:evaluation_metrics}.

\subsection{Results on Different Speech Generation Tasks}
\label{sec:ft}

We present the fine-tuning results of \myname{} on various speech generation tasks. These tasks include zero-shot TTS (without frame-level phoneme condition or duration prediction), voice conversion, speech enhancement, target speaker extraction, and the multimodal task of lip-to-speech.

\subsubsection{Zero-Shot TTS}

\paragraph{Implementation Details}
For zero-shot TTS, we fine-tune the pre-trained model on three different datasets: 1K hours and 10K hours randomly sampled from the Emilia dataset, and the LibriTTS~\cite{48008} dataset, which contains 0.58K hours of English speech data. We use two fine-tuning methods: full-scale fine-tuning and LoRA fine-tuning. For LoRA fine-tuning, we set the rank \( r = 32 \), resulting in only \textbf{32M trainable parameters}, including the newly added text embedding module. All fine-tuned models are trained using 4 GPUs for 200K steps.

\paragraph{Evaluation and Baseline} We evaluate our zero-shot TTS models using three test sets: 1) SeedTTS \textit{test-en}, a test set introduced in Seed-TTS \cite{anastassiou2024seed} comprising 1,000 samples extracted from English public corpora, including the Common Voice dataset \cite{ardila2019common}. 2) SeedTTS \textit{test-zh}, a test set introduced in Seed-TTS consisting of 2,000 samples extracted from Chinese public corpora, including the DiDiSpeech dataset \cite{guo2021didispeech}. 3) LibriSpeech \textit{test-clean} \cite{panayotov2015librispeech}, a widely used test set for TTS.
We compare our models with several recent zero-shot TTS models, including AR-based models: VALL-E~\cite{wang2023neural}, VoiceCraft~\cite{peng2024voicecraft}, XTTS-v2~\cite{casanova2024xtts}, and CosyVoice~\cite{du2024cosyvoice}; NAR-based models: VoiceBox~\cite{le2024voicebox}, and MaskGCT~\cite{wang2024maskgct}. Specifically, MaskGCT can be seen as Metis without pre-training.
For VoiceBox and NaturalSpeech 3, we only compare with them on LibriSpeech \textit{test-clean} since they are not open-source. See more details about these baselines in Appendix~\ref{appendix:baseline_tts}.

\paragraph{Result}
The results are presented in Table~\ref{tab:results-tts}, with additional results for LibriSpeech \textit{test-clean} set provided in Appendix~\ref{appendix:res_librispeech}.
The table shows that Metis-TTS {\scriptsize LoRA 32}, fine-tuned with only 1K hours of data and 32M trainable parameters, achieves performance on metrics such as WER, SIM, and DNSMOS that is comparable to or exceeds that of some baselines trained on datasets 10 to 100 times larger.
Furthermore, when scaled with full fine-tuning on 10K hours of data, Metis demonstrates an improved WER on SeedTTS \textit{test-en}, outperforming MaskGCT, which was trained on 100K hours of data, while maintaining comparable WER performance on SeedTTS \textit{test-zh}. When fine-tuned with only 580 hours of LibriTTS data, Metis achieves competitive WER and SIM performance, surpassing all baselines except MaskGCT on SeedTTS \textit{test-en}.
We also compare with models that are not pre-trained but trained for the same number of steps. The results show that the fine-tuned models converge faster and achieve better performance.
In addition, all fine-tuned models exhibit excellent SIM and DNSMOS, indicating high speech similarity and quality.

\begin{table}[ht]
\caption{Results on zero-shot TTS task.}
\label{tab:results-tts}
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|cccc}
    \toprule
    \textbf{Model} & \textbf{Training Data} & \textbf{WER}($\downarrow$) & \textbf{SIM}($\uparrow$) & \textbf{DNSMOS}($\uparrow$) \\
    \midrule
    \multicolumn{5}{c}{\textit{\textbf{SeedTTS test-en}}} \\
    \midrule
    Ground Truth & - & 2.14 & 0.73 & 3.53  \\
    \midrule
    VALL-E~\cite{wang2023neural} & 45K EN & 6.13 & 0.43 & 3.39 \\
    VoiceCraft~\cite{peng2024voicecraft} & 9K EN & 7.55 & 0.47 & 3.37 \\
    CosyVoice~\cite{du2024cosyvoice} & 170K Multi. & 4.08 & 0.64 & 3.64 \\
    XTTS-v2~\cite{casanova2024xtts} & 27K Multi. & 3.25 & 0.46 & 3.45 \\
    \midrule
    MaskGCT~\cite{wang2024maskgct} & 100K Multi. & 2.47 & 0.72 & 3.51 \\
    \midrule
    \myname{}-TTS \scriptsize{LoRA 32} & 1K Multi. & 4.78 & 0.72 & 3.47\\
    \myname{}-TTS \scriptsize{LoRA 32} & 10K Multi. & \textbf{4.55} & 0.72 & 3.45 \\
    \midrule
    \myname{}-TTS \scriptsize{LoRA 32} & 0.58K$^1$ EN. & 4.63 & 0.70 & 3.51\\
    \myname{}-TTS \scriptsize{fine-tune} & 0.58K$^1$ EN. & \textbf{3.04} & 0.68 & 3.47 \\
    \midrule
    \myname{}-TTS \scriptsize{fine-tune} & 1K Multi. & 3.86 & 0.71 & 3.46 \\
    \myname{}-TTS \scriptsize{fine-tune} & 10K Multi. & \textbf{2.28}  & 0.72 & 3.47 \\
    \myname{}-TTS \scriptsize{w.o. pre-train} & 10K Multi. & 4.91  & 0.69 & 3.42 \\
    \midrule
    \multicolumn{5}{c}{\textit{\textbf{SeedTTS test-zh}}} \\
    \midrule
    Ground Truth & - & 1.25 & 0.75 & 3.51 \\
    \midrule
    CosyVoice~\cite{du2024cosyvoice} & 170K Multi. & 4.09 & 0.75 & 3.71 \\
    XTTS-v2~\cite{casanova2024xtts} & 27K Multi. & 2.88 & 0.63 & 3.44 \\
    \midrule
    MaskGCT~\cite{wang2024maskgct} & 100K Multi. & 2.18 & 0.77 & 3.58 \\
    \midrule
    \myname{}-TTS \scriptsize{LoRA 32} & 1K Multi. & 5.21 & 0.77 & 3.57 \\
    \myname{}-TTS \scriptsize{LoRA 32} & 10K Multi. & \textbf{4.48}  & 0.77 & 3.55 \\
    \midrule
    \myname{}-TTS \scriptsize{fine-tune} & 1K Multi. & 4.23 & 0.77 & 3.54 \\
    \myname{}-TTS \scriptsize{fine-tune} & 10K Multi. & \textbf{2.30} & 0.77 & 3.55 \\
    \myname{}-TTS \scriptsize{w.o. pre-train} & 10K Multi. & 4.98  & 0.73 & 3.51 \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[1] This version of the model is trained on LibriTTS \cite{48008}.}
    \end{tablenotes}
    \vspace{-5mm}
\end{threeparttable}
}
\end{table}


\subsubsection{Voice Conversion}


\paragraph{Implementation Details}
Previous voice conversion systems typically extract timbre-independent features from input signals using methods such as information bottlenecks~\cite{qian2019autovc, chen2023streaming, jia2022zero}, timbre perturbation~\cite{li2023freevc, wang2023lm, anastassiou2024voiceshop}, or specialized loss functions~\cite{ju2024naturalspeech, jia2022zero}. However, these approaches often introduce complexity, risk the loss of semantic information, and may still inadvertently retain timbre-related details~\cite{lajszczak2024base, baas2023voice}.
In this work, we use a simpler yet effective solution inspired by~\citet{anastassiou2024seed, neekhara2023selfvc}. Specifically, we leverage a lightweight voice conversion model\footnote{\url{https://github.com/myshell-ai/OpenVoice}}~\cite{qin2023openvoice} to perform real-time voice conversion on the target speech using a randomly sampled prompt speech, thereby achieving timbre perturbation. The perturbed speech features are then used as input to predict the target speech based on the prompt.
Specifically, we employ the \texttt{w2v-bert-2.0} features of the perturbed speech as conditioning inputs and randomly extract a prefix of the target speech as the prompt.
Our experiments demonstrate that with minimal data and training steps, our pre-trained model can effectively and rapidly adapt to the voice conversion task.
We use both full-scale fine-tuning and LoRA fine-tuning. For LoRA fine-tuning, we set the rank \( r = 16 \), resulting in only \textbf{9M trainable parameters} excluding those of the MLP-based adapter. Additionally, we observe that for the voice conversion task, our pre-trained model converges on a single A100 GPU after just 10K steps of LoRA fine-tuning and \textbf{5K steps} of full-scale fine-tuning, using randomly sampled \textbf{0.4K hours} of training data.

\paragraph{Evaluation and Baseline}
We evaluate our models on the VCTK~\cite{veaux2017cstr} dataset. we randomly select 200 samples from the dataset as source speech, and for each sample, we randomly select another sample from the same speaker as the prompt speech. We also evaluate the models on other test sets, the results are shown in Appendix~\ref{appendix:res_seedvc}.
We compare our models with several recent voice conversion models, including HireSpeech++~\cite{lee2023hierspeech++}, LM-VC~\cite{wang2023lm}, UniAudio~\cite{yang2023uniaudio}, and Vevo~\cite{vevo}. More details about these baselines can be found in Appendix~\ref{appendix:baseline_vc}.


\paragraph{Result} The results are presented in Table~\ref{tab:results-vc}. The table shows 1) For similarity, our model outperforms all baseline systems, achieving a SIM score of 0.55, significantly higher than the best baseline result of 0.38. 2) For WER, \myname{}-VC {\scriptsize LoRA 16, $\text{cfg} = 0.0$} achieves a competitive result of 4.49, second only to Vevo. 3) For audio quality, as measured by DNSMOS and NISQA, our model shows competitive performance, with slight improvements over most baseline systems. In addition, our model requires only a significantly small amount of data to fine-tune for voice conversion.

\begin{table}
\caption{Results on voice conversion task.}
\label{tab:results-vc}
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|c|cccc}
    \toprule
    \textbf{Model} & \makecell{\textbf{Training} \\ \textbf{Data}} & \textbf{WER}($\downarrow$) & \textbf{SIM}($\uparrow$) & \textbf{DNSMOS}($\uparrow$) & \textbf{NISQA}($\uparrow$) \\
    \midrule
    \multicolumn{6}{c}{\textit{\textbf{VCTK}}} \\
    \midrule
    HierSpeech++~\cite{lee2023hierspeech++} & 2.8K & 4.87 & 0.38 & 3.40 & 3.79 \\
    LM-VC~\cite{wang2023lm} & 1.4K & 8.35 & 0.29 & 3.46 & 3.93 \\
    UniAudio~\cite{yang2023uniaudio} & 60K & 9.00 & 0.25 & 3.47 & 4.28 \\
    Vevo~\cite{vevo} & 60K & \textbf{3.48} & 0.38 & 3.47 & 4.30 \\
    \midrule
    \myname{}-VC \scriptsize{LoRA 16, $\text{cfg} = 0.0$} & 0.4K & \underline{4.49}  & \underline{0.50} & \underline{3.48} & \underline{4.46} \\
    \myname{}-VC \scriptsize{LoRA 16, $\text{cfg} = 2.0$} & 0.4K & 7.90 & \textbf{0.55} & 3.46 & 4.42 \\
    \midrule
    \myname{}-VC \scriptsize{fine-tune, $\text{cfg} = 0.0$} & 0.4K & 6.65 & 0.48 & \textbf{3.49} & \textbf{4.47} \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[1] For LoRA 16, we train on one A100 GPU for 10K steps. For fine-tuning, we train on one A100 GPU for 5K steps.}
        \footnotesize{\item[2] The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.}
    \end{tablenotes}
\end{threeparttable}
}
\vspace{-5mm}
\end{table}

\subsubsection{Target Speaker Extraction}

\paragraph{Implementation Details}  
For target speaker extraction, we randomly sample 10K hours of data from the pre-training dataset to create the fine-tuning training set without any filtering. During training, samples are dynamically mixed to simulate multi-speaker speech data. Specifically, a random prefix from a sample is extracted as the prompt, and the remaining portion is mixed with another randomly selected sample by directly adding their waveforms. The \texttt{w2v-bert-2.0} features of the mixed speech are then used as conditions. We use two fine-tuning methods: full-
scale fine-tuning and LoRA fine-tuning. For LoRA fine-tuning, we explore different LoRA ranks $r = 4, 16, 32$, resulting in LoRA modules with trainable parameters of 2M, 9M, and 18M, respectively.

\vspace{-3mm}
\paragraph{Evaluation and Baseline} We evaluate our models on the LibriMix \cite{cosentino2020librimix} test set. LibriMix is used exclusively for evaluation and is not included in the training set, thus highlighting the generalization capabilities of our models. In addition, we construct a test set of 1K samples from the Emilia dataset, which features greater diversity in both speakers and acoustic environments. We denote it by EmiliaMix test. We compare our models with several open-source expert models for target speaker extraction: WeSep \cite{wang2024wesep}, VoiceFilter \cite{wang2018voicefilter}, and TSELM \cite{tang2024tselm}. We also compare with UniAudio~\cite{yang2023uniaudio}. See more details about these baselines in Appendix~\ref{appendix:baseline_tse}.
\vspace{-3mm}

\paragraph{Result} The results are presented in Table~\ref{tab:results-tse}. The Table shows: 1) All fine-tuned models exhibit similar performance on audio quality metrics, while the WER decreases as the number of trainable parameters increases. 2) Our models demonstrate significant improvements across two test sets in audio quality metrics, DNSMOS and NISQA, compared to the baselines. \myname{}-TSE {\scriptsize LoRA 16} achieves state-of-the-art NISQA scores of 4.41 on LibriMix test and 4.46 on EmiliaMix test, surpassing the ground truth scores of 4.11 and 3.83, respectively. 3) \myname{}-TSE {\scriptsize fine-tune} achieves a competitive WER, although it is slightly lower than WeSep.

\begin{table}[ht]
\caption{Results on target speaker extraction task.}
\label{tab:results-tse}
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|cccccc}
    \toprule
    \textbf{Model} & \textbf{WER}($\downarrow$) & \textbf{SIG}($\uparrow$) & \textbf{BAK}($\uparrow$) & \textbf{OVRL}($\uparrow$) & \textbf{NISQA}($\uparrow$) & \textbf{SIM}($\uparrow$) \\
    \midrule
    \multicolumn{7}{c}{\textit{\textbf{LibriMix test}}} \\
    \midrule
    Ground Truth & 4.27 & 3.62 & 4.03 & 3.32 & 4.11 & 0.76 \\
    \midrule
    UniAudio~\cite{yang2023uniaudio} & 20.08 & 3.64 & \textbf{4.15} & 3.33 & 4.32 & 0.66 \\
    VoiceFilter~\cite{wang2018voicefilter} & 20.10 & 3.27 & 3.77 & 2.91 & 2.97 & 0.68 \\
    WeSep~\cite{wang2024wesep} &  \textbf{6.19} & 3.56 & 3.93 & 3.23 & 4.04 & 0.73 \\
    TSELM~\cite{tang2024tselm}  &  9.20 & 3.55 & 4.08 & 3.29 & 4.03 & 0.27 \\
    \midrule
    \myname{}-TSE \scriptsize{LoRA 4}  & 13.57 & \textbf{3.66}& \underline{4.02}& \underline{3.34}& 4.40& \textbf{0.75} \\
    \myname{}-TSE \scriptsize{LoRA 16}  &12.52 & \textbf{3.66}& \underline{4.02}& \textbf{3.35}& \textbf{4.41}& \textbf{0.75} \\
    \myname{}-TSE \scriptsize{LoRA 32}  & 9.65 & \textbf{3.66} & \underline{4.02} & \textbf{3.35} & \underline{4.38} & \textbf{0.75} \\
    \myname{}-TSE \scriptsize{fine-tune}  & \underline{6.31} & \underline{3.65} & \underline{4.02} & \underline{3.34} & 4.36 & \underline{0.74} \\
    \midrule
    \multicolumn{7}{c}{\textit{\textbf{EmiliaMix test}}} \\
    \midrule
    Ground Truth & 0.00 & 3.57 & 4.01 & 3.27 & 3.83 & 0.86 \\
    \midrule
    UniAudio~\cite{yang2023uniaudio} & 32.51 & 3.55 & \underline{4.03} & \underline{3.20} & 4.01 & 0.55 \\
    VoiceFilter~\cite{wang2018voicefilter} & 24.10 & 3.21 & 3.64 & 2.78 & 2.15 & 0.71 \\
    WeSep~\cite{wang2024wesep} &  \textbf{5.58} & 3.50 & 3.85 & 3.12 & 3.86 & \textbf{0.81} \\
    TSELM~\cite{tang2024tselm}  &  52.12 & 3.48 & \textbf{4.05} & \underline{3.20} & 3.87 & 0.26 \\
    \midrule
    \myname{}-TSE \scriptsize{LoRA 4} &  11.55 & \underline{3.59}& 3.84& \textbf{3.21}& 4.04 & 0.77 \\
    \myname{}-TSE \scriptsize{LoRA 16} & 10.48 & \textbf{3.60}& 3.85& \textbf{3.21}& \textbf{4.06} & 0.77 \\
    \myname{}-TSE \scriptsize{LoRA 32} &  8.72 & \textbf{3.60}& 3.86 & \textbf{3.21}& \underline{4.05} & \underline{0.78}\\
    \myname{}-TSE \scriptsize{fine-tune} & \underline{6.87} & \textbf{3.60}& 3.85& \textbf{3.21}& \textbf{4.06} & \underline{0.78} \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[1] The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.}
        \footnotesize{\item[2] For LibriMix, we use the original text for computing WER. For EmiliaMix, we use the ASR-transcribed text of the ground truth speech for WER calculation.}
    \end{tablenotes}
\end{threeparttable}
}
\vspace{-3mm}
\end{table}

\subsubsection{Speech Enhancement}

\paragraph{Implementation Details} For speech enhancement, 
we simulate noisy speech following previous works~\cite{liu2022voicefixer, li2024masksr, wang2024selm, wang2023tf}. We utilize noise datasets such as WHAM!~\cite{wichern2019wham} and DEMAND~\cite{thiemann2013diverse}, along with room impulse response (RIR) datasets~\cite{ko2017study}, OpenSLR26 and OpenSLR28, in accordance with the 2020 DNS-Challenge\footnote{\url{https://github.com/microsoft/DNS-Challenge}}.  
For clean speech data, we randomly sample 10K hours from our pre-trained dataset without any filtering. Inspired by prior works~\cite{liu2022voicefixer, li2024masksr}, we simulate speech degradation by probabilistically applying various distortions to the audio signals. These include adding noise within an SNR range of -5 to 20 dB with probability \( p = 0.9 \), introducing reverberation with \( p = 0.35 \), and limiting the speech signal bandwidth (randomly selected from 2 kHz, 4 kHz, or 8 kHz) with a probability of with \( p = 0.25 \). The \texttt{w2v-bert-2.0} features extracted from the degraded speech serve as input conditions for our models.
We fine-tune the models using both full-scale fine-tuning and LoRA fine-tuning. For LoRA fine-tuning, we also experiment with \( r = 4, 16, 32 \), corresponding to LoRA modules with trainable parameter counts of 2M, 9M, and 18M, respectively.

\paragraph{Evaluation and Baseline}  
We evaluate our models using the 2020 DNS-Challenge \cite{dubey2024icassp} test sets, which consist of three categories: 1) synthetic data with reverb, 2) synthetic data without reverb, and 3) real recordings. For comparison, we downsample the output of our models to 16 kHz.
We compare our models with several recent speech enhancement models, including TF-GridNet~\cite{wang2023tf}, VoiceFixer~\cite{liu2022voicefixer}, SELM~\cite{wang2024selm} and MaskSR~\cite{li2024masksr}. Notably, MaskSR is a model that also uses masked generative modeling but directly generates acoustic tokens for speech enhancement. See more details about these baselines in Appendix~\ref{appendix:baseline_se}.

\paragraph{Result} The results are presented in Table~\ref{tab:results-se}. The table shows that our models achieve state-of-the-art performance across all benchmarks.
1) \myname{}-SE achieves state-of-the-art results across all three test sets with the highest SIG, BAK, OVRL, and NISQA scores, showing significant improvements over previous baselines.  
2) Results among different fine-tuned versions are comparable, even for \myname{}-SE {\scriptsize LoRA 4}.
3) Notably, \myname{}-SE performs exceptionally well on the real recording dataset, further validating its practical applicability.

\begin{table}
\caption{Results on speech enhancement task.}
\label{tab:results-se}
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|ccccc}
    \toprule
    \textbf{Model} & \textbf{SIG}($\uparrow$) & \textbf{BAK}($\uparrow$) & \textbf{OVRL}($\uparrow$) & \textbf{NISQA}($\uparrow$) & \textbf{SIM}($\uparrow$) \\
    \midrule
    \multicolumn{6}{c}{\textit{\textbf{DNS2020 with reverb}}} \\
    \midrule
    TF-GridNet~\cite{wang2023tf} & 3.11 & 3.23 & 2.51 & 2.61 & 0.69 \\
    VoiceFixer~\cite{liu2022voicefixer} & 3.43 & 4.02 & 3.13 & 3.82 &  0.91 \\
    SELM~\cite{wang2024selm} & 3.16 & 3.58 & 2.70 & - & - \\
    MaskSR~\cite{li2024masksr} & 3.53 & 4.07 & 3.25 & - & - \\
    \midrule
    \myname{}-SE \scriptsize{LoRA 4} & \underline{3.67} & \underline{4.13} & \underline{3.43} & 4.54 & \underline{0.93} \\
    \myname{}-SE \scriptsize{LoRA 16} & \underline{3.67} & \underline{4.13} & \underline{3.43} & \textbf{4.57} & \textbf{0.94} \\
    \myname{}-SE \scriptsize{LoRA 32} & \textbf{3.68} & \textbf{4.14} & \underline{3.43} & 4.48 & \textbf{0.94} \\
    \midrule
    \myname{}-SE \scriptsize{fine-tune} & \textbf{3.68} & \textbf{4.14} & \textbf{3.44} & \underline{4.56} & \textbf{0.94} \\
    \midrule
    \multicolumn{6}{c}{\textit{\textbf{DNS2020 no reverb}}} \\
    \midrule
    TF-GridNet~\cite{wang2023tf}  & 3.54 & 4.05 & 3.27 & 4.35 & 0.68 \\
    VoiceFixer~\cite{liu2022voicefixer}  & 3.50 & 4.11 & 3.25 & 4.27 & \underline{0.96} \\
    SELM~\cite{wang2024selm}  & 3.51 & 4.10 & 3.26 & - & - \\
    MaskSR~\cite{li2024masksr} & 3.60 & 4.15 & 3.37 & - & - \\
    \midrule
    \myname{}-SE \scriptsize{LoRA 4} & \underline{3.65} & 4.15 & \underline{3.43} & \textbf{4.82} & \textbf{0.97} \\
    \myname{}-SE \scriptsize{LoRA 16} & \underline{3.65} & \underline{4.16} & \underline{3.43} & \underline{4.81} & \textbf{0.97} \\
    \myname{}-SE \scriptsize{LoRA 32}  & \textbf{3.66} & \textbf{4.17} & \textbf{3.44} & 4.77 & \textbf{0.97} \\
    \midrule
    \myname{}-SE \scriptsize{fine-tune} & 3.64 & \textbf{4.17} & \underline{3.43} & 4.76 & \textbf{0.97} \\
    \midrule
    \multicolumn{6}{c}{\textit{\textbf{DNS2020 Real Recording}}} \\
    \midrule
    VoiceFixer~\cite{liu2021voicefixer} & 3.31 & 3.93 & 3.00 & 3.66 & - \\
    SELM~\cite{wang2024selm} & \underline{3.59} & 3.44 & 3.12 & - & - \\
    MaskSR~\cite{li2024masksr}  & 3.43 & 4.03 & 3.14 & - &  - \\
    \midrule
    \myname{}-SE \scriptsize{LoRA 4} & \textbf{3.60} & \underline{4.02} & \underline{3.29} & 3.94 & - \\
    \myname{}-SE \scriptsize{LoRA 16} & \textbf{3.60} & \textbf{4.04} & \textbf{3.30} & \textbf{3.97} & - \\
    \myname{}-SE \scriptsize{LoRA 32} & \underline{3.59} & 3.99 & 3.26 & 3.92 & - \\
    \midrule
    \myname{}-SE \scriptsize{fine-tune} & \underline{3.59} & 4.01 & 3.27 & \underline{3.95} & - \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[1] The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.}
    \end{tablenotes}
\end{threeparttable}
}
\vspace{-3mm}
\end{table}

\subsubsection{Lip-to-Speech}

\paragraph{Implementation Details}
We use a combined data set comprising the training and sets of LRW\footnote{\url{https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrw1.html}}, LRS2\footnote{\url{https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html}}, and LRS3\footnote{\url{https://mmai.io/datasets/lip_reading/}} as the training data.
We use the same methodology in~\citet{ma2023auto} to pre-process the videos and use the visual speech recognition encoder in~\citet{ma2023auto} to extract visual features that served as the conditions and randomly extract a prefix of the target speech as the prompt.

\paragraph{Evaluation and Baseline} We compare our models with Lip2Speech-Unit~\cite{choi2023intelligible}, an expert lip-to-speech model. We use the test sets of LRS2 and LRS3 to evaluate our models following previous works~\cite{choi2023intelligible, kim2023lip}. Notably, over 60\% of the samples in the two test sets have durations of less than 2 seconds. Utilizing excessively short audio as a prompt may degrade the sound quality of the inference results, while directly using part of the target audio as a prompt may lead to information leakage. We use speech randomly selected from SeedTTS \textit{test-en} as prompts for each test case.


\paragraph{Result} The results are presented in Table~\ref{tab:results-l2s}. The table shows that \myname{}-L2S  outperforms the baseline Lip2Speech-Unit across all metrics. Specifically, the audio quality metrics show substantial improvements and the speaker similarity nearly doubles on both datasets (29.34 $\rightarrow$ 59.73 on LRS2, 32.05 $\rightarrow$ 56.74 on LRS3). The model also achieves a lower WER than the baseline.

\begin{table}[ht]
\caption{Results on lip-to-speech task.}
\label{tab:results-l2s}
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|cccc}
    \toprule
    \textbf{Model} & \textbf{WER}($\downarrow$) & \textbf{DNSMOS}($\uparrow$) & \textbf{NISQA}($\uparrow$) & \textbf{SIM}($\uparrow$) \\
    \midrule
    \multicolumn{5}{c}{\textit{\textbf{LRS2}}} \\
    \midrule
    Lip2Speech-Unit~\cite{kim2023lip} & 33.64 & 3.01 & 2.70 & 29.34 \\
    \midrule
    % \myname{}-L2S \scriptsize{LoRA 32} &  &  &  &  &  \\
    \myname{}-L2S \scriptsize{fine-tune} & \textbf{32.28} & \textbf{3.23} & \textbf{3.71} & \textbf{59.73} \\
    \midrule
    \multicolumn{5}{c}{\textit{\textbf{LRS3}}} \\
    \midrule
    Lip2Speech-Unit~\cite{kim2023lip} & 38.34 & 2.28 & 1.92 & 32.05 \\
    \midrule
    \myname{}-L2S \scriptsize{fine-tune} & \textbf{31.03} & \textbf{3.09} & \textbf{3.75} & \textbf{56.74} \\
    \bottomrule
    \end{tabular}%
\end{threeparttable}
}
\vspace{-3mm}
\end{table}

\subsection{Multi-Task Fine-Tuning}


In addition to fine-tuning the pre-trained model separately for different tasks, we explore the potential of jointly fine-tuning it on multiple tasks, resulting in a multi-task model, which we refer to as Metis-Omni. For this study, we select four tasks: zero-shot TTS, voice conversion, target speaker extraction, and speech enhancement. Further details and experimental results are provided in Appendix~\ref{appendix:omni} and Table~\ref{tab:metis-omni}.

We take target speaker extraction as an example to illustrate the effectiveness of \myname{}-Omni in enabling novel applications through task combinations. As shown in Table~\ref{tab:tse-add-text}, Metis-Omni outperforms baseline methods in terms of WER. Furthermore, by integrating text-to-speech and target speaker extraction tasks, the text-guided version of Metis-Omni achieves a remarkable WER reduction to 2.70, demonstrating a substantial improvement over all other models. Notably, despite the model not being explicitly trained on text-guided target speaker extraction, it generalizes well to this novel setting, highlighting the advantage of our system in leveraging multimodal conditional inputs to enable flexible and efficient task adaptation.

\begin{table}[ht]
\caption{Results of \myname{}-Omni on target speaker extraction.}
\label{tab:tse-add-text}
\centering
\resizebox{0.48\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|cccccc}
    \toprule
    \textbf{Model} & \textbf{WER}($\downarrow$) & \textbf{SIG}($\uparrow$) & \textbf{BAK}($\uparrow$) & \textbf{OVRL}($\uparrow$) & \textbf{NISQA}($\uparrow$) & \textbf{SIM}($\uparrow$) \\
    \midrule
    \multicolumn{7}{c}{\textit{\textbf{LibriMix test}}} \\
    \midrule
    Ground Truth & 4.27 & 3.62 & 4.03 & 3.32 & 4.11 & 0.76 \\
    \midrule
    WeSep~\cite{wang2024wesep} &  6.19 & 3.56 & 3.93 & 3.23 & 4.04 & 0.73 \\
    TSELM~\cite{tang2024tselm}  &  9.20 & 3.55 & \textbf{4.08} & 3.29 & 4.03 & 0.27 \\
    \midrule
    \myname{}-TSE \scriptsize{fine-tune}  & 6.31 & 3.65 & 4.02 & \underline{3.34} & 4.36 & \underline{0.74} \\
    \myname{}-Omni \scriptsize{fine-tune}  & \underline{5.90} & \underline{3.66} & \underline{4.03} & \textbf{3.36} & \textbf{4.40} & \textbf{0.75} \\
    \myname{}-Omni \scriptsize{fine-tune, text-guided}  & \textbf{2.70} & \textbf{3.67} & \underline{4.03} & \textbf{3.36} & \underline{4.39} & \textbf{0.75} \\
    \bottomrule
    \end{tabular}%
    \begin{tablenotes}
        \footnotesize{\item[1] The best and the second best result is shown in \textbf{bold} and by \underline{underlined}.}
    \end{tablenotes}
\end{threeparttable}
}
\vspace{-3mm}
\end{table}

\section{Conclusion}

In this work, we propose \textbf{\textit{\myname{}}}, a foundation model for unified speech generation that leverages large-scale unlabeled speech data for pre-training and can be effectively adapted to diverse speech generation tasks through fine-tuning. Our experiments demonstrate that \myname{} outperforms state-of-the-art task-specific and multi-task systems on zero-shot TTS, voice conversion, target speaker enhancement, speech enhancement, and lip-to-speech after fine-tuning, even with fewer than 20M trainable parameters or up to 300 times less training data for certain tasks while supporting multimodal conditional inputs.
In addition, we propose \myname{}-Omni, a version of our pre-trained model fine-tuned on multiple tasks, which demonstrates further improvements.

\section*{Impact Statement}

Given that Metis is a powerful foundation model capable of generating high-quality speech across multiple tasks, it also presents potential risks of misuse, such as voice spoofing, speaker impersonation, and unauthorized content generation. To mitigate these risks, it is essential to develop robust detection mechanisms for synthetic speech, establish safeguards to prevent malicious use, and implement a responsible reporting system for identifying and addressing misuse cases.

% \clearpage

\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Details of Two Types of Speech Discrete Representations}
\label{appendix:codec}

\myname{} employs two discrete speech representations for the two-stage generation, following the approach of~\citet{wang2024maskgct}.

1) \textbf{SSL tokens}: Derived from SSL features of large-scale speech self-supervised learning models~\cite{chung2021w2v, hsu2021hubert, chen2022wavlm, chiu2022self}, SSL tokens encapsulate both semantic and prosodic information, making them well-suited for conditional generation. To minimize information loss, we employ a vector quantization (VQ) model~\cite{van2017neural, esser2021taming} to quantize SSL features into discrete tokens, in contrast to the k-means approach used in previous works~\cite{borsos2023audiolm, kharitonov2023speak, betker2023better}.
SSL features are extracted from the 17th layer of \texttt{w2v-bert-2.0}\footnote{\url{https://huggingface.co/facebook/w2v-bert-2.0}}~\cite{chung2021w2v}. The VQ model has a codebook of 8,192 and a codebook dimension of 8. The model is trained using only reconstruction loss and VQ loss.

2) \textbf{Acoustic tokens}: Directly obtained from the waveform via vector quantization. The goal is to preserve all the information from the speech to reconstruct a high-quality waveform. We follow the recipe of DAC codec~\cite{kumar2024high} for model architecture, discriminators, and training losses, with modifications to adopt the Vocos~\cite{siuzdak2023vocos} decoder for more efficient training and inference. The 24 kHz speech waveform is compressed into discrete tokens using residual vector quantization (RVQ) across 12 layers, each with a codebook size of 1,024 and a codebook dimension of 8.

\section{Baselines}
\label{appendix:baseline}

\subsection{Zero-Shot TTS}
\label{appendix:baseline_tts}

\paragraph{VALL-E~\cite{wang2023neural}} It uses an AR transformer to predict codes from the first layer of EnCodec~\cite{defossez2022high} and a NAR transformer to predict codes from the remaining layers of EnCodec. We use the released checkpoint in Amphion~\cite{zhang2023amphion}\footnote{\url{https://github.com/open-mmlab/Amphion/tree/main/egs/tts/VALLE_V2}}
which is pre-trained on 45K hours of the MLS~\cite{pratap2020mls} English set.

\paragraph{VoiceCraft~\cite{peng2024voicecraft}} A token-filling neural codec language model for text editing and text-to-speech. It predicts multi-layer tokens in a delay pattern. We use the official code and checkpoint\footnote{\url{https://huggingface.co/pyp1/VoiceCraft/blob/main/830M_TTSEnhanced.pth}} which is pre-trained on 9K hours of GigaSpeech~\cite{chen2021gigaspeech} dataset.

\paragraph{CosyVoice~\cite{du2024cosyvoice}} A two-stage large-scale TTS system. The first stage is an autoregressive model and the second stage is a diffusion model. It is trained on 171K hours of multilingual speech data. We use the official code and checkpoint\footnote{\url{https://huggingface.co/model-scope/CosyVoice-300M}}.

\paragraph{XTTS-v2~\cite{casanova2024xtts}} An open-source multilingual TTS model that supports 16 languages. It is also based on an autoregressive model. We use the official code and checkpoint\footnote{\url{https://huggingface.co/coqui/XTTS-v2}}.

\paragraph{MaskGCT~\cite{wang2024maskgct}} An open-source large-scale NAR TTS system that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. It employs masked generative models for two-stage modeling and is trained on 100K hours of multilingual speech data. We use the official code and checkpoint\footnote{\url{https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct}}.

\paragraph{NaturalSpeech 3~\cite{ju2024naturalspeech}} A large-scale NAR TTS system featuring a factorized speech codec for speech decoupling representation and factorized diffusion models for speech generation. It achieves human-level naturalness on the LibriSpeech test set. We report the scores of LibriSpeech \textit{test-clean} obtained from the original paper.

\paragraph{VoiceBox~\cite{le2024voicebox}} A large-scale NAR multi-task speech generation model based on flow matching~\cite{lipman2022flow}. We report the scores of LibriSpeech \textit{test-clean} obtained from the original paper.

\subsection{Voice Conversion}
\label{appendix:baseline_vc}

\paragraph{HireSpeech++~\cite{lee2023hierspeech++}} A speech generation system designed based on the VITS architecture~\cite{kim2021conditional}. It
is trained on 2.8K hours sourced from Libri-light~\cite{kahn2020libri} and LibriTTS~\cite{zen2019libritts}. We use the official code and checkpoint\footnote{\url{https://github.com/sh-lee-prml/HierSpeechpp}}.

\paragraph{LM-VC~\cite{wang2023lm}} It uses an AR hierarchical transformer to predict SoundStream~\cite{zeghidour2021soundstream} codes from soft units similar to HuBERT~\cite{hsu2021hubert} k-means tokens, trained on the Libri-light dataset~\cite{kahn2020libri}. It is not open-source, we report the scores obtained from~\cite{vevo}.

\paragraph{UniAudio~\cite{yang2023uniaudio}} An AR model system can perform multiple audio generation tasks. It uses 500-cluster k-means tokens from HuBERT~\cite{hsu2021hubert} to predict their proposed acoustic codes for voice conversion. We use the official code and checkpoint\footnote{\url{https://github.com/yangdongchao/UniAudio}}. It uses 60K hours of Libri-light~\cite{kahn2020libri} for training voice conversion.

\paragraph{Vevo~\cite{vevo}} It is a versatile zero-shot voice imitation model which can control both timbre and style. It is trained on 60K hours of Libri-heavy~\cite{kang2024libriheavy}. We obtain the generated samples from the authors.

\subsection{Target Speaker Extraction}
\label{appendix:baseline_tse}

\paragraph{UniAudio~\cite{yang2023uniaudio}} An AR model system can perform multiple audio generation tasks including target speaker extraction. We use the official code and checkpoint.

\paragraph{VoiceFilter~\cite{wang2018voicefilter}}
A system that isolates a target speaker's voice from multi-speaker audio using a reference signal and neural networks for speaker embedding and spectrogram masking.
We use the checkpoint~\footnote{\url{https://huggingface.co/nguyenvulebinh/voice-filter}} provided from~\citet{nguyen2024convoifilter}.

\paragraph{WeSep~\cite{wang2024wesep}} A target speaker extraction model trained on LibriMix~\cite{cosentino2020librimix} and VoxCeleb~\cite{nagrani2017voxceleb}. We use the official code and  checkpoint\footnote{\url{https://huggingface.co/spaces/wenet-e2e/wesep-tse-2speaker-demo/tree/main}}.

\paragraph{TSELM~\cite{tang2024tselm}} A target speaker extraction model that leverages a NAR transformer to predict discrete speech tokens driven from WavLM~\cite{chen2022wavlm}. We use the official code and checkpoint\footnote{\url{https://huggingface.co/Beilong/TSELM}}.

\subsection{Speech Enhancement}
\label{appendix:baseline_se}

\paragraph{TF-GridNet~\cite{wang2023tf}}
A deep neural network for speech separation integrating full- and sub-band modeling in the time-frequency (T-F) domain. It can also used for speech enhancement. 
We use the checkpoint\footnote{\url{https://huggingface.co/kohei0209/tfgridnet_urgent25/tree/main}} provided from Interspeech URGENT 2025 Challenge\footnote{\url{https://urgent-challenge.github.io/urgent2025/}}.

\paragraph{VoiceFixer~\cite{liu2022voicefixer}} A generative framework to address the speech enhancement task. It consists of an analysis stage and a synthesis stage and employs a ResUNet~\cite{diakogiannis2020resunet} to model the analysis stage and a neural vocoder to model the synthesis stage. We use the official code and checkpoint\footnote{\url{https://github.com/haoheliu/voicefixer_main}}.

\paragraph{SELM~\cite{wang2024selm}} A speech enhancement model that leverages an AR transformer to predict discrete speech tokens driven from WavLM~\cite{chen2022wavlm}. We report the scores obtained from~\cite{li2024masksr}.

\paragraph{MaskSR~\cite{li2024masksr}} It uses masked generative models to predict acoustic tokens from DAC~\cite{kumar2024high} codec for speech enhancement. We report the scores obtained from the original paper.

\subsection{Lip-to-Speech}
\label{appendix:baseline_l2s}

\paragraph{Lip2Speech-Unit~\cite{kim2023lip}} A lip-to-speech model trained to generate multiple targets, mel-spectrograms and quantized self-supervised speech representations. We use the official code and checkpoint\footnote{\url{https://github.com/choijeongsoo/lip2speech-unitn}}.

\section{Evaluation Metrics}
\label{appendix:evaluation_metrics}

\paragraph{SIM} We evaluate speaker similarity between the prompt speech and the generated speech by computing the cosine similarity of the WavLM TDNN\footnote{\url{https://github.com/microsoft/UniSpeech/tree/main/downstreams/speaker_verification}}\cite{chen2022wavlm} speaker embeddings between the generated sample and the prompt. SIM is reported for tasks involving prompt speech, including zero-shot TTS, voice conversion, target speaker extraction, and lip-to-speech. For speech enhancement, we compute SIM between the generated speech and ground truth using a separate checkpoint\footnote{\url{https://huggingface.co/microsoft/wavlm-base-plus-sv}}.

\paragraph{WER} Word Error Rate measures the intelligibility of the generated speech. We use \texttt{whisper-large-v3}\footnote{\url{https://huggingface.co/openai/whisper-large-v3}}~\cite{radford2023robust} for all languages except Chinese, where we use \texttt{paraformer-zh}\footnote{\url{https://huggingface.co/funasr/paraformer-zh}}~\cite{gao2022paraformer, gao2023funasr} as the ASR model to calculate WER. WER is reported for zero-shot TTS, voice conversion, target speaker extraction, and lip-to-speech.

\paragraph{DNSMOS~\cite{reddy2021dnsmos}} DNSMOS is a neural network-based mean opinion score estimator that correlates strongly with human quality ratings. It comprises three components: 1) speech quality (\textbf{SIG}), 2) background noise quality (\textbf{BAK}), and 3) overall quality (\textbf{OVRL}). We report DNSMOS scores for all tasks while providing the average scores for zero-shot TTS and VC tasks, and all three metrics for the remaining tasks.

\paragraph{NISQA~\cite{mittag2021nisqa}} NISQA is a deep learning framework for speech quality prediction. We use the public checkpoint\footnote{\url{https://github.com/gabrielmittag/NISQA/blob/master/weights/nisqa.tar}}. We report NISQA for voice conversion, speech enhancement, target speaker extraction, and lip-to-speech.

\paragraph{QMOS} We use the Quality Mean Opinion Score (QMOS) for subjective listening tests to evaluate speech quality. QMOS is rated on a 5-point scale: 5 (Excellent), 4 (Good), 3 (Fair), 2 (Poor), and 1 (Bad).

\paragraph{SMOS} We use the Speaker Mean Opinion Score (SMOS) for subjective listening tests to assess speaker similarity in speech. SMOS is rated on a 5-point scale: 5 (Excellent), 4 (Good), 3 (Fair), 2 (Poor), and 1 (Bad). SMOS is reported for zero-shot TTS.

\clearpage

\section{Additional Experimental Results}
\label{appendix:res}

\subsection{Zero-Shot TTS Results on LibriSpeech}
\label{appendix:res_librispeech}

Table~\ref{tab:res_librispeech} shows zero-shot TTS results on LibriSpeech \textit{test-clean}. Metis-TTS achieves the highest SIM despite using significantly less training data, while maintaining a competitive WER.

\begin{table}[H]
\caption{Zero-shot TTS results on LibriSpeech.}
\label{tab:res_librispeech}
\centering
\begin{threeparttable}
    \begin{tabular}{l|c|cc}
    \toprule
    \textbf{Model} & \makecell{\textbf{Training} \\ \textbf{Data}} & \textbf{WER}($\downarrow$) & \textbf{SIM}($\uparrow$) \\
    \midrule
    \multicolumn{4}{c}{\textbf{\textit{LibriSpeech test-clean}}} \\
    \midrule
    VALL-E~\cite{ju2024naturalspeech} & 45K EN & 5.90 & 0.50 \\
    VoiceCraft~\cite{peng2024voicecraft} & 9K EN  & 4.68 & 0.45 \\
    NaturalSpeech 3~\cite{ju2024naturalspeech} & 60K EN & \textbf{1.94} & 0.67 \\
    VoiceBox~\cite{ju2024naturalspeech} & 60K EN & 2.03 & 2.03 \\
    \midrule
    \myname{}-TTS & 10K Multi. & 4.33 & \textbf{0.70} \\
    \bottomrule
    \end{tabular}%
\end{threeparttable}
\end{table}


\subsection{Voice Conversion Results on SeedVC}
\label{appendix:res_seedvc}

We also provide voice conversion results on SeedVC in Table~\ref{tab:results-seedvc}. Despite being fine-tuned on significantly less training data (0.4K hours compared to 2.8K hours used by HierSpeech++), Metis-VC achieves higher SIM scores on both SeedVC-en (0.76 vs. 0.56) and SeedVC-zh (0.63 vs. 0.39), demonstrating its strong capability in preserving speaker identity. Additionally, Metis-VC achieves a higher NISQA score than the baseline model, indicating improved speech quality.

\begin{table}[H]
\caption{Voice conversion results on SeedVC test sets.}
\label{tab:results-seedvc}
\centering
\begin{threeparttable}
    \begin{tabular}{l|c|cccc}
    \toprule
    \textbf{Model} & \makecell{\textbf{Training} \\ \textbf{Data}} & \textbf{WER}($\downarrow$) & \textbf{SIM}($\uparrow$) & \textbf{DNSMOS}($\uparrow$) & \textbf{NISQA}($\uparrow$) \\
    \midrule
    \multicolumn{6}{c}{\textit{\textbf{SeedVC-en}}} \\
    \midrule
    HierSpeech++~\cite{lee2023hierspeech++} & 2.8K & 4.48 & 0.56 & 3.57 & 3.47 \\
    \midrule
    \myname{}-VC \scriptsize{fine-tune, $\text{cfg} = 0.0$} & 0.4K & 7.05 & \textbf{0.76} & 3.51 & \textbf{4.14} \\
    \midrule
    \multicolumn{6}{c}{\textit{\textbf{SeedVC-zh}}} \\
    \midrule
    HierSpeech++~\cite{lee2023hierspeech++} & 2.8K & 5.45 & 0.39 & 3.50 & 4.14 \\
    \midrule
    \myname{}-VC \scriptsize{fine-tune, $\text{cfg} = 0.0$} & 0.4K & 6.82 & \textbf{0.63} & 3.39 & \textbf{3.82} \\
    \bottomrule
    \end{tabular}%
\end{threeparttable}
\end{table}


\subsection{Results of Subjective Evaluations}
\label{appendix:res_sub}

We conduct subjective listening tests for two representative tasks: zero-shot TTS and speech enhancement.
For zero-shot TTS, we randomly sampled 20 test samples from the results of SeedTTS \textit{test-en}. Both QMOS and SMOS are reported for zero-shot TTS. The results are shown in Table~\ref{tab:results-sub-tts}.
For speech enhancement, we randomly sampled 20 test samples from the results of DNS2020 with reverb. QMOS is reported for speech enhancement. The results are shown in Table~\ref{tab:results-sub-se}.

\begin{table}[H]
\caption{Results of Subjective Evaluations for Zero-shot TTS}
\label{tab:results-sub-tts}
\centering
\begin{threeparttable}
    \begin{tabular}{l|cc}
    \toprule
    \textbf{Model} & \textbf{QMOS}($\uparrow$) &  \textbf{SMOS}($\uparrow$) \\
    \midrule
    Ground Truth & 4.32 & 4.01 \\
    \midrule
    VALL-E~\cite{wang2023neural} & 3.32 & 3.58 \\
    VoiceCraft~\cite{peng2024voicecraft} & 3.58 & 3.65 \\
    CosyVoice~\cite{du2024cosyvoice} & 4.02 & 3.97 \\
    \midrule
    \myname{}-TTS & \textbf{4.21} & \textbf{4.19} \\
    \bottomrule
    \end{tabular}%
\end{threeparttable}
\end{table}

\begin{table}[H]
\caption{Results of Subjective Evaluations for Speech Enhancement}
\label{tab:results-sub-se}
\centering
\begin{threeparttable}
    \begin{tabular}{l|c}
    \toprule
    \textbf{Model} & \textbf{QMOS}($\uparrow$) \\
    \midrule
    Ground Truth & 4.57 \\
    \midrule
    TF-GridNet~\cite{wang2023tf} & 3.65 \\
    VoiceFixer~\cite{liu2022voicefixer} & 3.77 \\
    \midrule
    \myname{}-SE & \textbf{4.29} \\
    \bottomrule
    \end{tabular}%
\end{threeparttable}
\end{table}

\section{\myname{}-Omni: Multi-Task Fine-Tuning}
\label{appendix:omni}

In addition to fine-tuning the pre-trained model separately for different tasks, we explore the potential of jointly fine-tuning a pre-trained model on multiple tasks resulting a multi-task model, which we refer to as \textbf{Metis-Omni}. For this study, we select four tasks: zero-shot TTS, voice conversion, target speaker extraction, and speech enhancement. We show more details in Appendix~\ref{appendix:omni}.
A subset of 10K hours of data is randomly sampled from the Emilia dataset as the shared training data for these tasks. During training, the task proportions are set as $p = \{0.5, 0.1, 0.2, 0.2\}$ for zero-shot TTS, voice conversion, target speaker extraction, and speech enhancement, respectively.
As shown in Table~\ref{tab:metis-omni}, \myname{}-Omni achieves performance that is either on par with or surpasses task-specific fine-tuned models across all tasks. The only exception is the WER metric in zero-shot TTS, where performance is slightly lower. A possible reason for this is that, under the same number of training steps, multi-task fine-tuning allocates fewer steps to zero-shot TTS, leading to less task-specific optimization.

\begin{table}[ht]
\caption{Results of Metis-Omni on four speech generation tasks.}
\label{tab:metis-omni}
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{threeparttable}
    \begin{tabular}{l|l|l|cc}
    \toprule
\multirow{2}{*}{\textbf{Task}} & \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Performance}} \\ \cline{4-5}
& & & \textbf{Metrics} & \textbf{Results} \\

\midrule

\multirow{6}{*}{Zero-shot TTS} & \multirow{3}{*}{\textit{\textbf{SeedTTS-en}}} & MaskGCT~\cite{wang2024maskgct} & \multirow{3}{*}{WER($\downarrow$) $|$ SIM($\uparrow$) $|$ DNSMOS($\uparrow$)} & 2.47 $|$ 0.72 $|$ 3.52 \\
 &  & Metis-TTS & & \textbf{2.41} $|$ \textbf{0.72} $|$ \textbf{3.57} \\
 &  & Metis-Omni & & 4.78 $|$ 0.71 $|$ \textbf{3.57} \\ \cline{2-5}
 
 &  \multirow{3}{*}{\textit{\textbf{SeedTTS-zh}}}  & MaskGCT~\cite{wang2024maskgct} &  \multirow{3}{*}{WER($\downarrow$) $|$ SIM($\uparrow$) $|$ DNSMOS($\uparrow$)} & 2.18 $|$ 0.77 $|$ 3.58 \\
 &  & Metis-TTS & & \textbf{2.30} $|$ \textbf{0.77} $|$ 3.55 \\
 &  & Metis-Omni & & 4.39 $|$ \textbf{0.77} $|$ \textbf{3.60}  \\
 
\midrule

\multirow{3}{*}{VC} & \multirow{3}{*}{\textit{\textbf{VCTK}}} & LM-VC~\cite{wang2023lm} & \multirow{3}{*}{WER($\downarrow$) $|$ SIM($\uparrow$) $|$DNSMOS($\uparrow$)} & 8.35 $|$ 0.29 $|$ 3.46 \\
 &  & Metis-VC & & 6.65 $|$ \textbf{0.48} $|$ 3.49 \\
 &  & Metis-Omni & & \textbf{3.52} $|$ 0.34 $|$ \textbf{3.51} \\

\midrule

\multirow{9}{*}{SE} & \multirow{3}{*}{\textit{\textbf{DNS2020 with reverb}}} & TF-Grident~\cite{wang2023tf} & \multirow{3}{*}{SIG($\uparrow$) $|$ BAK($\uparrow$) $|$ OVRL($\uparrow$) $|$ NISQA($\uparrow$)} & 3.11 $|$ 3.23 $|$ 2.51 $|$ 2.61 \\
 &  & Metis-SE & & \textbf{3.68} $|$ \textbf{4.14} $|$ \textbf{3.44} $|$ \textbf{4.56} \\
 &  & Metis-Omni & & \textbf{3.68} $|$ 4.13 $|$ \textbf{3.44} $|$ 4.53 \\ \cline{2-5}
 
 &  \multirow{3}{*}{\textit{\textbf{DNS2020 no reverb}}}  & TF-Grident~\cite{wang2023tf} & \multirow{3}{*}{SIG($\uparrow$) $|$ BAK($\uparrow$) $|$ OVRL($\uparrow$) $|$ NISQA($\uparrow$)} & 3.54 $|$ 4.05 $|$ 3.27 $|$ 4.35 \\
 &  & Metis-SE & & 3.64 $|$ \textbf{4.17} $|$ 3.43 $|$ 4.77 \\
 &  & Metis-Omni & & \textbf{3.65} $|$ \textbf{4.17} $|$ \textbf{3.44} $|$ \textbf{4.79} \\ \cline{2-5}

&  \multirow{3}{*}{\textit{\textbf{Real Recording}}}  & VoiceFixer~\cite{liu2022voicefixer} & \multirow{3}{*}{SIG($\uparrow$) $|$ BAK($\uparrow$) $|$ OVRL($\uparrow$) $|$ NISQA($\uparrow$)} & 3.31 $|$ 3.93 $|$ 3.00 $|$ 3.66 \\
 &  & Metis-SE & & 3.59 $|$ \textbf{4.01} $|$ \textbf{3.27} $|$ 3.95 \\
 &  & Metis-Omni & & \textbf{3.60} $|$ 4.00 $|$ \textbf{3.27} $|$ \textbf{3.96} \\

\midrule

\multirow{6}{*}{TSE} & \multirow{3}{*}{\textit{\textbf{LibriMix test}}} & WeSep~\cite{wang2024wesep} & \multirow{3}{*}{SIG($\uparrow$) $|$ BAK($\uparrow$) $|$ OVRL($\uparrow$) $|$ NISQA($\uparrow$)} & 3.56 $|$ 3.39 $|$ 3.23 $|$ 4.04 \\
 &  & Metis-TSE & & 3.65 $|$ 4.02 $|$ 3.34 $|$ 4.36 \\
 &  & Metis-Omni & & \textbf{3.66} $|$ \textbf{4.03} $|$ \textbf{3.36} $|$ \textbf{4.40} \\ \cline{2-5}
 
 &  \multirow{3}{*}{\textit{\textbf{EmiliaMix test}}}  & WeSep~\cite{wang2024wesep} & \multirow{3}{*}{SIG($\uparrow$) $|$ BAK($\uparrow$) $|$ OVRL($\uparrow$) $|$ NISQA($\uparrow$)} & 3.50 $|$ 3.85 $|$ 3.12 $|$ 3.86 \\
 &  & Metis-TSE & & 3.60 $|$ \textbf{3.85} $|$ 3.21 $|$ 4.06 \\
 &  & Metis-Omni & &  \textbf{3.62} $|$ 3.82 $|$ \textbf{3.24} $|$ \textbf{4.11} \\
 
    \bottomrule
    \end{tabular}
\end{threeparttable}
}
\end{table}


\section{Speech Discrete Representation}
\label{appendix:speech_rep}
Speech representation is a crucial aspect of speech generation. Recently, some speech generation systems~\cite{borsos2023audiolm, borsos2023soundstorm, kharitonov2023speak, wang2023neural, ju2024naturalspeech, anastassiou2024seed, wang2024maskgct, vevo} have transitioned to using discrete speech representations, which can be broadly categorized into two types: 
1) \textbf{SSL tokens}: typically derived by quantizing speech self-supervised learning features~\cite{hsu2021hubert, chung2021w2v, zhang2023google, chiu2022self, chen2022wavlm}. Unlike acoustic tokens, SSL tokens are designed not for directly reconstructing waveform but for encoding essential semantic and prosody information in speech, making them more suitable for prediction in conditional generation models.
2) \textbf{Acoustic tokens}: typically obtained by training a VQGAN~\cite{van2017neural, esser2021taming} model for waveform reconstruction, as used in speech codecs~\cite{zeghidour2021soundstream, defossez2022high, kumar2024high}. Acoustic tokens are effective for reconstructing high-quality waveforms.
Currently, some speech generation systems~\cite{liu2024audioldm, borsos2023soundstorm, kharitonov2023speak, wang2024maskgct} utilize both types of representations for speech generation. In this work, we also adopt this two-stage paradigm, employing an MGM to generate SSL tokens from any condition and another MGM to generate acoustic tokens from SSL tokens.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Limitation and Future Work}

There are still some limitations that can be studied in the future.

\paragraph{Unified Audio Representation} In this work, we utilize two distinct discrete speech representations, SSL tokens and acoustic tokens, for two-stage modeling, with SSL tokens specifically designed for the speech domain. Developing a unified discrete representation for all audio types, such as speech, music, and sound effects, that can seamlessly integrate with conditional generation models is an important direction for future research. Additionally, unifying the characteristics of SSL tokens and acoustic tokens to enable both high-quality waveform reconstruction and effective conditional modeling is equally significant.

\paragraph{Few-Shot Task Learning} In this work, we adapt our pre-trained model to different characters through fine-tuning. In the field of NLP, large language models exhibit the remarkable ability to learn new tasks in a zero-shot manner without requiring additional training. This capability merits further investigation in future research.



\end{document}
