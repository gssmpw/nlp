\section{Related Work}
\paragraph{Masked Generative Models for Speech} Masked generative models (MGMs) are a family of generative models that typically employ non-autoregressive transformers____. These models have achieved significant success, demonstrating performance comparable to or even surpassing autoregressive and diffusion models in image____ and video____ generation, while offering a better balance between quality and speed.
In the speech domain, SoundStorm____ uses the semantic tokens from AudioLM____ and employs MGMs to generate acoustic tokens from a neural audio codec____, enabling applications like TTS and voice conversion. NaturalSpeech 3____ adopts MGMs to generate disentangled speech tokens. MaskGCT____ further leverages MGMs for zero-shot generation, eliminating the need for explicit text-speech alignment or phone-level duration prediction in non-autoregressive TTS models. MaskSR____ applies MGMs to speech enhancement tasks. In this work, we propose a unified speech generation framework based on MGMs.


\paragraph{Unified Speech Generation} Developing a unified framework capable of handling various tasks is a key research objective in artificial intelligence. In the field of speech generation, UniAudio____ employs an LLM for next-token prediction to generate multiple types of audio. Similarly, SpeechX____ leverages an LLM for unified zero-shot tasks such as TTS, noise suppression, and target speaker extraction. Both models achieve this by concatenating the condition and target speech tokens, followed by AR modeling.
However, these models require large amounts of paired training data for each task, failing to leverage the vast amount of unlabeled speech data effectively. VoiceBox____ employs flow matching to unify tasks such as zero-shot TTS, speech editing, and speech enhancement. However, it has notable limitations, such as requiring text and clean speech as references for speech enhancement and relying on phone durations during training for zero-shot TTS. Its successor, AudioBox____, extends VoiceBox to unified audio generation with natural language prompt control. Our work is partly inspired by SpeechFlow____, which uses flow matching____ to learn infilling during pre-training and fine-tunes with task-specific conditions for various speech generation tasks, such as zero-shot TTS and speech separation. However, it is limited to frame-level conditions, such as requiring a frame-level phoneme sequence for TTS. Additionally, predicting mel-spectrograms directly during pre-training may be suboptimal due to the need to predict extensive acoustic details.

Speech discrete representation is also highly relevant to our work, and we have included this part in Appendix~\ref{appendix:speech_rep}.