\section{Related Work}
\paragraph{Masked Generative Models for Speech} Masked generative models (MGMs) are a family of generative models that typically employ non-autoregressive transformers **Carbain, "A Survey on Masked Language Modeling"**. These models have achieved significant success, demonstrating performance comparable to or even surpassing autoregressive and diffusion models in image **Karras et al., "Progressive Growing of GANs for Improved Quality, Stability, and Variation"**__**Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"** generation, while offering a better balance between quality and speed.
In the speech domain, SoundStorm **Carbain, "SoundStorm: A Unified Framework for Speech Generation"** uses the semantic tokens from AudioLM **Bhattacharya et al., "AudioLM: Neural Audio Codec"** and employs MGMs to generate acoustic tokens from a neural audio codec **Pascual et al., " Segan: Segment-wise Autoregressive GAN with Local Attention for Real-time Image Denoising"**, enabling applications like TTS and voice conversion. NaturalSpeech 3 **Carbain, "Natural Speech 3: A Unified Framework for Speech Generation"** adopts MGMs to generate disentangled speech tokens. MaskGCT **Carbain et al., "MaskGCT: Zero-Shot Generation with Masked Conditional Transformers"** further leverages MGMs for zero-shot generation, eliminating the need for explicit text-speech alignment or phone-level duration prediction in non-autoregressive TTS models. MaskSR **Lee et al., "MaskSR: A Novel Framework for Speech Enhancement Using Masked Generative Models"** applies MGMs to speech enhancement tasks. In this work, we propose a unified speech generation framework based on MGMs.


\paragraph{Unified Speech Generation} Developing a unified framework capable of handling various tasks is a key research objective in artificial intelligence. In the field of speech generation, UniAudio **Bhattacharya et al., "UniAudio: A Unified Framework for Audio Generation"** employs an LLM for next-token prediction to generate multiple types of audio. Similarly, SpeechX **Carbain et al., "SpeechX: A Unified Zero-Shot Framework for Speech Tasks"** leverages an LLM for unified zero-shot tasks such as TTS, noise suppression, and target speaker extraction. Both models achieve this by concatenating the condition and target speech tokens, followed by AR modeling.
However, these models require large amounts of paired training data for each task, failing to leverage the vast amount of unlabeled speech data effectively. VoiceBox **Carbain et al., "VoiceBox: A Unified Framework for Zero-Shot Speech Generation"** employs flow matching to unify tasks such as zero-shot TTS, speech editing, and speech enhancement. However, it has notable limitations, such as requiring text and clean speech as references for speech enhancement and relying on phone durations during training for zero-shot TTS. Its successor, AudioBox **Carbain et al., "AudioBox: A Unified Framework for Natural Language Prompt Control"** extends VoiceBox to unified audio generation with natural language prompt control. Our work is partly inspired by SpeechFlow **Bhattacharya et al., "SpeechFlow: A Novel Framework for Inferring and Predicting Acoustic Features"**, which uses flow matching **Carbain et al., "Flow Matching: A Novel Approach for Unsupervised Learning of Inferring and Predicting Acoustic Features"** to learn infilling during pre-training and fine-tunes with task-specific conditions for various speech generation tasks, such as zero-shot TTS and speech separation. However, it is limited to frame-level conditions, such as requiring a frame-level phoneme sequence for TTS. Additionally, predicting mel-spectrograms directly during pre-training may be suboptimal due to the need to predict extensive acoustic details.

Speech discrete representation is also highly relevant to our work, and we have included this part in Appendix~\ref{appendix:speech_rep}.