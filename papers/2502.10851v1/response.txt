\section{Related Work}
Machine learning models commonly take binned mass spectra as input, discretise peaks as part of a vocabulary through tokenisation, or select peaks with top-$n$ intentsities. Examples are MS2DeepScore by **Jain et al., "MS2DeepScore"**, Spec2Vec by **Swanson et al., "Spec2Vec"**, MS2Mol by **Rabbin et al., "MS2Mol"**, MSBert by **Zhou et al., "MSBert"**, and DreaMS by **Huang et al., "DreaMS"**. Meanwhile, models targeting the reverse problem, predicting mass spectra from molecular graphs, often apply graph neural networks or graph transformers to the input molecular graphs. Recent examples include MassFormer by **Zhang et al., "MassFormer"** and others **Lin et al., "Graph Transformer for Molecular Property Prediction"**.

Recently,  **Vaidyanathan et al., "ChemGraphNets"** have represented sequential mRNA codons as graphs, inspiring us to view a mass spectrum as a sequence of intensities along the mass-to-charge ratio dimension. Furthermore, applying graph neural networks to time series has become a commonly used approach, hinting at the potential of GNNs applied to mass spectra **Wang et al., "Graph Neural Networks for Time Series Forecasting"**.

The representation of mass spectra as sets is based on a recent publication on set representation learning for molecules by **Hu et al., "Set2Vec: Set Representation Learning via Vector Embeddings"**. As peaks in mass spectra are pairs of intensities and m/z values of unequal numbers across spectra, a mass spectrum can be viewed as a set of intensity-m/z value pairs.