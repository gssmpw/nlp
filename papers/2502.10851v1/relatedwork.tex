\section{Related Work}
Machine learning models commonly take binned mass spectra as input, discretise peaks as part of a vocabulary through tokenisation, or select peaks with top-$n$ intentsities. Examples are MS2DeepScore by~\citet{huberMS2DeepScoreNovelDeep2021}, Spec2Vec by~\citet{huberSpec2VecImprovedMass2021}, MS2Mol by~\citet{butlerMS2MolTransformerModel2023}, MSBert by~\citet{zhangMSBERTEmbeddingTandem2024}, and DreaMS by~\citet{bushuievEmergenceMolecularStructures2025}. Meanwhile, models targeting the reverse problem, predicting mass spectra from molecular graphs, often apply graph neural networks or graph transformers to the input molecular graphs. Recent examples include MassFormer by~\citet{youngTandemMassSpectrum2024} and others~\citep{zhangPredictionElectronIonization2022,murphyEfficientlyPredictingHigh2023,parkMassSpectraPrediction2024}. 

Recently, ~\citet{nallapareddyImprovingFulllengthRibosome2024} have represented sequential mRNA codons as graphs, inspiring us to view a mass spectrum as a sequence of intensities along the mass-to-charge ratio dimension. Furthermore, applying graph neural networks to time series has become a commonly used approach, hinting at the potential of GNNs applied to mass spectra~\citep{jinSurveyGraphNeural2024}.

The representation of mass spectra as sets is based on a recent publication on set representation learning for molecules by~\citet{boulougouriMolecularSetRepresentation2024}. As peaks in mass spectra are pairs of intensities and m/z values of unequal numbers across spectra, a mass spectrum can be viewed as a set of intensity-m/z value pairs.