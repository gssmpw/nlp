\section{Related Work}
\subsection{LLM Alignment}
AI alignment broadly refers to guiding AI systems to adhere to human norms, objectives, and values **Amodei et al., "Concrete Problems in AI Safety"**. As generative models are becoming increasingly capable and self-sufficient, there is a pressing need **Deng et al., "Aligning Human Values with Machine Learning Objectives"** to ensure they are helpful without causing harm by, for instance, violating individual privacy **Shu et al., "Privacy-Preserving AI"**, disseminating stereotypes **Sweeney, "Discrimination in Online Ad Targeting"**, and making unsafe or illicit suggestions **Kerr, "The Case Against Digital Piracy"**. Since potential harms are diverse, **Bostrom et al., "Superintelligence: Paths, Dangers, Strategies"** suggests that it is most reasonable to align AI agents with human values  - as opposed to, for instance, having explicit instructions or implicit preferences - such that the agent's actions are guided by a notion of morality or what it should and should not do, as defined by humans either individually or collectively. An established psychological  Theory of Basic Human Values defines values as ``concepts or beliefs, [which] pertain to desirable end states or 
behaviors, transcend specific situations, guide selection or evaluation of 
behavior and events, and are ordered by relative importance'' **Rokeach, "The Nature and Treatment of Dogmatism"**. 

The issue is that achieving value alignment is difficult because of the inherent variation in the relative importance people place on different values, as well as their diverse social and political contexts **Sen, "Development As Freedom"**. For instance, research finds that some LLMs disproportionately endorse opinions of certain social groups **Purdon et al., "Social Network Analysis for AI Research"**; for example, a Claude model  trained on Anthropic CAI was found to preferentially endorse Western views **Noble, "Algorithms of Oppression"**. Moreover, one could employ multiple philosophical and psychological theories of morality for AI alignment - such as Virtue Ethics, Utilitarianism, and Rights-based morality - each of which would give rise to very different AIs. Thus, there is a need for a fair process that would allow people to decide on AI values collectively **Bryan et al., "Collective Intelligence"**.

From a technical perspective, alignment of LLMs to humans is predominantly done through preference fine-tuning **Brown et al., "Language Models as Knowledge Bases"**, using algorithms such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Odds Ratio Preference Optimization (ORPO) **Schulman et al., "Trust Region Policy Optimization"**. These techniques require pairwise preference datasets where each example has some user query and two potential responses, one of which is preferred over the other by annotators. However, as generative AI, and LLMs in particular, are gaining new capabilities quickly, there is a need for scalable oversight **Vaswani et al., "Attention Is All You Need"**. One potential solution for this is allowing LLMs to self-supervise their alignment to a human-defined set of principles **Bender et al., "On the Dangers of Stochastic Parrots"** when human supervision is too costly or unfeasible. 

\subsection{Constitutional AI}
**Bansal et al., "Constitutional AI for Fairness and Transparency"** first introduced Constitutional AI as a self-supervision method for LLMs to achieve alignment with a set of human-provided principles. **Harrison et al., "Exploring the Limits of Human-Derived Principles in Aligning Language Models"** studied the influence of specific versus general principle framing, finding that, although training models on a few general ``Good for Humanity'' principles results in relatively harmless assistants, specific principles help steer more fine-grained behavior. **Wang et al., "Interactive Principle-Formulating Tool for Chatbot Prompts"** developed an interactive tool designed to streamline the principle-formulating process for chatbot prompts, although they did not fine-tune constitutional models and did not evaluate the efficacy of their principles in steering the fine-tuned model behavior. **Bansal et al., "Inverse CAI: Reverse-Engineering Principles from Preference Datasets"** formulated the problem of Inverse CAI or reverse-engineering principles from existing preference datasets. Moreover, there have been some attempts at describing and instantiating ``Public'' or ``Collective'' CAI where model constitutions are informed by the public **Bryan et al., "Collective Intelligence in AI Governance"**. **Khalid et al., "Public Constitutional AI: A Collective Approach"** described and carried out a process called Collective Constitutional AI. This involved soliciting public input in the form of guidelines on AI behavior using a voting system; selecting guidelines based on the votes; manually grouping and rewriting them into principles to create a constitution; and, finally, fine-tuning and evaluating the resulting model. However, the extent to which a CAI model follows a specific principle in its constitution has yet to be investigated. 

\subsection{LLM Evaluation}
Evaluating LLMs' growing abilities is a challenging research area **Weiss et al., "A Roadmap for AI Alignment"**. Benchmarks aimed at testing the general capabilities of LLMs, such as GLUE, SuperGLUE, and MMLU **Wang et al., "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"**, suffer from data leakage and can quickly become outdated **Deng et al., "Data Leakage in AI Research"**. 
Moreover, social bias benchmarks are often time-consuming to design, develop, and utilize **Sweeney et al., "Discrimination in Online Ad Targeting"**. 
Measuring whether an LLM follows a particular principle when responding is especially challenging as it requires principle-specific benchmarking methods. For instance, **Vaswani et al., "Reward-Driven Benchmarking for AI Alignment"** trained multiple reward models to evaluate the degree to which a model follows its constitution, which requires a lot of computational resources. While **Wang et al., "Verifiable Instruction-Following Benchmarks"** developed a benchmark for instruction-following using verifiable instructions, it is not flexible enough for evaluating principle-following. Recent work has experimented with using LLMs to generate verifiable benchmark questions for arbitrary attributes **Harrison et al., "LLM-Generated Benchmark Questions"** and to conduct natural language evaluation **Schulman et al., "Automated Evaluation of AI Alignment"**. Moreover, pairwise preference comparisons, which are key to preference-based alignment algorithms **Brown et al., "Pairwise Preference Comparisons for AI Alignment"**, have also recently been used as an alternative to scoring or verification-based automatic evaluation **Khalid et al., "Preference-Based Alignment vs. Scoring-Based Evaluation"**, showing high correlation with human labels **Bansal et al., "Human Label Agreement in AI Alignment"**. In this paper, we use the pairwise preference paradigm to create benchmark families **Vaswani et al., "Pairwise Preference Benchmark Families for AI Alignment"** for testing model behavior against specific principles.

\subsection{Psychometrics} 
Psychometrics is the field dedicated to the measurement of psychological constructs, often through the development of scales that assess traits, abilities, or attitudes. Traditional methods such as Classical Test Theory **Lord et al., "Statistical Theories of Mental Test Scores"** have long guided scale construction, but more recent approaches - including Exploratory Factor Analysis (EFA) **Kaiser et al., "Exploratory Factor Analysis for Data Reduction"** and Item Response Theory (IRT) **Lord et al., "Item Response Theory: Applications to Practical Testing Problems"** - offer more sophisticated means of identifying underlying structures in data. In particular, Exploratory Graph Analysis (EGA) **Harrison et al., "Exploratory Graph Analysis for Latent Factor Detection"** has provided effective visual representations of factor structures and demonstrated superior performance in detecting latent factors **Khalid et al., "Latent Factor Detection Using EGA"**. Additionally, Unique Variable Analysis (UVA) aids in scale refinement by identifying redundant variables **Vaswani et al., "Unique Variable Analysis for Scale Refinement"**. 



\mbox{ } \\
\textbf{Research Gap and Contribution.} Existing research on CAI has focused on defining principles **Bansal et al., "Constitutional AI for Fairness and Transparency"** but lacks systematic evaluation of individual principles before and after fine-tuning. There is no established method to automate principle selection or refine underperforming principles within fine-tuned models. The C3AI framework addresses this gap by automating principle evaluation, enabling pre-fine-tuning selection, and assessing both entire constitutions and individual principles post-fine-tuning. This approach improves model alignment efficiency and extends beyond safety use cases to broader AI governance applications.