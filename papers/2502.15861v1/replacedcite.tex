\section{Related Work}
\subsection{LLM Alignment}
AI alignment broadly refers to guiding AI systems to adhere to human norms, objectives, and values ____. As generative models are becoming increasingly capable and self-sufficient, there is a pressing need ____ to ensure they are helpful without causing harm by, for instance, violating individual privacy ____, disseminating stereotypes ____, and making unsafe or illicit suggestions ____. Since potential harms are diverse, ____ suggests that it is most reasonable to align AI agents with human values  - as opposed to, for instance, having explicit instructions or implicit preferences - such that the agent's actions are guided by a notion of morality or what it should and should not do, as defined by humans either individually or collectively. An established psychological  Theory of Basic Human Values defines values as ``concepts or beliefs, [which] pertain to desirable end states or 
behaviors, transcend specific situations, guide selection or evaluation of 
behavior and events, and are ordered by relative importance'' ____. 

The issue is that achieving value alignment is difficult because of the inherent variation in the relative importance people place on different values, as well as their diverse social and political contexts ____. For instance, research finds that some LLMs disproportionately endorse opinions of certain social groups ____; for example, a Claude model  trained on Anthropic CAI was found to preferentially endorse Western views ____. Moreover, one could employ multiple philosophical and psychological theories of morality for AI alignment - such as Virtue Ethics, Utilitarianism, and Rights-based morality - each of which would give rise to very different AIs. Thus, there is a need for a fair process that would allow people to decide on AI values collectively ____.

From a technical perspective, alignment of LLMs to humans is predominantly done through preference fine-tuning ____, using algorithms such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Odds Ratio Preference Optimization (ORPO) ____. These techniques require pairwise preference datasets where each example has some user query and two potential responses, one of which is preferred over the other by annotators. However, as generative AI, and LLMs in particular, are gaining new capabilities quickly, there is a need for scalable oversight ____. One potential solution for this is allowing LLMs to self-supervise their alignment to a human-defined set of principles ____ when human supervision is too costly or unfeasible. 

\subsection{Constitutional AI}
____ first introduced Constitutional AI as a self-supervision method for LLMs to achieve alignment with a set of human-provided principles. ____ studied the influence of specific versus general principle framing, finding that, although training models on a few general ``Good for Humanity'' principles results in relatively harmless assistants, specific principles help steer more fine-grained behavior. ____ developed an interactive tool designed to streamline the principle-formulating process for chatbot prompts, although they did not fine-tune constitutional models and did not evaluate the efficacy of their principles in steering the fine-tuned model behavior. ____ formulated the problem of Inverse CAI or reverse-engineering principles from existing preference datasets. Moreover, there have been some attempts at describing and instantiating ``Public'' or ``Collective'' CAI where model constitutions are informed by the public ____. ____ described and carried out a process called Collective Constitutional AI. This involved soliciting public input in the form of guidelines on AI behavior using a voting system; selecting guidelines based on the votes; manually grouping and rewriting them into principles to create a constitution; and, finally, fine-tuning and evaluating the resulting model. However, the extent to which a CAI model follows a specific principle in its constitution has yet to be investigated. 

\subsection{LLM Evaluation}
Evaluating LLMs' growing abilities is a challenging research area ____. Benchmarks aimed at testing the general capabilities of LLMs, such as GLUE, SuperGLUE, and MMLU ____, suffer from data leakage and can quickly become outdated ____. 
Moreover, social bias benchmarks are often time-consuming to design, develop, and utilize ____. 
Measuring whether an LLM follows a particular principle when responding is especially challenging as it requires principle-specific benchmarking methods. For instance, ____ trained multiple reward models to evaluate the degree to which a model follows its constitution, which requires a lot of computational resources. While ____ developed a benchmark for instruction-following using verifiable instructions, it is not flexible enough for evaluating principle-following. Recent work has experimented with using LLMs to generate verifiable benchmark questions for arbitrary attributes ____ and to conduct natural language evaluation ____. Moreover, pairwise preference comparisons, which are key to preference-based alignment algorithms ____, have also recently been used as an alternative to scoring or verification-based automatic evaluation ____, showing high correlation with human labels ____. In this paper, we use the pairwise preference paradigm to create benchmark families ____ for testing model behavior against specific principles.

\subsection{Psychometrics} 
Psychometrics is the field dedicated to the measurement of psychological constructs, often through the development of scales that assess traits, abilities, or attitudes. Traditional methods such as Classical Test Theory ____ have long guided scale construction, but more recent approaches - including Exploratory Factor Analysis (EFA) ____ and Item Response Theory (IRT) ____ - offer more sophisticated means of identifying underlying structures in data. In particular, Exploratory Graph Analysis (EGA) ____ has provided effective visual representations of factor structures and demonstrated superior performance in detecting latent factors ____. Additionally, Unique Variable Analysis (UVA) aids in scale refinement by identifying redundant variables ____. 



\mbox{ } \\
\textbf{Research Gap and Contribution.} Existing research on CAI has focused on defining principles ____ but lacks systematic evaluation of individual principles before and after fine-tuning. There is no established method to automate principle selection or refine underperforming principles within fine-tuned models. The C3AI framework addresses this gap by automating principle evaluation, enabling pre-fine-tuning selection, and assessing both entire constitutions and individual principles post-fine-tuning. This approach improves model alignment efficiency and extends beyond safety use cases to broader AI governance applications.