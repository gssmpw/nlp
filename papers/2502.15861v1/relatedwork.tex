\section{Related Work}
\subsection{LLM Alignment}
AI alignment broadly refers to guiding AI systems to adhere to human norms, objectives, and values \cite{shen2024towards,ji2023ai}. As generative models are becoming increasingly capable and self-sufficient, there is a pressing need \cite{jobin2019global} to ensure they are helpful without causing harm by, for instance, violating individual privacy \cite{li2023privacy}, disseminating stereotypes \cite{abid2021persistent,hu2024}, and making unsafe or illicit suggestions \cite{ganguli2022red,deshpande2023toxicity,shevlane2023model}. Since potential harms are diverse, \citet{gabriel2020artificial} suggests that it is most reasonable to align AI agents with human values  - as opposed to, for instance, having explicit instructions or implicit preferences - such that the agent's actions are guided by a notion of morality or what it should and should not do, as defined by humans either individually or collectively. An established psychological  Theory of Basic Human Values defines values as ``concepts or beliefs, [which] pertain to desirable end states or 
behaviors, transcend specific situations, guide selection or evaluation of 
behavior and events, and are ordered by relative importance'' \cite{schwartz1992universals}. 

The issue is that achieving value alignment is difficult because of the inherent variation in the relative importance people place on different values, as well as their diverse social and political contexts \cite{kirk2024benefits,conitzer2024social,sorensen2024roadmap}. For instance, research finds that some LLMs disproportionately endorse opinions of certain social groups \cite{santurkar2023whose}; for example, a Claude model  trained on Anthropic CAI was found to preferentially endorse Western views \cite{durmus2023towards}. Moreover, one could employ multiple philosophical and psychological theories of morality for AI alignment - such as Virtue Ethics, Utilitarianism, and Rights-based morality - each of which would give rise to very different AIs. Thus, there is a need for a fair process that would allow people to decide on AI values collectively \cite{conitzer2024social}.

From a technical perspective, alignment of LLMs to humans is predominantly done through preference fine-tuning \cite{ouyang2022training,bai2022training,casper2023open}, using algorithms such as Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Odds Ratio Preference Optimization (ORPO) \cite{ouyang2022training,rafailov2024direct,hong-etal-2024-orpo}. These techniques require pairwise preference datasets where each example has some user query and two potential responses, one of which is preferred over the other by annotators. However, as generative AI, and LLMs in particular, are gaining new capabilities quickly, there is a need for scalable oversight \cite{bowman2022measuring}. One potential solution for this is allowing LLMs to self-supervise their alignment to a human-defined set of principles \cite{ganguli2023capacity, sun2023salmon,chen2024iteralign} when human supervision is too costly or unfeasible. 

\subsection{Constitutional AI}
\citet{bai2022constitutional} first introduced Constitutional AI as a self-supervision method for LLMs to achieve alignment with a set of human-provided principles. \citet{kundu2023specific} studied the influence of specific versus general principle framing, finding that, although training models on a few general ``Good for Humanity'' principles results in relatively harmless assistants, specific principles help steer more fine-grained behavior. \citet{petridis2024constitutionmaker} developed an interactive tool designed to streamline the principle-formulating process for chatbot prompts, although they did not fine-tune constitutional models and did not evaluate the efficacy of their principles in steering the fine-tuned model behavior. \citet{findeis2024inverse} formulated the problem of Inverse CAI or reverse-engineering principles from existing preference datasets. Moreover, there have been some attempts at describing and instantiating ``Public'' or ``Collective'' CAI where model constitutions are informed by the public \cite{abiri2024public,huang2024collective}. \citet{huang2024collective} described and carried out a process called Collective Constitutional AI. This involved soliciting public input in the form of guidelines on AI behavior using a voting system; selecting guidelines based on the votes; manually grouping and rewriting them into principles to create a constitution; and, finally, fine-tuning and evaluating the resulting model. However, the extent to which a CAI model follows a specific principle in its constitution has yet to be investigated. 

\subsection{LLM Evaluation}
Evaluating LLMs' growing abilities is a challenging research area \cite{bowman2021will,ganguli2023challenges, blodgett2021stereotyping,anwar2024foundational}. Benchmarks aimed at testing the general capabilities of LLMs, such as GLUE, SuperGLUE, and MMLU \cite{wang2018glue, wang2019superglue, hendrycks2020measuring}, suffer from data leakage and can quickly become outdated \cite{deng2023investigating}. 
Moreover, social bias benchmarks are often time-consuming to design, develop, and utilize \cite{blodgett2020language, parrish2021bbq,ganguli2023challenges,santurkar2023whose}. 
Measuring whether an LLM follows a particular principle when responding is especially challenging as it requires principle-specific benchmarking methods. For instance, \citet{kundu2023specific} trained multiple reward models to evaluate the degree to which a model follows its constitution, which requires a lot of computational resources. While \citet{zhou2023instruction} developed a benchmark for instruction-following using verifiable instructions, it is not flexible enough for evaluating principle-following. Recent work has experimented with using LLMs to generate verifiable benchmark questions for arbitrary attributes \cite{perez2023discovering} and to conduct natural language evaluation \cite{wang2023chatgpt,wang2023largelm}. Moreover, pairwise preference comparisons, which are key to preference-based alignment algorithms \cite{ouyang2022training}, have also recently been used as an alternative to scoring or verification-based automatic evaluation \cite{qin2023large,liu2024aligning,zhou2024fairer}, showing high correlation with human labels \cite{wang2023largelm}. In this paper, we use the pairwise preference paradigm to create benchmark families \cite{sorensen2024roadmap} for testing model behavior against specific principles.

\subsection{Psychometrics} 
Psychometrics is the field dedicated to the measurement of psychological constructs, often through the development of scales that assess traits, abilities, or attitudes. Traditional methods such as Classical Test Theory \cite{devellis2006classical} have long guided scale construction, but more recent approaches - including Exploratory Factor Analysis (EFA) \cite{fabrigar2012exploratory} and Item Response Theory (IRT) \cite{embretson2013item} - offer more sophisticated means of identifying underlying structures in data. In particular, Exploratory Graph Analysis (EGA) \cite{ega1} has provided effective visual representations of factor structures and demonstrated superior performance in detecting latent factors \cite{ega2,golino2022exploratory}. Additionally, Unique Variable Analysis (UVA) aids in scale refinement by identifying redundant variables \cite{uva}. 



\mbox{ } \\
\textbf{Research Gap and Contribution.} Existing research on CAI has focused on defining principles \cite{bai2022constitutional, petridis2024constitutionmaker, findeis2024inverse} but lacks systematic evaluation of individual principles before and after fine-tuning. There is no established method to automate principle selection or refine underperforming principles within fine-tuned models. The C3AI framework addresses this gap by automating principle evaluation, enabling pre-fine-tuning selection, and assessing both entire constitutions and individual principles post-fine-tuning. This approach improves model alignment efficiency and extends beyond safety use cases to broader AI governance applications.