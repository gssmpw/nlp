\section{Conclusion}

In this paper, we propose TD3 which distills a large discrete sequential recommendation dataset into an informative synthetic summary, which is decomposed into four factors inspired by Tucker decomposition in latent space. TD3 offers several advantages, including the decoupling of factors that influence the size of \syn, thereby reducing data dimensionality, computational costs, and storage complexity while preserving essential feature information. Additionally, we introduce an enhanced bi-level optimization approach featuring an augmented learner training strategy in the \emph{inner-loop}, ensuring the learner deeply fits the summary and a feature-space alignment surrogate objective in the \emph{outer-loop}, ensuring optimal learning of synthetic data parameters. Experiments and analyses confirm the effectiveness and necessity of the proposed designs.

While our work offers significant advantages, it does have certain limitations, notably the computational demands of the bi-level optimization process, which still can be challenging for scaling with larger models and datasets. However, this cost is often offset in scenarios where multiple models need training on the same dataset, substantially reducing training time in the future. Distillation becomes a one-time cost with long-term benefits for various tasks. In future work, we aim to develop a more time-efficient dataset distillation method that scales to larger datasets without sacrificing performance. 
Additionally, We also intend to use dataset distillation for cross-domain knowledge transfer, allowing the information from one domain to be reused in other domains, thereby enhancing the framework's versatility in different recommendation contexts.

% In this paper, we propose TD3, which distills a large discrete sequential recommendation dataset into a synthetic summary decomposed into four factors inspired by Tucker decomposition. TD3 reduces data dimensionality, computational costs, and storage complexity while preserving essential features. We introduce an enhanced bi-level optimization approach with an augmented learner training strategy in the \emph{inner-loop} for deep fitting and a feature-space alignment surrogate objective in the \emph{outer-loop} for optimal synthetic data learning. Experiments confirm the effectiveness of these designs. 
% While our work offers significant advantages, it does have certain limitations, notably the computational demands of the bi-level optimization process, which still can be challenging for scaling with larger models and datasets. However, this cost is often offset in scenarios where multiple models need training on the same dataset, substantially reducing training time in the future. Distillation becomes a one-time cost with long-term benefits for various tasks. In future work, we aim to develop a more time-efficient dataset distillation method that scales to larger datasets without sacrificing performance. 
% Additionally, We also intend to use dataset distillation for cross-domain knowledge transfer, allowing the information from one domain to be reused in other domains, thereby enhancing the framework's versatility in different recommendation contexts.
