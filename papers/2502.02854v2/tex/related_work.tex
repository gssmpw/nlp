\vspace{-0.5cm}

\section{Related Work} \label{sec:related_work}


\paragraph{Sequential Recommendation (SR)} aims to leverage historical sequences to better capture current user intent~\cite{quadrana2018sequence}. In recent years, there has been rapid advancement in \textbf{model-centric} SR approaches, focusing from Markov chains~\cite{he2016fusing} and factorization~\cite{rendle2010factorizing} to RNNs~\cite{GRU4Rec, narm}, CNNs~\cite{Caser, CosRec}, and GNNs~\cite{SRGNN, GCSAN, GCEGNN}. Models like SASRec~\cite{SASRec} and BERT4Rec~\cite{bert4rec} leverage self-attention mechanisms to learn the influence of each interaction on target behaviors.  Recently, the emergence of LLMs has further enriched SR model design, for instance, ~\cite{harte2023leveraging, yang2024sequential} leverage LLMs to uncover latent relationships~\cite{guo2024scaling}, while RecFormer~\cite{li2023text} represents items as item "sentences", and SAID~\cite{hu2024enhancing} employs LLMs to learn semantically aligned item ID embeddings. With the emergence of the concept of \textbf{data-centric} recommendation, more works shift the focus to recommendation data enhancement. DR4SR~\cite{yin2024dataset} proposes to regenerate an informative and generalizable training dataset for sequential recommendations. FMLP-Rec~\cite{zhou2022filter} and HSD~\cite{zhang2022hierarchical} adopt learnable filters for data denoising. DiffuASR~\cite{liu2023diffusion} proposes a diffusion augmentation for higher quality data generation. ASReP~\cite{ASReP} focused on generating fabricated data for long-tailed sequences.


\begin{figure}[t!] \centering
    \centering
    \includegraphics[width=\linewidth]{figure/tucker.pdf}
    \vspace{-0.2cm}
    \caption{An illustration of Tucker decomposition. The left part shows a three-dimensional synthetic sequence summary, with the third dimension representing the probability distribution over the entire item set. The right part illustrates the tucker decomposition, composed of a core tensor and factor matrices. The user tensor, temporal tensor, and core tensor are the parameters to be learned, while the item tensor shares values with the trained item embedding table.}
    \label{fig:tucker}
    \vspace{-0.4cm}
\end{figure}


\paragraph{Dataset Distillation (DD)} compresses large training datasets into smaller ones while preserving similar performance~\cite{maekawa2024dilm, gu2024efficient}. There have been several lines of methods, that prioritize different aspects of information. \textbf{Performance matching} based methods focus on optimizing loss at the final training stage. For example, Farzi~\cite{sachdeva2023farzi} distills auto-regressive data in latent space to produce a latent data summary and a decoder, although its parameters scale linearly with the vocabulary size and sequence length. $\infty$-AE~\cite{sachdeva2022infinite} uses neural tangent kernels (NTKs) to approximate an infinitely wide autoencoder and synthesizes fake users through sampling-based reconstruction, while DConRec~\cite{wu2023dataset} distills synthetic datasets by sampling user-item pairs from a learnable probabilistic matrix, both tailored for collaborative filtering data in the form of user-item-rating triples. Another line of research focuses on \textbf{data matching}, encouraging synthetic data to replicate the behavior of target data. CGM~\cite{wang2023gradient}, following the gradient matching paradigm~\cite{zhao2020dataset} that mimics the influence on model parameters by matching the gradients of the target and synthetic data in each iteration, optimizes a new form of synthetic data rather than condensing discrete one- or multi-hot data in CTR scenarios. Furthermore, TF-DCon~\cite{wu2023leveraging} utilizes large language models (LLMs) to condense item and user content for content-based recommendations, although this approach is hardly applicable to the contexts of ID-based sequential recommendation.



\paragraph{Tucker Decomposition (TD)} decomposes a tensor into a set of factor matrices and a smaller core tensor~\cite{tucker1964extension}. It can be viewed as a kind of principal component analysis approach for high-order tensors. In particular, when the super-diagonal elements in the core tensor of tucker equal 1 and other elements equal 0, tucker decomposition degrades into canonical decomposition~\cite{xiao2023tucker}. In three-mode case, A tucker decomposition of a tensor $X \in \mathbb{R}^{I_1\times I_2\times I_3}$ is:
\begin{equation}    
\boldsymbol{X}=\mathcal{G} \times_{1} \mathbf{A}^{(1)} \times_{2} \mathbf{A}^{(2)} \times_{3} \mathbf{A}^{(3)}=: \llbracket \mathcal{G} ; \mathbf{A}^{(1)}, \mathbf{A}^{(2)}, \mathbf{A}^{(3)} \rrbracket,
\end{equation}
where $\times_n$ indicating the tensor product along the n-th mode, each $A^{(n)} \in \mathbb{R}^{I_n \times R_n}$ is called the \textit{factor matrix}, and $\mathcal{G} \in \mathbb{R}^{R_1 \times R_2 \times R_3}$ is the \textit{core tensor}, show the level of interaction between all factors.







