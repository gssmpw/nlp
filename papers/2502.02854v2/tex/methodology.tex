\section{Methodology}
\vspace{1.5mm}

This section introduces TD3, which distills discrete and complex sequential recommendation datasets into fully expressive synthetic sequence summaries in latent space. 


\subsection{Overview}

\paragraph{Notation.}
Suppose we are given a large training dataset $\real \triangleq \{ \sentence_i \}_{i=1}^{ |\real| }$, where $\sentence_i \triangleq [x_{ij} \in \vocab]_{j=1}^{ |\sentence_i| }$ is an ordered sequence of items, with each item $x_{ij}$ belonging to the set of all possible items $\vocab$. We denote the user set of the training dataset as $\mathcal{U}$, where $|\mathcal{U}| = |\real|$. Our goal is to learn a differentiable function $\Phi_\theta$ (\ie SASRec) with parameters $\theta$, which predicts the next item $x_{i+1}$ given the previous sequence $x_{1:i}$. The parameters of this function are optimized by minimizing an empirical loss over the training set~:
\begin{equation} \label{eq2}
\begin{gathered}
    \theta^{\real} = \argmin{\theta} \mathcal{L}^{\real}(\theta) ~~, \\
    \mathcal{L}^{\real}(\theta) \triangleq \expectation{~\sentence \sim \real,~ x_i \sim \sentence}{\ell^{\real}(\Phi_{\theta}(\sentence_{1:i}),~ x_{i+1})} ~~,
\end{gathered}
\end{equation}
where function $\ell^{\real}(\cdot~,\cdot)$ represents the next item prediction loss, and $\theta^{\real}$ is the minimizer of $\mathcal{L}^{\real}$, which reflects the generalization performance of the model $\Phi_{\theta^{\real}}$. 
Our objective is to generate a small set of condensed synthetic sequence summary $\syn \in \mathbb{R}^{\mu \times \zeta \times |\vocab|}$ consisting of $\mu$ fake sequences of maximum length $\zeta$ and $|\syn| \ll |\real|$. Similar to \cref{eq2}, once the condensed set is learned, the parameters $\theta$ can be trained on this set as follows~:
\begin{equation} \label{eq3}
\begin{gathered}
    \theta^{\syn} = \argmin{\theta} \mathcal{L}^{\syn}(\theta) ~~, \\
    \mathcal{L}^{\syn}(\theta) \triangleq \expectation{~\summary \sim \syn,~ \Tilde{x}_i \sim \summary}{\ell^{\syn}(\Phi_{\theta}(\psi(\summary_{1:i}~, \embed)),~ \Tilde{x}_{i+1})} ~~,
\end{gathered}
\end{equation}
where $\summary_i \triangleq [~ \syn[i, j, :] ~]_{j=1}^{\zeta}$, $\ell^{\syn}(\cdot~,\cdot)$ measures the distance between probability distributions, $\psi(\cdot~,\cdot)$ is matrix product, $\mathbf{E}$ is the item embedding table and $\mathcal{L}^{\syn}$ is the generalization performance of $\Phi_{\theta^{\syn}}$. 
We wish the generalization performance of $\Phi_{\theta^{\syn}}$ to be close to $\Phi_{\theta^{\real}}$:
\begin{equation}
    \begin{aligned}
        & \expectation{~\sentence \sim \real,~ x_i \sim \sentence}{\ell^{\real}(\Phi_{\theta}(\sentence_{1:i}),~ x_{i+1})} \\ 
        \simeq \quad & \expectation{~\summary \sim \syn,~ \Tilde{x}_i \sim \summary}{\ell^{S}(\Phi_{\theta}(\psi(\summary_{1:i}~, \embed)),~ \Tilde{x}_{i+1})} ~~.
    \end{aligned}
\end{equation}
As the synthetic set $\syn$ is significantly smaller, we expect the optimization in \cref{eq3} to be significantly faster than in \cref{eq2} ~~. 



\begin{figure*}[t!] \centering
    \centering
    \includegraphics[width=\linewidth]{figure/framework.pdf}
    \vspace{-0.6cm}
    \caption{Illustration of TD3. In step 1, the learner is trained to get the best checkpoint for feature space alignment and item embedding for the \emph{shared item latent factor}, and checkpoints from early epochs are saved to form a pool for initialization in each \emph{outer-loop}. Step 2 visualizes a single \emph{outer-loop} step, where the meta-gradient is computed from both the test loss and the feature space alignment loss. Step 3 demonstrates that the distilled summary enables training of other networks with similar performance to models trained on real data, while significantly reducing training time and memory usage.}
    \label{fig:framework}
    \vspace{-0.3cm}
\end{figure*}



\paragraph{Problem.}

The objective of achieving comparable generalization performance by training on synthetic data can be formulated in an alternative way. As proposed in \cite{wang2018dataset}, this can be framed as a meta-learning problem using bi-level optimization. In this approach, the \emph{inner-loop} trains the learner models on synthetic data, while the \emph{outer-loop} evaluates its quality using $\ell^{\real}(\cdot~,\cdot)$ on the original dataset, updating the synthetic summary via gradient descent. More formally, the bi-level optimization problem can be expressed as~:
\begin{equation} \label{eq4}
\begin{gathered}
    \syn^{*} = \underset{\theta_0 \sim \Theta}{\mathbb{E}}[\mathcal{L}^{\real}(\theta^*)] ~~, \\
    \text{s.t.} \quad \theta^* = \argmin{\theta} \mathcal{L}^{\syn}(\theta ~|~ \theta_0) ~~.
\end{gathered}
\end{equation}
The primary approach for addressing bi-level optimization problems is \emph{truncated backpropagation through time} (T-BPTT)~\cite{williams1990efficient, puskorius1994truncated} in reverse mode. When the \emph{inner-loop} learner updated uses gradient descent with a learning rate $\eta$, the meta-gradient with respect to the distilled sequence summary is obtained as follows~:
\begin{equation}
    \mathcal{G} = - \eta \frac{\partial\mathcal{L}^{\syn}(\theta_{T})}{\partial\theta} \sum_{i=T-M}^{T-1} \Pi_{j=i+1}^{T-1} \left[1 - \eta \frac{\partial^{2}\mathcal{L}^{\syn}(\theta_{j})}{\partial\theta^{2}}\right] \frac{\partial^{2}\mathcal{L}^{\syn}(\theta_{i})}{\partial\theta \partial\syn} ~~,
\end{equation}
where $T$ represents the total optimization and unrolling steps that we perform in \emph{inner-step} with loss $\mathcal{L}^{S}(\theta)$, but T-BPTT only propagates backward through a smaller window of $M$ steps.



\subsection{Synthetic Summary Decomposition}


The discrete nature of user-item interaction records in sequential recommendation data complicates the direct use of gradient methods to distill an informative summary in the same format as the original data. Inspired by prior research \cite{sachdeva2023farzi, li2021data, maekawa2023dataset}, we choose to distill in the latent space. Consequently, we define the synthetic sequence summary as $\syn \in \mathbb{R}^{\mu \times \zeta \times |\mathcal{V}|}$, a three-dimensional probability tensor that contains $\mu$ synthetic users, each with up to $\zeta$ interaction records. The third dimension of $\syn$ represents the size of the entire item set $|\mathcal{V}|$, where $\syn_{ij:}$ captures the interaction information of synthetic user $i$ at position $j$ by synthesizing the total original item information, with each item weighted differently, ensuring that the summary preserves the critical points.

Considering that the size of S is $\mu \times \zeta \times |\mathcal{V}|$, an increase in any of its dimensions leads to substantial growth in the overall tensor size, thereby escalating computational and storage requirements, especially when the original item set is large. Inspired by Tucker decomposition, $\syn$ can be decomposed into the products of several smaller factor tensors and a core tensor, where the dimension of each sub-tensor is determined by only a single dimension, as visualized in \cref{fig:tucker} and formalized as follows:
\begin{equation} \label{eq7}
    \begin{gathered}
        \syn = \boldsymbol{G} \times_{1} \mathbf{U} \times_{2} \mathbf{T} \times_{3} \mathbf{V} =: ~\llbracket \boldsymbol{G} ~;~ \mathbf{U}, \mathbf{T}, \mathbf{V} \rrbracket ~~,
    \end{gathered}
\end{equation}
where $\mathbf{U} \in \mathbb{R}^{\mu \times d_1}$, $\mathbf{T} \in \mathbb{R}^{\zeta \times d_2}$, $\mathbf{V} \in \mathbb{R}^{|\mathcal{V}| \times d_3}$, $\mathbf{G} \in \mathbb{R}^{d_1 \times d_2 \times d_3}$, with $\times_n$ indicating the tensor product along the n-th mode and $d_n \ll |\vocab|$. Empirically, $d_1 = d_2$. More importantly, $\mathbf{V}$ is shared with the trained item embedding table, with no parameters needed to be trained. 

The advantage of this decomposition lies in its ability to decouple the factors influencing the size of $\syn$, thereby reducing data dimensionality as well as computational and storage complexity while preserving key feature information. This approach enables more efficient processing of high-dimensional data and enhances the understanding of its structure and characteristics.





\subsection{Enhanced Bi-Level Optimization}

Existing methods are computationally expensive for generating synthetic datasets with satisfactory generalizability, as optimizing synthetic data requires differently initialized networks~\cite{yu2023dataset}. To accelerate dataset distillation, we propose 1) \emph{augmented learner training}, which enables the learner model to effectively fit the synthetic summary, supporting precise and comprehensive updates of synthetic data. Additionally, as mentioned in~\cite{wu2018understanding, metz2019understanding}, bias and poorly conditioned loss landscapes arise from truncated unrolling in TBPTT, we further propose 2) \emph{feature space alignment} which aligns feature spaces from models trained on both original and synthetic data, combined with 3) \emph{random truncated backpropagation through time} (RaT-BPTT) proposed by ~\cite{feng2023embarrassingly} to reduce bias in TBPTT and create a more favorable loss landscape for optimization.

\subsubsection{Augmented Learner Training.}

As shown in \cref{eq7}, we define the synthetic sequences summary as a three-dimensional probabilistic tensor obtained from the factors of the tucker decomposition and a core matrix via the mode product operation: $\syn \in \mathbb{R}^{\mu \times \zeta \times |\vocab|}$. 
Under this settings, we use Kullback-Leibler (KL) Divergence as the loss function in~\cref{eq3}, and the inner objective is defined as:
\begin{equation}
    \begin{aligned}
    \mathbf{x} &= \syn[~:~,~:~\zeta~,~:~] ~~, \\
    \mathbf{y} &= \syn[~:~,~~\zeta~~,~:~] ~~, \\
    \hat{\mathbf{y}} &= \operatorname{Softmax}(\Phi_{\theta^{\syn}_j}(\psi(\mathbf{x}~, \embed))) ~~, \\
    \ell^{\syn}(\mathbf{y},~ \hat{\mathbf{y}}) &= \mathcal{D}_{KL}(\mathbf{y} ~~||~~ \hat{\mathbf{y}}) = \sum_{i=1}^{|\vocab|} \mathbf{y}_i \log \frac{\mathbf{y}_i}{\hat{\mathbf{y}}_i} ~~,
    \end{aligned}
\end{equation}
where $\mathbf{x}$ is the input, $\mathbf{y}$ is the target, $\Phi_{\theta^{\syn}_j}(\cdot)$ is the learner model trained on synthetic data in $j$-th step, $\hat{\mathbf{y}}$ is output of the learner model and $\mathcal{D}_{KL}(\cdot ~||~ \cdot)$ is the discrete pointwise KL-divergence.

However, this approach only uses the previous $1$ to $\zeta - 1$ interactions to predict the $\zeta$-th interaction probability. We propose further enhancing the prediction by randomly sampling middle positions, which can significantly improve the diversity of training, strengthen contextual understanding, enhance the model's generalization ability, and reduce reliance on specific sequence patterns. This strategy helps the model capture sequence information more comprehensively, improving its practical application performance.


\vspace{0.1cm}
\subsubsection{Feature Space Alignment.}


We propose an enhancement to the current \emph{outer-loop} test accuracy objective by integrating a metric that ensures the alignment of feature spaces between models trained on the original dataset and those trained on a synthetic sequence summary. As highlighted in~\cite{lei2023comprehensive}, the conventional meta-learning framework is predominantly concerned with equating the performance of models trained on original data with those trained on synthetic data. However, from the perspective of loss surfaces, this strategy primarily attempts to replicate the local minima of the target data through distilled data~\cite{li2018visualizing}. Despite its initial efficacy, this method faces significant challenges due to the presence of poorly conditioned loss landscapes~\cite{sachdeva2023data}. In light of these limitations, our goal is to optimize the synthetic data \syn such that the learner $\Phi_{\theta^{\syn}}$ trained on them achieves not only comparable generalization performance to $\Phi_{\theta^{\real}}$ but also converges to a similar solution in the feature space. The enhanced objective can be formulated as:
\begin{equation}
\begin{gathered}
    \syn^{*} = \argmin{\syn} \underset{\theta_0 \sim \Theta}{\mathbb{E}}[\mathcal{L}^{\real}(\theta^*) + \mathcal{L}^{\mathcal{F}}(\theta^*)] ~~, \\
    \text{s.t.} \quad \mathcal{L}^{\mathcal{F}}(\syn, \real; \theta^*) \triangleq \frac{1}{2} ~\Vert~ \Phi_{\theta^{\real}}(\real) - \Phi_{\theta^{*}}(\real) ~\Vert_{F}^{2} ~~, \\
    ~~~\qquad \mathcal{L}^{\real}(\theta) \triangleq \expectation{~\sentence \sim \real,~ x_i \sim \sentence}{\ell^{\real}(\Phi_{\theta^{*}}(\sentence_{1:i}),~ x_{i+1})} ~~, \\
    \quad \theta^* = \argmin{\theta} \mathcal{L}^{\syn}(\theta ~|~ \theta_0) ~~,
\end{gathered}
\end{equation}
where $\mathcal{L}^{\mathcal{F}}(\theta^*)$ is the feature space alignment loss with a mean squared error (MSE) by optimizing synthetic summary $\syn$ directly.
Unlike the most basic meta-learning framework, by ensuring that the feature representations of the synthetic data trained model are closely aligned with those of the original data trained model, we aim to foster a more robust and reliable learning process. This approach not only enhances the model's ability to generalize but also improves its stability across diverse datasets, thereby addressing the inherent challenges posed by poorly conditioned loss landscapes. Through this enhancement, we seek to advance the field of meta-learning, paving the way for more effective and efficient training paradigms, going beyond simply matching performance metrics.


\vspace{0.1cm}
\subsubsection{Random Truncated Backpropagation Through Time}

\input{tex/algorithm}

Training neural networks on distilled data is challenging largely due to the pronounced non-convexity of the optimization process. One common approach to capture long-term dependencies in this context is Backpropagation Through Time (BPTT), although it suffers from slow optimization and excessive memory demands. TBPTT, which limits unrolled steps, is a more efficient alternative. Yet, TBPTT introduces its drawbacks, such as bias from the truncation~\cite{wu2018understanding} and poorly conditioned loss landscapes, especially with long unrolls~\cite{vicol2021unbiased}. To address these issues, \cite{feng2023embarrassingly} propose the Random Truncated Backpropagation Through Time (RaT-BPTT) method, which combines randomization with truncation in BPTT. This approach unrolls within a randomly anchored and fixed-size window along the training trajectory and aggregates gradients within this window. The random window ensures that the RaT-BPTT gradient serves as a random subsample of the full BPTT gradient, covering the entire trajectory, while the truncated window improves gradient stability and reduces memory usage. As a result, RaT-BPTT enables faster training and better performance. It can be formulated as follows:
\begin{equation}
    \mathcal{G} = - \eta \frac{\partial\mathcal{L}^{\syn}(\theta_{T})}{\partial\theta} \sum_{i=M-W}^{M-1} \Pi_{j=i+1}^{M-1} \left[1 - \eta \frac{\partial^{2}\mathcal{L}^{\syn}(\theta_{j})}{\partial\theta^{2}}\right] \frac{\partial^{2}\mathcal{L}^{\syn}(\theta_{i})}{\partial\theta \partial\syn} ~~,
\end{equation}
where $M$ is the random number of total unrolled steps in the \emph{inner-loop}, and $W$ represents the number of steps included in the backward, with only the final $W$ steps being used for backpropagation.

Previous studies have demonstrated that diverse models in inner optimization improve robustness of the synthetic data~\cite{zhao2023dataset, cazenavette2022dataset}. Moreover, pretrained models significantly enhance dataset distillation by providing better initialization, faster convergence, and higher-quality synthetic data~\cite{lu2023can, sachdeva2023farzi}. Building on these insights, we maintain a pool of pretrained learner models from early training epochs, capturing various stages of learning. At each outer step of the distillation process, we randomly select models from this pool to ensure that the training signal remains diverse and representative for optimizing the synthetic dataset. The comprehensive training procedure for our proposed method is detailed in Algorithm~\ref{alg:alg}. 
