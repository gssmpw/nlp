\section{Introduction} \label{sec:intro}

\vspace{0.2mm}

\begin{figure}[t!] \centering
    \centering
    \includegraphics[width=\linewidth]{figure/data-centric.pdf}
    \vspace{-0.5cm}
    \caption{Comparison of the data-centric recommender system through the lens of dataset distillation approach with the traditional model-centric recommender system. The key difference lies in their distinct optimization objectives.}
    \label{fig:data-centric}
    \vspace{-0.4cm}
\end{figure}

To address the persistent challenge of information overload from the Internet, Recommendation Systems (RS) have become essential tools by suggesting personalized content to users~\cite{xu2024multi,wu2023survey,han2023guesr,wang2021hypersorec,zhang2024unified,yin2023apgl4sr}. Among them, Sequential Recommendation Systems (SRS) capture user preferences in the evolving form through chronological interaction sequences~\cite{zheng2024enhanced, end4rec, sequential_recommendation_survey1, SASRec, xie2024breaking, shen2024predictive}. However, as recommendation models become increasingly complex, the primary constraint affecting recommendation performance gradually shifts towards the quantity and quality of recommendation data~\cite{lai2024survey}, leading to the emergence of data-centric recommendations, as shown in ~\cref{fig:data-centric}.
Moreover, several emerging AI companies have prioritized data for its numerous benefits, including improved accuracy, faster deployment, and standardized workflows. These collective initiatives in academia and industry highlight the growing recognition of data-centric approaches as essential for innovation~\cite{landingai, scaleai, snorkelai, li2024configure}.


Several initiatives have already been dedicated to the data-centric movement. A notable work launched by~\cite{yin2024dataset} aims to acquire an informative and generalizable training dataset for sequential recommender systems and asks the participants to iterate on the sequential recommendation dataset regeneration mostly focusing on improving the data quality. Another separate line is dataset distillation (DD)~\cite{wang2018dataset}, which focuses on both data quality and data quantity. Unlike heuristic data pruning methods that directly select data points from original datasets, DD methods are designed to generate novel data points and have emerged as a solution for creating high-quality and informative data summaries. The utility of DD approaches has been witnessed in several fields, including federated learning~\cite{huang2024overcoming, wang2024aggregation, xiong2023feddm, liu2022meta}, continual learning~\cite{masarczyk2020reducing, gu2023summarizing, yang2023efficient, zhang2024learning}, graph neural network~\cite{zhang2024navigating, feng2023fair, gupta2023mirage, yang2024does} and recommender systems~\cite{sachdeva2023farzi, sachdeva2022infinite, wang2023gradient}.


Significant progress has been made in DD for non-sequential recommender systems~\cite{sachdeva2022infinite, wang2023gradient, wu2023leveraging, wu2023dataset}. Methods like $\infty$-AE~\cite{sachdeva2022infinite} and DConRec~\cite{wu2023dataset} distill user-item interaction matrices, while CGM~\cite{wang2023gradient} condenses categorical recommendation data in click-through rate (CTR) scenarios. Additionally, TF-DCon~\cite{wu2023leveraging} employs large language models (LLMs) to condense user and item content for content-based recommendations. However, applying DD to sequential recommendation systems presents challenges due to several inherent complexities.
(1) \textbf{Maintaining sequential correlations}:
User-item interactions are sequentially correlated, reflecting the dynamic evolution of user preferences. Existing DD methods generate multiple synthetic interactions independently. Although it is possible to trivially organize these interactions into a sequence, this fails to capture the sequential correlations essential for modeling user behavior over time.
(2) \textbf{Optimization dilemma}:
In DD, the distilled dataset is typically parameterized as a learnable matrix, enabling fully differentiable distillation through a bi-level optimization process~\cite{dempe2020bilevel}. In sequential settings, this optimization becomes more difficult because the parameterized dataset size increases with sequence length. The enlarged parameter space further exacerbates convergence issues in the bi-level optimization process.


To address these challenges, we introduce TD3
to efficiently reduce computational complexity and extract streamlined latent patterns by decomposing the summary into four components: (1) \textit{Synthetic User Latent Factor} ($\mathbf{U}$), which represents synthetic user representations; (2) \textit{Temporal Dynamics Latent Factor} ($\mathbf{T}$), which captures temporal contextual information; (3) \textit{Shared Item Latent Factor} ($\mathbf{V}$), which characterizes items within the set and aligns with the item embedding table; and (4) \textit{Relation Core} ($\mathbf{G}$), which models the interrelationships among the factors. After decomposition, each factor is represented as a two-dimensional tensor, with its size determined by the sequence number, maximum sequence length, and item set size. Additionally, component $\mathbf{V}$ shared with the item embedding table does not require learning during distillation, making TD3 suitable for large item sets and long sequences. To address the final challenge, we propose an enhanced bi-level optimization objective to align feature spaces from models trained on both original and synthetic data. During \emph{inner-loop} training, an augmentation technique allows the learner model to deeply fit the synthetic summary, ensuring accurate updates of it in the \emph{outer-loop}. This approach accelerates convergence and, in conjunction with RaT-BPTT~\cite{feng2023embarrassingly}, minimizes computational costs while ensuring effective distillation. The contributions are concretely summarized:

\vspace{-1pt}
\begin{itemize}[topsep=4pt, itemsep=2pt, leftmargin=0.5cm]
    \item We study a novel problem in sequential recommendation: distilling a compressed yet informative synthetic sequence summary that retains essential information from the original dataset.
    \item We introduce TD3, which employs Tucker decomposition to separate the factors influencing the size of the synthetic summary, thereby reducing computational and storage complexity.
    \item Augmented learner training in \emph{inner-loop} ensures precise synthetic data updates and feature space alignment loss is proposed beyond the na\"ive bi-level optimization objective for a better loss landscape to optimize while minimizing computational costs and preserving long dependencies through RaT-BPTT.
    \item Empirical studies on public datasets have confirmed the superiority and cross-architecture generalizability of TD3's designs.
\end{itemize}
