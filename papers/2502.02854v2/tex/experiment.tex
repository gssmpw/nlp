\input{table/dataset}
\vspace{-0.3cm}

\section{Experiments}

In this section, we present and analyze the experiments on four public datasets, aiming to 

\subsection{Settings}

\paragraph{Training Datasets.}

To evaluate the distillation method proposed in this paper, we conduct experiments on four commonly used and publicly available datasets with variable statistics in ~\cref{tab:data_stats}.
\begin{enumerate}[label=\arabic{enumi}), leftmargin=0.5cm]  %
    \item \emph{Amazon}\footnote{\url{http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/}} includes Amazon product reviews and metadata infomation. For our empirical study, we mainly focus on the categories of "Magazine\_Subscriptions".
    \item \emph{MovieLens}\footnote{\url{https://grouplens.org/datasets/movielens/}} is maintained by GroupLens and it contains movie ratings from the MovieLens recommendation service. We use the "ml-100k" and "ml-1m" versions for our experiments.
    \item \emph{Epinions}\footnote{\url{https://cseweb.ucsd.edu/~jmcauley/datasets.html\#social_data}} was collected by~\cite{zhao2014leveraging} from Epinions.com, a popular online consumer review website. It describes consumer reviews and also contains trust relationships amongst users and spans more than a decade, from January 2001 to November 2013.
\end{enumerate}

\paragraph{Evaluation Metrics.}

We adopt the leave-one-out strategy for evaluation, following prior research~\cite{devlin2018bert, apgl4sr, yin2024dataset}. For each sequence, the most recent interaction is used for testing, the second for validation, and the rest for training. To expedite evaluation, as in previous studies~\cite{SASRec}, we randomly sample 100 negative items to rank with the ground-truth item, which closely approximates full-ranking results while significantly reducing the computational cost. We assess performance using HR, NDCG, and MRR. HR@k checks if the target item appears within the top-k recommendations, NDCG@k considers the item's rank, and MRR@k computes the average reciprocal rank of the first relevant item, with k $\in \{5, 10, 20\}$.

\paragraph{Implementation Details.}

We implement TD3 using PyTorch and develop recommendation models based on the library of Recbole~\cite{recbole}. Throughout the distillation process, we use SASRec~\cite{SASRec} serves as the learner across all datasets, utilizing attention heads $\in \{1, 2\}$, layers $\in \{1, 2\}$, hidden size $\in \{64, 128\}$, inner size $\in \{64, 128, 256\}$, and attention dropout probability
of 0.5 and hidden dropout probability of 0.2. For magazine and epinions dataset, we set $d1,d2\in\{8,16\}$, while for ml-100k and ml-1m dataset, we set $d1,d2\in\{16,32,64\}$. To evaluate the cross-architecture generalization of the proposed TD3, we employ GRU4Rec~\cite{GRU4Rec}, BERT4Rec~\cite{bert4rec}, and NARM~\cite{narm} for performance assessment. For all distilled datasets, we apply the Adam optimizer~\cite{diederik2014adam} for both the \emph{inner-loop} and \emph{outer-loop} optimization. 

In the \emph{outer-loop}, the synthetic sequence summary optimizer is configured with a learning rate $\alpha \in \{0.01, 0.03\}$ and a weight decay of 0.0001, using a cosine scheduler to adjust the learning rate throughout the process. In the \emph{inner-loop}, the learner optimizer employs a learning rate $\eta \in \{0.003, 0.005, 0.01\}$, with a weight decay of 0.00005. The inner steps are set to 200, using a random truncated window of 40 for backpropagation through time in RaT-BPTT, implemented via the Higher~\cite{higher} package across all datasets.


\input{table/baseline}

\subsection{Overall Performance}
\vspace{1mm}

\input{table/overall}

We evaluated TD3's performance across various synthetic sequence summary sizes and diverse datasets. In \cref{tab:overall_results}, we compare models trained on full original datasets with those using various-sized synthetic sequence summaries. Additionally, \cref{tab:baselines} and \cref{fig:baseline_results} compare TD3's performance with Farzi and heuristic sampling methods: \emph{random sampling}, which selects sequences uniformly, and \emph{longest sampling}, which selects sequences in descending order of length. Our findings are as follows: 1) TD3 achieves comparable training performance, even with substantial data compression. This shows that small-batch synthetic summaries distilled from the original dataset effectively capture essential information, preserving data integrity for model training. 2) In datasets such as Magazine and Epinions, models trained on TD3-distilled summaries outperform those trained on the original datasets. This highlights the value of high-quality, smaller datasets over larger, noisier ones, underscoring the importance of data quality in model training. 3) As illustrated in \cref{tab:baselines} and \cref{fig:baseline_results}, TD3 is more sample-efficient than Farzi and heuristic methods, showing superior data utilization. 

These results illustrate the transformative potential of data distillation in improving sequential recommendation systems. This approach represents a shift towards a data-centric paradigm in recommender systems, where prioritizing data quantity and quality and strategic compression can create more robust and efficient algorithms, reducing computational costs and storage demands. This evolution paves the way for the next generation of recommendation algorithms, focusing on maximizing value from the minimal data.


\input{table/complexity}\vspace{-3mm}

\subsection{Time and Memory Analysis}

\vspace{1mm}

\subsubsection{Theoretical Memory Complexity}

As discussed in Section \ref{sec:related_work}, Farzi \cite{sachdeva2023farzi} decomposes synthetic data $\syn \in \mathbb{R}^{\mu \times \zeta \times |\vocab|}$ to a latent summary $\mathbf{D} \in \mathbb{R}^{\mu \times \zeta \times d}$ and a decoder $\mathbf{M} \in \mathbb{R}^{d \times |\vocab|}$, where $d \ll |\vocab|$. To highlight the advantages of employing Tucker decomposition for three-dimensional data, we perform a comparative analysis of our proposed approach with that of Farzi. In particular, we investigate the computational footprint associated with the bi-level optimization framework by evaluating memory usage during a single outer-loop step, providing insights into the efficiency gains achieved through our method as follows:
\begin{align*}
    \operatorname{Farzi~:} & 
    ~ \mathcal{O}\big( |\Phi| + |\mathcal{B}^{\real}| \cdot \zeta \cdot d_3 + |\mathcal{B}^{\syn}| \cdot \zeta \cdot |\vocab| + \mu \cdot \zeta \cdot d + d \cdot |\vocab| \big) \\
    \operatorname{TD3~:} & 
    ~ \mathcal{O}\big( |\Phi| + |\mathcal{B}^{\real}| \cdot \zeta \cdot d_3 + |\mathcal{B}^{\syn}| \cdot \zeta \cdot |\vocab| + (\mu + \zeta) \cdot d_1 + d_1^2 \cdot d_3 \big) 
\end{align*}
where $|\mathcal{B}^{\real}|$ and $|\mathcal{B}^{\syn}|$ denote the batch size of real and synthetic data, respectively, while $d_3$ represents the item embeddings' hidden dimension. Additionally, $d$, $d_1$, and $d_3$ are of the same order of magnitude and much smaller than $|\vocab|$. When the item set is very large, or the distilled sequence summary requires larger values of $\mu$ and $\zeta$, the inequality $(\mu + \zeta) \cdot d_1 + d_1^2 \cdot d_3 \ll \mu \cdot \zeta \cdot d + d \cdot |\vocab|$ holds, the method proposed in our work will offer a spatial advantage. 

\subsubsection{Empirical Computational Complexity}

The data distillation process consumes considerable wall-clock time and GPU memory, so we conducted a detailed quantitative analysis of these requirements for both distillation and model training on the original and distilled datasets, as shown in \cref{tab:time_memory}. Wall-clock time is reported in single A100 (80GB) GPU hours. While distillation generally takes longer and uses more memory than training on the original data, its cost is often amortizable in real-world scenarios where multiple models must be trained on the same dataset. 
The amortization, based on the ratios in the table, varies by dataset and distillation scale. Notably, for large datasets, where training time is typically lengthy, training on significantly reduced distilled data can shorten this process by several orders of magnitude. This trade-off substantially decreases future training time, making distillation a one-time cost that yields long-term benefits for various downstream tasks, such as hyperparameter tuning and architecture exploration. Hence, the distillation process and its amortization will be well justified.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/magazine.pdf}
    \end{subfigure}
    \hfill
    \vspace{1mm}
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figure/ml100k.pdf}
    \end{subfigure}
    \vspace{0.8mm}
    \caption{Illustration of the performance comparison of the TD3 against: \emph{Farzi}, \emph{random sampling} and \emph{longest sampling}.}
    \label{fig:baseline_results}
\end{figure}
\vspace{-1cm}


\subsection{Cross-Architecture Generalization}

\vspace{1mm}

Since the synthetic sequence summary is carefully tailored for optimizing a specific learner model, we assess its generalizability across various unseen architectures, as shown in \cref{tab:cross-arch}. We first distill the Epinions dataset using SASRec~\cite{SASRec}, resulting in a condensed synthetic summary of size $[50 \times 20]$. This summary is then used to train several alternative architectures, including GRU4Rec~\cite{GRU4Rec}, which models sequential behavior using Gated Recurrent Units (GRU); NARM~\cite{li2017neural}, which enhances GRU4Rec with an attention mechanism to emphasize user intent; and BERT4Rec~\cite{bert4rec}, which uses bidirectional self-attention to learn sequence representations. The models trained on the synthetic summary demonstrate strong generalization across diverse architectures, maintaining high predictive performance. In some cases, they even outperform models trained on the original dataset, highlighting the effectiveness of our proposed TD3 method in enabling cross-architecture transferability.

\input{table/cross-arch}





\subsection{Ablation Studies}

To analyze our method's components, we conducted ablation studies on \emph{ML-100k}. Results for \emph{Feature Space Alignment} (FSA) and \emph{Augmented Learner Training} (ALT) are in \cref{tab:ablation_results}. FSA and ALT complement each other. ALT significantly boosts TD3's performance by enhancing contextual understanding and reducing dependence on specific sequence patterns, improving sequence information capture and data updates in the \emph{outer-loop}. FSA alone also enhances performance across metrics by strengthening the objective function, aiding convergence to a similar solution in the feature space, and maintaining performance on original and synthetic data. Utilizing both in the \emph{inner-loop} and \emph{outer-loop} maximizes distillation benefits.

% To analyze our method's components, we performed ablation studies on \emph{ML-100k}. Results for \emph{Feature Space Alignment} (FSA) and \emph{Augmented Learner Training} (ALT) are shown in \cref{tab:ablation_results}. FSA and ALT complement each other, with ALT significantly enhancing TD3's performance by improving contextual understanding and sequence information capture. FSA alone boosts performance by strengthening the objective function and aiding convergence. Using both in the \emph{inner-loop} and \emph{outer-loop} maximizes distillation benefits.

\input{table/ablation}
