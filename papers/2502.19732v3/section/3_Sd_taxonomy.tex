\section{A Taxonomy for Generation and Refinement Frameworks}
\label{sec:sd_taxonomy}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/overview.pdf}
    \caption{A taxonomy of generation-refinement frameworks, showing two phases: (1) Generation of draft tokens through various methods and (2) Refinement through verification strategies.}
    \label{fig:overview}
\end{figure}

To systematically analyze approaches for breaking sequential dependencies in large models, we propose a unified taxonomy that categorizes methods based on their generation and refinement strategies. As shown in Figure~\ref{fig:overview}, our taxonomy decomposes these frameworks into two fundamental phases: \textit{Sequence Generation} and \textit{Sequence Refinement}. This decomposition not only encompasses traditional SD approaches but also captures a broader range of emerging methods that trade off between generation parallelism and output quality.

The sequence generation phase focuses on different strategies for producing draft tokens more efficiently than conventional auto-regressive decoding using a single larger model. These strategies range from simple approaches like random token sampling (used in conjunction with iterative decoding) to more sophisticated methods like retrieval-based generation and draft model prediction. Each generation method offers trade-offs in terms of computational cost and prediction quality. The sequence refinement phase then determines how these candidates are processed - either accepting them directly (with possible poorer quality), verifying a subset of tokens in a single pass, or refining the draft tokens through multiple iterations until convergence.


\tikzstyle{my-box}=[
 rectangle,
 draw=hidden-draw,
 rounded corners,
 text opacity=1,
 minimum height=1.5em,
 minimum width=5em,
 inner sep=2pt,
 align=center,
 fill opacity=.5,
 ]
 \tikzstyle{leaf}=[my-box, minimum height=1.5em,
 fill=hidden-orange!60, text=black, align=left,font=\scriptsize,
 inner xsep=2pt,
 inner ysep=4pt,
 ]

% \begin{figure*}[t]
% 	\centering
% 	\resizebox{\textwidth}{!}{
% 		\begin{forest}
% 			forked edges,
% 			for tree={
% 				grow=east,
% 				reversed=true,
% 				anchor=base west,
% 				parent anchor=east,
% 				child anchor=west,
%                 node options={align=center},
%                 align = center,
% 				base=left,
% 				font=\small,
% 				rectangle,
% 				draw=hidden-draw,
% 				rounded corners,
% 				minimum width=4em,
% 				edge+={darkgray, line width=1pt},
% 				s sep=3pt,
% 				inner xsep=2pt,
% 				inner ysep=3pt,
% 				ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
% 			},
% 			where level=1{text width=5.0em,font=\scriptsize}{},
% 			where level=2{text width=5.6em,font=\scriptsize}{},
% 			where level=3{text width=6.8em,font=\scriptsize}{},
% 			[
% 			Speculative Decoding Algorithms, ver
% 			[
% 			Auto-regressive \\ Decoding \\
%                 (\S\ref{sec:auto_regressive})
% 			[
% 			  Independent Drafter \\ 
% 			[
%                 \finetuneSym\knowledgeSym~BiLD~{\cite{kim2024speculative},}
%                 REST~{\cite{he2023rest},}
%                 \finetuneSym~Speculative RAG~{\cite{wang2024speculative},} \\
%                 \knowledgeSym~OSD~{\cite{liu2023online},} 
%                 ANPD~{\cite{ou2024lossless},}
% 			    \knowledgeSym~DistillSpec~{\cite{zhou2023distillspec},} 
%                 \knowledgeSym~FastDraft~{\cite{zafrir2024fastdraft},} \\
%                 \trainSym~Judge~{\cite{bachmann2025judge},}
%                 \knowledgeSym\trainSym\finetuneSym~{\cite{Liu2025},} 
% 			, leaf, text width=38.4em, align = left
% 			]
% 			]
% 			[
% 			  Dependent Drafter (\S\ref{sec:breaking_dependencies})
%                 [
% 			  Self-Speculative \\ Decoding 
% 			[
%                 \finetuneSym~PPD~{\cite{yang2023predictive},} 
%                 SPEED~{\cite{hooper2023speed},} 
%                 FREE~{\cite{bae2023fast},} \\
% 			\noFinetuneSym~Draft\&Verify~{\cite{zhang2023draft},} 
%                 \trainSym~LayerSkip~{\cite{elhoushi2024layer},} 
%                 \finetuneSym~Kangaroo~{\cite{liu2024kangaroo},} \\
%                 EESD~{\cite{liu2024speculative},}
%                 \noFinetuneSym~SWIFT~{\cite{xia2024swift},} 
%                 Speculative Streaming~{\cite{bhendawade2024speculative},} \\
%                 \noFinetuneSym~Draft on the Fly~{\cite{metel2024draft}}
% 			, leaf, text width=30em, align = left
% 			]
% 			]
% 			[
% 			  FFN Heads based \\ Drafting
% 			[
%                 \noFinetuneSym~EAGLE~{\cite{li2024eagle},}
%                 \knowledgeSym~Falcon~{\cite{gao2024falcon},}
%                 \knowledgeSym~HASS~{\cite{zhang2024learning},} \\
%                 \knowledgeSym~Hydra~{\cite{ankner2024hydra},} 
%                 \knowledgeSym~Mixture of Attentions ~{\cite{zimmer2024mixture}} 
% 			, leaf, text width=30em, align = left
% 			]
% 			]
% 			]
% 			]
%                 [
% 			  Multi-Tokens \\ Prediction
%                 [
% 			  Multi-head \\ Generation
% 			[
% 			\knowledgeSym\peftSym~Medusa~{\cite{cai2024medusa},} 
%                 \trainSym~{\cite{gloeckle2024better},}
%                 \finetuneSym~Amphista~{\cite{li2024amphista},}
%                 \knowledgeSym~CTC-based Drafting~{\cite{wen2024speculative},} 
%                 , leaf, text width=38.4em, align = left
% 			]
% 			]
%                 [
% 			  Jacobi Decoding
% 			[ 
%                 \noFinetuneSym~Jacobi~{\cite{santilli2023accelerating},}  
%                 \noFinetuneSym~LOOKAHEAD~{\cite{fu2024break},}  
%                 \finetuneSym~CLLMs~{\cite{kou2024cllms},}
%                 \noFinetuneSym~The N-Grammys~{\cite{stewart2024n}}
%                 , leaf, text width=38.4em, align = left
% 			]
% 			]
% 			]
%                 [
% 			  Blockwise \\ Operation
% 			[
% 			\finetuneSym\knowledgeSym~Blockwise~{\cite{stern2018blockwise},}
%                 \finetuneSym\knowledgeSym~SpecDec~{\cite{xia2023speculative},}
%                 ~{\cite{kim2024accelerating},} 
%                 Block verification~{\cite{sun2024optimal},}
%                 \noFinetuneSym~MTAD~{\cite{qin2024optimized}}
%                 , leaf, text width=45.67em, align = left
% 			]
% 			]
% 			[
% 			  Tree/Graph \\ Based
% 			[
%                 \noFinetuneSym~SpecTr~{\cite{sun2024spectr},} 
%                 \finetuneSym~SpecInfer~{\cite{miao2023specinfer},}  
%                 \finetuneSym~Staged SD~{\cite{spector2023accelerating},} 
%                 \noFinetuneSym~Sequoia~{\cite{chen2024sequoia},} 
%                 Medusa~{\cite{cai2024medusa},} \\
%                 \noFinetuneSym~EAGLE~{\cite{li2024eagle},}
%                 \noFinetuneSym~EAGLE-2~{\cite{li2024eagle2fasterinferencelanguage},}
%                 ProPD~{\cite{zhong2024propd},}
%                 \noFinetuneSym~OPT-Tree~{\cite{wang2024opt},} \\
%                 \noFinetuneSym~GSD~{\cite{gong2024graph},} 
%                 \noFinetuneSym~RSD~{\cite{jeon2024recursive},}
%                 \knowledgeSym~ReDrafter~{\cite{cheng2024recurrent},}
%                 \peftSym~Speculative Streaming~{\cite{bhendawade2024speculative},} 
%                 \noFinetuneSym~ADED~{\cite{liu2024adaptive},}\\
%                 \noFinetuneSym~DySpec~{\cite{xiong2024dyspec},}
%                 SpecHub~{\cite{sun2024spechub}} 
%                 , leaf, text width=45.67em, align = left
% 			]
% 			]
%                 [
% 			  Parallel Decoding
% 			[
%                 SPEED~{\cite{hooper2023speed},} 
%                 \noFinetuneSym~CS Drafting~{\cite{chen2023cascade},}  
%                 FREE~{\cite{bae2023fast},}
%                 \finetuneSym~PASS~{\cite{monea2023pass},}  
%                 \finetuneSym~Faster Cascades~{\cite{narasimhan2024faster},} \\
%                 \noFinetuneSym~PEARL~{\cite{liu2024parallel},} 
%                 \noFinetuneSym~Ouroboros~{\cite{zhao-etal-2024-ouroboros},} 
%                 \trainSym~ParallelSpec~{\cite{xiao2024parallelspec}}
%                 , leaf, text width=45.67em, align = left
% 			]
% 			]
% 			[
% 			  Dynamic Draft
% 			[
%                 \trainSym~SpecDec++~{\cite{huang2024specdec++},}  
%                 \noFinetuneSym~DDD~{\cite{brown2024dynamic},}
%                 \noFinetuneSym~DSBD~{\cite{qin2024dynamic},}
%                 ON-THE-FLY ADAPTATION~{\cite{liu2025a},} \\
%                 PEARL~{\cite{liu2024parallel}}
%                 MagicDec~{\cite{chen2024magicdec},}
%                 Multi-Draft Speculative Sampling~{\cite{khisti2024multi},} 
%                 ~{\cite{yin2024theoretical},} 
%                 ~{\cite{hu2025towards}}
%                 , leaf, text width=45.67em, align = left
% 			]
% 			]
% 			]
% 		\end{forest}
%   }
% \caption{Taxonomy of Speculative Decoding Algorithms.  \trainSym~Training from scratch, \finetuneSym~Finetuning, 
% \noFinetuneSym~No-Finetuning, \knowledgeSym~Knowledge Distillation, \peftSym~PEFT (LoRA).
% }
% \label{Speculative_decoding_algorithm}
% \end{figure*}

\begin{figure*}[t]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{forest}
			forked edges,
			for tree={
				grow=east,
				reversed=true,
				anchor=base west,
				parent anchor=east,
				child anchor=west,
                node options={align=center},
                align = center,
				base=left,
				font=\small,
				rectangle,
				draw=hidden-draw,
				rounded corners,
				minimum width=4em,
				edge+={darkgray, line width=1pt},
				s sep=3pt,
				inner xsep=2pt,
				inner ysep=3pt,
				ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
			},
			where level=1{text width=5.0em,font=\scriptsize}{},
			where level=2{text width=5.6em,font=\scriptsize}{},
			where level=3{text width=6.8em,font=\scriptsize}{},
			[
			Speculative Decoding Algorithms, ver
                [
			  Predefined Fill \\ Tokens \\ (\S\ref{sec:predefined_fill_tokens})
			[ 
                \noFinetuneSym~Jacobi~{\cite{santilli2023accelerating},}  
                \noFinetuneSym~LOOKAHEAD~{\cite{fu2024break},}  
                \finetuneSym~CLLMs~{\cite{kou2024cllms}}
                , leaf, text width=45.67em, align = left
			]
			]
                [
			  Retrieval-based \\ Methods \\      (\S\ref{sec:retrieval_based_methods})
			[ 
                \noFinetuneSym~LLMA~{\cite{yang2023inference},}
                \noFinetuneSym~REST~{\cite{he2023rest},}
                \finetuneSym~Speculative RAG~{\cite{wang2024speculative}}
                , leaf, text width=45.67em, align = left
			]
			]
                [
			  N-gram-based \\ Methods \\ (\S\ref{sec:ngram_methods})
			[ 
                \noFinetuneSym~ANPD~{\cite{ou2024lossless},}
                \noFinetuneSym~The N-Grammys~{\cite{stewart2024n},}
                \noFinetuneSym~ADED~{\cite{liu2024adaptive}}
                , leaf, text width=45.67em, align = left
			]
			]
			[
			Auto-regressive \\ Decoding \\
                (\S\ref{sec:auto_regressive})
			[
			  Independent Drafter \\ (\S\ref{sec:independent_drafter})
			[
                \finetuneSym\knowledgeSym~SpecDec~
                {\cite{xia2023speculative},}
                \trainSym~SpecDec++~{\cite{huang2024specdec++},}
                \finetuneSym\knowledgeSym~BiLD~{\cite{kim2024speculative},}
                \noFinetuneSym~ON-THE-FLY~{\cite{liu2025a},}
                \knowledgeSym~OSD~{\cite{liu2023online},} 
			\knowledgeSym~DistillSpec~{\cite{zhou2023distillspec},} 
                \knowledgeSym~FastDraft~ \\{\cite{zafrir2024fastdraft},} 
                \trainSym~Judge~{\cite{bachmann2025judge},} 
                \knowledgeSym\trainSym\finetuneSym~{\cite{Liu2025}} 
			, leaf, text width=38.4em, align = left
			]
			]
			[
			  Dependent Drafter \\ (\S\ref{sec:dependent_drafter})
                [
			  Layer-Skipping 
			[
                \finetuneSym~SPEED~{\cite{hooper2023speed},} 
                \peftSym~\knowledgeSym~FREE~{\cite{bae2023fast},} 
			\noFinetuneSym~Draft\&Verify~{\cite{zhang2023draft},} 
                \trainSym~LayerSkip~{\cite{elhoushi2024layer},} 
                \finetuneSym~Kangaroo~{\cite{liu2024kangaroo},} \\ 
                \knowledgeSym~EESD~{\cite{liu2024speculative},}
                \noFinetuneSym~SWIFT~{\cite{xia2024swift},} 
                \finetuneSym~Speculative Streaming~{\cite{bhendawade2024speculative},} 
                \noFinetuneSym~Draft on the Fly~{\cite{metel2024draft}}
			, leaf, text width=30em, align = left
			]
			]
			[
			  FFN Heads based \\ Drafting    
			[
                \noFinetuneSym~EAGLE~{\cite{li2024eagle},}
                \knowledgeSym~Falcon~{\cite{gao2024falcon},}
                \knowledgeSym~HASS~{\cite{zhang2024learning},} 
                \knowledgeSym~Hydra~{\cite{ankner2024hydra},} 
                \knowledgeSym~Mixture of Attentions ~{\cite{zimmer2024mixture}} 
			, leaf, text width=30em, align = left
			]
			]
			]
			]
                [
			  Multi-Token \\ Generation \\ (\S\ref{sec:multi_token_pre})
			[
                \finetuneSym\knowledgeSym~Blockwise~{\cite{stern2018blockwise},}
                 ~{\cite{kim2024accelerating},}
			\knowledgeSym\peftSym~Medusa~{\cite{cai2024medusa},} 
                \trainSym~{\cite{gloeckle2024better},}
                \finetuneSym~Amphista~{\cite{li2024amphista},} 
                \knowledgeSym~CTC-based Drafting~{\cite{wen2024speculative}} 
                , leaf, text width=45.67em, align = left
			]
			]
                [
			  Single-pass \\ Verification \\ (\S\ref{sec:refine:singlepass})
                [
			Linear Verification \\(\S\ref{sec:refine:linear})
			[
			    \finetuneSym\knowledgeSym~SpecDec~
                {\cite{xia2023speculative},}
                \noFinetuneSym~Draft\&Verify~{\cite{zhang2023draft},} 
                \noFinetuneSym~Fast Inference{~\cite{leviathan2023fast},}  
                \trainSym~{~\cite{chen2023accelerating},} 
                \noFinetuneSym~ Block verification~{\cite{sun2025block},}
                \noFinetuneSym~MTAD~{\cite{qin2024optimized},}
                ~{\cite{yin2024theoretical}} 
			, leaf, text width=38.4em, align = left
			]
                ]
                [
			Tree-based \\ Verification \\ (\S\ref{sec:refine:tree_based})
			[
			\noFinetuneSym~SpecTr~{\cite{sun2024spectr},} 
                \finetuneSym~SpecInfer~{\cite{miao2023specinfer},}  
                \finetuneSym~Staged SD~{\cite{spector2023accelerating},} 
                \noFinetuneSym~Sequoia~{\cite{chen2024sequoia},} 
                \knowledgeSym\peftSym~Medusa~{\cite{cai2024medusa},} 
                \noFinetuneSym~EAGLE~{\cite{li2024eagle},}
                \noFinetuneSym~EAGLE-2~{\cite{li2024eagle2fasterinferencelanguage},} \\
                \finetuneSym~ProPD~{\cite{zhong2024propd},}
                \noFinetuneSym~OPT-Tree~{\cite{wang2024opt},} 
                \noFinetuneSym~DSBD~{\cite{qin2024dynamic},}
                \noFinetuneSym~GSD~{\cite{gong2024graph},} 
                \noFinetuneSym~RSD~{\cite{jeon2024recursive},} 
                \knowledgeSym~ReDrafter~{\cite{cheng2024recurrent},}
                \peftSym~Speculative Streaming~{\cite{bhendawade2024speculative},} \\
                \noFinetuneSym~ADED~{\cite{liu2024adaptive},}
                \noFinetuneSym~DySpec~{\cite{xiong2024dyspec},}
                \noFinetuneSym~SpecHub~{\cite{sun2024spechub},} 
                \noFinetuneSym~Multi-Draft Speculative Sampling~{\cite{khisti2024multi},} 
                ~\noFinetuneSym~{\cite{hu2025towards}}
			, leaf, text width=38.4em, align = left
			]
                ]
			]
                [
			  Parallel SD \\ (\S\ref{sec:parallel-sd})
			[
                 \finetuneSym~SPEED~{\cite{hooper2023speed},} 
                \noFinetuneSym~CS Drafting~{\cite{chen2023cascade},}  
                \peftSym~\knowledgeSym~FREE~{\cite{bae2023fast},}
                \finetuneSym~PPD~{\cite{yang2023predictive},} 
                \finetuneSym~PASS~{\cite{monea2023pass},} 
                \finetuneSym~Faster Cascades~{\cite{narasimhan2024faster},} 
                \noFinetuneSym~PEARL~{\cite{liu2024parallel},} 
                \noFinetuneSym~Ouroboros~{\cite{zhao-etal-2024-ouroboros},} \\
                \trainSym~ParallelSpec~{\cite{xiao2024parallelspec},}
                \finetuneSym~SPACE~{\cite{yi-etal-2024-generation}}
                , leaf, text width=45.67em, align = left
			]
			]
                [
                Distributed  SD \\ (\S\ref{sec:edge-sd})
                [
                \noFinetuneSym~{SpecExec~\cite{svirschevski2024specexec},}
                \noFinetuneSym{EdgeLLM~\cite{xu2024edgellm},}
                \knowledgeSym~{Dovetail~\cite{zhang2024dovetail}}
                , leaf, text width=45.67em, align = left
                ]
                ]
                [
                Compiler/Hardware \\ (\S\ref{sec:batch-sd})
                [
                \noFinetuneSym~{SpecPIM~\cite{li2024specpim},}
                \noFinetuneSym~{MagicDec~\cite{chen2024magicdec},}
                \noFinetuneSym~{BASS~\cite{qian2024bass},}
                \noFinetuneSym~{SEED~\cite{wang2024seed},}
                \noFinetuneSym~{PipeInfer~\cite{butler2024pipeinfer},} 
                \knowledgeSym~{~\cite{wang2024mamba},}
                \knowledgeSym~{SKD~\cite{xu2024speculative},}
                \noFinetuneSym~{~\cite{wagner2024optimized},}
                \noFinetuneSym~{~\cite{yin2024theoretical}}
                , leaf, text width=45.67em, align = left
                ]
                ]
                [
                Vision \\ (\S\ref{sec:AR_visual})
                [
                \noFinetuneSym~{~\cite{wang2024continuous},}
                \noFinetuneSym~{LANTERN~\cite{jang2024lantern},}
                \noFinetuneSym~{SJD~\cite{teng2024accelerating}}
                , leaf, text width=45.67em, align = left
                ]
                ]
                [
                Multimodal \\ (\S\ref{sec:mutimodel})
                [
                \finetuneSym\peftSym~{VADUSA~\cite{li2024fast},}
                \finetuneSym~{~\cite{raj2024faster},}
                \trainSym~{~\cite{gagrani2024speculative},}
                \finetuneSym~{IbED~\cite{leebatch}}
                , leaf, text width=45.67em, align = left
                ]
                ]
                [
                Recommendation\\Systems \\ (\S\ref{sec:SR_apps})
                [
                \noFinetuneSym~{DARE~\cite{xi2024decoding},}
                \peftSym~{AtSpeed~\cite{lin2024efficient}}
                , leaf, text width=45.67em, align = left
                ]
                ]
			]
		\end{forest}
  }

\caption{Taxonomy of Speculative Decoding Algorithms. Symbols indicate implementation approach: \noFinetuneSym~Direct application (no training required), \trainSym~Full model training from scratch, \finetuneSym~Model fine-tuning, \peftSym~Parameter-efficient fine-tuning (PEFT), \knowledgeSym~Knowledge distillation from target model.}
\label{Speculative_decoding_algorithm}
\end{figure*}

\section{Sequence Generation Methods}
\label{sec:generation}

\subsection{Predefined Fill Tokens}
\label{sec:predefined_fill_tokens}
The simplest approach uses random initialization or predefined tokens (e.g., \texttt{PAD}). While computationally free, these methods provide poor initialization points, requiring multiple refinement iterations as discussed in Section~\ref{sec:refine:iterative}.

\subsection{Retrieval-based Methods}
\label{sec:retrieval_based_methods}
LLMA ~\cite{yang2023inference} first proposed exploiting overlaps between LLM outputs and reference documents to accelerate inference through parallel token verification while maintaining identical generation results. In retrieval-based approaches, REST~\cite{he2023rest} replaces smaller language models with exact suffix matching from a datastore to generate draft tokens. It builds a Trie (prefix tree) from retrieved continuations, where node weights reflect token sequence frequencies. Speculative RAG~\cite{wang2024speculative} use a fine-tuned specialist LM to generate complete answer drafts with supporting rationales. It clusters retrieved documents by similarity, generates diverse drafts from different document subsets, and employs self-consistency and self-reflection scores for draft evaluation instead of token-level verification.
\subsection{N-gram-based Methods}
\label{sec:ngram_methods}
Several approaches leverage n-gram patterns for efficient token generation. ANPD~\cite{ou2024lossless} replaces traditional draft models with an adaptive N-gram system that updates predictions based on context. LOOKAHEAD~\cite{fu2024break} uses n-gram verification by collecting and utilizing n-grams from previous iterations as draft tokens. The N-Grammys~\cite{stewart2024n} further develops this idea by creating a dedicated n-gram based prediction system that can operate without requiring a separate draft model.

\subsection{Auto-regressive Generation}
\label{sec:auto_regressive}
Most sequence generation methods employ auto-regressive drafting, where a smaller model generates draft tokens that are verified by a larger target model. This drafting paradigm has spawned numerous techniques that vary in how the draft model interacts with the target model.

\subsubsection{Independent Drafters}
\label{sec:independent_drafter}
Auto-regressive independent drafters are techniques in which smaller model(s) generate tokens one at a time while a separate larger target model subsequently verifies the draft tokens in parallel. SpecDec~\cite{xia2023speculative} pioneered this approach with an independent draft model using distinct attention queries for masked positions. SpecDec++~\cite{huang2024specdec++} improves SpecDec~\cite{xia2023speculative} by training a prediction head on top of the draft model that estimates the probability of token acceptance by the target model. Based on these predictions, it dynamically determines when to stop generating tokens and trigger verification.

 Recent works focus on dynamic adaptation and confidence monitoring. BiLD~\cite{kim2024speculative} triggers target model verification when draft confidence falls below a threshold, while ON-THE-FLY~\cite{liu2025a} dynamically adjusts window sizes based on prediction accuracy. OSD~\cite{liu2023online} enables online adaptation through knowledge distillation during inference, and DistillSpec~\cite{zhou2023distillspec} extends this by accessing target model logits for improved alignment.
\cite{Liu2025} introduces special tokens for draft models to autonomously determine target model consultation, eliminating separate verification at some performance cost. For mathematical applications, Judge\cite{bachmann2025judge} adds a learned verification layer atop the target model's embeddings, using contextual correctness assessment to reduce strict output alignment requirements.


\subsubsection{Dependent Drafters}
\label{sec:dependent_drafter}
The main drawbacks of independent drafting approaches are that  (1) the computation required to generate the draft tokens is fixed per tokens, meaning that computation is over-provisioned for many ``easy'' tokens and (2) the target model cannot reuse the features of the drafting process, increasing the amount of compute required. Self-speculative decoding approaches generate draft tokens by relying directly on a subset (\textbf{Layer Skipping}) or extension (\textbf{Dependent Heads}) of the target model.

\paragraph{Layer Skipping} Draft\&Verify~\cite{zhang2023draft}, SWIFT~\cite{xia2024swift}, and Draft on the Fly~\cite{metel2024draft} achieves fast draft token generation by selectively skipping some intermediate layers in the Draft process, and then verifies these drafts using the full LLM. In order to achieve good draft accuracy, they also designed an intermediate layer selection algorithm based on Bayesian optimization. LayerSkip~\cite{elhoushi2024layer} uses an early exiting~\cite{teerapittayanon2016branchynet} approach to dynamically output tokens at different depths of the target model. Kangaroo~\cite{liu2024kangaroo} also applied early exit by adopting a shallow sub-network to generate drafts and using a lightweight adapter module to bridge the performance gap with the full model, achieving efficient and accurate decoding. EESD~\cite{liu2024speculative} use Thompson Sampling Control~\cite{slivkins2019introduction} Mechanism to adaptively determines how many draft token will be generated. SPEED~\cite{hooper2023speed} combines speculative execution with parameter sharing, using early predictions to process multiple tokens in parallel through shared decoder layers, rather than waiting for each token to complete sequentially.

\paragraph{Dependent Heads}
Dependent head-based drafting eliminates the need for a separate draft model by adding lightweight feed-forward prediction heads using the hidden states of the target model. The main idea is that the first token in sequence generation block uses the target model as usual but the features at the end of the model are fed into additional heads to predict subsequent tokens without passing back through the entire target model.

EAGLE~\cite{li2024eagle} uses a trained head that takes in hidden states from the target model and generates subsequent draft tokens in an AR manner. Hydra~\cite{ankner2024hydra} use multiple decoding, one for each draft token position. 

EAGLE extensions have focused on improving parallel token generation and attention mechanisms. Falcon~\cite{gao2024falcon} introduces a semi-autoregressive framework combining LSTM layers and relaxed causal-masked self-attention to generate k tokens per forward pass, while HASS~\cite{zhang2024learning} enhances knowledge distillation by prioritizing high-probability tokens during training. Mixture of Attentions~\cite{zimmer2024mixture} incorporates multiple attention types (LSA, SA, and CA) for improved token prediction, and DeepSeek-V3~\cite{liu2024deepseek} adapts ~\cite{gloeckle2024better}'s multi-token approach (discussed in Section~\ref{sec:multi_token_pre}) while maintaining complete causal attention during inference. 


\subsection{Multi-token Prediction}
\label{sec:multi_token_pre}

\cite{stern2018blockwise} proposes adding multiple decoding heads on top of a model to predict $k$ future tokens in parallel, requiring training the entire model from scratch. Medusa~\cite{cai2024medusa} introduces a parameter-efficient approach, where lightweight decoding heads are fine-tuned on top of pre-trained language models. Each head is trained to predict a specific future position in the sequence without modifying the target model. ~\cite{gloeckle2024better} propose a multi-token prediction paradigm where a shared backbone optimized jointly with multiple prediction heads that enable propagation of information related to sequential tokens during training that can be discarded at inference to enable parallel generation (similar to Medusa). 

Recent improvements enhance Medusa's independent draft heads by modeling inter-token relationships. Amphista~\cite{li2024amphista} uses bi-directional self-attention to consider both past and future predictions, while CTC Drafting~\cite{wen2024speculative} employs Connectionist Temporal Classification (CTC) with blank tokens and repetition, followed by duplicate removal to generate draft sequences.


\section{Sequence Refinement Methods}
\label{sec:refine}

\subsection{Single-pass Verification}
\label{sec:refine:singlepass}

Single-pass verification represents the most common refinement strategy in draft-and-verify approaches, where drafted tokens are verified exactly once by the target model. 

\subsubsection{Linear Verification}
\label{sec:refine:linear}
Linear verification sequentially validates draft tokens against the target model's logit distributions, with early works like SpecDec~\cite{xia2023speculative} and Draft\&Verify~\cite{zhang2023draft} comparing drafted tokens against the target model's predictions. When a token fails verification (i.e., when the draft output doesn't match the target model's distribution), the system falls back to standard AR generation from that point.

Fast Inference ~\cite{leviathan2023fast} and ~\cite{chen2023accelerating} introduced speculative sampling to improve acceptance rates while approximately maintaining the target distribution. Their method accepts a token if the target model assigns equal or higher probability; otherwise, it accepts with probability $p(x)/q(x)$ or resamples from an adjusted distribution.

Block Verification~\cite{sun2025block} and MTAD~\cite{qin2024optimized} improve upon linear verification by examining the joint probability distribution of draft tokens as a chain of conditional probabilities. This block-based evaluation approach typically results in higher acceptance rates compared to token-by-token verification for similar quality.

\subsubsection{Tree-based Verification}
\label{sec:refine:tree_based}
Tree-based verification extends the single-pass paradigm by enabling parallel exploration of multiple completion paths. Unlike linear verification that processes a single sequence, tree-based methods construct and verify a tree of possible completions simultaneously, making more efficient use of parallel compute resources.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figures/survey_figure_treebased.pdf}
\caption{Illustration of tree-based speculative decoding, with token tree construction on the left
and tree attention mask on the right.}
\label{fig:TreeBased-SD}
\end{figure}

SpecInfer~\cite{miao2023specinfer} pioneered this approach by developing an efficient tree-based attention masking scheme that enables parallel verification while maintaining proper token dependencies. This innovation maintains generation quality while significantly increasing the number of tokens that can be verified in parallel.

Recent works have focused on optimizing tree structure and size to maximize computational efficiency. Sequoia~\cite{chen2024sequoia} introduces a hardware-aware tree optimizer that can maximize inference performance by selecting appropriate tree dimensions based on available computing resources. OPT-Tree~\cite{wang2024opt} searches for optimal tree structures to maximize expected acceptance length per decoding step. DSBD~\cite{qin2024dynamic} uses a small model to generate multiple candidate sequences via beam search, then the large model verifies these sequences layer by layer while dynamically adjusting the beam width based on acceptance probabilities to balance efficiency and quality. DySpec~\cite{xiong2024dyspec} enables dynamic tree expansion during runtime based on prediction confidence, while EAGLE2~\cite{li2024eagle2fasterinferencelanguage} incorporates context-aware tree construction to improve acceptance rates. DDD~\cite{brown2024dynamic} optimizes EAGLE2~\cite{li2024eagle2fasterinferencelanguage} 's tree drafting method by making the depth dynamic based on draft model confidence.

Several works have explored hybrid approaches that combine tree-based verification with other techniques. ProPD~\cite{zhong2024propd} integrates progressive refinement into the tree structure, while RSD~\cite{jeon2024recursive} employs recursive verification strategies. GSD~\cite{gong2024graph} and ADED~\cite{liu2024adaptive} extend tree-based methods to handle more complex dependency structures through graph-based representations and adaptive depth adjustment.

In terms of verifying multiple candidate draft tokens in parallel (also known as Multi-Draft Speculative Decoding, MDSD), ~\cite{hu2025towards} propose a hybrid sampling strategy that combines deterministic selection of high-probability tokens with random sampling of the final token, improving acceptance rates in certain scenarios.  ~\cite{khisti2024multi} introduce a two-phase verification method that uses importance sampling to select a draft token before applying single-draft verification, optimizing the process for parallel draft generation.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/survey_parallel.pdf}
    \caption{Comparison of speculative decoding approaches: (a) Sequential processing where draft generates tokens (0-3) before target verification. (b) Parallel processing where draft generates new tokens while target simultaneously verifies previous ones.}
    \label{fig:parallel-sd}
\end{figure}

\subsection{Iterative Decoding}
\label{sec:refine:iterative}
Iterative decoding methods extend the single-pass verification paradigm by allowing multiple refinement iterations on draft tokens until convergence. These approaches draw inspiration from classical numerical methods for solving systems of nonlinear equations, particularly the Jacobi and Gauss-Seidel iteration methods.

In ~\cite{santilli2023accelerating}, the authors reframe AR text generation as an iterative optimization problem. Their approach expresses token generation as a system where each position must output the most likely token given the current state of all other positions. Starting with a randomly initialized sequence, they adapt the Jacobi method to update all positions in parallel during each iteration until convergence. The authors prove that this process produces identical output to traditional AR decoding under greedy sampling. ~\cite{fu2024break} builds upon this framework with LOOKAHEAD decoding, which combines Jacobi iterations with n-gram verification to accelerate convergence by leveraging predictions from earlier steps.

 CLLMs~\cite{kou2024cllms} leverages consistency training to accelerate convergence by enabling better multi-token prediction in early iterations. 