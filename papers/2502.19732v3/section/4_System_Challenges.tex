\section{System-Level Optimizations and Implementation Strategies}
\label{sec:system-sd}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/sys_figure.pdf}
    \caption{Asynchronous and heterogeneous schedules.}
    \label{fig:sys-sd}
\end{figure}

\subsection{Parallel Speculative Decoding}
\label{sec:parallel-sd}

Traditional SD processes tokens sequentially, with the draft model generating tokens followed by target model verification, creating inherent bottlenecks. As shown in Figure~\ref{fig:parallel-sd}, parallel approaches overcome this limitation by enabling simultaneous operation - while the target model verifies earlier tokens, the draft model generates subsequent ones, enabling continuous overlapped execution. Recent methods build upon this paradigm: CS Drafting~\cite{chen2023cascade} employs vertical and horizontal cascade structures for 81\% speedup, PaSS~\cite{monea2023pass} uses look-ahead embeddings for 30\% speedup, and Faster Cascades~\cite{narasimhan2024faster} incorporates deferral rules for improved cost-quality trade-offs. PEARL~\cite{liu2024parallel} further advances this through pre-verify and post-verify strategies with adaptive draft lengths, achieving 4.43$\times$ speedup over AR decoding and 1.50$\times$ over standard SD AMUSD~\cite{mcdanel2024amusd} presents an asynchronous multi-device approach to SD, decoupling the draft and verify phases into continuous, asynchronous operations.


\subsection{Distributed Speculative Decoding}
\label{sec:edge-sd}
Edge computing environments impose stringent constraints on memory, compute power, and latency, necessitating specialized SD approaches to deploy LLMs effectively in resource-constrained settings. SpecExec~\cite{svirschevski2024specexec} is designed to harness the parallel processing power of consumer GPUs to accelerate LLM inference. By generating multiple tokens per target model iteration and constructing a ``cache'' tree of probable continuations, SpecExec efficiently validates these continuations with the target model in a single pass. EdgeLLM~\cite{xu2024edgellm} further optimizes on-device LLM inference through novel techniques for resource allocation and error correction, achieving great token generation speeds and significantly outperforming existing engines. Dovetail~\cite{zhang2024dovetail} represents a significant advancement in heterogeneous computing for LLM inference. By deploying the draft model on the GPU and the target model on the CPU, Dovetail reduces the granularity of data transfer and enhances the overall inference process. The introduction of Dynamic Gating Fusion (DGF) and optimizations for low-end hardware further improve the balance between latency and performance.


\subsection{Compiler and Hardware Optimization for Speculative Decoding}
\label{sec:batch-sd}
Efficient implementation of SD requires careful optimization of both hardware resources and compiler strategies to maximize throughput and minimize latency. SpecPIM~\cite{li2024specpim} presents a novel approach to accelerate speculative inference on a Processing-in-Memory (PIM) system through co-exploration of architecture and dataflow. This method constructs a design space that comprehensively considers algorithmic and architectural heterogeneity, enabling optimal hardware resource allocation for different models and computational patterns. ~\cite{wagner2024optimized} investigates improvements in speculative sampling on GPUs, achieving significant speed gains by parallelizing computations and using sigmoid approximations for softmax, though this comes with a minor reduction in accuracy.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{figures/AR_IMAGE.pdf}
    \caption{Flow of AR image generation with SD.}
    \label{fig:ar-mutimodal}
\end{figure}

Recent studies have focused on enhancing the throughput of LLMs using SD by optimizing batch processing and scheduling strategies. Figure~\ref{fig:sys-sd} illustrates two scheduling strategies for SD systems: (a) Asynchronous Schedule: The draft stage is followed by the verify stage, with optional stop signals determining further processing. This non-blocking approach enhances system efficiency. (b) Heterogeneous Schedule: Both CPU and GPU devices are utilized for different stages of the decoding process, enabling parallel processing and optimizing performance through resource allocation. Using Markov chain theory, \cite{yin2024theoretical} establishes SD's optimality among unbiased algorithms while highlighting the tradeoff between inference speed and output quality. Their analysis reveals that batch processing benefits are limited by the distribution gap between small and large models. MagicDec~\cite{chen2024magicdec} identifies the shift from compute-bound to memory-bound bottlenecks as batch size and sequence length increase, using sparse KV caches in draft models to optimize throughput. BASS~\cite{qian2024bass} extends SD to a batched setting with customized CUDA kernels for ragged tensors in attention calculations and dynamically adjusts draft lengths for better GPU utilization. SEED~\cite{wang2024seed} accelerates reasoning tree construction through scheduled speculative execution, using a rounds-scheduled strategy for conflict-free parallel processing. PipeInfer~\cite{butler2024pipeinfer} addresses single-request latency through pipelined speculative acceleration, reducing inter-token latency via asynchronous speculation and early cancellation. TRIFORCE~\cite{sun2024triforce} introduces a hierarchical SD mechanism with a dynamic sparse KV cache to achieve lossless acceleration of long sequence generation, significantly improving generation speed and efficiency while maintaining quality. ~\cite{zhao2024qspec} proposes QSPEC, a novel framework that combines weight-shared quantization schemes with SD, achieving up to 1.55Ã— acceleration without quality loss, paving the way for efficient and high-fidelity quantization deployment in diverse and memory-constrained settings. ~\cite{wang2024mamba} introduces a hardware-aware SD algorithm that accelerates the inference speed of Mamba and hybrid models. Inspired by SD, SKD~\cite{xu2024speculative} represents a novel, adaptive approach to knowledge distillation. By dynamically generating tokens and using the teacher model to filter or replace low-quality samples, it bridges the gap between supervised KD's reliance on static data and on-policy KD's susceptibility to low-quality outputs. This ensures a better alignment between training and inference distributions, and improved performance.