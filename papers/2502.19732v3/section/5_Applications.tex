\section{Multimodal Models and Applications}
\label{sec:sd_application}

\subsection{Speculative Decoding for Visual Output Generation}
\label{sec:AR_visual}
Researchers are now using SD to improve the efficiency of AR image generation~\cite{ding2021cogview,yu2022scaling, li2024autoregressive}. As shown in Figure~\ref{fig:ar-mutimodal}, this method greatly speeds up the process by reducing the inference steps needed for generating visual tokens.
For instance,~\cite{wang2024continuous} proposes a novel continuous SD method that designs a novel acceptance criterion for the diffusion distributions, significantly improving the efficiency of AR image generation. Similarly, LANTERN~\cite{jang2024lantern} presents a relaxed acceptance condition for the SD strategy to substantially speed up the inference process in visual AR models. Additionally, Speculative Jacobi Decoding (SJD)~\cite{teng2024accelerating} offers a training-free speculative Jacobi decoding technique that effectively accelerates text-to-image generation tasks.

\subsection{Speculative Decoding for Multimodal Output Generation}
\label{sec:mutimodel}

Recent advancements in SD have substantially improve the efficiency and quality of AR generation across various modalities. In the domain of speech synthesis, VADUSA~\cite{li2024fast} leverages SD to accelerate the inference process in AR text-to-speech (TTS) systems, which enhances the quality speech synthesis as well. Inspired by the flavor of SD, ~\cite{raj2024faster} introduces a multi-token prediction mechanism, offering substantial improvements in inference efficiency for speech generation.

In the context of multimodal large language models, ~\cite{gagrani2024speculative} investigates the integration of SD into the LLaVA 7B model to optimize inference efficiency. Their findings indicate that employing a lightweight, language-only draft model facilitates a memory-constrained acceleration of up to 2.37Ã—. Besides, IbED~\cite{leebatch} proposes the "In-batch Ensemble Drafting" method to further enhance the robustness and efficiency of SD. It adopts the ensemble techniques during batch-level inference, requires no additional model parameters and significantly increases the validation probability of draft tokens, thereby improving performance and robustness across diverse input scenarios.

\subsection{Recommendation Systems}
\label{sec:SR_apps}
LLM-based recommendation systems have shown great potential in enhancing personalized recommendations, but their high inference latency poses a significant challenge for real-world deployment. To address this, recent research has focused on optimizing decoding efficiency to accelerate recommendation generation. ~\cite{xi2024decoding} propose DARE that integrates retrieval-based SD to accelerate recommendation knowledge generation, thereby improving the deployment efficiency of LLM-based recommender systems in industrial settings. AtSpeed~\cite{lin2024efficient} combines strict top-K alignment (AtSpeed-S) and relaxed sampling verification (AtSpeed-R), to significantly accelerate LLM-based generative recommendation with speedup from 2$\times$ to 2.5$\times$, addressing inference latency challenges in top-K sequence generation.