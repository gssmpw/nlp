\section{The Sequential Bottleneck in Large Model Inference}
\label{sec:sequential_bottleneck}

\subsection{Understanding Sequential Dependencies}
\label{sec:sequential_dependencies}

Modern LLMs, such as the Llama series~\cite{touvron2023llama,touvron2023llama2,dubey2024llama} and the GPT series~\cite{radford2019language,brown2020language}, are built on transformer architectures consisting of stacked decoder blocks. As shown in Figure~\ref{fig:architech}(a), each decoder block contains two fundamental components: a Self-Attention (SA) block and a feed-forward network (FFN). During execution, the input of the SA block is first multiplied with three weight matrices $W_{Q}$, $W_{K}$, and $W_{V}$, yielding the outputs termed query ($q$), key ($k$), and value ($v$), respectively.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/overview_llm_intro.pdf}
    \caption{(a) The Llama architecture consists of stacked transformer decoder blocks. (b) Each decoder block contains a self-attention (SA) block and feedforward (FFN) block. (c) During the decoding stage, tokens are generated auto-regressively.}
    \label{fig:architech}
\end{figure*}

The computation flow, detailed in Figure~\ref{fig:architech}(b), shows how query and key vectors compute attention scores through matrix multiplication. After softmax normalization, these scores weight the value vectors, producing the SA output through a weighted sum and residual connection. This SA output feeds into the FFN, typically implemented as either a standard MLP~\cite{radford2018improving, radford2019language} or gated MLP~\cite{liu2021pay, touvron2023llama,touvron2023llama2}, with multiple fully connected layers and activation functions like GeLU~\cite{hendrycks2016gaussian} or SiLU~\cite{elfwing2018sigmoid}.

The core challenge emerges during inference, which consists of two main phases: prefill and decoding. While the prefill phase can process input sequences in parallel, the decoding phase introduces a critical bottleneck. As shown in Figure~\ref{fig:architech}(c), the model must predict each token sequentially, using both current and previous token information through their Key and Value (KV) vectors. These KV vectors are cached for subsequent predictions, leading to significant memory access latency as the sequence length grows.

\subsection{Breaking Sequential Dependencies}
\label{sec:breaking_dependencies}

Traditional approaches to accelerating LM inference have focused on reducing computational costs through model compression, knowledge distillation, and architectural optimizations. However, these methods primarily address individual computation costs rather than the fundamental sequential dependency that requires each token to wait for all previous tokens.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/sd_intro_new.pdf}
    \caption{Illustration of speculative decoding workflow.}
    \label{fig:sd_intro}
\end{figure}

Speculative decoding (SD)~\cite{stern2018blockwise} has emerged as a promising solution that directly targets this sequential bottleneck. As illustrated in Figure~\ref{fig:sd_intro}, this approach introduces a two-phase process where a smaller, faster \textit{draft model} first predicts multiple tokens in parallel, followed by verification using the target model. The draft model enables parallel token generation, breaking away from traditional token-by-token generation, while the target model's verification step maintains output quality through accept/reject decisions.

This strategy has proven particularly valuable for real-time applications like interactive dialogue systems, where response latency directly impacts user experience. The verification mechanism provides a crucial balance between generation speed and output quality, accepting correct predictions to maintain throughput while falling back to sequential generation when necessary to preserve accuracy.

While SD represents one successful approach to breaking sequential dependencies in autoregressive (AR) models, it belongs to a broader family of \textit{generation-refinement} methods. The following sections present a systematic taxonomy of these approaches, examining how different techniques balance the trade-offs between generation parallelism and output quality.