\section{Introduction}
\label{sec:introduction}


Large Models (LMs) have demonstrated remarkable capabilities across diverse domains, from text generation~\cite{brown2020language,45-llm-qa,34-llama} and translation~\cite{zhu2023multilingual,46-language-translation,huang2023towards} to image synthesis~\cite{ho2020denoising,yang2023diffusion,tian2024visual} and video generation~\cite{SORA,wu2023tune,opensora}. However, these models face a critical challenge: their inherently sequential nature creates significant latency bottlenecks, particularly for real-time applications. While traditional optimization approaches like quantization and pruning often compromise model quality for speed, recent research has focused on maintaining output quality while breaking sequential dependencies through novel algorithmic and system-level innovations.

Generation-refinement frameworks have emerged as a promising family of solutions that directly address these sequential bottlenecks. These approaches encompass a range of methods, from speculative decoding with draft models to iterative refinement techniques inspired by numerical optimization. The common thread among these approaches is their division of the generation process into two phases: an initial generation step that produces draft tokens in parallel, followed by a refinement step that ensures output quality. 

The implementation of these frameworks presents unique system-level challenges across different deployment scenarios. Edge devices require careful optimization of memory usage and computation patterns~\cite{svirschevski2024specexec,xu2024edgellm}, while distributed systems must manage complex communication patterns and load balancing. These system-level considerations have driven innovations in areas like kernel design, hardware acceleration, and batch processing optimization, significantly influencing both algorithmic choices and practical performance.

This survey synthesizes research across these approaches, examining both algorithmic innovations and their system implementations. We present a systematic taxonomy of generation-refinement methods, analyze deployment strategies across computing environments, and explore applications spanning text, images~\cite{wang2024continuous,jang2024lantern}, and speech~\cite{li2024fast,raj2024faster}. Our primary contributions include comprehensive analysis of system-level implementations and optimizations, detailed examination of applications across modalities, and identification of key research challenges in efficient neural sequence generation.
