\newpage
\appendix
\onecolumn
\section{Literature Review}\label{app:review}
We provide an overview of LLM-centric research of FLE presented in Figure~\ref{fig:review}.
\input{figures/review}

\section{Four Phases of Research Roadmap}\label{app:roadmap}
\paragraph{Stage 1: Rule-based Models (1960s--1990s).} Early solutions relied on handcrafted linguistic rules to process language in tightly constrained scenarios~\cite{grosan2011rule,c1993towards}. Classical platforms like PLATO~\cite{hart1981language} and Systran~\cite{toma1977systran} operated effectively for highly structured tasks (e.g., grammar drills) but struggled with complex, context-dependent interactions.

\paragraph{Stage 2: Statistical Models (1990s--2010s).} With the increased availability of digitized corpora, methods such as the early version of Google Translate~\cite{och2006statistical} and Dragon NaturallySpeaking~\cite{blair1997dragon} pioneered statistical pattern mining. These approaches leveraged large datasets to infer linguistic rules probabilistically, improving scalability yet still lacking deeper semantic understanding.

\paragraph{Stage 3: Neural Models (2010s--2020s).} The advent of deep learning architectures (e.g., RNNs~\cite{yu2019review} and Transformers~\cite{vaswani2017attention}) enabled more robust context modeling, sparking transformative applications like Grammarly~\cite{fitria2021grammarly} and Duolingo~\cite{vesselinov2012duolingo}. These systems offered enhanced personalization and feedback, significantly augmenting learners’ writing and reading comprehension.

\paragraph{Stage 4: Large Language Models (2020s--Present).} Today’s LLMs (e.g., ChatGPT~\cite{achiam2023gpt}) combine massive pre-training corpora with generative capabilities, achieving impressive results in multi-turn dialogue, individualized scaffolding, and multimodal integration. Tools such as Khanmigo~\cite{anand2023khan} demonstrate LLMs’ potential for real-time conversational practice, dynamic content creation, and inclusive educational support at scale.
