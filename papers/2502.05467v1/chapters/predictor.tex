\section{LLMs as Task Predictors}\label{sec:predictor}
\textit{Task-Based Language Learning (TBLL)}~\cite{nunan1989designing,willis2021framework} as a methodological approach is one of the critical factors of modern foreign language teaching. LLMs have demonstrated remarkable capabilities in understanding and generating human language, making them well-suited for addressing numerous tasks in FLE. These tasks can be broadly categorized into three types based on their nature and the role of LLMs: 1) \textit{Discriminative}, 2) \textit{Generative}, and 3) \textit{Mixed} of the above two roles.

\subsection{Discriminative Task Predictors}
Discriminative tasks in FLE primarily involve classifying learner inputs or grading their future performance. Below are some applications that are still calling for improvements:

% \paragraph{Automated Assessment.} The task aims to automatically grade students' assignments, including automatic essay scoring~\cite{sessler2024can,li2024applying,syamkumar2024improving}, short answer grading~\cite{schneider2023towards,henkel2024can}, and spoken language assessment~\cite{gao2023investigation,fu2024pronunciation}. For instance, LLMs can assess the grammatical accuracy, lexical variety, and coherence of essays, offering both holistic scores and detailed feedback on specific areas for improvement. Similarly, for spoken assessments, LLMs can evaluate pronunciation, fluency, and prosody~\cite{kopparapu2024spoken}, which are often challenging to assess consistently in traditional settings. However, this area still faces challenges, such as misalignment of assessment with expert instructors~\cite{kundu2024large} and a lack of empathy~\cite{sharma2024comuniqa}.

\paragraph{Automated Assessment.}  The task aims to automatically grade students’ assignments, including essay scoring~\cite{sessler2024can,li2024applying,syamkumar2024improving}, short answer grading~\cite{schneider2023towards,henkel2024can}, and spoken language evaluation~\cite{gao2023investigation,fu2024pronunciation}. LLMs can process learners’ submissions to judge grammar, lexical diversity, coherence, and even spoken fluency, providing instant feedback. This scalability is particularly appealing for large classes, where human evaluators are often overwhelmed and unable to provide timely, personalized critique~\cite{mizumoto2023exploring}.

% \paragraph{Knowledge Tracing.} Given sequences of learning interactions in online learning systems, Knowledge Tracing aims to monitor students’ evolving knowledge states during the learning process and predict their performance on future exercises. The measured knowledge states can be further applied to individualize students’ learning schemes to maximize their learning efficiency~\cite{shen2024survey,xu2023learning}. Recent studies have explored the incorporation of LLMs into knowledge tracing and have demonstrated the effectiveness and generalization in solving cold-start problems~\cite{zhan2024knowledge,jung2024clst}, but challenges still exist in this area~\cite{cho2024systematic}.


\paragraph{Knowledge Tracing.} Given sequences of learning interactions in online learning systems, Knowledge Tracing identifies and tracks students’ evolving mastery of target skills~\cite{shen2024survey,xu2023learning}. LLM-based methods have been explored in cold-start scenarios~\cite{zhan2024knowledge,jung2024clst}, offering strong generalization by inferring latent learner states from limited data. These approaches can support adaptive learning pathways, giving personalized recommendations based on predicted performance and knowledge gaps.


\paragraph{Discussion.} Despite their promise in automating and personalizing these discriminative tasks, LLMs still grapple with notable limitations that hinder their utility as robust tutoring tools. First, \emph{misalignment of assessment with expert instructors} poses risks: machine-generated scores may deviate from established rubrics or neglect qualitative nuances, leading to potential discrepancies in grading quality~\cite{kundu2024large}. Second, the \textit{lack of empathy} compounds this issue, as assessments devoid of human judgment risk discouraging learners or overlooking subtle motivational factors~\cite{sharma2024comuniqa}. Knowledge tracing approaches, while promising in cold-start scenarios, struggle with capturing the complexity of long-term learning trajectories and deeper cognitive processes~\cite{cho2024systematic}. These concerns point to the need for more transparent and human-centered methods in utilizing LLMs for assessment.

% Similarly, while LLM-based \textit{knowledge tracing} shows success in cold-start contexts, it remains unclear how well these models capture complex, long-term learner trajectories in real-world classrooms~\cite{cho2024systematic}. Critics argue that relying solely on algorithmic estimations of knowledge states may obscure students’ deeper cognitive processes, highlighting the need for more transparent and context-sensitive methods. Together, these concerns underscore that while LLMs offer scalable and immediate solutions, they must be carefully aligned with expert standards and learner-centered practices to realize their full potential in FLE.


\subsection{Generative Task Predictors}
Generative tasks involve producing new content or responses. LLMs are known to be adept in these tasks due to their natural language generation capabilities.

\paragraph{Grammatical Error Correction and Explanation.} In foreign language writing, errors often reveal learners’ gaps in grammar and vocabulary~\cite{hyland2006feedback}. LLMs can detect and correct these errors~\cite{bryant2023grammatical,ye-etal-2023-mixedit}, offering concise explanations~\cite{ye2024excgec} that reinforce language rules. By streamlining error detection and pedagogically framing corrections, learners deepen their linguistic understanding.

\paragraph{Feedback Generation.} Quizzes and exercises remain vital in FLE for practice and targeted remediation~\cite{rashov2024modern}. LLMs enhance this process by delivering prompt, personalized feedback that pinpoints strengths and addresses weaknesses~\cite{borges-etal-2024-teach}. This scalability enables learners to self-regulate and refine their skills without relying solely on human graders~\cite{stamper2024enhancing}.

\paragraph{Socratic Dialogue.} Moving beyond straightforward Q\&A, Socratic questioning promotes critical thinking and self-reflection~\cite{paul2007critical}. \textit{SocraticLM}~\cite{liusocraticlm}, for example, aligns an LLM with open-ended, inquiry-based teaching principles, guiding learners through iterative exploration rather than prescriptive correction. In theory, this fosters deeper conceptual understanding and active learner engagement.

\paragraph{Discussion.} Despite the promise of LLM-based generation in FLE, multiple uncertainties persist. \textit{Determining how to provide automatic feedback that genuinely maximizes learning outcomes} is an ongoing challenge~\cite{stamper2024enhancing}, particularly given education’s risk-averse culture and high accountability standards~\cite{xiao2024humanaicollaborativeessayscoring}. Moreover, while LLMs like SocraticLM have demonstrated success in domains like mathematics, their applicability to FLE contexts has not been thoroughly validated~\cite{liusocraticlm}. As such, the design of strategies and follow-up queries remains an open question in ensuring that these systems track and respond to learners' cognitive states effectively.


\subsection{Mixed Task Predictors}
Mixed tasks integrate discriminative and generative elements, requiring LLMs to evaluate learner inputs and generate meaningful feedback or suggestions. These tasks are particularly valuable in fostering an interactive and adaptive learning experience, as they bridge the gap between evaluation and instruction.

\paragraph{Automated Assessment with Feedback.} While discriminative systems for essay scoring and speech evaluation primarily focus on assigning grades, LLMs extend these capabilities by simultaneously generating formative feedback~\cite{katuka2024investigating,stahl2024exploring}. For example, an LLM can evaluate the coherence and lexical diversity of a written assignment, then offer specific revision strategies. In speaking practice, it can measure fluency and pronunciation accuracy while suggesting drills to refine intonation or stress patterns. Through this combination of scoring and tailored advice, learners gain a deeper understanding of their strengths and areas for improvement.

\paragraph{Error Analysis.} Error Analysis systematically uncovers and categorizes learners’ missteps, from syntactic lapses in writing to flawed pronunciations in speaking~\cite{james2013errors,erdougan2005contribution}. LLMs functioning in a mixed capacity can classify these errors and generate corrective guidance, providing revised sentences, clarifications of grammatical rules, or remediation exercises for identified weaknesses~\cite{myles2002second,mashoor2020error}. Such insight facilitates targeted interventions that enhance language proficiency across modalities, including reading and listening.

% \paragraph{Error Analysis.} Error Analysis is the systematic identification, classification, and explanation of errors made by learners in the process of acquiring a foreign language~\cite{james2013errors}. It is a critical component of language education, offering insights into learners' developmental stages and areas of difficulty~\cite{erdougan2005contribution}. This task is inherently mixed, as it requires discriminative capabilities to classify error types (e.g., grammatical, lexical, phonological) and generative capabilities to provide corrective feedback, explanations, and actionable improvement suggestions. For instance, in writing, LLMs can identify and categorize errors in syntax or word choice while generating revised sentences and explanations of the underlying grammar rules~\cite{myles2002second}. In reading, they can highlight misinterpretations of text and suggest strategies for improving comprehension~\cite{leu1982oral}. In listening and speaking~\cite{mashoor2020error}, LLMs can pinpoint pronunciation errors, mishearings, or unnatural phrasing, offering tailored exercises to enhance fluency and accuracy~\cite{kumar2025sealspeakererrorcorrection}. By addressing errors across these modalities, LLMs can deliver a holistic and adaptive learning experience that fosters deeper language proficiency.


\paragraph{Discussion.} Mixed-task systems hold promise by combining assessment and feedback generation, but they face notable challenges. One major issue is the \textit{weak alignment} between scoring mechanisms and the quality of feedback provided~\cite{stahl2024exploring}. For example, while essay scoring systems may deliver comprehensive evaluations, the feedback often lacks specificity, limiting its instructional value. Additionally, although error analysis has potential, \textit{the absence of standardized pedagogical benchmarks}, especially in oral tasks, hampers the reliability and comparability of LLM-based tools~\cite{leu1982oral}. Furthermore, by addressing errors across these modalities, LLMs can deliver a holistic and adaptive learning experience that fosters deeper language proficiency~\cite{zhao2024embodied}.


\begin{tcolorbox}[top=1pt, bottom=1pt, left=1pt, right=1pt]
\textbf{Our position.} While LLMs offer scalable solutions for task prediction in FLE, their current limitations—such as misalignment with expert assessments, lack of empathy, and weak alignment between assessment and feedback—require ongoing refinement. \textit{Future research} should focus on improving model transparency, enhancing the cultural and emotional sensitivity of LLMs, and refining task predictors to better reflect long-term learning trajectories and learner motivation. Additionally, developing standardized pedagogical benchmarks for error analysis, especially in oral tasks, will help ensure the consistency and reliability of LLM-generated feedback.
\end{tcolorbox}
