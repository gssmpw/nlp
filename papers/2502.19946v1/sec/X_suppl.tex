\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Additional Experimental Details}
\label{sec:addexperimental}
%-------------------------------------------------------------------------
\subsection{Additional Benchmark Details}
\label{sec:addbenchmark}
In this section, we provide detailed information on the two benchmarks used in our work.

\noindent{\textbf{OOD Benchmark.}}
 OOD benchmark is used to validate the model's ability to generalize to data of the same class but with different styles, assessing its robustness and effectiveness against distributional shifts. For the OOD benchmark, we used ImageNet~\cite{imagenet} along with four OOD sub-datasets to evaluate our method's performance on out-of-distribution data. These four datasets include ImageNet-A~\cite{imageneta}, ImageNet-R~\cite{imagenetr}, ImageNet-V2~\cite{imagenetv2}, and ImageNet-S~\cite{imagenetsk}. Below, we provide a brief overview of each OOD dataset.
\begin{itemize}
    \item \textbf{ImageNet-A}~\cite{imageneta}: ImageNet-A is a curated dataset containing 200 challenging classes of images for standard ImageNet-trained models. The dataset is composed of images from the real world that are likely to cause model misclassification, specifically selected to highlight the limitations of traditional models when recognizing out-of-distribution or adversarial samples. 
    \item \textbf{ImageNet-R}~\cite{imagenetr}: ImageNet-R is a dataset derived from ImageNet, specifically designed to test model robustness under significant changes in visual style, covering 200 classes. "R" stands for "Renditions," and the dataset includes images in a variety of artistic styles, such as paintings, cartoons, and sculptures. These images differ significantly from standard ImageNet photographs, making them particularly suitable for evaluating a model's ability to generalize beyond typical photographic representations. 
    \item \textbf{ImageNet-V2}~\cite{imagenetv2}: ImageNet-V2 is a dataset designed to evaluate the consistency and robustness of models trained on the original ImageNet dataset, consisting of 1000 classes. It was created by re-sampling the original ImageNet categories using methods that are similar but not identical to the original collection process. ImageNet-V2 aims to measure the generalization ability of models, as it mimics the distribution of the original dataset while incorporating new, previously unseen samples.
    \item \textbf{ImageNet-S}~\cite{imagenetsk}: ImageNet-S is a dataset derived from ImageNet, containing 1000 classes, specifically designed to evaluate a model's sensitivity to background changes and its ability to focus on salient features. "S" stands for "Sketches," and the dataset consists of black-and-white sketches of the original ImageNet classes. The simplified and abstract nature of the sketches challenges models to classify images based solely on basic contours and shapes, rather than relying on background context or texture information. 
\end{itemize}
\noindent{\textbf{Cross-Dataset Benchmark.}}
The cross-dataset benchmark consists of 10 image classification datasets, each representing a distinct domain and category, designed to evaluate the model's effectiveness and generalization capability across diverse scenarios. The benchmark includes the following datasets: Caltech101 for general image classification; OxfordPets (Pets), StanfordCars (Cars), Flowers102, Food101, and FGVCAircraft (Aircraft) for fine-grained image classification; EuroSAT for satellite imagery classification; UCF101 for action recognition; DTD for texture classification; and SUN397 for scene classification.

For the number of classes and the number of test samples for each dataset in both benchmarks, please refer to the table \ref{tab:supp_datasets}.
\input{tables/suppl/datasets}
%-------------------------------------------------------------------------
\subsection{Additional Comparison Methods Details}
\label{sec:addcmparison}
In this section, we provide a detailed description of the methods compared in our work.

\noindent{\textbf{CoOp}~\cite{coop}}: CoOp~\cite{coop} aims to perform automatic prompt optimization for vision-language models (e.g., CLIP) to achieve better few-shot learning and cross-domain generalization. CoOp replaces manually crafted prompt tokens with learnable context vectors while keeping the pre-trained model parameters unchanged. These context vectors are optimized by learning task-specific information from the data, significantly improving model performance.

\noindent{\textbf{CoCoOp}~\cite{cocoop}}: CoCoOp~\cite{cocoop} is an extension of the previous CoOp method. CoCoOp learns a lightweight neural network to generate context prompts conditioned on the input image, making the prompts dynamic rather than static, and adjusting them for each instance. This allows CoCoOp to better adapt to class variations, thereby enhancing the model's generalization ability to new classes.

\noindent{\textbf{Tip-Adapter}~\cite{tip}}: Tip-Adapter~\cite{tip} is designed to adapt the CLIP model for few-shot classification in a training-free manner. Tip-Adapter is based on a key-value cache model, constructing a non-parametric adapter from a small number of training samples without any additional training. It extracts features from few-shot images using CLIP's visual encoder and stores these features along with corresponding pseudo-labels in a cache, leveraging feature retrieval for inference. This approach enables the CLIP model to incorporate few-shot knowledge without retraining, achieving performance comparable to models that require training.

\noindent{\textbf{TPT}~\cite{tpt}}: TPT~\cite{tpt} dynamically adjusts adaptive prompts during testing, using only a single test sample without requiring additional training data or annotations. The method optimizes prompts by minimizing the marginal entropy between augmented views to ensure consistent predictions for different augmented versions of each test sample. Additionally, TPT introduces a confidence selection mechanism to filter out low-confidence augmented samples, thereby reducing the impact of noise.

\noindent{\textbf{DiffPT}~\cite{difftpt}}: DiffTPT~\cite{difftpt} utilizes a pre-trained diffusion model to generate diverse and informative augmented data, while maintaining prediction accuracy through cosine similarity filtering. This method combines traditional data augmentation with diffusion-based augmentation, enabling the model to improve its adaptability when encountering novel data without the need for retraining.

\noindent{\textbf{MTA}~\cite{mta}}: MTA~\cite{mta} employs a robust multimodal MeanShift algorithm to manage augmented views during testing by directly optimizing the quality evaluation of augmented views, referred to as the "inherence score." This method does not require prompt tuning and does not rely on complex training processes, enabling efficient adaptation to new data.

\noindent{\textbf{TDA}~\cite{tda}}: TDA~\cite{tda} uses a lightweight key-value cache to dynamically maintain a small number of pseudo-labels and test sample features. It gradually adapts to test data through progressive pseudo-label refinement, without requiring backpropagation, making it highly efficient. TDA also introduces a negative pseudo-label mechanism, which assigns pseudo-labels to certain negative classes to reduce the impact of noisy pseudo-labels. By combining both positive and negative caches, TDA significantly improves the model's classification accuracy and generalization ability without retraining, while also greatly reducing test time.

%-------------------------------------------------------------------------
\section{Additional Implementation of SOBA}
\label{sec:addsoba}
In this section, we provide a detailed description of the overall process of handling the feature space with basis vectors in our SOBA method.
\subsection{SOBA Process}
\label{sec:soba_process}
The SOBA process includes the following key steps: for each test sample $x_{test}$, the algorithm first extracts the image feature \( f_{test} \) and text features \( W_{t} \) using CLIP's visual encoder \(E_{v}(\theta_v)\) and text encoder \(E_{v}(\theta_v)\), and calculates the original CLIP logits by Eq. \ref{eq:clip_logits}. It then generates pseudo-labels by applying one-hot encoding to the original logits by Eq. \ref{eq:onehot}, and updates the dynamic queue, which stores the image features, pseudo-labels, and logits. After that, we compute the prototype for each pseudo-class and calculates the covariance matrix of the queue by Eq. \ref{eq:means} and Eq. \ref{eq:cov}. 

Next, the prototypes are rotated using the SOBA method to obtain new class prototypes by Eq. \ref{eq:main_mat}, and the transformed logits are computed based on these rotated prototypes by Eq. \ref{eq:trans_logits}. Finally, the algorithm combines the original logits and the transformed logits with a weighting factor $\alpha$ to produce the final prediction. It is worth noting that to ensure the stability and accuracy of the obtained orthogonal basis and class prototypes, we update the prototypes every 10\% of the test samples. This strategy allows the algorithm to optimize the model's adaptability while maintaining computational efficiency, and reduces the impact of bases constructed from too few samples on the final results. The overall process is presented in Algorithm \ref{alg:whole_loop}.
\input{tables/suppl/pseudo_code}
\input{tables/suppl/pseudo_update}
%-------------------------------------------------------------------------
\subsection{Queue Update Process}
\label{sec:queue_update}
In this section, we explain how to perform enqueue and dequeue operations on the queue.

First, for each test feature $x_{test}$, the algorithm checks whether the queue \( L^{t-1}_{\hat{l}} \) corresponding to the current pseudo-label \( \hat{l} \) is full. If the queue is not full, the current feature \( f_{test} \) and its corresponding pseudo-label \( \hat{l} \) are simply enqueued, generating a new queue \( L^{t} \). If the queue is full, the algorithm first calculates the maximum entropy \( H_{max} \) in the queue, which represents the average uncertainty of the current features. Then, the algorithm compares the entropy of the current feature's logits \( H(logits_{ori}) \) with the maximum entropy \( H_{max} \). If the current feature's entropy is smaller than the maximum entropy, it indicates that the feature is more certain, and the algorithm removes the feature with the highest entropy from the queue and enqueues the current feature; otherwise, the queue remains unchanged. Finally, the algorithm returns the updated queue \( L^{t} \), which helps manage the updates of features and pseudo-labels, ensuring that the queue adapts to new data over time. The overall process is presented in Algorithm \ref{alg:update}.

%-------------------------------------------------------------------------
\section{Additional Ablation Study}
\label{sec:abbstudy}
\input{tables/suppl/queue_capacity_corss_dataset}
This section supplements the ablation experiment on queue capacity on the cross-dataset benchmark. Due to the complexity of the datasets in the cross-dataset benchmark, the performance of each dataset may vary differently as the queue capacity increases. For the Pets dataset~\cite{pets}, the best accuracy is achieved when the queue capacity per class is 32. We believe the reason is that the differences between different classes in the Pets dataset are significant, as these classes not only exhibit distinct visual features (such as fur color, shape, and body size), but also show considerable diversity in terms of image background, posture, and camera angle. Therefore, increasing the queue capacity can better capture the information of the feature space, allowing the reconstructed basis and class prototypes to more effectively reflect the differences between classes. Finally, we used $K=16$ as the overall queue capacity for the cross-dataset benchmark.




% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% 
% \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.