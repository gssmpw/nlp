
\begin{figure*}[ht]
\centerline{\includegraphics[width=1\linewidth]{fig/main.pdf}}
\caption{An overview of our method. Our method uses a dynamic queue to store representative samples and generates predictions for test examples based on these samples. This prediction is combined with zero-shot CLIP predictions to produce the final inference. Specifically, we maintain a dynamic queue of representative samples, selected based on minimum entropy of CLIP's predictions. Using these stored samples, we construct a basis transformation to facilitate feature space rotation. As testing progresses, we continuously update and utilize these mappings, allowing the decision boundaries obtained through reconstruction to become more refined and accurate. Finally, we combine the inferences from CLIP with those from the dynamic queue to obtain the final prediction.
}
\label{fig:overview}
\vspace{-4mm}
\end{figure*}

\section{Method}
\label{sec:method}
\subsection{A Training-free Baseline}
\label{sec:Preliminaries}

CLIP~\cite{clip} is a pre-trained vision-language model composed of two parts: a visual encoder and a text encoder, which we represent separately \(E_{v}(\theta_v)\) and \(E_{t}(\theta_t)\). In classification tasks, given a test image $x_{test}$ and \textit{N} classes, CLIP uses \(E_{t}(\theta_t)\) and \(E_{v}(\theta_v)\)  to encode handcrafted text descriptions of the \textit{N} classes and $x_{test}$. After obtaining the corresponding text embeddings ${\mathbf{W}}_{t}$ and visual embedding ${\boldsymbol{f}}_{test}$, CLIP matches the image with the most relevant text description to produce the final prediction as follows:
\begin{equation}\label{eq:clip_logits}
    logits_{\rm{ori}}={\boldsymbol{f}}_{test}{\mathbf{W}}_{t}^{\rm{T}}.
\end{equation}

\indent{Before starting our method, we first construct a training-free baseline method. We utilize a dynamic queue to store representative samples and use these samples to assist in the prediction of test examples. This prediction is combined with the zero-shot CLIP predictions to produce the final inference. Specifically, we dynamically store \textbf{K} test examples for each pseudo-classes, along with their corresponding pseudo-labels $\hat{l}$, using minimum entropy as the criterion. Here, the pseudo-labels are obtained by one-hot encoding the predictions ${\boldsymbol{f}}_{test}{{\mathbf{W}}_{t}}^{\rm{T}}$ for each sample:}
\begin{equation}\label{eq:onehot}
    \hat{l}={\rm{OneHot}}({\boldsymbol{f}}_{test}{{\mathbf{W}}_{t}}^{\rm{T}}).
\end{equation}

When the queue reaches capacity \textbf{K}, we update the queue by replacing the test sample with the highest entropy using the principle of minimizing entropy. Then, during testing, we use an NCM classifier to assist with classification:
\begin{equation}\label{eq:ncm}
    logits_{\rm{NCM}}=\rm{sim}({\boldsymbol{f}}_{test},\mathbf{\mu})
\end{equation}
where $\rm{sim}$ is the cosine similarity, and $\mathbf{\mu}$ is the class mean for each category in the queue.

%-------------------------------------------------------------------------

\subsection{Theoretical Foundation}
During testing, pre-trained models like CLIP often experience reduced generalization due to distribution shifts between downstream tasks and the pre-training dataset. Current approaches focus on improving the selection of augmented views to mitigate this. However, the inference process still faces challenges because the decision boundary remains based on the original CLIP's feature space. For categories with initially poor predictions, the decision boundary in the original feature space limits the effectiveness of augmented view selection, preventing more accurate decisions. This limitation undermines the model's scalability in TTA scenarios.

In this paper, our motivation is to overcome the limitations of the original CLIP feature space for test-time adaptation, aiming to identify a suitable basis. By using the basis to map the original CLIP feature space into a new space, we strive to provide a more effective decision boundary for the inference process. To accomplish this, we propose a training-free feature space rotation method, SOBA, to achieve test-time adaptation of CLIP in downstream tasks.

Before describing our solution, we first present a general explanation of the feature space rotation with basis transformation proposed in this paper. We start by defining a set of feature vectors \( W \in \mathbb{R}^{n \times d} \) as a linear combination of standard orthogonal matrices \({\mathcal{E}}=\{{\mathbf{e}}_{ij}\}_{i,j}\), where \({\mathbf{e}}_{ij} \in \mathbb{R}^{n \times d}\) is defined as a matrix with the \((i,j)\)-th element equal to 1 and all other elements equal to 0. Therefore, we can express \( W \) as:
\begin{equation}\label{eq:ori}
    W= \sum_{i=1}^{n}\sum_{j=1}^{d} w_{ij}{\mathbf{e}}_{ij},
\end{equation}
where, $w_{ij}$ represents the $(i,j)$-th element of $W$, which is also the coefficient of ${\mathbf{e}}_{ij}$.

In this paper, we use an arbitrary basis ${\mathcal{B}}=\{{{\textbf{\textit{b}}}}_{ij}\in {\mathbb{R}}^{n \times d}\}_{i\in[n],j\in[d]}$ to extend $W$. Specifically, ${\mathcal{B}}$ serves as a standard orthogonal basis and must satisfy the following conditions:
\begin{equation}
 \begin{aligned}
    \left \langle{{\textbf{\textit{b}}}},{{\textbf{\textit{b}}}}' \right \rangle=0 \;{\rm{if}}\: {{\textbf{\textit{b}}}} \neq {{\textbf{\textit{b}}}}'\:{\rm{for}}\: {{\textbf{\textit{b}}}},{{\textbf{\textit{b}}}}' \in {\mathcal{B}},\\
    \left \| {{\textbf{\textit{b}}}} \right \| =\sqrt{\left \langle{{\textbf{\textit{b}}}},{{\textbf{\textit{b}}}} \right \rangle}=1 \;{\rm{for}} \,{\rm{all}}\:{{\textbf{\textit{b}}}}\in {\mathcal{B}},
\end{aligned} 
\end{equation}
where, $\left\| \cdot \right\|$ and $\langle \cdot \rangle$ represent the norm and inner product, respectively.


Since the vector hilbert space $\mathcal{H} := \mathbb{R}^{n \times d}$ satisfies the inner product operation $\langle{ \rm{C}}, {\rm{D}} \rangle = \text{trace}({\rm{C}}^{\rm{T}}{\rm{D}})$ (where $ {\rm{C}}, {\rm{D}} \in \mathcal{H}$), we can always express $W \in \mathcal{H}$ as a linear combination of orthogonal matrices in the basis $\mathcal{B}$ under any circumstances. Therefore, Eq.\ref{eq:ori} can be expanded into the following form:
\begin{equation}\label{eq:contrust}
    W=\sum_{{{\textbf{\textit{b}}}}\in{\mathcal{B}}}\left \langle W,{{\textbf{\textit{b}}}} \right \rangle{{\textbf{\textit{b}}}}=\sum_{i=1}^{n}\sum_{j=1}^{d}\left \langle W,{{\textbf{\textit{b}}}}_{ij}\right \rangle{{\textbf{\textit{b}}}}_{ij}.
\end{equation}

We observe that when ${\mathcal{B}} = {\mathcal{E}}$, Eq.\ref{eq:contrust} reduces to Eq.\ref{eq:ori}. Consequently, when all elements in ${\mathcal{B}}$ are orthogonal matrices, we can use ${\mathcal{B}}$ to project $W$ onto a new hypersphere through the mapping $\hat{w}= \{\left \langle W,{{\textbf{\textit{b}}}} \right \rangle \}_{{{\textbf{\textit{b}}}}\in{\mathcal{B}}}$. In Section \ref{sec:SOBA}, we will describe how to use SOBA to address challenges in the TTA task.
\input{tables/domain/OOD}
%-------------------------------------------------------------------------
\subsection{Space Rotation with Basis Transformation}
\label{sec:SOBA}
In this section, we first introduce how to construct an appropriate basis vector matrix using SOBA. Then, we explain how to implement it through parameter estimation.

%-------------------------------------------------------------------------

\noindent\textbf{Basis Construction.}
To identify an appropriate basis for reconstructing the matrix \( W \in \mathbb{R}^{n \times d} \), we begin by defining the basis using a pair of unitary matrices. Let \( P \in \mathbb{R}^{n \times n} \) and \( Q \in \mathbb{R}^{d \times d} \) be two arbitrary unitary matrices. We observe that the set \( \mathcal{B} = \{ {\textbf{\textit{b}}}_{ij} := p_{i} q_{j}^{\rm{T}} \in \mathbb{R}^{n \times d} \}_{i \in [n], j \in [d]} \) forms an orthogonal basis, where \( p_{i} \) and \( q_{j} \) represent the \( i \)-th column of \( P \) and the \( j \)-th column of \( Q \), respectively. Consequently, we can express Eq.\ref{eq:contrust} as follows: 

\begin{equation}\label{eq:main}
\begin{split}
W&=\sum_{i=1}^{n}\sum_{j=1}^{d}\left \langle W,{{\textbf{\textit{b}}}}_{ij}\right \rangle{{\textbf{\textit{b}}}}_{ij}\\
&=\sum_{i=1}^{n}\sum_{j=1}^{d}\left \langle W,p_{i} q_{j}^{\rm{T}}\right \rangle p_{i} q_{j}^{\rm{T}}\\
&=\sum_{i=1}^{n}\sum_{j=1}^{d}\hat{w}_{ij}p_{i} q_{j}^{\rm{T}} ,
\end{split}
\end{equation}
where $\hat{w} := \left \langle W, p_{i} q_{j}^{\rm{T}} \right \rangle$. In this case, the basis $\{ p_{i} q_{j}^{\rm{T}} \}_{i,j}$, constructed from a pair of unitary matrices $P$ and $Q$, maps $W$ into the form of $\hat{w}$. Now,  the current challenge is \textit{how to design \( P \) and \( Q \) to achieve a better basis transformation, thereby obtaining an improved space mapping to address distribution shifts in downstream tasks}. 

According to the theory of PCA~\cite{pca}, for a set of feature vectors, we can perform singular value decomposition on their covariance \( C\) to extract the main information:
\begin{equation}\label{eq:pca}
    C=Q_{c}\Sigma Q_{c}^{\rm{T}},
\end{equation}
where \(\Sigma\) is a diagonal matrix with singular values on its diagonal, and \(Q_{c}\) is the corresponding unitary matrix. As observed in the literature~\cite{lowrank}, the features obtained from deep neural networks are often low-rank, meaning that most singular values are close to zero. Due to this low-rank property, for any unitary matrix \( P \), setting \( Q = Q_{c} \) allows us to extract important information from \( W \) under the basis \( \mathcal{B} = \{p_{i} q_{j}^{\rm{T}} \} \) and map this information to \(\hat{w}\). We will introduce how to obtain the covariance matrix \( C \) in Eq.\ref{eq:cov}.
%-------------------------------------------------------------------------

%-------------------------------------------------------------------------
\noindent\textbf{Implementation.}
Subsequently, we will examine the implementation of our proposed method building upon the foundation of the baseline approach in \ref{sec:Preliminaries}. Based on the dynamic queue of the baseline, we utilize SOBA to map the stored features onto a hypersphere, thereby achieving feature reconstruction. The following describes how to implement Eq.\ref{eq:main}.
\input{tables/cross_dataset/CORSS}
\noindent{\textit{Implementation of W}: Similar to the NCM classifier~\cite{ncm}, we use the class mean \(\mathbf{\mu}=\{{\mathbf{\mu}_{k}}\}_{k=1}^{N}\) from the queue as the classifier weights. Setting \( W=\mathbf{\mu} \) in Eq.\ref{eq:main} gives us the mapped class mean \(\hat{\mathbf{\mu}}\). Here, we use the empirical mean to estimate the class mean:}
\begin{equation}\label{eq:means}
    {\mathbf{\mu}_{k}} = \frac{\sum_{i=1}^{M_{k}} \mathbb{I}_{\hat{l}=k} f_{test,i}}{\sum_{i=1}^{M_{k}} \mathbb{I}_{\hat{l}=k}},
\end{equation}
where, $M_{k}$ is the total number of class k. \(\hat{l}\) is the pseudo-label of samples in the queue.

\noindent{\textit{Implementation of \(P=\{p_{i}\}\) and \(Q=\{q_{j}\}\)}:
%在实践中，我们使用了一种非常简单的方式实现公式\ref{eq:main}。由于\(PP^{\rm{{T}}=I_{n}}\)和\(QQ^{\rm{{T}}=I_{d}}\)，我们可以得到：\(W=PP^{\rm{{T}}WQQ^{\rm{{T}}=\)
In practice, we implement Eq.\ref{eq:main} using a very straightforward approach. Due to the properties of the unitary matrix, we can obtain \(PP^{\rm{T}} = I_{n}\) and \(QQ^{\rm{T}} = I_{d}\). Then, we express \( W \) as following:
\begin{equation}\label{eq:main_mat}
    W=PP^{\rm{{T}}}WQQ^{\rm{{T}}}=P{\hat{W}}Q.
\end{equation}

%在整个流程中，由于酉矩阵的性质，我们令\(P=I_{n}}\)，根据\(\hat{w}_{ij}\)是\(\hat{W}\)的第(\textit{i}，\textit{j})个元素，我们只需要将酉矩阵乘\(W\)即可实现SOBA映射。在此期间，我们使用以下方式来估计协方差矩阵：
Throughout the process, we set \( P = I_{n} \) and \( Q = Q_{c} \)~(\(Q_{c}\) is obtained from Eq.\ref{eq:pca}). Since \(\hat{w}_{ij}\) is the \((i,j)\)-th element of \(\hat{W}\), we only need to multiply the unitary matrix by \( W \) to achieve the SOBA mapping. During this time, we estimate the covariance matrix using the following approach:
\begin{equation}\label{eq:cov}
    C= \frac{1}{N}\sum_{k=1}^{N} \frac{ {\textstyle \sum_{i=1}^{{M}_{k}}\mathbb{I}_{\hat{l}=k}(f_{test,i}-{\mathbf{\mu}_{k}})(f_{test,i}-{\mathbf{\mu}_{k}})^{\rm{T}}  } }{ {\textstyle \sum_{i=1}^{M_{k}}} \mathbb{I}_{\hat{l}=k}} ,
\end{equation}
%这里，为了减少计算量，我们采用了GDA~\cite{gda}假设计算协方差矩阵，即所有类别服从一个同协方差的分布。
where to reduce the computational burden, we adopt the GDA~\cite{gda} assumption for calculating the covariance matrix, which states that all classes follow a distribution with a common covariance.

%最终我们得到SOBA分类器如下：
Ultimately, we obtain the SOBA classifier as follows:
\begin{equation}\label{eq:trans_logits}
    logits_{\rm{trans}}={\rm{Linear}}({\boldsymbol{f}}_{test},\hat{\mathbf{\mu}}).
\end{equation}

%此外，在推理过程中，我们采用每10%测试样例更新一次协方差和均值的方式来进一步减少计算量。最终，我们的采用混合预测来整合最终的logits输出。因此，测试图像的输出logits计算为：
Additionally, during the inference process, we update the covariance and mean every 10\% of the test samples to further reduce the computational burden. Ultimately, we employ mixed predictions to consolidate the final logits output. Therefore, the output logits for the test images are calculated as follows:
\begin{equation}\label{eq:last}
    logits= {\boldsymbol{f}}_{test}{{\mathbf{W}}_{t}}^{\rm{T}}+{\alpha}\times logits_{\rm{trans}},
\end{equation}
where \(\alpha\) is a hyperparameter.

%-------------------------------------------------------------------------

% You must include your signed IEEE copyright release form when you submit your finished paper.
% We MUST have this form before your paper can be published in the proceedings.
% Please direct any questions to the production editor in charge of these proceedings at the IEEE Computer Society Press:
% \url{https://www.computer.org/about/contact}.