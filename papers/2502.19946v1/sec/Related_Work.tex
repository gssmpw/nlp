\section{Related Works}
\label{sec:rw}

%-------------------------------------------------------------------------
\noindent \textbf{Vision-Language Model.}
In recent years, vision-language models, as a novel tool capable of processing both visual and linguistic modalities, have garnered widespread attention. These models, such as CLIP~\cite{clip}, ALIGN~\cite{ALIGN}, BLIP~\cite{BLIP}, FILIP~\cite{filip}, etc., leverage self-supervised training on image-text pairs to establish connections between vision and text, enabling the models to comprehend image semantics and their corresponding textual descriptions. This powerful understanding allows vision-language models (e.g., CLIP) to exhibit remarkable generalization capabilities across various downstream tasks~\cite{downsteam1,downsteam2,downsteam3,h2b}. To further enhance the transferability of vision-language models to downstream tasks, prompt tuning and adapter methods have been applied. However, methods based on prompt tuning (such as CoOp~\cite{coop}, CoCoOp~\cite{cocoop}, Maple~\cite{maple}) and adapter-based methods (such as Tip-Adapter~\cite{tip}, CLIP-Adapter~\cite{clip_adapter}) often require large amounts of training data when transferring to downstream tasks, which conflicts with the need for rapid adaptation in real-world applications. Therefore, this paper focuses on test-time adaptation~\cite{tpt}, a method that enables transfer to downstream tasks without relying on training data.

%-------------------------------------------------------------------------
\noindent \textbf{Test-Time Adaptation.}
Test-time adaptation~(TTA) refers to the process by which a model quickly adapts to test data that exhibits distributional shifts~\cite{tta1,memo,ptta,domainadaptor,dota}. Specifically, it requires the model to handle these shifts in downstream tasks without access to training data. TPT~\cite{tpt} optimizes adaptive text prompts using the principle of entropy minimization, ensuring that the model produces consistent predictions for different augmentations of test images generated by AugMix~\cite{augmix}. DiffTPT~\cite{difftpt} builds on TPT by introducing the Stable Diffusion Model~\cite{stable} to create more diverse augmentations and filters these views based on their cosine similarity to the original image. However, both TPT and DiffTPT still rely on backpropagation to optimize text prompts, which limits their ability to meet the need for fast adaptation during test-time. TDA~\cite{tda}, on the other hand, introduces a cache model like Tip-Adapter~\cite{tip} that stores representative test samples. By comparing incoming test samples with those in the cache, TDA refines the modelâ€™s predictions without the need for backpropagation, allowing for test-time enhancement. Although TDA has made significant improvements in the TTA task, it still does not fundamentally address the impact of test data distribution shifts on the model and remains within the scope of CLIP's original feature space. We believe that in TTA tasks, instead of making decisions in the original space, it would be more effective to map the features to a different spherical space to achieve a better decision boundary.

%-------------------------------------------------------------------------
\noindent \textbf{Statistical Learning.}
Statistical learning techniques play an important role in dimensionality reduction and feature extraction. Support Vector Machines~(SVM)~\cite{svm} are primarily used for classification tasks but have been adapted for space mapping through their ability to create hyperplanes that separate data in high-dimensional spaces. The kernel trick enables SVM to operate in transformed feature spaces, effectively mapping non-linearly separable data. PCA~\cite{pca} is a linear transformation method that maps high-dimensional data to a new lower-dimensional space through a linear transformation, while preserving as much important information from the original data as possible.