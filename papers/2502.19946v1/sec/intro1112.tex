\section{Introduction}
\label{sec:intro}


\begin{figure*}[ht]
\centerline{\includegraphics[width=1\linewidth]{fig/head.pdf}}
\caption{(a) Feature confusion generated in the original CLIP space. It is evident that the original CLIP feature space contains confounding classes. For training-free methods, the lack of capability to adjust the feature space imposes limitations on their subsequent applicability. (b) Feature space reconstructed through transformation. We utilize new basis vectors (such as ${\textit{\textbf{b}}}_{1}$ and ${\textit{\textbf{b}}}_{2}$ in the Fig. (b)) to transform the feature space into a new space. In this space, we can address the confusion present in the original CLIP and overcome the limitations of training-free methods that cannot adjust the feature space.
 (c) Performance comparison on the OOD benchmark. Our method surpasses state-of-the-art methods almost on all datasets.}
\label{fig:head}
\vspace{-3mm}
\end{figure*}

Visual-language models~(VLM), such as CLIP~\cite{clip} and ALIGN~\cite{ALIGN}, have garnered significant attention from researchers due to their strong generalization capabilities in downstream tasks. Various efficient tuning methods, such as prompt tuning~\cite{coop,cocoop,maple} and adapter tuning~\cite{tip,clip_adapter}, have been proposed to leverage training data for enhancing the performance of VLMs on downstream tasks. While those have achieved notable results, their effectiveness is largely limited to the distribution of the current datasets, making it challenging to generalize to domains or distributions beyond the training data. 

In this context, the test-time adaptation~(TTA) was proposed to rapidly adapt to downstream data distributions by utilizing given test samples. Since it requires no training data or annotations, it holds broad application potential in real-world scenarios. The present mainstream TTA methods for VLMs can be divided into two categories: (i) Prompt-tuning TTA paradigm. TPT~\cite{tpt} and DiffTPT~\cite{difftpt} tune prompts through different data augmentation and confidence selection strategies, ensuring consistent predictions across different augmented views of each test data. (ii) Training-free TTA paradigm. TDA~\cite{tda} proposes a training-free dynamic adapter and maintains a high-quality test set to guide the test-time adaptation for VLM. Among them, the prompt-tuning TTA methods~\cite{tpt,difftpt} demand substantial computational resources and time, contradicting the need for rapid adaptation in real-world scenarios. Therefore, this paper focuses on the training-free TTA paradigm. 

Despite its decent performance, the training-free TTA method has a significant drawback, which stems from the characteristics of the training-free paradigm. Due to the inability to perform training, adjusting the feature space becomes very difficult, and thus, the effectiveness of the ``guidance'' entirely depends on the original CLIP feature space. As shown in Fig.~\ref{fig:head}~(a), the test samples inside the red circle are hard for CLIP to predict accurately due to the overlap of decision boundaries. Currently, methods like TDA \cite{tda}, which compare test samples with representative samples in the original feature space to assist prediction, clearly cannot address this inherent drawback.


Inspired by classical machine learning theories~\cite{svm,pca,lle}, we propose a novel training-free test-time adaptation method called \textbf{S}pace r\textbf{O}tation with \textbf{B}asis tr\textbf{A}nsformation~(\textbf{SOBA}). This method utilizes basis transformation techniques~\cite{la} to convert the original nonlinearly separable space into a new linearly separable space, thereby optimizing the decision boundary of the original CLIP model and effectively overcoming the limitations of the training-free TTA paradigm. Specifically, during testing, we first generate one-hot encodings from the CLIP predictions for the test samples. Based on these encodings, we assign pseudo-labels to each sample and store the sample features and their corresponding pseudo-labels. These pseudo-labels and features are then used together to construct a new feature space. To ensure that the reconstructed basis $\mathcal{B}$ better reflects the differences between features of different classes, we perform covariance singular value decomposition on the stored sample set, extract the key information~\cite{pca}, and construct the orthogonal basis $\mathcal{B}$. Based on the basis $\mathcal{B}$, we reconstruct the original feature space, making the features more linearly separable in the new space. Next, we leverage the mean vectors of different classes in the transformed space as class weights to aid classification decisions during testing, thereby enhancing classification accuracy. Additionally, given the sample quality requirements for constructing the orthogonal basis, we maintain a limited dynamic queue to store samples, thereby mitigating the impact of noisy samples on the basis construction process. The queue, guided by an entropy minimization criterion, progressively selects and merges low-entropy prediction samples through enqueue and dequeue operations, thereby improving the quality of pseudo-labels and samples, which in turn enhances the construction of the orthogonal basis. As shown in Fig. \ref{fig:head} (b), the new space formed by the basis $\mathcal{B}$ better highlights the inter-class differences, surpassing the limitations of the original CLIP feature space in Fig. \ref{fig:head} (a) and providing better guidance for the inference process.

In this paper, we present three key contributions. First, we analyze the limitations of current training-free TTA methods in adjusting the feature space. Inspired by machine learning theories, we propose a space rotation method based on basis transformation, which reshapes the feature space and effectively solves the issue of inseparability in the original feature space. Second, our method is efficient. Experiments on the ImageNet dataset show that our method improves testing speed by 13.96\% compared to the SOTA training-free method TDA~\cite{tda}, while its time cost is only 2.15\% of that of the tuning-based method TPT~\cite{tpt}. Finally, our method achieves state-of-the-art (SOTA) performance across various benchmarks (Fig.~\ref{fig:head} (c)), effectively addressing distribution shifts in downstream tasks.
