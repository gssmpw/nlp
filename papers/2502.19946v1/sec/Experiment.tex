

\section{Experiment}
\label{sec:experiment}
\input{tables/abstudy/components}

%----------------------------------------------------------------
\subsection{Experimental Setup}
\label{sec:setup}
\noindent{\textbf{Benchmarks.}}
Based on previous work~\cite{tpt,difftpt,tda,mta}, we selected the out-of-distribution (OOD) benchmark and the cross-dataset benchmark as the foundational experiments for our study.
\begin{itemize}
    \item For the \textbf{OOD benchmark}, we tested the effectiveness of our method on out-of-distribution datasets using ImageNet and its four OOD sub-datasets, which include ImageNet-A~\cite{imageneta}, ImageNet-R~\cite{imagenetr}, ImageNet-V2~\cite{imagenetv2}, and ImageNet-S~\cite{imagenetsk}. The purpose of the OOD benchmark is to evaluate the model's generalization ability to data from the same class but different domain distributions.
    \item For the \textbf{cross-dataset benchmark}, we used 10 public datasets to evaluate the cross-dataset classification capability of our method. Each dataset comes from different classes and domains, including: Aircraft~\cite{aircraft}, Caltech101~\cite{caltech101}, Car~\cite{cars}, DTD~\cite{dtd}, EuroSAT~\cite{eurosat}, Flowers102~\cite{flowers}, Food101~\cite{food101}, Pets~\cite{pets}, SUN397~\cite{sun397}, and UCF101~\cite{ucf101}.
\end{itemize}

\noindent{\textbf{Comparison Methods.}}
We compared our method with zero-shot CLIP~\cite{clip}, CoOp~\cite{coop}, CoCoOp~\cite{cocoop}, Tip-Adapter~\cite{tip}, and other state-of-the-art (SOTA) methods in the TTA domain that do not require a training set, including TPT~\cite{tpt}, DiffTPT~\cite{difftpt}, MTA~\cite{mta}, HisTPT~\cite{histpt}, and TDA~\cite{tda}. Among these, Tip-Adapter cannot be evaluated on the cross-dataset benchmark because it is unable to handle unseen classes during the testing phase. Additionally, we do not include MTA in the comparison for experiments with ResNet-50 as the backbone, as there is no data available for MTA on ResNet-50. Furthermore, MTA+Ensemble refers to the ensemble prediction method provided in the MTA paper. Notably, the decision boundary of TDA is based on the original CLIP's feature space, while our method transcends this space.


\noindent{\textbf{Implementation Details}.}
Our method is built upon the pre-trained CLIP~\cite{clip}, where the text encoder of CLIP is a Transformer~\cite{attention_is_all_you_need}, and the image encoder can be either ResNet~\cite{resnet} or Vision Transformer~\cite{vit}. Since our method is training-free, all text prompts are manually crafted. To construct the dynamic queue, we set the batch size to 1. For the OOD benchmark, we conduct a hyperparameter search on ImageNet and apply the resulting hyperparameters to the remaining four OOD datasets. In the case of the cross-dataset benchmark, due to the diversity and complexity of the datasets, the length of the dynamic cache queue varies for each dataset. This will be further explored in an ablation study provided in the Appendix. Additionally, we use top-1 accuracy as the evaluation metric for our experiments, and all experiments are performed on an NVIDIA Quadro RTX 6000 GPU.

%----------------------------------------------------------------
\subsection{Comparison with State-of-the-arts}
\label{sec:compare}
We compare our method against zero-shot CLIP, CoOp, CoCoOp, Tip-Adapter, TPT, DiffTPT, MTA, and TDA. Notably, CoOp, CoCoOp, and Tip-Adapter require a training set for optimization, while TPT, DiffTPT, MTA, TDA, and our method do not. Due to methodological constraints, Tip-Adapter cannot be tested on unseen classes, and MTA does not provide accuracy results for the ResNet-50 backbone. Like TPT, DiffTPT, MTA, and TDA, we evaluate our method on both the \textbf{OOD benchmark} and the \textbf{cross-dataset benchmark}.

%----------------------------------------------------------------
\noindent \textbf{Results on the Out-of-Distribution Benchmark.}
Table \ref{tab:ood-main} provides a comparison between our method and state-of-the-art (SOTA) approaches across different backbones on ImageNet and four out-of-distribution (OOD) datasets. Our method surpasses existing approaches on all OOD datasets. Notably, it outperforms TDA with an increase of 0.68\% in OOD average accuracy using the ResNet-50 backbone and \textbf{1.17\%} with the ViT-B/16 backbone. Additionally, our approach demonstrates a significant \textbf{3.43\%} improvement over MTA with the ViT-B/16 backbone. These results affirm the effectiveness of exploring new decision boundaries beyond the original CLIP decision surface, validating our approach. 

%----------------------------------------------------------------
\input{tables/abstudy/runtime}
\noindent{\textbf{Efficiency Comparison.}}
As shown in Table \ref{tab:runtime}, to assess the efficiency of our method using ResNet-50 as the backbone, we compared it with three existing test-time adaptation methods on the ImageNet dataset, focusing on inference speed and accuracy. The performance metrics for CLIP-ResNet-50, TPT, DiffTPT, and TDA are sourced from the TDA paper. While our method sacrifices slight efficiency compared to zero-shot CLIP, it achieves a 2.04\% accuracy improvement. Unlike TPT and DiffTPT, which require backpropagation, our method significantly outperforms them in efficiency. Compared to TDA, our method enhances both efficiency and accuracy, improving inference time by 2m 14s and accuracy by 0.5\%. These results demonstrate the efficiency and suitability of our approach for test-time adaptation.

\label{sec:abstudy}

\begin{figure*}[!htbp]
    \centering
    \centerline{\includegraphics[width=1\linewidth]{fig/abstudy.pdf}}
    \caption{
    Subfigure (a) shows a comparison with other classifiers, where our SOBA achieves the best performance. Subfigure (b) presents a study on different dynamic queue lengths. Subfigure (c) presents a study on the impact of the hyperparameter $\alpha$. All experiments in the figure are based on ViT-B/16 and conducted on ImageNet~\cite{imagenet}.
}
    \label{fig:ablations_onthers}
    \vspace{-2mm}
\end{figure*}
%----------------------------------------------------------------
\noindent \textbf{Results on the Cross-Datasets Benchmark.}
To further validate the feasibility and effectiveness of our approach, we conducted comparisons with SOTA methods across 10 datasets spanning diverse categories and domains. As shown in Table \ref{tab:cross-dataset}, our method consistently outperforms competitors on both backbones tested. Using ResNet-50, our approach achieved top performance on 6 out of 10 datasets, with an average accuracy improvement of \textbf{1.13\%} over TDA. With ViT-B/16, our method led on 7 out of 10 datasets, surpassing TDA with a \textbf{1.79\%} increase in average accuracy. The performance on the cross-dataset benchmark further demonstrates that our method remains effective even when faced with datasets from different classes and domains. Moreover, our method does not require additional training or backpropagation on both benchmarks, making it well-suited for testing adaptation tasks with CLIP.

%----------------------------------------------------------------
\subsection{Ablation Studies}
In this section, we conduct ablation experiments to analyze the effectiveness of our design. Our baseline method is the one mentioned in Section ~\ref{sec:Preliminaries}.

\noindent{\textbf{Effectiveness of SOBA.}}
To clearly illustrate the effectiveness of our method, we compare it with a simple yet effective baseline. In Table 3, we report the ablation experiments on the OOD benchmark and cross-dataset benchmark, respectively. Since the baseline method also does not involve backpropagation and is based on the original CLIP feature space, comparing it with this baseline allows us to directly observe the pure benefit of the space rotation provided by SOBA. 
%----------------------------------------------------------------

Compared to baseline, our work demonstrates significant improvements across nearly all datasets in both benchmarks. Compared to the baseline, on the OOD benchmark, our two evaluation metrics, \textit{average} and \textit{OOD average}, improved by \textbf{1.6\%} and \textbf{1.53\%}. On the cross-dataset benchmark, we achieved a \textbf{2.19\%} improvement in \textit{average}. Combining our finding with the comparisons to TDA in Section \ref{sec:compare}, that rely on the original CLIP feature space, we can conclude that applying a basis transformation to rotate the original space is a feasible solution to address the TTA problem, and it achieves better performance than the original CLIP feature space.

%----------------------------------------------------------------
\noindent{\textbf{Comparison with Other Classifiers.}}
In Fig. \ref{fig:ablations_onthers}(a), we present a comparison of our method with other classifiers. Due to changes in the feature space, directly minimizing the Manhattan~(L1) distance and Euclidean~(L2) distance to class centers is no longer applicable, and it even results in degradation compared to zero-shot CLIP. Our method, compared to the basic NCM classifier, achieves better decision boundaries by utilizing the rotated space, further addressing the test-time adaptation problem.

%----------------------------------------------------------------
\noindent{\textbf{Hyperparameter Aensitivity Analysis.}}
\begin{itemize}
    \item \textbf{Queue Capacity K.} In Fig. \ref{fig:ablations_onthers}(b), we report the impact of dynamic queue  Capacity. We find that as the  Capacity of the dynamic queue increases, the overall accuracy shows a trend of first increasing and then decreasing. This can be understood as follows: when the queue  Capacity is small, the stored features are very representative, but as the queue  Capacity increases, some easily confusable features are added, affecting subsequent judgments. In this paper, we select 16 as the storage limit for each class in our dynamic queue on the OOD benchmark.
    \item \textbf{$\alpha$.} In Fig. \ref{fig:ablations_onthers}(c), we illustrate the impact of $\alpha$ from Eq.\ref{eq:last}. Based on the performance on ImageNet, we ultimately select $\alpha=15$ as the final value.
\end{itemize}


