We present novel optimization guarantees for %the convergence of 
gradient descent for neural operators with smooth activations: % functions: 
%. We focus on neural operators with smooth activations and analyze two popular models: 
Deep Operator Networks and Fourier Neural Operators.
%, both in their simplest possible architectural configuration, i.e. feedforward networks. 
Our guarantees are based on the restricted strong convexity and smoothness of the loss, 
thus
%\pccomment{How many hidden layers are you using for each branch/trunk network? There is no specification.}
%
%Our analysis is the first of its kind for neural operators and 
%
%provides
providing 
an encompassing framework to neural operator optimization. 
\pcedit{We argue that increasing the width of the neural operators benefits our theoretical guarantees.} 
We also present empirical evaluations on prototypical 
operator learning problems 
to complement our theory.


\paragraph{Acknowledgements.} Part of the work was done when Pedro Cisneros-Velarde and Bhavesh Shrimali were affiliated with the University of Illinois Urbana-Champaign and was concluded during their current affiliations. The work was supported by the National Science Foundation (NSF) through awards IIS 21-31335, OAC 21-30835, DBI 20-21898, as well as a C3.ai research award.
