We now establish {\em two conditions}---Conditions \ref{cond:rsc} and~\ref{cond:smooth} below---for the convergence of gradient descent (GD) when minimizing a 
%of any model whose training can be expressed as the minimization of a 
loss function $\cL$.
%---including neural operators. 
We show that as long as these two conditions are satisfied, the loss will decrease in value. 
In the following sections we show how the empirical losses used for training DONs (Section~\ref{sec:optDON}) and FNOs (Section~\ref{sec:optFNO}), as in \eqref{eq:loss-don} and~\eqref{eq:loss-fno} respectively, satisfy these two conditions. % properties specific to these models.
%
    
%

\pcedit{We consider $\vtheta\mapsto\gL(\vtheta)$ to be continuously differentiable.} Let $\vtheta_0\in\R^p$ be a suitable initialization point and $\{\vtheta_t\}_{t\geq 1}$ be the sequence of iterates obtained from GD on loss $\cL$ for some step-size $\eta_t>0$, i.e.,
\begin{equation}
\vtheta_{t+1} = \vtheta_{t} - \eta_{t} \nabla_{\vtheta} \gL(\vtheta_t )~.
    \label{eq:gd_at_t}
\end{equation}
We consider a non-empty set $\cB(\vtheta_0)\subseteq\R^p$ around and including $\vtheta_0$. 

\begin{asmp}[{\bf Iterates inside $\cB(\vtheta_0)$}]
\label{asmp:iter-0}
All iterates $\{\vtheta_t\}_{t\geq 1}$ follow GD as in~\eqref{eq:gd_at_t} and are inside the set $\cB(\vtheta_0)$.
\end{asmp}

The first condition is based on the concept of Restricted Strong Convexity (RSC) being satisfied for $\cL$. % at time step $t$. 
\begin{defn}[{\bf Restricted strong convexity (RSC)}] A function $\mathcal{L}$ is said to satisfy $\alpha$-restricted strong convexity ($\alpha$-RSC) w.r.t.~the tuple $(\mathcal{S}, \vtheta)$ if for any $\vtheta^{\prime} \in \mathcal{S} \subseteq \mathbb{R}^p$ and some fixed $\vtheta \in \mathbb{R}^p$, we have 
\begin{align}
\label{eq:RSC-prim}
\mathcal{L}\left(\vtheta^{\prime}\right) \geq \mathcal{L}(\vtheta) + \left\langle\vtheta^{\prime}-\vtheta, \nabla_\vtheta \mathcal{L}(\vtheta)\right\rangle+\frac{\alpha}{2}\left\|\vtheta^{\prime}-\vtheta\right\|_2^2~, 
\end{align}
with $\alpha>0$.
\end{defn}
%
%
%

%Thus, the first condition of interest stipulates that the loss $\cL$ satisfies the RSC condition at step $t$.
%
\begin{cond}[{\bf RSC}]
Consider Assumption~\ref{asmp:iter-0}.
At step $t$, 
there exists a non-empty set $\mathcal{N}_t$ 
% B^{\mathrm{Euc}}_{\rho}(\vtheta_0)
such that:
\begin{itemize}
\item[(a)] %$\vtheta_0$
%$\vtheta_t,
$\mathcal{N}_t\subseteq \cB(\vtheta_0)$;
\item[(b)] one of these two conditions hold:
\begin{itemize}
    \item[(b.1)] $\vtheta_{t+1}\in \mathcal{N}_t$ with either $\vtheta_{t}\notin \mathcal{N}_t$ or $\gL(\vtheta_{t})\neq\inf_{\vtheta\in\mathcal{N}_t}\cL(\vtheta)$,\label{condb1} 
    \item[(b.2)] there exists some $\vtheta'\in\mathcal{N}_t$ such that $\cL(\vtheta')<\cL(\vtheta_{t})$;\label{condb2}
\end{itemize}
\item[(c)] $\gL$ satisfies $\alpha_t$-RSC w.r.t.~$(\mathcal{N}_t,\vtheta_t)$ for some $\alpha_t > 0$.
\end{itemize}
\label{cond:rsc}
\end{cond}

%
Note that $\gL$ need not be convex for it to satisfy $\alpha_t$-RSC.
%==
%==

The second condition is based on the smoothness of $\gL$. 
\begin{cond}[{\bf Smoothness}]
The function $\gL$ is $\beta$-smooth, i.e., for $\vtheta',\vtheta \in \cB(\vtheta_0)$
%B^{\mathrm{Euc}}_{\rho}(\vtheta_0)$ 
and some $\beta>0$, 
%\begin{align}
$\gL(\vtheta') \leq \gL(\vtheta) + \langle \vtheta'-\vtheta, \nabla_{\vtheta} \gL(\vtheta) \rangle + \frac{\beta}{2} \| \vtheta'-\vtheta \|^2_2$.
%\end{align}
\label{cond:smooth}
\end{cond}
As long as Conditions~\ref{cond:rsc} and~\ref{cond:smooth} are satisfied at step $t$ of the GD update in \eqref{eq:gd_at_t}, the loss is guaranteed to decrease with a suitable step-size choice. 
%
%
\begin{restatable}[{\bf Global loss reduction}]{theo}{GlobalLossSmooth}
Consider Assumption~\ref{asmp:iter-0} and Conditions~\ref{cond:rsc} and \ref{cond:smooth} with $\alpha_t \leq \beta$ at step $t$ of the GD update~\eqref{eq:gd_at_t} with step-size $\eta_t=\frac{\omega_t}{\beta}$ for some $\omega_t \in(0,2)$. 
\pcedit{If $\gL(\vtheta_t)\neq \underset{\vtheta \in \cB(\vtheta_0)}{\inf} \gL(\vtheta)$, then}
%Then,
%
we have $0 \leq \gamma_t := \frac{\underset{\vtheta \in \mathcal{N}_t}{\inf} \gL(\vtheta) - \underset{\vtheta \in \cB(\vtheta_0)}{\inf} \gL(\vtheta)}{\gL(\vtheta_t) - \underset{\vtheta \in \cB(\vtheta_0)}{\inf} \gL(\vtheta)} < 1$ 
%
and 
\begin{equation}
\begin{aligned}
    &\gL(\vtheta_{t+1}) - \underset{\vtheta \in \cB(\vtheta_0)}{\inf} \gL(\vtheta)\leq \left(1-\frac{\alpha_t \omega_t (1-\gamma_t)}{\beta}(2-\omega_t) \right) (\gL(\vtheta_t) - \underset{\vtheta \in \cB(\vtheta_0)}{\inf} \gL(\vtheta)).
\end{aligned}
\label{eq:conv-0}
\end{equation}
%
%==================================
%%%
%===============================
\label{theo:global_main}
\end{restatable}
Theorem~\ref{theo:global_main}'s proof is found in Appendix~\ref{app:rscopt}.
\pcedit{We note that if the infimum loss inside $\cB(\vtheta_0)$ is attained at time $t$, i.e., $\cL(\vtheta_t)=\underset{\vtheta \in \cB(\vtheta_0)}{\inf} \gL(\vtheta)$,
%\min_{\vtheta\in\cB(\vtheta_0)}\cL(\vtheta_0)$, 
then there is nothing to prove---hence the conditional in the second sentence of Theorem~\ref{theo:global_main}.}

\pcedit{
\begin{remark}[The RSC to smoothness ratio] 
\label{rem:ratio}
Theorem~\ref{theo:global_main} requires $\alpha_t/\beta\leq 1$, which needs to be proved for the particular function $\gL$ being considered. 
%
If \eqref{cond:rsc} were to hold for any $\vtheta,\vtheta'\in\cB(\vtheta_0)$, then $\gL$ would be a \emph{locally strongly convex} function in the set $\cB(\vtheta_0)$~\citep{Boyd_Vandenberghe_2004}. This is a stronger condition on $\gL$ which makes $\alpha$ in \eqref{cond:rsc} \emph{independent} from the choice of $\vtheta$ (in the context of Theorem~\ref{theo:global_main}, $\alpha_t$ would be independent from $t$), which immediately implies $\alpha/\beta<1$.
\qed
\end{remark}}

Our analysis is inspired by the recent works \citep{banerjee2022restricted} and~\citep{cisnerosvelarde2024optgenWeightNorm}, where optimization guarantees were done for feedforward networks and normalization. We abstract out from those special cases, and demonstrate that our analysis works for any losses satisfying Conditions~\ref{cond:rsc} and \ref{cond:smooth}---indeed, \citep{cisnerosvelarde2024optgenWeightNorm} particularly satisfies Condition~\ref{cond:rsc}(b.1) and   \citep{banerjee2022restricted} satisfies Condition~\ref{cond:rsc}(b.2). Thus, in the context of our paper, \textbf{the largest effort in %heavy lifting for 
establishing optimization guarantees for DONs and FNOs is to show these two models satisfy Conditions~\ref{cond:rsc} and \ref{cond:smooth} \pcedit{with $\alpha_t/\beta\leq 1$}}.