Replicating the success of deep learning in scientific computing such as developing neural PDE solvers, constructing surrogate models, and developing hybrid numerical solvers, has recently captured the interest of the broader scientific community~\citep{kutz2024promisdirectpde,kovachi2023neuraloperator}. In relevant applications to scientific computing, we often need to learn mappings between input and output function spaces. Neural operators have emerged as the prominent class of deep learning models used to learn such mappings~\citep{lu20201DeepONet}.
\pcedit{They have become a natural choice for learning solution operators of parametric PDEs and of inverse problems where multiple evaluations are needed under different parameters of the problem.}
%
%While there exist 
%Among the multiple models of neural operators, 
Two of the arguably most widely adopted neural operators are Deep Operator Networks (DONs)~\citep{lu20201DeepONet,wang_learning_2021} and {Fourier} Neural Operators (FNOs)~\citep{li_fourier_2021,li_markov_2021}. 

The fundamental idea of a neural operator is to parameterize mappings between function spaces with deep neural networks and proceed with its \emph{learning}, i.e., \emph{optimization}, as in a standard supervised learning setup. However, contrary to a classical supervised learning setting where we learn mappings between two finite-dimensional vector spaces, here we learn mappings between
\emph{infinite-dimensional} function spaces.
%
%the differences in the underlying learning problem and model architectures.
%
While there exist results on the universal approximation properties of DONs and FNOs \citep{deng2021convergence,kovachki2021universal}, to the best of our knowledge, 
there are \emph{no formal optimization convergence results} for the training of these two popular neural operator models. 
% 

To address this open problem, in this paper, \textbf{we establish \pcedit{such} optimization convergence guarantees for learning DONs and FNOs with gradient descent (GD)}.
%for learning DONs and FNOs. 
To achieve this, we first \textbf{propose a general framework that ensures the optimization of any loss using GD as long as two conditions are satisfied across iterations}. The conditions do not include convexity of the loss as that is not satisfied by models based on neural networks, including neural operators.
%
The first condition is based on restricted strong convexity (RSC), a recently introduced alternative~\citep{banerjee2022restricted} to the widely used neural tangent kernel (NTK) analysis~\citep{liu_linearity_2021,liu2022loss,allen-zhu_convergence_2019}. 
The second condition is based on a smoothness property of the loss function. For feedforward neural networks, the RSC condition relies on the second-order Taylor expansion of the loss~\citep{banerjee2022restricted,cisnerosvelarde2024optgenWeightNorm}, \pcedit{using 
%second-order information (Hessian) 
the \emph{Hessian} 
of the neural network} (i.e., second order structure), whereas the NTK approach relies on a kernel approximation of the training dynamics~\citep{jacot2018neural}, \pcedit{using 
%first-order information (gradient)
the \emph{gradient} of the network} (i.e., first order structure).
% of the neural network.}
%
For a specific model such as neural operators, \textbf{the \emph{technical challenge} in using our optimization framework} is to establish suitable properties of the loss, its gradient, and its Hessian in order to show that the RSC and smoothness conditions are indeed satisfied. Convexity is not one of the required conditions, so we are not attempting to show that the Hessian is positive semi-definite, as that will not be true for most neural models, including neural operators. 
  
%

Having defined a general optimization framework based on RSC and smoothness conditions, 
\textbf{the key novelty 
%and \pcedit{great effort} 
%associated heavy lifting 
of our current work is showing that the losses for DONs and FNOs 
%in fact 
provably satisfy these two conditions when the neural operators are wide, despite the substantial differences in their architectures and mathematical analyses}. 
%
For both DONs and FNOs, we need to bound the Hessian of their respective empirical losses
%, since the Hessian is critical 
and of the neural operator models themselves in order to 
determine whether the RSC and smoothness properties are satisfied. 

\textbf{The \emph{challenge} in the analysis of DONs} stems from the fact that the output of this neural operator is the inner product of two neural networks. This greatly complicates the Hessian structure of the loss compared to standard neural networks. Indeed, the Hessian now contains cross-interaction terms between two neural networks which have to be carefully analyzed and which require a more complex definition of the restricted set over which the RSC property is defined compared to standard neural networks.  

\textbf{The \emph{challenge} in the analysis of FNOs} stems from the fact that it contains, inside their neural network structure, learnable weights that define transformations in the Fourier domain---something absent in standard neural networks. 
%Such transformations are learnable weights by GD. 
This complicates the Hessian structure of the FNO since it contains parameters both in the data domain and transformed Fourier domain leading to cross-derivatives between parameters in data and Fourier domains. 
Thus, a more involved analysis than of standard neural networks is required. 
%
%  

Remarkably, we find that \textbf{the \emph{widths} of both neural operator models benefit our optimization guarantees in similar ways}. First, the widths appear in the RSC condition such that larger widths make this condition less restrictive. Second, larger widths enlarge the neighborhood around the initialization point where our optimization guarantees hold. Similar benefits from larger widths were found for standard neural networks by~\citet{banerjee2022restricted}, despite the substantial differences between our analyses and theirs (as mentioned in the \emph{challenges} above). 
%
%

%

Finally, to complement our theoretical results, we present empirical evaluations of DONs and FNOs and show the benefits of width on learning three popular operators in the literature~\citep{li_fourier_2021,lu20201DeepONet}: antiderivative, diffusion-reaction, and Burger's equation. \pcedit{Our experiments show that increasing the width leads to lower training losses 
%for all three problems 
and generally leads to faster convergence.}

%
%\pcdelete{
\textbf{Paper Organization}. 
\Secref{sec:related} presents related literature. 
\Secref{sec:modelSetup} outlines the architectures and learning problems for DONs and FNOs. 
\Secref{sec:optmain} establishes our general optimization framework, and \Secref{sec:optDON} and \Secref{sec:optFNO} establish convergence guarantees using this framework for DONs and FNOs respectively, highlighting the benefits of width.
\Secref{sec:Comparison} compares our results and known ones for standard neural networks. 
\Secref{sec:Experiments} presents empirical evaluations on the benefits of width. 
%We finally conclude by summarizing the main contributions in \Secref{sec:Discussion}.
\Secref{sec:Discussion} is the conclusion.

%}

\textbf{Notation}. $\norm{\,\cdot\,}_2$ denotes the $L_2$-norm or the induced matrix $L_2$-norm when the argument is a vector or a matrix, respectively. Given an operator/function $f$, $\operatorname{ran}(f)$ and $\operatorname{dom}(f)$ denote the range and domain of $f$, respectively.
%
