
We only provide a brief overview of the literature related to our work and provide a more extensive treatment in Appendix~\ref{app:related}. In the case of DONs, approximation~\citep{lu20201DeepONet} and generalization~\citep{kontolati2022_Over_parameterization} properties have been formally studied, as well as several applications of DONs~\citep{goswami_physics-informed_2022,wang_long-time_2021,diab2024u,centofanti2024learning,sun2023deepgraphonet}. Nevertheless, optimization guarantees for DONs is an open problem.
Approximation properties for FNOs have been formally studied~\citep{kovachki2021universal}, and diverse applications of FNOs and various Fourier-based operators have been formulated~\citep{li_multipole_2020,liu_learning-based_2022,wen_u-fnoenhanced_2022,pathak_fourcastnet_2022,centofanti2024learning,li2023fourier,yang2023fourier,harder2023hard}. Nevertheless, optimization guarantees for DONs is also an open problem. 
%
Though formal optimization guarantees for neural operators are largely absent, there is a more established literature on such guarantees for neural networks. We highlight two particular approaches for optimization analysis: based on the NTK approach~\citep{jacot2018neural,liu_linearity_2021,banerjee23a,du2019gradient,allen-zhu_convergence_2019} and on the RSC approach~\citep{banerjee2022restricted,cisnerosvelarde2024optgenWeightNorm}---our work is related to the latter.