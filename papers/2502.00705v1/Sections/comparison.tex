%
Our presented analysis provides sufficient conditions that guarantee the optimization of DONs and FNOs under gradient descent (GD). It is non-trivial that GD should converge for neural operators in a similar way to how it converges for feedforward neural networks (FFNs), i.e., by being particular instances of the general optimization framework from Section~\ref{sec:optmain}. Indeed, as indicated in Table~\ref{tab:comp}, there exist similarities and differences between our derivations for neural operators and the ones for FFNs. 
%
\begin{table*}[t!]
    \centering
    % \small
    \scriptsize % 0.645
    % \footnotesize
\begin{tabular}{@{}lcc@{}}%\begin{tabular}{@{}l*{5}{c}@{}} % USED TO BE 1.27
\toprule
    %\cline{1-3}
    %\cline{2-8} 
    & \textbf{Deep Operator Network} & \textbf{Fourier Neural Operator} \\ 
%   
\midrule
%\setlength\extrarowheight{3pt}
\textbf{\centering $Q^t_{\kappa}$ set} & More Complex & Similar \\%[0.25cm]
%\hline
%
\textbf{Hessian and gradient bounds of the neural operator model} & 
Similar* & More Complex\\%[0.25cm]
%\hline
%
\textbf{RSC and Smoothness characterization; computing the Hessian of $\gL$}
  & More Complex & Similar \\%[0.25cm]  
    \bottomrule
    \end{tabular}
%     
    \caption{We indicate whether a specific neural operator (DON or FNO) has a \emph{similar} or a
    \emph{more complex}
    %\emph{different} 
    derivation of specific mathematical objects or properties compared to a feedforward neural network (as in~\citep{banerjee2022restricted}). *The similarity is with respect to each
    \pcedit{individual network} of the DON.}
    %branch of the DON.}
    \label{tab:comp}
\end{table*}
%

\paragraph{\textbf{The challenge in the analysis of DONs.}} 
The fact that the output of a DON is an inner product of two FFNs~\eqref{eq:DONoutput}---the branch and trunk networks---makes the mathematical analysis of the RSC and smoothness properties more involved than the analysis associated to a single FFN. Indeed, the appearance of cross-interaction terms between the two FFNs complicates the Hessian structure of the empirical loss $\gL$ and 
%has to be carefully analyzed and is reflected in the 
requires a 
more complex definition of the RSC $Q^t_{\kappa}$ set compared to the one used for FFNs or FNOs. On the other hand, since the branch and trunk networks are \emph{individually} FFNs, their individual Hessian and gradient bounds are known. 

\paragraph{\textbf{The challenge in the analysis of FNOs.}} 
The fact that FNOs---unlike FFNs---include a series of learnable transformations in the Fourier domain makes the mathematical analyses of their Hessian and gradient bounds more involved than the ones for FFNs. Indeed, these Fourier transformations 
%in the neural network structure of FNOs 
introduce cross-derivatives between weights in data and Fourier domains in 
the Hessian that need to be carefully taken into account. On the other hand, since FNOs are composed of a single network, their $Q^t_{\kappa}$ set is similar to FFNs, as well as their RSC and smoothness analyses.
%
%


