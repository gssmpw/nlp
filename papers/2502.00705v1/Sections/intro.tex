Replicating the success of Deep Learning in scientific computing such as developing Neural PDE solvers, constructing surrogate models and developing hybrid numerical solvers has recently captured interest of the broader scientific community. Neural Operators \citet{li_fourier_2021,li_markov_2021} and Deep Operator Networks (DeepONets) \citet{lu20201DeepONet,wang_learning_2021} encompass two recent approaches aimed at learning mappings between function spaces. Contrary to a classical supervised learning setup which aims at learning mappings between two finite-dimensional vector spaces, these neural operators/operator networks aim to learn mappings between \emph{infinite-dimensional} function spaces. 
% The first approach directly parameterizes the unknown solution function as a deep neural network (DNN) which is then fed to a suitable loss function representative of the ``physics'' of the system, e.g. the initial/boundary conditions, underlying system parameters etc. The expressivity of these solutions is endowed by the Universal Approximation Theorem \citet{cybenkot_Univ_Approximation_1989,hornik1991approximation} whereas the physical constraints are encoded via the loss function. These approaches constitute what is popularly referred to as the Physics-Informed Neural Network (PINN) idea (see e.g. \citet{lagaris_artificial_1998,sirignano_dgm_2018,raissi_inferring_2017,raissi_physics-informed_2019}). A downside of this approach is in inverse problems when the solution needs to be evaluated for multiple initial/boundary conditions. 
The key underlying idea in both the approaches is to parameterize the solution operator as a deep neural network and proceed with learning as in a standard supervised learning setup. 
% These operators/operator networks can be implicit, e.g. defined by the solution of a partial differential equation (PDE) or explicit defined by the solution of a linear ordinary differential equation (ODE). 
Since a neural operator directly learns the mapping between the input and output function spaces, it is a natural choice for learning solution operators of parametric PDE's where the PDE solution needs to be inferred for multiple instances of these ``input parameters'' or in the case of inverse problems when the forward problem needs to be solved multiple times to optimize a given functional. \bsdelete{While constructing such neural operators constitutes a large body of work in itself, to the best of our knowledge, none of the preceding works have provided convergence guarantees on when and why these neural operators/operator networks and their underlying optimization algorithms converge to the global optimum}. \bscomment{While there exist results on the approximation properties and convergence of DeepONets; see, e.g., \citet{deng2021convergence} for a convergence analysis of DeepONets -- vis-a-vis their approximation guarantees-- for the advection-diffusion equation, there do not exist any optimization results on when and why GD converges during the optimization of the DeepONet loss.}

In this work we put forth theoretical convergence guarantees for DeepONets centered around over-parameterization and show that over-parameterization based on wider layers (for both branch and trunk net) provably helps in DeepONet convergence. This is reflected in Figure~\ref{fig:Fig1} which summarizes an empirical evaluation of over-parameterized DeepONets with ReLU and smooth activations on a prototypical operator learning problem. In order to complement our theoretical results, we present empirical evaluation of our guarantees on three template operator learning problems: {(i)} Antiderivative operator, {(ii)} Diffusion-Reaction PDE, and {(iii)} Burger's equation and demonstrate that wider DeepONets lead to overall lower training loss at the end of the training process. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[H]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/relu_Antiderivative.eps}
        \caption{ReLU Activation}
        \label{fig:LearningReLU}
    \end{subfigure}\qquad\begin{subfigure}[H]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/selu_Antiderivative.eps}
        \caption{Smooth Activation}
        \label{fig:LearningSeLU}
    \end{subfigure}
    \caption{Benefits of over-parameterization on learning of DeepONets for the Antiderivative Operator: $G_{\vtheta}(\vu)(x) = \int_{0}^x\vu(\xi)\, \mathrm{d}\xi$. In both cases $m$ denotes the width of the branch net and the trunk net. For both ReLU and smooth activations, increasing the width $m$ leads to much lower losses. Note that the y-axis is in log-scale.}
    \label{fig:Fig1}
\end{figure}

The rest of the paper is organized as follows. In \Secref{sec:related} we review the existing literature on neural operators, operator networks and over-parameterization based approaches for establishing convergence guarantees for deep networks. Next, we devote \Secref{sec:modelSetup} to briefly outline the the DeepONet model, the learning problem and the corresponding architecture. \Secref{sec:optimization} contains the first technical result of the paper. In \Secref{sec:optimization} we establish convergence guarantees for DeepONets with smooth activations (for both branch and trunk net) based on the Restricted Strong Convexity (RSC) of the loss. Next, in \Secref{sec:ntk}, we present the second technical result of the paper where we establish optimization guarantees for DeepONets with ReLU activations by showing that the Neural Tangent Kernel (NTK) of the DeepONet is positive definite at initialization. In \Secref{sec:Experiments} we present simple empirical evaluations of the main results by carrying out a parametric study based on increasing the DeepONet width and noting its effect on the total loss during training. We finally conclude by summarizing the main contributions in \Secref{sec:Discussion}.
% and note possible extensions of this work to other operator learning problems.