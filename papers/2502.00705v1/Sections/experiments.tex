We present experiments on
%empirical evaluations of 
the effect of over-parameterization on the training performance of DONs and FNOs, as measured by the empirical risk over a mini-batch of the training dataset using the Adam optimizer. We consider three prototypical operator learning problems in the literature~\citep{li_fourier_2021,lu20201DeepONet}: % for DONs: 
(a) the antiderivative (or integral) operator, (b) the diffusion-reaction operator, and (c) Burgers' equation. We do not consider vector-valued problems (e.g., Navier-Stokes) because they are not 
%(yet) 
covered by our theoretical framework. 
For definiteness, we consider the branch and trunk nets to have the same width $m$ (i.e., $m_f=m_g={m}$) for the DON, and the same width $m$ for the FNO. 
In all experiments, we increase the width from $m=10$ to $m=500$. 
For all networks, we use the Scaled Exponential Linear Unit (SELU)~\citep{selu-paper} as their smooth activation function. 
We monitor the training process over $80,000$ training epochs and report the resulting average loss. Note that the objective of this section is to show the effect of over-parameterization on the neural operator training and not to present any kind of comparison between the two neural operators. 
%

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[H]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{NewFigures/losses_AD_DON.pdf}
        \caption{Antiderivative}
    \end{subfigure}\begin{subfigure}[H]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{NewFigures/losses_DR_DON.pdf}
        \caption{Diffusion-Reaction}
    \end{subfigure}\vspace{1ex}
    \begin{subfigure}[H]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{NewFigures/losses_Burgers_DON.pdf}
        \caption{Burger's Equation}
    \end{subfigure}
    \caption{Training progress of DONs 
    %with smooth activations 
    as measured by the \pcedit{empirical} loss~\eqref{eq:empirical_risk} over 80,000 epochs. The $y$-axis is plotted on a \emph{log-scale} and the $x$-axis denotes the training epochs \% 100 (i.e., the loss is stored at every 100\textsuperscript{th} epoch). Wider networks typically lead to lower loss for all three problems.}
\label{fig:seLU_Loss_DON}
\end{figure*}
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[H]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{NewFigures/losses_AD_FNO.pdf}
        \caption{Antiderivative}
    \end{subfigure}\begin{subfigure}[H]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{NewFigures/losses_DR_FNO.pdf}
        \caption{Diffusion-Reaction}
    \end{subfigure}\vspace{1ex}
    \begin{subfigure}[H]{0.29\textwidth}
        \centering
        \includegraphics[width=\linewidth]{NewFigures/losses_Burgers_FNO.pdf}
        \caption{Burger's Equation}
    \end{subfigure}
    \caption{Training progress of FNOs 
    %with smooth activations 
    as measured by the \pcedit{empirical} loss~\eqref{eq:fno_loss} over 80,000 epochs. The setting of the plots is similar to Figure~\ref{fig:seLU_Loss_DON}. Wider networks typically lead to lower loss for all three problems.
    }\label{fig:seLU_Loss}
\end{figure*}

The results for DONs (Figure~\ref{fig:seLU_Loss_DON}) and FNOs (Figure~\ref{fig:seLU_Loss})
clearly show that \pcedit{both neural operators benefit from an increasing}
%We find that increasing the 
width $m$ \pcedit{since it} leads to overall lower training losses for all three learning problems and it generally leads to faster optimization convergence.
%
%
The Antiderivative operator 
is a linear operator and therefore is learned very accurately, especially for wider DONs and FNOs where the loss is around $10^{-12}$ and $10^{-5}$ respectively. 
%\end{remark}
%
The diffusion-reaction equation %also 
demonstrates lower loss with increasing width %, albeit 
less markedly than the antiderivative operator for DONs and more markedly for FNOs. This can be attributed in part to the fact that the operator is inherently nonlinear. 
Finally, regarding Burger's equation,
lower training losses and faster convergence is more markedly for FNOs than for DONs as the width increases. 

Additional information on the experimental settings and additional experiments are found in Appendix~\ref{app:exp_don_fno}.
