As in the case of DONs, we also focus on scalar input functions $u$. 
%, i.e., $\operatorname{ran}(u)\subseteq\R$. 
To pass the 
input function $u$,
we discretize it by sampling it 
on $\bar{R}$ locations, 
forming a vector of dimension $\bar{R}$.
Thus,
the encoder $P(u;\vtheta_p)(\bfx)$ in equation~\eqref{eq:continuous_fno} takes a vector of dimension $\bar{R}+d_x$ ($\bar{R}$ from the sampled $u$ and $d_x$ from the output location where we evaluate the operator on). For our purposes, we consider a fixed (not trainable) encoder with output dimension $d$: $P(u;\vtheta_p)(\bfx)\equiv P(u)(\bfx)\in\R^d$; and a linear decoder $Q(\aalpha^{({L+1})};\vtheta_q)(\vx)=\frac{1}{\sqrt{m}}\v^\top \aalpha^{({L+1})}(\vx)\in\R$ with $\vtheta_q\equiv \v\in\R^m$ assuming $\aalpha^{(L+1)}(\bfx)\in\R^m$. 
Thus, following~\citep{li_fourier_2021}, the FNO model is:
\begin{align*}
\aalpha^{(0)} &= P(u)(\bfx)\\
\aalpha^{(1)} &= \phi\left(
        \frac{1}{\sqrt{m}} W^{(1)} \aalpha^{(0)}
    \right)\\
\aalpha^{(l)} & = \phi\left(
        \frac{1}{\sqrt{m}} W^{(l)} \aalpha^{(l-1)} +
        \frac{1}{\sqrt{m}} F^{*} R^{(l)} F \aalpha^{(l-1)}
    \right),\; l\in \{2,\dots,L+1\}\\
    f(\vtheta;\vx)  &= \frac{1}{\sqrt{m}} \v^\top \aalpha^{(L+1)}~,
\end{align*}
where $\phi$ is a pointwise smooth activation function, $F$ is the discrete Fourier transform kernel (as a matrix) with $F^*$ being its conjugate transpose, 
%where 
the weight matrices are $W^{(1)}\in\R^{m\times d}$, $W^{(l)}\in\R^{m\times m}$ and $R^{(l)}\in\R^{m\times m}$ for layer $l\in\{2,\dots, L+1\}$ (all hidden layers have the same width $m$). The ij-entries of $W^{(l)}$ and $R^{(l)}$ are $w^{(l)}_{ij}$ and $r^{(l)}_{ij}$, respectively, for an appropriate $l$. With some abuse of notation, we denote the entire set of trainable parameters by $\vtheta = [\vtheta_w^{\top}\ \vtheta_r^{\top}]^{\top}$, with $\vtheta_w = [\text{vec}(W^{(1)})^{\top},\dots,\text{vec}(W^{(L+1)})^{\top}\ \mathbf{v}^{\top}]^{\top}$ and $\vtheta_r = [\text{vec}(R^{(2)})^\top,\dots,\text{vec}(R^{(L+1)})^{\top}]^{\top}$. We denote the number of parameters by $p_w + p_r$, where $\vtheta_w\in\R^{p_w}$ and $\vtheta_r\in\R^{p_r}$.  
%
%
Let $\vtheta_0$ be the parameter vector at initialization and $\vtheta_t$ be it at time step $t$.

We remark that our model uses an $m\times m$ Discrete Fourier Transform %(DFT) 
kernel $F$, whose $kj$-entry is $F_{kj}=e^{-\frac{2\pi \iota}{m}(k-1)(j-1)}$, with $\iota$ representing the imaginary unit.

\begin{asmp}[{\bf Activation functions}]
\label{asmp:Activation_Function_FNO}
The activation function $\phi$ is $1$-Lipschitz and $\beta_{\phi}$-smooth (i.e. $\phi_l^{\prime\prime}\leq \beta_{\phi}$) for some $\beta_{\phi} > 0$.
\end{asmp}
\begin{asmp}[{\bf Initialization of weights}]
\label{asmp:smoothinit_FNO}
All weights of the FNO are initialized independently as follows: (i) $w^{(l)}_{{0,\,ij}}\sim \gN (0, \sigma^2_{0_w})$ and $r^{(l)}_{{0,\,ij}}\sim \gN (0, \sigma^2_{0_r})$ for $l\in [L+1]$ where $\sigma_{0,w} = \frac{\sigma_{1,w}}{2(1+\frac{\sqrt{\log m}}{\sqrt{2m}})}$ and $\sigma_{0,r} = \frac{\sigma_{1,r}}{2(1+\frac{\sqrt{\log m}}{\sqrt{2m}})}$, where $\sigma_{1,w},\ \sigma_{1,r} > 0$; (ii) the decoder parameter $\v$ is a random vector with unit norm $\norm{\v}_2=1$. Further, we assume the encoder output satisfies $\norm{\aalpha^{(0)}}_2=\sqrt{d}$.
\end{asmp}

For a given parameter vector $\bar{\vtheta}\in\R^{p_w+p_r}$, we introduce the neighborhood set  
$B^{\mathrm{Euc}}_{\rho_w,\rho_r,\rho_1}(\bar{\vtheta})=\{\vtheta\in\mathbb{R}^{p_w+p_r}\,:\,\norm{W^{(l)}-\bar{W}^{(l)}}_2\leq \rho_w,\,l\in[L+1],\,\norm{R^{(l)}-\bar{R}^{(l)}}_2\leq \rho_r,\,l\in\{2,\dots,L+1\},\,\norm{\v-\bar{\v}}_2\leq \rho_1\}$ for $\rho_w,\rho_r,\rho_1>0$. 
%
We say that an element of $B^{\mathrm{Euc}}_{\rho_w,\rho_r,\rho_1}(\bar{\vtheta})$ is \emph{strictly inside} $B^{\mathrm{Euc}}_{\rho_w,\rho_r,\rho_1}(\bar{\vtheta})$ when it satisfies every inequality in the set's definition without equality.

The following assumption is analogous to Assumption~\ref{asmp:iter-0}.

\begin{asmp}[{\bf Iterates inside $B^{\mathrm{Euc}}_{\rho_w,\rho_r,\rho_1}(\vtheta_0)$}]
\label{asmp:iter-2}
All iterates $\{\vtheta_t\}_{t\geq 1}$ follow GD as in~\eqref{eq:gd_at_t} and are strictly inside the set $B^{\mathrm{Euc}}_{\rho_w,\rho_r,\rho_1}(\vtheta_0)$ for fixed $\rho_w,\rho_r,\rho_1>0$.
\end{asmp}

We also introduce the following auxiliary set. %
\begin{restatable}[{\bf $Q^{t}_{\kappa}$ sets for FNOs}]{defn}{qset_FNO} 
For an iterate $\vtheta_t$, let $\nabla_{\vtheta}\bar{G}_t =  \frac{1}{n} \sum_{i=1}^n \frac{1}{R}  \sum_{j=1}^{R} \nabla_{\vtheta}G_{\vtheta_t}(u^{(i)})(x_j)$. For $\kappa \in (0,1)$, define $Q^t_{\kappa} := \{ \vtheta \in \R^{p_w + p_r} \mid |\cos(\vtheta-\vtheta_t, \nabla_{\vtheta}\bar{G}_t)| \geq \kappa \}$.
\label{defn:qset_FNO}
\end{restatable}
Note that unlike DONs, the $Q^t_{\kappa}$ sets for FNOs are relatively simpler due to a single network architecture. 

Next, we prove the RSC and smoothness conditions (corresponding to Conditions~\ref{cond:rsc} and~\ref{cond:smooth}, respectively). 
%
Using the nomenclature of Section~\ref{sec:optmain}, the set $B^{\mathrm{Euc}}_{\rho_w,\rho_r,\rho_1}(\vtheta_0)$ corresponds to $\mathcal{B}(\vtheta_0)$, and 
$B^t_{\kappa} := Q_{\kappa}^t \cap B^{\mathrm{Euc}}_{\rho_w,\rho_r\rho_1}(\vtheta_0) \cap B^{\mathrm{Euc}}_{\rho_2}(\vtheta_t)$ corresponds to $\mathcal{N}_t$.

\begin{restatable}[{\bf RSC for FNOs}]{theo}{RSCLossFNO}
Consider Assumptions~\ref{asmp:Activation_Function_FNO}, \ref{asmp:smoothinit_FNO}, and~\ref{asmp:iter-2}, and $Q^t_{\kappa}$ as in Definition~\ref{defn:qset_FNO}. Then, the set $B^t_{\kappa} := Q_{\kappa}^t \cap B^{\mathrm{Euc}}_{\rho_w,\rho_r\rho_1}(\vtheta_0) \cap B^{\mathrm{Euc}}_{\rho_2}(\vtheta_t)$ is a non-empty set that satisfies Condition~\ref{cond:rsc}(a) and (b) for suitable $\rho_2$. 
Moreover, 
with probability at least $1-\frac{2(L + 2)}{m}$, at step $t$ of GD,  
%$\forall \vtheta^{\prime} \in B^t_{\kappa}$ 
the FNO loss $\gL$~\eqref{eq:loss-fno} satisfies 
equation~\eqref{eq:RSC-prim} with
\begin{equation}
\alpha_t = 2\kappa^2 \| \nabla_{\vtheta} \bar{G}_t \|_2^2 - \frac{c_1}{\sqrt{m}}~,
        \label{eq:RSCLoss_FNO}
\end{equation}
where $\nabla_{\vtheta}\bar{G}_t = \frac{1}{n} \sum_{i=1}^n \frac{1}{R}  \sum_{j=1}^{R} \nabla_{\vtheta}G_{\vtheta_t}(u^{(i)})(x_j)$, and for some constant $c_1 >0$ 
which depends polynomially on the depth $L$, and the radii $\rho_w$, $\rho_r$, $\rho_1$, and $\rho_2$ whenever $\sigma_{1,w}+\sigma_{1,r}\leq 1-\frac{\rho_w+\rho_r}{\sqrt{m}}$.
%====
%%
%====
%
Thus, the loss $\gL(\vtheta)$ satisfies RSC w.r.t $(B_{\kappa}^t, \vtheta_t)$, i.e., Condition~\ref{cond:rsc}(c), whenever $\| \nabla_{\vtheta} \bar{G}_t \|_2^2 = \Omega(\frac{1}{\sqrt{m}})$.
\label{theo:rsc_main_fno}
\end{restatable}
%

\begin{restatable}[{\bf Smoothness for FNOs}]{theo}{RSSFNO}
Under Assumptions~\ref{asmp:Activation_Function_FNO} and \ref{asmp:smoothinit_FNO}, with probability at least $ 1 - \frac{2(L+2)}{m}$, the FNO loss $\cL$~\eqref{eq:loss-fno} is $\beta$-smooth in $B^{\mathrm{Euc}}_{\rho_w,\rho_r\rho_1}(\vtheta_0)$ with $\beta$ being a positive constant which 
%
depends polynomially on the depth $L$, and the radii $\rho_w$, $\rho_r$, and $\rho_1$ whenever $\sigma_{1,w}+\sigma_{1,r}\leq 1-\frac{\rho_w+\rho_r}{\sqrt{m}}$.
%===
%%
%===
%
\label{theo:smooth_main_fno}
\end{restatable}

\begin{remark}[Ensuring that $\alpha_t/\beta<1$] 
Similar to our discussion in Remark~\ref{rem:abeta-DON}, we prove that 
%the needed ratio between the RSC and smoothness parameters is less than one, i.e., 
$\alpha_t/\beta<1$ in Proposition~\ref{prop:RSC-smooth-FNO} from Appendix~\ref{app:fnoopt}, satisfying the condition required in the statement of Theorem~\ref{theo:global_main}.
%
%In the appendix we provide a proof for $\alpha_t/\beta<1$.
%
\qed
\end{remark}

\paragraph{\textbf{Optimization Under Gradient Descent for FNOs.}} 
We have that 
Theorem~\ref{theo:rsc_main_fno} satisfies Condition~\ref{cond:rsc} and Theorem~\ref{theo:smooth_main_fno} satisfies Condition~\ref{cond:smooth}. We also proved that $\alpha_t/\beta<1$. Thus, when $\| \nabla_{\vtheta} \bar{G}_t \|_2^2 = \Omega(\frac{1}{\sqrt{m}})$, i.e., $\alpha_t>0$, a decrease on the loss function by GD is ensured with probability at least $1- \frac{2(L+2)}{m}$ towards its minimum value taken within the set $B^{\mathrm{Euc}}_{\rho_w,\rho_r\rho_1}(\vtheta_0)$ due to Theorem~\ref{theo:global_main}.
%
%

\begin{remark}[The effects of over-parameterization for FNOs]
Similar observations to Remarks~\ref{rem:RSC-m} and~\ref{rem:largerNeighb-m} hold for FNOs, i.e., that over-parameterization ensures (i) a better condition for ensuring the RSC property, and (ii) a larger neighborhood %\pcedit{(in terms of larger $\rho_w$ and $\rho_r$)} 
around the initialization point over which our guarantees hold. Item (ii) follows from the relationship $\rho_w+\rho_r\leq \sqrt{m}$ obtained when choosing $\sigma_{1,w}$ and $\sigma_{1,r}$ to ensure a polynomial dependence as in Theorems~\ref{theo:rsc_main_fno} and~\ref{theo:smooth_main_fno}.
%
\end{remark}