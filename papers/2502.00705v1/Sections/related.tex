
{\bf Learning Operators.}
Constructing operator networks for ordinary differential equations using learning-based approaches was first studied in~\citep{chen1995universal}, where a neural network with a single hidden layer was shown to approximate a nonlinear continuous functional.
%to arbitrary accuracy. 
This was, in essence, akin to the Universal Approximation Theorem for classical neural networks \citep{cybenkot_Univ_Approximation_1989,hornik_multilayer_1989,hornik1991approximation,lu_expressive_2017}. 
While this theorem only guaranteed the existence of a neural architecture, it was not practically realized until \cite{lu20201DeepONet} provided an extension of the theorem to \pcedit{DONs}. Since then, several works have pursued applications of DONs to different problems, e.g.,~\citep{goswami_physics-informed_2022,wang_long-time_2021,diab2024u,centofanti2024learning,sun2023deepgraphonet}, as well as improved the DON model itself, e.g., \cite{wang_learning_2021,qiu2024derivative}. 
From a standpoint of generalization, \citet{kontolati2022_Over_parameterization} studied the effects of over-parameterization on the generalization properties of DONs in the context of dynamical systems. Nonetheless, an optimization analysis of DONs is an open problem.

The operator learning paradigm has also been explored in parallel by other works seeking to directly parameterize the integral kernel in the Fourier domain using a deep network~\citep{bhattacharya_model_2021-1, bhattacharya_model_2021, li_fourier_2021, li_neural_2020, li_markov_2021}. Several subsequent extensions explored different architectures for Fourier-based operators tailored to specific problems~\citep{li_multipole_2020,liu_learning-based_2022,wen_u-fnoenhanced_2022,pathak_fourcastnet_2022,centofanti2024learning}. Other notable techniques include the use of a factorized spectral representation \cite{tran2021factorized}, using larger Fourier kernels to capture a broader set of frequencies \cite{qin2024toward}, employing FNOs in latent space in an encoder-decoder framework \cite{li2023fourier}. Recently, FNOs were used to accelerate simulations in climate science~\citep{yang2023fourier,harder2023hard}. \pcedit{Nevertheless,} while significant progress has been made for FNOs from an \pcedit{applied} perspective,
their formal optimization analysis 
\pcedit{is an open problem.} % has not been pursued.

{\bf Optimization Analysis of Neural Networks.}
Optimization of over-parameterized deep neural networks has been studied extensively, e.g., \citep{du2019gradient,Arora_Du_Neurips_2019,arora2019fine,allen-zhu_convergence_2019,liu_linearity_2021}. In particular, \citet{jacot2018neural} showed that the %neural tangent kernel (NTK) 
NTK of a deep network converges to an explicit kernel in the limit of infinite network width and stays constant during training. \citet{liu_linearity_2021} showed that this constancy arises due to the scaling properties of the Hessian of the predictor as a function of network width. 
\citet{banerjee23a} showed that a deep network whose width is effectively linear on the sample size can ensure convergence under appropriate initialization.
\citet{du2019gradient} and \citet{allen-zhu_convergence_2019} showed that GD converges to zero training error in polynomial time for deep over-parameterized models \pcedit{such as ResNets and CNNs.}
%
%
\citet{karimi2016linear} showed that the Polyak-Lojasiewicz (PL) condition, a 
%much 
weaker condition than strong convexity, can be used to explain the linear convergence of gradient-based methods. \citet{banerjee2022restricted} showed convergence of GD for feedforward networks using 
%restricted strong convexity (RSC),
RSC, 
which leads to a variant of the PL condition. \citet{cisnerosvelarde2024optgenWeightNorm} used RSC to prove the optimization of networks with weight normalization using GD.