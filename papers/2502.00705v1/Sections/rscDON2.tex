We consider, analogous to \citep{liu_loss_2021}, the branch net as a fully connected feedforward neural network:
\begin{align}
    \begin{aligned}
        \aalpha^{(0)}_{f} &= \bfu(\vx) \\
        \aalpha^{(l)}_{f} &= \phi\left( \frac{1}{\sqrt{m_{\rf}}} W^{(l)}_f\aalpha^{(l-1)}_{f}  \right),\; l \in [L-1]\\ 
        \vf =  \aalpha^{(L)}_{f} &= 
        % \frac{1}{\sqrt{m_{f}}}
        \frac{1}{\sqrt{m_{f}}} W_f^{(L)}\aalpha^{(L-1)}_f
%        f_k &= \aalpha^{(L)}_{f,k},\quad \forall k\in [K]~,
    \end{aligned}
    \label{eq:BranchNetRegularized}
\end{align}
where with some abuse of notation $\bfu(\vx):=[u(\vx_1),\dots,u(\vx_R)]^\top$ is the vector of all scalar evaluations of $u$ at each of the $R$ locations, $\phi$ is a pointwise smooth activation function, $\aalpha^{(l)}_f$ is the output at layer $l\in[L]$, and the weight matrices are $W^{(1)}_f\in\R^{m_f\times R}$ and $W^{(l)}_f \in \R^{m_f\times m_f}$ at layer $l\in\{2,\dots,L-1\}$. The branch net has width $m_f$ (all hidden layers have the same width). % and $L$ respectively. 
%
Similarly, the trunk net is a fully connected feedforward network:
\begin{align}
    \begin{aligned}
        \aalpha^{(0)}_{g} &= \vy\\
        \aalpha^{(l)}_{g} &= \phi\left( \frac{1}{\sqrt{m_{g}}} W^{(l)}_g \aalpha^{(l-1)}_{g}  \right),\; l \in [L-1]\\ 
        \vg = \aalpha^{(L)}_{g} &= 
        % \frac{1}{\sqrt{m_{g}}}
        \frac{1}{\sqrt{m_{g}}} W_g^{(L)}\aalpha^{(L - 1)}_{g}
%        g_k &= \aalpha^{(L)}_{g,k},\quad \forall k\in [K]~,
    \end{aligned}
    \label{eq:TrunkNetRegularized}
\end{align}
where $\vy\in\R^{d_y}$ is the output location, and the weight matrices are $W^{(1)}_g\in\R^{m_g\times d_y}$ and $W^{(l)}_g\in \R^{m_g\times m_g}$ at layer $l\in\{2,\dots,L-1\}$. The trunk net 
has width $m_g$
(all hidden layers have the same width).
%and $L$ respectively. 
Finally, we recall that we have $K$ outputs on each network, i.e., $W^{(L)}_f\in\mathbb{R}^{K\times m_f}$ and $W^{(L)}_g\in\mathbb{R}^{K\times m_g}$.
% 
Given $l\in[L]$, we denote by $(w^{(l)}_{f,{k}})^\top$ and $(w^{(l)}_{g,{k}})^\top$ the $k$-th row of the matrices $W^{(l)}_f$ and $W^{(l)}_g$ respectively, and by $w_{f,{ij}}^{(l)}$ and $w_{g,{ij}}^{(l)}$ their respective $ij$-entry. 
%
%
Using the notation in Section~\ref{subsec:DON_Setup}, the set of trainable parameters is $\vtheta = [\vtheta_f^{\top}\
\vtheta_g^{\top}]^{\top}\in\R^{p_f+p_g}$, with $\vtheta_f = [\text{vec}(W^{(1)}_f)^{\top},\dots,\text{vec}(W^{(L)}_f)^{\top}]^{\top}$ and $\vtheta_g = [\text{vec}(W^{(2)}_g)^\top,\dots,\text{vec}(W^{(L)}_g)^{\top}]^{\top}$. 
Let $\vtheta_0$ be the parameter vector at initialization and $\vtheta_t$ be it at time step $t$. 

We make the following assumptions for our analysis: 
%on the activations, the loss, and the weights:
\begin{asmp}[{\bf Activation functions}]
\label{asmp:Activation_Function}
The activation function $\phi$ of the DON is $1$-Lipschitz and $\beta_{\phi}$-smooth (i.e. $\phi^{\prime\prime}\leq \beta_{\phi}$) for some $\beta_{\phi} > 0$.
\end{asmp}
\begin{asmp}[{\bf Initialization of weights}]
\label{asmp:smoothinit}
All weights of the branch and trunk nets are initialized independently as follows: (i) $w^{(l)}_{f_{0,\,ij}}\sim \gN (0, \sigma^2_{f,0})$ and $w^{(l)}_{g_{0,\,ij}}\sim \gN (0, \sigma^2_{g,0})$ for $l\in [L-1]$ where {$\sigma_{f,0} = \frac{\sigma_0}{2(1+\frac{\sqrt{\log m_f}}{\sqrt{2m_f}})}$ and $\sigma_{g,0} = \frac{\sigma_0}{2(1+\frac{\sqrt{\log m_g}}{\sqrt{2m_g}})}$}, $\sigma_0>0$; (ii) $w^{(L)}_{f_{0},k}$ and $w^{(L)}_{g_{0},k}~$, $k\in[K]$, are random vectors with unit norms, i.e., $\norm{w^{(L)}_{f_{0},k}}_2=1$ and $\norm{w^{(L)}_{g_{0},k}}_2=1$. Further, we assume the input to the branches are normalized as $\norm{\vu(\vx)}_2=\sqrt{R}$ and $\norm{\vy}_2=\sqrt{d_y}$.
\end{asmp}

For a given parameter vector $\bar{\vtheta}=[\bar{\vtheta}_f^\top,\bar{\vtheta}_g^\top]\in\R^{p_f+p_g}$, we introduce the neighborhood set  
$B^{\mathrm{Euc}}_{\rho,\rho_1}(\bar{\vtheta})=\{\vtheta\in\mathbb{R}^{p_f+p_g}\,:\,\norm{W^{(l)}_f-\bar{W}^{(l)}_f}_2\leq \rho,\,\norm{W^{(l)}_g-\bar{W}^{(l)}_g }_2\leq \rho,\,l\in[L-1],\,\norm{w^{(L)}_{f,k}-\bar{w}^{(L)}_{f,k}}_2\leq\rho_1,\,\norm{w^{(L)}_{g,k}-\bar{w}^{(L)}_{g,k}}_2\leq\rho_1,\,k\in[K]\}$ for $\rho,\rho_1>0$. 
%
We say that an element of $B^{\mathrm{Euc}}_{\rho,\rho_1}(\bar{\vtheta})$ is \emph{strictly inside} $B^{\mathrm{Euc}}_{\rho,\rho_1}(\bar{\vtheta})$ when it satisfies every inequality in the set's definition without equality.
%
We also define $B^{\mathrm{Euc}}_{\rho}(\bar{\vtheta})$ as an Euclidean ball around $\bar{\vtheta}$ with radius $\rho>0$.

The following is an assumption analogous to the general Assumption~\ref{asmp:iter-0}.

\begin{asmp}[{\bf Iterates inside $B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0)$}]
\label{asmp:iter-1}
All iterates $\{\vtheta_t\}_{t\geq 1}$ follow GD as in~\eqref{eq:gd_at_t} and are strictly inside the set $B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0)$ for fixed $\rho,\rho_1>0$.
\end{asmp}

We now focus on showing that the two conditions needed for optimization using GD as discussed in Section~\ref{sec:optmain} are indeed satisfied by DONs. We start with the definition of a set $Q^t_{\kappa}$ parameterized by $\kappa \in (0, \frac{1}{2}]$, which will help construct the set $\cN_t$ in Condition~\ref{cond:rsc} for RSC. Due to the interaction of two neural networks (branch and trunk), the definition of $Q^t_{\kappa}$ looks seemingly involved. However, note that $Q^t_{\kappa}$ is only needed for establishing the RSC condition for the analysis and does not change the computation of the optimization algorithm, which is simply GD run over all the branch and trunk network parameters. 
%
%
\begin{restatable}[{\bf $Q^{t}_{\kappa}$ sets for DONs}]{defn}{qset} 
%
For an iterate $\vtheta_t = [\vtheta_{f,t}^{\top}\; \vtheta_{g,t}^{\top}]^{\top}$ and $\kappa \in (0,\frac{1}{\sqrt{2}}]$, we define the set:
{\small 
\begin{equation}
    \begin{aligned} 
Q^t_{\kappa} &:= \bigg\{ \vtheta' = {[{\vtheta'_{f}}^{\top}\; {\vtheta'_{g}}^{\top}]}^{\top}\in \R^{p_f+p_g}:\\
&\;\;|\cos(\vtheta' - \vtheta_t, \nabla_{\vtheta} \bar{G}_{\vtheta_t})| \geq \kappa~, \\
%
&\;\;(\vtheta'_f-\vtheta_{f,t})^\top\left(\frac{1}{n} \sum_{i=1}^n \frac{1}{q_i} \sum_{j=1}^{q_i} \ell'_{i,j} \sum_{k=1}^K \nabla_{\vtheta_{f}} f_k^{(i)} \nabla_{\vtheta_{g}} g_{k,j}^{(i)~\top}\right)(\vtheta'_g-\vtheta_{g,t})  \geq 0~,\\
&\;\;(\vtheta'_f-\vtheta_{f,t})^\top\left( \sum_{k=1}^K \nabla_{\vtheta_{f}} f_k^{(i)} \nabla_{\vtheta_{g}} g_{k,j}^{(i)~\top}\right)(\vtheta'_g-\vtheta_{g,t})\leq 0,\forall i\in[n],\forall j\in[q_i] ~ \bigg\}~,
    \end{aligned}
\label{defn:qset}
\end{equation}}where $\nabla_{\vtheta}\bar{G}_{\vtheta_t} = \frac{1}{n} \sum_{i=1}^n \frac{1}{q_i} \sum_{j=1}^{q_i} \nabla_{\vtheta}G_{\vtheta_t}(u^{(i)})(\vy^{(i)}_j)$, $\ell_{i,j}=(G_{\vtheta_t}(u^{(i)})(\vy_{j}^{(i)})-G^\dagger(u^{(i)})(\vy^{(i)}_j))^2$, and both $\nabla_{\vtheta_{f}}f^{(i)}_k$ and $\nabla_{\vtheta_{g}}g^{(i)}_{k,j}$ are evaluated on $\vtheta_t$.
%\begin{align}
\label{defn:qset_DON}
\end{restatable}

We now prove the RSC and smoothness conditions (corresponding to Conditions~\ref{cond:rsc} and~\ref{cond:smooth}, respectively).
Using the nomenclature of Section~\ref{sec:optmain}, the set $B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0)$ corresponds to $\mathcal{B}(\vtheta_0)$, and 
$B^t_{\kappa} := Q_{\kappa}^t \cap B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0) \cap B^{\mathrm{Euc}}_{\rho_2}(\vtheta_t)$ corresponds to $\mathcal{N}_t$.
%
%

\begin{restatable}[{\bf RSC for DONs}]{theo}{RSCLoss}
Consider Assumptions~\ref{asmp:Activation_Function}, \ref{asmp:smoothinit}, and~\ref{asmp:iter-1}, and $Q^t_{\kappa}$ as in Definition~\ref{defn:qset_DON}.
%with $\kappa\in(0,\frac{1}{2}]$. 
Then, the set $B^t_{\kappa} := Q_{\kappa}^t \cap B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0) \cap B^{\mathrm{Euc}}_{\rho_2}(\vtheta_t)$ is a non-empty set that satisfies Condition~\ref{cond:rsc}(a) and (b) for suitable $\rho_2$. 
%, and 
%(ii) 
Moreover, 
with probability at least {$1-2KL(\frac{1}{m_f}+\frac{1}{m_g})$}, at step $t$ of GD, % 
%$\forall \vtheta^{\prime} \in B^t_{\kappa}$,
the DON loss $\gL$~\eqref{eq:loss-don} satisfies
equation~\eqref{eq:RSC-prim} with
\begin{equation}
\alpha_t = 2\kappa^2 \| \nabla_{\vtheta} \bar{G}_t \|_2^2 - 
    c_1K^2\left(\frac{1}{\sqrt{m_f}}+\frac{1}{\sqrt{m_g}}\right)
        \label{eq:RSCLoss}
\end{equation}
where $\nabla_{\vtheta}\bar{G}_t = \frac{1}{n} \sum_{i=1}^n \frac{1}{q_i}  \sum_{j=1}^{q_i} \nabla_{\vtheta}G_{\vtheta_t}(u^{(i)})(\vy^{(i)}_j)$, 
and for some constant $c_1 >0$ 
%
which depends polynomially on the depth $L$, and the radii $\rho$, $\rho_1$, and $\rho_2$ whenever $\sigma_0\leq 1-\rho\max\{\frac{1}{\sqrt{m_f}},\frac{1}{\sqrt{m_g}}\}$.
%=======
%=========
Thus, the loss $\gL$ satisfies RSC w.r.t $(B_{\kappa}^t, \vtheta_t)$, i.e., Condition~\ref{cond:rsc}(c), whenever $\| \nabla_{\vtheta} \bar{G}_t \|_2^2 = \Omega(\frac{1}{\sqrt{m_f}}+\frac{1}{\sqrt{m_g}})$.
\label{theo:rsc_main_DON}
\end{restatable}
%

\begin{restatable}[{\bf Smoothness for DONs}]{theo}{RSS}
Under \TwoAsmpsref{asmp:Activation_Function}{asmp:smoothinit}, with probability at least {$1-2KL(\frac{1}{m_f}+\frac{1}{m_g})$}, the DON loss $\cL$~\eqref{eq:loss-don} is $\beta$-smooth in $B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0)$ with $\beta =c_2K^2$, where 
%
$c_2>0$ is a constant which depends polynomially on the depth $L$, and the radii $\rho$, $\rho_1$, and $\rho_2$ whenever $\sigma_0\leq 1-\rho\max\{\frac{1}{\sqrt{m_f}},\frac{1}{\sqrt{m_g}}\}$.
%===
%%
%%
%===
%
\label{theo:smooth_main}
\end{restatable}
%

\begin{remark}[Ensuring that $\alpha_t/\beta<1$] 
\label{rem:abeta-DON}
\pcedit{As mentioned in Remark~\ref{rem:ratio}}, in order to use the optimization framework from Section~\ref{sec:optmain}, the statement of Theorem~\ref{theo:global_main} requires 
$\alpha_t/\beta\leq 1$. We prove that this condition is satisfied with a strict inequality for DONs in Proposition~\ref{prop:RSC-smooth-DON} in Appendix~\ref{app:donopt}. 
\qed
\end{remark}

%
\paragraph{\textbf{Optimization Under Gradient Descent for DONs.}} We have that Theorem~\ref{theo:rsc_main_DON} satisfies Condition~\ref{cond:rsc} and Theorem~\ref{theo:smooth_main} satisfies Condition~\ref{cond:smooth}. We also proved that $\alpha_t/\beta<1$. Thus, when $\| \nabla_{\vtheta} \bar{G}_t \|_2^2 = \Omega(\frac{1}{\sqrt{m_f}}+\frac{1}{\sqrt{m_g}})$, i.e., $\alpha_t>0$, a decrease on the loss function by GD is ensured with probability at least $1-2KL(\frac{1}{m_f}+\frac{1}{m_g})$ towards its minimum value taken within the set $B^{\mathrm{Euc}}_{\rho,\rho_1}(\vtheta_0)$ due to Theorem~\ref{theo:global_main}.

\begin{remark}[The benefit of over-parameterization for the RSC property]
According to~\eqref{eq:RSCLoss}, $\| \nabla_{\vtheta} \bar{G}_t \|_2^2 = \Omega(\frac{1}{\sqrt{m_f}}+\frac{1}{\sqrt{m_g}})$ is needed to ensure that $\alpha_t>0$, i.e., to ensure that the empirical loss $\gL$ satisfies the RSC property at time $t$. Thus, as both widths $m_f$ and $m_g$ increase, $\gL$ attains the RSC property at a lower value of $\| \nabla_{\vtheta} \bar{G}_t \|_2^2$.
%
%
%
%
%
%
  \qed
  \label{rem:RSC-m}
\end{remark}
%
\begin{remark}[Over-parameterization allows for a larger neighborhood around initialization]
%
%
%
%
%
\pcedit{The condition $\sigma_0\leq 1-\rho\max\{\frac{1}{\sqrt{m_f}},\frac{1}{\sqrt{m_g}}\}$ (required for obtaining a polynomial dependence on $L$ for both RSC and smoothness parameters) implies %and the radii to be at most polynomial on $L$.}
%
%requires 
$\rho\leq\min\{m_f,m_g\}$ since $\sigma_0$ must be positive.}
%
%This means that}
%
%Moreover, since 
\pcedit{Thus,} it is possible to \pcedit{increase the} %make the 
radius $\rho$ %larger 
as we increase 
both $m_f$ and $m_g$. 
%
Thus, we can \pcedit{enlarge}
%it is possible to enlarge 
the neighborhood around the initialization point where our guarantees hold \pcedit{as the widths increase}.
%
\label{rem:largerNeighb-m}\qed
\end{remark}