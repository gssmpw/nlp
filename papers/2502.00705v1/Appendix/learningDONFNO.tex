%\bsdelete{\subsection{DON Architecture}\label{subsec:ArchitectureDON}
%We now briefly outline the architecture used {in the rest of the paper.} \pcdelete{throughout the analysis in Sections~\ref{sec:optimization} and \ref{sec:ntk} and in the numerical experiments in \Secref{sec:Experiments}.}We adopt fully connected feedforward neural networks (FNNs) for both the branch and trunk nets which is also the baseline DON model in \citet{lu20201DeepONet}.  \Figref{fig:deeponetArchitecture} in the Appendix shows a schematic of the architecture and the notation used throughout this paper. For the architecture we adopt the unstacked configuration (see, Fig 1d in \citet{lu20201DeepONet}). \pccomment{Not sure if this last sentence should be included, especially since citing the figure makes the paper look less self-contained.}}

%
%\bsdelete{{The DON training tuple is $\gD^{(i)}:=\left(\{u^{(i)}(x_r)\}_{r=1}^R,\, \{y^{(i)}_j\}_{j=1}^{q_i}, \{G(u^{(i)})(y^{(i)}_j)\}_{j=1}^{q_i}\right)$, $i\in[n]$ (recall that $n$ is the sample size).}
%\pcdelete{\begin{remark}[DON %Training Tuple]

%\end{remark}}}
%
%\bsdelete{{We denote the width of the branch net by $m_f$ and the trunk net by $m_g$. We assume that both nets have the same width in their hidden layers, and further assume $m_f = m_g = m$ in Section~\ref{sec:optimization}. Similarly, for the experiments we use $m_f = m_g$ unless otherwise stated, in which case (when $m_f\neq m_g$), the analysis in Section~\ref{sec:optimization} still holds with $m=\min (m_f, m_g)$.} \pccomment{I added that both branch and trunk nets have all hidden layers of the same size! This was missing!}}
\subsection{DON training tuple}
Each DON training data comprises of the tuple $\gD^{(i)}:=\left(\{u^{(i)}(x_r)\}_{r=1}^R,\, \{y^{(i)}_j\}_{j=1}^{q_i}, \{G(u^{(i)})(y^{(i)}_j)\}_{j=1}^{q_i}\right)$. The total training dataset comprises of all such training tuples $\gD = \{ \gD^{(i)}\}_{i=1}^n$.

\subsection{Motivation for FNOs}
FNOs are closely related to the notion of fundamental solutions. This allows us to write ${k_{l}:=}k\left(x, y, a(x), a(y) ; \vtheta_{\gF^{(l)}}\right) := k\left(x-y; \vtheta_{\gF^{(l)}}\right)$ \citep{li_fourier_2021}. Taking the Fourier Transform ($\mathscr{F}$) and applying the convolution theorem gives
$\left(\mathcal{K}^{(l)}(a ;  \vtheta_{\gF^{(l)}}) \alpha^{(l-1)}\right)(x)=
\mathscr{F}^{-1}\left(\mathscr{F}\left(k_l\right) \cdot \mathscr{F}\left(\alpha^{(l-1)}\right)\right)(x)$, $\forall x \in \gT$, $l\in [L]$.

This helps in parameterizing the \emph{kernel operator} $R^{(l)} = \mathscr{F}(k_l)$ directly in the Fourier space {and in simplified notation obtain}, 
$\left(\mathcal{K}^{(l)} \alpha^{({l-1})}\right)(x)
=\mathscr{F}^{-1}\left(R^{(l)} \cdot\left(\mathscr{F} \alpha^{({l-1})}\right)\right)(x)$, $\forall x \in \gT$, $l\in [L]$.

{Replacing this quantity back in~\eqref{eq:nonFnoLay},} 
following \citep{li_fourier_2021}, we define each Fourier block as follows
\begin{align}
    \alpha^{(l)}(x)
    =
    \phi\left(W^{(l)} \alpha^{(l-1)} 
    + 
    \mathscr{F}^{-1}\left(R^{(l)} \cdot \mathscr{F}\left(\alpha^{(l-1)}\right)\right)\right)(x), \quad x\in\gT, \quad l \in [L],
    \label{eq:ContinuousFNOBlocks}
\end{align}
where 
the Fourier transform of the input function $\alpha^{({l-1})}$ is $\mathscr{F}\alpha^{({l-1})}(\xi) := \int_{\gT} e^{-2\pi i <\xi ,y>} \alpha^{({l-1})}(y) dy$, $y\in \gT$.
%
{Notice that $R^{(l)}$ is defined by the set of unknown parameters $\vtheta_{\gF^{(l)}}$, whereas $\vtheta_{F^{(l)}}$ is defined by both the affine operator $W^{(l)}$ and parameters $\vtheta_{\gF^{(l)}}$.}
%
% \begin{remark}
% The input $a$, output $u$ and the intermediate functions $\alpha^{(l)}$, $l\in[L]$, in~\eqref{eq:ContinuousFNOBlocks} are all functions, {unlike the case of standard neural networks where they would be vectors.} \pcdelete{(like in standard neural networks).} \bscomment{maybe be a bit more precise on the what spaces these belong to}\pccomment{I agree -- at least the intermediate functions should be such that a Fourier transform exists (i.e., is well-defined)}\qed  
% \end{remark}
We now turn to the discrete version of \eqref{eq:ContinuousFNOBlocks} and the associated architecture.


{Since we have a discrete domain,} we employ the Discrete Fourier Transform (DFT). The entries of the ${m}\times {m}$ DFT kernel ($F$) can be written (up to a suitable scaling) as $F_{kj}:= e^{\frac{-2\pi i}{{m}}(k-1)(j-1)}$, where $k, j \in [{m}]$ which allows us to write its action on an input vector $\aalpha^{(l-1)}{\in\mathbb{R}^m}$ as
    \begin{equation}
        v_k := \sum_{j=1}^{{m}} F_{kj}\alpha^{(l-1)}_j = \alpha^{(l-1)}_j e^{\frac{-2\pi i}{{m}}(k-1)(j-1)},
    \end{equation}
where $i^2 = -1$.

% \pccomment{I think we should establish here our assumption that the decoder is linear and that the encoder is basically an identity function over the input data (at least, it seems to be the identify function to me, unless I am missing something!). The reason is because it is here where we define the architecture. For example, when we introduced the architecture of the DeepOnet, we established all of our architecture assumptions in the section without declaring it as a separate assumption environment.}

% {
% Taking the FNO~\eqref{eq:FNOTotal}, we establish the following correspondence of parameters: $\vtheta_p$ is defined by (????) \pccomment{Complete here!},   $\vtheta_F=[\operatorname{vec}(W^{(1)})^\top,\operatorname{vec}(R^{(1)})^\top, \dots,\operatorname{vec}(W^{(L)})^\top,\operatorname{vec}(R^{(L)})^\top]^\top$, and $\vtheta_q=\v$.}
% %
% Now, given observations $\{a(x_i),u(x_i)\}_{i=1}^n$, the FNO learning problem amounts to determining the optimal values of the parameters $\vtheta = [\vtheta_p^\top\;\vtheta_F^\top\;\vtheta_q^\top]^\top$ by minimizing the empirical risk: 
% % \bscomment{the FNO implementation also define the $L^2$ norm of the error, namely, $e^2 :=\int_D | (u(x) - G_{\vtheta}(a)(x)|^2 dx$ (and using quadrature to evaluate the integral). The FNO paper never explicitly said which error norm they used in the results!}
% \begin{align}
% \label{eq:fno-loss}
%     \vtheta^\dagger = \argmin_{\vtheta\in\Theta}\gL_{\text{fno}}(
%         G_{\vtheta}(\vu), G^{\dagger}(\vu)
%     )
%     =
%     \frac{1}{n}\sum_{i=1}^n \left( G^{\dagger}(\vu)(x_i) - G_{\vtheta}(\vu)(x_i)\right)^2
% \end{align}
% \bscomment{change the FNO notation to have $\vu$ as the input}
% {for some parameter domain $\Theta$ of interest.}
% }

% We now outline the FNO model used in the optimization analysis in Section~\ref{sec:optFNO}. To begin with, we use a feedforward architecture for the FNO blocks as in $1$D examples in \citep{li_fourier_2021}, i.e. when $\gT\subset \R$. We consider the simplest possible setting where both the input $u$ and FNO output $f$ are scalar-valued functions. \bsdelete{ and the domain $\gT$ \pccomment{Question: $\gT{\subseteq \mathbb{R}}$ or $\gT{\subseteq \mathbb{R}^d}$?} is approximated using a $n$-point discretization $\gT^h$, where $\gT^h=\left\{x_1, \cdots x_n\right\}$. \pccomment{Why do you have $\gT^h$ instead of $\gT^n$?}}
% The FNO training dataset $\gD$ corresponds to the point values of the functions $u$ and $f$, i.e. $D:=\left\{u_i, f_i\right\}_{i=1}^n$, where $a_i:=a\left(x_i\right)$ and $f_i = f(x_i)$.
% %
% The widths for each of the FNO blocks are denoted by $m_l$, $l\in [L]$. We will assume in this paper that all Fourier blocks have equal widths throughout the FNO. Furthermore, as is often done in practice, we use discrete fourier transform (DFT). {The DFT let us obtain the discrete counterpart of~\eqref{eq:ContinuousFNOBlocks} which, along with the incorporation of the encoder $P$ and decoder $Q$  as extra layers of the network, let us obtain the entire FNO model. To this end, we will use a slightly modified model with a single-layer feedforward encoder and a single linear layer decoder, namely
% \begin{equation}
% \label{eq:FNOTotal}
% \begin{aligned}
% \alpha^{(0)} &= u \\
% \alpha^{(l)} & = \phi\left(
%         \frac{1}{\sqrt{m}} W^{(l)} \alpha^{(l-1)} +
%         \frac{1}{\sqrt{m}} F^H R^{(l)} F \alpha^{(l-1)}
%     \right),\quad l\in [{L}]\\
%     {f := }  \alpha^{(L+{1})} &:= \frac{1}{\sqrt{m}} \v^T \alpha^{({L})}~,
% \end{aligned}
% \end{equation}
% where $W^{(l)}, R^{(l)} \in \R^{m \times m}$ for $l \in[L]$, $W^{(1)} \in \R^{m \times d}$, $R^{(1)} = 0$. {Notice that we have explicitly scaled the parameters $W^{l}$ and $R^{(l)}$ with $1/\sqrt{m}$, which will be helpful for our analysis.} Furthermore, we denote the parameters of the FNO by $\vtheta = \left[\vtheta_w^\top\ \vtheta_r^\top\ \mathbf{v}^\top \right]^{\top}$, where $\vtheta_w = \left[\text{vec}(W^{(1)})^{\top},\dots ,\text{vec}(W^{(L)})^{\top}\right]^{\top}$ and $\vtheta_r = \left[\text{vec}(R^{(1)})^{\top},\dots ,\text{vec}(R^{(L)})^{\top}\right]^{\top}$.