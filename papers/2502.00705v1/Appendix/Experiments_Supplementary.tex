In this section we expand on the mathematical description of each operator learning problem studied in Section~\ref{sec:Experiments}. We also present further results on how the accuracy of each neural operator model improves as the width $m$ increases. Finally, we provide details about the hyperparameters and datasets used in the training of the corresponding models. 

We remark that all experiments with widths $m\in\{10,50\}$ were run on a personal computer with one NVIDIA Quadro GPU, while the rest of widths were on Google Colab with single NVIDIA L4 and A100 GPUs. 

\subsection{Antiderivative Operator}
We consider a simple one-dimensional Antiderivative or Integral operator given by
\begin{equation}
    s(x) := G(u)(x) = \int_0^x u(\xi)\,\mathrm{d}\xi, \qquad x\in [0, 1]~.
\end{equation}
Note that $G(u)$ is a linear operator and therefore learnable up to high accuracy. This is evident from the training loss in Figure~\ref{fig:seLU_Loss_DON} as well as from the sample solutions presented in Figure~\ref{fig:DON_solns_vs_width} for DONs and in Figure~\ref{fig:FNO_solns_vs_width} for FNOs. We observe that overall an increase in the width $m$ leads to higher training accuracy and lower training loss.

The sample size of the training data is $n=2000$, with every input function $u^{(i)}$, $i\in[n]$, being a one-dimensional Gaussian Random Fields (GRF).
For DON training, we choose $R=100$ input locations and we choose $100$ output locations for each input function, i.e., $q_i=100$, $i\in[n]$ (according to the notation in Section~\ref{subsec:DON_Setup}).\footnote{During training; however, for each $i\in[n]$, instead of averaging the loss over all the $q_i$ points, we simply randomly choose one of the $q_i$ points and evaluate the loss on it. This is strictly done in the interest of computational efficiency, since it is known to not reduce the accuracy of the results for the Antiderivative operator; e.g.,  see~\citep{lu20201DeepONet}.}  
%
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/soln_DON_AD_FNO.pdf}
    \caption{Sample solutions obtained for the Antiderivative operator for DONs for $m\in \{10, 50, 500\}$ at the end of the training process (80,000 epochs) for a randomly chosen input function. The ``data'' refers to the ground truth (obtained by a standard numerical solver) and ``pred'' corresponds to the learned operator.}
    \label{fig:DON_solns_vs_width}
\end{figure}
%
For the FNO, the input function is also sampled across $100$ locations (i.e., $\bar{R}=100$ using the notation in Section~\ref{sec:optFNO}); however, since we are interested in the model to provide an output of $100$ output locations, we modify the FNO architecture to provide this vector-valued output.
\footnote{This means that we have $R=1$ (according to the notation in Section~\ref{subsec:FNO_setup}) with the understanding that for each input function, we output a vector of size $100$. This is done as an alternative to an FNO with a scalar output which is averaged across the $100$ locations (for which we would have $R=100$), which is what we described in Section~\ref{sec:modelSetup}. We remark that we considered this modification on the output of the FNO just for the sake of computational efficiency, and this only empirically works for the case of the Antiderivative operator. Again, as in the case of DONs, we randomly sample one of the output locations to compute the loss during training.} 
%
For all the experiments we fix the learning rate for the \texttt{Adam} optimizer at $10^{-3}$ and with full-batch training, i.e., the batch size of $2000$ for both DONs and FNOs. For testing the trained neural operators, we generate another one-dimensional GRF. 
%
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/soln_FNO_AD_FNO.pdf}
    \caption{Sample solutions obtained for the Antiderivative operator for FNOs for $m\in \{10, 50, 500\}$. The setting is the same as in Figure~\ref{fig:DON_solns_vs_width}.}
    \label{fig:FNO_solns_vs_width}
\end{figure}

\subsection{Diffusion-Reaction Operator}
We are interested in learning an operator $G: u(x) \to s(x,t)$ for the solution operator of the one-dimensional Diffusion-Reaction equation implicitly given by
\begin{equation}
\label{eq:dr-eq}
\begin{aligned}
    \frac{\partial s}{\partial t}&=D \frac{\partial^2 s}{\partial x^2}+k s^2+u(x), \quad(x, t) \in(0,1] \times(0,1]~,
\end{aligned}
\end{equation}
with $D>0$ and zero initial and boundary conditions, namely, 
\begin{equation*}
    s(0, t) = s(1, t) = 0 \quad \text{and}\quad s(x, 0) = 0~,
\end{equation*}
along with a forcing function $u(x)$ defined by a GRF. This is the same setup as in~\citep{physicsInformed202WangPerdikaris,lu20201DeepONet}. The corresponding solutions for DONs and FNOs are presented in Figures~\ref{fig:DON_DR_solns_vs_width} and~\ref{fig:FNO_DR_solns_vs_width} respectively. Again, a larger width $m$ leads to a more accurate solution.

The neural operator aims to learn a mapping from the forcing function to the solution at different times in the interval $(0,1]$, in other words, the forcing function would be the \emph{input function} as defined in Section~\ref{sec:modelSetup}. We make use of a slightly modified solver provided at \url{https://github.com/PredictiveIntelligenceLab/Physics-informed-DeepONets} to generate the training data for the equation. We generate solutions for $n=5000$ input functions which are sampled on $100$ points in the space dimension (i.e., the interval $(0,1]$ for $x$ in~\eqref{eq:dr-eq} is divided in $100$ points) so that $R=100$ for DON and $\bar{R}=100$ for FNO (according to the notations in Section~\ref{subsec:DON_Setup} and Section~\ref{sec:optFNO} respectively). For computing the solutions, we are interested in computing them at $100$ different times $t$ within the time interval $(0,1]$ in~\eqref{eq:dr-eq} (in order to be able to plot the two-dimensional map on $x$ and $t$ in Figures~\ref{fig:DON_DR_solns_vs_width} and~\ref{fig:FNO_DR_solns_vs_width}). This division of both spatial and time dimensions results in a grid of $10,000$ points that can be chosen as output locations.
%
For the training of DONs, for each $i\in[n]$, we only select $100$ scattered points from the grid of output locations (out of their $10,000$ points), so that $q^{(i)}=100$, which will become the input to the trunk net. 
However, for the training of FNOs, we do choose the full grid as output locations and thus we modify the FNO to provide $10,000$ outputs instead of the scalar output provided in our theoretical analysis.%: $100$ values of time $t$ per location $x$.
\footnote{A scalar output is needed if we were interested in evaluating the operator at only one specific spatial location $x$ and one specific value of time $t$; however, as can be seen in Figures~\ref{fig:DON_DR_solns_vs_width} and~\ref{fig:FNO_DR_solns_vs_width}, we are interested in plotting solutions at multiple locations and times.} We fix the diffusivity as $D = 0.01$.
%
%
For all the experiments we use a constant learning rate of $3\times 10^{-4}$ and \texttt{Adam} optimizer with a batch size of $4000$. 
For testing the trained neural operators, we generate another one-dimensional GRF. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/soln_DON_DR_FNO.pdf}
    \caption{Sample solutions $s(x,t)$ obtained for the Diffusion-Reaction operator for DONs for $m\in \{10,500\}$ given an input $u(x)$. The top row corresponds to $m=10$ and the bottom row to $m=500$. The third column represents the pointwise difference of the ground truth or ``Data'' (first column) minus the obtained results from the learned DON or ``Pred'' (second column).}
    \label{fig:DON_DR_solns_vs_width}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/soln_FNO_DR_FNO.pdf}
    \caption{Sample solutions obtained for the Diffusion-Reaction operator $s(x, t)$ for FNOs for $m\in \{10,500\}$. The setting of the plots is the same as in Figure~\ref{fig:DON_DR_solns_vs_width} where the top row corresponds to $m=10$ and the bottom row to $m=500$.}
    \label{fig:FNO_DR_solns_vs_width}
\end{figure}

\subsection{Burger's Equation}
The Burger's equation operator learns an operator $G: u(x) \to s(x, 1)$, where 
\begin{align}
    \begin{aligned}
        & \frac{\partial s}{\partial t}
        +
        s \frac{\partial s}{\partial x}
        -
        \nu \frac{\partial^2 s}{\partial x^2}
        =
        0, \quad(x, t) \in(0, 2\pi] \times(0,1] \\
        & s(x, 0)=u(x), \quad x \in(0,2\pi]
    \end{aligned}\label{eq:Burgers}
\end{align}
with $\nu>0$ and periodic boundary conditions
\begin{align*}
    \begin{aligned}
        & s(0, t)=s(2\pi, t) \\
        & \frac{\partial s}{\partial x}(0, t)=\frac{\partial s}{\partial x}(2\pi, t)~.
    \end{aligned}
\end{align*}
%
The corresponding solutions for DONs and FNOs are presented in Figures~\ref{fig:DON_solns_Burgers_vs_width} and~\ref{fig:FNO_solns_Burgers_vs_width} respectively. Again, a larger width $m$ leads to a more accurate solution.

The neural operator aims to learn a mapping from the initial condition to the solution at time $t=1$, i.e. the mapping from $u(x)$ to the final solution $s(x, 1)$.  This is the operator learning problem originally studied in~\citep{li_fourier_2021}. 
We note that the initial condition would then be the \emph{input function} as defined in Section~\ref{sec:modelSetup}. We make use of the datasets publicly available at \url{https://github.com/neuraloperator/neuraloperator}, specifically the \texttt{Burgers\_R10.mat} dataset available at \url{https://drive.google.com/drive/folders/1UnbQh2WWc6knEHbLn-ZaXrKUZhp7pjt-}, which comprises of 2048 input functions and corresponding final solution (i.e., $u^{(i)}$ with associated solution $s^{(i)}(\cdot,1)$, $i\in[2048]$). All solutions are calculated for a single viscosity $\nu=0.01$. 
%
For all the experiments we use a constant learning rate of $10^{-3}$ and \texttt{Adam} optimizer with a batch size of $800$. We test the trained neural operators on a simple GRF sampled from the training dataset.
%
%
%
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/soln_DON_Burgers_FNO.pdf}
    \caption{Sample solutions obtained for the Burger's equation for DONs for $m\in \{10, 50, 500\}$. The setting is similar to the one in Figure~\ref{fig:DON_solns_vs_width} where we plot the obtained solution from the learned operator (denoted by ``pred'') along with the ground truth (denoted by ``data'') for different widths. 
    }
    \label{fig:DON_solns_Burgers_vs_width}
\end{figure}
%
\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{Figures/soln_FNO_Burgers_FNO.pdf}
    \caption{Sample solutions obtained for the Burgers equation for FNOs for $m\in \{10, 50, 500\}$. The setting is the same as in Figure~\ref{fig:DON_solns_Burgers_vs_width}.}
    \label{fig:FNO_solns_Burgers_vs_width}
\end{figure}
