\subsection{Learning Operators}\label{subsec:LearningInfiniteDimensions}
\label{subsec:Learning_Operators}We briefly outline the notion of learning for neural operators \citep{li_fourier_2021,li_neural_2020,lu20201DeepONet}. 
Consider two separable Banach spaces, the input space $\gU$ and the output space $\gV$, and a possibly nonlinear operator $G^\dagger: \gU\to \gV$. 

The standard operator learning problem seeks to approximate $G^\dagger$ by a parametric operator $G_{\vtheta}: \gU\to\gV$ that depends on the parameter vector $\vtheta\in \Theta$ defined over some parameter space $\Theta$. 

This is done by proposing an optimization framework where we learn a vector $\vtheta^\dagger\in\Theta$ that ``best'' approximates $G^\dagger$ in some sense. 
Given observations $\{\vu^{(j)}\}_{j=1}^n\in\gU$ and $\{G^{\dagger}(\vu^{(j)})\}_{j=1}^n\in \gV$ where $\vu^{(j)}\sim\mu$, $j=1,\dots,n$, is an i.i.d sequence from the probability measure $\mu$ supported on $\gU$, we take $\vtheta^{\dagger}$ as the solution of the minimization problem
\begin{equation}
    \vtheta^{\dagger}
    =
    \argmin_{\vtheta \in \Theta} \mathbb{E}_{\vu \sim \mu}\left[
        \gC\left(G_{\vtheta}(\vu), G^{\dagger}(\vu)\right)
    \right],
    \label{eq:learningProblemInfiniteDimensions}
\end{equation}
where $\gC$ is a suitable cost functional that measures the discrepancy on the approximation between the operators $G_{\vtheta}(\vu)$ and $G^{\dagger}(\vu)$ for a given $\vu\in\gU$. This optimization problem is analogous to the notion of learning in finite dimensions, which is precisely the setup for which classical deep learning is used.

\subsection{DON Architecture}
The schematic for the Deep Operator Network's architecture is presented in Figure~\ref{fig:deeponetArchitecture}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/DeepONet_Architecture_ModNew.pdf}
    \caption{
    A schematic of the %\emph{unstacked} 
    DON architecture by \citet{lu20201DeepONet} used in our study. We refer to the notation used in our paper. Note that the input functions need not be sampled on a structured grid of points.
    }
    \label{fig:deeponetArchitecture}
\end{figure}

\subsection{FNO Architecture}
A schematic for the Fourier Neural Operator's architecture is presented in Figure~\ref{fig:fnoArchitecture}.
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/FNO_arch.pdf}
    \caption{
    A schematic of the FNO architecture by \citet{li_fourier_2021} used in our study. We refer to the notation used in our paper. ``Spectral convolution'' and ``bypass convolution'' are terms used in the FNO literature to denote the effect of the linear mappings in the spectral and spatial domain, respectively.
    %and point out that  its connection to FNO notation/terminology used in literature.
    }
    \label{fig:fnoArchitecture}
\end{figure}
