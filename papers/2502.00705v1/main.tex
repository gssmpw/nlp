\pdfoutput=1

\documentclass[letterpaper]{article}

\usepackage[margin=1in]{geometry}

\usepackage{microtype}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsfonts,amsmath,amssymb,amsthm}       % blackboard math symbols
\usepackage{natbib}
\usepackage{array}
\usepackage{todonotes}
%\usepackage[dvipsnames]{xcolor}
\usepackage{xcolor}
\usepackage{subcaption,float,graphicx}
\usepackage{thmtools,thm-restate}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{mathrsfs}
% \usepackage{mathpazo}
\usepackage[
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
]{hyperref}
% Assuming math_commands.tex and notation.tex are present in the same directory
\input{math_commands} 
\input{notation}

\usepackage{enumerate,color,multirow}
\usepackage{xpatch}
\usepackage[suppress]{color-edits}
\addauthor{ab}{violet}
%\addauthor{abb}{magenta}
\addauthor{bs}{blue}
\addauthor{pc}{olive}


\title{Optimization for Neural Operators can Benefit from Width}

\author{%
%\hspace*{-10mm}
  Pedro Cisneros-Velarde$^\dagger$\\
    \normalsize{VMware Research}\\
    \texttt{pacisne@gmail.com}
    \and
    Bhavesh Shrimali$^\dagger$\\
    \normalsize{Corporate Research, Kimberly-Clark}\\
    \texttt{bhavesh.shrimali@gmail.com}
    \and
  Arindam Banerjee \\ 
  \normalsize{University of Illinois Urbana-Champaign}\\
  \texttt{arindamb@illinois.edu}
} 


%\vspace{50pt}


\date{}



\begin{document}
\maketitle
\def\thefootnote{$\dagger$}\footnotetext{These authors contributed equally to this work.}\def\thefootnote{\arabic{footnote}}



\begin{abstract}
    Neural Operators that directly learn mappings between function spaces, such as Deep Operator Networks (DONs) and Fourier Neural Operators (FNOs), have received considerable attention. Despite the universal approximation guarantees for DONs and FNOs, there is currently no optimization convergence guarantee for learning such networks using gradient descent (GD). In this paper, we address this open problem by presenting a unified framework for optimization based on GD and applying it to establish convergence guarantees for both DONs and FNOs. 
    In particular, we show that the losses associated with both of these neural operators satisfy two conditions---restricted strong convexity (RSC) and smoothness---that guarantee a decrease on their loss values due to GD. Remarkably, these two conditions are satisfied for each neural operator due to different reasons associated with the architectural differences of the respective models. One takeaway that emerges from the theory is that wider networks should lead to better optimization convergence for both DONs and FNOs. 
    We present empirical results on canonical operator learning problems to support our theoretical results.
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{Sections/introduction}
%
\section{Related Work}
\label{sec:related}
\input{Sections/summary_work}
%
\section{Learning Neural Operators}
\label{sec:modelSetup}
\input{Sections/modelSetup}

% \section{Restricted Strong Convexity of Neural Operators Models}
% \label{sec:rscNeuralModels}
% \input{Sections/rsc}

\section{Optimization Convergence Framework}
\label{sec:optmain}
\input{Sections/rsc2}
%
\section{Optimization Analysis for DON}
\label{sec:optDON}
\input{Sections/rscDON2}
%
\section{Optimization Analysis for FNO}
\label{sec:optFNO}
\input{Sections/rscFNO}
%
\section{Comparison between Neural Operators and Feedforward Neural Networks}
\label{sec:Comparison}
\input{Sections/comparison}
%
\section{Experiments}
\label{sec:Experiments}
\input{Sections/experiments}
%\vspace*{-2ex}

\section{Conclusion}
\label{sec:Discussion}
\input{Sections/discussion}
%

%\newpage
%\bibliographystyle{unsrt}
\bibliographystyle{my-plainnat}
%\bibliographystyle{icml2025}
\bibliography{references}


% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn

\section{Related Work}
\label{app:related}
\input{Sections/related}


\section{Additional Information on Neural Operators}
\label{app:learning_fno_don}
\input{Appendix/basics}

%\section{Learning Neural Operators}
%\label{app:learning_fno_don}
%\input{Appendix/learningDONFNO}

\section{Optimization Convergence Analysis for Section~\ref{sec:optmain}}
\label{app:rscopt}
\input{Appendix/rscopt}


\section{Analysis for Deep Operator Networks}
\label{app:donopt}
\input{Appendix/donopt}


\section{Analysis for Fourier Neural Operators}
\label{app:fnoopt}
\input{Appendix/FNOhessian}

\section{Supplementary Information for the Experiments}
\label{app:exp_don_fno}
\input{Appendix/Experiments_Supplementary}

\end{document}