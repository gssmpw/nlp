\documentclass[mnsc, nonblindrev]{informs4_revised}
% \documentclass[mnsc, blindrev]{informs4}

%\OneAndAHalfSpacedXI
\OneAndAHalfSpacedXII % Current default line spacing
%%\DoubleSpacedXI
%%\DoubleSpacedXII

\usepackage{amsmath,amssymb,amsfonts}

\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%

\usepackage{rotating}
\usepackage{fancyvrb}
\usepackage{subfig}

\usepackage{enumerate}
\usepackage{multirow}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{bm}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{bbm}
\usepackage{hyperref}[hidelinks]
\usepackage[normalem]{ulem}
% multi-reference
\usepackage{bibunits}

%%% OPRE uses endnotes. If you do not use them, put a percent sign before
%%% the \theendnotes command. This template does show how to use them.
\usepackage{endnotes}
\let\footnote=\endnote
\let\enotesize=\normalsize
\def\notesname{Endnotes}%
\def\makeenmark{$^{\theenmark}$}
\def\enoteformat{\rightskip0pt\leftskip0pt\parindent=1.75em
	\leavevmode\llap{\theenmark.\enskip}}

\usepackage[dvipsnames]{xcolor}
\definecolor{redorange}{RGB}{255, 68, 51}
\usepackage{color-edits}
\addauthor[Ruohan]{rz}{Bittersweet}
\addauthor[QJ]{qj}{red}
\addauthor[CY]{cy}{blue}

% user-defined commands
\SetKwInput{KwInit}{Initialization}
\newcommand{\phat}{\hat{\mathbb{P}}}
\newcommand{\supp}[1]{\mathrm{supp}(#1)}
\newcommand{\dom}[1]{\mathrm{dom}(#1)}
\newcommand{\ptrue}{\mathbb{P}^{*}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\diff}{\mathrm{d}}


\newtheorem{lemmaAp}{Lemma}[section]
\newtheorem{propositionAp}{Proposition}[section]
\newtheorem{corollaryAp}{Corollary}[section]
\newtheorem{assumptionAp}{Assumption}[section]
\newtheorem{definitionAp}{Definition}[section]

% mutiple reference list
\defaultbibliographystyle{informs2014} 
\defaultbibliography{reference} % name of the .bib file without extensions

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
% \ECRepeatTheorems
\JOURNAL{}

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.
\MANUSCRIPTNO{}

\begin{document}
% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
\RUNAUTHOR{Yang et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{Fragility-aware Classification for Understanding Risk and Improving Generalization} 

\TITLE{Fragility-aware Classification for Understanding Risk and Improving Generalization} 


% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
\AUTHOR{Chen Yang\textsuperscript{1}, Zheng Cui\textsuperscript{2}, Daniel Zhuoyu Long\textsuperscript{3}, Jin Qi\textsuperscript{1}, Ruohan Zhan\textsuperscript{1*}}
\AFF{\textsuperscript{1}Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology, \EMAIL{cyangap@connect.ust.hk, jinqi@ust.hk, rhzhan@ust.hk}}
\AFF{\textsuperscript{2} School of Management, Zhejiang University, \EMAIL{zhengcui@zju.edu.cn}}
\AFF{\textsuperscript{3} Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, \EMAIL{zylong@se.cuhk.edu.hk}}
% \AUTHOR{}
% \AFF{Department of Industrial Engineering and Decision Analytics, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong, }
% Enter all authors 
}


\ABSTRACT{%

Classification models play a critical role in data-driven decision-making applications such as medical diagnosis, user profiling, recommendation systems, and default detection. Traditional performance metrics, such as accuracy, focus on overall error rates but fail to account for the confidence of incorrect predictions, thereby overlooking the risk of confident misjudgments.
This risk is particularly significant in cost-sensitive and safety-critical domains like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences.
To address this issue, we introduce the Fragility Index (FI), a novel metric that evaluates classification performance from a risk-averse perspective by explicitly capturing the tail risk of confident misjudgments. 
To enhance generalizability, we define FI within the robust satisficing (RS) framework, incorporating data uncertainty. 
We further develop a model training approach that optimizes FI while maintaining tractability for common loss functions. Specifically, we derive exact reformulations for cross-entropy loss, hinge-type loss, and Lipschitz loss, and extend the approach to deep learning models.
Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies misjudgment risk and FI-based training improves model robustness and generalizability. 
Finally, we extend our framework to deep neural network training, further validating its effectiveness in enhancing deep learning models.






% Classification models are widely used in data-driven decision-making applications including medical diagnosis, user profiling, recommendation systems, and default detection. Common performance metrics for classification, such as accuracy, are rooted in the error rate of the classifier's prediction.
% However, the error-rate metrics fail to distinguish between errors with differing confidence levels—i.e., the classifier’s certainty in its predictions—thereby overlooking the critical risk of confident misjudgments.
% This risk can be vital in cost-sensitive and safety-critical applications like medical diagnosis and autonomous driving, where overconfident false predictions may cause severe consequences.
% Furthermore, as learning models grow more complex, the overfitting and generalization issue becomes increasingly urgent and can lead to high risks in decision-making when deploying the model to an unseen environment. These two challenges invoke the need to better understand and evaluate the classification model.

% In this paper, we introduce a novel metric, the Fragility Index (FI), to assess the classification performance by characterizing the error from a risk-averse perspective. Unlike conventional metrics, FI explicitly captures the tail risk of confident misjudgment. 
% Moreover, to strengthen the generalization, we define FI under the robust satisficing (RS) framework to account the uncertainty of the data environment. Building on FI, we develop a comprehensive framework to train classification models based on FI and RS and show tractability for common loss functions. In particular, we obtain exact novel reformulations for cross-entropy loss, hinge-type loss and Lipschitz loss. We also explore extensions like incorporation with deep learning. Through synthetic experiments and real-world medical diagnosis tasks, we demonstrate that FI effectively identifies the risk of misjudgment and FI-based training can further enhance the performance. With FI, the decision-makers can better understand the risk of confident misjudgment and the fragility of the classification model, which is overlooked by conventional error-rate metrics but important in practice. With the risk better characterized, more informed and reliable decisions can be made.

% Experiments with real datasets demonstrate the new insights brought by FI and the advantages of classifiers selected under FI, which always improve the robustness and reduce the risk of large errors as compared to classifiers selected by alternative metrics.
}

\KEYWORDS{Classification; Risk Management; Robust optimization; Robust satisficing}

\maketitle
% \cycomment{I updated the sections 1 (intro), 2 (an elementary guide), 3 (FI metric) and 5 (Numerical) (2024/5/3)}

% \cycomment{11/27
% Comments on the modification and last discussion:

% - The current structure: intro - literature - FI metric - FI training - numerical of classical model - neural networks. The previous extension of the finite sample guarantee and randomized policy are all moved to the appendix. Instead, I add a subsection in FI training about the generalization guarantee. 

% - In the section of FI metric, I give general calculation scheme and mention that Wasserstein model is put in the appendix due to results are similar to KL-divergence. Now, KL-divergence seems serves as a good example to illustrate the calculation and some implications.

% - The current structure of the training section: model - KL - Wasserstsein - Convex OT - Generalization. 
% }
\begin{bibunit}

\section{Introduction}

Classification is a fundamental machine learning task with broad applications in operations management, including supply chain management \citep{kumar2022role}, healthcare operations \citep{pianykh2020improving}, and empirical theory building \citep{chou2023supervised}. The selection of appropriate performance metrics to evaluate classification models is crucial and remains a key focus in both academia and industry \citep{powers2020evaluation}.

Many popular metrics focus on the rate of \emph{false predictions} \citep{grandini2020metrics}, which we refer to as ``misjudgment". For instance, accuracy measures pointwise misjudgment by aggregating errors across the population, while AUC--the area under the Receiver Operating Characteristic (ROC) curve--captures pairwise misjudgment when scores for two samples from different categories are in false order \citep{ref_Cortes}. However, these metrics overlook the \emph{risk of misjudgment} \citep{norton2019maximization}, which reflects the potential consequences that a decision-maker faces when the model fails. Accounting for such risks is particularly critical in healthcare \citep{huang2020tutorial}, finance \citep{moscatelli2020corporate}, and other operations management contexts.

Meanwhile, there is growing interest in improving the \emph{generalizability} of model predictions, particularly when generalizing across different environments. Motivated by this, we develop a comprehensive framework for classification models. We start by proposing a new performance metric that simultaneously accounts for the \emph{risk of misjudgment} and \emph{generalizability}, and then design a model training framework that effectively controls risk and ensures generalization. We conclude our work by demonstrating the advantages of the resulting model through empirical evidence.


% Classification is one of the fundamental problems in machine learning, which aims to identify the category of a given sample based on its features. As a stepping stone, classification has been widely applied in operations management, such as supply chain management \citep{kumar2022role}, healthcare operations \citep{pianykh2020improving} and empirical theory building \citep{chou2023supervised}. Selecting an appropriate performance metric is vital for evaluating the performance of a classification model, and this topic remains open for discussion and has drawn the constant attention of the researchers even recently \citep{chicco2020advantages, powers2020evaluation}. Many popular metrics such as accuracy are based on the confusion matrix, which records the rate of true and false predictions among different classes \citep{grandini2020metrics}. Besides, the area under the Receiver Operating Characteristics (ROC) curve, known as AUC, also has gained broad popularity \citep{ref_Cortes, ling2003auc, yang2022auc}.

% However, these metrics are limited in many contexts of operations management. They primarily focus on the classification error rate but fail to capture the degree of these errors
% \citep{norton2019maximization}. 
% % \qjcomment{Rewrite this sentence?} \cycomment{Done.}  
% Quantifying the risk and uncertainty of the model predictions is essential in many applications and has drawn the attention of the machine learning community recently \citep{abdar2021review, gawlikowski2023survey}. This motivates us to investigate the risk issue in classification models. 

\subsection{Risk of misjudgment}
% Risk of confident misjudgment


While \emph{risk} has been widely studied in operations research, its role in classification--specifically the risk of misjudgment--remains underexplored. This risk reflects a model's confidence in making false predictions, which is particularly critical in cost-sensitive or safety-critical tasks \citep{ vaicenavicius2019evaluating}.
In disease detection and medical diagnosis, the confidence of a classifier directly influences doctors' decisions and the allocation of medical resources \citep{huang2020tutorial}. In financial applications such as fraud detection, confidence levels are essential for accurately assessing credit risk \citep{moscatelli2020corporate}. Similarly, in autonomous driving and object detection, classifier confidence affects path planning and is vital to prevent accidents \citep{bojarski2016end}. In all these scenarios, the risk of misjudgment can significantly impact downstream decision-making.

Although state-of-the-art classification models, particularly deep neural networks, achieve high accuracy, they often struggle to provide reliable confidence estimates and are prone to the risk of misjudgment \citep{ayhan2022test}. This issue worsens as the models become more complex and the dimensions of the data increase. For instance, \cite{guo2017calibration} compared LeNet \citep{lecun1998gradient} and ResNet \citep{he2016deep}, showing that while LeNet’s accuracy aligns well with its confidence, ResNet tends to be overconfident with respect to its accuracy. Although advanced classifiers achieve lower error rates, they are more prone to overconfident false predictions, increasing the risk in downstream decision-making by failing to signal when they are likely to be incorrect.


The classification literature largely overlooks the risk of misjudgment. To illustrate, consider the AUC metric in binary classification, where positive samples have label 1 and negative samples have label 0. AUC measures the average rate at which a randomly chosen positive sample receives a higher score than a randomly chosen negative sample \citep{hanley1982meaning}. That is, AUC reflects the pairwise error rate but not the magnitude of the error \citep{norton2019maximization}\endnote{This magnitude is also related to the classification margin and generalization performance \citep{vapnik2013nature}.}.  

The magnitude of the error is closely tied to the risk of misjudgment. For a given positive-negative sample pair, a large pairwise error occurs when the negative sample is assigned a much higher probability of being positive than the positive sample, indicating overconfidence in false predictions. By ignoring this magnitude, the AUC fails to capture critical risk information related to the classifier's confidence estimates.
Motivated by this, we propose a new classification metric that incorporates the magnitude of the error to better facilitate risk management. While we begin with the expectation of the error magnitude, we extend beyond it to further enhance generalization, as clarified below.




% To illustrate how conventional classification metrics overlook the risk, we delve into AUC in binary classification as an example. Given an input, many classifiers, such as logistic regression and neural networks, naturally yield a score or probability, a numeric value representing the degree that this input belongs to a class. Regarding the estimated probability of an input being positive, binary classification can be viewed as a ranking problem where we want to rank the positive samples higher than the negative samples.  
% % As shown by \cite{hanley1982meaning}, AUC measures the probability that the classifier ranks a randomly chosen positive sample higher than a randomly chosen negative sample. 
% Let the ranking error denote the difference between the estimated probability of being positive for a negative sample and that of a positive sample. AUC represents the occurrence rate that the ranking error is less than zero, i.e., correct ranking for a positive-negative sample pair \citep{hanley1982meaning, fawcett2006introduction}. Despite its high interpretability and widespread use, AUC fails to capture the magnitude of the ranking error \citep{norton2019maximization}. Given a dataset, AUC only counts the occurrence of correct ranking pairs but pays no attention to the value of the ranking error, while the magnitude embodies the quality of ranking. Moreover, the magnitude information can be significant in enlarging the classification margin and improving generalization \citep{vapnik1998statistical}. Overlooking the magnitude, AUC fails to identify the severity of false predictions and cannot provide a comprehensive evaluation of the classification model.

% The magnitude of ranking error is closely related to the risk of confident misjudgment in classification. Given a positive-negative sample pair, a large positive ranking error means that the negative sample has a much higher estimated probability of being positive than the positive sample, indicating that the classifier must assign high belief to false predictions.  
% % \rzcomment{need more transition from the last paragraph to this one, that is, try to link the risk to overconfidence.} 
% % \cycomment{I write more transition sentences to build the flow from ranking error, to confidence, to overconfidence and risk. I deleted bAUC as it is no longer the target we compare with and put the application paragraph after this paragraph to illustrate why overconfidence means high risk. Hope the flow is better now}
% State-of-the-art (SOTA) classification models, in particular the deep neural networks, often struggle to provide reliable probability estimation \citep{ayhan2022test}. The risk of confident misjudgment in modern classification models becomes more serious, as the models grow more sophisticated and the data dimension expands. For example, \cite{guo2017calibration} compared the LeNet \citep{lecun1998gradient} and ResNet \citep{he2016deep}, and found that LeNet's accuracy and estimated probability are well-matched, whereas ResNet's estimated probability is disproportionately high relative to its accuracy. Even though advanced classifiers are more accurate, they also exhibit a greater tendency to make overconfident false predictions. This could induce a high risk in follow-up decision-making because the classification model cannot precisely signal when they are likely to be incorrect. AUC, by neglecting the magnitude, cannot capture any risk information related to the classifier's estimated probability and confidence.  

% % \qjcomment{I'm also confused about the relationship between overconfidence and risk.} \cycomment{I put the application paragraph after this paragraph to illustrate why overconfidence means high risk regarding each application.}

% The magnitude information and probabilistic interpretation are desired in many applications. Controlling the risk of confident misjudgment is beneficial in many cost-sensitive or safety-critical tasks \citep{zadrozny2001obtaining,vaicenavicius2019evaluating}. It is also essential that the classifier provides a trustworthy estimated probability that this observation belongs to each class. For example, in disease detection and medical diagnosis, some classifier's predictions will be passed to doctors to make further judgment \citep{jiang2012calibrating}. If the classifier makes false predictions firmly and fails to warn doctors, misdiagnosis can occur, and the cost may be unacceptable. In liver transplantation,
% % \qjcomment{you mean transplantation?} \cycomment{Sure, modified}
% the estimated probability is also used to prioritize patients on the waiting list, and the magnitude of ranking error is vital to evaluate the sequencing decisions. \citep{huang2020tutorial}. A good ranking is pivotal to fully utilize the resources and save lives. Similar arguments also apply in financial applications such as default prediction. \cite{moscatelli2020corporate} indicated that estimated probability is paramount for assessing the level of credit risk, and the large ranking error can cause misestimation. In auto-driving, the classifiers of the object detection system should also be able to identify when they are likely to make a false prediction. The system should be conservative to avoid any potential accidents \citep{bojarski2016end}. In all these scenarios, large ranking errors and overconfidence in false predictions can lead to a high risk for subsequent decision-making, which underscores why the risk of confident misjudgment should be clarified more carefully and comprehensively.

% In our work, we would like to contribute from a different and novel perspective: risk management. While risk is omnipresent in many applications in operations research, it has not been extensively investigated regarding the error magnitude in the context of classification. The concept of risk aversion is also overlooked in machine learning model design. Regarding the ranking error, our goal is to evaluate and control the large magnitude, promoting us to propose a new performance metric that evaluates the ranking error in a risk-averse manner by specifically penalizing the large errors. The most intuitive approach to addressing magnitude might involve the expectation of the error. However, the plain expectation is risk-neutral and fails to emphasize the risk of large magnitude. Eventually, we design a new metric, the Fragility Index (FI), by considering the degree of expectation deviation under certain distributional shift. The reason to incorporate distributional shift relates to our second motivation of improving generalization.

% \rzcomment{better to have some  words here to summarize the fragility index  we are proposing. I think we may even bring it up here that we look into the magnitude of classification error, defined as $\mathbb{E}[\epsilon(p)]$.} \cycomment{Since we decide to remove all math from the introduction, I can only mention the definition in plain words here.}

\subsection{Generalization via robust satisficing}


Another motivation for our work is to enhance the generalizability of classifiers when faced with unseen samples. Generalization is a central concern in machine learning, as the distribution of testing samples often deviates significantly from that of the training data. In practice, models are frequently deployed in unfamiliar or even adversarial environments. Notably, improving generalizability also aligns with mitigating the risk of misjudgment discussed earlier. In adversarial settings, where testing distributions are manipulated to degrade performance, risky samples--those with high-confidence false predictions--become prime targets. By increasing the weight of such samples, an adversary can easily impair the classifier's performance. Thus, a classifier with strong generalizability must also minimize the occurrence of risky predictions.



The conventional empirical risk minimization (ERM) framework optimizes model performance in the training data set, but does not guarantee generalization to other environments \citep{vapnik2013nature}. To address this, various heuristics such as regularization and data augmentation \citep{shorten2019survey}, along with theoretical advancements such as Vapnik–Chervonenkis theory and PAC-Bayesian learning \citep{guedj2019primer}, have been developed to improve generalization performance.

More recently, incorporating distributional ambiguity between training and target environments has gained popularity for improving generalization \citep{sinha2017certifying}. In particular, robust satisficing (RS) has emerged as a promising framework for handling distributional ambiguity in target-oriented settings \citep{long2023robust}. From a satisficing perspective, RS sets a reference performance level and minimizes the model \emph{fragility}, which measures deviations from this reference due to distributional shifts. Compared to the widely adopted distributionally robust optimization (DRO), which optimizes worst-case performance over the ambiguity set \citep{kuhn2024distributionallyrobustoptimization}, RS is notably less conservative \citep{long2023robust}.


In this work, we adapt the RS framework to classification and introduce a new metric, the \emph{Fragility Index} (FI), to quantify the fragility of a classifier based on the expected error magnitude. This metric not only captures the risk of misjudgment, but also inherits the generalizability properties of RS.






% Another motivation for our work lies in the desire to enhance the generalization ability of the classifier when faced with unseen samples. The conventional training process in machine learning relies on the empirical risk minimization (ERM) framework, where the training objective is evaluated under the empirical distribution of the training dataset \citep{vapnik2013nature}. Generalization is a fundamental concern in machine learning because the distribution of testing samples may deviate significantly from that of the training samples. In practice, we also need to deploy our model to some unfamiliar environments. Various theories have been developed to address this issue, including Vapnik–Chervonenkis theory \citep{vapnik2013nature} and PAC-Bayesian learning \citep{guedj2019primer}. During model training, techniques like regularization \citep{loshchilov2017decoupled} and data augmentation \citep{shorten2019survey} are able to improve generalization. 
% % The sensitivity criteria are usually used to evaluate the generalization ability \citep{ref_Dimopoulos, forouzesh2021generalization}.
% % In addition, \cite{ref_Meir,ref_Rigollet} studied the generalization error bounds of classifiers, and \cite{ref_Anguita} also agrees that certain bounds have been shown to be an important indicator of the generalization ability for some specific classification tasks. 
% In particular, adversarial training, which incorporates adversarial samples during training, has been validated to be significant to generalization. \citep{taori2020measuring, wu2020adversarial, bai2021recent, najafi2019robustness}.

% In recent years, distributional ambiguity, which considers the training objective under a range of distributions, has emerged as a natural implementation of adversarial training \citep{sinha2017certifying}. It allows the data distribution to be arbitrary in a certain ambiguity set, and we need to regulate the worst-case performance across this ambiguity set. Robust satisficing (RS) is a new framework to incorporate the distributional ambiguity in a target-oriented setting \citep{long2023robust}. In RS, we minimize the degree of objective deviation over a target level caused by the distributional shift. The major constraint of RS serves as a natural generalization guarantee under distributional shift, and the optimal solution means the tightest bound. In contrast, distributionally robust optimization (DRO) is plain and minimizes the objective under the worst-case distribution \citep{rahimian2019distributionally, kuhn2019wasserstein}. However, the ambiguity set in DRO is usually defined as a ball centered around the empirical distribution of the training data, with the radius of the ball being a hyperparameter to regulate the degree of ambiguity. The ambiguity radius is hard to determine and interpret, which limits the use of DRO. In RS, we control the target level of the objective instead, which is more interpretable than the distributional ambiguity radius. 

% In our work, we tailor the RS framework to classification and define our new metric FI as the fragility of the expectation of the ranking error. Therefore, our FI admits the advantages of the fragility measure highlighted by \cite{long2023robust}. FI evaluates the degree of target violation of the ranking error under distributional shifts, so it inherits the spirit of adversarial training and provides a generalization guarantee on the out-of-sample performance. Moreover, there exists a surprising alignment between enhancing generalization ability and easing the risk of confident misjudgment. Consider the adversarial setting where nature can manipulate the testing sample distribution to impair the classifier's performance. If there are some risky samples where the classifier tends to make false predictions with high confidence, nature can easily achieve its goal by increasing the weight of these risky samples. Therefore, a classifier with a strong generalization ability should also avoid making too many risky predictions. By incorporating the RS framework, FI can inspect the risk of confident misjudgment, so guarantee the generalization ability simultaneously.

% \rzcomment{shall we visit  Figure \ref{fig:sketch_of_same_auc_error} somewhere around here saying that under this new Fragility index, we can distinguish between SVM and Logistic Regressor.} \cycomment{Sure. I do this for the new example in Figure \ref{fig:sketch_of_same_auc_error}.}
% Finally, we revisit the two classifiers in Figure \ref{fig:sketch_of_same_auc_error} to quickly demonstrate the effectiveness of FI. As mentioned, the logistic regression and random forest classifiers have the same AUC, but the distributions of ranking errors are different. The logistic regression has a longer tail for positive ranking errors, and higher confidence in making false predictions. In terms of FI, the logistic regression is $0.020$, and the random forest is $0.016$. Notice that the smaller value of FI means a smaller target violation in the RS problem \eqref{eq:rs_formulation}, so is better. Therefore, the random forest model is better regarding the FI, which is consistent with our observation in Figure \ref{fig:sketch_of_same_auc_error}. 

\subsection{Contributions}

In this work, we develop a novel classification framework that addresses two critical challenges: (i) controlling the risk of misjudgment, which conventional performance metrics such as accuracy and AUC fail to capture, and (ii) improving the generalizability of classifiers to unseen or adversarial samples. We address these challenges through the following key contributions.

\begin{itemize}
    \item \emph{A new performance metric--Fragility Index (FI)}: We introduce the Fragility Index, which quantifies a classifier's fragility--a concept derived from robust satisficing that reflects model generalizability--based on error magnitude, thereby capturing the risk of misjudgment. FI is risk-averse, penalizes large error magnitudes, and effectively bounds the tail risk of the error distribution. Additionally, FI is tractable, interpretable, and applicable to various classification models.
    
    \item \emph{A classifier training framework controlling FI}: We provide a computationally tractable framework to minimize the classifier's fragility. Our framework supports loss functions such as hinge-type loss and cross-entropy loss, and we provide the first exact reformulation results. Empirical evidence demonstrates that classifiers trained under this framework, referred to as FI-based classifiers, effectively control FI\footnote{In contrast, developing a framework to control for AUC remains challenging \citep{yang2022auc}.}.  We further explore its implications under shifted environments and establish statistical learning guarantees.

    \item \emph{Extensive empirical evaluations.} We validate the effectiveness of the FI as a performance metric and the FI-based classification models through experiments on both synthetic data and a practical medical diagnosis dataset. The results demonstrate that FI-based classifiers improve robustness and generalizability compared to conventional ERM classifiers.
    
    \item \emph{Extension to deep neural networks.} We finally extend our training framework to deep neural networks, exploring the application of FI in this setting. We demonstrate the effectiveness of FI-based models through image-based medical diagnosis tasks, highlighting their improved performance.
\end{itemize}


% \rzcomment{The contribution below is not up-to-date.} 
% \cycomment{Updated.}
% \rzcomment{In my understanding, the contribution is two-fold: (i) we propose a new evaluation metric FI for classifiers, which captures the aspects that existing method does not capture; and (ii) we propose learning algorithms that minimize FI, and the learned classifier has better performances as compared to standard learning methods such as ERM.}
% \cycomment{Now three points in total. Two for metric and training and the last for numerical}
% In short, we desire to develop a new classification framework to cover the two motivations of controlling risk and improving generalization. Our framework can embody the risk of confident misjudgment, which conventional performance metrics like AUC fail to capture. Meanwhile, it can also enhance the generalization ability of the classifier to unseen samples. The main contributions of our work can be summarized as follows:
% \begin{itemize} 
%     \item 
%     We propose a new performance metric, FI, to evaluate the risk of confident misjudgment by incorporating RS into the ranking error. FI captures the magnitude information and risk of the ranking error, an aspect that is often overlooked by existing metrics such as AUC.
%     We show that FI is risk-averse and penalizes the large positive magnitude. Moreover, FI can also inspect the tail risk of the ranking error distribution and serve as the key parameter in the tail bound. FI is highly tractable and interpretable and can be applied to various classification models.
    
%     \item 
%     Building on the performance metric FI, we design a comprehensive framework to train classification models incorporating FI and RS. Unlike AUC which is challenging to be taken as the training criterion, we develop a ``FI-wrapper'' for any score-based classification models by minimizing FI. To the best of our knowledge, we first provide novel and exact reformulations for the hinge-type loss and cross-entropy loss. We also explore the implications like connection to label noise and statistical guarantees.
%     % \rzcomment{are you trying to say the guarantees of the learned model? can this one be integrated with the  one above?} \cycomment{Everything related to training is combined.}
    
%     \item We validate the effectiveness of FI as a performance metric and FI-based classification models through experiments on synthetic data and practical medical diagnosis. We demonstrate that the FI-based classifiers can improve the robustness and generalization ability compared with conventional ERM classifiers.  
    
%     \item We extend our training framework to deep learning models, explore the application of FI in deep neural networks and demonstrate the strength of the FI-based model through image medical diagnosis tasks. 
% \end{itemize}

\subsection{Notations}
Vectors are denoted by boldface lowercase letters (e.g. $\BFbeta$), while matrices are represented by boldface uppercase letters (e.g. $\BFB$). Sets are denoted by calligraphic letters (e.g. $\mathcal{X}$). We use $\mathcal{P}(\mathcal{X})$ to denote the set of all distributions of a $n$-dimensional random vector with support $\mathcal{X} \subseteq \mathbb{R}^n$. For a distribution $\mathbb{P}$, $\supp{\mathbb{P}}$ denote the support of $\mathbb{P}$.  For a positive integer $m$, let $[m] = \{1, \dots, m\}$. We follow the convention that $\inf \emptyset = + \infty$. 

The norm of the vector $\BFx$ is denoted by $\|\BFx\|$, and its dual norm is defined by $\|\BFx\|_* = \sup_{\|\BFz\| \leq 1} \BFz^T\BFx$. For a function $f:\mathcal{X}\to \mathbb{R}$, the effective domain of $f$ is denoted as $\dom{f}:=\{\BFx\in\mathcal{X}|f(\BFx)<\infty\}$. The conjugate of a convex function $\rho(\BFx)$ is defined as $\rho^*(\BFz) = \sup_{\BFx \in \dom{\rho}} \BFz^T \BFx - \rho(\BFx)$. 
For a two-variable function $\ell(\BFx, \BFy)$, $\ell^{1*}(\BFz, \BFy) = \sup_{\BFx \in \dom{\ell(\cdot, \BFy)}} \BFz^T\BFx - \ell(\BFx, \BFy)$ denotes the convex conjugate to the first variable $\BFx$. For convenience, we use $\dom{\ell}$ to denote $\dom{\ell(\cdot, \BFy)}$ and $\dom{\ell^{1*}}$ to denote $\dom{\ell^{1*}(\cdot, \BFy)}$. 

Finally,  the characteristic function of a set $\mathcal{X}$ is denoted as $\delta_{\mathcal{X}}(\BFx)$, where $\delta_{\mathcal{X}}(\BFx) = 0$ if $\BFx \in \mathcal{X}$ and $+\infty$ otherwise. 
The conjugate of the convex-set characteristic function is defined as $\delta^*_{\mathcal{X}}(\BFz) = \sup_{\BFx \in \mathcal{X}} \BFz^T\BFx$. The positive part is denoted by $(\cdot)_+ = \max\{0, \cdot\}$.





% Vectors are denoted by boldface lowercase letters (e.g. $\BFbeta$). Matrices are denoted by boldface uppercase letters (e.g. $\BFB$). Sets are denoted by calligraphic letters (e.g. $\mathcal{X}$). We use $\mathcal{P}(\mathcal{X})$ to denote the set of all distributions of a $n$-dimensional random vector with support $\mathcal{X} \subseteq \mathbb{R}^n$. For a distribution $\mathbb{P}$, $\supp{\mathbb{P}}$ denote the support of $\mathbb{P}$. For a function $f(\cdot)$, $\dom{f}$ denotes the domain of $f$. For a positive integer $m$, let $[m] = \{1, \dots, m\}$. We consider the convention $\inf \emptyset = + \infty$. \cycomment{this is a convention in optimization. When a minimization problem is infeasible (empty feasible set), the objective value is set to infinity. Some monotonicity results need it.} The conjugate of a convex function $\rho(\BFx)$ is defined as $\rho^*(\BFz) = \sup_{\BFx \in \dom{\rho}} \BFz^T \BFx - \rho(\BFx)$. The characteristic function of a set $\mathcal{X}$ is denoted by $\delta_{\mathcal{X}}(\BFx) = 0$ if $\BFx \in \mathcal{X}$ and $+\infty$ otherwise. \cycomment{This is the indicator functions commonly used in convex analysis. It is also called the characteristic function. To avoid potential misleading, I unify to characteristic function in our paper} The conjugate of the convex-set characteristic function is defined as $\delta^*_{\mathcal{X}}(\BFz) = \sup_{\BFx \in \mathcal{X}} \BFz^T\BFx$. The norm of vector $\BFx$ is denoted by $\|\BFx\|$, and its dual norm is defined by $\|\BFx\|_* = \sup_{\|\BFz\| \leq 1} \BFz^T\BFx$. For a two-variable function $\ell(\BFx, \BFy)$, $\ell^{1*}(\BFz, \BFy) = \sup_{\BFx \in \dom{\ell(\cdot, \BFy)}} \BFz^T\BFx - \ell(\BFx, \BFy)$ denotes the convex conjugate to the first variable $\BFx$. For convenience, we use $\dom{\ell}$ to denote $\dom{\ell(\cdot, \BFy)}$ and $\dom{\ell^{1*}}$ to denote $\dom{\ell^{1*}(\cdot, \BFy)}$. 




\section{Related Literature}
Our work relates to the design of performance metrics for uncertainty in classification tasks. Conventional classification metrics such as accuracy and AUC are based on the error rate, which represents the probability that some error occurs \citep{bishop2006pattern}. Building on these, numerous refined performance metrics like the F1-score and the Mattheus correlation coefficient have been developed \citep{grandini2020metrics}. However, error-rate-based metrics overlook the magnitude of errors, rendering them inadequate for assessing risks associated with confident misjudgments. To address this limitation, \cite{norton2019maximization} extended the AUC to buffered AUC through the lens of buffered probability of exceedance. \cite{chaudhuri2022certifiable} considered risk measures leveraging both superquantile and buffered probability. These metrics are designed to capture the risk of large errors and enrich information about the model performance. \cite{yang2023fragility} proposed the concept of fragility in binary classification and explored the effect of risk and its role in model selection. Our work extends these foundations significantly by generalizing metric design to multi-class settings with deeper theoretical insights, and developing a comprehensive training framework integrating fragility index and RS.


The magnitude of the error directly relates to the estimated probability of classifiers and the risk of misjudgment, linking our work to uncertainty quantification and calibration in machine learning \citep{ghanem2017handbook}. 
In particular, the uncertainty issue has drawn increasing attention of the machine learning community because the deep neural networks reveal the tendency of overconfidence \citep{guo2017calibration}. Traditional approaches focus on improving calibration techniques, in which the performance of the calibration can be examined by comparing the estimated probability and accuracy \citep{abdar2021review}. 
Recent efforts also integrate uncertainty assessment directly into training processes \citep{vaicenavicius2019evaluating}. Most existing methods are grounded in the scope of machine learning and statistics, such as the Bayesian method, ensemble learning, and data augmentation \citep{nemani2023uncertainty}. In contrast, our work proposes a new metric based on the distributional ambiguity and the RS framework. Metrics like buffered probability and superquantile only consider the empirical data distribution and rely on the conditional expectation, so have a high requirement on the data quality to estimate accurately. Our metric specifically penalizes the large magnitude of the error and also connects to enhancing the generalization ability by leveraging distributional robustness.

Our work also relates to the RS and DRO literature, which integrates distributional ambiguity into optimization and machine learning models. A key implication of DRO is bridging regularization and distributional robustness \citep{gao2024wasserstein}. The equivalence of DRO and regularization has been identified in classical machine learning models and deep learning models \citep{sagawa2019distributionally}, providing theoretical evidence for various regularization schemes.
For example, \cite{ref_Torkamani} interpreted and derived the regularization in the support vector machine (SVM) from the robust optimization perspective. 
Meanwhile, DRO is highly tractable \citep{mohajerin2018data}. Due to these advantages, DRO has been applied in many fields such as classification \citep{wang2024wasserstein}, reinforcement learning \citep{liu2022distributionally} and deep learning \citep{bui2022unified}. \cite{long2023robust} extended the DRO to RS by considering a target-oriented reformulation. The key parameter in RS is the beforehand target level that we aim to reach, which is more interpretable than the radius of distributional ambiguity in DRO. RS has also been applied in supervised learning \citep{sim2021new}. In our work, RS is tailored to capture the risk of confident misjudgment and enhance the generalization ability. We propose a new metric to establish the connection between risk and generalization ability in classification and also design a training framework based on RS and FI.


\section{Fragility Index: A New Performance Metric}
\label{sec:FI}

\subsection{Setup}


We consider a multi-classification task where the input feature is $\BFx \in \mathcal{X}$ and the label is $y \in \mathcal{Y} = [C]$. Let $\hat{\pi}(\BFx, y)$ denote the empirical joint distribution of the observed samples. Our focus is on score-based classifiers. A classifier can be characterized by $C$ score functions $h_j: \mathcal{X} \to \mathbb{R}, j \in [C]$. A sample $\BFx$ is classified using the following rule:
$$
    y_{pred}= \arg \max_{j \in [C]} h_j(\BFx).
$$

The one-vs-all strategy \citep{grandini2020metrics} is a widely adopted way to decompose the multi-classification task into $C$ binary classification tasks based on each class. It has also been employed to extend binary metrics such as AUC to the multi-class setting \citep{yang2021learning}. With the score function $h_j$ for class $j$, samples belonging to class $j$, denoted as $\BFx^j$, are regarded as positive, while other samples $\BFx^{\lnot j}$ are regarded as negative. The \emph{ranking error} of score function $h_j$ is defined as 
\begin{equation*}
    \varepsilon(h_j)=h_j(\BFx^{\lnot j})-h_j(\BFx^j).
\end{equation*}
The widely adopted performance metric, AUC, measures the probability of correct ranking for a randomly selected positive sample and negative sample \citep{hanley1982meaning}. Formally, $  AUC(h_j) 
    = \mathbb{P}_{\hat{\pi}} (h_j(\BFx^{\lnot j}) \geq h_j(\BFx^j))
    = \phat (\varepsilon(h_j) \leq 0)$,
where $\hat{\mathbb{P}}$ is the empirical distribution of the ranking error $\varepsilon(h_j)$ induced by the data distribution $\hat{\pi}(\BFx, y)$.
% A perfect classifier assigns a higher score to the positive sample $\mathbf{x}^+$ than to the negative sample $\mathbf{x}^-$, implying $AUC = \mathbb{P}(\varepsilon(h) \leq 0 ) = 1.0$. 
However, AUC does not account for the \emph{magnitude} of the ranking error $\varepsilon(h_j)$. Regarding magnitude, a natural extension is to consider the expectation $\mathbb{E}_{\phat}[\varepsilon(h_j)]$, but this measure is risk-neutral--it treats both positive values (incorrect rankings) and negative values (correct rankings) equally. 

In practice, only large positive values of $\varepsilon(h_j)$, which represent the risk of misjudgment, should be penalized. This observation motivates us to propose a risk-averse performance metric. To achieve this, we adopt the RS framework and quantify the sensitivity of the expected $\varepsilon(h_j)$ under distributional shift. Moreover, we focus on the ranking error to echo AUC. The discussion will naturally adapt depending on the chosen error metric. We defer concrete examples to Appendix \ref{appe:fi}.



\subsection{Definition of Fragility Index}
We now introduce our new risk-averse performance metric under distributional shift. For a score function $h$ of a certain class, let $\hat{\mathbb{P}}$ be the empirical distribution of the ranking error $\varepsilon(h)$ and $\mathbb{P}$ be an arbitrary distribution of $\varepsilon(h)$. Motivated by \cite{long2023robust}, we define the \emph{fragility} of $h$ as the expected deviation of the ranking error under  distributional shift: 
$$
    \mathrm{fragility}(h) = \frac{|\mathbb{E}_{\mathbb{P}}[\varepsilon(h)] - \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] |}{\Delta (\mathbb{P},\hat{\mathbb{P}}) },
$$ 
where $\Delta(\mathbb{P}, \hat{\mathbb{P}})$ denotes the distributional distance between $\mathbb{P}$ and $\hat{\mathbb{P}}$,  such as the Wasserstein distance or Kullback–Leibler (KL) divergence.


Intuitively, a classifier is considered robust if its fragility across all classes is small, since its ranking error is less sensitive to distributional shift by definition. 
Let $\tau_j$ be the target value for the ranking error $\varepsilon(h_j)$ of class $j$, and we define the Fragility Index (FI) of the classifier as follows.
\begin{definition}[Fragility Index]
   Given a target value $\tau$ for the ranking error, the Fragility Index of a scoring function $h: \mathcal{X} \to \mathbb{R}$ is defined as
    \begin{equation} \label{eq:fi_def}
        \mathrm{FI}(h; \tau)=\inf\left\{ r \geq 0 \mid  \mathbb{E}_{\mathbb{P}}[\varepsilon(h)]\leq \tau + r \triangle (\mathbb{P},\hat{\mathbb{P}}),\ \forall \mathbb{P}\in \mathcal{P}(\mathcal{E})\right\},
    \end{equation}
    where $ \mathcal{P}(\mathcal{E})$ is the set of all distributions for $\varepsilon(h)$ that has the support $\mathcal{E} \subseteq \mathbb{R}$. For a multi-classification classifier with score $h_j, j\in [C]$, the overall FI is defined as 
    $$
        \mathrm{FI} = \frac{1}{C} \sum_{j\in[C]} \mathrm{FI}(h_j;\tau_j).
    $$
\end{definition}


The definition \eqref{eq:fi_def}  highlights that FI represents the minimum required violation of the target value $\tau$ when considering the ranking error across all possible distributions.
It therefore is closely related to the risk of misjudgment in classification. An adaptation of Proposition 6 in \cite{long2023robust} shows that there exists a normalized convex risk measure $\rho(\cdot)$ such that 
$$
    \mathrm{FI}(h; \tau) = \inf \{r\geq 0 \mid  r \rho(\varepsilon(h)/r) \leq \tau\}.
$$


Moreover,  FI inherits several favorable mathematical properties. In particular, we extend the results of Theorem 2 in \cite{long2023robust} to demonstrate these properties, as shown below.

\begin{theorem}
    \label{thm:fi_properties}
    The Fragility Index $\mathrm{FI}(h; \tau)$ has the following properties.
    \begin{enumerate}[(a)]
        \item (Positive homogeneity) For any positive number $\alpha > 0$, $\mathrm{FI}(\alpha h; \alpha \tau) = \alpha \mathrm{FI}(h; \tau)$.
        \item (Subadditivity) For any two scoring functions $h$ and $h'$ belonging to the same class, $\mathrm{FI}(h + h'; \tau + \tau') \leq \mathrm{FI}(h; \tau) + \mathrm{FI}(h'; \tau')$.
        \item (Prorobustness) If $\sup \{\varepsilon | \varepsilon \in \mathcal{E}\} \leq \tau$, then $\mathrm{FI}(h; \tau) = 0$.
        \item (Antifragility) If $\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] > \tau$, then $\mathrm{FI}(h; \tau) = \infty$.
        \item ($\tau$-FI tradeoff) For any $\tau_1 \geq \tau_2 \geq \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] $, $\mathrm{FI}(h, \tau_1) \leq \mathrm{FI}(h, \tau_2)$.
        \item (Monotonicity) Suppose $h$ and $h'$ are two scoring functions of class $j$. The distance metric $ \Delta(\mathbb{P}, \hat{\mathbb{P}})$ is specialized to the KL-divergence or Wasserstein distance. If for arbitrary sample pair $(\mathbf{x}^j, \mathbf{x}^{\lnot j})$, $h(\mathbf{x}^{\lnot j}) - h(\mathbf{x}^j) \leq h'(\mathbf{x}^{\lnot j}) - h'(\mathbf{x}^j)$, then we have $\mathrm{FI}(h) \leq \mathrm{FI}(h')$.
    \end{enumerate}
\end{theorem}


The \textit{Positive homogeneity} and \textit{Subadditivity} properties show the consequence under linear transformation, which coincides with the properties of coherent risk measure. The properties of \textit{Prorobustness} and \textit{Antifragility} characterize when the extreme values of FI can be achieved. The \textit{$\tau$-FI tradeoff} property indicates that FI is a non-increasing function of $\tau$, which is crucial for understanding how to select an appropriate  $\tau$. Finally, the \textit{Monotonicity} property states that if one random ranking error dominates another state-wise (i.e., it is always smaller in all scenarios), then the corresponding FI values will also reflect this dominance.



We conclude this section by discussing the choice of the target value $\tau$, a hyperparameter that needs to be pre-specified when calculating FI. 
One natural choice is $\tau = 0 $, where the positive part of $\varepsilon(h)$ (indicating misjudgment)  is offset by the negative part (indicating correct predictions).
Alternatively, $\tau$ can be determined based on the empirical performance. For instance, given a score function $h$, we can set $\tau = \lambda \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)]$, where $\lambda$ acts as a tolerance level on the empirical expectation. 
For the remainder of the paper, we focus on  $\tau=0$ in both theoretical derivations and numerical experiments, which choice also facilitates a direct comparison of AUC. For convenience, we  denote $\mathrm{FI}(h) = \mathrm{FI}(h; 0)$.
%{\color{red}QJ: a little bit confused here. How about $\mathbb{E}_{\hat{\mathbb{P}}}$ being  positive? then $\tau$ cannot be $0$. Do we need to make some assumption?}
%\cycomment{When $\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] > 0$ and $\tau=0$, Theorem \ref{thm:fi_properties} implies that the FI goes to infinity, meaning that the risk is very large. This case is not meaningless, so I don't know whether we need an assumption to rule it out. I delete the discussion `` $\lambda > 1.0$'', because it implies $\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] > 0$, but this rarely happens.}

% Consider a binary classification task. The input feature $\BFx \in \mathcal{X}$ and label $y \in \{-1, 1\}$ are random variables, and the observed samples are characterized by an empirical joint distribution $\hat{\pi}(\BFx, y)$. Let $\BFx^+$ and $\BFx^-$ denote the positive and negative samples, and denote their conditional distributions as $\BFx^+ \sim \hat{\pi}(\BFx| y = 1)$ and $\BFx^- \sim \hat{\pi}(\BFx| y = -1)$. We seek to figure out a scoring function $h: \mathcal{X} \to \mathbb{R}$ and a threshold $t\in \mathbb{R}$ to classify any sample $\mathbf{x}$ with the rule
% \begin{equation*}
%     y_{pred}=\left\{
%         \begin{array}{cc}
%             +1, & \mathrm{if} \ h(\mathbf{x})\geq t;\\
%             -1, & \mathrm{if} \ h(\mathbf{x})< t.
%         \end{array}
%         \right.
% \end{equation*}
% \cite{hanley1982meaning} showed that the AUC measures the probability of correct ranking for a randomly selected positive sample $\mathbf{x}^+$ and negative sample $\mathbf{x}^-$. Define the ranking error as 
% \begin{equation*}
%     \varepsilon(h)=h(\mathbf{x}^-)-h(\mathbf{x}^+).
% \end{equation*}
% % \rzcomment{What is the underlying joint distribution of $(x^-, x^+)$? should that also depend on the ratio of positive versus negative samples?}
% % \cycomment{$(x^-, x^+)$ follows the product of the empirical distributions of positive and negative samples ($\hat{\pi}(\BFx^+| y = 1) \hat{\pi}(\BFx^-| y = -1)$). They are both empirical distributions and I mention the independence between $x^-$ and  $x^+$, so the ratio seems not matter. I have added some explanation about the distributional interpretation of the samples.}
% Then, AUC can be defined as
% \begin{equation*}
%     AUC(h) 
%     = \mathbb{P}_{\hat{\pi}} (h(\mathbf{x}^+) \geq h(\mathbf{x}^-))
%     = \mathbb{P}_{\hat{\pi}} (\varepsilon(h) \leq 0).
% \end{equation*}
% An ideal classifier should always give the positive sample $\mathbf{x}^+$ a higher score than the negative sample $\mathbf{x}^-$, leading to $AUC = \mathbb{P}(\varepsilon(h) \leq 0 ) = 1.0$. As a performance metric, AUC reveals the ranking quality of the dataset. However, as discussed, AUC does not capture the magnitude of the ranking error. To take the magnitude into account, the expectation $\mathbb{E}_{\mathbb{P}}[\varepsilon(h)]$ is a straightforward choice, but it is risk-neutral and treats both positive values (incorrect ranking) and negative values (correct ranking) equally. Only the large positive values represent the risk of confident misjudgment. Therefore, we establish a new performance metric based on RS framework, by quantifying the sensitivity of the expected ranking error upon distributional shift.

% \begin{remark}
%     Different score functions $h$ have different ranges. Since we are interested in the magnitude, normalization is needed to make a fair comparison for inhomogeneous classifiers. One way is to normalize the score value to $[0, 1]$ by interpreting the score as the estimated probability of belonging to a certain class. Then, the range of the ranking error is regulated to $[-1, 1]$. 
% \end{remark}

% % With the RS framework in problem \eqref{eq:rs_formulation}, we can eventually define our FI by specializing the target value $\tau$ and the distributional distance $\Delta(\mathbb{P}, \hat{\mathbb{P}})$ to the ranking error. In this section, we first go through the FI definition and properties and then show some potential extensions with the distance $\Delta(\mathbb{P}, \hat{\mathbb{P}})$ dedicated to the KL-divergence and Wasserstein distance.

% Let $\hat{\mathbb{P}}$ be the empirical distribution and $\mathbb{P}$ be any distribution of the ranking error $\varepsilon(h)$. The distributional distance $\Delta(\mathbb{P}, \hat{\mathbb{P}})$, such as Wasserstein distance or Kullback–Leibler (KL) divergence, measures the difference between $\mathbb{P}$ and $\hat{\mathbb{P}}$. Then, we can consider the expectation deviation of the ranking error under the distributional shift as 
% $$
%     \mathrm{fragility} = \frac{|\mathbb{E}_{\mathbb{P}}[\varepsilon(h)] - \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] |}{\Delta (\mathbb{P},\hat{\mathbb{P}}) }.
% $$ 
% Intuitively, a classifier is considered robust if its fragility is small. Let $\tau$ be the target value that we want the expected ranking error to reach, and we define the Fragility Index (FI) with respect to target $\tau$ as
% % \rzcomment{should we also have reference value $\tau$ (which may be a function of the nomial distribution $\Bar{P}$) in the definition?}
% % \cycomment{I add $\tau$ in the definition, and mention how to set $\tau$.}

% \begin{definition}
%     For a scoring function $h: \mathcal{X} \to \mathbb{R}$, the Fragility Index of $h$ is defined as
%     \begin{equation} \label{eq:fi_def}
%         \mathrm{FI}(h; \tau)=\inf\left\{ k \geq 0 \middle|  \mathbb{E}_{\mathbb{P}}[\varepsilon(h)]\leq \tau + k \triangle (\mathbb{P},\hat{\mathbb{P}}),\ \forall \mathbb{P}\in \mathcal{P}(\mathcal{E})\right\},
%     \end{equation}
%     where $ \mathcal{P}(\mathcal{E})$ is the set of all distributions for $\varepsilon(h)$ that has the support $\mathcal{E} \subseteq \mathbb{R}$.
% \end{definition}
% \qjcomment{It seems FI is only a direct application of RS, so maybe we need to highlight the difference of difficulties when applying to the classification problems.}
% \cycomment{I try to add a paragraph to describe the nontriviality of FI by emphasizing the insights. Hope it answers the question.} 
% Even though FI seems an application of RS technically, it is nontrivial to apply RS in classification and realize the corresponding insights. The key is to figure out a suitable concept to consider RS, which is the ranking error in FI. By considering the ranking error, FI is closely related to AUC, allowing for a comparison between the two metrics. As mentioned in the introduction, FI possesses two-fold interpretations: risk of confident misjudgment and a means to enhance generalization. Moreover, we still need to specify the target $\tau$ and the distributional distance $\Delta(\mathbb{P},\hat{\mathbb{P}})$ in definition \eqref{eq:fi_def}. The choices of $\tau$ and $\Delta(\mathbb{P},\hat{\mathbb{P}})$ are noteworthy and can provide deeper insights.

% To further calculate FI, we must specify the target $\tau$. As illustrated, $0$ is an important reference value for the ranking error, because the positive part of the ranking error means incorrect ranking. Therefore, $\tau = 0 $ is a natural choice. Alternatively, we can also determine the target based on the empirical performance. For example, given a classifier with $h(\BFx)$, consider $\tau = \lambda \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)]$, where $\lambda \geq 1.0$. The control parameter $\lambda$ can be viewed as a tolerance level of the empirical expectation.  
% To make a comparison with AUC, we mainly focus on the case where $\tau = 0$ in the calculation and numerical experiments. For convenience, we denote $\mathrm{FI}(h) = \mathrm{FI}(h; 0)$.

% FI closely relates to the risk of confident misjudgment in classification, because FI admits a risk-averse interpretation as the fragility measure and possesses favorable mathematical properties. According to Proposition 6 of \cite{long2023robust}, there exists some normalized convex risk measure $\rho(\cdot)$ such that 
% $$
%     \mathrm{FI}(h; \tau) = \inf \{k\geq 0 | k \rho(\varepsilon(h)/k) \leq \tau\}.
% $$
% Moreover, the definition \eqref{eq:fi_def} also shows that FI coordinates the behavior of the expected ranking error across all possible distributions supported on $\mathcal{E}$, to control the maximum target violation degree. This consideration of distributional ambiguity also resembles a process of adversarial training.
% In contrast to DRO, it allows nature to take its course. The modeler does not need to specify the size of the ambiguity set, which instead is a crucial issue in traditional DRO. For more properties, we sum up by extending Theorem 2 of \cite{long2023robust}.
% \begin{theorem}
%     \label{thm:fi_properties}
%     The Fragility Index $\mathrm{FI}(h; \tau)$ has the following properties.
%     \begin{enumerate}[(a)]
%         \item (Positive homogeneity) For any positive number $\alpha > 0$, $\mathrm{FI}(\alpha h; \alpha \tau) = \alpha \mathrm{FI}(h; \tau)$.
%         \item (Subadditivity) For any two scoring functions $h_1$ and $h_2$, $\mathrm{FI}(h_1 + h_2; \tau_1 + \tau_2) \leq \mathrm{FI}(h_1; \tau_1) + \mathrm{FI}(h_2; \tau_2)$.
%         \item (Prorobustness) If $\sup \{\varepsilon | \varepsilon \in \mathcal{E}\} \leq \tau$, then $\mathrm{FI}(h; \tau) = 0$.
%         \item (Antifragility) If $\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] > \tau$, then $\mathrm{FI}(h; \tau) = \infty$.
%         \item ($\tau$-FI tradeoff) For any $\tau_1 \geq \tau_2 \geq \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] $, $\mathrm{FI}(h, \tau_1) \leq \mathrm{FI}(h, \tau_2)$.
%         \item (Monotonicity) Suppose $h_1$ and $h_2$ are two scoring functions and the distance metric $ \Delta(\mathbb{P}, \hat{\mathbb{P}})$ is specialized to the KL-divergence or Wasserstein distance. If for each positive-negative sample pair $(\mathbf{x}^+_i, \mathbf{x}^-_j)$, $h_1(\mathbf{x}^-_j) - h_1(\mathbf{x}^+_i) \geq h_2(\mathbf{x}^-_j) - h_2(\mathbf{x}^+_i)$, then $\mathrm{FI}(h_1) \geq \mathrm{FI}(h_2)$.
%     \end{enumerate}
% \end{theorem}

% % \rzcomment{For the properties 1 \& 2 above, are they defined to hold almost surely? note that $\varepsilon(h)$ is a random variable.} \cycomment{I regulate all properties according to the Theorem 2 of \cite{long2023robust}. To be different from \cite{long2023robust}, I consider the case with nonzero $\tau$, and add the last property of $\tau$-FI tradeoff.}

% The \textit{Positive homogeneity} and \textit{Subadditivity} properties show the consequence under linear transformation, which coincides with the properties of coherent risk measure. The properties of \textit{Prorobustness} and \textit{Antifragility} show when the extreme values of FI can be achieved. The \textit{$\tau$-FI tradeoff} property indicates that FI is a non-increasing function of $\tau$. This property is important to understand how to select the target $\tau$. The \textit{Monotonicity} property states that if one random ranking error is always smaller than another for all scenarios (state-wise dominance), so is FI. 


\subsection{Calculation of Fragility Index}

We proceed to discuss the calculation of FI. For a score function $h$ and its ranking error $\varepsilon(h)$, we define the function $G(r)$ as: 
\begin{equation}
    \label{eq:def_gk}
    G(r) :=  \sup_{\mathbb{P} \in \mathcal{P}(\mathcal{E})} \left\{
        \mathbb{E}_{\mathbb{P}}[\varepsilon(h)] - r D (\mathbb{P},\hat{\mathbb{P}})
    \right\} - \tau.
\end{equation}
We omit the dependence of $G(r)$ on $h$ and $\tau$ for concision. The following lemma shows that calculating FI can be transformed into a root-finding problem.
\begin{lemma}
    \label{lemma:root_finding}
    The function $G(r)$ in Equation \eqref{eq:def_gk} is decreasing with respect to $r$, and the FI defined in Equation \eqref{eq:fi_def} satisfies $G(\mathrm{FI}(h;\tau)) = 0$.
\end{lemma}
Since $G(r)$ is monotonic, its unique root can be calculated using a bisection algorithm, as detailed  in Appendix \ref{appe:fi}. The bisection scheme is universally optimal for any distributional distance metric. The final step  to calculate $G(r)$ is to specify the distance metric, which enables solving the inner maximization problem in $G(r)$ and subsequently executing the bisection algorithm to determine the root.  

We instantiate the calculation of FI under two widely used distance metrics: the KL-divergence and the Wasserstein distance. Below, we present the results for the KL-divergence, and defer the analogous analysis for the Wasserstein distance to Appendix \ref{appe:fi}.


\subsubsection*{FI under KL-divergence}
Let $\mathrm{FI}_{\mathrm{KL}}(h; \tau)$ denote the FI metric under the KL-divergence. The KL divergence between two distributions is defined as:
\begin{equation}
    \label{eq:kl_def}
    D_{\mathrm{KL}}(\mathbb{P} || {\hat{\mathbb{P}}}) := \left\{ \begin{array}{ll}
        \mathbb{E}_{\mathbb{P}}\Big[\ln\left( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}{\hat{\mathbb{P}}}}\right)\Big] & \text{if $\mathbb{P} \ll \hat{\mathbb{P}}$};\\
        \infty & \text{otherwise;} 
    \end{array}\right.
\end{equation}
where  the notation $\mathbb{P} \ll \hat{\mathbb{P}}$  denotes that $\mathbb{P} $ is absolutely continuous with respect to $\hat{\mathbb{P}}$, equivalent to restricting $\mathcal{E} = \supp{\hat{\mathbb{P}}}$. We immediately have the reformulation result of Lemma \ref{lemma:root_finding}:
\begin{lemma}\label{lemma:fi_kl}
    Suppose $\mathbb{E}_{\phat}[\varepsilon(h)] \leq \tau$ and $\sup \{\varepsilon | \varepsilon \in \supp{\phat}\} > \tau$.
    Then, $\mathrm{FI}_{\mathrm{KL}}(h; \tau)$ is determined by the unique root of $G_{\mathrm{KL}}(r)$, where
    \begin{equation}
        \label{eq:gk_kl}
        G_{\mathrm{KL}}(r) = \mathbb{E}_{\hat{\mathbb{P}}}[\exp(\varepsilon(h)/r)] - \exp(\tau / r).
    \end{equation}
\end{lemma} 
This analytical expression of $G_{\mathrm{KL}}(r)$   simplifies the calculation of FI under the KL-divergence. Moreover, it reveals that a ranking error distribution $\varepsilon(h)$ with a heavy tail in the positive domain leads to a larger $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$. This highlights FI's inherent risk aversion to large values of $\varepsilon(h)$.


The following result demonstrates that the FI metric effectively bounds large error magnitudes. This property is essential for risk management and is notably absent in conventional metrics such as AUC.
\begin{proposition}\label{prop:fi_kl_ub}
    Given a scoring function $h$ and its $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$, we have
    \[
    \hat{\mathbb{P}}(\varepsilon(h)\geq \theta)\leq \exp\left(- \frac{\theta - \tau}{\mathrm{FI}_{\mathrm{KL}}(h;\tau)}\right).
    \] 
\end{proposition}
Recall that positive $\varepsilon(h)$ corresponds to misjudgment, where a negative sample receives a higher score than a positive sample.  Proposition \ref{prop:fi_kl_ub} establishes that FI provides an enveloping bound on the probability that  $\varepsilon(h)$ exceeds any given $\theta$, effectively bounding the right tail of the empirical distribution of $\varepsilon(h)$. In fact, FI represents the exponential decay rate of the tail: the smaller the value of $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$,  the faster the tail probability decays.


More precisely, FI provides a bound on the quantile of the error distribution of  $\varepsilon(h)$, a measure commonly referred to as the Value at Risk ($\mathrm{VaR}$) in risk management.
\begin{corollary}
    \label{cor:fi_kl_var}
    Given a scoring function $h$ and its $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$, the $\mathrm{VaR}$ of the empirical ranking error $\varepsilon(h)$ at the level $1 - \alpha$ is bounded by
    \[
    \mathrm{VaR}_{1 - \alpha} (\varepsilon(h)) = \inf\left\{\theta \in \mathbb{R} ~\left|~ \hat{\mathbb{P}}(\varepsilon(h) \leq \theta) \geq 1 - \alpha \right.\right\} \leq \tau - \mathrm{FI}_{\mathrm{KL}}(h;\tau) \ln\alpha.
    \]
\end{corollary}

We conclude this section with concrete examples that illustrate how
$\mathrm{FI}_{\mathrm{KL}}$ captures the risk magnitude information that is overlooked by AUC.


\noindent \textbf{Example 1.} We first consider a binary classification problem of a toy example with only two positive samples and two negative samples. Suppose we have two classifiers, and the score represents the estimated probability of being positive. For classifier A, it rates the positive samples with $\{0.6, 0.9\}$, and the negative samples with $\{0.2, 0.7\}$. For classifier B, it rates the positive samples the same as classifier A, but the negative samples with $\{0.1, 0.8\}$. The ranking errors of the two classifiers are $\{-0.4, -0.7, 0.1, -0.2\}$ and $\{-0.5, -0.8, 0.2, -0.1\}$, respectively.
\begin{table}[htbp]
    \centering
    \begin{tabular}{cccc}
        \hline
         Classifier & Accuracy & AUC & $\mathrm{FI}_{\mathrm{KL}}$ \\
         \hline
         A & $0.75$ & $0.75$ & $0.02$ \\
         B & $0.75$ & $0.75$ & $0.04$ \\
         \hline
    \end{tabular}
    \caption{Performance metrics of the two classifiers in Example 1.}
    \label{tab:my_label}
\end{table}
Notice that both classifiers make a mistake in the second negative sample, but the mistake severity of classifier B is higher. However, neither AUC nor accuracy can differentiate the two classifiers. Moreover, regarding the risk-neutral expectation $\mathbb{E}[\varepsilon]$, the two classifiers are the same. For $\mathrm{FI}_{\mathrm{KL}}$, we have $\mathrm{FI}_{\mathrm{KL}}(h_A) = 0.02$ and $\mathrm{FI}_{\mathrm{KL}}(h_B) = 0.04$, showing that classifier B has a higher risk of confident misjudgment.

%\cycomment{Ruohan thinks the first example looks mainly due to the magnitude but not the RS and the tail risk. I agree with it. Since this is only a four-point distribution, the tail risk is mainly determined by the largest point. We cannot have a good-looking tail like example 2. If we want a toy example like this, it is hard to avoid the magnitude issue.}
%\rzcomment{I suggest we move Example 1 which seems a bit artificial, but I don't have a strong opinion. Also for exposition purpose, @Yangchen, can you have a table illustrating difference metric comparison between  two classifiers?} \cycomment{Done.}

\noindent \textbf{Example 2.} 
We next provide a more realistic example. Consider a 20-dimensional binary classification problem, with 2 informative features and 18 noise features. The two informative features are generated from two Gaussian clusters. We train the logistic regression and random forest classifiers with a balanced dataset of 100 samples. 
% Notice that both classifiers are probabilistic, so their score is exactly the probability of being positive. 
We calculate the ranking error, confidence of false predictions, and the $\mathrm{FI}_{\mathrm{KL}}$ of the two classifiers. The results are shown in Figure \ref{fig:fi_example}.

Even though the ranking error distributions are distinct, they admit nearly the same AUC of $0.904$. In particular for the part of positive ranking errors, which means the false predictions, the logistic regression induces a much longer tail than the random forest. This indicates that the logistic regression classifier could make more severe false predictions than the random forest classifier. This is clearly shown in the distribution of the confidence of the false predictions in Figure \ref{fig:fi_example}, where the confidence of the logistic regression is significantly larger than the random forest. Our FI can differentiate by giving the logistic regression a larger $\mathrm{FI}_{\mathrm{KL}}$ of $0.020$ and the random forest a smaller $\mathrm{FI}_{\mathrm{KL}}$ of $0.016$. Unfortunately, AUC misses all this information and cannot distinguish the two classifiers.

\begin{figure}[htbp]
    \vspace{-10pt}
    \centering
    \includegraphics[width=0.4\linewidth]{eg_intro_ranking_error.pdf}
    \includegraphics[width=0.4\linewidth]{eg_intro_confidence_wrong.pdf}
    \caption{Distribution of ranking errors and classifier's estimated probability of the false predictions of two classifiers in the example. }
    % \rzcomment{In the left panel, we should emphasize the positive part and distinguish that from the negative part--maybe use different transparency. Also, what is the estimated probability of false predictions in the right panel? is it pairwise (ranking error) or pointwise (accuracy)?}
    % \cycomment{I set a smaller transparency to the negative values. See if it is satisfactory. I also change the x label of the right panel to ``confidence''. I think in the current version, confidence is more clear. }
    % \rzcomment{Can we reduce the transparency for the positive values?}
    % }
    % \cycomment{Done. I also reduce the transparency in the confidence figure for consistency.}
    \label{fig:fi_example}
    \vspace{-10pt}
\end{figure}


% \subsection{Calculation of FI}
% Then, we discuss the calculation of FI. It turns out that calculating FI can be transformed into a root-finding problem, which can be easily solved by a bisection method. Define function $G(k)$ as
% \begin{equation}
%     \label{eq:def_gk}
%     G(k) =  \sup_{\mathbb{P} \in \mathcal{P}(\mathcal{E})} \left\{
%         \mathbb{E}_{\mathbb{P}}[\varepsilon(h)] - k D (\mathbb{P},\hat{\mathbb{P}})
%     \right\} - \tau,
% \end{equation}
% \begin{lemma}
%     \label{lemma:root_finding}
%     The function $G(k)$ in equation \eqref{eq:def_gk} is non-increasing with respect to $k$ and the FI defined in \eqref{eq:fi_def} satisfies $G(FI(h;\tau)) = 0$.
% \end{lemma}
% Since $G(k)$ is monotonic, its unique root can be calculated by a bisection algorithm in Appendix \ref{appe:fi}. The bisection scheme is universally optimal for any distributional distance metric. However, to calculate $G(k)$, we still need to specify the distance metric and solve the maximization problem in $G(k)$. We next consider the two most commonly used distance metrics, the KL-divergence and the Wasserstein distance for further analysis. Since the results are similar, we focus on the KL-divergence and leave the analysis of the Wasserstein distance in Appendix \ref{appe:fi}.

% Let $\mathrm{FI}_{\mathrm{KL}}(h; \tau)$ denote the FI with the KL-divergence defined as 
% % \begin{equation}\label{eq:fi_def_kl}
% %     \mathrm{FI}_{\mathrm{KL}}(h; \tau)=\inf\left\{ k\geq 0 \middle | \mathbb{E}_{\mathbb{P}}[\varepsilon(h)]\leq \tau +  k D_{\mathrm{KL}}(\mathbb{P} || \hat{\mathbb{P}}),\  \forall \mathbb{P}\in \mathcal{P}(\mathcal{E})\right\},
% % \end{equation}
% \begin{equation}
%     \label{eq:kl_def}
%     D_{\mathrm{KL}}(\mathbb{P} || {\hat{\mathbb{P}}}) := \left\{ \begin{array}{ll}
%         \mathbb{E}_{\mathbb{P}}\Big[\ln\left( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}{\hat{\mathbb{P}}}}\right)\Big] & \text{if $\mathbb{P} \ll \hat{\mathbb{P}}$};\\
%         \infty & \text{otherwise.} 
%     \end{array}\right.
% \end{equation}
% The notation $\mathbb{P} \ll \hat{\mathbb{P}}$ to denote that $\mathbb{P} $ is absolutely continuous with respect to $\hat{\mathbb{P}}$, which is equivalent to restricting $\mathcal{E} = \supp{\hat{\mathbb{P}}}$. Then, we have
% \begin{lemma}\label{lemma:fi_kl}
%     Suppose $\mathbb{E}_{\phat}[\varepsilon(h)] \leq \tau$ and $\sup \{\varepsilon | \varepsilon \in \supp{\phat}\} > \tau$.
%     Then, $\mathrm{FI}_{\mathrm{KL}}(h; \tau)$ is determined by the unique root of $G_{\mathrm{KL}}(k)$, where
%     \begin{equation}
%         \label{eq:gk_kl}
%         G_{\mathrm{KL}}(k) = \mathbb{E}_{\hat{\mathbb{P}}}[\exp(\varepsilon(h)/k)] - \exp(\tau / k).
%     \end{equation}
% \end{lemma} 
% Compared with the plain empirical expectation $\mathbb{E}_{\hat{\mathbb{P}}} [\varepsilon(h)]$, the exponential function in formula \eqref{eq:gk_kl} amplifies the effect of the positive value of ranking errors, which reveals the aversion to large values of $\varepsilon(h)$.
% Thus, a ranking error distribution with a longer tail in the positive domain tends to result in a larger value of $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$, indicating that our concern for the large magnitude is emphasized by this metric.

% Recall that large $\varepsilon(h)$ corresponds to confident misjudgment. To bridge FI and risk of confident misjudgment, we provide further implication of $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$ from the perspective of the decay rate. Notice that the distribution of $\varepsilon(h)$ is usually decaying in the positive domain. The decay rate is a key parameter for the tail of the distribution. We can derive the following proposition showing that FI can serve as a bound for the decay rate of the empirical distribution of $\varepsilon(h)$.
% \begin{proposition}\label{prop:fi_kl_ub}
%     Given a scoring function $h$ and its $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$, we have
%     \[
%     \hat{\mathbb{P}}(\varepsilon(h)\geq \theta)\leq \exp\left(- \frac{\theta - \tau}{\mathrm{FI}_{\mathrm{KL}}(h;\tau)}\right).
%     \] 
% \end{proposition}

% % \rzcomment{This proposition characterizes the tail bound of $\epsilon_h$ -- the smaller the fragility index, the faster the tail decays (and the lower risk the classifier has?)} \cycomment{Yes, FI reflects the tail risk, which is commonly considered in risk management.}
% % \rzcomment{In that case, shall we be specific in the implications of Proposition \ref{prop:fi_kl_ub}, using the exact term "tail risk" and mention its acknowledgement in the risk management.}
% % \cycomment{I mention the tail the risk. I also explain that we can use the bound to evaluate the VaR directly, which is one of the most commen tail risk measure.}
% Proposition \ref{prop:fi_kl_ub} indicates that $\mathrm{FI}_{\mathrm{KL}}$ can help us to establish an enveloping bound for the probability that $\varepsilon(h)$ exceeds any $\theta$. FI is exactly the exponential decay rate. The smaller the value of $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$, the faster the tail decays.
% % Notice that the bound in Proposition \ref{prop:fi_kl_ub} is meaningful for probability only when $\theta \geq \tau$. In the case of $\theta \geq \tau$, as the value of $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$ decreases, a smaller upper bound is placed on the probability of $\varepsilon(h)$ exceeding $\theta$, which corresponds to a lower risk of misclassification by the classifier. 
% Moreover, since FI is equivalent to the decay rate, it has a strong implication in evaluating the tail risk of the ranking error distribution. Value at risk ($VaR$), as one of the most popular tail risk measures, represents some quantile of a distribution \citep{christoffersen2011elements}. The bound in Proposition \ref{prop:fi_kl_ub} leads to an upper bound of $VaR$ directly.
% \begin{corollary}
%     \label{cor:fi_kl_var}
%     Given a scoring function $h$ and its $\mathrm{FI}_{\mathrm{KL}}(h;\tau)$, the VaR of the empirical ranking error $\varepsilon(h)$ at the level $1 - \alpha$ is bounded by
%     \[
%     \mathrm{VaR}_{1 - \alpha} (\varepsilon(h)) = \inf\left\{\theta \in \mathbb{R} ~\left|~ \hat{\mathbb{P}}(\varepsilon(h) \leq \theta) \geq 1 - \alpha \right.\right\} \leq \tau - \mathrm{FI}_{\mathrm{KL}}(h;\tau) \ln\alpha.
%     \]
% \end{corollary}

% Finally, we demonstrate the intuition of $\mathrm{FI}_{\mathrm{KL}}(h; \tau)$ by examples, where we show how $\mathrm{FI}_{\mathrm{KL}}$ captures the information overlooked by AUC.

% \rzcomment{the example is a bit artificial, we should describe a particular classification distribution, specify two different score functions, and then calculate the corresponding $\epsilon(\cdot)$.}
% \cycomment{I modified the examples. Based on all previous discussions, there are currently two examples: one has only four samples with utterly artificial score; the other is based on an artificial dataset. The first example is straightforward. For the second one, it is originally the figure example in the introduction. Since we do not have any notation and definition in the introduction any more, I move it here. We consider the logistic regression and random forest classifiers, which are both probabilistic. Therefore, their score is exactly the probability that a sample is positive. The range of the score is $[0, 1]$, so the issue of scaling score is avoided. Besides, our data is generated from two Gaussian clusters, which intrinsically involves the random sample setting. The Gaussian determines the distributions $\pi(x|y = 1)$ and $\pi(x|y = -1)$. Using Bayesian formula, we can obtain $\pi(y| x)$, which shows given feature $x$, the label is not deterministic. 
% }

% \noindent \textbf{Example 1.} Consider a binary classification problem of two positive samples and two negative samples. Suppose we have two classifiers, and the score represents the estimated probability of being positive. For classifier 1, it rates the positive samples with $\{0.6, 0.9\}$, and the negative samples with $\{0.2, 0.7\}$. For classifier 2, it rates the positive samples the same as classifier 1, but the negative samples with $\{0.1, 0.8\}$. The ranking errors of the two classifiers are $\{-0.4, -0.7, 0.1, -0.2\}$ and $\{-0.5, -0.8, 0.2, -0.1\}$, respectively. Notice that both classifiers make a mistake in the second negative sample, but the mistake severity of classifier 2 is higher. However, no matter AUC or accuracy cannot differentiate the two classifiers. Moreover, regarding the risk-neutral expectation $\mathbb{E}[\varepsilon]$, the two classifiers are the same. As for $\mathrm{FI}_{\mathrm{KL}}$, we have $\mathrm{FI}_{\mathrm{KL}}(h_1) = 0.02$ and $\mathrm{FI}_{\mathrm{KL}}(h_2) = 0.04$, showing that the classifier 2 has a higher risk of confident misjudgment. \cycomment{Ruohan thinks the first example looks mainly due to the magnitude but not the RS and the tail risk. I agree with it. Since this is only a four-point distribution, the tail risk is mainly determined by the largest point. We cannot have a good-looking tail like example 2. If we want a toy example like this, it is hard to avoid the magnitude issue.}

% \noindent \textbf{Example 2.} 
% Then, we look at another more realistic example. Consider a 20-dimensional binary classification problem, with 2 informative features and 18 noise features. The two informative features are generated from two Gaussian clusters. We train the logistic regression and random forest classifiers with a balanced dataset of 100 samples. 
% % Notice that both classifiers are probabilistic, so their score is exactly the probability of being positive. 
% We calculate the ranking error, confidence of false predictions, and the $\mathrm{FI}_{\mathrm{KL}}$ of the two classifiers. The results are shown in Figure \ref{fig:fi_example}.
% \begin{figure}[htbp]
%     \vspace{-10pt}
%     \centering
%     \includegraphics[width=0.4\linewidth]{eg_intro_ranking_error.pdf}
%     \includegraphics[width=0.4\linewidth]{eg_intro_confidence_wrong.pdf}
%     \caption{Distribution of ranking errors and classifier's estimated probability of the false predictions of two classifiers in the example.
%     }
%     \label{fig:fi_example}
%     \vspace{-10pt}
% \end{figure}
% Even though the ranking error distributions are distinct, they admit nearly the same AUC of $0.904$. In particular for the part of positive ranking errors, which means the false predictions, the logistic regression induces a much longer tail than the random forest. This indicates that the logistic regression classifier could make more severe false predictions than the random forest classifier. This is clearly shown in the distribution of the confidence of the false predictions in Figure \ref{fig:fi_example}, where the confidence of the logistic regression is significantly larger than the random forest. Our FI can differentiate by giving the logistic regression a larger $\mathrm{FI}_{\mathrm{KL}}$ of $0.020$ and the random forest a smaller $\mathrm{FI}_{\mathrm{KL}}$ of $0.016$. Unfortunately, AUC misses all this information and cannot distinguish the two classifiers. 

% \subsection{Extending Fragility Index}
% \label{sec:fi_extensions}
% We now present two extensions of the binary Fragility Index (FI): one for the multi-class classification setting and the other for incorporating error metrics beyond the ranking error.

% \paragraph{Multi-classification FI.} 
% The FI can be naturally extended to the multi-class classification problem using the widely adopted one-vs-all strategy \citep{grandini2020metrics}, which has also been employed to extend binary metrics like AUC to the multi-class setting \citep{yang2021learning}.
% Concretely, consider a classification task with $C$ classes. The multi-class problem can be decomposed into  $C$ binary classification subproblems. 
% For each class $i$, samples from class $i$ are labeled as positive, while  samples from all other classes are labeled as negative. We  then compute FI for each binary subproblem. Let $\mathrm{FI}_i$ denote the  FI for the $i$-th subproblem.
% The overall FI for the multi-class classification problem is obtained by aggregating the results as follows:
% $$
%     \mathrm{FI}_{\text{multi}} = \frac{1}{C} \sum_{i=1}^C \mathrm{FI}_i.
% $$

% \paragraph{FI based on other error metrics.} 
% The FI definition \eqref{eq:fi_def} can also be applied to other classifier outputs, such as loss functions, beyond the ranking error discussed earlier. The implications of FI will naturally adapt depending on the chosen error metric.
% % For example, if we consider the loss function values in the definition \eqref{eq:fi_def}, we should compare FI with the accuracy instead of AUC because the large loss implies false predictions. 
% We defer concrete examples to  Appendix \ref{appe:fi}.


\section{FI-based Model Training}
\label{sec:fi_training}
With the new performance metric FI introduced, we now turn to developing a model training framework that effectively optimizes this metric for the learned model. Recall that FI is defined based on the ranking error, which involves pairwise comparisons between positive and negative samples. This dependency however poses scalability challenges for large datasets if used directly as the objective function. Similar challenges have been observed in AUC-based learning \citep{yang2021learning} and contrastive learning \citep{chen2020simple}, where surrogate loss approximations and acceleration techniques have been proposed to alleviate computational burdens.


Our approach is to optimize a proxy objective of FI during model training. Recall that FI reflects the ``fragility'' of model performance in terms of the ranking error; accordingly, we optimize the ``fragility'' of model performance when evaluated using the conventional loss functions. We refer to models learned through this framework as FI-based models. Empirical results show that FI-based models significantly improve FI performance compared to models trained directly on the loss function, which we term ERM models.


% \rzcomment{Is it possible to provide some theoretical analysis on FI-based model to show that it improves the FI (defined with respect to ranking error) as compared to ERM model? I understand we have the empirical evidence in the experiment. Perhaps try to show that the fragility has been improved in terms of loss, which may help improve the fragility of the ranking error.}


Table \ref{tab:summary_reformulation} summarizes the reformulation results of our learning framework across various distance metrics and loss functions. Notably, we are the first to provide an exact convex reformulation for the multiclass cross-entropy loss (Theorem \ref{thm:cross_entropy_reformulation}) and the general hinge-type loss (Theorem \ref{thm:hinge_type_reformulation}) under the Wasserstein distance. We further instantiate these results in the context of label attack scenarios. We defer further generalization analysis in finite samples for the learned model to Appendix \ref{appe:finite_sample_guarantee} and the connection of our framework with DRO to Appendix \ref{appe:connection_dro}.


 
%So far, we have discussed establishing the FI as a performance metric based on the ranking error. A follow-up question is whether we can use the FI as the learning objective to train the classifier. Regarding the ranking error, this is challenging as the ranking error is based on pairwise comparison of the samples. Estimating the ranking error and its gradient is rather difficult for large-size data. Similar concern appears in AUC-based learning \citep{yang2021learning} and contrastive learning \citep{chen2020simple}, and some surrogate loss approximation and acceleration techniques are necessary to ease the computational burden. 
%
%Instead, like conventional model training based on loss minimization, we can consider FI based on the loss function. The loss of each sample also reflects the prediction quality of the classifier, which is similar to the ranking error. Meanwhile, the induced optimization problem by using FI on the loss is comparable to the conventional ERM problem, which also provides opportunities to correlate FI-based learning and ERM learning. 
%% Training directly based on the ranking error is challenging because the ranking error requires pairwise comparison and must lead to a quadratic complexity scaling with the sample size. Similar concerns also arise in AUC-based learning \citep{yang2021learning}.   
%We desire to establish a tractable training framework for various loss functions. Our results are summarized in Table \ref{tab:summary_reformulation}.
\begin{table}[htbp]
    \centering
    \begin{tabular}{c|c|c}
        \hline
        \textbf{Distance metric }                                              & \textbf{Support of feature}                        & \textbf{Convex reformulation results}         \\
        \hline
        KL-divergence                                                 & $\supp{\hat{\mathbb{P}}}$                               & Convex loss (Theorem \ref{thm:kl_reformulation})                  \\
        \hline
        1-Wasserstein distance                    & $\mathbb{R}^M$ & \makecell{  Cross-entropy loss (Theorem \ref{thm:cross_entropy_reformulation}) \\
        Hinge-type loss (Theorem \ref{thm:hinge_type_reformulation}) \\
        Lipschitz loss (Theorem \ref{thm:lipschitz_approx}) 
        } \\ 
        \hline
        \makecell{OT discrepancy with \\ convex transportation cost} & Convex set                      & Piecewise linear convex loss (Theorem \ref{thm:piecewise_reformulation})\\
        \hline    
    \end{tabular}
    \caption{Summary of the reformulation results for different distance metrics.}
    \label{tab:summary_reformulation}
    % \hspace{-10pt}
\end{table}
%We provide convex reformulations for various distance metrics. The two most popular loss functions, both the cross-entropy loss and the hinge loss, are covered in our framework. In particular, we are the first to provide an exact convex reformulation for multiclass cross-entropy loss (Theorem \ref{thm:cross_entropy_reformulation}) and the general hinge-type loss (Theorem \ref{thm:hinge_type_reformulation}) regarding the Wasserstein ambiguity. We also link our novel reformulation to label attack robustness. Moreover, we also provide a generalization guarantee and finite sample analysis for our reformulation. 

\subsection{Model Learning Framework}
We use vector $\BFx \in \mathcal{X}$ to denote the feature vector and scalar $y \in \mathcal{Y} = \{1, \dots, C\}$ to denote the label, where $C$ is the number of classes. 
Let  $\phi: \mathcal{X} \to \mathbb{R}^M$ be a pre-specified and known feature map. 
With a slight abuse of notation, we use the bold $\BFphi \in \mathit{\Phi} \subseteq \mathbb{R}^M$ to represent the transformed feature. 
% Throughout this section, we consider the joint distribution of the transformed feature and the label $(\BFphi, y)$. 
For the training dataset $\{(\BFphi_i, y_i)\}_{i=1}^N$, let $\phat$ represent the empirical joint distribution of the transformed features and their corresponding labels. The ERM loss is formulated as:
\begin{equation}
    \label{eq:loss_erm}
    L_{ERM}(\BFB) = \frac{1}{N} \sum_{i \in [N]} \ell(\BFB^T\phi(\BFx_i), y_i) + R(\BFB) = \mathbb{E}_{\phat}[\ell(\BFB^T\BFphi, y)] + R(\BFB)
\end{equation}
where $\BFB\in \mathbb{R}^{M \times C}$ is the parameter matrix to be learned,  and $R(\BFB)$ is a pre-specified convex nonnegative regularization term\endnote{Our learning framework accommodates a wide range of model classes, including linear models, sieve approximations, and last-layer neural network fine-tuning.}.
The parameter matrix $\BFB $ can be interpreted as a score matrix, where each column $\BFbeta_j$ determines the score for class $j$, calculated as $\BFbeta_j^T \BFphi, j \in [C]$.
Given a feature vector $\BFphi$, the corresponding prediction is determined by
$$
    y_{pred} = \arg \max_{y \in [C]} \BFbeta_y^T \BFphi.
$$
Our goal is to learn the parameter matrix $\BFB$.


Rather than minimizing the standard ERM loss in Equation \eqref{eq:loss_erm}, we focus on optimizing the \emph{fragility} of the learned model under distributional shifts. 
To this end, we adopt the RS framework and present our FI-based model training framework as follows.
\begin{equation}\label{eq:prob_general}
	\begin{aligned}
		\min_{k, \BFB} & \ k\\
		\text{s.t.}\  &\ \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^T\BFphi, y) \right] + R(\BFB)\leq \tau + k D (\mathbb{P},\hat{\mathbb{P}}), &\ \forall \mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}),\\
		& k\geq 0, \BFB \in \mathcal{B},
	\end{aligned}
\end{equation}
where $\mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ denotes the set of joint distributions for the feature $\BFphi \in \mathit{\Phi}$ and the label $y \in \mathcal{Y}$. The target $\tau$ is a constant that regulates the expected loss. The feasible region of the weight matrix $\BFB$ is convex and is denoted as $\mathcal{\BFB}$. 
% For completeness, We also establish a connection between  our formulation and conventional DRO, and develop a scheme to integrate RS and DRO in the Appendix \ref{appe:connection_dro}.
Similar to Lemma \ref{lemma:root_finding}, the optimal solution to Problem \eqref{eq:prob_general}, denoted as $k^*$, can be determined through a root-finding problem of a function $G(k)$, defined as 
\begin{equation*}
    G(k) := \min_{\BFB \in \mathcal{B}} \  \sup_{\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \left\{
        \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^T\BFphi,y \right)] - k D (\mathbb{P},\hat{\mathbb{P}})
        \right\} + R(\BFB) - \tau = 0,
\end{equation*}
where $ G(k)$ involves solving a  min-max problem. This problem will be reformulated analytically under specific distance metrics in subsequent sections.

\subsubsection*{Setting the target value $\tau$.} Next, we discuss the choice of the target $\tau$.
When the target $\tau$ is set too large, the problem \eqref{eq:prob_general} may result in a trivial solution where the optimal $k^* = 0$ corresponds to a naive classifier with $\BFB = \BFzero$. To avoid this, we impose the following assumption on the target $\tau$ to regulate the value of $k$:
\begin{assumption}
    \label{asmp:tau}
    (Nontriviality of $\tau$)
    There exist $\epsilon_1 > 0, \epsilon_2 > 0$ such that the target $\tau$ satisfies 
    $$
        \tau \leq \inf_{\BFB\in \mathcal{B}} \sup_{\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}), \ D (\mathbb{P},\hat{\mathbb{P}}) \leq \epsilon_2} \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^T\BFphi, y) \right] - \epsilon_1.
    $$
\end{assumption}

Assumption \ref{asmp:tau}  not only guides the choice of $\tau$ but also ensures a nonzero lower bound for $k^*$, as formalized in the following lemma.
\begin{lemma}
    \label{lemma:lower_bound_k}
    Under Assumption \ref{asmp:tau}, the optimal $k^*$ of Problem \eqref{eq:prob_general} satisfies
    % \qjcomment{where is the problem.} \cycomment{The ref is false, I modified.}
    $
        k^* \geq \frac{\epsilon_1}{\epsilon_2} > 0.
    $
\end{lemma}
 This nonzero lower bound is instrumental in the subsequent reformulation and analysis.
        

% Rather than minimizing the standard ERM loss in equation \eqref{eq:loss_erm}, we focus on optimizing the \emph{fragility} of the learned model with respect to   distributional shift in the feature $\phi$. 
% To achieve it, we adopt the  robust satisficing method \citep{long2023robust} and introduce our FI-based model training framework as follows.
     
% \begin{equation}\label{eq:prob_general}
% 	\begin{aligned}
% 		\min_{k, \BFB} & \ k\\
% 		\text{s.t.}\  &\ \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^T\BFphi, y) \right] + R(\BFB)\leq \tau + k D (\mathbb{P},\hat{\mathbb{P}}), &\ \forall \mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}),\\
% 		& k\geq 0, \BFB \in \mathcal{B},
% 	\end{aligned}
% \end{equation}
% where $\mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ is the set of joint distributions for feature $\BFphi \in \mathit{\Phi}$ and label $y \in \mathcal{Y}$. The target $\tau$ is a constant that regulates the expected loss. The feasible region of the weight matrix $\BFB$ is convex and denoted as $\mathcal{\BFB}$. For completeness, We also connect our formulation to conventional DRO and develop a scheme to incorporate RS and DRO in the appendix \ref{appe:connection_dro}.








% To ensure that the problem \eqref{eq:prob_general} is nontrivial, we have to rule out the case of $k^* = 0$, where $k^*$ denotes the optimal solution to problem \eqref{eq:prob_general}. When $k^* = 0$, it means that the target $\tau$ is too large to restrict the model's behavior. For example, if $\tau \geq \max_y\ell(\BFzero, y)$ and $R(\BFzero) = 0$, consider the solution $\BFB = \BFzero$, the constraint in problem \eqref{eq:prob_general} is always satisfied, so $k^* = 0$ as well. However, since all weights are zero, the induced classifier is nothing. Therefore, we consider the following assumption on the target $\tau$ to regulate the value of $k$.

% \begin{assumption}
%     \label{asmp:tau}
%     (Nontriviality of $\tau$)
%     There exist $\epsilon_1 > 0, \epsilon_2 > 0$ such that the target $\tau$ satisfies 
%     $$
%         \tau \leq \inf_{\BFB\in \mathcal{B}} \sup_{\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}), \ D (\mathbb{P},\hat{\mathbb{P}}) \leq \epsilon_2} \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^T\BFphi, y) \right] - \epsilon_1.
%     $$
% \end{assumption}
% Then, we can derive a nonzero lower bound of $k^*$ by the following lemma.
% \begin{lemma}
%     \label{lemma:lower_bound_k}
%     Under Assumption \ref{asmp:tau}, the optimal $k^*$ of Problem \eqref{eq:prob_general} satisfies
%     % \qjcomment{where is the problem.} \cycomment{The ref is false, I modified.}
%     $
%         k^* \geq \frac{\epsilon_1}{\epsilon_2} > 0.
%     $
% \end{lemma}
% As shown later, the nonzero lower bound of $k^*$ is necessary in the follow-up reformulation and analysis. Assumption \ref{asmp:tau} also suggests how to set $\tau$. Similar to Lemma \ref{lemma:root_finding}, $k^*$ can be determined by a root-finding problem of a function $G(k)$ defined as 
% \begin{equation*}
%     G(k) = \min_{\BFB \in \mathcal{B}} \  \sup_{\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \left\{
%         \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^T\BFphi,y \right)] - k D (\mathbb{P},\hat{\mathbb{P}})
%         \right\} + R(\BFB) - \tau = 0,
% \end{equation*}
% However, to solve the problem practically, we must address the min-max problem. We next consider the most commonly used distance metrics, the KL-divergence and the Wasserstein distance for further analysis. 


\subsection{Kullback-Leibler Divergence}
We now consider the KL-divergence as the distance metric, defined in Equation \eqref{eq:kl_def}, and set $D(\mathbb{P}, \hat{\mathbb{P}}) = D_{\mathrm{KL}} (\mathbb{P} || \phat)$. Since the empirical distribution $\hat{\mathbb{P}}$ is discrete with a finite sample size (i.e., its support $\mathrm{supp} (\hat{\mathbb{P}}) $ is finite), the condition $\hat{\mathbb{P}} \ll \mathbb{P}$ (absolute continuity) is equivalent to $\mathrm{supp} (\mathbb{P}) \subseteq \mathrm{supp} (\hat{\mathbb{P}})$. 
To regularize the space of $\mathbb{P}$, we  restrict its variation  to $\mathcal{P} (\mathrm{supp} (\hat{\mathbb{P}}))$. Then, the concrete formulation of the problem \eqref{eq:prob_general} under the KL-divergence becomes:
\begin{equation}\label{eq:prob_general_kl}
    \begin{aligned}
        \min_{k, \BFB} & \ k\\
        \text{s.t.}\  &\ \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^T\BFphi, y \right)] + R(\BFB)\leq \tau + k D_{\mathrm{KL}} (\mathbb{P} || \phat), &\ \forall \mathbb{P}\in \mathcal{P}(\mathrm{supp} (\hat{\mathbb{P}})),\\
        & k\geq 0, \BFB \in \mathcal{B}.
    \end{aligned}
\end{equation}

We provide a closed-form reformulation of the problem \eqref{eq:prob_general_kl} as follows. 
\begin{theorem}
    \label{thm:kl_reformulation}
    Under Assumption \ref{asmp:tau}, Problem \eqref{eq:prob_general_kl} is equivalent to:
    \begin{equation*}
        \label{eq:kl_reformulation}
        \begin{aligned}
            \min_{k \geq 0, \BFB \in \mathcal{B}} & \ k\\
            \text{s.t.} \hspace*{7pt} \  &\  k \ln \left( \mathbb{E}_{\phat}\left[ \exp \left(\frac{\ell \left(\BFB^T\BFphi, y \right)}{k}\right)\right] \right) + R(\BFB)  - \tau \leq 0.
        \end{aligned}
    \end{equation*}
    The problem is convex if the loss function $\ell(\BFB^T\BFphi, y)$ is convex in $\BFB$ for any $y\in\mathcal{Y}$. 
\end{theorem}
Theorem \ref{thm:kl_reformulation}  reveals a reweighing process for robust consideration under the KL divergence \citep{donsker1975asymptotic}.
Specifically, for a given input sample, the larger its loss, the greater its weight in the worst-case distribution. The model achieves robustness by prioritizing samples with higher loss values, effectively controlling the risk of misjudgment.
This approach is akin to the boosting scheme in ensemble learning, where the model improves generalization by focusing on samples harder to classify correctly. \citep{vapnik2013nature}. 
% Furthermore, the reformulation aligns closely with the tilted loss framework \citep{li2023tilted}.
Finally, when the loss function $\ell(\BFB^T\BFphi, y)$ is convex, such as hinge loss, the problem can be solved efficiently using the bisection algorithm in Appendix \ref{appe:fi} or other convex optimization methods.


\subsubsection*{Controlling FI under the hinge loss.}

% \rzcomment{Change it to standard binary classification with only vector parameter.} \cycomment{Done}

We  demonstrate that the FI-based training model \eqref{eq:prob_general_kl} under the KL divergence can effectively control FI, using the example of  a binary classification problem ($y = \{1, -1\}$) under the hinge loss:
$$
    \ell(\BFbeta^T\BFphi, y) = \max\{0, 1 - y \BFbeta^T \BFphi \}.
$$
Here, we abuse the notation and write the vector $\BFbeta = \BFbeta_+ - \BFbeta_-$, where $\BFbeta_+$ and $\BFbeta_-$ are the first and second column of $\BFB$. For a positive-negative sample pair $(\BFphi_+, \BFphi_-) $, the ranking error is calculated as
\begin{equation}
    \label{eq:ranking_error_hinge}
    \varepsilon(\BFB) = \BFbeta^T \BFphi_- - \BFbeta^T  \BFphi_+.
\end{equation}
Among $N$ samples, let $N_+$ and $N_-$ denote the numbers of positive and negative samples respectively. We then derive the following proposition.
\begin{proposition}
    \label{prop:kl_training_tail_bound}
    Let $k^*$ and $\BFB^*$ be the optimal solution to Problem \eqref{eq:prob_general_kl} given $\tau$ and $R(\cdot)$. Define $a = \max\{1, \frac{\ln N}{\ln N_+ + \ln N_-}\}$. For the ranking error $\varepsilon(\BFB^*)$ in Equation \eqref{eq:ranking_error_hinge} and its FI, we have
    \begin{gather*}
        \mathrm{FI}_{\mathrm{KL}}(\BFB^*; \tau - R(\BFB^*)) \leq a k^*, \\
        \phat(\varepsilon(\BFB^*) \geq \theta) \leq \exp\left(- \frac{ \theta - \tau + R(\BFB^*)}{a k^*}\right), \ \forall \theta \geq \tau - R(\BFB^*).
    \end{gather*} 
\end{proposition}
The above result demonstrates the close connection between the FI-based model and the FI upon the ranking error: even though Problem \eqref{eq:prob_general_kl} is not directly optimizing the FI upon the ranking error, the optimal fragility $k^*$ can also provide information for the FI and the tail risk of the ranking error. 




% We first consider the KL-divergence as the distance metric. Follow the definition of the KL-divergence in equation \eqref{eq:kl_def}, and let $D(\mathbb{P}, \hat{\mathbb{P}}) = D_{\mathrm{KL}} (\mathbb{P} || \phat)$. Since the empirical distribution $\hat{\mathbb{P}}$ is discrete, i.e., its support $\mathrm{supp} (\hat{\mathbb{P}}) $ is finite, the absolute continuity $\hat{\mathbb{P}} \ll \mathbb{P}  $ is equivalent to $\mathrm{supp} (\mathbb{P}) \subseteq \mathrm{supp} (\hat{\mathbb{P}})$. Hence, we restrict the variation of $\mathbb{P}$ to $\mathcal{P} (\mathrm{supp} (\hat{\mathbb{P}}))$. Then, the concrete problem \eqref{eq:prob_general} under the KL-divergence is
% \begin{equation}\label{eq:prob_general_kl}
%     \begin{aligned}
%         \min_{k, \BFB} & \ k\\
%         \text{s.t.}\  &\ \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^T\BFphi, y \right)] + R(\BFB)\leq \tau + k D_{\mathrm{KL}} (\mathbb{P} || \phat), &\ \forall \mathbb{P}\in \mathcal{P}(\mathrm{supp} (\hat{\mathbb{P}})),\\
%         & k\geq 0, \BFB \in \mathcal{B}.
%     \end{aligned}
% \end{equation}

% With the lower bound of $k$ given by Assumption \ref{asmp:tau}, we can apply the variational formula by \cite{donsker1975asymptotic} to reformulate our problem. 
% \begin{theorem}
%     \label{thm:kl_reformulation}
%     Under Assumption \ref{asmp:tau}, we have Problem \eqref{eq:prob_general_kl} is equivalent to 
%     \begin{equation*}
%         \label{eq:kl_reformulation}
%         \begin{aligned}
%             \min_{k \geq 0, \BFB \in \mathcal{B}} & \ k\\
%             \text{s.t.} \hspace*{7pt} \  &\  k \ln \left( \mathbb{E}_{\phat}\left[ \exp \left(\frac{\ell \left(\BFB^T\BFphi, y \right)}{k}\right)\right] \right) + R(\BFB)  - \tau \leq 0.
%         \end{aligned}
%     \end{equation*}
%     The problem is convex if the loss function $\ell(\BFB^T\BFphi, y)$ is convex in $\BFB$ for any $y\in\mathcal{Y}$. 
% \end{theorem}

% Theorem \ref{thm:kl_reformulation} provides a closed-form reformulation of problem \eqref{eq:prob_general_kl}. This reformulation also relates to the titled loss \citep{li2023tilted}. The minimization problem is convex if $\ell(\BFB^T\BFphi, y)$ is convex for any $y\in\mathcal{Y}$, which is not a strong requirement as a bunch of loss functions such as the hinge loss satisfies the convexity. Once the problem is convex, we can solve it by the bisection algorithm in Appendix \ref{appe:fi} or other convex optimization algorithms efficiently.

% Refering to the worst-case distribution mentioned in \cite{donsker1975asymptotic}, the KL-divergence-based model is equivalent to a reweighing process. Given an input sample, the larger its loss is, the larger the weight of this sample is in the worst-case distribution. The model achieves robustness by emphasizing the samples with larger loss values. This is consistent with the intuition of controlling the risk of confident misjudgment. Similar to the boosting scheme in ensemble learning, the model improves the generalization by focusing more on the hard samples  \citep{vapnik2013nature}.
% \rzcomment{Shall we provide an algorithm to delineate the reformulation under KL?} \qjcomment{Any tractable result for special case? e.g., polynomial solvable? or maybe you have discussed in Section 3.} \cycomment{interesting algorithm results is hard since the problem is convex. I mention that it is tractable by our bisection or other convex optimization algorithm. \cite{yan2024kl} focuses on KL-divergence but their algorithm is also the bisection like us. For this general formulation, I cannot figure out a sepcial case or structural results we can derive for solving $\BFB$ easily.}

% \cycomment{I add a new result to show that the optimal fragility $k^*$ of the problem \eqref{eq:prob_general_kl} can bound FI. This also enrich the subsection of KL-divergence. However, as I mentioned, I can only prove for some special cases like the hinge loss.}
% Moreover, we can show that the FI-based training model \eqref{eq:prob_general_kl} is consistent with controlling FI upon the ranking error. For example, consider a binary classification problem ($y \in \{-1, 1\}$) with the hinge loss 
% $$
%     \ell(\BFB^T\BFphi, y) = \max\{0, 1 + \max_{y' \neq y} (\BFbeta_{y'} - \BFbeta_y)^T \BFphi \}.
% $$
% For a positive-negative sample pair $(\BFphi_+, \BFphi_-) $, the ranking error can be calculated as
% \begin{equation}
%     \label{eq:ranking_error_hinge}
%     \varepsilon(\BFB) = (\BFbeta_+ - \BFbeta_-)^T \BFphi_- - (\BFbeta_+ - \BFbeta_-)^T  \BFphi_+.
% \end{equation}
% Among the $N$ samples, suppose the number of positive and negative samples is $N_+$ and $N_-$, respectively. Then, we can derive the following proposition.
% \begin{proposition}
%     \label{prop:kl_training_tail_bound}
%     Let $k^*$ and $\BFB^*$ be the optimal solution to problem \eqref{eq:prob_general_kl} given $\tau$ and $R(\cdot)$. Define $a = \max\{1, \frac{\ln N}{\ln N_+ + \ln N_-}\}$. For the ranking error $\varepsilon(\BFB^*)$ in equation \eqref{eq:ranking_error_hinge} and its FI, we have
%     \begin{gather*}
%         \mathrm{FI}_{\mathrm{KL}}(\BFB^*; \tau - R(\BFB^*)) \leq a k^*, \\
%         \phat(\varepsilon(\BFB^*) \geq \theta) \leq \exp\left(- \frac{ \theta - \tau + R(\BFB^*)}{a k^*}\right), \ \forall \theta \geq \tau - R(\BFB^*).
%     \end{gather*} 
% \end{proposition}
% Even though problem \eqref{eq:prob_general_kl} is not directly optimizing the FI upon the ranking error, the optimal fragility $k^*$ can also provide information for the FI and the tail risk of the ranking error. This example also demonstrates the close connection between the FI-based model and the FI upon the ranking error.


% Then, we bring more insights about the interplay between the target $\tau$ and the optimal solutions $\BFB^*$ and $k^*$ by a special case. Suppose we consider the linear scalar loss function, i.e. $\ell(\BFB^T\BFphi, y) = \BFbeta^T \BFphi$. Then, we have the following lemma capturing two different states of the optimal solution $\BFbeta^*$ and $k^*$. 
% \begin{lemma}
%     Define function $g(\BFu) = \frac{1}{N}\sum_{i=1}^N \mathrm{exp}\left(\BFu^T\BFphi_i\right)$ and its minimum $\BFu^*$. Then, there exists a critical point $\tau_c = \frac{\ln g(\BFu^*)}{\|\BFu^*\|_2}$ such that
%     \begin{enumerate}[(a)]
%         \item If $\tau_c \leq \tau \leq 0$, the optimal solution to \eqref{eq:prob_linear} is 
%         $$
%         \BFbeta^* = \frac{\tau}{\ln g(\BFu^*)} \BFu^*, k^* = \frac{\tau}{\ln g(\BFu^*)}
%         $$
%         \item If $\tau < \tau_c$, the constraint $\|\BFbeta \|_2 \leq 1$ is always binding and $k^* \geq \frac{\tau}{\ln g(\BFu^*)}$.
%     \end{enumerate}
%     \label{lemma:critical_point_kl_linear}
% \end{lemma}
% Lemma \ref{lemma:critical_point_kl_linear} shows that the optimal solution $\BFbeta^*$ and $k^*$ can be divided into two different states by the critical point $\tau_c$. This comes from the trade-off between achieving the target $\tau$ and minimizing the fragility $k$. Generally, the fluctuation caused by distributional shift is small when $\|\BFbeta\|$ is small. However, the ranking error is small when $\BFbeta$ is large because there always exists an optimal direction to minimize the ranking error. Increasing $\|\BFbeta\|$ along this direction will decrease the ranking error. For the two states proposed by Lemma \ref{lemma:critical_point_kl_linear}, we call the state $\tau \geq \tau_c$ as the ground state and the state $\tau < \tau_c$ as the excited state. In the ground state, $\tau$ is larger than the threshold, which means it is easy to satisfy the target requirement, so the model has more potential to diminish the fragility $k$. In the optimal solution of Lemma \ref{lemma:critical_point_kl_linear} (a), the direction of $\BFbeta^*$ is fixed, while its norm $\|\BFbeta^*\| \leq 1$ to keep $k$ in a low level. Hence, the optimal $k^*$ is linear in $\tau$, which is the ground level of this model. In the excited state, $\tau$ is smaller than the threshold, which implies that it is not that easy to satisfy the target requirement because the norm of $\|\BFbeta\|$ is restricted to 1. As the result of the trade-off, the model has to sacrifice the fragility $k$, which interprets that $k^*$ must be larger than the ground pattern. 

% This state transition is uniform in the robust satisfycing model as long as the trade-off between $\tau$ and $k$ exists. For large $\tau$, the model is in the ground state, and the optimal $k^*$ is at the possible lowest level. As $\tau$ decreases and reaching the target goes harder, the model has to excite the fragility $k$ to a higher level to achieve the target. 

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.3\linewidth]{kl_eg1.png}
%     \includegraphics[width=0.3\linewidth]{kl_eg2.png}
%     \includegraphics[width=0.3\linewidth]{kl_eg3.png}
%     \caption{The illustration of the optimal solution $\BFbeta^*$ and $k^*$ with respect to $\tau$. The left figure shows the trajectory of the optimal solution. The middle figure shows how $\theta$ changes with $\tau$. The right figure shows how the ranking error changes with $\tau$.}
% \end{figure}

\subsection{Wasserstein Distance}
\label{sec:wass_fi_training}
We now turn to the analytical reformation under the Wasserstein distance. The Wasserstein distance is defined via the optimal transport (OT) discrepancy between  two distributions $\mathbb{P}$ and $\hat{\mathbb{P}}$, expressed as:
\begin{equation}
    \label{eq:def_ot}
    D_c(\mathbb{P}, \hat{\mathbb{P}}) = \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \mathbb{E}_{\pi} [c(\BFphi, y, \hat{\BFphi}, \hat{y})],
\end{equation}
where $c(\BFphi, y, \hat{\BFphi}, \hat{y})$ denotes the OT cost, and
the set $\Pi(\mathbb{P}, \hat{\mathbb{P}}) = \{\pi \in \mathcal{P}((\mathit{\Phi} \times \mathcal{Y}) \times (\mathit{\Phi} \times \mathcal{Y})) | \pi_{(\BFphi, y)} = \mathbb{P}, \pi_{(\hat{\BFphi}, \hat{y})} = \hat{ \mathbb{P}}\}$, with $\pi_{(\BFphi, y)}$ and $\pi_{(\hat{\BFphi}, \hat{y})}$ being the marginal distributions of $\pi$ with respect to $(\BFphi, y)$ and $(\hat{\BFphi}, \hat{y})$, respectively. 

Unlike the KL-divergence, which requires the support of $\mathbb{P}$ to be a subset of the support of $\hat{\mathbb{P}}$, the Wasserstein ambiguity allows distributions to account for unseen samples.
However, this flexibility comes at the cost of increased optimization challenges. 
In particular, finding a convex reformulation for the Wasserstein distance is not straightforward. The difficulty arises from the nonconvexity introduced by the maximization of the convex conjugate of the cost function $c(\BFphi, y, \hat{\BFphi}, \hat{y})$. Similar challenges have been noted in DRO settings \citep{shafieezadeh2023new}\endnote{If the loss function is convex in the control parameters $\BFB$ but concave in the uncertain scenario $\BFphi$, the loss $\ell(\BFB^T\BFphi, y)$ admits convex reformulation; otherwise, a nonconvex robust counterpart is likely to emerge \citep{shafieezadeh2023new}.}.
This motivates us to focus primarily on the commonly used 1-Wasserstein distance, which features a linear convex conjugate:
\begin{equation}
    \label{eq:cost_1wass}
    c(\BFphi, y, \hat{\BFphi}, \hat{y}) = \|\BFphi - \hat{\BFphi}\| + \gamma \mathbb{I}(y \neq \hat{y}),
\end{equation}
where $\mathbb{I}$ is the indicator function.  We then  have the following reformulation result.


% In DRO, \cite{shafieezadeh2023new} gave a condition that the loss $\ell(\BFB^T\BFphi, y)$ admits convex reformulation if the loss function is convex in the control parameters $\BFB$ but concave in the uncertain scenario $\BFphi$. Otherwise, we will meet a nonconvex robust counterpart with a high chance. However, this criteria is too restrictive for our loss function $\ell(\BFB^T\BFphi, y)$. Considering the bilinear structure of $\BFB^T \BFphi$, if $\ell$ is convex in $\BFB$, $\ell$ must be convex in $\BFphi$. 
% Only when $\ell$ is linear, the conditions in \cite{shafieezadeh2023new} can be met. Therefore, it is not easy and obvious to find a convex reformulation for the Wasserstein distance. Notice that the nonconvexity stems from the maximization of the convex conjugate of $c(\BFphi, y, \hat{\BFphi}, \hat{y})$. This inspires us to consider some special cases of the cost function $c(\BFphi, y, \hat{\BFphi}, \hat{y})$ with linear convex conjugate. Therefore, we consider the 1-Wasserstein distance as     





\begin{lemma}
    \label{lemma:1wass_reformulation}
    Suppose the loss function $\ell(\BFu, y)$ is convex in $\BFu$ for every $y \in \mathcal{Y}$. Consider the Wasserstein distance in the
 equation \eqref{eq:def_ot} with cost $c(\BFphi, y, \hat{\BFphi}, \hat{y})$ specified in the  equation \eqref{eq:cost_1wass} and the support $\mathit{\Phi} = \mathbb{R}^M$. Problem \eqref{eq:prob_general} is equivalent to
    \begin{equation}
        \label{eq:1wass_reformulation}
        \begin{aligned}
            \min_{k\geq0, \BFB \in \mathcal{B}} & \ k \\
            \text{s.t.} \hspace*{10pt} & \frac{1}{N} \sum_{n \in [N]} \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - k \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} + R(\BFB) - \tau \leq 0,\\
            & \sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k.
        \end{aligned}
    \end{equation}
\end{lemma}
The reformulation result in Lemma \ref{lemma:1wass_reformulation} 
%is similar to Theorem 6.3 of \cite{mohajerin2018data},  as both reformulations
involves a convex maximization subproblem $\sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k$, which  is generally nonconvex.
As noted in \cite{rockafellar1970convex}, the optimal solution to such problems often lies at an extreme point of the feasible set, provided the function is not constant. Meanwhile, the Fenchel-Young inequality implies that the effective domain of a convex conjugate is also the set of its subgradients. These insights lead us to consider two possible reformulation paths: (i) for domains $\dom{\ell^{1*}}$ with a finite number of extreme points, the convex maximization can be reduced to finite constraints, resulting in a finite equivalent reformulation; and (ii) for Lipschitz-continuous loss functions, $\dom{\ell^{1*}}$ is bounded so the convex maximization may be tractable. We later conclude this section by discussing extensions to general OT cost functions. 


\subsubsection{Finite extreme points in $\dom{\ell^{1*}}$.}
This condition does not hold universally within the family of convex functions and is often associated with linearity, such as in the case of piecewise linear functions. However, two commonly used loss functions satisfy this condition: the cross-entropy loss and hinge-type loss. For these, we provide exact finite convex reformulation results below.



\paragraph{Cross-entropy loss.} The cross-entropy loss for the $n$-th sample is defined as
\begin{equation}
    \label{eq:cross_entropy_loss}
    \ell_{CE}(\BFB^T\hat{\BFphi}_n, \hat{y}_n) 
    % = - \ln \hat{p}_{n\hat{y}_n} 
    = \ln \left(\sum_{i \in [C]} \exp(\BFbeta_i^T \hat{\BFphi}_n)\right) - \BFe_{\hat{y}_n}^T \BFB^T \hat{\BFphi}_n,
\end{equation}
where $\BFe_{\hat{y}_n}$ is the one-hot vector of the label $\hat{y}_n$. Note that the cross-entropy loss is convex in $\BFbeta_j, j \in [C]$ due to the convexity of the log-sum-exp function. 
\begin{theorem}
    \label{thm:cross_entropy_reformulation}
    Supposing the loss function $\ell$ is the cross-entropy loss in Equation \eqref{eq:cross_entropy_loss}, the problem \eqref{eq:1wass_reformulation} can be approximated by the following problem
    \begin{equation}
        \label{eq:cross_entropy_reformulation}
        \begin{aligned}
            \min_{k\geq0, \BFB \in \mathcal{B}} & \ k \\
            \text{s.t.} \hspace*{10pt} & \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k (\|\hat{\BFphi}_n\| - \gamma)_+ + R(\BFB) - \tau \leq 0,\\
            & \|\BFbeta_i - \BFbeta_j\|_* \leq k, \ \forall i, j \in [C] \ \text{and}\ i < j.
        \end{aligned}
    \end{equation}
    Any solution of the problem \eqref{eq:cross_entropy_reformulation} is feasible for the problem \eqref{eq:1wass_reformulation}. When $\gamma \geq \max_{n\in[N]} \|\hat{\BFphi}_n\|$, the problem \eqref{eq:cross_entropy_reformulation} is exactly equivalent to the problem \eqref{eq:1wass_reformulation}.
    
\end{theorem}
% \cycomment{In terms of the insights, I mainly mention two points: 1. overcome the redundancy of the reference point in the cross-entropy parameterization. 2. the minimax criteria and link to the label-flipping attack.}

% We highlight that, unlike SOTA relaxed reformulation like \cite{chen2023distributionally}, 
To the best of our knowledge, we are the first to derive an exact reformulation for multi-class classification under cross-entropy loss by directly addressing the convex maximization in Lemma \ref{lemma:1wass_reformulation}, which stands in contrast to the state-of-the-art relaxed reformulations \citep{chen2023distributionally}.
Furthermore, the equivalence condition is not overly restrictive, as it can always be satisfied by appropriately scaling the feature map $\BFphi$;  in such cases, the term $k (\|\hat{\BFphi}_n\| - \gamma)_+ $ in the problem \eqref{eq:cross_entropy_reformulation}  vanishes, simplifying the reformulation further.


Our reformulation \eqref{eq:cross_entropy_reformulation} also yields several important implications. 
Firstly, the cross-entropy loss is invariant to arbitrary shifts of all weight vectors $\BFbeta_i$ by the same vector, since the constraints in the problem \eqref{eq:cross_entropy_reformulation} regulate only the pairwise weight differences $\|\BFbeta_i - \BFbeta_j\|_*$. Consequently, the classifier makes predictions by focusing on the relative scores between different classes, rather than their absolute values.


Moreover, due to the constraint $ \|\BFbeta_i - \BFbeta_j\|_* \leq k, \ \forall i, j \in [C] \ \text{and}\ i < j$, the problem \eqref{eq:cross_entropy_reformulation} effectively bounds the loss induced by misclassification between any two classes. This is particularly critical in defending against label-flipping attacks \citep{cina2023wild}, where an adversary introduces label noise to increase the loss.
Specifically, let $\hat{\BFy} \in \mathbb{R}^N$ denote the true label of the training samples, and let the adversary flip the labels to  $\BFy$ to maximize the loss. 
The number of flipped samples is given by $\Delta(\hat{\BFy}, \BFy) = \sum_{n\in[N]} \BFone(\hat{y}_n \neq y_n) $, and the cross-entropy loss under label $\BFy$ is denoted as $L_{CE}(\BFB; \BFy)$. When the label-flipping rate is at most $p$, the classifier's worst-case performance is tightly bounded by the optimal fragility $k^*$, as formalized in the following corollary.
\begin{corollary}
    \label{coro:flipping_label_attack}
    Let $k^*$ and $\BFB^*$ be the optimal solution of the problem \eqref{eq:cross_entropy_reformulation} upon the noised label $\BFy$, and $\bar{\Phi} = \max_{n\in[N]}{\|\hat{\BFphi}_n\|}$. Then, the worst-case performance under the label-flipping attack with rate $p$ is tightly bounded by
    $$
        \sup_{\{\BFy| \Delta(\hat{\BFy}, \BFy) \leq p N \}} \left\{L_{CE}(\BFB^*; \hat{\BFy}) - L_{CE}(\BFB^*; \BFy) \right\}\leq p \bar{\Phi} k^*.
    $$
\end{corollary} 
Corollary \ref{coro:flipping_label_attack} highlights that the optimal fragility $k^*$ serves as a critical indicator of robustness against label-flipping attacks, which will also be demonstrated later through experiments.


\paragraph{Hinge-type Loss.} 
%By using the name hinge-type loss, we refer to a series of loss functions that serve as a surrogate to the 0-1 loss based on the relative margin between classes. We follow the notation in \cite{glasmachers2016unified} and define the hinge-type loss as 
We follow \cite{glasmachers2016unified}  and define the family of hinge-type losses for multi-class classification as follows:
\begin{equation}
    \label{eq:hinge_type_loss}
    \ell_{hinge}(\BFB^T\BFphi, y) = \max_{y' \neq y} \rho((\BFbeta_{y} - \BFbeta_{y'})^T \BFphi),
\end{equation}
where $(\BFbeta_{y} - \BFbeta_{y'})^T \BFphi$ refers to the relative \emph{margin} between classes;  $\rho$ is a binary hinge-type loss, such as the hinge loss $\rho(u) = \max\{0, 1 - u\}$, the logistic loss $\rho(u) = \log(1 + e^{-u})$ and the smoothed hinge loss \citep{luo2021learning}. Under mild conditions, $\dom{\ell^{1*}}$ only contains finite extreme points. This allows us to derive the following reformulation.
\begin{theorem}
    \label{thm:hinge_type_reformulation}
    Consider the hinge-type loss function in equation \eqref{eq:hinge_type_loss}. If function $\rho$ is convex and subdifferentiable, and $\sup_{u\in\mathbb{R}} \partial \rho(u) = 0$ and $\inf_{u\in\mathbb{R}} \partial \rho(u) = - \theta$, the problem \eqref{eq:1wass_reformulation} can be approximated by the following problem
    \begin{equation}
        \label{eq:hinge_type_reformulation}
        \begin{aligned}
            \min_{k\geq0, \BFB \in \mathcal{B}} & \ k \\
            \text{s.t.} \hspace*{10pt} & \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k (2 \|\hat{\BFphi}_n\| - \gamma)_+ + R(\BFB) - \tau \leq 0,\\
            & \|\BFbeta_i - \BFbeta_j\|_* \leq \frac{k}{\theta}, \ \forall i, j \in [C] \ \text{and}\ i < j.
        \end{aligned}
    \end{equation}
    Any solution to the problem \eqref{eq:hinge_type_reformulation} is feasible for the problem \eqref{eq:1wass_reformulation}. When $\gamma \geq 2\max_{n\in[N]} \|\hat{\BFphi}_n\|$, the problem \eqref{eq:hinge_type_reformulation} is exactly equivalent to the problem \eqref{eq:1wass_reformulation}.
\end{theorem}

Theorem \ref{thm:hinge_type_reformulation} indicates that, the reformulation \eqref{eq:hinge_type_reformulation} remains valid as long as  the subgardient space of the loss function  is preserved, regardless of the modifications to  the loss function. For example, one may consider a smoothed version of hinge loss (originally non-smooth) to ensure differentiability;  this procedure of smoothness does not affect the reformulation \eqref{eq:hinge_type_reformulation} as long as the subgradient space remains unaffected. Conversely, for a general smooth loss function, a piecewise linear approximation may be applied, and the same conclusion holds provided the subgradient space is unchanged.

Moreover, the weight difference constraints, $\|\BFbeta_i - \BFbeta_j\|_* \leq \frac{k}{\theta}, \ \forall i, j \in [C] \ \text{and}\ i < j$,  appear in the reformulation, implying a similar robustness against the label-flipping attack holds for hinge-type loss, similar to the cross-entropy loss in Corollary \ref{coro:flipping_label_attack}.

\subsubsection{Lipschitz-continuous loss.}
We move on to Lipschitz-continuous loss functions. Formally, we consider functions satisfying the following assumption.
\begin{assumption}[Lipschitz-continuous Loss Functions]
    \label{asmp:lipschitz}
    (Lipschitz continuity) There exists $\omega_1, \omega_2 \geq 0$ such that the loss function $\ell(\BFu, y)$ satisfies\endnote{Notice that the vector norm $\|\cdot\|$ in Assumption \ref{asmp:lipschitz} is in the same order as the norm in $c(\BFphi, y, \hat{\BFphi}, \hat{y})$. }:
    \begin{align*}
        & |\ell(\BFu_1, y) - \ell(\BFu_2, y)| \leq \omega_1 \|\BFu_1 - \BFu_2\|, &\forall \BFu_1, \BFu_2 \in \dom{\ell^1}, y \in \mathcal{Y}, \\
        & |\ell(\BFu, y_1) - \ell(\BFu, y_2)| \leq \omega_2 \|\BFu\|, &\forall \BFu \in \dom{\ell^1}, y_1, y_2 \in \mathcal{Y}.
    \end{align*}
\end{assumption}
We consider matrix norm induced by this vector norm: $\|\BFB\| := \sup\{\|\BFB\BFzeta\| | \|\BFzeta\| = 1\}$. The following states the reformulation result for Lipschitz-continuous loss functions.
\begin{theorem}
    \label{thm:lipschitz_approx}
    Suppose Assumption \ref{asmp:lipschitz} holds, and $\ell(\BFu, y)$ is convex and subdifferentiable in $\BFu$ for any $y\in\mathcal{Y}$. Then the problem \eqref{eq:1wass_reformulation} can be approximated by the following problem
    \begin{equation}
        \label{eq:lipschitz_reformulation}
        \begin{aligned}
            \min_{k\geq0, \BFB \in \mathcal{B}} & \ k \\
            \text{s.t.} \hspace*{10pt} & \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k\left(\frac{\omega_2}{\omega_1}\|\hat{\BFphi}_n\| - \gamma\right)_+ + R(\BFB) - \tau \leq 0,\\
            &  \|\BFB\|_* \leq \frac{k}{\omega_1}, 
        \end{aligned}
    \end{equation}
    Any solution of the problem \eqref{eq:lipschitz_reformulation} is feasible for the problem \eqref{eq:1wass_reformulation}. Moreover, the problem \eqref{eq:lipschitz_reformulation} is exactly equivalent to the problem \eqref{eq:1wass_reformulation} when the following equivalence conditions hold:
    \begin{enumerate}[(a)]
        \item $\gamma \geq \max_{n\in[N]} \frac{\omega_2}{\omega_1}\|\hat{\BFphi}_n\|$,
        \item if for any $\BFv \in \mathcal{V} = \{\BFv \in \mathbb{R}^C| \|\BFv\|_* = 1\}$, there exist $\BFu \in \dom{\ell}$ and $y \in \mathcal{Y}$ such that $\BFv \in \frac{1}{\omega_1} \partial_u \ell(\BFu, y)$.
    \end{enumerate}
\end{theorem}

Although Theorem \ref{thm:lipschitz_approx} provides equivalence conditions for the approximation, verifying these conditions--particularly condition (b)--can be challenging, especially in high-dimensional settings. However, some common loss functions do satisfy this condition.
% The condition (b) implies , we expect our loss function to have certain symmetry. 
For example, the 2-norm regression loss function $\ell(\BFu, \BFy) = \|\BFu - \BFy\|_2$ is symmetric with respect to the rotation around $\BFy$; so its conjugate domain  $\dom{\ell^{1*}} = \{\BFzeta \in \mathbb{R}^C| \|\BFzeta\|_2 \leq 1\}$, ensuring that   condition (b) holds. 
% For classification, one may consider the one-hot encoding of the label $y$, so the same argument to the regression case can be applied.


We conclude this section by instantiating the 2-norm in Theorem \ref{thm:lipschitz_approx}.
The matrix 2-norm satisfies the inequality of $\|\BFB\|_2 \leq \|\BFB\|_F$, where $\|\BFB\|_F = \sqrt{\sum_{i \in [M]} \sum_{j \in [C]} B_{ij}^2}$ is the Frobenius norm. We therefore can derive a convex approximation of the problem \eqref{eq:lipschitz_reformulation} by replacing the matrix 2-norm with the Frobenius norm in Theorem \ref{thm:lipschitz_approx}.
Since Frobenius norm is widely used as a regularization term in neural network training \citep{tian2022comprehensive}, this result provides a theoretical foundation for its application in modern neural network optimization.



\subsubsection{Extending to general convex OT costs.}
We now extend our discussion to general convex OT costs:
$$
    c(\BFphi, y, \hat{\BFphi}, \hat{y}) = c_\phi(\BFphi, \hat{\BFphi}) + \gamma \mathbb{I}(y \neq \hat{y}).
$$ 
As with previous reformulation results, linearity is key to our analysis. Following the literature, we restrict our discussion to  \emph{piecewise linear convex loss functions} \citep{mohajerin2018data,sim2021new}, which include widely used loss functions such as the hinge loss. The following result provides an exact convex reformulation under this setting.  

% The particularity of the piecewise linear loss has also been investigated by DRO and RS literature like \cite{sim2021new}.

% We have discussed some reformulations upon $c(\BFphi, y, \hat{\BFphi}, \hat{y})$ in equation \eqref{eq:cost_1wass}. As mentioned, the norm cost in equation \eqref{eq:cost_1wass} works because its convex conjugate is linear. Linearity is the key to achieving finite reformulation. Therefore, for general cost function as
% $$
%     c(\BFphi, y, \hat{\BFphi}, \hat{y}) = c_\phi(\BFphi, \hat{\BFphi}) + \gamma \mathbb{I}(y \neq \hat{y}).
% $$ 
% We can also obtain the reformulation results by introducing linearity into the loss function $\ell(\BFB^T\BFphi, y)$. Analogous to \cite{mohajerin2018data}, we consider the piecewise linear convex loss function. Even though the piecewise linear convex loss function is limited regarding the whole space of convex functions, it is still sound as it contains one of the most popular loss functions, the hinge loss.
\begin{theorem}
    \label{thm:piecewise_reformulation}
    Suppose that the uncertainty set $\mathit{\Phi} \subseteq \mathbb{R}^M$ is convex and closed, and the cost function $c_\phi(\BFphi, \hat{\BFphi})$ is proper, convex in $\BFphi$ for every $\hat{\BFphi} \in \supp{\hat{\mathbb{P}}}$. The loss function $\ell$ is piecewise linear convex as $\ell(\BFu, y) = \max_{i\in[K_y]} \{\BFa_{yi}^T \BFu + b_{yi}\}$. Under Assumptions \ref{asmp:tau}, the problem \eqref{eq:prob_general} is equivalent to
    \begin{equation}
        \label{eq:piecewise_reformulation}
        \begin{aligned}
            \min_{k \geq 0, \BFB \in \mathcal{B}, \BFs, \BFv_{in}} & \ k \\
            \text{s.t.} \hspace*{20pt} & \frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0, \\
            & b_{y_ni} + \delta_\mathit{\Phi}^*(\BFv_{in}) + k c_{\phi}^{1*}((\BFB \BFa_{y_ni} - \BFv_{in})/k, \hat{\BFphi}_n ) -  k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \leq s_n, \\
            &\hspace{200pt} \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}, 
        \end{aligned}
    \end{equation}
    where the function $\delta_\mathit{\Phi}(\BFphi)$ represents the characteristic function of $\mathit{\Phi}$.
\end{theorem}
We provide concrete examples of piecewise linear loss functions to instantiate Theorem \ref{thm:piecewise_reformulation} in Appendix \ref{appe:fi_reformulation}, wherein we also discuss the connection to Lemma \ref{lemma:1wass_reformulation} and other reformulations.


% \paragraph{Example of Hinge Loss.}
% We follow the notation in \cite{glasmachers2016unified} to define the hinge loss as 
% \begin{equation}
%     \label{eq:hinge_type_loss}
%     \ell(\BFB^T\BFphi, y) = \Delta( \ell_{hinge}(\mu(\BFB^T\BFphi, y))).
% \end{equation}
% The function $\mu(\BFB^T\BFphi, y) \in \mathbb{R}^C$ is the margin function, such as the relative margin $\mu(\BFB^T\BFphi, y)_c = \BFbeta_{c}^T \BFphi - \BFbeta_{y}^T \BFphi$.
% % or the direct margin function 
% % $\mu(\BFB^T\BFphi, y)_c = \begin{cases}
% %     \BFbeta_{y}^T \BFphi, & \text{if} \ c = y, \\
% %     - \BFbeta_{c}^T \BFphi, & \text{if} \ c \neq y.
% % \end{cases}
% % $
% The function $\ell_{hinge}(\mu) $ is an element-wise hinge loss as 
% $\ell_{hinge}(\mu_i) = \max\{0, 1 - \mu_i\}$. The function $\Delta(\cdot)$ is the aggregation function, aggregating the hinge loss over all classes. There are two main choices of $\Delta(\cdot)$: the max function $\Delta(\BFx) = \max_{i \in [C]} x_i$ and the sum function $\Delta(\BFx) = \sum_{i \in [C]} x_i$. Notice that, for all the mentioned cases, the final loss function $\ell(\BFB^T\BFphi, y)$ in equation \eqref{eq:hinge_type_loss} is piecewise linear and convex. For example, if we choose the max aggregation and relative margin function, our loss can be reformulated as
% $$
%     \ell(\BFB^T\BFphi, y) = \max_{y' \neq y} \{ \max\{0, 1 + \BFbeta_{y'}^T \BFphi - \BFbeta_{y}^T \BFphi\}\} =  \max\{0, 1 + \max_{y' \neq y} \{ (\BFbeta_{y'} - \BFbeta_{y})^T \BFphi\}\},
% $$
% which is a piecewise linear convex function with $C$ pieces. Therefore, we can apply Theorem \ref{thm:piecewise_reformulation} to obtain a finite convex reformulation. 

\section{Numerical Experiments}
In this section, we present numerical experiments to evaluate the effectiveness of our FI metric and the FI-based training model, using both synthetic and real datasets. For synthetic data, we demonstrate that our model not only outperforms conventional ERM (Problem \eqref{eq:loss_erm}) but also provides insights into how FI affects the model training and evaluation process. For real data, we further validate the superiority of our FI-based model over ERM, highlighting its practical robustness.
% \rzcomment{may explain what is ERM} \cycomment{The word ERM has been used several times before. I also give a brief explanation here.}

\subsection{Synthetic data}
We begin with synthetic data to demonstrate that the FI-based model achieves better generalization and robustness compared to the conventional ERM model. We also explore how various factors, including the target parameter $\tau$, sample size, and train-test distributional differences, influence the model performance.

\subsubsection{Setup.} 

Consider a binary classification task with the label-flipping attack.
The labels for the two classes are $-1$ and $1$, with an equal number of samples in each class. 
The raw features $\BFx_+, \BFx_- \in \mathbb{R}^2$ are sampled from two Gaussian distributions with an appropriate overlap.  These features are then transformed using a polynomial feature map; for instance, the quadratic feature map is defined as:
$
    \phi_{poly-2}((x_1, x_2)^T) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2)^T.
$
The attack is introduced by flipping the label of a sample with a certain probability $p_{flip}$ after the sample is generated. The attacked data are used as the training set, while the testing set remains unaffected by label flipping. The probability $p_{flip}$ represents the degree of distributional shift between the training and testing datasets.

We use the ERM model as the benchmark, employing the hinge loss defined in the equation \eqref{eq:hinge_type_loss}. Given the connection between hinge loss and support vector machines (SVM), the benchmark model can be interpreted as an SVM with a polynomial kernel.
In comparison, we evaluate our FI-based models using the same hinge loss as the ERM model. One FI-based model is implemented under the KL-divergence framework from Theorem \ref{thm:kl_reformulation}, while the other is based on the 2-norm Wasserstein distance as described in Theorem \ref{thm:hinge_type_reformulation}.
The regularization term $R(\BFB)$ is determined via cross-validation. We consider both L1 regularizer $R_1(\BFB) = \alpha \sum_{ij} |B_{ij}| $ and L2 regularizer $R_2(\BFB) = \alpha \sum_{ij} B_{ij}^2$, with $\alpha \in \{0.001, 0.01, 0.1, 1\}$. 

When training the ERM model, we select the best regularizer through 5-fold cross-validation. For a fair comparison, the two FI-based models adopt the same regularizer as the ERM model.
The target $\tau$ in the FI-based models is determined by scaling the ERM training loss $\hat{L}_{ERM}$. Specifically, we set $\tau = \lambda \hat{L}_{ERM} $, where $\lambda \in \{1.1, 1.2\}$ is referred to as the target ratio. As discussed in Theorem \ref{thm:fi_properties}, the target ratio $\lambda$ governs the tradeoff between empirical performance and robustness. 
We repeat the experiment 300 times, each time generating different training and testing sets. The training sample size varies, while the testing sample size is fixed at 2,000. Model performance is evaluated on the testing set by averaging accuracy, AUC, and FI over the 300 repetitions.


\subsubsection{Results.} 

\paragraph{Effect of sample size.} 
We begin by examining the effect of training sample size, which we vary from 50 to 400. We consider a no-distribution-shift setting where the label-flipping rate $p_{flip}=0$ to eliminate the label attack effect.
Even without distributional shifts, robustness remains relevant, particularly for small sample sizes. Figure \ref{fig:synthetic_sample_size} presents our results, where larger values indicate better performance for accuracy and AUC, while smaller values indicate better performance for FI.
The Wasserstein model consistently outperforms the ERM model across all metrics. The KL-divergence model performs similarly to the ERM model in terms of accuracy and AUC but achieves better FI. Notably, we observe that the accuracy and AUC gaps between the Wasserstein and ERM models diminish as the training sample size increases; this is because larger training datasets sufficiently capture the information needed for model training, reducing the need for robustness considerations.




\paragraph{Effect of distribution shifts.} We fix the sample size at 50 and vary the label-flipping rate $p_{flip}$ from 0 to 0.4, where $p_{flip}$ reflects the degree of distributional shift. Note that when $p_{flip} = 0.4$, the training set becomes nearly random. 
Figure \ref{fig:synthetic_flip_rate} shows the performance metrics across different label-flipping rates. Both FI-based models outperform the ERM model in nearly all three metrics. Notably, for AUC, the gap between the FI-based models and the ERM model widens as $p_{flip}$ increases, highlighting the robustness advantage of the FI-based models.
\begin{figure}[htbp]
    \centering
    \subfloat[Accuracy]{\includegraphics[width=0.34\textwidth]{synthetic_2_size_acc_main.pdf}}
    \subfloat[AUC]{\includegraphics[width=0.34\textwidth]{synthetic_2_size_auc_main.pdf}}
    \subfloat[FI]{\includegraphics[width=0.34\textwidth]{synthetic_2_size_fi_main.pdf}}
    \caption{The relationship between the training sample size and the accuracy, AUC, and FI when $p_{flip} = 0$. The colors and line styles represent different models and parameters. The values $1.1$ and $1.2$ represent the target ratio $\lambda$ used in each model. The error bands are calculated by 95\% confidence interval.}
    \label{fig:synthetic_sample_size}
\end{figure}
\begin{figure}[htbp]
    \centering
    \subfloat[Accuracy]{\includegraphics[width=0.34\textwidth]{synthetic_2_flip_acc_main.pdf}}
    \subfloat[AUC]{\includegraphics[width=0.34\textwidth]{synthetic_2_flip_auc_main.pdf}}
    \subfloat[FI]{\includegraphics[width=0.34\textwidth]{synthetic_2_flip_fi_main.pdf}}
    \caption{The relationship between the label-flipping rate $p_{flip}$ and the accuracy, AUC, and FI when the sample size is 50. The colors and line styles represent different models. The values $1.1$ and $1.2$ represent the target ratio $\lambda$ used in each model. The error bands are calculated by 95\% confidence interval.}
    \label{fig:synthetic_flip_rate}
\end{figure}


\paragraph{Effect of target ratio $\lambda$.} We now look into the effect of target ratio $\lambda$ on model performance.  Figure \ref{fig:synthetic_sample_size} shows that the Wasserstein model performs better when $\lambda=1.2$ compared to $\lambda=1.1$. This is because a larger $\lambda$ allows more budget to minimize FI, enhancing robustness against small sample sizes. However, this does not imply that $\lambda$ should always be maximized. Under distributional shifts, as shown in Figure \ref{fig:synthetic_flip_rate}, the Wasserstein model with $\lambda = 1.2$ exhibits worse accuracy and AUC compared to $\lambda = 1.1$ when $p_{flip}$ is large, implying that increasing $\lambda$ is not always beneficial. 

The tradeoff between overfitting and underfitting explains this behavior. When $p_{flip} > 0$, the ERM model tends to overfit the noisy training data. The Wasserstein model mitigates overfitting by setting a larger target value $\tau$ ($\lambda>1$) relative to $\hat{L}_{ERM}$. However, if $\lambda$ becomes too large, the Wasserstein model may underfit, resulting in high bias. This excessive conservatism can degrade performance. Careful problem-specific tuning of the target ratio $\lambda$ is essential to balance overfitting and underfitting.

% Based on this analysis, we demonstrate that the FI-based model can achieve better generalization and robustness, particularly when there is a significant train-test difference. We also bring insights into how to select the target ratio $\lambda$. In addition, we replicate the results in a multi-classification context in the Appendix \ref{appe:experiment} and the findings are consistent with the binary case. 

\subsection{Real data}

We now conduct experiments with real data to demonstrate the effectiveness of our FI-based model. The datasets are sourced from Kaggle and the UCI Machine Learning Repository \citep{ref_Dua}, with applications ranging from medical diagnosis to credit approval.
A key distinction between real and synthetic data lies in the complexity of real-world features. Real data often have high-dimensional features, including both numeric and categorical variables. This complexity poses challenges in defining the ambiguity set and may lead to overly conservative results by accounting for unrealistic feature shifts. Moreover, feature selection plays a crucial role in determining model performance, which further complicates the training process.

\subsubsection{Setup.}
We present the results of the heart failure prediction dataset \citep{fedesoriano2021heart} and defer the results for other datasets to the Appendix \ref{appe:experiment}. Heart failure diagnosis is a safety-critical task with a strong emphasis on risk control. The goal is to predict whether a patient has heart disease based on attributes such as age, sex, blood pressure, and other health indicators.
The raw dataset consists of 918 samples, with a positive class ratio of 0.55. We apply one-hot encoding to transform categorical features into numerical ones, resulting in a final input feature dimension of 20.

We formulate a binary classification problem for the heart failure dataset, partitioning it into training and testing sets with a 
50\% split for each. We evaluate the same models as in the synthetic data experiments: the ERM model, the KL-divergence FI-based model, and the Wasserstein FI-based model. 
The regularization term $R(\BFB)$ is selected by cross-validation, as in the synthetic data setup. 
For the KL-divergence and Wasserstein FI-based models,
the target ratio $\lambda$ is selected from $\{1.03, 1.07, 1.1\}$ via a 5-fold cross-validation for the KL-divergence model and the Wasserstein model, respectively. The experiment is repeated 300 times, with the dataset randomly partitioned into training and testing sets for each run.

% We also discuss the results under distributional shift settings induced by label-flipping.

\subsubsection{Results.}
Figure \ref{fig:heartattack} presents model performances evaluated at accuracy, AUC, and FI respectively. One potential concern is that the linear model setups may not capture enough complexity of real-world data. To address this, we compare the predictive performance of our models with the XGBoost model, known for its high predictive power on real datasets \citep{chen2016xgboost}.
Under the same experimental settings with $p_{flip} = 0$, the XGBoost model achieves an accuracy of 0.861 and an AUC of 0.920, which is comparable to the performance of our models and confirms the validity of the results in Figure \ref{fig:heartattack}.
\begin{figure}[htbp]
    \centering
    \subfloat[Accuracy]{\includegraphics[width=0.34\textwidth]{heartattack_flip_acc.pdf}}
    \subfloat[AUC]{\includegraphics[width=0.34\textwidth]{heartattack_flip_auc.pdf}}
    \subfloat[FI]{\includegraphics[width=0.34\textwidth]{heartattack_flip_fi.pdf}}
    \caption{The results of the average accuracy, AUC, and FI on the heart failure prediction dataset. The error bands are calculated by $95\%$ confidence intervals.
    }
    \label{fig:heartattack}
\end{figure}

To better visualize the risk of misjudgment, we further examine the ranking error and prediction probabilities of the three models, using Platt scaling for probability calibration \citep{platt1999probabilistic}. As an example, we focus on the case where $p_{flip} = 0$, in which the accuracy and AUC of the three models are very similar. The results are presented in Figure \ref{fig:heartattack_error_confidence}.
\begin{figure}[htbp]
    \centering
    \subfloat[Ranking error]{\includegraphics[width=0.34\textwidth]{heart_attack_ranking_error_proba.pdf}}
    \subfloat[Confidence of false predictions]{\includegraphics[width=0.34\textwidth]{heart_attack_confidence_wrong.pdf}}
    \subfloat[Confidence of true predictions]{\includegraphics[width=0.34\textwidth]{heart_attack_confidence_correct.pdf}}
    \caption{The ranking error and classifiers' estimated probability of the three models on the heart failure prediction dataset when $p_{flip} = 0$.}
    \label{fig:heartattack_error_confidence}
\end{figure}
Figure \ref{fig:heartattack_error_confidence}(a) demonstrates that the tail risk of the ranking error distribution increases in the following order: the Wasserstein model, the KL-divergence model, and the ERM model. Figures \ref{fig:heartattack_error_confidence}(b) and (c) reveal that the Wasserstein model exhibits the rightmost peak for true predictions and the leftmost peak for false predictions, indicating better separation. In contrast, the ERM model displays the opposite pattern, with the KL-divergence model falling in between.
Overall, Figure \ref{fig:heartattack_error_confidence} highlights that the Wasserstein model effectively controls the risk of false predictions. This aligns with the FI results in Figure \ref{fig:heartattack}(c), further validating the effectiveness of FI in capturing the risk of misjudgment.


% In summary, the FI-based model can achieve better generalization and robustness than the ERM model in real data. The Wasserstein model confronts more challenges because the Wasserstein ambiguity may be too conservative by taking some unrealistic feature shift into account. Therefore, techniques in classification, such as feature encoding and normalization, can also help enhance the performance. 

\section{FI-based Deep Neural Network Training}
We extend our FI-based model to neural networks, demonstrating the potential of our framework for deep learning. Unlike the linear model considered in the problem \eqref{eq:prob_general}, incorporating FI into a network structure is not straightforward. The primary challenge lies in the backpropagation and optimization process. Most neural network training processes rely on stochastic gradient descent (SGD) and its extensions, which are designed primarily for optimizing constraint-free loss functions. In contrast, our FI formulations are constrained optimization problems. While constrained SGD techniques can be employed, their practical convergence and stability are often questionable due to the inherent complexities of neural networks, such as non-convexity.

To address this issue, we propose an approximate regularization scheme to incorporate FI into the neural network loss optimization, leveraging the Lagrangian multiplier method. Consider the multi-classification setup with label $y \in [C]$ as before, and a neural network whose last layer is fully connected. Then, the feature map $\phi$ can be interpreted as the neural network excluding the last layer, and $\BFB$ corresponds to the weights of the last layer. Considering the cross-entropy reformulation in Theorem \ref{thm:cross_entropy_reformulation} under the equivalence condition, its Lagrangian is:
$$
    \mathcal{L}(\BFB) = k + \lambda_0 (L_{ERM}(\BFB) - \tau) + \sum_{i,j\in[C], i>j} \lambda_{ij} \left(\|\BFbeta_{i} - \BFbeta_{j}\|_* - k\right),
$$
where $L_{ERM}(\BFB)$ is the empirical loss defined in the equation \eqref{eq:loss_erm}, and $\lambda_0$ and $\lambda_{ij}$ are the Lagrangian multipliers; recall that $\BFbeta_i$ denotes the weights to obtain the score for the $i$-th class. Inspired by the augmented Lagrangian method, we replace function $\|\BFbeta_{i} - \BFbeta_{j}\|_* - k$ with $(\|\BFbeta_{i} - \BFbeta_{j}\|_* - k)_+^2$ to enhance its convexity. %{\color{red}QJ: Is the value similar, or difference negligible in the optimal case?} \cycomment{I would say this is only an approximation inspired by augmented Lagrangian. In optimal case, they are the same as the constraint $\|\BFbeta_{i} - \BFbeta_{j}\|_* \leq k$ must hold. However, for neural networks, $L_{ERM}$ is non-convex, so the global optimality is not achievable.} 
We further replace the original multipliers $\lambda_{ij}$ by another predetermined large hyperparameter $\alpha$ to penalize the constraint violation. Dividing the whole formula by $\lambda_0$, we have 
\begin{equation*}
    L_{ERM}(\BFB) - \tau + \frac{1}{\lambda_0} \left(k + \frac{\alpha}{2} \sum_{i,j\in[C], i>j} (\|\BFbeta_{i} - \BFbeta_{j}\|_* - k)_+^2 \right).
\end{equation*}
%{\color{red}QJ: why half in the formula?} \cycomment{This follows from the convention in augmented Lagrangian method. We can also remove the half.}
Note that the parameter $\tau$ does not affect optimization, so we can omit it and obtain our final training objective as 
\begin{equation}
    \label{eq:fi_def_loss_nn}
    L_{ERM}(\BFB) + \underbrace{\frac{1}{\lambda_0} \left(k + \frac{\alpha}{2} \sum_{i,j\in[C], i>j}(\|\BFbeta_{i} - \BFbeta_{j}\|_* - k)_+^2 \right)}_{\text{FI-inducing regularizer}}.
\end{equation}

In the FI-inducing regularizer, there are two hyperparameters $\lambda_0$ and $\alpha$. Although the target $\tau$ is omitted, its role in controlling robustness is inherited by the multiplier $\lambda_0$. According to the trade-off between target $\tau$ and fragility $k$, a larger $\tau$ implies a smaller $k$ in solution, which is equivalent to a smaller $\lambda_0$ is adopted in equation \eqref{eq:fi_def_loss_nn}.
% The smaller $\lambda_0$ is, the more the fragility $k$ contributes to the objective, implying a larger target $\tau$. 
The key advantage of this augmented Lagrangian reformulation is its improved convergence properties when optimized using SGD. Moreover, our reformulation in function \eqref{eq:fi_def_loss_nn} remains compatible with any additional regularization $R(\BFB)$ already included in the loss $L_{ERM}(\BFB)$. 



\subsection{Image Diagnosis with FI-based ResNet}
% \rzcomment{should this be a standalone section as extension to FI-based neural nets? For that, we describe how to solve it at a high level, like the Eq.\eqref{eq:fi_def_loss_nn}; then we talk about image diagnosis as an application.} \cycomment{I moved the math to the alone section and only keep the numerical here.}

We apply the FI-induced neural network training objective \eqref{eq:fi_def_loss_nn} to MedMNIST, a large-scale medical image diagnosis dataset \citep{medmnistv1,medmnistv2}. MedMNIST is an MNIST-like collection of standardized medical images, with data scales ranging from 1,000 to 100,000 for 2D-image classification tasks and diverse objectives, such as predicting survival outcomes for colorectal cancer and diagnosing pneumonia. Each sample is a 
$28 \times 28$  biomedical image, presented in either grayscale or RGB.
Given the high cost of misclassification in medical diagnosis, robustness and sensitivity to risks and outliers are critical. The FI-based model, with its emphasis on risk control and robustness, is particularly well-suited for these challenges in medical image diagnosis.



We adopt the ResNet architecture as the backbone for neural network training \citep{he2016deep}, specifically using the ResNet-18 model for the 2D-image classification task. Following \cite{medmnistv2}, we employ the cross-entropy loss and the Adam optimizer \citep{kingma2014adam}, with a batch size of 128. We evaluate both the ERM model and the Wasserstein FI-based model.
For regularization, we implement conventional weight decay for all parameters in both models. The Wasserstein FI-based model further incorporates an FI-induced regularizer from objective function \eqref{eq:fi_def_loss_nn}, which applies only to the weight matrix of the last fully connected layer. The regularization coefficient is determined via Bayesian optimization. We also apply a conventional early stopping strategy and an adaptive learning rate scheduler.
Further training details, such as hyperparameters, are provided in Appendix \ref{appe:experiment}. Since the training, validation, and testing sets are predefined in the MedMNIST dataset, we report the average accuracy, AUC, cross-entropy, and FI on the testing set over 10 repetitions. The results are summarized in Table \ref{tab:medmnist}\endnote{As a sanity check, the AUC and accuracy values in Table \ref{tab:medmnist} are comparable to the benchmarks reported in \cite{medmnistv2}.}. 

% backup of the first version of the table
% \begin{table}[htbp]
%     \small
%     \begin{tabular}{cccccccccc}
%     \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset(\# classes)}}}  & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\#Train/\#Val/\#Test}}} & \multicolumn{4}{c}{\textbf{ERM}}                                                                      & \multicolumn{4}{c}{\textbf{Wasserstein}}                                                              \\
%     \multicolumn{1}{c}{}                                  & \multicolumn{1}{c}{}                                               & \multicolumn{1}{c}{\textbf{ACC}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{CE}} & \multicolumn{1}{c}{\textbf{FI}} & \multicolumn{1}{c}{\textbf{ACC}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{CE}} & \multicolumn{1}{c}{\textbf{FI}} \\
%     \hline

%     PathMNIST(9)  & 89,996 / 10,004 / 7,180 & 0.7486 &\textbf{ 0.9543} & 1.0024 & 0.6903 & \textbf{0.7584} & 0.9541 & \textbf{0.9207} & \textbf{0.5688} \\
%     DermaMNIST(7) & 7,007 / 1,003 / 2,005 & 0.7052 & \textbf{0.8828} & \textbf{0.8804} & 0.4368 & \textbf{0.7077} & 0.8778 & 0.8860 & \textbf{0.3332} \\
%     OCTMNIST(4) & 97,477 / 10,832 / 1,000 & 0.6520 & 0.9034 & 1.3705 & 0.6140 & \textbf{0.7050} & \textbf{0.9056} & \textbf{1.1257} &  \textbf{0.4917} \\
%     PneumoniaMNIST(2) & 4,708 / 524 / 624 & \textbf{0.8494} & \textbf{0.9365} & 0.6555 & 0.3800 & 0.8365 & 0.9350 & \textbf{0.5178} & \textbf{0.2780} \\
%     RetinaMNIST(5) & 1,080 / 120 / 400 & 0.4775 & 0.6709 & 1.5055 & 0.2974 & \textbf{0.5075} & 
%     \textbf{0.6869} & \textbf{1.3219} & \textbf{0.2159} \\
%     BreastMNIST(2) & 546 / 78 / 156 & 0.8526 & 0.8711 & 0.4721 & 0.2882 & \textbf{0.8654} & \textbf{0.8858} & \textbf{0.3953} & \textbf{0.2493} \\
%     BloodMNIST(8) & 11,959 / 1,712 / 3,421 & \textbf{0.8980} & 0.9862 & 0.4173 & 0.6026 & 0.8974 & \textbf{0.9877} & \textbf{0.3992} & \textbf{0.5221} \\
%     TissueMNIST(8) & 165,466 / 23,640 / 47,280 & \textbf{0.5726} & \textbf{0.8686} & \textbf{1.1850} & 0.5612 & 0.5475 & 0.8648 & 1.2788 & \textbf{0.5133} \\
%     OrganAMNIST(11) & 34,561 / 6,491 / 17,778 & 0.8524 & 0.9837 & 0.5879 & 0.6759 & \textbf{0.8712} & \textbf{0.9868} & \textbf{0.4394} & \textbf{0.4165} \\ 
%     OrganCMNIST(11) & 12,975 / 2,392 / 8,216 & 0.8627 & 0.9826 & 0.5559 & 0.6715 & \textbf{0.8673} & \textbf{0.9859} & \textbf{0.5066} & \textbf{0.6292} \\
%     OrganSMNIST(11) & 13,932 / 2,452 / 8,827 & 0.6920 & 0.9536 & 1.0417 & 0.6871 & \textbf{0.6975} & \textbf{0.9548} & \textbf{1.0489} & \textbf{0.6748} \\
%     \hline

%     \end{tabular}
%     \caption{The results of the ResNet-18 model on the MedMNIST dataset. For the performance metrics, ACC means accuracy, and CE means the cross entropy. The better results between the two models are highlighted in bold.}
%     \label{tab:medmnist}
% \end{table}

\begin{table}[htbp]
    \small
    \begin{tabular}{cccccccccc}
    \hline
    \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Dataset(\# classes)}}}  & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\#Train/\#Val/\#Test}}} & \multicolumn{4}{c}{\textbf{ERM}}                                                                      & \multicolumn{4}{c}{\textbf{Wasserstein FI}}                                                              \\
    \multicolumn{1}{c}{}                                  & \multicolumn{1}{c}{}                                               & \multicolumn{1}{c}{\textbf{ACC}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{CE}} & \multicolumn{1}{c}{\textbf{FI}} & \multicolumn{1}{c}{\textbf{ACC}} & \multicolumn{1}{c}{\textbf{AUC}} & \multicolumn{1}{c}{\textbf{CE}} & \multicolumn{1}{c}{\textbf{FI}} \\
    \hline

    BloodMNIST(8) & 11,959/1,712/3,421 & 0.9450 & 0.9963 & 0.1806 & 0.5912 & 0.9494 & 0.9961 & \textbf{0.1705} & \textbf{0.4529} \\
    BreastMNIST(2) & 546/78/156 & 0.8346 & 0.8605 & 0.6134 & 0.4227 & \textbf{0.8468} & 0.8667 & \textbf{0.3956} & \textbf{0.2000} \\
    DermaMNIST(7) & 7,007/1,003/2,005 & 0.7470 & \textbf{0.9123} & \textbf{0.7040} & 0.3270 & 0.7443 & 0.8936 & 0.7400 & \textbf{0.2428} \\
    OrganAMNIST(11) & 34,561/6,491/17,778 & 0.9100 & 0.9928 & \textbf{0.3852} & 0.5217 & 0.9028 & 0.9903 & 0.3965 & \textbf{0.4207} \\
    OrganCMNIST(11) & 12,975/2,392/8,216 & 0.8939 & 0.9900 & 0.4545 & 0.6712 & 0.8975 & 0.9860 & \textbf{0.4246} & \textbf{0.3808} \\
    OrganSMNIST(11) & 13,932/2,452/8,827 & 0.7626 & 0.9665 & 0.8373 & 0.5021 & 0.7617 & 0.9605 & \textbf{0.7796} & \textbf{0.3205} \\
    PathMNIST(9) & 89,996/10,004/7,180 & 0.8469 & 0.9686 & 0.6597 & 0.5293 & 0.8414 & 0.9598 & \textbf{0.6437} & \textbf{0.4163} \\
    PneumoniaMNIST(2) & 4,708/524/624 & 0.8635 & 0.9594 & 0.6224 & 0.3429 & 0.8710 & 0.9499 & \textbf{0.4737} & \textbf{0.2031} \\
    RetinaMNIST(5) & 1,080/120/400 & 0.4915 & \textbf{0.7206} & 1.2709 & 0.1946 & \textbf{0.5162} & 0.7083 & \textbf{1.2029} & \textbf{0.1212} \\
    TissueMNIST(8) & 165,466/23,640/47,280 & 0.6685 & 0.9226 & \textbf{0.9156} & 0.4799 & 0.6675 & 0.9210 & 0.9245 & \textbf{0.3865} \\
    \hline

    \end{tabular}
    \caption{The results of the ResNet-18 model on the MedMNIST dataset. For the performance metrics, ACC means accuracy, and CE means cross-entropy. The values are the average of 10 repetitions. The highlighted bold values represent the metrics that one model outperforms the other by at least $1\%$.}
    \label{tab:medmnist}
\end{table}


Table \ref{tab:medmnist} shows that the ERM model and the Wasserstein FI-based model perform comparably in terms of accuracy and AUC. However, the FI-based model significantly outperforms the ERM model in cross-entropy and FI. Notably, the improvement in cross-entropy highlights the strong out-of-sample performance of the FI-based model.
On the downside, the Wasserstein model requires more training epochs to converge, making it more computationally expensive. In our experiment, the Wasserstein model requires 17\% more epochs to stop than the ERM model on average.

% \rzcomment{add supporting numbers (computing specs, time, memory usage) to the last sentence.}
% \cycomment{Done.}
 


\section{Conclusion}
We study the classification problem with an emphasis on the risk of misjudgment and the generalization ability of classifiers. To bridge the gap between conventional metrics and the risk of misjudgment, we propose a novel performance metric, the Fragility Index (FI). FI evaluates large pairwise ranking errors from a risk-averse perspective. Unlike conventional metrics such as AUC, which focus solely on error rates, FI accounts for the fragility of ranking errors under distributional shifts. We demonstrate that FI has strong implications, including probabilistic interpretations and insights into error tail risk. 
% We also demonstrate the effectiveness of FI in evaluating classifiers through various examples and experiments.

We then propose a model training framework designed to effectively optimize FI by adapting the robust satisficing technique to minimize the fragility of loss functions. Our contributions include novel exact reformulation results for FI-based model training, covering cross-entropy loss, hinge-type loss, and Lipschitz loss. We further explore the generalization guarantees and finite-sample properties of the FI-based framework.
We finally introduce FI as a novel regularizer for deep neural network training, offering a scalable and compatible solution with existing neural network regularizers. The effectiveness of our FI-based model is validated through experiments on both synthetic and real datasets.




Classification models play an important role as prediction tools across diverse applications. For managers aiming to make better decisions, understanding the risk and uncertainty in model predictions is essential. Our work offers a new perspective on evaluating and training classification models, with potential benefits for safety-critical and cost-sensitive applications like healthcare and finance. Interesting future directions include extending our risk-aware framework to more advanced machine learning models, such as  Large Language Models that also grapple with the issue of overconfidence in producing misleading outputs.
% \rzcomment{add one or two sentences for future work.} \cycomment{Done.}

\theendnotes

% \bibliographystyle{informs2014}		
% \bibliography{reference}
\putbib

\end{bibunit}


\newpage
\begin{bibunit}


\begin{APPENDIX}{}

\numberwithin{equation}{section}

%The supplementary material mainly consists of the supplementary to the main text and mathematical proofs. 
There are three sections as supplementary: Section \ref{appe:fi} introduces the Wasserstein FI and the FI based on the loss function; Section \ref{appe:fi_training} contains the detailed reformulation building blocks, statistical properties of our FI-based model and the connection between our model and DRO;
% \qjcomment{Don't forget to change this}\cycomment{Done.} 
Section \ref{appe:experiment} provides extra experiments with messages similar to the main text. All proofs are listed in Section \ref{appe:proof}.

\section{Supplementary to FI (Section \ref{sec:FI} )}
\label{appe:fi}

\subsection{Algorithm for Solving FI under KL-divergence Distance}
The bisection algorithm for solving the FI under KL-divergence distance is shown in Algorithm \ref{alg:solve_k}. 
\begin{algorithm}[H]
    \caption{Solve $k$}
    \label{alg:solve_k}
    \KwIn{The function $G(\cdot)$, the initial $k_0$}
    \KwOut{The optimal $k^*$}
    \KwInit{Repeat $k_0 = 2 k_0$ until $G(k_0) > 0$ and $G(2 k_0) < 0$. Then, let $k_{\min} = k_0, k_{\max} = 2 k_0$.}
    \While{$k_{\max} - k_{\min} > \epsilon$}{
        $k = \frac{k_{\max} + k_{\min}}{2}$\;
        \If{$G(k) \leq 0$}{
            $k_{\max} = k$\;
        }
        \Else{
            $k_{\min} = k$\;
        }
    }
    \Return{$k^* = \frac{k_{\max} + k_{\min}}{2}$}
\end{algorithm}
% \begin{algorithm}
%     \caption{Algorithm for determining $\mathrm{FI}_{\mathrm{KL}}(h; \tau)$ by bisection method}
%     \label{alg:fi_kl}
%     \KwData{The empirical distribution of $\varepsilon(h)$, tolerance parameter $\epsilon $}
%     \KwIn{$\underline{k}= 0$, choose a positive value $k_0$}
%     \While{$G(k_0)>1$}{$\underline{k}\leftarrow k_0, k_0\leftarrow 2k_0$}
%     \ \ \ \ \ \ $\bar{k}=k_0$\\
%     \While{$|G(\underline{k})-G(\bar{k})|\leq \epsilon$}
%     {$k_{mid}=(\underline{k}+\bar{k})/2$\\
%     \uIf{$G(k_{mid})>1$}{$\underline{k}=k_{mid}$}
%     \Else{$\bar{k}=k_{mid}$}}
%     \KwOut{$\mathrm{FI}_{\mathrm{KL}}(h; \tau)=\bar{k}$}
%   \end{algorithm} 

\subsection{Solving FI under Wasserstein Distance ($\mathrm{FI}_{\mathrm{W}}$)}
% \rzcomment{Should we rename it as ``Solving FI under Wasserstein Distance''?} \cycomment{Done}
We next provide another example of FI, called $\mathrm{FI}_{\mathrm{W}}$, by adopting the 1-Wasserstein distance as the probability metric in (\ref{eq:fi_def}). $\mathrm{FI}_{\mathrm{W}}(h; \tau)$ is defined in the form of 
\begin{equation}
    \label{eq:fi_def_w}
    \mathrm{FI}_{\mathrm{W}}(h;\tau)=\min\left\{ k\geq 0 \middle | \mathbb{E}_{\mathbb{P}}[\varepsilon(h)]\leq \tau + k D_{\mathrm{W}}(\mathbb{P},\hat{\mathbb{P}}), \ \forall \mathbb{P}\in \mathcal{P}(\mathcal{E})\right\},
\end{equation}
where $\mathcal{P}(\mathcal{E})$ denotes the set of all distributions with support $\mathcal{E}=\left\{\varepsilon\in \mathbb{R}|\varepsilon\leq \bar{\varepsilon}\right\}$ and
\begin{equation}
    \label{eq:wass_def_in_fi}
    D_{\mathrm{W}} (\mathbb{P},\hat{\mathbb{P}}):=\inf\limits_{\mathbb{Q}\in \Pi(\mathbb{P}, \hat{\mathbb{P}}) } \mathbb{E}_{\mathbb{Q}} \left[| \varepsilon-\hat{\varepsilon} | \right],
\end{equation}
with $\Pi(\mathbb{P}, \hat{\mathbb{P}})$ denoting the set of joint distribution of $\varepsilon$ and $\hat{\varepsilon}$ with marginals $\mathbb{P}$ and $\hat{\mathbb{P}}$. 
Notice that only the upper bound of $\varepsilon$ is restricted because of the constraint direction in the definition \eqref{eq:fi_def_w}. Then, we show that $\mathrm{FI}_{\mathrm{W}}(h;\tau)$ can be calculated efficiently as a linear optimization problem.

\begin{propositionAp}\label{prop:fi_w_reformulation}
The FI defined in (\ref{eq:fi_def_w}) can be calculated by solving the following linear programming problem.
% \begin{equation}\label{eq:fi_def_w_equivalence}
%     \begin{aligned}
%         \mathrm{FI}_{\mathrm{W}}(h;\tau)=\min_{k, p_{ij} \geq 0} & \ k\\
%         \text{s.t.}\ & \sum\limits_{i=1}^{m^+}\sum\limits_{j=1}^{m^-}(1-p_{ij})\bar{\varepsilon} + p_{ij} \varepsilon_{ij}(h) \leq \tau\\
%         &p_{ij}\leq k & \forall i=1, \cdots, m^+, j=1,\cdots, m^-\\
%     \end{aligned}
% \end{equation}
\begin{equation}
    \small
    \label{eq:fi_def_w_equivalence}
    \mathrm{FI}_{\mathrm{W}}(h;\tau)=\min \left\{
        k \geq 0 \middle | \frac{1}{m^+ m^-}\sum_{i \in [m^+]}\sum_{j \in [m^-] }(1-p_{ij})\bar{\varepsilon} + p_{ij} \varepsilon_{ij}(h) \leq \tau;\ p_{ij}\leq \min\{1, k\}, \forall i \in [m^+], j \in [m^-]
    \right\}
\end{equation}
\end{propositionAp}

The advantage of $\mathrm{FI}_{\mathrm{W}}$ is that its support $\mathcal{E}$ is not restricted to $\supp{\hat{\mathbb{P}}}$ as in $\mathrm{FI}_{\mathrm{KL}}$. This means that $\mathrm{FI}_{\mathrm{W}}$ can involve the ranking error values that are unseen in the samples. However, as shown in the formulation \eqref{eq:fi_def_w_equivalence}, the value of $\mathrm{FI}_{\mathrm{W}}$ is very sensitive to the upper bound $\bar{\varepsilon}$, which is hard to determine. This is not a special issue in our setting. The quantification of support is important, and a common problem in the Wasserstein reformulation \citep{kuhn2019wasserstein}. 

\subsection{FI Upon the Loss Function}
In classification, the loss function is the objective we minimize during the training process. Therefore, its magnitude also reveals the classification quality. $0-1$ loss is one of the fundamental loss functions in classification, and it is exactly equivalent to the accuracy when considering the empirical expectation under training data. However, due to tractability issues, we usually adopt surrogate loss functions, such as hinge loss. For example, consider the linear classifier $\BFw^T \BFx$ with the hinge loss 
$$
    \ell(\BFx, y) = \max\{0, 1-y \BFw^T \BFx\},
$$ 
where $y \in \{-1, 1\}$ is the label. For observation $\BFx$, if $\ell(\BFx, y) > 1$, the classifier makes false predictions for this sample. Moreover, the hinge loss is also interpreted as the margin-maximization loss notably for SVM. Therefore, the value of the hinge loss also reveals the distance between the sample and the decision boundary and reflects the confidence level of the false prediction. Therefore, we can also define the FI based on the hinge loss as 
\begin{equation}
    \label{eq:fi_hinge}
    \mathrm{FI}(\ell; \tau) = \min\left\{ k \geq 0 \middle | \mathbb{E}_{\mathbb{P}}[\ell] \leq \tau + k D_{\mathrm{KL}}(\mathbb{P}, \hat{\mathbb{P}}_\ell), \ \forall \mathbb{P}\in \mathcal{P}(\mathcal{L})\right\}.
\end{equation} 
The distribution $\phat_\ell$ is the empirical distribution of the hinge loss upon samples. The FI based on the hinge loss inherits all the properties of risk aversion and tail risk from the original FI. It indicates the risk of large hinge loss values, which means the risk of large margin violations. Regarding the target $\tau$, it can be set based on $\mathbb{E}_{\phat_\ell}[\ell]$ for sure. Notice that $1$ is the threshold of correct and incorrect prediction, so $\tau = 1$ is also a natural choice. 

Unlike the FI based on the ranking error, the FI based on the hinge loss is sample-wise instead of sample-pair-wise. Since the basic criterion changes, it is hard to compare the FI in \eqref{eq:fi_hinge} with AUC or ranking-error-based FI. However, recall that the hinge loss is a surrogate loss for the $0-1$ loss, which is equivalent to the accuracy. Therefore, we can make a connection between the hinge-loss-based and accuracy. As mentioned 
$$
    \mathrm{Accuracy} = \mathbb{P}(y \BFw^T \BFx > 0) = \mathbb{P} (\ell(\BFx, y) < 1).
$$
Therefore, the hinge loss plays the same role as the ranking error in AUC. This also implies that the relationship between hinge-loss-based FI and accuracy is similar to the relationship between ranking-error-based FI and AUC. The insights and properties such as Theorem \ref{thm:fi_properties} remain in the context of hinge loss and accuracy. For conciseness, we do not enumerate these insights here. 

Even though we mainly focus on hinge loss as an example, for many general loss functions like cross-entropy loss or square loss, their values also convey information about the quality of prediction. Therefore, a similar argument can be applied to the FI based on these loss functions.

\section{Supplementary to FI-based Training (Section \ref{sec:fi_training})}
\label{appe:fi_training}
In this section, we will cover more details about the FI-based training framework. In section \ref{appe:fi_reformulation}, we first introduce some initial steps and supplementary illustration for the Wasserstein reformulation of our FI-based training model. In section \ref{appe:finite_sample_guarantee}, we introduce more performance guarantees about the FI-based training framework: starting from the generalization guarantee of the training loss and extending to the convergence and finite-sample guarantees about the training objective and optimal parameters. Moreover, in section \ref{appe:connection_dro}, we will bridge our framework with DRO and show the associated reformulation under the conventional DRO framework for comparison. We also introduce a way to integrate the RS and DRO framework to counter the potential over-conservativeness. 
% Finally, in section \ref{appe:randomized_policy}, we provide some insights into how our model performs under a randomized policy, i.e. allowing the weight matrix to be random. 


\subsection{Supplementary to Wasserstein reformulation results}\label{appe:fi_reformulation}
We first introduce the $c$-transformation of the loss function as the building block for all reformulation results in Section \ref{sec:wass_fi_training}. Then, we give two ancillary explanations: one is a remark for the support choice in Lemma \ref{lemma:1wass_reformulation}; the other serves as an example of Theorem \ref{thm:piecewise_reformulation}.

\subsubsection{The $c$-transformation}
We start from the general case of OT discrepancy. Consider the OT transportation cost function as 
$$
    c(\BFphi, y, \hat{\BFphi}, \hat{y}) = c_\phi(\BFphi, \hat{\BFphi}) + \gamma \mathbb{I}(y \neq \hat{y}).
$$ 
We next make the following mild assumptions.
\begin{assumptionAp} (Convexity)
    \label{asmp:wass_convexity}
    \begin{enumerate}[(a)]
        \item The uncertainty set $\mathit{\Phi} \subseteq \mathbb{R}^M$ is convex and closed. Moreover,
        the transportation cost function $c_\phi(\BFphi, \hat{\BFphi})$ is proper, convex in $\BFphi$ for every $\hat{\BFphi} \in \supp{\hat{\mathbb{P}}}$. 
        \item The loss function $\ell(\BFB^T\BFphi, y)$ is convex in $\BFB^T\BFphi$ for every $y \in \mathcal{Y}$, and integrable with respect to $\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ for any $\BFB \in \mathcal{B}$.
    \end{enumerate}
\end{assumptionAp}
We start by conducting the basic transformation in DRO and RS framework to handle the problem of the worst-case distributions. 
\begin{align*}
    &\phantom{=} \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \left\{\mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)]- k D_{\mathrm{W}} (\mathbb{P},\hat{\mathbb{P}})\right\} \\
    &= \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \left\{\mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)]- k \inf\limits_{\mathbb{Q}\in \Pi(\mathbb{P},\hat{\mathbb{P}}) } \mathbb{E}_{\mathbb{Q}} \left[c(\BFphi, y, \hat{\BFphi}, \hat{y}) \right]\right\} \\
    &= \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \sup\limits_{\mathbb{Q}\in \Pi(\mathbb{P},\hat{\mathbb{P}}) } \mathbb{E}_{\mathbb{Q}}\left[\ell(\BFB^T\BFphi, y) - k c(\BFphi, y, \hat{\BFphi}, \hat{y}) \right] \\
    &= \mathbb{E}_\mathbb{\hat{P}} \left[\sup\limits_{(\mathbb{Q|\hat{P}})\in (\Pi(\mathbb{P},\hat{\mathbb{P}})|\mathbb{\hat{P} })} \mathbb{E}_{\mathbb{Q|\hat{P}}}\left[\ell(\BFB^T\BFphi, y) - k c(\BFphi, y, \hat{\BFphi}, \hat{y}) \right] \right]\\
    & = \frac{1}{N} \sum_{n \in [N]}\sup_{y_n \in \mathcal{Y}} \left\{ \sup_{\BFphi \in \Phi} \left\{ \ell(\BFB^T\BFphi, y_n) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} - k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \right\}
    % & = \frac{1}{N} \sum_{n \in [N]} \sup_{\BFphi \in \Phi} \left\{ w^T \BFB^T \BFphi - k d(\BFphi, \hat{\BFphi}_n) \right\} 
    % & = - \frac{1}{N} \sum_{n \in [N]} \inf_{\BFphi \in \Phi} \left\{ k d(\BFphi, \hat{\BFphi}_n) - \ell(\BFB^T\BFphi, y) \right\} \\
    % & = - \frac{1}{N} \sum_{n \in [N]} \inf_{\BFphi \in \Phi} \left\{ k d(\BFphi, \hat{\BFphi}_n) - w^T \BFB^T \BFphi \right\} \\
\end{align*}

% Similarly, we define 
% \begin{equation}
%     \label{eq:wass_gk}
%     G(k) = \min_{\BFB \in \mathcal{\BFB}} \frac{1}{N} \sum_{n \in [N]} \sup_{\BFphi \in \Phi} \left\{ \ell(\BFB^T\BFphi, y) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} + R(\BFB) - \tau.
% \end{equation}
The transformation $\sup_{\BFphi \in \dom{c_{\phi}(\cdot, \hat{\BFphi})}} \left\{ \ell(\BFB^T\BFphi, y) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\}$ 
% \qjcomment{what is $\dom$ here? not consistent.}\cycomment{$\dom$ means the domain of a function. I mention this in the notation part.} 
is usually called the c-transformation with respect to the cost function $c_{\phi}(\BFphi, \hat{\BFphi}_n)$ \citep{tacskesen2023semi}. Notice $\BFphi$ is restricted to a given support $\Phi$ in the inner maximization. We extend the conventional support-free definition and call the following expression the c-transformed loss function in our work.
\begin{equation}
    \label{eq:c_transformed_loss}
    \ell_c(\BFB, k, \hat{\BFphi}, y) = \sup_{\BFphi \in \Phi} \left\{ \ell(\BFB^T\BFphi, y) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\}.
\end{equation}

In the context of traditional DRO, \cite{shafieezadeh2023new} proposed convexity conditions such that the problem \eqref{eq:c_transformed_loss} commits a finite convex reformulation. They require that the loss function is convex in the control parameters $\BFB$ but concave in the uncertain scenario $\BFphi$. However, this is too restrictive for our loss function $\ell(\BFB^T\BFphi, y)$. Considering the bilinear structure of $\BFB^T \BFphi$, if $\ell$ is convex in $\BFB$, $\ell$ must be convex in $\BFphi$. 
% \qjcomment{what do you mean ``the same''?} \cycomment{I mean if $\ell$ is convex in $\BFB$, $\ell$ must be convex in $\BFphi$. I already removed the ``same''.}
Only when $\ell$ is linear, the convexity conditions of \cite{shafieezadeh2023new} can be satisfied. Generally, if the loss function $\ell$ is convex, this is fine with the minimization of $\BFB$ but the maximization of $\BFphi$ in the c-transformation is hard. If the loss function $\ell$ is concave, the maximization of $\BFphi$ is fine, but the final problem may no longer be convex. 
% However, we can still find some special cases, besides the linear loss, such that problem \eqref{eq:prob_general} admits a finite convex reformulation. 
% \qjcomment{Finite convex?} \cycomment{I mean the problem is convex with finite constraints.}  

Even though the c-transform function $\ell_c(\BFB, k, \hat{\BFphi}, y)$ is generally not convex since we need to maximize the difference between two convex functions, we show that it can be transformed into a simpler form, which can inspire us to conduct further analysis.
% \rzcomment{we may consider  separating Theorem \ref{prop:convex_ot_reformulation} into two theorems. One for  the piecewise linear loss function, for which we provide the hinge loss as an example. The other for the convex function reformulation, for which we  discuss examples in the Lemma \ref{lemma:1wass_reformulation}, Theorem \ref{thm:lipschitz_approx}, Proposition \ref{prop:cross_entropy_reformulation}.} \cycomment{I separated them into two parts and added more structures.}
\begin{propositionAp}
    \label{prop:convex_ot_reformulation}
    Under Assumptions \ref{asmp:tau} and \ref{asmp:wass_convexity},
    the c-transformed loss function $\ell_c(\BFB, k, \hat{\BFphi}, y)$ defined in equation \eqref{eq:c_transformed_loss} is convex in $\BFB$. Moreover, 
    \begin{equation}
        \label{eq:c_transformed_loss_dual}
        \ell_c(\BFB, k, \hat{\BFphi}, y) = \sup_{\BFzeta \in \dom{\ell^{1*}}} \inf_{\BFtheta \in \dom{\delta^*_{\BFPhi}}} \left\{
            kc_{\phi}^{1*}((\BFB\BFzeta - \BFtheta)/k, \hat{\BFphi}) + \delta_{\mathit{\Phi}}^*(\BFtheta) - \ell^{1*}(\BFzeta, y) 
        \right\}
    \end{equation}
    In particular, when $\mathit{\Phi} = \mathbb{R}^M$, meaning that no restriction on the support of $\BFphi$, we have
    $$
        \ell_c(\BFB, k, \hat{\BFphi}, y) = \sup_{\BFzeta \in \dom{\ell^{1*}}} \left\{
            kc_{\phi}^{1*}((\BFB\BFzeta)/k, \hat{\BFphi}) - \ell^{1*}(\BFzeta, y).
        \right\}.
    $$
\end{propositionAp}
Recall that the superscript ``1'' in $c_{\phi}^{1*}$ and $\ell^{1*}$ means the convex conjugate to the first variable. Proposition \ref{eq:c_transformed_loss} is the main building block for Lemma \ref{lemma:1wass_reformulation}, which further helps to derive exact convex reformulations.
% The reformulation \eqref{eq:c_transformed_loss_dual} is generally nonconvex. 
The term $\delta_\mathit{\Phi}(\cdot)$ represents the characteristic function of $\mathit{\Phi}$ and $\delta_\mathit{\cdot}^*(\BFzeta)$ is its convex conjugate. For more examples of the characteristic function and its convex conjugate, we refer to Table B.2 of \cite{kuhn2019wasserstein} for more details.  

\subsubsection{Remark on Lemma \ref{lemma:1wass_reformulation}}
Even though we incorporate the support information $\mathit{\Phi}$ in Proposition \ref{prop:convex_ot_reformulation}, the support restriction is relaxed to the whole space $\mathbb{R}^M$ in Lemma \ref{lemma:1wass_reformulation}. We highlight that this relaxation is necessary for the equivalent reformulation. 
Even though the c-transformed loss function \eqref{eq:c_transformed_loss_dual} seems to be convex with support $\mathit{\Phi}$ and 1-Wasserstein distance, the final results are negative because the coupled constraints induced by the convex conjugate of $c_{\phi}^{1*}$. 
% \qjcomment{sentence too long, logic not clear.} 
% \cycomment{I delete the sentence and shorten this Remark.} 
To see this, considering $\mathit{\Phi} \subset \mathbb{R}^M$ and following the same logic as the proof of Lemma \ref{lemma:1wass_reformulation}, we can have
$$
    \ell_c(\BFB, k, \hat{\BFphi}, y) =   \sup_{\BFzeta \in \dom{\ell^{1*}}} \inf_{\substack{\BFtheta \in \dom{\delta^*_{\BFPhi}} \\ \|\BFB\BFzeta - \BFtheta\|_* \leq k}} \left\{
        (\BFB\BFzeta - \BFtheta)^T\hat{\BFphi} + \delta_\mathit{\Phi}^*(\BFtheta) - \ell^{1*}(\BFzeta, y)
    \right\} 
$$
Notice that in the inner minimization, the constraint $\|\BFB\BFzeta - \BFtheta\|_* \leq k$ is coupled with both $\BFtheta$ and $\BFzeta$, the decision variables of the inner and outer optimization problems. As shown in \cite{tsaknakis2023minimax}, even with the simplest linear coupled constraint, the coupled minimax problem is very challenging. Moreover, we are not allowed to exchange the $\sup$ and $\inf$ and obtain an inner problem without the coupled constraint. Even the minimax inequality does not hold for the coupled case according to the counter-example in \cite{tsaknakis2023minimax}. Therefore, we must release the restriction on the support $\mathit{\Phi}$ to obtain the reformulation \eqref{eq:1wass_reformulation}.

\subsubsection{Examples for Theorem \ref{thm:piecewise_reformulation}}
Particularly, we also show two concrete examples of Theorem \ref{thm:piecewise_reformulation} including the polyhedron and a convex set determined by finite convex functions, to show how to incorporate different uncertainty sets and their characteristic functions.

\begin{corollaryAp}
    \label{cor:eg_uncertainty}
    Consider the piecewise linear loss function $\ell(\BFu, y) = \max_{i\in[K_y]} \{\BFa_{yi}^T \BFu + b_{yi}\}$.
    \begin{enumerate}[(a)]
        \item Suppose the uncertainty set is a polytope as $\mathit{\Phi} = \{\BFphi \in \mathbb{R}^M: C \BFphi \leq \BFd \}$. Then, problem \eqref{eq:prob_general} is equivalent to 
        \begin{align*}
            \min_{\BFB \in \mathcal{B}, \BFs, \BFlambda_{in}} & \ k\\
            \text{s.t.} &\frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0\\
            & \ b_{y_ni} + \BFd^T \BFlambda_{in}  + k c_{\phi}^{1*}((\BFB \BFa_{y_ni} - C^T \BFlambda_{in})/k, \hat{\BFphi}_n ) -  k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \leq s_n, & \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}, \\
            & \BFlambda_{in} \geq 0, & \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}.
        \end{align*}
        \item Suppose the uncertainty set is given by $\mathit{\Phi} = \{\BFphi \in \mathbb{R}^M: f_j(\BFphi) \leq 0, j \in [J] \}$, where $f_j(\BFphi)$ are convex proper functions. Then, problem \eqref{eq:prob_general} is equivalent to
        \begin{align*}
            \min_{\BFB \in \mathcal{B}, \BFs, \BFlambda_{in}, \BFz_{inj}} & \ k\\
            \text{s.t.} & \frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0\\
            & \ b_{y_ni} + \sum_{j\in[J]} \lambda_{inj} f_j^*\left(\frac{\BFz_{inj}}{\lambda_{inj}}\right)  + k c_{\phi}^{1*}((\BFB \BFa_{y_ni} - \BFv_{in})/k, \hat{\BFphi}_n ) -  k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \leq s_n, \\
            & \hspace{280pt} \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}, \\
            & \sum_{j\in[J]} \BFz_{inj} = \BFv_{in}, \hspace{214pt}  \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y},\\
            & \BFlambda_{in} \geq 0, \hspace{245pt}  \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}.
        \end{align*}          
    \end{enumerate}
\end{corollaryAp}

In addition, Proposition \ref{prop:convex_ot_reformulation} is a generalization of Theorem 3.8 (i) of \cite{shafieezadeh2023new}. As mentioned by them, even though the reformulation in Proposition \ref{prop:convex_ot_reformulation} is still nonconvex, it may be easier to solve. For example, in the case of $\mathit{\Phi} = \mathbb{R}^M$, we are confronting a problem of the dimension of $\dom{\ell^{1*}}$ instead of the dimension of $\mathit{\Phi}$. For binary classification, the matrix $\BFB$ can be reduced to a single vector $\BFbeta$, and we only need to tackle a one-dimensional nonconvex problem, for which obtaining an analytical solution is possible. Suppose we have an oracle or algorithm to solve it, we can establish a stochastic subgradient for the minimization of $\BFB$ in $G(k)$, as $\ell_c$ is convex in $\BFB$. For more detailed handling of the reformulation \eqref{eq:c_transformed_loss_dual}, we refer to \cite{shafieezadeh2023new}.



\subsection{Finite-sample Guarantee}
\label{appe:finite_sample_guarantee}

For many data-driven and machine-learning models with limited data, one constant concern is how much we can trust the model trained on the finite dataset. Let $\ptrue$ denote the ground truth distribution of the dataset and $\phat_N$ denote the empirical distribution of $N$ samples. It is known that $\phat_N$ converges to $\ptrue$ as $N \to \infty$. However, this does not necessarily imply the consistent convergence of the classifiers or the loss function. 

Moreover, under the context of classification, we may not be satisfied with the asymptotics in the large-sample regime. Even though the data size in classification tasks is usually not small, the performance guarantee can still be weak due to the curse of dimensionality caused by the high-dimensional feature space \citep{gao2023finite}. Besides, we always conduct batch training in practice, which means that in each iteration, the update is only based on the batch size. Both the high-dimensional nature and the small batch size weaken the sound of the asymptotic guarantee in the large-sample regime, and a finite sample guarantee with a high convergence rate is more desired.

In this section, we first discuss the loss function's finite-sample guarantee and then delve into the induced classifiers.

\subsubsection{Generalization guarantee on loss function}
Regarding the loss function, we highlight that one extraordinary nature of the FI-based training framework \eqref{eq:prob_general} is that the constraint itself directly serves as a generalization guarantee as 
\begin{lemmaAp}
    \label{lemma:generalization}
    Suppose $k^*$ and $\BFB^*$ are the optimal solutions of problem \eqref{eq:prob_general}. Then, for any distribution $\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})$, we can obtain a tight generalization guarantee as
    $$
    \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^{*T}\BFphi, y\right)] - \mathbb{E}_{\hat{\mathbb{P}}}[\ell \left(\BFB^{*T}\BFphi, y\right)] \leq \tau - \mathbb{E}_{\hat{\mathbb{P}}}[\ell \left(\BFB^{*T}\BFphi, y\right)] + k^* D (\mathbb{P}, \hat{\mathbb{P}}).
    $$
\end{lemmaAp}
This bound is tight in the sense that there must exist a distribution $\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ that achieves the bound. Unlike DRO and other adversarial training frameworks, the FI-based model is oriented to minimize the worst-case target violation degree under the distribution shift, which is akin to a generalization guarantee. Therefore, the FI-based framework is much more straightforward and interpretable regarding ensuring generalization.

More usually, the generalization error given an $N$-sample empirical distribution $\phat_N$ is evaluated by comparing the empirical loss with the expected loss under the true data distribution $\ptrue$. It is known that $\phat_N$ converges to $\ptrue$ as $N \to \infty$. To bound the generalization error, we consider the light-tail assumptions. 
\begin{assumptionAp}
    \label{asmp:light_tail}
    (Light-tailed distribution) There is an exponent $a > 1$ such that $\mathbb{E}_{\ptrue}[\exp((\|(\BFphi, y)\|)^a)] < \infty$.
\end{assumptionAp}
Then, we can obtain the following generalization guarantee of order $O(N^{-\frac{1}{M + 1}})$ for the FI-based training framework. 
\begin{propositionAp}
    \label{prop:generalization_error}
    Suppose Assumption \ref{asmp:lipschitz} and \ref{asmp:light_tail} hold. Let $\BFB^*_N$ and $k_N^*$ be the optimal solution of problem \eqref{eq:prob_general}, under the empirical distribution $\phat_N$ and 1-Wasserstein distance in equation \eqref{eq:def_ot}. For sufficiently small $\epsilon > 0$, with probability at least $1 - \epsilon$, we have
    \begin{equation}
        \label{eq:generalization_error_bound}
        \begin{aligned}
            & \mathbb{E}_{\ptrue}[\ell (\BFB^{*T}_N\BFphi, y)] - \mathbb{E}_{\phat_N}[\ell (\BFB^{*T}_N\BFphi, y)] \leq 
            \min \left\{
                (1+\gamma)\omega \|\hat{\BFB}_N^{*T}\|_* \left(
                \frac{1}{C_2 N} \log \left(\frac{C_1}{\epsilon}\right)
            \right)^{\frac{1}{M+1}}, \right.\\
            &\hspace{150pt} \left.
            \tau - \mathbb{E}_{\phat_N}[\ell (\BFB^{*T}_N\BFphi, y)] + (1+\gamma) k_N^* \left(
                \frac{1}{C_2 N} \log \left(\frac{C_1}{\epsilon}\right)
            \right)^{\frac{1}{M+1}}
            \right\},
        \end{aligned}
    \end{equation}  
    where $C_1$ and $C_2$ are constants depending on only the feature dimension $M$ and the light-tailed exponent $a$ in Assumption \ref{asmp:light_tail}, and $\omega = \max\left\{\omega_1, \omega_2 \frac{\sup_{(\BFphi, y) \in \supp{\mathbb{P}^*}} \|\BFphi\|}{\gamma}\right\}$.
\end{propositionAp}
The bound \eqref{eq:generalization_error_bound} can be improved in constant factors by appropriately shrinking the ambiguity set $\mathcal{P}(\mathit{\Phi}, \mathcal{Y})$, and the details are defered in the Appendix \ref{appe:connection_dro}. 

% On top of the generalization guarantee, it is also important to answer whether the classifier trained on the empirical data $\phat_N$ can converge to the classifier of the true data distribution and how fast it converges. The convergence speed matters because we always conduct batch training in practice. In each iteration, the update is only based on a batch of the data. Both the high-dimensional feature space and the small batch size weaken the sound of the asymptotic guarantee in the large-sample regime, and a finite sample guarantee with fast convergence is more desired. In FI-based training, we can establish a finite sample guarantee for both the training objective and the classifier parameters, based on our convex reformulation. The details are deferred in the Appendix \ref{appe:finite_sample}. Moreover, we also explore the model performance under randomized policy and the discussion is in the Appendix \ref{appe:randomized_policy}.

\subsubsection{Finite-sample Guarantee of the learned classifier} \hfill

\textbf{Setup.} In this section, we aim to establish the convergence and finite sample guarantee of our FI-based trained classifier. 
% Our analysis is conducted based on convex optimization. We specialize the assumptions step-by-step to develop the performance guarantee from the asymptotic regime to the finite sample regime. 
In the beginning, we need to figure out the problem that problem \eqref{eq:prob_general} converges to as $N \to \infty$. Since $\phat_N$ converges to $\ptrue$, it is not surprising that our target problem can be obtained by replacing $\phat_N$ with $\ptrue$ in problem \eqref{eq:prob_general}, which is
\begin{equation}
    \label{eq:prob_true}
    \begin{aligned}
        \min_{k, \BFB} & \ k\\
        \text{s.t.}\  &\ \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^T\BFphi, y\right)] + R(\BFB)\leq \tau + k D_{c} (\mathbb{P}, \ptrue), &\ \forall \mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}),\\
        & k\geq 0, \BFB \in \mathcal{B}.
    \end{aligned}
\end{equation}
Let $k^*$ and $\BFB^*$ denote the corresponding optimal solution of problem \eqref{eq:prob_true}. Let $\hat{k}_N$ and $\hat{\BFB}_N$ denote the optimal solution under $\phat_N$. Since $\ptrue$ is generally inaccessible, nor are $k^*$ and $\BFB^*$. We desire to use the solution $\hat{k}_N$ and $\hat{\BFB}_N$ to estimate the solution $k^*$ and $\BFB^*$ under the $\ptrue$. This relates to the convergence and asymptotics of the sample average approximation (SAA) \citep{shapiro2021lectures}. 

To further establish the convergence, we consider the case when our model admits a finite convex reformulation. On the one hand, the convergence of SAA in convex optimization has been studied, and we can develop our specialized results based on the existing theory. On the other hand, we do show that our RS model can be reformulated as a finite convex optimization problem in many cases. Reviewing the reformulation we have achieved, we generally consider the following stochastic convex reformulation 
\begin{equation}
    \label{eq:reformulation_for_convergence}
    \begin{aligned}
        \min_{k, \BFB} & \ k\\
        \text{s.t.}\  & \mathbb{E}_\mathbb{P} [g_1(\ell(\BFB^T\BFphi, y), \BFB, k)] \leq 0, \\
        & g_i(\BFB, k) \leq 0, &\forall i = 2, \dots, S,
    \end{aligned}
\end{equation}
where $g_i, i \in [S]$ are all convex functions. The function $g_1$ corresponds to the target-violation constraint in our reformulations. For example, under the KL-divergence, $g_1(\ell(\BFB^T\BFphi, y), \BFB, k) = k \ln \left( \mathbb{E}_{\phat}\left[ \exp \left(\frac{\ell \left(\BFB^T\BFphi, y \right)}{k}\right)\right] \right) + R(\BFB)  - \tau$, while for the 1-Wasserstein distance, $g_1(\ell(\BFB^T\BFphi, y), \BFB, k) = \ell \left(\BFB^T\BFphi, y \right) + R(\BFB)  - \tau$.  The function $g_i, i \in {2, \dots, S}$ corresponds to the extra constraints, such as the norm constraint in Theorem \ref{thm:cross_entropy_reformulation}. 
% Notice that all the convex reformulation so far is consistent with the form of \eqref{eq:reformulation_for_convergence}. For KL-divergence and 1-Wasserstein distance, this is obvious. As for the piecewise linear loss function introduced in Theorem \ref{prop:convex_ot_reformulation}, we can implement \eqref{eq:reformulation_for_convergence} by eliminating the auxiliary variables. 

Then, we investigate the statistical properties of the model \eqref{eq:reformulation_for_convergence}. The main tool is the stochastic generalized equations (SGE) of the Karush-Kuhn-Tucker (KKT) system \citep{shapiro2021lectures}. As a result, some mild assumptions are required to guarantee the existence and uniqueness of the KKT system.
\begin{assumptionAp}
    \label{asmp:smooth_regularity}
    \begin{enumerate}[(a)]
        \item (Smoothness) The loss function $\ell$ and constraints function $g_i$ are smooth. 
        
        % and $|\ell(\BFB^T\BFphi)| \leq \bar{\ell} (\BFphi)$. There exists a function $\kappa(\BFphi): \Phi \to \mathbb{R}_+ $ such that 
        % $$
        %     |\ell(\BFB'\BFphi) - \ell(\BFB^T\BFphi)| \leq \kappa(\BFphi) \|\BFB' - \BFB \| .
        % $$
        % Moreover, $\exp(\bar{\ell}(\BFphi))$ and $\kappa(\BFphi)\exp(\bar{\ell}(\BFphi))$ are integrable on $\Phi$.

        \item (Regulariy) Under the true distribution $\mathbb{P} = \mathbb{P}^*$, the problem \eqref{eq:reformulation_for_convergence} satisfies the regularity condition Linear independence constraint qualification (LICQ) and the strong second-order sufficiency condition (SSOSC).
    \end{enumerate}
\end{assumptionAp}
Both assumptions are standard in the convex optimization. The smoothness ensures the first-order derivative uniquely exists, while the regularity ensures the consistency of the KKT condition and the optimality condition of problem \eqref{eq:reformulation_for_convergence}. For non-smooth loss functions such as the hinge loss, we can consider its smoothed version, as Theorem \ref{thm:hinge_type_reformulation} suggests that the smoothed loss will not modify the reformulation.
With little abuse of notation, we use $\BFbeta^T = (\BFbeta_1^T, \dots, \BFbeta_S^T) \in \mathbb{R}^{MC}$ denote the flattened vector of the weight matrix $\BFB$. Let $\BFeta$ denote the dual variables induced by the constraints of problem \eqref{eq:reformulation_for_convergence}, so define the Lagrangian function for fixed $\BFphi$ and $y$ as
$$
    \mathcal{L}(k, \BFbeta, \BFeta; \BFphi, y) = k + \eta_1 g_1(\ell(\BFB^T\BFphi, y), \BFB, k) + \sum_{i = 2}^S \eta_i g_i(\BFbeta, k).
$$
Then, consider vector gathering all the variables and the derivative of the Lagrangian function as
\begin{equation}
    \BFw = \left(\begin{array}{c}
        k \\
        \BFbeta \\
        \BFeta
    \end{array} \right) \in \mathbb{R}^{1 + MC + S }, \ 
    \BFPsi(\BFw; \BFphi, y) = \nabla_{\BFw} \mathcal{L}(\BFw; \BFphi, y)\ \text{and}\ \BFPsi(\BFw) = \mathbb{E}_{\ptrue} [ \BFPsi(\BFw; \BFphi, y) ].
\end{equation}
For convenience, let $M_w = MC + S + 1$ be the dimension of vector $\BFw$. We define the projection operator $P_k \BFw = k$, $P_\beta \BFw = \BFbeta$, and $P_\eta \BFw = \BFeta$. We also define another set $\BFGamma(\BFw) $ as 
\begin{align*}
    % &\BFW = \{ \BFw \in \mathbb{R}^{M_w} | k \geq \underline{k}, \BFbeta \in \mathcal{\BFB}, \eta_i (\|\BFbeta_i\| - 1) = 0, i \in [S] \}, \\
    \BFGamma(\BFw) = \{\BFgamma \in \mathbb{R}^{M_w} | P_k \BFgamma = 0, P_\beta \BFgamma = 0, P_\eta \BFgamma \geq \BFzero, (P_\eta \BFgamma)\circ (P_\eta \BFw) = 0 \},
\end{align*}
where $\circ$ denotes the Hadamard product. 
% The lower bound $\underline{k}$ is given in Lemma \ref{lemma:lower_bound_k}.  Both $\BFW$ and $\BFGamma(\BFw)$ are closed sets. The set $\BFW$ consists of the intrinsic feasibility requirement and the complementary slackness. The set $\BFGamma(\BFw)$ represents the slackness of $\BFPsi(\BFw)$. 
On top of the above notations, we finally rewrite the KKT condition under the $\ptrue$ as the following SGE
\begin{equation}
    \label{eq:sge_true}
    \BFzero \in \BFPsi(\BFw)  + \BFGamma(\BFw).
\end{equation}
In the same way, we can define the KKT condition under the empirical distribution by $\hat{\BFPsi}_N(\BFw) = \mathbb{E}_{\phat_N} [ \BFPsi(\BFw; \BFphi, y) ]$ and $\BFzero \in \hat{\BFPsi}_N(\BFw) + \BFGamma(\BFw)$. Let $\BFw^*$ and $\hat{\BFw}_N$ denote the optimal solution to the KKT condition under the true distribution and the empirical distribution, respectively. Then, we propose more assumptions to regulate the behavior of the SGEs.
\begin{assumptionAp}
    \label{asmp:sge_bound_integrable}
    \begin{enumerate}[(a)]
        \item (Boundedness) There exists a compact set $\mathcal{W} \in \mathbb{R}^{M_w}$ such that $\BFw^* \in \mathcal{W}$ and $\hat{\BFw}_N \in \mathcal{W}$.
        \item (Integrable domination) When $\BFw \in \mathcal{W}$, $\|\nabla_{k, \BFbeta}\ g_1(\ell(\BFB^T\BFphi, y), \BFB, k)\|$ and $\|\nabla_{k, \BFbeta}^2\ g_1(\ell(\BFB^T\BFphi, y), \BFB, k)\|$ are dominated by an integrable function on $\Phi \times \mathcal{Y}$, where $\nabla^2$ refers to the Hessian matrix operator.
    \end{enumerate}
\end{assumptionAp}
The Assumption \ref{asmp:sge_bound_integrable} (a) is to guarantee both optimal solutions are bounded in a compact set. Otherwise, the convergence may not be well-defined. The Assumption \ref{asmp:sge_bound_integrable} (b) is to guarantee the integrability of the SGE. Since only the constraint function $g_1$ involves $\BFphi$ and $y$, Assumption \ref{asmp:sge_bound_integrable} (b) implies that $\|\BFPsi(\BFw)\|$ and $\|\nabla_\BFw \BFPsi(\BFw)\|$ are dominated by an integrable function on $\Phi \times \mathcal{Y}$. Notice that for the KL-divergence case, Assumption \ref{asmp:sge_bound_integrable} (b) requires the Assumption \ref{asmp:tau} to be satisfied because the gradient will divergence when $k \to 0$.

\textbf{Asymptotics and finite sample guarantee. }
With the above assumptions, we can establish the convergence and asymptotics of the SGE.
\begin{propositionAp}
    \label{prop:convergence}
    Under Assumption \ref{asmp:smooth_regularity} and \ref{asmp:sge_bound_integrable}, we have
    \begin{enumerate}[(a)]
        \item (Strong regularity) The SGE \eqref{eq:sge_true} is strongly regular at $\BFw^*$.    
        \item (Convergence) Let $\BFw^*$ and $\hat{\BFw}_N$ denote the optimal solution to $\BFzero \in \BFPsi(\BFw)  + \BFGamma(\BFw)$ and $\BFzero \in \hat{\BFPsi}_N(\BFw)  + \BFGamma(\BFw)$, respectively. Then, $\hat{\BFw}_N$ converges to $\BFw^*$ almost surely as $N \to \infty$. 
        \item (Asymptotics) Let $\BFeta_+$ denote the collection of the positive components of $\BFeta^*$, and $\BFeta_0$ denote the collection of the zero components of $\BFeta^*$. Define $\BFw' = (k, \BFbeta^T, \BFeta_+^T )^T $ and $\BFPsi'(\BFw')$ by dropping the constraints related to $\BFeta_0$. Then, there exists a constant $N_C > 0$ such that $\hat{\BFeta}_{0N} = \BFeta_0^* = \BFzero$ if $N \geq N_C$. Moreover, if $\nabla_{\BFw'}\BFPsi'(\BFw')$ is invertible and $\hat{\BFPsi}_N'$ converges to $\BFPsi'$ in the speed of $O(N^{-\frac{1}{2}})$, we have
        $$
            N^{1/2} (\hat{\BFw}_N ' - \BFw'^{*}) \to \mathcal{N}(\BFzero, (\nabla_{\BFw'}\BFPsi'(\BFw'))^{-1}\Sigma' (\nabla_{\BFw'}\BFPsi'(\BFw'))^{-1}),
        $$
        where $\Sigma' $ is the covariance matrix of $\BFPsi'(\BFw'; \BFphi, y)$ under $\ptrue$.
    \end{enumerate}
\end{propositionAp}

Proposition \ref{prop:convergence} shows that the empirical solution $\hat{\BFw}_N$ converges to the true solution $\BFw^*$ as $N \to \infty$ and the asymptotics in the large-sample regime. As mentioned, this may not be significant in the context of classification. Our next step is to show that the convergence rate can be exponentially decayed with extra mild assumptions. 
\begin{assumptionAp}
    \label{asmp:lipschitz_sge_finite_mgf}
    \begin{enumerate}[(a)]
        \item (Lipschitz module)There exists an integrable function $\kappa_\BFPsi(\BFphi)$ such that
        $$
            \|\BFPsi(\BFw_1; \BFphi, y) - \BFPsi(\BFw_2; \BFphi, y) \| \leq \kappa_\BFPsi(\BFphi) \|\BFw_1 - \BFw_2\|, \ \forall \BFw_1, \BFw_2 \in \mathcal{W}.
        $$
        \item (Finite moment generating function) Define the moment generating of $\BFPsi(\BFw; \BFphi, y) - \mathbb{E}_\ptrue [\BFPsi(\BFw; \BFphi, y)]$ and $\kappa_\BFPsi(\BFphi)$ as 
        \begin{align*}
            M_i(t) &:= \mathbb{E}_{\ptrue} \left[ \exp \left( t \left( \BFPsi_i(\BFw; \BFphi, y) - \mathbb{E}_{\ptrue} [\BFPsi_i(\BFw; \BFphi, y)] \right) \right) \right], \\
            M_\kappa(t) &:= \mathbb{E}_{\ptrue} \left[ \exp \left( t \kappa_\BFPsi(\BFphi) \right) \right].
        \end{align*}
        For any $\BFw \in \mathcal{W}$, the moment generating functions $M_i(t), i \in [M_w]$ and $M_\kappa (t)$ have finite values for all $t$ in a neighborhood of zero.
    \end{enumerate}
\end{assumptionAp}

Assumption \ref{asmp:lipschitz_sge_finite_mgf} (a) also means the Lipschitz continuity of $\BFPsi(\BFw)$, which further refers to the Lipschitz continuity of the constraint $g_1$ and the loss function $\ell$. If only consider the cases of the KL-divergence and 1-Wasserstein distance that we have discussed, this assumption can be reduced to the Lipschitz continuity requirement of the loss function $\ell$ as Assumption \ref{asmp:lipschitz}. 
% The constraint $g_1$ is a linear function in the 1-Wasserstein case, so it is trivially Lipschitz continuous. For the KL-divergence case, the constraint $g_1$ is also Lipschitz continuous because $k$ is bounded from below by Assumption \ref{asmp:tau}. 
Assumption \ref{asmp:lipschitz_sge_finite_mgf} (b) is a standard assumption on the moment generating function for the large deviation theorem. Then, we can show the exponential convergence rate.
\begin{propositionAp}
    \label{prop:exponential_convergence}
    Suppose Assumption \ref{asmp:smooth_regularity}, \ref{asmp:sge_bound_integrable}, \ref{asmp:lipschitz_sge_finite_mgf} hold. Then, the following statements hold.
    \begin{enumerate}[(a)]
        \item For sufficiently small $\epsilon > 0$, there exists positive constants $\delta_1 (\epsilon)$ and $\delta_2 (\epsilon)$, which are independent of $N$, such that 
        \begin{equation}
            \label{eq:exp_converge_psi}
            \mathbb{P} \left\{
                \sup_{\BFw \in \mathcal{W}} \left\| \hat{\BFPsi}_N(\BFw) - \BFPsi(\BFw) \right\| \geq \epsilon
                \right\} \leq \delta_1(\epsilon) \exp(-\delta_2(\epsilon) N).
        \end{equation}
        
        \item Define the function 
        $$
            \rho(\epsilon) = \inf_{\BFw \in \mathcal{W}, \BFgamma \in \BFGamma(\BFw), \|\BFw - \BFw^*\| \geq \epsilon} \| \BFPsi(\BFw) + \BFgamma \|.
        $$
        Then, for sufficiently small $\epsilon > 0$, we have 
        $$
            \mathbb{P} \left\{
                \left\|\hat{\BFw}_N - \BFw^* \right\| \geq \epsilon
            \right\} \leq \delta_1(\rho(\epsilon)) \exp(-\delta_2(\rho(\epsilon)) N).
        $$
    \end{enumerate}
\end{propositionAp}

Proposition \ref{prop:exponential_convergence} shows that both $\hat{\BFPsi}_N(\BFw)$ and $\hat{\BFw}_N$ converge to $\BFPsi(\BFw)$ and $\BFw^*$ exponentially fast. This is impressive as it indicates that a relatively small sample size can already lead to a very good estimation. The exponential convergence of $\hat{\BFw}_N$ to $\BFw^*$ is sound as $\BFw$ includes the fragility $k$ and the weight matrix $\BFB$, which ensures the model trained on the finite dataset can be very close to the model under the true distribution. We also highlight the exponential convergence of $\hat{\BFPsi}_N(\BFw)$ because $\hat{\BFPsi}_N(\BFw)$ is the gradient of the Lagrangian function. During the iterative update with batch gradient, the exponential convergence of $\hat{\BFPsi}_N(\BFw)$ provides a theoretical guarantee for the gradient estimation when the batch size is relatively limited. 

% On top of Proposition \ref{prop:exponential_convergence}, we further consider the training loss and other performance metrics depending on $\BFw$. 
% Notice that Assumption \ref{asmp:lipschitz_sge_finite_mgf} implies that the loss function $\ell$ is Lipschitz continuous. There exists $\omega_1$ and $\omega_2$ such that Assumption \ref{asmp:lipschitz} holds. Then, we can have 
% \begin{propositionAp}
%     \label{prop:exponential_convergence_loss}
%     Under Assumption \ref{asmp:light_tail}, \ref{asmp:smooth_regularity}, \ref{asmp:sge_bound_integrable} and \ref{asmp:lipschitz_sge_finite_mgf}, and for sufficiently small $\epsilon > 0$, there exists positive constants $\delta_{\ell, 1} (\epsilon)$ and $\delta_{\ell, 2} (\epsilon)$, which are independent of $N$, such that
%     $$
%         \mathbb{P} \left\{
%             \left|\mathbb{E}_{\ptrue}[\ell(\BFB^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)] \right| \geq \epsilon
%         \right\} \leq \delta_{\ell, 1}(\epsilon) \exp(-\delta_{\ell, 2}(\epsilon) N).
%     $$
%     % $\mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N \BFphi, y)]$ converges to $\mathbb{E}_{\ptrue}[\ell(\BFB^* \BFphi, y)]$ exponentially fast with $N$.
% \end{propositionAp}
% Proposition \ref{prop:exponential_convergence_loss} extends the exponential convergence of $\hat{\BFw}_N$ to the training loss. We highlight the proof of Proposition \ref{prop:exponential_convergence_loss} is universal and can be extended to any Lipschitz metric depending on $\BFw$. 

Our analysis so far shows that if the model admits a convex reformulation, we can establish the exponential convergence from the empirical model to the model under the true distribution with various mild assumptions. The conclusions are independent of any parameters such as the target $\tau$ and the regularization $R(\BFB)$. However, to further understand the optimal solution $k^*$ and $\BFB^*$ of the problem \eqref{eq:prob_true}, the target $\tau$ is the most important meta-parameter. For more discussion on how the target $\tau$ could affect the statistical properties of the model, we refer to \cite{li2024statistical}.

\subsection{Connection with DRO}
\label{appe:connection_dro}
This section is devoted to uncovering the connection between our FI-based training framework and the conventional DRO framework. As mentioned by \cite{long2023robust}, DRO and RS are closely related. We first reimplement the main reformulation results in the DRO framework for comparison. Then, we also propose a way to integrate the RS and DRO framework to counter the potential over-conservativeness. However, we highlight that the close connection with DRO does not undermine our contribution. The DRO results are also built on the new reformulation techniques developed in our paper. Moreover, starting from FI, we propose diverse insights about risk and generalization beyond the conventional interpretation of DRO.

\subsubsection{DRO results}
Regarding the reformulation procedure, the two frameworks are quite similar and share the same key steps of addressing the c-transformed loss function Proposition \ref{prop:convex_ot_reformulation}. The main difference lies in the uncertainty control parameter, where DRO uses the radius $\epsilon$ to control the uncertainty size, while our FI-based training uses the FI $k$ to control the fragility. Considering the similarity, we mainly reimplement the main reformulation results in the DRO framework for comparison.

Since literature has already investigated much about DRO reformulation, we mainly focus on novel results and give reference to some existing results. For the KL-divergence, we refer to \cite{hu2013kullback}. For the Wasserstein distance with piecewise convex loss, the reformulation refers to Theorem 4.2 of \cite{mohajerin2018data}. These are existing results. Then, we turn to the novel cross-entropy loss and hinge-surrogate loss. Let $\mathcal{P}(\phat, \epsilon)$ be the ambiguity set in the DRO framework, which is a ball centered at $\phat$ with radius $\epsilon$ under the Wasserstein distance $D_{\mathrm{W}}$. DRO minimizes the worst-case expected loss upon an ambiguity set $\mathcal{P}(\phat, \epsilon)$ as 
\begin{equation*}
    % \label{eq:dro}
    \begin{aligned}
        &\phantom{=} \min_{\BFB \in \mathcal{B}} \sup_{\mathbb{P}\in \mathcal{P}(\phat, \epsilon)} \mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)] + R(\BFB)\\
        &= \min_{\BFB \in \mathcal{B}} \sup_{\mathbb{P}\in \mathcal{P}(\phat, \epsilon)} \min_{\lambda \geq 0} \mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)] + R(\BFB) + \lambda \left( \epsilon - D_{\mathrm{W}}(\mathbb{P}, \phat) \right)\\
        &= \min_{\BFB \in \mathcal{B}, \lambda \geq 0} \lambda\epsilon + R(\BFB) + \sup_{\mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)] - \lambda D_{\mathrm{W}}(\mathbb{P}, \phat) \\
        &= \min_{\BFB \in \mathcal{B}, \lambda \geq 0} \lambda\epsilon + R(\BFB) + \frac{1}{N} \sum_{n \in [N]} \sup_{y_n \in \mathcal{Y}} \left\{ \sup_{\BFphi \in \Phi} \left\{ \ell(\BFB^T\BFphi, y_n) - \lambda c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} - \lambda \gamma \mathbb{I}(y_n \neq \hat{y}_n) \right\}.
    \end{aligned}
\end{equation*}
Notice that the reformulation is similar to the FI-based training if replacing the dual variable $\lambda$ with the fragility $k$.

 
\begin{propositionAp}
    \label{prop:dro_reformulation}
    \begin{enumerate}[(a)]
        \item Consider the cross-entropy loss $\ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) 
        = \ln \left(\sum_{i \in [C]} \exp(\BFbeta_i^T \hat{\BFphi}_n)\right) - \BFe_{\hat{y}_n}^T \BFB \hat{\BFphi}_n$. The DRO problem with 1-Wasserstein distance can be reformulated as
        \begin{equation*}
            \label{eq:cross_entropy_reformulation_dro}
            \begin{aligned}
                \min_{\lambda\geq0, \BFB \in \mathcal{B}} &\ \lambda\epsilon + R(\BFB) + \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + \lambda (\|\hat{\BFphi}_n\| - \gamma)_+ \\
                \text{s.t.} \hspace{10pt} &\ \|\BFbeta_i - \BFbeta_j\|_* \leq \lambda, \ \forall i, j \in [C] \ \text{and}\ i < j.
            \end{aligned}
        \end{equation*}
        When $\gamma \geq \max_{n\in[N]} \|\hat{\BFphi}_n\|$, the reformulation is equivalent.

        \item Consider the hinge-type loss $\ell(\BFB^T\BFphi, y) = \max_{y' \neq y} \rho((\BFbeta_{y} - \BFbeta_{y'})^T \BFphi)$. Suppose the surrogate loss function $\rho$ is convex and subdifferentiable, and $\sup_{u\in\mathbb{R}} \partial \rho(u) = 0$ and $\inf_{u\in\mathbb{R}} \partial \rho(u) = - \theta$. The DRO problem with 1-Wasserstein distance can be reformulated as
        \begin{equation*}
            \label{eq:hinge_reformulation_dro}
            \begin{aligned}
                \min_{\lambda\geq0, \BFB \in \mathcal{B}} &\ \lambda\epsilon + R(\BFB) + \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + \lambda (2 \|\hat{\BFphi}_n\| - \gamma)_+ \\
                \text{s.t.} \hspace{10pt} &\ \|\BFbeta_i - \BFbeta_j\|_* \leq \frac{\lambda}{\theta}, \ \forall i, j \in [C] \ \text{and}\ i < j.
            \end{aligned}
        \end{equation*}
        When $\gamma \geq 2 \max_{n\in[N]} \|\hat{\BFphi}_n\|$, the reformulation is equivalent.
    \end{enumerate}
\end{propositionAp}
Proposition \ref{prop:dro_reformulation} demonstrates the close relation between the FI-based reformulation and the DRO reformulation. The structure of regulating the weight difference remains the same in the two frameworks. The difference lies in the objective function, where the FI-based training minimizes the fragility $k$ while the DRO minimizes the worst-case expected loss.

\subsubsection{Incoporate DRO and RS by shrinking ambiguity}
A constant concern about the model with distributional ambiguity is the over-conservativeness. From the perspective of DRO, the ambiguity radius $\epsilon$ is a hyperparameter that controls the trade-off between the robustness and the in-sample performance. To counter the potential over-conservativeness, the value of $\epsilon$ should be carefully tuned. In RS and FI-based training, we allow a much larger space of ambiguity, but consider the fragility $k$, which is the target violation divided by the statistical distance. The fragility $k$ may not suffer from the expansion of the ambiguity set because it is normalized by the statistical distance. However, in case the ambiguity set may still be too large, we can control the ambiguity in FI-based training directly by integrating the RS and DRO framework. 

Instead of allowing the distribution $\mathbb{P}$ to be arbitrary in the general ambiguity set $\mathcal{P}(\mathit{\Phi}, \mathcal{Y})$, we can restrict the ambiguity set to be the same as $\mathcal{P}(\phat, \epsilon)$ in DRO. Then, our problem becomes
\begin{equation}
    \label{eq:fi_dro}
    \begin{aligned}
        \min_{k, \BFB} & \ k\\
		\text{s.t.}\  &\ \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^T\BFphi, y) \right] + R(\BFB)\leq \tau + k D_{\mathrm{W}} (\mathbb{P},\hat{\mathbb{P}}), &\ \forall \mathbb{P}\in \mathcal{P}(\phat, \epsilon),\\
		& k\geq 0, \BFB \in \mathcal{B},
    \end{aligned}
\end{equation}
Then, the key part in the reformulation is to address
\begin{equation*}
    \label{eq:dro}
    \begin{aligned}
        &\phantom{=} \sup_{\mathbb{P}\in \mathcal{P}(\phat, \epsilon)} \mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)] - kD_{\mathrm{W}}(\mathbb{P}, \phat)\\
        &= \sup_{\mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \min_{\lambda \geq 0} \mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)] - kD_{\mathrm{W}}(\mathbb{P}, \phat) + \lambda \left( \epsilon - D_{\mathrm{W}}(\mathbb{P}, \phat) \right)\\
        &= \min_{\lambda \geq 0} \lambda\epsilon + \sup_{\mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})} \mathbb{E}_{\mathbb{P}}[\ell(\BFB^T\BFphi, y)] - (\lambda + k) D_{\mathrm{W}}(\mathbb{P}, \phat) \\
        &= \min_{\lambda \geq 0} \lambda\epsilon + \frac{1}{N} \sum_{n \in [N]} \sup_{y_n \in \mathcal{Y}} \left\{ \sup_{\BFphi \in \Phi} \left\{ \ell(\BFB^T\BFphi, y_n) - (\lambda + k) c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} - (\lambda + k) \gamma \mathbb{I}(y_n \neq \hat{y}_n) \right\}.
    \end{aligned}
\end{equation*}
Due to the close connection between the FI-based training and the DRO, their integration is straightforward and the reformulation is also similar. For example, we can reimplement the results of cross-entropy loss and hinge-surrogate loss as 
\begin{propositionAp}
    \label{prop:dro_fi_reformulation}
    \begin{enumerate}[(a)]
        \item Consider the cross-entropy loss $\ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) 
        = \ln \left(\sum_{i \in [C]} \exp(\BFbeta_i^T \hat{\BFphi}_n)\right) - \BFe_{\hat{y}_n}^T \BFB^T \hat{\BFphi}_n$. The problem \eqref{eq:fi_dro} with 1-Wasserstein distance can be reformulated as
        \begin{equation*}
            \label{eq:cross_entropy_reformulation_dro_fi}
            \begin{aligned}
                \min_{k, \lambda\geq0, \BFB \in \mathcal{B}} & \ k \\
                \text{s.t.} \hspace*{10pt} & \lambda \epsilon + \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + (k + \lambda) (\|\hat{\BFphi}_n\| - \gamma)_+ + R(\BFB) - \tau \leq 0,\\
                & \|\BFbeta_i - \BFbeta_j\|_* \leq k + \lambda, \ \forall i, j \in [C] \ \text{and}\ i < j.
            \end{aligned}
        \end{equation*}
        When $\gamma \geq \max_{n\in[N]} \|\hat{\BFphi}_n\|$, the reformulation is equivalent.

        \item Consider the hinge-type loss $\ell(\BFB^T\BFphi, y) = \max_{y' \neq y} \rho((\BFbeta_{y} - \BFbeta_{y'})^T \BFphi)$. Suppose the surrogate loss function $\rho$ is convex and subdifferentiable, and $\sup_{u\in\mathbb{R}} \partial \rho(u) = 0$ and $\inf_{u\in\mathbb{R}} \partial \rho(u) = - \theta$. The problem \eqref{eq:fi_dro} with 1-Wasserstein distance can be reformulated as
        \begin{equation*}
            \label{eq:hinge_reformulation_dro_fi}
            \begin{aligned}
                \min_{k, \lambda\geq0, \BFB \in \mathcal{B}} & \ k \\
                \text{s.t.} \hspace*{10pt} & \lambda \epsilon + \frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + (k + \lambda) (2\|\hat{\BFphi}_n\| - \gamma)_+ + R(\BFB) - \tau \leq 0,\\
                & \|\BFbeta_i - \BFbeta_j\|_* \leq \frac{k + \lambda}{\theta}, \ \forall i, j \in [C] \ \text{and}\ i < j.
            \end{aligned}
        \end{equation*}
        When $\gamma \geq 2 \max_{n\in[N]} \|\hat{\BFphi}_n\|$, the reformulation is equivalent.
    \end{enumerate}
\end{propositionAp}

Another important implication of the restricted FI-based model \eqref{eq:fi_dro} is related to the generalization guarantee. Even though Lemma \ref{lemma:generalization} is tight in the sense that there always exists a distribution $\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ such that the bound in Lemma \ref{lemma:generalization} turns out to be equal. However, one concern is that $\mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ is too broad such that the bound of Lemma \ref{lemma:generalization} is too loose in practical use. Naturally, shrinking the $\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})$ to $\mathcal{P}(\phat, \epsilon)$ is helpful to promote the generalization guarantee. Still, let $k^*_N$ and $\BFB^*_N$ denote the optimal solution of the problem \eqref{eq:prob_general} with $N$ samples. Let $k^\dagger_{N, \epsilon}$ denote the optimal solution of the problem \eqref{eq:fi_dro} given $\BFB = \BFB^*_N$. We have the following result. 
\begin{propositionAp}
    \label{prop:generalization_fi_dro}
    We have $k^*_N \geq k^\dagger_{N, \epsilon}$ for any $N$. 
    For sufficiently small $\delta$ and $\epsilon > (1+\gamma) \left(
                \frac{1}{C_2 N} \log \left(\frac{C_1}{\delta}\right)
            \right)^{\frac{1}{M+1}}$, with probability at least $1 - \delta$ with respect to the random sampling of the training data, we have
    \begin{equation}
        \label{eq:generalization_error_bound_fi_dro}
        \begin{aligned}
            & \mathbb{E}_{\ptrue}[\ell (\BFB^{*T}_N\BFphi, y)] - \mathbb{E}_{\phat_N}[\ell (\BFB^{*T}_N\BFphi, y)] \leq 
            \min \left\{
                (1+\gamma)\omega \|\hat{\BFB}_N^{*T}\|_* \left(
                \frac{1}{C_2 N} \log \left(\frac{C_1}{\delta}\right)
            \right)^{\frac{1}{M+1}}, \right.\\
            &\hspace{150pt} \left.
            \tau - \mathbb{E}_{\phat_N}[\ell (\BFB^{*T}_N\BFphi, y)] + (1+\gamma) k^\dagger_{N, \epsilon} \left(
                \frac{1}{C_2 N} \log \left(\frac{C_1}{\delta}\right)
            \right)^{\frac{1}{M+1}}
            \right\},
        \end{aligned}
    \end{equation}
\end{propositionAp}
Since $k^*_N \geq k^\dagger_{N, \epsilon}$, the bound \eqref{eq:generalization_error_bound_fi_dro} is better than the bound in Proposition \ref{prop:generalization_error} in constant factors.

% \subsection{Value of randomization}
% \label{appe:randomized_policy}
% % \rzcomment{may consider making it as a standalone section and adding simulations to justify the Proposition \ref{prop:randomization}.} \cycomment{I put this part in the extension. For simulation, I think probably no need. The results is not that exciting. If we consider the convex loss, the optimal solution is unique, so the randomization is useless. }
% Regarding the employment of the randomized policy, we have two motivations. First, the PAC-Bayes bound, one of the key theoretical generalization guarantees for machine learning models like the neural network, considers the aggregation of the randomized predictions (\cite{alquier2021user}). With the help of the randomized policy, PAC-Bayes bound depicts the relation between the empirical risk and the risk under the true distribution. Second, the DRO and RS problem can be regarded as a game between nature and the decision-maker. This makes it possible for the randomized policy to bring more value to the final pay-off since nature has to consider all possible actions \citep{delage2019dice}. Therefore, it is also interesting to quantify whether the value of the randomized policy can emerge. 

% Let $\mathbb{F}$ denote the distribution of the weight matrix $\BFB$. Then, the randomized problem is 
% % Let $p_1, \dots, p_r$ denote the probability of each policy, and $B_i$ is the corresponding weight matrix. Then, the randomized policy is defined as
% \begin{equation}\label{eq:prob_random_weight}
% 	\begin{aligned}
% 		\min_{k, \mathbb{F}} & \ k\\
% 		\text{s.t.}\  &\ \mathbb{E}_{\BFB \sim \mathbb{F}} \left[\mathbb{E}_{(\BFphi, y) \sim \mathbb{P}}\left[\ell(\BFB^T\BFphi, y)\right] + R(\BFB) \right]\leq \tau + k D (\mathbb{P},\hat{\mathbb{P}}), &\ \forall \mathbb{P}\in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}),\\
% 		& k\geq 0, \supp{\mathbb{F}} \subseteq \mathcal{\BFB}.
% 	\end{aligned}
% \end{equation}
% % \rzcomment{Are we dropping the regularization term $R(\BFB)$ here?} \cycomment{Typo.}

% Following the terminology in \cite{delage2019dice}, if the randomized policy can bring more value, we say it is randomization-receptive; if the randomized policy is no better than a deterministic policy, we say it is randomization-proof. It turns out this highly depends on the convexity of the loss function $\ell$, and for the convex loss function, our results are negative. 
% \begin{propositionAp}
%     \label{prop:randomization}
%     If the loss function $\ell$ is convex, the problem \eqref{eq:prob_random_weight} is randomization-proof. If the loss function $\ell$ is concave, the problem \eqref{eq:prob_random_weight} is randomization receptive.
% \end{propositionAp}

% Since the loss function is usually convex, Proposition \ref{prop:randomization} simply suggests that the randomized policy is no better than the deterministic policy. This is also consistent with the conclusion in \cite{delage2019dice}, as here we consider the expectation of the loss function $\mathbb{E}_{\mathbb{P}}[\ell]$, instead of an ambiguity risk measure, so randomization is not a good choice. 


\section{Supplementary Experiments}
\label{appe:experiment}
\subsection{Synthetic data}
% As mentioned, we first show the statistical significance of the results of the binary classification case on synthetic data. The training and evaluation process is exactly the same. We plot out the confidence intervals of the results by the $95\%$ bootstrap confidence intervals. For simplicity, we only show the results with target ratior $\lambda = 0.1$. The results are shown in Figure \ref{fig:synthetic_2_significance}.
% \begin{figure}[htbp]
%     \centering
%     \subfloat[Sample size v.s. Accuracy]{\includegraphics[width=0.32\linewidth]{synthetic_2_size_acc.pdf}}
%     \subfloat[Sample size v.s. AUC]{\includegraphics[width=0.32\linewidth]{synthetic_2_size_auc.pdf}}
%     \subfloat[Sample size v.s. FI]{\includegraphics[width=0.32\linewidth]{synthetic_2_size_fi.pdf}} 

%     \subfloat[$p_{flip}$ v.s. Accuracy]{\includegraphics[width=0.32\linewidth]{synthetic_2_flip_acc.pdf}}
%     \subfloat[$p_{flip}$ v.s. AUC]{\includegraphics[width=0.32\linewidth]{synthetic_2_flip_auc.pdf}}
%     \subfloat[$p_{flip}$ v.s. FI]{\includegraphics[width=0.32\linewidth]{synthetic_2_flip_fi.pdf}}
%     \caption{The confidence intervals of the results on synthetic data when the target ratio is $0.1$.}
%     \label{fig:synthetic_2_significance}
% \end{figure}
% Notice that the average results of the Wasserstein model are usually outside the error bands of the other two models, which indicates the statistical significance of the results. For the KL-divergence model, it overlaps with the ERM model in accuracy. For the AUC, it becomes significantly better when $p_{flip}$ is large. For the FI, it is slightly better than the ERM model, but the difference is not very significant. 

As mentioned, we also conduct experiments on the multi-classification case. We consider similar Gaussian clusters as the binary case, but we generate 4 classes. Following the same procedure of training and evaluation, we show the results of the multi-classification case on synthetic data. The results are shown in Figure \ref{fig:synthetic_4_class}.

Most information of the results Figure \label{fig:synthetic_4_class} is the same as the 2-class case. The difference is mainly in the performance of the Wasserstein model as $p_{flip}$ changes. However, in the 4-class case, the Wasserstein model performs worse than ERM when $p_{flip}$ is very large. To our analysis, this is due to the bias caused by the underfitting of the Wasserstein model. Since the 4-class problem is more complicated, it is much easier for the Wasserstein model to underfit the data. Therefore, the appropriate target ratio in this case should be even $\lambda \leq 1.05$ to avoid the underfitting of the Wasserstein model. However, when $p_{flip}$ is large, the training and testing sets are already very distinct, so the classification problem itself may not be meaningful.
\begin{figure}[htbp]
    \centering
    \subfloat[Sample size v.s. Accuracy]{\includegraphics[width=0.32\linewidth]{synthetic_4_size_acc_all.pdf}}
    \subfloat[Sample size v.s. AUC]{\includegraphics[width=0.32\linewidth]{synthetic_4_size_auc_all.pdf}}
    \subfloat[Sample size v.s. FI]{\includegraphics[width=0.32\linewidth]{synthetic_4_size_fi_all.pdf}}

    \subfloat[$p_{flip}$ v.s. Accuracy]{\includegraphics[width=0.32\linewidth]{synthetic_4_flip_acc_all.pdf}}
    \subfloat[$p_{flip}$ v.s. AUC]{\includegraphics[width=0.32\linewidth]{synthetic_4_flip_auc_all.pdf}}
    \subfloat[$p_{flip}$ v.s. FI]{\includegraphics[width=0.32\linewidth]{synthetic_4_flip_fi_all.pdf}}
    \caption{The results of the 4-class case on synthetic data.}
    \label{fig:synthetic_4_class}
\end{figure}

\subsection{Supplementary real-data experiments}
In this section, we will perform numerical experiments based on real data to show how our fragility index behaves compared with other performance metrics. We select 9 datasets from the UCI Machine Learning Repository \citep{ref_Dua}. We list the information of these 8 datasets in Table \ref{tab:info_data}. 
\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{|c|c|c|c|}
        \hline
        Dataset                                                                                 & Abbr. & Size & Feature \\ \hline
        Breast Cancer Coimbra                                                                   & BCC          & 116  & 10      \\ \hline
        Liver Disorders                                                                         & LD           & 345  & 7       \\ \hline
        ILPD (Indian Liver Patient Dataset)                                                     & ILDP         & 583  & 10      \\ \hline
        Statlog (German Credit Data)                                                            & SG           & 1000 & 20      \\ \hline
        Breast Cancer Wisconsin (Prognostic)                                                    & BCW          & 198  & 34      \\ \hline
        % Statlog (Australian Credit Approval)                                                    & SA           & 690  & 14      \\ \hline
        Diabetes                                                                                & DI           & 768  & 8       \\ \hline
        Ionosphere                                                                              & IO           & 351  & 34      \\ \hline
        \begin{tabular}[c]{@{}c@{}}Connectionist Bench \\ (Sonar, Mines vs. Rocks)\end{tabular} & CB           & 208  & 60      \\ \hline
        % Spambase                                                                                & SP           & 4602 & 57      \\ \hline
    \end{tabular}
    \caption{Information of the 8 datasets from UCI Repository.}
    \label{tab:info_data}
\end{table}
We follow the same training and evaluation process as the heart attack dataset. For simplicity, we only consider the label-flipping rate with $p_{flip} = 0.0$ and $0.1$. The results are shown in Table \ref{tab:results_uci}. 

\begin{table}[htbp]
    \small
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{$\bm{p_{flip} = 0.0}$} & \multicolumn{3}{c|}{$\bm{p_{flip} = 0.1}$}\\
        \cline{3-8}
        &  & \textbf{ACC} & \textbf{AUC} & \textbf{FI} & \textbf{ACC} & \textbf{AUC} & \textbf{FI} \\
        \hline
        \multirowcell{5}{BCC} & ERM & \makecell{0.695\\(0.671, 0.719)} & \makecell{0.789\\(0.766, 0.81)} & \makecell{0.154\\(0.13, 0.18)} & \makecell{0.635\\(0.603, 0.663)} & \makecell{0.717\\(0.69, 0.743)} & \makecell{0.115\\(0.098, 0.131)} \\
         & KL & \textbf{\makecell{0.702\\(0.675, 0.726)}} & \textbf{\makecell{0.789\\(0.766, 0.81)}} & \makecell{0.143\\(0.119, 0.171)} & \textbf{\makecell{0.638\\(0.608, 0.664)}} & \textbf{\makecell{0.719\\(0.689, 0.747)}} & \makecell{0.099\\(0.086, 0.113)} \\
         & Wass & \makecell{0.657\\(0.633, 0.678)} & \makecell{0.759\\(0.737, 0.783)} & \textbf{\makecell{0.111\\(0.095, 0.13)}} & \makecell{0.591\\(0.562, 0.619)} & \makecell{0.675\\(0.644, 0.704)} & \textbf{\makecell{0.079\\(0.068, 0.09)}} \\
        \hline
        \multirowcell{5}{BCW} & ERM & \makecell{0.756\\(0.74, 0.771)} & \makecell{0.645\\(0.595, 0.693)} & \makecell{0.034\\(0.018, 0.052)} & \makecell{0.75\\(0.735, 0.764)} & \makecell{0.611\\(0.56, 0.661)} & \makecell{0.029\\(0.019, 0.042)} \\
         & KL & \makecell{0.758\\(0.741, 0.772)} & \textbf{\makecell{0.752\\(0.727, 0.779)}} & \textbf{\makecell{0.032\\(0.019, 0.049)}} & \makecell{0.75\\(0.735, 0.764)} & \textbf{\makecell{0.711\\(0.676, 0.744)}} & \makecell{0.028\\(0.019, 0.039)} \\
         & Wass & \textbf{\makecell{0.758\\(0.743, 0.774)}} & \makecell{0.655\\(0.634, 0.677)} & \makecell{0.032\\(0.022, 0.045)} & \textbf{\makecell{0.758\\(0.742, 0.775)}} & \makecell{0.637\\(0.609, 0.665)} & \textbf{\makecell{0.025\\(0.021, 0.032)}} \\
        \hline
        \multirowcell{5}{ILDP} & ERM & \textbf{\makecell{0.712\\(0.703, 0.721)}} & \textbf{\makecell{0.691\\(0.669, 0.71)}} & \textbf{\makecell{0.013\\(0.013, 0.013)}} & \textbf{\makecell{0.712\\(0.703, 0.722)}} & \textbf{\makecell{0.703\\(0.683, 0.72)}} & \textbf{\makecell{0.013\\(0.013, 0.013)}} \\
         & KL & \makecell{0.712\\(0.704, 0.721)} & \makecell{0.678\\(0.658, 0.696)} & \makecell{0.013\\(0.013, 0.013)} & \makecell{0.712\\(0.703, 0.722)} & \makecell{0.68\\(0.661, 0.696)} & \makecell{0.013\\(0.013, 0.013)} \\
         & Wass & \makecell{0.712\\(0.703, 0.72)} & \makecell{0.568\\(0.551, 0.584)} & \makecell{0.033\\(0.032, 0.034)} & \makecell{0.712\\(0.703, 0.721)} & \makecell{0.556\\(0.539, 0.573)} & \makecell{0.035\\(0.035, 0.036)} \\
        \hline
        \multirowcell{5}{SG} & ERM & \makecell{0.757\\(0.75, 0.764)} & \makecell{0.791\\(0.784, 0.798)} & \makecell{0.173\\(0.163, 0.183)} & \makecell{0.727\\(0.716, 0.739)} & \makecell{0.762\\(0.747, 0.776)} & \makecell{0.091\\(0.072, 0.11)} \\
         & KL & \textbf{\makecell{0.759\\(0.751, 0.766)}} & \textbf{\makecell{0.792\\(0.785, 0.799)}} & \makecell{0.162\\(0.154, 0.171)} & \textbf{\makecell{0.728\\(0.717, 0.74)}} & \textbf{\makecell{0.764\\(0.752, 0.776)}} & \makecell{0.095\\(0.079, 0.112)} \\
         & Wass & \makecell{0.72\\(0.712, 0.727)} & \makecell{0.787\\(0.782, 0.793)} & \textbf{\makecell{0.083\\(0.074, 0.093)}} & \makecell{0.708\\(0.701, 0.715)} & \makecell{0.729\\(0.719, 0.74)} & \textbf{\makecell{0.047\\(0.042, 0.053)}} \\
        \hline
        \multirowcell{5}{LD}& ERM & \textbf{\makecell{0.681\\(0.669, 0.692)}} & \textbf{\makecell{0.72\\(0.706, 0.734)}} & \makecell{0.193\\(0.177, 0.211)} & \textbf{\makecell{0.626\\(0.605, 0.645)}} & \textbf{\makecell{0.654\\(0.631, 0.676)}} & \makecell{0.121\\(0.099, 0.142)} \\
         & KL & \makecell{0.674\\(0.662, 0.687)} & \makecell{0.715\\(0.702, 0.73)} & \makecell{0.161\\(0.147, 0.177)} & \makecell{0.624\\(0.604, 0.644)} & \makecell{0.643\\(0.617, 0.666)} & \makecell{0.103\\(0.084, 0.12)} \\
         & Wass & \makecell{0.59\\(0.57, 0.611)} & \makecell{0.689\\(0.671, 0.706)} & \textbf{\makecell{0.084\\(0.074, 0.095)}} & \makecell{0.581\\(0.564, 0.599)} & \makecell{0.576\\(0.545, 0.605)} & \textbf{\makecell{0.057\\(0.047, 0.067)}} \\
        \hline
        % \multirowcell{5}{SA} & ERM & \makecell{0.853\\(0.845, 0.861)} & \makecell{0.919\\(0.911, 0.925)} & \makecell{0.113\\(0.108, 0.119)} & \makecell{0.853\\(0.846, 0.861)} & \makecell{0.919\\(0.911, 0.927)} & \makecell{0.115\\(0.109, 0.123)} \\
        %  & KL & \makecell{0.853\\(0.845, 0.861)} & \textbf{\makecell{0.923\\(0.916, 0.931)}} & \makecell{0.113\\(0.108, 0.119)} & \textbf{\makecell{0.854\\(0.847, 0.861)}} & \textbf{\makecell{0.92\\(0.913, 0.927)}} & \makecell{0.113\\(0.108, 0.119)} \\
        %  & Wass & \textbf{\makecell{0.854\\(0.846, 0.861)}} & \makecell{0.915\\(0.907, 0.923)} & \textbf{\makecell{0.111\\(0.107, 0.114)}} & \makecell{0.854\\(0.846, 0.861)} & \makecell{0.913\\(0.905, 0.921)} & \textbf{\makecell{0.112\\(0.107, 0.114)}} \\
        % \hline
        \multirowcell{5}{DI} & ERM & \textbf{\makecell{0.775\\(0.767, 0.782)}} & \makecell{0.83\\(0.822, 0.838)} & \makecell{0.226\\(0.21, 0.241)} & \textbf{\makecell{0.772\\(0.764, 0.781)}} & \makecell{0.827\\(0.817, 0.836)} & \makecell{0.185\\(0.172, 0.2)} \\
         & KL & \makecell{0.772\\(0.765, 0.78)} & \textbf{\makecell{0.831\\(0.822, 0.839)}} & \makecell{0.207\\(0.193, 0.223)} & \makecell{0.772\\(0.764, 0.78)} & \textbf{\makecell{0.828\\(0.819, 0.837)}} & \makecell{0.17\\(0.158, 0.183)} \\
         & Wass & \makecell{0.772\\(0.765, 0.779)} & \makecell{0.829\\(0.82, 0.837)} & \textbf{\makecell{0.154\\(0.142, 0.166)}} & \makecell{0.725\\(0.713, 0.736)} & \makecell{0.815\\(0.805, 0.825)} & \textbf{\makecell{0.116\\(0.107, 0.126)}} \\
        \hline
        \multirowcell{5}{IO}& ERM & \makecell{0.867\\(0.856, 0.879)} & \makecell{0.899\\(0.885, 0.914)} & \makecell{0.103\\(0.088, 0.12)} & \makecell{0.851\\(0.836, 0.863)} & \makecell{0.886\\(0.873, 0.9)} & \makecell{0.082\\(0.074, 0.09)} \\
         & KL & \textbf{\makecell{0.872\\(0.859, 0.884)}} & \textbf{\makecell{0.904\\(0.89, 0.918)}} & \makecell{0.099\\(0.084, 0.117)} & \textbf{\makecell{0.852\\(0.839, 0.865)}} & \makecell{0.889\\(0.876, 0.902)} & \makecell{0.08\\(0.073, 0.087)} \\
         & Wass & \makecell{0.843\\(0.828, 0.856)} & \makecell{0.898\\(0.883, 0.912)} & \textbf{\makecell{0.076\\(0.065, 0.087)}} & \makecell{0.788\\(0.769, 0.805)} & \textbf{\makecell{0.898\\(0.884, 0.911)}} & \textbf{\makecell{0.053\\(0.048, 0.058)}} \\
        \hline
        \multirowcell{5}{CB} & ERM & \makecell{0.743\\(0.729, 0.758)} & \makecell{0.829\\(0.811, 0.844)} & \makecell{0.16\\(0.14, 0.179)} & \makecell{0.732\\(0.714, 0.75)} & \makecell{0.806\\(0.788, 0.822)} & \makecell{0.135\\(0.12, 0.153)} \\
         & KL & \textbf{\makecell{0.748\\(0.731, 0.763)}} & \textbf{\makecell{0.833\\(0.817, 0.849)}} & \makecell{0.152\\(0.133, 0.173)} & \textbf{\makecell{0.74\\(0.725, 0.755)}} & \makecell{0.81\\(0.793, 0.826)} & \makecell{0.124\\(0.111, 0.14)} \\
         & Wass & \makecell{0.74\\(0.724, 0.753)} & \makecell{0.826\\(0.81, 0.843)} & \textbf{\makecell{0.117\\(0.105, 0.131)}} & \makecell{0.714\\(0.688, 0.737)} & \textbf{\makecell{0.811\\(0.793, 0.831)}} & \textbf{\makecell{0.091\\(0.082, 0.1)}} \\
         \hline
    \end{tabular}
    \caption{Results of the ERM, KL-divergence and Wasserstein models on the 8 datasets from UCI Repository.}
    \label{tab:results_uci}
\end{table}
We observe that the KL-divergence model performs best in more cases than the other two models. This demonstrates the effectiveness of a robust reweighting strategy of the KL-divergence model. The Wasserstein model, on the other hand, is best in most FI, but not as good as the other two models regarding accuracy and AUC. This relates to the potential weakness of Wasserstein model as it may be too conservative because of considering the unrealistic support of the feature. 

\subsection{Training details of the FI-based ResNet on MedMNIST}
In this part, we illustrate the details of the hyperparameters and training process of the FI-based ResNet on MedMNIST. The training is conducted by PyTorch on an NVIDIA GeForce RTX 3070. We employ the default ResNet-18 architecture and the Adam optimizer. For the majority of the datasets, the default learning rate is set to $0.001$ and the weight decay, i.e. the L2 regularization, is set to $0.0001$. Both these two values are obtained by rounding from the Bayesian optimization hyperparameter tuning. Only for several extremely large datasets, we shrink the learning rate a little to control the volatility of the loss during the training loss. 

The training epochs are determined by the early stopping strategy, i.e., if the validation loss does not decrease for 30 epochs, the training will stop. Then, we will select the model with the best validation loss during the training process as our final model. For the learning rate, we consider the ``reduce learning rate on plateau'' strategy with the patience of 5 epochs and a factor of 0.1, i.e., if the validation loss does not decrease for 5 epochs, the learning rate will be reduced by a factor of 0.1. 

The above hyperparameters are the same for both ERM and FI-based ResNet. For the FI-based model, we also need to specify the coefficient of the FI-induced regularizer in \eqref{eq:fi_def_loss_nn}. We set the coefficient to $0.1$ in all experiments based on the Bayesian optimization results.

% \newpage
\section{Mathematical Proof}
\label{appe:proof}
\subsection{Proof of Theorem \ref{thm:fi_properties}.}
As mentioned in the main text, the Theorem \ref{thm:fi_properties} is an extension of the Theorem 2 of \cite{long2023robust}. Specifically, the first four properties are extended by incorporating the nonzero $\tau$. Therefore, the proof of positive homogeneity, subadditivity, prorobustness, and antifragility can be directly derived from that in \cite{long2023robust} so omitted. We start with the proof of the $\tau$-FI tradeoff. 

\textbf{$\tau$-FI tradeoff.} For $\tau_1 \geq \tau_2 \geq \mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)] $, we have by the definition of $\tau$-FI that for any $\mathbb{P} \in \mathcal{P}(\mathcal{E})$,
$$
    \mathbb{E}_\mathbb{P} [\varepsilon(h)] \leq \tau_2 + \mathrm{FI}(h; \tau_2) \Delta(\mathbb{P}, \hat{\mathbb{P}}) \leq \tau_1 + \mathrm{FI}(h; \tau_2) \Delta(\mathbb{P}, \hat{\mathbb{P}}).
$$
This means that $\mathrm{FI}(h; \tau_2)$ is always a feasible solution for the optimization problem given $\tau_1$. Therefore, we have $\mathrm{FI}(h; \tau_1) \leq \mathrm{FI}(h; \tau_2)$.

\textbf{Monotonicity} We consider the KL-divergence and Wasserstein distance separately. Suppose the ranking error of the score functions $h_1$ and $h_2$ are $\varepsilon_1$ and $\varepsilon_2$, respectively. Let $\hat{\varepsilon}_1$ and $\hat{\varepsilon}_2$ be the ranking error under the training set and let $\hat{\mathbb{P}}_1$ and $\hat{\mathbb{P}}_2$ denote their distributions respectively. We know that both $\hat{\varepsilon}_1$ and $\hat{\varepsilon}_2$ have $m_\varepsilon = m^+ m^-$ outcomes according to the $m^+$ positive samples and $m^-$ negative samples.

For the KL-divergence defined in equation \eqref{eq:kl_def}, we know that the any distribution $\mathbb{P} \ll \hat{\mathbb{P}}$, we have $\supp{\mathbb{P}} \subseteq \supp{\hat{\mathbb{P}}}$. Therefore, we can depict the distribution $\mathbb{P}$ by a weight vector in $\mathcal{W} = \{\BFw \in \mathbb{R}^{m_\varepsilon} | \sum_{i\in m_\varepsilon} w_i = 1, w_i \geq 0, i\in [m_\varepsilon]\}$. Let $\mathbb{P}_{i,\BFw}$ be the distribution corresponding to the weight vector $\BFw$ and the score function $h_i$. Let $\hat{\mathbb{P}}_1$ and $\hat{\mathbb{P}}_2$ denote the empirical distribution of the ranking error under score function $h_1$ and $h_2$. By the definition, we have $\hat{\mathbb{P}}_i = \mathbb{P}_{i, \frac{\bm{1}}{m_\epsilon}}$, where $\bm{1}$ is the vector whose components are all 1. According to the definition of the KL-divergence, the KL-divergence for two distributions under the same score function only depends on the induced vector. Therefore, given a vector $\BFw\in\mathcal{W}$, we have 
$$
    D_{\mathrm{KL}}(\mathbb{P}_{1,\BFw}\|\hat{\mathbb{P}}_1) = D_{\mathrm{KL}}(\mathbb{P}_{2,\BFw}\| \hat{\mathbb{P}}_2).
$$
For each sample pair $(\mathbf{x}^+_i, \mathbf{x}^-_j)$, we have 
$\hat{\varepsilon}_{1ij} \geq \hat{\varepsilon}_{2ij} $, which implies 
$$
    \mathbb{E}_{\mathbb{P}_{1,\BFw}}[\varepsilon] \geq \mathbb{E}_{\mathbb{P}_{2,\BFw}}[\varepsilon].
$$
Therefore, for any $r_1$ satisfying the constraint in the definition of $\mathrm{FI}_{\mathrm{KL}}(h_1; \tau)$, we have
$$
    \mathbb{E}_{\mathbb{P}_{2,\BFw}}[\varepsilon] \leq \mathbb{E}_{\mathbb{P}_{1,\BFw}}[\varepsilon] \leq \tau + r_1 D_{\mathrm{KL}}(\mathbb{P}_{1,\BFw}\|\hat{\mathbb{P}}_1) = \tau + r_1 D_{\mathrm{KL}}(\mathbb{P}_{2,\BFw}\| \hat{\mathbb{P}}_2).
$$
This implies that $r_1$ is also a feasible solution for the optimization problem of $\mathrm{FI}(h_2; \tau)$, so we have $\mathrm{FI}(h_1; \tau) \geq \mathrm{FI}(h_2; \tau)$.

For the Wasserstein distance defined in equation \eqref{eq:wass_def_in_fi}, we can conduct analogy proof by establishing the map from empirical distribution to any other potential worst-case distribution. We have
\begin{subequations}
\begin{align*}
    &\phantom{=} \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathcal{E})} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon]- k D_{\mathrm{W}} (\mathbb{P},\hat{\mathbb{P}})\right\} \\
    &= \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathcal{E})} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon]- k \inf\limits_{\mathbb{Q}\in \Pi(\mathbb{P},\hat{\mathbb{P}}) } \mathbb{E}_{\mathbb{Q}} \left[|\varepsilon - \hat{\varepsilon}| \right]\right\} \\
    &= \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathcal{E})} \sup\limits_{\mathbb{Q}\in \Pi(\mathbb{P},\hat{\mathbb{P}}) } \mathbb{E}_{\mathbb{Q}}\left[\varepsilon - k |\varepsilon - \hat{\varepsilon}| \right] \\
    &= \mathbb{E}_\mathbb{\hat{P}} \left[\sup\limits_{(\mathbb{Q|\hat{P}})\in (\Pi(\mathbb{P},\hat{\mathbb{P}})|\mathbb{\hat{P} })} \mathbb{E}_{\mathbb{Q|\hat{P}}}\left[\varepsilon - k |\varepsilon - \hat{\varepsilon}| \right] \right]\\
    & = \frac{1}{N} \sum_{i \in [m^+], j \in [m^-]} \sup_{\varepsilon \in \mathcal{E}} \left\{ \varepsilon - k |\varepsilon - \hat{\varepsilon}_{ij}| \right\} \\
    & = \frac{1}{N} \sum_{i \in [m^+], j \in [m^-]} \left\{ \bar{\varepsilon} - k |\bar{\varepsilon} - \hat{\varepsilon}_{ij}|, \hat{\varepsilon}_{ij} \right\} 
\end{align*}
\end{subequations}
Notice that the last step is due to the fact that the maximizer of $\varepsilon - k |\varepsilon - \hat{\varepsilon}_{ij}|$ is either $\bar{\varepsilon}$ or $\hat{\varepsilon}_{ij}$. It also implies that the worst-case conditional distribution of $(\mathbb{Q|\hat{P}})\in (\Pi(\mathbb{P},\hat{\mathbb{P}})|\mathbb{\hat{P} })$ is a single point distribution given $\hat{\varepsilon} = \hat{\varepsilon}_{ij}$ for any outcome. In other words, we know the potential worst-case distribution of $\mathbb{P} = \mathbb{Q|\hat{P}}$ is also a uniform distribution of $m_\varepsilon$ outcomes. 

We can represent the uniform distribution with $m_\varepsilon$ scenarios by a $m_\varepsilon$-dimension vector of all the outcomes. For a empirical distribution vector $\hat{\BFvarepsilon}$, consider 
$$
    \BFvarepsilon = \bar{\varepsilon} \BFone \land ( \hat{\BFvarepsilon} + \BFtheta),
$$
where $\land$ denotes the element-wise minimum, $\BFone$ is the $m_\varepsilon$-dimension vector with all elements being 1, and $\BFtheta \geq \BFzero $. Then, let $\mathbb{P}_\theta$ be the distribution corresponding to $\BFvarepsilon$. Notice that any uniform distribution of $m_\varepsilon$ outcomes larger than $\hat{\BFvarepsilon}$ can be represented by a vector $\BFtheta$. This expression provides us with tools to establish the potential worst-case distribution in the RS model.

Suppose we use two vectors $\hat{\BFvarepsilon}_1, \hat{\BFvarepsilon}_2 \in \mathbb{R}^{m_\varepsilon}$ to represent the ranking error outcomes of the score functions $h_1$ and $h_2$, respectively. Then, we have 
$
    \hat{\BFvarepsilon}_1 \geq \hat{\BFvarepsilon}_2.
$
Then, consider
$$
    \BFvarepsilon_1 = \bar{\varepsilon} \BFone \land (  \hat{\BFvarepsilon}_1 + \BFtheta), \quad \BFvarepsilon_2 = \bar{\varepsilon} \BFone \land  (\hat{\BFvarepsilon}_2 + \BFtheta),
$$
and let $\mathbb{P}_{1,\BFtheta}$ and $\mathbb{P}_{2,\BFtheta}$ be the corresponding distributions induced by $\BFvarepsilon_1$ and $\BFvarepsilon_2$ respectively. Since $\hat{\BFvarepsilon}_1 \geq \hat{\BFvarepsilon}_2$, we have $|\BFvarepsilon_1 - \hat{\BFvarepsilon}_1|_i \leq |\BFvarepsilon_2 - \hat{\BFvarepsilon}_2|_i$ for arbitrary $\BFtheta$ and $i$-th component. Then,
\begin{gather*}
    D_{\mathrm{W}}(\mathbb{P}_{1,\BFtheta}, \hat{\mathbb{P}}_1) = \frac{1}{m_\varepsilon} \| \BFvarepsilon_1 - \hat{\BFvarepsilon}_1 \|_1 \leq \frac{1}{m_\varepsilon} \| \BFvarepsilon_2 - \hat{\BFvarepsilon}_2 \|_1 = D_{\mathrm{W}}(\mathbb{P}_{2,\BFtheta}, \hat{\mathbb{P}}_2), \\
    \mathbb{E}_{\mathbb{P}_{1,\BFtheta}}[\varepsilon] \geq \mathbb{E}_{\mathbb{P}_{2,\BFtheta}}[\varepsilon].
\end{gather*}
Therefore, for any $r_1$ satisfying the constraint in problem \eqref{eq:fi_def} for score function $h_1$, we have
$$
    \mathbb{E}_{\mathbb{P}_{2,\BFtheta}}[\varepsilon] \leq \mathbb{E}_{\mathbb{P}_{1,\BFtheta}}[\varepsilon] \leq \tau + r_1 D_{\mathrm{W}}(\mathbb{P}_{1,\BFtheta}, \hat{\mathbb{P}}_1) \leq \tau + r_1 D_{\mathrm{W}}(\mathbb{P}_{2,\BFtheta}, \hat{\mathbb{P}}_2), \forall \BFtheta \in \mathbb{R}^{m_\epsilon}.
$$
Moreover, there exist $\mathit{\Theta} \subseteq \mathbb{R}^{m_\epsilon}$ such that $\mathcal{P}(\mathcal{E}) \subseteq \{\mathbb{P}_{1,\BFtheta}| \BFtheta \in \mathit{\Theta}\}$ and $\mathcal{P}(\mathcal{E}) \subseteq \{\mathbb{P}_{2,\BFtheta}| \BFtheta \in \mathit{\Theta}\}$.
This implies that $r_1$ is also a feasible solution for the optimization problem of $\mathrm{FI}(h_2; \tau)$, so we have $\mathrm{FI}(h_1; \tau) \geq \mathrm{FI}(h_2; \tau)$.

\hfill \Halmos

\subsection{Proof of Lemma \ref{lemma:root_finding}}
Consider $r_2 > r_1 \geq 0$. Let $ \mathbb{P}_1$ denote the optimal solution of the inner max problem of \eqref{eq:def_gk} with $k = r_1$. So is $ \mathbb{P}_2$ with $k = r_2$. Then, we have
\begin{align*}
    G(r_1) + \tau = \mathbb{E}_{\mathbb{P}_1}[\varepsilon(h)] - r_1 D (\mathbb{P}_1,\hat{\mathbb{P}}) &\geq \mathbb{E}_{\mathbb{P}_2}[\varepsilon(h)] - r_1 D (\mathbb{P}_2,\hat{\mathbb{P}}) \\
    &> \mathbb{E}_{\mathbb{P}_2}[\varepsilon(h)] - r_2 D (\mathbb{P}_2,\hat{\mathbb{P}}) \\
    & = G(r_2) + \tau
\end{align*}
Therefore, $G(k)$ is strictly decreasing with respect to $k$. As a result, the equation $G(k) = 0$ has a unique solution, which is exactly $FI_{\mathrm{KL}}(h;\tau)$.

\hfill \Halmos

\subsection{Proof of Lemma \ref{lemma:fi_kl}.}

According to Theorem \ref{thm:fi_properties}, we know that $\mathbb{E}_{\phat}[\varepsilon(h)] \leq \tau$ and $\sup \{\varepsilon | \varepsilon \in \supp{\phat}\} > \tau$ ensures that the optimal $r^*$ of the above problem is finite. Then,  
$$
    G(r) = 0 \Leftrightarrow \sup\limits_{\mathbb{P}\in \mathcal{P}(\Re)} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon(h)]- r D_{\mathrm{KL}}(\mathbb{P}, \hat{\mathbb{P}})\right\} = \tau. 
$$
When $r>0$, from \cite{ref_Follmer}, we have
\[
\sup\limits_{\mathbb{P}\in \mathcal{P}(\Re)} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon(h)]- r D_{\mathrm{KL}}(\mathbb{P}, \hat{\mathbb{P}})\right\}=r \ln \mathbb{E}_{\hat{\mathbb{P}}}\big[\mathrm{exp}\big(\varepsilon(h)/r\big)\big].
\]
Therefore, 
$$
    G(r) = 0 \Leftrightarrow  \mathbb{E}_{\hat{\mathbb{P}}}[\exp(\varepsilon(h)/r)] - \exp(\tau / r) = 0
$$

% Based on this equation, the equivalence is straightforward as 
% \begin{align*}
%     \mathrm{FI}(h; \tau) &=\inf\left\{ k \geq 0 \middle|  \mathbb{E}_{\mathbb{P}}[\varepsilon(h)]\leq \tau + k \triangle (\mathbb{P},\hat{\mathbb{P}}),\ \forall \mathbb{P}\in \mathcal{P}(\mathcal{E})\right\} \\
%     &=\inf\left\{ k \geq 0 \middle| k \ln \mathbb{E}_{\hat{\mathbb{P}}}\big[\mathrm{exp}\big(\varepsilon(h)/k\big)\big] \leq \tau  \right\} \\
%     &= \inf\left\{ k \geq 0 \middle| \mathbb{E}_{\hat{\mathbb{P}}}\big[\mathrm{exp}\big(\varepsilon(h)/k\big)\big] - \exp(\tau/k) \leq 0 \right\}.
% \end{align*}

% Then, we focus on the convexity of Problem (4).  Define the function $G(\cdot): \Re^{++}\rightarrow \Re$ as $G(k)=\mathbb{E}_{\hat{\mathbb{P}}}[\mathrm{exp}(\varepsilon(h)/k)]$. It suffices to show $\left\{k|G(k)\leq 1\right\}$ is a convex set. 

% Given $\varepsilon_1, \cdots, \varepsilon_I$ as all the samples from the empirical distribution $\hat{\mathbb{P}}$, we have $G(k)=\frac{1}{I}\sum_{i=1}^I \mathrm{exp}(\varepsilon_i/k)$. If $\varepsilon_i\geq 0, \forall i=1,\ldots,I$ or $\varepsilon_i\leq 0, \forall i=1,\ldots,I$, it is easy to see the quasi-convexity of $G(k)$ due to its monotonicity on $k$. Otherwise, assume $\varepsilon_i> 0, i\in I_1$ and $\varepsilon_i\leq0, i\in I_2$, where $I_1$ and $I_2$ are set of index that satisfy $I_1\cup I_2=\{1,\ldots,I\}, I_1\cap I_2=\emptyset$. We have  $\lim\limits_{k\downarrow 0} G(k)=+\infty, \lim\limits_{k\rightarrow +\infty} G(k)=1$ and the derivative of $G(k)$ is

% \[
% G'(k)=\frac{1}{I k^2} \biggl(\sum\limits_{i\in I_1}-|\varepsilon_i|\mathrm{exp}(|\varepsilon_i|/k)+\sum\limits_{i\in I_2}|\varepsilon_i|\mathrm{exp}(-|\varepsilon_i|/k)\biggr).
% \] 
% Then
% \[
%   k^2 G'(k)=\frac{1}{I} \biggl(\sum\limits_{i\in I_1}-|\varepsilon_i|\mathrm{exp}(|\varepsilon_i|/k)+\sum\limits_{i\in I_2}|\varepsilon_i|\mathrm{exp}(-|\varepsilon_i|/k)\biggr).
% \]
% It implies that $k^2 G'(k)$ is increasing on $k$ for $k>0$ and
% \[
%   \lim\limits_{k\downarrow 0} k^2 G'(k)=-\infty,
% \]
% \[
% \lim\limits_{k\rightarrow +\infty} k^2 G'(k)=\frac{1}{I} \sum\limits_{i\in I_1} -|\varepsilon_i|+\sum\limits_{i\in I_2}|\varepsilon_i|=-\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)],
% \]
% which results in two possibilities of $G(k)$'s monotonicity. In the first case, $\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)]>0$,
% which implies $\lim\limits_{k\rightarrow +\infty} k^2 G'(k)< 0$, thus $G'(k)<0$ for $k>0$, $G(k)$ is decreasing. In the second case, $\mathbb{E}_{\hat{\mathbb{P}}}[\varepsilon(h)]\leq 0$,
% which implies $\lim\limits_{k\rightarrow +\infty} k^2 G'(k)\geq 0$. Together with the fact that $\lim\limits_{k\downarrow 0} k^2 G'(k)=-\infty$ and $k^2 G'(k)$ is increasing for $k>0$, we conclude that there exists a unique $k_{min}>0$, such that $G'(k)\leq 0$ in $(0, k_{min})$, $G'(k)> 0$ in $(k_{min}, +\infty)$, then $G(k)$ is decreasing in $(0, k_{min})$, increasing in $(k_{min}, +\infty)$.
% Then for both two cases, we have $G(k)$ being a quasi-convex function.
 
% We thus prove that the feasible region of Problem (3), $\left\{k|G(k)\leq 1\right\}$, is a convex set based on the definition of quasi-convexity, which guarantees that Problem (3) is tractable.

% Let $f(k) = \sup\limits_{\mathbb{P}\in \mathcal{P}(\Re)} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon(h)]- k D_{\mathrm{KL}}(\mathbb{P}, \hat{\mathbb{P}})\right\}$. We now how that $f(k)$ is a decreasing function with respect to $k$. Consider $k_2 > k_1 \geq 0$, and let $\mathbb{P}_1$ and $\mathbb{P}_2$ be the optimal solutions of the inner min-max problem of $f(k)$ with $k = k_1$ and $k = k_2$, respectively. Then, we have
% \begin{align*}
%     f(k_1) &= \mathbb{E}_{\mathbb{P}_1}[\varepsilon(h)] - k_1 D_{\mathrm{KL}}(\mathbb{P}_1,\hat{\mathbb{P}}) \geq \mathbb{E}_{\mathbb{P}_2}[\varepsilon(h)] - k_1 D_{\mathrm{KL}}(\mathbb{P}_2,\hat{\mathbb{P}}) > \mathbb{E}_{\mathbb{P}_2}[\varepsilon(h)] - k_2 D_{\mathrm{KL}}(\mathbb{P}_2,\hat{\mathbb{P}}) = f(k_2).
% \end{align*}
% Therefore, $f(k) =\tau $ has a unique solution, which is the root of $G(k)$ and the optimal $k^*$ of the problem. Moreover, we know that $G(k) \geq 0 $ for $k < k^*$ and $G(k) \leq 0$ for $k > k^*$, which demonstrate the effectiveness of Algorithm \ref{alg:solve_k}.

\hfill \Halmos

\subsection{Proof of Proposition \ref{prop:fi_kl_ub}.}

We denote $\mathrm{FI}_{\mathrm{KL}}(h)$ by $r^*$ for notational simplicity. 
From the definition of $\mathrm{FI}_{\mathrm{KL}}$, we know that 
\[
r^* = \mathrm{FI}_{\mathrm{KL}}(h;\tau) = \inf\left\{r>0 \middle | \mathbb{E}_{\bar{\mathbb{P}}}\left[\exp\left(\frac{\varepsilon(h)}{r}\right)\right]\leq \exp\left(\frac{\tau}{r}\right) \right\}.
\]
Then we have $\mathbb{E}_{\bar{\mathbb{P}}}\left[\exp(\varepsilon(h)/r^*)\right]\leq \exp(\tau/r^*)$.
Given any $\theta$, 
\[
    \begin{array}{ll}
        \displaystyle \bar{\mathbb{P}}(\varepsilon(h)\geq \theta) 
        % & \displaystyle = \bar{\mathbb{P}}\left(\frac{\varepsilon(h)}{r^*}\geq \frac{\theta}{r^*}\right)\\
        & \displaystyle =\bar{\mathbb{P}}\left(\mathrm{exp}\left(\frac{\varepsilon(h)}{r^*}\right)\geq \mathrm{exp}\left(\frac{\theta}{r^*}\right)\right)\\
        & \displaystyle \leq \mathbb{E}_{\bar{\mathbb{P}}}\left[\exp\left(\frac{\varepsilon(h)}{r^*}\right)\right]/\exp\left(\frac{\theta}{r^*}\right)\\
        & \displaystyle \leq \exp\left(\frac{\tau - \theta}{r^*}\right),
    \end{array}
\]
where the first inequality is obtained by applying Markov Inequality.

\subsection{Proof of Corollary \ref{cor:fi_kl_var}.}
According to the definition of VaR, we have 
$$
    \alpha = \mathbb{P}(\varepsilon(h) \geq \mathrm{VaR}_{1 - \alpha}(\varepsilon(h))) \leq \exp\left(\frac{\tau - \mathrm{VaR}_{1 - \alpha}(\varepsilon(h))}{\mathrm{FI}_{\mathrm{KL}}(h;\tau)}\right).
$$
Then, we have 
$$
    \mathrm{VaR}_{1 - \alpha}(\varepsilon(h)) \leq \tau - \mathrm{FI}_{\mathrm{KL}}(h;\tau) \ln \alpha.
$$
\hfill \Halmos




\subsection{Proof of Lemma \ref{lemma:lower_bound_k}}
Without loss of generality, let $\BFB^*$ and $k^*$ be the optimal solution to problem \eqref{eq:prob_general}. For any distribution $ \mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y})$, the constraint in \eqref{eq:prob_general} must be satisfied. Consider the distribution $\mathbb{P} = \arg\sup_{\substack{\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}) \\ D_c (\mathbb{P},\hat{\mathbb{P}}) \leq \epsilon_2}} \ell(\BFB^*\phi, y)$
$$
    k^* \geq \frac{\mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^{*T}\BFphi, y) \right] + R(\BFB^*) - \tau }{D_c (\mathbb{P},\hat{\mathbb{P}})} \geq \frac{\epsilon_1}{\epsilon_2}.
$$
The first inequality is reformulated from the constraint in \eqref{eq:prob_general}. The second inequality is a consequence of 
$$
    \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^{*T}\BFphi, y) \right] + R(\BFB^*) - \tau  \geq  \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^{*T}\BFphi, y) \right] - \inf_{\BFB\in \mathcal{B}} \sup_{\substack{\mathbb{P} \in \mathcal{P}(\mathit{\Phi}, \mathcal{Y}) \\ D_c (\mathbb{P},\hat{\mathbb{P}}) \geq  \epsilon_2}} \mathbb{E}_{\mathbb{P}} \left[\ell(\BFB^{*T}\BFphi, y) \right] + R(\BFB^*) + \epsilon_1 \geq \epsilon_1,
$$
and $D_c (\mathbb{P},\hat{\mathbb{P}}) \leq \epsilon_2$.

\hfill \Halmos

\subsection{Proof of Theorem \ref{thm:kl_reformulation}}
The proof is straightforward by applying the variational formula on 
$$
    \sup_{\mathbb{P} \in \mathcal{\supp{\phat}}} \left\{
        \mathbb{E}_{\mathbb{P}}[\ell \left(\BFB^T\BFphi, y \right)] - k D (\mathbb{P},\hat{\mathbb{P}})
    \right\} = k \sup_{\mathbb{P} \in \mathcal{\supp{\phat}}} \left\{
        \mathbb{E}_{\mathbb{P}}\left[\frac{\ell \left(\BFB^T\BFphi, y \right)}{k}\right] - D (\mathbb{P},\hat{\mathbb{P}})
    \right\} = k \ln \left( \mathbb{E}_{\phat}\left[ \exp \left(\frac{\ell \left(\BFB^T\BFphi, y \right)}{k}\right)\right] \right).
$$
The convexity is guaranteed because it is the composition of the convex function $\ell(\cdot)$ and the perspective of the log-sum-exp function, which is convex and increasing. The worst-case distribution is also provided by \cite{donsker1975asymptotic}. 

\hfill \Halmos

\subsection{Proof of Proposition \ref{prop:kl_training_tail_bound}}

For the ranking error, we have 
\begin{align*}
    \varepsilon(\BFB) &= (\BFbeta_+ - \BFbeta_-)^T \BFphi_- - (\BFbeta_+ - \BFbeta_-)^T  \BFphi_+ \\
    & = (\BFbeta_+ - \BFbeta_-)^T \BFphi_- + (\BFbeta_- - \BFbeta_+)^T  \BFphi_+\\
    & \leq \max\{0, 1 + (\BFbeta_+ - \BFbeta_-)^T \BFphi_-\} + \max\{0, 1 + (\BFbeta_- - \BFbeta_+)^T  \BFphi_+\} \\
    & = \ell(\BFB^T\BFphi_-, -1) + \ell(\BFB^T\BFphi_+, 1).
\end{align*}
For convenience, let $\ell = \ell(\BFB^T\BFphi, y)$, $\ell_+ = \ell(\BFB^T\BFphi_+, 1)$ and $\ell_- = \ell(\BFB^T\BFphi_-, -1)$. Then, we have
\begin{align*}
    & \phantom{=} k \ln \left( \mathbb{E}_{\phat_\varepsilon}\left[ \exp \left(\frac{\epsilon(\BFB)}{k}\right)\right] \right) \\
    & \leq k \ln \left( \mathbb{E}_{\phat_\varepsilon}\left[ \exp \left(\frac{\ell_- }{k}\right) \exp \left(\frac{\ell_+ }{k}\right) \right] \right) \\
    & = k \ln \left( \mathbb{E}_{\phat_{\ell_-}}\left[ \exp \left(\frac{\ell_- }{k}\right) \right] \mathbb{E}_{\phat_{\ell_+}} \left[ \exp \left(\frac{\ell_+ }{k}\right) \right] \right) \\
    & = k \ln \left( \mathbb{E}_{\phat_{\ell_-}}\left[ \exp \left(\frac{\ell_- }{k}\right) \right] \right) + k \ln \left( \mathbb{E}_{\phat_{\ell_+}} \left[ \exp \left(\frac{\ell_+ }{k}\right) \right] \right) 
\end{align*}
Then, consider 
\begin{align*}
    & \phantom{=} \frac{k}{a} \ln \left( \mathbb{E}_{\phat_\ell} \left[ \exp \left(\frac{a \ell}{k}\right)\right] \right) - k \ln \left( \mathbb{E}_{\phat_{\ell_-}}\left[ \exp \left(\frac{\ell_- }{k}\right) \right] \right) - k \ln \left( \mathbb{E}_{\phat_{\ell_+}} \left[ \exp \left(\frac{\ell_+ }{k}\right) \right] \right) \\
    & = k \left(
        \ln N_+ + \ln N_- - \frac{\ln N}{a} + \ln \left(
            \frac{\left(N \mathbb{E}_{\phat_\ell} \left[ \exp \left(\frac{a \ell}{k}\right)\right] \right)^{1/a}} {N_- \mathbb{E}_{\phat_{\ell_-}} \left[ \exp \left(\frac{\ell_- }{k}\right) \right] + N_+ \mathbb{E}_{\phat_{\ell_+}} \left[ \exp \left(\frac{\ell_+ }{k}\right) \right]}
        \right)
    \right) \\
    & = k \left(
        \ln N_+ + \ln N_- - \frac{\ln N}{a} + \ln \left(
            \frac{\left(\sum_{n\in[N]} \left(\exp\left(\frac{\ell_n}{k}\right) \right)^a \right)^{1/a}} {\sum_{n\in[N]} \left(\exp\left(\frac{\ell_n}{k}\right) \right)}
        \right)
    \right) 
\end{align*}
Since $a = \max\{1, \frac{\ln N}{\ln N_+ + \ln N_-}\}$, we have 
$$
    \ln N_+ + \ln N_- - \frac{\ln N}{a} \geq 0.
$$
Since $a \geq 1$, for any vector $\BFx$, we have $\|\BFx\|_a \geq \|\BFx\|_1$, leading to 
$$
    \left(\sum_{n\in[N]} \left(\exp\left(\frac{\ell_n}{k}\right) \right)^a \right)^{1/a} \geq \sum_{n\in[N]} \left(\exp\left(\frac{\ell_n}{k}\right) \right) .
$$
In total, we have
$$
\ln N_+ + \ln N_- - \frac{\ln N}{a} + \ln \left(
    \frac{\left(\sum_{n\in[N]} \left(\exp\left(\frac{\ell_n}{k}\right) \right)^a \right)^{1/a}} {\sum_{n\in[N]} \left(\exp\left(\frac{\ell_n}{k}\right) \right)}
\right) \geq 0.
$$
This leads to $\frac{k}{a} \ln \left( \mathbb{E}_{\phat_\ell} \left[ \exp \left(\frac{a \ell}{k}\right)\right] \right) \geq k \ln \left( \mathbb{E}_{\phat_{\ell_-}}\left[ \exp \left(\frac{\ell_- }{k}\right) \right] \right) + k \ln \left( \mathbb{E}_{\phat_{\ell_+}} \left[ \exp \left(\frac{\ell_+ }{k}\right) \right] \right)$.

Consider the optimal solution $k^*$ and $\BFB^*$ under target $\tau$. We have
\begin{align*}
    & \phantom{=} \tau - R(\BFB^*) \\
    & = k^*  \ln \left( \mathbb{E}_{\phat_{\ell^*}} \left[ \exp \left(\frac{ \ell^*}{k^*}\right)\right] \right) \\
    & \leq a k^* \ln \left( \mathbb{E}_{\phat_{\ell_+^*}} \left[ \exp \left(\frac{ \ell_+^*}{a k^*}\right)\right]\right)  + a k^* \ln \left( \mathbb{E}_{\phat_{\ell_-^*}} \left[ \exp \left(\frac{ \ell_-^*}{a k^*}\right)\right]\right) \\
    & \leq a k^* \ln \left( \mathbb{E}_{\phat_\varepsilon}\left[ \exp \left(\frac{\epsilon(\BFB^*)}{a k^*}\right)\right] \right)
\end{align*}
This means that 
$$
    \mathbb{E}_{\phat_\varepsilon}\left[ \exp \left(\frac{\epsilon(\BFB^*)}{a k^*}\right)\right] - \exp \left(
        \frac{\tau - R(\BFB^*)}{a k^*}
    \right) \leq 0.
$$
According to Lemma \ref{lemma:fi_kl}, we have 
$$
    \mathrm{FI}_{\mathrm{KL}}(\BFB^*; \tau - R(\BFB^*)) \leq a k^*.
$$
Using Proposition \ref{prop:fi_kl_ub}, we have for any $\theta \geq \tau - R(\BFB^*)$
$$
    \phat(\varepsilon(\BFB^*) \geq \theta) \leq \exp\left(- \frac{ \theta - \tau + R(\BFB^*)}{\mathrm{FI}_{\mathrm{KL}}(\BFB^*; \tau - R(\BFB^*))}\right) \leq \exp\left(- \frac{ \theta - \tau + R(\BFB^*)}{a k^*}\right).
$$

% the following the proof of the old tail gurantee
% Similar to the proof of Proposition \ref{prop:fi_kl_ub}, we can obtain the tail bound of the loss function as 
% $$
%     \phat(\ell(\BFB^{*T}\BFphi, y) \geq \theta) \leq \exp\left(- \frac{ \theta - \tau}{k^*}\right).
% $$


% \begin{align*}
%     \phat(\varepsilon(\BFB^*) \geq \theta) &\leq \phat(\ell(\BFB^*\BFphi_-, -1) + \ell(\BFB^*\BFphi_+, 1) \geq \theta) \\
%     & \leq \phat(\ell(\BFB^*\BFphi_-, -1) \geq \theta/2 \ \text{or}\ \ell(\BFB^*\BFphi_+, 1) \geq \theta/2) \\
%     & \leq \phat(\ell(\BFB^*\BFphi_-, -1) \geq \theta/2) + \phat(\ell(\BFB^*\BFphi_+, 1) \geq \theta/2) 
% \end{align*}
% The first and third inequalities are straightforward. The second inequality is due to the fact that if $A + B \geq \theta$, then $A \geq \theta/2$ or $B \geq \theta/2$ must hold. This can be shown by contradiction easily.

% Then, let $K_+, K_-$ be the number of positive and negative samples such that the loss function is larger than $\theta/2$, respectively. We have
% \begin{align*}
%     & \phantom{=}\ a \phat(\ell(\BFB^{*T}\BFphi, y) \geq \theta/2) - \phat(\ell(\BFB^*\BFphi_-, -1) \geq \theta/2) - \phat(\ell(\BFB^*\BFphi_+, 1) \geq \theta/2) \\
%     & = a \frac{K_+ + K_-}{N_+ + N_-} - \frac{K_-}{N_-} - \frac{K_+}{N_+} \\
%     & = \frac{1}{N_+ + N_-} \left( \left(a - 1 - \frac{N_+}{N_-}\right)K_- + \left(a - 1 - \frac{N_-}{N_+}\right)K_+\right) 
% \end{align*}
% Since $a = 1 + \max\{N_+ / N_-, N_- / N_+\}$, we have $a - 1 - N_+ / N_- \geq 0$ and $a - 1 - N_- / N_+ \geq 0$. Therefore, 
% $$
%     a \phat(\ell(\BFB^{*T}\BFphi, y) \geq \theta/2) \geq \phat(\ell(\BFB^*\BFphi_-, -1) \geq \theta/2) + \phat(\ell(\BFB^*\BFphi_+, 1) \geq \theta/2).
% $$
% Then, 
% $$
%     \phat(\varepsilon(\BFB^*) \geq \theta) \leq \phat(\ell(\BFB^*\BFphi_-, -1) \geq \theta/2) + \phat(\ell(\BFB^*\BFphi_+, 1) \geq \theta/2) \leq a \phat(\ell(\BFB^{*T}\BFphi, y) \geq \theta/2) \leq a\exp\left(- \frac{\frac{\theta}{2}  - \tau}{k^*}\right).
% $$
    
\hfill \Halmos


\subsection{Proof of Lemma \ref{lemma:1wass_reformulation}}
For the norm cost, we have $c_{\phi}^{1*}(\BFzeta, \hat{\BFphi}) = \begin{cases}
    \BFzeta^T \hat{\BFphi}, & \|\BFzeta\|_* \leq 1, \\
    +\infty, & \|\BFzeta\|_* > 1.
\end{cases}$. Notice that it is a linear function if $\BFzeta \in \dom{c_{\phi}^{1*}(\cdot, \hat{\BFphi})}$, which creates conditions for a convex reformulation.

Using Proposition \ref{prop:convex_ot_reformulation}, we can have that when $\mathit{\Phi} = \mathbb{R}^M$ 
\begin{align*}
    \ell_c(\BFB, k, \hat{\BFphi}, \hat{y}) & = \sup_{\BFzeta \in \dom{\ell^{1*}}} \left\{
        kc_{\phi}^{1*}((\BFB\BFzeta)/k, \hat{\BFphi}) - \ell^{1*}(\BFzeta, \hat{y}).
    \right\}\\
    & = \begin{cases}
        \sup_{\BFzeta \in \dom{\ell^{1*}}} \left\{
            \BFzeta^T \BFB^T \hat{\BFphi} - \ell^{1*}(\BFzeta, \hat{y})
        \right\} = \ell(\BFB^T\hat{\BFphi}, \hat{y}), & \sup_{\BFzeta \in \dom{\ell^{1*}}}  \|\BFB\BFzeta\|_* \leq k, \\
        +\infty, & \text{otherwise}.
    \end{cases} 
\end{align*}
Notice that the matrix $\BFB$ is feasible to the original problem if and only if $\min_{\BFB \in \mathcal{B}} \frac{1}{N} \sum_{n \in [N]} \ell_c(\BFB, k, \hat{\BFphi}, \hat{y}_n)  + R(\BFB) - \tau \leq 0$. Therefore, we must avoid the case $\|\BFB\BFzeta\|_* > k$ to ensure the feasibility, and this implies the following constraint
$$
    \sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k.
$$
Hence, we can have 
\begin{align*}
    \min_{k\geq0, \BFB \in \mathcal{B}} & \ k \\
    \text{s.t.} \hspace*{10pt} & \frac{1}{N} \sum_{n \in [N]} \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} + R(\BFB) - \tau \leq 0,\\
    & \sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k.
\end{align*}
\hfill \Halmos



\subsection{Proof of Theorem \ref{thm:cross_entropy_reformulation}}
According to (\cite{boyd2004convex} Example 3.25), we can calculate the convex conjugate of the cross-entropy loss given label $y$ as
$$
    \ell_{CE}^{1*}(\BFzeta, y) = \begin{cases}
        \sum_{i \in [C]} (\BFzeta_i + \mathbb{I}(i = y )) \ln (\BFzeta_i + \mathbb{I}(i = y )), & \text{if}\ \BFzeta + \BFe_y \geq 0 \ \text{and}\ \BFone^T(\BFzeta + \BFe_y) = 1, \\
        +\infty, & \text{otherwise}.
    \end{cases}
$$
Therefore, we have $\dom{\ell^{1*}} = \{\BFzeta \in \mathbb{R}^C| \BFzeta \geq -\BFe_y, \BFone^T \BFzeta = 0\} = \{\BFzeta' - \BFe_y | \BFzeta' \in \Delta^{C - 1}\}$, where $\Delta^{C - 1}$ is the $(C-1)$-simplex. There are $C$ extreme points in this domain, and they are ${\BFe_1 - \BFe_y, \dots, \BFe_C - \BFe_y}$. Hence, 
$$
    \sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k \Leftrightarrow \|\BFbeta_i - \BFbeta_y\|_* \leq k, \ \forall i \in [C].
$$
These constraints are for the training sample with label $y$. Sum over all constraints induced by the training samples with all labels $y \in \mathcal{Y}$ and keep only the unique constraints, we have 
$$
    \|\BFbeta_i - \BFbeta_j\|_* \leq k, \ \forall i, j \in [C] \ \text{and}\ i < j.
$$
Then, for $y_n \neq \hat{y}_n$, we have 
\begin{align*}
    &\phantom{=} \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) - \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) \\
    &= (\BFbeta_{y_n} - \BFbeta_{\hat{y}_n})^T \hat{\BFphi}_n - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) \\
    &\leq \|\BFbeta_{y_n} - \BFbeta_{\hat{y}_n}\|_* \|\hat{\BFphi}_n\| - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) \\
    &\leq k( \|\hat{\BFphi}_n\| - \gamma)_+.
\end{align*}
The first inequality is due to the H\"{o}lder's inequality. The second inequality is due to the constraint $\|\BFbeta_i - \BFbeta_j\|_* \leq k$. notice that we obtain 0 when $y_n = \hat{y}_n$. Therefore, we have 
$$
    \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} \leq \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k ( \|\hat{\BFphi}_n\| - \gamma)_+.
$$
This implies that any feasible solution to the constraint $\frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k (\|\hat{\BFphi}_n\| - \gamma)_+ + R(\BFB) - \tau \leq 0$ is also feasible to the constraint $\frac{1}{N} \sum_{n \in [N]} \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n)\} + R(\BFB) - \tau \leq 0$. 

For the equivalence condition, when $\gamma \geq \max_{n\in[N]} \|\hat{\BFphi}_n\|$, we have for $y_n \neq \hat{y}_n$,
$$
    \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) - \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) \leq k ( \|\hat{\BFphi}_n\| - \gamma) \leq 0.
$$
Therefore, 
$$
    \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} = \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) = \ell(\BFB^T\hat{\BFphi}_n, y_n) + k ( \|\hat{\BFphi}_n\| - \gamma)_+.
$$
Hence, the reformulation is equivalent to the original problem.
\hfill \Halmos

\subsection{Proof of Corollary \ref{coro:flipping_label_attack}}
For the cross-entropy loss, we have 
$$
\begin{aligned}
    L_{CE}(\BFB^*; \hat{\BFy}) - L_{CE}(\BFB^*; \BFy) &= \frac{1}{N} \sum_{n\in[N]} (\BFbeta_{y_n}^* - \BFbeta_{\hat{y}_n}^*)^T \hat{\BFphi}_n \\
    &\leq \frac{1}{N} \sum_{n\in[N]} \|\BFbeta_{\hat{y}_n}^*  - \BFbeta_{y_n}^*\|_* \|\hat{\BFphi}_n\| \\
    &\leq p \bar{\Phi} k^* 
\end{aligned}
$$
The first inequality is due to the H\"{o}lder's inequality, and the second inequality is based on the constraint of the optimal solution $\BFB^*$. 

To show the tightness, it suffices to construct an instance that achieves the upper bound. Consider the rate $p$ such that $p N = 1$. Let $t$ specify the norm in the definition of $\bar{\Phi}$ as $\bar{\Phi} = \sup_{n\in[N]}{\|\hat{\BFphi}_n\|_t}$. Let $s$ be the conjugate as $\frac{1}{t} + \frac{1}{s} = 1$. Without loss of generality, suppose that $\|\BFbeta_1^* - \BFbeta_2^*\|_s = k^*$. Consider the there is a training sample $(\hat{\BFphi}, 1)$, such that $\|\hat{\BFphi}\|_t = \bar{\Phi}$, $\hat{\phi}_i(\beta_{1i}^* - \beta_{2i}^*)\geq 0, i \in [M]$, and $|\hat{\BFphi}|^t = \lambda|\BFbeta_1^* - \BFbeta_2^*|^s$. By the equality condition of H\"{o}lder's inequality, we have
$$
    (\BFbeta_1^* - \BFbeta_2^* )^T \hat{\BFphi} = \|\BFbeta_1^* - \BFbeta_2^*\|_s \|\hat{\BFphi}\|_t = k^* \bar{\Phi}.
$$
Consider the flipping label attack by changing the label of $(\hat{\BFphi}, 1)$ to 2, we have
$$
    L_{CE}(\BFB^*; \BFy) - L_{CE}(\BFB^*; \hat{\BFy}) = \frac{\bar{\Phi} k^*}{N}.
$$
For this instance, the loss difference is exactly the upper bound, which demonstrates the tightness of the bound.

\hfill \Halmos

\subsection{Proof of Theorem \ref{thm:hinge_type_reformulation}}
The proof mainly consists of three parts. First, we establish the equivalence between the effective domain of convex conjugate and the subderivative space. Then, we figure out the constraint $\|\BFbeta_i - \BFbeta_j\|_* \leq \frac{k}{\theta}$ for all $i, j \in [C]$ and $i < j$. Finally, we deal with the uncertainty of the label $\hat{y}_n$ similar to the proof of Theorem \ref{thm:cross_entropy_reformulation}.

First, we consider the following Lemma. 
\begin{lemmaAp}
    \label{lemma:equivalence_dom_sub}
    Suppose function $f:\mathbb{R}^n\to\mathbb{R}$ is convex and subdifferentialble. Then,
    $$
        \dom{f^*} = \{\BFp \in \mathbb{R}^n| \BFp \in \partial f(\BFx), \exists \BFx \in \dom{f}\}.
    $$
\end{lemmaAp}
Lemma \ref{lemma:equivalence_dom_sub} is a natural results of the equality condition of Fenchel–Young inequality \citep{rockafellar1970convex}. For completeness, we also give a proof at section \ref{sec:proof_fenchel_young}.
    
Then, we try to figure out all the extreme points of $\text{cl}[\dom{\ell^{1*}}]$, where $\mathrm{cl}[\cdot]$ means the closure operator. For the sample with label $y$, consider the points of $\BFzero$ and $ \theta(\BFe_{y'} - \BFe_{y}), y' \in \mathcal{Y}/\{y\}$. We show they are the extreme points of $\mathrm{cl}[\dom{\ell^{1*}}]$, i.e. let $\mathcal{D} = \mathrm{ConvexHull} \{\BFzero, \theta(\BFe_{y'} - \BFe_{y}), y' \in \mathcal{Y}/\{y\}\}$, so we want to show 
$$
    \mathrm{cl}[\dom{\ell^{1*}}] = \mathcal{D}.
$$

We show $\mathrm{cl}[\dom{\ell^{1*}}] \subseteq \mathcal{D}$ first. Notice that our loss function 
$$
    \ell(\BFB^T\BFphi, y) = \max_{y' \neq y} \rho((\BFbeta_{y} - \BFbeta_{y'})^T \BFphi) = \max_{y' \neq y} \rho((\BFe_{y} - \BFe_{y'})^T \BFB^T\BFphi).
$$
It is a piecewise convex function. In order to investigate its subgradient, we divide the discussion into two cases. First, consider the $\BFu$ such that $\ell(\BFu, y)$ is in the interior of piece $y'$. We have $\ell(\BFu, y) = \rho((\BFe_{y} - \BFe_{y'})^T \BFu) $, and 
$$
    \partial_\BFu \ell(\BFu, y) = \partial_\BFu \rho((\BFe_{y} - \BFe_{y'})^T \BFu) = - (\BFe_{y'} - \BFe_{y})^T \partial_v \rho(v)|_{v = (\BFe_{y'} - \BFe_{y})^T\BFu} .
$$
Since $\partial_v \rho(v)|_{v = (\BFe_{y'} - \BFe_{y})^T\BFu} \in [-\theta, 0]$, we have $\partial_\BFu \ell(\BFu, y) \in \mathcal{D}$.

Then, we consider the junction $\BFu$ of two pieces. Without loss of generality, suppose it is the junction of piece $y_1'$ and $y_2'$. Then, we know its subgradient is the convex set as 
$$
    \mathrm{ConvexHull}\{- (\BFe_{y'_1} - \BFe_{y})^T \partial_v \rho(v)|_{v = (\BFe_{y'_1} - \BFe_{y})^T\BFu}, - (\BFe_{y'_2} - \BFe_{y})^T \partial_v \rho(v)|_{v = (\BFe_{y'_2} - \BFe_{y})^T\BFu}\},
$$ which is a subset of $\mathcal{D}$. Consequently, we have $\mathrm{cl}[\dom{\ell^{1*}}] \subseteq \mathcal{D}$.

Then, we show $\mathcal{D} \subseteq \mathrm{cl}[\dom{\ell^{1*}}]$. Let $\BFzeta_\mathcal{D}$ denote one extreme point of $\mathcal{D}$. To show $\mathcal{D} \subseteq \mathrm{cl}[\dom{\ell^{1*}}]$, we need to find a sequence $\{\BFzeta_n\}$ such that $\BFzeta_n \in \dom{\ell^{1*}}$ and $\BFzeta_n \rightarrow \BFzeta_\mathcal{D}$ for arbitrary $\BFzeta_{\mathcal{D}}$. If $\BFzeta_\mathcal{D} = \BFzero$, this requirement is consistent with $\sup_{u\in\mathbb{R}} \partial \rho(u) = 0$; if $\BFzeta_\mathcal{D}$ is other extreme points, this requirement is consistent with $\inf_{u\in\mathbb{R}} \partial \rho(u) = - \theta$. Therefore, we have $\mathcal{D} \subseteq \mathrm{cl}[\dom{\ell^{1*}}]$. 

On top of that, reconsider the constraint in Lemma \ref{lemma:1wass_reformulation} as 
$$
    \sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k \Leftrightarrow 
    \sup_{\BFzeta \in \mathrm{cl}[\dom{\ell^{1*}}]} \|\BFB\BFzeta\|_* \leq k \Leftrightarrow
    \theta\|\BFbeta_{y'} - \BFbeta_y\|_* \leq k, \ \forall y' \in \mathcal{Y}/\{y\}.
$$
The first equivalence results from the fact that $\inf_{u\in\mathbb{R}} \partial \rho(u)$ and $\sup_{u\in\mathbb{R}} \partial \rho(u)$ both exist and are bounded. The second equivalence is due to $\mathrm{cl}[\dom{\ell^{1*}}] = \mathcal{D}$. Sum over all constraints induced by the training samples with all labels $y \in \mathcal{Y}$ and keep only the unique constraints, we have
$$
    \|\BFbeta_i - \BFbeta_j\|_* \leq \frac{k}{\theta}, \ \forall i, j \in [C] \ \text{and}\ i < j.
$$

Then, we conduct the same analysis similar to the cross-entropy case. Let $y_n^\dagger = \arg \max_{y' \neq y_n} \rho((\BFbeta_{y_n} - \BFbeta_{y'})^T \BFphi_n)$ and $\hat{y}_n^\dagger = \arg \max_{y' \neq \hat{y}_n} \rho((\BFbeta_{\hat{y}_n} - \BFbeta_{y'})^T \BFphi_n)$. We have for $y_n \neq \hat{y}_n$,
\begin{align*}
    &\phantom{=} \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) - \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) \\
    & = \max_{y' \neq y_n} \rho((\BFbeta_{y_n} - \BFbeta_{y'})^T \BFphi) - \max_{y' \neq \hat{y}_n} \rho((\BFbeta_{\hat{y}_n} - \BFbeta_{y'})^T \BFphi) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n)\\
    & = \rho((\BFbeta_{y_n} - \BFbeta_{y_n^\dagger})^T \BFphi) - \rho((\BFbeta_{\hat{y}_n} - \BFbeta_{\hat{y}_n^\dagger})^T \BFphi) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n)\\
    & \leq \theta |(\BFbeta_{y_n} - \BFbeta_{y_n^\dagger} - \BFbeta_{\hat{y}_n} + \BFbeta_{\hat{y}_n^\dagger})^T \BFphi| - k\gamma \mathbb{I}(y_n \neq \hat{y}_n)\\
    & \leq \theta (\|\BFbeta_{y_n} - \BFbeta_{y_n^\dagger}\|_* + \|\BFbeta_{\hat{y}_n} - \BFbeta_{\hat{y}_n^\dagger}\|_*) \|\BFphi\| - k\gamma \mathbb{I}(y_n \neq \hat{y}_n)\\
    &\leq 2k \|\hat{\BFphi}_n\| - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) \\
    &\leq k( 2\|\hat{\BFphi}_n\| - \gamma).
\end{align*}
The first inequality is due to the $\theta$-Lipschitz property of $\rho(\cdot)$. The second inequality is due to the triangle inequality and the H\"{o}lder's inequality. The third inequality is due to the constraint $\|\BFbeta_i - \BFbeta_j\|_* \leq \frac{k}{\theta}$. Notice that we obtain 0 when $y_n = \hat{y}_n$. Therefore, we have 
$$
    \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} \leq \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k ( 2\|\hat{\BFphi}_n\| - \gamma)_+.
$$
This implies that any feasible solution to the constraint $\frac{1}{N} \sum_{n \in [N]} \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k (2\|\hat{\BFphi}_n\| - \gamma)_+ + R(\BFB) - \tau \leq 0$ is also feasible to the constraint $\frac{1}{N} \sum_{n \in [N]} \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n)\} + R(\BFB) - \tau \leq 0$. 

For the equivalence condition, when $\gamma \geq 2 \max_{n\in[N]} \|\hat{\BFphi}_n\|$, we have for $y_n \neq \hat{y}_n$,
$$
    \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) - \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) \leq k ( 2\|\hat{\BFphi}_n\| - \gamma) \leq 0.
$$
Therefore, 
$$
    \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} = \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) = \ell(\BFB^T\hat{\BFphi}_n, y_n) + k ( 2\|\hat{\BFphi}_n\| - \gamma)_+.
$$
Hence, the reformulation is equivalent to the original problem.
\hfill \Halmos

\subsection{Proof of Theorem \ref{thm:lipschitz_approx}}
% This is a direct result of noticing that 
% $
%     \dom{\ell^{1*}}
% $
% is exactly the set of subgradients of $\ell(\BFu)$, which is a corollary of the Fenchel-Young inequality (\cite{rockafellar1970convex} Theorem 23.5).
We first show that $\|\BFzeta\|_* \leq \omega_1$ must hold, if $\BFzeta \in \dom{\ell^{1*}}$. Suppose $\BFzeta \in \dom{\ell^{1*}}$. Then, there exists $\BFu_1, \BFu_2 \in \mathbb{R}^C$ and $y \in \mathcal{Y}$ such that
$$
    \|\BFzeta\|_* \|\BFu_1 - \BFu_2\| = \BFzeta^T(\BFu_1 - \BFu_2) \leq \ell(\BFu_1, y) - \ell(\BFu_2, y) \leq \omega_1 \|\BFu_1 - \BFu_2\|.
$$
The first equality is from the H\"{o}lder's inequality with equal condition. The first inequality is by Lemma \ref{lemma:equivalence_dom_sub} and the definition of subgradient. The second inequality is the Lipschitz continuity. Since $\|\BFu_1 - \BFu_2\| > 0$, this simply implies that $\|\BFzeta\|_* \leq \omega_1$. Therefore, we have $\|\BFzeta\|_* \leq \omega_1$ if $\BFzeta \in \dom{\ell^{1*}}$, which implies that
$$
    \sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq \sup \{\|\omega_1 \BFB\BFzeta\|_* | \|\BFzeta\|_* = 1\} =  \omega_1 \|\BFB\|_*.
$$
As a result, the constraint $\|\BFB^T\|_* \leq \frac{k}{\omega_1}$ is more concervative than $\sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* \leq k$.

Then, for $y_n \neq \hat{y}_n$, we have
\begin{align*}
    &\phantom{=} \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) - \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) \\
    &\leq \omega_2 \|\BFB^T \hat{\BFphi}_n\| - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) \\
    &\leq \omega_2 \|\BFB^T\|_* \|\hat{\BFphi}_n\| - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) \\
    &\leq k \left(\frac{\omega_2}{\omega_1}\|\hat{\BFphi}_n\| - \gamma\right)  
\end{align*}
Similarly, since we obtain 0 when $y_n = \hat{y}_n$, we have 
$$
    \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} \leq \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) + k \left(\frac{\omega_2}{\omega_1}\|\hat{\BFphi}_n\| - \gamma\right)_+.
$$
Thus, both constraints are more conservative, leading to any feasible solution to problem \eqref{eq:lipschitz_reformulation} is also feasible to problem \eqref{eq:1wass_reformulation}.

Then, we show the equivalence condition. When $\gamma \geq \frac{\omega_2}{\omega_1} \max_{n\in[N]} \|\hat{\BFphi}_n\|$, we have
$$
    \ell(\BFB^T\hat{\BFphi}_n, y_n) - k\gamma \mathbb{I}(y_n \neq \hat{y}_n) - \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) \leq k \left(\frac{\omega_2}{\omega_1}\|\hat{\BFphi}_n\| - \gamma\right) \leq 0.
$$
Therefore, 
$$
    \max_{y_n \in \mathcal{Y}} \{ \ell(\BFB^T\hat{\BFphi}_n, y_n) - \gamma \mathbb{I}(y_n \neq \hat{y}_n)\} = \ell(\BFB^T\hat{\BFphi}_n, \hat{y}_n) = \ell(\BFB^T\hat{\BFphi}_n, y_n) + k \left(\frac{\omega_2}{\omega_1}\|\hat{\BFphi}_n\| - \gamma\right)_+.
$$

To achieve equivalence, we also want to achieve $\sup_{\BFzeta \in \dom{\ell^{1*}}} \|\BFB\BFzeta\|_* =\sup \{\|\omega_1 \BFB\BFzeta\|_* | \|\BFzeta\|_* = 1\}$, which induces another equivalence condition. Since the maximum of the convex maximization problem is achieved at an extreme point (\cite{rockafellar1970convex} \S 32), to achieve the equality, we must have $\{\|\omega_1\BFzeta\|_* | \|\BFzeta\|_* = 1\} \subseteq \dom{\ell^{1*}}$. This is equivalent to the condition that for any $\BFv \in \mathcal{V} = \{\BFv \in \mathbb{R}^C| \|\BFv\|_* = 1\}$, there exists $\BFu \in \dom{\ell}$ and $y \in [C]$ such that $\BFv \in \frac{1}{\omega_1} \partial_u \ell(\BFu, y)$.

\hfill \Halmos

\subsection{Proof of Theorem \ref{thm:piecewise_reformulation}}
Taking use of the piecewise linear formular of $\ell(\BFu, y)$, we have
\begin{align*}
    \min_{\BFB \in \mathcal{\BFB}, \BFs} & \ k \\
    \text{s.t.} & \ \frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0\\
    & \ \max_{y_n \in \mathcal{Y}} \left\{ \sup_{\BFphi \in \mathit{\Phi}} \left\{ \BFa_{y_ni}^T \BFB^T \BFphi + b_{y_ni} - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} - k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \right\} \leq s_n, & \forall i \in [K_y], n \in [N].
\end{align*}
Using Proposition \ref{prop:convex_ot_reformulation}, we can reformulate the inner maximization inside the constraint as
\begin{align*}
    &\phantom{=} \sup_{\BFphi \in \mathit{\Phi}} \left\{ \BFa_{y_ni}^T \BFB^T \BFphi + b_{y_ni} - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} \\
    &= \sup_{\BFphi \in \mathbb{R}^M} \left\{ \BFa_{y_ni}^T \BFB^T \BFphi + b_{y_ni} - k c_{\phi}(\BFphi, \hat{\BFphi}_n) - \delta_\mathit{\Phi}(\BFphi) \right\} \\
    &= \inf_{\BFv_{in} \in \dom{\delta^*_{\mathit{\Phi}}}} b_{y_ni} + \delta_\mathit{\Phi}^*(\BFv_{in}) + k c_{\phi}^{1*}((\BFB \BFa_{y_ni} - \BFv_{in})/k, \hat{\BFphi}_n ) 
\end{align*}
Plug it into the constraint and enumerate $y\in\mathcal{Y}$, we have
\begin{align*}
    \min_{k \geq 0, \BFB \in \mathcal{B}, \BFs, \BFv_{in}} & \ k \\
            \text{s.t.} \hspace*{20pt} & \frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0, \\
            & b_{y_ni} + \delta_\mathit{\Phi}^*(\BFv_{in}) + k c_{\phi}^{1*}((\BFB^T \BFa_{y_ni} - \BFv_{in})/k, \hat{\BFphi}_n ) -  k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \leq s_n, \\
            &\hspace{200pt} \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}. 
\end{align*}
For the convexity, the convexity of the convex conjugate $c_{\phi}^{1*}(\cdot, \hat{\BFphi}_n)$ is naive. For $\delta_\mathit{\Phi}^*(\BFv_{in}) = \sup_{\BFphi \in \mathit{\Phi}} \BFv_{in}^T \BFphi$, it is also obvious by the Danskin's theorem. Therefore, the overall formulation is convex. 


\hfill \Halmos

% \subsection{Proof of Theorem \ref{thm:stat_guarantee}}
% The theorem is resulted from the Proposition \ref{prop:convergence}, \ref{prop:exponential_convergence} and \ref{prop:exponential_convergence_loss}. The complete analysis is in appendix \ref{appe:finite_sample}.
% \hfill \Halmos

\subsection{Proof of Proposition \ref{prop:fi_w_reformulation}.}

For notational simplicity, we drop the dependence on the scoring function $p$ in this proof. Problem \eqref{eq:fi_def_w} can be reformulated as 
$$
    \begin{aligned}
        \min &\ k\\
        \text{s.t.} & \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathcal{E})} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon]- k D_{\mathrm{W}} (\mathbb{P},\hat{\mathbb{P}})\right\} \leq \tau, \\
        & k\geq 0
    \end{aligned}.
$$
According to the proof of Theorem \ref{thm:fi_properties}, we know 
$$
    \sup\limits_{\mathbb{P}\in \mathcal{P}(\mathcal{E})} \left\{\mathbb{E}_{\mathbb{P}}[\varepsilon]- k D_{\mathrm{W}} (\mathbb{P},\hat{\mathbb{P}})\right\} = \frac{1}{m^+ m^-} \sum_{i \in [m^+], j \in [m^-]} \sup_{\varepsilon \in \mathcal{E}} \left\{ \varepsilon - k |\varepsilon - \hat{\varepsilon}_{ij}| \right\}.
$$
By the strong duality, we have 
\begin{align*}
    & \phantom{\Leftrightarrow}\sup_{\varepsilon \in \mathcal{E}} \left\{ \varepsilon - k |\varepsilon - \hat{\varepsilon}_{ij}| \right\} \\
    & \Leftrightarrow 
    \begin{aligned}
        \inf_{p_1, p_2, p_3 \geq 0} &(p_1 - p_2)\hat{\varepsilon}_{ij} + p_3 \bar{\varepsilon} \\
        \text{s.t.}\hspace*{10pt} \ &p_1 - p_2 + p_3 = 1 , \\ 
        \  &p_1 + p_2 = k \\
    \end{aligned} \\
    & \Leftrightarrow \inf_{p \leq \min\left\{\frac{k + 1}{2}, k\right\}} (2p - k) \hat{\varepsilon}_{ij} + (1 - 2p + k) \bar{\varepsilon} 
\end{align*}

Then, we get \eqref{eq:fi_def_w_equivalence} in Proposition \ref{prop:fi_w_reformulation}.
\hfill \Halmos

\subsection{Proof of Proposition \ref{prop:convex_ot_reformulation}}
The convexity is a direct result of Danskin's theorem, which indicates that the maximization preserves the convexity of $\ell(\cdot, y)$. 

Notice that $k > 0$ can be treated as a constant here. Let $\delta_\mathit{\Phi}(\cdot)$ denote the characteristic function of the set $\mathit{\Phi}$. Then, we have 
\begin{align*}
    &\phantom{=} \sup_{\BFphi \in \mathit{\Phi}} \left\{ \ell(\BFB^T\BFphi, y) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) \right\} \\ 
    &= \sup_{\BFphi \in \mathbb{R}^M} \left\{ \ell^{1*1*}(\BFB^T \BFphi, y) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) - \delta_\mathit{\Phi}(\BFphi) \right\} \\
    &= \sup_{\BFphi \in \mathbb{R}^M} \sup_{\BFzeta \in \dom{\ell^{1*}}} \left\{ \BFzeta^T \BFB^T \BFphi - \ell^{1*}(\BFzeta, y) - k c_{\phi}(\BFphi, \hat{\BFphi}_n) - \delta_\mathit{\Phi}(\BFphi) \right\} \\
    &= \sup_{\BFzeta \in \dom{\ell^{1*}}} \sup_{\BFphi \in \mathbb{R}^M}  \left\{ \BFzeta^T \BFB^T \BFphi  - (k c_{\phi}(\BFphi, \hat{\BFphi}_n) + \delta_\mathit{\Phi}(\BFphi)) - \ell^{1*}(\BFzeta, y) \right\} \\
    &= \sup_{\BFzeta \in \dom{\ell^{1*}}} \inf_{\BFtheta \in \dom{\delta^*_{\BFPhi}}} \left\{
        kc_{\phi}^{1*}((\BFB\BFzeta - \BFtheta)/k, \hat{\BFphi}) + \delta_\mathit{\Phi}^*(\BFtheta) - \ell^{1*}(\BFzeta, y) 
    \right\}
\end{align*}
Notice that the last equality follows from the fact that $[f_1 + f_2]^*(\BFlambda) = \inf_{\BFtheta} f_1^*(\BFtheta) + f_2^*(\BFlambda - \BFtheta)$ and $[k c(\BFphi, y, \hat{\BFphi}, \hat{y})]^{1*} = k c_{\phi}^{1*} (\BFzeta/k, \hat{\BFphi})$. The first formula is known as the epi-addition or inf-convolution of the convex conjugate. For more details of the rigorous legality of the reformulation, we refer to the detailed proof in \cite{mohajerin2018data} Theorem 4.2.

When the support is extended to general $\mathit{\Phi} = \mathbb{R}^M$, we have 
$$
    \delta_\mathit{\Phi}^*(\BFtheta) = \sup_{\BFphi \in \mathit{\Phi}} \BFtheta^T \BFphi = \begin{cases}
        0, & \BFtheta = \BFzero, \\
        +\infty, & \BFtheta \neq \BFzero.
    \end{cases}
$$
Hence, it reduces to 
$$
\sup_{\BFzeta \in \dom{\ell^{1*}}} \inf_{\BFtheta \in \dom{\delta^*_{\BFPhi}}} \left\{
    kc_{\phi}^{1*}((\BFB\BFzeta - \BFtheta)/k, \hat{\BFphi}) + \delta_\mathit{\Phi}^*(\BFtheta) - \ell^{1*}(\BFzeta, y) 
\right\} = \sup_{\BFzeta \in \dom{\ell^{1*}}} \left\{
    kc_{\phi}^{1*}((\BFB\BFzeta)/k, \hat{\BFphi}) - \ell^{1*}(\BFzeta, y)
\right\}.
$$
\hfill \Halmos

\subsection{Proof of Corollary \ref{cor:eg_uncertainty}}
\textbf{(a)} We can derive the convex conjugate of the characteristic function as 
$$
    \delta_\mathit{\Phi}^*(\BFz) = \max_\BFphi \left\{
        \BFz^T \BFphi: C \BFphi \leq \BFd
    \right\} = \min_\BFlambda \left\{
        \BFlambda^T \BFd: \BFlambda \geq 0, C^T \BFlambda = \BFz
    \right\}
$$
Therefore, we have 
\begin{align*}
    \min_{\BFB \in \mathcal{B}, \BFs, \BFlambda_{in}} & \ k\\
    \text{s.t.} &\frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0\\
    & \ b_{y_ni} + \BFd^T \BFlambda_{in}  + k c_{\phi}^{1*}((\BFB\BFa_{y_ni} - C^T \BFlambda_{in})/k, \hat{\BFphi}_n ) -  k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \leq s_n, & \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}, \\
    & \BFlambda_{in} \geq 0, & \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}.
\end{align*}

\textbf{(b)} We can derive the convex conjugate of the characteristic function as
\begin{align*}
    \delta_\mathit{\Phi}^*(\BFz) &= \max_\BFphi \left\{
        \BFz^T \BFphi: f_j(\BFphi) \leq 0, j \in [J]
    \right\} \\
    &= \min_\BFlambda \left\{
        \left[\sum_{j\in[J]} \lambda_j f_j\right]^*(\BFz): \BFlambda \geq 0
    \right\} \\
    &= \min_{\BFlambda, \BFz_1, \dots, \BFz_J}\left\{
        \sum_{j\in[J]} \lambda_j f_j^*\left(\frac{\BFz_j}{\lambda_j}\right): \BFlambda \geq 0, \sum_{j\in[J]} \BFz_j = \BFz 
    \right\}
\end{align*}
Therefore, we have 
\begin{align*}
    \min_{\BFB \in \mathcal{B}, \BFs, \BFlambda_{in}, \BFz_{inj}} & \ k\\
    \text{s.t.} & \frac{1}{N} \sum_{n \in [N]} s_n + R(\BFB) - \tau \leq 0\\
    & \ b_{y_ni} + \sum_{j\in[J]} \lambda_{inj} f_j^*\left(\frac{\BFz_{inj}}{\lambda_{inj}}\right)  + k c_{\phi}^{1*}((\BFB \BFa_{y_ni} - \BFv_{in})/k, \hat{\BFphi}_n ) -  k \gamma \mathbb{I}(y_n \neq \hat{y}_n) \leq s_n, \\
    & \hspace{280pt} \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}, \\
    & \sum_{j\in[J]} \BFz_{inj} = \BFv_{in}, \hspace{214pt}  \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y},\\
    & \BFlambda_{in} \geq 0, \hspace{245pt}  \forall i \in [K_{y_n}], n \in [N], y_n \in \mathcal{Y}.
\end{align*}  

\hfill \Halmos

\subsection{Proof of Lemma \ref{lemma:generalization}}
The bound is a direct result of the constraint in the FI-based training problem. 

\hfill \Halmos

\subsection{Proof of Proposition \ref{prop:generalization_error}}
As a consequence of the light-tail assumption, we have the following bound on the Wasserstein distance between $\phat_N$ and $\ptrue$. 
\begin{lemmaAp}
    \label{lemma:wass_bound}
    (Theorem 2, \cite{fournier2015rate}) If Assumption \ref{asmp:light_tail} holds, we have 
    \begin{equation*}
        \mathbb{P}^N\left\{
            D_{\mathrm{W}}^\dagger(\phat_N, \ptrue) \geq \theta
        \right\} \leq \begin{cases}
            c_1 \exp(-c_2 N \theta^{\max\{m,2\}}), & \ \text{if}\ \theta \leq 1, \\
            c_1 \exp(-c_2 N \theta^a), & \ \text{if}\ \theta > 1,
        \end{cases} 
    \end{equation*}
    for all $N \geq 1$, $m\neq 2$ and $\theta > 0$, where $m$ is the dimension of $(\BFphi, y)$, and $c_1$ and $c_2$ are positive constants that only depends on $a$ and $m$.
\end{lemmaAp}
Notice that the Wasserstein distance in Lemma \ref{lemma:wass_bound} is defined as
$$
    D_{\mathrm{W}}^\dagger(\phat_N, \ptrue) = \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \mathbb{E}_{\pi} [\|(\BFphi, y) - (\hat{\BFphi}, \hat{y})\|],
$$
which is different from the OT-cost-based definition in equation \eqref{eq:def_ot}. However, when consider $c(\BFphi, y, \hat{\BFphi}, \hat{y}) = \|\BFphi - \hat{\BFphi}\| + \gamma \mathbb{I}(y \neq \hat{y})$, when can link the two definitions through the following relationship.
\begin{align*}
    & \phantom{=} D_c (\phat_N, \ptrue) \\
    & = \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \mathbb{E}_{\pi} [\|\BFphi - \hat{\BFphi}\| + \gamma \mathbb{I}(y \neq \hat{y})] \\
    & = \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \left\{\mathbb{E}_{\pi_{(\BFphi, \hat{\BFphi})}} [\|\BFphi - \hat{\BFphi}\|] + \gamma \mathbb{E}_{\pi_{(y, \hat{y})}} [\mathbb{I}(y \neq \hat{y})] \right\} \\
    & \leq \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \left\{\mathbb{E}_{\pi} [\|(\BFphi, y) - (\hat{\BFphi}, \hat{y})\|] + \gamma \mathbb{E}_{\pi} [\|(\BFphi, y) - (\hat{\BFphi}, \hat{y})\|] \right\} \\
    % & = (1 + \gamma) \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \mathbb{E}_{\pi} [\|(\BFphi, y) - (\hat{\BFphi}, \hat{y})\|] \\
    & = (1 + \gamma) D_{\mathrm{W}}^\dagger(\phat_N, \ptrue).
\end{align*}
The inequality is due to the fact that $x^{1/p} $ is an increasing function for $x \geq 0$ and $p \geq 1$. 

Then, we need to link $\mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^T \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)]$ to $D_c(\phat_N, \ptrue) = \inf_{\pi \in \Pi(\phat_N, \ptrue)} \mathbb{E}_{\pi} [c(\BFphi, y, \hat{\BFphi}, \hat{y})]$. The classifcal Kantorovich-Rubenstein duality (\cite{kantorovich1958space}) indicates relationship between the 1-Wasserstein distance and the Lipschitz continuity. Therefore, we first extend the Kantorovich-Rubenstein duality to general OT cost functions. 

\begin{lemmaAp}
    \label{lemma:kr_duality}
    (Extended Kantorovich-Rubenstein duality) For any cost function $c(\BFphi, y, \hat{\BFphi}, \hat{y})$ satisfies the triangle inequality $c(\BFphi, y, \hat{\BFphi}, \hat{y}) \leq c(\BFphi, y, \BFphi', y') + c(\BFphi', y', \hat{\BFphi}, \hat{y})$, we have
    \begin{equation}
        \label{eq:kr_duality}
        D_c(\phat_N, \ptrue) = \sup_{\|f\|_L \leq 1} \left\{
            \mathbb{E}_{\phat_N}[f(\BFphi, y)] - \mathbb{E}_{\ptrue}[f(\BFphi, y)]
        \right\},
    \end{equation}
    where $\|f\|_L$ denotes the Lipschitz module of function $f$ defined as 
    $$
        \|f\|_L = \inf_\omega \left\{\omega \geq 0 | f(\BFphi, y) - f(\BFphi', y')| \leq \omega c(\BFphi, y, \BFphi', y') , \forall (\BFphi, y), (\BFphi', y') \in \mathit{\Phi} \times \mathcal{Y} \right\}.
    $$
\end{lemmaAp}
For completeness, we provide a proof to Lemma \ref{lemma:kr_duality} at section \ref{sec:proof_kr_duality}. Building on Lemma \ref{lemma:kr_duality}, we only need to retrieve the Lipschitz module of the loss function $\ell(\hat{\BFB}_N \BFphi, y)$ to the OT cost function $c(\BFphi, y, \hat{\BFphi}, \hat{y})$. Using Assumption \ref{asmp:lipschitz}, we have 
\begin{align*}
    &\phantom{=} \left| \ell(\hat{\BFB}_N^{*T} \BFphi, y) - \ell(\hat{\BFB}_N^{*T} \BFphi', y') \right| \\
    & \leq |\ell(\hat{\BFB}_N^{*T} \BFphi, y) - \ell(\hat{\BFB}_N^{*T} \BFphi', y)| + |\ell(\hat{\BFB}_N^{*T} \BFphi', y) - \ell(\hat{\BFB}_N^{*T} \BFphi', y')| \\
    & \leq \omega_1 \|\hat{\BFB}_N^{*T}\|_* \|\BFphi - \BFphi'\| + \omega_2 \|\hat{\BFB}_N^{*T}\|_* \|\BFphi'\| \mathbb{I} (y \neq y') \\
    & \leq \|\hat{\BFB}_N^{*T}\|_* \max\left\{\omega_1, \omega_2 \frac{\sup_{(\BFphi, y) \in \supp{\mathbb{P}^*}} \|\BFphi\|}{\gamma}\right\} (\|\BFphi - \BFphi'\| + \gamma \mathbb{I} (y \neq y')) \\
    & = \|\hat{\BFB}_N^{*T}\|_* \max\left\{\omega_1, \omega_2 \frac{\sup_{(\BFphi, y) \in \supp{\mathbb{P}^*}} \|\BFphi\|}{\gamma}\right\} c(\BFphi, y, \BFphi', y') \\
\end{align*}

Let $\omega = \max\left\{\omega_1, \omega_2 \frac{\sup_{(\BFphi, y) \in \supp{\mathbb{P}^*}} \|\BFphi\|}{\gamma}\right\}$. Then, 
\begin{align*}
    & \phantom{=} \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)]  \\
    &\leq \sup_{ \|f\|_L \leq \|\hat{\BFB}_N^{*T}\|_* \omega} 
    \left\{
        \mathbb{E}_{\phat_N}[f(\BFphi, y)] - \mathbb{E}_{\ptrue}[f(\BFphi, y)]
    \right\} \\
    &= \|\hat{\BFB}_N^{*T}\|_* \omega D_c(\phat_N, \ptrue) \\
    & \leq (1 + \gamma) \|\hat{\BFB}_N^{*T}\|_* \omega D_{\mathrm{W}}^\dagger(\phat_N, \ptrue).
\end{align*}

Using Lemma \ref{lemma:wass_bound} and noticing that the feature dimension $M + 1\geq 2$, we have
\begin{align*}
    % &\phantom{\Leftrightarrow} \mathbb{P}\left\{
    %      D_{\mathrm{W}}(\phat_N, \ptrue) \geq \theta
    % \right\} 
    % \leq 
    % C_1 \exp\left(-C_2 N \theta^{M+1}\right) \\
    \mathbb{P}\left\{
         D_{\mathrm{W}}(\phat_N, \ptrue) \leq \left(\frac{1}{NC_2} \log\left(\frac{C_1}{\epsilon}\right)\right)^{\frac{1}{M+1}} 
    \right\} 
    \geq 
    1- \epsilon.
\end{align*}
Consequently, with probability at least $1-\epsilon$, we have
$$
    \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)] \leq (1+\gamma)\omega \|\hat{\BFB}_N^{*T}\|_* D_{\mathrm{W}}(\phat_N, \ptrue) \leq (1+\gamma)\omega \|\hat{\BFB}_N^{*T}\|_* \left(\frac{1}{NC_2} \log\left(\frac{C_1}{\epsilon}\right)\right)^{\frac{1}{M+1}}.
$$

The second term in the final bound is a straightforward extension of Lemma \ref{lemma:generalization}. 
\begin{align*}
    & \phantom{=} \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)]\\
    & \leq \tau - \mathbb{E}_{\hat{\mathbb{P}}}[\ell (\hat{\BFB}_N^{*T}\BFphi, y)] + k^*_N D_c (\mathbb{P}^*, \hat{\mathbb{P}}) \\
    & \leq \tau - \mathbb{E}_{\hat{\mathbb{P}}}[\ell (\hat{\BFB}_N^{*T}\BFphi, y)] + k^*_N(1 + \gamma) D_{\mathrm{W}}^\dagger (\mathbb{P}^*, \hat{\mathbb{P}}) 
\end{align*}
With probability at least $1-\epsilon$, we have
$$
    \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^{*T} \BFphi, y)] \leq \tau - \mathbb{E}_{\hat{\mathbb{P}}}[\ell (\hat{\BFB}_N^{*T}\BFphi, y)] + k^*_N(1 + \gamma) \left(\frac{1}{NC_2} \log\left(\frac{C_1}{\epsilon}\right)\right)^{\frac{1}{M+1}}.
$$


\hfill \Halmos


\subsection{Proof of Proposition \ref{prop:convergence}}
(a) For the strong regularity, we follow the definition in \cite{shapiro2021lectures}, which further dates back to \cite{robinson1980strongly}. We exhibit the definition here for completeness. 

\begin{definitionAp} 
    \label{def:strong_regularity}
    (Strong regularity, \cite{robinson1980strongly})
    Suppose that the mapping $\BFPsi(\BFw)$ is continuously differentiable. We say that a solution $\BFw^*$ of the SGE \eqref{eq:sge_true} is strongly regular if there exists a neighborhood $\mathcal{N}_1$ of $\BFzero \in \mathbb{R}^{M_w}$ and $\mathcal{N}_2$ of $\BFw^*$, such that for every $\BFdelta \in \mathcal{N}_1$, the (linearized) perturbed SGE 
    \begin{equation}
        \label{eq:strongly_regular}
        \BFzero \in \BFdelta + \BFPsi(\BFw^*) + \nabla \BFPsi(\BFw^*) (\BFw - \BFw^*) + \BFGamma(\BFw) 
    \end{equation}
    has a unique solution in $\mathcal{N}_2$. Let $\BFw(\BFdelta)$ denote this solution. Then, the mapping $\BFdelta \to \BFw(\BFdelta)$ is Lipschitz continuous on $\mathcal{N}_1$.
\end{definitionAp}

According to \cite{izmailov2003karush}, the strong regularity of the smooth KKT system is equivalent to LICQ and SSOSC, which are guaranteed by Assumption \ref{asmp:smooth_regularity} (b). 

(b) The convergence of $\hat{\BFw}_N$ is a direct result of the following theorem of \cite{shapiro2021lectures}. 
\begin{lemmaAp}
    \label{lemma:convergence_shapiro}
    (Theorem 5.15 of \cite{shapiro2021lectures})
    Let $\BFC$ be a compact set and $\BFw^*$ be a unique solution of the SGE \eqref{eq:sge_true} in $\BFC$. Suppose that:
    \begin{enumerate}[(a)]
        \item the set $\BFGamma(\BFw)$ is closed;
        \item for almost everywhere $\BFphi$, the mapping $\BFw \to \BFPsi(\BFw, \BFphi)$ is continuously differentiable on $\BFC$, and $\|\BFPsi(\BFw, \BFphi)\|_{\BFw\in\BFC}$ and $\|\nabla_{\BFw} \BFPsi(\BFw, \BFphi)\|_{\BFw\in\BFC}$ are dominated by an integrable function;
        \item the solution $\BFw^*$ is strongly regular;
        \item $\hat{\BFPsi}_N(\BFw)$ and $\nabla \hat{\BFPsi}_N(\BFw)$ converge with probability 1 to $\BFPsi(\BFw)$ and $\nabla \BFPsi(\BFw)$, respectively, uniformly on $\BFC$.
    \end{enumerate} 
    Then, with probability 1, $\hat{\BFw}_N$ converges to $\BFw^*$ as $N \to \infty$.
\end{lemmaAp}
The condition (a) is obvious from the definition of $\BFGamma(\BFw)$. The condition (b) is guaranteed by Assumption \ref{asmp:sge_bound_integrable} (b). The condition (c) is the part (a) of the Proposition. As for the condition (d), since the empirical distribution is based on $N$ iid samples from $\mathbb{P}^*$, the convergence of $\hat{\BFPsi}_N(\BFw)$ and $\nabla \hat{\BFPsi}_N(\BFw)$ is guaranteed by the integrable domination of $\BFPsi(\BFw, \BFphi)$ and $\nabla \BFPsi(\BFw, \BFphi)$, respectively (Dominated convergence theorem, Theorem 7.48, \cite{shapiro2021lectures}). Hence, $\hat{\BFw}_N$ converges to $\BFw^*$ as $N \to \infty$.

(c) We first show that $\hat{\BFeta}_{0N} = \BFeta_0^* = \BFzero$ holds with probability 1 if $N \geq N_C$. Let $\mathcal{S}_0$ denote the set of indices of  $\BFeta^*_0$ in $\BFeta$. According to the complementary slackness, we have $g_s(\BFbeta^*, k^*) < 0$ if $s \in \mathcal{S}_0$. Since $\hat{\BFbeta}_N$ converges to $\BFbeta^*$ and $\hat{k}_N$ converges to $k^*$, there exists a large $N_C$ such that when $N\geq N_c$, $g_s(\hat{\BFbeta}_N, \hat{k}_N) < 0$ for any $s \in \mathcal{S}_0$. By the complementary slackness again, we have $\hat{\BFeta}_{0N} = \BFzero$ if $N \geq N_C$. 

Then, we consider the asymptotics of $\BFw' = (k, \BFbeta^T, \BFeta_+^T )^T $. Notice that the original SGE \eqref{eq:sge_true} is reduced to 
$$
    \BFPsi'(\BFw'^*) = 0.
$$
As mentioned, the left-hand side $\BFPsi'(\BFw')$ is defined by dropping the constraints of $\BFeta_0$. This equation is well-defined in the sense that it only involves the variables $\BFw'$ because $\BFeta_0 = \BFzero$ is already ensured. Similarly, we can define the linearly perturbed SGE of $\BFPsi'(\BFw')$ as
$$
    \BFdelta + \nabla \BFPsi'(\BFw'^*) (\BFw' - \BFw'^*) = 0.
$$
Notice that the formulation is much simpler than \eqref{eq:strongly_regular} because $\BFGamma'(\BFw') = \{\BFzero\}$ and $\BFPsi'(\BFw'^*) = 0$. The strong regularity of $\BFPsi'(\BFw')$ remains unchanged. As a result, let $\tilde{\BFw}'$ denote the solution of the linearly perturbed SGE of $\BFPsi'(\BFw')$, so 
$$
    \tilde{\BFw}'(\BFdelta) = \BFw'^* - (\nabla \BFPsi'(\BFw'^*))^{-1} \BFdelta.
$$

Let $\mathcal{W}'$ denote the projection of the compact set $\mathcal{W}$ on $\BFw'$ and $M_w'$ denote its dimension. Therefore, $\mathcal{W}' \subset \mathbb{R}^{M_w'}$ is also compact. According to \cite{robinson1980strongly}, we define $C^1(\mathcal{W}', \mathbb{R}^{M_w'})$ as the function space of continuously differentiable functions on $\mathcal{W}'$. Then, $\BFPsi' \in C^1(\mathcal{W}', \mathbb{R}^{M_w'})$ and $\hat{\BFPsi'}_N \in C^1(\mathcal{W}', \mathbb{R}^{M_w'})$. Furthermore, there exists $\epsilon > 0$ such that for any $\BFu \in C^1(\mathcal{W}', \mathbb{R}^{M_w'})$ satisfying $\|\BFu - \BFPsi'\|_{1, \mathcal{W}'} \leq \epsilon$, the equation $\BFu(\BFw') = \BFzero$ has a unique solution $\BFw'_{c} = \BFw'_{c}(\BFu)$ in a neighborhood of the optimal solution $\BFw'^{*}$. Moreover, 
$$
    \BFw'_{c}(\BFu) = \tilde{\BFw}'(\BFu(\BFw'^*) - \BFPsi'(\BFw'^*) ) + o(\|\BFu - \BFPsi'\|_{1, \mathcal{W}'}).
$$ 
Notice that $\BFw'_{c}(\BFPsi') = \BFw'^{*}$. When $N$ is large enough such that $\|\hat{\BFPsi'}_N - \BFPsi'\|_{1, \mathcal{W}'} \leq \epsilon$, we have $\hat{\BFw}'_{c} (\hat{\BFPsi'}_N) = \hat{\BFw}_N$. Therefore, we can conclude that
\begin{align*}
    N^{1/2} (\hat{\BFw}_N' - \BFw'^*) &= N^{1/2} \left(
        \tilde{\BFw}'(\hat{\BFPsi'}_N(\BFw'^*) - \BFPsi'(\BFw'^*) ) - \BFw'^* + o(\|\hat{\BFPsi'}_N - \BFPsi'\|_{1, \mathcal{W}'})
    \right) \\
    & = - N^{1/2} (\nabla \BFPsi'(\BFw'^*))^{-1} (\hat{\BFPsi'}_N(\BFw'^*) - \BFPsi'(\BFw'^*) ) + o(N^{1/2} \|\hat{\BFPsi'}_N - \BFPsi'\|_{1, \mathcal{W}'}).
\end{align*}
Since the convergence rate of the error term $\|\hat{\BFPsi'}_N - \BFPsi'\|_{1, \mathcal{W}'}$ is $O(N^{-1/2})$, the remainder term $o(N^{1/2} \|\hat{\BFPsi'}_N - \BFPsi'\|_{1, \mathcal{W}'})$ is negligible as $N \to \infty$. By the central limit theorem, we have 
\begin{gather*}
    N^{1/2} (\hat{\BFPsi'}_N(\BFw'^*) - \BFPsi'(\BFw'^*) ) \to \mathcal{N}(\BFzero, \Sigma'), \\
    N^{1/2} (\hat{\BFw}_N' - \BFw'^*) \to \mathcal{N}(\BFzero, (\nabla_{\BFw'}\BFPsi'(\BFw'))^{-1}\Sigma' (\nabla_{\BFw'}\BFPsi'(\BFw'))^{-1}).
\end{gather*}
\hfill \Halmos

\subsection{Proof of Proposition \ref{prop:exponential_convergence}}
\textbf{(a)} This is a direct result of the uniform Large Deviation Theorem. Assumption \ref{asmp:sge_bound_integrable} ensures the existence of the compact feasible set and the integrable domination of the gradient of $\BFPsi(\BFw; \BFphi)$. Assumption \ref{asmp:lipschitz_sge_finite_mgf} regulates the related moment-generating function. By Theorem 7.65 of  \cite{shapiro2021lectures}, we have that for sufficiently small $\epsilon > 0$, there exists positive constants $\delta_{1i} (\epsilon)$ and $\delta_{2i} (\epsilon)$ for each $i \in [M_w]$ such that 
$$
    \mathbb{P} \left\{
        \sup_{\BFw \in \mathcal{W}} \left\| (\hat{\BFPsi}_N)_i(\BFw) - \BFPsi_i(\BFw) \right\| \geq \epsilon
    \right\} \leq \delta_{1i}(\epsilon) \exp(-\delta_{2i}(\epsilon) N).
$$
According to Theorem 2.9 of \cite{chen2019convergence}, we can obtain \eqref{eq:exp_converge_psi} by compositing all these inequalities. As to the determination of function $\delta_{1i}(\epsilon)$ and $\delta_{2i}(\epsilon)$, we refer to \cite{shapiro2021lectures} for the details. More nuanced assumptions are required to obtain the explicit form of $\delta_{1i}(\epsilon)$ and $\delta_{2i}(\epsilon)$.

\textbf{(b)} Suppose $\| \hat{\BFw}_N - \BFw^* \| \geq \epsilon$. Then, we have
\begin{align*}
    0 & = \inf_{\BFgamma \in \BFGamma(\hat{\BFw}_N)} \| \hat{\BFPsi}_N(\hat{\BFw}_N) + \BFgamma \| \\
    & = \inf_{\BFgamma \in \BFGamma(\hat{\BFw}_N)} \| \hat{\BFPsi}_N(\hat{\BFw}_N) + \BFgamma + \BFPsi(\hat{\BFw}_N) - \BFPsi(\hat{\BFw}_N) \|  \\
    & \geq \inf_{\BFgamma \in \BFGamma(\hat{\BFw}_N)} \| \BFPsi(\hat{\BFw}_N) + \BFgamma \| - \| \hat{\BFPsi}_N(\hat{\BFw}_N) - \BFPsi(\hat{\BFw}_N) \| \\
    & \geq \rho(\epsilon) - \sup_{\BFw \in \mathcal{W}} \left\| \hat{\BFPsi}_N(\BFw) - \BFPsi(\BFw) \right\| 
\end{align*}
The first inequality is due to the triangle inequality of $\|\BFa - \BFb\| \geq \|\BFa\| - \|\BFb\|$. The second inequality is the composition of 
\begin{gather*}
    \inf_{\BFgamma \in \BFGamma(\hat{\BFw}_N)} \| \BFPsi(\hat{\BFw}_N) + \BFgamma \| \geq \rho(\epsilon),\\
    \| \hat{\BFPsi}_N(\hat{\BFw}_N) - \BFPsi(\hat{\BFw}_N) \| \leq \sup_{\BFw \in \mathcal{W}} \left\| \hat{\BFPsi}_N(\BFw) - \BFPsi(\BFw) \right\|.
\end{gather*}
  
Therefore, if $\| \hat{\BFw}_N - \BFw^* \| \geq \epsilon$ holds, we must have $\sup_{\BFw \in \mathcal{W}} \left\| \hat{\BFPsi}_N(\BFw) - \BFPsi(\BFw) \right\| \geq \rho(\epsilon)$. Notice that for any $\epsilon > 0$, we have $\rho(\epsilon) > 0$ as a result of the uniqueness of the solution $\BFw^*$, which guarantees that the inequality $\sup_{\BFw \in \mathcal{W}} \left\| \hat{\BFPsi}_N(\BFw) - \BFPsi(\BFw) \right\| \geq \rho(\epsilon)$ is nontrivial. Therefore,  
$$
    \mathbb{P} \left\{
        \left\|\hat{\BFw}_N - \BFw^* \right\| \geq \epsilon
    \right\} \leq \mathbb{P} \left\{
        \sup_{\BFw \in \mathcal{W}} \left\| \hat{\BFPsi}_N(\BFw) - \BFPsi(\BFw) \right\| \geq \rho(\epsilon)
    \right\} \leq \delta_1(\rho(\epsilon)) \exp(-\delta_2(\rho(\epsilon)) N).
$$
\hfill \Halmos



% \subsection{Proof of Proposition \ref{prop:exponential_convergence_loss}}
% % As a consequence of Assumption \ref{asmp:light_tail_ptrue}, 
% % \begin{lemmaAp}
% %     \label{lemma:wass_bound}
% %     (Theorem 2, \cite{fournier2015rate}) If Assumption \ref{asmp:light_tail_ptrue} holds, we have 
% %     \begin{equation}
% %         \label{eq:wass_bound}
% %         \mathbb{P}^N\left\{
% %             D_{\mathrm{W}}(\phat_N, \ptrue) \geq \epsilon
% %         \right\} \leq \begin{cases}
% %             c_1 \exp(-c_2 N \epsilon^{\max\{m,2\}}), & \ \text{if}\ \epsilon \leq 1, \\
% %             c_1 \exp(-c_2 N \epsilon^a), & \ \text{if}\ \epsilon > 1,
% %         \end{cases} 
% %     \end{equation}
% %     for all $N \geq 1$, $m\neq 2$ and $\epsilon > 0$, where $m$ is the dimension of $(\BFphi, y)$, and $c_1$ and $c_2$ are positive constants that only depends on $a$ and $m$.
% % \end{lemmaAp}
% % Then, we have 
% Consider $\left|\mathbb{E}_{\ptrue}[\ell(\BFB^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)] \right|$ as
% \begin{align*}
%     &\phantom{=}\ \left|\mathbb{E}_{\ptrue}[\ell(\BFB^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)] \right| \\
%     &= \left|\mathbb{E}_{\ptrue}[\ell(\BFB^{*T} \BFphi, y)] - \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^T \BFphi, y)] + \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^T \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)] \right| \\
%     & \leq \left|\mathbb{E}_{\ptrue}[\ell(\BFB^{*T} \BFphi, y)] - \mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^T \BFphi, y)]\right| + \left|\mathbb{E}_{\ptrue}[\ell(\hat{\BFB}_N^T \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)]\right| \\
%     & \leq \omega_1 \mathbb{E}_{\ptrue} \left[\|\BFphi\|_*\right] \|\BFbeta^* - \hat{\BFbeta}_N\| + (1 + \gamma) \|\hat{\BFB}_N^{*T}\|_* \omega D_{\mathrm{W}}^\dagger(\phat_N, \ptrue) 
% \end{align*}

% The formmer term results from the Lipschitz continuity of $\ell$. The latter term has been discussed in the proof of Proposition \ref{prop:generalization_error} with $\omega = \max\left\{\omega_1, \omega_2 \frac{\sup_{(\BFphi, y) \in \supp{\mathbb{P}^*}} \|\BFphi\|}{\gamma}\right\}$. 

% Using Lemma \ref{lemma:wass_bound}, we have 
% \begin{align*}
%     & \phantom{=}\mathbb{P}\left\{
%         \left|\mathbb{E}_{\ptrue}[\ell(\BFB^{*T} \BFphi, y)] - \mathbb{E}_{\phat_N}[\ell(\hat{\BFB}_N^T \BFphi, y)] \right| \geq \epsilon
%     \right\} \\
%     & \leq \mathbb{P}\left\{
%         \omega_1\mathbb{E}_{\ptrue} \left[\|\BFphi\|_*\right] \|\BFbeta^* - \hat{\BFbeta}_N\| \geq \epsilon 
%     \right\} + \mathbb{P}\left\{
%         (1 + \gamma)\omega\|\hat{\BFB}_N^T\|_* D_{\mathrm{W}}(\phat_N, \ptrue) \geq \epsilon
%     \right\} \\
%     & \leq \delta_1\left(\rho\left(
%         \frac{\epsilon}{\omega_1\mathbb{E}_{\ptrue} \left[\|\BFphi\|_*\right]}
%     \right)\right) \exp\left(-\delta_2\left(\rho\left(
%         \frac{\epsilon}{\omega_1\mathbb{E}_{\ptrue} \left[\|\BFphi\|_*\right]}
%     \right)\right) N\right) +
%     c_1 \exp\left(-c_2 N \left(\frac{\epsilon}{(1 + \gamma)\omega \|\hat{\BFB}_N^T\|_*}\right)^{M+1}\right),
% \end{align*}
% which decays exponentially with $N$.

% \hfill \Halmos

\subsection{Proof of Proposition \ref{prop:dro_reformulation} and \ref{prop:dro_fi_reformulation}}
Based on the reformulation procedure in Appendix \ref{appe:connection_dro}, the proof is similar to the proof of Theorem \ref{thm:cross_entropy_reformulation} and Theorem \ref{thm:hinge_type_reformulation}. The detailed steps are omitted here.

\hfill \Halmos

\subsection{Proof of Proposition \ref{prop:generalization_fi_dro}}
The claim $k^*_N \geq k^\dagger_{N, \epsilon}$ is nothing but noticing that $k^*_N$ must be a feasible solution of problem \eqref{eq:fi_dro}. Then, replace $k^*_N $ with $ k^\dagger_{N, \epsilon}$ in the bound of Proposition \ref{prop:generalization_error}.

% Then, according to Lemma \ref{lemma:wass_bound}, we have that 
% $$
%     \mathbb{P} \left(\mathbb{P}^* \in \mathcal{P}\left(\hat{\mathbb{P}}, \left(
%         \frac{1}{C_2 N} \log \left(\frac{C_1}{\epsilon}\right)
%     \right)^{1/M}\right)\right) \geq 1 - \epsilon.
% $$
% In this case, we can have that 
% \begin{align*}
%     \mathbb{E}_{\mathbb{P}^*}[\ell(\hat{\BFB}_N \BFphi, y)] - \mathbb{E}_{\hat{\mathbb{P}}}[\ell(\hat{\BFB}_N \BFphi, y)] &\leq \tau - \mathbb{E}_{\hat{\mathbb{P}}}[\ell(\hat{\BFB}_N \BFphi, y)] + k^*_{N, \epsilon} D_{\mathrm{W}}(\hat{\mathbb{P}}, \mathbb{P}^*) \\
%     &\leq \tau - \mathbb{E}_{\hat{\mathbb{P}}}[\ell(\hat{\BFB}_N \BFphi, y)] + k^*_{N, \epsilon} \left(
%         \frac{1}{C_2 N} \log \left(\frac{C_1}{\epsilon}\right)
%     \right)^{1/M}.
% \end{align*}

\hfill \Halmos

% \subsection{Proof of Proposition \ref{prop:randomization}}
% This is a straightforward result of the Jensen's inequality. For the convex loss function $\ell$, we have
% $$
%     \mathbb{E}_{\mathbb{F}} \left[\mathbb{E}_{\mathbb{P}}\left[\ell(\BFB^T\BFphi, y)\right] \right] \geq \mathbb{E}_{\mathbb{P}} \left[\ell \left(\mathbb{E}_{\mathbb{F}}\left[\BFB^T\BFphi\right], y \right) \right].
% $$
% Notice that the right-hand side is equivalent to the case of the deterministic policy by replacing the expectation with the deterministic $B$. Therefore, the randomization always enlarges the loss function, so a larger $k$ is required to keep the constraint valid. This implies the model of the convex $\ell$ is randomization-proof. 

% On the contrary, if $\ell$ is concave, the inequality is reversed, and so is the conclusion.

% \hfill \Halmos

\subsection{Proof of Lemma \ref{lemma:equivalence_dom_sub}}
\label{sec:proof_fenchel_young}
Consider $\BFp \in  \{\BFp \in \mathbb{R}^n| \BFp \in \partial f(\BFx), \exists \BFx \in \dom{f}\}$. Let $\BFp \in \partial f(\BFx^\dagger)$. By the equality condition of Fenchel-Young inequality, we have 
$$
    f^*(\BFp) = \BFp^T \BFx^\dagger - f(\BFx^\dagger) < \infty.
$$
Therefore, we have $\{\BFp \in \mathbb{R}^n| \BFp \in \partial f(\BFx), \exists \BFx \in \dom{f}\} \subseteq \dom{f^*}$.

Consider $\BFp\in \dom{f^*}$. Since $f^*(\BFp) < \infty$, there must exist a $\BFx^\dagger \in \dom{f}$ such that 
$$
    f^*(\BFp) = \sup_{\BFx \in \dom{f}} \BFp^T \BFx - f(\BFx) = \BFp^T \BFx^\dagger - f(\BFx^\dagger).
$$
For arbitrary $\BFx\in\dom{f}$, we have 
$$
     \BFp^T \BFx - f(\BFx) \leq f^*(\BFp) = \BFp^T \BFx^\dagger - f(\BFx^\dagger) \Rightarrow \BFp^T(\BFx - \BFx^\dagger) \leq f(\BFx) - f(\BFx^\dagger).
$$
Therefore, we have $\BFp \in \partial f(\BFx^\dagger)$. Hence, $\dom{f^*} \subseteq \{\BFp \in \mathbb{R}^n| \BFp \in \partial f(\BFx), \exists \BFx \in \dom{f}\} $.

\hfill \Halmos

\subsection{Proof of Lemma \ref{lemma:kr_duality}}
\label{sec:proof_kr_duality}
The proof is extended from the standard proof of the Kantorovich-Rubinstein duality (\cite{kantorovich1958space}). 

Consider the OT discrepancy as 
$$
    D_c(\phat, \ptrue) = \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \mathbb{E}_{\pi} [c(\BFphi, y, \hat{\BFphi}, \hat{y})].
$$
Let $f(\BFphi, y)$ and $g(\hat{\BFphi}, \hat{y})$ be the dual variables of the constraint of the marginal distributino in $\Pi(\mathbb{P}, \hat{\mathbb{P}})$. Let $\mu_{\mathbb{P}}$ and $\mu_{\hat{\mathbb{P}}}$ be the measures of $\mathbb{P}$ and $\hat{\mathbb{P}}$, respectively. Then, the Lagrangian of the $\inf$ problem is
\begin{align*}
    \mathcal{L}(\pi, \BFtheta) =& \mathbb{E}_{\pi} [c(\BFphi, y, \hat{\BFphi}, \hat{y})] 
    + \int_{\mathit{\Phi}\times \mathcal{Y}} \left(
        \mu_{\mathbb{P}}(\BFphi, y) - \int_{\mathit{\Phi}\times\mathcal{Y}} \pi(\BFphi, y, \hat{\BFphi}, \hat{y}) \diff (\hat{\BFphi}, \hat{y})
    \right) f(\BFphi, y) \diff(\BFphi, y) \\
    & \phantom{\mathbb{E}_{\pi} [c(\BFphi, y, \hat{\BFphi}, \hat{y})]} + \int_{\mathit{\Phi}\times \mathcal{Y}} \left(
        \mu_{\hat{\mathbb{P}}}(\hat{\BFphi}, \hat{y}) - \int_{\mathit{\Phi}\times\mathcal{Y}} \pi(\BFphi, y, \hat{\BFphi}, \hat{y}) \diff (\BFphi, y)
    \right) g(\hat{\BFphi}, \hat{y}) \diff(\hat{\BFphi}, \hat{y}) \\
    =& \mathbb{E}_{(\BFphi, y) \sim \mathbb{P}} [f(\BFphi, y)] + \mathbb{E}_{(\hat{\BFphi}, \hat{y}) \sim \hat{\mathbb{P}}} [g(\hat{\BFphi}, \hat{y})] \\
    & + 
    \int_{\mathit{\Phi}\times \mathcal{Y} \times \mathit{\Phi}\times \mathcal{Y}} \left(
        c(\BFphi, y, \hat{\BFphi}, \hat{y}) - f(\BFphi, y) - g(\hat{\BFphi}, \hat{y})
    \right)  \diff \pi(\BFphi, y, \hat{\BFphi}, \hat{y}) 
\end{align*}
Based on the strong duality of the infinite linear programming, we have
$$
    D_c(\phat, \ptrue) = \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \sup_{f, g} \mathcal{L}(\pi, f, g) = \sup_{f, g} \inf_{\pi \in \Pi(\mathbb{P}, \hat{\mathbb{P}})} \mathcal{L}(\pi, f, g) = \sup_{f(\BFphi, y) + g(\hat{\BFphi}, \hat{y}) \leq c(\BFphi, y, \hat{\BFphi}, \hat{y})} \mathbb{E}_{\mathbb{P}}[f(\BFphi, y)] + \mathbb{E}_{\hat{\mathbb{P}}}[g(\hat{\BFphi}, \hat{y})].
$$

Consider the $1$-Lipschitz functions $h(\BFphi, y)$ satisfying
$$
    h(\BFphi, y) - h(\BFphi', y') \leq | h(\BFphi, y) - h(\BFphi', y')| \leq  c(\BFphi, y, \BFphi', y').
$$
We have 
$$
    \mathbb{E}_{\mathbb{P}}[h(\BFphi, y)] - \mathbb{E}_{\hat{\mathbb{P}}}[h(\hat{\BFphi}, \hat{y})] \leq  \sup_{f(\BFphi, y) + g(\hat{\BFphi}, \hat{y}) \leq c(\BFphi, y, \hat{\BFphi}, \hat{y})} \mathbb{E}_{\mathbb{P}}[f(\BFphi, y)] + \mathbb{E}_{\hat{\mathbb{P}}}[g(\hat{\BFphi}, \hat{y})] =  D_c(\phat, \ptrue).
$$
Then, we define function $p(\BFphi, y) $ as 
$$
    p(\BFphi, y) = \inf_{\BFphi', y'} \left\{
        c(\BFphi, y, \BFphi', y') - g(\BFphi', y')
    \right\}.
$$
Notice $p(\BFphi, y) \geq f(\BFphi, y)$ for any $f$ satisfying $f(\BFphi, y) + g(\BFphi', y') \leq c(\BFphi, y, \BFphi', y')$. Therefore, $p(\BFphi, y)$ is well-defined. Then, we show that $p(\BFphi, y)$ is 1-Lipschitz. By the triangular inequality, we have
$$
    p(\BFphi, y) \leq c(\BFphi, y, \BFphi', y') - g(\BFphi', y') \leq c(\BFphi, y, \BFphi'', y'') + c(\BFphi'', y'', \BFphi', y') - g(\BFphi', y').
$$
This inequality holds for any $\BFphi'$ and $y'$, so we have
$$
    p(\BFphi, y) \leq c(\BFphi, y, \BFphi'', y'') + \inf_{\BFphi', y'} \left\{
        c(\BFphi'', y'', \BFphi', y') - g(\BFphi', y')
    \right\} \leq c(\BFphi, y, \BFphi'', y'') + p(\BFphi'', y''),
$$
which implies that 
$
    p(\BFphi, y) - p(\BFphi'', y'') \leq c(\BFphi, y, \BFphi'', y'').
$
Due to the symmetry of $c(\BFphi, y, \BFphi', y')$, we have $ p(\BFphi'', \BFy'') - p(\BFphi, y) \leq  c(\BFphi, y, \BFphi'', y'')$. Therefore, $p(\BFphi, y)$ is 1-Lipschitz. 

When $f(\BFphi, y) + g(\hat{\BFphi}, \hat{y}) \leq c(\BFphi, y, \hat{\BFphi}, \hat{y})$, we have
$$
    f(\BFphi, y) \leq p(\BFphi, y) \leq c(\BFphi, y, \BFphi, y) - g(\BFphi, y) = - g(\BFphi, y),
$$
and 
$$
    D_c(\phat, \ptrue) = \sup_{f(\BFphi, y) + g(\hat{\BFphi}, \hat{y}) \leq c(\BFphi, y, \hat{\BFphi}, \hat{y})} \mathbb{E}_{\mathbb{P}}[f(\BFphi, y)] + \mathbb{E}_{\hat{\mathbb{P}}}[g(\hat{\BFphi}, \hat{y})] \leq \mathbb{E}_{\mathbb{P}}[p(\BFphi, y)] - \mathbb{E}_{\hat{\mathbb{P}}}[p(\hat{\BFphi}, \hat{y})].
$$
This shows there exists a 1-Lipschitz function $p(\BFphi, y)$ such that $D_c(\phat, \ptrue) \leq \mathbb{E}_{\mathbb{P}}[p(\BFphi, y)] - \mathbb{E}_{\hat{\mathbb{P}}}[p(\hat{\BFphi}, \hat{y})]$. 

Consequently, we have 
$$
    D_c(\phat, \ptrue) \leq \sup_{\|h\|_L \leq 1} \left\{
        \mathbb{E}_{\mathbb{P}}[h(\BFphi, y)] - \mathbb{E}_{\hat{\mathbb{P}}}[h(\hat{\BFphi}, \hat{y})]
    \right\} \leq  D_c(\phat, \ptrue),
$$
which is exactly the Kantorovich-Rubinstein duality.

\hfill \Halmos
\end{APPENDIX}

\putbib

\end{bibunit}

\end{document}