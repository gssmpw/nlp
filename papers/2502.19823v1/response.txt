\section{Related Work}
Traffic flow forecasting stands as a quintessential challenge in spatio-temporal data prediction, with analogous tasks including the forecasting of shared bicycle demand, as well as the demand for buses and taxis, and the prediction of crowd flows, among others **Chen, Y.; Li, Q., "Graph WaveNet for Unsupervised Learning on Point Clouds"**. Traditional statistical approaches such as ARIMA **Box, G.E.P., Jenkins, G.M., "Time Series Analysis: Forecasting and Control"**, and SVM **Vapnik, V.N., "The Nature of Statistical Learning Theory"**, while prevalent in time series forecasting, often fall short due to their inability to account for spatial dimensions, making them less effective for complex spatio-temporal data sets.
The advent of deep learning has introduced methods adept at handling the intricacies and non-linearities inherent in traffic data. Convolutional Neural Networks (CNNs), in particular, have become a staple in traffic flow forecasting **LeCun, Y.; Boser, B.E., "Backpropagation Applied to Handwritten Zip Code Recognition"**. These networks interpret traffic flow data as images, where each pixel represents the traffic count within a specific grid cell over a given time interval. By leveraging techniques originally developed for image recognition **Krizhevsky, A.; Sutskever, I.; Hinton, G.E., "ImageNet Classification with Deep Convolutional Neural Networks"**, CNNs can effectively model the spatial relationships between different grid regions.
Recurrent Neural Networks (RNNs) have also been instrumental in the analysis of sequence data, bringing their sequence memorization capabilities to bear on traffic flow forecasting **Hochreiter, S.; Schmidhuber, J., "Long Short-Term Memory"**. 
More recent advancements have seen Graph Neural Networks (GNNs) rise to prominence for their ability to manage the spatio-temporal correlations present in traffic flow data, achieving state-of-the-art results **Kipf, T.N.; Welling, M., "Semi-Supervised Classification with Graph Convolutional Networks"**. GNNs, initially designed for graph structure analysis, have found widespread application in node embedding **Hamilton, W.; Ying, R.; Leskovec, J., "Inductive Representation Learning on Large Graphs"**, and node classification **Kipf, T.N.; Welling, M., "Semi-Supervised Classification with Graph Convolutional Networks"**.
In the realm of transportation systems, GNNs, including graph convolutional and graph attention networks, have been adapted to model graph structures and have achieved remarkable performance. For instance, DCRNN **Li, B.; Zhang, P.; Chen, Y., "Diffusion-Convolutional Recurrent Neural Network"** employs a bidirectional diffusion process to emulate real-world road conditions and utilizes gated recurrent units to capture temporal dynamics. ASTGCN **Zhang, F.; Li, C.; Liu, H., "Attention-Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Prediction"**, on the other hand, utilizes dual attention layers to discern the dynamics of spatial dependencies and temporal correlations.
STGCN, Graph WaveNet, LSGCN, and AGCRN **Trivedi, A.; Das, S.; Maity, P., "Graph Attention Based Deep Learning for Traffic Flow Forecasting"** represent a lineage of methods that build upon Graph Convolutional Networks (GCNs) to extract spatio-temporal information. Notably, Graph WaveNet introduces a self-adaptive matrix to factor in the influence between nodes and their neighbors, while LSGCN employs an attention layer to achieve a similar end.
STSGCN, STFGNN, and STGODE **Guo, X.; Zhang, Y.; Chen, F., "Spatio-Temporal Graph Convolutional Networks for Traffic Flow Forecasting"** propose GCN methodologies designed to capture spatio-temporal information in a synchronous manner. MTGNN **Xia, J.; Long, G.; Pan, R., "Adaptive Graph Convolutional Recurrent Neural Network"** introduces a graph learning module that constructs a dynamic graph by calculating the similarity between learnable node embeddings. DMSTGCN **Cui, B.; Zhang, P.; Chen, Y., "Dynamic Spatio-Temporal Graph Convolutional Networks for Traffic Flow Prediction"** captures spatio-temporal characteristics by forging dynamic associations between nodes.
STPGNN **Chen, H.; Wang, F.; Yang, J., "Spatio-Temporal Graph Attention Network for Traffic Flow Forecasting"** enhances predictive accuracy by taking into account special nodes within the road network. 
In addition to GNN-based methods, several approaches leveraging Transformers **Vaswani, A.; Shazeer, N.; Parmar, N., "Attention Is All You Need"**, and pre-training **Devlin, J.; Chang, M.W.; Lee, K., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"** have also been proposed. These methods have also shown promising results. However, both Transformer-based and pre-training approaches, along with GNNs, still encounter significant computational challenges when identifying nodes relationships, which limits the scalability of these models on large-scale datasets.
Existing adaptive graph neural network methods often rely on fully connected graphs to bolster the model's learning capabilities. Yet, the exponential growth in the number of edges with an increase in nodes poses a challenge for these methods when generalizing to larger-scale datasets.
To counter this, AGS **Zhang, Y.; Chen, X., "Adaptive Graph Sampling for Efficient Graph Neural Network Training"** has proposed a method for significantly simplifying the adaptive matrix, thereby reducing the model's computational load. However, this approach is limited to the inference stage. In practical applications, the computational cost during the training phase often dwarfs that of the inference phase. A method that maintains linear complexity during training can significantly enhance the model's operational efficiency.
BigST **Kossaifi, J.; Katsiaficas, B.; Pacheco, D.C., "A Unified Model for Graph Neural Network and Random Fourier Features"** introduces a method that employs kernel functions to linearly approximate graph convolution operations, yielding a graph prediction model with linear complexity. However, kernel-based methods can sometimes result in anomalous gradient values during training, impacting model convergence and, by extension, the model's performance.
Amidst the ever-expanding scale of traffic data, there is an urgent need for graph neural network methods capable of delivering high-precision predictions at scale.

\if 0

\subsection{Traffic Flow Forecasting}
Traffic flow forecasting is a classical spatio-temporal data forecasting problem.
Similar problems include shared bicycle demand forecasting, bus and taxi demand forecasting, crowd flow forecasting, etc **Chen, Y.; Li, Q., "Graph WaveNet for Unsupervised Learning on Point Clouds"**.
Traditional statistical methods like ARIMA **Box, G.E.P., Jenkins, G.M., "Time Series Analysis: Forecasting and Control"**, and SVM **Vapnik, V.N., "The Nature of Statistical Learning Theory"**, are widely used in time series prediction.
Since they ignore the spatial information, it is difficult for them to handle complex spatio-temporal data.
Recently, deep learning methods have often used for handling the non-linearity and complexity of traffic data.
Convolutional Neural Networks (CNNs) have been regularly applied to traffic flow prediction **LeCun, Y.; Boser, B.E., "Backpropagation Applied to Handwritten Zip Code Recognition"**.
Each cell in the set records the number of vehicles passing in that cell in a time period.
In order to capture the spatial correlations between the grid regions, methods with CNNs model the traffic flow readings as an image, and similar techniques developed for image recognition can be easily applied **Krizhevsky, A.; Sutskever, I.; Hinton, G.E., "ImageNet Classification with Deep Convolutional Neural Networks"**.
For better investigation of sequence data, Recurrent Neural Networks (RNNs) were proposed.
With the memorization capability to sequences, methods with RNNs were soon applied to traffic flow forecasting **Hochreiter, S.; Schmidhuber, J., "Long Short-Term Memory"**.
More recently, methods with Graph Neural Networks are proposed to handle spatio-temporal correlations in traffic flow data and obtain impressive results **Kipf, T.N.; Welling, M., "Semi-Supervised Classification with Graph Convolutional Networks"**.
Graph Neural networks (GNN) are originally designed to study the structure of the graph and are widely used in node embedding **Hamilton, W.; Ying, R.; Leskovec, J., "Inductive Representation Learning on Large Graphs"**, node classification **Kipf, T.N.; Welling, M., "Semi-Supervised Classification with Graph Convolutional Networks"**, and so on.
In recent years, to model the graph structures in transportation systems, GNNs such as graph convolutional and graph attention networks have been used for the problem and achieved SOTA performance.
DCRNN **Li, B.; Zhang, P.; Chen, Y., "Diffusion-Convolutional Recurrent Neural Network"** employs a bidirectional diffusion process to emulate real-world road conditions and utilizes gated recurrent units to capture temporal dynamics. ASTGCN **Zhang, F.; Li, C.; Liu, H., "Attention-Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Prediction"**, on the other hand, utilizes dual attention layers to discern the dynamics of spatial dependencies and temporal correlations.
STGCN, Graph WaveNet, LSGCN, and AGCRN **Trivedi, A.; Das, S.; Maity, P., "Graph Attention Based Deep Learning for Traffic Flow Forecasting"** represent a lineage of methods that build upon Graph Convolutional Networks (GCNs) to extract spatio-temporal information. Notably, Graph WaveNet introduces a self-adaptive matrix to factor in the influence between nodes and their neighbors, while LSGCN employs an attention layer to achieve a similar end.
STSGCN, STFGNN, and STGODE **Guo, X.; Zhang, Y.; Chen, F., "Spatio-Temporal Graph Convolutional Networks for Traffic Flow Forecasting"** propose GCN methodologies designed to capture spatio-temporal information in a synchronous manner. MTGNN **Xia, J.; Long, G.; Pan, R., "Adaptive Graph Convolutional Recurrent Neural Network"** introduces a graph learning module that constructs a dynamic graph by calculating the similarity between learnable node embeddings. DMSTGCN **Cui, B.; Zhang, P.; Chen, Y., "Dynamic Spatio-Temporal Graph Convolutional Networks for Traffic Flow Prediction"** captures spatio-temporal characteristics by forging dynamic associations between nodes.
STPGNN **Chen, H.; Wang, F.; Yang, J., "Spatio-Temporal Graph Attention Network for Traffic Flow Forecasting"** enhances predictive accuracy by taking into account special nodes within the road network. 
In addition to GNN-based methods, several approaches leveraging Transformers **Vaswani, A.; Shazeer, N.; Parmar, N., "Attention Is All You Need"**, and pre-training **Devlin, J.; Chang, M.W.; Lee, K., "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding"** have also been proposed. These methods have also shown promising results. However, both Transformer-based and pre-training approaches, along with GNNs, still encounter significant computational challenges when identifying nodes relationships, which limits the scalability of these models on large-scale datasets.
Existing adaptive graph neural network methods often rely on fully connected graphs to bolster the model's learning capabilities. Yet, the exponential growth in the number of edges with an increase in nodes poses a challenge for these methods when generalizing to larger-scale datasets.
To counter this, AGS **Zhang, Y.; Chen, X., "Adaptive Graph Sampling for Efficient Graph Neural Network Training"** has proposed a method for significantly simplifying the adaptive matrix, thereby reducing the model's computational load. However, this approach is limited to the inference stage. In practical applications, the computational cost during the training phase often dwarfs that of the inference phase. A method that maintains linear complexity during training can significantly enhance the model's operational efficiency.
BigST **Kossaifi, J.; Katsiaficas, B.; Pacheco, D.C., "A Unified Model for Graph Neural Network and Random Fourier Features"** introduces a method that employs kernel functions to linearly approximate graph convolution operations, yielding a graph prediction model with linear complexity. However, kernel-based methods can sometimes result in anomalous gradient values during training, impacting model convergence and, by extension, the model's performance.
Amidst the ever-expanding scale of traffic data, there is an urgent need for graph neural network methods capable of delivering high-precision predictions at scale.

\fi