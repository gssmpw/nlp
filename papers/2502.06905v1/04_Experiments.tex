\section{Experiments}
\label{sec:experiment}
\input{04A_Table_CIFAR10_100}

\subsection{Experimental Settings}
We assessed the performance of our proposed method in three key scenarios: image classification, image classification with noisy labels and corrupted images. In addition, we validate cross-architecture generalization on three-layer CNN, VGG-16~\citep{simonyan2015deepconvolutionalnetworkslargescale}, ResNet-18 and ResNet-50~\citep{he2015deepresiduallearningimage}.

\textbf{Hyperparameters.}
For training CIFAR-10 and CIFAR-100, we train ResNet-18 for 200 epochs with a batch size of 128. SGD optimizer with momentum of 0.9 and weight decay of 0.0005 is used. The learning rate is initialized as 0.1 and decays with the cosine annealing scheduler. As \citet{zhang2024spanning} show that smaller batch size boosts performance at high pruning rates, we also halved the batch size for 80\% pruning, and for 90\% we reduced it to one-fourth. For ImageNet-1k, ResNet-34 is trained for 90 epochs with a batch size of 256 across all pruning ratios. An SGD optimizer with a momentum of 0.9, a weight decay of 0.0001, and an initial learning rate of 0.1 is used, combined with a cosine annealing scheduler.

\textbf{Baselines.} The baselines considered in this study are listed as follows\footnote{Infomax~\citep{tan2025data} was excluded as it employs different base hyperparameters in the original paper compared to other baselines and does not provide publicly available code. See Appendix~\ref{Appendix_Technical_Details_of_Baselines} for more discussion.}: (1) Random; (2) Entropy~\citep{coleman2020selectionproxyefficientdata}; (3) Forgetting~\citep{toneva2018empirical}; (4) EL2N~\citep{gordon2021data}; (5) AUM~\citep{pleiss2020identifyingmislabeleddatausing}; (6) Moderate~\citep{xia2022moderate}; (7) Dyn-Unc~\citep{he2024large}; (8) TDDS~\citep{zhang2024spanning}; (9) CCS\mbox{~\citep{zheng2022coverage}}; and (10) $\mathbb{D}^2$~\citep{maharana2023d2}. To ensure a fair comparison, all methods were trained with the same base hyperparameters for training, and the best hyperparameters reported in their respective original works for scoring. Technical details are provided in the \cref{Appendix_Technical_Details_of_Baselines}. 


\begin{wrapfigure}[16]{R}{0.5\textwidth}
\vspace{-15pt}
    \begin{center}
        \includegraphics[width=0.9\linewidth]{Figures/total_computation_time.pdf}
    \end{center}
    \vspace{-8pt}
    \caption{Comparison in total time spent on CIFAR datasets.}
    \label{fig:total_time_consumed}
    % \vspace{-10pt}
\end{wrapfigure}
\vspace{3pt}
\subsection{Image Classification Benchmarks}
\cref{tbl:main_cifar} presents the test accuracy for image classification results on CIFAR-10 and CIFAR-100. Our pruning method consistently outperforms other baselines, particularly when combined with Beta sampling. While the DUAL score exhibits competitive performance in lower pruning ratios, its accuracy degrades with more aggressive pruning. Our Beta sampling effectively mitigates this performance drop.

Notably, the DUAL score only requires training a single model for \emph{only 30 epochs}, significantly reducing the computational cost. In contrast, the second-best methods, Dyn-Unc and CCS, rely on scores computed over a full 200-epoch training cycle, making them considerably less efficient. Even considering subset selection, score computation, and subset training, the total time remains less than a single full training run, as shown in Figure~\ref{fig:total_time_consumed}. Specifically, on CIFAR-10, our method achieves lossless pruning up to a 50\% pruning ratio while saving 35.5\% of total training time. 

\input{04B_Table_ImageNet}
\vspace{2pt}
We also evaluate our pruning method on the large-scale dataset, ImageNet-1k. The DUAL score is computed during training, specifically at epoch 60, which is 33\% earlier than the original train epoch used to compute scores for other baseline methods. As shown in \cref{tab:imagenet_results}, Dyn-Unc performs worse than random pruning across all pruning ratios, and we attribute this undesirable performance to its limited total training epochs (only 90), which is insufficient for Dyn-Unc to fully capture the training dynamics of each sample. In contrast, our DUAL score, combined with Beta sampling, outperforms all competitors while requiring the least computational cost. The DUAL score's ability to consider both training dynamics and the difficulty of examples enables it to effectively identify uncertain samples early in the training process, even when training dynamics are limited. Remarkably, for 90\% pruned Imagenet-1K, it maintains a test accuracy of 60.0\%, surpassing the previous state-of-the-art (SOTA) by a large margin.
\vspace{15pt}
\subsection{Experiments under More Realistic Scenarios}
\subsubsection{Label Noise and Image Corruption}
Data affected by label noise or image corruption are difficult and unnecessary samples that hinder model learning and degrade generalization performance. Therefore, filtering out these samples through data pruning is crucial. Most data pruning methods, however, either focus solely on selecting difficult samples based on example difficulty~\citep{gordon2021data, pleiss2020identifyingmislabeleddatausing, coleman2020selectionproxyefficientdata} or prioritize dataset diversity~\citep{zheng2022coverage, xia2022moderate}, making them unsuitable for effectively pruning such noisy and corrupted samples.

In contrast, methods that select uncertain samples while considering training dynamics, such as Forgetting \citep{toneva2018empirical} and Dyn-Unc~\citep{he2024large}, demonstrate robustness by pruning both the hardest and easiest samples, ultimately improving generalization performance, as illustrated in Figure~\ref{fig:label_noise_20_ratio}. However, since noisy samples tend to be memorized after useful samples are learned~\citep{arpit2017closer, jiang2020characterizing}, there is a possibility that those noisy samples may still be treated as uncertain in the later stages of training and thus be included in the selected subset.

The DUAL score aims to identify high-uncertainty samples early in training by considering both training dynamics and example difficulty. Noisy data, typically under-learned compared to other challenging samples during this phase, exhibit lower uncertainty (Figure~\ref{fig:label_noise_visualization}, Appendix~\ref{Appendix_labelnoise_experiments}). Consequently, our method effectively prunes these noisy samples.

To verify this, we evaluate our method by introducing a specific proportion of symmetric label noise~\citep{patrini2017making, xia2020robust, li2022selective} and applying five different types of image corruptions~\citep{wang2018iterative, hendrycks2019benchmarking, xia2021instance}. We use CIFAR-100 with ResNet-18 and Tiny-ImageNet with ResNet-34 for these experiments. On CIFAR-100, we test label noise and image corruption ratios of 20\%, 30\%, and 40\%. For Tiny-ImageNet, we use a 20\% ratio of label noise and image corruption. We prune the label noise-added dataset using a model trained for 50 epochs and the image-corrupted dataset with a model trained for 30 epochs using DUAL pruningâ€”both significantly lower than the 200 epochs used by other methods. For detailed experimental settings, please refer to Appendix~\ref{Appendix_Technical_Details_of_Ours}.
\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/label_noise20_ratio.pdf}
        \caption{\label{fig:label_noise_20_ratio}Pruned mislabeled data ratio}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/label_noise20_testacc.pdf}
        \caption{\label{fig:label_noise_20_testacc}Test accuracy under label noise}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/imagecorruption_20_testacc.pdf}
    \caption{\label{fig:imagecorruption_20_testacc}Test accuracy under image noise}
    \end{subfigure}
    \caption{\label{fig:labelnoise_main}The left figure shows the ratio of pruned mislabeled data under 20\% label noise on CIFAR-100 trained with ResNet-18. When label noise is 20\%, the optimal value (black dashed line) corresponds to pruning 100\% of mislabeled data at a 20\% pruning ratio. The middle and right figures depict test accuracy under 20\% label noise and 20\% image corruption, respectively. Our method effectively prunes mislabeled data near the optimal value while maintaining strong generalization performance. Results are averaged over five random seeds.}
    \vspace{-10pt}
\end{figure*}
As shown in Figure~\ref{fig:labelnoise_main}, the left plot demonstrates that DUAL pruning effectively removes mislabeled data at a ratio close to the optimal. Notably, when the pruning ratio is 10\%, nearly \emph{all pruned samples are mislabeled data}.
Consequently, as observed in Figure~\ref{fig:label_noise_20_testacc}, DUAL pruning leads to improved test accuracy compared to training on the full dataset, even up to a pruning ratio of 70\%. At lower pruning ratios, performance improves as mislabeled data are effectively removed, highlighting the advantage of our approach in handling label noise.
Similarly, for image corruption, our method prunes more corrupted data across all corruption rates compared to other methods, as shown in Figure~\ref{fig:imagecorruption_203040_ratio},~\ref{fig:imagecorruption_all} in Appendix~\ref{Appendix_imagecorruption_experiments}. As a result, this leads to higher test accuracy, as demonstrated in Figure~\ref{fig:imagecorruption_20_testacc}. 

Detailed results, including exact numerical values for different corruption rates and Tiny-ImageNet experiments, can be found in Appendix~\ref{Appendix_labelnoise_experiments} and \ref{Appendix_imagecorruption_experiments}.  Across all experiments, DUAL pruning consistently shows \emph{strong noise robustness} and outperforms other methods by a substantial margin.

\subsubsection{Cross-Architecture Generalization}
We also evaluate the ability to transfer scores across various model architectures. To be specific, if we can get high-quality example scores for pruning by using a simpler architecture than one for the training, our DUAL pruning would become even more efficient in time and computational cost. Therefore, we focus on the cross-architecture generalization from relatively small networks to larger ones with three-layer CNN, VGG-16, ResNet-18, and ResNet-50. Competitors are selected from each categorized group of the pruning approach: EL2N from difficulty-based, Dyn-Unc from uncertainty-based, and CCS from the geometry-based group. 

For instance, we get training dynamics from the ResNet-18 and then calculate the example scores. Then, we prune samples using scores calculated from ResNet-18, and train selected subsets on ResNet-50. The result with ResNet-18 and ResNet-50 is described in Table~\ref{tab:04-cross-arch-r18-r50}. Surprisingly, the coreset shows competitive performance to the baseline, where the baseline refers to the test accuracy after training a coreset constructed based on the score calculated from ResNet-50.
For all pruning cases, we observe that our methods reveal the highest performances. Specifically, when we prune 70\% and 90\% of the original dataset, we find that all other methods fail, showing worse test accuracies than random pruning. 


\begin{table}[t]
\caption{Cross-architecture generalization performance on CIFAR-100 from ResNet-18 to ResNet-50. We report an average of five runs. `R50 $\rightarrow$ R50' stands for score computation on ResNet-50, as a baseline.}
\label{tab:04-cross-arch-r18-r50}
\setlength{\tabcolsep}{3.1pt}
\centering
\begin{tabular}{lcccc}
    \toprule
    \multicolumn{1}{c}{} & \multicolumn{4}{c}{ResNet-18 $\rightarrow$ ResNet-50} \\
    \midrule
    Pruning Rate ($\rightarrow$) & 30\% & 50\% & 70\% & 90\% \\
    \hline
    \midrule
    Random & 74.47 \scriptsize{$\pm 0.67$} & 70.09 \scriptsize{$\pm 0.42$} & 60.06 \scriptsize{$\pm 0.99$} & 41.91 \scriptsize{$\pm 4.32 $} \\
    EL2N  & 76.42 \scriptsize{$\pm 1.00$} & 69.14 \scriptsize{$\pm 1.00$} & 45.16 \scriptsize{$\pm 3.21$} & 19.63 \scriptsize{$\pm 1.15 $} \\
    Dyn-Unc  & 77.31 \scriptsize{$\pm 0.34$} & 72.12 \scriptsize{$\pm 0.68$} & 59.38 \scriptsize{$\pm 2.35$} & 31.74 \scriptsize{$\pm 2.31 $} \\
    CCS  & 74.78 \scriptsize{$\pm 0.66$} & 69.98 \scriptsize{$\pm 1.18$} & 59.75 \scriptsize{$\pm 1.41$} & 41.54 \scriptsize{$\pm 3.94 $} \\
    \midrule
    DUAL   & \textbf{78.03} \scriptsize{$\pm 0.83$} & 72.82 \scriptsize{$\pm 1.46$} & 63.08 \scriptsize{$\pm 2.45$} & 33.65 \scriptsize{$\pm 2.92 $} \\
    DUAL +$\beta$  & 77.82 \scriptsize{$\pm 0.65$} & \textbf{73.98} \scriptsize{$\pm 0.62$} & \textbf{66.36} \scriptsize{$\pm 1.66$} & \textbf{49.90} \scriptsize{$\pm 2.56 $} \\
    \hline
    \midrule
    DUAL (R50$\rightarrow$R50)  & 77.82 \scriptsize{$\pm 0.64$} & 73.66 \scriptsize{$\pm 0.85$} & 52.12 \scriptsize{$\pm 2.73 $} & 26.13 \scriptsize{$\pm 1.96 $} \\
    DUAL (R50$\rightarrow$R50)+$\beta$  & 77.57 \scriptsize{$\pm 0.23$} & 73.44 \scriptsize{$\pm 0.87$} & 65.17 \scriptsize{$\pm 0.96$} & 47.63 \scriptsize{$\pm 2.47 $} \\
    \bottomrule
    \vspace{-10pt}
\end{tabular}
\end{table}

We also test the cross-architecture generalization performance with three-layer CNN, VGG-16, and ResNet-18 in Appendix~\ref{Appendix_cross_architecture}. 
Even for a simple model like three-layer CNN, we see our methods show consistent performance, as can be seen in Table~\ref{tab:cnn-to-resnet18} in Appendix~\ref{Appendix_cross_architecture}. This observation gives rise to an opportunity to develop some small proxy networks to get example difficulty with less computational cost. 
Transfer across models with similar capacities, $e.g.$ from VGG-16 to ResNet-18 and vice versa, also supports the verification of cross-architecture compatibility.   

\subsection{Ablation Studies}
\paragraph{Hyperparameter Analysis.}
\begin{wrapfigure}[10]{r}{0.5\textwidth}
\vspace{-5mm}
    \begin{center}
        \includegraphics[width=0.92\linewidth]{Figures/TestAcc_TC_varying.pdf}
    \end{center}
    \vspace{-5mm}
    \caption{\textbf{Left}: Varying T with $J=10$ and $c_\gD=4$. \textbf{Right}: Varying $c_\gD$ with $T=30$ and $J=10$.}
    \label{fig:TC_varying}
\end{wrapfigure}
Here, we investigate the robustness of our hyperparameters, $T$, $J$, and $c_\gD$. We fix $J$ across all experiments, as it has minimal impact on selection, indicating its robustness (Fig~\ref{fig:J_varying}, Appendix~\ref{Appendix_Experiments}). In Figure~\ref{fig:TC_varying}, we assess the robustness of $T$ by varying it from 20 to 200 on CIFAR-100. We find that while $T$ remains highly robust in earlier epochs, increasing $T$ degrades generalization performance. This is expected, as larger $T$ overemphasizes difficult samples due to our difficulty-aware selection.  Thus, pruning in earlier epochs (from 30 to 50) proves to be more effective and robust. For the $c_\gD$, we vary it from 3 to 7 and find robustness, especially in the aggressive pruning regime. All results are averaged across three runs.

\paragraph{Beta Sampling Analysis.}
Next, we study the impact of our proposed pruning-ratio-adaptive Beta sampling on existing score metrics. We apply our Beta sampling strategy to prior score-based methods, including Forgetting, EL2N, and Dyn-Unc, using the CIFAR10 and CIFAR100 datasets. By comparing our sampling approach with vanilla threshold pruning, which selects only the highest-scoring samples, we observe that prior score-based methods become remarkably comparable to random pruning after Beta sampling is adjusted (see \cref{tab:abl_beta_cifar10_100_90_main}).

\begin{table*}[t]
\caption{Comparison on CIFAR-10 and CIFAR-100 for $90\%$ pruning rate. 
We report average accuracy with five runs. The best performance is in bold in each column.}
\label{tab:abl_beta_cifar10_100_90_main}
\setlength{\tabcolsep}{4.5pt}
\centering
\begin{tabular}{lcc|cc}
    \toprule
    \multicolumn{1}{c}{} & \multicolumn{2}{c}{CIFAR-10} & \multicolumn{2}{c}{CIFAR-100} \\
    \midrule
    Method & Thresholding & $\beta$-Sampling & Thresholding & $\beta$-Sampling \\
    \midrule
    Random &  \textbf{83.74}\scriptsize{$\pm$0.21} & 83.31 (-0.43) \scriptsize{$\pm$0.14} & \textbf{45.09} \scriptsize{$\pm$1.26} & 51.76 (+6.67) \scriptsize{$\pm$0.25} \\
    EL2N &  38.74 \scriptsize{$\pm$0.75} & 87.00 (+48.26) \scriptsize{$\pm$0.45} & 8.89 \scriptsize{$\pm$0.28} & 53.97 (+45.08)  \scriptsize{$\pm$0.63}  \\
    Forgetting &  46.64 \scriptsize{$\pm$1.90} & 85.67 (+39.03) \scriptsize{$\pm$0.13} & 26.87 \scriptsize{$\pm$0.73} & 52.40 (+25.53) \scriptsize{$\pm$0.43} \\
    Dyn-Unc &  59.67 \scriptsize{$\pm$1.79} & 85.33 (+32.14) \scriptsize{$\pm$0.20} & 34.57 \scriptsize{$\pm$0.69} & 51.85 (+17.28) \scriptsize{$\pm$0.35}   \\
    \midrule
    Ours & 54.95 \scriptsize{$\pm$0.42} & \textbf{87.09} (+31.51) \scriptsize{$\pm$0.36} & 34.28 \scriptsize{$\pm$1.39} & \textbf{54.54} (+20.26) \scriptsize{$\pm$0.09}  \\
    \bottomrule
\end{tabular}
\end{table*}


Even adapted for random pruning, our Beta sampling proves to perform well. Notably, EL2N, which performs poorly on its own, becomes significantly more effective when combined with our sampling method. Similar improvements are also seen with Forgetting and Dyn-Unc scores. This is because our proposed Beta sampling enhances the diversity of selected samples in turn, especially when used with example difficulty-based methods. More results conducted for 80\% pruning cases are included in the \cref{Appendix_beta_samapling}.
    

\paragraph{Additional Analysis.}
In addition to the main results presented in this paper, we also conducted various experiments to further explore the effectiveness of our method. These additional results include an analysis of coreset performance under a time budget ($e.g.$ other score metrics are also computed by using training dynamics up to epoch 30) and Spearman rank correlation was calculated between individual scores and the averaged score across five runs to assess the consistency of scores for each sample. Furthermore, additional results in extreme cases, 30\% and 40\% of label noise and image corruption can be found in \cref{Appendix_Experiments}.