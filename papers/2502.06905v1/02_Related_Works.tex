\section{Related Works}
Data pruning aims to remove redundant examples, keeping the most informative subset of samples, namely the coreset.
Research in this area can be broadly categorized into two groups: \textit{score-based} and \textit{geometry-based} methods. Score-based methods define metrics representing the difficulty or importance of data points to prioritize samples with high scores. Geometry-based methods, whereas, focus more on keeping a good representation of the true data distribution.
Recent studies proposed \textit{hybrid} methods that incorporate the example difficulty score with the diversity of coreset.

\noindent\textbf{Score-based.}~EL2N~\citep{gordon2021data} calculates $L2$ norms of the error vector as an approximation of the gradient norm.
Entropy~\citep{coleman2020selectionproxyefficientdata} quantifies the information contained in the predicted probabilities at the end of training. However, the outcomes of such ``snapshot'' methods differ significantly from run to run, making it difficult to obtain a reliable score in a single run, as can be seen in \cref{fig:rank_corr}, \cref{Appendix_Experiments}.

Methods using training dynamics offer more reliability as they incorporate information throughout an entire run of training. Forgetting~\citep{toneva2018empirical} score counts the number of forgetting events, a correct prediction on a data point is flipped to a wrong prediction during training. AUM~\citep{pleiss2020identifyingmislabeleddatausing} accumulates the gap between the target probability and the second-highest prediction probability. 
Dyn-Unc~\citep{he2024large}, which strongly inspired our approach, prioritizes the uncertain samples rather than typical easy samples or hard ones during model training. The prediction uncertainty is measured by the variation of predictions in a sliding window, and the score averages the variation throughout the whole training process.
TDDS~\citep{zhang2024spanning} averages differences of Kullback-Leibler divergence loss of non-target probabilities for $T$ training epochs, where $T$ is highly dependent on the pruning ratio.
Taking the training dynamics into account proves useful for pruning because it allows one to differentiate informative but hard samples from ones with label noise \citep{he2024large}. However, despite the stability and effectiveness of these methods, they fail to provide cost-effectiveness as it requires training the model on the entire dataset.

\noindent\textbf{Geometry-based.}~Geometry-based methods focus on reducing redundancy among selected samples to provide better representation. 
SSP~\citep{sorscher2022beyond} selects the samples most distant from k-means cluster centers, while Moderate~\citep{xia2022moderate} focuses on samples with scores near the median.
However, these methods often compromise generalization performance as they underestimate the effectiveness of difficult examples.

Recently, hybrid approaches have emerged that harmonize both difficulty and diversity. CCS~\citep{zheng2022coverage} partitions difficulty scores into bins and selects an equal number of samples from each bin to ensure a balanced representation.
$\mathbb{D}^2$~\citep{maharana2023d2} employs a message-passing mechanism with a graph structure where nodes represent difficulty scores and edges encode neighboring representations, facilitating effective sample selection. BOSS~\citep{acharyabalancing} introduces a Beta function for sampling based on difficulty scores, which resembles our pruning ratio-adaptive sampling; we discuss the key difference in \cref{sec:betapruning}. 
Our DUAL pruning is a score-based approach as it considers difficulty and uncertainty by the score metric. Additionally, diversity is introduced through our proposed Beta sampling, making it a hybrid approach.