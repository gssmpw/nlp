\section{Theoretical Results}
\label{sec:DUAL_theorem}
Throughout this section, we will rigorously prove Theorem~\ref{thm:main_shorter_time}, providing the intuition that Dyn-Unc takes longer than our method to select informative samples.

\subsection{Proof of \texorpdfstring{\cref{thm:main_shorter_time}}{the theorem}}

Assume that the input and output (or label) space are $\gX = \sR^n$ and $\gY = \{\pm1\}$, respectively. Let the model $f: \gX \to \sR$ be of the form $f(\vx; \vw) = \vw^\top \vx$ parameterized by $\vw\in\sR^n$ with zero-initialization. Let the loss be the exponential loss, $\ell(z) = e^{-z}$. Exponential loss is reported to induce implicit bias similar to logistic loss in binary classification tasks using linearly separable datasets \citep{soudry2018implicit, gunasekar2018implicit}.


The task of the model is to learn a binary classification. The dataset $\gD$ consists only two points, i.e. $\gD = \left\{ \left(\vx_1, y_1^*\right) , \left(\vx_2, y_2^*\right) \right\}$, where without loss of generality $y_i^* = 1$ for $i=1, 2$.
% It is trivial that $\gD$ is linearly separable.
The model learns from $\gD$ with the gradient descent. The update rule, equipped with a learning rate $\eta > 0$, is:
\begin{align*}
\begin{split}
    \vw_0 &= 0\\
    \vw_{t+1} & = \vw_t - \eta\nabla_\vw\left[ \sum_{i=1}^2\ell\left( f\left(\vx_i;\vw_t\right)\right) \right]\\
    & = \vw_t + \eta\left( e^{-\vw_t^\top \vx_1}\vx_1 + e^{-\vw_t^\top \vx_2}\vx_2 \right).
\end{split}
\end{align*}

For brevity, denote the model output of the $i$-th data point at the $t$-th epoch as $y_t^{(i)} \coloneq f(\vx_i; \vw_t)$. The update rule for the parameter is simplified as:

\begin{equation}
\label{eq:synth_param_update_rule}
    \vw_{t+1} = \vw_t + \eta\left( e^{-y_t^{(1)}}\vx_1 + e^{-y_t^{(2)}}\vx_2 \right).
\end{equation}

We also derive the update rule of model output for each instance:
\begin{equation}
\label{eq:synth_output_update_rule}
\begin{cases}
    \begin{aligned}
        y_{t+1}^{(1)} &= \vw_{t+1}^\top \vx_1 = \left( \vw_t + \eta\left( e^{-y_t^{(1)}}\vx_1 + e^{-y_t^{(2)}}\vx_2 \right) \right)^\top \vx_1\\
        &= y_{t}^{(1)} + \eta e^{-y_t^{(1)}} \lVert \vx_1 \rVert^2 + \eta e^{-y_t^{(2)}}\langle \vx_1, \vx_2 \rangle,\\
        y_{t+1}^{(2)} &= y_{t}^{(2)} + \eta e^{-y_t^{(2)}} \lVert \vx_2 \rVert^2 + \eta e^{-y_t^{(1)}}\langle \vx_1, \vx_2 \rangle.
    \end{aligned}
\end{cases}
\end{equation}

Assume that $\vx_2$ is farther from the origin in terms of distance than $\vx_1$ is, but not too different in terms of angle. Formally,
\begin{assumption}
\label{eq:synth_assump}
    $\lVert \vx_2 \rVert > 1$, $4\lVert \vx_1 \rVert^2 < 2\langle \vx_1, \vx_2 \rangle < \lVert \vx_2 \rVert^2$. Moreover, $\langle \vx_1, \vx_2 \rangle < \lVert \vx_1 \rVert\lVert \vx_2 \rVert$.
\end{assumption}
Under these assumptions, as $\langle \vx_1, \vx_2 \rangle$ > 0, $\gD$ is linearly separable. Also, notice that $\vx_1$ and $\vx_2$ are not parallel. Our definition of a linearly separable dataset is in accordance with \citet{soudry2018implicit}. A dataset $\gD$ is linearly separable if there exists $\vw^*$ such that $\langle \vx_i, \vw^* \rangle > 0, \forall i$.

\begin{theorem}
\label{prop:smaller_time}
    Let $V_{t;J}^{(i)}$ be the variance and $\mu_{t;J}^{(i)}$ be the mean of $\sigma(y_t^{(i)})$ within a window from time $t$ to $t+J-1$. Denote $T_v$ and $T_{vm}$ as the first time when $V_{t;J}^{(1)} > V_{t;J}^{(2)}$ and $V_{t;J}^{(1)}(1-\mu_{t;J}^{(i)}) > V_{t;J}^{(2)}(1-\mu_{t;J}^{(2)})$ occurs, respectively. Under \Cref{eq:synth_assump},
    % if $\eta < \frac{1}{\langle \vx_1, \vx_2 \rangle + \lVert \vx_2 \rVert^2} \log \frac{\langle \vx_1, \vx_2 \rangle}{\lVert \vx_1 \rVert^2}$
    if $\eta$ is sufficiently small then $T_{vm} < T_v$.
\end{theorem}

By \citet{soudry2018implicit}, the learning is progressed as: $\vw_t$, $y_t^{(1)}$, and $y_t^{(2)}$ diverges to positive infinity (Lemma 1) but $\vw_t$ directionally converges towards $L_2$ max margin vector, $\hat{\vw} = \vx_1 / \|\vx_1\|^2$, or $\lim_{t\to\infty} \frac{\vw_t}{\lVert \vw_t \rVert} = \frac{\hat{\vw}}{\lVert \hat{\vw} \rVert}$ (Theorem 3). Moreover, the growth of $\vw$ is logarithmic, i.e. $\vw_t \approx \hat{\vw}\log t$. We hereby note that Theorem 3 of ~\citet{soudry2018implicit} holds for learning rate $\eta$ smaller than a global constant. Since our condition requires $\eta$ to be sufficiently small, we will make use of the findings of Theorem 3.


\begin{lemma}
\label{lem:dyt_diverge}
    $\Delta y_t \coloneq y_t^{(2)} - y_t^{(1)}$ is a non-negative, strictly increasing sequence. Also, $\lim_{t\to\infty}\Delta y_t = \infty$.
\end{lemma}
\begin{proof}
    \leavevmode

    1) Since $\vw_0 = 0$, $y_0^{(1)} = 0 = y_0^{(2)}$ so $\Delta y_0 = 0$. By \Cref{eq:synth_output_update_rule} and \Cref{eq:synth_assump}, $\Delta y_1 = y_1^{(2)} - y_1^{(1)} = \eta\left( \lVert \vx_2 \rVert^2 - \lVert \vx_1 \rVert^2 \right) > 0$.
    
    2)
    \begin{align*}
        \Delta y_{t+1} - \Delta y_{t} &= \eta \left[ e^{-y_{t}^{(2)}}\left( \lVert \vx_2 \rVert^2 - \langle \vx_1, \vx_2 \rangle \right) + e^{-y_{t}^{(1)}}\left( \langle \vx_1, \vx_2 \rangle - \lVert \vx_1 \rVert^2 \right) \right]\\
        &\eqcolon K_1 e^{-y_{t}^{(1)}} + K_2 e^{-y_{t}^{(2)}} > 0,
    \end{align*}

    for some positive constant $K_1, K_2$.
    As $y_t^{(i)} = \vw_t^\top \vx_i$ would logarithmically grow in terms of $t$, $e^{-y_{t}^{(i)}}$ is decreasing in $t$. Moreover, as $y_t^{(1)} = \vw_t^\top \vx_1 \approx \hat{\vw}^\top \vx_1 \log t = \log t$, $e^{-y_{t}^{(1)}}$ is (asymptotically) in scale of $t^{-1}$ and so is $\Delta y_{t+1} - \Delta y_{t}$. Hence, $\left\{ \Delta y_t \right\}$ is non-negative and increases to infinity.
\end{proof}

The notation $\Delta y_t \coloneq y_t^{(2)} - y_t^{(1)}$ will be used throughout this section. Next, we show that, under \Cref{eq:synth_assump}, $y_{t+1}^{(1)} < y_t^{(2)}$ for all $t > 0$.
\begin{lemma}
\label{lem:yt2_too_large}
    For all $t > 0$, $y_{t+1}^{(1)} < y_t^{(2)}$.
\end{lemma}
\begin{proof}
    \leavevmode
    Notice that:
    \begin{equation*}
    \begin{cases}
        y_{1}^{(1)} = \eta \lVert \vx_1 \rVert^2 + \eta \langle \vx_1, \vx_2 \rangle\\
        y_{1}^{(2)} = \eta \lVert \vx_2 \rVert^2 + \eta \langle \vx_1, \vx_2 \rangle.
    \end{cases}
    \end{equation*}
    
    1) $y_2^{(1)} < y_1^{(2)}$:
    \begin{align*}
        y_2^{(1)} &= y_{1}^{(1)} + \eta e^{-y_1^{(1)}} \lVert \vx_1 \rVert^2 + \eta e^{-y_1^{(2)}}\langle \vx_1, \vx_2 \rangle\\
        &= \eta \left(e^{-y_1^{(1)}} + 1\right) \lVert \vx_1 \rVert^2 + \eta \left(e^{-y_1^{(2)}} + 1 \right)\langle \vx_1, \vx_2 \rangle\\
        &< \eta \times 2\lVert \vx_1 \rVert^2 + \eta \times 2\langle \vx_1, \vx_2 \rangle\\
        &< \eta \langle \vx_1, \vx_2 \rangle + \eta \lVert \vx_2 \rVert^2 = y_1^{(2)}.
    \end{align*}
    
    2) Assume, for $t>0$, $y_{t+1}^{(1)} < y_t^{(2)}$.
    \begin{align*}
        y_{t+2}^{(1)} &= y_{t+1}^{(1)} + \eta e^{-y_{t+1}^{(1)}} \lVert \vx_1 \rVert^2 + \eta e^{-y_{t+1}^{(2)}}\langle \vx_1, \vx_2 \rangle\\
        &< y_{t}^{(2)} + \eta e^{-y_{t}^{(1)}} \lVert \vx_1 \rVert^2 + \eta e^{-y_{t}^{(2)}}\langle \vx_1, \vx_2 \rangle\\
        &< y_{t}^{(2)} + \eta e^{-y_{t}^{(1)}} \langle \vx_1, \vx_2 \rangle + \eta e^{-y_{t}^{(2)}} \lVert \vx_2 \rVert^2 = y_{t+1}^{(2)}.
    \end{align*}
\end{proof}
By \Cref{lem:yt2_too_large}, for all $t>0$, $\left( y_{t}^{(2)}, y_{t+1}^{(2)} \right)$ lies entirely on right-hand side of $\left( y_{t}^{(1)}, y_{t+1}^{(1)} \right)$, without any overlap.

We first analyze the following term: $\frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}}$. Observe that:
\begin{align}
\begin{split}
\label{eq:ratio_y}
    \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} &= \frac{\eta e^{-y_t^{(1)}} \lVert \vx_1 \rVert^2 + \eta e^{-y_t^{(2)}}\langle \vx_1, \vx_2 \rangle}{\eta e^{-y_t^{(2)}} \lVert \vx_2 \rVert^2 + \eta e^{-y_t^{(1)}}\langle \vx_1, \vx_2 \rangle}\\
    &= \frac{\lVert \vx_1 \rVert^2 + e^{-\Delta y_t}\langle \vx_1, \vx_2 \rangle}{\langle \vx_1, \vx_2 \rangle + e^{-\Delta y_t} \lVert \vx_2 \rVert^2}.
\end{split}
\end{align}

It is derived that the fraction is an increasing sequence in terms of $t$. For values $a, b, c, c', d, d' > 0, \frac{a+c}{b+d} < \frac{a+c'}{b+d'} \Leftrightarrow ad'+cb+cd' < ad+c'b+c'd$. Taking:
\begin{align*}
\begin{cases}
a = \lVert \vx_1 \rVert^2 \\ b = \langle \vx_1, \vx_2 \rangle
\end{cases}
\begin{cases}
c = e^{-\Delta y_t}\langle \vx_1, \vx_2 \rangle \\ d = e^{-\Delta y_t} \lVert \vx_2 \rVert^2
\end{cases}
\begin{cases}
c' = e^{-\Delta y_{t+1}}\langle \vx_1, \vx_2 \rangle \\ d' = e^{-\Delta y_{t+1}} \lVert \vx_2 \rVert^2
\end{cases}
,
\end{align*}

we have
\begin{align*}
    &ad'+cb+cd'\\
    =\; &e^{-\Delta y_{t+1}}\lVert \vx_1 \rVert^2 \lVert \vx_2 \rVert^2 + e^{-\Delta y_t}\langle \vx_1, \vx_2 \rangle^2 + e^{-\Delta y_t}e^{-\Delta y_{t+1}} \langle \vx_1, \vx_2 \rangle \lVert \vx_2 \rVert^2\\
    <\; &e^{-\Delta y_{t}}\lVert \vx_1 \rVert^2 \lVert \vx_2 \rVert^2 + e^{-\Delta y_{t+1}}\langle \vx_1, \vx_2 \rangle^2 + e^{-\Delta y_t}e^{-\Delta y_{t+1}} \langle \vx_1, \vx_2 \rangle \lVert \vx_2 \rVert^2\\
    =\; &ad+c'b+c'd.
\end{align*}
The inequality holds by \Cref{lem:dyt_diverge} and the Cauchy-Schwarz inequality. Taking the limit of \Cref{eq:ratio_y} as $t\to\infty$, the ratio converges to:
\begin{equation}
\label{eq:limit_ratio_y}
    R\coloneq \frac{\lVert \vx_1 \rVert^2}{\langle \vx_1, \vx_2 \rangle}.
\end{equation}
For the later uses, we also define the initial ratio, which is smaller than 1:
\begin{equation}
    R_0\coloneq \frac{y_{1}^{(1)} - y_0^{(1)}}{y_{1}^{(2)} - y_0^{(2)}} = \frac{\lVert \vx_1 \rVert^2 + \langle \vx_1, \vx_2 \rangle}{\langle \vx_1, \vx_2 \rangle + \lVert \vx_2 \rVert^2} \;(\leq R).
\end{equation}

Now we analyze a similar ratio of the one-step difference, but in terms of $\sigma\left(y_t^{(i)}\right)$ instead of $y_t^{(i)}$. There, $\sigma$ stands for the logistic function, $\sigma(z) = \left( 1+e^{-z} \right)^{-1}$. Notice that $\sigma'(z) = \sigma(z)\left(1-\sigma(z)\right)$.

\begin{lemma}
\label{lem:ratio_sig_y}
    $\gamma_V(t) \coloneq \frac{\sigma\left(y_{t+1}^{(1)}\right) - \sigma\left(y_t^{(1)}\right)}{\sigma\left(y_{t+1}^{(2)}\right) - \sigma\left(y_t^{(2)}\right)}$ monotonically increases to $+\infty$.
\end{lemma}
\begin{proof}
\begin{align*}
    \gamma_V(t) &= \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}}\frac{\sigma'\left(\zeta_t^{(1)} \right)}{\sigma'\left(\zeta_t^{(2)}\right)} \hspace{1em} (\text{for some } \begin{cases}
        \zeta_t^{(1)}\in\left(y_t^{(1)}, y_{t+1}^{(1)}\right)\\ \zeta_t^{(2)} \in \left(y_t^{(2)}, y_{t+1}^{(2)}\right)
    \end{cases} \text{by the mean value theorem.})\\
    % \intertext{for some $\zeta_t^{(1)}\in\left(y_t^{(1)}, y_{t+1}^{(1)}\right), \zeta_t^{(2)} \in \left(y_t^{(2)}, y_{t+1}^{(2)}\right)$ by the mean value theorem.}
    &\geq \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}}\frac{\sigma'\left(y_{t+1}^{(1)} \right)}{\sigma'\left(y_{t}^{(2)} \right)} \hspace{1em}(\because \sigma'\text{: decreasing on } \sR^+)\\
    &= \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} \frac{e^{-y_{t+1}^{(1)}}  \left( 1 + e^{-y_{t+1}^{(1)}} \right)^{-2}}{e^{-y_{t}^{(2)}} \left( 1 + e^{-y_{t}^{(2)}} \right)^{-2}}\\
    &\geq \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} \frac{1}{4} e^{y_{t}^{(2)} - y_{t+1}^{(1)}} \hspace{1em}(\because \left( 1+e^{-z} \right)^{-2}\in[1/4, 1] \text{ on }\sR^+)\\
    &\geq \frac{R_0}{4} e^{y_{t}^{(2)} - y_{t+1}^{(1)}}.
\end{align*}
As $y_{t}^{(2)} - y_{t+1}^{(1)} = y_{t}^{(2)} - y_{t}^{(1)}-\eta \left( e^{-y_{t}^{(1)}}\lVert \vx_1 \rVert^2 + e^{-y_{t}^{(2)}}\langle \vx_1, \vx_2 \rangle \right) \to \infty$, $\gamma_V(t)\to \infty$. For the part that proves $\gamma_V(t)$ is increasing, see \Cref{sec:gamma_v_inc}.
\end{proof}

Notice that $\gamma_V(0) < 1$. \Cref{lem:ratio_sig_y} implies that there exists (unique) $T_v>0$ such that for all $t \geq T_v$, $\gamma_V(t) > 1$ holds, or $\sigma\left(y_{t+1}^{(1)}\right) - \sigma\left(y_t^{(1)}\right) > \sigma\left(y_{t+1}^{(2)}\right) - \sigma\left(y_t^{(2)}\right)$. Recall that the (sample) variance of a finite dataset $\mathcal{T} = \left\{\vx_1, \cdots, \vx_n\right\}$ can be computed as:
\begin{equation*}
    \text{Var}[\mathcal{T}] = \frac{1}{n(n-1)}\sum_{i=1}^{n-1}\sum_{j=i+1}^n\left(\vx_i-\vx_j\right)^2.
\end{equation*}

Hence, for given $J$, (which corresponds to the window size,) for all $t\geq T_v$,
\begin{align*}
    V_{t;J}^{(1)}\coloneq\text{Var}\left[\left\{ \sigma\left( y_\tau^{(1)} \right) \right\}_{\tau=t}^{t+J-1}\right] &= \frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(1)}\right) - \sigma\left( y_{t+k}^{(1)} \right) \right]^2\\
    &> \frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(2)}\right) - \sigma\left( y_{t+k}^{(2)} \right) \right]^2\\
    &= \text{Var}\left[\left\{ \sigma\left( y_\tau^{(2)} \right) \right\}_{\tau=t}^{t+J-1}\right] \eqcolon V_{t;J}^{(2)}.
\end{align*}

It is easily derived that the converse is true: If $\gamma_V(t)$ is increasing and $V_{t;J}^{(1)} > V_{t;J}^{(2)}$ then $\gamma_V(t) > 1$.

We have two metrics: the first is only the variance (which corresponds to the Dyn-Unc score) and the second is the variance multiplied by the mean subtracted from 1 (which corresponds to the DUAL pruning score). Both the variance and the mean are calculated within a window of fixed length. At the early epoch, as the model learns $\vx_2$ first, both metrics show a smaller value for $\vx_1$ than that for $\vx_2$. At the late epoch, now the model learns $\vx_1$, so the order of the metric values reverses for both metrics.


Our goal is to show that the elapsed time of the second metric for the order to be reversed is shorter than that of the first metric. Let $T_{vm}$ be that time for our metric. We represent the mean of the logistic output within a window of length $J$ and from epoch $t$, computed for $i$-th instance by $\mu_{t;J}^{(i)}$:
\begin{equation}
    \mu_{t;J}^{(i)} \coloneq \frac{1}{J}\sum_{\tau=t}^{t+J-1}\sigma\left(y_\tau^{(i)}\right).
\end{equation}
For $t\geq T_v$, we see that the inequality still holds:
\begin{align*}
    &V_{t;J}^{(1)}\left(1 - \mu_{t;J}^{(1)}\right)\\
    &= \left[\frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(1)}\right) - \sigma\left( y_{t+k}^{(1)} \right) \right]^2\right] \left[1-\frac{1}{J}\sum_{\tau=t}^{t+J-1} \sigma\left(y_\tau^{(1)}\right) \right]\\
    &> \left[\frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(2)}\right) - \sigma\left( y_{t+k}^{(2)} \right) \right]^2\right] \left[1-\frac{1}{J}\sum_{\tau=t}^{t+J-1} \sigma\left(y_\tau^{(2)}\right) \right]\\
    &= V_{t;J}^{(2)}\left(1 - \mu_{t;J}^{(2)}\right).
\end{align*}
as for all $t$, $\sigma\left(y_t^{(2)}\right) > \sigma\left(y_t^{(1)}\right)$. Indeed, $T_{vm}\leq T_v$ holds, but is $T_{vm} < T_v$ true? To verify the question, we reshape the terms for a similar analysis upon $\mu$:
\begin{align}
\label{eq:var_comp_ineq}
\begin{split}
    &V_{t;J}^{(1)}\left(1 - \mu_{t;J}^{(1)}\right)\\
    &= \left[\frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(1)}\right) - \sigma\left( y_{t+k}^{(1)} \right) \right]^2\right] \left[\frac{1}{J}\sum_{\tau=t}^{t+J-1} 1-\sigma\left(y_\tau^{(1)}\right) \right]\\
    &> \left[\frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(2)}\right) - \sigma\left( y_{t+k}^{(2)} \right) \right]^2\right] \left[\frac{1}{J}\sum_{\tau=t}^{t+J-1} 1-\sigma\left(y_\tau^{(2)}\right) \right]\\
    &= V_{t;J}^{(2)}\left(1 - \mu_{t;J}^{(2)}\right).
\end{split}
\end{align}

The intuition is now clear: for any time before $T_v$, we know that the variance of $\vx_1$ is smaller than that of $\vx_2$, if the ratio corresponding to $1-\sigma(y)$ is large, the factors could be canceled out and the inequality still holds. If this case is possible, definitely $T_{vm}<T_v$.


Now let us analyze the ratio of $1-\sigma\left( y_t^{(i)} \right)$.
\begin{lemma}
\label{lem:ratio_comp_y}
    $\gamma_M(t) \coloneq \frac{1-\sigma\left(y_t^{(1)}\right)}{1 - \sigma\left(y_t^{(2)}\right)}$ increases to $+\infty$.
\end{lemma}
\begin{proof}
\begin{align*}
    \gamma_M(t) &= \frac{1+e^{y_t^{(2)}}}{1+e^{y_t^{(1)}}}\\
    &= e^{\Delta y_t} - \frac{e^{\Delta y_t}-1}{1+e^{y_t^{(1)}}}\\
    &\geq e^{\Delta y_t} - \frac{e^{\Delta y_t}}{1+e^{y_t^{(1)}}}\\
    &= e^{\Delta y_t} \sigma\left( y_t^{(1)} \right).
\end{align*}
The quantity in the last line indeed diverges to infinity. We now show that $\gamma_M(t)$ is increasing.
\begin{align*}
    \gamma_M(t) &= e^{\Delta y_t} - \frac{e^{\Delta y_t}}{1+e^{y_t^{(1)}}} + \frac{1}{1+e^{y_t^{(1)}}}\\
    &= e^{\Delta y_t} \sigma\left( y_t^{(1)} \right) + 1-\sigma\left( y_t^{(1)}\right)\\
    &= \left( e^{\Delta y_t} -1 \right) \sigma\left( y_t^{(1)} \right) + 1\\
    &< \left( e^{\Delta y_{t+1}} -1 \right) \sigma\left( y_{t+1}^{(1)} \right) + 1 = \gamma_M(t+1).
\end{align*}
\end{proof}

    Notice that, for $a>c>0, b>d>0, \frac{a-c}{b-d}< \frac{a}{b} \Leftrightarrow \frac{a}{b} < \frac{c}{d}$. Recall from \Cref{lem:ratio_sig_y} that $\gamma_V(t) = \frac{1-\sigma\left(y_t^{(1)}\right) - \left[ 1-\sigma\left(y_{t+1}^{(1)}\right)\right]}{1-\sigma\left(y_t^{(2)}\right) - \left[ 1-\sigma\left(y_{t+1}^{(2)}\right)\right]}$, hence $\gamma_V(t) < \gamma_M(t)$. Moreover,
\begin{align*}
    \gamma_V(t) &\leq \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} \frac{\sigma'\left(y_{t}^{(1)} \right)}{\sigma'\left(y_{t+1}^{(2)} \right)}\\
    &= \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} e^{y_{t+1}^{(2)} - y_t^{(1)}} \left( \frac{1+e^{-y_{t+1}^{(2)}}}{1+e^{-y_{t}^{(1)}}} \right)^2\\
    &\leq \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} e^{y_{t+1}^{(2)} - y_t^{(1)}} \left( \frac{1+e^{-y_{t+1}^{(2)}}}{1+e^{-y_{t}^{(1)}}} \right)
    \hspace{1em} \because \left( \frac{1+e^{-y_{t+1}^{(2)}}}{1+e^{-y_{t}^{(1)}}} \right) \in (0, 1].\\
    &= \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} e^{y_{t+1}^{(2)} - y_t^{(2)}} e^{\Delta y_t} \left( \frac{1+e^{-y_{t+1}^{(2)}}}{1+e^{-y_{t}^{(1)}}} \right)\\
    &\leq \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}} e^{y_{t+1}^{(2)} - y_t^{(2)}} e^{\Delta y_t} \left( \frac{1+e^{-y_{t}^{(2)}}}{1+e^{-y_{t}^{(1)}}} \right)\\
    &\leq Re^{y_1^{(2)} - y_0^{(2)}}\gamma_M(t).
\end{align*}

Now we revisit \Cref{eq:var_comp_ineq}.
\begin{equation}
\begin{aligned}
\label{eq:var_mean_comp_ineq}
    \left[\frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(1)}\right) - \sigma\left( y_{t+k}^{(1)} \right) \right]^2\right] \left[\frac{1}{J}\sum_{\tau=t}^{t+J-1} 1-\sigma\left(y_\tau^{(1)}\right) \right]\\
    > \left[\frac{1}{J(J-1)}\sum_{k=0}^{J-2} \sum_{l=k+1}^{J-1} \left[ \sigma\left( y_{t+l}^{(2)}\right) - \sigma\left( y_{t+k}^{(2)} \right) \right]^2\right] \left[\frac{1}{J}\sum_{\tau=t}^{t+J-1} 1-\sigma\left(y_\tau^{(2)}\right) \right].
\end{aligned}
\end{equation}
Assume, for the moment, that for some constant $C>1$, $\sigma\left(y_{t+1}^{(1)}\right) - \sigma\left(y_t^{(1)}\right) > C^{-1}\left[ \sigma\left(y_{t+1}^{(2)}\right) - \sigma\left(y_t^{(2)}\right) \right]$ but $1 - \sigma\left(y_t^{(1)}\right) > C^{2}\left[ 1 - \sigma\left(y_t^{(2)}\right) \right]$ for all large $t$. Then the ratio of the first term of the left-hand side of \cref{eq:var_mean_comp_ineq} to the first term of the right-hand side is greater than $C^{-2}$. Also, the ratio of the second term of the left-hand side of \cref{eq:var_mean_comp_ineq} to the second term of the right-hand side is greater than $C^2$. If so, we observe that 1) the inequality in \cref{eq:var_mean_comp_ineq} holds, 2) as the condition $\gamma_V(t) \geq 1$ for $T_v$ now changed to $\gamma_V(t) \geq C^{-1}$ for $T_{vm}$, hence $T_{vm} < T_v$ is guaranteed. It remains to find the constant $C$. Recall that, for all $t$,
\begin{equation*}
    \gamma_V(t) \leq Re^{y_1^{(2)} - y_0^{(2)}}\gamma_M(t).
\end{equation*}
If we set $Re^{y_1^{(2)} - y_0^{(2)}} = C^{-3}$, when $\gamma_V(t)$ becomes at least $C^{-1}$, we have $\gamma_M(t) \geq C^2$, satisfying the condition for $T_{vm}$.
If the learning rate is sufficiently small, then $\gamma_V(t)$ cannot significantly increase in one step, allowing $\gamma_V(t)$ to fall between $C^{-1}$ and $1$. 
Refer to \cref{fig:gamma_v} to observe that the graph of $\gamma_V(t)$ resembles that of a continuously increasing function.

\subsubsection{Monotonicity of \texorpdfstring{$\gamma_V(t)$}{Lemma 1.5}}
\label{sec:gamma_v_inc}

Recall that:
\begin{align*}
    \gamma_V(t) &\coloneq \frac{\sigma\left(y_{t+1}^{(1)}\right) - \sigma\left(y_t^{(1)}\right)}{\sigma\left(y_{t+1}^{(2)}\right) - \sigma\left(y_t^{(2)}\right)}\\
    &= \frac{y_{t+1}^{(1)} - y_t^{(1)}}{y_{t+1}^{(2)} - y_t^{(2)}}\frac{\sigma'\left(\zeta_t^{(1)} \right)}{\sigma'\left(\zeta_t^{(2)}\right)}
\end{align*}
for some $\zeta_t^{(1)}\in\left(y_t^{(1)}, y_{t+1}^{(1)}\right), \zeta_t^{(2)} \in \left(y_t^{(2)}, y_{t+1}^{(2)}\right)$ by the mean value theorem. The first term is shown to be increasing (to $R$). $\gamma_V(t)$ is increasing if the second term is also increasing in $t$.


Let $\Delta \zeta_t \coloneq \zeta_t^{(2)} - \zeta_t^{(1)}$. By \Cref{lem:yt2_too_large}, $\Delta \zeta_t > 0$.
\begin{align*}
    \frac{\sigma'\left(\zeta_t^{(1)} \right)}{\sigma'\left(\zeta_t^{(2)}\right)} &= \frac{e^{-\zeta_t^{(1)}}}{e^{-\zeta_t^{(2)}}}\left( \frac{1+e^{-\zeta_t^{(2)}}}{1+e^{-\zeta_t^{(1)}}} \right)^2\\
    &= e^{\Delta \zeta_t}\left( \frac{1+e^{-\zeta_t^{(1)}-\Delta\zeta_t}}{1+e^{-\zeta_t^{(1)}}} \right)^2.
\end{align*}
Define $g(x, y) \coloneq e^x\left(\frac{1+e^{-y-x}}{1+e^{-y}}\right)^2$. The partial derivatives satisfy:
\begin{equation*}
\begin{cases}
    \nabla_x g = \frac{\left( e^y-e^{-x} \right) \left( e^{x+y}+1 \right)}{\left( 1+e^{y} \right)^2}>0 \text{ for } x>0 \text{ if } y>0\\
    \nabla_y g = \frac{2e^{y-x}\left( e^x-1 \right) \left( e^{x+y}+1 \right)}{\left( 1+e^{y} \right)^3}>0, \forall y \text{ if } x>0.
\end{cases}
\end{equation*}
Notice that $\frac{\sigma'\left(\zeta_t^{(1)} \right)}{\sigma'\left(\zeta_t^{(2)}\right)} = g(\Delta \zeta_t, \zeta_t^{(1)})$. Since $\zeta_t^{(1)}\in\left(y_t^{(1)}, y_{t+1}^{(1)}\right)$ is (strictly) increasing and positive, if we show that $\Delta \zeta_t$ is increasing in $t$, we are done. Our result is that, if $y_{t+1}^{(i)} - y_{t}^{(i)}$ is small for $i=1,2$, $\zeta_t^{(i)} \approx \left( y_t^{(i)} + y_{t+1}^{(i)} \right)/2$ so $\Delta \zeta_t \approx \left( \Delta y_t + \Delta y_{t+1} \right)/2$, which is indeed increasing.


In particular, if (, assume for now) for all $t$,
\begin{align}
\label{eq:zeta_close_middle}
    \zeta_t^{(i)} &\in \left( \frac{2y_t^{(i)}+y_{t+1}^{(i)}}{3}, \frac{y_t^{(i)}+2y_{t+1}^{(i)}}{3} \right)\\ \nonumber
    \Rightarrow \; \Delta \zeta_t &\in \left( \frac{\Delta y_t + \Delta y_{t+1}}{3} + \frac{y_t^{(2)}-y_{t+1}^{(1)}}{3}, \frac{\Delta y_t + \Delta y_{t+1}}{3} +\frac{y_{t+1}^{(2)}-y_{t}^{(1)}}{3} \right)\\ \nonumber
    \Rightarrow \; \Delta \zeta_t &< \frac{\Delta y_t + \Delta y_{t+1}}{3} +\frac{y_{t+1}^{(2)}-y_{t}^{(1)}}{3} \\\nonumber
    &<\frac{\Delta y_{t+1} + \Delta y_{t+2}}{3} + \frac{y_{t+1}^{(2)}-y_{t+2}^{(1)}}{3} &(\dag)\\\nonumber
    &< \Delta \zeta_{t+1}
\end{align}
$(\dag)$ holds by \Cref{eq:synth_assump}:
\begin{align*}
    (\dag) \Leftrightarrow \;&\Delta y_{t+2} - \Delta y_{t} > y_{t+2}^{(1)} - y_{t}^{(1)}, \forall t\\
    \Leftarrow \;&\Delta y_{t+1} - \Delta y_{t} > y_{t+1}^{(1)} - y_{t}^{(1)}, \forall t\\
    \Leftrightarrow \;&\eta \left[ e^{-y_{t}^{(1)}}\left( \langle \vx_1, \vx_2 \rangle - \lVert \vx_1 \rVert^2 \right) + e^{-y_{t}^{(2)}}\left( \lVert \vx_2 \rVert^2 - \langle \vx_1, \vx_2 \rangle \right) \right] > \\ &\eta \left[ e^{-y_t^{(1)}} \lVert \vx_1 \rVert^2 + e^{-y_t^{(2)}}\langle \vx_1, \vx_2 \rangle \right], \forall t.
\end{align*}
It remains to show \Cref{eq:zeta_close_middle}. To this end, we use \Cref{lem:small_step_MVT}.
\begin{lemma}
\label{lem:small_step_MVT}
    Let $z_2>z_1(\geq0)$ be reals and $\zeta \in \left(z_1, z_2\right)$ be a number that satisfies the following: $\sigma\left(z_2\right)-\sigma\left(z_1\right) = \left(z_2-z_1\right)\sigma'(\zeta)$. Denote the midpoint of $\left(z_1, z_2\right)$ as $m \coloneq \left(z_1 + z_2\right)/2$. For $(1 \gg)\epsilon > 0$, if $z_2-z_1 < \mathcal{O} \left( \sqrt{\epsilon} \right)$ then $\lvert \zeta - m \rvert < \epsilon$.
\end{lemma}
\begin{proof}
    Expand the Taylor series of $\sigma$ at $m$ for $z_i$:
    \begin{equation*}
        \sigma\left(z_i\right) = \sigma(m) + \sigma'(m)\left(z_i-m\right) + \frac{1}{2!}\sigma''(m)\left(z_i-m\right)^2 + \frac{1}{3!}\sigma'''(m)\left(z_i-m\right)^3 + \mathcal{O}\left( \lvert z_i-m \rvert^4 \right)
    \end{equation*}
    We have:
    \begin{equation*}
        \sigma\left(z_2\right) - \sigma\left(z_1\right) = \sigma'(m)\left(z_2-z_1\right) + \frac{1}{24}\sigma'''(m)\left(z_2-z_1\right)^3 + \mathcal{O}\left( \left(z_2-z_1\right)^5 \right)
    \end{equation*}
    \begin{equation*}
        \sigma'\left(\zeta\right) = \sigma'(m) + \frac{1}{24}\sigma'''(m)\left(z_2-z_1\right)^2 + \mathcal{O}\left( \left(z_2-z_1\right)^4 \right)
    \end{equation*}

    Now, expand the Taylor series of $\sigma'$ at $m$ for $\zeta$:
    \begin{equation*}
        \sigma'\left(\zeta\right) = \sigma'(m) + \sigma''(m)\left(\zeta -m\right) + \frac{1}{2!}\sigma'''(m)\left(\zeta -m\right)^2 + \mathcal{O}\left( \lvert \zeta-m \rvert^3 \right)
    \end{equation*}

    Comparing the above two lines gives
    \begin{equation*}
        24 \sigma^{\prime \prime}(m)(\zeta-m)+12 \sigma^{\prime \prime \prime}(m)(\zeta-m)^2=\sigma^{\prime \prime \prime}(m)\left(z_2-z_1\right)^2+\mathcal{O}\left(\left(z_2-z_1\right)^3\right)
    \end{equation*}

    If $\sigma'''(m) = 0$ then $|\zeta - m| = \mathcal{O}\left(\left(z_2-z_1\right)^3\right)$, so $z_2-z_1 = \mathcal{O}\left( \sqrt{\epsilon} \right)$ is sufficient.
    
    Otherwise, we can solve the above for $\zeta-m$ from the fact that $\sigma''(z) < 0$ for $z > 0$:
    \begin{align*}
        12\sigma'''(m)(\zeta-m)&=-12 \sigma^{\prime \prime}(m)-\sqrt{\left(12 \sigma^{\prime \prime}(m)\right)^2+12 \sigma^{\prime \prime \prime}(m)\left[\sigma^{\prime \prime \prime}(m)\left(z_2-z_1\right)^2+\mathcal{O}\left(\left(z_2-z_1\right)^3\right)\right]}\\
        &=\frac{12\sigma'''(m)^2(z_2-z_1)^2}{24\sigma''(m)} + \mathcal{O}\left(\left(z_2-z_1\right)^3\right)
    \end{align*}
    The last equality is from the Taylor series $\sqrt{1+\frac{a}{x^2}}-1 = \frac{a}{2x^2} + \mathcal{O}\left( a^2x^{-4} \right)$, or $\sqrt{x^2+a}-x = \frac{a}{2x} + \mathcal{O}\left( a^2x^{-3} \right)$.
    We have $\lvert \zeta-m \rvert = \Theta\left( (z_2-z_1)^2 \right)$.
\end{proof}

For $\lvert \zeta_t^{(i)} - \left( y_t^{(i)} + y_{t+1}^{(i)} \right)/2 \rvert < \left( y_{t+1}^{(i)} - y_{t}^{(i)} \right)/6$, it suffices to have $y_{t+1}^{(i)} - y_{t}^{(i)} < \mathcal{O}\left( \sqrt{\left( y_{t+1}^{(i)} - y_{t}^{(i)} \right)/6} \right)$. 
This generally holds for sufficiently small $\eta$.

\vspace{1em}
\subsection{Experimental Results under Synthetic Setting}
\label{sec:syn_expt}
This section displays the figures plotted from the experiments on the synthetic dataset. We choose $\gX = \sR^2$ and $\gD = \left\{\ ((0.1, 0.1), 1), ((10, 5), 1) \right\}$. We fix $J=10$ and $\eta=0.01$ (unless specified). The total time of training $T$ is specified for each figure for neat visualization. In this setting, the upper bound for the learning rate is $\log(75)/126.5 \approx 0.034$.

\begin{figure}[ht]
    \centering
    \label{fig:syn_weight_evolve}
    \includegraphics[width=0.65\linewidth]{Figures/figure_synthetic/weight_evolution.pdf}
    \caption{Illustration of the evolution of the weight as the model learns from the two-point dataset. Observe that the weight learns $\vx_2$ first (closer to the orange dashed line), but gradually moves towards $\vx_1$ (closer to the brown dashed line). Here $T=10,000$.}
\end{figure}

We also empirically validate our statements of \cref{sec:gamma_v_inc}. \cref{fig:empirical_valid_inc} shows that $\gamma_V(t)$ and $\Delta\zeta_t$ are indeed increasing functions. \cref{fig:empirical_valid_midpoint} shows that $\zeta_t^{(i)}$ is sufficiently close to the midpoint of the interval it lies in, $\left(y_t^{(i)}, y_{t+1}^{(i)}\right)$.

\begin{figure}[htbp] 
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/figure_synthetic/gamma_V.pdf}
        \caption{$\gamma_V(t)$ in log scale.}
        \label{fig:gamma_v}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/figure_synthetic/Dzeta.pdf} 
        \caption{$\Delta\zeta_t$}
        \label{fig:delta_zetat}
    \end{subfigure}
    \caption{Empirical validations of the critical statements in \cref{sec:gamma_v_inc}. We ran experiments and plot the results that both $\gamma_V(t)$ (left---in log scale) and $\Delta \zeta_t$ (right) are an increasing sequence in terms of $t$. Here, we set $\eta=0.0005$. The reason is that if the learning rate is larger, $\sigma(y_t^{(2)})$ quickly saturates to 1, leading to a possibility of division by zero in $\gamma_V(t)$ and degradation in numerical stability of $\Delta \zeta_t$. Moreover, notice that the graph of $\gamma_V(t)$ in the log scale closely resembles that of $\Delta\zeta_t$ in the original scale.}
    \label{fig:empirical_valid_inc}
\end{figure}

\begin{figure}[htbp] 
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/figure_synthetic/zeta1_almost_midpoint.pdf}
        \caption{$|\zeta_t^{(1)} - (y_t^{(1)} + y_{t+1}^{(1)})/2|$ in log scale.}
        \label{fig:zeta1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/figure_synthetic/zeta2_almost_midpoint.pdf} 
        \caption{$|\zeta_t^{(2)} - (y_t^{(2)} + y_{t+1}^{(2)})/2|$ in log scale.}
        \label{fig:zeta2}
    \end{subfigure}
    \caption{Empirical validations of the critical statements in \cref{sec:gamma_v_inc}. We ran experiments and plot the results that both $\zeta_t^{(1)}$ (left) and $\zeta_t^{(2)}$ (right) are extremely close to the midpoint $(y_t^{(1)} + y_{t+1}^{(1)})/2$ and $(y_t^{(2)} + y_{t+1}^{(2)})/2$, compared to the interval length, respectively. In both plots, the blue line is the true distance while the orange line is the interval length. Here, we set $\eta=0.0005$ for the same reasoning of \cref{fig:empirical_valid_inc}. Empirically, the noise introduced by MVT is too small to deny that $\Delta \zeta_t$ is an increasing sequence.}
    \label{fig:empirical_valid_midpoint}
\end{figure}

\pagebreak
We also show that we can observe the ``flow'' of the moon plot as in \cref{fig:Moon_plot} for the synthetic dataset.

\begin{figure}[ht]
    \centering
    
    \includegraphics[width=0.98\linewidth]{Figures/figure_synthetic/synthetic_moonplot.pdf}
    \caption{Evolution of $\vx_1, \vx_2$ by their mean and standard deviation in prediction probabilities at different epochs. The marker `o' and `x' stands for $\vx_1$ and $\vx_2$, respectively. The red color indicates the sample to be selected, and the blue color indicates the sample to be pruned. Observe that the path that each data point draws resembles is of moon-shape. Here $T=30,000$.}
    \label{fig:syn_moon}
\end{figure}