 \section{Introduction}

Advancements in deep learning have been significantly driven by large-scale datasets. However, recent studies have revealed a power-law relationship between the generalization capacity of deep neural networks and the size of their training data \citep{hestness2017deep, rosenfeld2019constructive, gordon2021data}, meaning that the improvement of model performance becomes increasingly cost-inefficient as we scale up the dataset size.

Fortunately, \citet{sorscher2022beyond} demonstrate that the power-law scaling of error can be reduced to exponential scaling with Pareto optimal data pruning. The main goal of dataset pruning is to identify and retain the most informative samples while discarding redundant data points for training neural networks. This approach can alleviate storage and computational costs as well as training efficiency.

However, many existing pruning methods require training a model with a full dataset over a number of epochs to measure the importance of each sample, which ironically makes the pruning process more expensive than just training the model once on the original large dataset. 
For instance, several score-based methods \citep{toneva2018empirical, pleiss2020identifyingmislabeleddatausing, gordon2021data, he2024large, zhang2024spanning} require training as they utilize the dynamics from the whole training process. Some geometry-based methods, \citep{xia2022moderate, yang2024mind} leverage features from the penultimate layer of the trained model, therefore training a model is also required.
Hybrid methods \citep{zheng2022coverage, maharana2023d2, tan2025data}, which address the difficulty and diversity of samples simultaneously, still hold the same limitation as they use existing score metrics. Having to compute the dot product of learned features to get the neighborhood information makes them even more expensive to utilize.

\begin{figure*}[t]
    \raggedleft
    \includegraphics[width=0.95\linewidth]{Figures/main_plot_random_dash_notitle.pdf}
    \caption{Test accuracy comparison on CIFAR datasets (\textbf{Left}: Results for CIFAR-10, \textbf{Right}: Results for CIFAR-100). The color represents the total computation time, including the time spent training the original dataset for score calculation, for each pruning method. Blue indicates lower computation time, while red indicates higher computation time. Our method demonstrates its ability to minimize computation time while maintaining SOTA performance.}
    \label{fig:main_figure_cifar10_100}
\end{figure*}

To address this issue, we introduce \textbf{Difficulty and Uncertainty-Aware Lightweight} (\textbf{DUAL}) score, a metric that measures the importance of samples in the early stage of training by considering both prediction uncertainty and the example difficulty. Additionally, at the high pruning ratio---when the selected subset is scarce---we propose \textbf{pruning-ratio-adaptive Beta sampling}, which intentionally includes easier samples with lower scores to achieve a better representation of the data distribution~\citep{sorscher2022beyond, zheng2022coverage, acharyabalancing}.

Experiments conducted on CIFAR and ImageNet datasets under various learning scenarios verify the superiority of our method over previous SOTA methods.
Specifically, on ImageNet-1k, our method reduces the time cost to 66\% compared to previous methods while achieving a SOTA 60\% test accuracy at the pruning ratio of 90\%. On the CIFAR datasets, as illustrated in \cref{fig:main_figure_cifar10_100}, our method reduces the time cost to just 15\% while maintaining SOTA performance. Especially, our
method shows a notable performance when artificial noise is added.