\section{Detailed Explanation about Our Method}
\label{Appendix_explanation_of_dual_pruning}

In this section, we provide details on the implementation used across all experiments for reproducibility. Appendix~\ref{Appendix_algorithm} presents the full algorithm for our pruning method, DUAL, along with the Beta sampling strategy. Additionally, in a later subsection, we visualize the selected data using Beta sampling.

Recall that we define our sampling distribution $\mathrm{Beta}(\alpha_r, \beta_r)$ as follows:
\begin{align}
\label{eq:alpha_beta_detail}
\begin{split}
    \beta_r &= C\left(1-\mu_\gD\right)\left(1-r^{c_\gD}\right)\\
    \alpha_r &= C-\beta_r,
\end{split}
\end{align}
where $\mu_\gD\in[0, 1]$ is the probability mean of the highest DUAL score training sample. To ensure stability, we compute this as the average probability mean of the 10 highest DUAL score training samples. Additionally, as mentioned earlier, we set the value of $C$ to 15 across all experiments. For technical details, we add 1 to $\alpha_r$ to further ensure that the PDF remains stationary at low pruning ratios.

We illustrate the Beta PDF, as defined above, in Figure~\ref{fig:beta_pdf} for different values of $c_\gD$. In both subplots, we set $\mu_\gD$ as 0.25. The left subplot shows the PDF with $c_\gD=5.5$, which corresponds to the value used in CIFAR-10 experiments, while the right subplot visualizes the PDF where $c_\gD=4$, corresponding to CIFAR-100.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{Figures/beta_pdf.pdf}
    \caption{Visualization of Beta distribution for varying $c_\gD$. The left subplot corresponds to the value used in CIFAR-10, and the right subplot corresponds to the value used in CIFAR-100.}
    \label{fig:beta_pdf}
\end{figure}

\clearpage
\subsection{Visualization of Selected Data with Beta Sampling}
\label{Appendix_coreset_visualization}
Here we illustrate the sampling probability of being selected into coreset, selected samples, and pruned samples in each figure when using the DUAL score combined with Beta sampling. As the pruning ratio increases, we focus on including easier samples.

\begin{figure}[htbp] 
    \centering
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/cifar100_beta_sample_pr0.3.png}
        \caption{CIFAR-100 at pruning ratio 30\%}
        \label{fig:cifar_beta_pr30}
    \end{subfigure}

    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/cifar100_beta_sample_pr0.5.png} 
        \caption{CIFAR-100 at pruning ratio 50\%}
        \label{fig:cifar_beta_pr50}
    \end{subfigure}
    
    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/cifar100_beta_sample_pr0.7.png} 
        \caption{CIFAR-100 at pruning ratio 70\%}
        \label{fig:cifar_beta_pr70}
    \end{subfigure}

    \begin{subfigure}{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/cifar100_beta_sample_pr0.9.png} 
        \caption{CIFAR-100 at pruning ratio 90\%}
        \label{fig:cifar_beta_pr90}
    \end{subfigure}

    \caption{Pruning visualization on CIFAR-100.}
    \label{fig:cifar_coreset_visualization_beta}
\end{figure}
\clearpage

\subsection{Algorithm of Proposed Pruning Method}
\label{Appendix_algorithm}
The detailed algorithms for DUAL pruning and Beta sampling are as follows:
\begin{algorithm}[htb]
\begin{algorithmic}
    \caption{DUAL pruning + $\beta$-sampling}
    \label{alg:DUAL}

    \INPUT Training dataset $\gD$, pruning ratio $r$, dataset simplicity $c_\gD$, training epoch $T$, window length $J$.
    
    \OUTPUT Subset $\gS\subset\gD$ such that $\lvert\gS\rvert = (1-r)\lvert\gD\rvert$
    
    \FOR{$(\vx_i, y_i) \in \gD$}
        \FOR{$k = 1, \cdots, T-J+1$}
            \STATE $\Bar{\mathbb{P}}_k(\vx_i, y_i) \leftarrow \frac{1}{J}\sum_{j=0}^{J-1} \mathbb{P}_{k+j}(y_i\mid \vx_i)$
            
            \STATE $\mathbb{U}_k(\vx_i, y_i) \leftarrow \sqrt{ \frac{1}{J-1} \sum_{j=0}^{J-1} \left[ \mathbb{P}_{k+j}(y_i\mid \vx_i) - \Bar{\mathbb{P}}_k(\vx_i, y_i) \right] ^2}$
            
            \STATE $\mathrm{DUAL}_k(\vx_i, y_i) \leftarrow (1-\Bar{\mathbb{P}}_k(\vx_i, y_i)) \times \mathbb{U}_k(\vx_i, y_i)$
        
        \ENDFOR
        
        \STATE $\mathrm{DUAL}(\vx_i, y_i) \leftarrow \frac{1}{T-J+1}\sum_{k=1}^{T-J+1} \mathrm{DUAL}_k(\vx_i, y_i)$
        
    \ENDFOR
    
    \IF{$\beta$-sampling}
    \FOR{$(\vx_i, y_i) \in \gD$}
        \STATE $\bar{\mathbb{P}}(\vx_i, y_i) \leftarrow \frac{1}{T}\sum_{k=1}^T \mathbb{P}_k(y_i \mid \vx_i)$
        
        \STATE $\varphi \left(\bar{\mathbb{P}}(\vx_i, y_i)\right) \leftarrow$ PDF value of $\mathrm{Beta}(\alpha_r, \beta_r)$ from \cref{eq:alpha_beta}
        
        \STATE $\Tilde{\varphi} (\vx_i) \leftarrow $ $\varphi\left(\Bar{\mathbb{P}} (\vx_i, y_i)\right) \times \mathrm{DUAL}(\vx_i, y_i)$
    
    \ENDFOR
    
    \STATE $\Tilde{\varphi}(\vx_i) \leftarrow \frac{\Tilde{\varphi}(\vx_i)}{\sum_{j \in \gD} \Tilde{\varphi}(\vx_j)}$
    
    \STATE $\gS \leftarrow$ Sample $(1-r)\lvert \gD \rvert$ data points according to $\Tilde{\varphi}(\vx_i)$
    
    \ELSE
    
    \STATE $\gS \leftarrow$ Sample $(1-r)\lvert \gD \rvert$ data points with the largest $\mathrm{DUAL}(\vx_i, y_i)$ score
    
    \ENDIF 
\end{algorithmic}
\end{algorithm}