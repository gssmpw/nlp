\section{Technical Details}

\subsection{Details on Baseline Implementation}
\label{Appendix_Technical_Details_of_Baselines}
\textbf{EL2N}~\citep{paul2021deep} is defined as the error $L2$ norm between the true labels and predictions of the model. The examples with low scores are pruned out. We calculated error norms at epoch 20 from five independent runs, then the average was used for Ethe L2N score.

\textbf{Forgetting}~\citep{toneva2018empirical} is defined as the number of forgetting events, where the model prediction goes wrong after the correct prediction, up until the end of training. Rarely unforgotten samples are pruned out.

\textbf{AUM}~\citep{pleiss2020identifyingmislabeleddatausing} accumulates the margin, which means the gap between the target probabilities and the second largest prediction of a model. They calculate the margin at every epoch and then transform it into an AUM score at the end of the training. Here samples with small margins are considered as mislabeled samples, thus data points with small AUM scores are eliminated.

\textbf{Entropy}~\citep{coleman2020selectionproxyefficientdata} is calculated as the entropy of prediction probabilities at the end of training, and then the samples that have high entropy are selected into coreset. 

\textbf{Dyn-Unc}~\citep{he2024large} is also calculated at the end of training, with the window length $J$ set as 10. Samples with high uncertainties are selected into the subset after pruning.

\textbf{TDDS}~\citep{zhang2024spanning} adapts different hyperparameter for each pruning ratio. As they do not provide full information for implementation, we have no choice but to set parameters for the rest cases arbitrarily.
The provided setting for (pruning ratio, computation epoch $T$, the length of sliding window $K$) is (0.3, 70, 10), (0.5, 90, 10), (0.7, 80, 10), (0.8, 30, 10), and (0.9, 10, 5) for CIFAR-100, and for ImageNet-(0.3, 20, 10), (0.5, 20, 10), (0.7, 30, 20). Therefore, we set the parameter for CIFAR-10 as the same as CIFAR-100, and for 80\%, 90\% pruning on ImageNet-1k, we set them as (30, 20), following the choice for 70\% pruning.

\textbf{CCS}~\citep{zheng2022coverage} for the stratified sampling method, we adapt the AUM score as the original CCS paper does. They assign different hard cutoff rates for each pruning ratio. For CIFAR-10, the cutoff rates are (30\%, 0), (50\%, 0), (70\%, 10\%), (80\%, 10\%), (90\%, 30\%). For CIFAR-100 and ImageNet-1k, we set them as the same as in the original paper.

\textbf{D2}~\citep{maharana2023d2} for $\mathbb{D}^2$ pruning, we set the initial node using forgetting scores for CIFAR-10 and CIFAR-100, we set the number of neighbors $k$, and message passing weight $\gamma$ the same as in the original paper.

Note that, Infomax~\citep{tan2025data} was excluded as it employs different base hyperparameters in the original paper compared to other baselines and does not provide publicly available code.  Additionally, implementation details, such as the base score metric used to implement Infomax, are not provided. As we intend to compare other baseline methods with the same training hyperparameters, we do not include the accuracies of Infomax in our tables. %apply any skills to boost the generalization performance specific to our method. 
To see if we can match the performance of Infomax, we tested our method with different training details. For example, if we train the subset using the same number of iterations (not epoch) as the full dataset and use a different learning rate tuned for our method, then an improved accuracy of 59\% is achievable for 90\% pruning on CIFAR-100, which surpasses the reported performance of Infomax. For the ImageNet-1k dataset, our method outperforms Infomax without any base hyperparameter tuning, while also being cost-effective.

\subsection{Detailed Experimental Settings}
\label{Appendix_Technical_Details_of_Ours}
Here we clarify the technical details of our work.
For training the model on the full dataset and the selected subset, all parameters are used identically except for batch sizes. For CIFAR-10/100, we train ResNet-18 for 200 epochs with a batch size of 128, for each pruning ratio \{30\%, 50\%, 70\%, 80\%, 90\%\} we use different batch sizes with \{128, 128, 128, 64, 32\}. We set the initial learning rate as 0.1, the optimizer as SGD with momentum 0.9, and the scheduler as cosine annealing scheduler with weight decay 0.0005.
For training ImageNet, we use ResNet-34 as the network architecture. For all coresets with different pruning rates, we train models for 300,000 iterations with a 256 batch size. We use the SGD optimizer with 0.9 momentum and 0.0001 weight decay, using a 0.1 initial learning rate. The cosine annealing learning rate scheduler was used for training. For a fair comparison, we used the same parameters across all pruning methods, including ours. All experiments were conducted using an NVIDIA A6000 GPU. We also attach the implementation in the supplementary material.

For calculating the DUAL score, we need three parameters $T$, $J$, and $c_\gD$, each means score computation epoch, the length of the sliding window, and hyperparameter regarding the training dataset. We fix $J$ as 10 for all experiments.
We use ($T$, $J$, $c_\gD$) for each dataset as follows. For CIFAR-10, we use (30, 10, 5.5), for CIFAR-100, (30, 10, 4), and for ImageNet-1k, (60, 10, 11).
We first roughly assign the term $c_\gD$ based on the size of the initial dataset and by considering the relative difficulty of each, we set $c_\gD$ for CIFAR-100 smaller than that of CIFAR-10. For the ImageNet-1k dataset, which contains 1,281,167 images, the size of the initial dataset is large enough that we do not need to set $c_\gD$ to a small value to intentionally sample easier samples. Also, note that we fix the value of $C$ of Beta distribution at 15 across all experiments. A more detailed distribution, along with visualization, can be found in Appendix~\ref{Appendix_explanation_of_dual_pruning}.

Experiments with label noise and image corruption on CIFAR-100 are conducted under the same settings as described above, except for the hyperparameters for DUAL pruning. For label noise experiments, we set $T$ to 50 and $J$ to 10 across all label noise ratios. For $c_\gD$, we set it to 6 for 20\% and 30\% noise, 8 for 40\% noise. For image corruption experiments, we set $T$ to 30, $J$ to 10, and $c_\gD$ to 6 across all image corruption ratios. 

For the Tiny-ImageNet case, we train ResNet-34 for 90 epochs with a batch size of 256 across all pruning ratios, using a weight decay of 0.0001. The initial learning rate is set to 0.1 with the SGD optimizer, where the momentum is set to 0.9, combined with a cosine annealing learning rate scheduler. For the hyperparameters used in DUAL pruning,  we set $T$ to 60, $J$ to 10, and $c_\gD$ to 6 for the label noise experiments. For the image corruption experiments, we set $T$ to 60, $J$ to 10, and $c_\gD$ to 2. We follow the ImageNet-1k hyperparameters to implement the baselines.