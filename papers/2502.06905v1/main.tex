\documentclass{article}

\usepackage[margin = 0.9in]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[C]{Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty}
\usepackage[parfill]{parskip}
\usepackage[labelfont=bf]{caption}
\usepackage[compress, sort, round]{natbib}
\usepackage[x11names,table,xcdraw]{xcolor}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{color, colortbl, xcolor}
\usepackage[T1]{fontenc}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{hyperref}
% \usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue, pdfborder={0 0 0}]{hyperref}

\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
% Use this instead of \paragraph{}
\def\tightparagraph#1{\noindent\textbf{#1}~~}

\title{\bfseries Lightweight Dataset Pruning without Full Training via Example Difficulty and Prediction Uncertainty}


\author{
    %1
    Yeseul Cho \thanks{Authors contributed equally to this paper.} \\
    Graduate School of AI, KAIST \\
    \texttt{cyseul@kaist.ac.kr} \\ \\
    %3
    Changmin Kang \\
    Graduate School of AI, KAIST \\
    \texttt{cmkang8128@kaist.ac.kr} \\ \\
    \and
    %2
    Baekrok Shin \footnotemark[1] \\
    Graduate School of AI, KAIST \\
    \texttt{br.shin@kaist.ac.kr} \\ \\
    %4
    Chulhee Yun \\
    Graduate School of AI, KAIST \\
    \texttt{chulhee.yun@kaist.ac.kr} \\
}

\date{}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{remark}
\newtheorem*{remark}{\textbf{Remark}} % No Number\usepackage[capitalize,noabbrev]{cleveref}
\input{math_commands}
\usepackage[textsize=tiny]{todonotes}

\begin{document}
\pagenumbering{arabic}

\maketitle

\begin{abstract}
Recent advances in deep learning rely heavily on massive datasets, leading to substantial storage and training costs.
Dataset pruning aims to alleviate this demand by discarding redundant examples.
However, many existing methods require training a model with a full dataset over a large number of epochs before being able to prune the dataset, which ironically makes the pruning process more expensive than just training the model on the entire dataset.
To overcome this limitation, we introduce a \textbf{Difficulty and Uncertainty-Aware Lightweight (DUAL)} score, which aims to identify important samples from the early training stage by considering both example difficulty and prediction uncertainty. To address a catastrophic accuracy drop at an extreme pruning, we further propose a ratio-adaptive sampling using Beta distribution.
Experiments on various datasets and learning scenarios such as image classification with label noise and image corruption, and model architecture generalization demonstrate the superiority of our method over previous state-of-the-art (SOTA) approaches. Specifically, on ImageNet-1k, our method reduces the time cost for pruning to 66\% compared to previous methods while achieving a SOTA, specifically 60\% test accuracy at a 90\% pruning ratio. On CIFAR datasets, the time cost is reduced to just 15\% while maintaining SOTA performance.
\footnote{Our codebase is available at \href{https://github.com/behaapyy/dual-pruning.git}{\texttt{github.com/behaapyy/dual-pruning
}}.}
\end{abstract}

\input{01_Introduction}
\input{02_Related_Works}
\input{03_Proposed_Method}
\input{04_Experiments}
\input{05_Conclusion}

\bibliography{references}
\bibliographystyle{plainnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\input{100_Appendix_Baselines}
\clearpage
\input{101_Appendix_Experiments}
\clearpage
\input{102_Appendix_DUAL}
\clearpage
\input{103_Appendix_Theorem_for_DUAL}
\end{document}
