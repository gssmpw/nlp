\section{Proposed Methods}
\subsection{Preliminaries}
Let $\gD \coloneq \left\{\left(\bm{x}_1, y_1\right), \cdots, \left(\bm{x}_n, y_n\right) \right\}$ be a labeled dataset of $n$ training samples, where $\bm{x}\in\gX \subset\sR^d$ and $y\in\gY \coloneq\{1, \cdots, C\}$ are the data point and the label, respectively. $C$ is a positive integer and indicates the number of classes. For each labeled data point $(\vx, y) \in \gD$, denote $\sP_k(y \mid \vx)$ as the prediction probability of $y$ given $\vx$, for the model trained with $k$ epochs. Let $\gS\subset\gD$ be the subset retained after pruning. Pruning ratio $r$ is the ratio of the size of $\gD\setminus\gS$ to $\gD$, or $r = 1-\frac{\lvert\gS\rvert}{\lvert\gD\rvert}$.

The Dynamic Uncertainty (Dyn-Unc) score~\citep{he2024large} prefers the most uncertain samples rather than easy-to-learn or hard-to-learn samples during model training. The uncertainty score is defined as the average of prediction variance throughout training. They first define the uncertainty in a sliding window of length $J$:
\begin{align}
\label{eq:U_score_window}
    \mathrm{U}_k(\bm{x}, y) \coloneq & {\sqrt{\frac{\sum_{j=0}^{J-1}\left[ \sP_{k+j}(y \mid \vx) - \Bar{\sP}_k \right]^2}{J-1}}}
\end{align}
where $\Bar{\sP}_k := \frac{\sum_{j=0}^{J-1} \sP_{k+j}  (y \mid \vx)}{J}$ is the average prediction of the model over the window $[k, k+J-1]$. Then taking the average of the uncertainty throughout the whole training process leads to the Dyn-Unc score:
\begin{equation}
\label{eq:Dyn_score}
    \mathrm{U}(\bm{x}, y) = \frac{\sum_{k=1}^{T-J+1} \mathrm{U}_k(\bm{x}, y)}{T-J+1}.
\end{equation}

\subsection{Difficulty \& Uncertainty-Aware Lightweight Score}
\label{sec:DUAL_score_compute}
Following the approach of \citet{swayamdipta2020dataset} and \citet{he2024large}, we analyze data points from ImageNet-1k based on the mean and standard deviation of predictions during training, as shown in \cref{fig:Moon_plot}. We observe data points typically ``flow'' along the ``moon'' from bottom to top direction. Data points starting from the bottom-left region with a low prediction mean and low standard deviation move to the middle region with increased mean and standard deviation, and those starting in the middle region drift toward the upper-left region with a high prediction mean and smaller standard deviation. 
This phenomenon is closely aligned with existing observations that neural networks typically learn easy samples first, then treat harder samples later ~\citep{bengio2009curriculum, arpit2017closer, jiang2020characterizing, shen2022data}. In other words, we see that the uncertainty of easy samples rises first, and then more difficult samples start to move and show an increased uncertainty score.

\cref{fig:Moon_plot} further gives a justification for this intuition. 
In \cref{fig:moon_evolution_60}, samples with the highest Dyn-Unc scores calculated at epoch 60 move upward by the end of training at epoch 90.  
It means that if we measure the Dyn-Unc score at the early stage of training, it gives the highest scores to relatively easy samples rather than the most informative samples. It seems undesirable that it results in poor test accuracy on its coreset as shown in \cref{fig:computation_30_cifar100} of Appendix~\ref{Appendix_Experiments}.

To capture the most useful samples that are likely to contribute significantly to dynamic uncertainty during the whole training process (of 90 epochs) at the earlier training stage ($e.g.$ epoch of 60), we need to target the samples located near the bottom-right region of the moon-shaped distribution, as \cref{fig:moon_evolution_90} illustrates.
Inspired by this observation, we propose a scoring metric that identifies such samples by taking the \textit{uncertainty of the predictions} and the \textit{prediction probability} into consideration.

% \begin{minipage}[t]{0.5\textwidth}
\begin{figure}[t]
    \captionsetup{type=figure}
    \centering
    % First subfigure
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/moon_evolution_6090.png}
        \caption{Dyn-Unc score calculated at epoch $T=60$.}
        \label{fig:moon_evolution_60}
    \end{subfigure}
    % Second subfigure
    \begin{subfigure}{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/moon_evolution_9060.png}
        \caption{Dyn-Unc score calculated at epoch $T=90$.}
        \label{fig:moon_evolution_90}
    \end{subfigure}
    % Overall figure caption
    \captionof{figure}{The left column visualizes the prediction mean and standard deviation for each data point collected up to epoch 60, while the right column stands for epoch 90. Samples are colored by normalized Dyn-Unc score for each row. The expected cases of the original Dyn-Unc, where the score computation time step is the same as the final epoch of training, are marked with bold outlines.}
    \label{fig:Moon_plot}
\end{figure}

Here, we propose the \textbf{Difficulty and Uncertainty-Aware Lightweight (DUAL) score}, a measure that unites example difficulty and prediction uncertainty. We define the DUAL score of a data point $(\vx, y)$ at $k \in [T-J+1]$ as
\begin{equation}
\label{eq:DUAL_score_window}
    \mathrm{DUAL}_k(\bm{x}, y) \coloneq \\ \underbrace{\left( 1-\Bar{\sP}_k \right)}_{(a)} \underbrace{\sqrt{\frac{\sum_{j=0}^{J-1}\left[ \sP_{k+j}(y \mid \vx) - \Bar{\sP}_k \right]^2}{J-1}}}_{(b)}
\end{equation}
where $\Bar{\sP}_k := \frac{\sum_{j=0}^{J-1} \sP_{k+j}  (y \mid \vx)}{J}$ is the average prediction of the model over the window $[k, k+J-1]$. Note that $\mathrm{DUAL}_k$ is the product of two terms: $(a)$ $1 - \Bar{\sP}_k $ quantifies the example difficulty averaged over the window; $(b)$ is the standard deviation of the prediction probability over the same window, estimating the prediction uncertainty.

Finally, the $\rm DUAL$ score of $(\vx, y)$ is defined as the mean of $\mathrm{DUAL}_k$ scores over all windows:
\begin{equation}
\label{eq:DUAL_score}
    \mathrm{DUAL}(\bm{x}, y) = \frac{\sum_{k=1}^{T-J+1} \mathrm{DUAL}_k(\bm{x}, y)}{T-J+1}.
\end{equation}
The DUAL score reflects training dynamics by leveraging prediction probability across several epochs It provides a reliable estimation to identify the most uncertain examples. 

A theoretical analysis of a toy example further verifies the intuition above. Consider a linearly separable binary classification task $\left\{(\vx_i\in\sR^n, y_i\in\{\pm1\})\right\}_{i=1}^N$, where $N=2$ with $\lVert \vx_1\rVert$ $\ll$ $\langle \vx_1, \vx_2 \rangle < \lVert \vx_2 \rVert$. Without loss of generality, we set $y_1 = y_2 = +1$.
A linear classifier, $f(\vx; \vw) = \vw^\top \vx$, is employed as the model in our analysis. The parameter $\vw$ is initialized at zero and updated by gradient descent. 
\citet{soudry2018implicit} prove that the parameter of linear classifiers diverges to infinity, but directionally converges to the $L_2$ maximum margin separator. This separator is determined by the support vectors closest to the decision boundary. If a valid pruning method encounters this task, then it should retain the point closer to the decision boundary, which is $\vx_1$ in our case, and prune $\vx_2$.

Due to its large norm, $\vx_2$ exhibits higher score values in the early training stage for both uncertainty and DUAL scores.
It takes some time for the model to predict $\vx_1$ with high confidence, which increases its uncertainty level and prediction mean, as well as for scores of $\vx_1$ to become larger than $\vx_2$ as training proceeds.
In \cref{thm:main_shorter_time}, we show through a rigorous analysis that the moment of such a flip in order happens strictly earlier for DUAL than uncertainty.

\begin{theorem}[Informal]
\label{thm:main_shorter_time}
    Define $\sigma(z) \coloneq (1+e^{-z})^{-1}$. Let $V_{t;J}^{(i)}$ be the variance and $\mu_{t;J}^{(i)}$ be the mean of $\sigma(f(\vx_i; \vw_t))$ within a window from time $t$ to $t+J$. Denote $T_v$ and $T_{vm}$ as the first time step when $V_{t;J}^{(1)} > V_{t;J}^{(2)}$ and $V_{t;J}^{(1)}(1-\mu_{t;J}^{(i)}) > V_{t;J}^{(2)}(1-\mu_{t;J}^{(2)})$ occur, respectively. If the learning rate is small enough, then $T_{vm} < T_v$.
\end{theorem}


Technical details about \cref{thm:main_shorter_time} are provided in \Cref{sec:DUAL_theorem}, together with an empirical verification of the time-efficiency of DUAL pruning over Dyn-Unc.

\begin{wrapfigure}[13]{r}{0.53\textwidth}
\vspace{-5mm}
    \begin{center}
        \includegraphics[width=0.95\linewidth]{Figures/moon_evolution_dual_6090.png}
    \end{center}
    \caption{DUAL score also targets uncertain samples during the early epoch. In the end, selected samples are finally located in the most uncertain region.}
    \label{fig:moon_plot_dual}
\end{wrapfigure}

Empirically, as shown in Figure~\ref{fig:moon_plot_dual}, the DUAL score targets data points in the bottom-right region during the early training phase, which eventually evolve to the middle-rightmost part by the end of training. This verifies that DUAL pruning identifies the most uncertain region faster than Dyn-Unc \emph{both in theory and practice}. The distinction arises from the additional consideration of an example difficulty in our method. We believe that this adjustment leads to improved generalization performance compared to Dyn-Unc, as verified through various experiments in later sections.

However, score-based methods, including our method, 
suffer from limitations due to biased representations, leading to poor coreset test accuracy at high pruning ratios. 
ratios. To address this, we propose an additional strategy to adaptively select samples according to a ratio for pruning.

\subsection{Pruning Ratio-Adaptive Sampling}
\label{sec:betapruning}
Since the distribution of difficulty scores is dense in high-score samples, selecting only the highest-score samples may result in a biased model \citep{zhou2023probabilisticbilevelcoresetselection, maharana2023d2, choi2024bws}.
To address this, we design a sampling method to determine the subset $\gS\subset\gD$, rather than simply pruning the samples with the lowest score. We introduce a Beta distribution that varies with the pruning ratio. The primary objective of this method is to ensure that the selected subsets gradually include more easy samples into the coreset as the pruning ratio increases.

However, the concepts of ``easy'' and ``hard'' cannot be distinguished solely based on uncertainty, or DUAL score. To address this, we use the \emph{prediction mean} again for sampling. 
We utilize the Beta probability density function (PDF) to define the selection probability of each sample.
First, we assign each data point a corresponding PDF value based on its prediction mean and weight this probability using the DUAL score. The weighted probability with the DUAL score is then normalized so that the sum equals 1, and then used as the sampling probability. To be clear, sampling probability is for selecting samples, \emph{not for pruning}. Therefore, for each pruning ratio 
$r$, we randomly select $(1-r)\cdot n$ samples without replacement, where sampling probabilities are given according to the prediction mean and DUAL score as described. The detailed algorithm for our proposed pruning method with Beta sampling is provided in Algorithm~\ref{alg:DUAL}, Appendix~\ref{Appendix_explanation_of_dual_pruning}.

We design the Beta PDF to assign a sampling probability concerning a prediction mean as follows:
\begin{align}
\label{eq:alpha_beta}
\begin{split}
    \beta_r &= C \cdot (1-\mu_\gD)  \left(1-r^{c_\gD}\right)\\
    \alpha_r &= C-\beta_r, \\
\end{split}
\end{align}
where $C > 0$ is a fixed constant, and the $\mu_D$ stands for the prediction mean of the highest score sample.
Recalling that the mean of Beta distribution is $\frac{\alpha_r}{\alpha_r + \beta_r}$, the above choice makes the mean of Beta distribution move progressively with $r$, starting from $\mu_\gD$ ($r \simeq 0$, small pruning ratio) to one. In other words, with growing $r$, this Beta distribution becomes skewed towards the easier region ($r \rightarrow 1$, large pruning ratio), which in turn gives more weight to easy samples. 
The tendency of evolving should be different with datasets, thus a hyperparameter $c_\gD \geq 1$ is used to control the rate of evolution of the Beta distribution. Specifically, the choice of \( c_\gD \) depends on the complexity of the initial dataset. For smaller and more complex datasets, setting \( c_\gD \) to a smaller value retains more easy samples. For larger and simpler datasets, setting \( c_\gD \) to a larger value allows more uncertain samples to be selected.
(For your intuitive understanding, please refer to  \cref{fig:beta_pdf} and \cref{fig:cifar_coreset_visualization_beta} in the \cref{Appendix_explanation_of_dual_pruning}.) This is also aligned with the previous findings from \citet{sorscher2022beyond}; if the initial dataset is small, the coreset is more effective when it contains easier samples, while for a relatively large initial dataset, including harder samples can improve generalization performance.


\begin{remark}
BOSS~\citep{acharyabalancing} also uses the Beta distribution to sample easier data points during pruning, similar to our approach. However, a key distinction lies in how we define the Beta distribution's parameters, \(\alpha_r\) and \(\beta_r\). While BOSS adjusts these parameters to make the mode of the Beta distribution's PDF scale linearly with the pruning ratio $r$, we employ a non-linear combination. This non-linear approach has the crucial advantage of maintaining an almost stationary PDF at low pruning ratios. This stability is especially beneficial when the dataset becomes easier where there is no need to focus on easy examples. Furthermore, unlike previous methods, we define PDF values based on the prediction mean, rather than any difficulty score, which is another significant difference.
\end{remark}
