\section{Method} \label{sec:method}

\subsection{Preliminaries}

NeRF~\cite{mildenhall2021nerf} represents a 3D scene using a multi-layer perceptron network (MLP) that parameterizes a continuous 5D radiance field. This field $f:(\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)$, maps a 3D coordinate $\mathbf{x} \in \mathbb{R}^3$ and a viewing direction $\mathbf{d} \in \mathbb{R}^2$ to an emitted color $\mathbf{c} \in \mathbb{R}^3$ and volume density $\sigma \in \mathbb{R}$. 
To render 2D views, NeRF employs volumetric rendering. In particular, rays are cast through the scene, with $f(\mathbf{x}, \mathbf{d})$ queried along each ray. The volume rendering equation is then used to composite color and opacity:
\begin{align}
C(r) = \int T(t) \mathbf{c} (r(t)) \sigma(r(t)) dt,
\end{align}

\noindent where $r(t)$ parameterizes the ray, $T(t)$ is the accumulated transmittance, and $\mathbf{c}(r(t))$ and $\sigma(r(t))$ are the color and density at point t.
NeRF is trained by minimizing a reconstruction loss between images rendered from the field and a set of ground-truth images with known camera poses. This differentiable process allows NeRF to implicitly learn scene geometry and appearance for photorealistic rendering from unseen viewpoints.

When considering feature distillation~\cite{kobayashi2022decomposing, kerr2023lerf, ye2023featurenerf}, one additionally learns an MLP that parameterizes a continuous 3D feature field $f_{feat}$. Unlike $f$, $f_{feat} : \mathbf{x} \rightarrow \mathbf{f}$ maps a 3D coordinate $\mathbf{x} \in \mathbb{R}^3$ to a feature value $\mathbf{f} \in \mathbb{R}^n$. 2D feature maps can then be rendered similarly to color values, using $\sigma$ predicted by $f$. That is:
\begin{align}
F(r) = \int T(t) \mathbf{f}(r(t)) \sigma(r(t)) dt,
\end{align}

\noindent where $r(t)$, $T(t)$ and $\sigma(r(t))$ are as above and $\mathbf{f}(r(t))$ represents the predicted feature value at point $t$ (identical for every view direction). In addition to image-based reconstruction loss, one also minimizes a reconstruction loss between rendered features and ground-truth features, obtained by applying a pre-trained 2D feature extractor (such as DINOv2~\cite{oquab2023dinov2}) to ground-truth RGB views. 



\subsection{Structurally Disentangled Feature Fields}
Fig.~\ref{fig:pipeline} illustrates our method, which we elaborate on below. 

\input{figures/pipeline/pipeline_pdf.tex}

\smallskip
\noindent \textbf{Signed distance representation.} Similar to previous work~\cite{wang2023unisdf}, our model's backbone is an MLP computing the signed distance function $d(\mathbf{x})$ (SDF) at each location $\mathbf{x}$. The scene's surface is defined at the zero level set of the function. 
$\mathbf{x}$ undergoes a contraction transformation~\cite{barron2022mipnerf360, wang2023unisdf}, limiting its values to $[0, 2)$.

The density $\sigma(\mathbf{x})$ for volume rendering is computed by $\sigma(\mathbf{x}) = \alpha \Psi_\beta(d(\mathbf{x}))$, where $\Psi_\beta$ is the cumulative distribution of a Laplace distribution with zero mean and a scale parameter $\beta$ that is learned during optimization. The benefit of the SDF representation is that the normal of the scene surface can be easily derived as the gradient of the distance function:
\begin{align} \label{eq:normal}
\mathbf{n}(\mathbf{x}) = \nabla d(\mathbf{x}) / || \nabla d(\mathbf{x}) ||_2.
\end{align}

\noindent The normal is used as input to the components in our model for computing the disentangled feature representation. 

To better reconstruct high-frequency details and to speed up training, we use the iNGP hash encoding~\cite{muller2022instant}. Each location $\mathbf{x}$ is mapped to a high-dimensional feature vector, which is the concatenation of the iNGP's pyramid-level features. This feature vector is the input to our SDF MLP. To ease notations, we omit in this section the notation of $\mathbf{x}$ for location-dependent quantities.

\smallskip
\noindent \textbf{Radiance and features components.} We decompose the appearance of the scene into two elements: a view-independent color $\mathbf{c}_{indep}$ and a view-dependent reflected color $\mathbf{c}_{ref}$. The view-independent color is calculated as:
\begin{align} \label{eq:c_indep}
\mathbf{c}_{indep} = f_{indep}(\mathbf{n}, \mathbf{b}),
\end{align}

\noindent where $f_{indep}$ is a learned MLP, $\mathbf{n}$ is the normal defined in Eq.~\ref{eq:normal}, and $\mathbf{b}$ is a bottleneck vector output from the SDF MLP. This vector enables additional degrees of freedom to accommodate second-order effects, such as varying illumination over the scene~\cite{verbin2022refnerf, wang2023unisdf}. $\mathbf{c}_{indep}$ represents the view-independent color and accordingly is independent of the viewing direction $\mathbf{d}$.

The other color component, $\mathbf{c}_{ref}$, is view dependent. It is responsible for reflectivity scene regions and models the reflected radiance in the scene. Following previous work~\cite{verbin2022refnerf}, we calculate the reflection for the viewing direction about the normal:
\begin{align} \label{eq:omega_r}
\mathbf{\omega}_r = 2(\mathbf{\omega}_o \cdot \mathbf{n}) \mathbf{n} - \mathbf{\omega}_o,
\end{align}

\noindent where $\mathbf{\omega}_o = -\mathbf{\hat{d}}$, is a unit vector pointing to the camera from a point in
space, and $\mathbf{\hat{d}}$ is a viewing direction. Then, we use Integrated Directional Encoding (IDE) \cite{verbin2022refnerf} in the computation of $\mathbf{c}_{ref}$:
\begin{align} \label{eq:reflectence_color}
\mathbf{c}_{ref} = f_{ref}(\mathbf{n}, \mathbf{b}, \text{IDE}(\mathbf{\omega}_r, \kappa)),
\end{align}
\noindent where $\kappa = 1/\rho$, and $\rho$ is the surface roughness predicted by the SDF MLP. 
Finally, the rendered color is given by:
\begin{align}
\mathbf{c} = \text{tonemap}(\mathbf{c}_{indep} + \mathbf{c}_{ref}),
\label{eq:learnable_weights_1}
\end{align}

\noindent where $\text{tonemap}(\cdot)$ is a tone mapping function converting linear color to the sRGB format and clipping the output to the range $[0, 1]$. Similarly, we decompose the feature field of the scene $\mathbf{f}$ as follows:
\begin{align}
\mathbf{f} = \mathbf{f}_{indep} + \mathbf{f}_{ref},
\label{eq:learnable_weights_2}
\end{align}

\noindent where $\mathbf{f}_{indep}$ and $\mathbf{f}_{ref}$ are computed in the same manner as $\mathbf{c}_{indep}$, Eq.~\ref{eq:c_indep} and $\mathbf{c}_{ref}$, Eq.~\cref{eq:omega_r,eq:reflectence_color}. The feature components are used as a \textit{semantic} representation for their color counterparts. We note that the semantic feature from the independent feature field only, $\mathbf{f}_{indep}$ can be used for understanding tasks such as 3D segmentation, as 3D segmentation is inherently view-independent. This is an example where a physical disentanglement (i.e., to view-dependent and independent components) can result in ``improving" or ``cleaning" undesirable feature components required for 3D segmentation. By jointly modeling features and colors, one can also enable a diversity of editing applications as illustrated in Sec.~\ref{sec:results}. 


\subsection{Training Objective}
\label{sec:training_objective}

The training of our structural decomposition of the scene is supervised by posed images and their corresponding DINOv2~\cite{oquab2023dinov2} feature maps. Given a set of posed images $\{C^{gt}_1, \dots, C^{gt}_m\}$, where $C^{gt}_i \in \mathbb{R}^{H \times W \times 3}$, we obtain associated 2D feature maps $\{F^{gt}_1, \dots, F^{gt}_m\}$ using the pretrained feature extractor DINOv2, where $F^{gt}_i \in \mathbb{R}^{H \times W \times n}$, and $n$ is the feature dimension.
The rendered color images are compared to the given images with an $l_2$ loss:
\begin{align}
\mathcal{L}_c = \frac{1}{m} \sum_i { || C_i - C^{gt}_i ||_2^2 },
\end{align}

\noindent where $C_i$ is the scene appearance rendered from our model for the view direction of image $C^{gt}_i$. Additionally, we regularize the SDF MLP learning with the eikonal loss~\cite{icml2020_2086, wang2023unisdf} to promote the approximation of a valid SDF:
\begin{align}
\mathcal{L}_{e} = \frac{1}{| \mathbf{x} |} \sum_{\mathbf{x}} {(|| \nabla \mathbf{d}(\mathbf{x}) ||_2 - 1)^2}.
\end{align}

We also encourage normal smoothness by comparing the computed normal $\mathbf{n}(\mathbf{x})$ from the SDF to the normal $\mathbf{n'}(\mathbf{x})$ predicted by the SDF MLP:
\begin{align}
\mathcal{L}_{p} = \sum_{\mathbf{x}} w_\mathbf{x} \: {|| \mathbf{n}(\mathbf{x}) - \mathbf{n'}(\mathbf{x})||^2}.
\end{align}

Further, we penalize back-facing normals using the orientation loss \cite{verbin2022refnerf}:
\begin{align}
\mathcal{L}_{o} = \sum_{\mathbf{x}} w_\mathbf{x} \: {\max(0, \mathbf{n}(\mathbf{x}) \cdot \mathbf{d}(\mathbf{x}))^2}.
\end{align}

For feature learning, we have found that optimizing the radiance and feature fields concurrently compromises the learning process of both. Thus, we first learn the decomposed appearance of the scene using the total loss:
\begin{align}
\mathcal{L} = \mathcal{L}_c + \lambda_e \mathcal{L}_e + \lambda_p \mathcal{L}_p + \lambda_o \mathcal{L}_o.
\end{align}

\noindent Then, we freeze the appearance model and train MLPs for optimizing our feature field decomposition using: 
\begin{align}
\mathcal{L}_f = \frac{1}{m} \sum_i { || F_i - F^{gt}_i ||_2^2 }.
\label{eq:full_objective}
\end{align}


\subsection{Segmentation and Editing}
\label{sec:objectsegmentation}

Our structurally decomposed feature field enables physically oriented segmentation and editing of the scene. We segment the scene as follows. First, we select a rendered feature component $F_{comp}(x, y)$, where $(x, y)$ is the pixel location, and $F_{comp}$ can be $F_{indep}$ or $F_{ref}$. Then, we correlate the selected feature with the corresponding decomposed feature field in 3D. The location of features with a correlation factor above a threshold belongs to the segmented region of interest in the scene. Once we obtain the region of interest, we can control and modify the physical properties of the scene \textit{locally}, such as the view-independent color, roughness, level of reflection, and more, as demonstrated in Sec.~\ref{sec:results} %

\subsection{How is Disentanglement Achieved?}

Two MLPs, $\mathbf{f}_{indep}$ and $\mathbf{f}_{dep}$, predict the view-independent and view-dependent 3D feature components (see Fig.~\ref{fig:pipeline}). To achieve the disentanglement, we enforce three constraints: (i). $\mathbf{f}_{indep}$ cannot model the view-dependent feature component. This is achieved by design, by \textbf{not} providing $\mathbf{f}_{indep}$ the view direction as input (unlike $\mathbf{f}_{dep}$). (ii). $\mathbf{f}_{indep}$ and $\mathbf{f}_{dep}$ together model the total feature value. This is enforced by summing the outputs of $\mathbf{f}_{indep}$ and $\mathbf{f}_{dep}$ together (Eq.~\ref{eq:learnable_weights_2}) and using volumetric rendering to ensure 2D rendered features match ground truth rendered features. (iii). $\mathbf{f}_{indep}$ models the view-independent feature component. While this is not explicitly enforced, we hypothesize that it is implicitly enforced by the model as it tries to be efficient (compressed). As $\mathbf{f}_{indep}$ does not utilize the view direction as input, for a given 3D position, it uses a single feature value for all view directions. $\mathbf{f}_{dep}$ can then model the residual view-dependent component only when this is \textbf{required}. For directions without reflections, for instance, this is not required, resulting in $\mathbf{f}_{dep}$ storing fewer feature values in total.
