\section{Related Work} \label{sec:related}

\subsection{Structured Novel View Synthesis}

Neural Radiance Field (NeRF)~\cite{mildenhall2021nerf} reconstructs a 3D scene from 2D images by mapping 3D spatial locations and viewing directions to their corresponding color and density values. %
Different works introduced physical modeling within a NeRF-like formulation to allow for the modeling of geometry, lighting, materials, reflections, and more. 
For instance, to model geometry, many works integrate a signed distance function (SDF) formulation~\cite{li2023neuralangelo, long2022sparseneus, oechsle2021unisurf, rosu2023permutosdf, wang2021neus, yariv2021volume, yu2022monosdf}. 
Other works extended NeRF to enable relighting by disentangling appearance into scene lighting and materials~\cite{bi2020neural, boss2021nerd, boss2021neural, srinivasan2021nerv, zhang2021physg, zhang2021nerfactor}. 



In the context of reflections modeling,  approaches such as \cite{ramamoorthi2009precomputation} represented appearance using disentangled view-dependent specular and reflective appearance. 
Ref-NeRF~\cite{verbin2022refnerf} modeled appearance through two disentangled radiance fields within an implicit radiance field formulation, one modeling view-independent diffuse properties and another modeling reflective properties.
BakedSDF~\cite{yariv2023bakedsdf} and  ENVIDR~\cite{liang2023envidr} extended this representation for the reconstruction of glossy surfaces and with material decomposition. UniSDF~\cite{wang2023unisdf} improved upon these parameterizations by including two separate radiance fields for modeling reflections, one for the reflected view and another for camera view based radiance. 
Unlike the above, our focus is on structural disentanglement for feature fields, modeling 3D features together with appearance. As far as we can ascertain, our work is the first to enable such disentanglement. 








\subsection{2D Feature Distillation}

Due to the scarcity of 3D data, recent work attempted to distill or lift 2D features trained using pre-trained self-supervised 2D models (such as DINO~\cite{caron2021emerging}, DINOv2~\cite{oquab2023dinov2} or CLIP-LSeg~\cite{li2022languagedriven}). In particular, \cite{kobayashi2022decomposing, tschernezki2022neural} extended NeRF to model not only appearance, but also semantics, through the use of an additional view-independent feature field, that maps a 3D point into a 3D feature. 
Other works embedded semantic information into NeRFs~\cite{siddiqui2023panoptic, yao2024matte}. Further works focused on integrating open-vocabulary text-based features, by embedding CLIP features into NeRF~\cite{kerr2023lerf} or Gaussian splatting~\cite{qin2023langsplat}. 
Although impressive, models assume that 3D features are captured using a single view-independent feature field (or a single 3D feature per point or Gaussian)
Our work instead focuses on the 3D model, capturing 3D features using multiple structurally disentangled feature fields, 
thus enabling multiple novel understanding and editing capabilities. 






