\section{Conclusion} \label{sec:conclusion}

We have presented a novel method for 3D scene understanding and editing by distilling pretrained 2D features into a structurally disentangled feature field representation. Our approach effectively captures both view-dependent and view-independent features, enabling superior 3D segmentation compared to prior work. We have demonstrated the unique capability of our method to segment and manipulate reflective components of objects, as well as edit object colors and roughness while preserving realistic reflections. These capabilities open up new avenues for physically based understanding and editing of 3D scenes, with potential applications in augmented reality and content creation.

In future work, beyond 3D segmentation, we aim to improve the 3D consistency of underlying foundation models such as DINOv2. To do so, one can rerender GT views only using the view-independent component, discarding their view-dependent component and then fintue DINOv2 on such features. This can enable the learning of view-independent 2D features in general, which could be useful for image/semantic correspondence. Further, we aim to extend our method to incorporate additional physical properties, such as lighting, for more comprehensive 3D scene manipulation, as well as additional features, such as those derived from text. 
