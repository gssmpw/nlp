\begin{abstract}

Recent work has demonstrated the ability to leverage or distill pre-trained 2D features obtained using large pre-trained 2D models into 3D features, enabling impressive 3D editing and understanding capabilities using only 2D supervision. 
Although impressive, the input 2D features may be 3D and physically inconsistent. Current models average such inconsistencies, resulting in inferior distilled 3D features. 
In this work, we propose instead to capture 3D features using multiple disentangled feature fields that capture different structural components of 3D features involving view-dependent and view-independent components. 
Subsequently, each element can be controlled in isolation, enabling semantic and structural understanding and editing capabilities. For instance, 3D segmentation can be achieved by considering only view-indepenent features, and discarding the view-dependent ones, resulting in significant performance compared to baselines, which average view-dependent features. Using a user click, one can segment 3D features corresponding to a given object and then segment, edit, or remove their view-dependent (reflective) properties, enabling 
a set of novel understanding and editing tasks. 



\end{abstract}


