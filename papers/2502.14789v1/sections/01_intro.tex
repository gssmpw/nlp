\section{Introduction}
\label{sec:intro}


A recent paradigm in computer graphics and computer vision is 2D feature distillation, whereby 2D image features obtained using large-scale self-supervised methods are ``distilled" to the parameters of an underlying 3D model. Doing so enables 3D semantic understanding and editing, given 2D RGB and feature supervision only. Several works~\cite{kobayashi2022decomposing, kerr2023lerf, ye2023featurenerf} considered the setting of novel view synthesis with an underlying NeRF~\cite{mildenhall2021nerf} or a Gaussian Splatting~\cite{kerbl20233dgd} model, achieving impressive 3D understanding and editing capabilities. Each 3D point (in the former) or Gaussian (in the latter) stores a feature value, which can be rendered to different views using volumetric rendering. The rendered views correspond to 2D feature maps obtained using self-supervised 2D methods such as DINOv2~\cite{oquab2023dinov2}. This paradigm was later extended to text~\cite{kerr2023lerf, qin2023langsplat}, enabling impressive 3D text-based understanding and editing capabilities.
Although impressive, current models assume that 3D features are captured using a single view-independent feature field (or a single 3D feature value per point or Gaussian). However, as demonstrated in Sec.~\ref{sec:feature_analysis}, input 2D features from SSL models contain 3D inconsistencies and are inherently view dependent (i.e., contain significant view-dependent variations). Current models average such view-dependent variations, resulting in inferior features for downstream performance on tasks such as 3D segmentation. 

In this work, we propose instead to capture 3D features using \textit{multiple disentangled feature fields} that capture different structural components of 3D features. While these structural components could involve a variety of physical properties such as lighting and deformations, we focus on the disentanglement of view-dependent and view-independent features. 
Our disentangled feature fields can be learned using 2D supervision only, in an unsupervised manner, thus enabling the disentanglement of 2D (and 3D) features into components that are view-independent and view-dependent (reflections and camera-dependent features). Subsequently, each component can be controlled separately, enabling semantic and structural understanding and editing capabilities. 
For instance, 3D segmentation can be achieved by considering only view-indepenent features, and discarding the view-dependent ones, resulting in significant performance compared to baselines, which average view-dependent features. 
Using a user click, one can segment an entire object in 3D or only its reflective component, edit view-independent object properties, such as its color, while adhering to reflections, or remove the object's reflections. An illustration is provided in Fig.~\ref{fig:teaser}. 








To achieve our desired disentanglement, we propose computing the feature value of a 3D point along a viewing direction as the combination of the outputs of two disentangled feature fields: (i). A reflected view feature field capturing view-dependent features that arise from specular object reflections, and (ii). A view-independent feature field capturing diffuse features of the object which depend only on the location of a 3D point and not the viewing direction.


We evaluate our approach on a variety of objects from a diverse set of scenes from the Shiny Blender dataset \cite{verbin2022refnerf} as well as real-world scenes from the RefNeRF real-world dataset \cite{verbin2022refnerf} and Mip-NeRF-360 dataset \cite{barron2022mipnerf360}.
In terms of 3D understanding, we consider the tasks of: (i). 3D segmentation, for which our structurally disentangled representation achieves superior results compared to a single holistic feature field \cite{kobayashi2022decomposing},
and (ii). Segmentation of view-dependent components in a scene. For example, one can segment only the reflective component of an object selected using a user click.  In terms of 3D editing, we demonstrate the applicability of our approach on (i). The ability to remove the reflective component of an object, and lastly (ii). The ability to edit individual 3D component. For example, changing the object's color while correctly adhering to reflections or manipulating its roughness. 



In summary, we offer the following contributions:
\begin{enumerate}[nolistsep]
\item To our knowledge, we are the first to consider a physically-inspired decomposition of a \textit{semantic} feature field. This is in contrast to previous works, such as RefNeRF \cite{verbin2022refnerf} and UniSDF \cite{wang2023unisdf}, which only consider appearance, and to DFF \cite{kobayashi2022decomposing}, which uses a single view-independent feature field. Our decomposition reveals a new scientific insight: the view-dependent features of a 2D foundation model \textit{can} be decomposed into a 3D feature field of view-independent and view-dependent components.
\item By doing so, we can: (i) Introduce novel applications, including view-dependent and view-independent segmentation and editing capabilities. (ii) Improve 3D segmentation by only using the view-independent component and discarding the view-dependent component, demonstrating state-of-the-art performance.
\item We improve the underlying reference features of the foundation model by \textit{decomposing} the view-dependent component. In contrast, baseline models such as DFF \textit{averages} all the view-dependent feature components by modeling only a single view-independent feature field.
\end{enumerate}




