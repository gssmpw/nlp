\input{figures/gt_pca/gt_pca}




\section{Experiments} \label{sec:results}

We evaluate our representation on multiple vectors. 
First, we demonstrate that input features from DINOv2 contain both view-dependent and view-independent information. 
Second, we consider the ability to segment objects in 3D space. Unlike previous methods, our disentangled feature fields capture both diffuse and reflective properties and allow for a better reconstruction of the semantic components in the scene. We also enable the segmentation of the reflective component of objects in isolation from different novel views.
Third, we consider the ability to remove view-dependent (reflective) components in the scene for specific semantic objects segmented using our approach. Fourth, we consider the ability to edit the scene, where one can manipulate or change only specific objects and their dependent (reflective) properties in isolation.  

\paragraph{Datasets.} We evaluate our method on synthetic scenes from the Shiny Blender \cite{verbin2022refnerf} dataset as well as real-world scenes from the RefNeRF real-world \cite{verbin2022refnerf} dataset and Mip-NeRF-360 dataset \cite{barron2022mipnerf360}. We consider a diverse set of scenes and objects from both real world and synthetic scenes, featuring multiple objects and variable lighting conditions, highlighting the generality of our approach. 
In particular, we consider 11 scenes and 25 objects from 3 real-world and synthetic datasets. This is comparable to recent work such as DFF or RefNeRF.  We consider the standard train-view/novel-view splits provided by the respective datasets and evaluate our model on such novel views. 
Additionally, our method supports incorporating arbitrary 2D semantic features extracted from models like CLIP-Lseg and DINOv2. 
The webpage provides associated videos depicting novel views and corresponding segmentation, removal, and editing results. We also provide implementation details in the appendix. 

\input{figures/segmentation/segmentation}










\input{figures/segmentation/iou_table}

\input{figures/seg_combination/seg_combination}

\subsection{Feature Analysis}
\label{sec:feature_analysis}




We first consider whether the distilled DINOv2 features capture both view-dependent and view-independent components. To this end, we visualize the PCA of the features for five ground-truth views of the Sedan scene from the real-world RefNeRF dataset. As seen in Fig.~\ref{fig:gt_pca}, while the features appear similar, there are notable differences, particularly in reflective regions such as the windshield (zoomed in). As further evidence, we note the recent work of \cite{el2024probing}, which demonstrates that features obtained from large foundation models, DINOv2 in particular, are not 3D view consistent. As such, applying our model has the advantage of disentangling view-dependent and view-independent components of a given feature view and can enhance downstream applications that require view-independent feature representations. This is illustrated in Sec.~\ref{sec:semantic_segmentation}, where we show that our view-independent feature field yields better 3D segmentation results than using the full (both dependent and independent) feature field or baselines.
Beyond 3D segmentation, one can render ground truth views only using the view-independent component, discarding their view-dependent component.  





\subsection{Semantic Segmentation}
\label{sec:semantic_segmentation}

\input{figures/seg_combination/seg_combination_2}




We consider our method's capability in segmenting both independent and view-dependent components. 



\subsubsection{Full Object Segmentation.}

In Fig.\ref{fig:segmentation}, we visualize our segmentation of three novel views for a real-world scene and a synthetic scene. The segmentation is obtained by clicking on each object and using our disentangled view-independent feature component, described in Sec.~\ref{sec:objectsegmentation}. We compare our method to Distilled Feature Field (DFF)~\cite{kobayashi2022decomposing}, the closest method to ours, which uses a single view-independent feature field. As RefNeRF is an improved appearance model, we also consider another baseline whereby appearance is obtained as in RefNeRF, using both view-dependent and view-independent feature fields, while semantics is obtained as in DFF, using a view-independent feature field. 
As can be seen, our method results in a superior segmentation and can successfully segment both reflective and non-reflective regions well, while the baseline is worse, particularly in reflective regions. Additional results are provided in the webpage. 


For numerical evaluation, we compare the segmentation of objects for both synthetic scenes and real-world scenes. We manually obtain ground-truth segmentations by first applying the Segment Anything Model~\cite{kirillov2023segment} and subsequently manually refining masks (see examples in the webpage). 
As can be seen in Tab.~\ref{tab:segmentation_iou}, our method results in better mean IoU scores. 


\subsubsection{Explicit modeling}
Our approach leverages explicit components to model the view-dependent and view-independent feature fields. Specifically, we model roughness and reflections explicitly, as detailed in Eq.~\ref{eq:omega_r} and Eq.~\ref{eq:reflectence_color} in Sec.~\ref{sec:method}. This enables novel applications that incorporate physical properties, such as object roughness editing. However, it is unclear whether explicit molding is preferable to implicit modeling of view-dependent and view-independent feature fields when one is interested in segmentation only. 
To evaluate the impact of explicit modeling, we considered a baseline that implicitly models view-dependent and independent feature fields by excluding the Reflection, IDE and roughness components (Eq.~\ref{eq:omega_r} and Eq.~\ref{eq:reflectence_color}). 
As seen in Tab.~\ref{tab:segmentation_iou}, this results in inferior performance. 




\subsubsection{View Dependent Segmentation.}                                                                              


We consider the ability to segment the view-dependent reflective surfaces of given objects. 
To this end, we begin by segmenting semantic objects using the view-independent features according to Sec.~\ref{sec:objectsegmentation} and subsequently select only a subset of points corresponding to features from the view-dependent (reflectance) feature field. 
Fig.~\ref{fig:segmentation_combination_1} illustrates for different novel views our success in segmentation for the Garden-spheres scene, for both the entire sphere as well as only the view-dependent reflection. 
Fig.~\ref{fig:segmentation_combination_2} illustrates the ability to segment the reflective region of the (1). bonnet-top and the windshield, (2). bonnet-top, and (3). windshield. 
As can be seen, only the reflective region of the desired semantic entity is depicted. 
Additional examples 
are provided in the webpage. 











\subsection{Removal}










\input{figures/edit/editing_color}

\input{figures/edit/removal}







Several works in the literature~\cite{mirzaei2023spin, weder2023removing, wang2023inpaintnerf360} consider the problem of 3D ``inpainting" or 3D object ``removal", where one wishes to remove a 3D foreground object, resulting in a realistic-looking background scene. Given our structurally disentangled representation, we can now explore a novel capability of ``inpainting" or ``removing" the reflective part of an object. This can be achieved by selecting the 3D points corresponding to a given object (using a click) and then rendering for those points only the color component from the view-independent radiance field.

Fig.~\ref{fig:removal_1} shows the removal of the reflective component of the bonnet-top and windshield for the car scene and the Garden-spheres scene. Our method enables the removal of the reflected radiance from the object and the retention of its diffuse color.  %

\subsection{Editing}







\subsubsection{Color Editing.}

We consider the ability to edit the color of an object while adhering to its reflections. In Fig.~\ref{fig:edit_color_1}, we change the color of the segmented 3D points of (i). the spheres, for the real-world Garden-spheres scene, and (ii). the toaster body for the synthetic toaster scene. This color change occurs either (1). using both radiance fields (independent and reflection) or (2). using the independent component only, leaving the view-dependent reflective component unchanged. Using the latter results in a more natural manipulation that correctly adheres to reflections. Additional examples are shown in the webpage. 
To assess our color editing abilities numerically, we conducted a user study on colored objects. We consider 5 colors and 5 scenes (1 object each) and asked users to assess: 1. Color faithfulness (``how well does the desired color match the object?"), 2. Realism (``how realistic does object look?"), 3. Reflections match the unedited scene (``how well do the reflections match the unedited scene?"). We considered 25 users and obtained a MOS score (1-worse, 5-best) of \textbf{4.6}/\textbf{3.1}/\textbf{4.6} vs. 2.0/2.8/\textbf{4.6} in comparison to DFF for questions 1/2/3 respectively. 

\subsubsection{Roughness Editing.}


Next, we consider the ability to manipulate physical components introduced by our architectural design. In Eq.~\ref{eq:reflectence_color}, we utilize the roughness parameter $\kappa$ that controls the roughness. To this end, we consider the ability to manipulate the roughness of individual objects in the scene. We do so by segmenting the 3D points of an object and varying the roughness parameter $\kappa$ for those points. Fig.~\ref{fig:roughness_2} illustrates two examples: (i). the spheres in the real-world scene of the Garden-spheres scene, and (ii). the helmet case for the synthetic helmet scene.  
Additional examples are provided in the webpage. 







\subsubsection{Ablation Study.}


In Fig.~\ref{fig:indep_ablation}, we consider an ablation in which our segmentation is performed using not only the independent feature component $\mathbf{f}_{indep}$, but also the independent and dependent components together, $\mathbf{f}_{indep} + \mathbf{f}_{ref}$. As can be seen, this results in a worse segmentation. 

We also consider two additional ablations: (1) We optimized the appearance and feature model together, as opposed to first training the appearance model only and then the feature model (see Sec.~\ref{sec:training_objective}). For 3D segmentation (as in Tab~\ref{tab:segmentation_iou}), on average, it is worse, e.g., we get mIOU 0.648 (vs our 0.856) for Shiny Blender. (2). We also conducted an ablation where we removed the tone mapping function (Eq.~\ref{eq:learnable_weights_1}). We observe that appearance is slightly worse. Specifically, for the Gardenphere scene we obtain PSNR/LPIPS/SSIM of 28.8/0.180/0.809 vs our 28.9/0.180/0812. This results in a similar minor performance drop for 3D segmentation.



\subsubsection{Limitations.}

While our method is designed for segmenting and editing reflective regions of semantic objects in a scene, it cannot do so at the instance-based level.
For example, for the spheres in Fig.~\ref{fig:teaser}, selecting one of the spheres will result in capturing and editing both spheres. 
As in standard multiview reconstruction, errors can occur when the number of multiview ground truth features is not sufficient, or when they are noisy erroneous. 
We note that our task is highly unsupervised, as we aim to disentangle semantics and appearance in 3D space, given only 2D entangled appearance and semantics supervision. Improved physical modeling of, e.g., reflections and enhanced and generalizable feed-forward models may result in improved performance. 


