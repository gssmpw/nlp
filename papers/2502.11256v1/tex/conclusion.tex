\section{Conclusion} \label{sec:conclusion}

We introduce \SYSTEM{}, the first evaluation framework for unveiling LLM serving's environmental impact by leveraging functional units as the basis for comparison. We explore how model size, quantization, and hardware affect carbon emissions. Our findings highlight opportunities for greener LLM deployment, paving the way for sustainable AI systems.

% In this paper, we proposed \SYSTEM{}, the first comprehensive framework for analyzing the environmental impact of LLM serving systems. \SYSTEM{} introduces \emph{functional unit} as the basis for comparison, enabling consistent evaluation across different configurations. We conducted three case studies to explore how model sizes, quantization techniques, and hardware platforms affect carbon emissions. Our empirical results reveal promising opportunities for reducing carbon emissions in current LLM serving systems. This study serves as an initial step toward building greener AI infrastructure and lays the groundwork for future research on environmental friendly LLM deployment. For example, adopting weight and activation quantization techniques or leveraging old-tier hardware can significantly lower emissions. These findings highlight several practical strategies to minimize the environmental impact of LLM serving systems while maintaining user experience.

\pagebreak

\section*{Limitations}

We discuss the limitations of this work as follows.

\paragraph{Model families.} Our case studies examine two widely used open-source LLM families, Qwen2.5 and Llama2, which we believe are representative of general LLM serving behaviors. However, we have not yet explored other model families, such as Mistral, or task-specific models like multimodal, vision-language, and code-focused LLMs. We leave these investigations for future work.

\paragraph{Hardware.} All our experiments are conducted on a single GPU to ensure fair comparisons, limiting us to models up to 32B. We have yet to explore the performance and power dynamics in a multi-GPU distributed environment, which would allow us to run larger models like Llama 70B. This setup introduces additional overhead, particularly from communication, making the results even more insightful. We leave this exploration for future work.

\paragraph{Quality metrics.} Quantitatively evaluating LLM output quality remains a challenging and open research question. We experimented with various metrics before selecting the reward model, a common approach in reinforcement learning from human feedback. While we believe our key findings remain robust regardless of the specific quality metric used, access to more advanced evaluation methods in the future could further enhance the accuracy and rigor of our work.


\section*{Ethical Statement}

This research aims to contribute to the development of sustainable and carbon efficient LLM serving systems. We are committed to conducting our work in a responsible manner, adhering to ethical guidelines and best practices. Our focus is on minimizing the environmental impact of LLM deployments while ensuring that the quality of the models and the performance of the systems meet the necessary standards for practical use.

We recognize the potential environmental consequences of the widespread use of LLMs, including energy consumption, electronic waste, and the environmental impact of hardware manufacturing. Therefore, we emphasize the importance of optimizing LLMs for lower energy and carbon emissions, not only in terms of performance but also through hardware reuse and longevity, as part of a more sustainable approach to AI infrastructure.

We strive to be transparent in our research methodologies and encourage further exploration of green AI practices. As we explore new avenues for improving LLM efficiency, we remain mindful of the broader social, economic, and environmental implications of deploying large-scale AI systems and aim to promote solutions that benefit both the technology and society at large.

We also recognize the importance of fairness and inclusivity, ensuring that our research does not disproportionately harm any community or group and aligns with the goal of creating AI systems that are accessible and beneficial to all.
