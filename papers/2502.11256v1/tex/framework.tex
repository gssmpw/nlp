\section{The Framework \SYSTEM{}} \label{sec:framework}



We present \SYSTEM{}, a \underline{\textbf{F}}unctional \underline{\textbf{U}}nit-based \underline{\textbf{E}}valuation framework for evaluating the environment impact of \underline{\textbf{L}}LMs. \SYSTEM{} enables a systematic and comprehensive analysis across various comparison configurations (e.g., model size, quantization, and hardware). Inspired by life cycle assessment in environmental sustainability~\cite{klopffer2014life}, the key insight is to establish a functional unit as a standardized basis for comparison. In LLM serving, a \emph{functional unit} (FU) represents a token characterized by its serving constraints during generation. In the \SYSTEM{} framework, we compare the environmental impact of tokens generated by different model configurations with the same performance and quality constraints.

\Cref{fig:framework} illustrates the four key steps of \SYSTEM{}. First, \SYSTEM{} identifies the inputs, including models, comparison configurations, and serving constraints. Next, it defines the FU based on these inputs. Then, experiments are conducted to profile performance and energy consumption. Finally, \SYSTEM{} quantifies the environmental impact --- focusing on carbon emissions in this work --- using the collected data. Next, we will introduce each step in detail.

\subsection{Step 1: Inputs}

The inputs to \SYSTEM{} include three key components:
\begin{itemize}
    \item \emph{Models:} The LLMs being compared, which can be different versions within the same model family or models from different families.  
    \item \emph{Comparison configurations:} The primary parameter that varies across comparisons. This paper focuses on three configurations: model size, quantization, and hardware.  
    \item \emph{Serving constraints:} The standardized basis for comparison, including workload intensity, performance constraint, and quality constraint. These constraints are critical in defining the FU.
\end{itemize}

\subsection{Step 2: Define Functional Unit}

In LLM serving, a \emph{functional unit} represents a token characterized by its workload intensity, performance, and quality constraints during generation. 

\noindent \textbf{Workload intensity.} \SYSTEM{} defines workload intensity as the request rate (QPS), measuring incoming user requests per second (req/s).

\noindent \textbf{Performance constraint.} \SYSTEM{} evaluates performance using two widely adopted metrics: Time-to-First-Token (TTFT) and Time-Per-Output-Token (TPOT). TTFT reflects how quickly the system responds to a new request by generating the first token, while TPOT quantifies the time per output token during decoding. Following prior work \citet{liu2024andes}, \SYSTEM{} sets a TTFT requirement of 1 second and a TPOT threshold of 200 ms, aligning with average human reading speed to ensure a smooth user experience.

\noindent \textbf{Quality constraint.} Quantitatively assessing output quality is challenging. While prior works (\citet{zhong2022unieval, yuan2021bartscore, jiang2023tigerscore}) have introduced various methods, they depend on either specific datasets or the need for reference answers. After evaluating multiple quality metrics, we adopt the reward model~\cite{liu2024skywork}, a common approach in reinforcement learning from human feedback training \cite{ouyang2022training}. Our experiments show that the reward model's scores align most closely with human preferences and effectively differentiate outputs across models. Using the reward model's score, we define \emph{Qscore} as a measure of output quality, where a higher Qscore reflects better quality and indicates that the output meets a certain quality threshold.

\noindent \textbf{An example of FU definition.} Based on these serving constraints, we define an example FU below:

\begin{tcolorbox}[def]
A token generated by an LLM at a request rate of 5 req/s, with a Qscore of 10, and performance constraints of 1s TTFT and 200ms TPOT.
\end{tcolorbox}

\subsection{Step 3: Profiling}


\SYSTEM{} profiles performance (TTFT and TPOT) and energy consumption by running LLMs under different configurations, based on the inputs given to \SYSTEM{} and the specified workload intensity. During profiling, Qscore is collected using an off-the-shelf reward model to evaluate output quality. For NVIDIA GPUs and Intel CPUs, power is measured every 200ms using NVIDIA (\verb|pynvml|) and Intel (\verb|psutil|) APIs for energy modeling, respectively.

\subsection{Step 4: Carbon Modeling}

Unlike prior work that profiles performance and energy without considering serving constraints, \SYSTEM{} defines and calculates \emph{carbon emission per FU} (CFU), measuring the emissions of FUs that meet certain serving constraints. Formally,
\[\Scale[0.99]{
\rm{CFU} = \frac{\rm{Total~carbon~emissions~for~all~tokens}}{N_{f}},}
 \]
\[ \Scale[0.9]{ N_{f} = \sum_{i=1}^N \mathbb{I}(\text{Q}_i \geq \alpha) \cdot \mathbb{I}(\text{TTFT}_i \leq \beta) \cdot \mathbb{I}(\text{TPOT}_i \leq \gamma) },\]

% \begin{align}
% \footnotesize N_{f} = \sum_{i=1}^N \mathbb{I}(\text{Q}_i \geq \alpha) \cdot \mathbb{I}(\text{TTFT}_i \leq \beta) \cdot \mathbb{I}(\text{TPOT}_i \leq \gamma) \nonumber
% \end{align}

\noindent where $N$ is the total number of output tokens, \( N_{f} \) is the total number of tokens considered FUs, $\text{Q}$ is the Qscore, $\alpha$, $\beta$, and $\gamma$ are the constraints for Qscore, TTFT, and TPOT, respectively. Note that we consider a token to meet the Qscore requirement if its corresponding response does, as Qscore is defined at the response level. Next, we describe how to calculate carbon emissions.

\noindent \textbf{Carbon emission calculation.} Following prior work~\cite{nguyen2024towards,li2024towards,shi2024greenllm,ding2024sustainable},  total carbon emissions in LLM serving include operational carbon emission $C_{\rm op}$ and embodied carbon emissions $C_{\rm em}$. We now describe how to calculate each.
\begin{itemize}
    \item \emph{Operational carbon} is calculated as the product of the energy consumed, \( E_{\rm op} \), and the carbon intensity of the energy source (\(\texttt{CI}\)). Carbon intensity is defined as the amount of $CO_{\rm 2eq}$ emitted per kilowatt-hour (\(kWh\)) of electricity used~\cite{maji2022carboncast,li2024uncertainty}. The operational carbon emission is thus given by:
\begin{align}\label{eq:carbon-op}
	C_{\rm op} = E_{\rm op} \cdot \texttt{CI}
\end{align}
\item \emph{Embodied carbon} of a hardware device is determined by factors such as processor chip area and memory capacity~\cite{gupta2022act,faiz2024llmcarbon}. The detail of modeling the total embodied carbon of a hardware device is in Appendix~\ref{sec:appendix_emb}. The embodied carbon emission of an LLM execution over time \( t \) is calculated by amortizing the hardware's total embodied carbon $C_{\rm em, total}$ over its lifetime ($\texttt{LT}$), typically 5 to 7 years~\cite{ostrouchov2020gpulife}. Thus, the embodied carbon for a time period \( t \) is given by:
\begin{align}\label{eq:carbon-eb}
	C_{\rm em} = \frac{t}{\texttt{LT}} \cdot C_{\rm em, total}
\end{align}
\item \emph{Total carbon} is thus given by:
\begin{align}\label{eq:carbon-eb}
	C_{\rm total} = E_{\rm op} \cdot \texttt{CI} + \frac{t}{\texttt{LT}} \cdot C_{\rm em, total}
\end{align}
\end{itemize}

\subsection{Summary and Implementation}

\SYSTEM{} provides a systematic framework for evaluating the environmental impact of LLM serving, using FU as a comparison basis. To demonstrate its effectiveness and generalizability, we will present three case studies exploring different comparison configurations: model size (\cref{sec:case1}), quantization (\cref{sec:case2}), and hardware (\cref{sec:case3}). For broadly applicable insights, we focus on two widely used model families, Qwen2.5~\cite{qwen2025qwen25technicalreport} and Llama2~\cite{touvron2023llama2openfoundation}, and conduct experiments using the open-source LLM serving platform vLLM~\cite{kwon2023efficient}. We use a carbon intensity of 518 gCO\textsubscript{2}eq, the 12-month average of our server's region, to calculate operational carbon emissions. All experiments were conducted in a single run with the LLM temperature set to 0 to minimize output randomness. We use the NewsQA \cite{trischler2016newsqa} summarization dataset for main results, as it tests language understanding without extra context. Results on other datasets are in the Appendix.



% The carbon emissions of an LLM serving system arise from two main sources: 1) the energy consumed by the hardware during model execution, and 2) the carbon emissions generated during the production and transportation of hardware components. Following previous work~\cite{gupta2022act}, we refer to the first as operational carbon $C_{\rm op}$ and the second as embodied carbon $C_{\rm emb}$.
% \begin{equation}
%     \text{Total CO}_{\rm 2eq} = C_{\rm op} + C_{\rm emb}
% \end{equation}

% \paragraph{Embodied carbon.} The embodied carbon of a hardware device is determined by factors such as processor chip area and memory capacity~\cite{gupta2022act,faiz2024llmcarbon}. The embodied carbon emission over a time period \( t \) is calculated by amortizing the total embodied carbon of the hardware across its lifetime ($\texttt{LT}$), typically ranging from 5 to 7 years~\cite{ostrouchov2020gpulife}. Thus, the embodied carbon for a time period \( t \) is given by:
% \begin{align}\label{eq:carbon-eb}
% 	C_{\rm emb} = \frac{t}{\texttt{LT}} \cdot C_{\rm emb, total}
% \end{align}

% \paragraph{Operational carbon.} The operational carbon emission of a hardware execution is calculated as the product of the energy consumed, \( E_{\rm op} \), and the carbon intensity of the energy source (\(\texttt{CI}\)). Carbon intensity is defined as the amount of $CO_{\rm 2eq}$ emitted per kilowatt-hour (\(kWh\)) of electricity used~\cite{maji2022carboncast,li2024uncertainty}. The operational carbon emission is thus given by:
% \begin{align}\label{eq:carbon-op}
% 	C_{\rm op} = E_{\rm op} \cdot \texttt{CI}
% \end{align}




% \subsection{Design Principles}
% The framework builds on three foundational principles:
% \paragraph{Functional Unit Equivalence} Environmental impacts can only be compared when models deliver equivalent products in their end-use applications. 
% \paragraph{Quantifiable Environmental Impact} Environmental impacts, such as carbon emissions, must be measured in clear, quantifiable terms to allow objective comparisons across models.
% \paragraph{Generalizability} The framework should be model, hardware, and optimization agnostic. It applies across architectures, parameter sizes, and optimization techniques, focusing solely on the functional output.

% \subsection{Framework Architecture}
% The framework takes three inputs: the LLM models themselves, their comparison configurations, and the basics of comparison. The models may differ in architecture, size, and underlying optimizations, while the comparison configurations encompass parameters such as model size, optimization methods (e.g., pruning or distillation), quantization strategies, and hardware specifications.

% \subsection{Functional Unit Definition} 
% At the heart of our framework lies the concept of a \emph{functional unit}. A functional unit describes a quantity of a product based on the performance it delivers in its end-use application. In the context of LLM serving, the functional unit is defined by meeting specific end-user requirements, such as quality, performance, and throughput. 

% \paragraph{Performance} \SYSTEM{} focuses on two key metrics: TTFT (time-to-first-token) and TPOT (time-per-output-token). TTFT reflects how quickly the system responds to a new request by generating the first token, while TPOT quantifies the time per output token during the decoding process. According to \citet{liu2024andes}, \SYSTEM{} set a TTFT requirement of 1 second and a TPOT threshold of 200ms based on average human reading speed. These metrics are called the system's Service Level Objectives (SLO), ensuring user experience.  

% \paragraph{Quality Assessment} Assessing text quality quantitatively remains a challenging task. While many works (\citet{zhong2022unieval, yuan2021bartscore, jiang2023tigerscore}) attempt to address this, they still have limitations, such as relying on specific datasets or requiring reference answers for evaluation. In \SYSTEM{}, we employ the reward model\cite{liu2024skywork} to score the generated text as quality assessment, a method commonly used in RLHF\cite{ouyang2022training} (Reinforcement Learning from Human Feedback) training. The reward model's scores reflect human preferences, offering a more flexible and robust quality assessment. 

% Based on the above metrics combined with throughput requirements, we can give an example definition of a functional unit: 

% \textit{
% A token generated by LLM under a request rate of 5 req/s, with a Qscore of 10 and the SLO constraint: TTFT=1s, TPOT=200ms
% }

% Mathematically, the FU can be expressed as:
% \begin{align}
% \text{FU}& = \{  (\text{Qscore},\, \text{TTFT},\, \text{TPOT},\, \text{QPS}) \in \mathbb{R}^4 \mid  \nonumber \\
%   &  \text{Qscore} = 10,\; \text{TTFT} = 1\,\text{s},\; \text{TPOT} = 0.2\,\text{s}, \text{QPS}=5 req/s\}.  \nonumber
% \end{align}

% \subsection{Measurement Pipeline}
% The measurement process begins by running the model under various configurations based on the given input of \SYSTEM{}, ensuring all evaluations are performed under a unified functional unit. During measurement, data metrics such as latency, energy consumption, and quality scores are collected for the model responses. The reward model will give quality scores, while real-time power is captured from GPUs/CPUs via NVIDIA/Intel API.



% \subsection{Environmental Impact Quantification}
% To quantify the environmental impact of an LLM serving system, we first focus on the carbon emissions associated with serving functional units. The key metric for this quantification is \textbf{ carbon per functional unit (CFU)}, which measures the carbon emissions per unit of service that meets certain requirements.

% \begin{equation}
% \text{CFU} = \frac{\text{Total CO}_{\rm 2eq}}{N_{f}}
% \end{equation}
% \begin{equation}
% N_{f} = \sum_{i=1}^N \mathbb{I}(\text{Q}_i \geq \gamma) \cdot \mathbb{I}(\text{TTFT}_i \leq 1) \cdot \mathbb{I}(\text{TPOT}_i \leq 0.2) \nonumber
% \end{equation}

% where the function \( N_{f} \) calculates the total number of tokens that are considered valid functional units.

% \paragraph{Carbon Emission Calculation}
% The carbon emissions of an LLM serving system arise from two main sources: 1) the energy consumed by the hardware during model execution, and 2) the carbon emissions generated during the production and transportation of hardware components. Following previous work~\cite{gupta2021chasing,gupta2022act}, we refer to the first as operational carbon $C_{\rm op}$ and the second as embodied carbon $C_{\rm emb}$.
% \begin{equation}
%     \text{Total CO}_{\rm 2eq} = C_{\rm op} + C_{\rm emb}
% \end{equation}

% \paragraph{Embodied carbon.} The embodied carbon of a hardware device is determined by factors such as processor chip area and memory capacity~\cite{gupta2022act,faiz2024llmcarbon}. The embodied carbon emission over a time period \( t \) is calculated by amortizing the total embodied carbon of the hardware across its lifetime ($\texttt{LT}$), typically ranging from 5 to 7 years~\cite{ostrouchov2020gpulife}. Thus, the embodied carbon for a time period \( t \) is given by:
% \begin{align}\label{eq:carbon-eb}
% 	C_{\rm emb} = \frac{t}{\texttt{LT}} \cdot C_{\rm emb, total}
% \end{align}

% \paragraph{Operational carbon.} The operational carbon emission of a hardware execution is calculated as the product of the energy consumed, \( E_{\rm op} \), and the carbon intensity of the energy source (\(\texttt{CI}\)). Carbon intensity is defined as the amount of $CO_{\rm 2eq}$ emitted per kilowatt-hour (\(kWh\)) of electricity used~\cite{maji2022carboncast,li2024uncertainty}. The operational carbon emission is thus given by:
% \begin{align}\label{eq:carbon-op}
% 	C_{\rm op} = E_{\rm op} \cdot \texttt{CI}
% \end{align}



