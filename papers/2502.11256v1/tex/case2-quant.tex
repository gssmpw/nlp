\section{Case Study: Quantization}\label{sec:case2} 

In this section, we explore how quantization affects the environmental impact of LLM serving. By reducing model weight and activation precision, quantization significantly decreases model size. For example, 4-bit quantization cuts model size by 4× compared to FP16. This reduction lowers memory usage and computational costs while maintaining accuracy. Using \SYSTEM{}, we investigate whether quantization, especially weight-only \cite{lin2024awq} and activation \cite{frantar2022gptq} quantization techniques, can improve carbon efficiency while maintaining output quality.

\subsection{Evaluation Methodology}

\paragraph{Setup.} We evaluate two widely used quantization methods: 4-bit AWQ \cite{lin2024awq} (weight-only) and W8A8 \cite{frantar2022gptq} (INT8 quantization for both weights and activations). We evaluate Qwen2.5 (7B, 14B, 32B) and Llama2 (7B, 13B) on an NVIDIA H100 GPU with an Intel Xeon 8480+ CPU. Qwen provides an official AWQ version, while Llama’s AWQ is from Hugging Face~\cite{huggingface_llama7_awq_2023, huggingface_llama13_awq_2023}. For W8A8, we quantize the models using LLM Compressor~\cite{vllm_llm_compressor_2023}, an open-source library designed for vLLM.


\paragraph{Benchmarking configurations.} Same as in \Cref{sec:case1}.

\subsection{Evaluation Results}

\fuelq{Is weight-only quantization always greener?}

\noindent The answer is \textbf{no}. \Cref{fig:quant_cs_qwen} shows the relative carbon emission savings per FU for AWQ compared to the FP16 version of Qwen under high (10) and low (-5) Qscores. Overall, AWQ’s carbon savings decline as QPS increases. For the 7B model, AWQ consistently reduces emissions, even under high Qscore. At QPS = 1 req/s and Qscore = 10, AWQ cuts emissions by over 20\% compared to FP16. This is because AWQ slightly increases the output quality of 7B (\Cref{tab:quant_qscore} in \Cref{sec:appendix_quant}), resulting in an increased number of FUs. On the other hand, the 14B model shows positive carbon savings at low Qscore (-5) but negative savings at high Qscore (10). The 32B model never achieves positive carbon savings, regardless of Qscore. We observe a similar trend for Llama in~\Cref{fig:quant_cs_llama}. As QPS increases, the carbon savings of AWQ over FP16 decline and can even become negative at high QPS. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/cs_qwen.pdf}
    \vspace{-0.15in}
    \caption{Carbon savings of AWQ Qwen compared to the FP16 version with Qscore low (-5) and high (10). Data are missing at higher QPS for 14B and 32B, as larger models cannot serve intensive workloads.}
    \label{fig:quant_cs_qwen}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/cs_llama.pdf}
    \vspace{-0.15in}
    \caption{Carbon savings of AWQ Llama compared to the FP16 version with Qscore low (-5) and high (10). Data are missing at higher QPS for 13B, as larger models cannot serve intensive workloads.}
    \label{fig:quant_cs_llama}
\end{figure}

To understand why AWQ does not always outperform FP16 in carbon savings, we analyze its impact on TTFT and TPOT speedup. Figures \ref{fig:quant_speedup_qwen} and \ref{fig:quant_speedup_llama} show that TPOT sees some speedup at low QPS but slows down at high QPS, while TTFT is always slower than FP16. This is because quantization reduces weight size, but weights are dequantized back to 16-bit during inference, adding overhead. AWQ improves TPOT in memory-bound cases at low QPS by reducing memory transfer, but this advantage diminishes as QPS increases and computation grows. Since TTFT is compute-intensive, AWQ provides no speedup.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/qwen_awq_speedup.pdf}
    \vspace{-0.15in}
    \caption{Latency speedup of AWQ Qwen compared to the FP16 version.}
    \label{fig:quant_speedup_qwen}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/llama_awq_speedup.pdf}
    \vspace{-0.15in}
    \caption{Latency speedup of AWQ Llama compared to the FP16 version.}
    \label{fig:quant_speedup_llama}
\end{figure}


\takeawaybox{
Weight-only quantization reduces carbon emissions at low QPS but loses its advantage as QPS increases.
}



% \fuelq{When does AWQ become greener?}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/quant/heatmap_qwen7_awq.pdf}
%     \caption{Comparison of Qwen-7B and 7B-AWQ in FUEL. Tile colors indicates the model with the greatest Carbon per Functional Unit. Tile values are Carbon Savings (\%) of Greenest Model in FUEL vs. Second Greenest}
%     \label{fig:quant_heat_awq_qwen7}
% \end{figure}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/quant/heatmap_qwen14_awq.pdf}
%     \caption{Comparison of Qwen-14B and 14B-AWQ in FUEL. Tile colors indicates the model with the greatest Carbon per Functional Unit. Tile values are Carbon Savings (\%) of Greenest Model in FUEL vs. Second Greenest}
%     \label{fig:quant_heat_awq_qwen14}
% \end{figure}


% \subsubsection{Weight and Activation Quantization}
\fuelq{Is activation quantization always greener?}

\noindent Unlike weight-only quantization, activation quantization applies to both weights and activations. We compared the relative carbon savings of W8A8 compared to the FP16 version under different Qscores and QPS, and the results show that the answer is \textbf{yes}. As shown in~\Cref{fig:quant_cs_qwen_w8a8}, W8A8 consistently reduces carbon emissions for Qwen models, regardless of quality requirements. Despite some accuracy loss in the 7B model (\Cref{tab:quant_qscore} in \Cref{sec:appendix_quant}), it still achieves a 5\% carbon reduction at Qscore = 10. Unlike AWQ, W8A8 maintains stable savings even as QPS increases.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/cs_qwen_w8a8.pdf}
    \vspace{-0.15in}
    \caption{Carbon savings of W8A8 Qwen compared to the FP16 version with Qscore low (-5) and high (10). Data are missing at higher QPS for 14B and 32B, as larger models cannot serve intensive workloads.}
    \label{fig:quant_cs_qwen_w8a8}
\end{figure}

We observe a similar trend for Llama in~\Cref{fig:quant_cs_llama_w8a8}. Notably, Llama 7B improved in output quality after quantization (\Cref{tab:quant_qscore} in \Cref{sec:appendix_quant}), saving over 15\% of carbon at Qscore = 10. This shows activation quantization can break the tradeoff between FP16 and AWQ, ensuring consistent carbon savings across different FUs.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/cs_llama_w8a8.pdf}
    \caption{Carbon savings of W8A8 Llama compared to the FP16 version with Qscore low (-5) and high (10).}
    \vspace{-0.15in}
    \label{fig:quant_cs_llama_w8a8}
\end{figure}

To understand why W8A8 always outperforms FP16 in carbon savings, we analyze its impact on TTFT and TPOT speedup. Figures \ref{fig:quant_speedup_qwen_w8a8} and \ref{fig:quant_speedup_llama_w8a8} show that W8A8 consistently speeds up TPOT and TTFT across all QPS ranges. This improvement comes from reducing both weight and activation precision, which decreases the amount of data movement and computation during inference. This makes W8A8 a more sustainable choice for LLM serving, as it strikes a balance between quality and performance.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/qwen_w8a8_speedup.pdf}
    \vspace{-0.15in}
    \caption{Latency speedup of W8A8 Qwen compared to the FP16 version.}
    \label{fig:quant_speedup_qwen_w8a8}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{llmfu-acl2025/figs/quant/llama_w8a8_speedup.pdf}
    \vspace{-0.15in}
    \caption{Latency speedup of W8A8 Llama compared to the FP16 version.}
    \label{fig:quant_speedup_llama_w8a8}
\end{figure}


\fuelq{Does a universal greenest quantization method exist?}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/quant/heatmap_qwen_quant.png}
    \vspace{-0.2in}
    \caption{Comparison of FP16, AWQ and W8A8 versions of Qwen 7B/14B in \SYSTEM{}. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of greenest quantization versioncompared to the second greenest.}
    \label{fig:quant_heat_w8a8_qwen}
\end{figure}

\noindent The answer is \textbf{no}. \Cref{fig:quant_heat_w8a8_qwen} shows the relative carbon savings of FP16, AWQ, and W8A8 models across various QPS and Qscores for Qwen 7B and 14B. For Qwen 14B, W8A8 outperforms in all scenarios, with carbon savings increasing as QPS rises. However, for Qwen 7B, AWQ maintains slightly better quality at low QPS, while W8A8 lags behind at high QPS and high-quality requirements due to its slight accuracy loss (\Cref{tab:quant_qscore} in \Cref{sec:appendix_quant}).

\takeawaybox{Weight and activation quantization methods, like W8A8, hold significant potential for reducing carbon emissions in LLM serving, particularly for larger models. }




% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/quant/heatmap_qwen7_w8a8.pdf}
%     \caption{Comparison of Qwen-7B, 7B-W8A8 and 7B-AWQ in FUEL. Tile colors indicate the model with the greatest Carbon per Functional Unit. Tile values are Carbon Savings (\%) of Greenest Model in FUEL vs. Second Greenest}
%     \label{fig:quant_heat_w8a8_qwen}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/quant/heatmap_qwen14_w8a8.pdf}
%     \caption{Comparison of Qwen-14B, 14B-W8A8 and 14B-AWQ in FUEL. Tile colors indicate the model with the greatest Carbon per Functional Unit. Tile values are Carbon Savings (\%) of Greenest Model in FUEL vs. Second Greenest}
%     \label{fig:quant_heat_w8a8_qwen14}
% \end{figure}



