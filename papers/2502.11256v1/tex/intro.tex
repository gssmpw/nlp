\section{Introduction} \label{sec:intro}

Large language models (LLMs) have been widely adopted in various industries due to their ability to perform complex language tasks~\cite{vu2023freshllms,shen2024hugginggpt,liu2024your}. However, LLM serving comes with significant environmental impacts, particularly in terms of carbon emissions. For instance, processing a single prompt on ChatGPT produces over 4 grams of CO\textsubscript{2}eq~\cite{chatgptcarbon2023}, which is over 20x the carbon emissions generated by a web search query~\cite{whyyourinternet2020}.

Recent studies have benchmarked the carbon emissions of LLM serving by analyzing performance (e.g., throughput, latency) and energy consumption, then modeling carbon emissions under varying conditions such as request rate, and input/output length~\cite{nguyen2024towards,li2024towards,shi2024greenllm,li2024sprout}. However, these efforts have two limitations: \textbf{(1)} they focus on individual LLMs rather than cross-model comparisons, and \textbf{(2)} they lack a standardized basis for fair carbon emission comparisons. These gaps limit the broader applicability and fairness of their analyses.

Building on principles from life cycle assessment in environmental sustainability~\cite{klopffer2014life}, we address these two limitations by introducing the concept of a \emph{functional unit} (FU) as a standardized basis for comparing LLMs. In LLM serving, an FU represents a token generation defined by workload intensity, performance, and quality constraints. Using this, we develop \SYSTEM{}, a \underline{\textbf{F}}unctional \underline{\textbf{U}}nit-based \underline{\textbf{E}}valuation framework for evaluating the environment impact of \underline{\textbf{L}}LMs. To demonstrate its effectiveness and generalizability, we conduct three case studies exploring model size, quantization, and hardware. Our key insights for building sustainable LLM serving systems include:  

\begin{itemize}
    \item \emph{Model size:} Larger models are greener in high output quality and low request rate, while smaller models excel as the request rate increases. 
    \item \emph{Quantization:} Quantization significantly lowers carbon emissions, especially for larger models.  
    \item \emph{Hardware:} Newer hardware offers better performance but is not always greener due to higher embodied carbon. Older hardware can lower carbon emissions while meeting quality and performance constraints.
\end{itemize}

The contributions of this paper are:
\begin{itemize}
    \item Introducing and defining FU for LLM serving from environmental sustainability.  
    \item Developing \SYSTEM{}, the first FU-based framework for assessing the environmental impact of LLM serving.
    \item Conducting case studies on model size, quantization, and hardware impact on carbon emissions.
\end{itemize}

% For example, comparing the environmental impact of a small-sized model and a large-sized model purely based on carbon emissions per token is insufficient. While the smaller model typically consumes less energy and have a lower carbon emission~\cite{shi2024greenllm}, its output quality may be significantly poorer, making it impractical for real-world deployment. Therefore, it would be misleading to conclude that the smaller model is inherently more sustainable, as the quality of its outputs must also be factored into the analysis.  