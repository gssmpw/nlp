\section{Code Availability}
Please find our code repository at \texttt{https://anonymous.4open.science/r/FUEL-76EF}. We provide a README file that offers instructions on how to get started and utilize \SYSTEM{}.

% In the following sections, we provide more detailed information about our implementation and results. Section~\ref{sec:appendix_impl} offers a deeper dive into embodied carbon modeling. Section~\ref{sec:appendix_model_size} presents additional results for the model size case study, supplementing our main findings. It includes further experiments on NewsQA summarization task and two additional datasets: Arena Hard and NewsQA, to support our conclusions further.
% Section~\ref{sec:appendix_quant} provides supplementary results for the quantization case study, with experiments conducted on the same datasets to demonstrate the impact of quantization methods. Finally, Section~\ref{sec:appendix_hardware} expands on our hardware case study, providing more comprehensive comparisons of model selections on different hardware.
We summarize the appendix as follows:

\begin{itemize}
    \item Section~\ref{sec:appendix_emb} provides a detailed description of embodied carbon modeling.
    \item Section~\ref{sec:appendix_model_size} presents additional results for the model size case study, including experiments on NewsQA and two additional datasets (Arena Hard and HumanEval).
    \item Section~\ref{sec:appendix_quant} provides supplementary results for the quantization case study on the same datasets.
    \item Section~\ref{sec:appendix_hardware} offers more detailed comparisons of model selections in the hardware case study.
\end{itemize}






% \section{More Implementation Details}
% \label{sec:appendix_impl}


\section{Embodied Carbon Modeling}\label{sec:appendix_emb}
We utilize the ACT~\cite{gupta2022act} embodied carbon modeling tool. The embodied carbon footprint can be divided into manufacturing and packaging carbon emissions. Manufacturing carbon arises from producing electronic components like transistors and resistors from raw materials, while packaging carbon is associated with assembling these components into chips and circuit boards:
\begin{equation}
    C_{\rm em} = C_{\rm manufacturing} + C_{\rm packaging}
\end{equation}

The manufacturing embodied footprint $C_m$ of processors and SoCs like CPUs and GPUs depends on several factors: die area ($A_{\text{die}}$), carbon intensity of the energy consumed by the fab ($\texttt{CI}_{\rm fab}$), energy consumed per unit area manufactured (EPA), the GHG emissions from gases and chemicals per unit area (GPA), the footprint of procuring raw materials per unit area (MPA), and fabrication yield ($\text{Yield}$, set to 0.875 as in \citet{gupta2022act}). The information is sourced from product data sheets and sustainability reports. The manufacturing embodied carbon of a processor can be calculated as:

\begin{equation}
    C_{\rm m} = \frac{(\texttt{CI}_{\rm fab} \times \text{EPA} + \text{GPA} + \text{MPA})\times A_{die}}{\text{Yield}}
\end{equation}

The packaging carbon emission $C_{\rm p}$ is calculated by the number of integrated circuits ($N_{\rm IC}$) with a packaging footprint. Following ACT,  we use an average packaging overhead of 150 gCO$_2$ per IC.
\begin{equation}
    C_{\rm p} = N_{\rm IC} \times 150
\end{equation}

In cloud environments or HPC clusters, it is often challenging to obtain details of DRAM specifications. Previous studies~\cite{li2023toward, mem2023} generally assume that the embodied carbon of DRAM is proportional to its capacity. Following prior work, we adopt a fixed rate of 65gCO$_2$/GB to estimate the embodied carbon of DRAM.



\section{Additional Results for Model Size Case Study}\label{sec:appendix_model_size}

\subsection{Results on NewsQA Summarization}
\label{sec:appendix_model_size_newsqa}
Figure~\ref{fig:size_carbon_per_token} illustrates the naive carbon emission per token for various model sizes across a range of QPS. This figure represents carbon per token without the use of \SYSTEM{}. Without considering server constraints, smaller models consistently exhibit lower carbon emissions per token, which does not reflect real-world serving requirements where larger models may be preferred for higher quality outputs.
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/size/carbon_per_token.pdf}
    \caption{Naive carbon emission per token for different model sizes across QPS range on NewsQA dataset.}
    \label{fig:size_carbon_per_token}
\end{figure}

\Cref{fig:size_newsqa_score_cdf} shows the cumulative percentage of quality scores $\geq$ a given threshold for different models on the NewsQA summarization task. This figure highlights significant differences between models, particularly between Llama 7B and 13B, and between Qwen 7B and 32B. This discrepancy demonstrates why smaller models may not be as advantageous when higher quality is required, as larger models provide better outputs.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{llmfu-acl2025/figs/size/newsqa_score_dist.pdf}
    \vspace{-0.3in}
    \caption{Qscore distribution of outputs across different model sizes on the NewsQA dataset.}
    \label{fig:size_newsqa_score_dist}
\end{figure}


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/size/newsqa_score_geq_cdf.pdf}
    \caption{Cumulative percentage of Qscore $\geq$ threshold on NewsQA dataset.}
    \label{fig:size_newsqa_score_cdf}
\end{figure}

\subsection{Results on Arena Hard}
The Arena Hard dataset~\cite{li2024arenahard} is a challenging benchmark designed to evaluate the instruction following capabilities of LLMs, which is derived from real user interactions on Chatbot Arena.

Figure~\ref{fig:arena_size_score_dist} shows the Qscore distribution for different model sizes on the Arena Hard dataset. As shown, larger models tend to achieve higher quality scores. However, compared to the quality distribution on the NewsQA summarization task, while larger models still perform better, the differences between model sizes on Arena Hard are less pronounced than on the NewsQA. However, the gap between Llama 7B and 13B remains significant. Figure~\ref{fig:arena_size_score_dist_cumu} also confirms this trend by showing the cumulative percentage of quality scores $\geq$ a given threshold for various model sizes on Arena Hard. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{llmfu-acl2025/figs/arena/size/arena_score_dist.pdf}
    \caption{Qscore distribution of outputs across different model sizes on the Arena Hard dataset.}
    \label{fig:arena_size_score_dist}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{llmfu-acl2025/figs/humaneval/size/humaneval_score_geq_cdf.pdf}
    \caption{Cumulative percentage of Qscore $\geq$ threshold for different model sizes on Arena Hard dataset.}
    \label{fig:arena_size_score_dist_cumu}
\end{figure}

\Cref{fig:arena_size_carbon_per_token_quality} shows carbon emissions per FU across model sizes under different Qscore settings at QPS = 1 req/s. For the Qwen model family, since the Qscore distribution gap has narrowed, we only observe the 32B model producing less carbon than the 7B model when the quality requirement becomes very high (Qscore > 15). On the other hand, due to the significant quality distribution gap between the Llama models, a slight increase in the quality requirement makes the Llama 13B model greener than the 7B model. Moreover, when the quality requirements become stricter, the carbon emission gap between Llama 13B and 7B becomes larger.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{llmfu-acl2025/figs/arena/size/carbon_per_token_quality.pdf}
    \caption{Carbon emission per FU for different model sizes on Arena Hard across Qscores at QPS=1 req/s.}
    \label{fig:arena_size_carbon_per_token_quality}
\end{figure}




% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/arena/size/carbonsaving_qwen_multiscore.pdf}
%     \caption{Carbon Saving percentage on Arena of Qwen 14B and Qwen 32B compared with Qwen 7b in FUEL with Qscore Requirement Low(-5) and High(25)}
%     \label{fig:arena_size_cs_qwen}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/arena/size/carbonsaving_arena_llama2_multiscore.pdf}
%     \caption{Carbon Saving percentage on Arena of Llama 13B compared with Llama 7B in FUEL with different Qscore requirement (-20 and 5)}
%     \label{fig:arena_size_cs_llama}
% \end{figure}
The result aligns well with our findings on the NewsQA dataset: if the quality requirement is high, larger models become a greener choice, especially when there are large differences in quality distribution across models of different sizes. \Cref{fig:size_arena_heatmap} shows the optimal model size choice across various Qscore and QPS conditions for the Qwen and Llama families. Due to the close quality distribution within the Qwen family, the advantage of larger models is constrained to the top-left corner (high Qscore, low QPS). In contrast, in the bottom right corner, as the quality requirement decreases and QPS increases, the 7B model becomes the greenest one.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/arena/size/arena_heatmap.png}
    \caption{Comparison of different model sizes on Arena Hard in \SYSTEM{}. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of the greenest size compared to the second greenest.}
    \label{fig:size_arena_heatmap}
\end{figure}

\subsection{Results on HumanEval}
The HumanEval dataset~\cite{chen2021humaneval} is a benchmark designed to evaluate the code generation ability of LLMs. It consists of Python coding problems and requires LLMs to implement the specific functions. 

\Cref{fig:humaneval_size_score_dist} shows the Qscore distribution for different model sizes on the HumanEval dataset. Qscore distribution for the Qwen models is much closer on this dataset. This is consistent with their technical report~\cite{qwen2025qwen25technicalreport}, which also highlights similar performance across models in the HumanEval evaluation. However, for the Llama models, the gap between the 7B and 13B model remains large, as expected. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{llmfu-acl2025/figs/humaneval/size/humaneval_score_dist.pdf}
    \caption{Qscore distribution for different model sizes on HumanEval dataset.}
    \label{fig:humaneval_size_score_dist}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.49\textwidth]{llmfu-acl2025/figs/humaneval/size/humaneval_score_geq_cdf.pdf}
    \caption{Cumulative percentage of Qscore $\geq$ threshold for different model sizes on HumanEval dataset.}
    \label{fig:humaneval_size_score_dist_cumu}
\end{figure}

\Cref{fig:humaneval_size_carbon_per_token_quality} shows carbon emissions per FU across model sizes on HumanEval dataset under different Qscore settings at QPS = 1 req/s. For the Qwen family, since the quality difference between the three model sizes on this task is not significant, increasing the Qscore does not lead to larger models demonstrating carbon emission saving over the 7B model. The carbon emissions per FU remain similar across model sizes even with higher Qscore requirements.
In contrast, for the Llama model family, due to the large quality gap between the 7B and 13B models, we observe that even at very low quality requirements (e.g., Qscore = -15), the 13B model exhibits lower carbon emissions than the 7B model. 



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{llmfu-acl2025/figs/humaneval/size/carbon_per_token_quality.pdf}
    \caption{Carbon emission per FU for different model sizes on HumanEval across Qscores at QPS=1 req/s.}
    \label{fig:humaneval_size_carbon_per_token_quality}
\end{figure}


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/humaneval/size/carbonsaving_humaneval_qwen_multiscore.pdf}
%     \caption{Carbon Saving percentage on HumanEval of Qwen 14B and Qwen 32B compared with Qwen 7b in FUEL with Qscore Requirement Low(-5) and High(15)}
%     \label{fig:humane_eval_size_cs_qwen}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/humaneval/size/carbonsaving_humaneval_llama2_multiscore.pdf}
%     \caption{Carbon Saving percentage on HumanEval of Llama 13B compared with Llama 7B in FUEL with different Qscore requirement (-20 and 5)}
%     \label{fig:humane_eval_size_cs_llama}
% \end{figure}
If we extend the Qscore requirement and QPS into two dimensions, as demonstrated in \Cref{fig:humaneval_size_heatmap}, we observe that on HumanEval, Qwen 14B only shows an incremental carbon saving of 1-2\% at QPS = 1 req/s, while in most other cases, Qwen 7B remains the greenest model. This is because the output quality of Qwen 7B is very close to that of Qwen 14B and 32B. For the Llama model family, the results align with the previous observations: at lower QPS and higher quality requirements, the 13B model becomes the greenest option, as it can produce higher-quality responses compared to the 7B model.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/humaneval/size/heatmap_humaneval.png}
    \caption{Comparison of different model sizes on HumanEval in \SYSTEM{}. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of the greenest size compared to the second greenest.}
    \label{fig:humaneval_size_heatmap}
\end{figure}

This experiment on the HumanEval dataset further highlights our previous conclusion that selecting the greenest model size requires a comprehensive consideration of both model output quality and workload intensity.


\section{Additional Results for Quantization Case Study}
\label{sec:appendix_quant}

\begin{table*}[!t]
\centering
\caption{Evaluation on different benchmarks for Qwen and Llama families with their quantized versions.}
\label{tab:benchmark_quant}
\begin{adjustbox}{width=\textwidth}
\normalsize
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule
\textbf{Model} & \textbf{Method}  & \textbf{ARC-c} & \textbf{GSM8k } & \textbf{HellaSwag} & \textbf{MMLU } & \textbf{TruthfulQA} & \textbf{Winogrande}  \\
\midrule
\multirow{3}{*}{\textbf{Qwen-7B}} & FP16  & 63.57 & 81.96 & 62.24 & 74.23 & 49.82 & 73.64 \\
 & AWQ  & 62.03 {\color{red} (-1.54)} & 79.61 {\color{red} (-2.35)} & 61.52 {\color{red} (-0.72)} & 73.33 {\color{red} (-0.9)} & 50.43 {\color{green} (+0.61)} & 74.11 {\color{green} (+0.47)} \\
 & W8A8  & 63.65 {\color{green} (+0.08)} & 82.11 {\color{green} (+0.15)} & 62.15 {\color{red} (-0.09)} & 74.18 {\color{red} (-0.05)} & 49.45 {\color{red} (-0.37)} & 74.35 {\color{red} (-0.71)} \\
\hline
\multirow{3}{*}{\textbf{Qwen-14B}} & FP16  & 69.54  & 79.23 & 65.73 & 79.87 & 52.26 & 80.66 \\
 & AWQ   & 68.00 {\color{red} (-1.54)}& 80.89{\color{green} (+1.66)} & 64.78 {\color{red} (-0.95)} & 78.88 {\color{red} (-0.99)} & 48.84 {\color{red} (-3.42)} & 79.48 {\color{red} (-1.18)} \\
 & W8A8  & 69.71 {\color{green} (+0.71)} & 79.83 {\color{red} (-0.6)} & 65.74 {\color{green} (+0.01)} & 79.93 {\color{green} (+0.06)} & 51.04 {\color{red} (-1.22)} & 81.14 {\color{green} (+0.48)} \\
\hline
\multirow{3}{*}{\textbf{Qwen-32B}} & FP16  & 71.42  & 75.89 & 67.11 & 83.28 & 51.16 & 80.03 \\
 & AWQ   & 69.88 {\color{red} (-1.54)}& 76.72{\color{green} (+0.83)} & 66.47 {\color{red} (-0.64)} & 82.40 {\color{red} (-0.88)} & 52.14 {\color{red} (-0.98)} & 79.72 {\color{red} (-0.31)} \\
 & W8A8  & 71.08 {\color{red} (-0.34)} & 75.82 {\color{red} (-0.07)} & 67.14 {\color{green} (+0.03)} & 83.15 {\color{red} (-0.13)} & 50.55 {\color{red} (-0.61)} & 80.43 {\color{green} (+0.4)} \\
 \hline
 \multirow{3}{*}{\textbf{Llama-7B}} & FP16  & 49.83  & 23.2 & 59.34 & 47.22 & 45.04 & 72.93 \\
 & AWQ   & 48.98 {\color{red} (-0.85)}& 21.23{\color{red} (-1.97)} & 58.61 {\color{red} (-0.73)} & 45.34 {\color{red} (-1.88)} & 43.57 {\color{red} (-1.47)} & 72.53 {\color{red} (-0.4)} \\
 & W8A8  & 50.34 {\color{green} (+0.51)} & 22.67 {\color{red} (-0.53)} & 59.3 {\color{red} (-0.04)} & 47.24 {\color{green} (+0.02)} & 44.80 {\color{red} (-0.24)} & 73.32 {\color{green} (+0.39)} \\
\hline
 \multirow{3}{*}{\textbf{Llama-13B}} & FP16  & 55.63  & 35.56 & 63.1 & 53.55 & 40.88 & 75.06 \\
 & AWQ   & 54.95 {\color{red} (-0.68)}& 31.69{\color{red} (-3.87)} & 62.13 {\color{red} (-0.97)} & 53.77 {\color{green} (+0.22)} & 41.37 {\color{green} (+0.49)} & 76.09 {\color{green} (+1.03)} \\
 & W8A8  & 55.29 {\color{red} (-0.34)} & 35.18 {\color{red} (-0.38)} & 63.08 {\color{red} (-0.02)} & 53.65 {\color{green} (+0.1)} & 41.49 {\color{green} (+0.61)} & 75.22 {\color{green} (+0.16)} \\
 \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

As shown in \Cref{tab:benchmark_quant}, we used LM Eval~\cite{eval-harness}, an open-source LLM evaluation tool, to assess the LLMs used in our experiments and their quantized versions. The evaluations were conducted on tasks from the Open LLM Leaderboard, including ARC-c~\cite{clark2018arc}, GSM8k~\cite{cobbe2021gsm8k}, HellaSwag~\cite{zellers2019hellaswag}, MMLU~\cite{hendrycks2020mmlu}, TruthfulQA~\cite{lin2021truthfulqa}, and Winogrande~\cite{sakaguchi2021winogrande}.

We also present the Qscore for the LLMs and their quantized versions across three datasets, as shown in \Cref{tab:quant_qscore}.


\begin{table}[!t]
\centering
\caption{Mean Qscore on three datasets for Qwen and Llama families with their quantized versions.}
\label{tab:quant_qscore}
\begin{adjustbox}{width=0.48\textwidth}
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Model} & \textbf{Method}  & \makecell{\textbf{Qscore}\\\footnotesize{NewsQA}} & \makecell{\textbf{Qscore}\\\footnotesize{ArenaHard}} & \makecell{\textbf{Qscore}\\\footnotesize{HumanEval}}  \\
\midrule
\multirow{3}{*}{\textbf{Qwen-7B}} & FP16  & 11.11 & 15.90 & 26.82  \\
 & AWQ  & 11.77 {\color{green} (+0.66)} & 14.17 {\color{red} (-1.73)} & 26.52 {\color{red} (-0.3)}  \\
 & W8A8  & 10.46 {\color{red} (-0.65)} & 15.76 {\color{red} (-0.14)} & 26.84 {\color{green} (+0.02)} \\
\hline
\multirow{3}{*}{\textbf{Qwen-14B}} & FP16  & 14.37 & 18.41 & 27.03  \\
 & AWQ  & 12.01 {\color{red} (-2.36)} & 15.33 {\color{red} (-3.08)} & 26.41 {\color{red} (-0.62)}  \\
 & W8A8  & 14.40 {\color{green} (+0.03)} & 18.49 {\color{green} (+0.08)} & 27.24 {\color{green} (+0.21)} \\
\hline
\multirow{3}{*}{\textbf{Qwen-32B}} & FP16  & 15.62 & 19.82 & 28.86  \\
 & AWQ  & 14.87 {\color{red} (-0.75)} & 18.89 {\color{red} (-0.93)} & 27.99 {\color{red} (-0.87)}  \\
 & W8A8  & 15.36 {\color{red} (-0.26)} & 19.73 {\color{red} (-0.09)} & 28.38 {\color{red} (-0.48)} \\
\hline
\multirow{3}{*}{\textbf{Llama-7B}} & FP16  & 6.81 & -7.93 & -4.19  \\
 & AWQ  & 6.49 {\color{red} (-0.32)} & -10.19 {\color{red} (-2.26)} & -8.23 {\color{red} (-4.04)}  \\
 & W8A8  & 6.87 {\color{green} (+0.06)} & -8.58 {\color{red} (-0.65)} & -4.23 {\color{red} (-0.04)} \\
\hline
\multirow{3}{*}{\textbf{Llama-13B}} & FP16  & 8.73 & -5.63 & 0.41  \\
 & AWQ  & 8.63 {\color{red} (-0.11)} & -6.34 {\color{red} (-0.71)} & -1.65 {\color{red} (-2.06)}  \\
 & W8A8  & 8.64 {\color{red} (-0.09)} & -5.69 {\color{red} (-0.06)} & 0.24 {\color{red} (-0.17)} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Results on NewsQA Summarization}
\Cref{fig:quant_slo_awq} and \Cref{fig:quant_slo_w8a8} illustrate the impact of the AWQ and W8A8 quantized versions on SLO attainment across the QPS range for the Qwen and Llama families. As shown in Figure \ref{fig:quant_slo_awq}, the AWQ version of the models fails to meet the SLO at lower QPS values. In contrast, the W8A8 quantized version improves efficiency, enabling models to serve a higher QPS.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{llmfu-acl2025/figs/quant/slo_attn.pdf}
    \caption{SLO attainment of Qwen and Llama families with AWQ version across QPS range.}
    \label{fig:quant_slo_awq}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{llmfu-acl2025/figs/quant/slo_attn_w8a8.pdf}
    \caption{SLO attainment of Qwen and Llama families with W8A8 version across QPS range.}
    \label{fig:quant_slo_w8a8}
\end{figure}

\Cref{fig:quant_newsqa_llama_heatmap} shows the comparison results among FP16, AWQ and W8A8 versions in Llama family. W8A8 has almost the lowest carbon emission under all conditions. 


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/quant/heatmap_llama.png}
    \caption{Comparison of FP16, AWQ and W8A8 versions of Llama 7B/13B in \SYSTEM{} on NewsQA dataset. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of greenest version compared to the second greenest.}
    \label{fig:quant_newsqa_llama_heatmap}
\end{figure}

\subsection{Results on Arena Hard}
\Cref{fig:arena_quant_heat_w8a8_qwen} shows the results of different quantization methods for Qwen 7B/14B models on Arena Hard dataset. This aligns well with the results on the previous NewsQA summarization dataset, as AWQ shows an advantage at low QPS on the smaller 7B model. When we use 14B model, W8A8 illustrates the great potential to save up to 50\% carbon emission under each scenario. We can see a similar trend in \Cref{fig:arena_quant_heat_w8a8_llama} on Llama models. 
 
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/arena/quant/arena_qwen_heatmap.png}
    \vspace{-0.2in}
    \caption{Comparison of FP16, AWQ and W8A8 versions of Qwen 7B/14B in \SYSTEM{} on Arena Hard dataset. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of the greenest version compared to the second greenest.}
    \label{fig:arena_quant_heat_w8a8_qwen}
\end{figure}



\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/arena/quant/arema_llama_heatmap.png}
    \vspace{-0.2in}
    \caption{Comparison of FP16, AWQ and W8A8 versions of Llama 7B/13B in \SYSTEM{} on Areana Hard dataset. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of greenest version compared to the second greenest.}
    \label{fig:arena_quant_heat_w8a8_llama}
\end{figure}


\subsection{Results on HumanEval}
As shown in \Cref{fig:humaneval_quant_heat_w8a8_qwen}, AWQ still becomes the greenest method when QPS is low, but W8A8 dominates in more conditions on the Qwen 7B model. This is because, after W8A8 quantization, the Qscore of Qwen 7B improves on the HumanEval dataset. For the Qwen 14B model, W8A8 is no longer the greenest method under all conditions. This is due to AWQ experiencing minimal accuracy degradation on this dataset, allowing it to retain its advantage at low QPS.


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/humaneval/quant/humaneval_qwen_heatmap.png}
    \vspace{-0.2in}
    \caption{Comparison of FP16, AWQ and W8A8 versions of Qwen 7B/14B in \SYSTEM{} on HumanEval dataset. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of greenest quantization version compared to the second greenest.}
    \label{fig:humaneval_quant_heat_w8a8_qwen}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/humaneval/quant/humaneval_llama_heatmap.png}
    \vspace{-0.2in}
    \caption{Comparison of FP16, AWQ and W8A8 versions of Llama 7B/13B in \SYSTEM{} on HumanEval dataset. Tile colors indicate the model with the lowest carbon per FU. Tile values are carbon savings (\%) of greenest quantization version compared to the second greenest.}
    \label{fig:humaneval_quant_heat_w8a8_llama}
\end{figure}


\begin{figure}[!t]
    \centering
    \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/hw/hw_heatmap_all.png}
    \caption{Comparison of model-hardware combinations for Qwen and Llama in \SYSTEM{}. Tile colors indicate the model-hardware with the lowest carbon per FU. Tile values are carbon savings (\%) of the greenest choice compared to the second greenest.}
    \label{fig:hw_heatmap_all}
\end{figure}

\section{Additional Results for Hardware Case Study}
\label{sec:appendix_hardware}
\Cref{fig:hw_heatmap_all} compares model and hardware combinations, further confirming our previous conclusion: older hardware can achieve lower carbon emissions. As shown in the figure, whether for the Qwen or Llama model families, the greenest choice at low QPS is consistently the L40 server. Once the hardware is fixed, we can apply insights from the model size case study to select the model size based on the quality requirement. This pattern reaffirms that choosing the optimal model and hardware combination requires a balance between performance needs and carbon efficiency.



% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/hw/heatmap_qwen.pdf}
%     \caption{Comparison of Qwen 7B and 14B on different hardware platforms in FUEL. Tile colors indicate the model with the greatest Carbon per Functional Unit. Tile values are Carbon Savings (\%) of Greenest Model in FUEL vs. Second Greenest}
%     \label{fig:hw_heatmap_qwen}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/hw/heatmap_llama.pdf}
%     \caption{Comparison of Llama 7B and 14B on different hardware platforms in FUEL. Tile colors indicate the model with the greatest Carbon per Functional Unit. Tile values are Carbon Savings (\%) of Greenest Model in FUEL vs. Second Greenest}
%     \label{fig:hw_heatmap_qwen}
% \end{figure}







% \section{Case Study: Model Architecture}
% \fuelq{Is newer model greener?}
% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/size/caron_per_token_qvsl.pdf}
%     \caption{Carbon emission per functional unit of Qwen 7B and Qwen 14B compared with similar size Llama 7b and Llama 13B evaluated in \SYSTEM{} with Qscore Requirement -5 and 10.}
%     \label{fig:size_newsqa_cs_qvsl7}
% \end{figure}


% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/size/ttft_tpot_qwen_llama7.pdf}
%     \caption{Latency-QPS Scaling: (a) Time to First Token (TTFT) and (b) Time Per Output Token (TPOT) across Models}
%     \label{fig:size_newsqa_ttft_tpot_qwen_llama7}
% \end{figure}

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.48\textwidth]{llmfu-acl2025/figs/size/ttft_tpot_qwen_llama14.pdf}
%     \caption{Latency-QPS Scaling: (a) Time to First Token (TTFT) and (b) Time Per Output Token (TPOT) across Models}
%     \label{fig:size_newsqa_ttft_tpot_qwen_llama14}
% \end{figure}





