\clearpage
\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.95\linewidth]{LaTeX/figs/prompt.png}
  \caption{The example of used prompts in LLMs Diagnosis. The Chinese in the input prompt is the exercise content and related knowledge concepts of the PTADisc dataset.}
\label{fig: prompt}
\end{figure*}

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.85\linewidth]{LaTeX/figs/diagnosis.png}
  \caption{The example of diagnostic results of LLMs Diagnosis.}
\label{fig: diagnosis}
\end{figure*}

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=0.9\linewidth]{LaTeX/figs/diagnosis2.png}
  \caption{Diagnostic results of an example student with different LLMs.}
\label{fig: diagnosis2}
\end{figure*}

\appendix



\section{Appendix}

% \subsection{Related Work}

% \subsubsection{Cognitive Diagnosis}
% Cognitive diagnosis, which originated from educational psychology, is a fundamental task in the field of intelligent education. It characterizes students' learning status and knowledge proficiency based on their responses to various questions~\cite{liu2021towards}. In the realm of cognitive diagnosis, existing research is mainly divided into two main categories: psychometric theory-based methods~\cite{lord1952theory,de2009dina,reckase200618} and neural network-based methods~\cite{wang2020neural,gao2021rcd,bi2023beta,liu2024inductive,wang2023self}.
% Psychometric theory-based methods, such as Item Response Theory (IRT)~\cite{lord1952theory}, Multidimensional IRT (MIRT)~\cite{reckase200618}, and Deterministic Inputs, Noisy And gate model (DINA)~\cite{de2009dina}, are designed to evaluate students' proficiency through latent factors utilizing psychological theories.
% %For example, IRT hypothesizes unidimensional independence and employs continuous latent variables to assess students’ learning status and exercises and MIRT extends IRT, adopting multidimensional latent features to model students and exercises.
% Neural network-based methods use deep neural networks to profile students' learning status, focusing on modeling the complex relationships among problems, knowledge concepts, and students. NCD~\cite{wang2020neural}, as a representative neural network-based method, first incorporates neural networks into cognitive diagnosis to effectively capture the fine-grained student-exercise relationships. RCD~\cite{gao2021rcd} and RDGT~\cite{yu2024rdgt} employ graph architectures to explore the relationships among exercises, knowledge concepts, and students. Recently, BETA-CD~\cite{bi2023beta} developed a reliable and rapidly adaptable cognitive diagnosis framework for new students through meta-learning. ACD~\cite{wang2024boosting} considered the connection between students' affective states and cognitive states in learning. However, few existing cognitive diagnosis methods take into account prior knowledge, which makes it challenging for them to generate accurate diagnoses. 

% \subsubsection{Large Language Models}
% With the rise of Transformer~\cite{vaswani2017attention}, large language models (LLMs) with extensive parameters and vast training data have gradually become mainstream. 
% %As the size of the models increases, LLMs have demonstrated powerful reasoning abilities and a wealth of knowledge, which smaller language models do not possess~\cite{wei2022emergent}.
% LLMs usually follow a pre-training and fine-tuning approach to accommodate various downstream tasks. They have significantly improved performance in numerous NLP applications, including text summarization~\cite{laskar2022domain,zhang2023summit}, sentiment analysis~\cite{hoang2019aspect,deng2023llms}, and machine translation~\cite{zhang2023prompting,moslem2023adaptive}. 
% %However, the pre-training and fine-tuning of these models require substantial computational power and extensive datasets, presenting considerable challenges for researchers. This has led to the development of alternative training strategies, parameter-efficient fine-tuning (PEFT), including prefix-tuning~\cite{li2021prefix}, prompt-tuning~\cite{lester2021power}, and Low-rank adaptation (LoRA)~\cite{hu2021lora}. Meanwhile, to maximize the usage of LLMs capabilities, more and more prompting strategies attempt to make LLMs more suitable for specific scenarios without changing their parameters, such as chain-of-thought prompting~\cite{wei2022chain} and In-context Learning~\cite{min2022rethinking}.

% The advanced comprehension and reasoning capabilities, along with the extensive knowledge repository of LLMs, naturally lead to potential applications in the realm of education.
% LLMs can provide researchers with new perspectives by simulating the roles of teachers or students~\cite{wang2024user,li2023adapting,xu2024eduagent,liu2024personality}.
% However, less exploration has been made to utilize LLMs for cognitive diagnosis. 
% The demonstrated success of LLMs in text summarization tasks and educational contexts indicates LLMs' capability to undertake cognitive diagnostic tasks.
% %Zhang~et.al~\cite{zhuang2023efficiently} attempt to evaluate LLMs cognitive abilities through theories and models of cognitive diagnosis 
% %探究如何评测LLM的能力，采用了认知诊断的理论和模型来评测LLM的认知能力
% %许多工作在模拟学生状态方面有着



\subsection{Details of LLMs Diagnosis}
In this section, we provide a detailed introduction to the process of diagnosing LLMs. Using the students and exercises in the Python dataset from PTADisc as an example, we illustrate the process of diagnosis generation. Figure~\ref{fig: prompt} and Figure~\ref{fig: diagnosis} demonstrate the prompts used in this process and the examples of diagnostic results of students and exercises.


\subsubsection{Example of Used Prompts}
Figure~\ref{fig: prompt} shows the prompts we used in the Collaborative Information Collection and Diagnosis Generation processes, specifically divided into system prompts $\mathcal{M}$ and input prompts $\mathcal{P}$. The system prompt defines the role of the LLMs and introduces the format of the input prompt. It also specifies the format of the output. Here, to facilitate understanding of the LLMs, we convert the response logs into JSON format and specify that the output should also be in JSON format. To enhance the logical consistency of the LLMs' output and reduce the hallucinations, we also require them to provide reasons for their diagnostic results.
The input prompt is composed of the corresponding response logs. For the student, it includes information about the exercises they have completed. For the exercise, it includes information about the exercise itself and the completion status of all students who participated in this exercise.
Since LLMs can handle multiple languages, such as Chinese and English, the texts in the dataset can be in English or other languages. In the example shown in Figure~\ref{fig: prompt}, the information on the exercises and knowledge concepts in the PTADisc dataset is in Chinese, so we directly use Chinese to construct the input prompt.
After obtaining collaborative information, namely the overall description of students and exercises, we include this information as the profile of students and exercises in the input prompts. This allows for a more comprehensive diagnosis of both students and exercises.

\subsubsection{Comparision of Diagnostic Results}
Figure~\ref{fig: diagnosis} shows the diagnostic result of an example student and exercise Collaborative Information Collection and Diagnosis Generation, where the main difference lies in whether collaborative information is included.
From the results, it is evident that obtaining collaborative information allows for a more comprehensive diagnosis of the student's cognitive status and the attributes of the exercises.
Compared to the student, the exercise benefits significantly more from collaborative information. This is because, for the exercise, having information on all participating students allows for a more comprehensive assessment of the exercise's attributes. For the student, the information included in their response logs about the exercises they completed is usually sufficient to diagnose their cognitive status, so the addition of collaborative information does not provide a substantial improvement.

As illustrated in the figure, knowledge-enriched LLMs are capable of diagnosing both students and exercises, thereby producing comprehensive and explainable diagnostic results. Such diagnostic results can be better integrated with CDMs to achieve more accurate and comprehensive cognitive diagnoses.

\subsubsection{Comparision of Different LLMs}
\label{sec:app_llm}

\input{LaTeX/tables/llm}
In this section, we replace GPT-3.5-turbo with different LLMs to measure the impact on the LLMs diagnosis process.
Specifically, we utilize OpenAI's newly released GPT-4o-mini and another popular and powerful LLM, GLM-4~\cite{glm2024chatglm} and Qwen~\cite{hui2024qwen2}, to conduct the experiments.

As demonstrated in Table~\ref{tab:llms}, using GPT-4o-mini and GLM-4 yields slightly better results. 
This is because GPT-4o-mini, a more powerful model released by OpenAI, possesses superior understanding and reasoning capabilities compared to GPT-3.5-turbo. On the other hand, GLM-4 has stronger Chinese comprehension abilities, which enhances its ability to understand the Chinese questions in the exercise. 
However, these LLMs, which also possess strong prior knowledge and reasoning abilities, have only slight differences in their capabilities. As a result, the diagnostic outcomes may not show significant variations, which explains why the final diagnostic results of different LLMs do not differ substantially.
%For more detailed diagnostic examples, please refer to Appendix.

To clearly demonstrate the differences in diagnostics generated by various LLMs, we selected the diagnostic result of a specific student as an example for display.
As shown in Figure~\ref{fig: diagnosis2}, the three LLMs provided diagnoses of the student's cognitive state from different perspectives.
It is obvious that their diagnoses about the student's understanding of specific knowledge concepts, such as `syntax', `input/output control', and `loop', are accurate and comprehensive. 
At the same time, these LLMs can leverage their capabilities to understand and reason about this information, resulting in varied summaries and generalizations of the knowledge points rather than simple listings. Additionally, LLMs can supplement the knowledge points based on the information contained in the exercise content, thereby obtaining a more fine-grained understanding of the student's cognitive status.


\subsection{Details of Baseline Models}
\noindent\textbf{IRT}~\cite{lord1952theory} is a basic CDM that represents students and exercises as unidimensional traits and captures their interactions utilizing a linear function.

\noindent\textbf{MIRT}~\cite{reckase200618} expands the traits of students and exercises in IRT to multiple dimensions.

\noindent\textbf{NCD}~\cite{wang2020neural} utilizes deep neural networks to model the interactions between students and exercises.

\noindent\textbf{DINA}~\cite{de2009dina} models the factors of students and exercises as binary vectors, including guessing and slipping parameters to evaluate performance.


\noindent\textbf{RCD}~\cite{gao2021rcd} captures the relationships between students, exercises, and knowledge concepts using Graph Convolutional Networks.

\noindent\textbf{SCD}~\cite{wang2023self} utilize self-supervised learning to enhance the modeling of students and exercises in graph-based cognitive diagnosis, alleviating the long-tail distribution problem.

\noindent\textbf{ACD}~\cite{wang2024boosting} takes into account the emotional state of students while answering questions, designs an emotional cognition module, and combines it with traditional cognitive models, achieving good results.