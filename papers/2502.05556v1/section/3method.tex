\begin{figure*}[ht]
  \centering
  
  \includegraphics[width=1\linewidth]{figs/method3.png}
  \caption{Framework overview. (a) LLM Diagnosis generates diagnoses for students and exercises using LLMs. (b) Cognitive Level Alignment integrates LLMs and CDMs to model students and exercises in both semantic space and behavioral space.}
  \vspace{-1.5em}
\label{fig: method}
\end{figure*}

\section{Methodology}

In this section, we first present the task definition and the general framework. Then, we show the detailed strategies employed within our framework.

\subsection{Task Definition}

Formally, suppose $\mathcal{S} = \{s_1, \cdots, s_{\left|\mathcal{S}\right|}\}$, $\mathcal{E} = \{e_1, \cdots, e_{\left|\mathcal{E}\right|} \}$ and $\mathcal{K} = \{k_1, \cdots, k_{\left|\mathcal{K}\right|} \}$ be the sets of students, exercises and knowledge concepts.
%in the intelligent educational data, where ${\left|\mathcal{S}\right|}$, ${\left|\mathcal{E}\right|}$, and ${\left|\mathcal{K}\right|}$ denotes the number of students, exercises, and knowledge concepts respectively. 
%Q-matrix $\mathbf{Q} =\{Q_{ij}\}_{\left|\mathcal{E}\right|\times \left|\mathcal{K}\right|}\in \{ 0,1\} _{\left|\mathcal{E}\right|\times \left|\mathcal{K}\right|}$ indicates the relationship between exercises and knowledge concepts, where $Q_{ij}=1$ if exercise $e_i$ relates to knowledge concept $c_j$, and $Q_{ij}=0$ otherwise. 
The response logs $\mathcal{R}$ of students are represented as triplets $(s_i, e_j, \mathcal{K}_j, r_{ij}) \in \mathcal{R}$, where $r_{ij}$ indicates whether student $s_i$ correctly answered the exercise $e_j$ and $\mathcal{K}_j$ denotes the knowledge concepts related to $e_j$.
In some datasets, exercise $e$ also includes the text content $t$ as its attributes.
The goal of cognitive diagnosis is to evaluate students' proficiency levels across various knowledge concepts by predicting their performance based on the response logs $\mathcal{R}$.
%and the Q-matrix $\mathbf{Q}$.


\subsection{Framework Overview}

%To enhance traditional CDMs, we incorporate LLMs, which possess rich prior knowledge, as a complementary resource. 
The proposed Knowledge-enhanced Cognitive Diagnosis (KCD) framework consists of two main modules: \textbf{LLM diagnosis} and \textbf{cognitive level alignment}, as illustrated in Figure~\ref{fig: method}.
This framework is designed to integrate collaborative information while leveraging the rich prior knowledge of LLMs.
Additionally, it aligns the semantic space of LLMs with the behavioral space of CDMs, thereby combining the strengths of both to optimize diagnostic performance.
%Meanwhile, it also aligns the semantic space of LLMs with the behavioral space of CDMs, thereby harnessing the strengths of both to optimize diagnostic accuracy and performance.

%This framework is designed to leverage the extensive prior knowledge of LLMs and preserve the fine-grained and precise diagnosis of the student's learning status made by conventional CDMs through the alignment of LLMs and CDMs.

The LLM Diagnosis module operates in two stages: collaborative information collection and diagnosis generation. In the first stage, collaborative information is gathered from the response logs. In the second stage, this information, together with the response logs, is utilized to assess students' cognitive statuses and the attributes of exercises. 
%Collaborative information collection aims to obtain collaborative information from response logs. Diagnosis generation then utilizes this collaborative information, along with the response logs, to determine students' cognitive statuses and the attributes of exercises.
The Cognitive Level Alignment module then introduces these LLM-generated diagnoses into conventional CDMs, enhancing the cognitive-level representation of students and exercises. 
%On the other hand, The cognitive level alignment module leverages the diagnoses of students and exercises generated by LLMs, introducing them to the conventional CDMs and enhancing the representation of students and exercises at the cognitive level. 
This module utilizes two alignment methods, behavioral space alignment and semantic space alignment, to align the textual diagnoses from the semantic space of LLMs and the behavioral space of CDMs.
The framework is model-agnostic, offering flexibility in selecting appropriate CDMs tailored to various educational scenarios, ultimately achieving optimal diagnostic results.

\subsection{LLM Diagnosis}
LLMs can be guided more effectively through carefully crafted natural language instructions, resulting in higher-quality outputs. In this section, we distinguish between two types of input instructions for LLMs: 
%For LLMs, carefully designed natural language instructions can guide them to accomplish the required tasks better and generate higher-quality outputs. In this section, we divide the input instructions for LLMs into 
system prompts $\mathcal{M}$ and input prompts $\mathcal{P}$. The system prompt $\mathcal{M}$ defines the tasks that LLMs need to perform and specifies the input and output formats, while the input prompt $\mathcal{P}$ consists of specific input data (\textit{i.e.}, students' response logs). 
%The LLM Diagnosis module focuses on leveraging the extensive prior knowledge of LLMs to diagnose students' cognitive status and attributes of exercises.


\subsubsection{Collaborative Information Collection.}
Experienced teachers enhance their diagnoses by utilizing information from other students and exercises. To mimic this capability, we introduce a collaborative information collection stage. This stage aims to extract student collaborative information from a student's performance across all completed exercises and exercise collaborative information from all participating students for a given exercise. 
%This collaborative information enriches the subsequent diagnostic process.

Specifically, we employ different instruction strategies to diagnose students and exercises. For students, the system prompt $\mathcal{M}_s$ defines the input prompt $\mathcal{P}_s$ format and guides LLMs in generating textual collaborative information. The input prompt $\mathcal{P}_s$ contains the problem content $t$, related knowledge concepts $k$, and the student’s response $r$ for all participated exercises $e$. The input of LLMs is formatted as:
\begin{figure}[!thb]
\centering
\vspace{-1.5em} 
%\setlength{\tabcolsep}{0.4mm}
\fcolorbox{black}{gray!6}{%
\parbox{0.9\linewidth}{%
\small
\noindent$\bullet$ \textbf{System Prompt}: You will serve as an experienced teacher to help me determine the student's learning status.
I will provide you with information about exercises that the student has finished, described as STUDY HISTORY, as well as his or her answer of those exercises...
%Given a programming question and a corresponding piece of buggy code written in \textless language\textgreater, please provide a program repair proposal for the buggy code.  Use `-' to represent the line that may need to be deleted or modified.\\
\vspace{0.2em} 
\rule{\linewidth}{0.48pt} 
% $\bullet$ \textbf{Programming Task}: $q$\\
% $\bullet$ \textbf{Buggy Code}: $c$\\
$\bullet$ \textbf{Input Prompt}:\\
\vspace{-0.1em} 
\hspace{1em} STUDY HISTORY: \\
\vspace{-0.1em}
\hspace{1em} \{content: $t$, concept: $k$, answer: $r$\}; ...\\
}%
}
%\caption{Prompt of Self-Debug Learning.} 
\label{fig:Fault-Driven-prompt}
\vspace{-1em} 
\end{figure}


Similarly, the input of LLMs for exercises is formatted in the same pattern, where $\mathcal{P}_e$ contains the exercise content $t$, related knowledge concepts $k$, and student responses $r$ for all participating students $s$.
% \begin{figure}[!thb]
% \centering
% %\setlength{\tabcolsep}{0.4mm}
% \fcolorbox{black}{gray!6}{%
% \parbox{0.9\linewidth}{%
% \small
% \noindent$\bullet$ \textbf{System Prompt}: You will serve as an experienced teacher to help me summarize the characteristics of the exercise and what kind of students might correctly answer this exercise.
% I will provide you with the BASIC INFORMATION (exercise content, related concept) of the exercise and also STUDY HISTORY of students for it...
% %Given a programming question and a corresponding piece of buggy code written in \textless language\textgreater, please provide a program repair proposal for the buggy code.  Use `-' to represent the line that may need to be deleted or modified.\\
% \vspace{0.2em} 
% \rule{\linewidth}{0.48pt} 
% % $\bullet$ \textbf{Programming Task}: $q$\\
% % $\bullet$ \textbf{Buggy Code}: $c$\\
% $\bullet$ \textbf{Input Prompt}:\\
% \vspace{-0.1em} 
% \hspace{1em}BASIC INFORMATION: \\
% \vspace{-0.1em}
% \hspace{1em} content: $t$, concept:$k$\\
% \vspace{-0.1em} 
% \hspace{1em}STUDY HISTORY: \\
% \vspace{-0.1em}
% \hspace{1em} \{answer: $r$\};...\\
% }%
% }
% %\vspace{-1em} 
% \end{figure}
In this way, we can get the collaborative information $\mathcal{I}$ through $\mathcal{I}=\text{LLMs}\left(\mathcal{M}, \mathcal{P}\right)$.
%Experienced teachers enhance their diagnoses by utilizing information from other students and exercises. To mimic this capability, we introduce a collaborative information collection stage. This stage aims to extract student collaborative information from a student's performance across all completed exercises and exercise collaborative information from all participating students for a given exercise. This collaborative information enriches the subsequent diagnostic process.
%Experienced human teachers can enhance their diagnosis of a specific student and exercise by utilizing information from other students and exercises. To achieve this capability, we introduce the collaborative information collection stage. This stage aims to obtain student collaborative information from a student's performance on all completed exercises, and obtain exercise collaborative information from all participated students for an exercise. The collaborative information facilitates the acquisition of more detailed diagnoses for the next stage.

%Specifically, we employ different instruction strategies to diagnose students and exercises. For students, the response logs for each student are utilized, with each input prompt $\mathcal{P}_s$ containing the problem content $t$, related knowledge concepts $k$, and the student’s response $r$ for all participated exercises $e$.
%we utilize the response logs of each student. Each input prompt $\mathcal{P}_s$ contains problem content $t$, related knowledge concepts $k$, and student response $r$ of all the participated exercises $e$. 
%The system prompt $\mathcal{M}_s$ defines the input prompt $\mathcal{P}_s$ format and guides LLMs in generating textual collaborative information. 
%The system prompt $\mathcal{M}_s$ illustrates the format of input prompt $\mathcal{P}_s$ and guides LLMs to generate textual collaborative information.
%Similarly, for exercises, the response logs are converted into records of participating students, with each input prompt $\mathcal{P}_e$ containing the exercise content $t$, related knowledge concepts $k$, and student responses $r$ for all participating students $s$.
%we utilize the same strategy to construct system prompt $\mathcal{M}_e$ and input prompt $\mathcal{P}_e$. Here, the response logs are transferred to the records of participating students. Each input prompt $\mathcal{P}_e$ contains the exercise's problem content $t$, related knowledge concepts $k$, and student response $r$ of all the participating students $s$.
%The formulation of collaborative information can be inferred by:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{I}_{s}=\text{LLMs}\left(\mathcal{M}_{s}, \mathcal{P}_{s}\right) \\
%     \mathcal{I}_{e}=\text{LLMs}\left(\mathcal{M}_{e}, \mathcal{P}_{e}\right)
%     \end{matrix}\right.
% \end{equation}
% where $\mathcal{I}_{s}$ denotes the collaborative information of students and $\mathcal{I}_{e}$ denotes the collaborative information of exercises.
%In this way, we can initially obtain the students' and exercises' collaborative information $\mathcal{I}$.

% The system prompt $\mathcal{M}_s$ illustrates the format of input prompt $\mathcal{P}_s$ and guides LLMs to generate textual collaborative information. The formulation of input prompt and collaborative information of students can be inferred by:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{P}_{s}=\text{Text}((t,k,r)_e) \\
%     \mathcal{I}_{s}=\text{LLMs}\left(\mathcal{M}_{s}, \mathcal{P}_{s}\right)
%     \end{matrix}\right.
% \end{equation}
% where $\text{Text}(\cdot)$ denotes the textual representation of the data. $((t,k,r)_e)$ denotes the $t$, $k$, and $r$ of each participated $e$ of student $s$. 
% $\mathcal{I}_{s}$ denotes the collaborative information of students.

% Similarly, for exercises, we utilize the same strategy to construct system prompt $\mathcal{M}_e$ and input prompt $\mathcal{P}_e$. Here, the response logs are transferred to the records of participating students. Each input prompt $\mathcal{P}_e$ contains the exercise's problem content $t$, related knowledge concept $k$, and student response $r$ of all the participating students $s$.
% The formulation of input prompt and collaborative information of exercises can be inferred by:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{P}_{e}=\text{Text}(t,k,(r)_s) \\
%     \mathcal{I}_{e}=\text{LLMs}\left(\mathcal{M}_{e}, \mathcal{P}_{e}\right)
%     \end{matrix}\right.
% \end{equation}
% where $(r)_s$ denotes the response of each participated student $s$ for exercise $e$. $\mathcal{I}_{e}$ denotes the collaborative information of exercises.


\subsubsection{Diagnosis Generation.}
Once the collaborative information $\mathcal{I}$ for students and exercises has been gathered, the next step is to generate diagnoses of students' cognitive statuses and exercise attributes.
% using the response logs and collaborative information.
%After obtaining the students' and exercises' collaborative information $\mathcal{I}$, we need to generate the students' cognitive status and exercise attributes in natural language format through response logs and collaborative information.
Firstly, we combine the collaborative information $\mathcal{I}$ obtained for each student and exercise with the corresponding response logs to provide more detailed information, formulating input prompt $\mathcal{P}^\prime$.
%  The corresponding input prompt $\mathcal{P}^\prime$ can be expressed as:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{P}^\prime_{s}=\text{Text}(i_s(t,k,r,i_e)_e)\\
%     \mathcal{P}^\prime_{e}=\text{Text}(t,k,i_e,(r,i_s)_s)
%     \end{matrix}\right.
% \end{equation}
Then, we adjust the content of system prompt $\mathcal{M}^\prime$ to define the new format of $\mathcal{P}^\prime$ and guide LLMs to generate the corresponding students' cognitive status and exercise attributes. We can get the diagnoses $\mathcal{T}$ through $\mathcal{T}=\text{LLMs}\left(\mathcal{M}^\prime, \mathcal{P}^\prime\right)$. 
% The entire process of the generation can be formulated as:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{T}_{s}=\text{LLMs}\left(\mathcal{M}^\prime_{s}, \mathcal{P}^\prime_{s}\right) \\
%     \mathcal{T}_{e}=\text{LLMs}\left(\mathcal{M}^\prime_{e}, \mathcal{P}^\prime_{e}\right)
%     \end{matrix}\right.
% \end{equation}
% where $\mathcal{T}$ denotes the diagnoses of students and exercises. $\mathcal{T}_{s}$ represents students' cognitive status and $\mathcal{T}_{e}$ represents exercises' attributes, both described in natural language.


\subsection{Cognitive Level Alignment}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.95\linewidth]{LaTeX/figs/method2.png}
%   \caption{An illustration of acquiring student and exercise embeddings from NCD model.}
% \label{fig: method2}
% \end{figure}

%perceptual, embedding, cognitive, symbolic

By leveraging LLMs, we can generate textual diagnoses of students' cognitive status and exercises' attributes. 
However, LLMs cannot fully comprehend response logs due to constraints on input length, which restrict the inclusion of student-exercise interactions.
Therefore, it is necessary to align these LLM-generated diagnoses with those produced by CDMs at the cognitive level. 
Since LLMs operate within a semantic space while CDMs work within a behavioral space, both need to be mapped to a common space for effective alignment. To achieve this, we propose two alignment methods: behavioral space alignment (KCD-Beh) and semantic space alignment (KCD-Sem).

Before implementing the alignment approach, we obtain the semantic representation of LLMs by encoding their textual diagnoses. Specifically, we utilize the text embedding model~\cite{su2023one}, which has demonstrated significant strength in textual representation, to encode the diagnoses as follows: $\mathbf{L}=\mathbf{E}(\mathcal{T})$,
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathbf{L}_s=\mathbf{E}(\mathcal{T}_{s})\\
%     \mathbf{L}_e=\mathbf{E}(\mathcal{T}_{e})
%     \end{matrix}\right.
% \end{equation}
where $\mathbf{E}(\cdot)$ denotes the text embedding models and $\mathbf{l}\in\mathbf{L}$ denotes the modeling of students and exercises generated by LLMs in semantic space. Meanwhile, we denote the representation embeddings of students and exercises by CDMs as $\mathbf{c}\in\mathbf{C}$ in behavioral space, such as Neural Cognitive Diagnosis (NCD)~\cite{wang2020neural}, as described in Figure~\ref{fig: method}.
%, as described in Figure~\ref{fig: method2}.



% \subsubsection{Aligment Theory}

% According to previous works~\cite{oord2018representation,ren2023representation}, the alignment process aims to maximize the mutual information $I(\mathbf{l} ;\mathbf{e} )$ between LLMs' diagnoses representation $\mathbf{l}$ and CDMs' diagnoses representation $\mathbf{c}$. 
% Here, the maximization of $I(\mathbf{l}_{i} ;\mathbf{c}_{i} )$ can be formulated as maximizing the following lower bound $F$:
% \begin{equation}
%     F(f)=\mathbb{E} \log{[\frac{f(\mathbf{l}_{i} ;\mathbf{c}_{i})}{\textstyle \sum_{\mathbf{l}_j\in \mathbf{L} }^{}f(\mathbf{l}_{j} ;\mathbf{c}_{i})}]}   
% \end{equation}
% where $f(\mathbf{l},\mathbf{c})\propto p(\mathbf{l}|\mathbf{c})/p(\mathbf{l})$ denotes the density ratio, serving as a measurement that reveals the similarities between $\mathbf{l}$ and $\mathbf{c}$. $\mathbf{L} =\left \{  \mathbf{l}_1, \cdots, \mathbf{l}_{\left|\mathbf{L}\right|}\right \} $ denotes the representation of LLMs' diagnoses and $\mathbf{C} =\left \{  \mathbf{c}_1, \cdots, \mathbf{c}_{\left|\mathbf{C}\right|}\right \} $ denotes the representation of CDMs' diagnoses. In this way, we can fully utilize the valuable information in $\mathbf{L}$ and $\mathbf{C}$, eliminate noise, and achieve more accurate diagnoses through an accurate modeling of $f(\mathbf{l},\mathbf{c})$.



\subsubsection{Behavioral Space Alignment.}

Behavioral space alignment involves mapping the LLM-generated models of students and exercises to the behavioral space of CDMs. 
%namely mapping the modeling of students and exercises by LLMs to the behavioral space of CDMs.
We employ contrastive learning, a widely-used technique for bidirectionally aligning different views~\cite{khosla2020supervised,cui2023generalized}, to align the representations of LLMs and CDMs within the behavioral space.
%Contrastive learning has been widely utilized for aligning different views bidirectionally~\cite{khosla2020supervised,cui2023generalized}, which is essential for aligning the modeling of LLMs and CDMs within behavioral space.
The intuition behind using contrastive learning is that $\mathbf{c}_i$ and $\mathbf{l}_i$ are most similar to each other within $\mathbf{L}$ since they represent the same student or exercise. 
%The intuition behind using contrastive learning to align the modeling of two models lies in the fact that for $\mathbf{c}_i$, $\mathbf{l}_i$ is always the most similar among $\mathbf{L}$ because they represent the same student or exercise. 
We apply a multi-layer perceptron (MLP) to map $\mathbf{l}$ from the LLMs’ semantic space to the CDMs’ behavioral space, denoted as $\mathbf{l}^\prime=\mathrm{MLP}(\mathbf{l})$.
%Here we utilize a multi-layer perception (MLP) to map $\mathbf{l}$ from LLMs semantic space to CDMs behavioral space, denoted as $\mathbf{l}^\prime=\mathrm{MLP}(\mathbf{l})$.

Specifically, we conduct contrastive learning from both global and local perspectives. 
Global contrast involves using the entire set $\mathbf{L}$, while local contrast selects a subset $\mathbf{L}^\prime \subset \mathbf{L}$, composed of the $k$ most similar students and exercises for each student and exercise. This subset is obtained by calculating the cosine similarity between each student and exercise with others and selecting the top $k$ most similar instances ($k=20$ in our experiments).
Global contrast captures general features, while local contrast captures fine-grained differences between similar students and exercises.
%Global contrast captures the global features of modeling, while local contrast captures the fine-grained differences between similar students and exercises.

During training, we use the InfoNCE~\cite{oord2018representation} loss function to calculate both global and local contrast loss values, aiming to maximize the mutual information between $\mathbf{c}$ and $\mathbf{l}$ within the behavioral space, denoted as:
\begin{equation}
    f=-\frac{1}{N} \sum_{i=1}^{N} \log \left(\frac{\exp \left(\frac{x_{i} \cdot y_{i^+}}{\tau}\right)}{\sum_{j=1}^{N} \exp \left(\frac{x_{i} \cdot y_{j^{-}}}{\tau}\right)}\right),
\end{equation}
where $x_{i}$ and $y_{i^+}$ are positive samples, $y_{j^{-}}$ represents the negative sample, $\tau$ denotes the temperature parameter, $N$ denotes the number of samples.
For CDMs, the loss function is denoted as $\mathcal{L}_{cdm}$. For example, the loss function of NCD is cross entropy between output $y$ and ground truth $r$:
\begin{equation}
    \mathcal{L}_{cdm}=-\sum_{i}\left(r_{i} \log y_{i}+\left(1-r_{i}\right) \log \left(1-y_{i}\right)\right).
\end{equation}

The complete loss function is formulated as:
%. The InfoNCE loss function is based on maximizing mutual information, which helps align the information contained in $\mathbf{c}$ and $\mathbf{l}$ within the behavioral space. The entire loss function can be formulated as:
\begin{equation}
    \left\{\begin{matrix}
    \mathcal{L}_{global}=f(\mathbf{c}_i, \mathbf{l}^\prime_i, \mathbf{L}^\prime),   \\
    \mathcal{L}_{local}=f(\mathbf{c}_i, \mathbf{l}^\prime_i, \mathbf{L}^\prime_k),   \\
    \mathcal{L}=\mathcal{L}_{cdm}+\alpha\mathcal{L}_{global}+\beta\mathcal{L}_{local},
    \end{matrix}\right.
\end{equation}
where $f(x_i, x_j, X_k)$ denotes the InfoNCE loss function, $x_i$ and $x_j$ are positive samples, $X_k$ represents the set of negative samples. The term $\mathcal{L}_{cdm}$ denotes the loss function of CDMs. $\mathcal{L}_{global}$ and $\mathcal{L}_{local}$ denote the global contrast loss function and local contrast loss function of behavioral space alignment. $\alpha$ and $\beta$ are hyper-parameters.
By optimizing this loss function $\mathcal{L}$, the CDMs can effectively incorporate the modeling information of students and exercises derived from LLMs, aligning them within the CDMs' behavioral space.

% Representation Space Alignment
%在CDMs的

% Contrastive learning has been widely utilized for aligning different views bidirectionally~\cite{khosla2020supervised,cui2023generalized}, which is essential for combining the perspectives of LLMs and CDMs.
% The intuition behind using contrastive alignment lies in the fact that for $\mathbf{c}_i$, $\mathbf{l}_i$ is always the most similar among $\mathbf{L}$ because they represent the same student or exercise.
% The process of contrastive alignment to implement density ratio $f(\mathbf{l},\mathbf{c})$ essentially involves pulling $\mathbf{c}_i$ and $\mathbf{l}_i$ closer together to align their representations. For the remaining samples, they are treated as negative samples and pulled apart.
% Formally, the entire process can be formulated as:
% \begin{equation}
%     f_{con}(\mathbf{l},\mathbf{c})=exp(sim(\mathbf{l},\mathbf{c}))
% \end{equation}
% where $sim(\cdot)$ denotes the cosine similarity. Here we utilize a multi-layer perception (MLP) to map $\mathbf{l}$ from LLMs semantic space to CDMs representation space, denoted as $\mathbf{l}=\mathrm{MLP}(\mathbf{l})$, ensuring that $\mathbf{l}$ and $\mathbf{c}$ are in the same representation space.

% After obtaining the density ratio $f$, we build a model-agnostic framework based on CDMs, which is formulated as:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{L}_{con}=-F(f_{con})   \\
%     \mathcal{L}=\mathcal{L}_{cdm}+\lambda\mathcal{L}_{con}
%     \end{matrix}\right.
% \end{equation}
% where $\mathcal{L}_{cdm}$ denotes the loss function of CDMs. $\mathcal{L}_{con}$ denotes the loss function of contrastive alignment. $\lambda$ is a hyper-parameter.
% By optimizing this loss function $\mathcal{L}$, the mutual information between $\mathbf{l}$ and $\mathbf{c}$ can be maximized.
% In these alignment approaches, the alignment of student representations $\mathbf{l}_s$ and $\mathbf{c}_s$ and the alignment of exercise representations $\mathbf{l}_e$ and $\mathbf{c}_e$ are the same. Therefore, we will uniformly represent them as $\mathbf{l}$ and $\mathbf{c}$ in the following context.

\subsubsection{Semantic Space Alignment.}

In addition to aligning within the behavioral space of CDMs, we also perform alignment in the semantic space of LLMs. Since the semantic space encapsulates rich features of students and exercises, inspired by masked autoencoders (MAE)~\cite{he2022masked}, we utilize a mask-reconstruction strategy to align the representations of the two models. 
%This approach, inspired by masked autoencoders (MAE)~\cite{he2022masked}, employs self-supervised contrastive learning to align the masked embeddings of $\mathbf{c}$ and $\mathbf{l}$. 
% In addition to aligning in the behavioral space of CDMs, we also conduct alignment in the semantic space of LLMs. Since the modeling of LLMs in the semantic space contains rich semantic features of students and exercises, we employ a mask-reconstruction approach to align the modeling of two models.
% This approach utilizes the mask-reconstruction generative modeling approach, a widely-used self-supervised mechanism in various fields~\cite{feichtenhofer2022masked,ye2023graph}. Typically, this approach models the data samples by reconstructing the input data that is partially masked.
% Inspired by masked autoencoder (MAE)~\cite{he2022masked}, we employ a self-supervised contrastive learning approach to align the masked embeddings of $\mathbf{c}$ and $\mathbf{l}$.
Initially, a multi-layer perceptron (MLP) maps $\mathbf{c}$ from the CDMs' behavioral space to the LLMs' semantic space, denoted as $\mathbf{c}^\prime=\mathrm{MLP}(\mathbf{c})$. We then apply a dynamic masking strategy that varies the mask ratio based on the frequency of occurrence of students and exercises. For frequently occurring instances, we increase the mask ratio to extract more semantic information from LLMs, while for less frequent instances, we reduce the mask ratio to minimize the introduction of noise. This process is represented as:
% Specifically, we first utilize an MLP to map $\mathbf{c}$ from CDMs behavioral space to LLMs semantic space, denoted as $\mathbf{c}^\prime=\mathrm{MLP}(\mathbf{c})$.
% Then, we adopt a dynamic masking strategy to set different mask ratios for students and exercises' representations $\mathbf{c}$ based on their occurrence frequency. For those with higher frequencies, we increase the mask ratio to obtain more semantic information from LLMs to assist in their modeling. For those with lower frequencies, we decrease the mask ratio to prevent noise from affecting them. This process can be formulated as:
\begin{equation}
    \widehat{\mathbf{c}_i} =\mathrm{MASK}(\mathbf{c}_i, ratio_i),
\end{equation}
where $\widehat{\mathbf{c}_i}$ represents the masked embeddings of student $i$, and $ratio_i$ denotes the mask ratio applied.


During training, the InfoNCE loss function is employed again to calculate the reconstruction loss $\mathcal{L}_{recon}$, which maximizes mutual information between $\mathbf{c}$ and $\mathbf{l}$, thereby aiding the accurate reconstruction of $\mathbf{c}$. The overall loss function is defined as:
%During the training process, we employ the InfoNCE loss function to form reconstruction loss, which maximizes the mutual information between $\mathbf{c}$ and $\mathbf{l}$. The entire loss function can be formulated as:

\begin{equation}
    \left\{\begin{matrix}
    \mathcal{L}_{recon}=f(\widehat{\mathbf{c}_i}^\prime, \mathbf{l}, \mathbf{L}), \\
    \mathcal{L}=\mathcal{L}_{cdm}+\lambda\mathcal{L}_{recon},
    \end{matrix}\right.
\end{equation}

The hyperparameter $\lambda$ controls the influence of the reconstruction loss on the overall optimization. By optimizing this combined loss function $\mathcal{L}$, the model can produce more accurate and robust representations by reconstructing the masked inputs, thereby aligning the representations from both CDMs and LLMs within the semantic space. This alignment enhances the ability of CDMs to incorporate the rich semantic information provided by LLMs, leading to improved diagnostic accuracy and robustness.


% Semantic Space Alignment
%在LLMs的semantic space进行对齐，使用基于MAE的reconstruction方法进行
% Reconstruction alignment utilizes the mask-reconstruction generative modeling approach, a widely-used self-supervised mechanism in various fields~\cite{feichtenhofer2022masked,ye2023graph}. This approach typically models the data samples by reconstructing the input data that is partially masked.
% Here we employ masked autoencoder (MAE) as the reconstruction alignment approach to implement the density ratio $f(\mathbf{l},\mathbf{c})$.
% Specifically, we replace part of the embeddings of $\mathbf{c}$ with learnable tokens randomly.
% Then, we map $\mathbf{c}$ into the semantic space of LLMs using an MLP network and reconstruct the representation of $\mathbf{c}$ using self-supervised contrastive learning.
% The entire process can be formulated as:
% \begin{equation}
%     f(\mathbf{l},\mathbf{c})=exp(sim(\mathbf{l},\mathbf{c}^\prime ))
% \end{equation}
% where $\mathbf{c}^\prime=\mathrm{MLP}(\mathrm{mask}(\mathbf{c}))$ denotes the $\mathbf{c}$ after masking and mapping. $\mathrm{mask}(\cdot)$ denotes the process of masking.

% After obtaining the density ratio $f$, we utilize the same approach to build the model-agnostic framework, which is formulated as:
% \begin{equation}
%     \left\{\begin{matrix}
%     \mathcal{L}_{rec}=-F(f_{rec})   \\
%     \mathcal{L}=\mathcal{L}_{cdm}+\lambda\mathcal{L}_{rec}
%     \end{matrix}\right.
% \end{equation}
% Similarly, optimizing the loss function $\mathcal{L}$ can lead to maximizing the mutual information between $\mathbf{l}$ and $\mathbf{c}$.