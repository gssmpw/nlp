\input{tables/dataset}
\section{Experiments}
\label{sec: exp}

In this section, we conduct experiments to answer the following research questions:
\begin{itemize}
\item \textbf{RQ1}: Can the proposed model effectively improve the performance of the original CDMs?  
\item \textbf{RQ2}: What is the impact of each component within the proposed method? 
\item \textbf{RQ3}: How does the proposed model perform on cold-start scenarios? 
% \item \textbf{RQ4}: What are the differences in diagnostic effectiveness when using different LLMs?
\item \textbf{RQ4}: How effective is the alignment of semantic and behavioral space embeddings during the cognitive level alignment process?
\end{itemize}

\subsection{Experimental Settings}

\subsubsection{Datasets}

\input{tables/baseline}
In our experiments, we utilize four courses, Python Programming (Python), Linux System (Linux), Database Technology and Application (Database), and Literature and History (Literature), from a publicly available dataset PTADisc~\cite{hu2023ptadisc}, which comes from real-world students' responses in the educational website PTA\footnote{\url{https://pintia.cn/}} and contains textual information of exercises and knowledge concepts. 
%Each response log in the dataset contains a student ID, an exercise ID, whether the student correctly answers the question, the content of the exercise, and the knowledge concepts related to the exercise.
The statistics of the datasets are presented in Table~\ref{tab: dataset}.
The datasets are divided into training, validation, and testing sets, with a ratio of 8:1:1.

\subsubsection{Evaluation Metrics}

Following previous works, we evaluate the students' cognitive status by predicting the performance of students on the testing set, as the cognitive status can not be directly observed. We adopt commonly used metrics, namely the Area Under a ROC Curve (AUC), the Prediction Accuracy (ACC), and the Root Mean Square Error (RMSE), to validate the effectiveness of the CDMs.
%In the subsequent tables, \textbf{bold} numbers represent the best performance, while \underline{underlined} numbers represent the second-best performance. 
For all the metrics, $\uparrow$ represents that a greater value is better, while $\downarrow$ represents the opposite.

\subsubsection{Baseline Methods}

To validate the effectiveness of the proposed method, we conduct experiments on several representative CDMs, including IRT~\cite{lord1952theory}, MIRT~\cite{reckase200618}, DINA~\cite{de2009dina}, NCD~\cite{wang2020neural}, RCD~\cite{gao2021rcd}, SCD~\cite{wang2023self} and ACD~\cite{wang2024boosting}.
 

\subsubsection{Implementation Details}

We utilize PyTorch to implement both the baseline methods and our proposed KCD framework. 
For the baseline models, We use the default hyper-parameters as stated in their papers and for KCD, we use the same hyper-parameter settings, such as training epoch, learning rate, and batch size.
We employ ChatGPT to represent LLMs (specifically, gpt-3.5-turbo-16k) and text-embedding-ada002 as the text embedding model. All the experiments are conducted on a GeForce RTX 3090 GPU.
We train the model on train set and at the end of each epoch, we evaluate the model on the validation set.
The hyper-parameter $\alpha$, $\beta$, and $\lambda$ was set to $0.04$, $0.015$, and $0.2$.
Since our dataset does not include affect labels, we utilize the unsupervised contrastive ACD model and employ NCD as the basic cognitive diagnosis module.
The behavioral space alignment approach is denoted as `-Beh' and the semantic space alignment approach is denoted as `-Sem'.
% We investigated the impact of the hyper-parameter $\lambda$, within the range $[0,0.2,\cdots,1]$ with a step size of $0.2$. Our analysis revealed that setting $\lambda$ to $0.1$ resulted in the best performance across all three datasets.

\begin{figure}[t]
  \centering
  
  \includegraphics[width=1.02\linewidth]{figs/experimentx.png}
  \caption{Performance comparison in cold (blue) and warm (red) scenarios on Python dataset.}
  \vspace{-2em}
\label{fig: experiment1}
\end{figure}

\subsection{Performance Comparison (RQ1)}
To demonstrate the effectiveness of our proposed method in improving cognitive diagnosis, we implement the framework on seven cognitive diagnosis models, and the results are shown in Table~\ref{tab:performance}. 
Additionally, we compared the performance of NCD in warm and cold scenarios, with the results illustrated in Figure~\ref{fig: experiment1}. Here we define the cold scenario as less than $3$ interactions in the training set for exercises and define the warm scenario as more than $10$ interactions in the training set for exercises. Following this definition, we divide the testing set into cold and warm subsets.
We have the following observations from the results: 

\begin{itemize}[leftmargin=*]
    \item[1)]  
    Both KCD-Beh and KCD-Sem achieve significant improvements compared to the basic CDMs.
    This indicates that our proposed framework is widely applicable to various CDMs, and both alignment methods can effectively align the behavioral space of CDMs and the semantic space of LLMs.
    In most models, the behavioral space alignment approach performs better, indicating that aligning in the behavioral space of CDMs can better align information from the semantic space of LLMs.
    \item[2)] Compared to basic CDMs, our proposed methods demonstrate improvements in both cold and warm scenarios, especially in cold scenarios. This indicates that our approach of introducing LLMs as knowledge enhancement effectively alleviates the cold-start issue.
\end{itemize}




\input{tables/ablation}
\subsection{Ablation Study (RQ2)}


To validate the effectiveness of different components of our proposed method, we conduct ablation experiments to verify several components utilized in LLM Diagnosis and Cognitive Level alignment, including the usage of collaborative information (denoted as `Coll. Info'), the local contrast and global contrast (denoted as `Local Con.' and `Global Con.'), and the dynamic masking strategy (denoted as `Dym. Mask').

Table~\ref{tab:ablation} demonstrates the results of the ablation study on Python dataset, comparing the model performance after removing specific components (denoted as `w/o'). `w/o Coll. Info' represents replacing collaborative information in the process of diagnosis generation and `w/o Dym. Mask' represents replacing dynamic masking strategy with a constant mask ratio.
Experimental results show that removing these components individually leads to a decline in the model's performance. This indicates that these components are crucial for the model's performance.


\begin{figure}[t]
  \centering
  
  \includegraphics[width=1\linewidth]{figs/drop.png}
  \caption{Performance on different dropout ratios.}
  
\label{fig: drop}
\end{figure}
\subsection{Performance on Cold-Start Scenarios (RQ3)}

we conduct additional experiments on sub-datasets with varying degrees of sparsity. Specifically, we apply random dropout to the training sets of the Python and Linux datasets at ratios of $10\%$, $20\%$, $30\%$, $40\%$, and $50\%$.

Figure~\ref{fig: drop} shows the results of the experiments on different dropout ratios. It is obvious that as the dropout ratio increases, both AUC and ACC decrease. This is because the training set becomes more sparse, approaching a cold-start scenario. 
Additionally, compared to ACC, AUC experiences a greater decline, which might be due to the different calculation methods of the two metrics. 
% For more sparse datasets, Python, AUC experience a more significant decrease compared to the Linux dataset. From the experimental results, it can be seen that our proposed method is effective across different dropout ratios, leading to significant improvements for CDMs. More specifically, from the different performances of NCD-Beh and NCD-Sem in the Linux and Python datasets, it can be seen that we can choose different alignment methods based on the dataset to achieve better diagnostic results.


\begin{figure}[t]
  \centering
  \vspace{-1em}
  \includegraphics[width=1\linewidth]{figs/experiment2.png}
  \caption{The t-SNE visualization of student embeddings on Literature dataset.}
  \vspace{-2em}
\label{fig: experiment2}
\end{figure}
\subsection{Visualization of Semantic and Behavioral Embeddings (RQ4)}


To validate the effectiveness of the two alignment processes, we utilize t-SNE~\cite{van2008visualizing} to visualize the distribution of features in LLMs semantic space and CDMs behavioral space. We randomly select 200 example students and map their behavioral embeddings and semantic embeddings to 2-dimensional space. NCD (w/o Alignment) represents the original CDMs without alignment.

Figure~\ref{fig: experiment2} demonstrates the integration of semantic and behavioral embeddings of NCD-Beh and NCD-Sem, with their distributions closely merged compared to original CDMs. This proves the effectiveness of the two alignment methods we proposed.

\begin{figure}[t]
  \centering
  
  \includegraphics[width=1\linewidth]{figs/case.png}
  \caption{The case study of a student on multiple knowledge concepts on Linux dataset.}
  \vspace{-2em}
\label{fig: case}
\end{figure}

\subsection{Case Study}


To more intuitively demonstrate the improvements our proposed methods bring to CDMs, we selected a diagnosis for a specific student in the Linux dataset and compared the prediction results of NCD with the diagnosis results of NCD-Beh.
As illustrated in Figure~\ref{fig: case}, we randomly choose a student, and list his mastery of some knowledge concepts predicted by NCD and our proposed NCD-Beh.
This student correctly answered the exercises related to `numerical encoding' and `process communication', showing mastery of these concepts. He answered other exercises incorrectly, indicating a lack of familiarity with the remaining knowledge concepts.
From the LLM's diagnostic results, it can be observed that the LLM captured similar question-answer information from the training set and made corresponding inferences. This played an important role in NCD-Beh's more accurate prediction of the student's mastery level.