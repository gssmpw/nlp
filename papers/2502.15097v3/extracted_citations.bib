@inproceedings{chen-yang-2023-unlearn,
    title = "Unlearn What You Want to Forget: Efficient Unlearning for {LLM}s",
    author = "Chen, Jiaao  and
      Yang, Diyi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.738",
    doi = "10.18653/v1/2023.emnlp-main.738",
    pages = "12041--12052",
    abstract = "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
}

@misc{eldan2023whosharrypotterapproximate,
      title={Who's Harry Potter? Approximate Unlearning in LLMs}, 
      author={Ronen Eldan and Mark Russinovich},
      year={2023},
      eprint={2310.02238},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02238}, 
}

@misc{maini2024tofu,
      title={TOFU: A Task of Fictitious Unlearning for LLMs}, 
      author={Pratyush Maini and Zhili Feng and Avi Schwarzschild and Zachary C. Lipton and J. Zico Kolter},
      year={2024},
      eprint={2401.06121},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{pawelczyk2023context,
  title={In-Context Unlearning: Language Models as Few Shot Unlearners},
  author={Pawelczyk, Martin and Neel, Seth and Lakkaraju, Himabindu},
  booktitle={ICML},
  year={2024}
}

@misc{shi2024musemachineunlearningsixway,
      title={MUSE: Machine Unlearning Six-Way Evaluation for Language Models}, 
      author={Weijia Shi and Jaechan Lee and Yangsibo Huang and Sadhika Malladi and Jieyu Zhao and Ari Holtzman and Daogao Liu and Luke Zettlemoyer and Noah A. Smith and Chiyuan Zhang},
      year={2024},
      eprint={2407.06460},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.06460}, 
}

@misc{zhang2024negativepreferenceoptimizationcatastrophic,
      title={Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning}, 
      author={Ruiqi Zhang and Licong Lin and Yu Bai and Song Mei},
      year={2024},
      eprint={2404.05868},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.05868}, 
}

