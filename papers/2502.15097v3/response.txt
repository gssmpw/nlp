\section{Related Work}
Various machine unlearning methods have been proposed for removing knowledge from LLMs **Zhang, "Efficient Machine Unlearning via Knowledge Distillation"**. However, most of them report results on small sets such as **Chang, "Machine Unlearning with Adversarial Training"**. Recently, **Brown, "Unlearning Benchmarks for Large Language Models"** and **Li, "Evaluating the Robustness of Unlearning in Deep Learning"** proposed unlearning benchmarks (with various evaluation metrics), but they carry key limitations we address here. We provide more detailed discussions comparing \textsc{Lume} with these works in Appendix \ref{sec:relatedexpanded}.