% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{carlini2019secret,
      title={The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks}, 
      author={Nicholas Carlini and Chang Liu and Úlfar Erlingsson and Jernej Kos and Dawn Song},
      year={2019},
      eprint={1802.08232},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{
speicher2024understanding,
title={Understanding the Mechanics and Dynamics of Memorisation in Large Language Models: A Case Study with Random Strings},
author={Till Speicher and Aflah Mohammad Khan and Qinyuan Wu and Vedant Nanda and Soumi Das and Bishwamittra Ghosh and Krishna P. Gummadi and Evimaria Terzi},
year={2024},
url={https://openreview.net/forum?id=ILStlRb1Sp}
}

@article{Groeneveld2023OLMo,
  title={OLMo: Accelerating the Science of Language Models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  journal={Preprint},
  year={2024}
}

@misc{eldan2023whosharrypotterapproximate,
      title={Who's Harry Potter? Approximate Unlearning in LLMs}, 
      author={Ronen Eldan and Mark Russinovich},
      year={2023},
      eprint={2310.02238},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02238}, 
}

@misc{shi2024musemachineunlearningsixway,
      title={MUSE: Machine Unlearning Six-Way Evaluation for Language Models}, 
      author={Weijia Shi and Jaechan Lee and Yangsibo Huang and Sadhika Malladi and Jieyu Zhao and Ari Holtzman and Daogao Liu and Luke Zettlemoyer and Noah A. Smith and Chiyuan Zhang},
      year={2024},
      eprint={2407.06460},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.06460}, 
}

@misc{maini2024tofu,
      title={TOFU: A Task of Fictitious Unlearning for LLMs}, 
      author={Pratyush Maini and Zhili Feng and Avi Schwarzschild and Zachary C. Lipton and J. Zico Kolter},
      year={2024},
      eprint={2401.06121},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{liu2022continual,
  title={Continual learning and private unlearning},
  author={Liu, Bo and Liu, Qiang and Stone, Peter},
  booktitle={Conference on Lifelong Learning Agents},
  pages={243--254},
  year={2022},
  organization={PMLR}
}

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{dolma,
  title = {{Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}},
  author={
    Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and
    Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and
    Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and
    Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and
    Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and
    Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and
    Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo
  },
  year = {2024},
  journal={arXiv preprint},
}

@misc{ji2024reversingforgetretainobjectivesefficient,
      title={Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference}, 
      author={Jiabao Ji and Yujian Liu and Yang Zhang and Gaowen Liu and Ramana Rao Kompella and Sijia Liu and Shiyu Chang},
      year={2024},
      eprint={2406.08607},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.08607}, 
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}
@inproceedings{duan2024membership,
      title={Do Membership Inference Attacks Work on Large Language Models?}, 
      author={Michael Duan and Anshuman Suri and Niloofar Mireshghallah and Sewon Min and Weijia Shi and Luke Zettlemoyer and Yulia Tsvetkov and Yejin Choi and David Evans and Hannaneh Hajishirzi},
      year={2024},
      booktitle={Conference on Language Modeling (COLM)},
}

@misc{zhang2024negativepreferenceoptimizationcatastrophic,
      title={Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning}, 
      author={Ruiqi Zhang and Licong Lin and Yu Bai and Song Mei},
      year={2024},
      eprint={2404.05868},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.05868}, 
}

@article{liu2024rethinking,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@inproceedings{carliniquantifying,
  title={Quantifying Memorization Across Neural Language Models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{chen-yang-2023-unlearn,
    title = "Unlearn What You Want to Forget: Efficient Unlearning for {LLM}s",
    author = "Chen, Jiaao  and
      Yang, Diyi",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.738",
    doi = "10.18653/v1/2023.emnlp-main.738",
    pages = "12041--12052",
    abstract = "Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.",
}


@inproceedings{pawelczyk2023context,
  title={In-Context Unlearning: Language Models as Few Shot Unlearners},
  author={Pawelczyk, Martin and Neel, Seth and Lakkaraju, Himabindu},
  booktitle={ICML},
  year={2024}
}

@article{chen2024machine,
  title={Machine Unlearning in Large Language Models},
  author={Chen, Kongyang and Wang, Zixin and Mi, Bing and Liu, Waixi and Wang, Shaowei and Ren, Xiaojun and Shen, Jiaxing},
  journal={arXiv preprint arXiv:2404.16841},
  year={2024}
}


@article{hendryckstest2021,
      title={Measuring Massive Multitask Language Understanding},
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      journal={Proceedings of the International Conference on Learning Representations (ICLR)},
      year={2021}
    }

@misc{GDPR,
  key = {General Data Protection Regulation},
  title = {General Data Protection Regulation},
  howpublished = {\url{https://gdpr-info.eu/}},
  note = {Accessed: 2024-03-29},
  year = {2018}
}

@misc{llamalicense,
 title = {Llama 2 Community License Agreement},
 howpublished = {\url{https://ai.meta.com/llama/license/}},
 node = {Accessed: 2024-10-15},
 year = {2023}
}
@misc{GDPR_right,
  title = {Art. 17 GDPRRight to erasure (‘right to be forgotten’)},
  howpublished = {\url{https://gdpr-info.eu/art-17-gdpr/}},
  note = {Accessed: 2024-03-29},
  year = {2018}
}

@misc{SocialSecurityChanging,
 author = {ssa},
 title = {Social Security is Changing the Way SSNs are Issued},
 howpublished = {\url{https://www.ssa.gov/kc/SSAFactSheet--IssuingSSNs.pdf}},
 note = {Accessed: 2024-10-07},
 year = {2011},
}

@misc{zhao2024makesunlearninghard,
      title={What makes unlearning hard and what to do about it}, 
      author={Kairan Zhao and Meghdad Kurmanji and George-Octavian Bărbulescu and Eleni Triantafillou and Peter Triantafillou},
      year={2024},
      eprint={2406.01257},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.01257}, 
}

@misc{neurips-2023-machine-unlearning,
    author = {Eleni Triantafillou and Fabian Pedregosa and Jamie Hayes and Peter Kairouz and Isabelle Guyon and Meghdad Kurmanji and Gintare Karolina Dziugaite and Peter Triantafillou and Kairan Zhao and Lisheng Sun Hosoya and Julio C. S. Jacques Junior and Vincent Dumoulin and Ioannis Mitliagkas and Sergio Escalera and Jun Wan and Sohier Dane and Maggie Demkin and Walter Reade},
    title = {NeurIPS 2023 - Machine Unlearning},
    publisher = {Kaggle},
    year = {2023},
    url = {https://kaggle.com/competitions/neurips-2023-machine-unlearning}
}

@misc{gómezrodríguez2023confederacy,
      title={A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing}, 
      author={Carlos Gómez-Rodríguez and Paul Williams},
      year={2023},
      eprint={2310.08433},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liang2023holistic,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{nytimeslawsuit,
    author    = {Michael M. Grynbaum and Ryan Mac},
    title     = {The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work},
    year      = {2023},
    url       = {https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html},
}

@article{artistslawsuit,
    author    = {Shanti Escalante-De Mattei},
    title     = {Artists Are Suing Artificial Intelligence Companies and the Lawsuit Could Upend Legal Precedents Around Art},
    year      = {2023},
    url       = {https://www.artnews.com/art-in-america/features/midjourney-ai-art-image-generators-lawsuit-1234665579/},
}


@article{zha2023alignscore,
  title={AlignScore: Evaluating factual consistency with a unified alignment function},
  author={Zha, Yuheng and Yang, Yichi and Li, Ruichen and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.16739},
  year={2023}
}

@Article{Ramakrishna2024,
 author = {Anil Ramakrishna and Jimit Majmudar and Rahul Gupta and Devamanyu Hazarika},
 title = {LLM-PIEval: A benchmark for indirect prompt injection attacks in large language models},
 year = {2024},
 url = {https://www.amazon.science/publications/llm-pieval-a-benchmark-for-indirect-prompt-injection-attacks-in-large-language-models},
}

@misc{bu2024unlearningmultitaskoptimizationnormalized,
      title={Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate}, 
      author={Zhiqi Bu and Xiaomeng Jin and Bhanukiran Vinzamuri and Anil Ramakrishna and Kai-Wei Chang and Volkan Cevher and Mingyi Hong},
      year={2024},
      eprint={2410.22086},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.22086}, 
}

@article{YAO2024100211,
title = {A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly},
journal = {High-Confidence Computing},
volume = {4},
number = {2},
pages = {100211},
year = {2024},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
author = {Yifan Yao and Jinhao Duan and Kaidi Xu and Yuanfang Cai and Zhibo Sun and Yue Zhang},
keywords = {Large Language Model (LLM), LLM security, LLM privacy, ChatGPT, LLM attacks, LLM vulnerabilities}
}

@article{wei2024jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ramakrishna-etal-2023-invite,
    title = "{INVITE}: a Testbed of Automatically Generated Invalid Questions to Evaluate Large Language Models for Hallucinations",
    author = "Ramakrishna, Anil  and
      Gupta, Rahul  and
      Lehmann, Jens  and
      Ziyadi, Morteza",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.360/",
    doi = "10.18653/v1/2023.findings-emnlp.360",
    pages = "5422--5429",
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@misc{zhu2023autodan,
      title={AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models}, 
      author={Sicheng Zhu and Ruiyi Zhang and Bang An and Gang Wu and Joe Barrow and Zichao Wang and Furong Huang and Ani Nenkova and Tong Sun},
      year={2023},
      eprint={2310.15140},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
