% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype,paralist}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{ucs}

\usepackage{booktabs}

\usepackage{arydshln}

\usepackage{multirow}

\usepackage[colorinlistoftodos]{todonotes}
\newcommand{\SideNote}[2]{\todo[color=#1,size=\small]{#2}} 
\newcommand{\kw}[1]{\SideNote{yellow}{#1 -KW}}
%\newcommand{\kw}[1]{}


\definecolor{Red}{rgb}{0.7,0.01,0.01}
\definecolor{Purple}{rgb}{.627,.124,.941}
%\newcommand{\kw}[1]{\textcolor{Red}{KW: #1}}
\newcommand{\vc}[1]{\textcolor{Purple}{VC: #1}}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{subcaption}

\usepackage[most]{tcolorbox}
\tcbuselibrary{listings}
\lstset{
    basicstyle=\ttfamily\scriptsize,
    % columns=fullflexible,
    frame=single,
    breaklines=true,
    showstringspaces=false,
    escapechar={|}
}

\title{LUME: LLM Unlearning with Multitask Evaluations}


\author{Anil Ramakrishna\textsuperscript{1}, Yixin Wan\textsuperscript{2}, Xiaomeng Jin\textsuperscript{3}, Kai-Wei Chang\textsuperscript{1,2}, Zhiqi Bu\textsuperscript{1}, \\ \textbf{Bhanukiran Vinzamuri}\textsuperscript{1}, \textbf{Volkan Cevher}\textsuperscript{1,4}, \textbf{Mingyi Hong}\textsuperscript{1,5}, \textbf{Rahul Gupta}\textsuperscript{1} \\
\textsuperscript{1}Amazon AGI, \textsuperscript{2}UCLA, \textsuperscript{3}UIUC, \textsuperscript{4}EPFL, \textsuperscript{5}University of Minnesota \\
\texttt{aniramak@amazon.com}
}



\begin{document}

\maketitle

\begin{abstract}
{Unlearning} aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (\textsc{Lume}) which features three tasks: (1) unlearn synthetically generated creative short novels,  (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We further release two fine-tuned LLMs of 1B and 7B parameter sizes as the target models. We conduct detailed evaluations of several recently-proposed unlearning algorithms and present results on carefully crafted metrics to understand their behavior and limitations. 

\end{abstract}

\section{Introduction}
\label{sec:intro}
Given government regulations, such as the European Unionâ€™s GDPR \textit{right to be forgotten} \cite{GDPR_right}, legal actions from original content creators \cite{nytimeslawsuit, artistslawsuit}, and a need to remove misinformation or toxic content from LLMs, there is an increasing demand for effective unlearning algorithms as retraining model from scratch is infeasible. We define effective unlearning algorithm as one which: ($i$) effectively removes information to be unlearned, ($ii$) uses computation commensurate with the size of the data to be forgotten, and ($iii$) retains model's overall performance after unlearning so that it is similar to a model candidate trained without the data to be forgotten. 

To evaluate the performance of unlearning algorithms in LLMs, there is a need for comprehensive benchmarks. 
While recent work, such as TOFU \cite{maini2024tofu} and MUSE  \cite{shi2024musemachineunlearningsixway}, provide promising first steps along this direction, they provide limited coverage focusing on synthetic question answers, and news/books respectively.  
Further, neither benchmarks cover Personally Identifiable Information (PII) information which is an important use case for unlearning in LLMs.

In this work, we develop a comprehensive new benchmark named \textsc{Lume} (LLM Unlearning with Multitask Evaluations) for unlearning creative, sensitive, and private content from LLMs. Our benchmark\footnote{created as part of the LLM Unlearning shared task at SemEval 2025} features three distinct tasks: synthetically generated creative short novels (\textit{task \#1}), synthetic biographies with PII (\textit{task \#2}), and public biographies (\textit{task \#3}) for an extensive assessment of unlearning algorithms. \textsc{Lume} tests for unlearning of both full documents and QA pairs for each task, with unlearning effectiveness measured using memorization, privacy leakage (via membership inference attack) and model utility tests. We evaluate several unlearning algorithms including current state of the art, and find that they do not yet effectively unlearn sensitive information without significantly degrading the model utility. %, thereby overcoming the key limitations described earlier. 
Our benchmark is publicly available\footnote{github.com/amazon-science/lume-llm-unlearning}; we also release two fine-tuned model checkpoints (1 billion\footnote{huggingface.co/llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning} and 7 billion\footnote{huggingface.co/llmunlearningsemeval2025organization/olmo-finetuned-semeval25-unlearning} parameters in size). 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{lume_examples.png}
    \caption{Examples of full documents and test prompts for the three tasks covered in LUME.}
    \label{fig:lume_approach}
\end{figure*}

\section{\textsc{Lume}: A Multitask Unlearning Benchmark for LLMs}
\label{sec:benchmark}


Given an LLM fine-tuned on a text corpus $D$,  our unlearning goal is to effectively remove information from a subset $F \subset D$ (i.e., the forget set) with computational effort proportional to its size. During unlearning, we only have access to $F$ and another subset $R \subset D$ (i.e., the retain set) to ensure performance outside $F$ is preserved.


\subsection{Benchmark Construction}
\label{sec:data_generator_llms}

We developed three distinct tasks to provide a comprehensive evaluation of LLM unlearning algorithms spanning creative documents, PII and biographies.
Figure \ref{fig:lume_approach} and Table \ref{tab:statistics} show example data and statistics of \textsc{Lume}, respectively. 

\vspace{2mm}
\noindent\textbf{Task 1 (Synthetic creative documents):} 
LLMs trained on Internet-scraped data are often exposed to copyrighted content, making unlearning a common requirement. 
However, evaluating effectiveness of unlearning on only real creative documents \cite{shi2024musemachineunlearningsixway,eldan2023whosharrypotterapproximate} is challenging as information to be removed may appear in other documents not being unlearned. For example, MUSE uses \textit{Harry Potter} books as its forget set, but similar content may appear in Wikipedia and social media. Motivated by this, in this task, we only include synthetically generated short novels, created using Mixtral 8x7B  \cite{jiang2023mistral}\footnote{{\textit{mistral.mixtral-8x7b-instruct-v0:1}} on Amazon Bedrock.} as our generator LLM.

For each document, we randomly select a genre from \textit{Action}, \textit{Fantasy}, \textit{Thriller}, \textit{Comedy}, \textit{Mystery}, \textit{Science Fiction}, \textit{Young Adult} and \textit{Romance}. One to four unique character names are generated using a random name generator (\url{pypi.org/project/unique-names-generator}), and locations are generated from the city list of a random address generator (\url{pypi.org/project/random-address}) for all genres except \textit{Fantasy}. For \textit{Fantasy}, we sample unique fantasy city names using a \textit{Dungeons and Dragons} town generator (\url{perchance.org/dndtowngen}). Given this information, we prompt the Mixtral model to create a short story with 150-200 words. 
To validate the generated stories, we conducted manual reviews where each short story was reviewed by two different authors of this work, and filtered out stories with similar content to prior reviewed stories. Our final dataset contains 393 unique short stories. 

\vspace{2mm}
\noindent\textbf{Task 2  (Synthetic biographies with sensitive PII):} 

We use rule based heuristics to generate personal biographies with following PII fields: a randomly generated name, a birthday randomly sampled between 01/01/1964 and 01/01/1991, a fake Social Security number (SSN) within the range 900-xx-xxxx (which can never belong to a real person \cite{SocialSecurityChanging}), a random phone number, an email address of the form \texttt{firstname\_lastname@me.com} and a non-existent physical home addresses obtained by combining a random street address from a US state with an alternate city and zip-code from a different state. For each synthetic individual, we prompt the Mixtral model to create a short biography by including the fictitious PII information. 

\vspace{2mm}
\noindent\textbf{Task 3 (Real biographies):}
To evaluate effectiveness of unlearning on real data, we include real biographies as the third task. Specifically, we sampled biographies spanning 100 to 200 words from Wikipedia documents released in the Dolma \cite{dolma} v1.6 corpus, which was part of the training dataset for the OLMo models~\cite{Groeneveld2023OLMo} we fine-tuned for this task. 

\begin{table}
\centering
\begin{tabular}{c:cc:c}
 & Forget & Retain &    \\ \midrule
Task 1 & 199    & 194    & 393 \\
Task 2 & 203    & 202    & 405 \\
Task 3 & 295    & 294    & 589 \\ \hdashline
       & 697    & 690    & 1,387  \\ 
\end{tabular}
\caption{Number of unique documents for both data subsets within each task. For each document, we create multiple regurgitation and knowledge datasets leading to 4,394 unique examples. }%\kw{can we also show number of tokens?}}
\label{tab:statistics}
\end{table}



\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{GA_task1.pdf}}\end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{GD_task1.pdf}}\end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{KL_task1.pdf}}\end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{NPO_task1.pdf}}\end{subfigure}

     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{GA_task2.pdf}}\end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{GD_task2.pdf}}\end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{KL_task2.pdf}}\end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{NPO_task2.pdf}}\end{subfigure}

     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{GA_task3.pdf}}
     \caption{GA}\end{subfigure}
    \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{GD_task3.pdf}}
     \caption{GD}\end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{KL_task3.pdf}}
     \caption{KL}\end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}{\includegraphics[scale=0.25]{NPO_task3.pdf}}
     \caption{NPO}\end{subfigure}
    \caption{Performance on \textit{retain} and \textit{forget} subsets for benchmarked unlearning algorithms for Tasks 1 to 3 (respectively from top to bottom). Reg: Regurgitation Rate ($r$), Kno: Knowledge Accuracy ($t$). Split refers to data subset (forget or retain) used in evaluations.}
    \label{fig:results}
\end{figure*}

\subsection{Unlearning Model Candidates}

We fine-tuned 1B (\texttt{OLMo-1B-0724-hf}) and 7B (\texttt{OLMo-7B-0724-Instruct-hf}) OLMo models~\cite{Groeneveld2023OLMo}
on all three tasks and release them as unlearning candidates. We selected OLMo because of its permissive license and open sourced training dataset (with logs) which enables downstream task specific analyses of model behavior. 

\subsection{Evaluation}
We use following metrics for detailed evaluation.

\noindent\textbf{Regurgitation Rate ($r$):} We create a \textit{sentence completion} prompt for each document by sampling a random position in second half of the document with the sentences before it as the input. We compute ROUGE-L \cite{lin-2004-rouge} scores for the model generated outputs with respect to the expected sentence completions. 

\noindent\textbf{Knowledge Test Accuracy ($t$):} We create a \textit{question answering} prompt for each document using an agentic workflow for Tasks 1 and 3 where we prompt the data generator LLM (see Appendix \ref{sec:question_generation_prompt}) with few-shot Chain of Thought prompting \cite{wei2022chain} and construct an unambiguous question with a single concise answer. We verify the quality of QA pairs using three verification LLMs.\footnote{We use Claude 3 ({\textit{anthropic.claude-3-sonnet-20240229-v1:0}}), Titan Text Express ({\textit{amazon.titan-text-express-v1}}) and Mixtral 8x7B for verification} 
We discard QA samples if any of the verification LLMs are unable to answer the question accurately with the corresponding document.
For Task 2, we use template based heuristics to frame 5 distinct questions corresponding to the PII fields, of the form: \textit{What is the birth date of John Smith?}. For all QA prompts, we use case insensitive exact match between model output and the groundtruth to measure prediction accuracy.

\noindent\textbf{Membership Inference Attacks (MIA) ($m$):} 
We use the black-box MIA attack framework from ~\cite{duan2024membership} to implement Loss based attacks to assess data leakage risk after unlearning. 
We use a subset of the memorized forget set of biographies from Task 3 as the member set and a disjoint sample of similar biographies not exposed to the model as the non-member set.  


\noindent\textbf{Model Utility ($u$):} We also test for overall model utility on MMLU \cite{hendryckstest2021}, a general benchmark for LLM utility.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}{\includegraphics[scale=0.35]{mia.pdf}}\label{fig:mia}\end{subfigure}
     \caption{MIA rates ($m$) per epoch.}
    \label{fig:mia}
\end{figure}

\vspace{-2mm}
\section{Experiments}
We benchmark several unlearning approaches on \textsc{Lume} and discuss our observations.
\vspace{-1mm}
\paragraph{Baseline Unlearning Algorithms:}

We test following popular unlearning algorithms on \textsc{Lume} (detailed review is in the Appendix).
\begin{compactitem}
\item \textbf{Gradient Ascent (GA)} reverses the gradient direction on the forget set $F$ to steer the model away this information. 

\item \noindent \textbf{Gradient Difference (GD)} \cite{liu2022continual} augments the gradient ascent objective applied on $F$ with a gradient descent objective on $R$.

\item \noindent \textbf{KL Regularization (KL)} \cite{maini2024tofu} augments the gradient ascent objective with a regularization term which minimizes the KL divergence with respect to the original model. 

\item \textbf{Negative Preference Optimization (NPO)} \cite{zhang2024negativepreferenceoptimizationcatastrophic} uses a modified version of Direct Preference Optimization, adapted to remove the sensitive information from $F$.

\end{compactitem}

 
Similar to TOFU and MUSE, we run each algorithm for 10 epochs with learning rate of $1e-5$ and batch size of $32$. 

\paragraph{Results:}
Figure \ref{fig:results} highlights epoch wise performance of each unlearning algorithm on forget and retain subsets.\footnote{due to space limitations, we present results only on the 7B model here.}
Across all tasks and on both forget/retain sets, at epoch 0 all metrics reveal perfect regurgitation, highlighting complete memorization by the fine-tuned models (without a drop in model utility as shown in Figure \ref{fig:mmlu} where the performance starts with baseline MMLU levels for OLMo 7B). 

As evidenced by the rapid drop in both regurgitation and knowledge scores as unlearning proceeds, none of the algorithms were successful in achieving the joint objectives of unlearning the forget set while retaining information from the retain set. Except NPO, all the approaches reach zero on both metrics across all three tasks, suggesting substantial degradation in model quality. NPO performs relatively better but also trends towards zero. The observed variance in unlearning performance for the three tasks suggests varying levels of unlearning difficulty for the samples from each task which was recently observed in \cite{zhao2024makesunlearninghard}. 

For GD, while performance drops rapidly on both forget and retain sets, performance on the retain set starts increasing with time. This is because of the objective used in GD which reduces the prediction loss on the retain set while jointly increasing loss on the forget set. As training proceeds, the impact of the gradient descent objective which increases memorization of the retain set. 

\paragraph{Privacy Leakage:}
Figure \ref{fig:mia} highlights the MIA success rates (AUC) for the unlearned checkpoints after each epoch. Initially, all models start with perfect memorization and hence have 100\% attack success rates, but as unlearning proceeds, GA, GD and KL drop to the desired attack success rate of 50\% (i.e. random chance levels), with GA observed to have the fastest drop. However, NPO attack success rates remain high after 10 epochs, suggesting that this approach does not truly remove the unlearned information and is vulnerable to privacy leakage from such attacks post unlearning. On the other hand, the MIA rates for GD continue dropping below 0.5, suggesting over-unlearning beyond epoch 7.   

\paragraph{Impact on Utility:}

\begin{figure}[t]
    \centering
     \begin{subfigure}[b]{0.35\textwidth}{\includegraphics[scale=0.35]{mmlu.pdf}}
     \label{fig:mmlu}\end{subfigure}
     \caption{MMLU rates ($u$) per epoch.}
    \label{fig:mmlu}
\end{figure}

We report aggregate scores among all 57 tasks of MMLU in Figure \ref{fig:mmlu}. We observe considerable performance drops in all approaches, highlighting the challenge in unlearning sensitive information without impacting model utility. GA had the highest drop suggesting substantial model degradation (owing to its unbounded loss term), followed by KL, GD and NPO. 

\section{Related Work}
Various machine unlearning methods have been proposed for removing knowledge from LLMs~\cite{zhang2024negativepreferenceoptimizationcatastrophic,pawelczyk2023context,chen-yang-2023-unlearn}. However, most of them report results on small sets such as ~\cite{eldan2023whosharrypotterapproximate}. Recently, \cite{maini2024tofu} and \cite{shi2024musemachineunlearningsixway} proposed unlearning benchmarks (with various evaluation metrics), but they carry key limitations we address here. We provide more detailed discussions comparing \textsc{Lume} with these works in Appendix \ref{sec:relatedexpanded}.

\section{Conclusion}
We propose \textsc{Lume}, a new benchmark covering three distinct tasks to evaluate unlearning in LLMs. 
Detailed experiments reveal the challenge presented by our benchmark since most algorithms fail to sufficiently unlearn the forget set without substantial degradations on the retain set and model utility. We hope our benchmark spurs further developments in LLM unlearning research.


\section*{Limitations and Future Work}
\cite{carliniquantifying} show that the risk of memorization increases with large model size. However, due to computational limitations and easy availability of large public LLMs, we only provide finetuned checkpoints for 1B and 7B OLMo, and defer release of larger models to future work. Moreover, licensing restrictions prevent us from releasing fine-tuned models based on few publicly available LLMs such as LLaMa \cite{llamalicense}. 

We acknowledge that LLM-generated data can exhibit specific biases found in their training data set. We partially mitigate this by seeding the generation prompt with pre-sampled character and location names to ensure diversity in generated content. We also conducted manual evaluations of the generated creative content to ensure its quality.
 

\section*{Ethical Considerations}
Task 2 deals with sensitive PII information which warrants careful considerations to avoid privacy leakage of individuals. We avoid this risk entirely by carefully designing the generation process so that it closely mimics real individuals, despite being generated synthetically. We also ensure all the tools used in generating our benchmark data are open sourced, thereby avoiding any licensing restrictions. 


\bibliography{custom}

\appendix 


\section{Expanded related work}
\label{sec:relatedexpanded}
Given the nascent stage of unlearning research in LLMs, few prior works exist which address the task of robustly evaluating the success of unlearning. \cite{neurips-2023-machine-unlearning} presented a new challenge task in which the goal was to to unlearn information contained in select images within the task of image based age prediction. 
While successful, the specific task addressed in this challenge was narrow, focusing only on image based age prediction - a classification problem with 10 classes with limited applicability in the unbounded text generation task of large language models. But the growing interest in LLMs and their tendency to generate unsafe \cite{wei2024jailbroken}, private \cite{YAO2024100211} or security violating \cite{Ramakrishna2024} content necessitates a distinct and focused evaluation benchmark for unlearning.

\cite{maini2024tofu} released a new evaluation framework named TOFU which partially addressed this task of evaluating LLM unlearning algorithms. Their framework was evaluated on question answering task applied on biographies of synthetically created fake authors. They train target models on this synthetic data and evaluate the ability of unlearning algorithms to forget a portion of this synthetic dataset. While being a promising first step, this work has a few key limitations: unlearning the targeted information required for the QA task does is unlikely to cause loss of any other substantial information, specially linguistic attributes such as grammar. Further, this work leverages GPT4 to generate the synthetic content, which may have downstream licensing implications owing to GPT4's proprietary license. 

More recently, \cite{shi2024musemachineunlearningsixway} released a benchmark named MUSE which evaluated model unlearning using real data set for containing news documents and Harry Potter book chapters. This benchmark released detailed evaluation metrics to robustly evaluate the unlearning algorithms. However since it only leverages real data set the benchmark does not provide a clean test bed to evaluate model performance. Specifically, the information contained in the unlearn documents may also appear in other disjoint training documents, limiting the effectiveness of unlearning. While the TOFU benchmark mentioned before avoids this by only using synthetic documents, the data set coverage is rather limited (it only containts biographic information). The benchmark developed in our work addresses both these shortcomings together and presents a single holistic testbed to evaluate model unlearning in LLMs. Further, our benchmark was created using a largely automated process, and supports reinstantiation for new applications similar to \cite{ramakrishna-etal-2023-invite}. 


\section{Long Form Example Generation}
\label{sec:long_form_section}
\begin{minipage}{\linewidth}
\begin{lstlisting}
Model Input:
Create a short novel of at least 150 words. The novel should be from fantasy genre and set in the city of Atlantis. It should include following characters: Kyle, Stan, Kenny and Eric.
    
Model Output: 
A short story about four friends lost in the magical realm below the oceans, known to ousiders as Atlantis. Kyle had always held a deep fascination for the deep blue ocean, and this naturally led him to take up a major in oceanic studies...
\end{lstlisting}
\end{minipage}

\section{Short Form Example Generation}
\label{sec:short_form_section}
\begin{minipage}{\linewidth}
\begin{lstlisting}
Model Input:
Create a biography for Jon Smith with date of birth: 1/2/1989, SSN: 900123456, phone number: 0987654321, email: jon_smith@me.com, home address: 10 Summertime Lane, New York City, NY, USA. 
    
Model Output: 
Jon Smith was both in New York City on the first of February in 1989,...
\end{lstlisting}
\end{minipage}

\section{Question Generation Prompt}
\label{sec:question_generation_prompt}
\begin{minipage}{\linewidth}
\begin{lstlisting}
Model Input:
You are given a short story. First, find all the proper nouns in this story. If it does not contain a proper noun, say "I can't use this statement since it does not contain any proper nouns.". If it contains proper nouns, use your reasoning to create an unambiguous question, for which there would be *only* one answer. Give a concise answer (i.e. one word or phrase) which accurately answers the question. If you cannot create such an unambiguous question, say "I'm unable to create an unambiguous question for this story". Use the examples below for reference.

Examples:
1. Example #1
2. Example #2
3. Example #3
4. Example #4
5. Example #5

Here's the story: <input_story>. Generate a question with an unambiguous answer using this story. 
\end{lstlisting}
\end{minipage}

\section{Further details on Unlearning Algorithms}
\label{sec:unlearning-method}
We review unlearning methods tested in this paper in the following. 
\begin{itemize}

\item \textbf{Gradient Ascent}: This is a straightforward algorithm for model unlearning where we reverse the direction of model update by flipping the sign in gradient descent, in order to steer the model away from the sensitive model outputs in the forget set. While easy to implement, this approach has a significant drawback since the gradient ascent training objective is unbounded, which can lead to model divergence with nonsensical outputs for all inputs. The loss term in this algorithm reverses sign of the standard training objective and is applied only on the forget set $F$ as shown below.
\begin{align*}
     -\mathcal{L}(F; \theta)
\end{align*}

\item \noindent \textbf{Gradient Difference} \cite{liu2022continual}: In this approach, we augment the gradient ascent objective applied on forget set, by adding a gradient descent objective on the retain set. By jointly optimizing on both sets, we steer the model away from regurgitating the sensitive information from the retain set, while ensuring it does not lose performance in the retain set. Despite being a promising alternative to Gradient Ascent, this quality of model performance on non-sensitive dataset depends on the size of the retain set used in model training, and can lead to poor generalization on new examples. The loss term jointly increases the likelihood of generating responses in the retain set $R$ while reducing the likelihood of generating $F$, as shown below.
\begin{align*}
     -\mathcal{L}(F; \theta) + \mathcal{L}(R; \theta)
\end{align*}

\item \noindent \textbf{KL Divergence} \cite{maini2024tofu} Similar to Gradient Difference, in this baseline, we augment the gradient ascent objective with a Kullback-Leibler Divergence term to ensure the model does not deviate too far from the original model. 

\item \textbf{Normalized Gradient Difference} \cite{bu2024unlearningmultitaskoptimizationnormalized}: In this baseline, we frame the gradient difference objective as a multi-task optimization problem where the gradient ascent loss term is bounded by normalizing in each training step, along with an automatic lr scheduler to balance the two objectives. 
      
\item \textbf{Negative Preference Optimization} \cite{zhang2024negativepreferenceoptimizationcatastrophic}: This baseline uses a modified version of the Direct Preference Optimization objective, adapted to remove the sensitive information from the forget set.

\end{itemize}

\end{document}