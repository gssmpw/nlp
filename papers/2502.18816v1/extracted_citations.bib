@inproceedings{abnar2020quantifying,
	title={Quantifying Attention Flow in Transformers},
	author={Abnar, Samira and Zuidema, Willem},
	booktitle={ACL},
	pages={4190--4197},
	year={2020}
}

@inproceedings{antol2015vqa,
	title={Vqa: Visual question answering},
	author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
	booktitle={ICCV},
	year={2015}
}
%	pages={2425--2433}

@article{bach2015pixel,
	title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
	author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
	journal={PloS one},
	volume={10(7):e0130140},
	year={2015},
}
%	pages={e0130140},
%	number={7},
%	publisher={Public Library of Science}

@inproceedings{chattopadhay2018grad,
	title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
	author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
	booktitle={WACV},
	pages={839--847},
	year={2018}
}

@inproceedings{chefer2021generic,
	title={Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers},
	author={Chefer, Hila and Gur, Shir and Wolf, Lior},
	booktitle={ICCV},
	pages={397--406},
	year={2021}
}

@inproceedings{chefer2021transformer,
	title={Transformer interpretability beyond attention visualization},
	author={Chefer, Hila and Gur, Shir and Wolf, Lior},
	booktitle={CVPR},
	pages={782--791},
	year={2021}
}

@inproceedings{chen2020uniter,
	title={Uniter: Universal image-text representation learning},
	author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	booktitle={ECCV},
	year={2020},
}
%	pages={104--120},
%	organization={Springer}

@article{chen2022prompt,
	title={Prompt learning with optimal transport for vision-language models},
	author={Chen, Guangyi and Yao, Weiran and Song, Xiangchen and Li, Xinyue and Rao, Yongming and Zhang, Kun},
	journal={arXiv:2210.01253},
	year={2022}
}

@inproceedings{du2022learning,
	title={Learning to prompt for open-vocabulary object detection with vision-language model},
	author={Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
	booktitle={CVPR},
	pages={14084--14093},
	year={2022}
}

@inproceedings{fong2017interpretable,
	title={Interpretable explanations of black boxes by meaningful perturbation},
	author={Fong, Ruth C and Vedaldi, Andrea},
	booktitle={ICCV},
	pages={3429--3437},
	year={2017}
}

@inproceedings{gu2019understanding,
	title={Understanding individual decisions of cnns via contrastive backpropagation},
	author={Gu, Jindong and Yang, Yinchong and Tresp, Volker},
	booktitle={ACCV},
	pages={119--134},
	year={2019}
}

@article{gu2021open,
	title={Open-vocabulary object detection via vision and language knowledge distillation},
	author={Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	journal={ICLR},
	year={2022}
}

@inproceedings{iwana2019explaining,
	title={Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation},
	author={Iwana, Brian Kenji and Kuroki, Ryohei and Uchida, Seiichi},
	booktitle={ICCVW},
	year={2019}
}
%	pages={4176--4185},
%	organization={IEEE}

@inproceedings{kim2023region,
	title={Region-aware pretraining for open-vocabulary object detection with vision transformers},
	author={Kim, Dahun and Angelova, Anelia and Kuo, Weicheng},
	booktitle={CVPR},
	year={2023}
}
%	pages={11144--11154}

@article{krishna2017visual,
	title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
	author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
	journal={IJCV},
	volume={123},
	pages={32--73},
	year={2017},
	publisher={Springer}
}

@inproceedings{kuo2023fvlm,
	title={F-vlm: Open-vocabulary object detection upon frozen vision and language models},
	author={Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia},
	booktitle={ICLR},
	year={2023}
}

@inproceedings{lee2021bbam,
	title={Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation},
	author={Lee, Jungbeom and Yi, Jihun and Shin, Chaehun and Yoon, Sungroh},
	booktitle={CVPR},
	year={2021}
}
%	pages={2643--2652}

@inproceedings{li2020oscar,
	title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
	author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
	booktitle={ECCV},
	year={2020},
}
%	pages={121--137},
%	organization={Springer}

@inproceedings{li2022blip,
	title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
	author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	booktitle={ICML},
	pages={12888--12900},
	year={2022}
}

@article{li2022exploring,
	title={Exploring visual interpretability for contrastive language-image pre-training},
	author={Li, Yi and Wang, Hualiang and Duan, Yiqun and Xu, Hang and Li, Xiaomeng},
	journal={arXiv:2209.07046},
	year={2022}
}

@inproceedings{li2022grounded,
	title={Grounded language-image pre-training},
	author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
	booktitle={CVPR},
	pages={10965--10975},
	year={2022}
}

@article{li2023clipsurgery,
	title={Clip surgery for better explainability with enhancement in open-vocabulary tasks},
	author={Li, Yi and Wang, Hualiang and Duan, Yiqun and Li, Xiaomeng},
	journal={arXiv:2304.05653},
	year={2023}
}

@inproceedings{liang2023open,
	title={Open-vocabulary semantic segmentation with mask-adapted clip},
	author={Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
	booktitle={CVPR},
	pages={7061--7070},
	year={2023}
}

@article{liu2023grounding,
	title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
	author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
	journal={arXiv preprint arXiv:2303.05499},
	year={2023}
}

@article{lundberg2017unified,
	title={A unified approach to interpreting model predictions},
	author={Lundberg, Scott M and Lee, Su-In},
	journal={NeurIPS},
	volume={30},
	year={2017}
}

@article{montavon2017explaining,
	title={Explaining nonlinear classification decisions with deep taylor decomposition},
	author={Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
	journal={Pattern recognition},
	volume={65},
	pages={211--222},
	year={2017},
	publisher={Elsevier}
}

@inproceedings{nam2020relative,
	title={Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks},
	author={Nam, Woo-Jeoung and Gur, Shir and Choi, Jaesik and Wolf, Lior and Lee, Seong-Whan},
	booktitle={AAAI},
	year={2020}
}
%	booktitle={Proceedings of the AAAI conference on artificial intelligence},
%	volume={34},
%	number={03},
%	pages={2501--2508}

@article{petsiuk2018rise,
	title={Rise: Randomized input sampling for explanation of black-box models},
	author={Petsiuk, Vitali and Das, Abir and Saenko, Kate},
	journal={arXiv:1806.07421},
	year={2018}
}
	%journal={arXiv preprint arXiv:1806.07421}

@inproceedings{petsiuk2021black,
	title={Black-box explanation of object detectors via saliency maps},
	author={Petsiuk, Vitali and Jain, Rajiv and Manjunatha, Varun and Morariu, Vlad I and Mehra, Ashutosh and Ordonez, Vicente and Saenko, Kate},
	booktitle={CVPR},
	pages={11443--11452},
	year={2021}
}

@inproceedings{plummer2015flickr30k,
	title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
	author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
	booktitle={ICCV},
	year={2015}
}
%	pages={2641--2649}

@article{qiang2022attcat,
	title={Attcat: Explaining transformers via attentive class activation tokens},
	author={Qiang, Yao and Pan, Deng and Li, Chengyin and Li, Xin and Jang, Rhongho and Zhu, Dongxiao},
	journal={NeurIPS},
	year={2022}
}
%	volume={35},
%	pages={5052--5064}

@inproceedings{ramaswamy2020ablation,
	title={Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization},
	author={Ramaswamy, Harish Guruprasad and others},
	booktitle={WACV},
	year={2020}
}
%	pages={983--991}

@article{ren2015faster,
	title={Faster r-cnn: Towards real-time object detection with region proposal networks},
	author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	journal={NeurIPS},
	volume={28},
	year={2015}
}

@inproceedings{ribeiro2016should,
	title={" Why should i trust you?" Explaining the predictions of any classifier},
	author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	booktitle={ACM SIGKDD},
	year={2016}
}
%	pages={1135--1144}

@inproceedings{selvaraju2017grad,
	title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
	author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	booktitle={ICCV},
	pages={618--626},
	year={2017}
}

@inproceedings{shrikumar2017learning,
	title={Learning important features through propagating activation differences},
	author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	booktitle={ICML},
	year={2017}
}
%	pages={3145--3153}

@inproceedings{wagner2019interpretable,
	title={Interpretable and fine-grained visual explanations for convolutional neural networks},
	author={Wagner, Jorg and Kohler, Jan Mathias and Gindele, Tobias and Hetzel, Leon and Wiedemer, Jakob Thaddaus and Behnke, Sven},
	booktitle={CVPR},
	pages={9097--9107},
	year={2019}
}

@inproceedings{wang2019camp,
	title={Camp: Cross-modal adaptive message passing for text-image retrieval},
	author={Wang, Zihao and Liu, Xihui and Li, Hongsheng and Sheng, Lu and Yan, Junjie and Wang, Xiaogang and Shao, Jing},
	booktitle={ICCV},
	year={2019}
}
%	pages={5764--5773}

@inproceedings{wang2020score,
	title={Score-CAM: Score-weighted visual explanations for convolutional neural networks},
	author={Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
	booktitle={CVPR Workshops},
	pages={24--25},
	year={2020}
}

@article{wang2020ss,
	title={SS-CAM: Smoothed Score-CAM for sharper visual feature localization},
	author={Wang, Haofan and Naidu, Rakshit and Michael, Joy and Kundu, Soumya Snigdha},
	journal={arXiv:2006.14255},
	year={2020}
}

@inproceedings{wang2023position,
	title={Position-guided Text Prompt for Vision-Language Pre-training},
	author={Wang, Jinpeng and Zhou, Pan and Shou, Mike Zheng and Yan, Shuicheng},
	booktitle={CVPR},
	pages={23242--23251},
	year={2023}
}

@article{wang2024visual,
	title={Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution},
	author={Wang, Ying and Rudner, Tim GJ and Wilson, Andrew G},
	journal={NeurIPS},
	year={2024}
}
%	volume={36}

@article{wu2023clipself,
	title={Clipself: Vision transformer distills itself for open-vocabulary dense prediction},
	author={Wu, Size and Zhang, Wenwei and Xu, Lumin and Jin, Sheng and Li, Xiangtai and Liu, Wentao and Loy, Chen Change},
	journal={arXiv preprint arXiv:2310.01403},
	year={2023}
}

@inproceedings{wu2023cora,
	title={Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching},
	author={Wu, Xiaoshi and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
	booktitle={CVPR},
	pages={7031--7040},
	year={2023}
}

@article{xie2022vit,
	title={Vit-cx: Causal explanation of vision transformers},
	author={Xie, Weiyan and Li, Xiao-Hui and Cao, Caleb Chen and Zhang, Nevin L},
	journal={arXiv preprint arXiv:2211.03064},
	year={2022}
}

@inproceedings{xu2015show,
	title={Show, attend and tell: Neural image caption generation with visual attention},
	author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
	booktitle={ICML},
	pages={2048--2057},
	year={2015}
}

@article{xu2021simple,
	title={A simple baseline for zeroshot semantic segmentation with pre-trained vision-language model},
	author={Xu, Mengde and Zhang, Zheng and Wei, Fangyun and Lin, Yutong and Cao, Yue and Hu, Han and Bai, Xiang},
	journal={ECCV},
	year={2022}
}

@article{yu2022coca,
	title={Coca: Contrastive captioners are image-text foundation models},
	author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	journal={arXiv:2205.01917},
	year={2022}
}

@inproceedings{yu2023x,
	title={X-pruner: explainable pruning for vision transformers},
	author={Yu, Lu and Xiang, Wei},
	booktitle={CVPR},
	pages={24355--24363},
	year={2023}
}

@article{yu2024fcclip,
	title={Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip},
	author={Yu, Qihang and He, Ju and Deng, Xueqing and Shen, Xiaohui and Chen, Liang-Chieh},
	journal={NeurIPS},
	volume={36},
	year={2024}
}

@inproceedings{zeiler2014visualizing,
	title={Visualizing and understanding convolutional networks},
	author={Zeiler, Matthew D and Fergus, Rob},
	booktitle={ECCV},
	pages={818--833},
	year={2014}
}

@inproceedings{zhang2021vinvl,
	title={Vinvl: Revisiting visual representations in vision-language models},
	author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
	booktitle={CVPR},
	pages={5579--5588},
	year={2021}
}

@inproceedings{zhong2022regionclip,
	title={Regionclip: Region-based language-image pretraining},
	author={Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others},
	booktitle={CVPR},
	pages={16793--16803},
	year={2022}
}

@inproceedings{zhou2022extract,
	title={Extract free dense labels from clip},
	author={Zhou, Chong and Loy, Chen Change and Dai, Bo},
	booktitle={ECCV},
	pages={696--712},
	year={2022},
	organization={Springer}
}

@article{zhou2022learning,
	title={Learning to prompt for vision-language models},
	author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	journal={IJCV},
	volume={130},
	number={9},
	pages={2337--2348},
	year={2022},
	publisher={Springer}
}
%	journal={International Journal of Computer Vision}

