\section{Related Works}
\label{sec:related}
\vspace{-0.1cm}

\zcy{We first briefly review the 
%contrastive language-image pre-training (
CLIP
 model, and then discus different types of visual explanation methods in computer vision. Finally, the related works for fine-grained image annotation and understanding in CLIP are introduced.}


\vspace{-0.1cm}
\subsection{Contrastive language-image pre-training}
%\vspace{-0.1cm}

Many multi-modal works have been developed and focus on the interaction of computer vision and natural language processing, such as text-image retrieval **Lin et al., "Object-Query Adversarial Learning for VQA"** ____, image captioning **Anderson et al., "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"** ____, visual question answering **Antol et al., "VQA: Visual Question Answering"** ____, and visual grounding ____.
Contrastive language-image pre-training (CLIP) performs contrastive learning on very large-scale web-curated image-text pairs. It shows  promising pre-trained representations with superior zero-shot transfer ability on diverse datasets and impressive fine-tuning performance on various downstream tasks. 
Subsequent works **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** improve the aspects of prompt design and optimization; ____ unifies the vision-language understanding and generation by adding text decoders with image-text cross-attention during pre-training; ____ builds an alignment between region feature or position information with fine-grained object descriptions. 
Although significant results have been achieved with CLIP and its development, less effort and exploration is focused on its interpretability through visual explanations. In this paper, we propose a novel visual explanation method, which generates high-quality heat maps that reveal the important regions or words used for CLIP's scoring of an image-text pair. 

\vspace{-0.1cm}
\subsection{Explainability in computer vision}
%\vspace{-0.1cm}

Since visualizing the importance of input features is a straightforward approach to interpret a model, many works visualize the internal representations of CNNs or Transformers with heat maps. Most explanation methods can be categorized as: CAM methods, perturbation methods, Shapley-value methods, or attribution propagation (relevance-based) methods.

CAM methods, such as CAM **Zhou et al., "Learning Deep Features for Discriminative Localization"** ____, Grad-CAM **Selvaraju et al., "Grad-Cam: Visual Explanations for Convolutional Neural Networks"** ____, and Grad-CAM++ ____, generate the explanation heat map from a selected  feature layer by linearly aggregating the activation maps with weights that indicates each feature's importance. Grad-CAM computes the weights with global average pooling on the gradients of the model's prediction w.r.t. the feature layer. Gradient-free CAMs **Srinivas et al., "Full Spectral Connections for Image Models"** ____ generate weights from the prediction score changes when perturbing features.
%use feature perturbations to generate weights from changes i prediction score changes. 

Perturbation-based methods ____ perturb the  input and observe the changes in output scores to determine the importance of input regions. Such black-box methods are intuitive and highly generalizable \abc{to different architecture and tasks}, but computationally intensive. The quality of these methods are often greatly influenced by the type or resolution of the perturbations used.
While having solid theoretical justification, Shapley-value methods ____ also suffer from large computational complexity. 
%, similar to perturbation methods.  
% while have a solid theoretical justification. However, 

Attribution propagation methods recursively decompose the network output into the contribution of early layers, based on the Deep Taylor Decomposition (DTD) **Montavon et al., "Explaining Non-Linear Classification Decisions with Deep Taylor Decomposition"** ____ . LRP ____ and its variants ____ propagate relevance from the prediction to the input image based on  DTD and generate class-agnostic explanations, while Contrastive-LRP ____ and SG-LRP ____ generate class-specific explanations.

\abc{These previous works are mainly proposed for interpreting CNN-based models.
Due to the introduction of self-attention mechanisms in Transformers,  recent works **Vig et al., "A Simple Neural Reasoner with Graph Attention"** have also looked at visual explanations for the  Transformer architecture.}
 %Some works ____ are proposed to interpret Transformers. 
 ____ proposed an Attention flow and Rollout method, which is based on all attention maps in the forward process of model. Since Rollout is class-agnostic, Transformer interpretability ____ and GAME ____ build class-specific relevance-based method for explaining transformer with the internal attention mechanism. However, we found that the explanation methods relying on attention maps in Transformer cannot generate satisfactory results with CLIP, possibly because the sparse attention patterns on the $\mathrm{softmax}$ map.
The recent M2IB ____ applies information bottleneck principle to CLIP, which develop an optimization objective to find the compressed representations for both image features and text features. However, a series of hyper-parameters are adopted during the optimization, which limits the generalization in practical applications.

Finally, existing approaches for explaining CLIP ____, 
% are based on similarity, %the similarity based methods for explaining CLIP , 
which use the cosine similarity map between the image local features and the text features as the explanation map, have the disadvantage that they are only based on the \emph{forward (bottom-up) process} and thus the attended features are not necessarily used in the final prediction. In contrast, we propose Grad-ECLIP as an effective approach to interpret CLIP, which highlight features that have largest influence on the prediction as measured by the gradient, which is a top-down process.


\vspace{-0.1cm}
\subsection{\zcy{Fine-grained image understanding with CLIP}}
%\vspace{-0.1cm}

CLIP and its variants ____ exhibit strong representation capabilities and exceptional generalizability through learning general visual-language representations by pre-training on noisy large-scale datasets. Despite the great achievements, CLIP has shown lack of the fine-grained alignment between image regions and text ____ due to its \emph{image-level} training, which matches an image as a whole to a text description. Thus, the model is unable to generate precise representations of an image region %representations 
for grounding textual concepts, which will limit the performance of CLIP on the downstream tasks that require region-aware ability. For example, in  dense prediction tasks, e.g. object detection and segmentation, the CLIP model is usually utilized as a classifier ____ or the teacher in distillation ____ to process %the already 
cropped object patches %as a whole 
to obtain region features. Some works such as F-vlm ____ , CORA ____ and FC-Clip ____ adopt the frozen CLIP model as backbone to produce spatial feature maps, but they all choose  the %Convolutional (
CNN-based CLIP, which can preserve more position information than the vision transformer (ViT-based) architecture. However, due to the image-level training, CLIP models still lack fine-grained alignment and are poor at generating precise image region representations ____.

To mitigate this issue, recent works ____ enhance the fine-grained understanding ability of CLIP by leveraging  region-text alignments \abc{during pre-training} 
%A series of works leverage region-level alignment and fine-grained understanding for vision-language pre-training 
____.
Since no region-text  annotations are provided in the image-text pair training data, most of these methods need to generate image regions with the corresponding text tags using off-the-shelf methods. Some works utilize the annotations in visual grounding datasets ____ or generate pseudo region-text pairs ____ with the help of high-performance detectors that are trained with a large number of object categories. 
RegionCLIP ____ adopts RPN ____ object proposals while PTP ____ coarsely crops patches, and then they both use CLIP as a classifier to obtain region labels with a large pre-defined pool of concepts, which are parsed from a text corpus.
These methods inevitably cost significant extra time and space to preprocess the region annotations, which cannot be neglected when using a huge amount of training data. Moreover, the range of concepts is also limited by the number of pre-defined categories. 
Another work CLIPSelf ____ facilitates the transfer of the global features of the cropped regions to dense feature extraction by self-distillation, which enhances the fine-grained understanding ability of CLIP. 

\vspace{-0.2cm}