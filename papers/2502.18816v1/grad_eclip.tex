\documentclass[10pt,journal,cspaper,compsoc]{IEEEtran}
\usepackage{times}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage[square,numbers]{natbib}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{stfloats}
\usepackage{ifthen}
\usepackage{times}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{subfigure}
\usepackage{soul}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{hyperref}



\usepackage[font=small]{caption}

\newcommand{\etal}{\textit{et al.}}
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\relu}{\mathrm{ReLU}}
\newcommand{\calI}{\mathcal{I}}
\newcommand{\calP}{\mathcal{P}}
\newcommand{\s}[1]{^{(#1)}}
\newcommand{\CUT}[1]{}


\newcommand{\calT}{\mathcal{T}}
\newcommand{\calL}{\mathcal{LP}}
\newcommand{\calA}{\mathcal{A}}

\newcommand{\real}{\mathbb{R}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\newcommand{\zcy}[1]{{#1}}
\newcommand{\zcyy}[1]{{#1}}
\newcommand{\zcyNOTE}[1]{\textcolor{cyan}{[ZCY:#1]}}

\newcommand{\NOTE}[1]{\textcolor{red}{[NOTE: #1]}}
\newcommand{\abc}[1]{{#1}}
\newcommand{\abcn}[1]{{#1}}


\usepackage{colortbl}
\definecolor{mygray}{gray}{.9}


%\usepackage[font=small]{caption}
%\usepackage{caption}
\usepackage{silence}
\WarningFilter{latex}{Text page}

\renewcommand{\dblfloatpagefraction}{.9}
\newcommand{\myparagraph}[1]{\noindent{\bf #1.}}

\begin{document}

\title{Grad-ECLIP: Gradient-based Visual \abc{and Textual} Explanations for CLIP}

\author{Chenyang~Zhao, Kun~Wang, Janet~H.~Hsiao and Antoni~B.~Chan 
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Chenyang Zhao, and Antoni~B.~Chan (corresponding author)  are with the Department of Computer Science, City University of Hong Kong. Janet~H.~Hsiao is with the Division of Social Science and Department of Computer Science \& Engineering, Hong Kong University of Science \& Technology, and Kun~Wang is with the SenseTime Group Ltd.
%\protect\\
E-mail: zhaocy2333@gmail.com, abchan@cityu.edu.hk.}
\thanks{}}


\markboth{Journal of \LaTeX\ Class Files,~Vol.~X, No.~X, XXX~XXXX}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}

\IEEEcompsoctitleabstractindextext{%
\begin{abstract}
	Significant progress has been achieved on the improvement and downstream usages of the Contrastive Language-Image Pre-training (CLIP) vision-language model, while less attention is paid to the interpretation of CLIP. We propose a Gradient-based visual \abc{and textual} Explanation method for CLIP (Grad-ECLIP), which interprets the matching result of CLIP for specific input image-text pair. By decomposing the architecture of the encoder and discovering the relationship between the matching similarity and intermediate spatial features, Grad-ECLIP produces effective heat maps that show the influence of image regions or words on the CLIP results. Different from the previous Transformer interpretation methods that focus on the utilization of self-attention maps, which are typically extremely sparse in CLIP, we produce high-quality visual explanations by applying channel and spatial weights on token features. 
	Qualitative and quantitative evaluations verify the effectiveness and superiority of Grad-ECLIP compared with the state-of-the-art methods. Furthermore, a series of analysis are conducted based on our visual and textual explanation results, from which we explore the working mechanism of image-text matching,  the strengths and limitations in attribution identification of CLIP, \abc{and the relationship between the concreteness/abstractness of a word and its usage in CLIP.}
	\zcy{Finally, based on the ability of explanation map that indicates text-specific saliency region of input image, we also propose an application with Grad-ECLIP, which is adopted to boost the fine-grained alignment in the CLIP fine-tuning.}
	The code of Grad-ECLIP is available here: https://github.com/Cyang-Zhao/Grad-Eclip.
\end{abstract}

\begin{IEEEkeywords}
	gradient-based explanation, visual and textual explanation, explainable AI, contrastive language-image pre-training, fine-grained understanding, open-vocabulary detection, deep learning

\end{IEEEkeywords}
}

\maketitle

\IEEEdisplaynotcompsoctitleabstractindextext
\IEEEpeerreviewmaketitle

\vspace{-0.1cm}
\section{Introduction}
\label{sec:intro}
\vspace{-0.1cm}

Recently, by learning the representations for matching caption text and its corresponding image, the Contrastive Language-Image Pre-training (CLIP)  model \cite{radford2021clip} has introduced a simple and effective dual-encoder 
pre-training paradigm for the interaction between natural language processing and computer vision. 
CLIP significantly improves the performance on various downstream tasks, such as classification \cite{changpinyo2021conceptual,cha2022domain}, retrieval \cite{luo2022clip4clip} and segmentation \cite{wang2022cris,xu2022groupvit}, with zero-shot and fine-tuning methodologies.  Inspired from CLIP, multi-modal pre-training has been further developed by exploring different perspectives, including unifying vision-language understanding and generation \cite{yu2022coca,li2022blip}, prompt design \cite{zhou2022learning,chen2022prompt}, and region-aware enhancement \cite{li2020oscar,wang2023position,zhong2022regionclip}. 
Although researchers devote many efforts into improving multi-modal pre-training or exploring the usages in downstream tasks, less attention has been focused on the interpretation or explanation of CLIP. 
%The explanation approaches produce heat maps representing the influence of different input elements on the model’s decision, which can help to identify failure modes, debug models and establish appropriate users’ confidence about models. 

Previous visual explanation works have considered interpreting the transformer architecture used by CLIP. Attention Rollout \cite{abnar2020quantifying} generates explanations by aggregating  attention maps computed along the forward pass of the model. 
Relevance-based methods \cite{chefer2021transformer,chefer2021generic} apply Layer-wise Relevance Propagation (LRP) \cite{bach2015pixel} and also rely on the attention mechanism in the model architecture. 
Since Rollout and many LRP variants are class-agnostic, Transformer interpretability \cite{chefer2021transformer} and Generic Attention-Model Explainability (GAME) \cite{chefer2021generic} build class-specific relevance-based explanations using the self-attention or co-attention. 
However, just %simply 
treating CLIP as a vision transformer (ViT) and generating visual explanations based on self-attention sometimes leads to confusing results because of sparse attention maps (see Fig.~\ref{fig:image1}e-g). 
% Moreover, the transformer explanation methods are not suitable with convolutional neural network (CNN)-based CLIP, like CLIP with ResNet \cite{he2016deep} as the visual encoder. 


\begin{figure}
	%		\vspace{-6pt}
	\begin{center}
		\includegraphics[width=\linewidth]{figures/image1_new.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Visual \abc{and textual} explanations of CLIP for the image with the text ``A dog is playing with frisbee'' using (a) CLIPSurgery \cite{li2023clipsurgery}; (b) MaskCLIP \cite{zhou2022extract}; (c) Grad-CAM \cite{selvaraju2017grad}; (d) RISE \cite{petsiuk2018rise};  (e) raw attention in the last layer; (f) Rollout \cite{abnar2020quantifying}; (g) GAME \cite{chefer2021generic}; (h) M2IB \cite{wang2024visual}; and (i) Our Grad-ECLIP. \zcy{For (e) to (i), textual explanations on the sentence are shown, where the degree of green color represents the word importance. Other methods (a-d) are not applicable on text. }
	}
	\vspace{-0.5cm}
	\label{fig:image1}
\end{figure}

%An existing method 
ECLIP \cite{li2022exploring} and CLIPSurgery \cite{li2023clipsurgery} (see Fig.~\ref{fig:image1}a) explores explanations for CLIP by computing an image-text similarity map, and solve the counter-intuitive problem that background patch features get higher similarity with the text feature than the foreground. However, to obtain reasonable similarity maps, new additional projection layers or changing the structure of the original CLIP are required. Although the parameters of CLIP encoders are frozen, learning more black-box parameters with extra data or modifying original model architecture makes the explanation less interpretable.
%it is not appropriate to involve extra parameters and training data for making explanation. 
MaskCLIP \cite{zhou2022extract} also provides a technique to calculate class-specific image-text similarity map. By passing the value features of the last attention layer through later linear layers as image patch features, the similarity map is able to localize the concept in the text (see Fig.~\ref{fig:image1}b), but has noisy backgrounds and confusingly highlights points on the locations unrelated to the explained target. The disadvantage of these similarity-map methods is that they are only forward processing, and the attended features are not necessarily used in the final prediction.


To better focus on the discriminative features used in the prediction, gradient-based methods with class-activation maps (CAM) \cite{zeiler2014visualizing}, such as Grad-CAM \cite{selvaraju2017grad}, Layer-CAM \cite{jiang2021layercam} and FullGrad \cite{NEURIPS2019_80537a94}, consider the gradient of the prediction with respect to features from a CNN layer as weights, and locates the class-specific discriminative regions by weighted aggregation of the features maps. Fig.~\ref{fig:image1}c shows the visualization when adapting Grad-CAM on CLIP, where the cosine similarity of image-text pair is adopted as the prediction and the gradients are calculated w.r.t.~the patch tokens from the ViT layers. 
%
Since there are no gradients w.r.t.~the patch tokens in final layer because they are not involved in the calculation of the matching score, feature outputs from the penultimate layer of ViT are adopted.
%Since there are no gradients w.r.t.~the patch tokens in final layer, which are not involved in the calculation of the matching score, feature outputs from the penultimate layer of ViT are adopted.
%However, t
The results of Grad-CAM  do not well explain CLIP, and suffer from the same problem of highlighting unrelated points as MaskCLIP, suggesting that the layer features of ViT are unsuitable for CAM methods.  


In this paper, we explore a more effective way to interpret CLIP, by analyzing how CLIP obtains the final feature embedding, and \zcy{deriving the relationship between the image and text embedding similarity score and intermediate features via a series of approximations.} 
Based on the CAM principle, we then propose a novel gradient-based visual explanation method for CLIP (Grad-ECLIP), which generates the importance heat map by aggregating the intermediate \abc{image} features with result-related \emph{channel} and \emph{spatial} importance weights.
%By decomposing the final layer of the transformer, 
Our proposed method  
uses the gradients of the image-text matching score %of the given image-text pair flowing to the output of 
w.r.t.~the
attention layer as the importance for feature channels. For the spatial importance, because the softmax attention typically yields sparse attention maps, we propose a loosened attention map for computing the spatial importance, which can better reflect the importance of more regions, as compared to directly using the strict softmax attention.
%a slack attention map to reflect importance for feature locations. 
Then our Grad-ECLIP explanation map is calculated with the \textit{values} in the attention layer as the feature map, weighted by the channel and spatial importances. %, our method successfully generates clear and reasonable explanation map for CLIP. 
\abc{The same method used to generate the explanation of the image encoder can also be applied to the text encoder to obtain a textual explanation for CLIP.}
Note that Grad-ECLIP is result-specific and is suitable for both the image and text encoders, i.e., the visual explanation on image is text-specific and the \abc{textual explanation of a sentence}  %word attention degrees in a sentence 
is image-specific. \zcy{For the example shown in Fig.~\ref{fig:image1}i, the heat map on the image shows the important region when matching the image with the specific text ``a dog is playing with frisbee'', while the degree of green color on the sentence represents the \abc{important words}, 
%word attention,
 where the most important words ``dog'' and ``frisbee'' correspond to the highlighted regions in the image heat map.}


In the experiments, we conduct both qualitative evaluation by visualization of explanation maps and quantitative evaluation compared with other %multi 
types of explanation methods, and show the superiority of our proposed Grad-ECLIP. \zcy{Moreover, in the qualitative evaluation, we demonstrate the generalizability of Grad-ECLIP by applying our method on diverse datasets of different domains and showing it is applicable to both ViT and CNN-based CLIP, as well as the ViT classifier and other transformer-based vision language models like BLIP \cite{li2022blip}.}
Then, using Grad-ECLIP, we further conduct a visualization-based analysis on CLIP, and reveal working mechanisms and advantages/limitations of the CLIP model, \abc{including the type of attributes and the concreteness/abstractness of words used by CLIP}. We hope our proposed method can be helpful for researchers to explore more properties of vision-language models like CLIP, \abc{as well as understand their current limitations and how this may affect downstream tasks}.

\zcy{Finally, we also present an application of Grad-ECLIP to fine-tuning the CLIP model to boost the fine-grained understanding. Since the ViT-based CLIP model has been shown to have limitations in producing dense representations, due to the pretraining focusing on the whole image-text matching \cite{zhong2022regionclip,wang2023position, kim2023region,wu2023clipself},
we propose to generate detailed region-phrase corresponding pairs via Grad-ECLIP so as to enhance the fine-grained understanding of CLIP model during fine-tuning. Experiments with zero-shot region classification and down-stream open-vocabulary detection application show that the Grad-ECLIP-enabled fine-tuning is effective.}


%We hope our explanation tool could be helpful for researchers to explore more properties about the paradigm of language-image pre-training, and inspire further development about the interaction of computer vision and language processing.

In summary, the contributions of this paper are five-fold: 
\begin{compactenum}
	\item We investigate Grad-ECLIP, a gradient-based visual and textual explanation approach for CLIP to produce high-quality result-specific heat maps for explaining the matching of image-text pairs. 
	
	\item We demonstrate the superiority of the proposed Grad-ECLIP with comprehensive evaluations 
	comparing with the state-of-the-art explanation methods for Transformers and CLIP.
	
	\item \zcy{We show the generalizability of Grad-ECLIP by presenting  explanations on  datasets of different domains, and verifying the applicability on both ViT and CNN-based CLIP, ViT classifier, and other transformer-based vision language models.}
	
	\item By using Grad-ECLIP, we explore the properties of CLIP, and reveal the model's ability of concept decomposition and addibility, strengths and weaknesses in attribution identification\zcy{, as well as the relationship  between word \abc{usage} and concreteness in the image and text matching}. 
	
	\item \zcy{We propose an application of Grad-ECLIP to boost the fine-grained understanding of CLIP via fine-tuning, which adopts the high-quality visual explanation map generated by Grad-ECLIP to produce detailed matching relationships between image regions and the corresponding semantic concepts. }
\end{compactenum}

%\NOTE{we also provide a more detailed derivation of Grad-EClip, based on the transformer model?}\zcyNOTE{Yes.}
\zcy{A preliminary version of our work appears in \cite{zhao2024gradient}. The extensions over the conference version are as the follows. First, \zcyy{we provide a more detailed derivation of Grad-ECLIP, %when introducing the method, 
based on the transformer model. Second,} we enrich the qualitative evaluation and verify the generalizability of Grad-ECLIP by adding: (1) the visualization on diverse datasets of different domains; (2) application on ViT-based classifier and other vision language models; (3) adaptation with CNN-based CLIP. Third, we include new ablation studies for the Grad-ECLIP design, including: (1) the effect of the proposed loosen spatial weight; (2) the influence of number of layers involved in the calculation; (3) the influence of multi-attention heads on the visual explanation results. Fourth, we add a new exploration  about the relationship between word concreteness and word usage in image-text matching with CLIP. Moreover, we introduce an application of Grad-ECLIP, which applies the visual explanation map to boost the fine-grained region and text matching when fine-tuning CLIP.}

\zcy{The remainder of this paper is organized as follows. The related works about CLIP, explainability in computer vision and fine-grained understanding in CLIP are briefly reviewed in \S\ref{sec:related}. Grad-ECLIP is introduced in \S\ref{sec:method}, and the experiment results are presented in \S\ref{sec:exp}, including qualitative evaluations, quantitative evaluations and ablation studies. We then perform  analysis of CLIP based on the proposed Grad-ECLIP in \S\ref{sec:analysis_clip}. Finally, we introduce an application of Grad-ECLIP in \S\ref{sec:app_fineclip} for boosting the fine-grained understanding of CLIP.}


\vspace{-0.2cm}
\section{Related Works}
\label{sec:related}
\vspace{-0.1cm}

\zcy{We first briefly review the 
%contrastive language-image pre-training (
CLIP
 model, and then discus different types of visual explanation methods in computer vision. Finally, the related works for fine-grained image annotation and understanding in CLIP are introduced.}


\vspace{-0.1cm}
\subsection{Contrastive language-image pre-training}
%\vspace{-0.1cm}

Many multi-modal works have been developed and focus on the interaction of computer vision and natural language processing, such as text-image retrieval \cite{wang2019camp}, image captioning \cite{xu2015show}, visual question answering \cite{antol2015vqa}, and visual grounding \cite{plummer2015flickr30k}.
Contrastive language-image pre-training (CLIP) performs contrastive learning on very large-scale web-curated image-text pairs. It shows  promising pre-trained representations with superior zero-shot transfer ability on diverse datasets and impressive fine-tuning performance on various downstream tasks. 
Subsequent works extend and improve CLIP from different aspects: \citep{zhou2022learning,chen2022prompt} improve the aspects of prompt design and optimization; \citep{yu2022coca,li2022blip} unifies the vision-language understanding and generation by adding text decoders with image-text cross-attention during pre-training; \citep{li2020oscar,wang2023position,zhong2022regionclip,wu2023clipself} builds an alignment between region feature or position information with fine-grained object descriptions. 
Although significant results have been achieved with CLIP and its development, less effort and exploration is focused on its interpretability through visual explanations. In this paper, we propose a novel visual explanation method, which generates high-quality heat maps that reveal the important regions or words used for CLIP's scoring of an image-text pair. 

\vspace{-0.1cm}
\subsection{Explainability in computer vision}
%\vspace{-0.1cm}

Since visualizing the importance of input features is a straightforward approach to interpret a model, many works visualize the internal representations of CNNs or Transformers with heat maps. Most explanation methods can be categorized as: CAM methods, perturbation methods, Shapley-value methods, or attribution propagation (relevance-based) methods.

CAM methods, such as CAM \cite{zeiler2014visualizing}, Grad-CAM \cite{selvaraju2017grad}, and Grad-CAM++ \cite{chattopadhay2018grad}, generate the explanation heat map from a selected  feature layer by linearly aggregating the activation maps with weights that indicates each feature's importance. Grad-CAM computes the weights with global average pooling on the gradients of the model's prediction w.r.t. the feature layer. Gradient-free CAMs \cite{ramaswamy2020ablation, wang2020score, wang2020ss} generate weights from the prediction score changes when perturbing features.
%use feature perturbations to generate weights from changes i prediction score changes. 

Perturbation-based methods \cite{ribeiro2016should, petsiuk2018rise,fong2017interpretable, lundberg2017unified, wagner2019interpretable, lee2021bbam, petsiuk2021black} perturb the  input and observe the changes in output scores to determine the importance of input regions. Such black-box methods are intuitive and highly generalizable \abc{to different architecture and tasks}, but computationally intensive. The quality of these methods are often greatly influenced by the type or resolution of the perturbations used.
While having solid theoretical justification, Shapley-value methods \cite{lundberg2017unified} also suffer from large computational complexity. 
%, similar to perturbation methods.  
% while have a solid theoretical justification. However, 

Attribution propagation methods recursively decompose the network output into the contribution of early layers, based on the Deep Taylor Decomposition (DTD) \cite{montavon2017explaining}. LRP \cite{bach2015pixel} and its variants \cite{lundberg2017unified,nam2020relative,shrikumar2017learning} propagate relevance from the prediction to the input image based on  DTD and generate class-agnostic explanations, while Contrastive-LRP \cite{gu2019understanding} and SG-LRP \cite{iwana2019explaining} generate class-specific explanations.

\abc{These previous works are mainly proposed for interpreting CNN-based models.
Due to the introduction of self-attention mechanisms in Transformers,  recent works \cite{qiang2022attcat,xie2022vit,yu2023x} have also looked at visual explanations for the  Transformer architecture.}
 %Some works \cite{qiang2022attcat,xie2022vit,yu2023x} are proposed to interpret Transformers. 
 \cite{abnar2020quantifying} proposed an Attention flow and Rollout method, which is based on all attention maps in the forward process of model. Since Rollout is class-agnostic, Transformer interpretability \cite{chefer2021transformer} and GAME \cite{chefer2021generic} build class-specific relevance-based method for explaining transformer with the internal attention mechanism. However, we found that the explanation methods relying on attention maps in Transformer cannot generate satisfactory results with CLIP, possibly because the sparse attention patterns on the $\mathrm{softmax}$ map.
The recent M2IB \cite{wang2024visual} applies information bottleneck principle to CLIP, which develop an optimization objective to find the compressed representations for both image features and text features. However, a series of hyper-parameters are adopted during the optimization, which limits the generalization in practical applications.

Finally, existing approaches for explaining CLIP \cite{li2022exploring, li2023clipsurgery, zhou2022extract}, 
% are based on similarity, %the similarity based methods for explaining CLIP , 
which use the cosine similarity map between the image local features and the text features as the explanation map, have the disadvantage that they are only based on the \emph{forward (bottom-up) process} and thus the attended features are not necessarily used in the final prediction. In contrast, we propose Grad-ECLIP as an effective approach to interpret CLIP, which highlight features that have largest influence on the prediction as measured by the gradient, which is a top-down process.


\vspace{-0.1cm}
\subsection{\zcy{Fine-grained image understanding with CLIP}}
%\vspace{-0.1cm}

CLIP and its variants \citep{zhou2022learning,yu2022coca,li2022blip} exhibit strong representation capabilities and exceptional generalizability through learning general visual-language representations by pre-training on noisy large-scale datasets. Despite the great achievements, CLIP has shown lack of the fine-grained alignment between image regions and text \cite{zhong2022regionclip,wang2023position, kim2023region,wu2023clipself} due to its \emph{image-level} training, which matches an image as a whole to a text description. Thus, the model is unable to generate precise representations of an image region %representations 
for grounding textual concepts, which will limit the performance of CLIP on the downstream tasks that require region-aware ability. For example, in  dense prediction tasks, e.g. object detection and segmentation, the CLIP model is usually utilized as a classifier \cite{xu2021simple, liang2023open} or the teacher in distillation \cite{gu2021open, du2022learning} to process %the already 
cropped object patches %as a whole 
to obtain region features. Some works such as F-vlm \cite{kuo2023fvlm}, CORA \cite{wu2023cora} and FC-Clip \cite{yu2024fcclip} adopt the frozen CLIP model as backbone to produce spatial feature maps, but they all choose  the %Convolutional (
CNN-based CLIP, which can preserve more position information than the vision transformer (ViT-based) architecture. However, due to the image-level training, CLIP models still lack fine-grained alignment and are poor at generating precise image region representations \cite{zhong2022regionclip, wang2023position, kim2023region,li2022grounded}.

To mitigate this issue, recent works enhance the fine-grained understanding ability of CLIP by leveraging  region-text alignments \abc{during pre-training} 
%A series of works leverage region-level alignment and fine-grained understanding for vision-language pre-training 
\cite{chen2020uniter,li2020oscar,zhang2021vinvl,zhong2022regionclip,li2022grounded}.
Since no region-text  annotations are provided in the image-text pair training data, most of these methods need to generate image regions with the corresponding text tags using off-the-shelf methods. Some works utilize the annotations in visual grounding datasets \cite{liu2023grounding,li2022grounded,krishna2017visual} or generate pseudo region-text pairs \cite{chen2020uniter,li2020oscar,zhang2021vinvl} with the help of high-performance detectors that are trained with a large number of object categories. 
RegionCLIP \cite{zhong2022regionclip} adopts RPN \cite{ren2015faster} object proposals while PTP \cite{wang2023position} coarsely crops patches, and then they both use CLIP as a classifier to obtain region labels with a large pre-defined pool of concepts, which are parsed from a text corpus.
These methods inevitably cost significant extra time and space to preprocess the region annotations, which cannot be neglected when using a huge amount of training data. Moreover, the range of concepts is also limited by the number of pre-defined categories. 
Another work CLIPSelf \cite{wu2023clipself} facilitates the transfer of the global features of the cropped regions to dense feature extraction by self-distillation, which enhances the local representations during the fine-tuning of CLIP model. However, preprocessing of generating region proposals by a well-trained detector is still required to obtain superior distillation performance -- \abc{when using randomly cropped patches, the performance \abcn{significantly drops.}}
%}suffers} \NOTE{correct?}\zcyNOTE{Not suffers, but has a significant performance drop, about 3 to 5 points}. 

In this paper, we build a novel fine-tuning framework for boosting the fine-grained understanding in CLIP, which adopts the proposed Grad-ECLIP to generate  region-aware attention maps for aligning with the corresponding text phrases (concepts). By simply inserting the Grad-ECLIP-based module into the fine-tuning, our method circumvents the resource-consuming preparation of the region annotations and the requirement of high-performance detectors.
% which are required to support recognizing a large amount of categories. 
The inputs of the proposed fine-tuning framework are still image-text pairs, just the same as in the pre-training of CLIP.  




\begin{figure}
	%		\vspace{-6pt}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{figures/framework_new.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Illustration of Grad-ECLIP. An image-text pair specific visual explanation is generated by weighting and aggregating the \textit{values} as feature map in the attention layer with spatial importance $\lambda_{i}$ and channel importance $w_{c}$. Gradients are propagated to the attention layer output to produce $w_{c}$, and the loosened attention map is applied as $\lambda_{i}$. 
%	\NOTE{the notation here is a bit confusing. $v_i$ should be a vector but here you say its a tensor. You can fix it by using $[v_i] = \mathbb{R}^{H x W x C}$ to denote the collection of $v_i$'s. Similarly for $w_i$, $w_c$. Also change $w_i$ to $\lambda_i$}\zcyNOTE{Done.}
	}
	\label{fig:framework}
	\vspace{-0.5cm}
\end{figure}


\vspace{-0.2cm}
\section{Grad-ECLIP: Gradient-based visual explanation for CLIP}
\label{sec:method}
\vspace{-0.1cm}

Our method serves as a gradient-based visual \abc{and textual} explanation for interpreting the CLIP matching performed on image-text pairs. We start with a brief introduction of CLIP. Then, by decomposing the layers of the transformer and exploring the relationship between the final output and intermediate features, we derive our gradient-based  explanation for CLIP (Grad-ECLIP).

\vspace{-0.2cm}
\subsection{Preliminary on CLIP}
\label{sec:pre_clip}
\vspace{-0.1cm}

CLIP learns both visual and language representations from large-scale %raw 
web-curated image-text pairs. It consists of an image encoder $\calI\left(\cdot\right)$ and a text encoder $\calT\left(\cdot\right)$, which are jointly trained to respectively extract image and text feature embedding in a unified representation space. Given image-text pair $(I,T)$, the matching score between their extracted image feature $F_{I}\in \mathbb{R}^{D}$ and text feature $F_{T}\in \mathbb{R}^{D}$ \abc{(both row vectors)} is:
\begin{align}
	S(F_{I}, F_{T}) = \cos(F_{I}, F_{T}) = \tfrac{F_{I}F_{T}^\mathsf{T}}{\left \| F_{I} \right \| \left \| F_{T} \right \| } .
	\label{eq:cos_similarity}
\end{align}

\abc{The model is trained using}
contrastive learning on the matching scores, regarding the ground-truth image-text pairs as positive samples and other mismatched pairs as negatives. 
% Here we focus on the CLIP model where both encoders are transformers. 
\zcy{In practice, both encoders can be implemented as transformers. 
In \S\ref{sec:grad-eclip} our method is derived based on the transformer architecture, and thus is suitable for interpreting both ViT-based image and transformer-based text encoders. 
Alternatively, the image encoder could be a CNN-based ResNet followed by an attention pooling layer, which 
% is alternative for image encoder. 
%The attention pooling layer 
is basically the same as an attention layer in the Transformer.
%Then, we declare that
Thus the proposed Grad-ECLIP is also applicable to the CNN-based CLIP, for which we display the visualization results for CLIP with ResNet \cite{he2016deep} backbone in \S\ref{sec:apply_cnn_clip}. }

\vspace{-0.2cm}
\subsection{\zcy{Methodology of Grad-ECLIP}}
\label{sec:grad-eclip}
\vspace{-0.1cm}

Here we present our derivation of Grad-ECLIP from the image viewpoint, where the visualization is generated on the input image $I$ and shows important regions related to producing the matching score $S_{T}(F_{I}) \triangleq S(F_{I}, F_{T})$, with the given specific text prompt $T$. The application of Grad-ECLIP from the text viewpoint, where the visualization is generated for the text prompt $T$ given the input image $I$, can be obtained analogously by considering the $[eos]$ token (end of sentence token) from the text encoder, which is analogous to the $[cls]$ token in the image encoder.


For a Transformer that consists of $N$ layers, following convention, we denote $x^{(n)}$ as the input of layer $L^{(n)}$ and output of layer $L^{(n+1)}$, where $n\in\left[0...N\right]$ is the layer index. $x^{(N)}$ is the input of the network, $x^{(1)}$ is the input of the last layer and $x^{(0)}=\calI(x^{(N)})$ is the output of the network. 
\zcyy{The image feature is $F_{I}=\calL(x_{cls}^{(0)})$, where $\calL$ denotes linear projections, and $x_{cls}$ is the feature vector from the $[cls]$ token.}
%The image feature is $F_{I}=\calL(x^{(0)}[cls])$, where $\calL$ denotes linear projections, and $[cls]$ represents the operation to get the feature vector on the class token 
%\NOTE{is $x^{(0)}[cls]$ the same as $x^{(0)}_{cls}$?}\zcyNOTE{Yes, same.}. 
Thus, except for the class token, all the final layer features of the other tokens (image patch tokens) are not used during contrastive learning of CLIP. Therefore, to interpret the $S_{T}(F_{I})$ w.r.t. an image feature map, we explore the relationship between the last layer class token feature $x_{cls}^{(0)} \in \real^C$ and the intermediate spatial feature maps. 

As shown in the illustration of Fig.~\ref{fig:framework}, looking closely into the last layer of the network, the image embedding from visual encoder can be formulated as:
%\NOTE{new derivation based on the aggregation of heat maps over layers}
\abcn{
\begin{align}
	F_{I} &= \calL(x_{cls}^{(0)})  = \calL(o_{cls}^{(0)}+x_{cls}^{(1)}) \\ 
	&\approx \calL(o_{cls}^{(0)}) + \calL(x_{cls}^{(1)}), 
\end{align}
where the approximation is based on assuming linearity of $\calL$.
Noting that $ \calL(x_{cls}^{(t)})  = \calL(o_{cls}^{(t)}+x_{cls}^{(t+1)})$, we  substitute recursively to obtain the approximation:
	\begin{align}
	F_{I} &\approx \calL(o_{cls}^{(0)}) + \cdots +  \calL(o_{cls}^{(N-1)}) + \calL(x_{cls}^{(N)}) \\
	&\triangleq \sum_{t=0}^{N} F_I^{(t)}, 
	\label{eqn:aggregate}
	\end{align}
where we define $F_I^{(t)} = \calL(o_{cls}^{(t)})$ for $t<N$, and $F_I^{(N)} = \calL(x_{cls}^{(N)})$. Thus from (\ref{eqn:aggregate}), the image feature $F_I$ is approximately an aggregation of features from each layer $F_{I}^{(t)}$.}

\abcn{
The feature vector from each layer is computed from a self-attention operation. For example, for the last layer ($t=0$), 
	\begin{align}
	F_I^{(0)} = \calL(o_{cls}^{(0)}) = \calL\big(\sum_{i}\mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{i}\big),
	\label{eq:feature}
	\end{align}}
where the output of attention layer ($\calA$) on the class token is
\vspace{-0.1cm}
\begin{align}
	o_{cls}^{(0)}=\calA(x^{(1)})[cls]=\sum_{i}\mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{i},
	\label{eq:out_cls}
\end{align}
and $\calA$ represents the attention layer in the Transformer, $q_{cls}$ is the \textit{query} embedding for the class token, while $k_{i}$ and $v_{i}\in\real^C$ represent the \textit{key} and \textit{value} embeddings at spatial location $i$, with $C$ as their channel dimension.\footnote{We skip the superscript $(0)$ on $\{q,k,v\}$ identifying the layer for  brevity.}
The $\mathrm{softmax}$ operation inside the attention layer measures the weight of the value on each location. 
Multi-heads are usually used in the attention layer to group the channel of $\{q, k, v\}$ into several heads, and (\ref{eq:out_cls}) is operated inside each head with the $\mathrm{softmax}$ calculated over subsets of the channels. Then the final attention layer output is obtained by concatenating the results of each head together. In  practice \abcn{for visualization}, we formulate the $o_{cls}^{(0)}$ with one attention head in the forward pass and operate the $\mathrm{softmax}$ over all channels as in (\ref{eq:out_cls}). 
%\NOTE{does it mean you just replace the set of softmax calculated over subsets of channels with just one softmax over all channels?} \zcyNOTE{Yes.}\NOTE{so for the pre-trained CLIP, you just replace the softmax to be over all channels? }\zcyNOTE{Yes, but only on the layers we use for calculating gradients. I remove the channel splitting operation and process softmax on all channels. }
We discuss the influence multi-heads to visual explanation in the \S\ref{sec:influence_heads}.  

%\NOTE{There is another way to derive this based on cosine score. It depends on how you implement the Eq.11.  When you take the gradient in (15), is it based on the inner product of the feature vectors?  (are features vectors normalized first?) or the cosine of the feature vectors?}\zcyNOTE{It is based on the inner product. The features are normalized first. }
%\NOTE{there is an assumption here that the feature vectors are unit length}
\abcn{
Assuming that the feature vectors $(F_T, F_I)$ are normalized and 
using (\ref{eqn:aggregate}), the matching score can be approximated as an aggregation of partial scores for feature vectors from each layer, 
\begin{align}
	S_{T}(F_{I}) &= \sum_{c}F_{T}[c]F_{I}[c] 
	\approx \sum_{c}F_{T}[c] \sum_{t} F_{I}^{(t)}[c]  \\
	&=\sum_{t} \Big( \sum_{c}F_{T}[c]  F_{I}^{(t)}[c] \Big) 
	\triangleq \sum_t S_T(F_I^{(t)}), 
	\label{eqn:scoreaggregate}
\end{align}
where $[c]$ selects the $c$-th channel, and $S_T(F_I^{(t)})$ denotes the score from layer $t$. Next, to calculate the heat map for the contribution of $o_{cls}$ on the partial matching score, we write the partial matching score as a function of its $o_{cls}$. Specifically, looking at the last layer ($t=0$) as an example,}
%From (\ref{eqn:scoreaggregate}), the matching score is approximately an aggregation of scores for feature vectors from each layer. }
\begin{align}
	S_{T}(F_{I}^{(0)}) &= \sum_{c}F_{T}[c]F_{I}^{(0)}[c] 
	\\ &= \sum_{c}F_{T}[c]\calL(o_{cls})[c]^{(0)} \triangleq f(o_{cls}), 
\end{align}
\abc{where we have defined the matching score as a function of $o_{cls}$, i.e., $f(o_{cls})$.
We define the approximation of the matching score as a weighted combination of the channel features in $o_{cls}$, we have}
 %$S_{T}(F_{I})$ by
\begin{align}
	%S_{T}(F_{I}) = 
	f(o_{cls}) \approx \tilde{f}(o_{cls}) \triangleq \sum_{c}w_{c}o_{cls}[c] = 
	w o_{cls}^{\mathsf{T}}
	% \\
%	\widetilde{S_{T}(F_{I})}=\sum_{c}w_{c}o_{cls} = \widetilde{f(o_{cls})},
	\label{eq:appr_s}
\end{align}
\abc{where $w_c$ is the weight for the $c$-th channel, and $w=[w_c]_c \in \real^C$ the corresponding weight vector for all channels.}
%\NOTE{the notation here is not precise, so I changed $w=[w_c]$ are the channel weights, and $\lambda = [\lambda_i]_i$ are the spatial weights. You need to update Fig 2.} \zcyNOTE{Done.}
%and $\dfrac{\partial \widetilde{f(o_{cls})}}{\partial o_{cls}}=w_{c}$. Then, let's watch the first derivatives, and set the optimization objective:
\abc{To obtain the channel weights $w$, we aim to match the first derivatives (gradients) of the original matching score $f$ and its approximation $\tilde{f}$, leading to the optimization problem:}
\abc{
\begin{align}
	w &= \mathop{\mathrm{argmin}}_{w}\big\| f'(o_{cls})-\tilde{f}'(o_{cls})\big\|^{2} \\
%	&= \mathop{\mathrm{argmin}} \sum_{c}\left( \dfrac{\partial f(o_{cls})}{\partial o_{cls}} - w_{c} \right) ^{2}.
	&= \mathop{\mathrm{argmin}}  \big\| \tfrac{\partial f}{\partial o_{cls}} - w \big\|^{2}, 
\end{align}
which has the solution
	\begin{align}
	w &= \tfrac{\partial f}{\partial o_{cls}} = \tfrac{\partial S_T(F_I)}{\partial o_{cls}} .
		\label{eq:w_c}
		\end{align}
Therefore, substituting (\ref{eq:out_cls}) and (\ref{eq:w_c}) into (\ref{eq:appr_s}), 
\begin{align}
	S_{T}(F_{I}) &\approx \sum_{c} w_{c}o_{cls}[c] \\
	&= \sum_{c} \tfrac{\partial S_{T}(F_{I})}{\partial o_{cls}[c]} \sum_{i} \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{ic} \\
	&= \sum_{i}  \Big[ \sum_{c} \tfrac{\partial S_{T}(F_{I})}{\partial o_{cls}[c]} \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})  v_{ic}\Big],
\end{align}
\abcn{where $v_{ic}$ is the c-th channel of $v_i$.}
Regarding $[v_{ic}]$ as the intermediate feature map, and $\lambda_{i} = \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})$, then the importance of the $i$-th spatial location is defined as:
%$i$-th pixel in the heat map  is defined as:
\begin{align}
	H_{i} = \relu\big( \sum_{c} w_{c}\lambda_{i} v_{ic}\big)
	\label{eq:hm}
\end{align}
where $\relu$ means that we only focus on positions that have a positive effect on the final score.
}


\CUT{
\NOTE{---- OLD DERIVATION ---------------------------------}
\begin{align}
	F_{I} &= \calL(x_{cls}^{(0)})  = \calL(o_{cls}^{(0)}+x_{cls}^{(1)}) \\ 
	&\approx \calL(o_{cls}^{(0)}) + \calL(x_{cls}^{(1)}) \\
	&\approx \calL(o_{cls}^{(0)}) \\
	&= \calL\big(\sum_{i}\mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{i}\big),
	\label{eq:feature}
\end{align}
\NOTE{Why is (2) to (3) an approximation if its a linear projection? is it just a linear layer, or there is a relu activation? is there a bias term?} \zcyNOTE{There are two linear layers with bias, and in the middle, there is a QuickGELU, which is $x \cdot sigmoid(1.702x)$. So it shouldn't be a approximation like from (2) to (3), since there is a non-linear sigmoid? Then how should we give an explanation that we just use the output of last layer's attention layer ($o_{cls}$) to do the derivation?}
\NOTE{For (3) to (4), what is the justification?} \zcyNOTE{Maybe it's not appropriate using approximation here. Since we just do the derivation on the last layer. For multi-layers, the heat maps are processed on each layer and aggregated.}
%\NOTE{what does the "c" in $v_{ic}$ represent? should it just be $v_i$?} \zcyNOTE{Yes, it should be just $v_i$, c is to represent the value on the $c_{th}$ dimension in $v_i$ vector, and no need to write it here. But in (13)(14), since we have sum over c, the c in $v_{ic}$ should be preserved. Is it correct?}
where the output of attention layer ($\calA$) on the class token is
\vspace{-0.1cm}
\begin{align}
	o_{cls}^{(0)}=\calA(x^{(1)})[cls]=\sum_{i}\mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{i},
	\label{eq:out_cls}
\end{align}
and $\calA$ represents the attention layer in the Transformer, $q_{cls}$ is the \textit{query} embedding for the class token, while $k_{i}$ and $v_{i}\in\real^C$ represent the \textit{key} and \textit{value} embeddings at spatial location $i$, with $C$ as their channel dimension. The $\mathrm{softmax}$ operation inside the attention layer measures the weight of the value on each location. 
Multi-heads are usually used in the attention layer to group the channel of $\{q, k, v\}$ into several heads, and (\ref{eq:out_cls}) is operated inside each head with the $\mathrm{softmax}$ calculated over subsets of the channels. Then the final attention layer output is obtained by concatenating the results of each head together. In  practice \abcn{for visualization}, we formulate the $o_{cls}^{(0)}$ with one attention head in the forward pass and operate the $\mathrm{softmax}$ over all channels as in (\ref{eq:out_cls}). 
\NOTE{does it mean you just replace the set of softmax calculated over subsets of channels with just one softmax over all channels?} \zcyNOTE{Yes.}\NOTE{so for the pre-trained CLIP, you just replace the softmax to be over all channels? }
We discuss the influence multi-heads to visual explanation in the \S\ref{sec:influence_heads}.  

%\NOTE{this derivation is rewritten -- some parts were not clear because you mixed using $w_c$ to mean both the vector and the c-th value.  Check it carefully. } \zcyNOTE{I see. Checked and no problem.}
Next, \textbf{only considering the last layer}, the matching score can be formed as:
\begin{align}
	S_{T}(F_{I}) = \sum_{c}F_{T}[c]F_{I}[c] = \sum_{c}F_{T}[c]\calL(o_{cls})[c] \triangleq f(o_{cls}), 
\end{align}
\abc{where $[c]$ selects the $c$-th channel, and we have defined the matching score as a function of $o_{cls}$.
We define the approximation of the matching score as a weighted combination of the channel features in $o_{cls}$,}
 %$S_{T}(F_{I})$ by
\begin{align}
	%S_{T}(F_{I}) = 
	f(o_{cls}) \approx \tilde{f}(o_{cls}) \triangleq \sum_{c}w_{c}o_{cls}[c] = 
	w o_{cls}^{\mathsf{T}}
	% \\
%	\widetilde{S_{T}(F_{I})}=\sum_{c}w_{c}o_{cls} = \widetilde{f(o_{cls})},
	\label{eq:appr_s}
\end{align}
\abc{where $w_c$ is the weight for the $c$-th channel, and $w=[w_c]_c \in \real^C$ the corresponding weight vector for all channels.}
%\NOTE{the notation here is not precise, so I changed $w=[w_c]$ are the channel weights, and $\lambda = [\lambda_i]_i$ are the spatial weights. You need to update Fig 2.} \zcyNOTE{Done.}
%and $\dfrac{\partial \widetilde{f(o_{cls})}}{\partial o_{cls}}=w_{c}$. Then, let's watch the first derivatives, and set the optimization objective:
\abc{To obtain the channel weights $w$, we aim to match the first derivatives (gradients) of the original matching score $f$ and its approximation $\tilde{f}$, leading to the optimization problem:}
\abc{
\begin{align}
	w &= \mathop{\mathrm{argmin}}_{w}\big\| f'(o_{cls})-\tilde{f}'(o_{cls})\big\|^{2} \\
%	&= \mathop{\mathrm{argmin}} \sum_{c}\left( \dfrac{\partial f(o_{cls})}{\partial o_{cls}} - w_{c} \right) ^{2}.
	&= \mathop{\mathrm{argmin}}  \big\| \tfrac{\partial f}{\partial o_{cls}} - w \big\|^{2}, 
\end{align}
which has the solution
	\begin{align}
	w &= \tfrac{\partial f}{\partial o_{cls}} = \tfrac{\partial S_T(F_I)}{\partial o_{cls}} .
		\label{eq:w_c}
		\end{align}
Therefore, substituting (\ref{eq:out_cls}) and (\ref{eq:w_c}) into (\ref{eq:appr_s}), 
\begin{align}
	S_{T}(F_{I}) &\approx \sum_{c} w_{c}o_{cls}[c] \\
	&= \sum_{c} \tfrac{\partial S_{T}(F_{I})}{\partial o_{cls}[c]} \sum_{i} \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{ic} \\
	&= \sum_{i}  \Big[ \sum_{c} \tfrac{\partial S_{T}(F_{I})}{\partial o_{cls}[c]} \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})  v_{ic}\Big],
\end{align}
\abcn{where $v_{ic}$ is the c-th channel of $v_i$.}
Regarding $[v_{ic}]$ as the intermediate feature map, and $\lambda_{i} = \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})$, then the importance of the $i$-th spatial location is defined as:
%$i$-th pixel in the heat map  is defined as:
\begin{align}
	H_{i} = \relu\big( \sum_{c} w_{c}\lambda_{i} v_{ic}\big)
	\label{eq:hm}
\end{align}
where $\relu$ means that we only focus on positions that have a positive effect on the final score.
}

\NOTE{---- END OLD DERIVATION ---------------------------------}
}

%%% OLD derivation %%%%%%%%%%%%%
\CUT{ OLD derivation...
Then, with \textbf{only considering the last layer}, the matching score can be formed as:
\begin{align}
	S_{T}(F_{I}) = \sum_{c}F_{T}F_{I} = \sum_{c}F_{T}\calL(o_{cls}) = f(o_{cls}).
\end{align}
We make the approximation of $S_{T}(F_{I})$ by
\begin{align}
	\widetilde{S_{T}(F_{I})}=\sum_{c}w_{c}o_{cls} = \widetilde{f(o_{cls})},
	\label{old_eq:appr_s}
\end{align}
and $\dfrac{\partial \widetilde{f(o_{cls})}}{\partial o_{cls}}=w_{c}$. 
\begin{align}
	w &= \mathop{\mathrm{argmin}}_{w}\big\| f'(o_{cls})-\tilde{f}'(o_{cls})\big\|^{2} \\
	&= \mathop{\mathrm{argmin}} \sum_{c}\left( \dfrac{\partial f(o_{cls})}{\partial o_{cls}} - w_{c} \right) ^{2}.
\end{align}
With 
\begin{align}
	\dfrac{\partial \sum_{c}\left( \dfrac{\partial f(o_{cls})}{\partial o_{cls}} - w_{c} \right) ^{2}}{\partial w_{c}} 
	= \sum_{c} 2 \left( \dfrac{\partial f(o_{cls})}{\partial o_{cls}} - w_{c} \right) 
	= 0,
\end{align}
we can get the solution of $w_{c}$, 
\begin{align}
	w_{c} = \frac{1}{C} \sum_{c} \frac{\partial f(o_{cls})}{\partial o_{cls}} =  \frac{1}{C} \sum_{c} \frac{\partial S_{T}(F_{I})}{\partial o_{cls}}.
	\label{old_eq:w_c}
\end{align}
Therefore, substitute (\ref{old_eq:out_cls}) and (~\ref{old_eq:w_c}) to (\ref{old_eq:appr_s}), 
\begin{align}
	\widetilde{S_{T}(F_{I})} &=\sum_{c} w_{c}o_{cls} \\
	&=\sum_{i} \frac{1}{C} \sum_{c} \frac{\partial S_{T}(F_{I})}{\partial o_{cls}} \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}})v_{ic},
\end{align}
regarding the $v_{ic}$ as the intermediate feature map, and $w_{i} = \mathrm{softmax}(\tfrac{q_{cls}k_{i}^\mathsf{T} }{\sqrt{C}}) $, the flattened heat map can be defined as 
\begin{align}
	H_{i} = \relu\left( \sum_{c} w_{c}w_{i} v_{ic}\right)
	\label{old_eq:hm}
\end{align}
where $\relu$ means that we only focus on positions that have a positive effect on the final score.
}
%%%%%%%%%%%%%%5

The weight $w_{c}$ represents the importance of each feature channel, and $\lambda_{i}$ %shows that the softmax attention 
represents the importance of the values at each location via the softmax attention. However, from the visualization, we discover that the  output of the $\mathrm{softmax}$ self attention function is extremely sparse. Important information may be encoded in different locations, but the $\mathrm{softmax}$ only selects the largest activation, which is not appropriate as a spatial weight.
Therefore, we  replace the $\mathrm{softmax}$ with a ``loosened'' correlation by applying 0-1 normalization on the similarities $[q_{cls} k_i^{\mathsf{T}}]_i$, i.e., $\lambda_{i}\approx\varPhi (q_{cls}k_{i}^\mathsf{T})$, where $\varPhi$ is the 0-1 normalization function applied over the set of similarities. In the experiments and \S\ref{sec:spatial_weight}, we compare using the loosened $\lambda_{i}$ and without $\lambda_{i}$ to show the effect of spatial weights, qualitatively and quantitatively.

Therefore, with \emph{the channel importance $w_{c}$} and \emph{the spatial importance $\lambda_{i}$}, where
\begin{align} 
	w_{c}=\tfrac{\partial S_{T}(F_{I})}{\partial o_{cls}^{(0)}[c]}, \hspace{0.1in} \lambda_{i}=\varPhi (q_{cls}k_{i}^\mathsf{T}),
	\label{eq:weight}
\end{align}
we  obtain the proposed Grad-ECLIP explanation map $H = [H_{i}]_i$ \abcn{for the last layer}, where $H_i$ is defined in (\ref{eq:hm}) using the last layer values $v$ as the feature map. 

{\bf Visual and textual explanations:}
For the image encoder, the flattened heat map $H_{i}$ is reshaped and interpolated to the original image's height and width, while for text encoder, the heat (importance) value on the $i_{th}$ tokens is remapped to the original word position in the sentence. 
Finally, based on the approximation in (\ref{eqn:scoreaggregate}), the final explanation can be  \emph{aggregated over all the layers} by recursively processing each layer \abcn{to obtain its heat map from (\ref{eq:hm}).}
In the experiments, we use the last layer to explain the image encoder, and the last eight layers for interpreting the text encoder. 
The ablation study for the influence of different number of layers involved in image and text explanation is shown in \S\ref{sec:influnce_layers}. 

{\bf CNN-based CLIP:}
\zcy{%As for applying the proposed Grad-ECLIP to CNN-based CLIP, since 
Our proposed Grad-ECLIP can also be applied to CNN-based CLIP. 
The CNN-based CLIP model is composed of a ResNet backbone followed by an attention pooling. Thus, we can use the final attention layer in the pooling to conduct our explanation, which uses the same implementation as for ViT-based CLIP. The visualizations shown in \S\ref{sec:apply_cnn_clip} verify the effectiveness of Grad-ECLIP on CNN-based CLIP model.}


\begin{figure*}
	\vspace{-0.2cm}
	\begin{center}
		\includegraphics[width=0.85\textwidth]{figures/vis_comp.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Comparison of heat maps from:
		(a) the raw self-attention map in the last ViT \abc{layer}; 
		(b) Rollout \cite{abnar2020quantifying}; 
		(c) Grad-CAM \cite{selvaraju2017grad}; 
		(d) GAME \cite{chefer2021generic}; 
		(e) MaskCLIP \cite{zhou2022extract}; 
		(f) CLIPSurgery \cite{li2023clipsurgery};
		(g) M2IB \cite{wang2024visual};
		(h) RISE \cite{petsiuk2018rise}; 
		(i) our proposed Grad-ECLIP.	
	Visual explanations are provided for the matching score between the image and the specific text prompts, which can be nouns (\eg, car, dog) or verbs (\eg,  holding, standing). From the comparison of visualizations, Grad-ECLIP exhibits superior explanation ability on different types of text prompts. 
}
\label{fig:vis_comp}
\vspace{-0.3cm}
\end{figure*}


\begin{figure*}[!h]
\begin{center}
	\includegraphics[width=0.85\textwidth]{figures/vis_img_text.pdf}
\end{center}
\vspace{-0.2cm}
\caption{Explanations for image-text pairs from MS COCO using:
	%, sampled from MS COCO and the corresponding caption annotation, 
	by (a) raw self-attention;  Transformer interpretation methods (b) Rollout, (c) GAME; and our method (d) Grad-ECLIP. The importances of words are visualized by the degree of green color.
}
\vspace{-0.2cm}
\label{fig:vis_img_txt}
\end{figure*}


\vspace{-0.2cm}
\section{Experiments with Grad-ECLIP}\label{sec:exp}
\vspace{-0.1cm}

In this section we conduct experiments on Grad-ECLIP to: 
1) evaluate its visual explanation qualitatively and quantitatively, and compare with the current SOTA methods; 
2) evaluate the processing time;
3) \zcy{conduct ablation studies, including the effect of spatial weight, the involved layers and attention heads.}

\zcy{Unless otherwise specified,} % specific note,} 
we conducted the experiments with the ViT-B/16 architecture.
% We considered two versions of our approach: the full version of Grad-ECLIP using $w_i$ defined in (\ref{eq:weight}), and a version without $w_i$ (denoted as ``w/o $w_i$'') that replaces the proposed spatial weights with $w_i=1$.
% \zcynew{In this section, all ``w/o $w_{i}$'' marks mean $w_{i}=1$ setting.}
%, i.e., $w_{i}=1$ instead of the proposed spatial weights}
%
We compared with representative baseline XAI methods from each category: 
1) attention map-based \textit{Rollout} \cite{abnar2020quantifying}, which takes into account all the attention maps computed along the forward pass, and \textit{raw attention} in the last visual encoder layer, both of which are not result-specific explanation; 
2) classical gradient-based method \textit{Grad-CAM} \cite{selvaraju2017grad}, which takes the image-text similarity as target and calculate the gradients w.r.t. the ViT layer output; 
3) relevance-based \textit{GAME} \cite{chefer2021generic}, which integrates the relevancies and gradients propagated through the network; 
4) cosine-based \textit{MaskCLIP} \cite{zhou2022extract} and \textit{CLIPSurgery} \cite{li2023clipsurgery}, which generates similarity value on each location by the cosine between text feature and processed values as local image features; 
5) M2IB \cite{wang2024visual}, which applies information bottleneck principle to generate explanation map for CLIP.
% in their method is also applied in our comparison experiments. 
Each baseline is built with different properties and assumptions over the architecture. We also show visualization comparisons with the typical black-box perturbation method RISE \cite{petsiuk2018rise}, but did not conduct quantitative comparisons with black-box perturbation and Shapely methods, due to their computational complexity and inherent differences with our proposed approach.


\begin{figure*}[thp]
	%		\vspace{-6pt}
	\centering
	\begin{center}
		\includegraphics[width=0.85\textwidth]{figures/vis_comp_domain.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Comparison of visual explanations for different methods on image samples from different image domains: \abc{natural images (ImageNet),  renditions (ImageNet-R), pencil sketch (ImageNet-Sketch), natural adversarial examples (ImageNet-A), web images with captions (Conceptual Captions), and chest X-ray (MSCXR). On MSCXR, explanations are provided for both the OpenAI CLIP model and a fine-tuned version.}}
	\vspace{-0.2cm}
	\label{fig:vis_domains}
	\vspace{-0.2cm}
\end{figure*}


\vspace{-0.1cm}
\subsection{Qualitative evaluation}
\label{exp:qual_eval}
\vspace{-0.1cm}

\zcy{In this section, we evaluate the proposed Grad-ECLIP qualitatively. 
%Concretely, t
The comparisons of visual explanations from different types of methods are presented in \S\ref{sec:vis_comp}, and the visual explanations on both image and text encoder with image-sentence pair examples are preseneted in \S\ref{sec:vis_img_sentence}. Then, we conduct the visualizations of explanations on diverse image domains %on diverse datasets of different domains 
in \S\ref{sec:vis_domain} and adapt the Grad-ECLIP to other Vision Language Models (VLMs) in \S\ref{sec:adapt_vlms}. Finally, we show that the Grad-ECLIP is applicable to CNN-based CLIP with ResNet backbone in \S\ref{sec:apply_cnn_clip}.} 


\vspace{-0.1cm}
\subsubsection{Comparison of visual explanations}
\label{sec:vis_comp}
%\vspace{-0.1cm}

We compare the visualizations of raw self-attention, Rollout, Grad-CAM, GAME, MaskCLIP, CLIPSurgery, M2IB, RISE and our Grad-ECLIP 
%(w/ or w/o $w_{i}$ in Eq.~\ref{eq:hm}) 
in Fig.~\ref{fig:vis_comp} with  images from ImageNet \cite{russakovsky2015imagenet} and MS COCO \cite{lin2014microsoft}. Except raw attention and Rollout, which are defined to be text-agnostic, the others are all text-specific, so we test the same image with two different text inputs on MS COCO. 
%As can be seen, 
Our Grad-ECLIP demonstrates a strong ability of generating clear and distinct text-specific heat maps, and gives reasonable explanation of verbs for interpreting CLIP. For example, the highlights for ``holding'' focus around the person's hands (the 5th row of Fig.~\ref{fig:vis_comp}i), while ``standing'' highlights the person's legs (the 6th row of Fig.~\ref{fig:vis_comp}i). We also notice that the sticks in the background are highlighted, which is probably because the sticks are regarded as ``standing'' in the snow. 

%Compared with the full Grad-ECLIP, the version w/o $w_i$ contains more noise near object boundaries and on the background (Fig.~\ref{fig:vis_comp}j), but are otherwise consistent with full Grad-ECLIP. The result of using $w_{i}=\mathrm{softmax}$ (Fig.~\ref{fig:vis_comp}i) is equivalent to raw attention (Fig.~\ref{fig:vis_comp}a). 
In contrast to our methods, Grad-CAM and MaskCLIP can produce highlights on the explained object, but both also generate significant noise. CLIPSurgery tends to put high and coarse attention on the object region, but also contains background noises. M2IB and MaskCLIP fail when the texts are verbs (``holding'' and ``standing''), while RISE performs the worst with interpreting CLIP model. The results of GAME and Rollout, which are both based on self-attentions of the model, generate confusing heat maps due to the sparse attention between tokens in some layers. 

\vspace{-0.1cm}
\subsubsection{Explanations on image-sentence pairs}
\label{sec:vis_img_sentence}
%\vspace{-0.1cm}

The explanation map from Grad-ECLIP can also be generated from text encoder viewpoint. 
Using the gradient of matching score and the feature embeddings of word tokens, Grad-ECLIP can show the importance of each word in the given sentence when matching with an image. Fig.~\ref{fig:vis_img_txt} shows example visual and textal explanations for image-text pairs from MS COCO.
Although Rollout and GAME can highlight important words in the sentence, Grad-ECLIP is the only one showing good correspondence between image attention regions and important words. From the explanation of the sentence, we can identify which words are more important for CLIP when matching with the specific image, and correspndingly the text-specific important regions on the image are shown in the visual explanation. This word importance visualization of the input text can be helpful when designing text prompts  for image-text dual-encoders  in practical applications. 


\vspace{-0.1cm}
\subsubsection{\zcy{Visualization examples on diverse image domains}} \label{sec:vis_domain}
%\vspace{-0.1cm}

We show the visualization comparison of different methods on the samples from different image domains, including the original ImageNet and ImageNet in different domains: rendition  (ImageNet-R \cite{hendrycks2021many}), pencil sketch (ImageNet-Sketch \cite{wang2019learning}), natural adversarial example (ImageNet-A \cite{hendrycks2021natural}), web images with captions (Conceptual Captions (CC) \cite{sharma2018conceptual}), and chest x-ray with text (MSCXR \cite{boecking2022making}) in Fig.~\ref{fig:vis_domains}. For the image-caption pairs from the web-collected CC and chest X-ray data MSCXR, we generate explanations for both image and text encoder, and compare with the other methods that also provide text encoder explanations, including the raw attention, Rollout, GAME, M2IB. 

Our Grad-ECLIP explanations provide interesting insights into how CLIP handles different image domains. In Fig.~\ref{fig:vis_domains} (top), given a normal banana image and text ``banana'', Grad-ECLIP reveals that the yellow color is dominant to CLIP. However, when given a pencil sketch without color (ImageNet-Sketch), Grad-ECLIP reveals that CLIP looks at the curvature of the banana. For the color sketch of the banana (ImageNet-R), Grad-ECLIP shows that the color of the banana is mainly used, and not the black curved lines. Thus, from these examples, we may infer that CLIP prefers using the yellow color over the curved shape for matching with the ``banana'' text. 
%
%\NOTE{didn't say anything about the cowboy example in CC.  Anything interesting to discuss?}
%
\zcyy{With the sample of a web image and caption in the CC dataset, Grad-ECLIP generates clear and reasonable visual and the corresponding textual explanation map, showing that ``cowboy'', ``horses'' and ``desert'' are the main used concepts in the matching (from high to low importance).}

Grad-ECLIP also provides interesting insights on how the original CLIP fails on novel domains.   The last 2 rows of Fig.~\ref{fig:vis_domains} show the explanations for chest x-ray images and text for the OpenAI CLIP model and a fine-tuned CLIP model (on MSCXR).   The Grad-ECLIP explanation shows that the original CLIP uses the whole lobe to match with the words ``defined'' and ``lobe''. In contrast, the fine-tuned CLIP locates the actual anomaly and matches it with the text ``defined opacities largely''. The reason is that the fine-tuned model is trained to the specific domain that matches the X-ray and the illness location descriptions, while the original OpenAI CLIP model is more general and apparently ``lobe'' is the key word and the main object in the image-text pair. 


\begin{figure}[t]
	%		\vspace{-6pt}
	\centering
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/adapt_vlms.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Grad-ECLIP visual explanations of the ViT classifier and BLIP.}
	\label{fig:vis_adapt}
	\vspace{-0.2cm}
\end{figure}

\CUT{
\begin{table}[t]
	\captionof{table}{Evaluation of Grad-ECLIP on the deletion/insertion perturbation metric for ViT-based classifier.}
	\label{tab:exp_vit}
	\vspace{-0.1cm}
	\centering
	\scriptsize
	\setlength\tabcolsep{1.0pt}    
	\renewcommand\arraystretch{1.2}
	\begin{tabular}{@{}l|cccccc@{}}
		\hline
		Method & Grad-CAM\cite{selvaraju2017grad} & ViT-cx\cite{xie2022vit} & T-Attr\cite{chefer2021transformer} & Bi-Attn\cite{chen2022beyond} & TIS\cite{englebert2023explaining} & Grad-ECLIP(ours)\\  \hline
		Deletion$\downarrow$  & 0.241 & 0.236 & 0.232 & 0.218 & 0.196  & \textbf{0.174}\\       
		Insertion$\uparrow$ & 0.737 & 0.722 & 0.741 & 0.760 & 0.761 & 0.727 \\   
		\hline
	\end{tabular}
	\vspace{-0.1cm}	
\end{table}
}

\vspace{-0.1cm}
\subsubsection{\zcy{Explaining ViT classifier and BLIP with Grad-ECLIP}} \label{sec:adapt_vlms}
\vspace{-0.1cm}

Since our explanation method is designed for CLIP encoders, which are Transformer-based, our method can be easily adapted to generate visual explanations for other Transformer-based models.
Here we adapt Grad-ECLIP to ViT-based classifier \cite{dosovitskiy2020image} and BLIP \cite{li2022blip} to show the generality of our method.
%\NOTE{anything special that needs to be done for this extension?} \zcyNOTE{I use the same code and settings to do the deletion and insertion for our method as the paper you give, ``Metric-driven attributions for vision transformers'', and copy the results of other compared methods.}

\zcyy{For explaining ViT-based classifier, the classification score on the corresponding category is used to calculate the gradient and generate the heat map. From the examples for ViT classifier in Fig.~\ref{fig:vis_adapt}, we can see that although there are some noises on the background, Grad-ECLIP can well mark out the important region on the image for the specific class. 
% since we don't have the actual images they use, then I cut this part and we can add it later if the reviewers ask for it. At that time, maybe the code from that paper will be released. 
\CUT{We also conduct the perturbation experiment with the code provided by the repository for deletion/insertion perturbation metric \cite{petsiuk2018rise} and compare with other explanation method for ViT-based classifier. The definition of deletion and insertion metric can be found in the quantitative evaluation section \S\ref{sec:del_ins}. Concretely, in this experiment, ViT-base 16x16 model is used and all input images are 224x224px, with performing a total of 224 perturbation steps and a step size of 224px. Then, the classification score changes are recorded along with the steps for calculating AUC (area under curve) as the deletion or insertion result. The results in Tab.~\ref{tab:exp_vit} compare with other related ViT explanation methods over 5000 ImageNet images with 5 images per class.
\zcyNOTE{Here we don't know how they sample the 5 images in one class. I use range(0,50,10) function. So, we may use the different images.}
From the quantitative evaluation, when adapting Grad-ECLIP to ViT-classifier, our method have an excellent performance with the best deletion result and a comparable insertion result, which indicates that the high responses on the heat map can successfully cover the important region for classifying the image.}}
 As for the VLMs, shown by the visual explanation results presented in Fig.~\ref{fig:vis_adapt}, when matching the same image-text pair, different models put attention on different regions. For example, BLIP notes the fins, while CLIP notes the fish body to match the image to ``tench''. When matching with the sentence ``a dog is playing with a frisbee'', BLIP puts more importance on the dog on the image, while CLIP shows places more importance on the frisbee.

Other VLMs like ALBEF \cite{li2021align} add additional attention layers after the encoders to fuse the image and text features, and thus our current method is not directly applicable since our method assumes that the last layer attention output has linear relationship with the final feature embedding. Our future work will investigate adapting our method to these modified ViT frameworks, e.g., the ALBEF model that uses cross attention to fuse image and text. Nonetheless, our ability to explain CLIP and other VLMs with similar architecture is significant considering that CLIP is by far the most widely used VLM.


\begin{figure}[t]
	%		\vspace{-6pt}
	\centering
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/app_vis_cnn.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Visual explanations of the CNN-based CLIP with ResNet50 architecture using Grad-ECLIP and other visual explanation methods.}
	\label{fig:vis_cnn}
	\vspace{-0.2cm}
\end{figure}

\vspace{-0.1cm}
\subsubsection{\zcy{Applying Grad-ECLIP to explain CNN-based CLIP}} \label{sec:apply_cnn_clip}
%\vspace{-0.1cm}

Although the methodology in \S\ref{sec:grad-eclip} for Grad-ECLIP is derived from  CLIP with Transformer architecture, here we show that our method is also applicable to CNN-based CLIP by using the attention layer in the final attention pooling. Figure~\ref{fig:vis_cnn} shows the visual explanation results for ResNet50-CLIP using Grad-ECLIP, and compared to other explanation methods that are compatible with CNN-based CLIP, including Grad-CAM, CLIPSurgery, and RISE. 
%
From the examples, 
%By showing the saliency map for two objects and two sentences, we presents that 
our Grad-ECLIP is able to generate clear and reasonable text-specific heat maps for interpreting CNN-based CLIP. In contrast to our method, Grad-CAM can produce similar highlight on the explained objects, but its heat map is more noisy in the background. Similar to its performance on ViT-based CLIP, CLIPSurgery generates rougher heat maps, which tend to put high values on a coarse region of the object. Meanwhile, RISE fails to explain CLIP model.


\begin{figure*}[thp]
	\begin{minipage}{.55\linewidth}
		\vspace{-0.1cm}	
		\captionof{table}{Faithfulness evaluation of \textbf{image} explanation on the \textit{ImageNet} validation set: AUC for Deletion and Insertion curves, based on  Top-1 (@1) or Top-5 (@5) classification accuracy. Either the ground-truth or the prediction are used as the text input into CLIP. The second best is shown with underline.}
		\vspace{-0.1cm}
		\label{tab:del_ins_imagenet}
		\centering
		\footnotesize
		\setlength\tabcolsep{3.0pt}    
		\renewcommand\arraystretch{1.2}  
		\begin{tabular}{@{}l|cc|cc|cc|cc@{}}
			\hline
			
			& \multicolumn{4}{c|}{Deletion$\downarrow$} & \multicolumn{4}{c}{Insertion$\uparrow$} \\ 
			& \multicolumn{2}{c|}{Ground-truth} & \multicolumn{2}{c|}{Prediction} & \multicolumn{2}{c|}{Ground-truth} & \multicolumn{2}{c}{Prediction} \\
			Method & @1 & @5 & @1 & @5  & @1 & @5 & @1 & @5  \\ 
			\hline
			raw attention  & 0.3831 & 0.6239  & - & - & 0.2492 & 0.4195 & -  & - \\ 
			Rollout & 0.4082 & 0.6556 & - & - & 0.2803 & 0.4665 & - & - \\  
			Grad-CAM  & 0.3417 & 0.5628 & 0.3518 & 0.5817 & 0.2682 & 0.4454 & 0.2526 & 0.4206 \\ 
			GAME  & 0.3356 & 0.5734 & 0.3497 & 0.5938 & 0.3611 & 0.5636 & 0.3425 & 0.5384 \\ 
			MaskCLIP  & 0.2848 & 0.4885 & 0.2886 & 0.4957 & 0.3335 & 0.5351 & 0.3275 & 0.5267 \\
			CLIPSurgery & 0.3115 &	0.5235 & 0.3217 & 0.5412&	0.3832	& \textbf{0.6021} &	\textbf{0.3727} & 0.5719\\
			M2IB & 0.3630 &	0.5953 & 0.3633	& 0.5951 & 0.3351 &	0.5411 & 0.3347 & 0.5410 \\
			Ours w/o $\lambda_{i}$ & \underline{0.2535} & \underline{0.4379} & \underline{0.2634} & \underline{0.4568} & \underline{0.3715} & 0.5831 &  0.3528 & \underline{0.5556} \\
			Ours  & \textbf{0.2464} & \textbf{0.4272} & \textbf{0.2543} & \textbf{0.4420}  & \textbf{0.3838} & \underline{0.5993} & \underline{0.3672} & \textbf{0.5749}  \\
			\hline
		\end{tabular}
	\end{minipage}
	\hspace{0.1cm}
	\begin{minipage}{.45\linewidth}
		\centering
		\begin{center}
			\includegraphics[width=0.92 \textwidth]{figures/imagenet_del_ins_new.pdf}
		\end{center}
		\vspace{-0.1cm}
		\caption{Classification accuracy at Top-1 vs. (a) Deletion steps and (b) Insertion steps, on the \textit{ImageNet} validation dataset with visual explanation heat maps from our Grad-ECLIP (solid) and other methods (dash).}
		\vspace{-0.1cm}
		\label{fig:faithfulness}
	\end{minipage}
\end{figure*}




\begin{figure*}[th]
	\begin{minipage}{.6\linewidth}
		\vspace{-0.1cm}	
		\captionof{table}{Evaluation of \textbf{image} explanation faithfulness on \textit{MS COCO image-text retrieval (Karpathy's split)} validation dataset: AUC for Deletion and Insertion curves for performance on  image retrieval (IR) and text retrieval (TR) tasks.}
		\label{tab:del_ins_mscoco}
		\centering
		\footnotesize
		\setlength\tabcolsep{3.5pt}  
		\renewcommand\arraystretch{1.2}  
		\vspace{-0.1cm}	
		\begin{tabular}{@{}l|cc|cc|cc|cc@{}}
			\hline
			
			 & \multicolumn{4}{c|}{Deletion$\downarrow$} & \multicolumn{4}{c}{Insertion$\uparrow$} \\ 
			 & \multicolumn{2}{c|}{IR} & \multicolumn{2}{c|}{TR} & \multicolumn{2}{c|}{IR} & \multicolumn{2}{c}{TR} \\ 
			Method & @1 & @5 & @1 & @5 & @1 & @5 &  @1 & @5   \\ \hline
			raw attention  & 0.1708 & 0.3554 & 0.1923 & 0.3720  & 0.1247 & 0.2552 & 0.1544 & 0.2969 \\       
			Rollout   & 0.1948 & 0.3946 & 0.2268 & 0.4238  & 0.1294 & 0.2932 & 0.1753 & 0.3503 \\ 
			Grad-CAM   & 0.1717 & 0.3502 & 0.2161 & 0.4008 & 0.1027 & 0.2216 & 0.1152 & 0.2327  \\
			GAME  & 0.1706 & 0.3552 & 0.1982 & 0.3800 & 0.1537 & 0.3083 & \textbf{0.2097} & 0.3735 \\  
			MaskCLIP   & 0.1321 & 0.2841 & \textbf{0.1516} & 0.2949  & 0.1423 & 0.2953  & 0.1891 & 0.3514  \\ 
			CLIPSurgery & 0.1794 & 0.3652 & 0.2381 & 0.4292 & 0.1419 & 0.2941 & 0.1771 & 0.3384 \\
			M2IB & 0.1797 & 0.3671 & 0.2057 & 0.3905 & 0.1469 & 0.3004 & 0.2058 & 0.3691 \\
			Ours w/o $\lambda_{i}$ & 0.1390  & 0.2940  & 0.1827 & 0.3386 & 0.1403 & 0.2895 & 0.1735 & 0.3279  \\ 
			Ours    & \textbf{0.1246} & \textbf{0.2670} & 0.1550 & \textbf{0.2933}  & \textbf{0.1576} & \textbf{0.3203}  & 0.2056 & \textbf{0.3761} \\ 
			\hline
		\end{tabular}
	\end{minipage}
	\hspace{0.1cm}
	\begin{minipage}{.35\linewidth}
		\captionof{table}{Evaluation of \textbf{text} explanation faithfulness on \textit{MS COCO image-text retrieval (Karpathy's split)} validation dataset: AUC for Deletion and Insertion curves with reporting image retrieval (IR) and text retrieval (TR) performance.}
		\label{tab:del_ins_text_mscoco}
		\vspace{-0.1cm}	
		\centering
		\footnotesize
		\setlength\tabcolsep{4.0pt}    
		\renewcommand\arraystretch{1.2}
		\begin{tabular}{@{}l|cc|cc@{}}
			\hline
			& \multicolumn{2}{c|}{Deletion$\downarrow$} & \multicolumn{2}{c}{Insertion$\uparrow$} \\ 
			Method & IR & TR & IR & TR \\ \hline
			raw attention  & 0.2843 & 0.4917  & 0.0065 & 0.0328     \\       
			Rollout   & 0.1221 & 0.2389 & 0.1052 & 0.2070 \\ 
			GAME  & 0.1083 & 0.2084 &  0.1146  &  0.2301 \\  
			M2IB  & 0.2139 &	0.4256 &	0.0063 &	0.0375 \\
			Ours w/o $\lambda_{i}$ & 0.1116  & 0.2113 & 0.1123 & 0.2361 \\ 
			Ours    & \textbf{0.0996} & \textbf{0.1770} & \textbf{0.1292} & \textbf{0.2536}\\ 
			\hline
		\end{tabular}	
	\end{minipage}
\end{figure*}



\vspace{-0.1cm}
\subsection{Quantitative evaluation}
\label{exp:quan_eval}
\vspace{-0.1cm}
In this section, we perform quantitative evaluations of Grad-ECLIP comparing with baselines.  
In \S\ref{sec:del_ins}, 
the explanation faithfulness is evaluated by the Deletion and Insertion metrics \cite{samek2016evaluating,chattopadhay2018grad,wang2020score,wang2020ss,petsiuk2021black}, which are also called perturbation tests \cite{chefer2021transformer, chefer2021generic}. 
Moreover, in \S\ref{sec:pg_segtest},  we evaluate localization ability, when considering each visualization as a soft-segmentation of the image, using PointGame \cite{zhang2018top, chenyang2022odam} and segmentation tests \cite{chefer2021transformer}. \zcy{Finally, in \S\ref{sec:process_time}, we evaluate the processing time of Grad-ECLIP compared with other visual explanation methods. 
%
In this section, in order to understand the effect of the spatial importance term in (\ref{eq:hm}), we also present Grad-ECLIP without using the spatial weight $\lambda_i$, i.e., setting $\lambda_i=1$, which is denoted as 
%In these quantitative evaluations, we also provide and compare the results of Grad-ECLIP that replaces the spatial weight $w_{i}$ by $1$, which is denoted as 
``w/o $\lambda_{i}$''.}

% And the visualization and more demonstration of the ablation study about the spatial weight can be seen in \S\ref{sec:spatial_weight}.}


\vspace{-0.1cm}	
\subsubsection{Deletion and Insertion}\label{sec:del_ins}
\vspace{-0.1cm}

A faithful explanation method should produce heat maps highlighting the important content in the image that has  greatest impact on the model prediction. Deletion (negative perturbation) replaces input image pixels by random values step-by-step with the important pixels removed first based on the ordering of the heat map values, while recording the drop in prediction performance.  Insertion adds image pixels to an empty image step-by-step based on the heat map importance, and records the performance increase. \abc{For deletion, larger drops are better, while for insertion larger increase is better.}  We consider each step as 0.5\% of number of image pixels, and record results for $100$ steps. The model performance is measured using top-1 or top-5 zero-shot classification accuracy on the validation set of %
%he top-1 zero-shot classification performances changing along with steps on the validation set of 
ImageNet \cite{russakovsky2015imagenet} (ILSVRC) 2012, consisting of 50K images from 1000 classes.
\abcn{In particular, the perturbed image and each of the class names is fed into CLIP, and then classes with the highest scores are selected.}
%\NOTE{how to do this since they are VLMs? put in the 1000 classes and select the classes with highest score?} \zcyNOTE{Yes.}

The insertion/deletion curves for top-1 accuracy are presented in Fig.~\ref{fig:faithfulness}, and the corresponding area under the curve (AUC) with top-1 and top-5 accuracy are presented in Tab.~\ref{tab:del_ins_imagenet}.
Steeper drop of performance with deletion steps corresponds to a lower deletion AUC, while quicker increase of performance with insert steps outputs a higher insertion AUC.  Our method obtains the fastest performance drop for Deletion and largest performance increase for Insertion compared with most related works, showing that regions highlighted in our heat maps better represent explanations of CLIP.  CLIPSurgery has comparable results to ours for Insertion, while performs poorly when evaluated with Deletion. The reason is that CLIPSurgery exhibits heat maps with nearly the same high values % and high heat map values 
on the explained target region, so that the deletion operation fails to delete the most important pixels on the image at the beginning steps, which causes the deletion curve to decrease gradually, producing the high deletion AUC. Since CLIPSurgery  can locate the explanation target with high values on the heat map, it performs well in the Insertion test. Our method without using the %loosened attention 
\zcyy{spatial importance} (w/o $\lambda_{i}$) has slightly worse performance, but is still better than other baselines. 
%\NOTE{unclear. saying "loosened attention'' implies that you are using the normal softmax attention. But actually, you are just removing the spatial importance altogether. Is it correct?} \zcyNOTE{Yes. I change the loosened attention to spatial importance.}
As with \cite{chefer2021transformer, chefer2021generic}, we also use both the ground-truth class and the predicted class as the text prompt to generate heat maps, and our method is consistent with them, showing gains when using ground-truth text prompts. 


\abc{We next evaluate the Deletion and Insertion performance for image and text retrieval tasks on the Karpathy's validation split of MS COCO.
To evaluate the image explanations,} \zcyy{image pixels are removed or added step-by-step based on the text-specific explanation heat map. With the modified image replacing the original image, we record the image and text retrieval performance (recall @ top-1 and top-5 matching) to draw the deletion and insertion curve.}
%\NOTE{unclear how this is done. for image retrieval, are we trying to retrieve the same image? for text retreival, are we truing to retrieve the corresponding text? What metric is being used? recall \@ X?} \zcyNOTE{For image retrieval, we are trying to retrieval the modified image with the corresponding text. And for text retrieval, we are trying to use the modified image to find the corresponding text.}
Then, the AUC results of Deletion and Insertion with text-specific image explanations are reported in Tab.~\ref{tab:del_ins_mscoco}. 
%\NOTE{anything particular to say?}
\zcyy{Grad-ECLIP surpasses the other methods on most metrics, which further demonstrates that our method produce high-quality visual explanation, regardless if the text is the class categories as in ImageNet or long captions as in MS COCO.}

Finally, we evaluate \textit{the faithfulness of our text explanations} using the \textit{text version} of Deletion and Insertion metric, % is conducted on explanation for text, 
where words are deleted or inserted based on the order of importance in the text heat map. Using images and caption annotations in MS COCO Karpathy's split, we record the image-text retrieval performance  \zcyy{for the modified caption}, changing with total 5 steps with one word deleted/inserted at a time. 
The results in Tab.~\ref{tab:del_ins_text_mscoco} show that Grad-ECLIP has the highest faithfulness (best deletion and insertion scores) compared with the other Transformer explanation methods. This demonstrates that Grad-ECLIP also has the excellent ability for image-specific text explanations. 

%The results on MS COCO further demonstrate that our method produces high-quality visual and textual explanations of the image and text encoder, for the specific image and text pair, regardless if the text is the class categories as in ImageNet or long captions as in MS COCO.



\vspace{-0.1cm}
\subsubsection{Point Game and Segmentation Test}\label{sec:pg_segtest}
\vspace{-0.1cm}

We next evaluate the localization ability of the visual explanations. 
We adopt the ImageNet-Segmentation (ImageNet-S) \cite{gao2022luss} validation set, which has segment annotations on 12,419 images of 919 categories from ImageNet. Point Game (PG) is a commonly used metric to evaluate the localization correctness of a visual explanation. PG counts a hit score if the location with the largest value in the %text-specific
visual explanation
 heat map lies within the object region, which can be defined by the class segmentation mask. Then the PG accuracy is measured by averaging over all samples. Since PG only considers the maximum point, but not the spread of the heat map, we also conduct energy-PG \cite{wang2020score}, which calculates the proportion of heat map energy within the ground-truth mask versus the whole map. Similar to the evaluation by \cite{chefer2021transformer, chefer2021generic}, regarding the heat maps as soft-segmentation results, we adopt pixel accuracy (Pixel Acc.), average precision (AP), and averaged mask intersection over union (maskIoU) as additional metrics. %evaluation metrics.

The results for localization are shown in Tab.~\ref{tab:image_seg}. 
Both versions of Grad-ECLIP significantly outperform other explanation methods on PG and energy-PG,
% especially on PG, 
which demonstrates that Grad-ECLIP can well reveal that the important pixels for CLIP are inside the object region.
% % object regions  of CLIP on the object with 
%when the correct category is provided as the text prompt. 
Comparing Grad-ECLIP with and without $\lambda_i$, Grad-ECLIP without $\lambda_{i}$ obtains relatively higher performance  on pixel accuracy and maskIoU, since heat maps that contain more high-value pixels within the ground-truth mask have advantage on these two metrics. In Fig.~\ref{fig:w_i}(b,c), using $\lambda_{i}$ reduces the values on the mask while removing the surrounding noise. Due to the similar reason, CLIPSurgery obtains higher pixel accuracy and maskIoU, since it tends to put high heat map values on all the pixels of object region, and gets higher score when aggregating the heatmaps inside the object mask in these two evaluations. However, the lower PG, energy-PG and AP demonstrate that there are more high values generated outside of the object boundary. Better segmentation does not necessarily result in faithful explanations, in terms of both insertion and deletion metrics, as indicated in Table~\ref{tab:del_ins_imagenet}.

\begin{table}[!h]
	\captionof{table}{Evaluation of localization ability using the Point Game (PG and energy-PG) and Segmentation test (Pixel Acc., AP and MaskIoU) on the \textit{ImageNet-S} validation dataset.}
	\label{tab:image_seg}
	\vspace{-0.1cm}
	\centering
	\footnotesize
	\setlength\tabcolsep{3.5pt}    
	\renewcommand\arraystretch{1.2}
	\begin{tabular}{@{}l|cc|ccc@{}}
		\hline
		Method & PG & energy-PG & Pixel Acc. & AP & maskIoU  \\  \hline
		raw attention  & 0.1219 & 0.1321 & 0.0278 & 0.2877 & 0.0013 \\       
		Rollout & 0.1375 & 0.2835 & 0.2524 & 0.3345 & 0.011 \\   
		Grad-CAM & 0.1845 & 0.3154 & 0.5457 & 0.4050 & 0.1251 \\
		GAME & 0.4706 & 0.4438 & 0.4765 & 0.4072 & 0.089 \\   
		MaskCLIP & 0.4041 & 0.1408 & 0.718 & 0.4557 &  0.2481 \\  
		CLIPSurgery	& 0.5759 &	0.3983 &	\textbf{0.7546} &	0.4608 &	\textbf{0.3471} \\
		M2IB &	0.264 &	0.3557 &	0.6194 &	0.4003 & 	0.1474 \\
		Ours w/o $\lambda_{i}$ & \underline{0.8356} & \underline{0.4409} & \underline{0.7365} & \underline{0.5163} & \underline{0.3314} \\  
		Ours & \textbf{0.8899} & \textbf{0.5997} & 0.7056 & \textbf{0.5662} & 0.2869 \\  
		\hline
	\end{tabular}
	\vspace{-0.1cm}	
\end{table} 



\begin{table*}[!h]
	\vspace{-0.1cm}
	\captionof{table}{Comparison of the average processing time (on RTX3090 GPU) per image for generating the explanation map.}
	%	 \zcyy{All methods are processed on the RTX 3090 GPU.}
	%	\NOTE{what GPU is used?}
	%}
\label{tab:proc_time}
\vspace{-0.1cm}
\centering
\footnotesize
\setlength\tabcolsep{3.5pt}    
\renewcommand\arraystretch{1.2}
\begin{tabular}{@{}l|ccccccccc@{}}
	\hline
	Method & raw attention & Rollout & Grad-CAM & GAME & MaskCLIP & CLIPSurgery & M2IB & RISE & Grad-ECLIP(Ours)  \\  \hline
	time (s/img)  & 0.0117 & 0.0298 & 0.0114 & 0.0228 & 0.0117 & 2.9423 & 0.5781 & 6.2376 & 0.0165 \\       
	\hline
\end{tabular}
\vspace{-0.2cm}	
\end{table*} 

\vspace{-0.1cm}
\subsubsection{Processing time comparison}\label{sec:process_time}
\vspace{-0.1cm}

In  Tab.~\ref{tab:proc_time}, we show the average processing time per image, which counts the total duration from inputting the image and text into CLIP to obtaining the explanation map. Since the gradient can be easily and quickly obtained through the autograd function of Pytorch, both our method and Grad-CAM takes similar processing time as the raw attention and MaskCLIP, which obtain their heat maps from the forward pass of the model and some other minor operations. Note that for gradient-based methods, the backpropagation does not need to go all the way to the input layer, but stops at an intermediate upper layer, and thus the extra computation required is not much. RISE needs the longest processing time, which is a  common drawback of perturbation-based methods.


\vspace{-0.1cm}
\subsection{Ablation study}
\vspace{-0.1cm}

In this section, we conduct ablation studies to illustrate the influence of the proposed loosened spatial weight (\S\ref{sec:spatial_weight}), the number of involved layers (\S\ref{sec:influnce_layers}) and multi attention heads (\S\ref{sec:influence_heads}) in the calculation of Grad-ECLIP.

\vspace{-0.2cm}
\subsubsection{\zcy{Effect of the loosened spatial weight}}
\label{sec:spatial_weight}

%\NOTE{the two versions of w/o is too confusing. To be consistent, I changed "w/o" to only mean $\lambda_i=1$, really without the spatial importance.} \zcyNOTE{OK.}

Here we conduct ablation study on the $\lambda_{i}$ in Eq.~\ref{eq:weight} to show the effect of the proposed spatial weight. 
\abc{We consider two versions of Grad-ECLIP with modified spatial importance $\lambda_i$: the first version uses the softmax attention rather than 0-1 normalization (denoted as $\lambda_i=\mathrm{softmax}$); the second version removes the spatial importance completely by setting $\lambda_i=1$.}

% (denoted as ``w/o $\lambda_i$'') that replaces the proposed one with a version that uses the original  softmax ($\lambda_{i}=\mathrm{softmax}$) and removing it entirely ($\lambda_{i}=1$). 

A comparison of visualizations is presented in  Fig.~\ref{fig:w_i}.
Compared with the heat maps generated by the full Grad-ECLIP in Fig.~\ref{fig:w_i}c, the version that removes spatial importance altogether ($\lambda_{i}=1$) contains more noise near object boundaries and on the background (Fig.~\ref{fig:w_i}b), but are otherwise consistent with full Grad-ECLIP. The result of using $\lambda_{i}=\mathrm{softmax}$ (Fig.~\ref{fig:w_i}a) is equivalent to raw attention (Fig.~\ref{fig:vis_comp}a) due to the output of the $\mathrm{softmax}$ being extremely sparse. We also provide the quantitative comparisons of Grad-ECLIP using $\lambda_{i}=1$  in the quantitative evaluation oin \S\ref{sec:del_ins} and \S\ref{sec:pg_segtest}, denoted as ``w/o $\lambda_i$''.



\begin{figure}
	\vspace{-0.1cm}
	\centering
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/vis_wi.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The effect of spatial weight $\lambda_{i}$. We compare the explanation maps of Grad-ECLIP with a version without the proposed $\lambda_i$ (denoted as ``w/o $\lambda_i$'', which replaces the proposed spatial weights with (a) $\lambda_i=\mathrm{softmax}$, (b) $\lambda_i=1$), and (c) the full Grad-ECLIP.}
	\vspace{-0.2cm}
%	\NOTE{need to update the figure captions to change $w_i$ and remove "w/o $w_i$".} \zcyNOTE{Done.}}
	\label{fig:w_i}
\end{figure}




\vspace{-0.2cm}
\subsubsection{\zcy{Effect of number of layers on visual explanation}}
\label{sec:influnce_layers}
As introduced in \S\ref{sec:grad-eclip}, the explanation can be aggregated over all the layers in Transformer by recursively processing each layer with Eq.~\ref{eq:hm}. In this section, we conduct the experiments to discuss  the influence of using different number of layers to generate heat maps for image and text.

With different layer number $N$, the visualizations %on image 
with specific texts are shown in Fig.~\ref{fig:layers_img}, and the corresponding caption explanations are shown in Fig.~\ref{fig:layers_text}.  $N=1$ means the visualization is generated only with the final Transformer layer, while $N=12$ means all the layers are involved. The image  explanations become worse when increasing the number of layers involved, since the features in lower layer may introduce more noise to the heat map. Therefore, it is the best to just use the last layer in the calculation of image visual explanation, where this conclusion is consistent with the classical gradient-based CAM methods.

As for the text explanation, there is no obvious difference of the visualization quality, since the highlights are basically focusing on ``dog'', ``car'' and ``traffic lights'' with some minor variations. Therefore, we perform the Deletion and Insertion experiments as in \S\ref{exp:quan_eval} on the text explanation maps based on different number of layers $N$. The results are shown in the following Table~\ref{tab:del_ins_layer_text}. The explanation faithfulness has the trend that it first increases with more layers used and then goes down with the lower-layer features involved ($N>8$). Therefore, we aggregate the last eight layers maps for interpreting the text encoder in our experiments.


\begin{figure}[t]
	\vspace{-0.1cm}
	\centering
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/app_layers_img_short.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The \textbf{image} visual explanations generated when aggregating over $N$ layers of the image transformer encoder.}
	\label{fig:layers_img}
	\vspace{-0.1cm}
\end{figure}

\begin{figure}[t]
	\vspace{-0.2cm}
	\centering
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/app_layers_text.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The \textbf{textual} explanations generated when aggregating over  $N$ layers of the text transformer encoder.}
	\label{fig:layers_text}
	\vspace{-0.2cm}
\end{figure}


\begin{table}
	\vspace{-0.3cm}
	\captionof{table}{The \textbf{text} explanation faithfulness vs. the number of transformer layers aggregated for the explanation. Evaluating on \textit{MS COCO image-text retrieval (Karpathy's split)} validation dataset: AUC for Deletion and Insertion curves with reporting image retrieval (IR) and text retrieval (TR) performance.}
	\vspace{-0.2cm}
	\label{tab:del_ins_layer_text}
	\centering
	\footnotesize
	\setlength
	\tabcolsep{10.0pt}  
	\renewcommand\arraystretch{1.2}  
	\begin{tabular}{@{}l|cc|cc@{}}
		\hline
		& \multicolumn{2}{c|}{Deletion$\downarrow$} & \multicolumn{2}{c}{Insertion$\uparrow$} \\ 
		$N$ & IR & TR & IR & TR \\ \hline
		1  & 0.1118 & 0.2087 & 0.1059 & 0.2196 \\       
		2  & 0.1021 & 0.1826 & 0.1186 & 0.2351 \\ 
		4  & \textbf{0.0995} & 0.1786 & 0.1242 & 0.2428 \\  
		6  & 0.0989 & \textbf{0.1761} & 0.1273 & 0.2490 \\ 
		8  & 0.0996 & 0.1770 & \textbf{0.1292} & \textbf{0.2536} \\ 
		10 & 0.1008 & 0.1843 & 0.1288 & 0.2472 \\
		12 & 0.1095 & 0.2087 & 0.1219 & 0.2364 \\ 
		\hline
	\end{tabular}
	\vspace{-0.1cm}
\end{table} 


\vspace{-0.1cm}
\subsubsection{\zcy{Effect of multi attention heads on visual explanation}}
\label{sec:influence_heads}

As mentioned in (\ref{eq:out_cls}) of \S\ref{sec:grad-eclip}, for producing  the Grad-ECLIP visual explanation, we set CLIP to perform the forward pass with a single head in the attention layer instead of the original multi-head attention layer. In Fig.~\ref{fig:heads}, we show the visualization of explanation maps when using multi-head attention layers, compared to using a single head. Comparing Fig.~\ref{fig:heads} (a) and (b), using multi-head attention results in some surrounding context information is also highlighted with the explained object. 

%To further investigate, w
We further produce the heat maps for \emph{each} attention head, using the $q\in \mathbb{R}^{D/12} $, $k\in \mathbb{R}^{D/12}$, $v\in \mathbb{R}^{D/12}$, and attention output $o_{cls}\in \mathbb{R}^{D/12}$, where $D$ is channel number before going into multi heads, and visualize them in Fig.~\ref{fig:heads} (c) for the target ``dog'', and (d) for the ``car''.
The visual explanation in each head highlights different regions, not only seeing the target object. We can infer that the channels assigned to each heads can preserve different information, and  the $\mathrm{softmax}$ inside each head helps the model to encode more context information. 
In contrast, with the single head setting, the $\mathrm{softmax}$ is performed over all channels, which selects out the most important information, and our explanation method can show the model's attention on the specific explained target, as shown in Fig.~\ref{fig:heads} (b).  


\begin{figure}
	\vspace{-0.1cm}
	\centering
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/app_heads.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The visual explanation maps with using (a) multi-head attention layer; (b) single-head attention layer; (c) each head in multi-head attention layer for text ``dog''; (d) each head in multi-head attention layer for text ``car''}
	\vspace{-0.1cm}
	\label{fig:heads}
\end{figure}



\vspace{-0.1cm}
\section{Analysis of CLIP using Grad-ECLIP}\label{sec:analysis_clip}
\vspace{-0.1cm}

Useful explanation methods can be used to identify failure modes, establish appropriate users' confidence and give insight to developers to improve models. Therefore, in this section, we use the visual explanation maps \zcy{and textual explanations} generated by Grad-ECLIP to give examples of how to explore the mechanism in text and image matching, and analyze the strengths, weaknesses, and preference of CLIP model. We hope that our explanation tool can help researchers discover more interesting properties of pre-trained image-language  models, and inspire further development of these models.
%
\abc{Here we analyze three aspects of CLIP: 1) the decomposition and addibility in image-text matching (\S\ref{sec:concept_decomp}); 2) types of attributes that can be identified by CLIP (\S\ref{sec:diagn_attr}); 3) the concreteness/abstractness of words learned by CLIP (\S\ref{sec:att_concreteness}).}

\begin{figure}
	%		\vspace{-6pt}
	\centering
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figures/decompose.pdf}
	\end{center}
	\vspace{-0.4cm}
	\caption{Visual explanation heat maps generated for single words and word phrases using Grad-ECLIP on CLIP. The dashed box contains examples where the text does not match the image.}
	\label{fig:word_comb}
	\vspace{-0.5cm}
\end{figure}

\subsection{Concept decomposition and addibility in image-text matching}\label{sec:concept_decomp}

Examining the visualizations shown in Fig.~\ref{fig:vis_comp}h, CLIP can well recognize the single concepts (nouns) and has good attention about actions (verbs). An interesting question is how does it process the combination of words, \eg, adjective and noun, verb and noun? To examine the working function of phrase matching, we conducted  experiments comparing the explanation heat maps for  single words and combined phrases using Grad-ECLIP. 

The results are shown in Fig.~\ref{fig:word_comb}. Considering adjective-noun combinations in (a), the highlights are put on all three toys when matching with ``toy'', and CLIP can successfully highlight the correct toy when the color adjective is included in the text. In the case of ``young horse'' in (b), the other horses are still highlighted, while the highlights on the young one is strengthened by adding the attribute ``young''. The examples of verb-noun cases in (c) and (d) also show similar addibility pattern on the heat maps: (c) with the verb ``stand'', the region of person's leg is highlighted along with the``skis''; (d) with the verb ``feed'', the people's hands are also highlighted together with sheep. We also show some non-existent concepts or strange word combinations in the dashed box of Fig.~\ref{fig:word_comb}, e.g., ``white horse'' in (b), ``feed skis'' in (c), ``stand on sheep'' in (d).
In these cases, the visualization shows that CLIP will mainly focus on the reasonable part of the concept, such as ``horse'', ``skis'' and ``sheep''. For the non-existent ``white'' concept in image (b), the visual explanation does not highlight  anything.

Therefore, we infer that when processing the matching of image and phrases, the model has the ability of decomposition and addibility of different concepts. 
This can help the model to generalize to different scenarios and could be the source of the strong zero-shot ability of CLIP. 


\begin{figure}
	\vspace{-0.1cm}
	\centering
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/vis_attributes.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Visual explanations on image matching with different kinds of attributions: (a) shape; (b) material; (c) color; (d) size; (e) position. For visualization, the ground-truth corresponding to the text prompt are outlined with white boxes, except for cases involving relative adjectives, \eg ``big'', ``small'', ``left'', ``right''. The text explanation maps are also shown for ``small cube'' and ``left red thing'' combinations.}
	\label{fig:attributes}
	\vspace{-0.1cm}
\end{figure}

\vspace{-0.1cm}
\subsection{Diagnostics on attribution identification}\label{sec:diagn_attr}

In Fig.~\ref{fig:word_comb}(a), we see that CLIP has an ability to distinguish color attributes, and mark out the corresponding regions on image. 
To explore further, we conduct an experiment to test CLIP's ability to identify different types of object attributes. We adopt an example image from CLEVR \cite{johnson2017clevr}, a diagnostic dataset for visual reasoning, and visualize image-text matching with various attributes:
%including 
shape (sphere, cylinder, cube), material (metal, matte, plastic), color (red, yellow, blue), size (big, small), position (left, right). 

Fig.~\ref{fig:attributes} shows the visual explanation heat maps  generated with each image-attribution pair. We have the following findings: 1) for shape and material, the heat maps can show partial correct attention with some obvious objects, such as the metal sphere for ``sphere'' and the highlighted cylinder and cube for ``matte''. However, there are also false positive and false negative errors in (a) and (b). Thus, CLIP possess a certain but limited knowledge about object shapes and materials. 2) For the color attribute in row (c), the results further verify that the model can have good ability to distinguish different colors. 3) For comparative attributes, size (big or small) in (d) and position (left or right) in (e), the visual explanations also show that CLIP produces some erroneous results.  For example, there are little differences between the heat maps of ``small cube'' and ``cube'' in (a), or ``left red thing'' and ``red'' in (c), 
which demonstrates that the word ``cube'' and ``red'' take the major role in the matching. This is also confirmed by the text heat maps in the figure.

Overall, from the above analysis, we infer that CLIP has advantages with common perceptual attributes like color, but cannot well handle physical attributes like shape and material, and is weak at grounding objects with comparative attributes, like size and position relationships. Related to the addibility of concepts in the \S\ref{sec:concept_decomp}, % analysis that the 
it is reasonable to expect that attributes that have concrete visual appearance, e.g., color, will contribute more to the matching score, compared with the abstract comparative attributes.

\begin{figure}
	\vspace{-0.1cm}
	\centering
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/word_concreteness.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The average word importance (via Grad-ECLIP) vs.~word concreteness for the top-1000 most frequent words in the MS COCO Karpathy's validation split. The red dash line shows the linear regression result over the word samples; $r^2=0.32, p<0.001$.
%	\NOTE{``word attention'' might be confused with self-attention, so better to use ``word importance''.  Change the y-axis title. } \zcyNOTE{Done.}
	}
	\label{fig:concreteness}
	\vspace{-0.1cm}
\end{figure}

\begin{figure*}
	\centering
	\begin{center}
		\includegraphics[width=0.85\textwidth]{figures/word_frequency.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{The average word importance (via Grad-ECLIP) vs.~the word frequency in the OpenWordText dataset for the top-1000 most frequent words in the MS COCO Karpathy's validation split. The  colors represent the concreteness level of each word according to \cite{brysbaert2014concreteness}. (right) zoom-in of the blue and purple boxes to show the word examples with high word importance (blue box) and low word importance (purple box).
%	\NOTE{usually we will use base 10 for log plots.  In matplotlib, you can use the semilogx function directly on the original data.}
%	\NOTE{change word attention to word importance.} \zcyNOTE{Done.}
	}
	\label{fig:frequency}
	\vspace{-0.1cm}
\end{figure*}



\vspace{-0.2cm}
\subsection{\zcy{Relationship between word importance and concreteness}}\label{sec:att_concreteness}
\vspace{-0.1cm}

From \S\ref{sec:diagn_attr}, 
%the previous analysis, 
we have inferred that the concrete \abc{visual} concepts \abc{(e.g., ``red'' and ``blue'')}  contribute more to the image and text matching than the abstract attributes \abc{(e.g., ``left'' and ``right'')}. Therefore, based on the text explanation obtained from Grad-ECLIP, we further explore the relationship between word importance in matching and word concreteness, and analyze which type of concepts and words (concrete vs. abstract) that the CLIP model has actually learned and uses most often for matching. 
For an image-text pair, we calculate the textual explanation from Grad-ECLIP, and then obtain the importance value of each word by normalizing
% the results of text explanation from Grad-ECLIP, 
such that the maximum word importance value is $1$ in the sentence. 
We then calculate the average word importance  on the top-1000 most frequent words in the MS COCO caption (Karpathy's split) validation set.
For the word concreteness, we adopt the open sourced database from \cite{brysbaert2014concreteness}, which provides the concreteness for 40,000 common English words measured by human rating. The concreteness is a value from 1 to 5, where 5 means the most concrete and 1 means the most abstract.

Based on the 1000 selected words, Fig. ~\ref{fig:concreteness} presents a scatter plot of average word importance versus concreteness value.
We  perform linear regression analysis on this data (red dashed line), 
%which is plotted as the red dash line in the figure, 
and the regression result was statistically significant ($r^2=0.32$, $p<0.001$).
% The $r^2$ and $p$-value are $0.32$ and $5.07e-69$, respectively. 
\abc{The scatter distribution and linear regression result reveal that CLIP places higher word importance to more concrete words, and vice versa, less word importance on more abstract words. \textit{Therefore, the words that CLIP has learned for matching are biased towards concrete words.} 
}

%there exists correlation between  more concrete words are more possible to get higher attention than the abstract words during the matching.     

\abc{CLIP's learned bias towards concrete words could be due to frequency bias in the training set, i.e., concrete words could appear more frequently during training.} 
To investigate this possibility, 
%To figure out whether the frequency of word in the training set affect its ability to be learned, 
we attempt to count the number of occurrences of the selected words in CLIP's training set WebImageText \cite{radford2021clip}.
However, since   WebImageText is not publicly available, 
we instead compute these statistics from the OpenWebText \cite{Gokaslan2019OpenWeb} dataset, which follows the same 
methodology 
%production idea and method to simulate and 
to reproduce the data characteristics and structure of the WebImageText corpus. 

The relationship between average word importance and the word frequency in the training corpus is plotted in Fig.~\ref{fig:frequency}, with different sample colors %on the sample points 
representing the levels of concreteness.
%, where red means the most concrete, while blue means the most abstract. From the figure, we find that
There is no obvious relationship between the word attention and the frequency.
%\zcyy{, with linear regression resulting $r^2=0.016$, $p=0.006>0.001$.} \NOTE{can also do a linear regression test and report the $r^2$ and p-value here} \zcyNOTE{Done.}.
Many high frequency words obtain a low word importance when the concreteness value is very low, and on the opposite, low frequency words of concrete concepts may obtain a high word importance.  Fig.~\ref{fig:frequency} (right) shows the zoom-in on the samples in the blue box, which have high average word importance and in the purple box with low average importance. Comparing these two boxes, the words with concrete visual meaning, especially nouns, are indeed more likely to obtain higher importance than the abstract words. Therefore, the analysis using Grad-ECLIP text explanation further supports the conclusion from \S\ref{sec:diagn_attr} -- 
 %, which conducts on several samples with image explanation, that
\textit{concrete visual concepts are learned better and contribute more to the CLIP image-text matching score than abstract concepts, \abcn{and this phenomenon is not due to word frequency bias.}}

%\NOTE{it may be interesting to also look at negative cases -- abstract words with higher importance, and concerte words with low importance.} \zcyNOTE{There are some negative cases in the figure, for example, ``boarder'' and ``produce'' have concreteness from 3 to 4, but high importance. ``lawn'', ``outfit'', and ``person'' have high concreteness with low importance. But I have no idea how to explain them for now. }




\vspace{-0.1cm}
\section{\zcy{Application of Grad-ECLIP to boost fine-grained understanding of CLIP}}
\label{sec:app_fineclip}
\vspace{-0.1cm}

CLIP has been shown to have limitations in understanding fine-grained details, such as %struggling with
poor 
 region recognition when using its dense features, due to the pre-training focusing on matching the whole image \abc{(via the $[cls]$ token)} to a text description.
 Our Grad-ECLIP generates text-specific explanation maps for the image encoder, which highlights the detailed regions on the input image when CLIP matches the image-text pair.  
Therefore, aiming to boost the fine-grained understanding ability of CLIP, here we propose a fine-tuning method for CLIP that adopts Grad-ECLIP to indicate the fine-grained alignment between image regions and corresponding textual attributes.
%
As a result, the fine-tuned CLIP has a %With developing the 
representation space where visual and semantic features are both globally and locally aligned, %the fine-tuned CLIP model significantly 
which significantly improves its region aware ability for dense prediction tasks, as well as maintaining its performance on image-level tasks.    

\begin{figure}[t]
	\vspace{-6pt}
	\centering
	\begin{center}
		\includegraphics[width=0.45 \textwidth]{figures/fine_grained_clip_new.pdf}
	\end{center}
	\vspace{-0.3cm}
	\caption{Overview of the proposed fine-grained fine-tuning of CLIP using Grad-ECLIP. Multiple phrases or words representing objects (e.g., ``dog'', ``black car'' and ``traffic lights'') are separated out by parsing the input caption
	via the Natural Language Toolkit (NLTK). 
	With the visual explanation heat maps generated by Grad-ECLIP, object-specific region feature embeddings ($F_{r}$) are obtained through weighted aggregation of the image dense feature ($F_{d}$). Finally, the CLIP fine-tuning is composed of two losses: the global loss as in the original pre-training of CLIP, and the local loss, which aligns the image region features ($F_{r}$) and the corresponding phrase features ($F_{p}$).
%	\NOTE{change "Weighted Aggregate" to "Weighted Aggregation"} \zcyNOTE{Done.}
	}
	\label{fig:fine_grained_clip}
	\vspace{-0.3cm}
\end{figure}

\vspace{-0.1cm}
\subsection{Grad-ECLIP-based fine-grained fine-tuning of CLIP}
\label{sec:app_method}
\vspace{-0.1cm}

The framework of our proposed method is shown in Fig.~\ref{fig:fine_grained_clip}, where the fine-tuned model includes an image encoder $\calI\left(\cdot\right)$ and a text encoder $\calT\left(\cdot\right)$, which is the same model architecture as the original CLIP. In the next two subsections, we will introduce the global loss and local loss adopted in the fine-grained fine-tuning, respectively. 

\subsubsection{Global loss}
%\NOTE{changed index to $b$ so it is not confused with space $i$.} \zcyNOTE{OK.}

%First, let's recall the preliminary on CLIP in \S\ref{sec:method}. 
The input of CLIP is a batch of image-text pairs $\lbrace (I_{b},T_{b}) \rbrace_{b=1}^B $. After passing through the image encoder and text encoder, the model outputs the corresponding global image and text embeddings $\lbrace (F_{I_{b}}, F_{T_{b}} ) \rbrace_{b=1}^B$, respectively. The fine-tuning adopts the same global contrastive learning as in the pre-training, which realizes the instances-level alignment and helps to maintain the model's ability for multi-modal global representation. The cosine similarity $S(F_{I}, F_{T})$ between image embedding $F_{I}$ and text embedding $F_{T}$ is calculated as in Eq.~\ref{eq:cos_similarity}. The constrastive loss is applied to push CLIP to learn the global representations by maximizing the cosine similarities of the corresponding image and text embeddings, while minimizing the cosine similarities of other non-paired ones in the batch, which is defined as:
\begin{equation}
\begin{aligned}
L_{global} = -\dfrac{1}{2B} \sum_{b=1}^{B} \left(  \log\tfrac{\exp\left( S(F_{I_{b}}, F_{T_{b}})/\tau \right) }{\sum_{b'=1}^{B} \exp\left( S(F_{I_{b}}, F_{T_{b'}})/\tau \right) } + \right.\\
	    \left. \log\tfrac{\exp\left( S(F_{T_{b}}, F_{I_{b}})/\tau \right) }{\sum_{b'=1}^{B} \exp\left( S(F_{T_{b}}, F_{I_{b'}})/\tau \right) } \right),
\end{aligned}
\end{equation}
where $\tau$ is the trainable temperature parameter.
%\NOTE{tau is trainable?} \zcyNOTE{Yes. The same as the original CLIP.}

\subsubsection{Local loss}
\abc{The local loss is based on the extracting the dense feature map from the image and performing fine-grained matching of region features to the corresponding text descriptions.}
%In this part, we first briefly explain the generation of image dense feature, and introduce the fine-grained matching of region features and the corresponding text descriptions, and show the calculation of local loss.

\vspace{0.1cm}
\myparagraph{Image dense feature}
Following %previous works 
\cite{zhou2022extract, wu2023clipself}
we extract the dense feature map of the input image from a ViT-based encoder by
to slightly modifying the last transformer layer to keep the projection and norm layers, and discard the self-attention. This modification is experimentally shown to be capable of preserving more spatial detailed features in the output token embeddings. 
%
Specifically, in the last transformer layer, with the input $x=(x_{cls}, x_{1},...,x_{h\times w})$ comprising a $[cls]$ embedding and $h \times w$ spatial token embeddings, the output of the attention layer is $o=v=\calL(x)$ instead of the $o=\calA(x)=\mathrm{softmax}(\tfrac{qk^\mathsf{T} }{\sqrt{C}})v$ in (\ref{eq:out_cls}). Then, the $[cls]$ embedding is removed and the final spatial token embeddings are reshaped into an $h \times w$ image dense feature map $F_{d}$, from which we extract fine-grained representations \abc{for specific image regions.}
%for specific concept word and phrases. 



\vspace{0.1cm}
\myparagraph{Image-text fine-grained matching}
\abc{Next we design a region feature and text matching scheme based on Grad-ECLIP to automatically obtain region-text alignment pairs. Note that our method does not require any manual or network-based region proposal or label annotations.}
%, which can be produced along with the training procedure. 
%With the help of Grad-ECLIP and without using any manual or network-based region proposal and label annotation pre-processing, we design a simple 
%
%\NOTE{changed index to $t$ so it is not confused with space $i$.} \zcyNOTE{OK.}
%
For the caption $T$ in each image-text pair $(I, T)$, we use the Natural Language Toolkit (NLTK) \cite{bird2009natural} to parse and extract the phrases that contain object concepts, by setting the separation and selection rules as ``adjective + noun''. For the example in Fig.~\ref{fig:fine_grained_clip}, the NLTK extracts ``dog'', ``black car'', and ``traffic lights'' from the input text ``a dog in a black car waiting for traffic lights''. Then, these extracted words or phrases $\lbrace p_{t} \rbrace_{t=1}^n$, where $n$ is the maximum number of extracted concepts from each caption, are sent to the text encoder, resulting in a set of phrase embeddings $\{F_{p_{t}}\}_t$.
% together with the caption. When the embedding of the whole text $F_{T}$ is used to match with the image embedding $F_{I}$ via global loss, 
%
\abc{A region embedding $F_{r_t}$ is then calculated for each phrase embedding.}
Specifically, the phrase embeddings $F_{p_{t}}$ is used to calculate the cosine similarity with the image embedding $F_I$. Grad-ECLIP is then applied to the calculated score to obtain a %visual explanation 
heat map $H_t$ according to the procedure in \S\ref{sec:grad-eclip}, which reveals the important spatial locations for matching with this specific phrase. 
%Then, we can obtain the explanation heat map $H_{i}$ for each phrase with Grad-ECLIP as the procedure in \S\ref{sec:grad-eclip}, and the heat maps show the importance of each location for matching the image with each specific phrase. 
Therefore, we adopt the explanation heat maps as weights for aggregating the image dense feature $F_{d}$, resulting in the region embedding for the phrase: $F_{r_{t}} = \sum_{hw} H_{t} \cdot F_{d}$, where $\sum_{hw}$ is the sum operator over spatial coordinates. 

Finally, for the region embeddings  %$F_{r}$ 
and corresponding phrase embeddings, %$F_{p}$, 
we adopt focal loss \cite{ren2015faster} to match the positive pairs and distance the negative pairs:
\begin{equation}
\begin{aligned}
	L_{local} = -\sum_{t}\left( 1-S\left( F_{r_{t}}, F_{p_{t}}\right)\right)^{2} \log  S\left( F_{r_{t}}, F_{p_{t}}\right) \\
	- \sum_{t} \sum_{t' \neq t} S\left( F_{r_{t}}, F_{p_{t'}}\right)^{2} \log \left( 1- S\left( F_{r_{t}}, F_{p_{t'}}\right) \right),
\end{aligned}
\end{equation}
where $S$ represents the cosine similarity function, and $t$, $t'$ means the $t$-th and $t'$-th phrase in the same batch. By adding the local loss to the global loss as the total loss, our proposed fine-grained fine-tuning successfully boosts the representation alignment between image region and corresponding textual concepts, while maintains the image-level performance at the same time.

\subsection{Experiments with fine-grained fine-tuning CLIP}
In these experiments, we use our proposed Grad-ECLIP to enable fine-tuning of CLIP to enhance the fine-grained understanding.

\subsubsection{Experiment settings}
The experiments are conducted based on the pre-trained models from EVA-CLIP \cite{sun2023eva} considering its high efficiency and capacity, followed the previous work \cite{wu2023clipself}.
For the fine-tuning experiments, unless otherwise specified, %of CLIP model, without specific note, 
we adopt the Conceptual Caption (CC3M) dataset \cite{sharma2018conceptual} as the training set, which collects about 3 million image-text pairs from the internet. 
 During fine-tuning, the input image size is 224x224, which is the same as the original pre-training of CLIP.
Two RTX 6000 Ada are used with batch size 64 on each, learning rate of 1e-5, and weight decay of 0.1.   

\subsubsection{Experiment results}
\vspace{0.1cm}

\begin{table*}
	\captionof{table}{Evaluating the fine-grained representation of the fine-tuned CLIP via zero-shot classification on the MS COCO validation dataset. We report the Top-1 and Top-5 mean accuracy on both object bounding boxes and panoptic masks (thing and stuff). Besides our Grad-ECLIP, we also adopt Grad-CAM and MaskCLIP to produce explanation map in the calculation of local loss for comparison. The red arrow shows the performance improvement brought by the local loss based on our Grad-ECLIP, compared with just using the global loss, which equivalent to ordinary fine-tuning. The gray row is the baseline CLIP before fine-tuning.}
	\label{tab:app_zero_shot}
	\vspace{-0.1cm}
	\centering
	\footnotesize 
	\renewcommand\arraystretch{1.2}
	\setlength\tabcolsep{4.5 pt}    
	\begin{tabular}{l|c|cc|c|ll|ll|ll}
		\hline
		& & \multicolumn{2}{c|}{Fine-tuning}  & & \multicolumn{2}{c|}{Boxes} & \multicolumn{2}{c|}{Thing Masks} & \multicolumn{2}{c}{Stuff Masks}  \\
		Method  & Model & Global Loss & Local Loss & Explanation Map & Top1 & Top5 & Top1 & Top5 & Top1 & Top5   \\  \hline
		\rowcolor{lightgray}		CLIP  & ViT-B/16 &  - & -  & - & 41.4 & 63.6 & 30.6 & 53.8 & 13.9 & 36.6 \\  %\hline
		
		& \multirow{4}*{ViT-B/16} & $\surd$ & - & - &  42.9 & 64.8 & 32.9 & 56.4  & 14.7 & 38.7 \\    
		Fine-tuned &  & $\surd$ & $\surd$ &  Grad-CAM & 54.2 & 74.7 & 46.5 & 69.8 & 13.2 & 42.2 \\
		CLIP    &  & $\surd$ & $\surd$ &  MaskCLIP & 54.3 & 75.5 & 47.4 & 70.9 & 17.0 & 47.9 \\
		&  & $\surd$ & $\surd$ &  Grad-ECLIP (Ours) &\textbf{57.3}\red{$_{\uparrow14.4}$} & \textbf{78.3}\red{$_{\uparrow13.5}$} & \textbf{49.3}\red{$_{\uparrow16.4}$} & \textbf{72.2}\red{$_{\uparrow15.8}$} & \textbf{18.3}\red{$_{\uparrow3.6}$} & \textbf{51.1}\red{$_{\uparrow12.4}$} \\     
		\hline %\hline
		\rowcolor{lightgray}		CLIP & ViT-L/14 &  - & -  & - & 58.1 & 78.9 & 49.8 & 72.6 & 13.1 & 33.9 \\  %\hline
		Fine-tuned & \multirow{2}*{ViT-L/14} & $\surd$ & - &  - & 62.6 & 83.1 & 54.7 & 77.5 & 16.2 & 39.2 \\
		CLIP    &  & $\surd$ & $\surd$ &  Grad-ECLIP (Ours)  & \textbf{71.7}\red{$_{\uparrow9.1}$} & \textbf{89.7}\red{$_{\uparrow6.6}$} & \textbf{63.4}\red{$_{\uparrow8.7}$} & \textbf{85.6}\red{$_{\uparrow8.1}$} & \textbf{20.5}\red{$_{\uparrow4.3}$}  & \textbf{53.6}\red{$_{\uparrow14.4}$} \\
		\hline
		
	\end{tabular}
	\vspace{-0.1cm}	
\end{table*} 


\myparagraph{Evaluating the fine-grained representation}
To evaluate the dense representation ability of the fine-tuned CLIP, we use the mean accuracy (mAcc) of classifying region boxes annotated in the val2017 split of MS COCO and panoptic masks (including ``things'' with 80 classes and ``stuffs'' with 91 classes) annotated in MS COCO Panoptic dataset \cite{kirillov2019panoptic}.
% as the metric. 
For classification, RoI % (Region of Interest) 
pooling is used to extract region box embeddings, while mask pooling is used to extract the mask embeddings from the image dense feature maps. \zcyy{The classification is performed by selecting the highest score when matching with the text embeddings of the classes.}
% of the CLIP ViT. 
%\NOTE{what is the classifier?}

The results are shown in Tab.~\ref{tab:app_zero_shot}. Compared with the pre-trained CLIP base model, fine-tuning with just the global loss on CC3M dataset can slightly increase the zero-shot classification performance --  this setting is equivalent to continuing to train CLIP in the original way, which we denote as \textit{ordinary FT} (fine-tuning) in the following descriptions. When we use both the global and local loss with the help of our Grad-ECLIP explanation maps, the fine-tuned model's performances on region classification obtained significant improvements, for both  boxes and masks. In Tab.~\ref{tab:app_zero_shot}, we also list the absolute increases of each metric compared with ordinary FT (global loss only) using the red values,
%by the values with red arrows, which intuitively 
which shows %exhibit
 the effectiveness of the local loss on boosting the fine-grained representation of CLIP ViT model.

To demonstrate the effectiveness of our Grad-ECLIP, we also conduct the fine-tuning with the local loss using two other plug-in and low computation cost explanation methods, Grad-CAM and MaskCLIP. With the heat maps from these two methods, the region-aware matching also obtains obvious performance improvements compared with the ordinary FT, but there is still a gap compared with using our Grad-ECLIP. The comparison results further demonstrate that the high-quality and accuracy of the visual explanation maps generated by Grad-ECLIP. 



\begin{table}
	\captionof{table}{Results on open-vocabulary object detection on MS COCO val set. F-ViT is the two-stage detector baseline built on the frozen original CLIP ViT, % without fine-tuning, 
		and $\dagger$ means the ViT backbone is initialized with the CLIP model fine-tuned (FT) with the corresponding method on the dataset in brackets (CC3M or MS COCO Karpathy trainset). Ordinary FT is equivalent to just using the global loss in the fine-tuning.}
	\label{tab:app_ovd}
	\vspace{-0.1cm}
	\centering
	\footnotesize
	\renewcommand\arraystretch{1.2}
	\setlength\tabcolsep{2.5 pt}    
	\begin{tabular}{@{}lc|lll@{}}
		\hline
		Method  & Backbone & AP$_{50}^{novel}$ & AP$_{50}^{base}$ & AP$_{50}^{all}$    \\  \hline
		OV-RCNN \cite{zareian2021open}  & ResNet50 & 17.5 & 41.0 & 34.9 \\
		RegionCLIP \cite{zhong2022regionclip}  & ResNet50 & 26.8 & 54.8 & 47.5 \\
		Detic \cite{zhou2022detecting} & ResNet50 & 27.8 & 51.1 & 45.0 \\
		VLDet \cite{lin2022learning} & ResNet50 & 32.0 & 50.6 & 45.8 \\
		F-VLM \cite{kuo2023fvlm} & ResNet50 & 28.0 & - & 39.6 \\
		CORA \cite{wu2023cora} & ResNet50 & 35.1 & 35.5 & 35.4 \\
		RO-ViT \cite{kim2023region} & ViT-B/16 & 30.2 & - & 41.5 \\
		RO-ViT \cite{kim2023region} & ViT-L/16 & 33.0 & - & 47.7 \\ \hline
		
		F-ViT  & ViT-B/16 & 19.4 & 43.3 & 37.0 \\
		\hspace{4pt}+CLIPSelf \cite{wu2023clipself} (CC3M)$^\dagger$  & ViT-B/16 & 13.4 & 39.3 & 32.5 \\
		\hspace{4pt}+Ordinary FT (CC3M)$^\dagger$  & ViT-B/16 & 19.5 & 43.4 & 37.1 \\
		\hspace{4pt}+Our FT (CC3M)$^\dagger$  & ViT-B/16 & 27.4\red{$_{\uparrow8.0}$} & 43.8\red{$_{\uparrow0.5}$} & 39.5\red{$_{\uparrow2.5}$} \\
		\hspace{4pt}+CLIPSelf \cite{wu2023clipself} (MS COCO)$^\dagger$  & ViT-B/16 & 25.2 & 42.2 & 37.7 \\
		\hspace{4pt}+Ordinary FT (MS COCO)$^\dagger$  & ViT-B/16 & 20.1 & 43.8 & 37.6 \\
		\hspace{4pt}+Our FT (MS COCO)$^\dagger$  & ViT-B/16 & 26.7\red{$_{\uparrow7.3}$} & 44.2\red{$_{\uparrow0.9}$} & 39.6\red{$_{\uparrow2.6}$} \\ \hline
		F-ViT  & ViT-L/14 & 28.3 & 52.5 & 46.2 \\
		\hspace{4pt}+Ordinary FT (CC3M)$^\dagger$  & ViT-L/14 & 31.1 & 53.4 & 47.6 \\
		\hspace{4pt}+Our FT (CC3M)$^\dagger$  & ViT-L/14 & \textbf{39.4}\red{$_{\uparrow11.1}$} & \textbf{53.6}\red{$_{\uparrow1.1}$} & \textbf{49.9}\red{$_{\uparrow3.7}$} \\
		%		\hspace{4pt}+Ordinary FT (MS COCO)$^\dagger$  & ViT-L/14 &  &  &  \\
		%		\hspace{4pt}+Our FT (MS COCO)$^\dagger$  & ViT-L/14 &  &  &  \\ 
		\hline		
		
	\end{tabular}
	\vspace{-0.1cm}	
\end{table} 


\vspace{0.1cm}
\myparagraph{Application to open-vocabulary detection (OVD) task}
We adopt the fine-tuned CLIP as the backbone for %open-vocabulary object detection (
OVD to verify the fine-grained understanding by a down-stream % task that requires 
%fine-grained 
localization task. Following the previous work CLIPSelf \cite{wu2023clipself}, which is a state-of-the-art CLIP fine-tuning scheme for increasing the fine-grained region representation via self-distillation, we build open-vocabulary object detectors based on the F-ViT \cite{wu2023clipself} architecture, which is a two-stage detector using a frozen CLIP ViT as the backbone.  In the \abc{CLIP} fine-tuning stage, we adopt the image-text pairs in CC3M or MS COCO Karpathy train set as the inputs, and for MS COCO Karpathy train set, we filter out the same image samples that are also in the OVD val set to ensure there is no risk of label leakage.  
 The OVD models are trained on the OV-COCO benchmark \cite{chen2015microsoft}, and %
% As for the OVD training hyper-parameters, 
we use AdamW optimizer with batch size of 64, learning rate of 1e-4, and weight decay of 0.1.
For evaluation, we report box AP (average precision) at IoU (Intersection over Union) of base, novel and all categories as with previous works \cite{zhong2022regionclip,kuo2023fvlm,wu2023cora,kim2023region,wu2023clipself}.
% for evaluation. 
%
Since there is no extra region box annotations used in our fine-tuning, for fair comparison, we implement fine-tuning of the CLIPSelf version using 
%with their version 
%under the same training set with the version of 
image patch distillation, which also has no extra region proposal annotation requirements. 

The  results are presented in Tab.~\ref{tab:app_ovd}. F-ViT is the baseline that initializes the detector backbone with the \abc{original} pre-trained CLIP model, and other versions with $\dagger$ initialize the backbone with the CLIP models fine-tuned by various methods and datasets. 
%our method and competing methods.
%From the results shown in Tab.~\ref{tab:app_ovd}, 
With the ViT-B/16 backbone, ordinary FT produces similar performance as the baseline, while our FT significantly improves the OVD results, especially on the novel categories. Since the base categories have explicit annotated bounding boxes and labels during OVD training, the performance on the unseen novel categories better illustrates the fine-grained understanding ability brought by the CLIP model. 
In contrast to CLIPSelf where the downstream OVD performance is much influenced by the dataset used for fine-tuning, our method obtains better performances regardless of the fine-tuning dataset (CC3M or MS COCO), 
%with no matter CC3M or MS COCO as the fine-tuning set, 
which indicates that our method can effectively and stably boost the dense representation ability of the CLIP model. 
Finally, we conduct the experiments with the ViT-L/14 architecture and further improve the OVD performance on the novel categories by a large extent.  
%obtain state-of-the-art performance. 
\zcyy{Compared with the existing OVD methods, mostly relying on ResNet-based encoder or modified ViT encoder, and requiring pre-training from scratch on prepared large-scale data with extra region information, our method achieves superior performance with just fine-tuning the CLIP on the easily-obtained image-text pairs.} 

%\NOTE{anything to say about the other methods (top of the table)? All these methods are OVD using the same setting as ours, or is ours different? They all use CLIP-trained backbones?} \zcyNOTE{These methods have different settings as ours, some change the ViT architecture or use ResNet, and basically pre-train from scratch.}

\begin{table}
	\captionof{table}{Evaluating the influence of fine-grained fine-tuning on image-level representation by a zero-shot retrieval task using Flicker30k. R@i denotes the recall accuracy with top $i$ matching. CLIP is the pre-trained CLIP model from EVA-CLIP \cite{sun2023eva}, which is the base model the fine-tuning (FT) methods adopt. Ordinary FT is equivalent to just using the global loss in the fine-tuning.}
	\label{tab:app_retrieval}
	\vspace{-0.1cm}
	\centering
	\footnotesize
	\renewcommand\arraystretch{1.2}
	\setlength\tabcolsep{3.5 pt}    
	\begin{tabular}{@{}l|c|ccc|ccc@{}}
		\hline
		& & \multicolumn{3}{c|}{text-to-image} & \multicolumn{3}{c}{image-to-text}  \\
		Method & Model & R@1 & R@5 & R@10 & R@1 & R@5 & R@10    \\  \hline
		%\rowcolor{mygray} CLIP \cite{radford2021clip} & ViT-B/16  & 71.6 & 90.3 & 94.1 & 84.0 & 96.1 & 98.2  \\  
		%\rowcolor{mygray} GLoRIA \cite{huang2021gloria} & ViT-B/16 & 68.4 & 88.9 & 93.2 & 78.0 & 95.5 & 98.0 \\
		%\rowcolor{mygray} FILIP \cite{yao2021filip} & ViT-B/16 & 55.8 & 81.5 & 87.9 & 69.0 & 89.8 & 94.0 \\
		%\rowcolor{mygray} PACL \cite{mukhoti2023open} & ViT-B/16 & 54.9 & 80.7 & 87.3 & 69.6 & 89.7 & 94.2 \\
		%\rowcolor{mygray} SPARC \cite{bica2024improving} & ViT-B/16 & 72.0 & 91.2 &  94.9 & 84.4 & 97.6 & 98.7 \\ \hline
		 CLIP \cite{sun2023eva} & ViT-B/16 & 73.6 & 90.9 &  94.8 & 88.6 & 97.1 & 99.1 \\
		\hspace{4pt}+CLIPSelf  \cite{wu2023clipself} & ViT-B/16 &  46.9 & 72.8 & 81.1 & 51.4 & 76.9 & 85.5 \\
		\hspace{4pt}+Ordinary FT & ViT-B/16 &   72.7 & 90.7 & 94.5 & 86.7 & 96.7 & 98.5 \\
		\hspace{4pt}+Our FT & ViT-B/16  & 72.8 & 91.0 & 94.7 & 88.3 & 97.4 & 98.9 \\  \hline
		CLIP \cite{sun2023eva} & ViT-L/14 & 78.8 & 93.9 & 96.8  & 90.4 & 98.8 & 99.4 \\
		\hspace{4pt}+Ordinary FT & ViT-L/14 &  \textbf{80.2} & 94.5 & \textbf{97.1} & 91.6 & 98.9 & 99.7 \\
		\hspace{4pt}+Our FT & ViT-L/14  & \textbf{80.2} & \textbf{94.6} & \textbf{97.1} & \textbf{92.1} & \textbf{99.1} & \textbf{99.8} \\  \hline		
		
	\end{tabular}
	\vspace{-0.1cm}	
\end{table} 


\vspace{0.1cm}
\myparagraph{Evaluating the influence on image-level representation}
We next explore the influence of our fine-tuning on the image-level representation ability, to see if there are negative effects to image-level representation when improving fine-grained representations. 
%Since we add a region alignment loss to improve the fine-grained matching besides the image-level matching, 
%Therefore, w
We report the recall accuracy of image-to-text and text-to-image retrieval task with the Flickr30k \cite{plummer2015flickr30k} validation set. As the results shown in Tab.~\ref{tab:app_retrieval}, after fine-tuning the CLIP ViT-B/16 model on the CC3M dataset, our FT has successfully preserve the image-level retrieval performance, which is similar to the ordinary FT.  In contrast, CLIPSelf has largely lost the ability of its global image-level representation. 
% global embedding expression ability.
Finally, the superior results on ViT-L/14 model compared with the baseline CLIP further demonstrate the effectiveness of our FT in maintaining both the global representation and stable image-level matching. 




%For the results shown in Tab.~\ref{tab:app_retrieval}, the previous works \cite{huang2021gloria,yao2021filip,mukhoti2023open,bica2024improving} in gray background pursue fine-grained understanding by introducing losses of token-level, and reproducing the pre-training with large-scale dataset (approximately billion data pairs). 




\vspace{-0.1cm}
\section{Conclusion}\label{sec:conclusion}
\vspace{-0.1cm}
In this paper, we propose Grad-ECLIP, a novel white-box gradient-based visual \abc{and textual} explanation method for CLIP, the dual-encoder pre-trained model for image-text matching. Grad-ECLIP is applicable to both the image and text encoders, producing heat maps that indicate the importance of image regions or words for the image-text matching score. Qualitative and quantitative evaluations demonstrate the advantages of Grad-ECLIP compared with existing explanation methods designed for transformers/CLIP, \zcy{and the adaptation experiments exhibit the generalizability of our method.} We also adopt Grad-ECLIP to analyze the properties of the pre-trained CLIP model, where we discover its ability of concept decomposition and addibility,  advantages/limitations on different attribute identification, \abc{and its bias towards learning concrete words over abstract words}. By introducing these analyses as examples, we hope the proposed interpretation method can be used to help with both development and understanding  of VLMs. \zcy{Finally, we propose a fine-tuning scheme, which adopts the Grad-ECLIP explanation map to obtain region-text pairs for a local loss that boosts the fine-grained understanding of CLIP.}
In future work, we will consider how to associate individual words from the sentence to regions in the image, and vice versa. 
\abc{Future work can also consider how to extend Grad-ECLIP to other VLMs  with modified dual-encoder architectures, e.g ALBEF with additional cross-attention layers to fuse image and text features.}


\small
\vspace{-0.1cm}
\section*{Acknowledgements}
This work was supported by Strategic Research Grants from City University of Hong Kong (Project. Nos. 7005840 and 7005995) and Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone (HZQB-KCZYZ-2021045).

\footnotesize
\bibliographystyle{IEEEtran}
\bibliography{references}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/chenyang.jpg}}]{Chenyang Zhao}
	received the B.Eng. degree in Electrical Engineering from Xiamen University, Xiamen, China, and M.S. degree in Computer Science from School of Electronic and Computer Engineering, Peking University, Shenzhen, China, in 2016 and 2019, respectively. She is currently working towards the Ph.D. degree in Computer Science at the City University of Hong Kong. Her research interests include explainable AI and object detection.
\end{IEEEbiography}


\vspace{-0.1cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/kun_wang.jpg}}]{Kun Wang}
	 is a senior researcher at SenseTime Group Limited. He holds an MPhil degree from the Department of Electronic Engineering at the Chinese University of Hong Kong. His research interests focus on computer vision and representation learning. Presently, he is engaged in developing applications utilizing large language models and large multi-modal models within the industry.
	
\end{IEEEbiography}
\vspace{-0.1cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/janet2012.jpg}}]{Janet H. Hsiao}
	received the B.S. degree in Computer Science \& Information Engineering from National Taiwan University, the M.S. degree in Computing Science from Simon Fraser University, and the Ph.D. degree in Informatics from University of Edinburgh. She is currently a Professor in the Division of Social Science and Department of Computer Science \& Engineering at Hong Kong University of Science and Technology. She is also a Fellow of the Cognitive Science Society and serves on the Governing Board. Her research interests include cognitive science, computational modelling, learning and visual cognition, and explainable AI.
\end{IEEEbiography}

\vspace{-0.1cm}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{bio/antoni.jpg}}]{Antoni B. Chan}
	received the B.S. and M.Eng. degrees in electrical engineering from Cornell University, Ithaca, NY, in 2000 and 2001, and
	the Ph.D. degree in electrical and computer engineering from the University of California, San Diego (UCSD), San Diego, in 2008. He is currently a Professor in the Department of Computer Science, City University of Hong Kong. His research interests include computer vision, machine learning, pattern recognition, and music analysis.
\end{IEEEbiography}

\end{document}



