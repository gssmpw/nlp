@article{xie2022vit,
	title={Vit-cx: Causal explanation of vision transformers},
	author={Xie, Weiyan and Li, Xiao-Hui and Cao, Caleb Chen and Zhang, Nevin L},
	booktitle={IJCAI},
	pages={1569--1577},
	year={2023}
}

@inproceedings{chefer2021transformer,
	title={Transformer interpretability beyond attention visualization},
	author={Chefer, Hila and Gur, Shir and Wolf, Lior},
	booktitle={CVPR},
	year={2021}
}
%	pages={782--791},

@article{chen2022beyond,
	title={Beyond intuition: Rethinking token attributions inside transformers},
	author={Chen, Jiamin and Li, Xuhong and Yu, Lei and Dou, Dejing and Xiong, Haoyi},
	journal={TMLR},
	year={2022}
}

@inproceedings{englebert2023explaining,
	title={Explaining through Transformer Input Sampling},
	author={Englebert, Alexandre and Stassin, S{\'e}drick and Nanfack, G{\'e}raldin and Mahmoudi, Sidi Ahmed and Siebert, Xavier and Cornu, Olivier and De Vleeschouwer, Christophe},
	booktitle={ICCV},
	pages={806--815},
	year={2023}
}

@misc{Gokaslan2019OpenWeb,  
	title={OpenWebText Corpus},
	author={Aaron Gokaslan and Vanya Cohen},
	howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}}, 
	year={2019}
}

@article{brysbaert2014concreteness,
	title={Concreteness ratings for 40 thousand generally known English word lemmas},
	author={Brysbaert, Marc and Warriner, Amy Beth and Kuperman, Victor},
	journal={Behavior research methods},
	volume={46},
	pages={904--911},
	year={2014},
	publisher={Springer}
}


@inproceedings{zareian2021open,
	title={Open-vocabulary object detection using captions},
	author={Zareian, Alireza and Rosa, Kevin Dela and Hu, Derek Hao and Chang, Shih-Fu},
	booktitle={CVPR},
	pages={14393--14402},
	year={2021}
}

@inproceedings{zhou2022detecting,
	title={Detecting twenty-thousand classes using image-level supervision},
	author={Zhou, Xingyi and Girdhar, Rohit and Joulin, Armand and Kr{\"a}henb{\"u}hl, Philipp and Misra, Ishan},
	booktitle={ECCV},
	year={2022},
}
%	pages={350--368},
%	organization={Springer}

@inproceedings{wu2023cora,
	title={Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching},
	author={Wu, Xiaoshi and Zhu, Feng and Zhao, Rui and Li, Hongsheng},
	booktitle={CVPR},
	pages={7031--7040},
	year={2023}
}

@article{chen2015microsoft,
	title={Microsoft coco captions: Data collection and evaluation server},
	author={Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	journal={arXiv preprint arXiv:1504.00325},
	year={2015}
}

@inproceedings{kim2023region,
	title={Region-aware pretraining for open-vocabulary object detection with vision transformers},
	author={Kim, Dahun and Angelova, Anelia and Kuo, Weicheng},
	booktitle={CVPR},
	year={2023}
}
%	pages={11144--11154},

@article{lin2022learning,
	title={Learning object-language alignments for open-vocabulary object detection},
	author={Lin, Chuang and Sun, Peize and Jiang, Yi and Luo, Ping and Qu, Lizhen and Haffari, Gholamreza and Yuan, Zehuan and Cai, Jianfei},
	journal={arXiv preprint arXiv:2211.14843},
	year={2022}
}

@inproceedings{zhao2024gradient,
	title={Gradient-based Visual Explanation for Transformer-based CLIP},
	author={Zhao, Chenyang and Wang, Kun and Zeng, Xingyu and Zhao, Rui and Chan, Antoni B},
	booktitle={ICML},
	year={2024}
}
%	pages={61072--61091},

@article{yao2021filip,
	title={Filip: Fine-grained interactive language-image pre-training},
	author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
	journal={arXiv preprint arXiv:2111.07783},
	year={2021}
}

@inproceedings{huang2021gloria,
	title={Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition},
	author={Huang, Shih-Cheng and Shen, Liyue and Lungren, Matthew P and Yeung, Serena},
	booktitle={ICCV},
	pages={3942--3951},
	year={2021}
}

@inproceedings{mukhoti2023open,
	title={Open vocabulary semantic segmentation with patch aligned contrastive learning},
	author={Mukhoti, Jishnu and Lin, Tsung-Yu and Poursaeed, Omid and Wang, Rui and Shah, Ashish and Torr, Philip HS and Lim, Ser-Nam},
	booktitle={CVPR},
	pages={19413--19423},
	year={2023}
}

@article{bica2024improving,
	title={Improving fine-grained understanding in image-text pre-training},
	author={Bica, Ioana and Ili{\'c}, Anastasija and Bauer, Matthias and Erdogan, Goker and Bo{\v{s}}njak, Matko and Kaplanis, Christos and Gritsenko, Alexey A and Minderer, Matthias and Blundell, Charles and Pascanu, Razvan and others},
	journal={arXiv preprint arXiv:2401.09865},
	year={2024}
}

@inproceedings{sharma2018conceptual,
	title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
	author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
	booktitle={ACL},
	pages={2556--2565},
	year={2018}
}

@article{sun2023eva,
	title={Eva-clip: Improved training techniques for clip at scale},
	author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
	journal={preprint arXiv:2303.15389},
	year={2023}
}
%	journal={arXiv preprint arXiv:2303.15389},

@article{xu2021simple,
	title={A simple baseline for zeroshot semantic segmentation with pre-trained vision-language model},
	author={Xu, Mengde and Zhang, Zheng and Wei, Fangyun and Lin, Yutong and Cao, Yue and Hu, Han and Bai, Xiang},
	journal={ECCV},
	year={2022}
}

@inproceedings{liang2023open,
	title={Open-vocabulary semantic segmentation with mask-adapted clip},
	author={Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana},
	booktitle={CVPR},
	pages={7061--7070},
	year={2023}
}

@article{gu2021open,
	title={Open-vocabulary object detection via vision and language knowledge distillation},
	author={Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
	journal={ICLR},
	year={2022}
}

@inproceedings{du2022learning,
	title={Learning to prompt for open-vocabulary object detection with vision-language model},
	author={Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
	booktitle={CVPR},
	pages={14084--14093},
	year={2022}
}

@inproceedings{kuo2023fvlm,
	title={F-vlm: Open-vocabulary object detection upon frozen vision and language models},
	author={Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia},
	booktitle={ICLR},
	year={2023}
}

@article{yu2024fcclip,
	title={Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip},
	author={Yu, Qihang and He, Ju and Deng, Xueqing and Shen, Xiaohui and Chen, Liang-Chieh},
	journal={NeurIPS},
	volume={36},
	year={2024}
}

@article{liu2023grounding,
	title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
	author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
	journal={arXiv preprint arXiv:2303.05499},
	year={2023}
}

@article{wu2023clipself,
	title={Clipself: Vision transformer distills itself for open-vocabulary dense prediction},
	author={Wu, Size and Zhang, Wenwei and Xu, Lumin and Jin, Sheng and Li, Xiangtai and Liu, Wentao and Loy, Chen Change},
	journal={arXiv preprint arXiv:2310.01403},
	year={2023}
}

@inproceedings{chen2020uniter,
	title={Uniter: Universal image-text representation learning},
	author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
	booktitle={ECCV},
	year={2020},
}
%	pages={104--120},
%	organization={Springer}

@book{bird2009natural,
	title={Natural language processing with Python: analyzing text with the natural language toolkit},
	author={Bird, Steven and Klein, Ewan and Loper, Edward},
	year={2009},
	publisher={O'Reilly Media, Inc.}
}

@inproceedings{zhang2021vinvl,
	title={Vinvl: Revisiting visual representations in vision-language models},
	author={Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng},
	booktitle={CVPR},
	pages={5579--5588},
	year={2021}
}

@inproceedings{lin2017focal,
	title={Focal loss for dense object detection},
	author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
	booktitle={ICCV},
	pages={2980--2988},
	year={2017}
}
@article{ren2015faster,
	title={Faster r-cnn: Towards real-time object detection with region proposal networks},
	author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	journal={NeurIPS},
	volume={28},
	year={2015}
}

@article{krishna2017visual,
	title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
	author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
	journal={IJCV},
	volume={123},
	pages={32--73},
	year={2017},
	publisher={Springer}
}


@inproceedings{radford2021clip,
	title={Learning transferable visual models from natural language supervision},
	author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
	booktitle={ICML},
	year={2021}
}
%	pages={8748--8763},

@inproceedings{cha2022domain,
	title={Domain generalization by mutual-information regularization with pre-trained models},
	author={Cha, Junbum and Lee, Kyungjae and Park, Sungrae and Chun, Sanghyuk},
	booktitle={ECCV},
	year={2022},
}
%	pages={440--457},
%	organization={Springer}

@inproceedings{changpinyo2021conceptual,
	title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
	author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
	booktitle={CVPR},
	pages={3558--3568},
	year={2021}
}

@inproceedings{wang2022cris,
	title={Cris: Clip-driven referring image segmentation},
	author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
	booktitle={CVPR},
	year={2022}
}
%	pages={11686--11695},

@article{samek2016evaluating,
	title={Evaluating the visualization of what a deep neural network has learned},
	author={Samek, Wojciech and Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert},
	journal={IEEE Trans. Neural Netw Learn Syst},
	volume={28(11)},
	pages={2660--73},
	year={2016},
	publisher={IEEE}
}
%	number={11},

@inproceedings{wang2020score,
	title={Score-CAM: Score-weighted visual explanations for convolutional neural networks},
	author={Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
	booktitle={CVPR Workshops},
	pages={24--25},
	year={2020}
}

@article{wang2020ss,
	title={SS-CAM: Smoothed Score-CAM for sharper visual feature localization},
	author={Wang, Haofan and Naidu, Rakshit and Michael, Joy and Kundu, Soumya Snigdha},
	journal={arXiv:2006.14255},
	year={2020}
}

@inproceedings{petsiuk2021black,
	title={Black-box explanation of object detectors via saliency maps},
	author={Petsiuk, Vitali and Jain, Rajiv and Manjunatha, Varun and Morariu, Vlad I and Mehra, Ashutosh and Ordonez, Vicente and Saenko, Kate},
	booktitle={CVPR},
	pages={11443--11452},
	year={2021}
}


@inproceedings{xu2022groupvit,
	title={Groupvit: Semantic segmentation emerges from text supervision},
	author={Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong},
	booktitle={CVPR},
	year={2022}
}
%	pages={18134--18144},

@article{luo2022clip4clip,
	title={Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning},
	author={Luo, Huaishao and Ji, Lei and Zhong, Ming and Chen, Yang and Lei, Wen and Duan, Nan and Li, Tianrui},
	journal={Neurocomputing},
	volume={508},
	pages={293--304},
	year={2022},
	publisher={Elsevier}
}

@article{zhou2022learning,
	title={Learning to prompt for vision-language models},
	author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
	journal={IJCV},
	volume={130},
	number={9},
	pages={2337--2348},
	year={2022},
	publisher={Springer}
}
%	journal={International Journal of Computer Vision},


@inproceedings{li2020oscar,
	title={Oscar: Object-semantics aligned pre-training for vision-language tasks},
	author={Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
	booktitle={ECCV},
	year={2020},
}
%	pages={121--137},
%	organization={Springer}

@article{chen2022prompt,
	title={Prompt learning with optimal transport for vision-language models},
	author={Chen, Guangyi and Yao, Weiran and Song, Xiangchen and Li, Xinyue and Rao, Yongming and Zhang, Kun},
	journal={arXiv:2210.01253},
	year={2022}
}
@inproceedings{wang2023position,
	title={Position-guided Text Prompt for Vision-Language Pre-training},
	author={Wang, Jinpeng and Zhou, Pan and Shou, Mike Zheng and Yan, Shuicheng},
	booktitle={CVPR},
	pages={23242--23251},
	year={2023}
}
@inproceedings{zhong2022regionclip,
	title={Regionclip: Region-based language-image pretraining},
	author={Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others},
	booktitle={CVPR},
	pages={16793--16803},
	year={2022}
}


@inproceedings{li2022grounded,
	title={Grounded language-image pre-training},
	author={Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others},
	booktitle={CVPR},
	pages={10965--10975},
	year={2022}
}

@inproceedings{xu2015show,
	title={Show, attend and tell: Neural image caption generation with visual attention},
	author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
	booktitle={ICML},
	pages={2048--2057},
	year={2015}
}

@inproceedings{antol2015vqa,
	title={Vqa: Visual question answering},
	author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
	booktitle={ICCV},
	year={2015}
}
%	pages={2425--2433},

@inproceedings{plummer2015flickr30k,
	title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
	author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
	booktitle={ICCV},
	year={2015}
}
%	pages={2641--2649},

@inproceedings{kirillov2019panoptic,
	title={Panoptic segmentation},
	author={Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
	booktitle={CVPR},
	pages={9404--9413},
	year={2019}
}

@inproceedings{chefer2021transformer,
	title={Transformer interpretability beyond attention visualization},
	author={Chefer, Hila and Gur, Shir and Wolf, Lior},
	booktitle={CVPR},
	pages={782--791},
	year={2021}
}
@inproceedings{chefer2021generic,
	title={Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers},
	author={Chefer, Hila and Gur, Shir and Wolf, Lior},
	booktitle={ICCV},
	pages={397--406},
	year={2021}
}
@article{bach2015pixel,
	title={On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
	author={Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
	journal={PloS one},
	volume={10(7):e0130140},
	year={2015},
}
%	pages={e0130140},
%	number={7},
%	publisher={Public Library of Science}

@article{lundberg2017unified,
	title={A unified approach to interpreting model predictions},
	author={Lundberg, Scott M and Lee, Su-In},
	journal={NeurIPS},
	volume={30},
	year={2017}
}
@inproceedings{nam2020relative,
	title={Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks},
	author={Nam, Woo-Jeoung and Gur, Shir and Choi, Jaesik and Wolf, Lior and Lee, Seong-Whan},
	booktitle={AAAI},
	year={2020}
}
%	booktitle={Proceedings of the AAAI conference on artificial intelligence},
%	volume={34},
%	number={03},
%	pages={2501--2508},

@inproceedings{shrikumar2017learning,
	title={Learning important features through propagating activation differences},
	author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	booktitle={ICML},
	year={2017}
}
%	pages={3145--3153},

@article{wang2024visual,
	title={Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution},
	author={Wang, Ying and Rudner, Tim GJ and Wilson, Andrew G},
	journal={NeurIPS},
	year={2024}
}
%	volume={36},

@inproceedings{iwana2019explaining,
	title={Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation},
	author={Iwana, Brian Kenji and Kuroki, Ryohei and Uchida, Seiichi},
	booktitle={ICCVW},
	year={2019}
}
%	pages={4176--4185},
%	organization={IEEE}

@inproceedings{he2016deep,
	title={Deep residual learning for image recognition},
	author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle={CVPR},
	pages={770--778},
	year={2016}
}
@article{li2022exploring,
	title={Exploring visual interpretability for contrastive language-image pre-training},
	author={Li, Yi and Wang, Hualiang and Duan, Yiqun and Xu, Hang and Li, Xiaomeng},
	journal={arXiv:2209.07046},
	year={2022}
}


@article{xie2022vit,
	title={Vit-cx: Causal explanation of vision transformers},
	author={Xie, Weiyan and Li, Xiao-Hui and Cao, Caleb Chen and Zhang, Nevin L},
	journal={arXiv preprint arXiv:2211.03064},
	year={2022}
}
@inproceedings{yu2023x,
	title={X-pruner: explainable pruning for vision transformers},
	author={Yu, Lu and Xiang, Wei},
	booktitle={CVPR},
	pages={24355--24363},
	year={2023}
}

@article{li2023clipsurgery,
	title={Clip surgery for better explainability with enhancement in open-vocabulary tasks},
	author={Li, Yi and Wang, Hualiang and Duan, Yiqun and Li, Xiaomeng},
	journal={arXiv:2304.05653},
	year={2023}
}
@inproceedings{zhou2022extract,
	title={Extract free dense labels from clip},
	author={Zhou, Chong and Loy, Chen Change and Dai, Bo},
	booktitle={ECCV},
	pages={696--712},
	year={2022},
	organization={Springer}
}

@article{qiang2022attcat,
	title={Attcat: Explaining transformers via attentive class activation tokens},
	author={Qiang, Yao and Pan, Deng and Li, Chengyin and Li, Xin and Jang, Rhongho and Zhu, Dongxiao},
	journal={NeurIPS},
	year={2022}
}
%	volume={35},
%	pages={5052--5064},

@article{zhang2018top,
	title={Top-down neural attention by excitation backprop},
	author={Zhang, Jianming and Bargal, Sarah Adel and Lin, Zhe and Brandt, Jonathan and Shen, Xiaohui and Sclaroff, Stan},
	journal={IJCV},
	volume={126(10)},
	pages={1084--102},
	year={2018},
}
%	number={10},
%	publisher={Springer}

@inproceedings{chenyang2022odam,
	title={ODAM: Gradient-based Instance-specific Visual Explanation for Object Detection},
	author={Zhao, Chenyang and Chan, Antoni Bert},
	booktitle={ICLR},
	year={2022}
}

@inproceedings{abnar2020quantifying,
	title={Quantifying Attention Flow in Transformers},
	author={Abnar, Samira and Zuidema, Willem},
	booktitle={ACL},
	pages={4190--4197},
	year={2020}
}

@article{montavon2017explaining,
	title={Explaining nonlinear classification decisions with deep taylor decomposition},
	author={Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\"u}ller, Klaus-Robert},
	journal={Pattern recognition},
	volume={65},
	pages={211--222},
	year={2017},
	publisher={Elsevier}
}

@inproceedings{gu2019understanding,
	title={Understanding individual decisions of cnns via contrastive backpropagation},
	author={Gu, Jindong and Yang, Yinchong and Tresp, Volker},
	booktitle={ACCV},
	pages={119--134},
	year={2019}
}


@inproceedings{johnson2017clevr,
	title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
	author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
	booktitle={CVPR},
	year={2017}
}
%	pages={2901--2910},

@inproceedings{zeiler2014visualizing,
	title={Visualizing and understanding convolutional networks},
	author={Zeiler, Matthew D and Fergus, Rob},
	booktitle={ECCV},
	pages={818--833},
	year={2014}
}


@inproceedings{selvaraju2017grad,
	title={Grad-cam: Visual explanations from deep networks via gradient-based localization},
	author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	booktitle={ICCV},
	pages={618--626},
	year={2017}
}

@inproceedings{NEURIPS2019_80537a94,
	author = {Srinivas, Suraj and Fleuret, Fran\c{c}ois},
	booktitle = {NeurIPS},
	title = {Full-Gradient Representation for Neural Network Visualization},
	volume = {32},
	year = {2019}
}

@inproceedings{ribeiro2016should,
	title={" Why should i trust you?" Explaining the predictions of any classifier},
	author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	booktitle={ACM SIGKDD},
	year={2016}
}
%	pages={1135--1144},

@inproceedings{chattopadhay2018grad,
	title={Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks},
	author={Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
	booktitle={WACV},
	pages={839--847},
	year={2018}
}

@article{russakovsky2015imagenet,
	title={Imagenet large scale visual recognition challenge},
	author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
	journal={IJCV},
	volume={115},
	pages={211--252},
	year={2015},
	publisher={Springer}
}

@inproceedings{lin2014microsoft,
	title={Microsoft coco: Common objects in context},
	author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
	booktitle={ECCV},
	year={2014},
}
%	pages={740--755},
%	organization={Springer}

@article{gao2022luss,
	title={Large-scale Unsupervised Semantic Segmentation},
	author={Gao, Shanghua and Li, Zhong-Yu and Yang, Ming-Hsuan and Cheng, Ming-Ming and Han, Junwei and Torr, Philip},
	journal={TPAMI},
	year={2022}
}

                      
@inproceedings{wang2019camp,
	title={Camp: Cross-modal adaptive message passing for text-image retrieval},
	author={Wang, Zihao and Liu, Xihui and Li, Hongsheng and Sheng, Lu and Yan, Junjie and Wang, Xiaogang and Shao, Jing},
	booktitle={ICCV},
	year={2019}
}
%	pages={5764--5773},


@article{petsiuk2018rise,
	title={Rise: Randomized input sampling for explanation of black-box models},
	author={Petsiuk, Vitali and Das, Abir and Saenko, Kate},
	journal={arXiv:1806.07421},
	year={2018}
}
	%journal={arXiv preprint arXiv:1806.07421},

@inproceedings{fong2017interpretable,
	title={Interpretable explanations of black boxes by meaningful perturbation},
	author={Fong, Ruth C and Vedaldi, Andrea},
	booktitle={ICCV},
	pages={3429--3437},
	year={2017}
}

@inproceedings{wagner2019interpretable,
	title={Interpretable and fine-grained visual explanations for convolutional neural networks},
	author={Wagner, Jorg and Kohler, Jan Mathias and Gindele, Tobias and Hetzel, Leon and Wiedemer, Jakob Thaddaus and Behnke, Sven},
	booktitle={CVPR},
	pages={9097--9107},
	year={2019}
}

@inproceedings{lee2021bbam,
	title={Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation},
	author={Lee, Jungbeom and Yi, Jihun and Shin, Chaehun and Yoon, Sungroh},
	booktitle={CVPR},
	year={2021}
}
%	pages={2643--2652},


@article{jiang2021layercam,
	title={Layercam: Exploring hierarchical class activation maps for localization},
	author={Jiang, Peng-Tao and Zhang, Chang-Bin and Hou, Qibin and Cheng, Ming-Ming and Wei, Yunchao},
	journal={TIP},
	volume={30},
	pages={5875--5888},
	year={2021},
	publisher={IEEE}
}


@inproceedings{ramaswamy2020ablation,
	title={Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization},
	author={Ramaswamy, Harish Guruprasad and others},
	booktitle={WACV},
	year={2020}
}
%	pages={983--991},


@inproceedings{hendrycks2021many,
	title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
	author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
	booktitle={ICCV},
	pages={8340--8349},
	year={2021}
}

@article{wang2019learning,
	title={Learning robust global representations by penalizing local predictive power},
	author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},
	journal={NeurIPS},
	year={2019}
}
%	volume={32},

@inproceedings{hendrycks2021natural,
	title={Natural adversarial examples},
	author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
	booktitle={CVPR},
	pages={15262--15271},
	year={2021}
}

@inproceedings{boecking2022making,
	title={Making the most of text semantics to improve biomedical vision--language processing},
	author={Boecking, Benedikt and Usuyama, Naoto and Bannur, Shruthi and Castro, Daniel C and Schwaighofer, Anton and Hyland, Stephanie and Wetscherek, Maria and Naumann, Tristan and Nori, Aditya and Alvarez-Valle, Javier and others},
	booktitle={ECCV},
	year={2022},
}
%	pages={1--21},
%	organization={Springer}

@article{dosovitskiy2020image,
	title={An image is worth 16x16 words: Transformers for image recognition at scale},
	author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
	journal={arXiv preprint arXiv:2010.11929},
	year={2020}
}

@inproceedings{li2022blip,
	title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
	author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	booktitle={ICML},
	pages={12888--12900},
	year={2022}
}

@article{yu2022coca,
	title={Coca: Contrastive captioners are image-text foundation models},
	author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
	journal={arXiv:2205.01917},
	year={2022}
}

@article{li2021align,
	title={Align before fuse: Vision and language representation learning with momentum distillation},
	author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
	journal={NeurIPS},
	volume={34},
	pages={9694--9705},
	year={2021}
}