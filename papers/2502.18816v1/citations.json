[
  {
    "index": 0,
    "papers": [
      {
        "key": "wang2019camp",
        "author": "Wang, Zihao and Liu, Xihui and Li, Hongsheng and Sheng, Lu and Yan, Junjie and Wang, Xiaogang and Shao, Jing",
        "title": "Camp: Cross-modal adaptive message passing for text-image retrieval"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "xu2015show",
        "author": "Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua",
        "title": "Show, attend and tell: Neural image caption generation with visual attention"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "antol2015vqa",
        "author": "Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi",
        "title": "Vqa: Visual question answering"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "plummer2015flickr30k",
        "author": "Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana",
        "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhou2022learning",
        "author": "Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",
        "title": "Learning to prompt for vision-language models"
      },
      {
        "key": "chen2022prompt",
        "author": "Chen, Guangyi and Yao, Weiran and Song, Xiangchen and Li, Xinyue and Rao, Yongming and Zhang, Kun",
        "title": "Prompt learning with optimal transport for vision-language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "yu2022coca",
        "author": "Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui",
        "title": "Coca: Contrastive captioners are image-text foundation models"
      },
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2020oscar",
        "author": "Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others",
        "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks"
      },
      {
        "key": "wang2023position",
        "author": "Wang, Jinpeng and Zhou, Pan and Shou, Mike Zheng and Yan, Shuicheng",
        "title": "Position-guided Text Prompt for Vision-Language Pre-training"
      },
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      },
      {
        "key": "wu2023clipself",
        "author": "Wu, Size and Zhang, Wenwei and Xu, Lumin and Jin, Sheng and Li, Xiangtai and Liu, Wentao and Loy, Chen Change",
        "title": "Clipself: Vision transformer distills itself for open-vocabulary dense prediction"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zeiler2014visualizing",
        "author": "Zeiler, Matthew D and Fergus, Rob",
        "title": "Visualizing and understanding convolutional networks"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "selvaraju2017grad",
        "author": "Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv",
        "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chattopadhay2018grad",
        "author": "Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N",
        "title": "Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ramaswamy2020ablation",
        "author": "Ramaswamy, Harish Guruprasad and others",
        "title": "Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization"
      },
      {
        "key": "wang2020score",
        "author": "Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia",
        "title": "Score-CAM: Score-weighted visual explanations for convolutional neural networks"
      },
      {
        "key": "wang2020ss",
        "author": "Wang, Haofan and Naidu, Rakshit and Michael, Joy and Kundu, Soumya Snigdha",
        "title": "SS-CAM: Smoothed Score-CAM for sharper visual feature localization"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ribeiro2016should",
        "author": "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
        "title": "\" Why should i trust you?\" Explaining the predictions of any classifier"
      },
      {
        "key": "petsiuk2018rise",
        "author": "Petsiuk, Vitali and Das, Abir and Saenko, Kate",
        "title": "Rise: Randomized input sampling for explanation of black-box models"
      },
      {
        "key": "fong2017interpretable",
        "author": "Fong, Ruth C and Vedaldi, Andrea",
        "title": "Interpretable explanations of black boxes by meaningful perturbation"
      },
      {
        "key": "lundberg2017unified",
        "author": "Lundberg, Scott M and Lee, Su-In",
        "title": "A unified approach to interpreting model predictions"
      },
      {
        "key": "wagner2019interpretable",
        "author": "Wagner, Jorg and Kohler, Jan Mathias and Gindele, Tobias and Hetzel, Leon and Wiedemer, Jakob Thaddaus and Behnke, Sven",
        "title": "Interpretable and fine-grained visual explanations for convolutional neural networks"
      },
      {
        "key": "lee2021bbam",
        "author": "Lee, Jungbeom and Yi, Jihun and Shin, Chaehun and Yoon, Sungroh",
        "title": "Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation"
      },
      {
        "key": "petsiuk2021black",
        "author": "Petsiuk, Vitali and Jain, Rajiv and Manjunatha, Varun and Morariu, Vlad I and Mehra, Ashutosh and Ordonez, Vicente and Saenko, Kate",
        "title": "Black-box explanation of object detectors via saliency maps"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lundberg2017unified",
        "author": "Lundberg, Scott M and Lee, Su-In",
        "title": "A unified approach to interpreting model predictions"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "montavon2017explaining",
        "author": "Montavon, Gr{\\'e}goire and Lapuschkin, Sebastian and Binder, Alexander and Samek, Wojciech and M{\\\"u}ller, Klaus-Robert",
        "title": "Explaining nonlinear classification decisions with deep taylor decomposition"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "bach2015pixel",
        "author": "Bach, Sebastian and Binder, Alexander and Montavon, Gr{\\'e}goire and Klauschen, Frederick and M{\\\"u}ller, Klaus-Robert and Samek, Wojciech",
        "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lundberg2017unified",
        "author": "Lundberg, Scott M and Lee, Su-In",
        "title": "A unified approach to interpreting model predictions"
      },
      {
        "key": "nam2020relative",
        "author": "Nam, Woo-Jeoung and Gur, Shir and Choi, Jaesik and Wolf, Lior and Lee, Seong-Whan",
        "title": "Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks"
      },
      {
        "key": "shrikumar2017learning",
        "author": "Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul",
        "title": "Learning important features through propagating activation differences"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "gu2019understanding",
        "author": "Gu, Jindong and Yang, Yinchong and Tresp, Volker",
        "title": "Understanding individual decisions of cnns via contrastive backpropagation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "iwana2019explaining",
        "author": "Iwana, Brian Kenji and Kuroki, Ryohei and Uchida, Seiichi",
        "title": "Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "qiang2022attcat",
        "author": "Qiang, Yao and Pan, Deng and Li, Chengyin and Li, Xin and Jang, Rhongho and Zhu, Dongxiao",
        "title": "Attcat: Explaining transformers via attentive class activation tokens"
      },
      {
        "key": "xie2022vit",
        "author": "Xie, Weiyan and Li, Xiao-Hui and Cao, Caleb Chen and Zhang, Nevin L",
        "title": "Vit-cx: Causal explanation of vision transformers"
      },
      {
        "key": "yu2023x",
        "author": "Yu, Lu and Xiang, Wei",
        "title": "X-pruner: explainable pruning for vision transformers"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "qiang2022attcat",
        "author": "Qiang, Yao and Pan, Deng and Li, Chengyin and Li, Xin and Jang, Rhongho and Zhu, Dongxiao",
        "title": "Attcat: Explaining transformers via attentive class activation tokens"
      },
      {
        "key": "xie2022vit",
        "author": "Xie, Weiyan and Li, Xiao-Hui and Cao, Caleb Chen and Zhang, Nevin L",
        "title": "Vit-cx: Causal explanation of vision transformers"
      },
      {
        "key": "yu2023x",
        "author": "Yu, Lu and Xiang, Wei",
        "title": "X-pruner: explainable pruning for vision transformers"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "abnar2020quantifying",
        "author": "Abnar, Samira and Zuidema, Willem",
        "title": "Quantifying Attention Flow in Transformers"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chefer2021transformer",
        "author": "Chefer, Hila and Gur, Shir and Wolf, Lior",
        "title": "Transformer interpretability beyond attention visualization"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "chefer2021generic",
        "author": "Chefer, Hila and Gur, Shir and Wolf, Lior",
        "title": "Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "wang2024visual",
        "author": "Wang, Ying and Rudner, Tim GJ and Wilson, Andrew G",
        "title": "Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "li2022exploring",
        "author": "Li, Yi and Wang, Hualiang and Duan, Yiqun and Xu, Hang and Li, Xiaomeng",
        "title": "Exploring visual interpretability for contrastive language-image pre-training"
      },
      {
        "key": "li2023clipsurgery",
        "author": "Li, Yi and Wang, Hualiang and Duan, Yiqun and Li, Xiaomeng",
        "title": "Clip surgery for better explainability with enhancement in open-vocabulary tasks"
      },
      {
        "key": "zhou2022extract",
        "author": "Zhou, Chong and Loy, Chen Change and Dai, Bo",
        "title": "Extract free dense labels from clip"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "zhou2022learning",
        "author": "Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei",
        "title": "Learning to prompt for vision-language models"
      },
      {
        "key": "yu2022coca",
        "author": "Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui",
        "title": "Coca: Contrastive captioners are image-text foundation models"
      },
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      },
      {
        "key": "wang2023position",
        "author": "Wang, Jinpeng and Zhou, Pan and Shou, Mike Zheng and Yan, Shuicheng",
        "title": "Position-guided Text Prompt for Vision-Language Pre-training"
      },
      {
        "key": "kim2023region",
        "author": "Kim, Dahun and Angelova, Anelia and Kuo, Weicheng",
        "title": "Region-aware pretraining for open-vocabulary object detection with vision transformers"
      },
      {
        "key": "wu2023clipself",
        "author": "Wu, Size and Zhang, Wenwei and Xu, Lumin and Jin, Sheng and Li, Xiangtai and Liu, Wentao and Loy, Chen Change",
        "title": "Clipself: Vision transformer distills itself for open-vocabulary dense prediction"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "xu2021simple",
        "author": "Xu, Mengde and Zhang, Zheng and Wei, Fangyun and Lin, Yutong and Cao, Yue and Hu, Han and Bai, Xiang",
        "title": "A simple baseline for zeroshot semantic segmentation with pre-trained vision-language model"
      },
      {
        "key": "liang2023open",
        "author": "Liang, Feng and Wu, Bichen and Dai, Xiaoliang and Li, Kunpeng and Zhao, Yinan and Zhang, Hang and Zhang, Peizhao and Vajda, Peter and Marculescu, Diana",
        "title": "Open-vocabulary semantic segmentation with mask-adapted clip"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "gu2021open",
        "author": "Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin",
        "title": "Open-vocabulary object detection via vision and language knowledge distillation"
      },
      {
        "key": "du2022learning",
        "author": "Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi",
        "title": "Learning to prompt for open-vocabulary object detection with vision-language model"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "kuo2023fvlm",
        "author": "Kuo, Weicheng and Cui, Yin and Gu, Xiuye and Piergiovanni, AJ and Angelova, Anelia",
        "title": "F-vlm: Open-vocabulary object detection upon frozen vision and language models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "wu2023cora",
        "author": "Wu, Xiaoshi and Zhu, Feng and Zhao, Rui and Li, Hongsheng",
        "title": "Cora: Adapting clip for open-vocabulary detection with region prompting and anchor pre-matching"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "yu2024fcclip",
        "author": "Yu, Qihang and He, Ju and Deng, Xueqing and Shen, Xiaohui and Chen, Liang-Chieh",
        "title": "Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      },
      {
        "key": "wang2023position",
        "author": "Wang, Jinpeng and Zhou, Pan and Shou, Mike Zheng and Yan, Shuicheng",
        "title": "Position-guided Text Prompt for Vision-Language Pre-training"
      },
      {
        "key": "kim2023region",
        "author": "Kim, Dahun and Angelova, Anelia and Kuo, Weicheng",
        "title": "Region-aware pretraining for open-vocabulary object detection with vision transformers"
      },
      {
        "key": "li2022grounded",
        "author": "Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others",
        "title": "Grounded language-image pre-training"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "chen2020uniter",
        "author": "Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing",
        "title": "Uniter: Universal image-text representation learning"
      },
      {
        "key": "li2020oscar",
        "author": "Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others",
        "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks"
      },
      {
        "key": "zhang2021vinvl",
        "author": "Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng",
        "title": "Vinvl: Revisiting visual representations in vision-language models"
      },
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      },
      {
        "key": "li2022grounded",
        "author": "Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others",
        "title": "Grounded language-image pre-training"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "liu2023grounding",
        "author": "Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others",
        "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection"
      },
      {
        "key": "li2022grounded",
        "author": "Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and others",
        "title": "Grounded language-image pre-training"
      },
      {
        "key": "krishna2017visual",
        "author": "Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others",
        "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "chen2020uniter",
        "author": "Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing",
        "title": "Uniter: Universal image-text representation learning"
      },
      {
        "key": "li2020oscar",
        "author": "Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others",
        "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks"
      },
      {
        "key": "zhang2021vinvl",
        "author": "Zhang, Pengchuan and Li, Xiujun and Hu, Xiaowei and Yang, Jianwei and Zhang, Lei and Wang, Lijuan and Choi, Yejin and Gao, Jianfeng",
        "title": "Vinvl: Revisiting visual representations in vision-language models"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "zhong2022regionclip",
        "author": "Zhong, Yiwu and Yang, Jianwei and Zhang, Pengchuan and Li, Chunyuan and Codella, Noel and Li, Liunian Harold and Zhou, Luowei and Dai, Xiyang and Yuan, Lu and Li, Yin and others",
        "title": "Regionclip: Region-based language-image pretraining"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "ren2015faster",
        "author": "Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian",
        "title": "Faster r-cnn: Towards real-time object detection with region proposal networks"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "wang2023position",
        "author": "Wang, Jinpeng and Zhou, Pan and Shou, Mike Zheng and Yan, Shuicheng",
        "title": "Position-guided Text Prompt for Vision-Language Pre-training"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "wu2023clipself",
        "author": "Wu, Size and Zhang, Wenwei and Xu, Lumin and Jin, Sheng and Li, Xiangtai and Liu, Wentao and Loy, Chen Change",
        "title": "Clipself: Vision transformer distills itself for open-vocabulary dense prediction"
      }
    ]
  }
]