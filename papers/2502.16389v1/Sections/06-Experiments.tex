\section{Experimental results}
\label{sec:experiments}

In our experiments, we evaluate the anomaly detection performance of Xen on the largest on-road video anomaly dataset, named the Detection of Traffic Anomaly (DoTA) dataset~\citep{yao2022dota}. DoTA is comprised of diverse video clips collected from dashboard cameras in different areas (e.g., East Asia, North America, Europe) under different weather (e.g., sunny, cloudy, raining, snowing) and lighting conditions (day and night). Each video in the dataset is recorded at $10$ fps and contains one anomalous event. Due to different video sources, camera intrinsics and poses are different across videos.

We use the same training and test split of DoTA as those in~\cite{yao2022dota} and further augment the test split with $88$ videos, which are provided in the dataset but unlabeled, by following the annotation principles in the original paper. The final training set consists of 3275 videos and the test set contains 1490 videos. Each video is annotated with anomaly start and end time, based on which a frame is labeled as $0$ or $1$ for a normal or anomalous time step, respectively, for evaluation purposes only. Notably, the anomaly start time in DoTA is defined as the time when the anomaly is inevitable, which is often prior to an actual accident (e.g., a car crash). Therefore, the evaluation reflects the model performance of early anomaly detection.

There exist $9$ categories of on-road anomaly in DoTA, and each category is further split into ego involved and non-ego involved cases, resulting in $18$ categories in total. For quick reference, we list the $9$ ego involved categories in Table~\ref{table:anomaly-category-DoTA}. To represent non-ego involved counterparts, we append a letter of ``N" to each label in Table~\ref{table:anomaly-category-DoTA}. For example, ST-N stands for non-ego involved collisions with another vehicle that starts, stops, or is stationary. We refer to~\cite{yao2022dota} for data samples in DoTA. Based on the analysis on common anomaly patterns in Section~\ref{sec:overview}, we classify all the anomaly categories in Table~\ref{table:anomaly-category-DoTA} as \textit{ego involved anomalies}, VO-N, OO-N, UK-N as \textit{non-ego involved individual anomalies}, and the rest $6$ categories as \textit{non-ego involved interactive anomalies}. Unsupervised AD models can only be trained with normal data, so we use the frames before the anomaly start time in each video in the training set for training. During evaluation, in contrast to the DoTA paper where the videos under unknown categories or without objects were ignored~\citep{yao2022dota}, we evaluate on all test videos as we believe that an anomaly detector needs to be robust in any driving scenarios.

\begin{table}[t]
  \begin{center}
    \caption{On-road anomaly category in DoTA~\citep{yao2022dota}.}
    \label{table:anomaly-category-DoTA}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ l | l }
      \toprule
      Label & Anomaly Category Description \\
      \midrule
      ST & \makecell[lt]{Collision with another vehicle that starts, stops, \\ or is stationary} \\
      AH & Collision with another vehicle moving ahead or waiting \\
      LA & \makecell[lt]{Collision with another vehicle moving laterally in \\ the same direction} \\
      OC & Collision with another oncoming vehicle \\
      TC & \makecell[lt]{Collision with another vehicle that turns into or \\ crosses a road} \\
      VP & Collision between vehicle and pedestrian \\
      VO & Collision with an obstacle in the roadway \\
      OO & Out-of-control and leaving the roadway to the left or right \\
      UK & Unknown \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

To realize Xen, we train the scene expert (including FFP and STR), the interaction expert, and the behavior expert individually. An Adam optimizer~\citep{kingma2014adam} without weight decay is used for all the experts. Common training hyperparameters are listed in Table~\ref{table:training-hyperparameters}. The model-specific implementation details are described as follows.

\paragraph{Scene expert:} The disparity values are clipped at $2000$ before normalization. The weights in the training objective~(\ref{eq:str-objective}) are specified as $\lambda_\text{d}=100$ and $\lambda_\text{tv}=0.1$.

\paragraph{Interaction expert:} The history horizon $T_\text{int}$ is set to be $3$. The embedding function $\phi_\text{encoder}$ is constructed by $2$ fully connected (FC) layers with $(32, 64)$ hidden units. The non-linear function $\phi_\text{z}$ is implemented as one FC layer with $4$ hidden units. Both the encoder and decoder GRU use a hidden size of $128$. In the decoder, the embedding function $\phi_\text{decoder}$ has the same structure as $\phi_\text{encoder}$, and the function $\psi_\text{decoder}$ is constructed by $2$ FC layers with $(64, 8)$ hidden units. ReLU activation functions are applied after each FC layer except for the last layer in $\psi_\text{decoder}$. The capacity of the priority queue $N_\text{max}$ is set to be $20$.

\paragraph{Behavior expert:} The prediction horizon $\delta$ is set to be $10$. The function $\phi_\text{encoder}^\text{box}$ and $\phi_\text{encoder}^\text{flow}$ have the same structure of $2$ FC layers with $(512, 64)$ hidden units. The embedding function $\phi_\text{encoder}^\text{image}$ is constructed by $2$ convolution layers with filter number $(160, 32)$ and filter size $3 \times 3$, a flattening operation, and one FC layer with $512$ hidden units. Each convolution layer is followed by a $2 \times 2$ max pooling layer. We implement $\phi_\text{encoder}^\text{fusion}$ with $2$ FC layers with $(512, 512)$ hidden units. All the GRUs have a hidden size of $512$. In the decoder, the non-linear function $\psi_\text{decoder}$ is constructed by $2$ FC layers with $(32, 4)$ hidden units. ReLU activation functions are applied after each max pooling layer and FC layer except for the last layer in $\phi_\text{encoder}^\text{fusion}$ and $\psi_\text{decoder}^\text{box}$.

Both the interaction and behavior expert use a low-pass Butterworth filter of order $2$ and cut-off frequency $0.2$ Hz.

\begin{table}[t]
  \begin{center}
    \caption{Determination of training hyperparameters. The learning rate is kept constant during training for each model.}
    \label{table:training-hyperparameters}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ l | c  c  c  c }
      \toprule
      Hyperparameter & FFP & STR & Interaction expert & Behavior expert \\
      \midrule
      Batch size & 4 & 4 & 64 & 16 \\
      Learning rate & 0.0002 & 0.0002 & 0.0002 & 0.0005 \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

\subsection{Baselines and metrics}
We evaluate the performance of the proposed method on the test set, along with the following baseline methods:
\begin{itemize}
\item
\textit{Conv-AE}~\citep{hasan2016learning}: An AE-based model which encodes temporally stacked RGB images with convolution layers and decodes the resulting features with deconvolution layers to reconstruct the input. The MSE over pixels in a frame is computed as the anomaly score.
\item
\textit{AnoPred}~\citep{liu2018future}: A unet-based model which takes four preceding RGB frames as input to directly predict a future frame. The negative PSNR of the predicted frame is used as the anomaly score.
\item
\textit{FOL}~\citep{yao2019unsupervised}: A recurrent encoder-decoder network which sequentially processes the past ego motions, object's bounding boxes, and optical flow features to predict future locations of the object over a short horizon. Among the three strategies of computing anomaly scores proposed in the original paper, we use the best-performing prediction consistency~(\ref{eq:fol-anomaly-score}) as our baseline.
\item
\textit{FOL-Ensemble}~\citep{yao2022dota}: An ensemble method which achieves the state-of-the-art results on the DoTA dataset. The object anomaly score, generated by FOL, is first mapped to per-pixel scores by putting a Gaussian function at the center of each object. Such a pseudo anomaly score map is then averaged with the error map generated by AnoPred to produce the fused anomaly score map. The averaged score over all the pixels in the fused score map serves as the final anomaly score.
\end{itemize}

All the baselines are adapted to DoTA based on the corresponding released code and original paper. We also perform an expert-level ablation study by sequentially removing the experts from Xen to investigate the necessity of each module. In the rest of this section, we use \textbf{Xen-S} to denote the ensemble of FFP and STR in the scene expert, \textbf{Xen-SI} the ensemble of the scene and interaction expert, and \textbf{Xen-SIB} the ensemble of all the three experts. The Kalman filter design remains unchanged for the ablated versions of Xen-SIB with only necessary modifications to account for the reduced dimensionality of the system. For a fair comparison, we use the same optical flow estimation network and object detection and tracking network to generate data for all the benchmarked methods if needed. All the models are trained on the same dataset. During evaluation, at the beginning of a video when a method is not able to produce an anomaly score due to insufficient observations as input, we simply assign a score assuming that the frame is normal.

In Xen, the start time of Kalman filtering plays an important role in final results. By default, we start the filtering only when all the experts start to produce anomaly scores, otherwise we keep applying the initialization step~(\ref{eq:kf-initialization}) by changing the subscript to the corresponding time step. An alternative way is to start the filtering immediately as a video starts. In other words, the initialization~(\ref{eq:kf-initialization}) is applied only at time $1$ and the assigned scores serve as the measurements of Kalman filter at the first few frames of a video. We mark the variants using the second filtering mechanism with $^\star$ in the rest of this paper.

Quantitatively, we compare different methods using the following two metrics:
\begin{itemize}
\item
\textit{F1-score}: A comprehensive threshold-dependent index considering precision and recall, which can be written as $2 \cdot \text{precision} \cdot \text{recall} / (\text{precision} + \text{recall})$.
\item
\textit{AUC}: A threshold-independent index indicating the area under the receiver operating characteristics curve. AUC describes the ability to distinguish between positive and negative samples for anomaly detection models.
\end{itemize}

Following common practice in video AD literature, we concatenate the anomaly score vectors of all the videos before computing the metrics rather than average the metrics of each video. However, prior works often apply a video-wise min-max normalization on scores before the concatenation~\citep{hasan2016learning,liu2018future,liu2021hybrid,yao2022dota}. Such a normalization step can lead to overoptimistic performance, especially when each video contains an anomaly. For example, consider a case where video A of length $5$ has a ground truth label of $(0, 0, 1, 1, 1)$ and predicted anomaly scores of $(0.3, 0.5, 0.6, 0.7, 0.6)$ and video B has the same ground truth but different anomaly scores of $(1.2, 1.0, 1.6, 2.0, 1.8)$. The concatenated scores with and without video-wise normalization is shown in Figure~\ref{fig:metrics-eg}. It is apparent that such an anomaly detector is not perfect since the predicted anomaly scores of the last three \textit{anomalous} frames in video A are lower than those of the first two \textit{normal} frames in video B. With video-wise normalization, however, the results falsely suggest that the anomaly detector is perfect with an AUC of $1$. From another perspective, the min-max normalization implicitly provides additional information that each video contains at least one anomaly as the maximum anomaly score is guaranteed to be $1$ in each video. Furthermore, the normalization is simply impossible in online AD. Therefore, we compute the metrics without video-wise min-max normalization for more realistic evaluation in this work.

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.49\linewidth}
    \captionsetup{justification=centering}
    \includegraphics[width=\linewidth]{Figures/fig9a_unnormalized_score_final.pdf}
    \caption{Without normalization}
  \end{subfigure}
  \begin{subfigure}[b]{0.49\linewidth}
    \captionsetup{justification=centering}
    \includegraphics[width=\linewidth]{Figures/fig9b_normalized_score_final.pdf}
    \caption{With normalization}
  \end{subfigure}
  \caption{\textbf{An illustrative example showing that the video-wise min-max normalization distorts the actual model performance.} The results from two videos are concatenated. The ground truth label is annotated on top. Frame-level anomaly score is shown above each bar.}
  \label{fig:metrics-eg}
\end{figure}

\begin{table}[t]
  \begin{center}
    \caption{Comparison of anomaly detection performance with different methods on DoTA. The variants of Xen without $^\star$ start Kalman filtering only when all the experts start to produce anomaly scores, while those with $^\star$ start filtering since the video start time.}
    \label{table:overall-results}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{  l  l  c  c  }
      \toprule
      Method & Input & AUC & F1-score \\
      \midrule
      Conv-AE & RGB & $58.7$ & $50.8$ \\
      AnoPred & RGB & $67.9$ & $56.0$ \\
      FOL & Flow + Box + Ego & $65.0$ & $53.7$ \\
      FOL-Ensemble & RGB + Flow + Box + Ego & $68.4$ & $56.2$ \\
      \midrule
      Xen-S & RGB + Flow + Depth & $69.1$ & $56.4$ \\
      Xen-SI & RGB + Flow + Depth + Box & $72.5$ & $58.2$ \\
      Xen-SIB & RGB + Flow + Depth + Box & $\mathbf{74.4}$ & $\mathbf{60.0}$ \\
      \midrule
      Xen-S$^\star$ & RGB + Flow + Depth & $71.4$ & $57.6$ \\
      Xen-SI$^\star$ & RGB + Flow + Depth + Box & $73.9$ & $59.4$ \\
      Xen-SIB$^\star$ & RGB + Flow + Depth + Box & $\mathbf{75.1}$ & $\mathbf{60.5}$ \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

\subsection{Overall results}
The overall results are presented in Table~\ref{table:overall-results}. As shown, Xen-SIB achieves the best AUC and highest F1-score with a large margin over the four baselines. Although we were able to reproduce the performance of FOL in the original paper~\citep{yao2022dota} with $0.1$ difference in AUC, the AD metrics drop significantly after evaluating on all test videos and removing the video-wise min-max normalization. We argue that this is partially due to the fact that object-centric methods can completely fail in \textit{ego involved anomalies} where no traffic participants are detected within the field of view. FOL-Ensemble alleviates such a problem by fusing the scores from the frame-level method AnoPred and the object-centric method FOL. In fact, all the ensemble methods (e.g., Xen-SI and Xen-SIB), which combine frame-level and object-level features for AD, show superior AUC and F1-score compared to their individual component (e.g., Xen-S) in the ensemble. With a closer observation, we found that the STR alone in the scene expert produces an AUC of $64.1$, which is much higher than that from Conv-AE. Considering that STR and Conv-AE are both reconstruction-based method and have similar architecture, such a result indicates that the spatiotemporal representation (i.e., optical flow and disparity) of a scene is more effective as inputs for AD than the simple RGB representation. By combining the reconstruction-based STR and the prediction-based FFP through Kalman filter, our Xen-S can already achieve reasonable improvement over the baselines.

To our knowledge, Xen-SIB is the first that is built upon a holistic analysis on anomaly patterns. Our ablation study shows that the detection of each edge in Figure~\ref{fig:anomaly-patterns} contributes to the overall performance of Xen-SIB. Specifically, Xen-SI achieves higher AUC and F1-score than Xen-S by including the edge of \textit{non-ego involved interactive anomalies} so that anomalous interactions between other road participants can be monitored and detected. Xen-SIB, which encompasses all the edges in Figure~\ref{fig:anomaly-patterns}, shows the best AD performance among all the ablated versions of Xen-SIB due to the ability to model the most diverse normal patterns in driving scenarios. Notably, compared to applying Kalman filter after all the experts start to produce anomaly scores, filtering immediately since time $1$ consistently performs better in all the ablations. Such an improvement is expected. On the one hand, videos often start with normal scenarios. On the other hand, we assign scores assuming that the frame is normal for an expert at the beginning of a video when the observations are insufficient to make an inference. The effects of these assigned scores are then propagated through the system dynamics~(\ref{eq:system-dynamics}), biasing the immediately subsequent anomaly scores to be low. Although making use of assigned scores, such a filtering mechanism is indeed applicable in the real world. However, the performance gap between the two filtering techniques will be smaller with the increase of the video length.

\begin{table}[t]
  \begin{center}
    \caption{Comparison of anomaly detection performance with different methods in ego involved and non-ego involved cases. The variants of Xen without $^\star$ start Kalman filtering only when all the experts start to produce anomaly scores, while those with $^\star$ start filtering since the video start time. In general, frame-level methods (i.e., Conv-AE, AnoPred, and Xen-S) perform well in ego involved cases, object-centric methods (i.e., FOL) excel in non-ego involved cases, and ensemble methods take advantage of both types of methods.}
    \label{table:ego-non-ego-results}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ l  c  c  c  c  c }
      \toprule
      \multirow{2}{*}{Method} & \multicolumn{2}{c}{Ego involved} & & \multicolumn{2}{c}{Non-ego involved} \\
                              & AUC & F1-score & & AUC & F1-score \\
      \midrule
      Conv-AE & $63.9$ & $51.9$ & & $51.2$ & $49.5$ \\
      AnoPred & $73.0$ & $58.7$ & & $60.8$ & $52.6$ \\
      FOL & $64.8$ & $53.2$ & & $65.6$ & $54.3$ \\
      FOL-Ensemble & $73.4$ & $59.2$ & & $61.3$ & $52.4$ \\
      \midrule
      Xen-S & $74.4$ & $58.9$ & & $62.0$ & $53.2$ \\
      Xen-SI & $76.0$ & $60.7$ & & $67.7$ & $54.9$ \\
      Xen-SIB & $\mathbf{77.3}$ & $\mathbf{62.3}$ & & $\mathbf{70.6}$ & $\mathbf{57.1}$ \\
      \midrule
      Xen-S$^\star$ & $76.7$ & $60.1$ & & $64.5$ & $54.3$ \\
      Xen-SI$^\star$ & $77.4$ & $62.2$ & & $69.1$ & $55.9$ \\
      Xen-SIB$^\star$ & $\mathbf{77.9}$ & $\mathbf{62.9}$ & & $\mathbf{71.3}$ & $\mathbf{57.4}$ \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

\begin{table*}[t]
  \begin{center}
    \caption{Comparison of anomaly detection performance with different methods on each type of anomaly. Conv-AE, Xen-S, and the variants of Xen using immediate filtering since the video start time are omitted for brevity.}
    \label{table:fine-grained-results}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ l  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c  c }
      \toprule
      & \multicolumn{9}{c}{Ego} & & \multicolumn{6}{c}{Non-ego interactive} & & \multicolumn{3}{c}{Non-ego individual} \\
      \cmidrule{2-10}
      \cmidrule{12-17}
      \cmidrule{19-21}
      & ST & AH & LA & OC & TC & VP & VO & OO & UK & & ST-N & AH-N & LA-N & OC-N & TC-N & VP-N & & VO-N & OO-N & UK-N \\
      \midrule
      Method & \multicolumn{20}{c}{AUC} \\
      \midrule
      AnoPred & $72.3$ & $75.7$ & $74.7$ & $68.7$ & $73.6$ & $70.2$ & $78.4$ & $72.0$ & $64.9$ & & $63.6$ & $58.4$ & $59.6$ & $63.9$ & $62.6$ & $64.2$ & & $57.9$ & $59.0$ & $60.8$ \\
      FOL & $72.7$ & $73.4$ & $66.5$ & $\mathbf{76.6}$ & $72.4$ & $67.1$ & $54.4$ & $44.1$ & $56.5$ & & $\mathbf{75.2}$ & $\mathbf{68.2}$ & $62.8$ & $66.8$ & $68.4$ & $74.8$ & & $57.3$ & $64.8$ & $60.9$ \\
      FOL-Ensemble & $73.0$ & $76.4$ & $75.3$ & $69.3$ & $74.3$ & $70.1$ & $78.9$ & $71.9$ & $65.1$ & & $65.3$ & $59.0$ & $59.8$ & $64.4$ & $63.4$ & $65.6$ & & $58.3$ & $59.3$ & $61.5$ \\
      Xen-SI (ours) & $71.8$ & $79.1$ & $\mathbf{79.4}$ & $71.0$ & $76.8$ & $69.2$ & $79.5$ & $\mathbf{74.8}$ & $70.9$ & & $70.9$ & $65.9$ & $65.0$ & $72.1$ & $70.7$ & $73.7$ & & $60.0$ & $66.6$ & $67.4$ \\
      Xen-SIB (ours) & $\mathbf{74.5}$ & $\mathbf{80.2}$ & $\mathbf{79.5}$ & $75.7$ & $\mathbf{79.2}$ & $\mathbf{72.7}$ & $\mathbf{80.0}$ & $74.1$ & $\mathbf{71.5}$ & & $70.5$ & $67.9$ & $\mathbf{66.8}$ & $\mathbf{75.5}$ & $\mathbf{72.3}$ & $\mathbf{77.1}$ & & $\mathbf{60.2}$ & $\mathbf{72.3}$ & $\mathbf{71.2}$ \\
      \midrule
      Method & \multicolumn{20}{c}{F1-score} \\
      \midrule
      AnoPred & $50.0$ & $56.9$ & $62.2$ & $52.4$ & $56.5$ & $50.4$ & $56.1$ & $68.1$ & $57.4$ & & $38.9$ & $51.3$ & $55.7$ & $44.3$ & $49.0$ & $46.7$ & & $50.9$ & $57.7$ & $\mathbf{62.3}$ \\
      FOL & $52.9$ & $55.6$ & $58.0$ & $56.1$ & $55.3$ & $46.6$ & $42.3$ & $35.0$ & $46.6$ & & $\mathbf{47.9}$ & $53.4$ & $54.7$ & $45.8$ & $53.5$ & $53.3$ & & $51.2$ & $58.4$ & $55.6$ \\
      FOL-Ensemble & $51.0$ & $57.7$ & $62.8$ & $52.8$ & $56.9$ & $\mathbf{50.8}$ & $57.9$ & $68.2$ & $57.6$ & & $39.1$ & $51.3$ & $55.5$ & $44.8$ & $49.4$ & $47.3$ & & $\mathbf{51.5}$ & $56.8$ & $61.0$ \\
      Xen-SI (ours) & $52.5$ & $60.5$ & $64.8$ & $53.9$ & $58.2$ & $48.3$ & $\mathbf{60.7}$ & $\mathbf{68.8}$ & $\mathbf{58.5}$ & & $44.0$ & $\mathbf{54.1}$ & $54.7$ & $46.9$ & $54.3$ & $51.7$ & & $47.6$ & $59.2$ & $62.1$ \\
      Xen-SIB (ours) & $\mathbf{54.2}$ & $\mathbf{62.6}$ & $\mathbf{65.5}$ & $\mathbf{57.0}$ & $\mathbf{60.9}$ & $50.3$ & $59.9$ & $67.9$ & $57.2$ & & $47.0$ & $53.4$ & $\mathbf{56.1}$ & $\mathbf{51.6}$ & $\mathbf{55.9}$ & $\mathbf{59.5}$ & & $50.6$ & $\mathbf{62.8}$ & $60.8$ \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table*}

\subsection{Per-class results}
Table~\ref{table:ego-non-ego-results} shows the results of different methods broken out into ego involved and non-ego involved cases. In general, non-ego involved anomalies are more difficult to detect than ego involved ones, possibly because non-ego anomalies are often accompanied with small anomalous regions and low camera visibility of objects. By comparing Xen-SI with frame-level methods (e.g., AnoPred and Xen-S), we found that the improvement in non-ego involved cases is much more significant than that in ego involved cases, which is expected since the interaction expert mainly enhances the object-level AD performance. A similar improvement is observed when further including the behavior expert in Xen-SIB, which achieves the best AUC and F1-score in both ego involved and non-ego involved cases. By contrast, the object-centric method FOL performs relatively well in non-ego involved cases but struggles to excel in ego involved cases, resulting in overall inferior performance compared to Xen variants. Despite the fusion of frame-level and object-centric method, FOL-Ensemble fails to provide as large improvements as Xen-SIB does over each component (e.g., AnoPred) in the ensemble, which highlights the importance of effective fusion mechanism for AD in complex driving scenarios. Last but not least, in both ego involved and non-ego involved cases, Xen variants with immediate filtering since time $1$ still consistently outperform those with filtering since the time when all experts start to produce anomaly scores, which agrees with the findings in Table~\ref{table:overall-results}.

More fine-grained results on the AD performance for each type of anomaly are presented in Table~\ref{table:fine-grained-results}. We observe that Xen-SI and Xen-SIB exhibit the highest AUC and F1-score in most columns. FOL demonstrates good AD performance in some of the non-ego involved anomalies (e.g., ST-N and AH-N), but completely fails in several ego involved anomalies (e.g., VO and OO), which is expected as no anomalous road participants can be detected in such cases. Second, by comparing Xen-SI and Xen-SIB, we found that the behavior expert mainly helps improve the detection of \textit{non-ego involved individual anomalies}, which meets our design goal. Meanwhile, \textit{non-ego involved interactive anomalies} are also better detected with the behavior expert in Xen-SIB as interactive anomalies can also contain anomalous behaviors of individual objects. Third, certain classes, especially the non-ego involved ones, are especially challenging for all the anomaly detectors. For example, obstacles in non-ego vehicle-obstacle anomalies (VO-N) are often occluded and the tracking of the impacted vehicle is often lost after the collision, making the abnormal motions invisible to the anomaly detector; and vehicles in non-ego lateral anomalies (LA-N) often move towards each other slowly before the accident, making the anomaly relatively subtle and hard to detect.

\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/fig10_qualitative_results_revision.pdf}
  \caption{\textbf{Qualitative comparisons of different methods in typical scenarios.} Anomaly scores from different methods lie in different ranges, so each plot contains $2$ scales for different models. The PSNR from AnoPred is negated to serve as the anomaly score. The bounding boxes in sample video frames mark the anomaly participants. The time index of each frame is annotated on the top right corner of the image.}
  \label{fig:qualitative-results}
\end{figure*}

Figure~\ref{fig:qualitative-results} shows the anomaly detection results of different methods in several typical scenarios. In the first example with an ego vehicle-obstacle anomaly (VO), the ego car collides with two roadblocks while moving forward. FOL fails the AD task as no road participants are detected in the anomalous region, and the peaks of the scores are due to the intermittent detection of distant cars in the scene. AnoPred and FOL-Ensemble successfully catch the anomaly after the collision due to the visual inconsistency of the frames but refuse to raise the score at the beginning of the anomaly. By contrast, Xen-SIB is able to produce a high anomaly score earlier than AnoPred and FOL-Ensemble, as the STR in the scene expert can directly detect abnormal scenes (e.g., near-collisions) through spatiotemporal reconstruction. The second example shows a non-ego lateral anomaly (LA-N) between the white car and the red truck. Such an anomaly is challenging for frame-level methods as the anomalous objects only occupy a small region of the scene. Due to this reason, AnoPred fails to increase the score during the anomaly. From the perspective of the ego camera, car motions in this example are not as dramatic as those in a severe proximate accident. As a result, FOL predictions are relatively consistent for each vehicle during the anomaly, leading to miss detection. Thanks to the expert ensemble, Xen-SIB is able to monitor both abnormal individual motions and abnormal relative motions between two road participants and thus is the only method that detects the anomaly successfully. The third example contains a non-ego turning anomaly (TC-N), where the white car collides with a motorcyclist while turning. Xen-SIB generates high scores during the anomaly while the other three methods either produce indistinguishable scores (AnoPred) or result in false alarms (FOL and FOL-Ensemble). The last example demonstrates the importance of object-centric methods through another non-ego turning anomaly (TC-N). Two cars are involved in a side collision at an intersection. AnoPred still fails in such a non-ego involved anomaly. With the fusion of FOL and AnoPred, relatively high anomaly scores appear in the middle of the anomaly using FOL-Ensemble but only last for less than $3$ seconds. By making use of a more effective ensemble of frame-level and object-centric methods, Xen-SIB manages to catch the anomaly over a longer window than the baseline.

\begin{figure*}
  \centering
  \includegraphics[width=0.9\linewidth]{Figures/fig11_behavior_expert_ablation.pdf}
  \caption{\textbf{Qualitative comparisons of scaled and unscaled anomaly scores from the behavior expert.} The scaled and unscaled scores are generated by the model in the third and fourth row in Table~\ref{table:behavior-expert-ablations}, respectively.}
  \label{fig:behavior-expert-ablation}
\end{figure*}

\subsection{Additional analysis on behavior expert}
Object-centric methods have been shown critical in traffic AD in first-person videos~\citep{yao2019unsupervised,yao2022dota,fang2022traffic}. As one of the most effective object-centric approaches, FOL inspires our work and serves as the backbone of the behavior expert. To further verify the benefits of the modifications we made from FOL for the behavior expert, we perform an additional ablation study. Specifically, we aim to answer the following two questions:
\begin{enumerate*}[label=(\arabic*)]
\item
does each input modality (i.e., bounding boxes, optical flow features, and the RGB image) jointly contribute to the detection of anomalous objects and
\item
does the scaling of scores with bounding box heights benefit the final AD performance?
\end{enumerate*}

\begin{table}[t]
  \begin{center}
    \caption{Ablation study on the behavior expert.}
    \label{table:behavior-expert-ablations}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ c  c  c  c  c }
      \toprule
      Bounding box & Optical flow & Image & Scaling & AUC \\
      \midrule
      \checkmark & - & - & \checkmark & $67.0$ \\
      \checkmark & \checkmark & - & \checkmark & $68.2$ \\
      \checkmark & \checkmark & \checkmark & \checkmark & $\mathbf{68.4}$ \\
      \checkmark & \checkmark & \checkmark & - & $66.7$ \\
      \bottomrule
    \end{tabular}}
  \end{center}
\end{table}

We compute the AUC of different ablated versions of the behavior expert on the test set without expert ensemble for a direct comparison. The results are summarized in Table~\ref{table:behavior-expert-ablations}. Optical flow provides additional information on object motions and thus facilitate the prediction of future object locations in normal scenarios. As a result, by comparing the first and the second row, a large improvement in AUC is observed with the inclusion of optical flow features in the input. Additionally, by encoding the perceptual information in the RGB image, the behavior expert is made aware of the environment and is able to further enhance the AD performance. Although the improvement is minor compared to the addition of optical flows, we choose to keep the image pipeline in the behavior expert as the backbone MobileNetV2 has been shown efficient for online tasks~\citep{orsic2019defense}. More importantly, we found that the scaling of anomaly scores with bounding box heights is essential to achieve good AD performance. Without the scaling, the AUC with all three input modalities is even lower than that with only bounding boxes as input but with scaling. Such a phenomenon is due to the fact that STD values of bounding box coordinates may not faithfully reflect the prediction consistency of a model, as detailed in Section~\ref{subsec:bm} and Figure~\ref{fig:height-scaling-eg}.

Figure~\ref{fig:behavior-expert-ablation} further demonstrates the effect of score scaling in two videos with non-ego out-of-control anomalies (OO-N). In both examples, the anomalous car starts swerving in the distance and keeps moving towards the ego car. No matter how far the anomalous object is, the anomaly is present as long as the object can be detected by the camera. Without scaling, however, the score from the behavior expert remains low at the beginning of the anomaly as STD values of the predicted bounding box coordinates are generally small for distant objects. By contrast, the score rectified by bounding box heights can indicate the anomaly more confidently than without scaling in both cases, leading to a clearer separation between the normal and the anomaly. As a result of the ablation study, we adopt the third row in Table~\ref{table:behavior-expert-ablations} with all the inputs and score scaling as the behavior expert in Xen-SIB.

\subsection{Model complexity}
We further conduct a comparative study on model complexity of different methods. Focusing on anomaly detection, we exclude the complexity of other auxiliary tasks (e.g., object detection, optical flow estimation, depth estimation) in each method as these tasks are already performed on self-driving cars~\citep{shi2022csflow,godard2019digging,du2020online}. The results are summarized in Table~\ref{table:model-complexity}. In general, methods that require direct computation on image pixels has a larger number of parameters and FLOPs than those relying on bounding boxes. Compared to the other ensemble method, FOL-ensemble, Xen-SIB has fewer parameters but more FLOPs, mainly due to the scene expert. Notably, the three experts in Xen-SIB are independent of each other by design and can make inference simultaneously. As a result, the inference time of Xen-SIB will be the maximum of that of the three experts. Furthermore, the scene expert can also be parallelized to improve the efficiency bottleneck in Xen-SIB, as FFP (Figure~\ref{fig:ffp}) and STR (Figure~\ref{fig:str}) are independent.

\begin{table}[t]
  \begin{center}
    \caption{Model complexity of different methods with hyperparameters specified in the experiments.}
    \label{table:model-complexity}
    \begin{tabular}{  l  c  c  }
      \toprule
      Method & \# params & FLOPs \\
      \midrule
      Conv-AE & $11.3$M & $114.9$G \\
      AnoPred & $7.7$M & $42.2$G \\
      FOL & $5.2$M & $32.8$M \\
      FOL-Ensemble & $12.9$M & $42.2$G \\
      \midrule
      Scene expert & $8.1$M & $57.9$G \\
      Interaction expert & $0.2$M & $0.5$M \\
      Behavior expert & $3.5$M & $10.8$M \\
      \midrule
      Xen-SIB & $11.8$M & $57.9$G \\
      \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\subsection{Anomaly type classification by unsupervised learning}
Although no class labels on the anomaly type were used during training, Xen-SIB is able to classify videos by taking advantage of the experts, which are specialized for different anomaly patterns. As an extension to bring additional insight to the detection, we introduce binary classification of ego involved and non-ego involved anomalies for each video. The outputs of such a classifier can have practical implications: an ego involved anomaly requires immediate action from the ego car while a non-ego involved anomaly may only need extra attention.

To construct a classifier from Xen-SIB, we make use of the first four states of Kalman filter. Specifically, for a video of length $T$, we take the mean of the highest $10\%$ of the values along the time axis for each state $x_{1:T}^\text{exp}$, denoted as $m^\text{exp}$. Under the assumption that the scene expert mainly captures ego-involved anomalies and the interaction and behavior expert mainly capture non-ego involved anomalies, the decision is made by:
\begin{equation}
\label{eq:classifier-decision}
y
=
\begin{cases}
\text{ego involved,} & m^\text{ffp} + m^\text{str} > m^\text{int} + m^\text{beh}\\
\text{non-ego involved,} & m^\text{ffp} + m^\text{str} <= m^\text{int} + m^\text{beh}
\end{cases}
\end{equation}
where $y$ is the predicted class label.

We evaluate the classifier on all the test videos. The results are summarized in Table~\ref{table:confusion-matrix-classifier}. It has been shown in prior works that traffic anomaly classification is challenging, with all the benchmarked \textit{supervised} methods suffering from low accuracy on DoTA~\citep{yao2022dota}. The problem is made more difficult and seemingly impossible under the setting of unsupervised learning in this paper. Nevertheless, Xen-SIB still performs much better than a random classifier, achieving $67.3\%$ and $57.7\%$ accuracy for ego involved and non-ego involved anomalies, respectively. With a closer observation, we found that the scene expert can detect some of the non-ego involved anomalies when the anomalous object is proximate to the ego car and occupies a large region of the frame. Meanwhile, the behavior expert can also produce high anomaly scores for some of the ego involved anomalies when the object's motion in the image plane becomes unpredictable due to the sudden change of the ego car's dynamics. As a result of the above two phenomena, the assumption behind the decision~(\ref{eq:classifier-decision}) is not always true for all driving videos, leading to a misclassification of the anomaly type.

\begin{table}[t]
  \begin{center}
    \caption{Confusion matrix over all the test videos.}
    \label{table:confusion-matrix-classifier}
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{ c  l  c  c }
      \multicolumn{2}{c}{} & \multicolumn{2}{c}{\bfseries Predicted} \\
      \multicolumn{2}{c}{} & ego involved & non-ego involved \\
      \cmidrule{2-4}
      \bfseries Actual & ego involved & $67.3$ & $32.7$ \\
      & non-ego involved & $42.3$ & $57.7$ \\
      \cmidrule{2-4}
    \end{tabular}}
  \end{center}
\end{table}