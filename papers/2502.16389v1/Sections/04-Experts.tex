\section{Anomaly detection experts in autonomous driving}

In this section, we present the model architecture, training objective, and anomaly score generation of each expert. Specifically, \textit{scene}, \textit{interaction}, and \textit{behavior} experts are introduced for \textit{ego involved}, \textit{non-ego involved interactive}, and \textit{non-ego involved individual} anomalies, respectively, as identified in Section~\ref{sec:overview}. Targeting online AD, our overall design philosophy is to keep each module parallelizable to one another and easy to implement. All the experts are trained independently with a large-scale dataset of normal driving videos without any anomalies.

\subsection{Scene expert}

Some of the on-road anomalies are often accompanied with large unexpected scene changes in egocentric videos, especially for \textit{ego involved anomalies}. In events where the ego dynamics deviate significantly from the nominal one (e.g., when the ego car swerves, rotates suddenly due to a side collision, is rear-ended), future frames become unpredictable from the perspective of the ego camera, leading to a large prediction error in video frames. However, false negatives can still happen if we only rely on detecting large inconsistency across frames. For example, when the ego car slips forward slowly and hits another car in the back, future frames can still be predicted accurately due to the slow motion of the car, making the system refuse to raise an alert. A useful hint for anomaly detection in such a case is the rarity of the scene itself: the preceding car would never be this close to the ego camera (spatial features) and keep getting closer (temporal features) in normal scenarios.

In order to detect \textit{ego involved anomalies} in both cases of significant dynamics change and slow motions, we assemble the frame-level \textit{scene expert} with two submodules: future frame prediction (FFP) and spatial-temporal reconstruction (STR).

\subsubsection{Future frame prediction.}
FFP aims to detect anomalies by monitoring large differences between the predicted future frame and the actual future frame. The preceding $T_\text{ffp}$ frames are used for prediction.

It is known that optical flow information plays an important role in predicting future images: given the image $I_t$ and the optical flow $f_t$ between $I_t$ and $I_{t+1}$, the future image $I_{t+1}$ can be predicted accurately by shifting pixels~\citep{fang2022traffic}. However, $f_t$ is unknown at time $t$. A common solution is to utilize the optical flow information retrospectively during training: the predicted image $\hat{I}_{t+1}$ is first generated, then the difference between the induced optical flow and the ground truth optical flow at time $t$ is incorporated into the loss function for optimization~\citep{liu2018future,ye2019anopcn}. Alternatively, we explicitly predict the scene motion $\hat{f}_t$, which will also be utilized in STR, before frame prediction. The network structure of such an optical flow prediction (OFP) module is shown in Figure~\ref{fig:ofp}. The preceding $T_\text{ffp}$ RGB images $\mathbf{I}_{t - T_\text{ffp} + 1:t} \coloneqq (I_{t - T_\text{ffp} + 1}, I_{t - T_\text{ffp} + 2}, \cdots, I_t)$ are first processed by pretrained RAFT~\citep{teed2020raft}, an optical flow estimation network, to generate scene motions $\mathbf{f}_{t - T_\text{ffp} + 1:t - 1} \coloneqq (f_{t - T_\text{ffp} + 1}, f_{t - T_\text{ffp} + 1}, \cdots, f_{t - 1})$ in the past $T_\text{ffp} - 1$ frames. We perform RAFT inference at a relatively high resolution of $384 \times 672$ to ensure the quality of the estimated optical flow, which is then downsampled to $256 \times 256$ for fast training\endnote{Each entry value in the predicted optical flow should also be scaled proportionally to the ratio change of downsampling to maintain the validity of the flow at the new resolution.}. The resulting optical flows $\mathbf{f}_{t - T_\text{ffp} + 1:t - 1}$ in pixel units are then concatenated temporally and fed as the input to subsequent layers of OFP, which are based on u-net~\citep{ronneberger2015u}, to predict the current scene motion $\hat{f}_t$. The resolution of each feature map at the same level stays unchanged by using a padding of 1 in each convolution layer to avoid the cropping operation in the u-net paper. To reach a balance between the computational complexity and performance, we set $T_\text{ffp}$ as 4, similar to prior works~\citep{liu2018future,fang2022traffic}.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Figures/fig3_ofp.pdf}
  \caption{\textbf{Model architecture of OFP.} Each box corresponds to a multi-channel feature map. The number of channels is denoted on top of the box. The spatial resolution is provided at the lower left edge of the box. Arrows with different colors denote different operations.}
  \label{fig:ofp}
\end{figure}

Upon obtaining $\hat{f}_t$ from OFP, the future frame can be predicted by combining $I_t$ and $\hat{f}_t$. The details of FFP are illustrated in Figure~\ref{fig:ffp}. The channel-wise concatenation of the current downscaled RGB image and the predicted optical flow are processed sequentially by a convolution layer, two residual convolution blocks, and a final convolution layer to generate a future RGB image $\hat{I}_{t+1}$. The spatial resolution of all the feature maps, including the input and output, within FFP is kept as $256 \times 256$. Tanh is used as the final activation function as the pixel values in images are normalized to $[-1, 1]$.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Figures/fig4_ffp.pdf}
  \caption{\textbf{Model architecture of FFP.} Each circle and box represents an operation. The last number in the box of each convolution layer denotes the number of filters.}
  \label{fig:ffp}
\end{figure}

We train both networks jointly in one stage. To make the prediction close to the ground truth, $L_2$ and gradient loss are used, following prior works~\citep{liu2018future}. The $L_2$ loss aims to guarantee the similarity of all pixels in RGB space by penalizing the distance between a predicted frame $\hat{I}$ and the corresponding actual frame $I$:
\begin{equation}
L_2(\hat{I}, I) = \| \hat{I} - I \|_2^2.
\end{equation}
The gradient loss encourages a sharp synthetic image with the form of:
\begin{equation}
\begin{aligned}
L_\text{grad}(\hat{I}, I) = \sum_{i,j,c} &\left|| \hat{I}_{i,j,c} - \hat{I}_{i-1,j,c} | - | I_{i,j,c} - I_{i-1,j,c} |\right| \\
+
&\left|| \hat{I}_{i,j,c} - \hat{I}_{i,j-1,c} | - | I_{i,j,c} - I_{i,j-1,c} |\right|,
\end{aligned}
\end{equation}
where $i,j,c$ denote the spatial index of an element within an RGB image. In addition, we introduce an optical flow loss based on smooth $L_1$ loss~\citep{girshick2015fast} to supervise the learning of the intermediate optical flow:
\begin{equation}
L_\text{of} (\hat{f}, f) = \text{smooth}_{L_1} (\hat{f} - f),
\end{equation}
where $\hat{f}$ is the output of OFP, and $f$ is given by applying RAFT on the two corresponding ground truth images.

The overall training objective of FFP is a an addition of the intensity, gradient, and optical flow loss:
\begin{equation}
\begin{aligned}
L_\text{ffp} = L_2 (\hat{I}_{t+1}, I_{t+1}) + L_\text{grad} (\hat{I}_{t+1}, I_{t+1}) + L_\text{of} (\hat{f}_t, f_t).
\end{aligned}
\end{equation}
Although the relative weights between the three losses can be further tuned, we choose an identical value for all the terms for simplicity.

We denote the anomaly detector using a trained FFP as a function $g_\text{ffp}: \mathbf{I}_{t - T_\text{ffp}:t} \mapsto s_t^\text{ffp}$, which takes as input $T_\text{ffp} + 1$ images and outputs an anomaly score $s_t^\text{ffp}$ for time $t$. Specifically, the first $T_\text{ffp}$ images are used to produce the predicted frame $\hat{I}_t$, and the anomaly score is calculated as the negative Peak Signal to Noise Ratio (PSNR):
\begin{equation}
s_t^\text{ffp} = -\text{PSNR}(\hat{I}_t, I_t) = 10\log_{10} \left(\frac{1}{M} \| \hat{I}_t - I_t \|_2^2\right),
\end{equation}
where $M$ is the number of pixels in an image, and $\hat{I}_t$ and $I_t$ are normalized to $[0, 1]$ before anomaly score computation. A higher anomaly score $s_t^\text{ffp}$ (i.e., lower PSNR) indicates a higher probability of an anomaly present at time $t$.

\subsubsection{Spatial temporal reconstruction.} Reconstructing sensor signals is another pervasive approach in unsupervised AD as unseen scenarios can be directly monitored and detected. Frame reconstruction can complement frame prediction in cases where the anomaly is not accompanied with large inconsistency across frames, such as a slight rear-end collision due to slipping or scratches with guardrails due to gradual deviation from the lane center.

One of the key design choices in scene reconstruction is the input space. A natural idea is to represent a scene in RGB or grayscale values~\citep{luo2017remembering,hasan2016learning}. However, in complex driving scenarios, the intra-class variance of samples using such a representation is high, thus leading to frequent false alarms. For example, if there is a colorful car for some special events driving in front of the ego car, the scene is likely to be categorized as an anomaly as such a car painting has never been observed in normal training data, although the scene should belong to normal cases as all the driving-related events appear normal. As a result, we express driving scenes in depth and motion space, delivering spatial and temporal features of a scene, respectively. Such an input space can selectively ignore novel appearances of normal objects and focus more on cues that indeed lead to anomalous events.

STR is designed to detect anomalies by observing the reconstruction error of the spatiotemporal features of a scene. Our network structure based on Conv-AE~\citep{hasan2016learning} is shown in Figure~\ref{fig:str}. Although the scene motion $f_t$ is unknown at time $t$, an estimation $\hat{f}_t$ can be predicted from $\mathbf{I}_{t - T_\text{ffp} + 1:t}$ by OFP. We further generate a disparity map $d_t$ (i.e., inverse depth map up to scale) of the scene $I_t$ using pretrained MiDaS~\citep{ranftl2020towards}. Due to the scale ambiguity in monocular depth estimation, a min-max normalization on disparity values within an image is often performed~\citep{ranftl2020towards,godard2019digging,godard2017unsupervised}. However, such a \textit{relative} depth tends to eliminate informative distinctions between frames. For example, consider a scene with only a car close to the ego car and another scene with only a distant car. The disparity values of these two cars will be identical after normalization even though the metric depths are different. Therefore, we remove the min-max normalization to preserve global distinctions and assume that the estimated disparity and the actual metric depth are correlated. Disparity values are then clipped and divided by an upper bound globally on the entire dataset for a normalization to $[0, 1]$.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{Figures/fig5_str.pdf}
  \caption{\textbf{Model architecture of STR.} The optical flow and disparity map are concatenated and reconstructed by a fully convolutional autoencoder. The annotations can be interpreted in the same way as those in Figure~\ref{fig:ofp}.}
  \label{fig:str}
\end{figure}

The concatenation of the predicted optical flow and the disparity map is then processed by a convolutional autoencoder. The bottleneck in the middle of the network, with the fewest channels and lowest resolution among all the feature maps, ensures that only representative common features of normal scenes are extracted and that the reconstruction will be of low quality if such normal patterns are absent. Although memory-augmented AE has been shown effective in surveillance~\citep{gong2019memorizing,park2020learning}, we choose a simple architecture since the diversity in egocentric driving videos can require a significant amount of memory. Notably, we replace each deconvolution layer in Conv-AE with a nearest-neighbor interpolation followed by two convolution layers to counteract the checkerboard artifacts in reconstructed signals~\citep{odena2016deconvolution}. Similar to OFP and FFP, a padding of 1 is used to maintain the resolution for each convolution layer.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{Figures/fig6_im.pdf}
  \caption{\textbf{Model architecture of the interaction expert.} The network follows an encoder-decoder structure, where a pair of trajectories is compressed into a low-dimensional space and then decoded to generate the parameters for trajectory pair reconstruction. The start-of-sequence state is denoted by $\langle \text{SOS} \rangle$. The hidden state of both GRUs are initialized to zeros.}
  \label{fig:im}
\end{figure*}

The OFP parameters are frozen during the training of STR. An $L_1$ loss and a total variation loss are used to supervise the reconstruction of the original signal and combat further against the checkerboard artifacts for smoothness, respectively:
\begin{gather}
L_1 (\tilde{s}, s) = \| \tilde{s} - s \|_1, \\
L_\text{tv} (\tilde{s}) = \sum_{i,j,c} \left(\tilde{s}_{i,j,c} - \tilde{s}_{i-1,j,c}\right)^2 + \left(\tilde{s}_{i,j,c} - \tilde{s}_{i,j-1,c}\right)^2,
\end{gather}
where $s$ and $\tilde{s}$ are the original and reconstructed signal, respectively. The overall training objective of STR can be expressed as:
\begin{equation}
\label{eq:str-objective}
\begin{aligned}
L_\text{str}
=
&\left(
\lambda_\text{d} L_1 (\tilde{d}_t, d_t)
+
L_1 (\tilde{\hat{f}}_t, \hat{f}_t)
\right) \\
&+
\lambda_\text{tv}
\left(
\lambda_\text{d} L_\text{tv}(\tilde{d}_t)
+
L_\text{tv} (\tilde{\hat{f}}_t)
\right),
\end{aligned}
\end{equation}
where $\lambda_\text{d}$ is a scalar controlling the relative weight between the loss on the disparity and optical flow map and $\lambda_\text{tv}$ is a scalar balancing the $L_1$ and total variation loss. During evaluation, the anomaly detector using STR can be viewed as a function $g_\text{str}:\mathbf{I}_{t-T_\text{ffp}+1:t} \mapsto s_t^\text{str}$, in which the anomaly score $s_t^\text{str}$ is set to be the same as $L_\text{str}$ at time $t$.

With FFP and STR, our scene expert offers two anomaly scores based on frame prediction and reconstruction:
\begin{equation}
g_\text{s} (\mathbf{o}_t)
=
\{ g_\text{ffp} (\mathbf{I}_{t - T_\text{ffp}:t}), g_\text{str} (\mathbf{I}_{t - T_\text{ffp}+1:t}) \}
=
\{s_t^\text{ffp}, s_t^\text{str}\}.
\end{equation}
As can be seen, FFP starts producing anomaly score $s_t^\text{ffp}$ at time $T_\text{ffp} + 1$ while STR starts producing anomaly score $s_t^\text{str}$ at time $T_\text{ffp}$ in a video.
% To keep consistency within the scene expert, we assume that the first anomaly scores generated by the two submodules in a video are both at time $T_\text{s} \coloneqq T_\text{ffp} + 1$, although the STR could have produced a score at time $T_\text{ffp}$.

\subsection{Interaction expert}
\label{subsec:im}
Although the scene expert is good at detecting \textit{ego-involved anomalies} by learning frame-level normal patterns, non-ego involved anomalies can be easily missed due to the relatively small scales of anomaly participants in frames. Some object-level anomalies are often accompanied with abnormal \textit{interactions}, which we define as the relative motion between two objects. For example, a collision between two cars is often preceded by a shrinking relative distance without sufficient deceleration, which is not a common interaction pattern in normal driving scenarios. An anomaly detector modeling interactions can also potentially raise alarms earlier than that modeling individual behaviors as anomalous interactions (e.g., approaching at high relative velocity) appear earlier than anomalous individual behaviors (e.g., sudden stop of a car after a collision).

Our interaction expert models normal interactions by reconstructing motions of two objects, and an anomaly is detected whenever a high reconstruction error is observed. We denote a detected object's bounding box as $X_t=[c_t^x, c_t^y, w_t, h_t]$, where $[c_t^x, c_t^y]$ is the location of the center of the box and $w_t$ and $h_t$ are the width and height of the box, respectively. Given a pair of bounding box trajectories in the image plane $\{X_{t - T_\text{int} + 1:t}^i, X_{t - T_\text{int} + 1:t}^j\}$, where $T_\text{int}$ is the history horizon of the interaction expert and $i,j$ denote the IDs of the two objects, our time series model reconstructs the two trajectories simultaneously. We use pretrained Mask R-CNN~\citep{he2017mask}, following prior works~\citep{yao2022dota,yao2020and,fang2022traffic}, and StrongSORT~\citep{du2023strongsort} to acquire and associate object bounding boxes across frames at the original resolution, respectively. Each bounding box is normalized with respect to the dimensions of the images to facilitate efficient training.

Our gated recurrent unit based autoencoder (GRU-AE) for trajectory pair reconstruction, shown in Figure~\ref{fig:im}, is inspired by the work on driver trait modeling~\citep{liu2022learning}. For brevity, we denote each pair of trajectories fed as input to GRU-AE at time $t$ as a one-indexed sequence of length $T_\text{int}$, as shown in the input and output in Figure~\ref{fig:im}. Each pair of bounding boxes at time $k$ within a trajectory pair is flattened into a 1D vector to represent the joint localization of the two objects. The encoder GRU then applies a non-linear embedding function $\phi_\text{encoder}$ to each flattened pair of bounding boxes and feeds the embedded features to the GRU cell:
\begin{equation}
\label{eq:im-encoder}
h_k^\text{e} = \text{GRU} \left( h_{k-1}^\text{e}, \phi_\text{encoder} ([X_k^i, X_k^j]) \right),
\end{equation}
where $h_k^\text{e}$ is the hidden state of the encoder GRU at time $k \in (1, \dots, T_\text{int})$. After the entire trajectory pair is processed, we take the last hidden state $h_{T_\text{int}}^e$ as the feature of the interaction and create a bottleneck through another fully connected layer $\phi_\text{z}$ to distill representative normal patterns:
\begin{equation}
z = \phi_\text{z} (h_{T_\text{int}}^\text{e}).
\end{equation}
We choose an AE over a variational autoencoder (VAE) since we observe that any regularization of the latent space significantly degrades the reconstruction performance of the network in our experiments, possibly due to the highly diverse interaction patterns in normal driving videos.

GRU-AE is tasked with reconstructing the original bounding box trajectories. However, directly outputting the absolute coordinates of bounding boxes from the network is inefficient to achieve our goal: we care more about object \textit{motions} than object \textit{positions}. In other words, it is the motion that the two cars are approaching each other, not where the two cars collide in the frame, that leads to an anomaly. Therefore, we use the first pair of bounding boxes in the sequence as two anchor boxes, similar to the idea in object detection~\citep{redmon2018yolov3}, and the network only outputs the parameters $[P_k^i, P_k^j]$ used for transforming the anchor boxes into target boxes. More specifically, for an object (ID omitted for brevity), the parameter $P_k$ at time $k$ consists of four elements $p_k^x$, $p_k^y$, $p_k^w$, $p_k^h$ for the transformation:
\begin{equation}
\label{eq:new-box-from-anchor-box}
\begin{aligned}
\tilde{c}_k^x &= c_1^x + p_k^x, \\
\tilde{c}_k^y &= c_1^y + p_k^y, \\
\tilde{w}_k &= w_1 e^{p_k^w}, \\
\tilde{h}_k &= h_1 e^{p_k^h},
\end{aligned}
\end{equation}
where $\tilde{X}_k=[\tilde{c}_k^x, \tilde{c}_k^y, \tilde{w}_k, \tilde{h}_k]$ is the reconstructed bounding box at time $k$ and $X_1=[c_1^x, c_1^y, w_1, h_1]$ is the anchor box. Such a characterization allows the network to focus more on the modeling of motions than absolute positions of bounding boxes, which provide little information on the normality of the scene\endnote{The parameters at the first time step output by a properly trained GRU-AE should always be around the origin as the target at the first time step is just the anchor box.}.

We assume that the interaction pattern stays constant over a small window along a trajectory. As a result, in the decoding stage, the latent state $z$ is concatenated with the decoder output at the previous time step $[P_{k-1}^i, P_{k-1}^j]$ to form a joint state at time $k$, instead of being used to initialize the decoder GRU. Such a joint state is then embedded by a function $\phi_\text{decoder}$, fed into the decoder GRU cell, and processed by another non-linear function $\psi_\text{decoder}$ to generate the parameters for reconstruction at time $k$:
\begin{equation}
\label{eq:im-decoder}
\begin{aligned}
&h_k^\text{d} = \text{GRU} \left( h_{k-1}^\text{d}, \phi_\text{decoder} ([P_{k-1}^i, P_{k-1}^j, z]) \right), \\
&[P_k^i, P_k^j] = \psi_\text{decoder} (h_k^\text{d}),
\end{aligned}
\end{equation}
where $h_k^\text{d}$ is the hidden state of the decoder GRU at time $k \in (1, \dots, T_\text{int})$. At the first time step, we use a special start-of-sequence (SOS) state, similar to the start-of-sequence symbol in natural language processing~\citep{grefenstette2015learning}, to generate $[P_1^i, P_1^j]$. The steps in~(\ref{eq:im-decoder}) are repeated until the parameters for the entire trajectory are generated.

The interaction expert is designed for non-ego involved anomalies. Therefore, the network should pay more attention to small-scale traffic participants as the anomalies associated with large objects are likely to be detected by the scene expert. To this end, instead of performing explicit data mining to filter out trajectory pairs with large bounding boxes, we incorporate our emphasis on small objects into the objective function. Furthermore, to combat against the input noise introduced by the object detection and tracking algorithms, we weight the loss based on the standard deviation of the bounding box coordinates over time (i.e., we trust stable detection more than the unstable one). As a result, the objective for training our GRU-AE is a scaled root mean squared error (RMSE) between the original and reconstructed bounding boxes within a trajectory \textit{pair}:
\begin{equation}
\label{eq:im-loss}
L_\text{int}^{ij} = \sum_{id \in \{i,j\}} \sum_{k=1}^{T_\text{int}} \left( \lambda_\text{h}^{-1} \lambda_\text{std}^{-1} \| \tilde{X}_k^{id} - X_k^{id} \|_2^2 \right)^\frac{1}{2},
\end{equation}
where $\lambda_\text{h}$ is the average height of the original bounding boxes of the two objects, serving as a proxy of the distance from the ego car to the objects, and $\lambda_\text{std}$ is the average standard deviation (STD) of the original bounding box coordinates of the two objects within the time window:
\begin{equation}
\begin{aligned}
\lambda_\text{h} &= \frac{1}{2T_\text{int}} \sum_{id \in \{i,j\}} \sum_{k=1}^{T_\text{int}} h_k^{id}, \\
\lambda_\text{std} &= \frac{1}{8} \sum_{id \in \{i,j\}} \sum_{y \in \{c^x,c^y,w,h\}} \text{STD} (y_{1:T_\text{int}}^{id}).
\end{aligned}
\end{equation}
We further clip $\lambda_\text{std}$ with a lower-bound $\tau_\text{std} > 0$ for numerical stability. With the introduced scaled loss, GRU-AE is able to focus more on the interactions of distant objects and thus better complement the scene expert.

Notably, the number of \textit{pairs} of objects grows quadratically with the number of objects. To set an upper limit on the computational complexity of the interaction expert, we establish and maintain a priority queue within each time window with capacity $N_\text{max}$ to store pairs of trajectories that are most likely to have interactions between each other based on the relative distance of the bounding boxes before the forward pass of GRU-AE. More specifically, each pair of trajectories is associated with a \textit{distance score}, defined as the proximity of the two bounding boxes with the consideration of the box size:
\begin{equation}
\begin{aligned}
ds^{ij} = \min_{1 \le k \le T_\text{int}} &|c_k^{x,i} - c_k^{x,j}| - (w_k^i + w_k^j)/2 \\
+ &|c_k^{y,i} - c_k^{y,j}| - (h_k^i + h_k^j)/2.
\end{aligned}
\end{equation}
A lower \textit{distance score} means that the two objects are closer to each other and thus are more likely to produce an interactive anomaly. Within a time window, the priority queue maintains at most $N_\text{max}$ pairs of trajectories with the lowest \textit{distance score}, which are then fed into GRU-AE for anomaly detection.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{Figures/fig7_bm.pdf}
  \caption{\textbf{Model architecture of the behavior expert.} The object's bounding box, object's optical flow features, and image are fed to different encoders, then fused and decoded to generate the parameters for transforming the anchor box into the predicted bounding boxes within the prediction horizon. The processing pipelines for the bounding box, optical flow features, and image are in blue, purple, and yellow, respectively. The hidden state of the GRUs in the encoder is initialized to zeros.}
  \label{fig:bm}
\end{figure*}

We follow common practice in AE-based anomaly detection by calculating the anomaly score in the same way as the training objective~(\ref{eq:im-loss}) for each pair of trajectories. During evaluation on a video, we denote $PQ_t$ as the set of pairs of trajectories stored in the priority queue that are present from time $t - T_\text{int} + 1$ to $t$ inclusively. The anomaly score $s_t^\text{int}$ at time $t$, generated by the interaction expert, is the average $L_\text{int}$ value over all pairs of trajectories in $PQ_t$:
\begin{equation}
s_t^\text{int} = \frac{1}{|PQ_t|} \sum_{ij \in PQ_t} L_\text{int}^{ij},
\end{equation}
where $|PQ_t|$ is the cardinality of $PQ_t$ that is at most $N_\text{max}$. If no valid pairs of trajectories are detected from time $t - T_\text{int} + 1$ to $t$, $s_t^\text{int}$ is set to be $0$. To compensate for possible discontinuities in the anomaly score $s_t^\text{int}$ over time due to missing objects or incorrect ID associations in the detection and tracking algorithms, we further apply a digital low-pass filter on $s_t^\text{int}$ for smoothness. Such a filter is causal and thus can be utilized for online AD.

\subsection{Behavior expert}
\label{subsec:bm}
In cases where a non-ego involved anomaly only contains a single agent, the interaction expert can fail easily. For example, if there is only one vehicle visible in the field of view and starts swerving due to the snow, the interaction expert will not be activated as no interactions between road participants exist. A useful hint for anomaly in such a case is the observed unusual behavior: a car would never zigzag aggressively in normal driving scenarios.

Our behavior expert models normal behaviors by predicting future locations of a road participant, and an anomaly is detected whenever an object's observed trajectory deviates significantly from the predicted trajectory. In addition to the past bounding boxes of an object, prior works also include ego motions as one of the inputs to compensate for the object location change due to such motions~\citep{yao2019unsupervised,yao2022dota}. However, Monocular SLAM algorithms~\citep{mur2015orb,mur2017orb,campos2021orb}, used to estimate ego motions, require known camera intrinsics, which is often unavailable in a large-scale dataset with videos from multiple sources. Moreover, the tracking is often lost in featureless scenes, such as a highway in a rural area in a plain. Therefore, we remove the ego motions from our inputs to facilitate a more robust system.

Another useful hint for the prediction of future object localization is the global information contained in an image. For example, a normal trajectory of a vehicle is often constrained by obstacle-free drivable areas, within which the predicted future locations should reside. As a result, we made our behavior expert aware of the environment by incorporating the latest full RGB image into the inputs. Furthermore, we include the past optical flow features of each object as a final part of our inputs to enhance the perception on object motions, similar to prior works~\citep{fang2022traffic,yao2022dota}.

For each object, given the past bounding box trajectory $(X_1, X_2, \cdots, X_{t})$, the past optical flow features $(f_1^\text{obj}, f_2^\text{obj}, \cdots, f_t^\text{obj})$, and the original image $I_t$, our future object localization model predicts a trajectory $(\hat{X}_{t+1|t}, \hat{X}_{t+2|t}, \cdots, \hat{X}_{t+\delta|t})$ based on the information up until time $t$, where $\delta > 0$ is the prediction horizon. Each bounding box is normalized with respect to the dimensions of the images as in the interaction expert, and the object's optical flow features are extracted by a $5 \times 5$ RoIAlign~\citep{he2017mask} operation from the frame-level optical flow fields. Our time series model, shown in Figure~\ref{fig:bm}, follows an encoder-decoder structure, similar to GRU-AE proposed in Section~\ref{subsec:im} but without a bottleneck. One major difference between the behavior and interaction expert is that the behavior expert only needs to process the latest data (i.e., $X_t$, $f_t^\text{obj}$, and $I_t$) at a time, as the historical information of the bounding boxes and optical flow features is propagated through the hidden states of the two GRUs since \textit{the first appearance} of the object. Specifically, the bounding box $X_t$ and the flattened optical flow features $f_t^\text{obj}$ are processed at time $t+1$ to update the hidden state of the two GRUs:
\begin{equation}
\begin{aligned}
h_t^\text{box} &= \text{GRU} \left(h_{t - 1}^\text{box}, \phi_\text{encoder}^\text{box} (X_t) \right), \\
h_t^\text{flow} &= \text{GRU} \left( h_{t - 1}^\text{flow}, \phi_\text{encoder}^\text{flow} (f_t^\text{obj}) \right),
\end{aligned}
\end{equation}
where $\phi_\text{encoder}^\text{box}$ and $\phi_\text{encoder}^\text{flow}$ are two non-linear functions. The image pipeline uses a MobileNetV2 backbone~\citep{sandler2018mobilenetv2} pretrained on semantic segmentation task in Cityscapes~\citep{cordts2016cityscapes}. We construct the feature generator for images by truncating the MobileNetV2 right after the last bottleneck block and appending a few additional convolution and fully connected layers, summarized as a non-linear function $\phi_\text{encoder}^\text{image}$:
\begin{equation}
e_t^\text{image} = \phi_\text{encoder}^\text{image} \left( \text{MobileNetV2} (I_t) \right),
\end{equation}
where $e_t^\text{image}$ is the image embedding at time $t$.

Considering a relatively long prediction horizon, we initialize the hidden state of the decoder GRU with a fusion of the latest information \textit{at time} $t$ from all three encoders:
\begin{equation}
h_{0|t}^\text{decoder} = \phi_\text{encoder}^\text{fusion} \left( \left[\text{AvgPool}(h_t^\text{box}, h_t^\text{flow}), e_t^\text{image}\right] \right),
\end{equation}
where $\phi_\text{encoder}^\text{fusion}$ is another non-linear function and AvgPool is the element-wise average of two vectors, following~\cite{yao2022dota,yao2019unsupervised}. Based on the information up until time $t$, the model recurrently outputs the parameters $P_{t+k|t}, 1 \le k \le \delta$ for synthesizing the predicted bounding box from the anchor box by following~(\ref{eq:new-box-from-anchor-box}) within the prediction horizon. The anchor box is set to be the last bounding box $X_t$, and thus the first input to the decoder GRU $P_{t|t}$ is a zero vector. More specifically, the parameters for box transformations are generated in an autoregressive manner:
\begin{equation}
\begin{aligned}
h_{k|t}^\text{decoder} &= \text{GRU} \left( h_{k - 1|t}^\text{decoder}, P_{t + k - 1|t} \right), \\
P_{t + k|t} &= \psi_\text{decoder}^\text{box} (h_{k|t}^\text{decoder}),
\end{aligned}
\end{equation}
where $h_{k|t}^\text{decoder}$ is the $k$-th hidden state of the decoder GRU that is initialized with the information up to time $t$, $\psi_\text{decoder}^\text{box}$ is a non-linear funciton, and $k \in (1, \cdots, \delta)$. We train the behavior expert with the mean squared error (MSE) between the predicted and target future bounding boxes.

For each object, the behavior expert predicts bounding boxes of the next $\delta$ time steps until the tracking is lost. Therefore, at time $t$, each object has $\delta$ bounding boxes predicted from time $t - \delta$ to $t - 1$, respectively\endnote{Within the first $\delta$ time steps since the object is detected (i.e., when $1 \le t \le \delta$), there will be only $t - 1$ bounding boxes predicted from the previous time steps at time $t$. After the tracking of the object is lost, there will be only $\delta - \Delta t_\text{lost}$ bounding boxes predicted from the previous time steps at time $t$, where $\Delta t_\text{lost}$ is the number of elapsed time steps since the lost of tracking.}. It has been shown that monitoring the \textit{prediction consistency} delivers superior AD performance than monitoring the prediction accuracy. In particular, at time $t$, the \textit{standard deviation} of the $\delta$ predicted bounding boxes from the previous time steps is computed and averaged over all tracked objects as the anomaly score~\citep{yao2022dota}:
\begin{equation}
\label{eq:fol-anomaly-score}
s_t^\text{std} = \frac{1}{4N} \sum_{i=1}^N \sum_{\hat{y} \in \{\hat{c}^x,\hat{c}^y,\hat{w},\hat{h}\}} \text{STD} ((\hat{y}_{t|t - \delta}^i, \cdots, \hat{y}_{t|t - 1}^i)),
\end{equation}
where $N$ is the number of tracked objects at time $t$. A low STD indicates that the behavior of the object follows normal patterns and thus the predictions are stable, while a high STD suggests anomalous behaviors (e.g., the predictions made before and after a collision for a specific future time step are highly inconsistent).

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.09\linewidth}
    \captionsetup{justification=centering}
    \includegraphics[width=\linewidth]{Figures/fig8a_height_scaling_eg_small.pdf}
    \caption{}
    \label{subfig:height-scaling-eg-small}
  \end{subfigure} \hspace{5mm}
  \begin{subfigure}[b]{0.3\linewidth}
    \captionsetup{justification=centering}
    \includegraphics[width=\linewidth]{Figures/fig8b_height_scaling_eg_large.pdf}
    \caption{}
    \label{subfig:height-scaling-eg-large}
  \end{subfigure}
  \caption{\textbf{An illustrative example showing the limitation of the original anomaly score.} The bounding box predictions for a small object and a large object are shown in (a) and (b), respectively. The two cases result in identical STD values even though the prediction consistency is perceptually different.}
  \label{fig:height-scaling-eg}
\end{figure}

In this paper, we make a simple yet important modification to~(\ref{eq:fol-anomaly-score}) by scaling the STD value with the height of the bounding boxes to further improve the AD performance. It is empirically observed that the prediction consistency is positively correlated with the distance from the ego car to the object. In other words, future locations of a more distant and smaller object can be predicted more stably than a closer and larger object, possibly due to the fact that adjacent objects often exhibit large motions because of perspective projection. Formally, let $\hat{y}$ be one of the elements of a predicted bounding box vector $[\hat{c}^x, \hat{c}^y, \hat{w}, \hat{h}]$ and $y$ be the corresponding ground truth element. For simplicity, we assume that a prediction \textit{in normal cases} is a sample of a random variable with a Gaussian distribution, whose mean is the ground truth value and variance is \textit{proportional} to the power of the height of the predicted bounding box, and that the prediction error is agnostic to the time step. Thus, by omitting the subscript of time steps, we arrive at $\hat{y} \sim \mathcal{N}(y, \alpha \hat{h}^\beta)$, where $\alpha$ and $\beta$ are two positive numbers. We further assume that $\beta=2$ for simplicity. As a result, the STD of $\hat{y}$'s within the prediction horizon is $\sqrt{\alpha} \hat{h}$, which is then used to compute the anomaly score. However, such a score can be misleading, as whether or not an object's behavior is anomalous should not depend on the height of the object. Therefore, we simply scale the STD value by $\hat{h}$ to resolve such an issue.

Figure~\ref{fig:height-scaling-eg} shows the benefit of such a scaled loss from another perspective. In this illustrative example, we keep the size of the predicted bounding box unchanged \textit{within each case} and shift the center of the bounding box by the same amount \textit{across the two cases}. As a result, the anomaly scores computed for the small and the large object are identical according to~(\ref{eq:fol-anomaly-score}). However, it is apparent that the predictions in Figure~\ref{subfig:height-scaling-eg-small} are less consistent than those in Figure~\ref{subfig:height-scaling-eg-large}, and an anomaly is more likely to be present in~\ref{subfig:height-scaling-eg-small}. By scaling the STD values with the height of the predicted bounding box, we obtain a higher anomaly score for~\ref{subfig:height-scaling-eg-small} than~\ref{subfig:height-scaling-eg-large}, which meets our expectations. The final anomaly score from the behavior expert is computed as:
\begin{equation}
s_t^\text{beh} = \frac{1}{4N} \sum_{i=1}^N \lambda_{\hat{h}}^i \sum_{\hat{y} \in \hat{Y}} \text{STD} ((\hat{y}_{t|t - \delta}^i, \cdots, \hat{y}_{t|t - 1}^i)),
\end{equation}
where $\hat{Y} = \{\hat{c}^x,\hat{c}^y,\hat{w},\hat{h}\}$ and $\lambda_{\hat{h}}^i$ is the average height of the predicted bounding boxes for the $i$-th object at time $t$:
\begin{equation}
\lambda_{\hat{h}}^i = \frac{1}{\delta} \sum_{k=1}^\delta \hat{h}_{t|t - k}^i.
\end{equation}
Similar to the interaction expert, we also apply a digital low-pass filter on top of $s_t^\text{beh}$ for smoothness over time.