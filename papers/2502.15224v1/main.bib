% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}
@article{merchant2023scaling,
  title={Scaling deep learning for materials discovery},
  author={Merchant, Amil and Batzner, Simon and Schoenholz, Samuel S and Aykol, Muratahan and Cheon, Gowoon and Cubuk, Ekin Dogus},
  journal={Nature},
  volume={624},
  number={7990},
  pages={80--85},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{pyzer2022accelerating,
  title={Accelerating materials discovery using artificial intelligence, high performance computing and robotics},
  author={Pyzer-Knapp, Edward O and Pitera, Jed W and Staar, Peter WJ and Takeda, Seiji and Laino, Teodoro and Sanders, Daniel P and Sexton, James and Smith, John R and Curioni, Alessandro},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={84},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{hayes2025simulating,
  title={Simulating 500 million years of evolution with a language model},
  author={Hayes, Thomas and Rao, Roshan and Akin, Halil and Sofroniew, Nicholas J and Oktay, Deniz and Lin, Zeming and Verkuil, Robert and Tran, Vincent Q and Deaton, Jonathan and Wiggert, Marius and others},
  journal={Science},
  pages={eads0018},
  year={2025},
  publisher={American Association for the Advancement of Science}
}
@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{lu2024ai,
  title={The ai scientist: Towards fully automated open-ended scientific discovery},
  author={Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  journal={arXiv preprint arXiv:2408.06292},
  year={2024}
}
@article{altmae2023artificial,
  title={Artificial intelligence in scientific writing: a friend or a foe?},
  author={Altm{\"a}e, Signe and Sola-Leyva, Alberto and Salumets, Andres},
  journal={Reproductive BioMedicine Online},
  volume={47},
  number={1},
  pages={3--9},
  year={2023},
  publisher={Elsevier}
}
@article{dinu2024symbolicai,
  title={SymbolicAI: A framework for logic-based approaches combining generative models and solvers},
  author={Dinu, Marius-Constantin and Leoveanu-Condrei, Claudiu and Holzleitner, Markus and Zellinger, Werner and Hochreiter, Sepp},
  journal={arXiv preprint arXiv:2402.00854},
  year={2024}
}
@article{sheth2024causalgraph2llm,
  title={CausalGraph2LLM: Evaluating LLMs for Causal Queries},
  author={Sheth, Ivaxi and Fatemi, Bahare and Fritz, Mario},
  journal={arXiv preprint arXiv:2410.15939},
  year={2024}
}
@article{jiralerspong2024efficient,
  title={Efficient causal graph discovery using large language models},
  author={Jiralerspong, Thomas and Chen, Xiaoyin and More, Yash and Shah, Vedant and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2402.01207},
  year={2024}
}
@article{long2023can,
  title={Can large language models build causal graphs?},
  author={Long, Stephanie and Schuster, Tibor and Pich{\'e}, Alexandre},
  journal={arXiv preprint arXiv:2303.05279},
  year={2023}
}
@article{choi2022lmpriors,
  title={Lmpriors: Pre-trained language models as task-specific priors},
  author={Choi, Kristy and Cundy, Chris and Srivastava, Sanjari and Ermon, Stefano},
  journal={arXiv preprint arXiv:2210.12530},
  year={2022}
}
@article{kiciman2023causal,
  title={Causal reasoning and large language models: Opening a new frontier for causality},
  author={K{\i}c{\i}man, Emre and Ness, Robert and Sharma, Amit and Tan, Chenhao},
  journal={arXiv preprint arXiv:2305.00050},
  year={2023}
}
@article{hager2024evaluation,
  title={Evaluation and mitigation of the limitations of large language models in clinical decision-making},
  author={Hager, Paul and Jungmann, Friederike and Holland, Robbie and Bhagat, Kunal and Hubrecht, Inga and Knauer, Manuel and Vielhauer, Jakob and Makowski, Marcus and Braren, Rickmer and Kaissis, Georgios and others},
  journal={Nature medicine},
  volume={30},
  number={9},
  pages={2613--2622},
  year={2024},
  publisher={Nature Publishing Group US New York}
}
@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}
@article{GSM8K,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@article{ASDiv,
  title={A diverse corpus for evaluating and developing English math word problem solvers},
  author={Miao, Shen-Yun and Liang, Chao-Chun and Su, Keh-Yih},
  journal={arXiv preprint arXiv:2106.15772},
  year={2021}
}
@inproceedings{mawps,
  title={MAWPS: A math word problem repository},
  author={Koncel-Kedziorski, Rik and Roy, Subhro and Amini, Aida and Kushman, Nate and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies},
  pages={1152--1157},
  year={2016}
}
@article{SVAMP,
  title={Are NLP models really able to solve simple math word problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  journal={arXiv preprint arXiv:2103.07191},
  year={2021}
}
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, P},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@article{xsum,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}
@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@misc{claude3.5_haiku,
  author = {Anthropic},
  title = {Claude 3.5 Haiku Model Card},
  year = {2024},
  url = {https://www.anthropic.com/news/claude-3-5-haiku},
  note = {Accessed: 2024-10-22}
}

@misc{gemini1.5_pro,
  author = {Google DeepMind},
  title = {Gemini 1.5: Unlocking multimodal understanding at scale},
  year = {2024},
  url = {https://deepmind.google/technologies/gemini/},
  note = {Technical Report}
}

@misc{llama3.1_70b,
  author = {Meta AI},
  title = {Llama 3.1: Open Foundation and Fine-Tuned Chat Models},
  year = {2024},
  url = {https://ai.meta.com/blog/llama3-1/},
  note = {Model Release Notes}
}

@misc{gpt4o,
  author = {OpenAI},
  title = {GPT-4o System Architecture},
  year = {2024},
  url = {https://openai.com/index/gpt-4o/},
  note = {Technical Overview}
}

@article{qwen2.5_72b,
  author = {Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Ruiyang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  title = {Qwen2.5: The Next Generation of Large Language Models with Hybrid Scaling},
  year = {2024},
  journal = {arXiv preprint},
  url = {https://arxiv.org/abs/2405.xxxxx}
}


@article{wang2023codet5,
  title={Codet5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}

@inproceedings{cheng2023adapting,
  title={Adapting large language models via reading comprehension},
  author={Cheng, Daixuan and Huang, Shaohan and Wei, Furu},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{yang2023coupling,
  title={Coupling large language models with logic programming for robust and general reasoning from text},
  author={Yang, Zhun and Ishay, Adam and Lee, Joohyung},
  journal={arXiv preprint arXiv:2307.07696},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@inproceedings{xu2022systematic,
  title={A systematic evaluation of large language models of code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent Josua},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  pages={1--10},
  year={2022}
}

@article{zhu2024promptbench,
  title={Promptbench: A unified library for evaluation of large language models},
  author={Zhu, Kaijie and Zhao, Qinlin and Chen, Hao and Wang, Jindong and Xie, Xing},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={254},
  pages={1--22},
  year={2024}
}

@article{zhang2024benchmarking,
  title={Benchmarking large language models for news summarization},
  author={Zhang, Tianyi and Ladhak, Faisal and Durmus, Esin and Liang, Percy and McKeown, Kathleen and Hashimoto, Tatsunori B},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={39--57},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{liu2023llmrec,
  title={Llmrec: Benchmarking large language models on recommendation task},
  author={Liu, Junling and Liu, Chao and Zhou, Peilin and Ye, Qichen and Chong, Dading and Zhou, Kang and Xie, Yueqi and Cao, Yuwei and Wang, Shoujin and You, Chenyu and others},
  journal={arXiv preprint arXiv:2308.12241},
  year={2023}
}

@article{valmeekam2024planbench,
  title={Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change},
  author={Valmeekam, Karthik and Marquez, Matthew and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}



@misc{openrouter,
  author       = {OpenRouter},
  title        = {OpenRouter: A unified interface for LLMs},
  howpublished = {\url{https://openrouter.ai}},
  year         = {2025},
  note         = {Accessed: 2025-02-16}
}

