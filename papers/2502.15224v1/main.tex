% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{adjustbox}
\usepackage{amsmath} 
\usepackage{bbm}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{subcaption}
\usepackage{pdfpages}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs}

\author{
 \textbf{Tingting Chen\textsuperscript{1}$^*$},
 \textbf{Srinivas Anumasa\textsuperscript{1}\thanks{Equal contribution}},
 \textbf{Beibei Lin\textsuperscript{1}},
 \textbf{Vedant Shah\textsuperscript{2}}
\\
 \textbf{Anirudh Goyal\textsuperscript{3}},
 \textbf{Dianbo Liu\textsuperscript{1}},
\\
\\
 \textsuperscript{1}National University of Singapore
 \textsuperscript{2}Mila-Quebec AI institute
 \textsuperscript{3}Meta (Facebook)
\\
 \href{mailto:tingting.c@u.nus.edu@domain}{tingting.c@u.nus.edu},
 \href{mailto:srinu\_pd@domain}{srinu\_pd@nus.edu.sg}
}

\begin{document}
\maketitle
\begin{abstract}
Given the remarkable performance of Large Language Models (LLMs), an important question arises: Can LLMs conduct human-like scientific research and discover new knowledge, and act as an AI scientist? Scientific discovery is an iterative process that demands efficient knowledge updating and encoding. It involves understanding the environment, identifying new hypotheses, and reasoning about actions; however, no standardized benchmark specifically designed for scientific discovery exists for LLM agents. In response to these limitations, we introduce a novel benchmark, \textit{Auto-Bench}, that encompasses necessary aspects to evaluate LLMs for scientific discovery in both natural and social sciences. Our benchmark is based on the principles of causal graph discovery. It challenges models to uncover hidden structures and make optimal decisions, which includes generating valid justifications. By engaging interactively with an oracle, the models iteratively refine their understanding of underlying interactions, the chemistry and social interactions, through strategic interventions. We evaluate state-of-the-art LLMs, including GPT-4, Gemini, Qwen, Claude, and Llama, and observe a significant performance drop as the problem complexity increases, which suggests an important gap between machine and human intelligence that future development of LLMs need to take into consideration.

\end{abstract}

%%%%%%%%% Framework Figure %%%%%%%%%
\begin{figure*}[t]
\centering
{\includegraphics[width=0.95\textwidth]{Fig/Framework.pdf}
}\\
\caption{
The framework of our Autonomous Cycle. (A) represents the complete benchmarking cycle. The LLMs are provided with task descriptions, previous interventions they proposed, and the corresponding observations. Based on this information, the LLMs generate adjacency matrices and proposes a new intervention to gather additional data. A new observation is then obtained and added to the input. (B) outlines the conditions for terminating or continuing the loop. The loop terminates when the generated adjacency matrix matches the underlying causal graph; otherwise, it continues. To simulate real-world scientific problems, we include two experimental settings: chemistry and social networks.
}
\label{Framework}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Large language models (LLMs) built on transformer architectures have revolutionized natural language processing by surpassing existing models on numerous benchmarks \cite{xsum,rajpurkar2016squad,wang2018glue}. Modern LLMs—such as Claude-3-5 \cite{claude3.5_haiku}, GPT-4o \cite{gpt4o}, Gemini \cite{gemini1.5_pro}, Llama-3.1 \cite{llama3.1_70b}, and Qwen2.5 \cite{qwen2.5_72b}—are trained on trillions of tokens, enabling them to achieve state-of-the-art performance across a diverse range of natural language processing tasks.

LLM models encapsulate vast amounts of world knowledge acquired through extensive training, much like a human who has conducted in-depth research and amassed a broad understanding of the world—knowledge that can drive new discoveries. This raises an obvious question: Can LLM models conduct scientific research and generate novel findings \cite{lu2024ai}? So far, these models have been shown to accelerate various aspects of the research process, such as manuscript writing and coding \cite{altmae2023artificial,dinu2024symbolicai}. However, scientific discovery involves much more than refining text or code. It requires models to understand existing knowledge, augment it with new information, make optimal decisions, and, most importantly, employ reasoning skills to explain the roadmap that leads to those decisions.

There is a growing interest in using LLM models for scientific discovery across various domains, such as material discovery \cite{merchant2023scaling,pyzer2022accelerating} and synthetic biology \cite{jumper2021highly,hayes2025simulating}. Although state-of-the-art LLM models have demonstrated promising capabilities for targeted scientific progress \cite{lu2024ai}, they are not yet ready for fully autonomous decision-making \cite{hager2024evaluation}. Before entrusting these models with critical applications that require full autonomy, it is essential to identify their strengths and weaknesses in the context of scientific discovery. This necessitates the design of benchmarks which evaluates the models for their ability to perform scientific discoveries. 

Numerous benchmarks \cite{ASDiv,GSM8K,SVAMP,mawps}, related to numerical problem-solving ability have been developed to assess reasoning performance, each offering a diverse range of challenges and difficulty levels. Similarly, code completion benchmarks\cite{xu2022systematic} and others which focus on sequence completion\cite{chang2024survey, zhu2024promptbench, zhang2024benchmarking, valmeekam2024planbench} exists.  LLM models are also evaluated in decision-making applications \cite{yao2022react}, where they engage in iterative interactions with an agent to execute actions. However, these benchmarks do not evaluate LLM models from a scientific discovery perspective—which requires not only understanding but also adapting to environmental changes and making optimal decisions through sound reasoning. This motivates us to design a new benchmark that captures and evaluates the models' abilities in understanding, reasoning, and decision-making.

In this work, we introduce two benchmarks based on causal graph discovery principles. These benchmarks are designed to assess both the capabilities and limitations of various LLM models in decision-making tasks. Decision-making requires understanding, reasoning, and the discovery of new knowledge, and our evaluation focuses on the models' ability to uncover the hidden structures within underlying graphs. This setup features an Oracle with which the models interact continuously to receive feedback that improves their understanding of the graphs. More details regarding the experimental setup are provided in the following sections.

This paper makes the following key contributions: 
\begin{itemize} 
    \item We present a novel benchmark for evaluating scientific research capabilities in LLMs. 
    \item We systematically analyze LLM performance across different models, highlighting the limitations of current state-of-the-art approaches. 
    \item We investigate the effectiveness of chain-of-thought prompting and identify key failure patterns, offering insights for future architectural improvements. 
\end{itemize}

From our experiments, we observed that the complexity of causal graphs significantly affects the performance of LLMs. For example, increasing the number of nodes in both chemistry and social networks leads to a dramatic decrease in the average cycles required to obtain the correct answers. To further analyze this failure, we introduce an experiment on long-term trajectory tracing, which will be explained in Section~\ref{Trajectory}. We found that as the trajectory length increases, LLMs fail to accurately trace state changes. This finding suggests a significant gap between machine and human intelligence in information processing, which should be addressed in future research.

\section{Related Work}
Existing benchmarks \cite{ASDiv,GSM8K,SVAMP,mawps}  assess the reasoning capabilities of LLMs by posing queries and evaluating their responses. However, these benchmarks do not fully capture the scientific discovery potential of LLMs. For true scientific discovery, LLMs must integrate information gained from critical decisions, which in turn influences their future decision-making performance. 

Our benchmark is based on the principles of causal graph structure discovery, albeit with a few relaxations. While it shares similarities with existing methods in causal discovery \cite{jiralerspong2024efficient,long2023can,choi2022lmpriors,kiciman2023causal}, its distinctiveness emerges from the iterative updating of knowledge based on the model's decisions about the underlying graphs. Traditional LLM-based causal graph discovery methods typically rely on meta-data associated with variables, querying whether an edge exists between two nodes (A, B), sometimes evaluating all possible pairs and other times employing strategies to reduce the number of queries. 

Unlike these approaches that primarily focus on querying and response generation, our setup prompts models to suggest optimal interventions, enabling them to uncover hidden structures. This experimental framework is analogous to scientific discovery, where the model continuously updates its knowledge through a series of interventions (experiments) to reveal new insights (causal graph discovery).

%%%%%%%%% Chemistry Figure %%%%%%%%%
\begin{figure}[t]
\centering
{\includegraphics[width=0.95\linewidth]{Fig/Chemistry.pdf}
}\\
\caption{Illustration of the Chemistry setting. The brackets indicate (molecule index, molecule state). Figures (a) and (b) illustrate the change in state after an intervention on molecule 0. Figures (c) and (d) present a case where causal graph A and causal graph B result in the same observations.}
\label{Chemistry}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}
\label{Methodology}
%
In our framework, we introduce two benchmarks: (i) Chemistry and (ii) Social Network. Both benchmarks consist of graphs with nodes, but they differ in how an intervention on one node affects its connected nodes. More details are provided in the following section. The ultimate goal for the LLM model is to uncover the hidden dynamics among the nodes, specifically, to determine their connections via an adjacency matrix, analogous to making new scientific discoveries.

Scientific breakthroughs require multiple experimental cycles: (1) formulating a hypothesis based on existing observations; (2) designing experiments to test that hypothesis; (3) gathering new observations from these experiments; and (4) refining the hypothesis based on the new data. Similarly,  LLM model undergoes a sequence of cycles, referred to as autonomous cycles. It starts with an initial hypothesis (an estimated adjacency matrix) and, based on current state observations, suggests an intervention that leads to new observations. Ultimately, the model aims to converge on a hypothesis (adjacency matrix) that matches the underlying ground truth. Fig.~\ref{Framework} illustrates our benchmarking framework with a fully autonomous cycle.   
 To evaluate the performance of LLMs, we further introduce an assessment of long-term trajectory information tracing using structured matrices.

\subsection{Chemistry Environment}
\label{Chemistry Environment}
In this setting, we simulate chemical reactions using Directed Acyclic Graphs (DAGs), as illustrated in Fig.~\ref{Chemistry}(a) and (b). In these graphs, each node represents a molecule, and molecules are connected by directed edges. For each node a state value is assigned randomly from the set $\{0, ..., S\}$, where $S$ is the maximum state value.   An intervention on a molecule causes all downstream molecules to change their state to a random state, while the state of the intervened molecule remains unaffected. 

Let $H_{ch} \in \{0,1\}^{N \times N}$ be the adjacency matrix representing the relationships between molecules, where  $N$ denotes the number of molecules. Each element of the matrix is either 1 or 0, with 1 indicating the presence of a directed edge and 0 indicating its absence. The ultimate goal for the LLMs is to infer this underlying matrix $H_{ch}$. 

We will begin our experiment with an initial prompt that encapsulates with all the details like, problem statement, ultimate goal and suggesting the model to request an intervention to learn more about the DAG.   More details are provided in the Appendix~\ref{Appendix}. In summary, the model is instructed to understand the current adjacency matrix, with the current state values and prompted to suggest an intervention. After receiving a response from the model, based on the suggested intervention, the Oracle who is aware of the ground truth implements the intervention and note down the new observations. In the consecutive cycles, the prompt includes,      description prompting, previous interventions performed by LLMs, and all observations corresponding to these interventions in the form of an observation matrix will be provided to the LLMs. The observation matrix which is denoted as $G_{ch} \in \{0, ..., S\}^{M \times N}$, where $M$ represents the number of cycles, $N$ represents the number of molecules. This cycle will continue once the hypothesis of LLMs $K_{ch}$ matches underlying ground-truth $H_{ch}$ or reaches predefined cycle limitation.   

\subsubsection{Evaluation} For a given DAG, multiple adjacency matrices can exhibit similar causal behavior. As illustrated in Fig.~\ref{Chemistry}(c) and (d), even though the graph structures differ, interventions yield similar effects on the remaining nodes. Therefore, it is essential to consider this property when evaluating the similarity between generated matrices and the ground truth. Where \(\left(K_{ch}\right)^n\) denotes the matrix \(K_{ch}\) raised to the \(n^{\text{th}}\) power (i.e., \(K_{ch}\) multiplied by itself \(n\) times).

\[
H(i,j) = 
\begin{cases}
1, & \text{if } \displaystyle\sum_{n=1}^{M} \left(K_{ch}\right)^n(i,j) > 0, \\
0, & \text{otherwise,}
\end{cases} \quad \forall\, i,j.
\]

Given the hypothesis matrix produced by the model, we compute the corresponding reachability matrix, as demonstrated above, for both the hypothesis and ground truth matrices. Then the resultant matrices are compared to each other to evaluate the similarities. 

%It is also possible that two causal graphs may lead to same observations, as indicated in Fig.~\ref{Chemistry} (c)\&(d). This situation should also be considered correct. Therefore, we also 

\subsection{Social Networks}
\label{Social Networks}
Similar to our Chemistry setting, to imitate real-world social networks, relationships between individuals are represented using graphs, as illustrated in Fig.~\ref{Chemistry}. The main difference lies in the structure of the connections between nodes and the way they influence each other. 

In the social network, the connections between individuals are undirected. An intervention on a specific person increases this person's state by 1, as well as the states of their neighboring nodes. Therefore, in this case, the adjacency matrix $H_{so} \in \{0,1\}^{N \times N}$ should be symmetric, where $N$ represents the number of individuals in the social network. The observation matrix here is denoted as $G_{so} \in \{0, 1, 2...\}^{M \times N}$, where $M$ represents the number of cycles, $N$ represents the number of individuals.

%%%%%%%%% Social Figure %%%%%%%%%
\begin{figure}[t]
\centering
{\includegraphics[width=0.95\linewidth]{Fig/Social.pdf}
}\\
\caption{Illustration of the Social Network setting. The brackets indicate (person index, person state). Figures (a) and (b) illustrate the change in state after an intervention on person 0.}
\label{Social}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%% Exp Table %%%%%%%%%
\begin{table*}[t!]
\centering
\caption{Performance comparison of multiple LLMs. We present the average number of iterations each LLM requires to obtain the correct answer, along with the success rate.}
\begin{adjustbox}{max width=0.9\textwidth}
\begin{tabular}{cccccc}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Benchmarking\\ (Setting)\end{tabular}} & \multicolumn{5}{c}{Model used for evaluation (Average Iterations, Successful Rate)}           \\ \cline{2-6} 
                                                                        & Claude-3-5-haiku  & Gemini-1.5-pro     & Llama-3.1-70b-instruct & Gpt-4o             & Qwen2.5-72b-instruct \\ \hline\hline
\begin{tabular}[c]{@{}c@{}}Social Network\\ (Persons: 3)\end{tabular}   & (\textbf{1}, 20\%)& (3, 60\%)          & (3, 70\%)              & (2, \textbf{100\%})& (2, \textbf{100\%})  \\       
\begin{tabular}[c]{@{}c@{}} (Persons: 5)\end{tabular}   & ($\infty$, 0\%)   & (5, 50\%)          & (6, 30\%)              & \textbf{(4, 100\%)}& (5, \textbf{100\%})  \\
\begin{tabular}[c]{@{}c@{}}(Persons: 10)\end{tabular}  & ($\infty$, 0\%)   & ($\infty$, 0\%)    & ($\infty$, 0\%)        & \textbf{(8, 30\%)} & ($\infty$, 0\%)      \\ 
\hline
\begin{tabular}[c]{@{}c@{}}Chemistry\\ (Nodes: 3; States:3)\end{tabular}   & (\textbf{3}, 25\%)& (4, 95\%)          & (5, 15\%)              & (4, \textbf{100\%})& (4, \textbf{100\%})  \\
\begin{tabular}[c]{@{}c@{}} (Nodes: 3; States: 5)\end{tabular}  & (6, 5\%)          & (\textbf{4}, 85\%) & (\textbf{4}, 35\%)     & \textbf{(4, 100\%)}& \textbf{(4, 100\%)}  \\
\begin{tabular}[c]{@{}c@{}} (Nodes: 10; States: 5)\end{tabular} & ($\infty$, 0\%)   & (11, \textbf{15\%})& ($\infty$, 0\%)        & \textbf{(10, 15\%)}& ($\infty$, 0\%)      \\ \hline
\end{tabular}
\end{adjustbox}
% \vspace{-3mm}
\label{Benchmark}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments} 

To ensure a comprehensive comparison, we evaluate several state-of-the-art models, including Claude-3-5 \cite{claude3.5_haiku}, GPT-4o \cite{gpt4o}, Gemini 1.5 Pro \cite{gemini1.5_pro}, Llama-3.1 (70B) \cite{llama3.1_70b}, and Qwen2.5 (72B) \cite{qwen2.5_72b}. All experiments are conducted on the OpenRouter platform \cite{openrouter}, and the experimental budget is 365 USD. To ensure robust results and minimize the impact of random fluctuations, we conduct 20 independent trials for each chemistry setup and 10 trials for social network.
Below, we provide details on the models, experimental setup, and parameter configurations.

\textbf{Chemistry} To provide a comprehensive evaluation, we consider three different of configurations of $N$ and $S$ values (3,3),(3,5), and (10,5). %(3 nodes, 3 states), (3 nodes, 5 states), and (10 nodes, 5 states). 
The purpose of the three different configurations is to measure the trend of reasoning capabilities in LLMs with the change of graph complexity—specifically, as the number of nodes and the diversity of causal properties (satte values) increase. This setup enables us to investigate whether LLMs can maintain consistent reasoning performance as the causal graph becomes more complex, and to determine if there is a threshold beyond which their performance deteriorates. 

\textbf{Social Network} In this experiments, similar to Chemistry, we introduce three configurations: (3 persons), (5 persons), and (10 persons). The three different configurations aim to assess the capability of LLMs in managing increasing network complexity as the number of persons and potential relationships grows. This design enables us to investigate whether the reasoning performance of LLMs remains robust when processing larger, more intricate networks.

\subsection{Analysis}
The experimental results are presented in Table~\ref{Benchmark}. In the experiment, we set a predefined cycle limit equal to twice the number of nodes. In a specific round, if the LLM fails to infer the underlying causal graphs within this limit, it is considered to have failed the game for that round. The success rate represents the proportion of rounds in which the LLM successfully obtained the correct answer. The average number of iterations is calculated by averaging all iterations required in successful rounds. We analyze these results from several perspectives.

\noindent \textbf{Social Network}
Table~\ref{Benchmark} indicates that GPT-4o and Qwen2.5 demonstrate superior understanding and reasoning capabilities compared to other LLMs, such as Claude-3-5, Gemini-1.5, and Llama-3.1. For instance, GPT-4o and Qwen2.5 achieve 100\% success rate in the settings with 3 and 5 people, whereas the success rate of the other three LLMs remains below 70\%. 

As expected, as the number of people increases, the success rate of all LLMs decreases. For example, the success rate of Gemini-1.5 is 60\%, 50\%, and 0\% in the settings with 3, 5, and 10 people, respectively. The primary reason for this decline is that existing LLMs struggle to extract key information from complex social networks, leading to inferior performance.

For the most powerful LLMs, GPT-4o and Qwen2.5, performance remains stable in understanding and inferring relationships within the Social Network when the number of people is fewer than 5. However, when the number of people increases to 10, their performance declines significantly. For instance, GPT-4o achieves 100\% success rate in the setting with 5 people, whereas its success rate drops to only 30\% in the setting with 10 people. This indicates that there is still significant room for improvement in existing LLMs' ability to understand and infer relationships within complex social networks. 

Moreover, Table~\ref{Benchmark} shows the average number of iterations required for LLMs to successfully justify their reasoning. It can be observed that the average number of iterations is very low when LLMs address simple social networks. For example, the average number of iterations for Claude-3-5 and Qwen2.5 is 1 and 2, respectively. As the number of people increases, the average number of iterations increases as well. The primary reason for this is that, when the number of people is high, LLMs may need more iterations to extract key information and infer relationships. These experiments reveal that the problem-solving capabilities of existing LLMs in complex social networks are inefficient, leaving significant room for improvement. 

Based on the above analysis, future LLMs may focus on improving LLMs' ability to efficiently extract key information and infer relationships within complex social networks. This could involve enhancing the models' understanding of network structures, improving their ability to handle large-scale relational data, and optimizing their reasoning algorithms. Additionally, even when information extraction and inference are accurate, the speed of inferring relationships decreases as the complexity of the social network increases. Future advancements should aim to address this limitation, ensuring faster and more accurate reasoning in extended interactions or more complex scenarios.


\noindent \textbf{Chemistry}
Unlike the experiments on the Social Network, which focus on undirected causal graphs, the Chemistry experiments assess the capability of LLMs to address directed causal graphs. The experimental results are shown in Table~\ref{Benchmark}. It can be observed that Gemini-1.5, GPT-4o, and Qwen2.5 demonstrate superior understanding and reasoning capabilities in the Chemistry setting. For example, the success rates of Gemini-1.5, GPT-4o, and Qwen2.5 are (95\%, 85\%), (100\%, 100\%), and (100\%, 100\%) in the settings of (3 nodes, 3 states) and (3 nodes, 5 states). In contrast, the success rates of Claude-3-5 and Llama-3.1 are lower than 50\% in both settings.

It is surprising that with the increase in the number of states, the powerful LLMs, GPT-4o and Qwen2.5, can maintain the success rates and do not require additional iterations for analysis. For instance, the average iterations and success rates of GPT-4o and Qwen2.5 are (4, 100\%) and (4, 100\%) in the settings of (3 nodes, 3 states) and (3 nodes, 5 states), respectively. This indicates that these two LLMs can handle complex state changes in directed causal graph settings. 

As expected, as the number of nodes increases, success rate of all LLMs decreases and average iterations increases. For example, the success rate of Gemini-1.5 is 85\% in the (3 nodes, 5 states) setting and decreases to 15\% in the (10 nodes, 5 states) setting. The success rate of Gpt-4o is 100\% in the (3 nodes, 5 states) setting and decreases to 15\% in the (10 nodes, 5 states) setting. Most of LLMs, including Claude-3.5-haiku, Llama-3.1-70b-instruct, and Qwen2.5-72b-instruct, fail to win in every round of the task. This indicates that understanding and reasoning in complex directed causal graph settings still leaves a large room for improvement for existing LLMs.

As for average iterations, we can find that LLMs need more iterations to address more complex directional causal graphs. Although GPT-4o and Qwen2.5 can quickly find the relations when the number of nodes is low, they struggle to handle many nodes.

Based on the above analysis, future LLMs should focus on improving performance in multi-object interactions, as their performance drops significantly. Additionally, as the number of nodes increases, the analysis speed decreases substantially. Future advancements should aim to address this limitation. 

\noindent \textbf{Chemistry vs Social Network}
Although the overall success rate of LLMs in the Chemistry setting (directional causal graphs) is slightly higher than that in the Social Network setting (non-directional causal graphs), the average number of iterations is similar. Surprisingly, although Gemini-1.5 struggles with non-directional causal graphs (with a success rate of around 50\%-60\% in simple cases), it can effectively handle simple directional causal graph cases (with a success rate greater than 85\%). In conclusion, existing LLMs still leave significant room for improvement in understanding and reasoning in complex Social Network and Chemistry settings.

\section{Evaluation on Long-Term Trajectory Tracking}
As mentioned above, we observe that existing LLMs often fail to provide useful feedback due to their limited ability to capture long trajectory information. Consequently, experiments solely based on causal graphs may not fully reflect the true reasoning capabilities of these models. To address this issue, we conduct additional experiments aimed at evaluating the capability of LLMs in capturing long trajectory information. In the following subsections, we first introduce our long-term trajectory measurement, then summarize key findings, and discuss their implications in detail.
\label{Trajectory}
\subsection{Task Definition}
In each round, a matrix with shape $M \times N$ is given to the LLM, where $M$ represents the trajectory length and $N$ denotes the number of nodes. Each element in the matrix is an integer value varies from 0 to $P$, where $P$ represents different color states. Formally, let $X \in \{0,..., P\}^{M \times N}$ denote the given observation matrix. The task requires an LLM to infer the color trajectory matrix $Y \in \{0,1\}^{(M-1) \times N}$, where:
\begin{equation}
    Y_{i,j} = \begin{cases}
        1, & \text{if } X_{i,j} \neq X_{i+1,j} \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}
for all $i \in \{0, ..., M-2\}$ and $j \in \{0, ..., N-1\}$. To reduce the impact of outliers, this process will be conducted for $R$ rounds. 

\subsection{Prompting and Model Evaluation}
To assess LLMs' ability to comprehend and reason over structured information with long trajectory, we employ a structured zero-shot prompting strategy. The prompt explicitly describes the task, including matrix interpretation and expected output format. We evaluate multiple LLMs, including GPT-4o, Gemini, and Qwen, under identical conditions. Models are queried with the prompt and required to generate structured JSON outputs containing only the color trajectory matrix $Y$.

We conduct systematic evaluations across various trajectory lengths ($M \in \{3,5,10,15,20\}$) to measure the robustness of each model. To evaluate the performance of each LLM, we develop two types of metrics, noted as Average Trajectory Accuracy (AT-Acc) and Overall Accuracy (OA-Acc). 

Overall Accuracy is computed as the fraction of correctly predicted color trajectory matrix:
\begin{equation}
    \text{OA-Acc} = \frac{1}{R} \sum_{i=1}^{R} \mathbbm{1} (Y_i = \hat{Y}_i)
\end{equation}
where $\hat{Y}$ represents the model-generated color trajectory matrix, ${Y}$ represents the ground-truth color trajectory matrix, ${R}$ represents number of rounds, and $\mathbbm{1}$ is the indicator function. In our experiment, we use $R = 100$. 

Average Trajectory Accuracy aims to measure the average accuracy of the color change between each pair of rows. For example, the color change between row $0$ and row $1$ is noted as the first trajectory. To achieve that, a matrix $T \in \{0,1\}^{R \times (M-1)}$ is further constructed, where:
\begin{equation}
    T_{r,i} = \begin{cases}
        1, & \text{if } Y_{i,r} = \hat{Y}_{i,r} \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}
for all $r \in \{0, ..., R-1\}$ and $i \in \{0, ..., M-2\}$. $Y_{i,r}$ represents row $i$ of matrix $Y$ in round $r$. The Average Trajectory Accuracy is computed as following:

\begin{equation}
    \begin{aligned}
        \text{AT-Acc}_j &= \frac{1}{R} \sum_{i=1}^{R} T_{i,j}, \\
        &\forall j \in \{1, \dots, M-1\}
    \end{aligned}
\end{equation}

\subsection{Chain-of-Thought Prompting}
Chain-of-Thought (CoT) prompting has been proven effective in enhancing the reasoning ability of large language models (LLMs). To further explore its effectiveness in long-term trajectory tracing, we incorporate CoT prompting before computing the final matrix. In our experiment, we use a simple prompt: \textit{"Please also include the reason for your answer."} We compare zero-shot and CoT-augmented responses, analyzing accuracy improvements across sequences.




\begin{table*}[t!]
\centering
\caption{Overall accuracy comparison of multiple LLMs. We report performance across various trajectory lengths, both with and without CoT prompting.}
\begin{adjustbox}{max width=0.9\textwidth}
\begin{tabular}{cccccc}
\hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Trajectory Length\\ \end{tabular}} & \multicolumn{5}{c}{Model used for evaluation (W/O CoT, With CoT)}        \\ \cline{2-6} 
                                                                                  & Claude-3-5-haiku & Gemini-1.5-pro & Llama-3.1-70b-instruct & Gpt-4o & Qwen2.5-72b-instruct \\ \hline\hline
3                    & (69\%,52\%) & (42\%,51\%) & (6\%,8\%) & \textbf{(41\%,99\%)} & \textbf{(6\%,30\%)}           \\
5                    & (26\%,29\%) & (18\%,18\%) & (0\%,1\%) & \textbf{(13\%,100\%)}& \textbf{(1\%,31\%)} \\
10                   & (0\%,0\%)   & (0\%,2\%)   & (0\%,0\%) & \textbf{(0\%,91\%)}  & \textbf{(0\%,30\%)} \\
15                   & (1\%,0\%)   & (0\%,0\%)   & (0\%,0\%) & \textbf{(0\%,28\%)}  & (0\%,5\%)  \\
20                   & (0\%,0\%)   & (0\%,0\%)   & (0\%,0\%) & (0\%,0\%)            & (0\%,3\%)  \\
25                   & (0\%,0\%)   & (0\%,0\%)   & (0\%,0\%) & (0\%,1\%)            & (0\%,0\%)  \\
30                   & (0\%,0\%)   & (0\%,0\%)   & (0\%,0\%) & (0\%,0\%)            & (0\%,0\%)  \\ \hline
\end{tabular}
\end{adjustbox}
% \vspace{-3mm}
\label{Acc_vs_NoO}
\end{table*}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Fig/Acc_vs_Pairs_NR.pdf}
        \caption{Without CoT Prompting}
        \label{fig:Acc_vs_Pairs_NR}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Fig/Acc_vs_Pairs_R.pdf}
        \caption{With CoT Prompting}
        \label{fig:Acc_vs_Pairs_R}
    \end{subfigure}
    \caption{Average Trajectory Accuracy vs. Trajectory}
    \label{Acc_vs_Pairs}
\end{figure*}

\subsection{Experiment Results} 
In this section, we present and analyze the results of our long trajectory measurement. These results offer insights into the understanding and reasoning capabilities of LLMs across different settings, particularly their ability to capture long trajectory information. In the following subsections, we summarize key findings and discuss their implications in detail.

\subsubsection{Analysis of Trajectory Measurement}
The experimental results are presented in Table~\ref{Acc_vs_NoO} and Figure~\ref{Acc_vs_Pairs}. We analyze these results from two perspectives: 

\noindent \textbf{Overall Accuracy vs Trajectory Length}
Table~\ref{Acc_vs_NoO} shows the Overall Accuracy of various large language models (LLMs) with respect to the trajectory length. The models are evaluated both with and without Chain-of-Thought (CoT) prompting. It can be observed that the accuracy of LLMs significantly degrades as the trajectory length increases. For example, Claude achieves an accuracy of 69\% with 3 observations but drops to 0\% when the trajectory length exceeds 20. 

Additionally, we can observe that for GPT-4o and Qwen2.5, CoT prompting generally improves the models' performance, particularly with shorter observation sequences. However, performance declines as the sequence length increases. For example, with trajectory length of 5, GPT-4o achieves an accuracy of 13\% without using CoT, but this increases to 100\% when CoT is applied. However, CoT prompting does not improve the performance of models with longer observation sequences, meaning that its benefits are limited to shorter input lengths. In future work, exploring more robust methods for handling longer observation sequences may help mitigate the performance degradation observed as input lengths increase. 

\noindent \textbf{Average Trajectory Accuracy vs Trajectory}  Given trajectory length of 20, Figure \ref{Acc_vs_Pairs} shows the Average Trajectory Accuracy of various large language models (LLMs) with respect to the trajectories. The results in this figure are averaged from 100 trials. It can be observed that as the trajectory increases, the accuracy of the models gradually decreases. This suggests that existing large language models struggle to handle long-term trajectory information, a phenomenon we refer to as temporal attention decay. Additionally, we find that certain models, such as GPT-4o and Qwen2.5, benefit from CoT prompting, which significantly improves their ability to capture long-term temporal dependencies. However, the accuracy of Qwen2.5 with CoT in the 19th trajectory remains below 70\%. This indicates that future work should focus on further improving the models' ability to manage long temporal sequences, potentially by exploring more advanced techniques for enhancing temporal attention mechanisms.

\section{Conclusion} 
In this paper, we present \textit{Auto-Bench}, a novel benchmark designed to evaluate the scientific discovery capabilities of LLMs, which challenges models to uncover hidden structures and make optimal decisions through iterative interactions with an oracle. Specifically, our benchmark is built around two core settings: Chemistry and Social Networks. The Chemistry setting simulates chemical reactions to assess LLMs' ability to understand and reason about directed graphs, while the Social Network setting evaluates their performance in undirected graphs by modeling real-world social interactions. 
%
By gradually increasing the complexity of the causal graphs, we conduct a comprehensive analysis of LLMs' capacity for iterative knowledge refinement. Our experiments with state-of-the-art models such as GPT-4, Gemini, Qwen, Claude, and Llama reveal a significant performance drop as the problem complexity increases. Notably, the performance of LLMs is constrained by their ability to capture and maintain long trajectory information. This study not only highlights current limitations but also provides a foundation for future work aimed at enhancing LLMs’ iterative reasoning and knowledge updating capabilities in scientific research.

\section{Limitations}
While our benchmark provides a novel framework for evaluating LLMs in scientific discovery through causal reasoning, it has several limitations. First, the framework assumes that causal relationships can be inferred purely from structured matrix observations and predefined interventions, which may not fully capture the complexities of real-world scientific reasoning that often involves unstructured data, domain-specific knowledge, and hypothesis generation beyond direct observations. Second, the benchmark currently focuses on discrete state changes and predefined causal structures, limiting its applicability to more dynamic, continuous, or probabilistic causal systems. Additionally, the evaluation primarily relies on adjacency matrix reconstruction, which may not comprehensively measure an LLM’s ability to reason about causality in more abstract or interdisciplinary contexts. Lastly, the benchmark does not account for external knowledge retrieval, a critical component of human scientific discovery, which could lead to underestimating an LLM’s full potential. Future work should address these challenges by incorporating more diverse data modalities, probabilistic causal reasoning, and interactive learning environments that better simulate the iterative nature of real-world scientific discovery.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{main}

\clearpage
\appendix
\section{Appendix}
\label{Appendix}
\begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=0.95\textwidth]{Fig/Prompt.pdf}
\end{minipage}
\includepdf[pages=2-, fitpaper=true, scale=0.73, pagecommand={}]{Fig/Prompt.pdf}

\end{document}
