% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{linguex}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{tabularx}

\usepackage[textsize=scriptsize,textwidth=2cm]{todonotes}

\renewenvironment{quote}{%
  \list{}{%
    \rightmargin0.3cm
    \leftmargin0.3cm  
    \rightmargin\leftmargin
  }
  \item\relax
}
{\endlist}

\title{Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs}


\author{Leonie Weissweiler \quad \quad \quad Kyle Mahowald \\
  The University of Texas at Austin\\
  \texttt{\{weissweiler,kyle\}@utexas.edu}\\\And 
  \quad \quad  \quad Adele E. Goldberg \\
  \quad \quad \quad Princeton University\\
  \quad  \quad \quad \texttt{adele@princeton.edu} \\   
  }

\date{}
\begin{document}
\maketitle
\begin{abstract}
Linguistic evaluations of how well LMs generalize to produce or understand novel text often implicitly take for granted that natural languages are generated by symbolic rules. Grammaticality is thought to be determined by whether or not sentences obey such rules. Interpretation is believed to be compositionally generated by syntactic rules operating on meaningful words. Semantic parsing is intended to map sentences into formal logic.
Failures of LMs to
obey strict rules have been taken to reveal that LMs do not produce or understand language like humans. 
 Here we suggest that LMs' failures to obey symbolic rules may be a feature rather than a bug, because natural languages are not based on rules. New utterances are produced and understood by a combination of flexible interrelated and context-dependent schemata or \textit{constructions}. 
 We encourage researchers to reimagine appropriate benchmarks and analyses that acknowledge the rich flexible generalizations that comprise natural languages. 
\end{abstract}

\section{Introduction}

\begin{table*}[]
    \centering
    \footnotesize
    \begin{tabular}{ll}
        \toprule
        \textbf{Constructions: Learned Pairings of Form and Function} &  \textbf{Rules}  \\ \midrule
        (Partially-filled) words, common and rare (partially-filled) schemata  & Common abstract patterns  \\ 
        Wide range of functions & Only abstract functions  \\
        Combination of open slots and/or fixed lexical units & Include only variables \\
        Context-sensitive (and plentiful) & Context-free (and few)  \\
        Inter-related within a complex network  & Unstructured list  \\
        Sensitive to similarity and frequency & Insensitive to similarity or frequency \\
        Slots constrained in open-ended range of ways & Variables constrained by gram. category  \\
         \bottomrule
    \end{tabular}
    \caption{Differences between constructions and rules}
    \label{tab:rules_vs_cxns}
\end{table*}

How well do large Language Models (LMs) 
generalize beyond their training data? 
The majority of work intended to address this question has presumed that symbolic rules for syntax and semantics are required to generalize: producing acceptable new forms and compositional meanings. 
If you learn a new color term (`simony') and a new count noun (`blurk'), you know how to combine them and have a strong intuition about what `a simony blurk' must be. 
Symbolic rules are crucial for generalizations in math, logic, formal syntax, and programming languages.They are valid \textit{in general} and contain variables that can be instantiated by any instance of a general type (e.g., numbers in math; propositions in logic; grammatical categories in phrase structure rules). 

Because earlier statistical models (e.g., n-gram or Markov models) seemed unable to generalize fully or capture non-local dependencies \cite{chomsky1957},  rules seemed to many to be the only game in town for human language, too. After all, if a standard bigram model hadn't seen `simony blurk' before, it would be unable to interpret it.
Influential thinkers
argued that neural networks, which did not involve rules, would never be appropriate models of human cognition for this reason \cite{fodor1988connectionism, pinker1988, marcus1998, fodor2002compositionality, marcus2001, calvo2014architecture}.

Yet today's LMs arose from statistical, distributional parallel models \cite{mikolov2013a,rumelhart1986} rather than rule-based natural language technologies.
Though they do not rely on hard-coded rules, LMs ability to produce coherent, naturalistic language and respond appropriately is unparalleled by purely symbolic systems \cite{piantadosi2023modern, goldberg2024,weissweiler-etal-2023-construction,hofmann2024derivational}.

Despite the game-changing performance of LMs, they have inherited some of the skepticism that was directed at their forbearers 
\cite{marcus2001,dentella2023,leivada2024}. And NLP researchers continue to test whether the new models learn syntactic, semantic or compositional rules: 
e.g.,  Natural Language Inference \cite{bowman-etal-2015-large}, Semantic Parsing \cite{palmer2005,reddy-etal-2017-universal}, tests of binary grammatical acceptability \cite{warstadt-etal-2019-neural} and rule-based compositionality \cite{kim-linzen-2020-cogs}. Together, such tasks made up more than half of the GLUE benchmark \cite{wang-etal-2018-glue}, created to evaluate language models on their skill at being ``general, flexible, and robust.''

Lackluster performance on rule-based tasks, particularly in the early days of LMs, was taken to imply that although LMs may appear to be mastering natural language, they are merely imitating shallow surface patterns \cite{lake2018, kim-linzen-2020-cogs, weissenhorn-etal-2022-compositional,bolhuis2023}. 
In a survey of 79 NLP researchers, \citet{mccurdy-etal-2024-toward} reported that 87\% believed LMs were not sufficiently compositional and a sizeable proportion (39\%) believed explicit discrete symbolic rules were required.


Evaluations of LMs' ability to follow algebraic or logical rules did expose certain shortcomings in their ability to reason abstractly. However, rules are not sufficient for mastering natural language, and they are only necessary in limited cases, if at all. 
Therefore, we suggest that rule-based evaluations of LMs' skill with \textit{natural language} have been over-emphasized. 

\textbf{Rules are not sufficient for generalization} because humans depart from rule-based generalizations in a multitude of cases.  For LMs to use language like humans, richer interpretations are required for thousands of collocations, conventional metaphors, idioms, and context-dependent interpretations.
To the extent that apparent failures of rule-based compositionality in LMs reflect human-like behavior \cite{hu-etal-2024-llm,lampinen2024}, we further suggest that algebraic \textbf{rules are not necessary for generalization} for natural language \cite[e.g.,][]{hofmann2024derivational, mcclelland1999}. 

We propose that natural language requires mastering a network of hundreds of thousands of context-dependent, gradient, flexible schemata or \textit{constructions}, which may contain `slots' that constrain their fillers and how those fillers are interpreted, as LMs do \cite{tseng-etal-2022-cxlm}.
Constrained slots allow constructions to be combined in new ways, flexibly adapting to context. For instance, the construction '<time period> ago' can coerce a temporal interpretation of filler phrases that do not designate time periods (e.g., `three rest stops ago'). Differences between rules and constructions are indicated in Table \ref{tab:rules_vs_cxns}.
Once languages are recognized to include a vast network of restricted types of constructions (and slots), which are sensitive to similarity, frequency and context, it is unproblematic to allow rule-like constructions as a limiting case. We suggest that researchers should move past evaluating LMs on how well they obey rigid rules and focus more on the \textit{extent to which} and \textit{how} LMs manage to produce and comprehend human-like natural languages in all their context specificity and complexity.

Many of our points are not new. While early AI relied on algebraic rules \cite{minsky1969, lenat1995}, many researchers soon realized that rules were too brittle to scale up beyond highly restricted domains such as artificial block worlds \cite{winograd1980}. Neural network researchers have continuously argued against the usefulness of rules, primarily in the domain of words and inflectional morphology \cite[e.g.,][]{rumelhart1986, rogers2004, elman2009, christiansen1999, macdonald1994}.

Our contribution is to review leading paradigms used in LM evaluation for syntax (§\ref{sec:syntax}), semantics (§\ref{sec:semantics}), and compositionality (§\ref{sec:compositionality}). We explain how rules are implicitly assumed in each case, briefly describing how the assumptions arose, and why we feel they are problematic.  We propose constructions as an alternative theoretical basis (§\ref{sec:cxns}), encouraging the field to evaluate LMs on the extent to which LMs learn and represent the complex network of constructions that comprises each language and \textit{how} they generalize.

\section{Formal Syntax in LM Evaluation}
\label{sec:syntax}

\paragraph{Syntax as Rules}
The notion that natural languages are generated by syntactic rules such as phrase structure rules, movement rules, or the operation `Merge' has been assumed by most versions of generative grammar since \citet{chomsky1957}. Syntactic rules are intended to operate on broad and clearly defined  `grammatical categories' (e.g., Nouns, Verbs, Adjectives), which are understood to combine in rule-like fashion to create larger units (e.g., Noun Phrases, Verb Phrases, Adjective Phrases). The Lexicon, or system of words, was kept separate and distinct, as words, but not rules, were recognized to be influenced by frequency, similarity, meaning, or context \cite{pinker1999}. 

An alternative to the rule-based approach in linguistics is the constructionist approach. The latter recognizes that grammatical patterns \textit{are} sensitive to frequencies, context, and can convey meaning, information structure and other functions---an approach we discuss in detail in Section \ref{sec:cxns}.


\paragraph{Grammaticality Tasks}

LMs' syntactic knowledge is regularly evaluated by classification tasks that require models to distinguish grammatical from ungrammatical sentences.
CoLA \cite{warstadt-etal-2019-neural}, which includes example sentences from linguistics textbooks, is commonly used to evaluate such binary classifications. While it might be natural to assume that textbook examples represent extreme ends of a grammaticality spectrum,  this is not the case. \citet{juzek-2024-syntactic} collected human acceptability judgments for part of the dataset and reported that humans assign systematically gradient judgments.

Human judgments on sentences depend on frequency, plausibility, complexity, memory demands, potential alternatives, and context \cite{grodner2005,  schutze2013judgment, robenalt2015, gibson1993,fang2023}.
The amount of exposure to written language and even training in linguistics also influences judgments. For instance, \citet{dabrowska2010} found that laypeople's judgments on sentences containing long-distance dependencies were more sensitive to lexical content than linguists' judgments were.

The recognition that human judgments are gradient can have profound consequences. For instance,
\citet{dentella2023} compared humans and LMs against predetermined binary acceptability labels, reporting that LMs' performance correlated poorly. 
However, comparing gradient perplexity-based judgments with the human judgments collected by \citet{dentella2023} revealed a strong positive correlation \cite{hu2024language}. 



\paragraph{Dependency Evaluation}

The task of parsing text for universal dependencies \cite[UD,][]{de-marneffe-etal-2021-universal} was a well-established task before transformer-based LMs \cite{zeman-etal-2017-conll,zeman-etal-2018-conll}.  After \citet{hewitt-manning-2019-structural} showed BERT \cite{devlin-etal-2019-bert} to be somewhat skilled in UD, UD became the default operationalization of syntax in the NLP world \cite{amini-etal-2023-naturalistic,kryvosheieva-levy-2025-controlled,muller-eberstein-etal-2022-probing} and in discussions of inductive biases \cite{lindemann-etal-2024-strengthening,glavas-vulic-2021-supervised}. 
UD annotations are partially determined by semantics which draws them closer to the approach advocated here; but UD analyses presume a universal set of grammatical relations, which is problematic \cite[e.g.,][]{croft2001radical}. Moreover, annotation is inconsistent for the long tail of language phenomena, including head-less constructions (e.g., \textit{the Xer, the Yer} construction) or idioms.  Therefore, evaluating LM accuracy on UD annotations can give misleading results. 


\paragraph{The Chomsky Hierarchy}
The assumption that natural language syntax is based on formal rules is connected to the claim that it is located somewhere above regular language on the Chomsky hierarchy \cite{chomsky1956}. But transformers cannot handle context-free grammars in general \cite{someya-etal-2024-targeted,strobl-etal-2024-formal}, which would seem to undermine their ability to model human language \textit{in principle}. This predicts that LMs' successes must only be apparent. But the constructionist approach recognizes that languages are context-dependent, rather than being generated by strict rules. Therefore, under the constructionist approach, the limitations of transformer models as formal models are not limitations \textit{qua} models of human language.


\section{Formal Semantics in LM Evaluation}
\label{sec:semantics}

\paragraph{Formal Semantics}

Formal logic was developed as a branch of mathematics, used to prove mathematical and philosophical theorems, and identify provability gaps \cite{frege1918,russell1905,goedel1931}. It is based on algebraic rules operating on clearcut and broadly defined categories (e.g., propositions).
Notably, logicians did not generally assume nor endorse using this formalism to represent the meanings of natural language utterances \cite{carnap1937,baker1986language}.
Natural language differs from formal logic in many ways. Formal logic fails to capture the different meanings of  \textit{and} and \textit{but}; or \textit{all, every} and \textit{each}. 
It does not capture ambiguity or context effects \cite{wittgenstein1953,russin2024}. It does not provide a natural way to capture anything other than propositions \cite[e.g., commands, questions, wishes,][]{austin1975}, nor does it naturally distinguish presuppositions and assertions \cite{strawson1967}. 


\paragraph{Semantic Parsing}

Semantic parsing evaluations arose as an extension of syntactic parsing. They require models to map sentences into rule-based symbolic representations \cite{banarescu-etal-2013-abstract} 
to evaluate semantic understanding in LMs \cite[][see also §\ref{sec:compositionality}]{li-etal-2023-slog,qiu-etal-2022-improving,shaw-etal-2021-compositional}.  At times,
semantic parsing is applied explicitly to restricted domains designed to obey rules, but such domains are necessarily limited. For instance, \citet{piantadosi2016} trained a model on representations of `the girl,' `the cat,' `the hedgehog,' `the cat loves the girl,' and `the hedgehog sees the cat,' and so on to test whether the model predicted a formal semantic representation for `The girl loves the hedgehog.'  However, note that if `mosquitoes' were substituted for `the cat,'  different interpretations of `love' would be evoked (`Mosquitoes love the girl' vs.`The girl loves mosquitoes'), not to mention markedly different degrees of plausibility. 

\paragraph{Natural Language Inference}

Natural Language Inference tasks label the second of two sentences an entailment, contradiction, or neutral.   While entailment and contradiction are key concepts in mathematical proofs, their importance in language understanding has been overstated as in the quote from \citet{bowman-etal-2015-large} \cite[emphasis added, see also][]{katz1972,vanBenthem2008}:

\begin{quote}
    The semantic concepts of 
    entailment and contradiction are central to \textit{all} aspects of natural language meaning. 
\end{quote}

\noindent 
NLI tasks were originally used to train models  \cite{superglue,dagan2005,nie-etal-2020-adversarial}. In the age of LMs, they are used as a zero-shot evaluation metric to assess natural language understanding  \cite{zhou-etal-2024-constructions,mccoy-etal-2019-right}.
But this may lead us to underestimate LMs. The recognition of necessary and plausible inferences is an important aspect of natural language understanding, but the  NLI task is oversimplified: it fails to account for the communicative goal or context-dependent interpretations.

Humans' goal is to make sense of others' messages, so we assume others are trying to be relevant and helpful and do our best to assign coherent meanings to all utterances \cite{grice1975}. For example, outside of logic classes or heated arguments, people rarely conclude that two statements made by the same person are contradictory. If someone utters: `The boy is depressed; The boy is not depressed,' listeners do not throw up their hands and shout "contradiction." Instead, they may infer that clinicians disagree about whether the boy is depressed, or understand that the boy is sad at the moment but not truly depressed. Humans also assign distinct context-dependent interpretations to apparent tautologies such as  `Either it's alive or it's not' and `If it snows, it snows." Therefore, NLI tasks that rely on judging contradictions or entailments may over- or under-estimate how well LMs understand natural language the way people do.

\section{Compositionality in LM Evaluation}
\label{sec:compositionality}

\paragraph{Compositionality in Linguistics}
As computer coding languages became more and more widespread, rule-based semantics and syntax took root in linguistics.  A Principle of Compositionality combined the two traditions. It states that the meaning of a sentence is determined by the meanings of the words and the syntactic rules used to combine them \cite{montague1970, partee1984nominal, dowty1979, jackendoff1992semantic, fodor2002compositionality}. This is a bottom-up process:  syntactic rules combine words, which have determinant meanings. Context was not supposed to influence the interpretation of words in a top-down manner. Instead, downstream inferences were invoked to address the obvious fact that interpretation does depend on context.
As \citet{fodor1988connectionism} state, ``a lexical item must make approximately the same
semantic contribution to each expression in which it occurs''. Yet they, like Carnap and Frege, acknowledge: ``It’s uncertain exactly how compositional natural
languages actually are'' \cite{fodor1988connectionism}.

The standard argument in favor of compositionality is outlined in  \citet{white-etal-2024-early}:

\begin{enumerate}[topsep=1pt,itemsep=0ex,partopsep=1ex,parsep=1ex]
\item 
People tend to agree on the interpretation of new sentences. 
$\Rightarrow$ There must be some set of rules that determine the meaning of new sentences. \label{rules_exist}
\item Sentences are generated by a relatively small set of syntactic rules that combine words \cite{chomsky1957}. \label{rules_generate}
$\Rightarrow$ Meaning is determined by meaningful words and the syntactic rules used to combine them.
\end{enumerate}


\noindent While people more or less agree on the meanings of new sentences in context, this does not entail that meaning is determined by rule-based algorithms operating on familiar words (contra [1]). 
People also generally agree on the interpretations of pointing gestures and novel words,  and yet in each of these cases, the shared interpretations must be gleaned from non-linguistic context (in the case of pointing gestures), or from a combination of linguistic and non-linguistic context.

Similarly, the meanings of familiar collocations, compounds and idioms are not determined by general rules. For instance, a compositional rule involving set-intersection may be appealing for `<color term> noun' combinations in the domain of artificial block worlds (e.g., a green cube is something that is both a cube and green). However, violations of such rules abound e.g., green tea is more yellow than green and Cambridge blue is actually green. Even more common are instances that evoke richer meanings than predicted by any algebraic rule: e.g.,  a green light implies that forward motion or progress is permitted, and a green card provides a path toward citizenship in the US.

%Is it reasonable to recognize that the meanings of gestures, words, and idioms are not determined by compositional rules, while presuming that compositional rules are required for non-idiomatic novel phrases? We suggest it is not: several linguistic examples of non-compositional but productive patterns will be provided in Sec. \ref{sec:compositionality}.

Critically, people tend to mostly agree on the interpretations of utterances in context-dependent ways. Consider `the Persian cat is on the rug.' If the goal of the speaker is only to find the furball, there need be no commitment to the cat being a thoroughbred Persian breed. Likewise, the speaker need not be committed to the cat being wholly on, rather than adjacent to, the rug. Or, comprehenders may appreciate the statement as ironic, if the cat is hairless. And the rug need not have been created or used as a rug for people to share the same intended interpretation of the sentence.  

The second premise in the standard argument is also problematic. Rules massively over-generalize, which is why they were never used for production. That is, rules predict all manner of odd locutions \cite{pawley1983,sag2002}: e.g.,  `Meeting you is pleasing to me'; `The tall winds hit the afraid boy'; `Explain him the problem.' Humans are sensitive to the frequencies of various types of word combinations and judge formulations unnatural whenever there exists a more conventional way to express the intended message in context \cite[e.g.,][]{goldberg2019explain}. 
 

\paragraph{Evaluating LMs for Compositionality}
Compositionality benchmarks combine elements from one or another evaluation paradigm already described. %
 \citet{kim-linzen-2020-cogs}'s compositional generalization challenge (COGS) tested whether models could formalize into formal semantics, any sentence generated by a small set of syntactic rules.  
They anticipated generalizations from sentences like `Jane gave the cake to John' to `Jane gave John the cake.' The models were found to perform poorly. Speakers' choice between these two constructions is highly sensitive to information structure, dialect \cite{bresnan2010}, and the relative frequencies and similarities of verbs witnessing in each version \cite{goldberg2019explain,leong2024, ambridge2014}. Insofar as natural language is not amenable to logical representations, %as we are suggesting,
 failures of LMs to map to such representations may be consistent with human interpretation.
%Testing LMs not only for their ability to make such a choice, but also for their sensitivity to factors that influence it, could make our assessment of their abilities more accurate.

Other compositionality benchmarks adopt NLI tasks, which commonly presume interpretation is determined by rules.  
For example, in the context of robotic agents interpreting instructions, \citet[p.1]{lake2018} state:
\begin{quote}
Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb `dax', he or she can immediately understand the meaning of `dax twice'... 
\end{quote}
The robotic agents struggled to interpret the rule-based command, though it was appropriate in the narrow domain tested. Notably, the rule does not extend to a broader swath of language. For instance, unbounded actions are not countable, so if `twice' appears at all, it is likely followed by a comparative phrase (e.g., `work twice as hard') and a very different meaning than performing an action two times. Other cases require knowledge of specific combinations: `to think twice,' which means 'to hesitate' and `going twice' tends to evoke the context of an auction. Familiar phrases with meanings not fully captured by compositional rules are common: By one estimate, we learn tens of thousands of them \cite{jackendoff2002}. 
Importantly, we largely agree on their interpretations, even though each means something more or different than predicted simply by the words and their syntactic combination.  In this way, phrasal combinations regularly involve subregularities or item-specific interpretations not predicted by a general algebraic rule. 


Another example comes from the seemingly innocuous algebraic rule stated below:
\begin{quote}
If X is more Y than Z, then Z is less Y than Z, irrespective of the specific meanings of X, Y, and Z. \cite[p.5]{dasgupta2020} 
\end{quote}

\noindent
The rule is intended to capture that `Anne is more cheerful than Bob' should both contradict `Anne is less cheerful than Bob', and entail `Bob is less cheerful than Anne.' NLI models that failed to draw these inferences were considered lacking. 
Yet natural language rarely relies on free variables. The content of X, Y, and Z  matter.  No one would infer that because Anne$_{x}$ is more cheerful$_{y}$ than careful$_{z}$, that `Careful$_{z}$ is less cheerful$_{y}$ than Anne$_{x}$.' Perhaps more importantly, if a speaker uttered `Anne is higher than Bob and Bob is higher than Anne,' listeners would likely infer either that Bob climbed above Anne in the time it took to utter the first clause or that Bob has been smoking. 


\section{Constructions}
\label{sec:cxns}


We have argued against the idea that natural language is generated or interpreted by symbolic rules, but we agree that speakers are generally able to agree on the meanings of new sentences well enough for communication to be successful. This section briefly explains how constructions offer an alternative with respect to each of the differences cited in Table \ref{tab:rules_vs_cxns}. Language is generated by flexibly combining constructions, which comprise a rich and complex ConstructionNet for each language. These include words, but are far broader than the traditional lexicon, encompassing schemata larger than individual words as well (Table \ref{tab:cxns_w_examples}). 


\paragraph{(Partially-filled) Words, Common and Rare Schemata} 
We use the term `construction' to refer to a learned association between a formal pattern and a range of related functions. This simple definition treats words, idioms, rare \textit{and} common grammatical patterns as constructions, and recognizes that each case may include open `slots' (Table \ref{tab:cxns_w_examples}). Formal attributes may include phonology, lexical content, grammatical categories, word order, discontinuous elements, and/or intonation. 

\paragraph{Wide Range of Functions} 
Constructions' functions vary widely: 
Constructions may convey rich, specific contentful meaning in the case of words, collocations, idioms. A plethora of other constructions are productive but constrained in a wide variety of semi-specific ways; argument structure constructions convey `who did what to whom'; discourse structuring constructions indicate which subparts of a sentence are at-issue or backgrounded. Constructions exist to ask questions, express surprise or disapproval, for example. Any level of construction can be associated with specific registers, genres, and/or dialects. 

\begin{table}[]
    \centering
    \footnotesize
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{>{\raggedright\arraybackslash}p{0.9\linewidth}}
        \toprule
        \textbf{Types of Construction and Examples} \\ \midrule
        \textbf{Words}: pregame; running; nevertheless; ago \\ 
        \textbf{Partially filled words (morphemes)}: pre-N$_{\text{event}}$; V-ing \\
        \textbf{Collocations, fixed idioms}: high winds; jump the shark \\
        \textbf{Partially-filled productive constructions}: X is the new Y; It's Adj of <agent> VP$_{\text{to}}$ \\
        \textbf{Partially-filled argument structure constructions}: give <recipient> a call \\
        \textbf{Argument structure constructions}: Intransitive; Caused-motion; Double Object; Resultative \\
        \textbf{Discourse-structuring constructions}: get-passive; information questions; it-clefts; relative clauses \\
        \bottomrule
    \end{tabular}
    \caption{Example constructions at varying levels of complexity and abstraction}
    \label{tab:cxns_w_examples}
\end{table}

\paragraph{Sensitive to Similarity and Frequency}
Language users are sensitive to the frequencies of constructions. For instance, the passive construction is far more frequent in Turkish than English and young Turkish speakers learn the construction far earlier than English-speaking children \cite{slobin1986}.
Constructions are also influenced by similarity: Instances of a construction prime instances of the same or closely related construction \cite[e.g.,][]{dubois2014, pickering2008}.

\paragraph{Productive Constructions May Include Fixed Lexical Units}
 As shown in Table \ref{tab:cxns_w_examples}, syntax, semantics and morphology are interrelated rather than assigned to distinct levels. This is useful because even productive hierarchical constructions often include particular words and semantic constraints. For example, an English construction that implies real or metaphorical motion allows a wide range of verbs but requires the particular noun `way' (
{'He charmed his way into the meeting'}.)

\paragraph{Plentiful and Context-Sensitive}
The broad definition of constructions as pairings of form \textit{and} function, including words \textit{and} grammatical patterns, is another difference between constructions and rules. Constructions are far richer and more plentiful than the class of rules is commonly envisioned to be. Constructions also do far more work than rules since they capture frequency information and contextual constraints, while rules are presumed to be context-free and uninfluenced by frequency.


\paragraph{Interrelated System, Not Unstructured List}
Unlike rules, which are commonly presented as unstructured lists, constructions comprise a network of interrelated statistical patterns. This allows for the fact that languages have families of related constructions. It also allows for the simple fact that productive constructions simultaneously co-exist with specific conventional instances. For instance, the English `double object' construction is productive, and speakers are also familiar with dozens of conventional instances  (e.g., `give <someone> the time of day', `throw <someone> a bone'). 

\paragraph{Construction Slots Are Constrained}
The open `slots' of constructions are constrained in a wide variety of ways. For instance, the English double-object construction can appear with a wide range of verbs, but prefers simple verbs to those that sound Latinate (e.g., 'She told them something' vs. 'She proclaimed them something'). The English comparative suffix `-er' (e.g., `calmer', `quicker') is available for most single-syllable adjectives that allow a gradient interpretation, but it is not used with
 past participles adjectives (? `benter'). 




\paragraph{An Example} Consider `X is the new Y'. It is productive and can be used to create new utterances e.g.,  `Semiconductor chips are the new oil.' As is typical of productive constructions, the generalization co-exists with several familiar instances (e.g., `50 is the new 40'; `Orange is the new black'). The construction is not an algebraic rule. Its slots, indicated by X and Y, are not variables that range freely over fixed syntactic categories. Instead, `X' must be construed (playfully) as currently functioning in the culture as `Y' used to. Therefore not all combinations of slot fillers make sense:  (e.g., ? `Orange is the new oil'). Adding a parallelism constraint between X and Y is insufficient since `103 is the new 101' also makes little sense. 
Finally, instances of the construction are not amenable to translations into formal logic, which would presumably treat `Orange is the new black' as equivalent to `Black is the old orange,' which does not conventionally evoke the same meaning.


\section{Implications Beyond Natural Language}
\label{sec:beyond}

The current observations help make sense of LM behavior outside the domain of pure language. Even in domains that are rule-like by design, certain types of non-compositional behavior exist, likely due to their interface with natural language.
For instance, LMs have been found unreliable at drawing the following inference, which the authors dubbed the \textit{reversal curse}: 
``if `A is B' [...] is true, then `B is A' follows by the symmetry property of the identity relation" \cite[p. 2]{berglund2023reversal}.

\textit{Why} are LMs prone to the reversal curse?  
Although the quote above is stated in natural language, it does not apply to natural language sentences, which are actually rarely reversible.  For example, `A mental illness is the same as a physical illness' means something very different than  `A physical illness is the same as a mental illness' \cite[see also][]{tversky1977features, talmy1975}.
Even simple conjunctions are not generally reversible in natural language. For instance, `night \& day' and `day \& night' are both acceptable, but their interpretations differ:  the former conveys a stark contrast (e.g., `as different as night and day'), the latter suggests a relentless activity or process (e.g., `he worried day and night').  In summary, it is perhaps reasonable to expect truly symmetric knowledge to be reversible. But LMs are trained on natural language and natural language utterances are not symmetric.

Human reasoning depends on the context in which performance is tested \cite{klauer2000belief, wason1968, tverskykahneman} and how instructions are formulated \cite{evans1994}.
\citet{lampinen2024} find that LMs and humans are influenced by semantic context in similar ways \cite[see also][]{mccoy2023embers}. 
Even math is not fully rule-compositional when equations are intended for communication: for instance, $2 + 2 = 4$ means something different than $4 = 2 + 2$ \cite{mirin2022mathematicians}, a preference picked up on by LMs \cite{boguraev2024}.

\section{New Directions}
\label{sec:new}

Natural languages involve complex and context-sensitive systems of constructions, which vary from being wholly fixed to highly abstract and productive. Constructions are combined when a unit, potentially itself composed of constructions, fills a slot in another construction.
Viewing language as a system of constructions rather than words and rules may fundamentally change how the successes and failures of models are construed, and new goals and questions come into focus. 

\paragraph{Balancing Constructions and Rules}
Coding languages are compositional by design. They are unambiguous with variables filled by any instance of a clearly defined and general type. Accordingly, increasing the proportion of code in pretraining improves performance on tasks that rely on rule-based compositionality such as logic and math \cite[e.g.,][]{kim2024,madaan-etal-2022-language}. 
Yet \citet{petty2024} report that adding code to pretraining data hurts performance on naturalistic language processing including tasks involving the English passive \cite{mueller-etal-2022-coloring} and BigBench's Implicatures and CommonMorpheme tasks  \cite{bigbench}.
\citet{sprague2024} report that  while Chain-of-Thought prompting had been believed helpful across-the-board, performance only improves on problems that require algorithmic reasoning. 
Thus, adding code to pre-training or using CoT prompts benefit tasks that are designed to be rule-compositional but may be detrimental to to natural language tasks.
\paragraph{Better Datasets}

Rather than using abstract rules to generate stimuli for natural language benchmarks, ecologically valid stimuli may be more usefully collected or adapted from natural corpora and then normed for naturalness and plausibility. Since human judgments are highly context-dependent, benchmark tasks should ideally also vary contexts systematically \cite[see, e.g.,][]{ross-etal-2024-artificial}. 

It is also important to avoid inadvertently training human coders to give the type of responses only suitable in logic or coding classes. If people are instructed to interpret `red X' as 'X that is red for any X,' they can do so. Yet in natural contexts, people understand that red grapefruits are closer to pink, red hair is more orange, a red book may be about communism, and crossing a red line may have consequences.
Finally, a variety of items, participants, and contexts ought to be valued as much as a variety of models.


\paragraph{Probing for Constructions}
Recognizing that LMs are already extremely skilled at producing and responding to several natural languages allows for a shift in research agendas. The interesting question may now be not \textit{if}, but \textit{how} LMs achieve such remarkable skills. We can now also ask how well LMs capture appropriately nuanced interpretations and relationships among constructions. New ways of probing LMs make this possible and this toolkit will only grow. 


As is familiar from the lexicon, constructions are related to one another because human memory is highly associative. 
\citet{misra-mahowald-2024-language} have demonstrated that even when all instances of a rare non-compositional construction are ablated from training data, non-trivial learning of the construction remains, enabled by the presence of related constructions in training. 
Another type of relationship among constructions are the relationships between conventional instances and productive generalizations. Nearly every productive construction co-exists with at least a few formulaic instances, and LMs offer ways of testing relationships among instances that give rise to productive generalizations. 
Other recent work includes
\citet{weissweiler-etal-2022-better}, who found LMs reliably discriminate instances of the English Comparative Correlative from superficially similar expressions. \citet{tayyar-madabushi-etal-2020-cxgbert} tested a dataset of automatically induced constructions and reported that BERT \cite{devlin-etal-2019-bert} could determine whether two sentences contained instances of the same construction. However,  
\citet{zhou-etal-2024-constructions} found LMs failed to distinguish entailment differences between the causal excess construction (e.g., `so heavy that it fell') and two structurally similar constructions  (`so happy that she won'; `so certain that it rained'). Similarly addressing challenging construction semantics, \citet{weissweiler2024hybrid} showed LMs to struggle with the meaning of the caused-motion construction. 

\citet{tseng-etal-2022-cxlm} showed that LMs gradiently predict appropriate slot fillers. 
\citet{li-etal-2022-neural} probed RoBERTa's implicit semantic representations of four argument structure constructions and found similarities in behavior in the model and a sorting task done by humans. \citet{potts2023characterizing} found that despite its rarity, LMs acquire the Preposing in PP construction \cite{CGEL}.

\section{Conclusion}
\label{sec:conclusion}

Generalization is a key component of human language---and a big part of why LMs are successful at processing language. But we have argued that evaluations of the linguistic abilities of LMs are too often based on an assumption that generalization requires algebraic rules operating on words. But natural languages are not Lego sets. We suggest instead that language involves flexible combinations of rich and varied constructions of varying sizes, complexity, and degrees of abstraction, which differ from algebraic rules in many ways. By designing new evaluations that accurately reflect the complexities of language, we can avoid under- or overestimating language models. The extent to which LMs produce and interpret combinations of constructions has been limited to date. LMs offer fertile ground for new types of evaluations and new analyses that offer deeper understanding of the remarkable skills required for natural language. 

\section*{Limitations}

The claims here are based on existing evaluations of LMs. LMs are rapidly improving in a variety of ways. While we have aimed to discuss benchmarks and evaluations in ways that reflect the historical trajectory as well as the present-day landscape, it is always possible that newer models could behave differently than our characterizations.

Another potential limitation is that evaluating LMs is a moving target. While we give a number of suggestions for evaluation, we also recognize that the kinds of linguistic evaluations needed may move away from tests of grammaticality (which seems largely mastered) and towards more general kinds of language understanding.
The rapid pace of LM technology makes future-proofing such designs potentially difficult.

Finally, while we focus mostly on constructionist approaches, there are related usage-based approaches from the functionalist tradition that would likely make similar predictions. 
Future work can further flesh out these directions.

\section*{Acknowledgments}
We thank Kanishka Misra and Will Merrill for helpful discussions and feedback. We are grateful to audiences at the NSF-sponsored New Horizons in Language Science workshop and the Analytical approaches to understanding neural networks summmer school
sponsored by Simon's Foundation for helpful feedback. Leonie Weissweiler was supported by a postdoctoral fellowship of the German Academic Exchange Service (DAAD).

\bibliography{anthology, literatur,kyle}


\end{document}
