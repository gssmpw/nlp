\section{Related Work}
Model merging has gained popularity in LLM research ____. By amalgamating multiple homologous LLMs into a single model, this technique has been applied to address several challenges, such as building multi-task experts ____, detoxification ____, and preference alignment ____. Model merging methods are primarily based on two fundamental approaches: weight averaging ____ and task arithmetic ____. 

Weight-based model merging methods design rules or matrices to determine merging coefficients. For example, RegMean ____ optimizes a linear regression problem for linear weights, Fisher-Merging ____ uses the Fisher information matrix to assess parameter importance. Some works explore the space of these coefficients using parameter searching algorithms, such as evolutionary algorithms ____ or Bayesian optimization ____. Although these methods demonstrate effectiveness, they suffer from inefficiency: parameter search is time-consuming, and solving the objectives requires substantial computation resources. 

Subspace-based model merging methods focus on eliminating insignificant parameters and merging sparse models within the parameter subspace to reduce interference. TIES ____ trims individual models based on parameter magnitudes, while Model Breadcrumbs ____ refines this by removing both low-magnitude and high-magnitude outliers. DARE ____ emphasizes the importance of rescaling after sparsification, and TALL-Mask ____ creates task-specific mask matrices based on predefined thresholds to filter out irrelevant parameters. However, these methods are limited to specific patterns, such as sign conflicts or threshold-based filtering, and magnitude-based sparsification remains suboptimal. To better address the interference problem, we propose a solution based on parameter saliency sparsification and a mutually exclusive iterative merging framework.