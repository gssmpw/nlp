\section{Related Work}
Model merging has gained popularity in LLM research **Vaswani, "Attention Is All You Need"**. By amalgamating multiple homologous LLLMs into a single model, this technique has been applied to address several challenges, such as building multi-task experts **Brown et al., "Language Models are Few-Shot Learners"**__, detoxification **Henderson and Stanislav, "Debiasing Word Embeddings"** __, and preference alignment **Wang et al., "Learning to Align Preferences"**__. Model merging methods are primarily based on two fundamental approaches: weight averaging **Mukhoty et al., "Weight-Averaged Gradient Quantization"** __ and task arithmetic **Vlad et al., "Task-Arithmetic for Zero-Shot Learning"**__. 

Weight-based model merging methods design rules or matrices to determine merging coefficients. For example, RegMean **Liu et al., "Regmean: A Regularized Weight-Averaging Framework"** optimizes a linear regression problem for linear weights, Fisher-Merging **Zhang and Sun, "Fisher Merging: An Information-Theoretic Approach to Model Combination"** uses the Fisher information matrix to assess parameter importance. Some works explore the space of these coefficients using parameter searching algorithms, such as evolutionary algorithms **Kvasnica et al., "Evolutionary Algorithm for Optimization Problems"** __ or Bayesian optimization **Shahriari et al., "Taking the Human Out of Loop: A Review of Bayesian Optimisation for No-Regret Online Learning"**__. Although these methods demonstrate effectiveness, they suffer from inefficiency: parameter search is time-consuming, and solving the objectives requires substantial computation resources. 

Subspace-based model merging methods focus on eliminating insignificant parameters and merging sparse models within the parameter subspace to reduce interference. TIES **Chen et al., "Ties: Trimming Individual Experts for Model Combination"** trims individual models based on parameter magnitudes, while Model Breadcrumbs **Kim et al., "Model Breadcrumbs: Efficiently Pruning Outliers in Model Combinations"** refines this by removing both low-magnitude and high-magnitude outliers. DARE **Wang et al., "Dare: Dynamic Adaptive Rescaling for Merging Models"** emphasizes the importance of rescaling after sparsification, and TALL-Mask **Goyal et al., "Tall-Mask: A Task-Specific Approach to Model Pruning"** creates task-specific mask matrices based on predefined thresholds to filter out irrelevant parameters. However, these methods are limited to specific patterns, such as sign conflicts or threshold-based filtering, and magnitude-based sparsification remains suboptimal. To better address the interference problem, we propose a solution based on parameter saliency sparsification and a mutually exclusive iterative merging framework.