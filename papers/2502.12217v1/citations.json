[
  {
    "index": 0,
    "papers": [
      {
        "key": "metagpt",
        "author": "Zhou, Yuyan and Song, Liang and Wang, Bingning and Chen, Weipeng",
        "title": "MetaGPT: Merging Large Language Models Using Model Exclusive Task Arithmetic"
      },
      {
        "key": "yang2024model",
        "author": "Yang, Enneng and Shen, Li and Guo, Guibing and Wang, Xingwei and Cao, Xiaochun and Zhang, Jie and Tao, Dacheng",
        "title": "Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "cai2023robust",
        "author": "Cai, Ruisi and Zhang, Zhenyu and Wang, Zhangyang",
        "title": "Robust weight signatures: gaining robustness as easy as patching weights?"
      },
      {
        "key": "fusechat",
        "author": "Wan, Fanqi and Zhong, Longguang and Yang, Ziyi and Chen, Ruijun and Quan, Xiaojun",
        "title": "Fusechat: Knowledge fusion of chat models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2024separate",
        "author": "Hu, Xinshuo and Li, Dongfang and Hu, Baotian and Zheng, Zihao and Liu, Zhenyu and Zhang, Min",
        "title": "Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation"
      },
      {
        "key": "zhang2023composing",
        "author": "Zhang, Jinghan and Liu, Junteng and He, Junxian and others",
        "title": "Composing parameter-efficient modules with arithmetic operation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dogerm",
        "author": "Lin, Tzu-Han and Li, Chen-An and Lee, Hung-yi and Chen, Yun-Nung",
        "title": "Dogerm: Equipping reward models with domain knowledge through model merging"
      },
      {
        "key": "rame2024rewarded",
        "author": "Rame, Alexandre and Couairon, Guillaume and Dancette, Corentin and Gaya, Jean-Baptiste and Shukor, Mustafa and Soulier, Laure and Cord, Matthieu",
        "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "modelsoups",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ta",
        "author": "Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "matena2022merging",
        "author": "Matena, Michael S and Raffel, Colin A",
        "title": "Merging models with fisher-weighted averaging"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "evolutionary",
        "author": "Akiba, Takuya and Shing, Makoto and Tang, Yujin and Sun, Qi and Ha, David",
        "title": "Evolutionary optimization of model merging recipes"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "liu2024checkpoint",
        "author": "Liu, Deyuan and Wang, Zecheng and Wang, Bingning and Chen, Weipeng and Li, Chunshan and Tu, Zhiying and Chu, Dianhui and Li, Bo and Sui, Dianbo",
        "title": "Checkpoint Merging via Bayesian Optimization in LLM Pretraining"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ties",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit",
        "title": "Ties-merging: Resolving interference when merging models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "model-breadcrumbs",
        "author": "Davari, MohammadReza and Belilovsky, Eugene",
        "title": "Model breadcrumbs: Scaling multi-task model merging with sparse masks"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "dare",
        "author": "Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin",
        "title": "Language models are super mario: Absorbing abilities from homologous models as a free lunch"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "tallmask",
        "author": "Wang, Ke and\nDimitriadis, Nikolaos and\nOrtiz{-}Jim{\\'{e}}nez, Guillermo and\nFleuret, Fran\\c{c}ois and\nFrossard, Pascal",
        "title": "Localizing Task Information for Improved Model Merging and Compression"
      }
    ]
  }
]