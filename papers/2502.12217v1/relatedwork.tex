\section{Related Work}
Model merging has gained popularity in LLM research \cite{metagpt, yang2024model}. By amalgamating multiple homologous LLMs into a single model, this technique has been applied to address several challenges, such as building multi-task experts \cite{cai2023robust, fusechat}, detoxification \cite{hu2024separate, zhang2023composing}, and preference alignment \cite{dogerm, rame2024rewarded}. Model merging methods are primarily based on two fundamental approaches: weight averaging \cite{modelsoups} and task arithmetic \cite{ta}. 

Weight-based model merging methods design rules or matrices to determine merging coefficients. For example, RegMean \cite{jin2022dataless} optimizes a linear regression problem for linear weights, Fisher-Merging \cite{matena2022merging} uses the Fisher information matrix to assess parameter importance. Some works explore the space of these coefficients using parameter searching algorithms, such as evolutionary algorithms \cite{evolutionary} or Bayesian optimization \cite{liu2024checkpoint}. Although these methods demonstrate effectiveness, they suffer from inefficiency: parameter search is time-consuming, and solving the objectives requires substantial computation resources. 

Subspace-based model merging methods focus on eliminating insignificant parameters and merging sparse models within the parameter subspace to reduce interference. TIES \cite{ties} trims individual models based on parameter magnitudes, while Model Breadcrumbs \cite{model-breadcrumbs} refines this by removing both low-magnitude and high-magnitude outliers. DARE \cite{dare} emphasizes the importance of rescaling after sparsification, and TALL-Mask \cite{tallmask} creates task-specific mask matrices based on predefined thresholds to filter out irrelevant parameters. However, these methods are limited to specific patterns, such as sign conflicts or threshold-based filtering, and magnitude-based sparsification remains suboptimal. To better address the interference problem, we propose a solution based on parameter saliency sparsification and a mutually exclusive iterative merging framework.