LLMs have revolutionized natural language processing (NLP), providing remarkable capabilities in tasks such as text generation, question answering, and summarization. Despite these advancements, deploying LLMs in real-world production systems remains challenging due to limitations like hallucinations, where factually incorrect or fabricated information is presented with high confidence. Additionally, LLMs often lack access to proprietary or domain-specific knowledge that was unavailable during pretraining, reducing their effectiveness for specialized applications.  

\begin{figure*}[h]
    \caption{Pipeline}
    \includegraphics[width=\textwidth]{pipeline.pdf} 
    \label{fig:flow}
\end{figure*}

To overcome these limitations, RAG has emerged as a paradigm that enhances LLMs by incorporating external knowledge sources at inference time. RAG systems retrieve relevant information from external databases and integrate it into the prompt without modifying the LLM's underlying model weights. This dynamic retrieval mechanism helps reduce hallucinations and improve factual accuracy, especially for complex or domain-specific queries. However, current RAG implementations primarily rely on dense vector similarity search, which splits text into chunks and retrieves information based on vector proximity. While effective for straightforward queries, these methods often struggle to capture the nuanced contextual relationships required for more intricate information needs.  

Knowledge Graphs (KGs) provide a complementary approach to information retrieval in RAG systems. Unlike vector-based methods that treat text as unstructured tokens, KGs represent knowledge as structured triplets in the form of (entity, relationship, entity), enabling precise queries based on explicit facts. By offering interpretable, mutable, and expandable representations of domain-specific knowledge, KGs improve reasoning capabilities, reduce irrelevant content retrieval, and enhance query precision.  



In this work, we focus on constructing a domain-specific knowledge graph for the Adobe Experience Platform (AEP), a comprehensive customer data management and analytics solution that powers personalized experiences through batch processing, customer segmentation, and real-time data insights. AEP's extensive corpus of documentation contains proprietary and dynamic domain-specific concepts that general-purpose LLMs cannot effectively process. Addressing this challenge requires a structured, scalable approach to extracting and organizing domain knowledge.  


Our framework begins with \textit{document cleaning and summarization} to distill raw content from over 17,000 URLs into concise summaries. We employ LLM-based \textit{dynamic entity discovery}, starting from an empty set of entities that evolves as new entities are identified across documents. Relationships between these entities are extracted as triplets, with a \textit{confidence score} assigned to each triplet to indicate its reliability. Provenance is ensured by linking each triplet to its source document, enabling transparency and traceability. Post-processing steps reduce semantic duplication and overlap, further refining the graph's quality.  

By integrating structured knowledge into retrieval-augmented systems, our domain-specific knowledge graph enhances the factual reliability and contextual accuracy of AI- assistant for Adobe Experience Platform. 