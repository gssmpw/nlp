Our proposed KG-RAG approach was evaluated against the existing production system as a baseline. The production system retrieves the top documents based on the cosine similarity of embeddings, then provides those documents, along with the user prompt, to the LLM to generate the final answer. Adobe's agnostic approach to LLMs allows us to select the best-in-class technology for the task at hand. Currently, the AI Assistant in AEP\cite{adobe_ai_assistant_2025} leverages Microsoft's Azure OpenAI Service to answer queries.



The results demonstrate a significant improvement in answer relevance with the integration of the KG-RAG retriever. For the results shown in Table \ref{tab:results}, we retrieved the top-1 (top-k, where k=1) tuple from the KG for each query and appended it as additional context to the user query before passing it to the generative model. The model-generated response was then evaluated by an LLM judge, categorizing answers into three relevance types: \textit{Irrelevant}, \textit{Moderately Relevant}, and \textit{Fully Relevant}. Table \ref{tab:results} compares the performance of the baseline with the KG-RAG system based on these relevance annotations.


The baseline production system, produced a substantial number of irrelevant answers (52 instances). By integrating the KG-RAG system, irrelevant responses were reduced by 51.9\%, highlighting the effectiveness of KG retrieval in filtering noise and aligning content with user queries. Moderately relevant responses increased by 71\%. Most notably, fully relevant responses nearly doubled, from 17 to 32, marking an 88.2\% improvement.



In addition to evaluating using LLM as a judge, we also computed the average cosine similarity between answers generated by KG-RAG and baseline snippets. The KG-RAG system achieved an average cosine similarity of 0.89 with the \textit{fully relevant} snippets. This high score indicates greater semantic overlap, reinforcing the KG-RAG approach's ability to generate answers that more closely resemble annotated correct answers in meaning, beyond surface-level wording.

Despite its advantages, the implementation faced several challenges. Balancing noise reduction and entity coverage was challenging. Scalability and latency were also concerns, particularly when processing large document collections. Ensuring the quality of the KG required regular validation, automated checks, and periodic human review to prevent the generation of a noisy or inconsistent KG.
\begin{table}[htbp]
\centering
\resizebox{\columnwidth}{!}{
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l|c|c|c}
\hline
\textbf{Response Category} & \textbf{Baseline} & \textbf{KG-RAG} & \textbf{Improvement (\%)} \\
\hline\hline
Irrelevant & 52 & 25 & $-$51.9 \\
Moderately Relevant & 31 & 53 & $+$71.0 \\
Fully Relevant & 17 & 32 & $+$88.2 \\
\hline
\end{tabular}
}

\caption{Comparison of response relevance between baseline and KG-RAG systems. The baseline system retrieves the top documents based on the cosine similarity of embeddings and provides those documents, along with a prompt, to the LLM to generate the final answer.}
\vspace{-3.75em}
\label{tab:results}
\end{table}

Although the system was developed within Adobe's ecosystem, the approach should be adaptable to other enterprise contexts. The document processing pipeline, KG construction, and entity resolution techniques can be customized to fit different domains. Furthermore, the KG-RAG system can be integrated with various LLM providers and adjusted to meet specific performance and retrieval requirements.



\textit{\textbf{Limitations:}}  
While the data used in this study is real-world and collected from production traffic, it may not fully capture the diversity of business data, and sandbox testing may not fully reflect the complexities of live production systems. The small dataset, though relevant, limits generalizability, and future work will focus on expanding it to enhance robustness. Additionally, the use of LLMs as evaluators introduces potential biases and variability. To mitigate this, multiple evaluation methods were employed to cross-validate results, and extensive tuning was performed to align LLM judgments with human-annotated preferences, achieving 85\% agreement. Due to proprietary restrictions, the prompts and code cannot be released; however, detailed methodology descriptions are provided to ensure reproducibility.


 
\textit{\textbf{Future Work:}} We aim to improve the accuracy of entity and relation extraction by utilizing fine-tuned models for embedding generation. We also plan to explore top-k retrieval to complement the reliance on the top-1 result, which could enhance coverage and increase the likelihood of retrieving the most relevant answers. These improvements will aim to refine accuracy and broaden the applicability of our approach, supporting more robust and scalable KG based solutions for answering user queries.