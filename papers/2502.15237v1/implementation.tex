
We now unpack the high-level data flow of our pipeline (Figure \ref{fig:kg_pipeline}) for the system, referred to as KG-RAG throughout this paper, and provide implementation details for each step.

\subsection{Document cleaning and summarization}
In this phase, we focus on preparing the raw documents to make them suitable for entity extraction. This involves cleaning and summarizing the proprietary AEP document content from the 17,000 URLs collected. The first step in the document preparation pipeline is to clean the raw content extracted from these URLs by removing noise. For example, the original documents may contain HTML tags, images, videos, or scripts, which are removed to focus solely on the textual content. URLs and hyperlinks are identified and removed to avoid introducing irrelevant external references into the summary. Additionally, non-relevant metadata, such as author information, and footnotes, are excluded to maintain the contents relevance.

\begin{figure}[h]
    
    \includegraphics[width=0.45\textwidth]{evaluation.pdf} 
    \vspace{-2em}
    \caption{ Evaluation framework for response relevance. The pipeline compares baseline production answers with KG-RAG-enhanced responses.}
    \vspace{-2.0em}
    \label{fig:eval0}
\end{figure}

After cleaning the document, the next step is to generate a concise summary using an LLM. This reduces the document length, thus simplifying the document by highlighting key content and removing unnecessary details, aiding in the identification of relevant entities and relationships at the later stage.


\subsection{Seed Concept, Entity Discovery and Deduplication}

For each document, a summarized version was provided as input, along with a list of seed concepts or previously discovered entities (starting with an empty list). This list served as context for the LLM to identify new entities while avoiding redundancy. The LLM outputted a list of entities found in the document, including both new and known ones. 

A step was implemented to prevent semantic overlap, comparing newly identified entities to existing ones. A graph \( G \) can be described as \( G = (E, R) \), where \( E \) represents the collection of nodes (entities) and \( R \) denotes the set of edges (relations). Let \( e_i \) and \( e_j \) be two entities in \( E \). To improve the quality of the constructed KG, we impose the constraint that no two entities are semantically identical. Specifically, the following condition must hold:
 \vspace*{-1mm}
\begin{equation}
    \forall e_i, e_j \in E, \; i \neq j \quad 
    \label{overlap}
\end{equation}

If an entity was already in the KG (based on exact or close matches), it was excluded from being added again. As each new document was processed, the list was updated 
with any new, non-duplicate entities, ensuring the KG remained consistent and free from redundancy.

\subsection{Relation Extraction and Confidence Scoring}

After identifying the entities, the next step involves extracting the relationships between them, forming triplets. Once the list of entities has been identified in the previous step, it is sent, along with the current document to the LLM that then extracts potential relationships between the entities based on their contextual occurrence within the document. For instance, if the document contains the entities "segment" and "customer data," the model may extract a relationship such as "segment classifies customer data." Each triplet thus contributes to the construction of the KG, establishing connections between the entities.

Once the relationships are extracted, a confidence score is assigned by the LLM to each triplet to assess its reliability. This score ranges from 0 (low confidence) to 1 (high confidence), reflecting the certainty of the extracted relationship. For example, a triplet like (Batch processing, enhances, segmentation accuracy) may receive a high confidence score if it aligns with established industry practices, while a more ambiguous triplet such as (Batch processing, enhances, memory) might be assigned a lower confidence score due to its lack of clear association in the context. The confidence score allows us to filter the KG based on its value. In this paper, we use a threshold of 0.6 for confidence score for constructing the KG for the KG-RAG system. The threshold value was determined through extensive A/B testing.

\subsection{Provenance}

For each relationship in the KG, we maintain a record of its origin to ensure provenance. This is achieved by including the URL of the document from which the relationship was extracted. By incorporating provenance in this manner, we enable traceability, allowing the relationships within the KG to be verified and validated through reference to the original documents. While we do not specifically calculate how provenance influences query ranking, this feature primarily serves to allow users to interact with the source metadata and verify the origins of the information.

