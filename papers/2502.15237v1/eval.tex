This section describes the evaluation framework used to measure the relevance of AI-generated answers. The process is summarized in the flowchart shown in Figure \ref{fig:eval0}.




\subsection{Dataset}

We use a dataset comprising 100 manually annotated question-answer pairs focused on the `segment' domain. These questions were collected based on production traffic, and the annotations were provided by domain experts at Adobe. The responses in the dataset were generated by the existing production system, also referred to as the baseline system, which retrieves the top documents based on the cosine similarity of embeddings and provides those documents, along with a prompt, to the LLM to generate the final answer. For each question, the answer generated by the AI assistant was presented to the annotators, who evaluated its relevance on a three-point scale: \textit{Fully Relevant} answers completely address the question,\textit{ Moderately Relevant} answers partially address the question but may lack completeness, and \textit{Irrelevant} answers fail to address the question meaningfully. To improve annotation consistency, detailed guidelines were provided to the annotators for scoring the responses. This dataset serves as a ground truth for evaluating the relevance and quality of generated answers using KG-RAG.




\subsection{LLM as Judge}


Manual annotation of responses is costly and time-consuming, making it challenging to scale evaluations effectively. To address this limitation, we utilize a LLM as a judge, offering a scalable and efficient alternative. Given the open-ended nature of answers, rule-based evaluation methods are impractical. Similarly, traditional text-matching metrics such as BLEU\cite{papineni2002bleu} and ROUGE\cite{lin2004rouge} are inadequate for assessing semantic relevance, as they rely heavily on surface-level word overlap rather than contextual meaning. The judge LLM evaluates relevance by interpreting the semantic content of responses, providing a more robust assessment aligned with human judgment.



\textbf{\textit{Prompt Design Strategies:}} To further enhance the judge LLM's performance on the evaluation task, we employ two strategies: 

\begin{itemize}
    \item In-context Learning (ICL) \cite{dong2022survey}: This method integrates task demonstrations into the prompt as illustrations. In our work, we randomly select 2-4 demonstrations from the training set, excluding these examples from the evaluation set.

    \item Chain-of-thought (CoT) \cite{wei2022chain}: This approach structures the input prompt to mimic human reasoning. To mitigate potential bias, chain-of-thought prompts were tested with diverse examples covering different semantic complexities. In our work, the judge model is required to generate an explanation first before providing the final judgment.
\end{itemize}


Our LLM as a judge aligns closely with human-annotated preferences, achieving \textbf{85\% }agreement. The Kernel Density Estimate (KDE) plot in Figure \ref{fig:kde} shows the distribution of relevance scores assigned by human evaluators and the LLM judge. The x-axis represents the relevance score on a scale from 0 to 3, where 1 indicates irrelevant answers and 3 indicates fully relevant answers. The y-axis shows the density or frequency of scores in that range. While the curves broadly overlap, indicating general agreement between human and LLM judgments, there is a some divergence at the upper end of the scale where the LLM curve shows less overlap with the human curve. When faced with ambiguous cases, the LLM appears to err on the side of caution by classifying more responses as `moderately relevant' compared to human evaluators. This conservative bias in the LLM judge actually helps prevent overestimation of system performance.

\subsection{Cosine Similarity for Semantic Matching}

In addition to LLM-based evaluation, we leverage cosine similarity \cite{gomaa2013survey} between sentence embeddings to measure semantic closeness between AI-generated answers and annotated ground-truth responses. Cosine similarity captures the contextual relationship between two vectors by measuring the cosine of the angle between them, making it invariant to the magnitude of the vectors and suitable for comparing textual content.

We generate embeddings for both the AI assistants answer without KG-RAG and answers after using KG-RAG using the bert-base-uncased model from HuggingFace \footnote{https://huggingface.co/google-bert/bert-base-uncased}. The cosine similarity between two vectors \( q \) and \( c \), representing the AI-assistant answers before and after incorporating KG-RAG, is defined as:
 \vspace*{-1mm}
\begin{equation}    
\text{Similarity}(q, c) = \cos(\theta) = \frac{\sum_{i=1}^{n} q_i c_i}{\sqrt{\sum_{i=1}^{n} q_i^2} \sqrt{\sum_{i=1}^{n} c_i^2}}
\label{cosine}
\end{equation}

Here, \( q_i \) represents the embedding of the AI-assistant answer of the baseline production, and \( c_i \) represents the embedding of the answer with KG-RAG. The cosine similarity provides a value between -1 and 1, where 1 indicates identical semantic content, 0 indicates no similarity, and -1 indicates complete dissimilarity.




% The cosine similarity between these embeddings provides a quantitative metric for semantic relevance. Unlike word-overlap-based metrics, cosine similarity focuses on conceptual similarity, making it particularly effective for evaluating open-ended text responses. 


This combination of LLM-based scoring and cosine similarity offers a comprehensive and scalable evaluation framework that balances qualitative judgment with quantitative metrics.

\begin{figure}[h]
    
    \includegraphics[width=0.45\textwidth]{fig3.png} 
    \vspace{-2em}
    \caption{LLM-as-Judge KDE plot showing the distribution of relevance scores for baseline production and KG-RAG.}
    \vspace{-2.0em}
    \label{fig:kde}
\end{figure}