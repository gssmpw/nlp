\section{\spaarc}
\label{sec:spaar}
We propose \spaarc, a prefetching framework that addresses the limitations of existing edge caching approaches for MAR applications. It incorporates both the proximity of virtual objects to users and object associations to make informed decisions on edge server caching. Its prefetching policy is complementary to the underlying caching algorithm employed by the edge cache, and is meant to enhance the cache performance.
%We first present the \spaarc{} architecture, followed by a discussion of its main techniques: association and proximity.

%    In this section, we discuss \spaarc{}, the spatial proximity and association based prefetching for augmented reality cache.
    % \vspace{-3mm}
    \subsection{System Architecture}
        Figure \ref{fig:marspaarc} illustrates the MAR pipeline incorporating \spaarc{} into the edge cache.  Sensor data from the user's device is transmitted to the nearest edge server, where video frames are pre-processed and object detection is performed.  Extracted features are then used for object recognition. If a corresponding virtual object exists, the system attempts to retrieve it from the edge cache.  Upon a cache miss, the \spaarc{} algorithm is invoked to identify all necessary objects for prefetching and request them from the cloud, along with the object that caused the cache miss. After receiving the virtual objects, template matching is performed to estimate their pose. This pose information, along with the virtual objects, is sent to the object tracking module on the user's device.  The tracking module identifies objects across frames, and this information, combined with data from the edge server, is used by the rendering module to overlay virtual content onto the user's display.
        
        % The MAR Pipeline for with \spaarc{} is shown in Figure \ref{fig:marspaarc}. Sensor data is transmitted from the user's device to the nearest edge server. The edge server pre-processes the video frames and applies object detection algorithms. Subsequently, features are extracted and utilized by object recognition algorithms to determine the most accurate item description. If a virtual object is linked to the identified item, the system attempts to retrieve it from the cache. In case of a cache miss, the \spaarc{} algorithm is invoked. This algorithm identifies all necessary objects, including the missed object, and sends a request to the cloud. Upon receiving the virtual objects, template matching is performed to estimate their pose. This pose information, along with the virtual objects, is transmitted to the object tracking module on the user's device. The tracking module identifies objects across frames, and this information, combined with data from the edge server, is used by the rendering module to overlay virtual content onto the user's display.

        \begin{figure}[t]
            \centering
            \includegraphics[scale=0.1]{images/spaar/marspaarc.png}
            \caption{MAR Pipeline with \spaarc{} on the edge cache}
            \label{fig:marspaarc}
            \vspace{-5mm}
        \end{figure}

        %\spaarc{} runs on top of existing cache eviction policies and 
        We next discuss the two main \spaarc{} techniques that drive its prefetching:  association and proximity.
        
        \subsection{Association}
        \label{ssec:spaarc_assoc}
        Intuitively, if a user accesses a virtual object in an MAR scenario, they are likely to soon access related objects (e.g., milk and eggs in the grocery scenario), which should be prefetched to the edge cache for faster access. \spaarc's Association technique generates association rules using {\em Association rule mining (ARM)} \cite{bib:armorig, bib:arm} to predict frequently co-accessed virtual objects based on user interaction history. 
        
        ARM identifies relationships between items in a transaction database by discovering {\em frequent itemsets}: groups of items that frequently co-occur in transactions. The {\em support} $(S)$ of an itemset indicates the proportion of transactions containing that itemset, with frequent itemsets exceeding a predefined minimum support threshold. Based on these frequent itemsets, ARM generates {\em association rules} of the form  $A \implies B$, where $A$ (antecedent) and $B$ (consequent) are disjoint itemsets. The {\em confidence} $(C)$ of a rule represents the conditional probability of observing the consequent in transactions containing the antecedent, controlled by a minimum confidence threshold.  The {\em lift} $(L)$ of a rule measures the strength of the association between the antecedent and consequent, quantifying how much more likely they are to co-occur than if they were statistically independent. Support of an itemset $X$ ($S(X)$), confidence of a rule $A \implies B$ ($C(A \implies B)$) and lift of the rule ($L(A \implies B)$) are formally represented as follows:
            \begin{align}
                S(X) & = \frac{T(X)}{T} \\
                C(A \implies B) & = \frac{S(A \implies B)}{S(A)}\\
                L(A \implies B) &= \frac{S(A \cup B)}{S(A) * S(B)}
            \end{align}
            where $T(X)$ and $T$ are the number of transactions containing $X$ and the total number of transactions respectively. We use ARM algorithms to either generate rules a priori from history user interactions or dynamically during the process.

            \begin{figure}
                \centering
                \includegraphics[scale=0.11]{images/spaar/spaarcflowchart.png}
                \caption{\spaarc{} workflow}
                \label{fig:spaarcworkflow}
                % \vspace{-5mm}
            \end{figure}

% AC: I've added the following. Please see if this makes sense
%Traditional ARM is executed on a static set of historical data, w
%As opposed to traditional use of ARM, 
\spaarc{} uses an ARM algorithm to generate a set of associated objects for prefetching whenever there is a cache miss.
However, %n a cache prefetching scenario, 
all such associated objects are not equally relevant for prefetching. Objects that have been accessed more frequently and more recently are more likely to be more useful for caching. Thus, \spaarc{} prioritizes relevant objects within an association set using an {\em association factor} value for each object. This factor reflects the combined influence of frequency and recency of association with the missing object. Objects with higher association factors are prioritized for caching due to their increased likelihood of subsequent user interaction. %\circled{3}.

We calculate the association factor ($A$) of a virtual object $vo$ as follows:
            \begin{align}
                A(vo)_{new} & = (F(vo) * \alpha) + A(vo)_{old} * (1 - \alpha)
            \end{align}
            where $\alpha = \frac{2}{1 + window}$, $window$ is the window frame of previous object interactions which are relevant, $F(vo)$ is the number of times the virtual object $vo$ is reference in the window frame. The value of $window$ could be tuned according to the recency requirement. A larger window implies longer trend and a smaller one for shorter trend. If the access patterns are not changing frequently, a larger window size would suffice. Association factor threshold value should be set such that low association objects are filtered. The higher the threshold value, higher the chance that the selected objects are accessed in the near future.
%generated by the ARM algorithm.

       

        \subsection{Proximity}
        \label{sssec:prox}
            Since relevant AR objects are based on the field of view of the user, the system only needs to prefetch objects that are in the user's physical vicinity. \spaarc{} incorporates spatial proximity to refine the prefetching list generated by the Association component. Proximity refers to the distance of virtual objects $(Prox(vo))$ from the user's location. For instance, in the grocery store scenario (Figure \ref{fig:motiveapp}), an association rule might suggest prefetching ``milk" alongside ``apples" and ``bananas". However, if the user is not yet in the aisle containing milk, immediate prefetching of the ``milk" virtual object is unnecessary. \spaarc{} prioritizes contextual relevance by employing a {\em lazy prefetching} strategy, deferring the prefetching of ``milk" until the user is in closer proximity. This ensures that prefetched objects are highly relevant to the user's immediate surroundings. A {\em proximity threshold} $(Prox(threshold))$ is used to identify objects within a specific distance from the user. This threshold is domain-specific and depends on the application use case and physical environment. %and may require expert input or a trial-and-error approach for optimal tuning \circled{4}.

            % The \spaarc{} policy leverages spatial proximity to further refine the prefetching list generated by the Association. In this context, proximity refers to the distance of relevant objects ($Prox(vo)$) from the user's location. For example (Figure \ref{fig:motiveapp}), an association rule might suggest prefetching "milk" alongside "apples" and "bananas." However, if user trajectory data indicates "milk" is located in the next aisle, immediate prefetching would be unnecessary. \spaarc{} prioritizes contextual relevance by deferring (lazy prefetching) the prefetching of "milk" until the user physically enters the aisle where it resides. This approach ensures that prefetched objects are highly relevant to the user's immediate surroundings, enhancing the overall user experience. We use a proximity threshold ($Prox(threshold)$) to identify objects within a specific distance of the user. This measure is domain specific and may need expert input ore trial and error approach.

            

             \subsection{SAAP Workflow}
            Figure \ref{fig:spaarcworkflow} illustrates the workflow of the \spaarc{} framework. Upon encountering a cache miss for virtual object $vo$ \circled{1}, \spaarc's Association module utilizes the ARM-generated rules to identify potentially associated objects based on the user's access history and the missing object \circled{2}. To refine this selection and prioritize relevant objects, it uses the association factor for each object. Objects with association factors higher than the Association factor threshold are prioritized for caching due to their increased likelihood of subsequent user interaction \circled{3}.
            Next, the Proximity module filters the selected objects further to those within the Proximity threshold of the user \circled{4}. The filtered out relevant objects are set aside for lazy fetch later when the user moves closer to them \circled{5}.
            Once all the virtual objects to be fetched are identified, the request is send to the cloud \circled{6}. On retrieval, the objects are stored in cache and the missed virtual object is returned. Note that \spaarc{} works in a complementary manner to the cache that continues to use its caching algorithms, e.g., for evicting any objects needed to make space in the cache.

            
    % \vspace{-1.5mm}
    \subsection{Adaptive Tuning}
        In ARM, selecting the minimum support and minimum confidence thresholds is often domain-dependent, requiring expert knowledge. To automate \spaarc, we focus on tuning the minimum support parameter. Minimum support is prioritized as it directly influences the generation of frequent itemsets, which are fundamental to the subsequent steps in the process. Other parameters can be tuned in a similar manner.

        \begin{table}
                \caption{Algorithm \ref{alg:minsuptune} Notations}
                \label{tab:annot}
                \begin{center}
                    % \vspace{-4mm}
                    \begin{tabular}{|P{1cm}|p{7cm}|}\hline
                        Notation & Remark\\
                        \hline\hline
                        $\delta$  & hit rate degradation threshold\\
                        \hline
                        $\beta$  &  minimum support\\
                        \hline
                        $\gamma$  & minimum confidence\\
                        \hline
                        $T$ & transactions\\
                        \hline
                        $\zeta$ & lift\\
                        \hline
                        $D$ & number of evenly spaced minimum support values to be selected in the given range that determines the granularity of the search for the optimal minimum support value\\
                        \hline
                        $\kappa$ & kurtosis, statistical measure of the "tailedness" of the distribution of the lift of generated rules\\
                        \hline
                        $\eta$ & threshold for the ratio of the number of association rules to the number of frequent itemsets that helps to control the number of the generated rules\\
                        \hline
                        $\theta$ & used to determine when the distribution of lift has changed significantly, indicating the generation of large number of rules, many of which may be irrelevant\\
                        \hline
                    \end{tabular}
                \end{center}
                % \vspace{-3mm}
            \end{table}
            
        \noindent \textbf{Minimum support}: 
            Algorithm \ref{alg:minsuptune} outlines the procedure for tuning the minimum support parameter (Notations described in Table \ref{tab:annot}). The key insight is that the quality of the parameter value (and hence the corresponding association rules) is measured by its impact on the cache performance (hit rate).
            
            The TuneMinSup function dynamically adjusts the minimum support based on cache hit rate degradation. It determines whether to generate new association rule sets, select from existing ones, or continue with the current/next set, depending on the degree of hit rate degradation relative to a predefined threshold.

            The GenARules function identifies the bounds for minimum support and generates $N$ new association rule sets, ranging from low to high support values, based on the last $n$ transactions. The value of $n$ can be adjusted to meet the application's association recency requirements.
        
            The SetARules function selects the appropriate rule set to use. Whenever new $N$ rulesets are generated in the increasing order of minimum support, the function selects the middle ruleset. Otherwise, it goes in the increasing or decreasing minimum support direction, depending on the hit rate degradation. This dynamic adjustment allows the system to adapt to changing access patterns and maintain optimal performance.
        
        \begin{algorithm}
        \footnotesize{
            \caption{Minimum support tuning and ruleset generation}
            \label{alg:minsuptune}
            \begin{algorithmic}[1]
                \Procedure{TuneMinSup}{$\delta, \gamma, T$}
                    \State $hrd \gets getDegradation()$
                    \If {$hrd > 2 * \delta$}
                        \State $GenARules(T, \gamma)$
                    \EndIf
                    \State $SetARules()$
                \EndProcedure

                \Procedure{GenARules}{$T, \gamma$}
                    \State $\{\beta_{low}, \beta_{high}\} \gets getMinSupBound(T)$
                    \State $\{\beta_{temp}\} \gets divideMinSupBound(\{\beta_{low}, \beta_{high}\}, D)$
                    \State $\kappa_{prev}, \kappa_{curr} \gets NULL$
                    \State $\{\beta_{newlow}, \beta_{newhigh}\} \gets \{\infty, -\infty\}$
                    \For{$\beta_{t} \in \{\beta_{temp}\}$} \Comment Largest to smallest minsup
                        \State $fqitemsets \gets genFreqItemsets(T, \beta_{t})$
                        \If{$fqitemsets.size > 0$}
                            \State $arules \gets genARules(fqitemsets, \gamma)$
                            \State $arules \gets arules[\zeta \ge 1]$
                            \If $\frac{arules.size}{fqitemsets.size} > \eta$
                                \State $break$
                            \Else
                                \State $\kappa_{curr} \gets kurtosis(arules[\zeta])$
                                \If {$abs(\kappa_{prev} - \kappa_{curr}) > \theta$}
                                    \State $break$
                                \Else
                                    \State $\beta_{newlow} \gets min(\beta_{newlow}, \beta_{t})$
                                    \State $\beta_{newhigh} \gets max(\beta_{newhigh}, \beta_{t})$
                                    \State $\kappa_{prev} \gets \kappa_{curr}$
                                \EndIf
                            \EndIf
                        \EndIf
                    \EndFor

                    \State $ruleset \gets genARuleSets(T, \{\beta_{newlow}, \beta_{newhigh}\}, $ $\gamma, N)$ \Comment Generate N rulesets and store
                    \State $SetARules()$
                \EndProcedure
            \end{algorithmic}
            }
        \end{algorithm}
        
        In the GenARules function, the initial lower and upper bounds for minimum support are first determined. Since popular items typically represent a small percentage of the total unique items in most datasets \cite{bib:spmf}, the lower bound is set to the average support of all items \cite{bib:minsup}.  From this initial range, $D$ evenly spaced minimum support values are selected. For each value, association rules are generated with a fixed minimum confidence. During rule generation, the process terminates if the rule count ratio exceeds a predefined threshold $(\eta)$, indicating the potential inclusion of irrelevant rules. Otherwise, the kurtosis value is recorded. If the difference in kurtosis between the current and previous rule sets exceeds another threshold $(\theta)$, the process terminates, signaling the generation of a large number of rules with relatively low lift values.  This step helps to prevent overfitting and ensures the selection of meaningful rules. If neither termination condition is met, the current minimum support range is updated. Finally, $N$ rule sets are generated from the identified minimum support bounds and utilized accordingly. This adaptive approach allows for efficient and effective rule generation tailored to the specific characteristics of the workload.