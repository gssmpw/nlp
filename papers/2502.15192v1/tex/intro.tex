\section{Introduction}
\label{sec:intro}
    Mobile Augmented Reality (MAR) \cite{bib:arsurvey1} is a transformative technology with applications across diverse domains, from gaming \cite{bib:pgorest} and education \cite{bib:eduar} to healthcare \cite{bib:healthar} and manufacturing \cite{bib:manuf}. However, its reliance on data-intensive computations and need for low latency presents unique challenges.
    
    % Mobile Augmented Reality (MAR) \cite{bib:arsurvey1} holds immense potential as a transformative technology, with applications in various domains including gaming \cite{bib:pgorest}, education \cite{bib:eduar}, advertisement \cite{bib:adar},  healthcare \cite{bib:healthar}, tourism \cite{bib:healthar}, and manufacturing \cite{bib:manuf}. However, MAR presents unique challenges due to its reliance on data-intensive computations and stringent low-latency requirements. 

    Figure \ref{fig:marpipeline} illustrates a typical MAR pipeline, comprising MAR Device (end-user device) and MAR Tasks (compute-intensive tasks and their associated resources) \cite{bib:marpipeline}. These tasks can be distributed across the user device, edge, and/or cloud based on resource availability. The key stages in this pipeline include (1) Object detection and feature extraction: identifying potential target objects in a camera frame and extracting their unique features; (2) Object recognition and pose estimation: matching extracted features to a database to identify the object and estimate its real-world pose, potentially aided by template matching \cite{bib:tempmatch}; (3) Object tracking: continuously monitoring the identified object across frames to optimize performance \cite{bib:objtrack}; and (4) Image rendering and virtual content overlay: retrieving and overlaying corresponding virtual content onto the real-world object, achieving the core AR functionality. Efficient data management, particularly the storage of virtual object content, is crucial for optimal performance in this pipeline.
    
    % A typical MAR pipeline is shown in Figure \ref{fig:marpipeline}, where the components are grouped into MAR Device and MAR Tasks \cite{bib:marpipeline}. The MAR Device represents the end-user device and MAR Tasks represent the the compute-intensive tasks and the resources used for executing them. MAR tasks could be present on the end-user device, edge and/or cloud depending on the resource availability. There are several key stages in the AR pipeline. textit{Object Detection and Feature Extraction}: The system begins by identifying potential target objects within a camera frame using object detection techniques. For each identified Region of Interest (ROI), feature extraction isolates key characteristics that uniquely define the object. \textit{Object Recognition and Pose Estimation}: Leveraging the extracted features, object recognition performs a database lookup to identify the most likely corresponding object from a known set. Template matching \cite{bib:tempmatch} can further validate the recognition and estimate the object's pose (position and orientation) in the real world. \textit{Object Tracking}: Once recognized, object tracking \cite{bib:objtrack} facilitates continuous monitoring of the object across subsequent frames, reducing the need for repetitive recognition in every frame and optimizing performance. \textit{Image Rendering and Virtual Content Overlay}: Finally, the image rendering stage retrieves the corresponding virtual content (often a 3D model) based on the recognized object's identity. This virtual content is then overlaid onto the real-world object within the camera frame, achieving the core AR functionality of augmenting the user's perception. One critical challenge in this pipeline is data management, particularly the question of where to store virtual object content within the MAR pipeline to ensure optimal performance.

    \begin{figure}[t]
        \centering
        \includegraphics[scale=0.1]{images/intro/TypicalMAR.png}
        \caption{A Typical Mobile Augmented Reality (MAR) Pipeline}
        \label{fig:marpipeline}
        \vspace{-7mm}
    \end{figure}
    
    Traditional approaches to managing virtual object data in MAR, such as on-device storage \cite{bib:ondevice} or cloud offloading \cite{bib:overlay}, face limitations.  Increasingly complex and interactive virtual objects demand significant storage, rendering on-device storage impractical for lightweight mobile devices.  On the other hand, on-demand fetching from the cloud suffers from high latency \cite{bib:clt1, bib:clt2}, hindering smooth real-time AR experience.
    
    % While traditional approaches involve storing the virtual object on mobile devices \cite{bib:ondevice} or offloading the computation onto the cloud \cite{bib:overlay}, these methods fall short in multiple ways. The ever-increasing complexity and interactivity of virtual objects lead to larger file sizes, making mobile device storage impractical in the future trend towards lightweight devices. On-demand fetching from the cloud is also hindered by high mobile-to-cloud latency \cite{bib:clt1, bib:clt2}.

    Edge computing offers an attractive alternative for MAR applications \cite{bib:arsurvey1,bib:arvrcache}. Processing data closer to the user at the network edge reduces latency, enabling real-time interaction and a more immersive experience.  Further, edge computing decreases bandwidth demands by reducing data transmission to the cloud, leading to increased efficiency and cost savings.  Offloading processing to the edge also enhances reliability, allowing MAR applications to function even with intermittent network connectivity. Finally, the ability of edge infrastructure to store virtual objects locally further improves application performance and responsiveness.
   
    
    % MAR applications could significantly benefit from edge computing paradigms \cite{bib:arsurvey1,bib:arvrcache}. By processing data closer to the user at the network edge, latency is dramatically reduced, enabling real-time interaction and a more immersive user experience. Furthermore, edge computing mitigates the bandwidth demands inherent to AR applications by reducing the volume of data transmitted to the cloud. This results in increased efficiency and cost savings. Additionally, offloading processing to the edge enhances the reliability of MAR applications, enabling continued functionality even with intermittent network connectivity.  The inherent capacity of edge infrastructure to store virtual objects locally further contributes to the enhanced performance and responsiveness of MAR applications.

    % While the emergence of 5G and future 6G networks promises to reduce last-mile transfer times, widespread 5G infrastructure deployment remains in its early stages \cite{bib:5glag}.  Assuming future 5G availability, the primary bottleneck in the MAR pipeline shifts to the edge-to-cloud network.  Although edge resources are more abundant than those on mobile devices, they remain limited and heterogeneous. Therefore, efficient strategies are crucial to ensure the availability of necessary virtual objects at the edge for timely augmentation.
    
    % The emergence of 5G (and potentially 6G) networks promises reduced last-mile transfer times. However, widespread deployment of 5G hardware and software infrastructure remains in its early stages \cite{bib:5glag}. Assuming future availability of 5G, the primary bottleneck within the MAR pipeline shifts to the edge-to-cloud network. 

    % While Content Delivery Networks may appear to be a viable alternative for storing virtual objects, their primary function is optimized for content delivery and static storage. In contrast, edge servers offer the capability for virtual object processing and can offload some of the data-intensive computations inherent to AR applications \cite{bib:arsurvey1, bib:offload, bib:nimbus}.. This distinction makes edge servers a more suitable choice for managing and delivering dynamic virtual content in MAR applications.
    
    % Content Delivery Networks (CDNs) might seem like a viable alternative to edge-based storage. However, CDNs are optimized for content delivery and static storage, while edge servers offer the potential for virtual object processing and offloading some of the data-intensive computations associated with AR applications \cite{bib:arsurvey1, bib:offload, bib:gtoffload, bib:nimbus}.

 
    Edge caching is a critical technique for latency-sensitive applications, including AR \cite{bib:cachecompar, bib:cachemec,bib:onlinecoll, bib:arvrcache, bib:artactile}.  To achieve immersive AR experience, caching is necessary across various stages of the MAR processing pipeline.  Some existing approaches, such as CARS \cite{bib:carsar}, Precog \cite{bib:precog} and SEAR \cite{bib:sear}, focus on caching image recognition results at the edge, while others advocate for caching virtual objects directly on user devices \cite{bib:localcache}.  Prefetching AR data to further enhance cache hit rates remains an active research area.  Current prefetching techniques primarily address location-based services (LBS) and video content.  In LBS, prefetching typically involves caching objects near the user's location on either the device or an edge server \cite{bib:dreamstore, bib:locar,bib:advlocar}.  For video content, prefetching leverages user interest to cache video segments on nearby edge servers, often in conjunction with techniques like tile caching \cite{bib:explorevr, bib:cubist, bib:coopec, bib:tile}. Edge caching resources, although more abundant than mobile device storage, are still limited and heterogeneous. Therefore, it is challenging to ensure the right set of virtual objects are readily available for augmentation at the edge.
    
    % Edge caching, as explored in \cite{bib:cachecompar, bib:cachemec,bib:onlinecoll, bib:arvrcache, bib:artactile}, is a crucial technique for latency-sensitive applications. In the context of Augmented Reality (AR), achieving an immersive experience necessitates caching across various stages of the AR processing pipeline. For instance, CARS \cite{bib:carsar}, Precog \cite{bib:precog} and SEAR \cite{bib:sear} focus on storing image recognition results at the edge, while \cite{bib:localcache} advocate for caching virtual objects directly on the user's device. Prefetching AR data to enhance cache hit rates remains an active area of research. Existing works in prefetching primarily address location-based services (LBS) and video content. For LBS, prefetching involves storing all objects near a user's location on either their mobile device or an edge server \cite{bib:dreamstore, bib:locar,bib:advlocar}. In video content scenarios, prefetching leverages user interest to strategically cache video segments on nearby edge servers, enabling faster rendering alongside existing techniques like tile caching \cite{bib:explorevr, bib:cubist, bib:coopec, bib:tile}.


    This paper investigates optimizing edge prefetching in AR applications by analyzing user-object interaction patterns and AR-specific properties. Given the context-dependent nature of AR interactions, we argue that effective prefetching strategies should prioritize objects likely to enter the user's field of view (FoV).  We hypothesize that predictable patterns in user access behavior can be leveraged to anticipate future interactions and prefetch relevant virtual objects at the edge, thereby improving rendering performance and enhancing the overall AR experience. 
    
    Based on these insights, we present an {\em MAR edge cache prefetching framework called \spaarc{}} that exploits two key properties for AR object prefetching: (i) {\em object associations} (how likely multiple objects are to be accessed together by a user), and (ii) {\em spatial proximity} (how close an object is to a user's FoV). Our approach uses association mining to determine object associations, and distance thresholding to identify spatial proximity. It further incorporates factors such as recency of associations and lazy prefetching to ensure efficient utilization of edge resources and network bandwidth. We also present an adaptive tuning algorithm to automatically tune the parameters of these algorithms. 
    We note that \spaarc{} is designed to be complementary to the underlying caching policies, and its prefetching can be used on top of any caching algorithm to further improve the cache performance.
    \if 0
    However, recognizing the resource constraints of edge servers, we emphasize the importance of selective caching. Our approach incorporates factors such as recency of associations, proximity, and a lazy prefetching strategy to ensure efficient utilization of edge resources.
    \fi
    
    % This paper explores the optimization of edge prefetching in Augmented Reality (AR) applications through the analysis of user-object interaction patterns and AR-specific properties. We posit that prefetching strategies should prioritize objects likely to enter the user's Field of View (FoV) in the near future, given the context-dependent nature of AR interactions. Moreover, we hypothesize that predictable patterns in user behavior can be leveraged to anticipate future interactions and to prefetch relevant virtual objects at the edge, thereby improving rendering performance and enhancing the overall AR experience. However, acknowledging the resource constraints of edge servers, we emphasize the importance of selective caching. Our approach incorporates factors such as recency of associations, proximity, and a lazy prefetching strategy to ensure judicious utilization of edge resources.

    %We introduce \spaarc{}, a prefetching policy that leverages object association and user proximity to optimize edge caching strategies for MAR. 
    This paper makes the following research contributions:
    
    \noindent$\bullet$ Analysis of %the necessity for virtual object caching and 
    the rationale for using object association and proximity to achieve efficient MAR edge caching.
    
    \noindent$\bullet$ The design of \spaarc{}: a cache prefetching framework that utilizes virtual object associations and user proximity to improve edge cache performance, along with an adaptive parameter tuning algorithm.
    
    \noindent$\bullet$ A comprehensive evaluation of \spaarc{}, using both synthetic and real-world traces, showing that \spaarc{} significantly improves hit rates (by 3-40\%) compared to various baseline caching algorithms.


    \if 0

    \noindent$\bullet$ Analysis of the necessity for virtual object caching and the rationale for using object association and proximity to achieve efficient caching.
    
    \noindent 2. \spaarc{}: A cache prefetching policy that utilizes virtual object associations and user proximity to improve hit rates.
    
    \noindent 3. Comprehensive evaluation of \spaarc{}, including an analysis of the impact of various tunable parameters.

    \noindent 4. Demonstration, using both synthetic and real-world traces, that \spaarc{} significantly improves hit rates (by 3-40\%) and reduces on-demand misses (by 23-31\%) compared to baseline caching algorithms.
\fi
    % We propose \spaarc{}, a novel prefetching policy that considers object association and user proximity to determine optimal edge caching strategies. Our paper makes the following key contributions:
    % \begin{enumerate}
    %     \item Analysis of the necessity for virtual object caching and the rationale behind using object association and proximity for effective caching.
    %     \item \spaarc{}: a cache prefetching policy that utilizes virtual object associations and user proximity to improve cache hit rates.
    %     \item In-depth evaluation of the proposed prefetching policy, including an analysis of the impact of various tunable parameters.
    %     \item Demonstration using synthetic and real traces that \spaarc{} significantly improves hit rates by 3-40\% and reduces on-demand misses by 29-60\% for the baseline caching algorithms.
    % \end{enumerate}

    % In Section \ref{sec:motive}, we will explore the main motivations for the need of a virtual object prefetching policy and a brief background on the techniques that build up the policy. Section \ref{sec:spaar} introduces the \spaarc{} framework and the mechanics of the prefetching policy. We also cover the different tunable parameters involved in the framework. Section \ref{sec:eval} discusses the experiments conducted to analyze the efficiency of \spaarc{} over existing cache eviction policies. Finally, we conclude the paper with the insights made from the \spaarc{} policy in Section \ref{sec:conclude}.