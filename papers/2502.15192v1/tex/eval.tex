% \vspace{-2.5mm}
\section{Evaluation}
\label{sec:eval}
\subsection{Experimental setup and methodology}
   % In this section, we evaluate \spaarc{} by measuring its benefit over the underlying caching algorithms. We also perform a sensitivity analysis of various \spaarc{} parameters under different workloads and cache configurations. %analyze different tunable parameters of \spaarc{} to understand the sensitivity of the system to different datasets. 
    We use a combination of simulations and real testbed experiments.
    Our experimental testbed utilizes a 64GB Intel Xeon E5-2620 with 24 cores (end-users), a 16GB AWS Local Zone EC2 t3.xlarge (edge server), and a 16GB AWS EC2 t2.xlarge (cloud server).  Evaluation is performed using four synthetic datasets and a real-world dataset \cite{bib:spmf}. 
    
    % In this section, we evaluate the impact \spaarc{} has over cache eviction algorithms. We also analyze different tunable parameters of \spaarc{} to understand the sensitivity of the system to different datasets. The real world experiments are conducted on the 64GB Intel(R) Xeon(R) CPU E5-2620 with 24 cores for the end-users, a 16GB AWS Local Zone EC2 t3.xlarge with 4 vCPUs for the edge server and a 16 GB AWS EC2 t2.xlarge with 4 vCPUs for the cloud server. Four synthetic datasets and a real world dataset\cite{bib:spmf} are used for the evaluation of \spaarc{}. To mimic the real world environment, we designed an AR Workload Simulator, that generates the AR environment, user access patterns and user movement.

    We designed an AR workload simulator that replicates key aspects of user behavior and object placement: (1) Environment: Constructs a geographical region with obstacles using collider data \cite{bib:collider}, mimicking real-world scenarios with varying object placement densities . The distance between objects is within the range of [10, 15] unit distance. (2) User Behavior: Models user movement, gaze direction, and interaction time with virtual objects. (3) User Traffic: Simulates user arrivals using a Poisson process and interaction time with each object using a normal distribution, capturing both regular and peak usage scenarios.
    % \subsection{AR Workload Simulator}
    %     \begin{figure*}
    %     \centering
    %     \includegraphics[scale=0.30]{images/eval/workload.png}
    %     \caption{(a)Sample geographical region for simulation (b) Potential location for real objects (c) Placement of real objects on obstacle boundaries (d) User movement from one entry point to another in the given region}
    %     \label{fig:workload}
    % \end{figure*}
    % \label{ssec:arsim}
    %     The AR workload simulator replicates key aspects of user behavior and object placement within a virtual environment.
    %     \begin{itemize}
    %         \item Geographical Region and Obstacles: The simulator constructs a geographical region populated with obstacles using collider data from \cite{bib:collider} (Figure \ref{fig:workload}.(a)). This environment mimics real-world scenarios (dense grocery stores and sparse tourist destinations) where virtual objects are often associated with physical structures.
    %         \item Object Placement: First, identify potential locations for placing real objects. Then virtual objects are strategically placed on the boundaries of obstacles, reflecting real-world object distribution (Figure \ref{fig:workload}.(b), (c)). The simulator allows for user-defined object density, enabling the evaluation of prefetching under sparse or dense object distributions. In the datasets generated, the distance between two objects is in the range [10, 15] units.
    %         \item User Movement and Behavior: The simulator models user movement and behavior within the geographical region. This includes simulating user locomotion, changes in gaze direction, and time spent interacting with virtual objects of interest (Figure \ref{fig:workload}.(d)).
    %         \item User Arrival and Interaction: User arrival patterns are modeled using a Poisson distribution, while interaction time with each virtual object follows a normal distribution. This allows the simulator to generate realistic user traffic patterns, including both regular and peak usage scenarios.
    %     \end{itemize}
    \begin{table}
            \caption{Datasets}
            \label{tab:datasets}
            \begin{center}
                \begin{tabular}{|l||*{5}{p{1cm}|}}\hline
                
                    Dataset & Type & Random Itemset Support & Number of items & Number of users & Scenario\\
                    \hline\hline
                    DS30  & Synthetic & 30\% & 20-100 & 100 - 1k & Library \\
                    \hline
                    DS45  & Synthetic & 45\% & 20-100 & 100 - 1k & Shopping mall\\
                    \hline
                    DS60  & Synthetic & 60\% & 20-100 & 100 - 1k & Zoo/
                    aquarium\\
                    \hline
                    DS75 & Synthetic & 75\% & 20-100 & 100 - 1k & Gallery guided tour\\
                    \hline
                    Grocery & Real & Varying & 169 & 9k & Grocery store\\
                    \hline
                \end{tabular}
            \end{center}
            % \vspace{-6mm}
        \end{table}

    \begin{figure*}
            \centering
            \includegraphics[scale=0.5]{images/eval/best_hr.png}
            \caption{Hit rates across datasets. The top hit rates achieved by \spaarc{} compared to baselines.}
            \label{fig:besthr}
            % \vspace{-5mm}
        \end{figure*}
    \subsection{Workloads}
        We utilize both synthetic and real world datasets (Table \ref{tab:datasets}) for our experiments.
        
        
        \subsubsection{Synthetic Workloads}
            Existing AR workload datasets often focus on a single user's perspective, limiting their applicability for evaluating caching strategies that consider interactions from multiple users. To address this gap, we leverage the AR Workload Simulator to generate a suite of multi-user workloads. To generate our workloads, we first defined the percentage of unique items that would form these frequent itemsets. Subsequently, we created transactions where the support for these pre-defined itemsets was fixed. It is important to distinguish this ``support" from the minimum support parameter used in frequent itemset generation algorithms. Here, support is solely used for dataset creation, and the resulting dataset might contain additional frequent itemsets beyond the pre-defined ones. We utilize four synthetic datasets (DS30, DS45, DS60, DS75) to evaluate the effectiveness of prefetching under diverse user interaction patterns. The support level for randomly chosen itemsets is systematically varied across the datasets (Table \ref{tab:datasets}). By adjusting the support level, we can assess caching performance under different user access patterns reflected by the frequency of item co-occurrence. Each dataset corresponds to a different real-world MAR scenario, as shown in the table. Depending on the type of experiment we vary the number of users, and the number of items for each dataset. Unless otherwise specified, the default configuration utilizes 100 users, 50 objects and a cache size of 20\% of total size of 50 objects. The size of virtual objects are in the range [10, 15] MB \cite{bib:carsar, bib:objaverse}.

        \subsubsection{Real Workload}
            SPMF \cite{bib:spmf} consists of multiple real world workloads with varying transaction size and item counts. We mainly focus on the grocery dataset for our experiments. It consists of 169 items and 9k transactions.
    % \vspace{-6mm}
    
    \subsection{Baseline Caching Algorithms}
        Since \spaarc{} is complementary to the underlying cache algorithm, we consider four native cache eviction policies in our evaluation: FIFO, LRU, LFU and Popularity (POP). Comparison is carried out {\em with and without} \spaarc{} integration with the cache. %Then we run sensitivity analysis across \spaarc{}-integrated cache policies. We also have a overhead comparison to traditional association based prefetching to \spaarc{}. 
        Unless specified, we are not tuning \spaarc{} in the experiments in this section. The results of auto-tuning algorithm are presented separately.

    \subsection{Benefit of \spaarc{} integration}
        \begin{table}
            \caption{Percentage hit rate improvement of \spaarc{} over baseline}
            \label{tab:percenthr}
            \begin{center}
                \begin{tabular}{|l||*{4}{c|}}\hline
                    \diagbox[height=2\line]{Dataset}{Cache}& LRU & LFU & FIFO & POP\\
                    \hline\hline
                    DS30  & 31.15 & 24.55 & 40.6 & 32.01\\
                    \hline
                    DS45  & 20.33 & 7.64 & 26.18 & 25.05\\
                    \hline
                    DS60  & 24.44 & 12.28 & 27.56 & 32.4\\
                    \hline
                    DS75 & 12.87 & 3.27 & 22.44 & 9.65\\
                    \hline
                \end{tabular}
            \end{center}
            % \vspace{-6mm}
        \end{table}
        
        This experiment evaluates \spaarc's effectiveness when integrated with cache eviction algorithms (baselines).  We varied both minimum support and minimum confidence from 30\% to 75\% while keeping other parameters constant (association factor at 1, and proximity threshold at 15). As shown in Figure \ref{fig:besthr} and Table \ref{tab:percenthr}, \spaarc{} significantly improves hit rates compared to baselines by 3.27\% to 40.6\% across all datasets.

        \spaarc{} demonstrates the highest relative improvement over the FIFO baseline, particularly with the DS30 dataset. This highlights \spaarc's ability to mitigate FIFO's limitation of potentially evicting frequently accessed items by proactively caching those likely to be accessed in the near future. Similar improvements are observed for LRU and POP. However, LFU exhibits lower hit rates due to its tendency to evict recently added, associated items.
        
        Furthermore, hit rates generally increase from DS30 to DS75, indicating that both baseline algorithms and \spaarc{} benefit from a greater number of relevant associations. Subsequent results provide a detailed analysis of each parameter's impact within specific configurations\footnote{For space reasons, we present results with a subset of the datasets and baseline algorithms. The trends hold for the omitted results.}.
        
        % This experiment evaluates the effectiveness of \spaarc{} when it is integrated with cache eviction algorithms. We will be refering to cache eviction algorithm without \spaarc{} as baselines from hereon. The evaluation considers various settings for minimum support, minimum confidence, association factor, and proximity threshold. As illustrated in Figure \ref{fig:besthr} and Table \ref{tab:percenthr}, \spaarc{} achieves significant improvements in hit rates compared to the baselines, ranging from 3.27\% to 40.6\% across all datasets.

        % Further analysis reveals that the most effective configuration for \spaarc{} across all cache eviction policies utilizes an association factor of 1 and a proximity threshold of 15 (details presented later). Notably, the values for minimum support and minimum confidence depend on the specific cache eviction policy. Due to this variability, these values are not reported here. However, subsequent sections present a detailed analysis of each parameter's impact within a specific configuration, providing insights into the sensitivity of cache eviction policies when using \spaarc{}.

        % It could be seen from Figure \ref{fig:besthr} and Table \ref{tab:percenthr}, that \spaarc{} provided the highest relative improvement over FIFO caching baseline in hit rate with DS30. FIFO evicts items in the order of caching. Hence, even if an item is being accessed frequently by multiple users, it will end up getting evicted for a less popular item. \spaarc{} is able to mitigate this issue as the object that will be accessed by users in the immediate future is brought into the cache by using association and proximity. The same effect could be observed for LRU and POP. However, for LFU, which keeps track of the frequency of items to decide whether an items should be cached, the newly cached associated items will be the ones to be evicted first, resulting in higher misses, which is observable from its low hit rates.

        % Another observation is the increase in the value of hit rates. From DS30 to DS75, the hit rate improves as there are more relevant associations. This is applicable to both baselines and \spaarc{}.

    \subsection{Impact of minimum support}
        This experiment investigates the impact of minimum support on \spaarc's performance. We varied the minimum support threshold from 30\% to 75\% while keeping other parameters constant (minimum confidence at 45\%, association factor at 1, and proximity threshold at 15). Figure \ref{fig:msup} shows the hit rate variation for DS30 and DS75, with similar trends observed for DS45 and DS60. This behavior is consistent across different minimum confidence values. As a critical parameter in association rule mining, higher minimum support thresholds result in fewer frequent itemsets and consequently, fewer association rules, potentially leading to lower hit rates.

        \begin{figure}[h]
            \centering
            \includegraphics[scale=0.5]{images/eval/msup.png}
            \caption{Varying minimum support at fixed minimum confidence of 45\%. }
            \label{fig:msup}
            % \vspace{-2mm}
        \end{figure}
            
    \subsection{Impact of minimum confidence}
        This experiment analyzes the influence of minimum confidence on \spaarc's performance. We varied the minimum support threshold from 30\% to 75\% while keeping other parameters constant (minimum confidence at 45\%, association factor at 1, and proximity threshold at 15). Figure \ref{fig:mconf} shows this effect on DS30 and DS75, with similar trends observed for DS45 and DS60 across different minimum support values. As a key parameter in ARM, increasing the minimum confidence threshold initially improves or maintains hit rates before causing a decline. This trend reflects the trade-off between rule quality and quantity. Higher thresholds reduce the number of rules, improving quality but potentially excluding relevant associations. 
        
        \begin{figure}[h]
            \centering
            \includegraphics[scale=0.5]{images/eval/mconf.png}
            \caption{Varying minimum confidence at fixed minimum support of 30\%.}
            \label{fig:mconf}
            % \vspace{-2mm}
        \end{figure}

    \subsection{Impact of Association factor threshold}
        This experiment examines the impact of the association factor on \spaarc's cache hit rates. With minimum support and minimum confidence fixed at 30\% and 45\% respectively, the association factor was varied from 0.1 to 4. Figure \ref{fig:afactor} illustrates the results. As dataset support increases (i.e., associated objects are accessed more frequently), the influence of the association factor on identifying relevant objects diminishes.  For DS30, the highest hit rate is achieved with an association factor of 0.1, demonstrating a clear impact. However, for DS75, where frequent itemsets have higher support, the association factor has a less pronounced effect. This suggests that the association factor plays a more critical role in scenarios with lower object access frequencies.
        \begin{figure}[h]
            \centering
            \includegraphics[scale=0.4]{images/eval/afactor.png}
            \caption{Varying association factor at a fixed minimum support of 30\% and minimum confidence of 45\%.}
            \label{fig:afactor}
            % \vspace{-4mm}
        \end{figure}

    \subsection{Impact of Proximity with Association}
            Figure \ref{fig:proximity} illustrates the influence of the proximity threshold on cache hit rates for \spaarc{}-integrated baselines. We varied the proximity threshold from 5 to 20 unit distance while fixing the minimum support at 30\%, minimum confidence at 45\%, and association factor threshold at 1. The proximity threshold has a significant impact on hit rates. A very low threshold might neglect relevant objects located slightly further away, potentially missing prefetching opportunities. Conversely, an excessively high threshold could lead to prefetching irrelevant objects that are not close enough for immediate user interaction, wasting cache resources. Therefore, selecting an appropriate proximity threshold is crucial for optimizing \spaarc{}'s effectiveness. Similar results are observed for DS45 and DS60.
                \begin{figure}[h]
                    \centering
                    \includegraphics[scale=0.4]{images/eval/proximity.png}
                    \caption{Varying proximity threshold at a fixed minimum support of 30\% and minimum confidence of 45\%}
                    \label{fig:proximity}
                    % \vspace{-3mm}
                \end{figure}
    %\subsection{Overhead of \spaarc{}}
    
        Figure \ref{fig:overhead_fetches} %This experiment analyzes the 
        shows the relative performance and overhead of using association only  and association+proximity (\spaarc) on top of the baseline caching algorithm, using the DS30 dataset with fixed parameters.
        The left graph shows that the hit rate improvement associated with association and \spaarc{} are 3- 20\% and 8- 36\% respectively. 
        The right graph measures the overhead of using \spaarc{} by comparing on-demand fetches and prefetches across the baseline, baseline with association-based prefetching, and baselines with \spaarc{}. 
        %, results (Figure \ref{fig:overhead_fetches}) 
        It shows that both association-based prefetching and \spaarc{} reduce on-demand fetches compared to the baseline by 2-17\% and 23-31\% respectively, as prefetching caches relevant objects. However, this introduces a prefetch overhead. Notably, association-based prefetching alone has a significantly higher overhead (0.51-2.12X) than \spaarc{} (0.12-0.70X) due to its lack of irrelevant object filtering. 
        Overall, the reduced on-demand fetches lead to improved hit rate and lower latency. While the prefetching occurs in the background without degrading user experience, it still leads to additional bandwidth consumption %necessitates optimization, 
        which we plan to address through parameter tuning and encoding in future work.

            % In this experiment, we delve deeper to examine the overhead associated with using the \spaarc{} policy. We compare the number of on-demand fetches and prefetches across baselines, baselines with just association and baselines with \spaarc{}. We utilize the DS30 dataset with minimum support and minimum confidence fixed at 30\%. The association factor is set to 1 and proximity threshold to 15 as needed. The hit rate improvement associated with association and \spaarc{} are 3- 20\% and 8- 36\% respectively (figure not shown).
    
            % \begin{figure}
            %     \centering
            %     \includegraphics[scale=0.5]{images/eval/overhead_hr.png}
            %     \caption{Hit rate for DS30 dataset with minimum support and confidence to 30\%}
            %     \label{fig:overhead_hr}
            % \end{figure}
            % Figure \ref{fig:overhead_fetches} illustrates the total number of fetches (on-demand fetches triggered by user interaction and prefetches initiated by \spaarc{}). Both association-based prefetching and \spaarc{} reduces the number of on-demand fetches compared to the baseline. This is because prefetching caches relevant objects, minimizing the need for real-time fetching during user interactions. However, there is an additional prefetch overhead that is associated. Association alone prefetching has a very high overhead (0.51-2.12 X) compared to \spaarc{} (0.12-0.70X) as it is not filtering out the irrelevant objects. The prefetching of objects happens in the background, so services are not interrupted. Hence we would see the on-demand trend for end-to-end latency. There is a need to reduce the extra bandwidth consumption in \spaarc{} which is one of the requirements for an MAR applications. We will be working towards optimizing this overhead via parameter tuning and encoding in a future work.
    
                \begin{figure}[h]
                    \centering
                    \includegraphics[scale=0.45]{images/eval/overhead.png}
                    \caption{Hit rate and fetch comparison across baseline, association-only prefetch and \spaarc{}. %It could be seen that \spaarc{} achieves high hit rate at the expense of communication overhead.}
                    }
                    \label{fig:overhead_fetches}
                    % \vspace{-2mm}
                \end{figure}

    \subsection{Number of Users}
        This experiment evaluates the impact of user count on \spaarc's performance by varying the number of users from 100 to 1000 for FIFO cache policy. Figure \ref{fig:vusers_vitems}(a) shows the best hit rates achieved for DS30 across all minimum support and minimum confidence values.  \spaarc{}-FIFO improves hit rates by 5-40\% when compared to FIFO. Importantly, \spaarc{} maintains consistent hit rate improvements across the varying user counts in each dataset. This suggests that \spaarc's performance is robust to changes in user count.
        \begin{figure}[h]
            \centering
            \includegraphics[scale=0.5]{images/eval/vusers_items.png}
            \caption{Varying number of users and items. %(a) \spaarc{} maintains a constant hit rate across users suggesting robustness to user count variation (b) Increasing item count decreases hit rate improvement as object placements are closer with chances of relevant objects in view increasing}
            }
            \label{fig:vusers_vitems}
            % \vspace{-5mm}
        \end{figure}

    \subsection{Number of Unique Objects}
        This experiment examines the impact of virtual object count on \spaarc's effectiveness, a crucial factor varying significantly across application domains.  We varied the number of virtual objects from 20 to 100 for DS30 workload for 500 users on FIFO. Figure \ref{fig:vusers_vitems}(b) presents the best achievable hit rates for \spaarc{}-FIFO compared to FIFO.

        Results show a decreasing trend in hit rate improvement as object count increases within the same region. For instance, it is 96\% for 20 objects, 40\% for 50 objects, and 15\% for 100 objects.  Similar trends are observed in other datasets and cache policies. This can be attributed to increased object density, leading to more objects falling within the user's field of view (FoV).  Consequently, baseline caching policies are more likely to cache relevant objects by chance, reducing the relative advantage of \spaarc{}'s targeted prefetching strategy.
            
    
    % \textbf{\textit{Varying Cache Size}}:
    %     This experiment examines the influence of cache size on the effectiveness of \spaarc{}. Figure \ref{fig:vcache} presents the results, with the left side showing the baseline hit rates and the right side showcasing the hit rates achieved by \spaarc{}-integrated baselines. The cache size is varied from 20\% to 60\% of the total size of all unique virtual objects across all four datasets. It's important to note that the \spaarc{} results (right side) correspond to the best hit rates obtained by tuning minimum support, minimum confidence with association factor set 1 and proximity threshold set to 15.

    %     As expected, the hit rate increases for both baselines and \spaarc{}-integrated baselines with a larger cache size. However, \spaarc{} demonstrates a more significant improvement in hit rates, particularly for smaller cache sizes. This is because \spaarc{}'s ability to prefetch relevant objects that are likely to be accessed in the near future becomes especially valuable when cache space is limited. By strategically prefetching these objects, \spaarc{} mitigates cache misses that would otherwise occur. For cache size of 20\%, the improvement is in the range 7\% - 40\%, for 40\% it is atmost 5\% and for 60\% it is atmost 1.2\%.
    %         \begin{figure}[h]
    %             \centering
    %             \includegraphics[scale=0.4]{images/eval/vcache.png}
    %             \caption{Varying cache size across different workloads. \spaarc{} is able to provide higher hit rates with increasing cache size. The improvement is evident in lower cache sizes.}
    %             \label{fig:vcache}
    %         \end{figure}

    \subsection{Minimum support tuning}
        In this experiment, we focus on tuning minimum support value with a fixed minimum confidence of 10\%, cache capacity of 10\%, hit rate degradation threshold of 5\%, initial transaction count for generating association rules of 100 and DS30 workload of 1000 users. Figure \ref{fig:tuningsynth} shows the hit rate associated with FIFO baseline, FIFO with \spaarc{} (no tuning) and FIFO with tunable \spaarc{}. It can be seen than dynamic tuning of \spaarc{} leads to higher hit rates. %The average hit rates of FIFO, \spaarc{}and \spaarc{}-Tune are 36\%, 40\% and 44\%. 
        \spaarc{}-Tune achieves 10\% and 22\% better hit rate compared to \spaarc{} and FIFO baseline. The hit rates are recorded every 10 views (1 viewpoint) and checked for degradation. If it is above a threshold, the association rule sets are generated. During the generation of association rules, incoming requests are served in parallel. Once the association rule set is selected, it is used for identify associated objects.
        
        \begin{figure}[t]
            \centering
            \includegraphics[scale=0.4]{images/eval/spaarctunesynth.png}
            \caption{Minimum support tuning on synthetic workload. %Tuned \spaarc{} is able to perform 10\% and 22\% better than \spaarc{} and FIFO.}
            }
            \label{fig:tuningsynth}
            % \vspace{-4mm}
        \end{figure}
    
    \subsection{Real World experiments}
        %In this section, we will focus on one cache eviction algorithms, namely FIFO, with \spaarc{} and the effect dynamic tuning has over the hit rate performance. 
        We use a real world dataset from SPMF \cite{bib:spmf} that consist of 169 items and 1000 transactions. We use Amazon Local zone for the edge server and AWS EC2 for the cloud server. The cache capacity is set to 10\%, hit rate degradation to 5\% and initial transaction count to 100 for rule generation. Figure \ref{fig:tuningreal} shows the hit rate performance. The hit rates are recorded every 100 views (1 viewpoint) and checked for degradation. It could be seen that \spaarc{} with dynamic tuning is able to perform better than \spaarc{} and FIFO as time passes by. Initially, it takes time to warm-up and then the hit rate starts increasing. %The average hit rates of FIFO, \spaarc{} and \spaarc{}-Tune are 37\%, 39\% and 41\%. 
        \spaarc{}-Tune achieves 6\% and 11\% better hit rate compared to \spaarc{} and FIFO baseline.
        % \vspace{-4mm}
        \begin{figure}[h]
            \centering
            \includegraphics[scale=0.4]{images/eval/spaarctunereal.png}
            \caption{Minimum support tuning on real workload. \spaarc{} is able to perform 6\% and 11\% better compared to \spaarc{} and FIFO.}
            \label{fig:tuningreal}
            % \vspace{-4mm}
        \end{figure}