\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{authblk}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{wrapfig}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{placeins}
\usepackage{float}
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{cleveref}


%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{COAST: Intelligent Time-Adaptive Neural Operators}
% \fancyhead[RE]{Firstauthor and Secondauthor} 
% Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{COAST: Intelligent Time-Adaptive Neural Operators
%%%% Cite as
%%%% Update your official citation here when published 
%\thanks{\textit{\underline{Citation}}: 
%\textbf{Authors. Title. Pages.... DOI:000000/11111.}} 
}

\author{
    \textbf{Zhikai Wu}$^{\mathrm{1}}$
    \hspace{0.28em}\textbf{Shiyang Zhang}$^2$
    \hspace{0.28em}\textbf{Sizhuang He}$^2$
    \hspace{0.28em}\textbf{Sifan Wang}$^2$
    \hspace{0.28em}\textbf{Min Zhu}$^2$
    \hspace{0.28em}\textbf{Anran Jiao}$^2$
    \hspace{0.28em}\textbf{Lu Lu}$^2$$^*$
    \hspace{0.28em}\textbf{David van Dijk}$^2$$^*$
}




\begin{document}
\maketitle

\footnotetext[1]{
Peking University \hspace{0.28em}$^2$Yale University \hspace{0.28em}
$^*$Correspondence to: Lu Lu <lu.lu@yale.edu>,
David van Dijk <david.vandijk@yale.edu>.
}

\begin{abstract}
We introduce Causal Operator with Adaptive Solver Transformer (COAST), a novel neural operator learning method that leverages a causal language model (CLM) framework to dynamically adapt time steps. 
Our method predicts both the evolution of a system and its optimal time step, intelligently balancing computational efficiency and accuracy. 
We find that COAST generates variable step sizes that correlate with the underlying system intrinsicities, both within and across dynamical systems. 
Within a single trajectory, smaller steps are taken in regions of high complexity, while larger steps are employed in simpler regions. 
Across different systems, more complex dynamics receive more granular time steps. 
Benchmarked on diverse systems with varied dynamics, COAST consistently outperforms state-of-the-art methods, achieving superior performance in both efficiency and accuracy. 
This work underscores the potential of CLM-based intelligent adaptive solvers for scalable operator learning of dynamical systems.
\end{abstract}
% keywords can be removed
\keywords{Scientific Machine Learning\and Operator Learning\and Partial Differential Equations\and Adaptive Solver\and Large Language Model}

\section{Introduction}

% PDE solving, mention adaptive numerical solvers

Partial differential equations (PDEs) are fundamental to modeling complex 
physical phenomena, from fluid dynamics to quantum mechanics \citep{evans1998partial}. 
While traditional numerical methods such as finite differences and finite elements 
have been the cornerstone of scientific computing \citep{leveque2007finite}, 
they often struggle with modern challenges involving multi-scale dynamics 
and complex geometries. Adaptive solvers have emerged as a powerful solution, 
dynamically adjusting spatial and temporal discretization based on local error 
estimates \citep{rannacher2003adaptive}. These methods significantly improve 
computational efficiency by concentrating computational resources where needed most. 
However, they face inherent limitations: high computational overhead for frequent 
refinement, careful parameter tuning requirements, and challenges with stiff problems 
\citep{hairer1993solving}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/figure1_v2.pdf}
    \caption{\textbf{Overview of the COAST framework for continuous operator learning.} 
    COAST is an intelligent, time-adaptive neural operator that predicts both the system's 
    future evolution and the optimal time step. It takes historical frames \((u_{j-2}, u_{j-1}, u_j)\) 
    as input and produces a next-step prediction \(u'_t\). Intermediate frames \((u_{j+1}, u_{j+2})\) 
    are then interpolated so the model can be evaluated at the same time points as the ground-truth 
    data \((GT_{j+1}, GT_{j+2})\). By adaptively determining time steps, COAST provides 
    a continuous-time modeling capability—balancing accuracy with efficiency by reducing the 
    number of function evaluations needed to achieve high-fidelity results.}
    \label{fig:coast-approach}
\end{figure*}


% Operator Learning

Recent advances in machine learning, particularly operator learning frameworks, 
have opened new avenues for PDE solving \citep{li2020fourier}. Instead of traditional 
numerical approximations, these approaches learn mappings between function spaces, 
enabling rapid solution prediction for entire families of PDEs. Several architectures 
have demonstrated remarkable success: Fourier Neural Operators (FNO) leverage the 
spectral domain for efficient learning \citep{li2020fourier}, DeepONet employs the 
universal approximation theorem for operators \citep{lu2021learning}, and 
transformer-based models like Oformer \citep{li2022transformer}, DPOT \citep{hao2024dpot}, 
 CViT \citep{wang2024bridging}, Transolver \citep{wu2024transolver} 
adapt attention mechanisms for PDE solving. These methods have achieved impressive 
results across various applications, from fluid dynamics \citep{azizzadenesheli2024neural}, 
solid mechanics \citep{wang2024micrometer}, to heat transfer \citep{roy2024deep}, 
often leading to reasonable accuracy and great computational efficiency.



Despite these advances, current machine learning approaches for PDE solving face a significant limitation: they typically operate with fixed time steps. This constraint bec
omes particularly problematic when dealing with problems exhibiting multiple time scales or rapid temporal variations. To address this limitation, we design Causal Operator with Adaptive
Solver Transformer (COAST) as shown in \cref{fig:coast-approach}, which is motivated by adaptive numerical solvers and can dynamically adjust their temporal resolution based on solution dynamics. Our contribution is summarized as below:
\begin{itemize}[leftmargin=*]
    \item \textbf{Causal Operator with Adaptive Solver Transformer (COAST). } We introduce COAST, a neural operator that self-determines the solution time step sizes. COAST can autonomously decide the prediction step and solve efficiently in continuous time using interpolation methods.
    \item \textbf{State-of-the-art Performance. } We propose a method for training and evaluating time-adaptive solvers based on interpolation. Using this method, COAST outperforms state-of-the-art methods on diverse challenging benchmarks. 
    \item \textbf{Intelligence demonstrated by COAST in solving physics systems. } COAST predicts different step size distributions when confronted with different systems and states, which can be corroborated with some intrinsic properties in dynamical systems. This reflects the intelligence that causal language models exhibit in solving physical systems.
    
\end{itemize}




\section{Method}
COAST is a novel neural operator architecture in the core of a causal language model (CLM). COAST gives a time-adaptive and temporally continuous approach to solve time-dependent physical systems by leveraging a trainable spatiotemporal encoding for time-sequential spatial inputs on continuous time points, a causal language model to output spatially-temporally coupled embedding of the next predicted step, and an interpret-modify mechanism to interpret the next solving time step from the coupled embedding and modify the embedding using the interpreted time step to obtain the spatial embedding of the predicted solution.

\subsection{Architecture Description}
The COAST architecture consists of four main components: a spatio-temporal encoder, a causal language model, an interpret-modify mechanism, and an interpolation decoder. 

\paragraph{Spatio-temporal Embeddings}  The COAST encoder takes as input a gridded representation of the input states $\textbf{u}$, yielding a spatio-temporal data tensor $ \textbf{u} \in \mathbb{R}^{T\times H\times W\times D}$ with $D$ channels, and a time series $\textbf{T}_{seq}$ made up of $T$ float numbers standing for the relative times of each of the 2D spatial frames $\textbf{S}_i \in \mathbb{R}^{H\times W\times C}$.
\footnote{
When the input frames are all on fixed time points, the $\textbf{T}_{seq}$ can be omitted because now the $\textbf{T}_{seq}$ can be computed mechanically by the model itself.} 
We patchify our input into $\textbf{u}_p \in \mathbb{R}^{T\times \frac{H}{P}\times \frac{W}{P}\times C}$ by tokenizing each $\textbf{S}_i$ independently using CNN with overlapped kernels. We then add trainable 2D spatial positional embeddings to each $\textbf{S}_i$ and use a common FiLM layer to embed the time series $\textbf{T}_{seq}$ into each $\textbf{S}_i$ and then we get $\textbf{u}_{st} \in \mathbb{R}^{T\times \frac{H}{P}\times \frac{W}{P}\times C}$. 
For convenience in the next step, we transpose $\textbf{u}_{st}$ to $\textbf{u}_{st}' \in \mathbb{R}^{(\frac{H}{P}\times \frac{W}{P})\times T\times C}$.
$$
%\begin{equation}
\textbf{u}_s = \textbf{u}_p + \textbf{PE}_s,\quad \textbf{PE}_s \in \mathbb{R}^{1\times \frac{H}{P}\times \frac{W}{P}\times C}
%\end{equation}
$$
$$
%\begin{equation}
\textbf{u}_{st}' = \mathrm{transpose}(\textbf{FiLM}(\textbf{T}_{seq}, \textbf{u}_s))
%\end{equation}
$$

\paragraph{Causal Language Model Predictor}  We use a causal language model(CLM), specified as GPT-2, as the core of our model. The channel number $C$ of $\textbf{u}_{st}'$ should be equal to the embedding dimension of the CLM. Then the input embedding $\textbf{u}_{st}'$ can be considered as $(\frac{H}{P}\times\frac{W}{P})$ sentences made up of $T$ tokens. We input the $\textbf{u}_{st}'$, where batch size is $(\frac{H}{P}\times\frac{W}{P})$, into the first hidden states of the CLM. For each sentence, we take the last token of its last hidden states as output. We assert that the output tokens $\textbf{z} \in \mathbb{R}^{(\frac{H}{P}\times\frac{W}{P})\times 1 \times C}$ carries information on both the predicted spatial solution and the predicted proper time-step size.

\paragraph{Interpret-Modify Mechanism}  To interpret the predicted time step size from $\textbf{z}$, we apply an MLP model, named interprator, on each token $\textbf{z}_i$ in $\textbf{z}$ independently to get $(\frac{H}{P}\times \frac{W}{P})$ time step sizes. Given that each output token is actually a spatial patch of the total space and they should have the same status in terms of time, one interprator is enough to be applied on all tokens. We take the average of the $(\frac{H}{P}\times \frac{W}{P})$ time step sizes as the predicted time step size $dt_{\mathrm{orig}}$. 
$$
dt_{\mathrm{orig}} = \mathrm{mean}(\textbf{MLP}(\textbf{z}_i))
$$
In many cases, $dt_{\mathrm{orig}}$ is likely to converge to $0$ during training in order to make it easier to predict the spatial solution more correctly. But too small $dt_{\mathrm{orig}}$ will impede the extension of the rollout and make our adaptive time-stepping meaningless. In addition, sometimes too large $dt_{\mathrm{orig}}$ can also push the model's training and prediction in a direction we don't expect. Therefore, we set a lower bound and a upper bound on $dt_{\mathrm{orig}}$ to get $dt$ and add a term\footnote{We choose to use a piecewise power-exponential function of $dt$ as $\textbf{Loss}_{dt}$. The details are provided in \cref{loss_dt}.} to the loss function to penalize comparatively small $dt$. 
$$
dt = \mathrm{clip}(dt_{\mathrm{orig}}, LB, UB)
$$
$$
\textbf{Loss} = \textbf{Loss}_{\mathrm{spatial}} + \textbf{Loss}_{dt}
$$
To get the final predicted spatial embedding, we use $dt$ to modify the output tokens $\textbf{z}$. We apply another FiLM layer on each token of $\textbf{z}$ and rearrange the first dimension of z in two dimensions to get $\textbf{z}_s \in \mathbb{R}^{\frac{H}{P}\times\frac{H}{P}\times 1\times C}$
$$\textbf{z}_s = \textbf{FiLM}(dt, \textbf{z})$$

\paragraph{Interpolation Decoder} To reconstruct the output spatial state $\textbf{u}' \in \mathbb{R}^{H\times W\times D}$ from $\textbf{z}_s$, we learn a transposed-convolution model with overlapped kernels just mirror-symmetric with the encoder's structure.

Finally, if we want to get a solution state $\textbf{u}_s$ on any continuous time point $t = t_s$ between [0, $dt$], just use $t_s$ to perform linear interpolation between the last input state $\textbf{u}_{-1}$, where $t = 0$, and $\textbf{u}'$, where $t = dt$.

If the target time point $t_s$ is larger than $dt$, just forward the model over and over again until the sum of time step sizes is over $t_s$.


% \subsection{Theoretical Insight}
\iffalse
CLM as Predictor
Interpret-Modify Mechanism
\fi



%\subsection{COAST vs. other Approaches}
%\textbf{COAST vs. other Transformer-based Approaches. }

\subsection{COAST vs. Traditional Adaptive Solvers}

Traditional adaptive solvers have been widely used in numerical computation to address challenges in solving differential equations, optimization problems, and other complex models. Key methodologies include \textbf{Adaptive Mesh Refinement (AMR)} \cite{BERGER1984484, BERGER198964}, which dynamically refines grids for PDEs with localized features like shocks, and \textbf{adaptive time-stepping} \cite{hairer1993solving}, which adjusts step sizes in ODE solvers based on error estimates. Error estimation techniques, such as those by \cite{babuska1978posteriori}, have been critical in guiding adaptive refinement in finite element methods, while adaptive quadrature methods \cite{forsythe1977computer} optimize numerical integration by adjusting sampling points. In optimization, adaptive strategies like the Nelder-Mead method \cite{nelder1965simplex} dynamically modify search parameters to improve convergence. Despite their success, traditional adaptive solvers face challenges such as computational overhead and scalability, which have spurred recent efforts to integrate them with machine learning techniques. For instance, physics-informed neural networks \cite{raissi2019physics} and data-driven sparse identification \cite{brunton2016discovering} have shown promise in enhancing adaptive solvers by leveraging data to predict optimal refinement strategies or model dynamics. This work builds on these foundations, exploring the synergy between classical adaptive methods and modern data-driven approaches to overcome existing limitations.







\section{Experiments}

We compare COAST against popular neural operators on four challenging benchmarks in physical sciences. In addition to demonstrating COAST's performance against strong baselines, we analyze its loss accumulation behavior and its efficiency in terms of inference time. Furthermore, we investigate the relationship between the adaptive time-stepping behavior and the underlying physical intrinsicities, which lie in different temporal regions and varying system parameters. Finally, we perform ablation studies to evaluate the scalability of COAST in terms of adaptivity and accuracy.

\paragraph{COAST model setup}  For all experiments, unless otherwise stated, we use a patch size of $32\times32$ for tokenizing inputs and an embedding dimension of $768$ for CLM predictor. 
We use a piecewise quadratic-type function as $\textbf{Loss}_{dt}$ to impose a penalty on $dt$.

\paragraph{Baselines}  
We benckmark our framework against operatoer learning framework Fourier Neural Operator \citep{li2021fourierneuraloperatorparametric} and DPOT \citep{hao2024dpotautoregressivedenoisingoperator} and vision models Dilated ResNet \citep{stachenfeld2022learnedcoarsemodelsefficient}, CNextU-Net \citep{liu2022convnet2020s}and AViT \citep{du2024avitadaptingvisiontransformers}. For all baselines, we adhere to the training and evaluation protocols outlined in the The Well datasets \cite{ohana2024thewell} or follow the configurations recommended in the original papers. Detailed implementation settings for the baseline models are provided in \cref{model-details}.

\paragraph{Evaluation method for adaptive solvers} We can find that the existing datasets and the operator learning models are all on fixed time points. But for COAST, as an adaptive solver, we can query an arbitrary continuous future time $t_s$ as the target time point where the expected solution is located. In order to make a more reasonable comparison with other baselines, we propose a method to evaluate the accuracy of adaptive solvers by obtaining the longest fixed-step sequence after an adaptive prediction. 

We now set the time interval between two temporally-neighboring $2D$ frames in the original input data $\textbf{u}_{in} \in \mathbb{R}^{T\times H\times W\times D}$ to be the unit time $1$, and the time of the last $2D$ frame $\textbf{u}_{-1}$ is set to be $0$ as a starting time point. We use an interpolation method to obtain the output solution $\textbf{u}_{pred}$ of COAST on fixed time points. When we get $dt$ from COAST, let $\textbf{T}' = \lfloor dt \rfloor$. Then for each $t_s$ from $1$ to $\textbf{T}'$, perform linear interpolation between the last input state $\textbf{u}_{-1}$, where $t = 0$, and $\textbf{u}'$, where $t = dt$.
Note that now the lower bound for $dt$ can be set slightly larger than $1$ to ensure that we can obtain at least one solution on the $t=1$ time point. Details are shown in \cref{alg:OneStepPrediction}. 

\begin{wrapfigure}[12]{r}{0.45\textwidth} 
\vspace{-0.928em}
\begin{minipage}{0.45\textwidth}
\vspace{-0.928em}
\begin{algorithm}[H]
   \caption{One-step prediction}
   \label{alg:OneStepPrediction}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $\textbf{u}_{in}$
   %\REPEAT
   %\STATE Initialize $noChange = true$.
   \STATE $\textbf{u}', dt = \textbf{COAST}(\textbf{u}_{in})$
   \STATE $ T' = \lfloor dt \rfloor $
   \STATE Initialize empty sequence  $\textbf{u}_{pred}$ 
   \FOR{$t_s=1$ {\bfseries to} $T'$}

   \STATE $\textbf{u}_{new} = \mathrm{Interpolate}(t_s, (\textbf{u}_{-1},0), (\textbf{u}',dt))$
   \STATE Add $\textbf{u}_{new}$ to $\textbf{u}_{pred}$
   \ENDFOR
   \STATE {\bfseries Output:} $\textbf{u}_{pred}$
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}



\textbf{Task description. } We let models (including all baselines and our COAST) accept input data $\textbf{u}_{in} \in \mathbb{R}^{T\times H\times W\times D}$ for $T=4$ moments and generates a predicted sequence $\textbf{u}_{pred} \in \mathbb{R}^{T'\times H\times W\times D}$ for the next $T'$ moments. In the training phase, we use $T'$ to stand for the default output sequence length, which is minimized to 1 and does not exceed 8, for the baseline models, or the adaptive output sequence length of the COAST. We use the real data $\textbf{u}_{true}$ of these next $T'$ moments as labels and calculate the average loss across the $T'$ predicted solutions.
\footnote{
For our COAST, $\textbf{Loss}_{dt}$ is also taken into account.} In the validation phase, we make $T'=8$ so that the model should rollout to autoregressively generates $\textbf{u}_{pred}$ to find the average loss between $\textbf{u}_{pred}$ and $\textbf{u}_{true}$.


Full details of the training and evaluation procedures are provided in \cref{train-eval}.

\subsection{Rollout Simulation Results}

Here we provide our main results across four challenged benchmarks from the dataset \textbf{The-Well} \citep{ohana2024thewell}. The full details on the underlying equations, dataset generation and problem setup for each case are provided in \cref{benchmarks}.

\begin{itemize}[leftmargin=*]
    \item \textbf{Active matter (AM)} \citep{maddu2024learning}: Active matter systems, composed of energy-transforming agents that generate orientation-dependent viscosity and transmit forces, exhibit complex nonlinear spatiotemporal dynamics in viscous fluids.
    \item \textbf{Turbulent radiative layer (TR) }\citep{fielding2020multiphase}:  Turbulent mixing between cold dense gas clumps and hot ambient gas generates rapidly cooling intermediate-temperature regions, where the competition between radiative energy loss and turbulent velocity fields nonlinearly regulates cold phase growth or dissolution.
    \item \textbf{Viscoelastic fluids (VF)} \citep{beneitez2024multistability}:  viscoelastic FENE-P fluid flow in wall-bounded geometries, resolving coupled Navier-Stokes and nonlinear conformation tensor dynamics to study multiscale elasto-inertial phenomena.
    \item \textbf{Rayleigh-Bénard convection (RB)} \citep{burns2020dedalus}: A buoyancy-driven turbulent flow arising from thermally induced density gradients in fluid layers bounded by contrasting thermal boundary conditions, exhibits nonlinear multiscale transport phenomena critical to geophysical, astrophysical, and engineered systems.
\end{itemize}

We presents the results of COAST against several competitive and heavily optimized baselines in \cref{rollout}. Our proposed method achieves the lowest VRMSE loss on almost all baselines. The best results are shown in bold and the next best results are underlined.
Additional visualizations of our models are shown in \cref{TR}, \cref{AM} and \cref{viz}.


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/TR.pdf}}
%\textwidth]
\caption{\textit{Turbulent radiative layer} benchmark. Representative COAST rollout prediction of the $density$ field, and point-wise error against the ground truth.}
\label{TR}
\end{center}
\vskip -0.2in
\end{figure*}



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/AM.pdf}}
%\textwidth]
\caption{\textit{Active matter} benchmark. Representative COAST rollout prediction of the $D_{xx}$ field, and point-wise error against the ground truth.}
\label{AM}
\end{center}
\vskip -0.2in
\end{figure*}


%\FloatBarrier
\begin{table*}[ht]
\caption{VRMSE loss (lower is better) of 5‐step rollouts on four benchmark PDEs (AM, TR, VF, and RB with various Rayleigh numbers Ra).  The best result in each column is bolded, and the second‐best result is underlined.  COAST ranks first in nearly every setting, placing second in only one column.
}
\label{rollout}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lcccccccc}
\toprule
Model& AM & TR & VF &   &   & RB &   &   \\ 
 &   &   &  & Ra=1e6 & Ra=1e7 & Ra=1e8 & Ra=1e9 & Ra=1e10 \\ 
\midrule
FNO & 1.663 & 0.765 & 0.711 & 0.628 & 0.664 & 0.674 & 0.706 & 0.710 \\ 

Dilated Resnet & 0.848 & 0.514 & 0.636 & 0.263 & 0.408 & 0.360 & 0.496 & 0.570 \\ 

CNextU-Net & 0.573 & 0.614 & 0.561 & \underline{0.205} & \underline{0.316} & \underline{0.361} & \textbf{0.379} & \underline{0.485} \\ 

AViT & 0.903 & \underline{0.513} & \underline{0.526} & 0.348 & 0.549 & 0.516 & 0.556 & 0.604 \\ 

DPOT & \underline{0.521} & 0.538 & 0.655 & 0.213 & 0.351 & 0.393 & 0.463 & 0.513 \\ 

COAST (ours) & \textbf{0.376} & \textbf{0.441} & \textbf{0.334} & \textbf{0.117} & \textbf{0.228} & \textbf{0.278} & \underline{0.404} & \textbf{0.386}\\ 

\bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in

\end{table*}

\paragraph{Loss accumulation} Loss accumulation is a major problem for operator learning models. During the rollout process, the model adds the predicted results to the input sequence to predict the solution further in time, so the error of the previous prediction results with the real data will exacerbate the error of the later prediction. Moreover, none of the models use the full length of rollout predictions in training, so examining the behavior of loss accumulation is important for evaluating the capability of models. 

For the four benchmarks, we show the average loss for each moment of the rollout process of COAST and other baseline models in \cref{loss-accumulation}. As shown in \cref{loss-accumulation}, COAST not only achieves the lowest loss on the four benchmarks, but also exhibits the lowest level of error accumulation behavior. This illustrates the stability of our model in long time period prediction.
\footnote{In order to present the results more clearly, here we have averaged the results of training on each of the $5$ sub-datasets of Rayleigh-Bénard convection (RB), rather than disaggregating them as we done in \cref{rollout}.}


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/Loss_accumulation.pdf}}
%\textwidth]
\caption{VRMSE loss (y‐axis) at each of eight rollout steps (x‐axis) for four PDE benchmarks (AM, TR, VF, and RB‐avg). COAST (red) shows the lowest—or near‐lowest—error across all time steps compared to the other baseline methods.}
\label{loss-accumulation}
\end{center}
\vskip -0.2in
\end{figure}


\subsection{Inference Time}

The speed of rollout inference is another important metric for evaluating operator learning models, which can reflect the efficiency of a operator-learning solver. As an adaptive solver, COAST can use fewer prediction steps for rollout inference. The distribution of the number of steps used by COAST to perform rollout prediction on each of the four benchmark datasets is shown in \cref{step-infer-extend} (a), where the rollout length is $8$.

\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figures/steps_infer_entend.pdf}}
\caption{(a) Distribution of the number of time steps taken by COAST to roll out over all benchmarks. (b) Average rollout inference time of COAST and other baselines for the next 8 moments versus their VRMSE losses; bubble size indicates the number of parameters. (c) Average inference time of COAST and other baselines at rollout lengths $8$, $16$, $32$, and $64$. Notably, in (b) COAST attains lower error (higher accuracy) yet remains as fast as some baselines with larger errors and similar parameter counts, while in (c) COAST's inference time grows only modestly across longer rollouts, unlike the steeper curves for other methods.
}
\label{step-infer-extend}
\end{center}
\vskip -0.2in
\end{figure*}


Thus, COAST has the potential to have higher solving efficiency than other baseline models, although the CLM structure makes it have a relatively large number of parameters. 
We compared the average rollout inference time of COAST and other baseline models over the 4 benchmarks when the rollout length is $8$. The results are presented in \cref{step-infer-extend} (b).

Further, a rollout process of length 8 does not seem to be sufficient to fully utilize the high inferential efficiency of COAST as an adaptive solver. Our interpolation method makes COAST predict step lengths that tend to exceed the time span of the interpolation, and the percentage of this exceeding interval will be larger in rollouts of shorter lengths. In addition, in longer intervals, COAST will have more opportunities to generate more longer step lengths. Therefore, to more fully evaluate the inference efficiency of COAST, we extend the length of the rollout to $16$, $32$, and $64$ to compute the average inference time of COAST and other baseline models, and the results can be seen in \cref{step-infer-extend} (c).


\subsection{Significance of Variable Step Sizes
%that correlate with the underlying system parameters
}

As shown in \cref{step-infer-extend} (a), the number of rollout steps used by COAST shows a distribution on the four benchmarks, which comes from the fact that the predicted step size given by COAST is variable. We next explore the relationship between this variability and the nature of the systems themselves. 
And in doing so, we can explain the reasons for this variability in the length of steps.

\paragraph{Cross dynamics analysis}
\cref{dt_vs_param} shows the distributions of time step sizes given by the model when rolling out inferences for subdatasets with different parameters. 
Here the step sizes are averaged over each rollout trajectory.
It can be seen that under the same dataset, different subdatasets can be distinguished according to the distribution of step sizes predicted by COAST.


\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/dt_vs_param.pdf}}
\caption{
Violin plots of COAST's predicted time‐step distributions across different parameters in two benchmarks: (a) active matter (AM) and (b) Rayleigh–Bénard convection (RB).  The $p$‐values from a Mann‐Whitney U test (indicated as ***, **, *) confirm that step sizes differ significantly between adjacent parameter settings.  Notably, as the parameters vary, COAST's predicted step sizes shift accordingly, illustrating that the model learns and adapts its temporal resolution to the underlying dynamical complexity of each system.}
\label{dt_vs_param}
\end{center}
\vskip -0.2in
\end{figure*}


Further, we explore the physical meaning behind these parameters or types. In the cases we have shown, these parameters or types can characterize the complexity of their systems. Specifically, the absolute value of $\alpha$ in the active matter (AM) problem has a negative correlation with system complexity, while the Rayleigh number $(Ra$) in the Rayleigh-Bénard convection (RB) problem has a positive correlation with system complexity. The specific impact of each parameter on their systems can be seen in \cref{am_details} and \cref{rb_detail}. As shown in \cref{dt_vs_param}, for systems of higher complexity, the model tends to predict smaller step sizes and more concentrated distributions, and vice versa. This indicates that the model learns to predict the step size based on the complexity of the system. 
For systems of higher complexity, where changes between temporally-neighboring frames are generally more complex, the model exhibits more ``cautious" characteristics. 
On the other hand, for systems of lower complexity, the model tends to make more ``bold" predictions with longer step sizes to improve prediction efficiency.
This is a strong indication of the intelligence of COAST, and in particular its core model CLM, in dealing with the solution of PDE equations.

\paragraph{Cross time regions analysis}
Similarly, we explore the adaptive behavior of the model in different time intervals of the same dynamics. \cref{dt_vs_time} shows the average time step sizes predicted by COAST for each moment in the same type of dynamics. 
\footnote{
Note that here we are averaging the step sizes over the same time points in the rollout for different trajectories, unlike averaging the average step size across the trajectories in \cref{dt_vs_param} (b). Here there is a chance that larger step sizes will be multi-counted over multiple time points, so the step size plot line here will be slightly higher overall than the data in \cref{dt_vs_param} (b).
}

\begin{wrapfigure}{r}{0.68\textwidth} 
\vspace{-0.928em}
\begin{minipage}{0.68\textwidth}
\vspace{-0.928em}
\begin{figure}[H]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=0.68\columnwidth]{figures/dt_vs_time.pdf}}
\caption{
Average predicted time‐step size from COAST across eight rollout points in Rayleigh–Bénard convection (RB) for different parameter values. Each line shows how COAST adaptively adjusts its temporal resolution over the course of the simulation. As the predictive horizon extends and the system's evolution becomes more complex, COAST gradually decreases the step size.
}
\label{dt_vs_time}
\end{center}
\vskip -0.2in
\end{figure}
\end{minipage}
\end{wrapfigure}


It can be seen that COAST gradually reduces the step size as the prediction proceeds on the Rayleigh–Bénard convection solving problems. This is because the whole system is in a process of changing from stabilization to perturbation during the phase where we perform rollout, and the complexity of the system gradually increases, and the model needs to predict more cautiously to reduce the error.\footnote{We have attached a visualization in the \cref{viz} to illustrate this process.} This can show that COAST not only exhibits the capacity to make reasonable step-size decisions in the face of different types of system, but also adjusts the step-size in real time during the rollout process, demonstrating the intelligence in solving problems with PDE systems.





\subsection{Scalability
%and ablation study
}

\begin{wrapfigure}[7]{r}{0.45\textwidth} 
\vspace{-0.968em}
\begin{minipage}{0.45\textwidth}
\vspace{-0.968em}
\begin{table}[H]
\caption{Details of COAST model variants
}
\vspace{-0.108em}
\label{param}
\vskip -0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\columnwidth}{!}{
\renewcommand{\arraystretch}{1.2}
\vspace{-0.108em}
\begin{tabular}{l|cccc}
    \toprule
    \textbf{Model} & \textbf{Embedding dim} & \textbf{Blocks} & \textbf{Heads} & \textbf{\# Params} \\
    \midrule
       COAST-S  & 256 & 8 & 6 & 7.3M\\
        COAST-M & 384 & 12 & 8 & 20M\\
       COAST-L  & 768 & 12 & 12 & 105M\\
    \bottomrule
\end{tabular}
}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in

\end{table}
\end{minipage}
\end{wrapfigure}



Here we verify the effect of the model's scalability on the \textit{turbulent radiative layer} (\textbf{TR}) benchmark.
%and the interpret-modify mechanism on the model's performance.

%\textbf{Scalability. }
We tested our model with three different parameter sizes and obtained the rollout accuracy as shown in \cref{scale} (a). The COAST configurations are detailed in \cref{param}. This reflects that the prediction accuracy of our model is positively correlated with the number of parameters and has a good scalability.


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/scale.pdf}}
%\textwidth]
\caption{(a) Convergence of test rollout errors for COAST at three different model sizes (7 M, 20 M, and 105 M parameters). As the model size grows, the error decreases more rapidly and converges to a lower value.
(b) Corresponding average rollout step sizes during training. The larger, more accurate model can afford to take fewer steps in each rollout—thus using larger time steps—while still maintaining lower error.
}
\label{scale}
\end{center}
\vskip -0.2in
\end{figure*}





Interestingly, as shown in \cref{scale} (b), a smaller number of parameters leads to a larger average step size. One potential explanation for this is as follows. In training our loss function contains two terms, spatial a spatial loss$\textbf{Loss}_{spatial}$ and a step-size loss $\textbf{Loss}_{dt}$, where $\textbf{Loss}_{dt}$ imposes a greater penalty on smaller step sizes. When the model is not sufficiently large to extract the complex dependencies in the space, it becomes challenging to continue decreasing $\textbf{Loss}_{spatial}$. Consequently, the model shifts to lowering $\textbf{Loss}_{dt}$ by lengthening the step size.
%\textbf{Interpret-Modify Mechanism. }
%To verify the role of our interpret-modify mechanism, we performed an ablation experiment. In this experiment, COAST-IM performed both interpretation and modification; whereas COAST-I performed only interpretation but no modification, i.e., it did not use the interpreted $dt$ to modify the output tensor $\textbf{z}$ of the CLM.

%(Accurancy)

%(Time-adaptive behavior)

%%%%%%%%%%%%%just for test
\section{Discussion}

\paragraph{Summary }
This work introduces COAST, a new neural operator architecture that utilizes causal language models at its core to address the challenges of learning complex physical systems.
COAST combines the strengths of causal language models and adaptive solution methods to achieve state-of-the-art accuracy and minimal error accumulation behavior on challenging benchmarks in energy transformation, fluid dynamics, and thermodynamic processes.
Our approach demonstrates the potential of employing advanced causal language models to develop more flexible and accurate machine learning models for the physical sciences. Key innovations of our work include:
(a) an efficient solver on continuous time for autonomous decision prediction step sizes,
(b) a rational method for evaluating time-adaptive solvers,
and (c) an exploration of the deep understanding of PDE systems embodied in causal language models for operator learning.

Our empirical results in various PDE benchmark tests show that COAST's time-adaptive approach endows it with higher solving efficiency in prediction over longer sequences. This time-adaptive behavior helps to build more general solvers. In addition, the step size distributions predicted by COAST when confronted with different systems and states can also be used to explore some of the more intrinsic properties of dynamical systems. The broader impact of this work based on COAST is that it has the potential to accelerate scientific discovery by more efficiently and accurately modeling complex physical systems over longer time horizons, with applications ranging from energy transformation modeling to engineering design.



\paragraph{Limitations \& Future Work } While COAST advances neural operator capabilities, several limitations need attention. First, current experiments focus on systems with regular geometries and uniform grids, leaving performance on complex geometries (e.g., fractured porous media, turbulent multiphase flows) unexplored.  Second, while the architecture shows empirical stability in moderate rollout lengths, its error propagation behavior under extended autoregressive prediction horizons remains unexamined. Third, the current implementation operates as a specialized solver rather than a generalizable framework, limiting direct applicability to PDE systems requiring coupled multi-physics modeling.

Future research should prioritize extending COAST’s framework toward multimodal PDE foundation models capable of unifying diverse physical systems under a single architecture. This could involve integrating physical constraints via hybrid symbolic-neural frameworks that enforce various physical laws. Another particularly promising direction lies in coupling COAST with LLMs—such integration could enable cross-modal reasoning where textual system descriptions guide dynamics prediction, or conversely, where learned physical representations enhance LLMs’ capacity for quantitative scientific reasoning. We believe that addressing these challenges will enable the synergistic integration of physics-informed machine learning and foundation models, paving the way for next-generation computational tools across scientific domains.



%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\newpage
\appendix
\onecolumn


\section{Experimental Details}
\subsection{Training and Evaluation}
\label{train-eval}

\subsubsection{Training recipe} We use a unified training recipe for all COAST experiments. We employ AdamW optimizer \citep{loshchilov2019decoupledweightdecayregularization} with a weight decay $10^{-5}$. Our learning rate schedule includes an initial linear warm up phase of 5 epochs, starting from zero and gradually increasing to $5\times 10^{-5}$, followed by an exponential decay at a rate of 0.9 for every 5,000 steps. The spatial loss function is a one step mean squared error (MSE) between the model predictions and the corresponding targets at the predicted time steps, average over all coordinates:


$$
\mathrm{MSE} = \frac{1}{B}\frac{1}{D}\sum_{i=1}^{B}\sum_{j=1}^{Q}\sum_{k=1}^{D}|\hat{s}_i^{(k)}(\mathrm{y_j)}-s_i^{(k)}(\mathrm{y_j)}|_2^2,
$$

where $s_i^{k}(\mathrm{y_j)}$ denotes the $k$-th variable of the $i$-th sample in the training dataset, evaluated at a
query coordinate $\mathrm{y_j}$ , and $\hat{s}$ denotes the corresponding model prediction. All models are trained for the same number of epochs with a equal batch size $B$ on each benchmark.

For COAST, we need to consider the add a term to the loss function to penalize comparatively small $dt$ the model outputs. Here we choose to use a piecewise power-exponential function of $dt$ as $\textbf{Loss}_{dt}$, where $0<\epsilon <1$  and $n$ is the power that larger than $1$:
\label{loss_dt}
$$
\textbf{Loss}_{dt}=
\left\{
\begin{array}{cl}
(1+\epsilon-dt)^n &  dt \leq 1+\epsilon \\
0 &  dt > 1+\epsilon \\
\end{array} \right.
$$

Based on our ablation study, changing the values of $\epsilon$ and $n$ does not affect the training results. Without loss of generality, we take $\epsilon$ to be $0.5$ and $n$ to be $2$.


\subsubsection{Evaluation } 
After training, we obtain the predicted trajectory by performing an auto-regressive rollout of 8 time points on the test dataset. We evaluate model accuracy using the VRMSE loss (Variance Scaled Root Mean Squared Error), recommended in The-Well \cite{ohana2024thewell}:

$$
\mathrm{VRMSE}(u,v)=\langle|u-v|\rangle^2/(\langle|u-\bar{u}|^2\rangle+\epsilon)^{1/2}
$$

Note that, since $\mathrm{VRMSE}(u,\bar{u})\approx1$, having $\mathrm{VRMSE}>1$ indicates worse results than an accurate estimation of the spatial mean $\bar{u}$.

\subsection{Model Details}
\label{model-details}

%\textbf{FiLM Layer. }

\paragraph{COAST. } We use a patch size of $32\times 32$ for embedding data. The encoder consists of 3-layer convolutional neural network (CNN) layers with padding and overlapped kernels and the decoder just mirrors it. We use an 2-layer MLP to intepret $dt$ from output tokens by CLM. We use two FiLM layer in COAST, one is for embedding time series and another is for output tokens modification using output $dt$. We use a GPT-2 structure with multi-attention heads as our core causal language model. We use GELU as active function in the structures above.

\paragraph{FNO. } Our FNO model employs 3 Fourier neural blocks, each with 128 channels and 16 Fourier modes. 

\paragraph{Dilated ResNet. } We adopt the Dilated ResNet which has a hidden dimention of 96 and 32 residual blocks. 

\paragraph{CNextU-Net. } We employs a U-Net with a Conv Next structure reported by The -Well\citep{ohana2024thewell}, which has 8 blocks per stage and 42 initial features.

\paragraph{AViT. } We adopt an AViT model with a hidden dimension of 768 and 12 attention heads.

\paragraph{DPOT. } We employs a DPOT with an AFNO as mixing type and Exp-MLP time aggregation. The patch size is 4 and blocks number is 16. For challenging comparison, We set its embedding dimension to 1024, depth to 16 and number of attention head to 16. The out layer of the model has a dimension of 32. The active function in it is also GELU.


\subsection{Benchmarks}
\label{benchmarks}

\paragraph{Dataset. }
In all the following benchmarks, we make use of the datasets released by The Well \citep{ohana2024thewell}. This dataset consists of 15T data of discretized initial conditions on diverse types and parameter-sets. 

Below, we collect and summarize the descrition from The Well of each dataset we used.  

\paragraph{Problem setup. } We follow the problem setup by The Well \citep{ohana2024thewell}. Our objective is to predict the future solution within 8 moments. We compare the COAST model's performance against several strong baseline neural operators: FNO\citep{li2020fourier}, Dilated Resnet\citep{yu2017dilatedresidualnetworks}, CNextU-Net\citep{liu2022convnet2020s}, AViT\citep{du2024avitadaptingvisiontransformers} and DPOT\citep{hao2024dpot}, most of those are well performed on The Well datasets reported by (the well). We use different training, validation and testing data split\citep{ohana2024thewell}.

%\textbf{Training. }




\subsubsection{Active Matter (AM)}
\label{am_details}
This dataset comprises simulations of a continuum theory describing the dynamics of \(N\) rod-like active particles in a Stokes fluid within a two-dimensional domain of linear size \(L\). The data include 81 time-steps of \(256 \times 256\) resolution per trajectory, with fields such as concentration (scalar), velocity (vector), orientation tensor, and strain-rate tensor. Simulations explore parameter variations in alignment (\(\zeta\)), dipole strength (\(\alpha\)), and other coefficients, capturing phenomena like energy transfer across scales, vorticity-orientation coupling, and the isotropic-to-nematic phase transition. Periodic boundary conditions and uniform Cartesian grids are employed, with data stored at 0.25-second intervals over a 20-second timespan. Refer to \cite{maddu2024learning} for details on problem formulation and detailed equations.

Note that \(\alpha\) is the dimensionless active dipole strength. Base on the original paper\cite{maddu2024learning}, the greater the absolute value of \(\alpha\), the faster the system approaches order/stability. It can be analogous to viscosity in the fluid problem to some extent. So smaller \(|\alpha|\) means higher complexity.




\subsubsection{Turbulent Radiative Layer (TR)}
This dataset explores the dynamics of turbulent radiative layers in astrophysical systems, where hot and cold gases mix, leading to the formation of intermediate-temperature gas that rapidly cools. The simulations model the Kelvin-Helmholtz instability in a 2D domain, with cold, dense gas at the bottom and hot, dilute gas at the top. The data capture key phenomena such as mass flux from the hot to cold phase, turbulent velocities, and the distribution of mass across temperature bins. The dataset includes 101 timesteps of 384$\times$128 resolution for 90 trajectories, varying the cooling time \( t_{\rm cool} \) across nine values. Simulations were performed using Athena++ on a uniform Cartesian grid with periodic boundary conditions in the x-direction and zero-gradient in the y-direction. This dataset provides insights into the phase structure, energetics, and dynamics of multiphase gas in astrophysical environments, such as the interstellar and circumgalactic media.

\begin{align}
\frac{ \partial \rho}{\partial t} + \nabla \cdot \left( \rho \vec{v} \right) &= 0 \\
\frac{ \partial \rho \vec{v} }{\partial t} + \nabla \cdot \left( \rho \vec{v}\vec{v} + P \right) &= 0 \\
\frac{ \partial E }{\partial t} + \nabla \cdot \left( (E + P) \vec{v} \right) &= - \frac{E}{t_{\rm cool}} \\
E = P / (\gamma -1) \, \, \gamma &= 5/3
\end{align}

where $\rho$ is the density, $\vec{v}$ is the 2D velocity, $P$ is the pressure, $E$ is the total energy, and $t_{\rm cool}$ is the cooling time.


\subsubsection{Viscoelastic Fluids (VF)}
This dataset explores the multistability of viscoelastic fluids in a two-dimensional channel flow, capturing four distinct attractors: the laminar state (LAM), a steady arrowhead regime (SAR), Elasto-inertial turbulence (EIT), and a chaotic arrowhead regime (CAR). These states coexist for the same set of parameters, with their emergence dependent on initial conditions. The dataset includes snapshots of these attractors as well as edge states, which lie on the boundary between basins of attraction and provide insight into transitions between flow regimes. The data were generated using direct numerical simulations of the FENE-P model, solving for velocity, pressure, and the conformation tensor fields. Key phenomena include chaotic dynamics in EIT and CAR, as well as the multistability of the system. The dataset, comprising 260 trajectories with 512$\times$512 resolution, is valuable for studying viscoelastic turbulence and evaluating simulators capable of capturing these complex flow behaviors. Simulations were performed using the Dedalus framework, with parameters set to $Re=1000$, $Wi=50$, $\beta=0.9$, $\epsilon=2\times10^{-6}$, and $L_{max}=70$.

\begin{align*}
Re(\partial_t \mathbf{u^\ast} + (\mathbf{u^\ast}\cdot\nabla)\mathbf{u^\ast} ) + \nabla p^\ast &= \beta \Delta \mathbf{u^\ast} + (1-\beta)\nabla\cdot \mathbf{T}(\mathbf{C^\ast}),\\
\partial_t \mathbf{C^\ast} + (\mathbf{u^\ast}\cdot\nabla)\mathbf{C^\ast} +\mathbf{T}(\mathbf{C^\ast}) &= \mathbf{C^\ast}\cdot\nabla \mathbf{u^\ast} + (\nabla \mathbf{u^\ast})^T \cdot \mathbf{C^\ast} + \epsilon \Delta \mathbf{C^\ast}, \\
\nabla \mathbf{u^\ast} &= 0,
\end{align*}

\begin{equation*}
\textrm{with} \quad \mathbf{T}(\mathbf{C^\ast}) = \frac{1}{\text{Wi}}(f(\textrm{tr}(\mathbf{C^\ast}))\mathbf{C^\ast} - \mathbf{I}),\qquad
\textrm{and} \quad f(s) := \left(1- \frac{s-3}{L^2_{max}}\right)^{-1}.
\end{equation*}

where $\mathbf{u^\ast} = (u^\ast,v^\ast)$ is the streamwise and wall-normal velocity components, $p^\ast$ is the pressure, $\mathbf{C^\ast}$ is the positive definite conformation tensor which represents the ensemble average of the produce of the end-to-end vector of the polymer molecules. In 2D, 4 components of the tensor are solved: $c_{xx}^\ast, c_{yy}^\ast, c_{zz}^\ast, c_{xy}^\ast$. $\mathbf{T}(\mathbf{C^\ast})$ is the polymer stress tensor given by the FENE-P model.



\subsubsection{Rayleigh-Bénard Convection (RB)}
\label{rb_detail}
This dataset comprises simulations of two-dimensional, horizontally periodic Rayleigh-Bénard convection, capturing the dynamics of fluid motion driven by thermal gradients. The system consists of a fluid layer heated from below and cooled from above, leading to the formation of convective cells and complex flow patterns. The dataset includes 200 timesteps of 512 $\times$ 128 resolution for 1,750 simulations, varying the Rayleigh number ($10^6$ to $10^{10}$), Prandtl number (0.1 to 10), and initial buoyancy perturbations. Fields such as buoyancy (scalar), pressure (scalar), and velocity (vector) are provided, with periodic boundary conditions horizontally and Dirichlet conditions vertically. The data, generated using the Dedalus framework, offer insights into turbulent eddies, convection cells, and the sensitivity of flow structures to initial conditions. This dataset is valuable for studying thermal convection phenomena and validating numerical models in fluid dynamics.

The time domain problem is formulated as
\begin{align*}
\frac{\partial b}{\partial t} - \kappa \Delta b & = -u\nabla b,
\\
\frac{\partial u}{\partial t} - \nu \Delta u + \nabla p - b \vec{e}_z & = -u \nabla u,
\end{align*}
with boundary conditions

\begin{align*}
b(z=0) = Lz ~~~,~~~ b(z=Lz) = 0
\\
u(z=0) = u(z=Lz) = 0
\end{align*}

Note that the Rayleigh number ($Ra$) satisfy the relation: $(viscosity)\ \nu =(\frac{Ra}{Prandtl})^{-\frac{1}{2}}$. It means that the greater $Ra$ means smaller viscosity, then the system will approaches order/stability slower. So greater $Ra$ means higher complexity.

\subsection{Visualization}
\label{viz}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[height=0.9\textheight]{Appendix_figure/AM.pdf}}
%\textwidth]
\caption{\textit{Active Matter (AM).} Representative COAST rollout prediction of all the different fields in y direction, and point-wise error against the ground truth.}
\label{am-viz}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[height=0.9\textheight]{Appendix_figure/TR.pdf}}
%\textwidth]
\caption{\textit{Turbulent Radiative Layer (TR).} Representative COAST rollout prediction of all the different fields in y direction, and point-wise error against the ground truth.}
\label{tr-viz}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[height=0.9\textheight]{Appendix_figure/VF.pdf}}
%\textwidth]
\caption{\textit{Viscoelastic Fluids (VF).} Representative COAST rollout prediction of all the different fields in y direction, and point-wise error against the ground truth.}
\label{vf-viz}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[height=0.9\textheight]{Appendix_figure/RB.pdf}}
%\textwidth]
\caption{\textit{Rayleigh-Bénard Convection (RB).} Representative COAST rollout prediction of all the different fields, and point-wise error against the ground truth.}
\label{rb-viz}
\end{center}
\vskip -0.2in
\end{figure*}


\end{document}
