\section{Background and Related Work}
\label{sec:related_work}

Bias, in statistical theory, implies the presence of a "true" value from which the biased estimate systematically differs. However, in practice, we generally lack a ground truth, making it infeasible to directly assess bias for individual articles or even entire publishers. This assertion might seem counterintuitive, as we often perceive articles as biased when we read them. This perception arises because we implicitly treat our subjective opinions as the "truth," even though they are likely biased as well \cite{pronin2004objectivity}. In the absence of a universally accepted ground truth, we instead need to identify bias by observing patterns and differences in how news is reported between publishers, or within publishers over time. In this section, we anchor our discussion of media bias in the concepts of selection and framing biases and consider their relevance to designing media bias tools grounded in HCI principles. We also situate the \mbd\ among existing media bias detection tools and discuss their limitations. Finally, we review current literature on LLMs for annotation tasks and explain why these models are well-suited for analyzing media bias. 

\subsection{Agenda-Setting, Framing, and the Role of HCI in Designing Tools for Media Bias}
\label{sec:related_work_media_bias}

Media bias presents major implications for the HCI community as online platforms increasingly become the primary medium for news consumption. Within the communications field, media bias is often analyzed through two key theories: agenda-setting and framing. Agenda-setting theory argues that the media doesn't tell us what to think but rather what to think about \cite{agenda_setting_1972, mccombs2005agenda}. The process of \emph{selection bias}---choosing certain stories or facts to report while omitting others---directly contributes to the media's agenda-setting power, shaping public discourse by directing attention to specific topics over others \cite{dearing1996agenda}. For example, a news outlet might publish many stories on a frontrunner's campaign rallies while giving limited coverage to other candidates. Agenda-setting theory is highly relevant to HCI because digital platforms increasingly mediate what content users see \cite{Naser2020RelevanceAC}. Algorithms, interface design, and personalized recommendation systems often act as agenda-setters, determining what content is presented or excluded from users' screens \cite{maudet,Eslami2016FirstI}. This phenomenon raises critical questions in HCI about how systems shape user awareness and understanding, as well as how tools can be designed to challenge these embedded biases.

Beyond selecting which issues to emphasize, the media also influences public perception via the manner in which information is presented---a process known as framing \cite{weaver2007thoughts}. \emph{Framing bias} builds on selection bias by not just determining what is covered, but how it is covered. This involves the inclusion or omission of specific details and perspectives, the tone or language used, the context provided, or the omission of background information, all of which can significantly alter how an issue is perceived by the audience \cite{entman1993framing,gentzkow2006media, chong2007framing}. For instance, framing a protest as a "riot" in one article versus a "peaceful demonstration" in another can significantly influence how readers perceive the event \cite{brown_protest_2021, susanszky2022media}. Similarly, framing immigration as an "opportunity for economic growth" versus an "immigration crisis" can sway public perception on the issue \cite{Igartua2009ModeratingEO}. Framing effects relate to HCI through a shared focus on how the presentation and emphasis of information influences its importance or impact \cite{hartmann_framing}. Media bias tools informed by HCI principles can play a critical role in revealing framing biases \cite{dingler_workshop}. For example, tools that allow users to analyze data from multiple perspectives \cite{ainsworth2008educational}, explore specific points in more detail \cite{springer2019progressive}, or compare diverse sources \cite{bhuiyan2023newscomp,braaten2011measuring} can foster a deeper understanding of how the same event is portrayed in different ways.

Although the concepts of selection and framing bias are well-known in media studies and communications \cite{o2009news, d2000media, matthes2009s}, their impact has become even more pronounced in today's digital age \cite{harcup2017news, bourgeois2018selection}. The rapid dissemination and broad reach of online news allow information consumers to quickly access content that reinforces their pre-existing opinions. Research indicates that online news consumption exhibits a polarized pattern, with users spending significantly more time on news sources that align with their political leanings compared to those that do not \cite{garimella2021political}. Tools that make media bias visible---especially in terms of selection and framing biases---can help users recognize these patterns and engage more critically with their news consumption \cite{markus}. By applying HCI principles to these tools, we can design interfaces that promote media literacy and encourage balanced engagement with digital news platforms.

\subsection{Examining Current Approaches and Limitations in Assessing Media Bias}
\label{sec:related_work_existing_tools}

Scholars have proposed numerous taxonomies to understand media bias, yet no universally accepted set of media bias metrics or standard measurements exist \cite{puglisi2015empirical, gentzkow2015media, morstatter_2018, hamborg2019automated, huang2024uncovering}. Current methods often reduce news publishers to a single metric by assigning a political lean rating from "Left" to "Right" \cite{allsides,mediabiasfactcheck,groundnews}. These tools simplify bias labels to make it easier for users to digest, such as using 2-dimensional axes with political lean and reliability \cite{adfontesmedia} or a 1-dimensional categorization of publishers across the political spectrum \cite{allsides}. Although useful, these static classifications fail to capture within-publisher differences in bias over time, across topics, or even across individual articles. 

Evaluating bias at the article level is less common and more challenging. Earlier tools evaluating article level bias relied on basic NLP techniques, such as keyword frequency analysis, to cluster similar stories in the media \cite{park2009newscube}. However, these methods are limited in their ability to account for context and the evolving nature of news stories. Recent advancements in LLMs offer improved contextual understanding capabilities which can more accurately help identify media bias in news coverage. Even though other proprietary tools that incorporate AI features have emerged \cite{biasly,allsides}, these platforms generally do not update to reflect media bias in recent news coverage. Furthermore, the lack of transparency around their AI systems makes it unclear which models they use and how they arrive at their conclusions.

The \mbd\ addresses these limitations by moving beyond oversimplified metrics and publisher level analyses. Rather than simplifying media bias to one or two dimensions, the \mbd\ captures framing and selection biases through a multidimensional analysis of publisher coverage over time. This approach allows users to compare tone, political lean, and content focus between publishers, and across topics, resulting in a more comprehensive and dynamic approach to media bias. The \mbd\ is intentionally designed to provide near real-time updates, allowing users to explore how media coverage---and the biases within it---shift dynamically as new information emerges. Furthermore, we make our underlying methodology, including our model usage, prompt phrasing, and human in the loop verification framework, readily available to all users of our tool (see Appendix \ref{sec:website_methodology}).

\subsection{Incorporating Large Language Models (LLMs) for Media Bias Detection}
\label{sec:relate_work_llm_bias}

LLMs, such as GPT-4, utilize deep contextual embeddings to capture subtle semantic relationships and nuances within text \cite{brown2020language,achiam2023gpt, radford2019language}. Studies have shown that LLMs perform well in generative tasks such as summarization \cite{liu-etal-2024-learning, ravaut-etal-2024-context, liu2023learning}, as well as discriminative tasks like sentiment analysis and topic classification \cite{chang2024survey, pena2023leveraging, zhang2023sentiment}. More recent studies have shown that state-of-the-art LLMs exhibit increasingly complex reasoning and problem-solving capabilities \cite{bubeck2023sparks}. These findings suggest that LLMs can be especially useful for the complex task of media bias detection, where understanding context, tone, and subtle language is crucial.

While traditional machine learning models used in annotation tasks require task-specific training, instruction-tuned pre-trained LLMs can generalize to different tasks in a zero-shot setting \cite{wang_humanllm_2024}. Prior work has shown that using LLMs to automate the initial stages of data annotation enables researchers to quickly process large volumes of content that would be unmanageable with manual annotation alone \cite{ziems2024can}. This scalability is critical given the speed with which major news outlets publish new articles, which must be analyzed in near real-time to capture evolving narratives and help readers interpret the news. By incorporating LLMs into the \mbd's news analysis pipeline, we can efficiently extract detailed information like sentence-level codings and classifications by topic, subtopic, article type, tone, and political lean, while significantly increasing the volume of articles we can annotate. However, it is important to acknowledge potential limitations of LLMs in zero-shot settings, such as political biases in their outputs and the influence of this bias on the tool's results. These challenges are explored in greater depth in Section \ref{sec:limitations}.

The integration of LLMs into traditionally human processes is informed by a growing body of research that shows that human-AI collaboration enhances the accuracy and reliability of automated systems \cite{uchendu2023does, goel2023llms}. Although LLMs can quickly process large datasets and perform initial classifications, human oversight is crucial for providing context that LLMs do not have for current events and verifying the model's outputs for consistent standards of accuracy \cite{wang_humanllm_2024, amirizaniani2024developing}. In the \mbd, humans play an important role at every step, from generating a targeted list of relevant news topics for LLMs to classify, to continuously monitoring the model's classifications over time.

We intentionally chose to evaluate the responses generated by LLMs through a process of human annotation that emphasizes validation over independent labeling. Rather than having annotators blindly label the data, we engage them in a validation task where they read GPT-generated responses to assess their coherence and soundness. Although having humans independently label content and then comparing it to GPT's output could yield interesting insights, it is important to recognize that human disagreements often occur, even among reasonable individuals. Particularly in complex tasks, such as reading an entire article and labeling its lean and tone, as well as extracting facts, achieving high agreement is challenging \cite{mitchell2018distinguishing}. Our methodology accounts for this inherent subjectivity and aims to ensure that topics presented on the \mbd\ are coherent, reasonable, and free from overt inaccuracies. By involving humans in the validation process, we impose a layer of quality control that acknowledges the absence of a singular ground truth. % Human oversight ensures that the topics are contextually relevant and that the classifications are reasonable.