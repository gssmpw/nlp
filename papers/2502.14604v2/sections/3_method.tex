\begin{figure*}[!t]
\begin{center}
\includegraphics[width=\textwidth]{figs/framework.pdf}
\end{center}
  \vspace{-.1in}\caption{Overview of the proposed framework. We use the detection results from ZS-CLIP as pseudo-labels to train the Adaptive Noise Detector (AdaND). In the early stage, we directly use the ZS-CLIP to distinguish clean-noise samples, while in the later stage, we use the AdaND instead. The predicted clean samples are then classified based on the text-based classifier.
   To further handle the clean data stream case, we intentionally inject Gaussian noise as additional noisy samples to avoid wrongly assigning too many clean samples as noisy ones.
}
  \label{fig:framework}
  \vspace{-15pt}
\end{figure*}

\vspace{-.1in}
\section{Method}\label{sec:method}
This section demonstrates how to develop the framework that decouples the classifier and detector to better cope with the ZS-NTTA task based on the analysis presented in Sec.~\ref{sec:failure-case-study}. The proposed framework focuses on training an adaptive noise detector to distinguish noisy samples while keeping the classifier frozen.
Speciﬁcally, our method consists of two modules: (1) an Adaptive Noise Detector (AdaND), 
and (2) intentionally injecting Gaussian noise to cover the clean data stream case. The overall framework is illustrated in Figure~\ref{fig:framework}.

\subsection{Adaptive Noise Detector}
\vspace{-5pt}
We use the image feature $\mathcal{I}(x)$ extracted from the frozen model as the training data during TTA. 
Given that ZS-CLIP can effectively distinguish most ID and noisy samples, we use the detection results from ZS-CLIP as pseudo-labels in test-time throughout the process.
In addition, we employ a single linear layer as the noise detector, leveraging the standard cross-entropy loss for training, \textit{i.e.}, $\mathcal{L} = -\sum_{i=1}^C y_i^\text{pse} \log(\hat{y}_i), \hat{y}_i=\nicefrac{e^{z_i}}{\sum_{j=1}^C e^{z_j}}$. Here, $y_i^\text{pse}$ is the pseudo-label generated by ZS-CLIP, $z_j$ denotes the logit of the noise detector for class $i$, and $C=2$.
Our computational overhead is low since only the noise detector is updated during training. 

After each training step, the test sample will be re-evaluated for clean-noise detection and classification using its image feature. Since the noise detector may not adapt sufficiently in the early steps of the data stream, we divide the clean-noise detection process into two stages. 
In the first stage, \textit{e.g.}, for the initial $N$ optimization steps, we use the output from ZS-CLIP as the detection result. 
In the second stage, we switch to using the output from the noise detector as the detection result.
We also use the adaptive threshold in Eq.~\eqref{eq:adaptive_threshold} as the detection threshold rather than directly set $\lambda=0.5$.


To handle scenarios involving a single input sample, \textit{i.e.}, the batch size is $1$, we introduce a queue with a capacity of $L$ to store the outputs from the noise detector.
We update the noise detector with the queue's data every $L$ samples, and empty the queue after each update. Note that each sample yields an immediate test result upon input and does not require the accumulation of $L$ samples.
What's more, our queue only stores the input features, outputs, and pseudo-labels, ensuring privacy while maintaining minimal and negligible computational and storage overheads.

\vspace{-5pt}
\subsection{Gaussian Noise Injecting}
\vspace{-5pt}

\paragraph{How to handle the clean data stream without data stream prior?}
Although the noise detector effectively differentiates between noisy and clean samples within a noisy data stream, it encounters challenges when the test data lacks noisy samples. In such cases, the detector tends to misclassify many clean samples as noisy, leading to a significant drop in performance.
To address this, we intentionally inject noise as additional noisy samples to cover the clean data stream case. In this way, all manually injected noise will be included in the adaptive threshold calculation, preventing the misclassification of clean samples as noisy. During testing, we exclusively consider samples from the original data stream, excluding manually injected noise samples.

\vspace{-5pt}
\paragraph{How to choose the appropriate noise before inference?}
The injected noise must 1) lie outside the ID label space and 2) be easily accessible without incurring extra costs for auxiliary data collection. The choice of injected noise is flexible; for simplicity and effectiveness, we choose Gaussian noise. 
During testing, we insert a Gaussian noise sample for every $M$ input sample in the data stream, regardless of whether the stream is clean or noisy. Note that we don't have prior knowledge about whether the data stream is clean or noisy.
The detailed algorithm for AdaND is provided in Algorithm~\ref{alg:ours} in Appendix~\ref{app: baseline methods}.



% $\mathcal{L} = -\sum_{i=1}^C y_i \log(\hat{y}_i), \hat{y}_i=\nicefrac{e^{z_i}}{\sum_{k=1}^C e^{z_k}}.$

% \paragraph{Confidence-aware cross-entropy loss}
% To reduce the impact of mislabeled samples in pseudo-labels on the noise detector, we introduce a confidence-aware cross-entropy loss:
% \begin{equation}
% % \small
%     \mathcal{L} = -\sum_{i=1}^C y_i \log(\hat{y}_i),~~~\hat{y}_i=\frac{e^{z_i}}{\sum_{k=1}^C e^{z_k}}.
% \end{equation}
% Here, $z_i$ is the output of the noise detector, $y_i$ is the corresponding pseudo-labels, and $C=2$. For $\hat{x}_\text{ID}$, $w=S(\hat{x}_\text{ID})$, and for $\hat{x}_\text{OOD}_j$, $w=1 - S(\hat{x}_\text{OOD})$. That is, $w$ can be regarded as the confidence in pseudo-label. The gradient of $\mathcal{L}$ with respect to $z_i$ is:
% \begin{equation}
% \frac{\partial \mathcal{L}}{\partial z_i} = w \cdot (\hat{y}_i - y_i).
% \end{equation}
% When $w$ is small, it is likely to be an incorrectly labeled sample. $w$ will reduce the gradient's magnitude, diminishing its impact on the noise detector and vice-versa.
% 动态权重调整：通过根据置信度调整每个样本的权重，模型可以更加关注那些它更有信心的预测，同时减少对噪声数据的依赖。
% 减少噪声影响：对于置信度低的样本（即可能的噪声样本），降低其权重可以在损失函数中减少它们的影响，从而有助于提高模型的总体性能和鲁棒性。

% \begin{equation}
%     \begin{aligned}
%     & W := W - \eta \cdot w^2 \cdot \left(\sigma(z) - y\right) \cdot x^T \\
%     & b := b - \eta \cdot w^2 \cdot \left(\sigma(z) - y\right)
%     \end{aligned}
% \end{equation}


% \begin{equation}
%     \mathcal{L} = -\sum_{i=1}^2 y_i \log(\hat{y}_i),~~~\hat{y}_i=\frac{e^{w\cdot z_i}}{e^{w\cdot z_1} + e^{w\cdot z_2}}.
% \end{equation}

% \begin{equation}
% \frac{\partial \mathcal{L}}{\partial z_i} = w \cdot (\frac{e^{w\cdot z_i}}{e^{w\cdot z_1} + e^{w\cdot z_2}} - y_i) = w \cdot (\frac{1}{1 + e^{w\cdot (z_j - z_i)}} - y_i).
% \end{equation}

% \begin{equation}
% \frac{\partial \mathcal{L}}{\partial z_i} = w \cdot (\frac{e^{w\cdot z_i}}{e^{w\cdot z_1} + e^{w\cdot z_2}} - y_i) = \frac{w}{1 + e^{w\cdot (z_j - z_i)}}.
% \end{equation}



% \begin{equation}
%     \mathcal{L}_i = -w_i \cdot \log(\hat{y}_i),~~~\hat{y}_i=\frac{e^{z_i}}{\sum_{k=1}^C e^{z_k}}.
% \end{equation}