\section{Introduction}
Machine learning models suffer performance degradation when the target distribution differs from the source distribution. To mitigate this issue, test-time adaptation (TTA)~\citep{wang2021tent, niu2023towards, wang2022continual, gao2023back, liang2023ttasurvey} has been introduced, aiming to enhance models' generalization to the target distribution in test-time. However, TTA assumes the labels of testing samples are within the in-distribution (ID) label space, which is not practical in an open-world setting~\citep{yang2022openood, yang2021generalized} where models often encounter noisy samples\footnote{Noisy samples refer to data that lie outside the ID label space, whereas clean samples stay within it.}.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{figs/tease.pdf}
\end{center}
\vspace{-.1in}
  \caption{Comparison between TTA, noisy TTA, zero-shot OOD detection, and the proposed zero-shot noisy TTA. Only zero-shot noisy TTA focuses on both clean/noisy classification accuracy and performs in a task-agnostic / zero-shot manner. ZS-NTTA requires online detection of noisy samples. }\label{fig:setting-diff}
  \vspace{-10pt}
\end{figure*}
This paper introduces the \textit{Zero-Shot Noisy TTA}~(ZS-NTTA) setting, which leverages off-the-shelf pre-trained vision-language models~(VLMs)~\citep{radford2021learning} to adapt target data containing noisy samples during test-time in a zero-shot way.
Different from Zero-Shot Out-Of-Distribution ~(ZS-OOD) Detection~\citep{ming2022delving, esmaeilpour2022zero, wang2023clipn}, ZS-NTTA requires detecting noisy samples online and emphasizes classification accuracy more.
Recently, several works~\citep{li2023robustness, gong2023sotta} have explored the challenge of noisy samples in TTA, which require task-specific models that are pre-trained with specific source datasets.
However, \citet{li2023robustness} requires prototypes of the training data, which are unavailable in VLMs. 
On the other hand, \citet{gong2023sotta} focuses solely on the classification of clean data, neglecting the recognition of noisy samples.
The comparison of different settings is illustrated in Figure~\ref{fig:setting-diff}.
\begin{wrapfigure}[22]{r}{0.4\textwidth}
  \begin{center}
    % \vspace{-0.4cm}
    \vspace{-.15in}
    \includegraphics[width=0.4\textwidth]{figs/violin_plot_ranking.pdf}
  \end{center}
  \vspace{-0.4cm}
  \caption{Performance ranking distribution of five TTA methods across $44$ ID-OOD dataset pairs. The ranks of different methods on one ID-OOD pair are ranked according to accuracy $\text{Acc}_\text{H}$. A rank closer to $1$ denotes better performance, and a larger bottom area reflects superior overall performance. We also evaluate these methods using absolute accuracy in Figure~\ref{fig:absolute_acc} in Appendix~\ref{app:failure case}.}
  \label{fig:violin}
\end{wrapfigure}

\vspace{-.15in}
In this paper, we first build the ZS-NTTA benchmarks by leveraging CLIP as the VLM and evaluate the performance of existing TTA methods.
We equip each method with the advanced OOD detection technique~\citep{ming2022delving} and an adaptive threshold to filter out noisy samples.
Figure~\ref{fig:violin} shows the performance rankings of existing methods in ZS-NTTA across $44$ ID-OOD dataset pairs. 
We find the zero-shot CLIP~(ZS-CLIP), which is frozen during adaptation, shows promising performance, particularly in distinguishing between clean and noisy samples. Despite filtering out noisy samples before updating the model, most TTA methods still underperform ZS-CLIP. 

We design three model adaptation pipelines to understand the above phenomenon and analyze the impact of noisy and clean samples on gradients during adaptation.
Our findings reveal that noisy samples commonly lead to much larger gradients, often by an order of magnitude, compared to clean samples. 
Therefore, for methods~\citep{wang2021tent} that continuously optimize the parameters during the adaptation, the model is prone to overfitting to noisy samples.
Furthermore, even for methods~\citep{shu2022test} that reset parameters at each step, their ability to distinguish between clean and noisy samples will be diminished after each update with noisy data.
This underscores the detrimental effect of unfiltered noisy samples on model adaptation, outweighing the benefits of clean samples. 
Moreover, since these TTA methods implement ID classification and noise detection sub-tasks with the adapting classifier, the ability of models to handle both sub-tasks will be significantly reduced.
Thus, we raise a question:
\begin{center}
\textit{How to effectively detect noisy samples to mitigate their negative impacts in test-time adaptation?}
\end{center}
To this end, we propose a novel framework inspired by the above observation, which decouples the classifier and detector with a focus on developing an individual detector while keeping the classifier (including the backbone) frozen. 
This framework offers two key benefits: 1) better distinguishing between noisy and clean samples, and 2) preventing detrimental effects caused by the classifier adapting to noisy samples.
Technically, we propose \textbf{Ada}ptive \textbf{N}oise \textbf{D}etector, termed AdaND. 
Given that ZS-CLIP can effectively distinguish the most clean and noisy samples, we utilize data filtered by ZS-CLIP to train a detector while keeping the rest of the model frozen during the testing phase. 
When encountering clean data streams, the detector tends to misclassify numerous clean samples as noisy ones.
To handle such a situation, we propose intentionally introducing Gaussian noise during adaption, leading to an effective detector that is robust to both clean and noisy scenarios.

AdaND offers several advantages: 1) \textit{Zero-shot}: By leveraging off-the-shelf VLMs, AdaND can accommodate various ID datasets and scale effectively to ImageNet and its variants; 2) \textit{Noise-agnostic}: AdaND can handle a range of noise scenarios, including various types of noisy samples and different noise ratios (including scenario with exactly clean data); 3) \textit{High-performance}: AdaND exhibits superior performance in ZS-NTTA. In addition, AdaND can extend to ZS-OOD detection task and produce state-of-the-art performance; 4) \textit{Low computational overhead}: The computational cost of AdaND is comparable to that of frozen CLIP. 
Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=.1in]
\item We propose a more practical setting, \textit{i.e.}, Zero-Shot Noisy TTA (ZS-NTTA), and build benchmarks for evaluation. 
Based on the built benchmarks, we analyze why adapted methods suffer from performance decline and underperform the model-frozen method in ZS-NTTA~(Sec.~\ref{sec:setting} \& Sec.~\ref{sec:analysis}).


\item We propose AdaND, a simple and effective method that can cover both noisy and clean data streams in ZS-NTTA, offering computational efficiency comparable to model-frozen method~(Sec.~\ref{sec:method}).

\item Our method demonstrates superior performance in both ZS-NTTA and ZS-OOD detection tasks. Notably, in ImageNet, AdaND achieves a $8.32\%$ improvement in $\text{Acc}_\text{H}$ compared to existing TTA methods and a $9.40\%$ improvement in FPR95 over current OOD detection methods~(Sec.~\ref{sec:exp}).
\end{itemize}