\section{Experimental Details}\label{app:exp-details}
\subsection{Dataset Details}\label{app:dataset_details}
The division between ID and OOD datasets in the ZS-NTTA benchmarks is shown in Table~\ref{tab:id-ood-pairs}. Note that the label spaces of the ID and OOD datasets do not overlap. We also report the ratio of class numbers between noisy and clean datasets in Table~\ref{tab:id-ood-pairs-class-ratio}. To avoid label space overlap between ID and OOD datasets, the iNaturalist, SUN, and Places datasets used in our experiments are subsets constructed by~\cite{huang2021mos}.


\begin{table}[ht]
% \vspace{-10pt}
  \caption{ID/OOD Dataset Division. $\checkmark$ indicates an ID-OOD pair, while $\times$ indicates it is not.}
%   \scriptsize
  \label{tab:id-ood-pairs}
  \vspace{5pt}
  \centering
  \resizebox{0.6\linewidth}{!}{%
  \begin{tabular}{l*{6}c}
    \toprule
    ID  & iNaturalist & SUN &  Texture   & Places  & SVHN  & LSUN  \\
  \midrule
    CIFAR-10 & $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
    CIFAR-100& $\times$ & $\times$ & \checkmark & \checkmark & \checkmark & \checkmark \\
    CUB-200-2011&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    STANFORD-CARS&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    Food-101&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    Oxford-IIIT Pet&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    ImageNet&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    ImageNet-K&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    ImageNet-A&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    ImageNet-V2&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
    ImageNet-R&\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & $\times$ \\
  \bottomrule
  \end{tabular}
}
\end{table}


\begin{table}[ht]
  \caption{Number of classes in ID and OOD datasets. Each row shows an ID-OOD dataset pair with their respective number of classes.}
  \label{tab:id-ood-pairs-class-ratio}
  \vspace{5pt}
  \centering
  \resizebox{0.6\linewidth}{!}{%
  \begin{tabular}{l*{6}c}
    \toprule
    ID  & iNaturalist & SUN &  Texture   & Places  & SVHN  & LSUN  \\
  \midrule
    CIFAR-10 & $\times$ & $\times$ & 10:47 & 10:50 & 10:10 & 10:10 \\
    CIFAR-100& $\times$ & $\times$ & 100:47 & 100:50 & 100:10 & 100:1 \\
    CUB-200-2011& 200:110 & 200:50 & 200:47 & 200:50 & $\times$ & $\times$ \\
    STANFORD-CARS& 196:110 & 196:50 & 196:47 & 196:50 & $\times$ & $\times$ \\
    Food-101& 101:110 & 101:50 & 101:47 & 101:50  & $\times$ & $\times$ \\
    Oxford-IIIT Pet& 37:110 & 37:50 & 37:47 & 37:50 & $\times$ & $\times$ \\
    ImageNet& 1000:110 & 1000:50 & 1000:47 & 1000:50 & $\times$ & $\times$ \\
    ImageNet-K& 1000:110 & 1000:50 & 1000:47 & 1000:50 & $\times$ & $\times$ \\
    ImageNet-A& 200:110 & 200:50 & 200:47 & 200:50 & $\times$ & $\times$ \\
    ImageNet-V2& 1000:110 & 1000:50 & 1000:47 & 1000:50 & $\times$ & $\times$ \\
    ImageNet-R& 200:110 & 200:50 & 200:47 & 200:50 & $\times$ & $\times$ \\
  \bottomrule
  \end{tabular}
}
\end{table}





\subsection{Implementation Details}\label{app: baseline methods}
For the ZS-NTTA task, we integrated the advanced OOD detection method, $i.e.,$ MCM~\citep{ming2022delving}, into each comparative approach to filter out noisy samples. 
For the Tent and TPT methods, all hyperparameter settings are kept consistent with their original papers. 
And we use the layer normalization in Tent when the image encoder is ViT-B/16 or ViT-L/14.
For the SoTTA method, considering the generalization across ID datasets and based on the performance of different thresholds in the memory bank, we set the confidence level of the memory bank to $0.5$. For the ZS-OOD detection task, we directly used the results reported in MCM, CLIPN, and NegLabel. Max-Logit and Energy are implemented by ourselves based on CLIP backbone. Additionally, to clearly illustrate our method, we present AdaND in Algorithm~\ref{alg:ours}.


\begin{algorithm}
	\caption{AdaND for ZS-NTTA and ZS-OOD detection tasks.}
	\label{alg:ours}
	\begin{algorithmic}[1]
	    \Require{test data stream $\{x_i\}_{i=1}^{T}$, ID class names $\mathcal{Y}_\text{id}$, text encoder $\mathcal{T}$, image encoder $\mathcal{I}$, noise detector $f$, queue $Q$ with capacity $L$, $K=\text{len}(\mathcal{Y}_\text{id})$, temperature $\tau=0.01$, $M=8$.}
            
            \For{test-time $i \in \{1, \cdots, T\}$}
                \State{Calculate cosine similarity scores:}
                \State{\{$s_k(x_i) \gets \frac{\mathcal{I}(x_i) \cdot \mathcal{T}(t_k)}{\lVert \mathcal{I}(x_i)\rVert \cdot \lVert \mathcal{T}(t_k) \rVert}\}^{\scriptscriptstyle K}_{i=1},~~~t_k \in \mathcal{Y}_\text{id}$}
                \State{Calculate OOD score:}
                \State{$S(x_i) \gets \max_k \frac{e^{s_k(x_i)/\tau}}{\sum_{j=1}^K e^{s_j(x_i)/\tau}}$}
                \State{Calculate $\lambda_\text{ZS-CLIP}$ by Eq.~\ref{eq:adaptive_threshold}}
                
                \If{$S(x_i) > \lambda_\text{ZS-CLIP}$}
                    \State{$y_i^\text{pse} = 1$} \Comment{Pseudo-label: clean sample.}
                \Else
                    \State{$y_i^\text{pse} = -1$} \Comment{Pseudo-label: noisy sample.}
                \EndIf
                
                \State{$\text{logit} = f(\mathcal{I}(x_i))$}
                \State{Update queue $Q$:}
                \State{$Q \gets Q \cup \{\mathcal{I}(x_i), \text{logit}, y_i^\text{pse}\}$}
                \If{$\text{len}(Q) = L$}
                    \State{Train noise detector $f$:}
                    \State{Calculate loss $\mathcal{L}$ using standard CE loss, input data: $Q$}
                    \State{Update $f$ using $\mathcal{L}$}
                    \State{$Q \gets \emptyset$}\Comment{Empty queue $Q$.}
                \EndIf

                \If{$i \bmod M = 0$} \Comment{Gaussian noise injection.}
                    \State{$g \sim \mathcal{N}(0, 1)$}
                    \State{Add noise sample to queue $Q$:}
                    \State{$\text{logit}_{g_i} = f(\mathcal{I}(g))$}
                    \State{$Q \gets Q \cup \{\mathcal{I}(g), \text{logit}_{g}, -1\}$}
                    \If{$\text{len}(Q) = L$}
                        \State{Train noise detector $f$:}
                        \State{Calculate loss $\mathcal{L}$ using standard CE loss, input data: $Q$}
                        \State{Update $f$ using $\mathcal{L}$}
                        \State{$Q \gets \emptyset$}
                    \EndIf
                \EndIf
                \State{Generate output:}
                \If{$i < N$} \Comment{Stage 1: use ZS-CLIP.}
                    \State{output $\gets$ $\arg \max_k \frac{e^{s_k(x_i)/\tau}}{\sum_{j=1}^K e^{s_j(x_i)/\tau}}$ if $S(x_i) > \lambda_\text{ZS-CLIP}$ else $-1$}
                \Else \Comment{Stage 2: use noise detector.}
                    \State{$S(x_i) \gets \max_k \frac{e^{z_k}}{\sum_{j=1}^2 e^{z_j}}$}
                    \State{Calculate $\lambda_\text{AdaND}$ by Eq.~\ref{eq:adaptive_threshold}}
                    \State{output $\gets$ $\arg \max_k \frac{e^{s_k(x_i)/\tau}}{\sum_{j=1}^K e^{s_j(x_i)/\tau}}$ if $S(x_i) > \lambda_\text{AdaND}$ else $-1$}
                \EndIf
            \EndFor
            \Return{output}
	\end{algorithmic}
\end{algorithm}


\subsection{Environment}\label{app:env}
The experiments presented in this paper are conducted utilizing PyTorch 1.13~\citep{paszke2019pytorch} and Python 3.10.8 within an Ubuntu 22.04 LTS environment, running on NVIDIA A100 80GB PCIe GPUs and AMD EPYC 7H12 CPU.