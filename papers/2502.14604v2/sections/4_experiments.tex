% \vspace{-.1in}
\vspace{-5pt}
\section{Experiments}\label{sec:exp}
\vspace{-5pt}
\subsection{Setups}\label{sec:exp-setup}
\vspace{-5pt}
\paragraph{Compared Methods and Evaluation Metrics.} 
We compare our method with existing TTA methods mentioned in Sec.~\ref{sec:benchmark-methods} on the ZS-NTTA task using $11$ ID datasets from Sec.~\ref{sec:analysis-dataset}, evaluating with $\text{Acc}_\text{S}$, $\text{Acc}_\text{N}$, and $\text{Acc}_\text{H}$.
Additionally, we compare with leading OOD detection methods on the ZS-OOD task, including Energy\citep{liu2020energy}, MaxLogit~\citep{hendrycks2019scaling}, MCM~\citep{ming2022delving}, CLIPN~\citep{wang2023clipn}, and NegLabel~\citep{jiang2024neglabel}, using AUROC and FPR95 as metrics.
Please see Appendix~\ref{app: baseline methods} for the implementation details of compared methods.



\vspace{-10pt}
\paragraph{AdaND Setups.}
In our main results, we maintain consistent hyper-parameters across all datasets. 
Specifically, we use CLIP~\citep{radford2021learning} as our VLM, with ViT-B/16~\citep{dosovitskiy2020image} as the image encoder and masked self-attention Transformer~\citep{vaswani2017attention} as the text encoder, both keeping frozen. 
We employ a single linear layer as our noise detector, which remains learnable throughout the TTA process. 
We optimize with Adam~\citep{kingma2014adam}, using a learning rate of $0.0005$ and no weight decay. 
Gaussian noise is injected every $8$ samples ($M=8$). The noise detector’s queue length ($L$) is set to $128$, and the adaptive threshold’s queue length ($N_q$) follows OWTTT~\citep{li2023robustness} with a value of $512$. 
We use $N=10$ for the first stage. As for the ZS-OOD detection task, we use MCM~\citep{ming2022delving} score from the output logit of the noise detector as our score function. Unless otherwise specified, we set the batch size ($bs$) to $1$ for AdaND.

\vspace{-3pt}
\subsection{Main Results}
\vspace{-2pt}
\paragraph{Zero-Shot Noisy TTA Task.}
\begin{table}[t]
\caption{Zero-shot noisy TTA results for CUB-200-2011, STANFORD-CARS, Food-101, and Oxford-IIIT Pet as the ID datasets. The \textbf{bold} indicates the best performance on each dataset.
}\label{tab:bird200}
\vspace{-5pt}
\centering{
\setlength\tabcolsep{5pt} 
\resizebox{\linewidth}{!}{
\begin{tabular}{ll*{15}{c}}
\toprule
\multirow{2}*{ID}&\multirow{2}*{Method}&\multicolumn{3}{c}{iNaturalist}&\multicolumn{3}{c}{SUN}&\multicolumn{3}{c}{Texture}&\multicolumn{3}{c}{Places}&\multicolumn{3}{c}{Avg}\\
\cmidrule{3-17}
&&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$\\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}\cmidrule(lr){15-17}
\multirow{5}*{CUB-200-2011}
& ZS-CLIP
& 38.13 & 88.06 & 53.22 & 38.10 & 87.86 & 53.15 & 37.56 & 79.11 & 50.94 & 38.00 & 87.81 & 53.04 & 37.95 & 85.71 & 52.59\\
& Tent
& 37.02 & 46.95 & 41.40 & 38.61 & 55.55 & 45.56 & 34.98 & 41.77 & 38.07 & 40.41 & 74.83 & 52.48 & 37.75 & 54.78 & 44.38\\
& SoTTA
& 41.67 & 84.37 & 55.79 & 42.08 & 86.83 & 56.69 & 41.44 & 77.58 & 54.02 & 42.04 & 86.52 & 56.59 & 41.81 & 83.82 & 55.77\\
& TPT
& 37.41 & 89.57 & 52.78 & 37.49 & 89.67 & 52.87 & 36.88 & \textbf{81.67} & 50.81 & 37.44 & 89.45 & 52.79 & 37.30 & 87.59 & 52.31\\
\rowcolor{gray!22}
\cellcolor{white}
& AdaND~(Ours)
& \textbf{52.34} & \textbf{96.40} & \textbf{67.84} & \textbf{52.41} & \textbf{93.91} & \textbf{67.27} & \textbf{51.82} & 81.24 & \textbf{63.28} & \textbf{51.82} & \textbf{91.51} & \textbf{66.17} & \textbf{52.10} & \textbf{90.77} & \textbf{66.14}\\

\midrule

\multirow{5}*{STANFORD-CARS}
& ZS-CLIP
& 50.18 & 96.62 & 66.05 & 53.48 & 98.81 & 69.40 & 53.59 & 99.05 & 69.55 & 53.36 & 98.05 & 69.11 & 52.65 & 98.13 & 68.53\\
& Tent
& 44.12 & 52.33 & 47.88 & 54.27 & 94.51 & 68.95 & 54.60 & 97.37 & 69.97 & 54.33 & 96.65 & 69.56 & 51.83 & 85.22 & 64.09\\
& SoTTA
& 51.51 & 92.84 & 66.26 & 54.81 & 97.57 & 70.19 & 55.06 & 98.50 & 70.64 & 54.75 & 96.96 & 69.98 & 54.03 & 96.47 & 69.27\\
& TPT
& 49.24 & 96.97 & 65.31 & 52.40 & 98.83 & 68.49 & 52.75 & 99.27 & 68.89 & 52.42 & 98.39 & 68.40 & 51.70 & 98.36 & 67.77\\
\rowcolor{gray!22}
\cellcolor{white}
& AdaND~(Ours)
& \textbf{62.80} & \textbf{99.79} & \textbf{77.09} & \textbf{62.73} & \textbf{99.82} & \textbf{77.04} & \textbf{62.91} & \textbf{99.75} & \textbf{77.16} & \textbf{62.76} & \textbf{99.29} & \textbf{76.91} & \textbf{62.80} & \textbf{99.66} & \textbf{77.05}\\
\midrule

\multirow{5}*{Food-101}
& ZS-CLIP
& 80.60 & 94.76 & 87.11 & 80.75 & 96.08 & 87.75 & 80.51 & 93.12 & 86.36 & 80.62 & 94.62 & 87.06 & 80.62 & 94.65 & 87.07\\
& Tent
& 75.83 & 25.09 & 37.70 & 82.86 & 85.10 & 83.97 & 82.54 & 87.03 & 84.73 & 82.26 & 80.13 & 81.18 & 80.87 & 69.34 & 71.90\\
& SoTTA
& 81.84 & 84.09 & 82.95 & 82.49 & 93.34 & 87.58 & 82.05 & 90.10 & 85.89 & 82.44 & 91.62 & 86.79 & 82.20 & 89.79 & 85.80\\
& TPT
& 79.70 & 94.93 & 86.65 & 79.92 & 96.19 & 87.30 & 79.70 & 93.86 & 86.20 & 79.76 & 95.14 & 86.77 & 79.77 & 95.03 & 86.73\\
\rowcolor{gray!22}
\cellcolor{white}
& AdaND~(Ours)
& \textbf{86.50} & \textbf{99.87} & \textbf{92.71} & \textbf{86.40} & \textbf{99.64} & \textbf{92.55} & \textbf{86.44} & \textbf{96.51} & \textbf{91.20} & \textbf{86.42} & \textbf{99.40} & \textbf{92.46} & \textbf{86.44} & \textbf{98.85} & \textbf{92.23}\\

\midrule

\multirow{5}*{Oxford-IIIT Pet}
& ZS-CLIP
& 78.58 & 88.30 & 83.16 & 79.75 & 87.30 & 83.35 & 80.20 & 91.16 & 85.33 & 79.59 & 84.17 & 81.82 & 79.53 & 87.73 & 83.41\\
& Tent
& 80.07 & 78.09 & 79.07 & 81.19 & 68.30 & 74.19 & 81.48 & 74.72 & 77.95 & 80.64 & 62.51 & 70.43 & 80.84 & 70.91 & 75.41\\
& SoTTA
& 80.07 & 83.54 & 81.77 & 81.78 & 83.83 & 82.79 & 82.09 & 87.52 & 84.72 & 81.49 & 81.25 & 81.37 & 81.36 & 84.03 & 82.66\\
& TPT
& 77.56 & 89.71 & 83.19 & 78.87 & 89.82 & 83.99 & 79.17 & 92.26 & 85.22 & 78.62 & 87.32 & 82.74 & 78.56 & 89.78 & 83.78\\
\rowcolor{gray!22}
\cellcolor{white}
& AdaND~(Ours)
& \textbf{85.81} & \textbf{98.78} & \textbf{91.84} & \textbf{85.82} & \textbf{98.19} & \textbf{91.59} & \textbf{85.86} & \textbf{98.68} & \textbf{91.82} & \textbf{85.88} & \textbf{96.58} & \textbf{90.92} & \textbf{85.84} & \textbf{98.06} & \textbf{91.54}\\
\bottomrule
\end{tabular}}
}
\vspace{-15pt}
\end{table}

\begin{table}[t]
\caption{Zero-shot noisy TTA results for ImageNet and its variants as the ID datasets. The \textbf{bold} indicates the best performance on each dataset.
}\label{tab:imagenet}
\centering{
\setlength\tabcolsep{5pt} 
\resizebox{\linewidth}{!}{
\begin{tabular}{ll*{15}{c}}
\toprule
\multirow{2}*{ID}&\multirow{2}*{Method}&\multicolumn{3}{c}{iNaturalist}&\multicolumn{3}{c}{SUN}&\multicolumn{3}{c}{Texture}&\multicolumn{3}{c}{Places}&\multicolumn{3}{c}{Avg}\\
\cmidrule{3-17}
&&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$\\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}\cmidrule(lr){9-11}\cmidrule(lr){12-14}\cmidrule(lr){15-17}
\multirow{5}*{ImageNet}
& ZS-CLIP
& 54.01 & 86.53 & 66.51 & 53.43 & 83.96 & 65.30 & 52.71 & 78.52 & 63.08 & 53.35 & 80.50 & 64.17 & 53.38 & 82.38 & 64.77\\
& Tent
& 48.56 & 35.74 & 41.18 & 55.44 & 75.54 & 63.95 & 54.94 & 70.93 & 61.92 & 55.76 & 73.98 & 63.59 & 53.67 & 64.05 & 57.66\\
& SoTTA
& 53.15 & 62.68 & 57.52 & 53.16 & 68.76 & 59.96 & 53.64 & 68.05 & 59.99 & 53.60 & 69.16 & 60.39 & 53.39 & 67.16 & 59.47\\
& TPT
& 52.58 & 88.93 & 66.09 & 51.91 & 86.09 & 64.77 & 51.11 & 80.01 & 62.38 & 51.80 & 82.89 & 63.76 & 51.85 & 84.48 & 64.25\\
\rowcolor{gray!22}\cellcolor{white}
& AdaND~(Ours)
& \textbf{63.26} & \textbf{96.87} & \textbf{76.54} & \textbf{61.34} & \textbf{89.44} & \textbf{72.77} & \textbf{62.45} & \textbf{83.54} & \textbf{71.47} & \textbf{61.92} & \textbf{84.82} & \textbf{71.58} & \textbf{62.24} & \textbf{88.67} & \textbf{73.09}\\

\midrule

\multirow{5}*{ImageNet-K}
& ZS-CLIP
& 34.17 & 83.46 & 48.49 & 33.46 & 81.20 & 47.39 & 32.61 & 75.57 & 45.56 & 33.40 & 77.10 & 46.61 & 33.41 & 79.33 & 47.01\\
& Tent
& 30.46 & 26.86 & 28.55 & 36.57 & 71.82 & 48.46 & 36.37 & 66.63 & 47.06 & 36.87 & 70.32 & 48.38 & 35.07 & 58.91 & 43.11\\
& SoTTA
& 36.18 & 61.70 & 45.61 & 36.28 & 67.19 & 47.12 & 35.91 & 65.31 & 46.34 & 36.57 & 67.09 & 47.34 & 36.23 & 65.32 & 46.60\\
& TPT
& 32.16 & 86.52 & 46.89 & 31.55 & 83.86 & 45.85 & 30.74 & \textbf{77.39} & 44.00 & 31.56 & 80.05 & 45.27 & 31.50 & 81.95 & 45.50\\
\rowcolor{gray!22}\cellcolor{white}
& AdaND~(Ours)
& \textbf{40.97} & \textbf{93.54} & \textbf{56.98} & \textbf{40.25} & \textbf{85.06} & \textbf{54.64} & \textbf{38.31} & 74.43 & \textbf{50.58} & \textbf{39.60} & \textbf{79.57} & \textbf{52.88} & \textbf{39.78} & \textbf{83.15} & \textbf{53.77}\\

\midrule

\multirow{5}*{ImageNet-A}
& ZS-CLIP
& 34.73 & 80.69 & 48.56 & 34.20 & 78.83 & 47.70 & 33.97 & 76.60 & 47.07 & 33.96 & 75.11 & 46.77 & 34.22 & 77.81 & 47.53\\
& Tent
& 34.99 & 77.19 & 48.15 & 34.83 & 77.05 & 47.97 & 34.36 & 75.19 & 47.17 & 34.60 & 73.83 & 47.12 & 34.70 & 75.81 & 47.60\\
& SoTTA
& 36.85 & 76.83 & 49.81 & 36.47 & 77.08 & 49.51 & 35.60 & 75.37 & 48.36 & 36.07 & 73.87 & 48.47 & 36.25 & 75.79 & 49.04\\
& TPT
& 34.12 & 81.17 & 48.04 & 33.20 & 80.23 & 46.97 & 33.12 & 79.92 & 46.83 & 33.05 & \textbf{77.00} & 46.25 & 33.37 & 79.58 & 47.02\\
\rowcolor{gray!22}\cellcolor{white}
& AdaND~(Ours)
& \textbf{43.59} & \textbf{91.19} & \textbf{58.98} & \textbf{41.96} & \textbf{80.93} & \textbf{55.27} & \textbf{45.04} & \textbf{79.97} & \textbf{57.62} & \textbf{42.85} & 72.13 & \textbf{53.76} & \textbf{43.36} & \textbf{81.06} & \textbf{56.41}\\

\midrule

\multirow{5}*{ImageNet-V2}
& ZS-CLIP
& 48.01 & 85.72 & 61.55 & 47.37 & 83.23 & 60.38 & 46.81 & 77.54 & 58.38 & 47.39 & \textbf{79.41} & 59.36 & 47.39 & 81.47 & 59.92\\
& Tent
& 47.94 & 76.98 & 59.08 & 48.28 & 80.50 & 60.36 & 47.56 & 74.47 & 58.05 & 48.34 & 77.37 & 59.50 & 48.03 & 77.33 & 59.25\\
& SoTTA
& 48.24 & 78.59 & 59.78 & 47.80 & 78.67 & 59.47 & 47.27 & 74.82 & 57.94 & 48.26 & 76.05 & 59.05 & 47.89 & 77.03 & 59.06\\
& TPT
& 46.63 & 88.37 & 61.05 & 46.12 & 85.58 & 59.94 & 45.21 & 79.14 & 57.55 & 46.02 & 81.95 & 58.94 & 46.00 & 83.76 & 59.37\\
\rowcolor{gray!22}\cellcolor{white}
& AdaND~(Ours)
& \textbf{56.32} & \textbf{97.06} & \textbf{71.28} & \textbf{54.78} & \textbf{86.64} & \textbf{67.12} & \textbf{57.28} & \textbf{80.61} & \textbf{66.97} & \textbf{55.81} & 79.24 & \textbf{65.49} & \textbf{56.05} & \textbf{85.89} & \textbf{67.72}\\

\midrule

\multirow{5}*{ImageNet-R}
& ZS-CLIP
& 61.99 & 94.39 & 74.83 & 61.82 & 88.95 & 72.94 & 60.91 & 77.05 & 68.04 & 61.68 & 84.86 & 71.44 & 61.60 & 86.31 & 71.81\\
& Tent
& 65.22 & 91.45 & 76.14 & 65.06 & 85.61 & 73.93 & 63.33 & 69.99 & 66.49 & 64.93 & 82.38 & 72.62 & 64.64 & 82.36 & 72.30\\
& SoTTA
& 66.78 & 86.98 & 75.55 & 66.71 & 83.99 & 74.36 & 65.92 & 72.69 & 69.14 & 66.60 & 80.53 & 72.91 & 66.50 & 81.05 & 72.99\\
& TPT
& 60.95 & 94.80 & 74.20 & 60.85 & 89.98 & 72.60 & 59.98 & 77.79 & 67.73 & 60.67 & 85.79 & 71.08 & 60.61 & 87.09 & 71.40\\
\rowcolor{gray!22}\cellcolor{white}
& AdaND~(Ours)
& \textbf{72.21} & \textbf{99.59} & \textbf{83.72} & \textbf{71.02} & \textbf{95.94} & \textbf{81.62} & \textbf{70.44} & \textbf{81.43} & \textbf{75.54} & \textbf{70.85} & \textbf{92.14} & \textbf{80.10} & \textbf{71.13} & \textbf{92.28} & \textbf{80.25}\\
\bottomrule
\end{tabular}}
}
\vspace{-5pt}
\end{table}

Table~\ref{tab:bird200} and Table~\ref{tab:imagenet} present a detailed comparison of ZS-NTTA task results across various ID datasets.
On ImageNet, AdaND enhances the average performance by $8.32\%$ in terms of $\text{Acc}_\text{H}$.
Although we filter the data using the MCM score and adaptive threshold, a considerable portion of noisy data remains unfiltered. 
Consequently, when Tent leverages the filtered data to update the model's normalization layers, it inadvertently causes a substantial performance decline.
SoTTA improves data selection by focusing on the highest confidence samples, slightly outperforming ZS-CLIP on some datasets, but the gains are limited.
Despite TPT resetting the model before each sample input, the unfiltered noisy data causes TPT to perform worse than ZS-CLIP on most ID datasets. 
Since our method decouples the classifier and detector, which focuses on developing the noise detector and keeping the classifier frozen, our AdaND can better identify noisy samples and prevent unfiltered ones from affecting the classifier.
Due to space constraints, the results for CIFAR-10/100 are provided in Table~\ref{tab:CIFAR} in Appendix~\ref{app:main-cifar}.
In summary, our AdaND demonstrates superior performance over the compared methods, achieving the best results across all datasets. 


\begin{table}[t]
\caption{Runtime and GPU memory with varying batch sizes ($bs$) on ImageNet for a single sample.
}\label{tab:runtime}
\centering{
\setlength\tabcolsep{5pt} 
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccc|ccc}
\toprule
Resource & ZS-CLIP~($bs=1$) & SoTTA~($bs=1$)  & TPT~($bs=1$) & Ours~($bs=1$) & ZS-CLIP~($bs=128$) & Tent~($bs=128$) & Ours~($bs=128$) \\
\midrule
Time~(s)$\downarrow$
&0.1125 & 0.1193& 0.3219&0.1272 & 0.0015 & 0.0037 & 0.0017\\
Memory~(GiB)$\downarrow$ 
&3.80 & 9.13 &21.23 &3.83  & 4.54 &14.99 &4.57 \\
\bottomrule
\end{tabular}}
}
\vspace{-10pt}
\end{table}

\begin{table}[t]
\caption{Zero-shot OOD detection results for ImageNet as the ID dataset. The \textbf{bold} indicates the best.
}\label{tab:ood-detection}
\vspace{-5pt}
\centering{
\setlength\tabcolsep{5pt} 
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}*{Method}&\multicolumn{2}{c}{iNaturalist}&\multicolumn{2}{c}{SUN}&\multicolumn{2}{c}{Texture}&\multicolumn{2}{c}{Places}&\multicolumn{2}{c}{Avg}\\
\cmidrule{2-11}
&AUROC$\uparrow$&FPR95$\downarrow$&AUROC$\uparrow$&FPR95$\downarrow$&AUROC$\uparrow$&FPR95$\downarrow$&AUROC$\uparrow$&FPR95$\downarrow$&AUROC$\uparrow$&FPR95$\downarrow$\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}\cmidrule(lr){10-11}
Max-Logit
& 89.31 & 61.66 & 87.43 & 64.39 & 71.68 & 86.61 & 85.95 & 63.67 & 83.59  & 69.08\\
Energy
& 85.09 & 81.08 & 84.24 & 79.02 & 65.56 & 93.65 & 83.38 & 75.08 & 79.57  & 82.21\\
MCM
& 94.61 & 30.91 & 92.57 & 37.59 & 86.11 & 57.77 & 89.77 & 44.69 & 90.77  & 42.74\\
CLIPN
& 95.27 & 23.94 & 93.93 & 26.17 & 90.93 & 40.83 & 92.28 & 33.45 & 93.10 & 31.10\\
NegLabel
&\textbf{99.49} &\textbf{1.91} &95.49 &20.53 &90.22 &43.56 &91.64 &35.59 &94.21 &25.40\\
\rowcolor{gray!22}
AdaND~(Ours)
& 98.91 & 4.19 & \textbf{95.86} & \textbf{17.08} & \textbf{93.01} & \textbf{21.76} & \textbf{94.55} & \textbf{20.95} & \textbf{95.58}  & \textbf{16.00}\\
\bottomrule
\end{tabular}}
}
\vspace{-5pt}
\end{table}


\begin{table}[!t]
\caption{Ablation studies for each module in the method using ImageNet as the ID dataset. Results in noisy data stream are averaged over four OOD datasets: iNaturalist, SUN, Texture, and Places. `$\times$' indicates the exclusion of a module and `$\checkmark$' indicates inclusion of a module.}\label{tab:ablation-module}
\vspace{-5pt}
\centering{
\setlength\tabcolsep{10pt} 
\fontsize{8}{7}\selectfont
\begin{tabular}{cc*{6}{c}}
\toprule
\multirow{2}*{Noise Detector}&\multirow{2}*{Gaussian Noise}&\multicolumn{3}{c}{Clean Data Stream}&\multicolumn{3}{c}{Noisy Data Stream}\\
\cmidrule{3-8}
&&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$&$\text{Acc}_\text{S}$&$\text{Acc}_\text{N}$&$\text{Acc}_\text{H}$\\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}
$\times$ & $\times$
& 47.68 & - & -  & 53.38 & 82.38 & 64.77\\
$\times$ & $\checkmark$
& 50.07 & - & -  & 53.95 & 81.65 & 64.95\\
$\checkmark$ & $\times$
& 37.54 & - & -  & 60.64 & \textbf{91.73} & 73.00\\
\rowcolor{gray!22}
$\checkmark$ & $\checkmark$
& \textbf{63.96} & - & -  & \textbf{62.24} & 88.67 & \textbf{73.09}\\
\bottomrule
\end{tabular}}
\vspace{-15pt}
\end{table}

Table~\ref{tab:runtime} shows the time and computational resources required to test a single sample on the ImageNet. All comparisons were conducted on the same 80G A800 GPU. We tested $6,400$ samples and then averaged the results to ensure result stability.
Since our method freezes the VLM and uses only a single linear layer for the noise detector, our time consumption is nearly equivalent to ZS-CLIP. 
Tent's result is reported with $bs=128$, as performance drops significantly at $bs=1$ (See Table~\ref{app-tab:tent-bs1}).
TPT consumes the most time and memory due to its $64$-fold data augmentation and gradient backpropagation through the entire text encoder.
Our method proves to be more resource-efficient than Tent, SoTTA, and TPT.



\vspace{-5pt}
\paragraph{Zero-Shot OOD Detection Task.} 
The results for ZS-OOD detection are presented in Table~\ref{tab:ood-detection}, using ImageNet as the ID dataset. Our approach demonstrates competitive performance compared to state-of-the-art OOD detection methods, with significant improvements of $1.37\%$ in AUROC and $9.40\%$ in FPR95.
Notably, CLIPN requires an additional dataset to train a text encoder, and NegLabel needs to mine extra semantic information from a large-scale corpus database. In contrast, our method requires no additional external data, making it simpler and more efficient.
The results indicate that learning an adaptive noise detector is a simple yet effective strategy for ZS-OOD detection task.



\vspace{-5pt}
\subsection{Ablation Studies}
\vspace{-5pt}
\paragraph{The Effectiveness of Noise Detector and Injected Noise.}
We evaluate the effectiveness of the noise detector and Gaussian noise modules under both clean and noisy data streams using ImageNet as the ID dataset, as shown in Table~\ref{tab:ablation-module}.
Without Gaussian noise, the noise detector alone is ineffective for clean data streams.
When the noise detector is not present, the performance in noisy data streams decreases significantly. Our full method is both effective under clean and noisy data streams, demonstrating the soundness of our design. 
Results for other ID datasets are in Table~\ref{app-tab:ablation-module}. 


\vspace{-5pt}
\paragraph{Intentionally Injected Noise in AdaND.}
We conduct ablation studies on intentionally injected noise from two aspects: noise types (Gaussian, Uniform, Salt-and-pepper, Poisson) and injection frequency (every $2$, $4$, or $8$ test samples). As shown in Table~\ref{app-tab:CIFAR-inject-noise}, 
all noise types effectively manage both clean and noisy data streams,  demonstrating that our method is robust to the choice of injected noise. 
Table~\ref{app-tab:gaussian-rate} shows the results for noise injection frequency using Gaussian noise.
Our experiments show that injecting Gaussian noise every $2$, $4$, or $8$ samples yields excellent performance. Considering efficiency and performance, we choose to inject Gaussian noise every $8$ samples.

\vspace{-10pt}
\paragraph{Simulating Real-world Adaptation.} 
We simulate real-world adaptation by mixing ID and OOD datasets from two perspectives. The first involves varying noise ratios ($0\%, 25\%, 50\%, 75\%$)  to mimic real-world conditions. The second considers the order of ID and OOD samples, which we simulate using different random seeds. Table~\ref{app-tab:rate-of-noise} presents the results for data streams with varying noise ratios. Since we cannot assume prior knowledge of whether a data stream is clean or contains noisy samples, all methods retain an adaptive OOD detection threshold module. As a result, comparative methods exhibit significant performance degradation on clean data streams. In contrast, our method, which deliberately injects Gaussian noise, effectively addresses clean data streams. Moreover, as the proportion of noise in the data stream increases, most comparative methods show a marked decline in performance, whereas our method continues to deliver strong results across different noise ratios. The experimental results for different random seeds are provided in Table~\ref{app-tab:CIFAR-seed} and Table~\ref{app-tab:CIFAR-seed-auroc}. Due to computational constraints, we only conduct experiments using CIFAR-10 and CIFAR-100 as ID datasets, with random seeds ranging from $0$ to $4$. The results demonstrate that our method consistently achieves superior performance, regardless of the input order of the data streams.



\vspace{-10pt}
\paragraph{Hyper-parameters Selection in AdaND.}
% We conduct ablation experiments to explain how hyper-parameters are selected in AdaND. 
We conducted ablation experiments in AdaND with varying queue capacities $L$ ($32, 64, 128, 256, 512$). 
As shown in Table~\ref{app-tab:CIFAR-l}, our method demonstrates insensitivity to the choice of $L$, and we selected $L=128$ for the main experiments
Table~\ref{app-tab:CIFAR-nq} presents the ablation studies on the queue length $N_q$, which is used to update the score distribution for determining the adaptive threshold.
Similar to $L$, AdaND is also robust to changes in $N_q$, and following OWTTT, we set $N_q=512$. 
The results for different values of $N$ are shown in Table~\ref{app-tab:CIFAR-init-steps}. We found that $N=10$ optimization steps are sufficient to initialize the noise detector. 
In summary, AdaND exhibits low sensitivity to hyper-parameter selection, allowing us to use consistent hyper-parameter settings across all datasets, which yields the best results compared to other methods.


We explore the performance of using different backbones in Table~\ref{app-tab:CIFAR-vlm}. Our AdaND is significantly better than the other methods when using different backbones. We also discuss using pseudo-labels generated by the noise detector as pseudo-labels in Appendix~\ref{app:sec-noisy-detector-output}. Using noise detector outputs as pseudo-labels can improve performance on some datasets but cause intolerable drops in others.
