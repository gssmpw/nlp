% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[]{acl}
\definecolor{darkred}{rgb}{0.8, 0.0, 0.0}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{seqsplit}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{float}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{color}
\usepackage{xcolor, colortbl}
\usepackage[most]{tcolorbox}
\usepackage{soul}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\lz}[1]{\textcolor{blue}{{#1}}}
\newcommand{\green}[1]{\textcolor{green}{{#1}}}
\newcommand{\red}[1]{\textcolor{red}{{#1}}}
% \newcommand{\wanlong}[1]{{ \color{teal} [Wanlong says ``#1'']}}



\newcommand{\mycolorbox}[2]{%
  \colorbox{#1}{\raisebox{0pt}[8pt][0pt]{#2}}%
}

\definecolor{lightpink}{rgb}{0.945, 0.816, 0.804}
\definecolor{lightgreen}{rgb}{0.851, 0.906, 0.839}
\definecolor{lightblue}{rgb}{0.8, 0.9, 1}
\definecolor{lightyellow}{rgb}{0.992, 0.949, 0.816}

\newtcolorbox[list inside=prompt,auto counter,number within=section]{prompt}[1][]{
    colbacktitle=black!60,
    coltitle=white,
    fontupper=\footnotesize,
    boxsep=5pt,
    breakable,
    enhanced,
    left=0pt,
    right=0pt,
    top=0pt,
    bottom=0pt,
    boxrule=1pt,
    #1   
}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Large Language Models Penetration in Scholarly Writing and Peer Review}

\newcommand{\cuhksz}{$^1$}
\newcommand{\uestc}{$^2$}
\newcommand{\whut}{$^3$}
\newcommand{\ku}{$^4$}
\newcommand{\sribd}{$^5$}



% Li, Ruijie, Xunlian, Daniel, Haizhou

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}
% Li Zhou\cuhksz, Ruijie Zhang\uestc, Xunlian Dai\whut, Daniel Hershcovich\ku, Haizhou Li\cuhksz$^,$\sribd
\author{
 \textbf{Li Zhou\cuhksz},
 \textbf{Ruijie Zhang\uestc},
 \textbf{Xunlian Dai\whut},
 \textbf{Daniel Hershcovich\ku},
 \textbf{Haizhou Li\cuhksz$^,$\sribd}
\\
{\cuhksz}The Chinese University of Hong Kong, Shenzhen \\ 
{\uestc}University of Electronic Science and Technology of China \\ 
{\whut}Wuhan University of Technology {\ku}University of Copenhagen \\
{\sribd}Shenzhen Research Institute of Big Data\\
 \small{
   % \textbf{Correspondence:} 
   \texttt{lizhou21@cuhk.edu.cn}
 }
}

\begin{document}
\maketitle
\begin{abstract}

While the widespread use of Large Language Models (LLMs) brings convenience, it also raises concerns about the credibility of academic research and scholarly processes. To better understand these dynamics, we evaluate the penetration of LLMs across academic workflows from multiple perspectives and dimensions, providing compelling evidence of their growing influence. We propose a framework with two components: \texttt{ScholarLens}, a curated dataset of human-written and LLM-generated content across scholarly writing and peer review for multi-perspective evaluation, and \texttt{LLMetrica}, a tool for assessing LLM penetration using rule-based metrics and model-based detectors for multi-dimensional evaluation. Our experiments demonstrate the effectiveness of \texttt{LLMetrica}, revealing the increasing role of LLMs in scholarly processes. These findings emphasize the need for transparency, accountability, and ethical practices in LLM usage to maintain academic credibility.

% We propose a framework with two components: \texttt{ScholarLens}, a curated dataset for technical measurement and development, includes both human-written and LLM-generated content across scholarly writing and peer review, enabling multi-perspective evaluation. and \texttt{LLMetrica}, a tool to assess LLM penetration, combines rule-based metrics to capture linguistic and semantic features with model-based detectors to identify LLM-generated content, supporting multi-dimensional evaluation.
% \texttt{ScholarLens} includes both human-written and LLM-generated content across scholarly writing and peer review, enabling multi-perspective evaluation. 
% \texttt{LLMetrica} combines rule-based metrics to capture linguistic and semantic features with model-based detectors to identify LLM-generated content, supporting multi-dimensional evaluation.


% Specifically, We propose a framework with two key components: \texttt{ScholarLens}, a curated dataset for developing technical measurement methods, and \texttt{LLMetrica} (\S\ref{sec: LLMetria}), a set of metrics to assess LLM penetration in scholarly workflows.
% \texttt{ScholarLens} includes both human-written and LLM-generated content, considering both scholarly writing and peer review, providing a 
% foundation for multiple perspectives evaluation; 
% \texttt{LLMetrica} combines rule-based metrics to capture linguistic and semantic feature preference of LLM-generated text, along with training scholar-specific model-based detectors to better identify whether content is LLM-generated. 

% While the widespread use of Large Language Models (LLMs) brings convenience, it also raises concerns about the credibility of academic research and scholarly processes.
% In this paper, we quantify the increasing penetration of LLMs across various academic workflows, examining their influence from multiple perspectives.
% 区分LLM 辅助角色的重要性


% The rapid advancements in Large Language Models (LLMs) have significantly impacted scholars, influencing activities ranging from research authorship to peer review. This has raised concerns among key figures, including journal editors, conference chairs, and authors.
% Understanding these dynamics is crucial for addressing risks to academic integrity and developing strategies for the ethical and effective use of LLMs. 
% In this paper, we quantify recent trends in LLM penetration across various aspects of academia, exploring their impact from multiple dimensions.
% Specifically, we construct a comprehensive dataset \texttt{ScholarLens} that includes both human- and LLM-written content, sourced from both author and reviewer perspectives.
% Additionally, we propose a set of quantifiable metrics \texttt{LLMetrica} for evaluating LLM penetration, incorporating interpretable measures alongside trained LLM detection models.
% By leveraging \texttt{ScholarLens} and \texttt{LLMetrica}, we present a structured framework for evaluating LLM penetration,  revealing notable trends in the increasing adoption of LLMs, highlighting areas of concern and offering actionable strategies for managing their integration. 


% The emergence of large language models (LLMs), particularly ChatGPT, has significantly influenced academic writing, necessitating an exploration of their impact on scholarly communication.
% we focus on three aspects about academic domain: title generation, abstract polish, and meta-review creation.
% We develop \lz{a comprehensive evaluation suite} and an \lz{AIGC detection tool} to analyze academic content produced before and after the introduction of ChatGPT. 
% \lz{We find that ……}
% \lz{These insights underscore……}
% These insights underscore the profound significance of establishing frameworks for editors and chairs to monitor AI engagement, ensuring the integrity and quality of academic discourse in an increasingly automated landscape.

\end{abstract}


\input{section/01-introduction}
\input{section/02-related-works}
\input{section/03-scholarlens}
\input{section/04-llmetrica}
\input{section/05-experiments}
\input{section/06-conclusion}
\input{section/07-limitations}
% \section*{Acknowledgments}



% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom, anthology}

\appendix

\input{section/08-1-appendix-prompt}
\input{section/08-2-appendix-dataset}
\input{section/08-3-appendix-general}
\input{section/08-4-appendix-case-study}

\end{document}
