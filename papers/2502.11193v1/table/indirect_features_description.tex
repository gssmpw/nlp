\begin{table*}[ht]
\centering
\footnotesize 
\begin{tabular}{@{}l|p{0.4\textwidth}|p{0.4\textwidth}@{}}
\toprule
\textbf{Relatedness}         & \textbf{Description} & \textbf{Computing Method} \\ \midrule
\textbf{bert score}      
 &BERT score is a metric that evaluates the alignment between two texts by comparing their contextual embeddings. It compares words in the candidate text with words in the reference text one by one, accumulating word vector similarities to get an overall similarity score. Precision, Recall, and F1 scores are calculated separately. We compute the BERT score between the meta-review and the three reviews, then compute the average and variance to measure their similarity.  &The BERT score between two texts is typically calculated using the cosine similarity of their contextual embeddings. Given a reference text R(here is review) and a candidate text C(here is meta-review), BERT score evaluates Precision, Recall, and F1 metrics as follows:  
 
\text{Precision (P)}

$P = \frac{1}{|C|} \sum_{c \in C} \max_{r \in R} \text{cos\_sim}(c, r)$ 
\medskip

\text{Recall (R)}

$R = \frac{1}{|R|} \sum_{r \in R} \max_{c \in C} \text{cos\_sim}(r, c)$ 
\medskip

\text{F1 Score}

$F1 = \frac{2 \cdot P \cdot R}{P + R}$ 
 \\ 
\textbf{sentence bert}       & Sentence-BERTScore is a metric that evaluates the alignment between two texts by comparing their contextual embeddings, optimized for sentence-level similarity. It encodes each sentence or short text into a single vector for the whole sentence, then calculates the similarity between two sentence vectors (with cosine similarity) to measure how similar the sentences are. We compute the Sentence-BERTScore between the meta-review and each of the three reviews, then calculate the average and variance to measure their overall similarity. &  
$\mathbf{E}_{\text{meta-review}}$ represents the sentence embedding of the meta-review.
$\mathbf{E}_{\text{review}}$ represents the sentence embedding of a single review.

\text{Similarity} = 
$\frac{\mathbf{E}_{\text{meta-review}} \cdot \mathbf{E}_{\text{review}}}{\|\mathbf{E}_{\text{meta-review}}\| \|\mathbf{E}_{\text{review}}\|}$

\\ 
\midrule

\textbf{Writers' attitude}          & &  \\ \midrule
\textbf{Emotions}   &Measure the distribution of emotions in the text. There are various types of emotions, such as disappointment, sadness, annoyance, joy, anger, caring, optimism, curiosity, gratitude, and more. These emotions cover a range from positive to negative, helping us to understand the emotional tone and intent of the writing. & We use $SamLowe/roberta-base-go\_emotions$,a model trained on the Go-Emotions dataset to classify over 28 emotions. \\

\textbf{Persuasiveness}   &Binary classification (Non-persuasive, Persuasive)
 &To finish the task, we use the $paragon-analytics/roberta\_persuade model$, a binary classifier, to finish this task.\\ 
\textbf{Convincingness}   &Binary classification (Non-Convincingness, Convincingness) &To finish the task, we use $jakub014/bert-base-uncased-IBM-argQ-30k-finetuned-convincingness-IBM$, a model trained on the assumption that convincingness is linked to
high quality arguments which are typically clear, relevant, and with high impact.\\ \midrule


\bottomrule
\end{tabular}
\caption{Indirect Features with brief descriptions}
\label{tab:Indirect_Linguistic_Features_description}
\end{table*}
