

\section{Related works}




In recent years, discussions in the scientific community have focused on improving the peer review process~\cite{gurevych_et_al:DagRep.14.1.130, kuznetsov2024natural} to address issues like misalignment between reviewers and paper topics, as well as social~\cite{huber2022nobel, tomkins2017reviewer, manzoor2021uncovering} and cognitive biases~\cite{lee2015commensuration, stelmakh2021prior}. Proposed solutions include enhancing structural incentives for reviewers~\cite{rogers-augenstein-2020-improve}, using natural language processing for intelligent support~\cite{kuznetsov-etal-2022-revise, zyska-etal-2023-care, dycke-etal-2023-nlpeer, guo-etal-2023-automatic, kumar-etal-2023-reviewers}, and other policy recommendations~\cite{dycke-etal-2022-yes}. Additionally, some studies focus on the collection and analysis of review data~\cite{kennard-etal-2022-disapere, staudinger-etal-2024-analysis, darcy-etal-2024-aries}.\footnote{\href{https://arr-data.aclweb.org/}{ACL Rolling Review Data Collection (ARR-DC)}.} 
\textbf{However, these efforts largely center on human reviewers—what if, instead, the reviewers are AI?}


The recurrent use of specific syntactic templates in LLM-generated text largely reflects patterns from the training data~\cite{shaib-etal-2024-detection}, highlighting the model's memorization capacity~\cite{karamolegkou-etal-2023-copyright, zeng-etal-2024-exploring, zhu-etal-2024-beyond}.






AgentReview~\cite{jin2024agentreview}

BUST: Benchmark for the evaluation of detectors of LLM-Generated Text~\cite{cornelius-etal-2024-bust}


\subsection{Large Language Models Penetration}


\lz{\cite{wei2024understanding}: this paper performs the first analysis of the impact that AIGC has had on the social media ecosystem, through the lens of Pixiv.}

With the rapid development of large language models (LLMs), the penetration of LLM technology into various fields has deepened,and especially academia, such as science journal writing~\cite{jiang2024llm}, Academic information seeking~\cite{wang2024solution} by using a SOAY tool to enhances LLMs' academic paper retrieval, and peer review generation tasks~\cite{su-etal-2023-reviewriter},
which provides a tool for writing peer reviews in German.
Simultaneously, some related studies have proposed methods utilizing large language models (LLMs) to facilitate the work of paper reviewers.
~\cite{weber2024other} provide a fine-tuned model used to automate peer reviews which is named RoboReviewe,
and ~\cite{gao2024reviewer2optimizingreviewgeneration} purpose propose an efficient two-stage review generation framework for helping reviewer to find the deficiency of the sketched review.
Meanwhile, ~\cite{santu2024promptingllmscomposemetareview} Introduced a method to compose meta-review drafts from peer-review narratives.

Although the use of LLM-generated review/meta-review has facilitated the paper review process,
the widespread application of these technologies raises concerns about the level of engagement and
effort put in by reviewers. 
~\cite{jin-etal-2024-agentreview} reported that 15.8\% of ICLR 2024 reviews were AI-assisted, with AI-assisted reviews consistently assigning higher scores than their human counterparts and AI-assisted reviews contributed to a 4.9 percentage point increase in acceptance rates for borderline submissions.
~\cite{du-etal-2024-llms} collaborated with 40 experts in LLM-related reviewing to conduct a fine-grained evaluation of reviews generated by LLMs.
Moreover, ~\cite{zhou-etal-2024-llm} shows that LLM generate review prefers excessively positive word than human.
both of them reveal a signal of the importance to investigate the involvement of AI in the this process.In this work, we make a distinction from previous studies by focusing on the participation rate of AI-generated text in the paper review process.

LLMs have developed a strong capability in text generation tasks. Specifically in the academic domain, \cite{birhane2023science} have highlighted the convenience of LLMs in peer review and abstract writing. During the peer review process, some reviewers utilize LLMs for summary tasks to aid in reading, and even instruct LLMs to generate review sketches\cite{zou2024chatgpt}. Additionally, some meta-reviewers will employ LLMs to summarize reviewers' suggestions for creating meta-reviews\cite{santu2024prompting}. However, the summary results produced by LLMs may not fully capture the research information of a paper. Reviewers who overly rely on these summary results may compromise the objectivity and authenticity of the review process.

On the other hand, in abstract writing tasks, LLMs are often used for polishing and text completion\cite{khalifa2024using}, such as generating the Background for the abstract. This, however, poses the risk of making published outputs more uniform, potentially reducing the distinctive voice and style of human-authored works. In above, We believe that studying the penetration of LLMs from these two aspects can effectively reflect the extent of LLM penetration in academia.
% \lz{Summarize current research on LLM penetration across multiple domains, particularly highlighting advancements and impacts within the academic domain.}

% \lz{Perhaps we can summarize an analytical perspective of the existing work and compare it}



\subsection{Evaluation Metric}

% \lz{@Li Zhou: including Detection Model, Linguistic Feature metric, and so on}

% candidate:
% automatic metric: BLEU, ROUGE, ……
% Linguistic Features: ……
