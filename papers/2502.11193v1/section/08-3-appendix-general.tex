\section{General Linguistic Metrics Implementation}
\label{app:general}
% We have 10 general linguistic metric: Average Word Length (AWL), Long Word Ratio (LWR), Stopword Ratio (SWR), Type Token Ratio (TTR), Average Sentence Length (ASL), Dependency Relation Variety (DRV), Subordinate Clause Density (SCD), Flesch Reading Ease (FRE), Sentiment Polarity Score (PS), and Sentiment Subjectivity Score (SS).
For word-level metrics, \texttt{NLTK} tokenization is used, and only alphabetic words are considered.
For sentence-level metrics, \texttt{spaCy} is used to process the text and extract features such as sentence length and the dependency relation label of each word. 
Additionally, Sentiment Polarity Score (PS) and Sentiment Subjectivity Score (SS) are evaluated using \texttt{TextBlob}, while FRE is calculated using \texttt{Textstat}.


% \lz{wait}


\section{Fine-Grained Detection Model}
\label{app:Fine-Grained}
Using the existing meta-review data from \texttt{ScholarLens} (including both human-written and LLM-synthesized versions), we apply the LLM-refined abstract construction method to generate an LLM-refined version for each human-written meta-review. We utilize GPT-4o as the single LLM source and train a fine-grained three-class detector on the meta-reviews using the same data split. This trained detection model is then used to predict the 2024 ICLR meta-reviews, with approximately 35.32\% predicted as LLM-refined and 1.39\% as LLM-synthesized, which  show that the LLM-refined role plays a more dominant part in LLM penetration.