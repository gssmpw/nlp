
% \lz{look through: start}

\section{\texttt{ScholarLens} Curation}
\label{sec: ScholarLens}
% To investigate the penetration rate of LLMs in these stages, 

In this section, we detail the process of curating \texttt{ScholarLens}, including the consideration of data types and the setup for collecting both human-written and LLM-generated text.

% the collection setup of human-written and LLM-generated text.



% criteria for paper selection, the collection of human-written and LLM-generated reviews, the annotation procedure, and the measures taken to ensure data quality.

% \subsection{What Aspects of LLMs in Academia Are Considered?}




% \subsection{How to Construct LLM-Generated Data for Comparison?}

% \begin{table}[]
% \centering
% \scalebox{0.85}{
% \begin{tabular}{@{}l|rrrrrr@{}}
% \toprule
% \textbf{Source} & \textbf{$\leq 2019$} & \textbf{$2020$} & \textbf{$2021$} & \textbf{$2022$} & \textbf{$2023$} & \textbf{$2024$} \\ \midrule
% \textbf{ICLR}   & 2831           & 2213          & 2594          & 2619          & 3797          & 5780          \\
% \textbf{ACL}    &                &               &               &               &               &               \\
% \textbf{EMNLP}  &                &               &               &               &               &               \\
% \textbf{}       &                &               &               &               &               &               \\ \bottomrule
% \end{tabular}}
% \end{table}

\subsection{Data Types}

% We develope a dataset, \texttt{ScholarLens}, including both human from two distinct perspectives: ``refined'' and ``synthesized''. The ``refined'' perspective focuses on enhancing and polishing existing drafts, while the ``synthesized'' perspective involves producing content based on provided texts.  In this section, we describe the process of curating \texttt{ScholarLens}, including considerations of academic aspects and the collection of LLM-generated text details.

We formalize a research paper as $\mathrm{P}=\left\{ \mathrm{T},\mathrm{A},\mathrm{C},\mathrm{R},\mathrm{MR} \right\}$ where $\mathrm{T}$, $\mathrm{A}$, and $\mathrm{C}$ represent the title, abstract, and main content, respectively, and $\mathrm{R}=\left\{ r_i \right\}$ denotes individual reviews, with $\mathrm{MR}$ representing the meta-review summarizing feedback from multiple reviewers.
Since the process of creating a research paper involves both author drafting and peer review stages, our dataset includes content from both the author and (meta-)reviewer roles. 
When creating LLM-generated text, we consider two perspectives: `refined', which enhances existing drafts, and `synthesized', which summarizes and generates content from provided texts.


% \paragraph{Author Role}
For the author role, we focus on abstract writing, as it is a key element for summarizing the paper and is easily accessible, making it ideal for this study. Specifically, we adopt the `refined' approach, where the original human-written abstract is input into the LLM to generate a refined version.
For the (meta-)reviewer roles, we focus on their comment content. To simulate human-written reviews and meta-reviews, the LLM-generated version primarily adopts the `synthesized' perspective. 
% Specifically, we provide a generation template as a guideline for the LLM.
The review process requires the full text of the paper as input, while the meta-review process includes all associated reviews of the paper.
All prompts used to create LLM-generated content are provided in the Appendix~\ref{app: prompts}.


% For author roles, our primary focus is on the integration of large language models (LLMs) in refining abstract content. The abstract serves as the core element of a paper, offering a concise summary that is both easily accessible and reliably extracted from public sources. Specifically, we employ an LLM as a `polisher', where the original human-written abstract is input into the model, which then generates a refined version. To ensure diversity and robustness in the refinement process, we design a prompt set, consisting of five distinct prompts. One prompt is randomly selected during each refinement cycle. 



% \paragraph{(Meta-)Reviewer Roles}


% A technical reviewer provides detailed feedback on the paper, evaluating aspects such as research methods, writing structure, experimental results, and novelty. In contrast, a meta-reviewer synthesizes the comments from multiple reviewers, offering a comprehensive summary that directly influences the final recommendation regarding the paper's acceptance.
% To simulate the process of human-written reviews and meta-reviews, the LLM-generated version primarily adopts the `synthesized' perspective. Specifically, we provide a generation template format as a guideline for the LLM. The review process requires the full text of the paper as input, while the meta-review process necessitates the inclusion of all the reviews of the paper. Detials of the generation tempalte are also shown in the Appendix~\ref{app: prompts}.


\subsection{Data Collection Setup}




Considering the challenges associated with parsing full-text papers, typically in PDF format, and the high computational cost of generating LLM-based reviews from lengthy input data, the \texttt{ScholarLens} collection integrates pre-existing review data with self-constructed abstracts and meta-reviews.

For the self-constructed data, we first collect raw data from all main conference papers in ICLR up to 2019, totaling 2,831 papers, through the OpenReview website. 
This selection is motivated by two factors: first, ICLR's peer review process provides comprehensive and detailed (meta-)review data; second, by focusing on papers before 2019, we can assume that the source data remains entirely human-written, as it predates the release of ChatGPT. For each paper, we generate two types of LLM-generated content: LLM-refined abstracts and LLM-synthesized meta-reviews. Both types are created using three advanced closed-source LLMs: GPT-4o, Gemini-1.5~\cite{team2024gemini}, and Claude-3 Opus~\cite{claude3modelcard}. This ensures that for every human-written version of the content, there is a corresponding LLM-generated version from each of the three models.
For the pre-existing review data, we directly leverage the review data from ReviewCritique~\cite{du-etal-2024-llms}\footnote{Note that ReviewCritique includes LLM-generated reviews for only 20 papers.}, which incorporates the same three LLMs.
 Details of \texttt{ScholarLens} are in Appendix~\ref{app:dataset}, including statistics and LLM settings.

% For each paper, we create two types of LLM-generated content: LLM-refined abstracts and LLM-synthesized meta-reviews. Both are created using three of the most advanced closed-source LLMs: GPT-4o, Gemini-1.5~\cite{team2024gemini}, and Claude-3 Opus~\cite{claude3modelcard}.
% This ensures that, for every human-written version of the content, there is a corresponding version generated by each of the three LLMs.
% Furthermore, due to the challenges associated with parsing full-text papers, typically in PDF format, and the high computational cost of constructing LLM-generated reviews from long input data, we do not generate reviews directly. Instead, we leverage review data from ReviewCritique~\cite{du-etal-2024-llms}\footnote{ReviewCritique only includes LLM-generated reviews for 20 papers.}, which incorporates the three same LLMs. \lz{The statistics of \texttt{ScholarLens} are shown in Table~\ref{tab: AcademicLens-statistics} in the Appendix~\ref{app:dataset}.}

% This choice is driven by two considerations: firstly, ICLR's peer review process offers the most comprehensive and detailed data available; secondly, selecting papers published before 2019 ensures that the source data remains purely human-written, as it predates the release of ChatGPT.
% For each paper, we generate three types of LLM-generated content: coarse-grained refined abstracts, fine-grained synthesized abstracts, and LLM-synthesized meta-reviews. The fine-grained synthesized abstracts are produced using GPT-4o, while the other two types of content are generated with three of the most advanced closed-source LLMs: GPT-4o, Gemini-1.5, and Claude-3 Opus. Furthermore, due to the challenges of parsing the full-body text of papers, which are typically in PDF format, and the high computational cost associated with constructing LLM-generated reviews from lengthy input data, we do not generate them directly. Instead, we utilize review data from ReviewCritique, which incorporates the same three LLMs.



% This choice is motivated by the comprehensive nature of ICLR's peer review process and the desire to ensure the data is purely human-written, as it predates the release of ChatGPT.


  
% In this section, we describe the three focal aspects of the academia domain. For each aspect, we provide a detailed definition and explain the construction of the corresponding dataset, which serves as the foundation for our experiments.


% \subsubsection{Raw Data}
% The existing peer review platform called OpenReview is dedicated to advancing science through improved peer review, offering a configurable system that supports various levels of openness in the review process, including well-known conferences such as ICLR, ACL, and others.
% In our study,we base on the official OpenReview API,  collecting submission and review data from ICLR, a leading machine learning conference, spanning the years 2018 to 2024. And the collection includes a total of 19,847 main conference submissions, each with its corresponding reviews and meta-reviews.

% With the widespread use and growing popularity of AI models such as ChatGPT, and as former research points out, there is growing suspicion that academic reviewers may have begun utilizing AI tools in their review processes in recent years. Therefore, we set the year 2020, the release year of GPT-3, as a cutoff point, presupposing that the data before 2020 had negligible or no AI involvement, and that the data from 2020 to 2024 is likely to have AI involvement, separating data into two sub-datasets, named non-AI data and prob-AI data, where the non-AI data includes 2,844 out of 19,847 abstracts, corresponding to 2,844 meta-reviews and 8,592 reviews, while the prob-AI data includes (XXXX). \textbf{Our framework aims to generate an AI context based on the non-AI data and evaluate the effectiveness of the metrics we established below. So that we could assess AI penetration in the prob-AI data, thereby revealing the extent of AI involvement in academic peer review in recent years.}

% \subsubsection{Meta-review}

% \input{figure/data_pair_extraction_and_split}

% \input{table/polishing_prompt}



% We collected the submission and review data from ICLR, a major conference in machine learning. The data was gathered through the official OpenReview API, consisting of submissions and reviews from conferences held between 2018 and 2024. The collection includes a total of 19,847 main conference submissions, each with its corresponding reviews.

% Next, we selected data from 2017 to 2019 for cleaning, extracting 2,844 abstracts, 8,592 official reviews, and 2,844 meta-reviews.

% To generate AI meta-reviews, we designed a simplified prompt:\\

% \textit{the prompt we use to generate meta-review}\\

% The prompt adapts based on the review content and abstract, guiding ChatGPT-4o model to generate a comprehensive meta-review.

% During the analysis, we observed that some meta-reviews followed a specific structure, such as listing strengths (Pros) followed by weaknesses (Cons). To accurately simulate this structure, we measured the occurrence frequency of this specific format in the collection and appropriately introduced a template into the generation prompts based on these proportions.

% We used the official review and abstract for each submission as inputs. Using our prompts, the ChatGPT-4o model generated AI meta-reviews that matched the original word count.

% To simulate the process of a meta-reviewer’s feedback, we combine the review context with the paper abstract to guide LLMs in producing the meta-review where we use the gpt-4o-2024-08-06(hereinafter referred to as GPT-4o) model here.Additionally, since the review criteria for ICLR change annually, the review template is typically determined by these evolving criteria, which vary across years. To capture this diversity in reviews, we analyze the frequency of review templates used from 2018 to 2024. This allows us to refactor the reviews according to the frequency distribution, ensuring that the generated meta-reviews reflect the shifting review criteria over time. Based on this analysis, we design the following prompt to guide the meta-review generation:\\
% \textit{Please write a meta review of the given reviewers' response around \{word\_count\} words. Do not include any section titles or headings. Do not reference individual reviewers by name or number. Instead, focus on synthesizing collective feedback and overall opinion.}

% \textit{Please include the given format in your meta review: \{selected\_format\}}

% \textit{\#\#\# }

% \textit{Abstract: \{abstract\}}

% \textit{\#\#\#}

% \textit{Reviewers' feedback:}

% \textit{\{review\_text\}}




% In total, we generated 2,844 AI-generated meta-reviews. We then labeled the AI-generated meta-reviews as "1" and the original meta-reviews as "0," forming a dataset of 5,686 entries.

% In this study, we use the Longformer model and tokenizer, optimized for a binary classification task. In our cleaned dataset, each entry includes human-written meta-reviews from ICLR and AI-generated meta-review by ChatGPT-4o.

% We first extract these pairs, then shuffle them using a fixed random seed to ensure consistency across runs.
% Then we divide the data pairs into three sets—training, validation, and test—in a 7:2:1 ratio. Finally, we format the data for model input. The “result” texts are labeled as 1 and “meta-review” texts as 0. Figure~\ref{fig:data_pair_extraction_and_split} presents a specific example.

% Key training settings, like learning rate, batch size, and training epochs, are shown in the table.~\ref{tab:training_parameters}





% \textbf{Coarse-grained}: We consider coarese-grained aspect and use the abstract data above, 2,844 abstracts from ICLR papers. The method we obtain the AI-generated abstracts is also created by GPT-4o, and we randomly select one of the prompts for each generation, the prompts could be seen in % \ref{tab:polishing_prompts}
% , At the end, we get 2,844 abstracts polished by GPT-4o, intent to finding the discrepancy of the original abstracts and the AI-generated revisions.
% \textbf{Fine-grained}, In more detail, we consider the complexity of AI involvement in abstract writing: polishing, generation, and even hybrid approaches (polishing + generation). To explore this, we use the GPT-4 model for a text completion task. Usually, a paper abstract consists of several parts: Background, Motivation, Problem Statement, Approach, Results/Key Findings, Conclusion, and Implications. 

% First, we use GPT-4 to pre-label the parts of the abstract and manually annotate them for proofreading. Then, we apply the following operations to each abstract with probabilities of 0.4, 0.4, and 0.2, respectively: delete Background, delete Implications, and delete both. Based on these results, we provide three different prompts to instruct the LLM to complete the context, yielding 2,844 results labeled as 'human + generate.' At the same time, we perform the same operations on polished abstracts, which are labeled 5as 'human + polishing + generate.'

% Above all, in the fine-grained dataset, each entry consists of four different types of abstracts: 'human', 'polishing', 'human + generate', and 'human + polishing + generate'.

% \subsubsection{Review}
% As the amount of reviews reach over 8000 pairs, 
% We will also focus on AI involvement in peer review. In the ICLR dataset, each paper is associated with 3 to 5 peer reviews. For simplicity, we select 100 papers mentioned in {}. Each of these papers carries 3 to 5 peer reviews, and 20 of them additionally have 3 peer reviews generated by GPT-4o, Claude-3 (Opus, 2024-02-29), and Gemini-1.5-Pro (version 002), respectively. This dataset not only allows us to contrast the differences between human and AI writing, but also helps indirectly assess the robustness of the ICLR dataset we are using.
% 为什么选取其他数据集，选取了怎么构造，有什么意义--侧面印证我们的metric
% As the number of reviews exceeds 8,000 pairs, generating AI reviews for such a large volume becomes increasingly cumbersome. For simplicity, we select the 100 high-quality papers mentioned in {}. Each of these papers has 3 to 5 peer reviews, and 20 of them are further supplemented with 3 additional peer reviews generated by GPT-4o, Claude-3 (Opus, 2024-02-29), and Gemini-1.5-Pro (version 002), respectively. This curated dataset not only allows us to directly compare the differences between human and AI-generated writing, but also provides an indirect means of evaluating the robustness and reliability of the metrics on ICLR dataset in this work.



% \subsubsection{Title}




% \subsection{Linguistic Feature Comparison} 
% \input{table/linguistic_features}
% To systematically compare the linguistic feature differences between human-written meta-review and AI-generated ones, we introduce seven linguistic feature metrics. Table~\ref{tab:feature_comparison} presents a comparison of these linguistic features. Average word and sentence count measure the number of words and sentences in a news article. Sentiment polarity score represents the emotional tone of a text, ranging from -1 to 1, with higher values indicating more positive sentiment, and lower values reflecting more negative sentiment. Grammatical errors measure the number of grammatical mistakes that occur per 1,000 words. Syntactic diversity measure the structural complexity by analyzing clause patterns. Vocabulary richness measures lexical diversity, ranging from 0 to 1, with higher values indicating greater lexical variation. Readability score measures the complexity of a text, with higher values indicating greater reading difficulty. 

% As shown in Table~\ref{tab:feature_comparison}, we observe that the lengths and number of sentences between human-written and AI-generated meta-reviews are roughly similar due to the prompt settings during generation. Human-written meta-reviews exhibit a higher frequency of grammatical errors, employ a broader range of sentence structures, and demonstrate lower readability.

% In contrast, LLM-polished and LLM-extended news, incorporating more human inputs, are significantly richer and more comprehensive. The various types of news exhibit trivial differences in their sentiment polarity scores. From other linguistic features, human writing shows greater lexical and syntactic variation with lower reading difficulty, whereas LLM writing is more standardized, featuring fewer grammatical errors and minimal use of informal writing styles. 