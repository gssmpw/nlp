
\section{Case Study Details}
\label{app: cs-details}
\subsection{Word-Level Algorithm and Experiments}
\subsubsection{Hypothesis Testing Algorithm}
\label{app:TestingAlgorithm}
Building on the Two-Sample t-test, we propose a word-proportion-based method to identify word-level LLM preferences.
Specifically, given a set of pairs of human-written and LLM-generated texts $\mathcal{D} =\left\{ \left( x_{i}^{h}, x_{i}^{l} \right) \right\} $, where where $x_i^{h}$ represents the human-written case and $x_i^{l}$ represents the corresponding LLM-generated version, our goal is to determine whether a word $w$ is preferentially generated by the LLM.

\paragraph{(i) Word Proportion} 
We define the proportion of word $w$ appearing in the human-written set $\left\{ x_{i}^{h} \right\} $ and the LLM-generated set $\left\{ x_{i}^{l} \right\} $ as $\hat{p}_h\left( w \right) $ and $\hat{p}_l\left( w \right) $, respectively, representing the fraction of texts in which $w$ occurs:
\begin{equation}
    \hat{p}_h\left(w\right) ={\small{\frac{\mathrm{cnt}_h\left( w \right) +\epsilon}{\left| \mathcal{D} \right|}}}
\end{equation}
\begin{equation}
    \hat{p}_l(w) ={\small{\frac{\mathrm{cnt}_h\left( w \right) +\epsilon}{\left| \mathcal{D} \right|}}}
\end{equation}
where $\mathrm{cnt}\left( w \right) =\sum\nolimits_i^{}{\mathbb{I} \left( w\in x_{i}^{} \right)}$ counts the number of texts in the set ${x_i}$ where the word $x$ appears, with $\mathbb{I}(w \in x_i)$ being an indicator function that returns 1 if $w$ appears in $x_i$, and $\epsilon=1$ as a smoothing constant to account for words that do not appear in a given text.




\paragraph{(ii) Hypothesis Setting}
Then, we define the following two hypotheses:
\begin{itemize}
    \item \textbf{Null hypothesis} ($H_0$): $\hat{p}_h\left( w \right) \geqslant \hat{p}_l\left( w \right) $, suggesting that LLMs do \textbf{not} preferentially generate the word $w$.
    % suggests that the word $x$ appears more frequently, or equally, in human-written text compared to LLM-generated text, suggesting that LLMs do not preferentially generate the word $x$.
    % This implies that the proportion of the word's occurrence in LLM-generated texts is not higher than that in human-written texts.(i.e., the word is preferred by LLMs)
    \item \textbf{Alternative hypothesis} ($H_1$): $\hat{p}_h\left( w \right) < \hat{p}_l\left( w \right) $, suggesting that LLMs preferentially generate the word $w$.
    % which suggests that the word $x$ appears more frequently in LLM-generated text than in human-written text, implying a preference by LLMs.
    
    % This indicates that the proportion of the word's occurrence in LLM-generated texts is significantly higher than that in human-written texts.(i.e., the word is preferred by LLMs)
\end{itemize}



% Here, $\hat{p}_1$ and $\hat{p}_2$ represent the proportions of the word's occurrence in human-written texts and LLM-generated texts.

\paragraph{(iii) Hypothesis Testing}

Considering that the variance of word proportion may differ between the two text groups, we adopt Welch's t-test to quantify the difference. Specifically, the test statistic and degrees of freedom are computed as follows:

\begin{equation}
t(w)=\frac{\hat{p}_l\left( w \right) -\hat{p}_h\left( w \right)}{\sqrt{\small{\frac{s_{h}^{2}+s_{l}^{2}}{\left| \mathcal{D} \right|}}}}
\end{equation}
\begin{equation}
    df(w)=\frac{\left( \frac{s_{h}^{2}+s_{l}^{2}}{\left| \mathcal{D} \right|} \right) ^2}{\frac{(s_{h}^{2}/\left| \mathcal{D} \right|)^2+(s_{l}^{2}/\left| \mathcal{D} \right|)^2}{\left| \mathcal{D} \right|-1}}
\end{equation}

where $s_h$ and $s_l$ represent the standard deviations of the corresponding word proportions, calculated as follows:
\begin{equation}
    s=\sqrt{\frac{\hat{p}(1-\hat{p})}{\left| \mathcal{D} \right|}}
\end{equation}


% Here, $SE$ represents the standard error of the difference between the two proportions.


% \subparagraph{Standard Error Calculation}
% The standard error $SE_i$ of the sample proportion in the $i$-th type of text is:
% \begin{equation}
% SE_i=\sqrt{\frac{\hat{p}_i(1 - \hat{p}_i)}{n_i+\alphUI_{smooth}\cdot|V|}}
% \end{equation}
% The standard error $SE$ of the difference between the two proportions is:
% \begin{equation}
% SE=\sqrt{SE_1^2 + SE_2^2}
% \end{equation}

% \subparagraph{Degrees of Freedom Calculation}
% In general, for two independent samples with variances $s_1^2$ and $s_2^2$ and sample sizes $n_1$ and $n_2$, the original formula for the degrees of freedom $df$ in Welch's $t$-test is:
% \begin{equation}
% df = \frac{\left(\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}\right)^2}{\frac{(s_1^2 / n_1)^2}{n_1 - 1}+\frac{(s_2^2 / n_2)^2}{n_2 - 1}}
% \end{equation}
% In our context, with $s_i^2 = SE_i^2$ and considering the adjusted sample sizes $n_i'=n_i+\alphUI_{smooth}\cdot|V|$, the simplified formula for degrees of freedom is:
% \begin{equation}
% df=\frac{(SE_1^2 + SE_2^2)^2}{\frac{SE_1^4}{n_1+\alphUI_{smooth}\cdot|V| - 1}+\frac{SE_2^4}{n_2+\alphUI_{smooth}\cdot|V| - 1}}
% \end{equation}

\paragraph{(iv) Hypothesis Decision}
We define the critical t-value, $t_c$, as the threshold for rejecting or accepting the null hypothesis. 
It is calculated using the inverse of the cumulative distribution function (CDF) of the t-distribution:
\begin{equation}
    t_c(w) = t_{\alpha, \, df(w)}^{-1}
\end{equation}
where $\alpha=0.05$ is the significance level.
If $t(w) > t_c(w)$, we reject the null hypothesis and conclude that the word occurs significantly more often in LLM-generated texts than in human-written texts. In this way, we can identify the words favored by LLMs.


\subsubsection{Experimental Setup}

% \lz{@Ruijie, write: how we filter these words}
% 例如需要描写到的：考虑到一些词，在上下文可能以名词出现、也可能以动词出现，所以我们没有直接以word本身作为最小单位去filter，而是使用(word, pos)为最小单位……我们使用什么词性分析；我们不考虑停用词；我们不考虑大小写……我们使用什么作为排序的准则……

To address part-of-speech variability of the same word (e.g., `record' functioning as both a noun and a verb in a sentence), we adopt (word, POS) pairs as the fundamental unit for analysis rather than isolated words. We use SpaCy for POS tagging, and stopwords are excluded from consideration. 
Using the proposed Hypothesis Testing Algorithm (\S\ref{app:TestingAlgorithm}), we filter the LLM-preferred word set and rank these words based on their Word Usage Increase Ratio (WUIR), defined as follows:
\begin{equation}
    \mathrm{WUIR}\left(w\right) ={
    \small{\frac{\mathrm{cnt}_l\left( w \right) -\mathrm{cnt}_h\left(w\right)}{\mathrm{cnt}_h\left(w\right) +\epsilon}}}
\end{equation}


% \paragraph{(i) Preprocessing Rule}
% Before filtering, we implement three preprocessing rule on texts:
% \textbf{Stopword Removal}: Exclude tokens matching NLTK's English stopword list.
% \textbf{Case Insensitivity}: Convert all words to lowercase before processing.

% \paragraph{(ii) Word-POS Pair Unit}
% To address lexical ambiguity (e.g., words like "record" functioning as both nouns and verbs), we adopt (word, POS) pairs as the fundamental unit for analysis rather than isolated words. We use Spacy for POS tagging.

% \paragraph{(iii) Hypothesis Testing}
% For words that share the same POS(such as when focusing on adjectives (ADJ), we only consider words of this specific POS), we implement the Hypothesis Testing Algorithm. Through this process, we are able to identify the set of words that are preferred by LLMs.

% \paragraph{(iv) Ranking}
% The final word set is sorted by the metric 
% \begin{equation}
% \frac{\left( \sum_i \mathbb{I}(w \in x_i^l) + \epsilon \right) - \left( \sum_i \mathbb{I}(w \in x_i^h) + \epsilon \right)}{\sum_i \mathbb{I}(w \in x_i^h) + 1}
% \end{equation}
% in descending order.


\subsubsection{Results: LLM-Preferred words}
% \lz{@Ruijie, show the words result and briefly describe them }% 先展示GPT-4o的，分别简要描述在abstract和meta-review的倾好词，最后简要提及一下Claude和Gemini的结果，如表所示，不需要额外描述多的。
%For GPT4o, in meta-reviews, \textit{long words} account for 39.96\% and \textit{complex-syllabled words} account for 67.82\%. In abstracts, the proportions are 40.48\% and 73.45\% respectively.

%For GPT4o, in the meta - review, we obtained a set of preferred words consisting of 600 NOUN, 371 VERB, 275 ADJ, and 43 ADV.
%For its abstract, the set of preferred words we counted is composed of 201 NOUN, 422 VERB, 127 ADJ, and 65 ADV.
%The Top-30 LLM-preferred words of GPT4o in the meta-review and abstract are shown in Table~\ref{tab:4m}~\ref{tab:4a}.

\input{table/gpt4o-abstract}
\input{table/gpt4o-meta}

Tables~\ref{tab:4a} and~\ref{tab:4m} display the top-30 preferred words across four key part-of-speech (POS) categories in GPT-4o-generated abstracts and meta-reviews, with long words accounting for 40.48\% and 39.96\%, and complex-syllabled words for 73.45\% and 67.82\%. Furthermore, Tables~\ref{tab:ga} and~\ref{tab:gm} show the top-30 preferred words in four key POS categories for Gemini-generated abstracts and meta-reviews, while Tables~\ref{tab:ca} and~\ref{tab:cm} display the same for Claude-generated abstracts and meta-reviews. All show a high proportion of long words and complex-syllabled words.


\input{table/gemini-abstract}
\input{table/gemini-meta}

\input{table/claude-abstract}
\input{table/claude_meta}

% The Top-30 LLM-preferred words of GPT4o, Claude and Gemini are presented in the Table~\ref{tab:4m}, Table~\ref{tab:4a}, Table~\ref{tab:gm}, Table~\ref{tab:ga}, Table~\ref{tab:cm} and Table~\ref{tab:ca}.

% Furthermore, the Statistical results of \textit{long words} and \textit{complex-syllabled words} are presented in Table~\ref{tab:word-level-long} and Table~\ref{tab:word-level-complex}. Take GPT4o for example, in meta-review, \textit{long words} account for 39.96\% and \textit{complex-syllabled words} account for 67.82\%. In abstract, the proportions are 40.48\% and 73.45\% respectively. The results reflect LLMs' preference for \textit{long words} and \textit{complex-syllabled words}.


% Moreover, we use the preferred word set to validate the correctness of our Fine-Grained Detection Model \S\ref{app:Fine-Grained} on ICLR 2024 meta-reviews. The model categorizes the meta-reviews into three groups: human-written, LLM-synthesized, and LLM-refined. To simplify the analysis, the latter two groups were combined into an LLM-mixed group, which represents all content involving LLMs. We calculate the percentage of words in each meta-review that appeared in our preferred word set. After that, we separately compute the average of these percentages for meta-reviews in Human group and LLM-mixed group. The ratios for Human and LLMs-mixed group are 35.15\% and 44.95\% respectively. The significantly higher matching ratio of LLM-mixed group further validates the correctness of our model.

% \begin{table}[]
%     \centering
%     \begin{tabular}{l|l|r}
%         \hline
%         {Category}&role & ratio\\
%         \hline
%         \multirow{3}{*}{Meta-review} 
%             & GPT4o &39.96\\
%             & Gemini &34.58\\
%             & Claude &43.93 \\
%         \hline
%         \multirow{3}{*}{Abstract} 
%             & GPT4o &40.48\\
%             & Gemini &44.35\\
%             & Claude &42.81\\
%         \hline
%         \multirow{1}{*}{ICLR 2024} 
%             & LLMs &36.92\\
%         \hline
%     \end{tabular}
%     \caption{long words percentage for LLM preferred words}
%     \label{tab:word-level-long}
% \end{table}

% \begin{table}[]
%     \centering
%     \begin{tabular}{l|l|r}
%         \hline
%         {Category}&role &ratio\\
%         \hline
%         \multirow{3}{*}{Meta-review} 
%             & GPT4o &67.82\\
%             & Gemini &62.07\\
%             & Claude &70.49 \\
%         \hline
%         \multirow{3}{*}{Abstract} 
%             & GPT4o &73.45\\
%             & Gemini &74.80\\
%             & Claude &75.00\\
%         \hline
%         \multirow{1}{*}{ICLR 2024} 
%             & LLMs &64.18\\
%         \hline
%     \end{tabular}
%     \caption{complex-syllabled words percentage for LLM preferred words}
%     \label{tab:word-level-complex}
% \end{table}








\subsection{Pattern-Level Feature Statistics}
We identify the following pattern-level features in human-written (meta-)reviews: \textit{personability}, characterized by frequent use of the first person to express opinions; \textit{interactivity}, marked by the inclusion of questions; and \textit{attention to detail}, demonstrated by citing relevant literature to support arguments. 
To compare these pattern-level features between human-written and LLM-generated content, we calculate two metrics for each pattern in both meta-reviews and reviews within the ScholarLens dataset: \textbf{Feature Proportion (FP)} and \textbf{Feature Intensity (FI)}. FP is defined as the proportion of instances exhibiting the feature within the target data group, while FI is the average number of occurrences of the feature within instances that exhibit it. We report the FP and FI values for each pattern in meta-reviews and reviews across different data types—Human-written, GPT-4-generated, Gemini-generated, and Claude-generated— as shown in Table~\ref{tab:pattern_comparison}.
% (\textit{Personability} Pattern Comparison), Table~\ref{tab:Interactivity_c} (\textit{Interactivity} Pattern Comparison), and Table~\ref{tab:Attention_c} (\textit{Attention to Detail} Pattern Comparison).
The results show that, in both meta-reviews and reviews, the FP and FI values for each pattern feature in the Human-written data type are significantly higher than those in the LLM-generated versions.
% These suggest that humans have a higher breadth of coverage and intensity of usage for first person sentences, question sentences, and URL links.


% \lz{@Ruijie, description: how to filter? what's the result?} % 统计信息论证case study中提及的pattern features
% \subsubsection{Experimental Setup}




% To evaluate \textit{personability},\textit{interactivity} and \textit{attention to detail},we first define two general metrics: Coverage Rate ($CR_f$) and Usage Intensity ($UI_f$) for $f \in \{fp, q, l\}$. Here, $fp$ means first person sentence, $q$ means question sentence, and $l$ means URL link.

% $CR_f$ represents the proportion of texts in the entire sample that contain at least one instance of $f$, reflecting the breadth of coverage of $f$.
% $UI_f$ represents the average number of occurrences of $f$ within texts that already contain it, reflecting the intensity of usage of $f$.
% \begin{equation}
% CR_f = \frac{\sum_{i=1}^{N} \mathbb{I}(f \in x_i)}{N}
% \end{equation}
% \begin{equation}
% UI_f = \frac{\sum_{i=1}^{N} c_f(x_i)}{\sum_{i=1}^{N} \mathbb{I}(f \in x_i)}
% \end{equation}

% where \( \{x_i\} \) denotes a text set. \( N \) is the total number of samples. $\sum_{i=1}^{N} \mathbb{I}(f \in x_i)$ counts the number of texts in the set ${x_i}$ where $f$ appears. $\sum_{i=1}^{N} c_f(x_i)$ counts the total number of $f$.

% Based on the above general definitions, the specific metrics corresponding to \textit{interactivity}, \textit{personability}, and \textit{attention to details} are as follows: First Person Coverage Rate ($CR_{fp}$) and First Person Usage Intensity ($UI_{fp}$), Question Coverage Rate ($CR_{q}$) and Question Usage Intensity ($UI_{q}$), Link Coverage Rate ($CR_{l}$) and Link Usage Intensity ($UI_{l}$). 

%\paragraph{(i) Preprocessing Rule}
%Before filtering, we first implement two preprocessing rule on the texts:

%\textbf{Case Insensitivity}: Convert all words to lowercase.
%\textbf{Regular expression cleaning}: \verb|re.sub(r'[\r\n*\\]+', ' ', text)|

%\paragraph{(a) Personability Filtering}
%Using NLTK to tokenize a text into sentences and check if a sentence contains \"i\" or \"we\", we calculate First-Person Presence Ratio ($CR_{fp}$) and Average First-Person Quantity ($UI_{fp}$), defined as follows:

%\begin{equation}  
%CR_{fp} = \frac{\sum_{i=1}^{N} \mathbb{I}(fp \in x_i)}{N}  
%\end{equation}  
%where $ \mathbb{I}(fp \in x_i) $ is an indicator function that returns 1 if $ x_i $ contains at least one first-person sentence, and $ N $ is the total number of meta-reviews (or reviews).  

%\begin{equation}  
%UI_{fp} = \frac{\sum_{i=1}^{N} c_{fp}(x_i)}{\sum_{i=1}^{N} \mathbb{I}(fp \in x_i)}  
%\end{equation}  
%where $ c_{fp}(x_i) $ is the count of first-person sentences in $ x_i $.  



%\paragraph{(b) Interactivity Filtering}
%Using NLTK to tokenize a text into sentences and check if a sentence end with a question mark, we calculate Question Presence Ratio ($CR_{q}$) and Average Question Quantity ($UI_{q}$), defined as follows: 

%\begin{equation}  
%CR_q = \frac{\sum_{i=1}^{N} \mathbb{I}(q \in x_i)}{N}  
%\end{equation}  
%where $ \mathbb{I}(q \in x_i) $ is an indicator function that returns 1 if $ x_i $ contains at least one question.  

%\begin{equation}  
%UI_q = \frac{\sum_{i=1}^{N} c_q(x_i)}{\sum_{i=1}^{N} \mathbb{I}(q \in x_i)}  
%\end{equation}  
%where $ c_q(x_i) $ is the count of questions in $ x_i $.  

%\paragraph{(c) Attention to Detail Filtering}
%Matching the links in texts, we calculate Link Presence Ratio ($CR_{l}$) and Average Link Quantity ($UI_{l}$), defined as follows: 
%\begin{equation}  
%CR_l = \frac{\sum_{i = 1}^{N} \mathbb{I}(l \in x_i)}{N}  
%\end{equation}  
%where $ \mathbb{I}(l \in x_i) $ is an indicator function that returns 1 if $ x_i $ contains at least one link.

%\begin{equation}  
%UI_l = \frac{\sum_{i = 1}^{N} c_l(x_i)}{\sum_{i = 1}^{N} \mathbb{I}(l \in x_i)}  
%\end{equation}  
%where $ c_l(x_i) $ is the count of links in $ x_i $.

% \subsubsection{Results}
% Table~\ref{tab:personability_c}, ~\ref{tab:Interactivity_c} and ~\ref{tab:Attention_c} separately display the results of $CR_{fp}$, $UI_{fp}$, $CR_{q}$, $UI_{q}$, $CR_{l}$ and $UI_{l}$ metrics on the comment-based data. In both meta-review and review, the values of these six metrics in humans are significantly higher than those in LLMs. These suggest that humans have a higher breadth of coverage and intensity of usage for first person sentences, question sentences, and URL links.
%These suggest a higher frequency of first person, question, and relevant literature in humans.

%First, we conducted an automated evaluation on the comment-based data. 


%\subparagraph{(a) Personability Filtering result}
%As shown in Table~\ref{tab:personability_c}, in the comment-based data, Human exhibits much higher $CR_{fp}$ and $UI_{fp}$ values than LLMs, indicating that Human uses the first person more frequently.

%As shown in Table~\ref{tab:personability_f}, in ICLR 2024, the values of $CR_{fp}$ and $UI_{fp}$ for LLM-synthesized are significantly lower than those for LLM-refined, LLM-mixed, and Human. Although the values for LLM-refined and the LLM-mixed are higher than those for LLM-synthesized, they are still lower than those for Human.

%\subparagraph{(b) Interactivity Filtering result}
%As shown in Table~\ref{tab:personability_c}, in the comment-based data, the $CR_{q}$ and $UI_{q}$ values for LLMs are all 0. Humans exhibit significantly higher scores for the two values than LLMs, indicating that humans interact more frequently(incorporate questions more often). 

%As shown in Table~\ref{tab:personability_f}, in ICLR 2024, LLM-synthesized's $CR_{q}$ and $UI_{q}$ are still 0, significantly lower than those for LLM-refined, LLM-mixed, and Human. The $CR_{q}$ and $UI_{q}$ values for LLM-refined and LLM-mixed are lower than those for Human.

%\subparagraph{(c) Attention to Detail Filtering result}
%As shown in Table~\ref{tab:Attention_c}, in the comment-based data, the $CR_{l}$ and $UI_{l}$ values for LLMs are all 0. Humans exhibit significantly higher scores for the two values than LLMs, indicating that humans pay more attention to details(cite more relevant literature). 

%As shown in Table~\ref{tab:Attention_f}, in ICLR 2024, the $CR_{l}$ and $UI_{l}$ values for LLMs are still 0. The two values for LLM-refined and LLM-mixed are lower than those for Human.




% Under a given significance level $\alpha$ (we set $\alpha = 0.0001$), we calculate the critical value $t_{critical}$ of the $t$-distribution:
% \begin{equation}
% t_{critical}=t_{1 - \alpha, df}
% \end{equation}

% If the calculated $t$-score is greater than the critical value $t_{critical}$, i.e., $t > t_{critical}$, we reject the null hypothesis and conclude that the word occurs significantly more often in LLM-generated texts than in human-written texts. In this way, we can identify the words favored by LLMs, which appear significantly more frequently in LLM-generated texts.


% \paragraph{Detailed Calculation Process of $t$}
% \subparagraph{Sample Proportion Calculation}\label{subsubsec:hypothesis_calculation_judgment}
% For the $i$-th type of text ($i = 1$ for human-written texts and $i = 2$ for LLM-generated texts), the sample proportion $\hat{p}_i$ of the word is calculated as:
% \begin{equation}
% \hat{p}_i=\frac{x_i+\alphUI_{smooth}}{n_i+\alphUI_{smooth}\cdot|V|}
% \end{equation}
% where $x_i$ is the frequency of the word's occurrence in the $i$-th type of text, $n_i$ is the total number of words in the $i$-th type of text, $\alphUI_{smooth}$ is the smoothing parameter used to avoid zero counts, and $|V|$ is the number of different words that appear in the entire text data.


% \subsubsection{Part-of-Speech(POS) Filtering}
% To focus on semantically meaningful word categories, 
% we use \textbf{NLTK}'s \textbf{word\_tokenize} to tokenize the text and \textbf{pos\_tag} to assign fine-grained part-of-speech(POS) tags. Then, we map these fine-grained tags to coarse-grained ones, retaining only nouns, verbs, adjectives and adverbs. The mapping rules are provided in table~\ref{tab:POS}.

% We conducted the hypothesis-testing separately for each of the POS categories in the meta-reviews generated and abstracts polished by three models: GPT4o, Claude, and Gemini. In total, we processed 4×2×3 groups of data, through which we identified the preferences of different LLMs for words of various parts of speech.

% %To be more specific, take the adjective "innovative" in the meta-reviews generated by GPT4o as an example. In human-written meta-reviews, this word only appeared 7 times. However, in the texts generated by GPT4o (llm-generated meta-reviews), it appeared 973 times. The $t$-score calculated for this word was 30.887, which is much higher than the critical value of 3.719 when $\alpha = 0.0001$. Thus, we reject the null hypothesis, indicating that GPT4o shows a clear preference for the word "innovative" during the process of generating meta-reviews.




% \subsubsection{word preferences}
% The \textbf{t-score} represents the statistical significance. \textbf{h\_cnt} indicates the number of occurrences of the word in human-written meta-review, and \textbf{llm\_cnt} represents the number of occurrences in llm-generated meta-review. ($\alpha$) is set to 0.0001, and the corresponding $t_{critical}$ is 3.7192040400651254.

\subsection{Validation of Detection Model Reliability}
We use our filtered full LLM-preferred word set and the identified pattern-level features to validate the reliability of our detection models.
Specifically, we classify all meta-reviews from ICLR 2024 into two groups based on the fine-grained detection results in Appendix~\S\ref{app:Fine-Grained}: human-written and LLM-generated (including LLM-refined and LLM-synthesized prediction). For each meta-review, we calculate the proportion of words that belong to the full GPT-4-preferred word set, defined as the ratio of matching words to the total number of words in the set. The average ratios for the human-written and LLM-generated groups are 35.15\% and 44.95\%, respectively. 
We then compute the FR and FI values for each pattern feature in each group, with results shown in Table~\ref{tab:pattern_vali}. The FR and FI values for predicted LLM-generated text are lower than those for predicted human-written text.
% These results  demonstrate the reliability of our model's detection capabilities.
These results provide evidence of the reliability of our model's detection capabilities.
% Then we compute the FR and FI value of each pattern feature in each group, the results are shown in Table~\ref{tab:pattern_vali}, 预测为LLM-generated text呈现的FR和FI value 低于预测为human-written的text


% \clearpage


\input{table/pattern_comparison}

\input{table/pattern_valid}



% Moreover, We use six metrics mentioned earlier to validate the correctness of our Fine-Grained Detection Model \S\ref{app:Fine-Grained} on ICLR 2024 meta-reviews.
%Then, we further verified our results on the ICLR 2024 Meta-reviews (three-class role recognition). We take the union of the LLM-synthesized and LLM-refined as LLM-mixed.
% Table~\ref{tab:personability_f}, ~\ref{tab:Interactivity_f} and ~\ref{tab:Attention_f} separately display the results of the six metrics. The values of the six metrics all show that LLM-synthesized < LLM-refined $\approx$ LLM-mixed < human-written. These further validate the correctness of our model.

% \subsection{Sentence and paper level}

% \clearpage
% \begin{minipage}{0.48\textwidth}
% \begin{table}[H]
%     \centering
%     \begin{tabular}{l|l|r|r}
%         \toprule
%         {\textbf{Data Type}}&\textbf{Resource }&\textbf{FR (\%)}&\textbf{FI}\\
%         \midrule
%         \multirow{5}{*}{Meta-review} 

%             & Human & 32.00& 1.62\\
%             & GPT4o & 0.07 & 1.00\\
%             & Gemini & 0.11& 1.33\\
%             & Claude & 0.07& 1.00\\
%         \midrule
%         \multirow{4}{*}{Review} 
%             & Human & 76.32 & 2.07\\
%             & GPT4o &0.00&0.00\\
%             & Gemini &0.00&0.00\\
%             & Claude & 0.00&0.00\\
%         \bottomrule
%     \end{tabular}
%     \caption{\textit{Personability} Pattern Comparison.}
%     \label{tab:personability_c}
% \end{table}


% \begin{table}[H]
%     \centering
%     \begin{tabular}{l|l|r|r}
%         \hline
%         {\textbf{Data Type}}&\textbf{Resource }&\textbf{FR (\%)}&\textbf{FI}\\
%         \toprule
%         \multirow{5}{*}{Meta-review} 

%             & Human& 2.01& 1.54 \\
%             & GPT4o& 0.00&0.00 \\
%             & Gemini& 0.00&0.00 \\
%             & Claude& 0.00&0.00 \\
%         \midrule
%         \multirow{4}{*}{Review} 
%             & Human& 17.11& 1.85 \\
%             & GPT4o& 0.00&0.00 \\
%             & Gemini& 0.00&0.00 \\
%             & Claude& 0.00&0.00 \\
%         \bottomrule
%     \end{tabular}
%     \caption{\textit{Interactivity} Pattern Comparison.}
%     \label{tab:Interactivity_c}
% \end{table}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{l|l|r|r}
%         \toprule
%         {\textbf{Data Type}}&\textbf{Resource }&\textbf{FR (\%)}&\textbf{FI}\\
%         \midrule
%         \multirow{5}{*}{Meta-review} 
%             & Human & 1.48&1.60\\
%             & GPT4o & 0.00&0.00\\
%             & Gemini & 0.00&0.00\\
%             & Claude & 0.00&0.00\\
% %            & meta\_overall & 0/519\\
% %            & meta\_overall\_except & 392/17003 \\
%         \bottomrule
%         \multirow{4}{*}{Review} 
%             & Human & 7.89&3.17\\
%             & GPT4o & 0.00&0.00\\
%             & Gemini & 0.00&0.00\\
%             & Claude & 0.00&0.00\\
%         \hline
%     \end{tabular}
%     \caption{\textit{Attention to Detail} Pattern Comparison.}
%     \label{tab:Attention_c}
% \end{table}
% \end{minipage}
% \hfill

% \begin{minipage}{0.48\textwidth}
% \begin{table}[H]
%     \centering
%     \begin{tabular}{l|l|r|r}
%         \hline
% \textbf{Role} &\textbf{FR (\%)}&\textbf{FI}\\
%         \hline

% LLM-synthesized&6.12&1.00\\
% LLM-refined&34.32&1.53\\
% LLM-generated&32.61&1.53\\
% human-written &39.50&1.77\\
% %            & meta\_meta & 10.77&1.18\\
% %            & meta\_meta\_comp & 40.11&1.72 \\
%         \hline
%     \end{tabular}
%     \caption{\textit{Personability} Pattern Comparison in ICLR 2024 Meta-reviews (detected by Fine-Grained Detection Model)}
%     \label{tab:personability_f}
% \end{table}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{l|r|r}
%         \hline
% \textbf{Role} & \textbf{FR (\%)}&\textbf{FI}\\
%         \hline

% LLM-synthesized &0.00&0.00\\
% LLM-refined &2.02 &1.53\\
% LLM-generated &1.89 &1.53\\
% human-written &4.87 &1.88\\
% %            & meta\_meta & 2/520&1.00\\
% %            & meta\_meta\_comp & 229/5260&1.84 \\
%         \hline
%     \end{tabular}
%     \caption{\textit{Interactivity} Pattern Comparison in ICLR 2024 Meta-reviews (detected by Fine-Grained Detection Model)}    \label{tab:Interactivity_f}
% \end{table}


% \begin{table}[H]
%     \centering
%     \begin{tabular}{l|r|r}
%         \hline
%         \textbf{Role} & \textbf{FR (\%)}&\textbf{FI}\\
%         \hline
%             LLM-synthesized &0.00&0.00\\
%             LLM-refined &1.57&1.13\\
%             LLM-generated &1.47&1.06\\
%             human-written &3.77&2.25\\
% %            & meta\_meta & 1/520\\
% %            & meta\_meta\_comp & 109/5260 \\
%         \hline
%     \end{tabular}
%     \caption{\textit{Attention to Detail} Pattern Comparison in ICLR 2024 Meta-reviews (detected by Fine-Grained Detection Model)}
%     \label{tab:Attention_f}
% \end{table}
% \end{minipage}





























