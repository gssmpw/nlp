
\section{\texttt{LLMetrica} Framework}
\label{sec: LLMetria}
In this section, we introduce the \texttt{LLMetrica} framework, designed to evaluate the penetration rate of LLM-generated content in scholarly writing and peer review. 
The framework includes rule-based metrics for assessing linguistic features and semantic similarity, as well as model-based detectors fine-tuned specifically to identify LLM-generated content within the scholarly domain.

% which includes rule-based metrics for evaluating linguistic features and semantic similarity, along with model-based detectors fine-tuned specifically to identify LLM-generated content within the scholarly domain.


% \subsection{Linguistic and Semantic Metrics}  
\subsection{Rule-Based Metrics: Preference}  
\label{sec:metric}
Rule-Based Metrics define a metric function $m$ to measure the feature value $v$ of an input text $x$, i.e., $v=m(x)$, enabling the comparison and evaluation of feature preferences in LLM-generated text. 
Specifically, we use 10 general linguistic feature metrics and design 4 specialized semantic feature metrics to capture both linguistic and semantic characteristics.


% Linguistic and semantic metrics are proposed to analyze and compare the structural, syntactic, and meaning-based features of texts, helping to identify LLM preferences and infer trends in its generated text.

\subsubsection{General Linguistic Features}


General linguistic features are applicable to all types of text and can be categorized into word-level, sentence-level, and other related metrics.
Specifically, word-level metrics include Average Word Length (AWL), Long Word Ratio (LWR), Stopword Ratio (SWR), and Type Token Ratio (TTR). Given the nature of scholarly writing, the threshold for `long word' is set at 10. 
For sentence-level metrics, we include Average Sentence Length (ASL), Dependency Relation Variety (DRV), and Subordinate Clause Density (SCD).
DRV quantifies the diversity of dependency relations within the text using Shannon entropy~\cite{lin1991divergence}, while SCD focuses on dependency relations such as `advcl', `ccomp', `xcomp', `relcl', and `acl'~\cite{nivre-etal-2017-universal}.
In addition, we incorporate Flesch Reading Ease (FRE)~\cite{farr1951simplification} to evaluate the overall readability of the text, Sentiment Polarity Score (PS, range: [-1, 1], negative→positive) to assess the sentiment, and Sentiment Subjectivity Score (SS, range: [0, 1], objective → subjective) to measure the degree of subjectivity or objectivity in the text. The implementation details of these metrics are provided in the Appendix~\ref{app:general}.



\subsubsection{Specific Semantic Features}
% applicable to all types of text and can be categorized into word-level, sentence-level, and other related metrics.
Inspired by \citet{du-etal-2024-llms}, which shows that human-written reviews have greater diversity and segment-level specificity than LLM-generated ones, we design four semantic metrics to analyze meta-reviews and reviews, focusing on overall semantic similarity and sentence-level specificity.



% \citet{du-etal-2024-llms} demonstrate that human-written reviews typically exhibit greater diversity and higher segment-level specificity compared to those generated by LLMs. Building on this insight, we conduct a quantitative analysis of MR and R, focusing specifically on two aspects: (i) the overall semantic similarity to the reference text, and (ii) sentence-level specificity at a finer granularity.



\paragraph{Overall Semantic Similarity}

We propose two semantic similarity metrics:
(i) \textbf{MRSim}: measures the similarity between the MR and its reference set R, defined as the average semantic similarity between MR and each review $r_i \in R$.
(ii) \textbf{RSim}: measures the similarity among reviews within R, defined as the maximum similarity among all pairs of reviews in R. The formulas are:
% The mathematical expressions are as follows:
\begin{equation}
\small{\mathrm{MRSim}=\frac{1}{\left| \mathrm{R} \right|}\sum_{r_i\in \mathrm{R}}{\mathrm{sim} \left( \mathrm{MR}, r_i \right)} }
\end{equation}
\begin{equation}
\small{\mathrm{RSim}=\underset{r_i,r_j\in \mathrm{R},r_i\ne r_j}{\max}\mathrm{sim} \left( r_i, r_j \right) }
\end{equation}
Using maximum similarity for RSim accounts for the fact that not all reviews in R are LLM-generated, as averaging could obscure key differences. Focusing on the maximum similarity highlights the strongest alignment, offering a more accurate measure of overall similarity.

% The use of maximum similarity for RSim considers that not all reviews in R are LLM-generated, so averaging could obscure key differences, while focusing on the maximum similarity captures the strongest alignment, providing a more accurate measure of overall similarity.



\begin{figure*}[ht]
    \centering
    % \vspace{1cm}
    \begin{minipage}{1\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{fig/feature.pdf}

        \subcaption{Comparison of Features for ALL Data Types}
    \end{minipage}

    \begin{minipage}{1\linewidth}
        \centering
        \scalebox{0.9}{
        \begin{tabular}{@{}l|lll@{}}
        \toprule
          Data Type  & ↑                        & ↓                          & →                 \\ \midrule
        \textbf{Abstract}    & 3 (\textbf{AWL}, \textbf{LWR}. TTR)         & 5 (\textbf{SWR}, ASL, DRV, SCD, \textbf{FRE}) & 2 (PS,SS)           \\
        \textbf{Meta-Review} & 3 (\textbf{AWL}, \textbf{LWR}. TTR)         & 3 (\textbf{SWR}, DRV, \textbf{FRE})           & 4 (ASL, SCD, PS, SS) \\
        \textbf{Review}      & 5 (\textbf{AWL}, \textbf{LWR}, ASL, PS, SS) & 3 (\textbf{SWR}, TTR, \textbf{FRE})           & 2 (DRV, SCD)         \\ \bottomrule
        \end{tabular}}

        \subcaption{Feature preference of LLM-generated text: ↑ indicates an increase across all LLMs, ↓ indicates a decrease, → indicates inconsistency. \textbf{Bold} denotes consistent trends across all data types.}
    \end{minipage}


    \caption{Comparison of Human-Written and LLM-Generated Text Based on \textbf{General Features} in \texttt{ScholarLens}}
    \label{fig:compare_general}
\end{figure*}


% \begin{figure*}[ht]
%     \centering
%     \includegraphics[width=1.0\linewidth]{fig/feature.pdf}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure*}






\paragraph{Sentence-Level Specificity}
Building on ITF-IDF~\cite{du-etal-2024-llms}\footnote{Unlike ITF-IDF~\cite{du-etal-2024-llms}, we only measure the SF-IRF within a single paper, considering the meta-review and review levels.} and the classic TF-IDF framework, we introduce the \textbf{SF-IRF} (\textbf{S}entence \textbf{F}requency-\textbf{I}nverse \textbf{R}everence \textbf{F}requency) metric to quantify the significance of sentences within a (meta-)review. 
Specifically, for a given target (meta)-review $r$ consisting of $n$ sentences, $\mathrm{SF}\text{-}\mathrm{IRF}\left( s,r,\mathrm{R}_{\mathrm{ref}} \right)$ captures the importance of a sentence $s$ in $r$ by considering: (i) its frequency of occurrence within $r$ (SF), and (ii) its rarity across the reference reviews $\mathrm{R}_\mathrm{ref}$ (IRF). The metric is formally defined as:
\begin{equation}
\footnotesize{
    \begin{aligned}
        \mathrm{SF}\text{-}\mathrm{IRF}\left( s,r,\mathrm{R}_{\mathrm{ref}} \right) &= \mathrm{SF}\left( s,r \right) \cdot \mathrm{IRF}\left( s, \mathrm{R}_{\mathrm{ref}} \right) \\
        &= \frac{O_{s}^{r}}{n} \cdot \log \left( \frac{m}{Q_{s}^{\mathrm{R}_{\mathrm{ref}}}} \right)
    \end{aligned}}
\end{equation}
Here, if $r$ represents a review, then $\mathrm{R}_{\mathrm{ref}}=\mathrm{R}-r$; if $r$ is a meta-review, then $\mathrm{R}_{\mathrm{ref}}=\mathrm{R}$. $O_{s}^{r}$ quantifies the ``soft'' occurrence of sentence $s$ within the target review $r$, while $Q_{s}^{\mathrm{R}_{\mathrm{ref}}}$ represents the ``soft'' count of reviews in $\mathrm{R}_{\mathrm{ref}}$ that contain the sentence $s$. Additionally, $m$ denotes the total number of reviews in $\mathrm{R}_{\mathrm{ref}}$. $O_{s}^{r}$ and $Q_{s}^{\mathrm{R}_{\mathrm{ref}}}$ are computed as follows:
\begin{equation}
\footnotesize{
    O_{s}^{r} = \sum\limits_{\tilde{s} \in r} \mathbb{I} \left( \mathop {\mathrm{sim}} \left( s, \tilde{s} \right) \geqslant t \right) \cdot \mathop {\mathrm{sim}}  \left( s, \tilde{s} \right)}
\end{equation}
\begin{equation}
\footnotesize{
    Q_{s}^{R_{\mathrm{ref}}} = \sum\limits_{\tilde{r} \in  R_{\mathrm{ref}}} \mathbb{I} \left( \mathop {\max \mathrm{sim}} \limits_{\tilde{s} \in \tilde{r}} \left( s, \tilde{s} \right) \geqslant t \right) \cdot \mathop {\max \mathrm{sim}} \limits_{\tilde{s} \in \tilde{r}} \left( s, \tilde{s} \right)}
\end{equation}
A segment $s$ is counted when its similarity exceeds the threshold $t$, with the corresponding similarity score.
% Following \citet{du-etal-2024-llms}, 
We use SentenceBERT~\cite{reimers-gurevych-2019-sentence} to calculate the all similarities, with $t$ set to 0.5.


% \subsection{Detection Models}
\subsection{Model-Based Detectors: Distinction}
\label{sec:detectors}
Model-based detectors are designed to train scholar-specific detection models 
$f$, capable of accurately identifying whether a scholarly input text $x$ is human-written or LLM-generated. We use our curated \texttt{ScholarLens} dataset to train these models, collectively referred to as ScholarDetect.




Specifically, we split abstracts and meta-reviews data within \texttt{ScholarLens} into training and test sets in a 7:3 ratio, based on the human-written versions. 
The corresponding LLM-generated content is partitioned accordingly, ensuring that each piece of LLM-generated text is paired with its human-written counterpart. 
All reviews data in \texttt{ScholarLens} are incorporated into the test set, ensuring a comprehensive evaluation.\footnote{Data statistics after splitting for model training and evaluation are in Table~\ref{tab:data_split}.}
All test sets
% , which include LLM-generated text from multiple LLM resources, 
serve as benchmarks to assess the performance of both baseline models and the trained ScholarDetect models.
We create three types of detection models based on the training data: one using only abstracts, one using only meta-reviews, and one using a hybrid of both.
To maintain class balance in the training data, we ensure a 1:1 ratio between human-written and LLM-generated version. We employ two strategies: one using a single LLM (GPT-4o, Gemini, or Claude), and another using a mixed-LLM approach, where each human-written piece is paired with LLM-generated content from a randomly selected model.
As a result, the ScholarDetect framework involves a total of 12 distinct detection models.


