
\section{Experiments}
\label{sec:Experiments}
We first demonstrate the effectiveness of rule-based metrics (\S\ref{sec: Features Comparison}) and ScholarDetect models (\S\ref{sec:ScholarDetect Evaluation}) in \texttt{LLMetrica}, then apply these methods to real-world conference data to assess and predict LLM penetration trends (\S\ref{sec:Temporal Analysis}). Finally, case studies are used to explore the specific differences between human-written and LLM-generated content (\S\ref{sec: Case Study}).

% case studies provide deeper insights into the distinguishing features of human-written vs. LLM-generated content (\S\ref{sec: Case Study}).

% we conduct case studies to gain deeper insights into the distinctive characteristics of human-written and LLM-generated content (\S\ref{sec: Case Study}).





\subsection{Features Comparison: Human vs LLM}
\label{sec: Features Comparison}
We apply the proposed rule-based metrics (\S\ref{sec:metric}) to  \texttt{ScholarLens} to compare the features of human-written and LLM-generated texts, and find that the feature preferences of LLM-generated texts can be effectively compared and evaluated.

% Results are shown in Figure~\ref{fig:compare_general} (general features) and Figure~\ref{fig:compare_specific} (specific features).


\input{table/detection_compare}
\paragraph{General Linguistic Features}

Figure~\ref{fig:compare_general} shows trends in the characteristics of LLM-generated texts, with slight variations across different data types. Each metric reflects the consistency of features across texts generated by the three LLMs in at least one data type, demonstrating the `comparability' effectiveness of the chosen metrics.
Moreover, regardless of the data type or LLM used, LLM-generated texts consistently show higher values for Average Word Length (AWL) and Long Word Ratio (LWR), and lower values for Stopword Ratio (SWR) and Readability (FRE). This suggests that LLM-generated texts tend to use longer words, avoid excessive stopwords, and have lower readability.
For shorter text types, such as abstracts and meta-reviews, the observed increase in Type Token Ratio (TTR) reflects greater lexical diversity in LLM-generated texts.
This may be due to the conciseness inherent in short-form LLM-generated content. In contrast, for longer reviews, TTR decreases, potentially highlighting the limitations of LLMs in producing long-form content~\cite{wang-etal-2024-m4, wu2025survey}. Longer reviews may lack specificity~\cite{du-etal-2024-llms}, resulting in redundancy and repetitive segments.
Additionally, LLM-generated reviews tend to be more positive and subjective, suggesting a more favorable tone and less neutral objectivity. This aligns with \citet{jin-etal-2024-agentreview}, who found that LLM-generated reviews generally assign higher scores and show a higher acceptance rate.

\paragraph{Specific Semantic Features}
Figure~\ref{fig:compare_specific} shows the preferences of human-written and LLM-generated texts in both meta-reviews and reviews, based on four specific semantic features.
Notably, Since each LLM generates only one review per paper in \texttt{ScholarLens}, while each paper usually has multiple reviews, we combine reviews from all three LLMs into a unified set, so comparisons do not distinguish between them.
Comparative results show that LLM-generated meta-reviews exhibit higher semantic similarity to the referenced reviews, with lower sentence specificity.
This suggests that sentences within LLM-generated meta-reviews are more semantically similar to each other (prone to redundancy) and tend to mirror the content of the referenced reviews. 
A similar trend is observed for reviews, where the two specific features also show consistent patterns.
It is important to note that for the reviews, we assume all are LLM-generated in this experiment, which may amplify the differences in these semantic features. In reality, having more than two LLM-generated reviews per paper may be uncommon, which would likely reduce the observed disparity.

% Figure~\ref{fig:compare_specific} shows that LLM-generated meta-reviews exhibit higher semantic similarity to the referenced reviews. Additionally, the specificity of segments is lower, indicating that segments within LLM-generated meta-reviews are more semantically similar to each other (prone to redundancy) and tend to mirror the content of the referenced reviews. Similarly, for reviews, the trends observed in the two specific features are also consistent.
% It is important to note that for the reviews, we assume all are LLM-generated in this experiment, which may amplify the differences in the two semantic features. In reality, having more than two LLM-generated reviews per paper is rare, which would likely reduce the observed disparity.


\begin{figure}[t]
    \centering
    \begin{minipage}{0.55\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/meta_specific_compare.pdf}
        \captionsetup{font=footnotesize}
        \subcaption{Meta-Review}
    \end{minipage}
    \hfill
    \begin{minipage}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/review_specific_compare.pdf}
        \captionsetup{font=footnotesize}
        \subcaption{Review}
    \end{minipage}
    \hfill
    \caption{Comparison of Human-Written and LLM-Generated Text Based on \textbf{Specific features} for Review and Meta-Review.}
    \label{fig:compare_specific}
\end{figure}
% AgentReview~\cite{jin-etal-2024-agentreview}.

\subsection{ScholarDetect Evaluation: Detectability}
\label{sec:ScholarDetect Evaluation}
We evaluate the trained model-based detectors, ScholarDetect (\S\ref{sec:detectors}), on the \texttt{ScholarLens} test sets and find that scholarly LLM-generated texts can be effectively identified.




\paragraph{Experimental Setup}
\textbf{(i)} \textbf{Training Setup}: 
We adopt Longformer~\cite{Beltagy2020Longformer} as the base model for training our ScholarDetect detection models, as it has shown competitive performance among pretrained language models~\cite{li-etal-2024-mage, cheng2024beyond}. Specifically, we train for five epochs in each configuration of the training set, using a learning rate of 2-e5.
\textbf{(ii)} \textbf{Metric}: 
For evaluation metrics, we report the F1 score for each class (human-written and LLM-generated), as well as the overall weighted F1 score to account for class imbalance. 
Each experimental setup (training data type and LLM-generated text source) is evaluated through three random trials, and we report the average performance along with the standard deviation. 
\textbf{(iii)} \textbf{Baselines}:
We compare the performance of three advanced detection model baselines: MAGE~\cite{li-etal-2024-mage}, RAIDetect~\cite{dugan-etal-2024-raid}, and HNDCDetect~\cite{cheng2024beyond}.
\textbf{(iv)} \textbf{Test Sets}:
All models are evaluated on test sets from three data types: abstract, meta-review, and review, with the first two being shorter texts and reviews being long-form. 
The LLM-generated data includes tasks such as refinement (for abstracts) and summarization (for meta-reviews and reviews).

\paragraph{Experimental Results}
The detection performance comparison results are presented in Table~\ref{tab:fine-detection}. Our trained ScholarDetect models consistently outperform the existing advanced baseline models, underscoring the importance of developing detection systems specifically tailored for the scholarly domain.
The training approach that combines mixed LLM sources and hybrid data types yields the best overall performance, demonstrating robustness across various LLM sources and data types.
Interestingly, the model trained on meta-reviews performs best when tests on reviews, likely because both data types share a similar comment-based focus and offer a ``synthesized'' perspective in LLM-generated text. 
This is further supported by ScholarDetect$_\text{Abs}$, which struggles to identify LLM-generated reviews when trained only on abstracts (F1: LLM < Human).
Additionally, when trained on a single LLM source, the GPT-4o-based detectors show the strongest generalization, especially on the abstract test set. 
Most ScholarDetect models outperform human-written text in detecting LLM-generated content on the meta-review and review test sets, but the reverse is true for the review test set.


% Here we present the results of training longformer. The loss rate of the trained model is shown in the table~\ref{tab:longformer_training_loss}, and the model is named according to the timestamp at which it was trained.

% We used our trained models to detect the metareview data from ICLR 2020-2024. We found that the AI generation rate was low from 2020 to 2023, but there was a sharp increase in 2024. Although the training was based on metareview data, we also applied our models to analyze abstracts, where a similar sharp rise was observed in 2024. Please see figure~\ref{fig:AI-generated metareview percentage} and figure~\ref{fig:AI-processed abstract percentage} for details.



\begin{figure}[t]
    \centering
    \begin{minipage}{0.98\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/feature_trend_Abstract.png}
        \captionsetup{font=footnotesize}
        \subcaption{Abstract}
    \end{minipage}
    \hfill
    \begin{minipage}{0.98\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/feature_trend_Meta-Review.png}
        \captionsetup{font=footnotesize}
        \subcaption{Meta-Review}
    \end{minipage}
    \hfill
    \begin{minipage}{0.98\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/feature_trend_Review.png}
        \captionsetup{font=footnotesize}
        \subcaption{Review}
    \end{minipage}
    \hfill

    \caption{Temporal trends based on \textbf{four robust general linguistic metrics}.
    % \red{$\boldsymbol{\uparrow \downarrow \rightarrow}$} represents the feature preference of LLM-generated text, as identified in the comparisons in Figures~\ref{fig:compare_general} and~\ref{fig:compare_specific}. \green{$\boldsymbol{\checkmark}$} indicates alignment between the feature trend and preference, signifying increased LLM penetration.
    }
    \label{fig:trend_general}
\end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{minipage}{1\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/specific_trend_meta.PNG}
%         \captionsetup{font=footnotesize}
%         \subcaption{Meta-Review}
%     \end{minipage}
%     \hfill
%     \begin{minipage}{1\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/specific_trend_review.PNG}
%         \captionsetup{font=footnotesize}
%         \subcaption{Review}
%     \end{minipage}
%     \hfill
%     \caption{Temporal trends based on \textbf{specific semantic metrics}.}
%     \label{fig:trend_specific}
% \end{figure}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/specific_trend_meta.png}
        \captionsetup{font=footnotesize}
        \subcaption{Meta-Review}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{fig/specific_trend_review.png}
        \captionsetup{font=footnotesize}
        \subcaption{Review}
    \end{minipage}
    \hfill
    \caption{Temporal trends based on \textbf{specific semantic metrics}.}
    \label{fig:trend_specific}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/detection_trend_conference.png}
    \caption{Abstarct: Trend based on detection model.}
    \label{fig:trend_detect_abs}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{fig/detection_trend_review.png}
    \caption{Abstarct: Trend based on detection model.}
    \label{fig:trend_detect_review}
\end{figure}


\subsection{LLM Penetration: Temporal Analysis}
\label{sec:Temporal Analysis}
We apply the proposed rule-based metrics and model-based detectors to assess and detect LLM penetration in recent scholarly texts (up to 2024), including abstracts, meta-reviews, and reviews. 

% We apply the proposed rule-based metrics and model-based detectors to assess and detect LLM penetration in recent scholarly texts (up to 2024), including abstracts, meta-reviews, and reviews.
% specifically, for rule-based metrics, we only adopt the four most robust general linguistic metrics across three data types.

\paragraph{Trend in Rule-based Evaluation}
For the general linguistic metrics, we use only the four most robust (AWL, LWR, SWR, FRE), which show consistent preferences across the three data types, and we adopt all four specific semantic metrics.
Figure~\ref{fig:trend_general} illustrates the trend in general linguistic features across three data types in ICLR, while Figure~\ref{fig:trend_specific} shows the trend in specific semantic features for meta-reviews and reviews. 
Almost all the metrics show consistent LLM preference trends across their associated data types, with an overall year-on-year increase, supporting the rising trend of LLM penetration in scholarly writing.
Interestingly, among these metrics used to evaluate reviews, four show anomalous trend changes in 2023, highlighting the difficulties of using rule-based metrics to track LLM penetration in the complex and varied nature of review data.



% Additionally, six features in review data show anomalous trend changes in 2023, highlighting the challenges of using rule-based metrics to track LLM penetration.
% For the specific metrics, except for the SFIRF feature in reviews, which shows the same anomalous trend changes in 2023, the remaining three metrics also demonstrate a clear increasing trend in LLM penetration.



% These trends partially demonstrate the increasing LLM penetration but also highlight the limitations and challenges of relying on simple rule-based metrics.
% Specifically, regarding the general metrics, the four features (AWL, LWR, SWR, FRE) with consistent preferences in Figure~\ref{fig:compare_general}(b) demonstrate increasing LLM penetration across all data types, except for an outlier in the FRE feature for reviews in 2023. This underscores the robustness of these metrics. 
% These general metrics capture LLM penetration most effectively for abstracts, but fewer features exhibit this trend for meta-reviews and reviews, likely due to the more standardized nature of abstracts compared to the complexity of comment-type data. 
% Additionally, six features in review data show anomalous trend changes in 2023, highlighting the challenges of using rule-based metrics to track LLM penetration.
% For the specific metrics, except for the SFIRF feature in reviews, which shows the same anomalous trend changes in 2023, the remaining three metrics also demonstrate a clear increasing trend in LLM penetration.

% Specifically, the red arrows next to the title of each sub-figure represent the feature preference of LLM-generated text, as identified in the feature comparisons presented in Figures~\ref{fig:compare_general} and~\ref{fig:compare_specific}. The green checkmarks above the x-axis indicate that the feature trend for each corresponding metric aligns with the feature preference, signifying an increase in LLM penetration.

% \red{$\boldsymbol{\uparrow \downarrow \rightarrow}$} represent the feature preference of LLM-generated text, as identified in the feature comparisons presented in Figures~\ref{fig:compare_general} and~\ref{fig:compare_specific}. \green{$\boldsymbol{\checkmark}$} indicates that the feature trend for each corresponding metric aligns with the feature preference, signifying an increase in LLM penetration.

\paragraph{Trend in Model-based Detection}
Based on the performance shown in Table~\ref{tab:fine-detection} and the available evaluation data, we select ScholarDetect$_\text{Hybrid}$, which performs best on abstracts, to detect instances of LLM-assisted writing in seven conference abstracts.
% Based on the performance presented in Table~\ref{tab:fine-detection} and the available evaluation data sources, we adopt ScholarDetect$_\text{Hybrid}$, which performs best on abstracts, to probe seven conference abstracts for identifying instances of LLM-assisted writing.
Additionally, we utilize three variants of ScholarDetect (Abs, Meta, Hybrid) to analyze all ICLR meta-reviews and reviews. The detected LLM penetration rates (i.e., the proportion of text predicted to be LLM-generated) are presented in Figures~\ref{fig:trend_detect_abs} and~\ref{fig:trend_detect_review}.
In the abstract evaluation data, the LLM penetration rate across all involved conferences increases starting in 2023 and continues to rise in 2024, likely driven by ChatGPT’s initial release in November 2022 and its subsequent updates.
In contrast, a noticeable increase appears in 2024 for comment-based data, particularly in reviews, although the overall rate remains lower than in abstracts.
This may be attributed to the 2023 update of ChatGPT\footnote{\href{https://help.openai.com/en/articles/6825453-chatgpt-release-notes}{ChatGPT — Release Notes}}, which enabled PDF uploads and content analysis, as well as the higher standards required for LLM-generated content in reviews, which limit the penetration rate.
Specifically, ScholarDetect$_\text{Hybrid}$ predicts the highest LLM penetration rate for two comment-based data types in 2024.  For shorter meta-review texts, ScholarDetect$_\text{Abs}$'s rate is close to ScholarDetect$_\text{Hybrid}$ but higher than ScholarDetect$_\text{meta}$. We hypothesize this is due to the greater role of LLMs in refining these texts. Based on insights from ~\citet{cheng2024beyond}, we propose a fine-grained LLM-generated text detection approach using three-class role recognition (human-written, LLM-synthesized, LLM-refined) for meta-reviews. Our results show that the LLM-refined role plays a more dominant part in LLM penetration.\footnote{Experimental details of the three-class LLM role recognition are in Appendix~\ref{app:Fine-Grained}.}



\begin{table}[t]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{@{}l|p{8cm}}
    \toprule
    \textbf{POS} & \textbf{ Top 10 GPT-4o Preferred Words in Meta-Reviews} \\ \midrule
    \textbf{\small NOUN}   & \small \underline{\textbf{refinement}}, \underline{\textbf{advancements}}, \underline{\textbf{methodologies}}, \underline{\textbf{articulation}}, 
    \textbf{highlights}, \underline{reliance}, \underline{\textbf{enhancement}}, \underline{\textbf{underpinnings}}, \underline{\textbf{enhancements}}, \underline{\textbf{transparency}}  \\ \midrule
    \textbf{\small VERB}   & \small \underline{enhance}, \underline{enhancing}, deemed, \underline{\textbf{showcasing}}, express, \underline{offering}, \underline{enhances}, \underline{\textbf{recognizing}}, commend, praised                  \\ \midrule
    \textbf{\small ADJ}  & \small \underline{\textbf{innovative}}, \underline{\textbf{collective}}, \underline{enhanced}, \underline{\textbf{established}}, \underline{notable}, \underline{outdated}, varied, \underline{undefined}, \underline{\textbf{comparative}}, \underline{\textbf{noteworthy}}
            \\ \midrule
    \textbf{\small ADV}  & \small \underline{\textbf{collectively}}, \underline{\textbf{inadequately}}, \underline{\textbf{reportedly}}, \underline{\textbf{comprehensively}}, \underline{robustly}, \underline{\textbf{occasionally}}, \underline{\textbf{predominantly}}, \underline{notably}, \underline{\textbf{innovatively}}, \underline{\textbf{effectively}}
          \\ 
  \bottomrule
  \toprule
    \textbf{POS} & \textbf{ Top 10 GPT-4o Preferred Words in Abstracts} \\ \midrule
    \textbf{\small NOUN}   & \small abstract, \underline{\textbf{advancements}}, realm, \underline{\textbf{alterations}}, aligns, \underline{\textbf{methodologies}}, \underline{clarity}, \underline{\textbf{adaptability}}, \underline{surpasses}, \underline{\textbf{examination}}
  \\ \midrule
    \textbf{\small VERB}   & \small \underline{enhancing}, \underline{\textbf{necessitates}}, \underline{\textbf{necessitating}}, \underline{featuring}, \underline{revised}, \underline{\textbf{influenced}}, \underline{\textbf{encompassing}}, \underline{enhances}, \underline{\textbf{showcasing}}, \underline{surpasses}
                  \\ \midrule
    \textbf{\small ADJ}  & \small \underline{\textbf{innovative}}, \underline{\textbf{exceptional}}, \underline{pertinent}, \underline{intricate}, \underline{pivotal}, \underline{\textbf{necessitate}}, \underline{\textbf{distinctive}}, \underline{enhanced}, akin, potent
            \\ \midrule
    \textbf{\small ADV}  & \small \underline{\textbf{inadequately}}, \underline{\textbf{predominantly}}, \underline{\textbf{meticulously}}, \underline{\textbf{strategically}}, \underline{notably}, abstract, swiftly, \underline{\textbf{additionally}}, \underline{adeptly}, \underline{thereby}
          \\ 
  \bottomrule
    \end{tabular}
    }
    \caption{Top-10 LLM-preferred words in GPT-4o-generated vs. human-written meta-reviews and abstracts. \textbf{Bold} denotes \textit{long words}, and \underline{underlined} denotes \textit{complex-syllabled words}.}
    \label{tab:word_case}
\end{table}

\subsection{Case Study}
\label{sec: Case Study}
To investigate the specific differences between LLM-generated and human-written text, we focus on GPT-4o, conducting case studies at both the word and pattern levels. 
(i) At the word level, we design a Two-Sample t-test based on word proportions~\cite{cressie1986use, 10.1093/biomet/34.1-2.28}\footnote{\url{https://www.statology.org/two-sample-t-test/}. Method details of case studies and additional results are in the Appendix~\ref{app: cs-details}.} to identify the LLM-preferred words.
Table~\ref{tab:word_case} shows the top 10 preferred words in four key part-of-speech (POS)
% \footnote{\lz{We have tried various POS tagging tools such as NLTK, spaCy, Stanford CoreNLP and others. All the tools that we have tried have the problem of mislabeling the POS of minor words.}} 
categories from GPT-4o-generated abstracts and meta-reviews, compared to those in human-written versions. We find that LLMs tend to generate \textit{long words} ($\geq$ 10 letters) and \textit{complex-syllabled words} ($\geq$ 3 syllables)~\cite{gunning1952technique}. This further supports the reliability of the four general linguistic metrics for assessing LLM penetration.
Moreover, GPT-4o shows a strong preference for the word `enhance' in scholarly writing and peer reviews, with its variants appearing in the top 10 list.
(ii) At the pattern level, manual inspection of paired data samples from comment-based data\footnote{Conducted by one of the authors on 100 paired meta-reviews and 20 paired reviews.}, followed by automated evaluation of the full dataset, reveals that human-written (meta-)reviews exhibit: 
\textit{personability}, frequently using the first person to express opinions; \textit{interactivity}, often incorporating questions; and \textit{attention to detail}, citing relevant literature to support arguments. 


%\subsubsection{Word-level}
% Considering the validity of word-level preference, we introduce a two-sample statistical hypothesis-testing framework~\cite{cressie1986use, 10.1093/biomet/34.1-2.28}\footnote{\url{https://www.statology.org/two-sample-t-test/}} to explore LLMs' preferred words. The details are presented in the \ref{subsubsec:hypothesis_calculation_judgment} section of the appendix.

% A manual inspection of a selected word set reveals some typical features of words preferred by LLMs:
% (1) \textbf{Rich and precise in meaning}: Words that convey a complex and exact sense. (e.g., "Applicability"(noun) specifically refers to "the applicability of a theory)
% (2) \textbf{Strong sense of academic formality}: Words used to maintain a serious and formal tone. (e.g., “Enhance” implicitly conveys the precise meaning of "systematic improvement", adding a serious and formal style to texts)

%Table~\ref{tab:gpt4o_preferred_meta_t5} presents the top 5 most preferred words for GPT4o, categorized by POS in the meta-review. These words reflect the two features mentioned before.

%For instance: 
%"Comprehensive"(adjective) precisely means "thorough and without omission". 
%"Applicability"(noun) specifically refers to "the applicability of a theory" .
%"Enhance" (verb) is more formal than "improve" and implicitly conveys the precise meaning of "systematic improvement".
%"particularly"(adverb) precisely means "especially", adding a serious and formal style to texts.



% \begin{table}[h]
%    \centering
%    \scalebox{0.9}{
%        \begin{tabular}{@{}l|p{2cm}|c|p{2cm}@{}}
%            \toprule
%            \textbf{POS} & \begin{tabular}[c]{@{}c@{}}\textbf{word}\end{tabular} & \textbf{POS} & \begin{tabular}[c]{@{}c@{}}\textbf{word}\end{tabular} \\
%            \midrule
%            \multirow{5}{*}{\textbf{Noun}} 
%            & clarity & \multirow{5}{*}{\textbf{Adj}} 
%            & innovative \\
%            & applicability &  & potential \\
%            & comparisons &  & comprehensive \\
%            & validation &  & theoretical \\
%            & improvements &  & insufficient \\
%            \midrule
%            \multirow{5}{*}{\textbf{Verb}} 
%            & existing & \multirow{5}{*}{\textbf{Adv}} 
%            & particularly \\
%            & introduces &  & however \\
%            & enhance &  & effectively \\
%            & presents &  & collectively \\
%            & lacks &  & especially \\
%            \bottomrule
%        \end{tabular}
%    }
%    \caption{Top5 preferred words for GPT4o categorized by POS(meta-review)}
%    \label{tab:gpt4o_preferred_meta_t5}
% \end{table}

%\subsubsection{Sentence and paper Level}
%Further more, we conducted an analysis of the differences between human-written and llm-generated texts at the sentence and paper level, focusing on meta-reviews and reviews.
%Furthermore, we proposed three major categories of features - \textbf{language style features}, \textbf{structural pattern features}, and \textbf{content functional features} - to analyse the differences between human-written and LLM-generated texts at the sentence and paper levels, focusing on meta-reviews and reviews.

%\paragraph{meta-review:} In meta-reviews, LLMs have features as follows:
%\textbf{language style features:}
%(1) \textbf{Question Usage Style:} LLMs don't like to use questions. Instead, they prefer to state the information directly.
%(2) \textbf{First-Person Term Frequency:} First-person expressions such as “we...” and “I...” are rare in LLM-generated meta-reviews.
%\textbf{structural pattern features:}
%(3) \textbf{No additional Citations:} LLMs do not cite additional materials, such as citations like "[1] https://arxiv.org/pdf/2112.06905.pdf" in review.
%(4) \textbf{Preferred Opening Phrases:} Different LLMs have their own favored starting patterns. E.g., the pattern “(The) Reviewers...” is preferred by Gemini and Claude.
%\textbf{content functional features:}
%(5) \textbf{Content Focus Difference:} LLMs mainly focus on providing summaries in meta-reviews. In contrast, human offer more detailed suggestions or different opinions.% or express the reviewers' attitudes towards the paper.

%The specific examples and data used for this analysis are included in the appendix for reference.

%\paragraph{review:} %The review consists of five parts: 1. Summary of the Paper; 2. Strengths and Weaknesses; 3. Clarity, Quality, Novelty, and Reproducibility; 4. Summary of the Review. 
%In reviews, LLMs exhibit the same features in terms of (1) \textbf{Question Usage Style} and (2) \textbf{First-Person Term Frequency} as in meta - reviews. (3) \textbf{No additional Citations:}
%LLMs also exhibit some more specific features
%(4) \textbf{Content Telling Style:} In the "Summary of the Paper" part, LLMs directly state the paper's content, while human often mention the authors' thoughts lhor thinks...", which rarely appears in LLM - generated paper summaries.
%(5) \textbf{Preferred Opening Phrases:} In the "Summary of the Review" section of review, GPT4o, Gemini and Claude all prefer to "This/The paper...".
%(6) \textbf{Cliche-filled summary:} In the "Summary or the review" part, LLMs often make broad and empty statements and exaggerate by overusing general phrases like "significant contribution".%, and this situation is more obvious in reviews than in meta-reviews.
%(6) \textbf{No additional Citations:} LLMs do not cite additional materials, such as citations like "[1] https://arxiv.org/pdf/2112.06905.pdf" in review.

%The specific examples and data used for this analysis are included in the appendix for reference.
