\section{Related Work}
\label{sec:related_work}

\noindent\textbf{Instructable Retrieval.}
%
Multimodal representation for retrieval**Faghri et al., "VSE++: Improving Visual-Semantic Embeddings with Multimodal and Multi-Task Learning"** has been a prominent area of research demonstrating strong performance. %
%
To handle compositional requests that require reasoning about the visual content, recent work focused on composed retrieval**Goyal et al., "Unifying Text-to-Image Synthesis with Adversarial Training"**.
However, these methods focus on limited specific retrieval domains and can only follow predefined instruction templates.
%
To address this limitation, recent models have been made instructable**Torkzadehmahani et al., "Instructable Neural Image Retrieval for Compositional Queries"** to capture richer multimodal relationships. 
MagicLens**Zhu et al., "Magic Lens: A Visual Interface for Image Retrieval with Multimodal Interaction"** is a simple instructable model trained on a curated large dataset including instructions, while UniIR**Huang et al., "Unified Image and Text Embeddings via Cross-Modal Hashing"** proposed training CLIP/BLIP variants conditioned on prompted instructions.
%
While these approaches have demonstrated significant progress, they lack expressiveness in the multimodal output space due to the limitations of CLIP models. 
In contrast, methods based on LMMs**Huang et al., "L2S-Net: Learning Robust Visual-Semantic Embeddings for Image Retrieval"** that are fine-tuned for retrieval do not preserve the original capabilities of the LMMs.
%
%
\modelname aims to overcome this issue and preserve capabilities by design by enabling a frozen LMM to retrieve relevant content and generate textual responses tailored to both the input question and the retrieved visual content.
%
%

\noindent\textbf{Retrieval via Generation.}
LMMs**Huang et al., "L2S-Net: Learning Robust Visual-Semantic Embeddings for Image Retrieval"** have demonstrated remarkable reasoning capabilities when processing language and visual elements (see survey in**Radford et al., "Learning to Generate Long-Term Structure of a Sentence with Graph-to-Sequence Model"**).
%
Recently, LMMs were combined with diffusion models to generate images along with textual answers**Nash et al., "Generative Models for Visual Question Answering"**.
%
%
Despite providing an interesting user experience, the generated images are by nature not grounded to real entities, which is critical for retrieval applications, such as online shopping, news, or Wikipedia.
%
%
These models are based on memory-intensive diffusion models and trained with a reconstruction objective that is not in line with retrieval.
%
%
In contrast, \modelname uses a retrieval approach that preserves the factuality of the images that are shown to the user, while commenting them in a generative way. 
It is lightweight, as it does not use diffusion models, and the LMMs are frozen, thus preserving their reasoning capabilities of other non-retrieval tasks.


\noindent\textbf{Retrieval-Augmented Generation (RAG).}
The parametric way that language models are trained and store knowledge limits the ease to expand or revise their memory with more recent information.
RAG**Lewis et al., "Pre-training by Prediction"** and Multimodal RAG**Bajaj et al., "Multimodal Retrieval-Augmented Generation"** have been proposed as a non-parametric way to provide up-to-date knowledge to an LMM by adding relevant content to the prompt before processing by the LMM.
Using this augmented prompt, the LMM can select, summarize and alter the entities to generate its output.
 %
\modelname differs from traditional RAG in that it processes user input via the LMM to build a better query and outputs the retrieved entity unaltered.
Then, \modelname's entity adapter module is trained to feed optimized entity representations to the LMM, which guides the extraction of the information necessary to answer the request.
%
%
%
%
%
%
%

\noindent\textbf{\tasknametitle (\tasknameshort).}
\tasknameshort is an underexplored task that requires not only retrieving relevant images, as in composed retrieval, but also generating plausible textual answers that refer to the image and contain additional information.
While preliminary work**Alayrac et al., "Visual BERT: A Simple and Efficient Model for Visual-Semantic Reasoning"** and datasets**Huang et al., "L2S-Net: Learning Robust Visual-Semantic Embeddings for Image Retrieval"** focuse on some aspects of this task, they have two key limitations: 
1) The complexity of the textual answers is constrained, typically limited to a few words or basic captions.
2) The textual answers are aligned with the image, either providing redundant information (e.g., image captions) and failing to add useful complementary information.
To fully cover the \tasknameshort task, we claim that the answers should be multimodal in nature, with the image and text working in tandem to provide a richer, more comprehensive response.
The modalities should have a clear connection, yet offer complementary information that enhances the overall response.
To alleviate the limitations of existing \tasknameshort datasets, we created two challenging human-curated ones that we hope will foster future research in this area.