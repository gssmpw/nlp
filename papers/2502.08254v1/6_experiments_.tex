\section{Experiments}
\label{sec:experiments}

We evaluate the multimodal retrieval and commenting capabilities of our methods with respect to the state of the art in Sec.~\ref{ssec:exp_retrieval} and Sec.~\ref{ssec:exp_commenting}, respectively.


\begin{table*}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{l c ccc c ccc c ccc c ccc c ccc c}
    \toprule
    & 
    & \multicolumn{3}{c}{\textbf{\fashioniq}~\cite{guo2019fashion}} &
    & \multicolumn{3}{c}{\textbf{\cirr}~\cite{Liu_2021_ICCV}} &
    & \multicolumn{3}{c}{\textbf{OVEN}~\cite{hu2023open}} &
    & \multicolumn{3}{c}{\textbf{InfoSeek}~\cite{chen2023can}} &
    & \multicolumn{3}{c}{\textbf{\wikicomment}} &
    \textbf{Average} \\
    \cmidrule{3-5} \cmidrule{7-9} \cmidrule{11-13} \cmidrule{15-17} \cmidrule{19-21} 
    &
    & \textbf{R@10} & \textbf{R@20} & \textbf{R@50} &
    & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} &
    & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} &
    & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & 
    & \textbf{R@1} & \textbf{R@5} & \textbf{R@10} & 
    \\
    \midrule
    CLIP (zero-shot)~\cite{radford2021clip} & 
    & $5.50$ & $8.26$ & $13.84$ & 
    & $0.86$ & $12.13$ & $18.30$ & 
    & $10.79$ & $24.12$ & $30.47$ &
    & $8.73$ & $21.79$ & $29.62$ & 
    & $13.24$ & $25.04$ & $30.50$ & 
    $16.88$ 
    \\
    UniIR CLIP$_\text{SF}$~\cite{wei2023uniir} & 
    & $24.59$	& $32.22$	& $\mathbf{43.66}$ & 	
    & $9.50$	& $45.01$	& $58.61$ & 	
    & $49.50$	& $68.86$	& $74.56$ & 	
    & $\mathbf{27.08}$	& $\mathbf{49.04}$	& $\mathbf{58.24}$ & 	
    & $21.73$	& $40.72$	& $46.76$	& 
    $43.34$ 
    \\    
    %
    %
    %
    %
    %
    %
    %
    %
    \modelname (ours) &  
    & $\mathbf{25.20}$	& $\mathbf{32.40}$	& $43.33$	& 
    & $\mathbf{16.45}$	& $\mathbf{47.24}$	& $\mathbf{59.76}$	& 
    & $\mathbf{51.08}$	& $\mathbf{70.22}$	& $\mathbf{75.54}$	& 
    & $26.41$	& $48.01$	& $57.24$   &    
    & $\mathbf{40.14}$   & $\mathbf{57.70}$	& $\mathbf{67.05}$ & 
    $\mathbf{47.85}$ 
    \\   
    \bottomrule
    \end{tabular}
    }
    \caption{\underline{Main results on retrieval}. Retrieval results in terms of recall on different datasets while comparing with state of the art methods.}
    \label{tab:retrieval_main}
\end{table*}

\subsection{Retrieval Results}
\label{ssec:exp_retrieval} 

\paragraph{Implementation Details.} 
We choose InternVL2-4B~\cite{chen2024internvl} as the base, frozen LMM and use it to extract hidden states $\hiddenstate(q)$ corresponding to queries.
Specifically, we take the 3072-dimensional hidden state of the last transformer layer after processing of the last input token before generating the response.
%
We train the encoder $\psi_\mmmod$ and adapter $\psi_\lmmmod$ using the UniIR framework~\cite{wei2023uniir} so as to align the queries and targets using contrastive loss~\cite{ilharco2021openclip} in two stages.
First, we train the hidden state adapter exclusively to align them to UniIR embeddings, so we can obtain a good initialization.
Second, as depicted in Fig.~\ref{fig:comment_retrieval}, we additionally unlock parameter $\beta$ and the multimodal encoder, initialized from a pre-trained CLIP ViT-L-14 model~\cite{radford2021clip}. We append instructions in the query as proposed by~\cite{wei2023uniir} and train a joint model on all 5 datasets\footnote{Fashion-IQ, CIRR, OVEN, and InfoSeek are a subset of M-BEIR~\cite{wei2023uniir} that has multimodal inputs.}, i.e Fashion-IQ, CIRR, OVEN, InfoSeek and Wiki-CoR. %
For Wiki-CoR, we randomly sample comment or caption as text with 50\%/50\% chance. We train for 40 epochs using learning rate 1e-5, batch size 55, effectively 110 thanks to gradient accumulation of 2, on a single host with 8$\times$48GB NVIDIA L40S GPUs.



\paragraph{Metrics.} 
We use standard \emph{recall@k} metrics to measure the retrieval performance: we compute the neighbourhoods of the query vectors and verify if they contain the target image specified in the ground-truth annotations of the datasets.
We compare with pretrained CLIP ViT-L-14~\cite{radford2021clip} and the state-of-the-art model UniIR CLIP$_\text{SF}$~\cite{wei2023uniir}.
%
%
%

\paragraph{Results.} 
The main results are shown in Table~\ref{tab:retrieval_main}.
First, we observe a low zero-shot performance on all datasets.
This is in line with observations from previous work on composed retrieval~\cite{wei2023uniir,zhang2024magiclens}, which showed that CLIP models cannot easily compose multimodal features to match multimodal answers that are semantically different from the original query.
Second, we see that \modelname is able to leverage the hidden states of the LMM, leading to improved performance, especially for the first ranks (recall@1 and 5), over all datasets but InfoSeek. 
%
We also show qualitative results of UniIR and \modelname in Fig.~\ref{fig:qualitative_retrieval}. In the first example, UniIR retrieves visually similar traditional clothing and headwear. Instead, \modelname correctly retrieves art forms from Bali and Java (mentioned in the captions), consistent with the query (Indonesia). In the challenging second example, the model needs to infer from the query image that it's a hair dryer and connect the textual ``show in use''. While UniIR fails, \modelname correctly retrieves the ground truth image on rank 3. Also the other examples on CIRR and OVEN show that \modelname is able to combine query and image text in an abstract way to retrieve visually dissimilar correct targets, while UniIR focuses on visual similarity.

\begin{figure*}
  \centering
   \includegraphics[width=0.9\linewidth]{figures/qual_results_retrieval.jpg}
   \caption{\underline{Qualitative results retrieval.} We show retrieved images for UniIR and \modelname on three datasets. Captions are not displayed because of space limits.}
   \label{fig:qualitative_retrieval}
\end{figure*}

\paragraph{Ablation Study.} 
Table~\ref{tab:retrieval_ablation} shows an ablation study, where we train our model without \wikicomment and we remove the LMM adapter.
For this analysis, we study recall@1 because this will be the retrieved image that the LMM will comment (Sec.~\ref{ssec:exp_commenting}).
The inclusion of our adapter (row 1 vs 2) enhances performance across both datasets, leveraging LMM hidden states. This improvement comes at a negligible computational cost during inference since the LMM hidden state is computed already, highlighting our model's efficiency and effectiveness.
When we do not train on \wikicomment data, we observe a big drop in performance, highlighting how distinctive \wikicomment is from other Wikipedia datasets such as OVEN and InfoSeek.
%
%
Our model convincingly outperforms the zero-shot baseline (row 1 vs 4), showcasing its robust generalization capabilities and underscores the value of our training approach.
%
 

\begin{table}[]
    \centering
    \resizebox{0.43\textwidth}{!}{%
    \centering
    \begin{tabular}{l c c c c c}
    \toprule
    %
    & \textbf{\cirr}~\cite{Liu_2021_ICCV} &
    %
    & \textbf{\wikicomment}&
    \\
    %
    %
    %
    %
    \midrule  
    \modelname
    %
    & $\underline{16.45}$ & 
    & $\mathbf{40.14}$ &
    %
    \\
    ~w/o adapter
    %
    & $16.35$ & 
    & $\underline{39.14}$ &
    %
    \\
    ~w/o \wikicomment
    %
    & $\mathbf{17.29}$ & 
    & $23.02$ &
    %
    \\
    ~w/o training (zero-shot)
    %
    & $0.86$ & 
    & $13.24$ &
    %
    \\ 
    \midrule
    UniIR CLIP$_\text{SF}$~\cite{wei2023uniir}
    %
    & $9.50$ & 
    & $21.73$ &
    %
    \\  
    \bottomrule
    \end{tabular}
    }
    \caption{\underline{Ablation study on retrieval}. Retrieval results in terms of recall@1 on different dataset.} %
    \label{tab:retrieval_ablation}
\end{table}


%

\newcommand{\metricsize}{\scriptsize}

\begin{table*}[]
    \centering
    \resizebox{\textwidth}{!}{%
%
\begin{tabular}{l cccccc c cccccc}
\toprule
 & \multicolumn{6}{c}{\textbf{\cirrcomment}} & & \multicolumn{6}{c}{\textbf{\wikicomment}} \\ \cmidrule{2-7} \cmidrule{9-14}
 & \textbf{\metricsize METEOR} & \textbf{\metricsize BLEU} & \textbf{\metricsize ROUGE-1}  & \textbf{\metricsize ROUGE-2} &  \textbf{\metricsize BEM} & \textbf{\metricsize SigLIP-R@1} & %
 & \textbf{\metricsize METEOR} & \textbf{\metricsize BLEU} & \textbf{\metricsize ROUGE-1} & \textbf{\metricsize ROUGE-2} &  \textbf{\metricsize BEM} & \textbf{\metricsize SigLIP-R@1} \\
\midrule
InternVL2~\cite{chen2024internvl} & $0.192$ & $0.013$ & $0.280$ & $0.082$ & $11.9\%$ & $41.4\%$ & & $0.128$ & $0.009$ &  $0.237$ & $0.089$ & $31.8\%$ & $49.1\%$ \\
EchoSight~\cite{yan-xie-2024-echosight} + InternVL2 & $0.311$ & $0.038$ & $0.397$ & $0.119$ & $31.6\%$ & $41.0\%$ & & $0.204$ & $0.018$ & $0.276$ & $0.065$ & $16.2\%$ & $32.1\%$ \\
UniIR CLIP$_\text{SF}$~\cite{wei2023uniir} + InternVL2 & $0.313$ & $0.039$ & $0.399$ & $0.121$ & $32.0\%$ & $42.6\%$ & & $0.212$ & $0.025$ & $0.293$ & $0.078$ & $21.4\%$ & $41.2\%$ \\
\modelname & $\mathbf{0.444}$ & $\mathbf{0.115}$ & $\mathbf{0.516}$ & $\mathbf{0.280}$ & $\mathbf{50.4\%}$ & $\mathbf{45.1\%}$ & & $\mathbf{0.379}$ & $\mathbf{0.128}$ & $\mathbf{0.446}$ & $\mathbf{0.196}$ & $\mathbf{39.3\%}$ & $\mathbf{66.3\%}$ \\
\midrule
\modelname retriever + RAG & $0.318$ & $0.041$ & $0.400$ & $0.123$ & $33.7\%$ & $43.3\%$ & & $0.236$ & $0.037$ & $0.323$ & $0.098$ & $28.7\%$ & $55.2\%$ \\
\modelname w/o trained adapter & $0.317$ & $0.029$ & $0.346$ & $0.106$ & $28.8\%$ & $33.7\%$ & & $0.264$ & $0.033$ & $0.285$ & $0.089$ & $23.7\%$ & $52.4\%$ \\
\modelname w/ shared adapter & $0.407$ & $0.060$ & $0.392$ & $0.181$ & $55.9\%$ & $42.7\%$ & & $0.374$ & $0.127$ & $0.447$ & $0.195$ & $39.3\%$ & $67.6\%$ \\
%
\modelname w/ UniIR CLIP$_\text{SF}$ & $0.439$ & $0.113$ & $0.510$ & $0.276$ & $49.8\%$ & $44.0\%$ & & $0.347$ & $0.106$ & $0.411$ & $0.168$ & $34.7\%$ & $55.4\%$ \\
\midrule
\emph{UniCoRN w/ GT target oracle} & $0.462$ & $0.130$ & $0.534$ & $0.297$ & $56.1\%$ & $55.9\%$ & & $0.443$ & $0.176$ & $0.517$ & $0.258$ & $55.8\%$ & $92.1\%$ \\
\bottomrule
\end{tabular}
%
}
\vspace*{-2mm}
\caption{\underline{Main results and ablation study on commenting}. Commenting results by computing several language-based metrics between the predicted comments and the ground-truth (GT) ones on the \cirrcomment and \wikicomment dataset. (Top) Comparison with the state of the art; (Middle) Ablation study for retrieval and generation in \modelname; (Bottom) Upper-bound using GT target documents as an oracle.}
\label{tab:main_commenting}
\end{table*}

\begin{table}[]
    \centering
    \resizebox{0.48\textwidth}{!}{%
%
\begin{tabular}{l ccccc}
\toprule
 & \multicolumn{5}{c}{\textbf{\wikicomment}} \\ \cmidrule{2-6}
 & \textbf{\metricsize METEOR} & \textbf{\metricsize BLEU} & \textbf{\metricsize ROUGE-1} & \textbf{\metricsize ROUGE-2} & \textbf{\metricsize SigLIP-R@1} \\
\midrule
\modelname & $\mathbf{0.379}$ &  $\mathbf{0.128}$ & $\mathbf{0.446}$ & $\mathbf{0.196}$ & $\mathbf{66.3\%}$ \\
~w/o Wikipedia desc. & $0.302$ & $0.069$ & $0.386$ & $0.143$ & $55.1\%$ \\
~image-only & $0.295$ & $0.065$ & $0.379$ & $0.137$ & $50.6\%$ \\
%
~caption-only & $0.244$ & $0.034$ & $0.299$ & $0.090$ & $47.3\%$ \\
\midrule
\modelname retr. + RAG & $0.236$  & $0.037$ & $0.323$ & $0.098$ & $55.2\%$ \\
~w/o Wikipedia desc. & $0.227$  & $0.032$ & $0.321$ & $0.098$ & $51.5\%$ \\
~image-only & $0.198$  & $0.028$ & $0.314$ & $0.094$ & $49.6\%$ \\
~caption-only & $0.258$  & $0.024$ & $0.277$ & $0.079$ & $46.0\%$ \\
\bottomrule
\end{tabular}
%
}
\vspace*{-2mm}
\caption{\underline{Modality ablation study for entity encoding}. We compare results on \wikicomment using \modelname and RAG when removing images, captions and Wikipedia descriptions when providing the entities to the model.}
\label{tab:commenting_ablation_wiki}
\vspace*{-5mm}
\end{table}


\subsection{Commenting Results}
\label{ssec:exp_commenting} 

\paragraph{Implementation Details.} 
%
To implement retrieval-aware commenting, we need to modify standard LMM procedures, because we perform retrieval during generation and output images interleaved with text. We do so by extending the generation function of LMMs with the following changes.
%
First, we specify a \emph{retrieval token}, the prediction of which will trigger the retriever \retriever.
%
We use the start-of-sentence tag, as our \taskname datasets do not exhibit comments prior to retrieval.
The retriever yields a reference to the target entity, which we process via feature extraction and the Entity Adapter to make the LMM understand the retrieved content.
We convert all previously predicted output tokens to input embeddings and append the embedding of the retrieved entity, then feed this as input to the LMM for predicting the next tokens, until the end-of-sentence token is detected.

In addition, we train our Entity Adapters specifically so that InternVL2-4B processes retrieved content differently than content input by a user.
We therefore fine-tune the Entity Adapters separately on \cirrcomment and \wikicomment.
%
For \cirrcomment, we have 28,225 valid training conversations, where the retrieved entities are simply the target images.
For \wikicomment, we have 50,126 valid training conversations, where the retrieved entities are Wikipedia images with caption, as well as metadata from the corresponding Wikipedia page (title, description).
We train both adapters using the InternVL2 framework~\cite{chen2024internvl} for two epochs on a single host with 4$\times$L40S GPUs, using a learning rate of $1.0e^{-4}$, and evaluate on their respective evaluation sets as we described in Sec.~\ref{sec:datasets}.
%

\begin{figure*}
  \centering
   \includegraphics[width=0.95\linewidth]{figures/qual_results_commenting.pdf}
   \caption{\underline{Qualitative results.} We show retrieved images and comments for UniIR with RAG and \modelname on two different datasets. Comments highlighted in red indicate responses that either do not answer the original question or are not related to the retrieved image. }
   \label{fig:qualitative_commenting}
\end{figure*}

\paragraph{Metrics.}
We evaluate the predicted comments with respect to the corresponding ground-truth (GT) comments in the golden test set using standard metrics for language understanding: \begin{enumerate*}[\roman*)]
\item METEOR~\cite{denkowski:lavie:meteor-wmt:2014}, the harmonic mean of word precision and recall that takes into account stemming and synonymy;
%
\item BLEU~\cite{10.3115/1073083.1073135}, a metric with high correlation with human judgments of translation quality;
\item ROUGE-1~\cite{lin2004rouge} (resp. ROUGE-2) measuring the overlap of words (resp, bi-grams) between the two comments.
\item BEM~\cite{bem-emnlp2022}, a BERT-based~\cite{bert-2018} measure to evaluate the equivalence of answers to a question.
\end{enumerate*}
%
We also use a separate vision-language model, namely ViT-SO400M-14-SigLIP-384~\cite{zhai2023sigmoid}, to measure if the predicted comment can be used to retrieve the GT comment with high accuracy, thus measuring if they are specific and distinct from other comments.
For this approach, we use again \emph{recall@1}. %

%
%
%

%


%
%
%
%
%
%

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\paragraph{Results.} 
In the top section of Table~\ref{tab:main_commenting}, we report the results
%
for \modelname as well as three state-of-the-art LMM approaches that can produce comments based on multimodal inputs:
\begin{enumerate*}[\roman*)]
\item InternVL2~\cite{chen2024internvl}, a state-of-the-art LMM that is prompted to generate a comment for the unseen image based only on the multimodal user query;
\item EchoSight~\cite{yan-xie-2024-echosight}, a state-of-the-art multimodal Retrieval-Augmented Generation (RAG) and re-ranking approach, using InternVL2 as the base LMM for comment generation.
\item a system with the state-of-the-art compositional retriever UniIR~\cite{wei2023uniir} that we use to build a multi-image multi-modal RAG prompt for InternVL2;
\end{enumerate*}
The results show that InternVL2 is able to generate comments that contain relevant information (METEOR 0.192/0.128 on \cirrcomment/\wikicomment resp.).
This entails that the LMM is able to interpret the user query, justifying that its hidden state has indeed the potential to contribute to the retrieval component.
However, the comments are not very similar to the ground-truth: this is clearly due to the absence of retrieval capabilities in InternVL2, which thus has to produce a plausible comment without having access to any information about the content present in the retrieval dataset.
Using EchoSight~\cite{yan-xie-2024-echosight} or UniIR~\cite{wei2023uniir} to retrieve an entity and providing it in the prompt for InternVL2 improves results significantly on all metrics (\eg, METEOR 0.313/212), with a slight edge to the compositional retriever (UniIR) over the reranking one (EchoSight).
This proves that conditioning the generation on actual retrieved images is critical to ensure the relevance of the comments, and explains the broad success of RAG methods.
Then, using \modelname, we further improve to 0.444/0.379 METEOR.
Notably, the improvements are consistent over all metrics on both datasets.
These results show the effectiveness of our approach: without losing any other ability of InternVL2, \modelname is able to optimize its processing of user queries and retrieval results so as to output comments that are significantly better than static prompts built via RAG.

\paragraph{Ablation Study.} 
In the middle section of Table~\ref{tab:main_commenting}, we further analyze the main contributions to the performance of \modelname.
First, when using \modelname's retrieval results in a RAG prompt for InternVL2, we observe only a slight improvement over UniIR+RAG.
Similarly, using \modelname's commenting abilities on UniIR retrieval results performs only slightly worse than our full \modelname.
Thus, the improvements in retrieval are only a part of the contribution to the commenting performance.
In contrast, using the pre-trained input visual encoder from InternVL2 to encode retrieved documents does indeed drop the performance to the level for RAG.
This highlights that our contribution of training dataset-specific entity embeddings makes the biggest contribution in making the base LMM provide more relevant and well-formatted comments.
Training a share entity encoder leads to mixed results, with a slight improvement on BEM and SigLIP at the expense of other metrics.

In Table~\ref{tab:commenting_ablation_wiki}, we further decompose the contribution of the various modalities available in the retrieved documents.
Since \cirrcomment only has target images, we focus on \wikicomment, in which retrieved documents contain an image and its caption on Wikipedia, as well as the title and description of the corresponding page in Wikipedia as metadata.
\modelname, like RAG, uses all information by default in our experiments (to build the Entity encoding, resp. the prompt).
Using the \modelname retriever and the trained adapter, we show the drop of performance that occurs when removing, in turn,
\begin{enumerate*}[\roman*)]
\item the Wikipedia title and description:~$-12.1\%$  METEOR, relatively;
\item all textual inputs (\emph{image-only}):~$$-19.1\%$$;
\item image and Wikipedia metadata (\emph{caption-only}):~$-13.1\%$.
\end{enumerate*}
Since the drops are consistent for all metrics and both comment generation models (RAG and \modelname) based on the same inputs\footnote{Except the caption-only version of RAG for the METEOR score, which does better without the image.}, we can therefore conclude that all three are needed to provide relevant answers. %


Finally, we analyze the qualitative performance of UniIR with RAG and \modelname in Fig.~\ref{fig:qualitative_commenting} with success cases for \modelname.
The first row shows an example where both models retrieve the correct image. 
However, the comments provided by RAG do not answer the question correctly, as indicated by the part of the comment highlighted in red. 
The second row presents an example where the UniIR model was unable to retrieve the correct image, and therefore it could not answer the question correctly. 
This is similar to the third example, where the UniIR model retrieved the wrong query image, yet provided a comment that follows the instructions but does not actually correspond to the retrieved image. 