\section{\modelname}
\label{sec:method}

With \modelname, we propose to expand the capabilities of a frozen base LMMs with two novel interconnected modules: 
%
1)~Comment-aware retrieval, which is responsible for understanding the visio-linguistic representation of the query question and image, and projecting it to the target comment and image space for retrieval (Sec.~\ref{ssec:comment_retrieval}); 
2)~Retrieval-aware generation, which learns to leverage the retrieved entity to generate an answer to the user question (Sec.~\ref{ssec:retrieve_generate}).

\subsection{Comment-aware Retrieval}
\label{ssec:comment_retrieval}

Given a query text and image $q = (q^\textmod, q^\imgmod)$, we aim to train a retrieval model \retriever, with parameters \retrieverparams, that is capable of understanding $q$ and retrieve a related target entity composed of text and image $d = (d^\textmod, d^\imgmod)$, such that:
\begin{equation}
    d = \argmax_{d' \in \mathcal{D}}~{\retriever(q, d')} \label{eq:retrieval}
\end{equation}
where $\mathcal{D}$ is a database of multimodal entities (a.k.a documents).
The best definition for the retriever \retriever depends on the target application.
For cross-modal and multimodal retrieval, CLIP-like models~\cite{radford2021clip, gao2023softclip, gao2022pyramidclip, ilharco2021openclip, wei2023uniir} are the \emph{de-facto} choice based on their 
%
performance in diverse scenarios. %
However, they are limited in the visio-linguistic understanding capabilities, especially when moving to more complex applications like composed retrieval and \taskname.
To address this limitation, we leverage the reasoning capabilities of LMMs in the training process of the retriever integrated with CLIP, as shown in Fig.~\ref{fig:comment_retrieval}.

\begin{figure}[t]
  \centering
   \includegraphics[width=.8\linewidth]{figures/retrieval.png}
   \vspace{-3mm}
   \caption{\underline{Comment-aware Retrieval}. The query is inputed to both a CLIP-trained image-text encoder and an LMM. The LMM representation is projected to the space of the image-text encoder. Alignment of query and targets is done using contrastive loss.}
   \label{fig:comment_retrieval}
   \vspace{-3mm}
\end{figure}


The multimodal query $q$ is fed into the base LMM to obtain its output hidden state  $h\!=\!\hiddenstate(q)$.
In parallel, the query is fed into a multimodal encoder $\psi_\mmmod(q) = \psi^\textmod(q^\textmod) + \psi^\imgmod(q^\imgmod)$, where $\psi^\textmod$ and $\psi^\imgmod$ are text and image CLIP-like encoders, respectively.
Then, we define a Hidden State Adapter $\psi_\lmmmod$ to map $h$ into the same space as $\psi_\mmmod$, as follows:
\begin{equation}
    \psi_\lmmmod(h) = \text{FC}(\text{GeLU}(\text{FC}(h)))
\end{equation}
where FCs are fully connected layers and GeLU is the activation function.
The final embedding for $q$ combines the CLIP-based embeddings and the adapter output:
\begin{equation}
    \psi(q) = \beta \cdot \psi_\lmmmod(\hiddenstate(q)) + (1-\beta) \cdot \psi_\mmmod(q) \label{eq:mm_fusion}
\end{equation}
where $\beta$ is a learnable weight part of $\theta$.
%
Note, this score-fusion approach to CLIP is inspired by UniIR~\cite{wei2023uniir}, where the authors show this is one of the most effective ways to combine multimodal inputs for multimodal retrieval.

Finally, we define \retriever as the dot product of query and document embeddings: $\retriever(q, d) = \psi(q)^T\!\cdot\!\psi_\mmmod(d)$.
As shown in Fig.~\ref{fig:comment_retrieval}, the multimodal target documents are not using the LMM and therefore only use the multimodal encoder $\psi_\mmmod$.
This asymmetrical approach between query and document has several advantages: 1) it saves cost during the indexing phase by avoiding to use an expensive LMM on the potentially large pool of multimodal documents, and 2) it bridges the domain gap between the queries and the database via fine-tuning of the adapter. 
In contrast with other methods that use LMMs for indexing~\cite{jiang2024vlm2vec, lin2024mmembeduniversalmultimodalretrieval}, our method keeps indexing lightweight.
%



We train our retriever using contrastive loss~\cite{radford2021clip}. Unlike previous work~\cite{wei2023uniir, lin2024mmembeduniversalmultimodalretrieval,jiang2024vlm2vec}, we combine target captions and more complex target comments that cover aspects beyond the query and target images, as we will describe in Sec.~\ref{sec:datasets}.
This makes our model aware of complex comments during training and thus will better fit with retrieval-aware generation as described in Sec.~\ref{ssec:retrieve_generate}.
%


\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{figures/comment4.png}
   \vspace*{-8mm}
   \caption{\underline{Retrieval-aware Generation}. The query image and text are fed to the LMM, which asks the retriever for relevant entities.
   The best entity is provided to the user and adapted into the LMM, so it can attend to it for generating a useful comment.}
   %
   \label{fig:retrieve_generate}
   \vspace{-3mm}
\end{figure}

\subsection{Retrieval-aware Generation}
\label{ssec:retrieve_generate}

We now detail how we enable an LMM to ingest the query question and image, encode retrieved entities, and generate an answer to the question.
The process is shown in Fig.~\ref{fig:retrieve_generate}.
\vspace*{-3mm}
\subsubsection{Retrieved Entities as Input Modality}

In essence, a Large Language Model (LLM) predicts the probability of the next token $\tau_n\!\in\!\{1, \ldots, V\}$, where $V$ is the vocabulary size, given an input sequence of $n$ tokens: $p(\tau_{n} | \tau_0, \ldots, \tau_{n-1})$.
The first layer of the LLM projects the tokens into an input embedding space, mapping each of the $V$ possible \emph{text} tokens to a vector $\varphi^\textmod(\tau_k)\!=\!\phi^\textmod_k \in \mathbb{R}^{d}$, where $d$ is the input embedding size of the LLM.
Therefore, we can also see the LLM as predicting the next token based on a sequence of text-based embeddings:
\begin{equation}
    p(\tau_{n} ~ | ~ \phi^\textmod_0, \ldots, \phi^\textmod_{n-1}).
    \label{eq:llm_embd}
\end{equation}
LMMs~\cite{liu2023improvedllava, chen2024internvl} extend this principle by adapting visual data into input image tokens $\tau^\imgmod$ or embeddings $\phi^\imgmod$.
%
Notably, Eq.~\ref{eq:llm_embd} is unchanged for LMMs, except that the inputs are now a mix of text $\phi^\textmod$ and image $\phi^\imgmod$ embeddings in $\mathbb{R}^d$.

As described in Sec.~\ref{ssec:comment_retrieval}, the retriever finds the most relevant multimodal document $d_m$ in the database based on the multimodal query $q$ and the projected hidden state of the LMM.
%
As an important contribution of our method, we then inject $d_m$ into the LMM as a new input modality $\retrmod$ in order to condition the rest of the generation process, and therefore ensure that the model attends to it while generating the textual comment.
This is important because of the finite nature of any database: the retrieved documents can substantially differ from the vector used as query.
Thus, using solely the user input to condition the generation may lead to comments that are irrelevant to the retrieved entity.
%

Similarly to other input modalities, features of the retrieved document are extracted and adapted into the input embedding space of the LMM:
\begin{equation}
  \varphi^\retrmod(d_m) = [\phi^\retrmod_{m,0}, \ldots, \phi^\retrmod_{m,l_m}] = \phi^\retrmod_m \in \mathbb{R}^{d\times l_m},
\end{equation}
where $l_m$ is the length of the representation of the document $d_m$.
In this way, the LMM can attend to the retrieved data in addition to all previous inputs as it continues generation:
\begin{equation}
    p(\tau_{n} ~ | \cdots, \phi^\textmod_k, \cdots, \phi^\imgmod_p, \cdots, \phi^\retrmod_m, \cdots).
    \label{eq:llm_embd2}
\end{equation}
However, the frozen LMM was not trained to distinguish entities that are retrieved from a database and treat them appropriately to generate a convincing answer.
Thus, we make the entity adapter a trainable module as described next. 

%

\subsubsection{Entity Adapter Module}

The definition of $\varphi^\retrmod(d_m)$ depends on the nature of the dataset and the entities it contains: text, images, audio, video, pre-computed features, or any combination thereof.
%
We propose to design an Entity Adapter module $\varphi^\retrmod_{\xi}(d_m)$, as shown in Fig.~\ref{fig:retrieve_generate}, with parameters $\xi$, which can be fine-tuned on training data.
More specifically for our \tasknameshort tasks, the entities are images, or multimodal combinations of images and text data (e.g. titles, captions, ...), that are used to generate the answer.
As a consequence, the Entity Adapter treats the various modalities differently, \ie, $\varphi^\retrmod_{\xi}(d_m) = (\varphi^\imgmod_{\xi}(d^\imgmod_m), \varphi^\textmod_{\xi}(d^\textmod_m))$.
In our datasets, the textual components are natural text, so we provide them directly to the LMM, without fine-tuning the token embeddings, \ie, $\varphi^\textmod_{\xi}(d_m)=\phi^\textmod$.
%
%
%
%
%
%
For the visual part, we use the same architecture as the base LMM~\cite{chen2024internvl}, with the adapter:
\begin{equation}
\varphi^\imgmod_{\xi}(d_m)=\text{FC}(\text{GeLU}(\text{FC}(\text{LayerNorm}(x_m)))), \label{eq:entityadapter}
\end{equation}
where $\{x_m\}$ are the ViT image features of the variable number of tiles of the image of $d^\imgmod_m$.

\subsubsection{Training}
To train our novel Entity Adapter, we leverage a dataset $\mathcal{D}$ of ground-truth multimodal triplets $(q, d, c)$, where $q$ is the multimodal query, $d$ is the ground-truth target document, and $c$ is the ground-truth answer that relates the query and the target.
The training of $\xi$ is performed for the task of next token prediction of $c$, using the cross-entropy loss:
%
\begin{equation}
\mathcal{L}_{CE}(\xi) = \sum_{(q, d, c)\in\mathcal{D}} \quad
\sum_{\tau_n\in c} -\delta_n.\log(p_n(\xi)),
\end{equation}
where $p_n(\xi)\!=\!p(\tau_n | \phi^\imgmod_q, \phi^\textmod_q, \nonumber \varphi^\retrmod_{\xi}(d), \phi^\textmod_0, \ldots, \phi^\textmod_{n-1})$,
and $\delta_n$ is the ground-truth distribution for $\tau_n$.
%
%
As shown in Fig.~\ref{fig:retrieve_generate}, in practice, all other parts of the model  are kept frozen during training and only the adapter in Eq.~\ref{eq:entityadapter} is trained. 
For faster convergence, this adapter is initialized from the pretrained visual adapter of the original LMM.

With this approach, despite keeping the base LMM frozen, we are achieving two objectives: \begin{enumerate*}[label=\arabic*)]\item we align the multimodal entities with the LMM input embeddings, thus helping the LMM extract the relevant information from the entities and closing any domain gap between the data used to pre-train the LMM and the retrieval dataset, and \item we encode commenting instructions in the adapter, thus optimizing the inputs to the LMM to bias it to generate useful, high-quality comments for documents of this dataset. %
\end{enumerate*}
As a consequence, we train separate, specialized Entity Adapters for different tasks and datasets.

%
%




%
%

%

%
%
%
%
%
%
%

%


%
%
%

%
%

%

%

%
%

%
%
%

%

%

%