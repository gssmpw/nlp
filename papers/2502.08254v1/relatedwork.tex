\section{Related Work}
\label{sec:related_work}

\noindent\textbf{Instructable Retrieval.}
%
Multimodal representation for retrieval~\cite{radford2021clip, li2022blip, zhai2023sigmoid, chen2023pali, yu2022coca, wang2023image, zhou2022non, zhai2019large, yao2021filip, gao2022pyramidclip, lee2022uniclip, ilharco2021openclip, schuhmann2021laion400m, bao2022vlmo} has been a prominent area of research demonstrating strong performance. %
%
To handle compositional requests that require reasoning about the visual content, recent work focused on composed retrieval~\cite{vo2019composing, Baldrati_2022_CVPR, Baldrati_2023_ICCV, Goenka_2022_CVPR, Chen_2020_CVPR, Wu_2021_CVPR, Gu_2024_CVPR, Vaze_2023_CVPR, Suo_2024_CVPR, Wan_2024_CVPR, Liu_2024_WACV, karthik2023vision}.
However, these methods focus on limited specific retrieval domains and can only follow predefined instruction templates.
%
To address this limitation, recent models have been made instructable~\cite{zhang2024magiclens, wei2023uniir, jiang2024vlm2vec, lin2024mmembeduniversalmultimodalretrieval, qi2024roravlm, karthik2023vision, levy2024chatting} to capture richer multimodal relationships. 
MagicLens~\cite{zhang2024magiclens} is a simple instructable model trained on a curated large dataset including instructions, while UniIR~\cite{wei2023uniir} proposed training CLIP/BLIP variants conditioned on prompted instructions.
%
While these approaches have demonstrated significant progress, they lack expressiveness in the multimodal output space due to the limitations of CLIP models. 
In contrast, methods based on LMMs~\cite{lin2024mmembeduniversalmultimodalretrieval, jiang2024vlm2vec} that are fine-tuned for retrieval do not preserve the original capabilities of the LMMs.
%
%
\modelname aims to overcome this issue and preserve capabilities by design by enabling a frozen LMM to retrieve relevant content and generate textual responses tailored to both the input question and the retrieved visual content.
%
%


\noindent\textbf{Retrieval via Generation.}
LMMs~\cite{openai2024gpt4,geminiteam2024gemini,anthropic2024claude,zhu2024minigpt,liu2023llava,liu2023improvedllava,chen2024far,jiang2024mantis, wang2024qwen2, ye2024mplug, chen2024internvl, mckinzie2024mm1, wu2023next, dubey2024llama, agrawal2024pixtral, lin2024mmembeduniversalmultimodalretrieval} have demonstrated remarkable reasoning capabilities when processing language and visual elements (see survey in~\cite{yin2023survey}).
%
Recently, LMMs were combined with diffusion models to generate images along with textual answers~\cite{tian2024mminterleaved, li2023textbind, koh2023grounding, wu2023next, ge2023making, yu2023language, team2024chameleon}.
%
%
Despite providing an interesting user experience, the generated images are by nature not grounded to real entities, which is critical for retrieval applications, such as online shopping, news, or Wikipedia.
%
%
These models are based on memory-intensive diffusion models and trained with a reconstruction objective that is not in line with retrieval.
%
%
In contrast, \modelname uses a retrieval approach that preserves the factuality of the images that are shown to the user, while commenting them in a generative way. 
It is lightweight, as it does not use diffusion models, and the LMMs are frozen, thus preserving their reasoning capabilities of other non-retrieval tasks.


\noindent\textbf{Retrieval-Augmented Generation (RAG).}
The parametric way that language models are trained and store knowledge limits the ease to expand or revise their memory with more recent information.
RAG~\cite{lee2019latent, lewis2020retrieval, guu2020retrieval, karpukhin2020dense,fevry2020entities} and Multimodal RAG~\cite{qi2024roravlm, hu2023reveal, yu2024visrag, chen2022murag, qu2024alleviating, caffagni2024wiki, yan-xie-2024-echosight} have been proposed as a non-parametric way to provide up-to-date knowledge to an LMM by adding relevant content to the prompt before processing by the LMM.
Using this augmented prompt, the LMM can select, summarize and alter the entities to generate its output.
 %
\modelname differs from traditional RAG in that it processes user input via the LMM to build a better query and outputs the retrieved entity unaltered.
Then, \modelname's entity adapter module is trained to feed optimized entity representations to the LMM, which guides the extraction of the information necessary to answer the request.
%
%
%
%
%
%
%



\noindent\textbf{\tasknametitle (\tasknameshort).}
\tasknameshort is an underexplored task that requires not only retrieving relevant images, as in composed retrieval, but also generating plausible textual answers that refer to the image and contain additional information.
While preliminary work~\cite{wei2023uniir, jiang2024vlm2vec, qi2024roravlm, karthik2023vision} and datasets~\cite{hu2023open, chen2023can, mensink2023encyclopedic} focuse on some aspects of this task, they have two key limitations: 
1) The complexity of the textual answers is constrained, typically limited to a few words or basic captions.
2) The textual answers are aligned with the image, either providing redundant information (e.g., image captions) and failing to add useful complementary information.
To fully cover the \tasknameshort task, we claim that the answers should be multimodal in nature, with the image and text working in tandem to provide a richer, more comprehensive response.
The modalities should have a clear connection, yet offer complementary information that enhances the overall response.
To alleviate the limitations of existing \tasknameshort datasets, we created two challenging human-curated ones that we hope will foster future research in this area.