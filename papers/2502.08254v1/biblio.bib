@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@misc{wu2023survey,
      title={A Survey on Large Language Models for Recommendation}, 
      author={Likang Wu and Zhi Zheng and Zhaopeng Qiu and Hao Wang and Hongchao Gu and Tingjia Shen and Chuan Qin and Chen Zhu and Hengshu Zhu and Qi Liu and Hui Xiong and Enhong Chen},
      year={2023},
      eprint={2305.19860},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{yin2023survey,
  title={A Survey on Multimodal Large Language Models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}

@misc{openai2024gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{anthropic2024claude,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  year={2024}
}

@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Llama Team},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      publisher={NeurIPS},
      year={2023},
}

@misc{liu2023improvedllava,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
      publisher={arXiv:2310.03744},
      year={2023},
}

@misc{chen2024far,
      title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites}, 
      author={Zhe Chen and Weiyun Wang and Hao Tian and Shenglong Ye and Zhangwei Gao and Erfei Cui and Wenwen Tong and Kongzhi Hu and Jiapeng Luo and Zheng Ma and Ji Ma and Jiaqi Wang and Xiaoyi Dong and Hang Yan and Hewei Guo and Conghui He and Botian Shi and Zhenjiang Jin and Chao Xu and Bin Wang and Xingjian Wei and Wei Li and Wenjian Zhang and Bo Zhang and Pinlong Cai and Licheng Wen and Xiangchao Yan and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang},
      year={2024},
      eprint={2404.16821},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jiang2024mantis,
      title={MANTIS: Interleaved Multi-Image Instruction Tuning}, 
      author={Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max Ku and Qian Liu and Wenhu Chen},
      year={2024},
      eprint={2405.01483},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{mao2023unitrec,
    title = "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
    author = "Mao, Zhiming  and
              Wang, Huimin  and
              Du, Yiming  and
              Wong, Kam-Fai",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.100",
    pages = "1160--1170"
}

@misc{zheng2023generative,
      title={Generative Job Recommendations with Large Language Model}, 
      author={Zhi Zheng and Zhaopeng Qiu and Xiao Hu and Likang Wu and Hengshu Zhu and Hui Xiong},
      year={2023},
      eprint={2307.02157},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{xiao2022,
author = {Xiao, Shitao and Liu, Zheng and Shao, Yingxia and Di, Tao and Middha, Bhuvan and Wu, Fangzhao and Xie, Xing},
title = {Training Large-Scale News Recommenders with Pretrained Language Models in the Loop},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539120},
doi = {10.1145/3534678.3539120},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4215–4225},
numpages = {11},
keywords = {training framework, pretrained language models, news recommendation, efficiency and effectiveness},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{penha2020,
author = {Penha, Gustavo and Hauff, Claudia},
title = {What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412249},
doi = {10.1145/3383313.3412249},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {388–397},
numpages = {10},
keywords = {probing, conversational search, conversational recommendation},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@ARTICLE{malkov2020hnsw,
  author={Malkov, Yu A. and Yashunin, D. A.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}, 
  year={2020},
  volume={42},
  number={4},
  pages={824-836},
  keywords={Routing;Complexity theory;Search problems;Data models;Approximation algorithms;Biological system modeling;Brain modeling;Graph and tree search strategies;artificial intelligence;information search and retrieval;information storage and retrieval;information technology and systems;search process;graphs and networks;data structures;nearest neighbor search;big data;approximate search;similarity search},
  doi={10.1109/TPAMI.2018.2889473}
}

@article{douze2024faiss,
      title={The Faiss library},
      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
      year={2024},
      eprint={2401.08281},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{decao2021autoregressive,
  author    = {Nicola {De Cao} and
               Gautier Izacard and
               Sebastian Riedel and
               Fabio Petroni},
  title     = {Autoregressive Entity Retrieval},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=5k8F6UU39V},
}

@article{radford2021clip,
  author       = {Alec Radford and
                  Jong Wook Kim and
                  Chris Hallacy and
                  Aditya Ramesh and
                  Gabriel Goh and
                  Sandhini Agarwal and
                  Girish Sastry and
                  Amanda Askell and
                  Pamela Mishkin and
                  Jack Clark and
                  Gretchen Krueger and
                  Ilya Sutskever},
  title        = {Learning Transferable Visual Models From Natural Language Supervision},
  journal      = {CoRR},
  volume       = {abs/2103.00020},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.00020},
  eprinttype    = {arXiv},
  eprint       = {2103.00020},
  timestamp    = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@software{ilharco_gabriel_2021_5143773,
  author       = {Ilharco, Gabriel and
                  Wortsman, Mitchell and
                  Wightman, Ross and
                  Gordon, Cade and
                  Carlini, Nicholas and
                  Taori, Rohan and
                  Dave, Achal and
                  Shankar, Vaishaal and
                  Namkoong, Hongseok and
                  Miller, John and
                  Hajishirzi, Hannaneh and
                  Farhadi, Ali and
                  Schmidt, Ludwig},
  title        = {OpenCLIP},
  month        = jul,
  year         = 2021,
  note         = {If you use this software, please cite it as below.},
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.5143773},
  url          = {https://doi.org/10.5281/zenodo.5143773}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}


@inproceedings{zhai2023sigmoid,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11975--11986},
  year={2023}
}

@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@inproceedings{wang2023image,
  title={Image as a foreign language: Beit pretraining for vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19175--19186},
  year={2023}
}

@article{chen2023pali,
  title={Pali-3 vision language models: Smaller, faster, stronger},
  author={Chen, Xi and Wang, Xiao and Beyer, Lucas and Kolesnikov, Alexander and Wu, Jialin and Voigtlaender, Paul and Mustafa, Basil and Goodman, Sebastian and Alabdulmohsin, Ibrahim and Padlewski, Piotr and others},
  journal={arXiv preprint arXiv:2310.09199},
  year={2023}
}

@inproceedings{lavie2007meteor,
author = {Lavie, Alon and Agarwal, Abhaya},
title = {Meteor: an automatic metric for MT evaluation with high levels of correlation with human judgments},
year = {2007},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.},
booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
pages = {228–231},
numpages = {4},
location = {Prague, Czech Republic},
series = {StatMT '07}
}

  


@misc{alayrac2022flamingo,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{sun2024emu2,
      title={Generative Multimodal Models are In-Context Learners}, 
      author={Quan Sun and Yufeng Cui and Xiaosong Zhang and Fan Zhang and Qiying Yu and Zhengxiong Luo and Yueze Wang and Yongming Rao and Jingjing Liu and Tiejun Huang and Xinlong Wang},
      year={2024},
      eprint={2312.13286},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{li2023textbind,
  title={TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild},
  author={Li, Huayang and Li, Siheng and Cai, Deng and Wang, Longyue and Liu, Lemao and Watanabe, Taro and Yang, Yujiu and Shi, Shuming},
  year={2023}
}

@article{tian2024mminterleaved,
  title={MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer}, 
  author={Tian, Changyao and Zhu, Xizhou and Xiong, Yuwen and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Chen, Yuntao and Lu, Lewei and Lu, Tong and Zhou, Jie and Li, Hongsheng and Qiao, Yu and Dai, Jifeng},
  journal={arXiv preprint arXiv:2401.10208},
  year={2024},
}

@inproceedings{zhu2024recsurvey,
author = {Zhu, Jieming and Zhou, Xin and Wu, Chuhan and Zhang, Rui and Dong, Zhenhua},
title = {Multimodal Pretraining and Generation for Recommendation: A Tutorial},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641248},
doi = {10.1145/3589335.3641248},
abstract = {Personalized recommendation stands as a ubiquitous channel for users to explore information or items aligned with their interests. Nevertheless, prevailing recommendation models predominantly rely on unique IDs and categorical features for user-item matching. While this ID-centric approach has witnessed considerable success, it falls short in comprehensively grasping the essence of raw item contents across diverse modalities, such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, particularly in the realm of multimedia services like news, music, and short-video platforms. The recent surge in pretraining and generation techniques presents both opportunities and challenges in the development of multimodal recommender systems. This tutorial seeks to provide a thorough exploration of the latest advancements and future trajectories in multimodal pretraining and generation techniques within the realm of recommender systems. The tutorial comprises four talks, addressing multimodal pretraining, multimodal fusion, multimodal generation, and presenting successful stories alongside open challenges in the field of recommendation. Our target audience encompasses scholars, practitioners, and other parties interested in this domain. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and foster meaningful discussions on the future development of this evolving landscape.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2024},
pages = {1272–1275},
numpages = {4},
keywords = {multimodal fusion, multimodal generation, multimodal pretraining, recommender systems},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{zhao2017memory,
  title={Memory-augmented attribute manipulation networks for interactive fashion search},
  author={Zhao, Bo and Feng, Jiashi and Wu, Xiao and Yan, Shuicheng},
  booktitle=cvpr,
  year={2017}
}

@inproceedings{han2017automatic,
  title={Automatic spatially-aware fashion concept discovery},
  author={Han, Xintong and Wu, Zuxuan and Huang, Phoenix X and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S},
  booktitle=iccv,
  year={2017}
}

@inproceedings{ak2018learning,
  title={Learning attribute representations with localization for flexible fashion search},
  author={Ak, Kenan E and Kassim, Ashraf A and Hwee Lim, Joo and Yew Tham, Jo},
  booktitle=cvpr,
  year={2018}
}

@inproceedings{Vo_2019_CVPR,
author = {Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James},
title = {Composing Text and Image for Image Retrieval - an Empirical Odyssey},
booktitle = cvpr,
year = {2019}
}

@inproceedings{guo2018dialog,
  title={Dialog-based interactive image retrieval},
  author={Guo, Xiaoxiao and Wu, Hui and Cheng, Yu and Rennie, Steven and Tesauro, Gerald and Feris, Rogerio},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}

@article{guo2019fashion,
  title={The Fashion IQ Dataset: Retrieving Images by Combining Side Information and Relative Natural Language Feedback},
  author={Guo, Xiaoxiao and Wu, Hui and Gao, Yupeng and Rennie, Steven and Feris, Rogerio},
  journal={arXiv preprint arXiv:1905.12794},
  year={2019}
}

@inproceedings{hou2021learning,
  title={Learning attribute-driven disentangled representations for interactive fashion retrieval},
  author={Hou, Yuxin and Vig, Eleonora and Donoser, Michael and Bazzani, Loris},
  booktitle={Proceedings of the IEEE/CVF International conference on computer vision},
  pages={12147--12157},
  year={2021}
}

@inproceedings{chen2020image,
  title={Image search with text feedback by visiolinguistic attention learning},
  author={Chen, Yanbei and Gong, Shaogang and Bazzani, Loris},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3001--3011},
  year={2020}
}

@InProceedings{Baldrati_2022_CVPR,
    author    = {Baldrati, Alberto and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto},
    title     = {Effective Conditioned and Composed Image Retrieval Combining CLIP-Based Features},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {21466-21474}
}

@InProceedings{Goenka_2022_CVPR,
    author    = {Goenka, Sonam and Zheng, Zhaoheng and Jaiswal, Ayush and Chada, Rakesh and Wu, Yue and Hedau, Varsha and Natarajan, Pradeep},
    title     = {FashionVLP: Vision Language Transformer for Fashion Retrieval With Feedback},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {14105-14115}
}

@InProceedings{Chen_2024_CVPR,
    author    = {Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
    title     = {InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {24185-24198}
}

@inproceedings{zhu2024minigpt,
    title={Mini{GPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
    author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
    booktitle={ICLR},
    year={2024},
}

@inproceedings{peng2024grounding,
    title={Grounding Multimodal Large Language Models to the World},
    author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei},
    booktitle={ICLR},
    year={2024},
}

@article{BOBADILLA2013109,
title = {Recommender systems survey},
journal = {Knowledge-Based Systems},
volume = {46},
pages = {109-132},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113001044},
author = {J. Bobadilla and F. Ortega and A. Hernando and A. Gutiérrez},
}

@Article{electronics11010141,
AUTHOR = {Ko, Hyeyoung and Lee, Suyeon and Park, Yoonseo and Choi, Anna},
TITLE = {A Survey of Recommendation Systems: Recommendation Models, Techniques, and Application Fields},
JOURNAL = {Electronics},
VOLUME = {11},
YEAR = {2022},
NUMBER = {1},
ARTICLE-NUMBER = {141},
URL = {https://www.mdpi.com/2079-9292/11/1/141},
ISSN = {2079-9292},
}

@misc{zhou2023comprehensivesurveymultimodalrecommender,
      title={A Comprehensive Survey on Multimodal Recommender Systems: Taxonomy, Evaluation, and Future Directions}, 
      author={Hongyu Zhou and Xin Zhou and Zhiwei Zeng and Lingzi Zhang and Zhiqi Shen},
      year={2023},
      eprint={2302.04473},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2302.04473}, 
}

@article{10.1145/3407190,
author = {Deldjoo, Yashar and Schedl, Markus and Cremonesi, Paolo and Pasi, Gabriella},
title = {Recommender Systems Leveraging Multimedia Content},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3407190},
doi = {10.1145/3407190},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {106},
numpages = {38},
}

@inproceedings{Chaganty_2023, series={SIGIR ’23},
   title={Beyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset},
   url={http://dx.doi.org/10.1145/3539618.3591881},
   DOI={10.1145/3539618.3591881},
   booktitle={Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
   publisher={ACM},
   author={Chaganty, Arun Tejasvi and Leszczynski, Megan and Zhang, Shu and Ganti, Ravi and Balog, Krisztian and Radlinski, Filip},
   year={2023},
   month=jul, collection={SIGIR ’23} }

@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv preprint arXiv:1909.08593},
  year={2019}
}

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}


@ARTICLE{9726844,
  author={Wu, Yuxia and Liao, Lizi and Zhang, Gangyi and Lei, Wenqiang and Zhao, Guoshuai and Qian, Xueming and Chua, Tat-Seng},
  journal={IEEE Transactions on Multimedia}, 
  title={State Graph Reasoning for Multimodal Conversational Recommendation}, 
  year={2023},
  volume={25},
  number={},
  pages={3113-3124},
  keywords={Cognition;Visualization;History;Recommender systems;Electronic mail;Databases;Training;Conversation;knowledge graph;recommend- ation systems},
  doi={10.1109/TMM.2022.3155900}
}

@inproceedings{10.1145/3404835.3462970,
author = {Liao, Lizi and Long, Le Hong and Zhang, Zheng and Huang, Minlie and Chua, Tat-Seng},
title = {MMConv: An Environment for Multimodal Conversational Search across Multiple Domains},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462970},
doi = {10.1145/3404835.3462970},
pages = {675–684},
numpages = {10},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3543829.3543837,
author = {Wu, Yaxiong and Macdonald, Craig and Ounis, Iadh},
title = {Multimodal Conversational Fashion Recommendation with Positive and Negative Natural-Language Feedback},
year = {2022},
isbn = {9781450397391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543829.3543837},
doi = {10.1145/3543829.3543837},
booktitle = {Proceedings of the 4th Conference on Conversational User Interfaces},
articleno = {6},
numpages = {10},
keywords = {conversational recommendation, fashion, multimodal, positive \& negative feedback},
location = {Glasgow, United Kingdom},
series = {CUI '22}
}

@inproceedings{10.1145/3581783.3613755,
author = {Du, Wenzhe and Haoyang, Su and Cam-Tu, Nguyen and Sun, Jian},
title = {Enhancing Product Representation with Multi-form Interactions for Multimodal Conversational Recommendation},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613755},
doi = {10.1145/3581783.3613755},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {6491–6500},
numpages = {10},
keywords = {product modeling, multimodal conversational recommendation},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{mayer2003three,
  title={Three facets of visual and verbal learners: Cognitive ability, cognitive style, and learning preference.},
  author={Mayer, Richard E and Massa, Laura J},
  journal={Journal of educational psychology},
  volume={95},
  number={4},
  pages={833},
  year={2003},
  publisher={American Psychological Association}
}

@inproceedings{hu2023open,
  title={Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities},
  author={Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12065--12075},
  year={2023}
}

@inproceedings{kazemzadeh2014referitgame,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}

@article{chen2023can,
  title={Can pre-trained vision and language models answer visual information-seeking questions?},
  author={Chen, Yang and Hu, Hexiang and Luan, Yi and Sun, Haitian and Changpinyo, Soravit and Ritter, Alan and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2302.11713},
  year={2023}
}

@inproceedings{mensink2023encyclopedic,
  title={Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories},
  author={Mensink, Thomas and Uijlings, Jasper and Castrejon, Lluis and Goel, Arushi and Cadar, Felipe and Zhou, Howard and Sha, Fei and Araujo, Andr{\'e} and Ferrari, Vittorio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3113--3124},
  year={2023}
}

@inproceedings{wu2021fashion,
  title={Fashion iq: A new dataset towards retrieving images by natural language feedback},
  author={Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},
  booktitle={Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition},
  pages={11307--11317},
  year={2021}
}

@inproceedings{liu2021image,
  title={Image retrieval on real-life images with pre-trained vision-and-language models},
  author={Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2125--2134},
  year={2021}
}

@article{burns2023suite,
  title={A suite of generative tasks for multi-level multimodal webpage understanding},
  author={Burns, Andrea and Srinivasan, Krishna and Ainslie, Joshua and Brown, Geoff and Plummer, Bryan A and Saenko, Kate and Ni, Jianmo and Guo, Mandy},
  journal={arXiv preprint arXiv:2305.03668},
  year={2023}
}


@inproceedings{koh2023grounding,
  title={Grounding language models to images for multimodal inputs and outputs},
  author={Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={17283--17300},
  year={2023},
  organization={PMLR}
}

@article{wu2023next,
  title={Next-gpt: Any-to-any multimodal llm},
  author={Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2309.05519},
  year={2023}
}

@article{ge2023making,
  title={Making llama see and draw with seed tokenizer},
  author={Ge, Yuying and Zhao, Sijie and Zeng, Ziyun and Ge, Yixiao and Li, Chen and Wang, Xintao and Shan, Ying},
  journal={arXiv preprint arXiv:2310.01218},
  year={2023}
}

@article{yu2023language,
  title={Language Model Beats Diffusion--Tokenizer is Key to Visual Generation},
  author={Yu, Lijun and Lezama, Jos{\'e} and Gundavarapu, Nitesh B and Versari, Luca and Sohn, Kihyuk and Minnen, David and Cheng, Yong and Birodkar, Vighnesh and Gupta, Agrim and Gu, Xiuye and others},
  journal={arXiv preprint arXiv:2310.05737},
  year={2023}
}

@article{team2024chameleon,
  title={Chameleon: Mixed-modal early-fusion foundation models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@article{levy2024chatting,
  title={Chatting makes perfect: Chat-based image retrieval},
  author={Levy, Matan and Ben-Ari, Rami and Darshan, Nir and Lischinski, Dani},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wei2023uniir,
  title={Uniir: Training and benchmarking universal multimodal information retrievers},
  author={Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu},
  journal={arXiv preprint arXiv:2311.17136},
  year={2023}
}

@article{zhang2024magiclens,
  title={Magiclens: Self-supervised image retrieval with open-ended instructions},
  author={Zhang, Kai and Luan, Yi and Hu, Hexiang and Lee, Kenton and Qiao, Siyuan and Chen, Wenhu and Su, Yu and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2403.19651},
  year={2024}
}

@article{jiang2024vlm2vec,
  title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks},
  author={Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu},
  journal={arXiv preprint arXiv:2410.05160},
  year={2024}
}

@misc{qi2024roravlm,
      title={RoRA-VLM: Robust Retrieval-Augmented Vision Language Models}, 
      author={Jingyuan Qi and Zhiyang Xu and Rulin Shao and Yang Chen and Jing Di and Yu Cheng and Qifan Wang and Lifu Huang},
      year={2024},
      eprint={2410.08876},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.08876}, 
}

@article{karthik2023vision,
  title={Vision-by-language for training-free compositional image retrieval},
  author={Karthik, Shyamgopal and Roth, Karsten and Mancini, Massimiliano and Akata, Zeynep},
  journal={arXiv preprint arXiv:2310.09291},
  year={2023}
}

@inproceedings{vo2019composing,
  title={Composing text and image for image retrieval-an empirical odyssey},
  author={Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6439--6448},
  year={2019}
}

@InProceedings{Baldrati_2023_ICCV,
    author    = {Baldrati, Alberto and Agnolucci, Lorenzo and Bertini, Marco and Del Bimbo, Alberto},
    title     = {Zero-Shot Composed Image Retrieval with Textual Inversion},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {15338-15347}
}

@InProceedings{Chen_2020_CVPR,
author = {Chen, Yanbei and Gong, Shaogang and Bazzani, Loris},
title = {Image Search With Text Feedback by Visiolinguistic Attention Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@InProceedings{Wu_2021_CVPR,
    author    = {Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio},
    title     = {Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {11307-11317}
}

@InProceedings{Gu_2024_CVPR,
    author    = {Gu, Geonmo and Chun, Sanghyuk and Kim, Wonjae and Kang, Yoohoon and Yun, Sangdoo},
    title     = {Language-only Training of Zero-shot Composed Image Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {13225-13234}
}

@InProceedings{Vaze_2023_CVPR,
    author    = {Vaze, Sagar and Carion, Nicolas and Misra, Ishan},
    title     = {GeneCIS: A Benchmark for General Conditional Image Similarity},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {6862-6872}
}

@InProceedings{Suo_2024_CVPR,
    author    = {Suo, Yucheng and Ma, Fan and Zhu, Linchao and Yang, Yi},
    title     = {Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {26951-26962}
}

@InProceedings{Wan_2024_CVPR,
    author    = {Wan, Yongquan and Wang, Wenhai and Zou, Guobing and Zhang, Bofeng},
    title     = {Cross-modal Feature Alignment and Fusion for Composed Image Retrieval},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2024},
    pages     = {8384-8388}
}

@InProceedings{Liu_2024_WACV,
    author    = {Liu, Zheyuan and Sun, Weixuan and Hong, Yicong and Teney, Damien and Gould, Stephen},
    title     = {Bi-Directional Training for Composed Image Retrieval via Text Prompt Learning},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2024},
    pages     = {5753-5762}
}


@article{zhou2022non,
  title={Non-Contrastive Learning Meets Language-Image Pre-Training},
  author={Zhou, Jinghao and Dong, Li and Gan, Zhe and Wang, Lijuan and Wei, Furu},
  journal={arXiv preprint arXiv:2210.09304},
  year={2022}
}

@article{zhai2019large,
  title={A large-scale study of representation learning with the visual task adaptation benchmark},
  author={Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others},
  journal={arXiv preprint arXiv:1910.04867},
  year={2019}
}

@article{yao2021filip,
  title={FILIP: fine-grained interactive language-image pre-training},
  author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
  journal={arXiv preprint arXiv:2111.07783},
  year={2021}
}

@article{goel2022cyclip,
  title={Cyclip: Cyclic contrastive language-image pretraining},
  author={Goel, Shashank and Bansal, Hritik and Bhatia, Sumit and Rossi, Ryan and Vinay, Vishwa and Grover, Aditya},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={6704--6719},
  year={2022}
}

@article{gao2022pyramidclip,
  title={Pyramidclip: Hierarchical feature alignment for vision-language model pretraining},
  author={Gao, Yuting and Liu, Jinfeng and Xu, Zihan and Zhang, Jun and Li, Ke and Ji, Rongrong and Shen, Chunhua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35959--35970},
  year={2022}
}


@article{lee2022uniclip,
  title={UniCLIP: Unified Framework for Contrastive Language-Image Pre-training},
  author={Lee, Janghyeon and Kim, Jongsuk and Shon, Hyounguk and Kim, Bumsoo and Kim, Seung Hwan and Lee, Honglak and Kim, Junmo},
  journal={arXiv preprint arXiv:2209.13430},
  year={2022}
}

@article{schuhmann2021laion400m,
  title={Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
  author={Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:2111.02114},
  year={2021}
}

@software{ilharco2021openclip,
  title = {OpenCLIP},
  author = {Ilharco, Gabriel and Wortsman, Mitchell and Carlini, Nicholas and Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Namkoong, Hongseok and Miller, John and Hajishirzi, Hannaneh and Farhadi, Ali and Schmidt, Ludwig},
  year = 2021,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.5143773},
}

@article{gao2023softclip,
  title={SoftCLIP: Softer Cross-modal Alignment Makes CLIP Stronger},
  author={Gao, Yuting and Liu, Jinfeng and Xu, Zihan and Wu, Tong and Liu, Wei and Yang, Jie and Li, Ke and Sun, Xing},
  journal={arXiv preprint arXiv:2303.17561},
  year={2023}
}

@article{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32897--32912},
  year={2022}
}

@inproceedings{desai2023hyperbolic,
  title={Hyperbolic image-text representations},
  author={Desai, Karan and Nickel, Maximilian and Rajpurohit, Tanmay and Johnson, Justin and Vedantam, Shanmukha Ramakrishna},
  booktitle={International Conference on Machine Learning},
  pages={7694--7731},
  year={2023},
  organization={PMLR}
}

@article{wang2024qwen2,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{ye2024mplug,
  title={mplug-owl3: Towards long image-sequence understanding in multi-modal large language models},
  author={Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2408.04840},
  year={2024}
}

@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

@article{mckinzie2024mm1,
  title={Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  author={McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
  journal={arXiv preprint arXiv:2403.09611},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{caffagni2024wiki,
  title={Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs},
  author={Caffagni, Davide and Cocchi, Federico and Moratelli, Nicholas and Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={1818--1826},
  year={2024}
}

@article{agrawal2024pixtral,
  title={Pixtral 12B},
  author={Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Chaplot, Devendra and Chudnovsky, Jessica and Garg, Saurabh and Gervet, Theophile and Ghosh, Soham and H{\'e}liou, Am{\'e}lie and Jacob, Paul and others},
  journal={arXiv preprint arXiv:2410.07073},
  year={2024}
}


@article{wu2017visual,
  title={Visual question answering: A survey of methods and datasets},
  author={Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and Van Den Hengel, Anton},
  journal={Computer Vision and Image Understanding},
  volume={163},
  pages={21--40},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{anderson2018bottom,
  title={Bottom-up and top-down attention for image captioning and visual question answering},
  author={Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6077--6086},
  year={2018}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}

@inproceedings{schwenk2022okvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European conference on computer vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{goyal2017making,
  title={Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@inproceedings{shah2019kvqa,
  title={Kvqa: Knowledge-aware visual question answering},
  author={Shah, Sanket and Mishra, Anand and Yadati, Naganand and Talukdar, Partha Pratim},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={8876--8884},
  year={2019}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}


@article{lee2019latent,
  title={Latent retrieval for weakly supervised open domain question answering},
  author={Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1906.00300},
  year={2019}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@article{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2004.04906},
  year={2020}
}

@article{fevry2020entities,
  title={Entities as experts: Sparse memory access with entity supervision},
  author={F{\'e}vry, Thibault and Soares, Livio Baldini and FitzGerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom},
  journal={arXiv preprint arXiv:2004.07202},
  year={2020}
}

@inproceedings{hu2023reveal,
  title={Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory},
  author={Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Wang, Zirui and Chang, Kai-Wei and Sun, Yizhou and Schmid, Cordelia and Ross, David A and Fathi, Alireza},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={23369--23379},
  year={2023}
}

@article{yu2024visrag,
  title={VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@article{chen2022murag,
  title={Murag: Multimodal retrieval-augmented generator for open question answering over images and text},
  author={Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W},
  journal={arXiv preprint arXiv:2210.02928},
  year={2022}
}

@article{qu2024alleviating,
  title={Alleviating hallucination in large vision-language models with active retrieval augmentation},
  author={Qu, Xiaoye and Chen, Qiyuan and Wei, Wei and Sun, Jishuo and Dong, Jianfeng},
  journal={arXiv preprint arXiv:2408.00555},
  year={2024}
}

@article{ow1988filtered,
  title={Filtered beam search in scheduling},
  author={Ow, Peng Si and Morton, Thomas E},
  journal={The International Journal Of Production Research},
  volume={26},
  number={1},
  pages={35--62},
  year={1988},
  publisher={Taylor \& Francis}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{fan2018hierarchical,
  title={Hierarchical neural story generation},
  author={Fan, Angela and Lewis, Mike and Dauphin, Yann},
  journal={arXiv preprint arXiv:1805.04833},
  year={2018}
}

@article{lin2024mmembeduniversalmultimodalretrieval,
      title={MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs}, 
      author={Sheng-Chieh Lin and Chankyu Lee and Mohammad Shoeybi and Jimmy Lin and Bryan Catanzaro and Wei Ping},
      journal={arXiv preprint arXiv:2411.02571},
      year={2024},
}
    
@InProceedings{Liu_2021_ICCV,
    author    = {Liu, Zheyuan and Rodriguez-Opazo, Cristian and Teney, Damien and Gould, Stephen},
    title     = {Image Retrieval on Real-Life Images With Pre-Trained Vision-and-Language Models},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {2125-2134}
}

@misc{suhr2019corpusreasoningnaturallanguage,
      title={A Corpus for Reasoning About Natural Language Grounded in Photographs}, 
      author={Alane Suhr and Stephanie Zhou and Ally Zhang and Iris Zhang and Huajun Bai and Yoav Artzi},
      year={2019},
      eprint={1811.00491},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00491}, 
}

@article{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2020},
      booktitle = {ICLR},
}

@misc{rombach2021highresolution,
      title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
      year={2021},
      eprint={2112.10752},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@InProceedings{denkowski:lavie:meteor-wmt:2014,
  author    = {Michael Denkowski and Alon Lavie},
  title     = {Meteor Universal: Language Specific Translation Evaluation for Any Target Language},
  booktitle = {Proceedings of the EACL 2014 Workshop on Statistical Machine Translation},
  year      = {2014},
}

@ARTICLE{1966SPhD...10..707L,
       author = {{Levenshtein}, V.~I.},
        title = "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}",
      journal = {Soviet Physics Doklady},
         year = 1966,
        month = feb,
       volume = {10},
        pages = {707},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1966SPhD...10..707L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{10.3115/1073083.1073135,
author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
title = {BLEU: a method for automatic evaluation of machine translation},
year = {2002},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1073083.1073135},
doi = {10.3115/1073083.1073135},
abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {311–318},
numpages = {8},
location = {Philadelphia, Pennsylvania},
series = {ACL '02}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@inproceedings{
burns2023wiki,
title={A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding},
author={Andrea Burns and Krishna Srinivasan and Joshua Ainslie and Geoff Brown and Bryan A. Plummer and Kate Saenko and Jianmo Ni and Mandy Guo},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
year={2023},
url={https://openreview.net/forum?id=rwcLHjtUmn}
}

@inproceedings{yan-xie-2024-echosight,
title = "{E}cho{S}ight: Advancing Visual-Language Models with {W}iki Knowledge",
author = "Yan, Yibin  and
Xie, Weidi",
editor = "Al-Onaizan, Yaser  and Bansal, Mohit  and Chen, Yun-Nung",
booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
month = nov,
year = "2024",
address = "Miami, Florida, USA",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2024.findings-emnlp.83",
pages = "1538--1551",
} 

@inproceedings{bem-emnlp2022,
title	= {Tomayto, Tomahto. Beyond Token-level Answer Equivalence for Question Answering Evaluation},
author	= {Jannis Bulian and Christian Carl Friedrich Buck and Wojciech Paweł Gajewski and Benjamin Boerschinger and Tal Schuster},
year	= {2022},
URL	= {https://aclanthology.org/2022.emnlp-main.20.pdf},
booktitle	= {EMNLP 2022}
}

@inproceedings{bert-2018,
title	= {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
author	= {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina N. Toutanova},
year	= {2018},
URL	= {https://arxiv.org/abs/1810.04805}
}