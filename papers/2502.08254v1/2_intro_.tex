\section{Introduction}
\label{sec:introduction}
%
Multimodal retrieval, the task of retrieving relevant information across text, visual and other modalities, has gained significant attention in recent years~\cite{radford2021clip,li2022blip,zhai2023sigmoid,chen2023pali, yu2022coca, wang2023image, zhou2022non, zhai2019large, yao2021filip, goel2022cyclip, gao2022pyramidclip, lee2022uniclip, ilharco2021openclip, schuhmann2021laion400m, gao2023softclip, bao2022vlmo, desai2023hyperbolic} demonstrating strong performance on a variety of tasks in supervised, zero-shot, and out-of-domain applications.
However, recent methods still struggle to capture complex, compositional requests that require specific reasoning about visual content.
On the other hand, LMMs~\cite{openai2024gpt4,geminiteam2024gemini,anthropic2024claude,zhu2024minigpt,liu2023llava,liu2023improvedllava,chen2024far,jiang2024mantis, wang2024qwen2, ye2024mplug, chen2024internvl, mckinzie2024mm1, wu2023next, dubey2024llama, agrawal2024pixtral} can deal with complex visio-linguistic requests with language output, without the inherent ability to retrieve and output relevant images to support the answer.
This raises an intriguing question for which we seek to make advancements in our work: \emph{How can we integrate the multimodal reasoning capabilities of LMMs with the retrieval capabilities of composed multimodal models?} 
%

\begin{figure}[t]
  \centering
   \includegraphics[width=1\linewidth]{figures/teaser_v2.png}
   \vspace{-2mm}
   \caption{\underline{\tasknamebos}. Given an query image and question, \modelname can retrieve an image from a database and can produce a textual answer that offers further clarification and details.}
   \label{fig:main_figure}
   \vspace{-4mm}
\end{figure}


%
Let us consider a user asking ``what is the larval state of this butterfly?'' as in Fig.~\ref{fig:main_figure}.
The model must not only recognize the species of butterfly, but also understand the query and find an image depicting the corresponding caterpillar stage, which has little visual similarity with the query.
%
While prior work~\cite{han2017automatic,ak2018learning,Vo_2019_CVPR, guo2018dialog,guo2019fashion, hou2021learning,chen2020image, Baldrati_2022_CVPR, Goenka_2022_CVPR} has explored composed retrieval techniques for such structured requests, these approaches have limitations in providing comprehensive responses.
On the other hand, Visual Question Answering (VQA) methods~\cite{antol2015vqa, wu2017visual, anderson2018bottom, schwenk2022okvqa, singh2019towards, shah2019kvqa, goyal2017making, gurari2018vizwiz, yan-xie-2024-echosight}, even when leveraging LMMs, are restricted to answering solely with language.
An ideal model should both retrieve a relevant image and generate explanatory text that complements and further explains the visual information.
Referring back to Fig.~\ref{fig:main_figure}, the model returns an image of the butterfly's larval stage and a comment, \eg ``The larval stage of the Old World swallowtail is a caterpillar, which can be seen feeding on a wild carrot plant in the image.''.
We refer to this novel multimodal task as \emph{\tasknametitle}~(\tasknameshort).


%
%
%
%
%
In this work, we propose \modelname, a novel Unified Commented Retrieval Network that leverages the complementary strengths of discriminative cross-modal matching and generative language models to perform \tasknameshort. 
First, we present a retrieval module that aligns the hidden state of LMMs with the retrieval space of CLIP models.
This allows us to integrate the deep visio-linguistic reasoning of LMMs with powerful multimodal and cross-modal retrieval capabilities. 
The retrieval model is trained to be aware of the complex answers and comments, rather than simple image captions like in previous work~\cite{wei2023uniir, lin2024mmembeduniversalmultimodalretrieval, jiang2024vlm2vec}.
Second, we introduce an entity adapter module to inject the retrieved entities back into the LMM, so they can condition the generated answers and comments.
This technique enables interleaved, integrated multimodal output at inference time, which goes beyond post-processing of output tokens
%
or prompt engineering of the LMM, \eg with RAG~\cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}.
By training these new modules for \tasknameshort, we strengthen the connection between the initial question and the retrieved entities, enabling the generation of more coherent and relevant textual responses.
Notably, our contributions are additions to a frozen LMM, so, unlike~\cite{jiang2024vlm2vec, lin2024mmembeduniversalmultimodalretrieval}, we guarantee the preservation of all its original capabilities (such as captioning, VQA, grounding, and more) within our unified framework.

%
Existing multimodal tasks and datasets~\cite{hu2023open, kazemzadeh2014referitgame, chen2023can, wu2021fashion, liu2021image} have simplistic (1-3 words) answers that do not explain why and how the retrieved entity is relevant to the initial query, and thus are not well suited for training and evaluating models for  \tasknameshort.
In this work, we introduce two challenging human-curated \tasknameshort datasets based on the \cirr~\cite{Liu_2021_ICCV} and \wikimm dataset~\cite{burns2023wiki}, that blend aspects of composed retrieval and VQA. 
Given an input image and question, the goal is to retrieve from a large candidate pool the entity that answers the question, and produce an additional textual response that offers further clarification and details (Fig.~\ref{fig:main_figure}). 
This task echoes research showing that humans learn better when they can jointly examine textual responses alongside the relevant visuals~\cite{mayer2003three}.
%
%

%
Our evaluations on diverse datasets for composed retrieval and \taskname show significant improvements of \modelname over state-of-the-art models.
In particular, we show an average improvement for composed retrieval of $+4.5\%$ on \fashioniq, \cirr, \oven, \infoseek, and the proposed \wikicomment dataset in terms of recall when compared to UniIR~\cite{wei2023uniir}.
Moreover, we show an improvement of +14.9\% in terms of METEOR score over a RAG approach combining UniIR and InternVL2 on the \tasknameshort task.
%


%
%