[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021clip",
        "author": "Alec Radford and\nJong Wook Kim and\nChris Hallacy and\nAditya Ramesh and\nGabriel Goh and\nSandhini Agarwal and\nGirish Sastry and\nAmanda Askell and\nPamela Mishkin and\nJack Clark and\nGretchen Krueger and\nIlya Sutskever",
        "title": "Learning Transferable Visual Models From Natural Language Supervision"
      },
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      },
      {
        "key": "zhai2023sigmoid",
        "author": "Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas",
        "title": "Sigmoid loss for language image pre-training"
      },
      {
        "key": "chen2023pali",
        "author": "Chen, Xi and Wang, Xiao and Beyer, Lucas and Kolesnikov, Alexander and Wu, Jialin and Voigtlaender, Paul and Mustafa, Basil and Goodman, Sebastian and Alabdulmohsin, Ibrahim and Padlewski, Piotr and others",
        "title": "Pali-3 vision language models: Smaller, faster, stronger"
      },
      {
        "key": "yu2022coca",
        "author": "Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui",
        "title": "Coca: Contrastive captioners are image-text foundation models"
      },
      {
        "key": "wang2023image",
        "author": "Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others",
        "title": "Image as a foreign language: Beit pretraining for vision and vision-language tasks"
      },
      {
        "key": "zhou2022non",
        "author": "Zhou, Jinghao and Dong, Li and Gan, Zhe and Wang, Lijuan and Wei, Furu",
        "title": "Non-Contrastive Learning Meets Language-Image Pre-Training"
      },
      {
        "key": "zhai2019large",
        "author": "Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others",
        "title": "A large-scale study of representation learning with the visual task adaptation benchmark"
      },
      {
        "key": "yao2021filip",
        "author": "Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing",
        "title": "FILIP: fine-grained interactive language-image pre-training"
      },
      {
        "key": "gao2022pyramidclip",
        "author": "Gao, Yuting and Liu, Jinfeng and Xu, Zihan and Zhang, Jun and Li, Ke and Ji, Rongrong and Shen, Chunhua",
        "title": "Pyramidclip: Hierarchical feature alignment for vision-language model pretraining"
      },
      {
        "key": "lee2022uniclip",
        "author": "Lee, Janghyeon and Kim, Jongsuk and Shon, Hyounguk and Kim, Bumsoo and Kim, Seung Hwan and Lee, Honglak and Kim, Junmo",
        "title": "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training"
      },
      {
        "key": "ilharco2021openclip",
        "author": "Ilharco, Gabriel and Wortsman, Mitchell and Carlini, Nicholas and Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Namkoong, Hongseok and Miller, John and Hajishirzi, Hannaneh and Farhadi, Ali and Schmidt, Ludwig",
        "title": "OpenCLIP"
      },
      {
        "key": "schuhmann2021laion400m",
        "author": "Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran",
        "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs"
      },
      {
        "key": "bao2022vlmo",
        "author": "Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu",
        "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vo2019composing",
        "author": "Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James",
        "title": "Composing text and image for image retrieval-an empirical odyssey"
      },
      {
        "key": "Baldrati_2022_CVPR",
        "author": "Baldrati, Alberto and Bertini, Marco and Uricchio, Tiberio and Del Bimbo, Alberto",
        "title": "Effective Conditioned and Composed Image Retrieval Combining CLIP-Based Features"
      },
      {
        "key": "Baldrati_2023_ICCV",
        "author": "Baldrati, Alberto and Agnolucci, Lorenzo and Bertini, Marco and Del Bimbo, Alberto",
        "title": "Zero-Shot Composed Image Retrieval with Textual Inversion"
      },
      {
        "key": "Goenka_2022_CVPR",
        "author": "Goenka, Sonam and Zheng, Zhaoheng and Jaiswal, Ayush and Chada, Rakesh and Wu, Yue and Hedau, Varsha and Natarajan, Pradeep",
        "title": "FashionVLP: Vision Language Transformer for Fashion Retrieval With Feedback"
      },
      {
        "key": "Chen_2020_CVPR",
        "author": "Chen, Yanbei and Gong, Shaogang and Bazzani, Loris",
        "title": "Image Search With Text Feedback by Visiolinguistic Attention Learning"
      },
      {
        "key": "Wu_2021_CVPR",
        "author": "Wu, Hui and Gao, Yupeng and Guo, Xiaoxiao and Al-Halah, Ziad and Rennie, Steven and Grauman, Kristen and Feris, Rogerio",
        "title": "Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback"
      },
      {
        "key": "Gu_2024_CVPR",
        "author": "Gu, Geonmo and Chun, Sanghyuk and Kim, Wonjae and Kang, Yoohoon and Yun, Sangdoo",
        "title": "Language-only Training of Zero-shot Composed Image Retrieval"
      },
      {
        "key": "Vaze_2023_CVPR",
        "author": "Vaze, Sagar and Carion, Nicolas and Misra, Ishan",
        "title": "GeneCIS: A Benchmark for General Conditional Image Similarity"
      },
      {
        "key": "Suo_2024_CVPR",
        "author": "Suo, Yucheng and Ma, Fan and Zhu, Linchao and Yang, Yi",
        "title": "Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval"
      },
      {
        "key": "Wan_2024_CVPR",
        "author": "Wan, Yongquan and Wang, Wenhai and Zou, Guobing and Zhang, Bofeng",
        "title": "Cross-modal Feature Alignment and Fusion for Composed Image Retrieval"
      },
      {
        "key": "Liu_2024_WACV",
        "author": "Liu, Zheyuan and Sun, Weixuan and Hong, Yicong and Teney, Damien and Gould, Stephen",
        "title": "Bi-Directional Training for Composed Image Retrieval via Text Prompt Learning"
      },
      {
        "key": "karthik2023vision",
        "author": "Karthik, Shyamgopal and Roth, Karsten and Mancini, Massimiliano and Akata, Zeynep",
        "title": "Vision-by-language for training-free compositional image retrieval"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2024magiclens",
        "author": "Zhang, Kai and Luan, Yi and Hu, Hexiang and Lee, Kenton and Qiao, Siyuan and Chen, Wenhu and Su, Yu and Chang, Ming-Wei",
        "title": "Magiclens: Self-supervised image retrieval with open-ended instructions"
      },
      {
        "key": "wei2023uniir",
        "author": "Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu",
        "title": "Uniir: Training and benchmarking universal multimodal information retrievers"
      },
      {
        "key": "jiang2024vlm2vec",
        "author": "Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu",
        "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks"
      },
      {
        "key": "lin2024mmembeduniversalmultimodalretrieval",
        "author": "Sheng-Chieh Lin and Chankyu Lee and Mohammad Shoeybi and Jimmy Lin and Bryan Catanzaro and Wei Ping",
        "title": "MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs"
      },
      {
        "key": "qi2024roravlm",
        "author": "Jingyuan Qi and Zhiyang Xu and Rulin Shao and Yang Chen and Jing Di and Yu Cheng and Qifan Wang and Lifu Huang",
        "title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models"
      },
      {
        "key": "karthik2023vision",
        "author": "Karthik, Shyamgopal and Roth, Karsten and Mancini, Massimiliano and Akata, Zeynep",
        "title": "Vision-by-language for training-free compositional image retrieval"
      },
      {
        "key": "levy2024chatting",
        "author": "Levy, Matan and Ben-Ari, Rami and Darshan, Nir and Lischinski, Dani",
        "title": "Chatting makes perfect: Chat-based image retrieval"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhang2024magiclens",
        "author": "Zhang, Kai and Luan, Yi and Hu, Hexiang and Lee, Kenton and Qiao, Siyuan and Chen, Wenhu and Su, Yu and Chang, Ming-Wei",
        "title": "Magiclens: Self-supervised image retrieval with open-ended instructions"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wei2023uniir",
        "author": "Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu",
        "title": "Uniir: Training and benchmarking universal multimodal information retrievers"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "lin2024mmembeduniversalmultimodalretrieval",
        "author": "Sheng-Chieh Lin and Chankyu Lee and Mohammad Shoeybi and Jimmy Lin and Bryan Catanzaro and Wei Ping",
        "title": "MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs"
      },
      {
        "key": "jiang2024vlm2vec",
        "author": "Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu",
        "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "openai2024gpt4",
        "author": "OpenAI",
        "title": "GPT-4 Technical Report"
      },
      {
        "key": "geminiteam2024gemini",
        "author": "Gemini Team",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      {
        "key": "anthropic2024claude",
        "author": "Anthropic, AI",
        "title": "The claude 3 model family: Opus, sonnet, haiku"
      },
      {
        "key": "zhu2024minigpt",
        "author": "Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny",
        "title": "Mini{GPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
      },
      {
        "key": "liu2023llava",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual Instruction Tuning"
      },
      {
        "key": "liu2023improvedllava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae",
        "title": "Improved Baselines with Visual Instruction Tuning"
      },
      {
        "key": "chen2024far",
        "author": "Zhe Chen and Weiyun Wang and Hao Tian and Shenglong Ye and Zhangwei Gao and Erfei Cui and Wenwen Tong and Kongzhi Hu and Jiapeng Luo and Zheng Ma and Ji Ma and Jiaqi Wang and Xiaoyi Dong and Hang Yan and Hewei Guo and Conghui He and Botian Shi and Zhenjiang Jin and Chao Xu and Bin Wang and Xingjian Wei and Wei Li and Wenjian Zhang and Bo Zhang and Pinlong Cai and Licheng Wen and Xiangchao Yan and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang",
        "title": "How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites"
      },
      {
        "key": "jiang2024mantis",
        "author": "Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max Ku and Qian Liu and Wenhu Chen",
        "title": "MANTIS: Interleaved Multi-Image Instruction Tuning"
      },
      {
        "key": "wang2024qwen2",
        "author": "Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others",
        "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution"
      },
      {
        "key": "ye2024mplug",
        "author": "Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren",
        "title": "mplug-owl3: Towards long image-sequence understanding in multi-modal large language models"
      },
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      },
      {
        "key": "mckinzie2024mm1",
        "author": "McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others",
        "title": "Mm1: Methods, analysis \\& insights from multimodal llm pre-training"
      },
      {
        "key": "wu2023next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      },
      {
        "key": "dubey2024llama",
        "author": "Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others",
        "title": "The llama 3 herd of models"
      },
      {
        "key": "agrawal2024pixtral",
        "author": "Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Chaplot, Devendra and Chudnovsky, Jessica and Garg, Saurabh and Gervet, Theophile and Ghosh, Soham and H{\\'e}liou, Am{\\'e}lie and Jacob, Paul and others",
        "title": "Pixtral 12B"
      },
      {
        "key": "lin2024mmembeduniversalmultimodalretrieval",
        "author": "Sheng-Chieh Lin and Chankyu Lee and Mohammad Shoeybi and Jimmy Lin and Bryan Catanzaro and Wei Ping",
        "title": "MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yin2023survey",
        "author": "Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong",
        "title": "A Survey on Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "tian2024mminterleaved",
        "author": "Tian, Changyao and Zhu, Xizhou and Xiong, Yuwen and Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Chen, Yuntao and Lu, Lewei and Lu, Tong and Zhou, Jie and Li, Hongsheng and Qiao, Yu and Dai, Jifeng",
        "title": "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer"
      },
      {
        "key": "li2023textbind",
        "author": "Li, Huayang and Li, Siheng and Cai, Deng and Wang, Longyue and Liu, Lemao and Watanabe, Taro and Yang, Yujiu and Shi, Shuming",
        "title": "TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"
      },
      {
        "key": "koh2023grounding",
        "author": "Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel",
        "title": "Grounding language models to images for multimodal inputs and outputs"
      },
      {
        "key": "wu2023next",
        "author": "Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng",
        "title": "Next-gpt: Any-to-any multimodal llm"
      },
      {
        "key": "ge2023making",
        "author": "Ge, Yuying and Zhao, Sijie and Zeng, Ziyun and Ge, Yixiao and Li, Chen and Wang, Xintao and Shan, Ying",
        "title": "Making llama see and draw with seed tokenizer"
      },
      {
        "key": "yu2023language",
        "author": "Yu, Lijun and Lezama, Jos{\\'e} and Gundavarapu, Nitesh B and Versari, Luca and Sohn, Kihyuk and Minnen, David and Cheng, Yong and Birodkar, Vighnesh and Gupta, Agrim and Gu, Xiuye and others",
        "title": "Language Model Beats Diffusion--Tokenizer is Key to Visual Generation"
      },
      {
        "key": "team2024chameleon",
        "author": "Team, Chameleon",
        "title": "Chameleon: Mixed-modal early-fusion foundation models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lee2019latent",
        "author": "Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina",
        "title": "Latent retrieval for weakly supervised open domain question answering"
      },
      {
        "key": "lewis2020retrieval",
        "author": "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\\\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\\\"a}schel, Tim and others",
        "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks"
      },
      {
        "key": "guu2020retrieval",
        "author": "Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei",
        "title": "Retrieval augmented language model pre-training"
      },
      {
        "key": "karpukhin2020dense",
        "author": "Karpukhin, Vladimir and O{\\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau",
        "title": "Dense passage retrieval for open-domain question answering"
      },
      {
        "key": "fevry2020entities",
        "author": "F{\\'e}vry, Thibault and Soares, Livio Baldini and FitzGerald, Nicholas and Choi, Eunsol and Kwiatkowski, Tom",
        "title": "Entities as experts: Sparse memory access with entity supervision"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "qi2024roravlm",
        "author": "Jingyuan Qi and Zhiyang Xu and Rulin Shao and Yang Chen and Jing Di and Yu Cheng and Qifan Wang and Lifu Huang",
        "title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models"
      },
      {
        "key": "hu2023reveal",
        "author": "Hu, Ziniu and Iscen, Ahmet and Sun, Chen and Wang, Zirui and Chang, Kai-Wei and Sun, Yizhou and Schmid, Cordelia and Ross, David A and Fathi, Alireza",
        "title": "Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory"
      },
      {
        "key": "yu2024visrag",
        "author": "Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others",
        "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents"
      },
      {
        "key": "chen2022murag",
        "author": "Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W",
        "title": "Murag: Multimodal retrieval-augmented generator for open question answering over images and text"
      },
      {
        "key": "qu2024alleviating",
        "author": "Qu, Xiaoye and Chen, Qiyuan and Wei, Wei and Sun, Jishuo and Dong, Jianfeng",
        "title": "Alleviating hallucination in large vision-language models with active retrieval augmentation"
      },
      {
        "key": "caffagni2024wiki",
        "author": "Caffagni, Davide and Cocchi, Federico and Moratelli, Nicholas and Sarto, Sara and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita",
        "title": "Wiki-LLaVA: Hierarchical Retrieval-Augmented Generation for Multimodal LLMs"
      },
      {
        "key": "yan-xie-2024-echosight",
        "author": "Yan, Yibin  and\nXie, Weidi",
        "title": "{E}cho{S}ight: Advancing Visual-Language Models with {W}iki Knowledge"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wei2023uniir",
        "author": "Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu",
        "title": "Uniir: Training and benchmarking universal multimodal information retrievers"
      },
      {
        "key": "jiang2024vlm2vec",
        "author": "Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu",
        "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks"
      },
      {
        "key": "qi2024roravlm",
        "author": "Jingyuan Qi and Zhiyang Xu and Rulin Shao and Yang Chen and Jing Di and Yu Cheng and Qifan Wang and Lifu Huang",
        "title": "RoRA-VLM: Robust Retrieval-Augmented Vision Language Models"
      },
      {
        "key": "karthik2023vision",
        "author": "Karthik, Shyamgopal and Roth, Karsten and Mancini, Massimiliano and Akata, Zeynep",
        "title": "Vision-by-language for training-free compositional image retrieval"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "hu2023open",
        "author": "Hu, Hexiang and Luan, Yi and Chen, Yang and Khandelwal, Urvashi and Joshi, Mandar and Lee, Kenton and Toutanova, Kristina and Chang, Ming-Wei",
        "title": "Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities"
      },
      {
        "key": "chen2023can",
        "author": "Chen, Yang and Hu, Hexiang and Luan, Yi and Sun, Haitian and Changpinyo, Soravit and Ritter, Alan and Chang, Ming-Wei",
        "title": "Can pre-trained vision and language models answer visual information-seeking questions?"
      },
      {
        "key": "mensink2023encyclopedic",
        "author": "Mensink, Thomas and Uijlings, Jasper and Castrejon, Lluis and Goel, Arushi and Cadar, Felipe and Zhou, Howard and Sha, Fei and Araujo, Andr{\\'e} and Ferrari, Vittorio",
        "title": "Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories"
      }
    ]
  }
]