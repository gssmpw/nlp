\section{\tasknametitle~(\tasknameshort) Task}
\label{sec:datasets}

As already discussed in related work, the related task of composed retrieval is well explored in the literature, and there exist a number of datasets for it.
Some datasets for this task, \eg, \cirr~\cite{Liu_2021_ICCV} and \fashioniq~\cite{guo2019fashion}, include multimodal queries, while the targets are only images.
%
Other relevant datasets, \eg, \oven~\cite{hu2023open} and \infoseek~\cite{chen2023can}, have both multimodal queries and targets, however the connection between inputs and outputs are simple high-level entities.
Overall, existing datasets either lack target text altogether or feature overly simplistic textual responses that are directly related to the target image. 
These datasets are not well-suited to evaluate the full extent of the \taskname task, where the goal is not only to retrieve a relevant target image, but also to generate a detailed, complex textual response that refer to both the query and target image.
Therefore, we propose a technique to automatically augment some existing datasets with textual answers and comments to the multimodal questions and image-only answers.
In this work, we focus on 2 different domains, images of real-world object categories with \cirr~\cite{Liu_2021_ICCV} and Wikipedia concepts with \wikimm~\cite{burns2023suite}, but the approach itself generalizes to other domains.
%

%
The proposed approach to obtain a tuple of a query image, a textual question, a target image and a textual answer for \tasknameshort is composed by three parts: 
1) pair related images; 
2) generate questions and answers related to those images; and 
3) manual annotation to create a golden set for evaluation.
We couple images that belong to similar concepts, which definition depends on the task.
For \cirr, image pairs are already present in the original set, as images were grouped together based on visual similarity. %
For \wikimm, we pair images belonging to the same Wikipedia article, because that denotes a semantic link.
We subsample a set of 50K samples for training and 2K for testing.

In the second step, we aim at generating questions and answers that can relate the two images, with the goal of simulating the behavior of a user that asks a question about an uploaded image, and obtain an answer with text and the target image supporting the text.
We automatically generate these questions and answers by prompting using Anthropic Claude 3.5 Sonnet.
For \cirr, the question is the textual modification instruction that is already included in the original set, so we only generate the textual answer.
For \wikimm, we generate a question-answer pair that is related to the query and target images, their captions and the Wikipedia article.
This task is more challenging and results in noisier data.
We name these datasets \cirrcomment and \wikicomment, respectively, and show examples\footnote{The Supplementary Material shows more examples of the datasets and the prompts used to generate them.} in Fig.~\ref{fig:qualitative_commenting}.

Finally, we create golden sets for evaluation purposes by asking annotators to validate the plausibility of the question and the quality of the generated text, manually adjusting them if necessary.
We performed human auditing of the generated comments on the validation set of \cirrcomment and found that $\sim\!97\%$ of the audited comments were marked as high quality.
For the test set of \wikicomment, we conducted a manual review by filtering out image pairs that are not related to each other, questions that are irrelevant and comments that are not aligned with the question.
In addition, annotators performed the necessary edits of the question and answer pairs to improve their quality.
This results in a golden set of 695 out of 1768 ($\sim\!39\%$), showing the importance of having an annotation process for evaluation, that is often not considered in previous datasets. 
