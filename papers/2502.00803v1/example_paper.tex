%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage{xcolor}

% Define custom colors
\definecolor{darkred}{rgb}{0.6, 0, 0}
\definecolor{darkblue}{rgb}{0, 0, 0.6}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% ProPINN
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{ProPINN: Demystifying Propagation Failures in Physics-Informed Neural Networks}

\begin{document}

\twocolumn[
\icmltitle{ProPINN: Demystifying Propagation Failures in \\ Physics-Informed Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\begin{icmlauthorlist}
\icmlauthor{Haixu Wu}{software}
\icmlauthor{Yuezhou Ma}{software}
\icmlauthor{Hang Zhou}{software}
\icmlauthor{Huikun Weng}{software}
\icmlauthor{Jianmin Wang}{software}
\icmlauthor{Mingsheng Long}{software}
\end{icmlauthorlist}

\icmlaffiliation{software}{School of Software, BNRist, Tsinghua University. \\Haixu Wu $<$wuhx23@mails.tsinghua.edu.cn$>$}

\icmlcorrespondingauthor{Mingsheng Long}{mingsheng@tsinghua.edu.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Scientific machine learning, PDE solver}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Physics-informed neural networks (PINNs) have earned high expectations in solving partial differential equations (PDEs), but their optimization usually faces thorny challenges due to the unique {derivative}-dependent loss function. By analyzing the loss distribution, previous research observed the \emph{propagation failure} phenomenon of PINNs, intuitively described as the correct supervision for model outputs cannot ``propagate'' from initial states or boundaries to the interior domain. Going beyond intuitive understanding, this paper provides the first formal and in-depth study of propagation failure and its root cause. Based on a detailed comparison with classical finite element methods, we {ascribe} the failure to the conventional single-point-processing architecture of PINNs and further {prove} that propagation failure is essentially caused by the lower \emph{gradient correlation} of PINN models on nearby collocation points. Compared to superficial loss maps, this new perspective provides a more precise quantitative criterion to identify where and why PINN fails. The theoretical finding also inspires us to present a new PINN architecture, named ProPINN, which can effectively unite the gradient of region points for better propagation. ProPINN can reliably resolve PINN failure modes and significantly surpass advanced Transformer-based models with 46\% relative promotion.
\end{abstract}

\section{Introduction}
Accurately solving physics equations is essential to both scientific and engineering domains \cite{Wazwaz2002PartialDE,roubivcek2013nonlinear}. However, it is usually hard to obtain the analytic solution of PDEs. Therefore, classical numerical methods \cite{solin2005partial,dhatt2012finite} have been widely explored and served as a foundation for modern engineering \cite{ames2014numerical}. Recently, deep models have empowered significant progress in various domains and have also been applied in solving PDEs \cite{wang2023scientific}. As one pioneering progress, physics-informed neural networks~(PINNs) are proposed and widely studied \cite{raissi2019physics,Hao2022PhysicsInformedML}, which can approximate PDE solutions by formalizing equation constraints, initial and boundary conditions as loss function and enforcing the outputs and gradients of neural networks to satisfy target PDEs. {Taking advantage of} the automatic differentiation feature of deep learning frameworks \cite{jax2018github,Paszke2019PyTorchAI}, PINN can accurately calculate the derivation without domain discretization, posing a promising direction for solving PDEs.

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/intro.pdf}}
\vspace{-5pt}
	\caption{Comparison of PINN and our proposed ProPINN on the Convection benchmark ($\frac{\partial u}{\partial t}+50\frac{\partial u}{\partial x}=0$). In addition to the error map and residual loss, we also plot the gradient correlation of corresponding models between nearby points, which is newly proposed in this paper to identify the propagation failure. A low gradient correlation value (the darker color) indicates {that} the area is hard to propagate.}
	\label{fig:compare}
\end{center}
\vspace{-20pt}
\end{figure*}

Although PINNs have attracted great attention, they still face serious challenges in enabling robust training and can fail in some ``simple'' PDEs, which are called PINN failure modes \cite{krishnapriyan2021characterizing}. Researchers have attempted to tackle the training difficulties with new sampling strategies \cite{wu2023comprehensive,r3_wang_2023}, loss functions \cite{yu2022gradient,wu2024ropinn}, optimizers \cite{rathore2024challenges}, automatic differential methods \cite{shi2024stochastic}, etc. Especially, \citet{r3_wang_2023} noticed a special failure phenomenon during PINN optimization: the interior domain solely optimized by equation constraints will likely converge to a trivial solution. As shown in Figure \ref{fig:compare}(b), the equation constraint loss of PINN is sufficiently small but the approximated solution is still far from the ground truth. They attributed this to the failure in propagating the supervision of correct solution value from initial states or boundaries to interior points, calling it \emph{propagation failure}. Despite this concept seeming intuitive, the formal definition of ``propagation'' processes in PINN optimization and the root cause of propagation failure are still underexplored.

Some works start from observable loss maps and propose new sampling strategies to accumulate collocation points to areas with high residual losses to break the propagation ``barriers'' \cite{r3_wang_2023,anonymous2024lpinn}. Although they provide practical remedial measures, these loss-oriented methods may not solve the propagation issue at its root. As illustrated in Figure \ref{fig:compare}(b), we can find that PINN fails at the beginning according to the actual error map, while the residual loss is dispersive, especially for high-residual parts. To demystify the propagation failure of PINNs, we draw inspiration from traditional numerical methods, which rarely suffer from propagation issues. From the comparison with finite element methods (FEMs, \citet{dhatt2012finite}), we realize that the primary cause of propagation failure lies in the conventional design principle of PINN model architectures. Unlike FEMs that discretize the input domain as connected meshes, PINNs usually treat the input domain as a set of independently processed collocation points, which makes the optimization of different positions relatively independent, thereby reducing the ``interaction'' among PINN outputs on nearby positions and resulting in poor propagation.

Based on the above analysis, this paper theoretically proved that the lower \emph{gradient correlation} of PINNs among nearby points is a necessary and sufficient condition for propagation failure. Going beyond the residual loss mainly focused on by previous research \cite{wu2023comprehensive,r3_wang_2023}, gradient correlation provides a {foundational} understanding of propagation failure, which can serve as a precise criterion to identify propagation issues. For example, in Figure~\ref{fig:compare}(b), the area with the lowest gradient correlation {corresponds well to} the zone that first appears to have high error. {With the idea of} enhancing gradient correlation, we present ProPINN as a simple but effective PINN architecture, which can efficiently unite region gradients to boost propagation. Experimentally, ProPINN successfully mitigates the propagation failure (Figure~\ref{fig:compare}(c)) and achieves consistent state-of-the-art in various PDEs. %, surpassing advanced Transformer-based models. 
Our contributions are summarized as follows:
\vspace{-5pt}
\begin{itemize}
    \item We {initially} define the \emph{propagation failure} from the model architecture perspective and prove that {the root cause of failure is} gradient correlation, which can also serve as a precise criterion to identify PINN failures.
    \item ProPINN with multi-region mixing mechanism is presented as an efficient architecture, which can tightly unite the optimization of collocation points within a region for tackling propagation failure in PINNs.
    \item ProPINN can reliably mitigate PINN failure modes and achieve state-of-the-art with 46\% relative gain in typical PDE-solving tasks with favorable efficiency.
\end{itemize}

\section{Preliminaries}

A PDE defined in $\Omega\subseteq \mathbb{R}^{d+1}$ with equation constraints $\mathcal{F}$, initial {conditions} $\mathcal{I}$ and boundary conditions $\mathcal{B}$ {writes as}:
\begin{equation}\label{equ:pde}
\begin{split}
&\mathcal{F}(u)(\mathbf{x})=0, \mathbf{x}\in\Omega;\\
&\mathcal{I}(u)(\mathbf{x})=0, \mathbf{x}\in\Omega_{0};\ \mathcal{B}(u)(\mathbf{x})=0, \mathbf{x}\in\partial\Omega.
\end{split}
\end{equation}
{Here} $\mathbf{x}\in\Omega\subseteq \mathbb{R}^{d+1}$ denotes the position information of input points and $u:\mathbb{R}^{d+1}\to\mathbb{R}^{m}$ represents the target PDE solution~\cite{Wazwaz2002PartialDE,evans2010partial}. Usually, $\mathbf{x}=(x_1, \cdots, x_d, t)$ contains both spatial and temporal position information and $\Omega_0$ correspond to the $t=0$ situation. A physics-informed neural network $u_\theta$ will approximate the PDE solution $u$ by optimizing the following loss function:
\begin{equation}\label{equ:pinn_loss}
\begin{split}
    &\mathcal{L}(u_{\theta})=\frac{\lambda_{\text{res}}}{n_{\text{res}}}\sum_{{i=1}}^{n_{\text{res}}}\|\mathcal{F}(u_{\theta})(\mathbf{x}_{\text{res}}^{i})\|^2\\
    &+\frac{\lambda_{\text{ic}}}{n_{\text{ic}}}\sum_{{i=1}}^{n_{\text{ic}}}\|\mathcal{I}(u_{\theta})(\mathbf{x}_{\text{ic}}^{i})\|^2+\frac{\lambda_{\text{bc}}}{n_{\text{bc}}}\sum_{{i=1}}^{n_{\text{bc}}}\|\mathcal{B}(u_{\theta})(\mathbf{x}_{\text{bc}}^{i})\|^2,
\end{split}
\end{equation}
where $\lambda_{\ast}$ and $n_{\ast}$ represent the loss weights and {the} numbers of collocation points respectively \cite{raissi2019physics}. 

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/compare_new.pdf}}
\vspace{-5pt}
	\caption{Comparison between (a) FEMs and (b) PINNs. Blue arrows highlight quantities with direct interactions during training. Compared to FEMs, solution values of PINNs among different positions are under an implicit correlation by updating model parameter $\theta$.}
	\label{fig:intro}
\end{center}
\vspace{-20pt}
\end{figure*}

\vspace{-5pt}
\paragraph{Propagation failures} Unlike the conventional supervised learning that directly constrains the model output, the residual loss ($\mathcal{F}$ item in Eq.~\eqref{equ:pinn_loss}) only describes the {derivative} relation on different positions in $\Omega$ (e.g.~$\frac{\partial u_{\theta}}{\partial x_i}, \frac{\partial u_{\theta}}{\partial t}$) and the direct supervision for model outputs $u_\theta(\boldsymbol{x})$ only exists on initial state $\Omega_0$ or boundaries $\partial\Omega$. For interior points, without any supervision for model output and only constraining model's gradients may lead to a trivial solution. For example, without considering initial and boundary conditions, the all-zero function $u_\theta=0$ is also a solution for the convection equation $\frac{\partial u_\theta}{\partial t}+50\frac{\partial u_\theta}{\partial x}=0$. Thus, \emph{propagation failure} is proposed \cite{r3_wang_2023}. Their key idea is that to obtain a correct solution in the whole domain, the correct supervision of model outputs must propagate from the initial or boundary points to the interior domain during training. Although \citeauthor{r3_wang_2023}~provided an intuitive description of why propagation fails in PINN by analyzing the loss distribution, a formal and in-depth understanding of the root cause of propagation failure is underexplored, which is formally proved in our paper from the model architecture perspective.

\vspace{-5pt}
\paragraph{Training strategies} To tackle optimization challenges of PINNs, training strategies have been widely explored, which can be roughly categorized into the following two branches.

The first branch attempts to develop sampling strategies to calibrate collocation points at each iteration. These works mainly focus on the areas with high residual loss \cite{krishnapriyan2021characterizing,wang20222,wu2023comprehensive,anonymous2024lpinn}. Especially, to mitigate the propagation failure described above, R3 \cite{r3_wang_2023} is proposed by accumulating sampled collocation points around the high-residual area to break propagation ``barriers''. However, all these methods primarily attempt to remedy propagation failure by sampling points, overlooking the inherent deficiency of PINN architecture: independent optimization among points, which is explored in depth by our work.

Besides, PINN loss contained {multiple} components to be optimized (Eq.~\eqref{equ:pinn_loss}). Thus, loss reweighting is also essential. NTK~\cite{Wang2020WhenAW} proposes to adjust $\lambda_{\ast}$ to balance the convergence rate of different loss components. Considering the temporal causality of PDEs, causal PINN \cite{wang2024respecting} is presented to increase the loss weights of late points based on the accumulated residual of their previous time. Unlike these methods, this paper focuses on the model architecture, which is orthogonal to the loss-oriented work.

\vspace{-5pt}
\paragraph{Model architectures} Vanilla PINN \cite{raissi2019physics} is essentially a multilayer perception (MLP). Afterward, QRes~\cite{bu2021quadratic} and FLS \cite{wong2022learning} enhance the model capacity and position embedding respectively. Further, PirateNet \cite{wang2024piratenets} leverages residual networks for better scalability. However, all of these methods still process collocation points independently, overlooking spatiotemporal correlations of PDEs. Recently, PINNsFormer \cite{zhao2023pinnsformer} first introduced Transformer \cite{NIPS2017_3f5ee243} to PINNs and adopted the attention mechanism to capture temporal correlations among different points. Subsequently, SetPINN \cite{nagda2024setpinns} extends Transformer to a {general spatiotemporal framework}. Unlike these models, our proposed ProPINN stems from the in-depth study of propagation failures {without relying on} the computation-intensive Transformer backbone, achieving better performance and efficiency.

% \vspace{-5pt}
\section{Method}
\label{sec:method}

As aforementioned, we focus on the \emph{propagation failure} of PINNs, which is one of the foundation problems of PINN optimization. This section will first discuss the propagation properties of PINNs, where we take insights from FEMs to give an intrinsic understanding of why propagation failures exist. Based on the theoretical results, we present ProPINN as a simple but effective PINN architecture, which achieves favorable propagation by uniting region gradients. All the proofs of theorems can be found in Appendix~\ref{appdix:proof}.


% \vspace{-5pt}
\subsection{Demystify Propagation Failure}

Previous work \cite{r3_wang_2023} attributed the propagation failure to ``some collocation points start converging to trivial solutions before the correct solution from initial/boundary points is able to reach them.'' However, we find that, in FEMs, the iteration will start from a trivial estimation and the interior areas also {hold incorrect} output supervision in the beginning iterations, while they present quite robust performance and do not affect by propagation issues. Thus, beyond direct and intuitive understandings, we believe there exists {an unexplored root cause for propagation failures}.

To demystify propagation failures of PINNs, we make a detailed comparison between FEMs and PINNs on their parameter updating processes. As presented in Figure \ref{fig:intro}, the parameters of FEMs are defined on discretized mesh points and nearby points will directly affect each other during optimization, which can be formally stated as follows.

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/ProPINN_new.pdf}}
\vspace{-5pt}
	\caption{Overall architecture of ProPINN from both (a) forward and (b) backward perspectives, where the single input point is augmented to point sets in multiscale regions, which can efficiently unite model parameter gradients on different positions within multiple regions.}
	\label{fig:propinn}
\end{center}
\vspace{-20pt}
\end{figure*}

\begin{theorem}[\textbf{Propagation in FEMs}]\label{theorem:fem}\cite{dhatt2012finite} 
Suppose that FEMs discretize $\Omega$ into computation meshes with $n$ nodes $\{\mathbf{x}_i\}_{i=1}^n$ and approximate the PDE solution by optimizing coefficients of basis functions $\{\Psi_i\}_{i=1}^n$, which are defined as region linear interpolation. Denote the coefficient of basis function $\Psi_i$ is $u_i$, which is also the solution value of the $i$-th node. With Jacobi method, the interaction among solution values $\{u_i\}_{i=1}^n$ at the $k$-th step is:
    \begin{equation}\label{equ:fem_propagation}
        u_j^{(k+1)}=\frac{1}{D(\Psi_j, \Psi_j)\
        }\bigg(b_j-\sum_{i\neq j}D(\Psi_i, \Psi_j)u_i^{(k)}\bigg),
    \end{equation}
where $\{b_j\}_{j=1}^{n}$ are constants related to external force. $D(\cdot, \cdot)$ is a variational version of PDE equation $\mathcal{F}(\cdot)$, which could be non-zero values for overlapped basis functions.
\end{theorem}

\begin{remark}[\textbf{FEMs are under active propagation}] Theorem~\ref{theorem:fem} demonstrates that coefficients of basis functions with overlap area are explicitly correlated. Thus, the change of $u_i^{(k)}$ will directly affect the value of its adjacent nodes, ensuring an active propagation among the whole domain.
\end{remark}

\begin{remark}[\textbf{Physical meanings of Eq.~\eqref{equ:fem_propagation}}] The parameter update in FEMs relies on $D(\cdot,\cdot)$, which is also named as stiffness matrix in solid mechanics \cite{yang1986stiffness}. $D(\Psi_i, \Psi_j)$ describes the force on the $j$-th node to make region balance when the $i$-th node has a unit displacement.
\end{remark}



Inspired by analyses of FEMs, we define the propagation in PINNs in terms of the influence of each point's value change on other points, {yielding the first formal measurement of the propagation failure}. Since in PINNs, the model output is determined by model parameter $\theta$, we represent ``value change'' from the gradient perspective, namely $\left.\frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}$. Thus, the propagation failure of PINNs can be defined as follows.

\begin{definition}[\textbf{Propagation failure in PINN}]\label{theorem:opt_block} {In spirit of} the physics meaning of Eq.~\eqref{equ:fem_propagation}, we define the ``stiffness'' coefficient between $\mathbf{x}$ and $\mathbf{x}^\prime$ for PINN $u_\theta$ as
\vspace{-5pt}
\begin{equation}\label{equ:core}
\begin{split}
D_{\text{PINN}}(\mathbf{x}, \mathbf{x}^{\prime}) = \bigg\|u_\theta(\mathbf{x}^\prime)-u_{\theta-\left.\lambda\frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}}(\mathbf{x}^\prime)\bigg\|,
\end{split}
\end{equation}
which {measures} the impact on {model output} at $\mathbf{x}^\prime$ after updating PINN with step size $\lambda$ at $\mathbf{x}$. This formula is analogous to applying a fixed force at $\mathbf{x}$ and observing {a} displacement at $\mathbf{x}^\prime$. If $\mathbf{x}$ and $\mathbf{x}^\prime$ are adjacent and $D_{\text{PINN}}(\mathbf{x}, \mathbf{x}^{\prime})$ is less than {an} empirically defined threshold $\epsilon$, we consider that propagation failure has occurred between $\mathbf{x}$ and $\mathbf{x}^\prime$.
\end{definition}

\begin{remark}[\textbf{Region propagation}]\label{remark:region}
In FEMs (Figure \ref{fig:intro}(b)), only overlapped basis functions (corresponding to adjacent points) could have a non-zero stiffness coefficient. Thus, we only discuss propagation among nearby points in Eq.~\eqref{equ:core}.
\end{remark}

Based on the above formal definition, we can further derive the root cause for propagation failures of PINNs as follows.

\begin{theorem}[\textbf{Gradient correlation}]\label{theorem:grad_corr} Given a PINN $u_\theta$ and adjacent points $\mathbf{x},\mathbf{x}^\prime\in\Omega$, the necessary and sufficient condition of propagation failure between $\mathbf{x}$ and $\mathbf{x}^\prime$ is a small gradient correlation, which is formally defined as follows
\begin{equation}\label{equ:core_understand}
\begin{split}
G_{u_\theta}(\mathbf{x}, \mathbf{x}^\prime)=\bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}^\prime}\bigg>\bigg\|.
\end{split}
\end{equation}
\end{theorem}
\paragraph{Why propagation failures exist in PINNs} According to the definition of Eq.~\eqref{equ:core_understand}, the gradient correlation is an inner product of high-dimensional tensors, where $\left. \frac{\partial u_\theta}{\partial \theta}\right|_{\ast}\in\mathbb{R}^{m\times |\theta|}$, $|\theta|$ denotes the parameter size (usually $>10^3$) and $m$ is the output dimension. Thus, these gradient tensors are easy to be orthogonal in the high-dimensional space \cite{Ball1997AnEI}, especially when different positions are independently optimized. For example, position $\mathbf{x}$ and position $\mathbf{x}^\prime$ could correspond to different parts of model parameters. Further, this analysis also provides insights for ``why PINNs cannot benefit from large models'' \cite{wang2024piratenets}, since larger parameter size $|\theta|$ more likely causes orthogonal gradients.


\subsection{Propagation Physics-Informed Neural Networks}\label{sec:propinn}
Theorem \ref{theorem:grad_corr} highlights that the gradient correlation is the foundation item that affects the propagation of PINNs. Note that the gradient correlation is essentially determined by the model architecture. Thus, we present ProPINN as a new PINN architecture to enhance the gradient correlation by introducing unified region gradient to each collocation point, which can effectively boost the propagation (Figure \ref{fig:compare}(c)).

As shown in Figure \ref{fig:propinn}, the key component of ProPINN is a multi-region mixing mechanism, which is a well-thought-out design considering both efficiency and performance.

\vspace{-5pt}
\paragraph{Differential perturbation} Given a single collocation point $\mathbf{x}\in\Omega\subseteq\mathbb{R}^{(d+1)}$, we first augment it by perturbating its position within multiscale regions. Note that different from PINNsFormer \cite{zhao2023pinnsformer} and SetPINN \cite{nagda2024setpinns} that only utilize augmented representations of multiple points but detach the gradient backpropagation, ProPINN leverages the perturbation as a differential layer and project all the augmented points into deep representations with shared projector $\mathcal{P}(\cdot)$:
\begin{equation}\label{equ:region_aug}
\begin{split}
&\operatorname{Diff\text{-}Aug}(\mathbf{x})=\bigg\{\mathbf{x}, \Big\{\{\mathbf{x}+\boldsymbol{\delta}_r^i\}_{i=1}^{k_r}\Big\}_{r=1}^{\text{\#scale}} \bigg\},\\
&\mathbf{z}_{\text{point}} = \mathcal{P}(\mathbf{x}), \left\{\mathbf{z}_{\text{region}}^{i,r}\right\}_{i=1}^{k_r} = \left\{\mathcal{P}(\mathbf{x}+\boldsymbol{\delta}_r^i)\right\}_{i=1}^{k_r}
\end{split}
\end{equation}
where $\{\boldsymbol{\delta}_r^i\}_{i=1}^{k_r}$ are random perturbations {for $\mathbf{x}$} within the $r$-th region whose size is $[-R_r,R_r]^{d+1}$, {and $k_r$ is the corresponding number of perturbations. We denote the number of scales (regions) considered as \#scale}. $\mathbf{z}_{\text{point}},\mathbf{z}_{\text{region}}^{i,r}\in\mathbb{R}^{d_{\text{model}}}$ are representations of the original point $\mathbf{x}$ and its multi-region augmentation $(\mathbf{x}+\boldsymbol{\delta}_r^i)$ respectively. $\mathcal{P}:\mathbb{R}^{d+1}\to\mathbb{R}^{d_{\text{model}}}$ is a lightweight MLP layer for position encoding. 

Notably, the design in considering different-scale regions not only covers multiscale properties of PDEs but also can simulate non-uniform meshes in FEMs, where each point can selectively aggregate information from multiple regions.

In the forward perspective, the above design can naturally augment the receptive field. Moreover, from the backward perspective shown in Figure \ref{fig:propinn}(b), the differential design can also aggregate gradients of collocation points within multiscale regions, thereby enhancing the gradient correlation on projection parameter $\theta_{\mathcal{P}}$ among adjacent points.


\vspace{-5pt}
\paragraph{Multi-region mixing} After shared projection, we can obtain $(1+\sum_{r=1}^{\text{\#scale}} k_r)$ representations from different positions. Previous studies, such as PINNsFormer and SetPINN, apply attention mechanism \cite{NIPS2017_3f5ee243} among collocation points to capture complex spatiotemporal dependencies. However, since attention involves inner products among representations, it will also bring huge computation costs in both forward and backpropagation processes, especially for PINNs that usually need to calculate high-order gradients.

Instead of directly modeling dependencies among collocation points, we propose an efficient multi-region mixing mechanism. It will first average pool the representations in various regions to generate multiple region representations. Owing to the special property of PDEs, the pooled multiscale representations are still under the same PDE but with different coefficients \cite{graham2007domain}. Thus, compared to complex dependencies among different positions, the relation among different coefficient PDEs is much more steady, allowing us to adopt a simple linear mixing layer rather than the attention mechanism, which is formalized as
\begin{equation}\label{equ:multiregion}
\begin{split}
\mathbf{z}_{\text{region}}^r&=\operatorname{Pooling}\Big(\left\{\mathbf{z}_{\text{region}}^{i,r}\right\}_{i=1}^{k_r}\Big),r=1,\cdots,\text{\#scale}\\
\mathbf{z} &= \mathcal{M}\left(\mathbf{z}_{\text{point}},\mathbf{z}_{\text{region}}^1,\cdots,\mathbf{z}_{\text{region}}^{\text{\#scale}}\right),
\end{split}
\end{equation}
where $\mathcal{M}:\mathbb{R}^{(1+{\text{\#scale}})\times d_{\text{model}}}\to\mathbb{R}^{d_{\text{model}}}$ is an MLP layer for {mixing} multi-region representations. Afterward, the {mixed} representation $\mathbf{z}$ is projected to the target dimension by {another} MLP layer $\mathcal{H}:\mathbb{R}^{d_{\text{model}}}\to\mathbb{R}^{m}$ {which} eventually generates the {PDE} solution, namely $u_\theta(\mathbf{x})=\mathcal{H}(\mathbf{z})\in\mathbb{R}^{m}$.

\vspace{-5pt}
\paragraph{Gradient analysis} As presented in Figure \ref{fig:compare} (c), by uniting region gradients, ProPINN can successfully boost the gradient correlation for better propagation, which can also be theoretically understood through the following theorem.

\begin{assumption}[\textbf{Correlation among region gradients}]\label{assumption:pos_correlation} Given PINN $u_\theta$, we assume that there {exists} a region size $R>0$, s.t.~$\forall \mathbf{x},\mathbf{x}^\prime\in\Omega$ with $\|\mathbf{x}-\mathbf{x}^\prime\|\leq R$, their gradient correlation is non-negative, namely $G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime)\ge 0$.
\end{assumption}

\begin{theorem}[\textbf{Gradient correlation improvement}]\label{theorem:enhance_grad} Under Assumption~\ref{assumption:pos_correlation} with region $R$, given $k$ perturbations $\{\boldsymbol{\delta}_i\}_{i=1}^k$ with $\|\boldsymbol{\delta}_i\|\leq \frac{R}{3}$ and define $u^{\text{region}}_\theta(\mathbf{x})=u_{\theta}(\mathbf{x})+\frac{\sum_{i=1}^k u_\theta(\mathbf{x}+\boldsymbol{\delta}_i)}{k}$, then $\forall \mathbf{x},\mathbf{x}^\prime\in \Omega$, if $\|\mathbf{x}-\mathbf{x}^\prime\|\leq\frac{R}{3}$, we have
\begin{equation}\label{equ:enhanced}
\begin{split}
G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime)\leq G_{u^{\text{region}}_\theta}(\mathbf{x},\mathbf{x}^\prime).
\end{split}
\end{equation}
\end{theorem}
\begin{remark}[\textbf{Efficient design in ProPINN}] Theorem~\ref{theorem:enhance_grad} demonstrates that aggregating region points by differential perturbation defined in Eq.~\eqref{equ:region_aug} can enhance the gradient correlation of nearby points. Considering the efficiency, we limit the region aggregation only within the lightweight projection layer $\mathcal{P}$ in ProPINN instead of the whole model.
\end{remark}

\vspace{-5pt}
\paragraph{Efficiency analysis} Compared to single-point-processing architectures, the extra computation of ProPINN comes from augmentation in Eq.~\eqref{equ:region_aug}. Supposed that flops of $\mathcal{P}(\cdot)$ is $\operatorname{ops}(\mathcal{P})$, the extra cost of ProPINN is $\sum_{r=1}^{\text{\#scale}} k_r\operatorname{ops}(\mathcal{P})$. Although this seems like a large overload, it will not affect the efficiency of ProPINN significantly in practice. This efficiency benefits from the parallel computation in both forward and backward computation graphs, as well as the lightweight design of the projection layer $(\mathcal{P})$. In our experiments, ProPINN is 2-3$\times$ faster than other explicit dependency modeling methods (e.g.~PINNsFormer and SetPINN) and comparable with other single-point-processing PINNs (e.g.~QRes and FLS) but with 60\%+ {relative error reduction}.

\section{Experiments}
We widely test ProPINN in extensive PDE-solving tasks and provide detailed comparisons with advanced PINN architectures in performance, efficiency and scalability. All the implementation details can be found in Appendix \ref{appdix:imple}.

\begin{table}[h]
\vspace{-10pt}
	\caption{Summary of benchmarks, covering both standard PDE-solving benchmarks and complex fluid dynamics. \#Dim denotes the dimension of input domain, ``+T'' refers to ``time-dependent''.}
	\label{tab:dataset}
	\vskip 0.1in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{4pt}
			\scalebox{1}{
			\begin{tabular}{lccc}
				\toprule
			   Type & \#Dim & Benchmarks & Property \\
                \midrule
                & \multirow{4}{*}{1D+T}  & Convection & \multirow{3}{*}{Failure Modes} \\
        	 Standard & & 1D-Reaction &  \\
             Takes & & Allen-Cahn & \citeyearpar{krishnapriyan2021characterizing} \\
             \cmidrule{3-4}
                 & & 1D-Wave & High-order \\
                  \midrule
		    Complex & \multirow{2}{*}{2D+T}  & Karman Vortex & \multirow{2}{*}{Navier-Stokes} \\	     
                   Fluid &  & Fluid Dynamics &  \\
				\bottomrule
			\end{tabular}}
		\end{sc}
	\end{small}
	\vspace{-10pt}
\end{table}

\begin{table*}[t]
	\caption{Performance comparison of different PINN architectures on standard PDE-solving tasks, which usually appear failure modes~\cite{krishnapriyan2021characterizing}. Both rMAE and rRMSE are recorded. Smaller values indicate better performance. For clarity, the best result is in bold and the second best is underlined. Promotion refers to the relative error reduction w.r.t.~the second best model ($1-\frac{\text{Our error}}{\text{The second best error}}$). 
    }
	\label{tab:mainres_standard}
	\vspace{-5pt}
	\vskip 0.15in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{6pt}
			\begin{tabular}{l|cccccccc}
				\toprule
                    \multirow{3}{*}{Model$^\ast$} & \multicolumn{2}{c}{Convection} & \multicolumn{2}{c}{1D-Reaction} & \multicolumn{2}{c}{Allen-Cahn} & \multicolumn{2}{c}{1D-Wave}  \\
                    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
				& $\mathrm{rMAE}$ & $\mathrm{rRMSE}$ & $\mathrm{rMAE}$ & $\mathrm{rRMSE}$ & $\mathrm{rMAE}$ & $\mathrm{rRMSE}$ & $\mathrm{rMAE}$ & $\mathrm{rRMSE}$ \\
				\midrule
                    Vanilla PINN \cite{raissi2019physics} & \textcolor{gray}{0.778} & \textcolor{gray}{0.840} & \textcolor{gray}{0.982} & \textcolor{gray}{0.981} & 0.350 & 0.562 & 0.326 & 0.335 \\
                    QRes \cite{bu2021quadratic} & \textcolor{gray}{0.746} & \textcolor{gray}{0.816} & \textcolor{gray}{0.979} & \textcolor{gray}{0.977} & \textcolor{gray}{0.942} & \textcolor{gray}{0.946} & \textcolor{gray}{0.523} & \textcolor{gray}{0.515}  \\
                    FLS \cite{wong2022learning} & \textcolor{gray}{0.674} & \textcolor{gray}{0.771} & \textcolor{gray}{0.984} & \textcolor{gray}{0.985} & 0.357 & 0.574 & 0.102 & 0.119  \\
                    KAN \cite{liu2024kan} & \textcolor{gray}{0.922} & \textcolor{gray}{0.954} & 0.031 & 0.061 & 0.352 & 0.563 & 0.499 & 0.489  \\
                    PirateNet \cite{wang2024piratenets} & \textcolor{gray}{1.169} & \textcolor{gray}{1.287} & 0.017 & 0.044 & \underline{0.098} & \underline{0.179} & \underline{0.051} & \underline{0.055}  \\
                    PINNsFormer \cite{zhao2023pinnsformer} & \underline{0.023} & \underline{0.027} & \underline{0.015} & \underline{0.030} & 0.331 & 0.529 & 0.270 & 0.283  \\
                    SetPINN \cite{nagda2024setpinns} & 0.028 & 0.031 & 0.018 & 0.046 &  0.381 & 0.601 & 0.347 & 0.332  \\
                    \midrule
                    \textbf{ProPINN (Ours)} & \textbf{0.018} & \textbf{0.020} & \textbf{0.010} & \textbf{0.020} & \textbf{0.036} & \textbf{0.087} & \textbf{0.016} & \textbf{0.016}  \\
                    Promotion & 22\% & 26\% & 33\% & 33\% & 63\% & 51\% & 69\% & 71\% \\
				\bottomrule
			\end{tabular}
		\end{sc}
    \begin{tablenotes}
      \footnotesize
      \item[] $\ast$ SetPINN \cite{nagda2024setpinns} only provide the rRMSE performance in their official paper. Thus, its rMAE is based on our reproduction. To highlight the model capability in mitigating PINN failure modes, we colored failed cases ($\mathrm{rMAE}>0.5$) in \textcolor{gray}{gray}.
      \end{tablenotes}
	\end{small}
    \vspace{-5pt}
\end{table*}

\paragraph{Benchmarks} As listed in Table \ref{tab:dataset}, we experiment with six PDE-solving tasks, covering diverse 1D and 2D time-dependent PDEs. Specifically, solutions of Convection, 1D-Reaction and Allen-Cahn contain some steep areas, which are challenging to approximate and have been used to demonstrate PINN failure modes \cite{krishnapriyan2021characterizing} and propagation failures \cite{r3_wang_2023}. Besides, 1D-Wave involves second-order derivatives, making it hard to optimize. In addition to the above standard benchmarks, we also test ProPINN with extremely challenging fluid dynamics, which are governed by intricate Navier-Stokes equations \cite{constantin1988navier}. \emph{Karman Vortex} is proposed by \citet{raissi2019physics}, which describes the fluid dynamics around a cylinder, exhibiting the famous Karman vortex street \cite{wille1960karman}. \emph{Fluid Dynamics} is from \citet{wang2023expert} and involves fast dynamics of fluid on a torus. %More details can be found in Appendix \ref{appdix:datset}.

\vspace{-5pt}
\paragraph{Baselines} In addition to vanilla PINN \citeyearpar{raissi2019physics}, we also compare ProPINN with other six PINN architectures. QRes \citeyearpar{bu2021quadratic}, FLS \citeyearpar{wong2022learning}, KAN \citeyearpar{liu2024kan} and PirateNet \citeyearpar{wang2024piratenets} are under the conventional PINN architecture, where different collocation points are independently optimized. PINNsFormer \citeyearpar{zhao2023pinnsformer} and SetPINN \citeyearpar{nagda2024setpinns} are based on the Transformer backbone to capture spatiotemporal correlations among PDEs. PirateNet and PINNsFormer are previous state-of-the-art models. In addition, we also integrate ProPINN with sampling strategy R3 \cite{r3_wang_2023} and loss reweighting method NTK \cite{Wang2020WhenAW} to verify that these methods contribute orthogonally to us.

\vspace{-5pt}
\paragraph{Implementations} For all benchmarks, we set the number of regions ${\text{\#scale}}=3$ with region size $\{R_1,R_2,R_3\}=\{10^{-2},5\times10^{-2},9\times 10^{-2}\}$, number of perturbations $\{k_1,k_2,k_3\}=\{3^{(d+1)},5^{(d+1)},7^{(d+1)}\}$ and representation dimension $d_{\text{model}}=32$. For Convection, 1D-Reaction, Allen-Cahn, 1D-Wave and Karman Vortex, we follow \cite{zhao2023pinnsformer} and train the model with L-BFGS optimizer \cite{liu1989limited} for 1,000 iterations in Pytorch~\cite{Paszke2019PyTorchAI}. As for Fluid Dynamics, we follow \cite{wang2023expert} and experiment with JAX \cite{jax2018github}. Relative L1 error (rMAE) and relative Root Mean Square Error (rRMSE) are used as metrics.

\begin{figure}[t]
\begin{center}
% \vspace{-10pt}
\centerline{\includegraphics[width=\columnwidth]{fig/efficiency.pdf}}
    \vspace{-7pt}
	\caption{Efficiency comparisons on Convection. Models under the single-point-processing paradigm are colored in \textcolor{darkblue}{blue}, while other models that consider point correlations are in \textcolor{darkred}{red}.}
	\label{fig:efficiency}
\end{center}
\vspace{-28pt}
\end{figure}

\subsection{Standard Benchmarks} 

\paragraph{Main results} From comparison results presented in Table~\ref{tab:mainres_standard}, we can obtain the following key observations.

ProPINN successfully mitigates PINN failure modes in Convection, 1D-Reaction and Allen-Chan. Its outstanding performance in 1D-Wave also verifies its capability in handling high-order PDEs. Notably, ProPINN also beats the latest Transformer-based models PINNsFormer \citeyearpar{zhao2023pinnsformer} and SetPINN \citeyearpar{nagda2024setpinns} with 46\% rMAE reduction averaged from four PDEs, highlighting the advantage of our method.

It is also observed that only model architectures that consider interactions among multiple points (PINNsFormer, SetPINN and ProPINN) consistently work for all four tasks. All the models under the single-point-processing paradigm fail in Convection. This justifies our discussion about the deficiency of single-point PINN architectures, where independent optimization is easy to cause propagation failure.

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case.pdf}}
\vspace{-5pt}
	\caption{Visualization of model approximated solutions. Error map ($u_\theta-u$) is also plotted. See Appendix \ref{appdix:more_vis} for more visualizations.}
	\label{fig:case}
\end{center}
\vspace{-20pt}
\end{figure*}

\vspace{-5pt}
\paragraph{Efficiency comparison} To verify the practicability of our method, we also provide the efficiency comparison in Figure~\ref{fig:efficiency}. It is observed that ProPINN is about 2-3$\times$ faster than recent Transformer-based models: PINNsFormer and SetPINN. Also, benefiting from our lightweight design for the projection layer and parallel computing, ProPINN is comparable to single-point-processing PINNs in efficiency but brings more than 60\% error reduction, achieving a favorable trade-off between performance and efficiency.

\vspace{-5pt}
\paragraph{Visualization} To clearly compare model capacity in solving PDEs, we also visualize approximated solutions in Figure \ref{fig:case}. We can find that PirateNet under the single-point-processing paradigm fails in handling the steep variations in Convection. As for the 1D-Wave with high-order derivatives, we find that PINNsFormer yields an insufficient performance, which may be because of the optimization difficulty of the attention mechanism under high-order loss. In contrast, ProPINN achieves consistent performance in tackling above-mentioned challenges, highlighting its effectiveness.

\begin{figure}[t]
\begin{center}
% \vspace{-10pt}
\centerline{\includegraphics[width=\columnwidth]{fig/complex_physics.pdf}}
    \vspace{-10pt}
	\caption{Visualization of the target solution in (a) Karman Vortex and (b) Fluid Dynamics tasks, which involves complex spatiotemporal dynamics and multi-physics (velocity and pressure) interactions in 2D+Time space governed by Navier-stokes equations.}
	\label{fig:fluid_vis}
\end{center}
\vspace{-25pt}
\end{figure}

\subsection{Complex Physics}

In addition to the standard benchmarks discussed above, we also attempt to evaluate the model's capability for handling complex physics. As a long-standing mathematical problem, Navier-Stokes equations \cite{temam2001navier} for fluid dynamics have shown significant challenges and profound importance in real-world applications \cite{doering1995applied}. Thus, in this paper, we also experiment with Karman Vortex and Fluid Dynamics tasks, which involve extremely intricate PDEs and spatiotemporal dynamics as shown in Figure \ref{fig:fluid_vis}.

\begin{table}[t]
\vspace{-10pt}
	\caption{Comparison of different PINN architectures in solving 2D time-dependent Navier-Stokes equations, including Karman Vortex and Fluid Dynamics presented in Figure \ref{fig:fluid_vis}. ``Nan'' indicates that this model comes across the training instability problem.}
	\label{tab:physics_res}
	\vskip 0.1in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{3pt}
			\scalebox{1}{
			\begin{tabular}{l|cccc}
				\toprule
                \multirow{3}{*}{Model$^\ast$} & \multicolumn{2}{c}{\scalebox{0.9}{Karman Vortex}}  & \multicolumn{2}{c}{\scalebox{0.9}{Fluid Dynamics}} \\
                    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
			      & \scalebox{0.9}{$\mathrm{rMAE}$} & \scalebox{0.9}{$\mathrm{rRMSE}$} & \scalebox{0.9}{$\mathrm{rMAE}$} & \scalebox{0.9}{$\mathrm{rRMSE}$} \\
			    \midrule
                    Vanilla PINN \citeyearpar{raissi2019physics} & \textcolor{gray}{13.08} & \textcolor{gray}{9.08} & 0.3659 & 0.4082 \\
                    QRes \citeyearpar{bu2021quadratic} & \textcolor{gray}{6.41} & \textcolor{gray}{4.45} & 0.2668 & 0.3144 \\
                    FLS \citeyearpar{wong2022learning} & \textcolor{gray}{3.98} & \textcolor{gray}{2.77} & \underline{0.2362} & \underline{0.2765}\\
                    KAN \citeyearpar{liu2024kan} & \textcolor{gray}{1.43} & \textcolor{gray}{1.25}  & / & / \\
                    PirateNet \citeyearpar{wang2024piratenets} & \textcolor{gray}{1.24} & \textcolor{gray}{1.16}  & 0.4550 & 0.5232 \\
                    PINNsformer \citeyearpar{zhao2023pinnsformer} & 0.384 & 0.280  & Nan & Nan \\
                    SetPINN \citeyearpar{nagda2024setpinns} & \underline{0.287} & \underline{0.218} & Nan & Nan \\
                    \midrule
                    \textbf{ProPINN (Ours)} & \textbf{0.161} & \textbf{0.124} & \textbf{0.1834} & \textbf{0.2172}\\
                    Promotion & 44\% & 43\% & 22\% & 21\%\\
				\bottomrule
			\end{tabular}}
		\end{sc}
            \begin{tablenotes}
      \footnotesize
      \item[] $\ast$ Since Fluid Dynamics is based on JAX \cite{jax2018github} and it is hard to transfer the Pytorch implementation of KAN to JAX, we did not test it in this task, labeled in ``/''.
      \end{tablenotes}
	\end{small}
	\vspace{-10pt}
\end{table}

\vspace{-5pt}
\paragraph{Main results} Compared to PDEs shown in Figure \ref{fig:case}, fluid simulation tasks in this section are much more complex. As presented in Table \ref{tab:physics_res}, ProPINN still achieves the best performance with over 30\% relative promotion on average than the previous best model. Notably, ProPINN performs fairly well in the Fluid Dynamics task, which requires the model to accurately calculate velocity and pressure solutions for the whole spatiotemporal sequence purely based on equation supervision. As shown in Figure \ref{fig:fluid_vis} (b), this task is under rapid and ever-changing dynamics and the last frame is far from its initial state, making this task extremely challenging. Besides, we also observe that PINNsFormer and SetPINN suffer from the training instability problem, which confirms our previous discussion on the training difficulties of the attention mechanism, further demonstrating the effectiveness of our MLP-based design in multi-region mixing.

\begin{figure*}[t]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case_fluid.pdf}}
\vspace{-5pt}
	\caption{Visualization of the Fluid Dynamics task. Error map ($u_\theta-u$) is also plotted. See Appendix \ref{appdix:more_vis} for more visualizations.}
	\label{fig:case_fluid}
\end{center}
\vspace{-25pt}
\end{figure*}

\vspace{-5pt}
\paragraph{Visualization} As presented in Figure \ref{fig:case_fluid}, ProPINN can accurately simulate the future variations of fluid, including complex vortexes and distortions. This indicates that ProPINN can give a precise solution for Navier-Stokes equations, especially in processing the convection term $({u}\cdot\nabla){u}$, which is nonlinear and extremely complex.



\subsection{Model Analysis}

\begin{figure}[t]
\begin{center}
% \vspace{-10pt}
\centerline{\includegraphics[width=\columnwidth]{fig/ablation.pdf}}
    \vspace{-7pt}
	\caption{Analyses on ProPINN design: rMAE change under (a) ablations on multi-region mixing in Eq.~\eqref{equ:multiregion} and differential perturbation in Eq.~\eqref{equ:region_aug}, (b) integration with other techniques.}
	\label{fig:ablation}
\end{center}
\vspace{-28pt}
\end{figure}

\paragraph{Ablations} To verify the effectiveness of our design in ProPINN, we provide a detailed ablation in Figure \ref{fig:ablation}(a). Specifically, we experiment with only one region in Eq.~\eqref{equ:multiregion}, i.e.~$\#\text{scale}=1$ and detaching gradients of augmented points in Eq.~\eqref{equ:region_aug}. Results demonstrate that both multi-region mixing and differential perturbation are essential for the final performance. Especially, the non-differential perturbation will seriously damage the model performance, even though it utilizes more collocation points to augment the receptive field. This finding further confirms that compared with augmenting representation, uniting gradients of region points is more important for PINN optimization, verifying that region gradient correlations are the key factor.

\vspace{-5pt}
\paragraph{Integrating with other strategies} As we discussed in related work, ProPINN mainly focuses on architecture design, which is orthogonal to previous research on training strategies. To verify their orthogonal contributions, we integrate ProPINN with the loss reweighting method NTK \cite{Wang2020WhenAW} and sampling strategy R3~\cite{r3_wang_2023}. As illustrated in Figure \ref{fig:ablation}(b), these methods can further boost the performance of ProPINN. As for 1D-Reaction which is relatively simple, ProPINN's performance is nearly {optimal}, thus the integration does not bring further promotion.

\begin{figure}[t]
\begin{center}
% \vspace{-10pt}
\centerline{\includegraphics[width=\columnwidth]{fig/scale_new.pdf}}
    \vspace{-10pt}
	\caption{Performance changes of (a) ProPINN and (b) vanilla PINN when increasing the number of layers.}
	\label{fig:scale}
\end{center}
\vspace{-25pt}
\end{figure}

\vspace{-5pt}
\paragraph{Model scalability} Theorem \ref{theorem:grad_corr} attributes the propagation failure to lower region gradient correlations. As we analyzed before, since gradient correlation is defined as the inner product of high-dimensional gradient tensors in size $m\times|\theta|$, the correlation is easier to be orthogonal when adding model parameters. Thus, vanilla PINN presents the performance drop in larger models (Figure \ref{fig:scale}(b)). In contrast, ProPINN successfully mitigates propagation issues by introducing region gradients, which not only tackle PINN failure modes but also empower the model with favorable scalability, {benefiting from scaling up the model parameters}.

\section{Conclusion}

This paper focuses on the propagation failures of PINNs and provides the first formal and in-depth study of this crucial phenomenon. Going beyond the intuitive understanding, we theoretically proved that the root cause of propagation failures is the lower gradient correlation among nearby points, which can serve as a precise criterion for PINN failures. Inspired by the above theoretical analyses, ProPINN is presented as a new PINN architecture, which can effectively unite the gradient of region points for better propagation. Experimentally, ProPINN can naturally enhance region gradient correlation and achieve remarkable promotion on standard benchmarks and challenging PDE-solving tasks with a favorable trade-off between performance and efficiency. 


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Proof of Theorems in the Main Text}\label{appdix:proof}

This section will present proofs for theorems in Section \ref{sec:method}.

\subsection{Propagation in FEMs (Theorem \ref{theorem:fem})}
This theorem is based on the general formalization of FEMs, which can be directly derived from textbook \cite{dhatt2012finite}. Here, we reorganize the formalization to highlight the interaction among solution values of different areas in FEMs.
\begin{proof}
Without loss of generality, we consider the following PDE, which is defined in $\Omega\cup \partial \Omega$ with the following constraints:
\begin{equation}\label{equ:pde_proof}
\begin{split}
    & \mathcal{F}(u)(\mathbf{x})=f(\mathbf{x}),\mathbf{x}\in\Omega;\\
    & u(\mathbf{x})=0, \mathbf{x}\in\partial\Omega,
\end{split}
\end{equation}
where $f$ represents the function of external force. Suppose that the solution $u\in\mathbb{U}$, where $\forall v\in\mathbb{U},v|_{\partial\Omega}=0$ and is with corresponding differential property to make the equation constraint $\mathcal{F}$ meaningful, then we can obtain the following variational formalization of PDE in Eq.~\eqref{equ:pde_proof}:
\begin{equation}
    \int_{\Omega}(\mathcal{F}(u)-f)v \mathrm{d}x=0,\ \forall v\in\mathbb{U},
\end{equation}
where $x$ refers to the variable of one dimension in $\Omega$. 

Based on the integration by parts technique, it is easy to derive that
\begin{equation}
    \int_{\Omega}(\mathcal{F}(u)-f)v \mathrm{d}x=\left. \mathcal{F}^\prime(u) v\right|_{\partial\Omega}-\int_{\Omega}\mathcal{F}^\prime(u)\frac{\partial v}{\partial x}\mathrm{d}x-\int_\Omega fv\mathrm{d}x=-\int_{\Omega}\mathcal{F}^\prime(u)\frac{\partial v}{\partial x}\mathrm{d}x-\int_\Omega fv\mathrm{d}x,
\end{equation}
where $\frac{\partial\mathcal{F}^\prime(u)}{\partial x}=\mathcal{F}(u)$. For clarity, we define $D(u,v)=\int_{\Omega}\mathcal{F}^\prime(u)\frac{\partial v}{\partial x}\mathrm{d}x$ and $B(v)=-\int_\Omega fv\mathrm{d}x$. Thus, based on the above variational derivation, the PDE solving process is to find $u\in\mathbb{U}$ to satisfy the following equation:
\begin{equation}
    D(u,v)-B(v)=0,\forall v\in \mathbb{U}.
\end{equation}

The key idea of FEM is to find an approximated solution on the computation mesh. Specifically, it is to find $\widehat{u}\in\widehat{\mathbb{U}}$ to satisfy the above-derived constraint, where $\widehat{\mathbb{U}}$ is the subspace of $\mathbb{U}$ and essentially a linear space formed by $n$ basis functions $\{\Psi_1,\cdots,\Psi_{n}\}$. Thus, the above variational formalization of PDE can be transformed into an approximated problem, namely, find $\widehat{u}\in\widehat{\mathbb{U}}$, s.t. $D(\widehat{u},\widehat{v})-B(\widehat{v})=0, \forall \widehat{v}\in\widehat{\mathbb{U}}$.

Usually, $\{\Psi_1,\cdots,\Psi_{n}\}$ are defined as the linear interpolation functions of a region, namely:
\begin{equation}\label{equ:basis_def}
    \Psi_i(\mathbf{x}) = \begin{cases}
1 & \text{if } \mathbf{x}=\mathbf{x}_i, \\
\operatorname{Linear}\text{-}\operatorname{Interpolation}(\mathbf{x}) & \text{if } \mathbf{x}\in\text{Region}(\mathbf{x}_i)\\
0 & \text{if } \mathbf{x}\in\Omega\backslash\text{Region}(\mathbf{x}_i)
\end{cases},\quad i=1,\cdots,n.
\end{equation}
where $\text{Region}(\mathbf{x}_i)$ denotes the triangular mesh adjacent to $\mathbf{x}_i$ and $\Psi_i$ is zero on the boundary of $\text{Region}(\mathbf{x}_i)$.

Since $\widehat{\mathbb{U}}$ is a linear space formed by $n$ basis functions $\{\Psi_1,\cdots,\Psi_{n}\}$, $\widehat{u}=\sum_{i=1}^n u_i\Psi_i$ and $\widehat{v}=\sum_{i=1}^n v_i\Psi_i$. Thus, the PDE is approximated by solving the following equation set:
\begin{equation}
    \sum_{j=1}^n D(\Psi_j,\Psi_i)u_j-B(\Psi_i)=0,\quad i=1,\cdots,n.
\end{equation}
It is worth noticing that according to the definition in Eq.~\eqref{equ:basis_def}, basis function $\Psi_i$ is zero in all the other nodes $\mathbf{x}_j, i\neq j$. Thus, the $i$-th coefficient $u_i$ is also the approximated solution value on node $\mathbf{x}_i$.

According to the updating strategy of Jacobi iterative method \cite{goldstine1959jacobi}, we can directly obtain:
\begin{equation}
        u_j^{(k+1)}=\frac{1}{D(\Psi_j, \Psi_j)\
        }\bigg(b_j-\sum_{i\neq j}D(\Psi_i, \Psi_j)u_i^{(k)}\bigg),
\end{equation}
where $b_j=B(\Psi_j)$ is a constant related to the external force $f$ and $u_j^{(k+1)}$ represent the value of coefficient at the $j$-th step, which is also equal to the solution value at the $j$-th node.
\end{proof}


\subsection{Gradient Correlation (Theorem \ref{theorem:grad_corr})}

According to Definition~\ref{theorem:opt_block}, if $\mathbf{x}$ and $\mathbf{x}^{\prime}$ are adjacent and $D_{\text{PINN}}(\mathbf{x}, \mathbf{x}^{\prime})$, defined in Eq.~\eqref{equ:core}, is less than a empirically defined threshold $\epsilon$, we consider that propagation failure has occurred between $\mathbf{x}$ and $\mathbf{x}^\prime$. In this theorem, we want to prove that the propagation failure is equivalent to a small gradient correlation $G_{u_\theta}(\mathbf{x}, \mathbf{x}^\prime)$ between two adjacent points $\mathbf{x}$ and $\mathbf{x}^{\prime}$.

\begin{proof}
We notice that the PINN $u_{\theta}$ can be regarded as a multivariate function with respect to the parameters $\theta$ and the input variables~$\mathbf{x}$, i.e.,~ $u(\theta,x) = u_{\theta}(x)$. Since $u(\theta,x)$ is infinitely differentiable with respect to both $\theta$ and $\mathbf{x}$, and let $\lambda$ be a sufficiently small step size, we proceed to rewrite the Eq.~\eqref{equ:core} by employing a Taylor expansion centered at $(\theta, \mathbf{x}^{\prime})$,
\begin{equation}
\begin{split}
D_{\text{PINN}}(\mathbf{x}, \mathbf{x}^{\prime}) &= \bigg\|u_\theta(\mathbf{x}^\prime)-u_{\theta-\left.\lambda\frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}}(\mathbf{x}^\prime)\bigg\| \\
& = \bigg\|u(\theta, \mathbf{x}^\prime)-u({\theta-\lambda\frac{\partial u_\theta}{\partial \theta}(\mathbf{x})}, \mathbf{x}^\prime)\bigg\| \\
& = \bigg\| \bigg<\frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime), \lambda\frac{\partial u_\theta}{\partial \theta}(\mathbf{x})\bigg> + \mathcal{O}(\lambda^2) \bigg\| \\
& = \lambda G_{u_\theta}(\mathbf{x}, \mathbf{x}^\prime) + \mathcal{O}(\lambda^2) \\
& \approx \lambda G_{u_\theta}(\mathbf{x}, \mathbf{x}^\prime).
\end{split}
\end{equation}
Therefore, since the functions 
$D_{\text{PINN}}(\mathbf{x}, \mathbf{x}^{\prime}) $ and $G_{u_\theta}(\mathbf{x}, \mathbf{x}^\prime)$ differ only by a factor of $\lambda$, the sufficient smallness of $D$ guarantees the sufficient smallness of $G$, and the reverse is also true.
\end{proof}

\subsection{{Gradient Correlation Improvement} (Theorem \ref{theorem:enhance_grad})}
This theorem demonstrates that uniting region gradients can boost the gradient correlation among nearby points. This theorem can be proved under the Assumption \ref{assumption:pos_correlation}, which assumes the positive gradient correlation of nearby points.

\begin{proof}
In order to demonstrate the effectiveness of uniting region gradients, it is equivalent to prove that,
\begin{equation}
\begin{split}\label{proof:enhance_grad}
&\hspace{15pt} G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime) \leq G_{u^{\text{region}}_\theta}(\mathbf{x},\mathbf{x}^\prime) \\
&\Leftrightarrow G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime)  \leq G_{u_{\theta}(\mathbf{x})+\frac{1}{k}\sum_{i=1}^k u_\theta(\mathbf{x}+\boldsymbol{\delta}_i)}(\mathbf{x},\mathbf{x}^\prime) \\
&\Leftrightarrow \bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}^\prime}\bigg>\bigg\| \leq \bigg\|\bigg< (\frac{\partial u_\theta}{\partial \theta}(\mathbf{x}) + \frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}+\boldsymbol{\delta}_i)), (\frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime) + \frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime+\boldsymbol{\delta}_i))\bigg>\bigg\| \\
&\Leftrightarrow \bigg<\frac{\partial u_\theta}{\partial \theta} ({\mathbf{x}}), \frac{\partial u_\theta}{\partial \theta}({\mathbf{x}^\prime})\bigg> \leq \bigg< (\frac{\partial u_\theta}{\partial \theta}(\mathbf{x}) + \frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}+\boldsymbol{\delta}_i)), (\frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime) + \frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime+\boldsymbol{\delta}_i))\bigg> \\
& \Leftrightarrow 0 \leq  \bigg< \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}), \frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime+\boldsymbol{\delta}_i))\bigg>+ \bigg<\frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}+\boldsymbol{\delta}_i)) , \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime)\bigg> + \bigg<\frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}+\boldsymbol{\delta}_i)) ,\frac{1}{k}\sum_{i=1}^k \frac{\partial u_\theta}{\partial \theta}(\mathbf{x}^\prime+\boldsymbol{\delta}_i)) \bigg> \\
& \Leftrightarrow 0 \leq \frac{1}{k}\sum_{j=1}^k G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime+\delta_j)+\frac{1}{k}\sum_{i=1}^k G_{u_\theta}(\mathbf{x}+\delta_i,\mathbf{x}^\prime) + \frac{1}{k^2}\sum_{i=1}^k\sum_{j=1}^k G_{u_\theta}(\mathbf{x}+\delta_i,\mathbf{x}^\prime+\delta_j).
\end{split}
\end{equation}

From the pre-defined perturbations $\{\boldsymbol{\delta}_i\}_{i=1}^k$ with $\|\boldsymbol{\delta}_i\|\leq \frac{R}{3}$ and the given $\mathbf{x}$ and $\mathbf{x^\prime}$ such that $\|\mathbf{x}-\mathbf{x}^\prime\|\leq\frac{R}{3}$, it follows that,
\begin{equation}
    \begin{split}
        \|(\mathbf{x}+\delta_i)-\mathbf{x}^\prime\|\leq{R},\; \|\mathbf{x}-(\mathbf{x}^\prime + \delta_j)\|\leq{R},\; \|(\mathbf{x} + \delta_i)-(\mathbf{x}^\prime + \delta_j)\|\leq{R},\; \forall i, j \in \{1,\dots,k\}.
    \end{split}
\end{equation}
Given the choice of $R$ in Assumption \ref{assumption:pos_correlation}, we obtain that,
\begin{equation}
    \begin{split}
        G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime+\delta_j)\geq0,\; G_{u_\theta}(\mathbf{x}+\delta_i,\mathbf{x}^\prime)\geq0,\; G_{u_\theta}(\mathbf{x}+\delta_i,\mathbf{x}^\prime+\delta_j)\geq0,\; \forall i, j \in \{1,\dots,k\}.
    \end{split}
\end{equation}


Consequently, the conclusion in Eq.~\eqref{proof:enhance_grad} and Theorem~\ref{theorem:enhance_grad} has been proven.
\end{proof}

\subsection{{Justification} of Assumption \ref{assumption:pos_correlation}}



\paragraph{Theoretical understanding} Firstly, we want to highlight that if we change the positive constraint of region size $R$ to non-negative, Assumption \ref{assumption:pos_correlation} is always true. This can be directly proved by the following derivation:
\begin{equation}
G_{u_\theta}(\mathbf{x},\mathbf{x})=\bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}\bigg>\bigg\|\ge 0.
\end{equation}

Further, we still consider the positive constraint of region size $R$. If $\|\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}\|\neq 0$ at $\mathbf{x}$, then $G_{u_\theta}(\mathbf{x},\mathbf{x})>0$. According to the boundedness of $\frac{\partial^2 u_\theta}{\partial\theta\partial\mathbf{x}}$, there must exist a region $R_\mathbf{x}=\frac{\bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}\bigg>\bigg\|}{2\bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}},\frac{\partial^2 u_\theta}{\partial\theta\partial\mathbf{x}}\bigg>\bigg\|}>0$ s.t. $\forall \mathbf{x}^\prime\in\Omega$, if $\|\mathbf{x}^\prime-\mathbf{x}\|\leq R_\mathbf{x}$, we have
\begin{equation}
\begin{split}
    G_{u_\theta}(\mathbf{x},\mathbf{x}^\prime)&=\bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}^\prime}\bigg>\bigg\|\\
    &= \bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}+(\mathbf{x^\prime}-\mathbf{x})\frac{\partial^2 u_\theta}{\partial\theta\partial\mathbf{x}}+\mathcal{O}((\mathbf{x^\prime}-\mathbf{x})^2)\bigg>\bigg\|\\
    &\ge \bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}\bigg>\bigg\|-R_{\mathbf{x}}\bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}},\frac{\partial^2 u_\theta}{\partial\theta\partial\mathbf{x}}\bigg>\bigg\|\\
    &= \frac{1}{2} \bigg\|\bigg<\left. \frac{\partial u_\theta}{\partial \theta} \right|_{\mathbf{x}}, \left. \frac{\partial u_\theta}{\partial \theta}\right|_{\mathbf{x}}\bigg>\bigg\|\\
    &\ge 0.
\end{split}
\end{equation}
Thus, the unguaranteed part in Assumption \ref{assumption:pos_correlation} is that $R=\min_{\mathbf{x}\in\Omega} R_{\mathbf{x}}>0$, namely \emph{is there a unified region size for all collocation points}. The following experimental statistics can verify this question well.

\paragraph{Experimental statistics} We also count the proportion of points that satisfy Assumption \ref{assumption:pos_correlation} in each PDE. Specifically, we take $10^4$ equally spaced points at each PDE. For each collocation point, we consider its gradient correlation with nearby points, whose distance is around $10^{-2}$. As presented in Table \ref{tab:assumption}, if we set $R$ as $10^{-2}$, we can find all the collocation points are under positive region gradient correlation, indicating that Assumption \ref{assumption:pos_correlation} can be well guaranteed in practice.

\begin{table}[h]
\vspace{-10pt}
	\caption{Statistics for Assumption \ref{assumption:pos_correlation}.}
	\label{tab:assumption}
	\vskip 0.1in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{6pt}
			\scalebox{1}{
			\begin{tabular}{l|cccc}
				\toprule
			   Statistics of $10^4$ Points & Convection & 1D-Reaction & Allen-Cahn & 1D-Wave \\
                \midrule
                Positive Ratio of region Gradient Correlations &  100\% & 100\% & 100\% & 100\% \\
				\bottomrule
			\end{tabular}}
		\end{sc}
	\end{small}
	\vspace{-10pt}
\end{table}

\section{Implementation Details}\label{appdix:imple}

In this section, we will provide details about benchmarks, experiment settings, model configurations and metrics.


\subsection{Benchmark Description}\label{appdix:datset}

As shown in Figure \ref{fig:benchmark_summary}, we evaluate ProPINN on six tasks, which include four standard benchmarks: Convection, 1D-Reaction, Allen-Cahn and 1D-Wave and two complex physics modeling tasks: Karman Vortex and Fluid Dynamics.

\begin{figure*}[b]
\begin{center}
\vspace{-10pt}
\centerline{\includegraphics[width=\textwidth]{fig/benchmarks.pdf}}
\vspace{-5pt}
	\caption{Summary of experimental benchmarks. We visualize the solution map of each PDE-solving task.}
	\label{fig:benchmark_summary}
\end{center}
\vspace{-20pt}
\end{figure*}

\paragraph{Convection} This problem describes a hyperbolic PDE, which can be formalized as follows:
\begin{equation}
    \begin{split}
       \text{Equation constraint:  } &\frac{\partial u}{\partial t}+\beta\frac{\partial u}{\partial t}=0,  \ x\in(0,2\pi), t\in(0,1),\\
       \text{Initial condition:  } &u(x,0)=\sin(x), \  x\in[0,2\pi],\\
       \text{Boundary condition:  } & u(0,t) = u(2\pi, t), \  t\in[0,1],
    \end{split}
\end{equation}
where the convection coefficient $\beta$ is set as 50. Its analytic solution is $u(x,t)=\sin(x-\beta t)$. As presented in Figure \ref{fig:benchmark_summary}(a), this problem involves rapid variations along the temporal dimension, making it hard for neural networks to approximate. Thus, this problem is widely tested in characterizing PINN failure modes \cite{krishnapriyan2021characterizing} and evaluating new architectures \cite{zhao2023pinnsformer,nagda2024setpinns,wu2024ropinn}. 

\paragraph{1D-Reaction} This problem is about a non-linear ODE for chemical reactions, whose equation is formalized as follows:
\begin{equation}
\begin{split}
    \text{Equation constraint:  } & \frac{\partial u}{\partial t}-\rho u(1-u) = 0, \ x\in(0,2\pi), t\in(0,1),\\
    \text{Initial condition:  } &u(x, 0) = \exp{\left(-\frac{(x-\pi)^2}{2(\pi / 4)^2}\right)}, \ x\in[0,2\pi], \\ 
    \text{Boundary condition:  } &u(0, t) = u(2\pi, t),\ t\in[0,1],
\end{split}
\end{equation}
where the PDE coefficient $\rho$ is set as 5. The analytic solution for this PDE is $u(x,t)=\frac{h(x)e^{\rho t}}{h(x)e^{\rho t}+1-h(x)}$, where $h(x)=\exp{\left(-\frac{(x-\pi)}{2(\pi/4)^2}\right)}$. As shown in Figure \ref{fig:benchmark_summary}(b), this task also presents rapid variations in some areas, making it hard to solve~\cite{krishnapriyan2021characterizing}. We also experiment on this problem following previous studies \cite{zhao2023pinnsformer}.

\paragraph{Allen-Cahn} This problem is a typical reaction-diffusion equation, which is defined as:
\begin{equation}
    \begin{split}
        \text{Equation constraint:  } &\frac{\partial u}{\partial t}-0.0001\frac{\partial^2 u}{\partial x^2}+5u^3-5u = 0, \ x\in(-1,1), t\in(0,1),\\
        \text{Initial condition:  } & u(x, 0)=x^2\cos(\pi x),  \ x\in[-1,1],\\
\text{Boundary condition:  } & u(-1,t)=u(1,t), \ t\in[0,1],\\
\text{Boundary condition:  } & \frac{\partial u(-1,t)}{\partial x}=\frac{\partial u(1,t)}{\partial x}, \ t\in[0,1].
    \end{split}
\end{equation}
Since this PDE does not have an analytic solution, following previous studies \cite{raissi2019physics}, we adopt the results pre-calculated by traditional spectral methods~\cite{kopriva2009implementing} as the reference. As illustrated in Figure \ref{fig:benchmark_summary}(c), this task also includes the sharp area, making it usually studied as PINN failure modes \cite{krishnapriyan2021characterizing}.

\paragraph{1D-Wave} This problem is a hyperbolic PDE, which involves high-order derivatives in its equation constraint:
\begin{equation}
    \begin{split}
        \text{Equation constraint:  } &\frac{\partial^2 u}{\partial t^2}-4\frac{\partial^2 u}{\partial x^2} = 0, \ x\in(0,1), t\in(0,1),\\
        \text{Initial condition:  } & u(x, 0)=\sin(\pi x)+\frac{1}{2}\sin(\beta\pi x),  \ x\in[0,1],\\
\text{Initial condition:  } & \frac{\partial u(x,0)}{\partial t}=0, \ x\in[0,1],\\
\text{Boundary condition:  } & u(0,t)=u(1,t)=0, \ t\in[0,1],
    \end{split}
\end{equation}
where its periodic coefficient $\beta$ is set as 3 and the analytic solution is $u(x,t)=\sin(\pi x)\cos(2\pi t)+\frac{1}{2}\sin(\beta \pi x)\cos(2\beta\pi t)$. This solution presents periodic patterns, which require the neural network to fit a periodic output space (Figure \ref{fig:benchmark_summary}(d)).

\paragraph{Karman Vortex} This task describes the incompressible fluid moving past a cylinder, which involves the famous Karman vortex street phenomenon \cite{wille1960karman} as shown in Figure \ref{fig:benchmark_summary}(e) and is governed by the following Navier-Stokes equation:
\begin{equation}
    \begin{split}
        \frac{\partial u}{\partial t}+(u\frac{\partial u}{\partial x}+v\frac{\partial u}{\partial y})&=-\frac{\partial p}{\partial x}+0.01(\frac{\partial^2 u}{\partial x^2}+\frac{\partial^2 u}{\partial y^2})\\
        \frac{\partial v}{\partial t}+(u\frac{\partial v}{\partial x}+v\frac{\partial v}{\partial y})&=-\frac{\partial p}{\partial y}+0.01(\frac{\partial^2 v}{\partial x^2}+\frac{\partial^2 v}{\partial y^2})\\
        \frac{\partial u}{\partial x} + \frac{\partial v}{\partial y}&=0,
    \end{split}
\end{equation}
where $u,v$ represents the velocity along the x-axis and y-axis respectively and $p$ denotes the pressure field. Since we cannot obtain the analytic solution of the Navier-Stokes equation, following previous studies \cite{raissi2019physics}, we experiment with the high-resolution data calculated by spectral/hp-element solver NekTar \cite{karniadakis2005spectral}. 

Specifically, the generated fluid sequence contains 200 frames. The task is to reconstruct the pressure field $p$ with physics loss, which contains the above three equations and the supervision of ground truth velocity.

\paragraph{Fluid Dynamics} This problem is from JAX-PI \cite{wang2023expert}, using the 2D incompressible Navier-Stokes equations in fluid dynamics and appropriate initial conditions to simulate fluid flow in a torus. The governed PDEs and initial conditions we used are shown as follows:
\begin{equation}\label{equ:nsFlow}
\begin{split}
\frac{\partial w}{\partial t} + (u\frac{\partial w}{\partial x}+v\frac{\partial w}{\partial y}) &= \frac{1}{\text{Re}} (\frac{\partial^2 w}{\partial x^2}+\frac{\partial^2 w}{\partial y^2}), \ \ \ (t,x,y)\in[0, T] \times \Omega \\
\frac{\partial u}{\partial x} + \frac{\partial v}{\partial y} &= 0, \quad\quad\quad\ (t,x,y)\in[0, T] \times \Omega \\
w(0, x, y) &= w_0(x, y), \ (x,y)\in\Omega
\end{split}
\end{equation}
where $u$ is the two-dimensional velocity vector of the fluid, $w=\nabla\times u$ is the vorticity, with $T$ set to 10s,  $\Omega$ set to $[0, 2\pi]^2$, and the Reynolds number 
$\text{Re}$ set to 100. $w_0$ denotes the given initial condition of vorticity.

The task is to simulate the future 10 seconds of fluid vorticity field $w$ solely based on the initial condition. However, the convective term $(u\cdot\nabla)u$ in the Navier-Stokes equations is nonlinear, which can lead to chaotic behavior. Thus, small changes in initial conditions can result in significantly different outcomes, making this task extremely difficult \cite{constantin1988navier}. As presented in Figure \ref{fig:benchmark_summary}(f), the physics field in this task is quite complex.


\subsection{Experiment Settings}\label{appdix:experiment}

We repeat all the experiments three times and report the average performance in the main text. Here are detailed configurations for each benchmark. The standard deviations can be found in Appendix \ref{appdix:std}.

\paragraph{Standard Benchmarks} For Convection, 1D-Reaction and 1D-Wave, we following the experiment settings in PINNsFormer~\cite{zhao2023pinnsformer}. Specifically, each experiment selects $101\times 101$ collocation points in the input domain and sets loss weights $\lambda_\ast=1$. All the models are trained with L-BFGS optimizer \cite{liu1989limited} for 1,000 iterations in Pytorch~\cite{Paszke2019PyTorchAI}. As for Allen-Cahn, we also implement this PDE in Pytorch with all the configurations same as PINNsFormer but set $\lambda_{\text{res}}=\lambda_{\text{bc}}=1$ and $\lambda_{\text{ic}}=10$ for all models to fit the complex initial condition.

\paragraph{Karman Vortex} As we stated before, this task is supervised by the equation constraints and ground truth of velocity. Concretely, we randomly select 2500 collocation points from the whole spatiotemporal sequence and train the model with L-BFGS optimizer for 1,000 iterations in Pytorch with loss weights $\lambda_\ast=1$ following PINNsFormer \cite{zhao2023pinnsformer}.

\paragraph{Fluid Dynamics} In this task, we use the JAX-PI framework\footnote{\url{https://github.com/PredictiveIntelligenceLab/jaxpi}}. Here are the detailed settings.

Firstly, to tackle the long temporal interval, we split the temporal domain into 10 windows and each window is trained separately in sequence. For each window, a total of 150,000 training steps are used, with each training step sampling uniformly distributed points in both the temporal and spatial domains, totaling 4096 coordinates $(t, x, y)$. Then, the corresponding governed PDEs and initial conditions (Eq.~\eqref{equ:nsFlow}) are used as loss functions for training. Specifically, for the first window, the initial condition is the value of the exact solution at the beginning time, and subsequent windows use the solution at the last timestamp from the previous window as the initial condition. Although this time-marching strategy may lead to error accumulation due to the setting of initial conditions, excellent models and training methods can maintain very low relative errors even in the last window. Note that JAX-PI also utilizes some tricks to ensure the final performance, such as random Fourier feature embedding \cite{tancik2020fourier}. We also maintain these tricks in all the models to ensure that the only variable is model architecture for rigorous comparison.

\subsection{Model Configuration}

As we highlighted in the main text, we insist on the lightweight design in our proposed ProPINN. Specifically, the projection layer $\mathcal{P}$ involves two linear layers with an in-between activation function, where the input dimension $(d+1)$ is firstly projected to $8$ and then to $32$, which is the same as the embedding size of PINNsFormer \cite{zhao2023pinnsformer}. And the multi-region mixing layer $\mathcal{M}$ also involves two linear layers with an in-between activation, which is only applied to the region dimension. Specifically, $\mathcal{M}$ will first project the \#scale scales to $8$ and then to $1$ after an activation layer. As for the final projection layer $\mathcal{H}$, it consists of three linear layers with inner activations, whose hidden dimension is set as $64$.

In our experiments, we also compare ProPINN with seven baselines. Here are our implementation details for these baselines:
\begin{itemize}
    \item For vanilla PINN \cite{raissi2019physics}, QRes \cite{bu2021quadratic}, FLS \cite{wong2022learning} and PINNsFormer \cite{zhao2023pinnsformer}, we follow the Pytorch implementation of these models provided in PINNsFormer and reimplement them in JAX to fit the Fluid Dynamics task in JAX-PI \cite{wang2023expert}. Specifically, vanilla PINN, QRes and FLS are all with 4 layers with 512 hidden channels. PINNsFormer contains 1 encoder layer and 1 decoder layer with 32 hidden channels for the attention mechanism and 512 hidden channels for the feedforward layer.
    \item As for SetPINN \cite{nagda2024setpinns}, we implement this model based on their official paper, which can reproduce the results reported in their paper. Also, we reimplement it in JAX for the Fluid Dynamics task. Specifically, same to the official configuration in PINNsFormer, SetPINN is experimented with 1 encoder layer and 1 decoder layer, which contains 32 hidden channels for the attention mechanism and 512 hidden channels for the feedforward layer.
    \item For KAN \cite{liu2024kan}, we adopt their official implementation and set hyperparameters following \cite{wu2024ropinn}.
    \item For PirateNet \cite{wang2024piratenets}, its official implementation is in JAX. Thus, we directly test their official version in the Fluid Dynamics task and reimplement it in Pytorch for other benchmarks. Specifically, it contains 256 hidden channels for representations and 4 layers following its official configuration.
\end{itemize}


\subsection{Metrics}

As we stated before, we evaluate the model-predicted solution based on relative L1 error (rMAE) and relative Root Mean Square Error (rRMSE). For ground truth $u$ and model prediction $u_\theta$, these two metrics can be calculated as follows:
\begin{equation}
    \text{rMAE: }\sqrt{\frac{\sum_{i=1}^n\left|u_{\theta}(\mathbf{x}_i)-u(\mathbf{x}_i)\right|}{\sum_{i=1}^n\left|u(\mathbf{x}_i)\right|}}\quad\text{rRMSE: }\sqrt{\frac{\sum_{i=1}^n\left(u_{\theta}(\mathbf{x}_i)-u(\mathbf{x}_i)\right)^2}{\sum_{i=1}^n\left(u(\mathbf{x}_i)\right)^2}},
\end{equation}
where $\{\mathbf{x}_i\}_{i=1}^n$ are selected collocation points for evaluation.

\section{Hyperparameter Analysis}
As a supplement to ablations in Figure \ref{fig:ablation} of the main text, we also test the model performance under different hyperparameter configurations, including the number of perturbations at each scale $(k_1, k_2,k_3)$, size of perturbation region $(R_1,R_2,R_3)$ and the number of scales $\#$scale. The results are presented in Figure \ref{fig:hyper}, where we can obtain the following observations.

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/hyperparameter_new.pdf}}
\vspace{-5pt}
	\caption{Model analysis with respect to different hyperparameter configurations of ProPINN. For (c), two scales correspond to $(R_1,R_2)=(0.01, 0.05)$ and three scales correspond to $(R_1,R_2,R_3,R_4)=(0.01, 0.03, 0.07,0.09)$.}
	\label{fig:hyper}
\end{center}
\vspace{-20pt}
\end{figure*}
 (1) \emph{More perturbations will generally boost the model performance.} As stated in Section \ref{sec:propinn}, we adopt differential perturbation to unite gradients of region points. Thus, adding perturbation points will enhance the connection of region gradients, thereby benefiting the final performance, while this will also bring more computation costs. Thus, we choose $(k_1,k_2,k_3)=(3^2,5^2,7^2)$ for a better trade-off between efficiency and performance.
 
(2) \emph{The size of the perturbation region is up to the PDE property.} As Remark \ref{remark:region} discussed, the gradient correlations only consider the points within a region. Therefore, in the Convection equation that involves the rapid variation in the PDE solution, enlarging perturbation size to $R_3=0.13$ (the whole domain is $[0,2\pi]\times[0,1]$) may introduce noise to the learning process, making the final performance degenerate seriously. Also, it is observed that ProPINN performs relatively steadily on all other benchmarks in different perturbation regions. Thus, we believe the effect of this hyperparameter is up to a certain PDE. And our design in choosing $(R_1,R_2,R_3)$ as $(0.01,0.05,0.09)$ can be a generalizable choice.

(3) \emph{Adding scales can also boost the model performance.} As shown in Figure \ref{fig:ablation} of main text, only one scale in ProPINN will bring a serious performance drop. Further, in Figure \ref{fig:hyper}(c), we increase the number of scales from 2 to 4. It is observed that the model performance is generally improved, especially for Allen-Cahn, which is with a complex solution, thereby can benefit more from adding scales. These results further demonstrate the effectiveness of our design in multi-region mixing.



\section{More Visualizations}\label{appdix:more_vis}
In the main text, we have already provided the visualization comparison on Convection and 1D-Wave in Figure \ref{fig:case} and Fluid Dynamics in Figure \ref{fig:case_fluid}. Here we also provide comparisons of the other three benchmarks in Figure \ref{fig:case_more} and Figure \ref{fig:case_vortex_more}. 

It is easy to observe that ProPINN is better than PINNsFormer in handling the areas with rapid variations, including the corner of 1D-Reaction, the middle region of Allen-Cahn and the vortex of Karman Vortex, which come from its better propagation property. Besides, we can find that PirateNet under single-point-processing paradigm fails to capture the vortex dynamics, indicating the inherent deficiency of single-point-processing models in complex physics simulation.


\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case_more.pdf}}
\vspace{-5pt}
	\caption{Visualizations of model approximated solution on 1D-Reaction and Allen-Cahn. Error map ($u_\theta-u$) is also plotted.}
	\label{fig:case_more}
\end{center}
\vspace{-20pt}
\end{figure*}

\begin{figure*}[h]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{fig/case_fluid_vortex.pdf}}
\vspace{-10pt}
	\caption{Comparison of model approximated solutions on Karman Vortex. Error map ($u_\theta-u$) is also plotted.}
	\label{fig:case_vortex_more}
\end{center}
\vspace{-20pt}
\end{figure*}

\section{Standard Deviations}\label{appdix:std}
All the experiments are repeated three times and the standard deviations are listed in Table \ref{tab:mainres_standard_std}. We can find that in all benchmarks, ProPINN surpasses the second-best model with high confidence.

\begin{table*}[h]
	\caption{Standard deviations of ProPINN on all experiments. For clarity, we also list the performance of the second-best model. P-value $<$ 0.05 indicates that ProPINN surpasses the second-best model with high confidence.}
	\label{tab:mainres_standard_std}
	\vspace{-5pt}
	\vskip 0.15in
	\centering
	\begin{small}
		\begin{sc}
			\renewcommand{\multirowsetup}{\centering}
			\setlength{\tabcolsep}{3pt}
			\begin{tabular}{lcccccccc}
				\toprule
                    \multirow{3}{*}{rMAE} & \multicolumn{4}{c}{Standard Benchmarks} & \multicolumn{2}{c}{Complex Physics} & \\
                    \cmidrule(lr){2-5}\cmidrule(lr){6-7}
				& Convection & 1D-Reaction & Allen-Cahn & 1D-Wave & \scalebox{0.9}{Karman Vortex} & Fluid Dynamics \\
				\midrule
                    \multirow{2}{*}{Second-best Model} & 0.023\scalebox{0.7}{$\pm 0.002$} & 0.015\scalebox{0.7}{$\pm 0.002$} & 0.098\scalebox{0.7}{$\pm 0.007$} & 0.051\scalebox{0.7}{$\pm 0.008$} & 0.287\scalebox{0.7}{$\pm 0.08$} & 0.2362\scalebox{0.7}{$\pm 0.013$} \\
                    & \scalebox{0.8}{(PINNsFormer)} & \scalebox{0.8}{(PINNsFormer)} & \scalebox{0.8}{(PirateNet)} & \scalebox{0.8}{(PirateNet)} & \scalebox{0.8}{(SetPINN)} & \scalebox{0.8}{(FLS)} \\
                    \cmidrule(lr){2-7}
                    ProPINN & 0.018\scalebox{0.7}{$\pm 0.001$} & 0.010\scalebox{0.7}{$\pm 0.001$} & 0.036\scalebox{0.7}{$\pm 0.006$} & 0.016\scalebox{0.7}{$\pm 0.004$} & 0.161\scalebox{0.7}{$\pm 0.03$} & 0.1834\scalebox{0.7}{$\pm 0.010$} \\
                    \midrule
                    P-Value & 0.015 & 0.015 & 0.000 & 0.001 & 0.026 & 0.003 \\
				\bottomrule
			\end{tabular}
		\end{sc}
	\end{small}
    % \vspace{-5pt}
\end{table*}

\section{Limitations and Future Work}

One potential limitation of ProPINN lies in its application to extremely high-dimensional PDEs, such as the Hamilton-Jacobi-Bellman equation in optimal control \cite{bardi1997optimal} and the Schrodinger equation in quantum physics \cite{berezin2012schrodinger}, which may involve millions of dimensions. These high-dimensional PDE-solving tasks will require the model to augment many points within each region, bringing huge computation overload.

This limitation can be resolved by integrating the ``amortization'' technique \cite{hu2023bias,shi2024stochastic,hu2024tackling} with ProPINN, which can ``amortize the computation over the optimization process via randomization''. Since this paper mainly focuses on propagation failures of PINNs instead of high-dimensional issues and we have conducted extensive experiments in PINN failure modes and complex physics to verify the model effectiveness, we would like to leave the topic as our future work. Also, we would like to highlight that the favorable results in solving Navier-Stokes equations are already valuable for extensive real-world applications, such as aerodynamic simulation \cite{mccormick1994aerodynamics}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
