% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xspace, soul}
\usepackage{threeparttable}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{pifont}
\usepackage{amssymb,enumitem}
\usepackage{comment}
\usepackage{hyperref,graphicx}
\usepackage{marvosym}
\usepackage{booktabs, makecell, rotating, siunitx}
\usepackage{tcolorbox}  % For styled boxes
\usepackage{listings}   % For code formatting
\usepackage{xcolor}     % For custom colors
\usepackage{colortbl}
\usepackage{subcaption}
\usepackage{longtable} % for 'longtable' environment
\usepackage{pdflscape} % for 'landscape' environment
\usepackage{tcolorbox}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\mollm: Generalizing Large Language Models for Multi-property Molecule Optimization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{Vishal Dey \\
%   Department of Computer Science and Engineering \\
%   The Ohio State University \\
%   Columbus, OH 43210, USA \\
%   \texttt{dey.78@buckeyemail.osu.edu} \\\And
%   Xiao Hu \\
%   Department of Computer Science and Engineering \\
%   The Ohio State University \\
%   Columbus, OH 43210, USA \\
%   \texttt{hu.2823@buckeyemail.osu.edu} \\} \\\And
%   Xia Ning \\

\author{
 \textbf{Vishal Dey\textsuperscript{1}$^*$},
 \textbf{Xiao Hu\textsuperscript{1}$^*$},
 \textbf{Xia Ning\textsuperscript{1,2,3,4}}
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1} Department of Computer Science
and Engineering, The Ohio State University, USA \\
 \textsuperscript{2} Translational Data Analytics Institute, The Ohio State University, USA \\
 \textsuperscript{3}Department of Biomedical Informatics, The Ohio State University, USA \\
 \textsuperscript{4} College of Pharmacy, The Ohio State University, USA
 % \textsuperscript{5}Affiliation 5
\\
 \small{
   \textbf{Correspondence:} \href{mailto:email@domain}{ning.104@osu.edu}
 }
}

\include{define}

\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{Equal Contribution}\def\thefootnote{\arabic{footnote}}

\setlength{\fboxsep}{1pt} 

\begin{abstract}
%%
% Lead optimization, a critical stage in the drug discovery pipeline,
% involves optimizing molecules to satisfy specific therapeutic requirements.
% %
% This involves optimizing multiple properties simultaneously,
% requiring to balance complex property trade-offs.
%
Despite recent advancements,
most computational methods for molecule optimization are constrained
to single- or double-property optimization tasks
and
suffer from poor scalability and generalizability to novel optimization tasks.
%
Meanwhile, Large Language Models (LLMs) demonstrate
remarkable out-of-domain generalizability to novel tasks.
%
To demonstrate LLMs' potential for molecule optimization,
we introduce \MOptData, 
the first high-quality instruction-tuning dataset specifically
focused on complex multi-property molecule optimization tasks. 
%
Leveraging \MOptData, we develop {\mollm}s, 
a series of instruction-tuned LLMs
for molecule optimization.
%
Extensive evaluations across 5 in-domain and 5 out-of-domain
tasks demonstrate that {\mollm}s 
consistently outperform state-of-the-art baselines. 
%
{\mollm}s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs.
%
Such strong generalizability
demonstrates the tremendous potential of {\mollm}s 
as foundational models for molecule optimization,
thereby tackling novel optimization tasks without resource-intensive retraining. 
\MOptData, models, and code are accessible through 
\url{https://github.com/ninglab/GeLLMO}.
%\url{https://anonymous.4open.science/r/LLM4Opt-8CBE/}.
%\xia{URL to your data and code here}

\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%
Drug discovery is a costly and
time-consuming process, costing over
\$2 billion and a decade~\cite{sertkaya2024costs}.
One of the most critical stages~\cite{Hughes2011principles}
in this process is
lead optimization~\cite{Sun2022why},
%
where a molecule with promising bioactivity against a drug target
%(i.e., a hit molecule)
is optimized into a lead molecule 
%that meets specific 
%therapeutic requirements.
%
% Poorly optimized leads are a major cause of late-stage failures, underscoring the critical importance of this stage.
%
by improving multiple molecular properties simultaneously.
%while retaining the core scaffold~\cite{}.
%to ensure the drug's efficacy and safety.
For example, a hit molecule to treat schizophrenia is optimized such that
it can permeate the blood-brain barrier~\cite{Pollak2018} to reach the 
DRD2 target~\cite{Seeman2006} in the brain,
while balancing lipophilicity, solubility
%for oral bioavailability 
and toxicity. %to minimize adverse effects.
%
Improving all these properties together requires
balancing multiple trade-offs~\cite{Nicolaou2013} and conflicting objectives~\cite{Nicolotti2011}, making
multi-property optimization extremely challenging.


% Despite advancements in computational methods
% for molecule optimization,
% existing approaches~\cite{} remain constrained in scope and applicability.
% %
Most computational methods~\cite{gao2022sample} 
for molecule optimization
focus on single- or double-property tasks,
leaving multi-property optimization tasks largely unexplored.
%
% Furthermore, these methods are often evaluated
% on specialized properties specific to drug targets (e.g., JNK3), 
% rather than fundamental molecular properties relevant to general drug discovery pipelines.
Existing approaches~\cite{kim2024genetic,yang2021hit}
%based on genetic algorithms
%and reinforcement learning~\cite{yang2021hit} %for multi-property optimization 
use predefined fitness and reward functions~\cite{LUUKKONEN2023}, respectively,
to model property trade-offs.
However, designing such functions for each task 
demands significant effort and domain expertise.
%to fully capture nuanced trade-offs for each task. 
%
In contrast, other methods~\cite{chen2021deep,wu2024leveraging}
obviate the need for such functions,
%by learning %structural modifications 
%directly from pairs
%of hit and lead molecules.
%However, these methods 
but depend on scarce task-specific data, limiting their scalability and adaptability.
%
% Additionally, these methods often generate molecules with entirely new scaffolds, thus failing to retain the core scaffold retention
% -- a key requirement for lead optimization~\cite{}.
%
%
Additionally, existing methods lack generalization to
unseen tasks, hindering their practical applicability
to emerging therapeutic requirements.


% \paragraph{Limitation of the existing work}
% \begin{enumerate}
% \item \textbf{Lack of Realistic Multi-property Tasks and Benchmarking}
% - Only optimize one or two properties, this oversimplified setting fails to reflect real-world drug discovery challenges.
% %- Performance limitation on multi-objective optimization.
% - Absence of meaningful/realistic tasks.

% \item \textbf{Lack of adaptability to unseen tasks}
% - Cannot adapt to OOD tasks with unseen property combinations.
% - Poor adaptation to varying therapeutic objectives.
% \item \textbf{Poor scalability}
%     Inflexible architectures and input requirements restrict adaptation to diverse practical molecule optimization scenarios.
% \end{enumerate}

% \paragraph{Contributions}
% \begin{enumerate}
% \item Novel instruction-tuning dataset focused on realistic multi-property molecule optimization involving at least 3 properties.

% \item Generalist \mollm as a unified foundational model capable to handle on diverse optimization tasks; does not require lot of task-specific labeled data.

% \item Comprehensive evaluation across IND and OOD tasks, establishing new benchmarks and demonstrating the potential of LLMs
% \end{enumerate}

% To address these gaps, we propose a novel framework that 
% leverages the adaptability of large language models (LLMs) 
% for multi-property molecular optimization.
% %
%Recently, large language models (LLMs)~\cite{naveed2023comprehensive}
Large language models (LLMs)~\cite{naveed2023comprehensive}
have demonstrated remarkable generalization to unseen tasks 
across diverse domains~\cite{chang2024} recently. 
However, their potential
in challenging, multi-property molecule optimization tasks
remains largely unidentified.
%
To fully identify LLMs' potential,
we introduce \MOptData, the first 
high-quality instruction-tuning dataset 
specifically focused on complex, multi-property
tasks, each aiming to improve at least 3 properties simultaneously.
%
This is in stark contrast to
existing instruction-tuning datasets~\cite{ye2025drugassist}
that are limited to single- and double-property tasks.
%
% Moreover, \MOptData includes tasks that capture 
% more realistic therapeutic requirements than \DrugMOpt.
% such as improving BBBP, DRD2 inhibition and plogP 
% or improving BBBP, plogP and QED.
%%
% By framing optimization goals as textual instructions, \MOptData introduces a level of flexibility and semantic richness that mirrors real-world therapeutic objectives.


\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/pipeline_v1.pdf}
    \caption{Overview of \MOptData and \mollm}
    \label{fig:Overview}

\vspace{-15pt}
\end{figure*}



%%
Leveraging \MOptData, 
we develop a series of
\underline{Ge}neralizable \underline{LLM}s for 
\underline{M}ulti-property \underline{M}olecule \underline{O}ptimization, denoted 
as {\mollm}s,
%task-specific and generalist LLMs
by instruction-tuning general-purpose LLMs.
%for multi-property molecule optimization.
%such as Mistral-7B-Instruct and Llama3.1-8B-Instruct.
%
Task-specific {\mollm}s are fine-tuned on individual tasks,
learning precise optimization tailored to specific therapeutic contexts.
%
Generalist {\mollm}s, on the other hand, are fine-tuned on multiple tasks 
%spanning different property combinations,
which enables them to learn and reason property trade-offs across diverse therapeutic contexts.
%
% Via instruction tuning,
% the general-purpose LLMs gain 
% the flexibility to dynamically adapt to complex multi-property optimization tasks. 
%
Moreover, fine-tuning using diverse tasks and instructions
%in \MOptData 
enables generalist {\mollm}s to 
effectively handle unseen tasks
and instructions.
%


All {\mollm} models are extensively
evaluated against strong general-purpose LLMs,
state-of-the-art foundational LLMs for chemistry and
task-specific non-LLMs
across 5 in-domain (IND) and 5 out-of-domain (OOD) tasks.
Our experimental results demonstrate the following key findings:

%\noindent
\textbf{(1)} Both task-specific and generalist {\mollm}s significantly outperform state-of-the-art baselines, 
including powerful closed-source LLMs,
across all IND %(Section~\ref{sec:results:ind}) 
and OOD tasks, %(Section~\ref{sec:results:ood}), 
with significant improvements of up to 186.6\% over the best baselines.

\textbf{(2)} Compared to task-specific {\mollm}s, 
generalist {\mollm}s excel on 3 out of 5 IND tasks and
demonstrate competitive performance on the other 2 tasks,
with remarkable gains of up to 91.3\% %in {\SR} 
on more complex tasks, such as \BDPQ.
%(Section~\ref{sec:results:ind}).

%\noindent
\textbf{(3)} Generalist \mollmSixGen models show superior
generalization to OOD tasks %and unseen instructions, 
outperforming strong baselines
by as much as 159.9\%.




Figure~\ref{fig:Overview} presents the overall scheme of \mollm.
To the best of our knowledge,
\MOptData is the first large-scale, high-quality instruction-tuning dataset specifically designed
for multi-property molecule optimization.
%
% Our work introduces the first generalist model training framework and
% a foundational model for molecule optimization.
%
Notably, the strong generalization ability of our generalist
{\mollm}s demonstrates their tremendous potential
to accelerate drug discovery by tackling novel optimization tasks
without resource-intensive retraining.
%thereby facilitating drug discovery.
%
Dataset, models and code are accessible
through \url{https://github.com/ninglab/GeLLMO}.
%Dataset and models are accessible through hugging
%\url{https://anonymous.4open.science/r/LLM4Opt-8CBE/}.

% \paragraph{Summary of findings:}
% \begin{enumerate}
%     \item Effectiveness of \mollm in both in-domain and OOD settings, consistently outperforming state-of-the-art baselines, including general-purpose LLMs, chemistry-specific LLMs, and task-specific non-LLM methods
    
%     \item Generalist \mollm models achieve remarkable generalization to unseen tasks, significantly outperforming 5-shot in-context learning by Claude-3.5 on OOD tasks. 
    
%     \item Furthermore, task-specific \mollm models excel in highly specialized tasks with strong property correlations. These results highlight the complementary nature of task-specific and generalist \mollm.
% \end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{sec:rel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Various computational approaches have been developed for molecule optimization~\cite{you2018graph, blaschke2020reinvent, xie2021mars, bung2022silico,sun2022molsearch}.
%
%
For example, Modof~\cite{chen2021deep}, MIMOSA~\cite{fu2021mimosa},
and f-RAG~\cite{lee2024molecule} 
perform substructure modifications over molecular graphs.
%at predicted disconnection sites 
%while preserving core scaffolds. %each with distinct modification strategies.
%
Chemformer~\cite{irwin2022chemformer} and \PMol~\cite{wu2024leveraging} treat optimization as a translation over SMILES~\cite{Weininger1988smiles} strings, 
and learn the required modification from molecule pairs.
%incorporating task-specific guidance in different ways.
%
GraphGA~\cite{jensen2019graph} and MolLeo~\cite{wang2024efficient} leverage 
genetic algorithms
to evolve molecules via genetic algorithm. %for crossover and mutation operations.
% While effective in improving 1 or 2 properties, their efficacy in multi-property optimization tasks remains largely unexplored.
%
These methods~\cite{kim2024genetic,yang2021hit} often require designing non-trivial fitness or reward functions
to capture nuanced trade-offs among multiple properties. 
%
% However, designing such functions is highly non-trivial, requiring domain expertise to capture nuanced trade-offs. 
% %
% Pairwise approaches such as \PMol~\cite{wu2024leveraging} circumvent this by learning from molecule pairs,
% but suffer from scalability due to limited task-specific labeled data. 
%
Moreover, such methods 
%focus on controllable molecule generation~\cite{wang2023retrievalbased}, 
tends to generate
molecules with entirely new scaffolds, 
limiting their applicability \emph{in vitro} optimization.


%Similarly, f-RAG~\cite{lee2024molecule} breaks down the molecules into hard fragments and soft fragments for molecular optimization, where hard fragments are explicitly included in generation while soft fragments guide the process through embedding fusion. 
%
%MIMOSA uses pre-trained GNNs for fragment modification prediction and MCMC sampling to iteratively search promising molecules based on weighted scoring of properties.
%
%Chemformer~\cite{irwin2022chemformer} and \PMol~\cite{wu2024leveraging} approach optimization by translating the source molecules to the optimized molecules represented by SMILES strings
%
%Chemformer~\cite{irwin2022chemformer} incorporates task-specific information through direct task prompt prefixing of the source molecule's SMILES string, while \PMol~\cite{wu2024leveraging} employs specialized task-specific atomic embeddings from a pre-trained GNN to guide the optimization process.
%
%GraphGA~\cite{jensen2019graph} and MolLeo~\cite{wang2024efficient} implement genetic-algorithm-based approaches, starting with a population of molecules with low property values and evolving them through predefined modification rules or by incorporating chemistry language models for crossover and mutation operations.
%

% \vishal{Computational methods
% for improving molecular properties\cite{} have been extensively
% developed,
% where each method either modifies the structures of
% a given molecule (i.e., molecule optimization),
% or generates entirely new ones (i.e., molecule generation).
% %
% These methods vary in how they perform structural modifications
% and in their molecular representations. 
% For example,
% GraphGA~\cite{jensen2019graph},
% Modof~\cite{chen2021deep},
% MIMOSA~\cite{fu2021mimosa},
% and f-RAG~\cite{lee2024molecule}
% perform fragment-level modifications on molecular graphs
% by removing, adding, and replacing 
% substructures at predicted disconnection sites.
% %
% In contrast, methods such as 
% SMILES GA~\cite{},
% MolLeo~\cite{wang2024efficient},
% Genetic-GFN~\cite{}, and
% Prompt-MolOpt~\cite{}
% directly modify SMILES strings to generate new molecules
% with improved properties.
% %
% }

% \vishal{
% Most existing methods (except Modof and Prompt-MolOpt)
% are generation-based,
% and thus generate molecules with entirely 
% new scaffolds.
% This limits their applicability in realistic
% settings since retaining the core scaffold is a key requirement
% for \emph{in vitro} optimization.
% %
% Furthermore, these methods are primarily evaluated on improving
% single or double properties.
% Thus, their efficacy in more complex and realistic
% multi-property optimization tasks remain largely unexplored.
% }
%

% Multi-property molecule optimization \cite{} is a critical challenge in drug discovery due to the complex trade-offs between properties. 
%
% Furthermore, methods leveraging genetic algorithms \cite{kim2024genetic} or reinforcement learning \cite{bung2022silico} 
% require design of non-trivial task-specific fitness or reward functions
% to model property trade-offs. 
% %
% % However, designing such functions is highly non-trivial, requiring domain expertise to capture nuanced trade-offs. 
% % %
% Pairwise approaches like Modof~\cite{chen2021deep} and \PMol~\cite{wu2024leveraging} circumvent this by learning from molecule pairs, but are constrained by scare task-specific labeled data. 
% %
% Furthermore, %the generative nature of 
% most methods tend to generate molecules with new scaffolds, limiting their applicability, as retaining the core scaffold is a key requirement for \emph{in vitro} optimization.


% To address this, recent approaches like \PMol
% build separate models for each property combination (i.e., each optimization task),
% leading to poor scalability.
% These limitations underscore the need for a single unified model -- similar to a foundation model\cite{} --
% that can flexibly and effectively handle diverse
% optimization tasks.



% %
% Furthermore, the lack of labeled paired data in such multi-property optimization tasks 
% hinders effective training for pairwise optimization methods such as Modof and \PMol. 
% %
% The genetic algorithm-based methods~\cite{fu2021mimosa,jensen2019graph} and reinforcement learning approaches~\cite{bung2022silico} attempt to handle multiple objectives through weighted summation, designing such weights proves highly non-trivial and struggles to capture nuanced property relationships.
% %
% Beyond this, the generative nature of most methods (except Modof and \PMol) makes them tend to generate molecules with entirely new scaffolds. This limits their applicability in realistic settings since retaining the core scaffold is a key requirement for \emph{in vitro} optimization.
% %

Recently, LLMs~\cite{chang2024} have emerged as a promising option for molecule optimization. 
%
For example,
ChatDrug~\cite{liu2023chatgpt} and Re3DF~\cite{le2024utilizing} leverage 
LLMs to optimize a molecule iteratively
through multi-turn dialogues.
% and incorporating domain knowledge through retrieval of similar molecules from databases and validation feedback from expert tools for iterative optimization. 
%
%Taking a different approach,
DrugAssist~\cite{ye2025drugassist}
instruction-tuned Llama2-7B-Chat~\cite{touvron2023llama} 
on each optimization task. 
%using their curated \DrugMOpt dataset.
%
While these approaches offer flexible 
task formulation through natural language, they still face
several limitations.
%
ChatDrug incurs high costs due to multiple API calls, 
and
instruction-tuning in DrugAssist relies on task-specific data,
limiting scalability and adaptability to more complex multi-property tasks.
%
%Moreover, most methods~\cite{chen2021deep,wu2024leveraging,}need to build separate models for each property combination (i.e., each optimization task), leading to poor scalability.
%Some approaches~\cite{jensen2019graph, wang2024efficient, lee2024molecule} require an initial pool of molecules or fragments to start with, which poorly aligns with real-world drug discovery scenarios where optimization typically starts from a single promising hit molecule.
%
%Additionally 
%Furthermore, optimizing multiple properties simultaneously is a fundamental challenge in drug discovery\cite{} due to complex trade-offs among such properties.
%

%

%All these methods are limited in their applicability to real-world drug discovery scenarios, particularly in handling multi-property optimizations. Most methods only demonstrate effectiveness on simple tasks involving single or double properties, with no systematic exploration on multi-property optimizations.
%
%Specifically, approaches like Modof and \PMol heavily rely on training data, limiting their performance when training data is scarce for multi-objective optimization. And they can only perform the tasks existing in the training data, suggesting the low generalizability. 
%

%


% \vishal{Recently, LLMs have been explored for molecule optimization. 
% %
% ChatDrug~\cite{liu2023chatgpt} and Re3DF~\cite{le2024utilizing} use 
% multi-turn dialogue-based generation using ChatGPT, incorporating domain knowledge 
% via retrieval~\cite{}
% and validation feedback from expert tools for iterative optimization. 
% %
% DrugAssist~\cite{ye2025drugassist} takes a different approach by instruction-tuning Llama2-7B-Chat\cite{} 
% for single- and double-property optimization tasks
% leveraging their curated \DrugMOpt dataset.
% %
% However, these approaches are limited in their scalability
% and applicability to complex multi-property optimization tasks.
% %
% For example,
% while ChatDrug can be used to optimize multiple properties, it requires multiple expensive API calls.
% %
% On the other hand, DrugAssist provides task-specific LLMs tuned on single- and double-property optimization,
% leaving multi-property optimizations unexplored.
% %
% }



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\MOptData Dataset}
\label{sec:data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\paragraph{Comparison among \DrugMOpt and \MOptData:}
To address these gaps, we introduce \MOptData, the first
instruction-tuning dataset specifically focusing on
realistic multi-property optimization tasks.
% improving at least 3 properties.
%
%
Different from \DrugMOpt~\cite{ye2025drugassist},
which focuses on single- and double-property tasks, 
\MOptData emphasizes
on tasks with at least 3 properties for evaluating LLMs
in in-domain and out-of-domain settings.
%
Table~\ref{tbl:relwork_data} highlights the notable
differences between the two datasets.
%
% Leveraging \MOptData, we develop \mollm, a series of task-specific and generalist LLMs through instruction-tuning
% that demonstrates {\mollm}s' superiority in
% tackling diverse, realistic challenges
% in molecule optimization.



%\xia{
\textbf{Problem Definition:} 
A molecule optimization task is to transform a hit $M_x$ --  a molecule exhibiting initial bio-activity
against a therapeutic target into a lead molecule $M_y$ – an improved molecule for drug development, 
through structural modification over $M_x$, such that 
%
\textbf{(1)} $M_y$ is structurally similar to $M_x$ (similarity constraint), and 
\textbf{(2)} $M_y$ is better than $M_x$ in terms of all desired properties of interest 
(property constraint). 
%
The desirability of a property is determined by the therapeutic goal, where
improved properties indicate more suitable toward a successful drug candidate. 
%
For example, for drug candidates targeting the central nervous system (CNS), 
higher blood-brain barrier permeability (BBBP) is desired to allow the molecules 
to reach the brain or spinal cord, 
whereas for those targeting the peripheral nervous system (PNS), 
lower BBBP is desired instead to prevent the drugs from damaging the CNS.
%
Under the property constraint, the molecule pair $(M_x$, $M_y)$ 
is represented as $(M_x \prec_{\Delta_p} M_y)_{\forall p \in P}$, 
indicating that $M_y$ is better than $M_x$ on each property $p$ of all the desired 
properties $P$ with a property-specific difference $\Delta_p$. 
%}



% \paragraph{Problem Definition:}
% %%
% Each molecule optimization task aims to
% transform a molecule that has a desired binding activity against a drug target
% (i.e., a hit molecule)
% $M_x$ 
% into another molecule with desired properties (i.e., a lead molecule)
% $M_y$
% by modifying structures of $M_x$.
% %
% The desirability of a property
% is determined by the therapeutic goal.
% %
% For example,
% higher
% blood-brain barrier permeability (BBBP) is desired for lead molecules
% targeting the central nervous system (CNS),
% whereas lower BBBP is desired if the drug target is not in the CNS
% to avoid off-target side-effects.
%
%to achieve desirable value in each property $p\in\mathcal{P}$.
%


% Depending on the therapeutic goal,
% each optimization task may require improvement in multiple desired properties.
% To achieve this,
% $M_x$ is modified to $M_y$ such that:
% %must satisfy two constraints simultaneously:
% (1) $M_y$ is structurally similar to $M_x$ (similarity constraint),
% %
% (2) $M_y$ achieves improvement in each property over $M_x$
% (property constraint),
% denoted as $M_x \prec_{\Delta_p} M_y$, implying
% that $M_y$ is optimized over $M_x$ with an improvement
% of at least $\Delta_p$ in each property, where
% $\Delta_p$ is a predefined threshold specific to each property $p$.
% %


% For example, an optimization task for CNS drug candidates
% may aim to
% simultaneously improve BBBP
% and mutagenicity (Mutag).
% In this case, higher BBBP and lower Mutag
% are desirable.
% Specifically,
% BBBP must increase by at least $\Delta_\text{BBBP}$ and
% Mutag must decrease by at least $\Delta_\text{Mutag}$.




% \vishal{Give a high-level summary of the tasks and lists the design principles, motivations and practical significance.
% Convert the below section into a paragraph.}
% \hl{TODO: A data statistics table}

% %----------------------------------------%
% \subsection{Design Priniciples}
% %----------------------------------------%
% \begin{itemize}
%     \item \textbf{Comprehensive Coverage:}
%     \begin{itemize}
%         \item Incorporates established tasks from existing literature for comparison.
%         \item Introduces novel task combinations with meaningful practical applications.
%         \item Tasks reflect real-world drug discovery challenges.
%     \end{itemize}
    
%     \item \textbf{Rigorous Structure:}
%     \begin{itemize}
%         \item Systematic organization of molecule pairs.
%         \item Carefully curated and validated property values.
%         \item Standardized format for consistent evaluation.
%     \end{itemize}

%     \item \textbf{Instruction Diversity:}
%     \begin{itemize}
%         \item Multiple instruction templates for each task to ensure the training to be generalizable (equipping language processing ability).
%     \end{itemize}
% \end{itemize}

% \xiao{xiao: I think we don't need the first row block, and the last row block here, the most important thing we want to claim in our benchmark is the tasks right, I feel it wouldn't be that important to show how many models trained or compared}
In this paper, we introduce \MOptData,
the first high-quality
instruction-tuning dataset to %adapt and 
evaluate
models in molecule optimization tasks.
%
Our design is based on four key principles:
%%
% \begin{enumerate}
%
% \item 
\textbf{(1) Pairwise optimization:} 
%
\MOptData contains a comprehensive set of molecule pairs
satisfying the similarity constraint (Tanimoto similarity > 0.6) and property constraint over 
multiple desired properties.
% \st{, where each pair consists of 
% a hit molecule and an
% optimized molecule with minimal structural modifications
% and improved properties.}
% \st{Learning from such pairs, rather than individual molecules, 
% explicitly captures the modification needed for optimization.}
%\xia{
Such molecule pairs enable opportunities for molecule optimization models 
to learn the association between the structural differences 
and the property improvement among the pairing molecules, 
and apply such associations for new lead optimization.
%}
%
% \st{This mirrors the \emph{in vitro} molecule
% optimization setting,
% which focuses on modifying a hit molecule into a lead molecule
% with better properties while retaining the core scaffold.}
%
%
%
% \item
\textbf{(2) Comprehensive coverage:}
\MOptData covers more molecular properties,
and extends beyond single- and double-property tasks in
existing molecular optimization benchmarks. 
It introduces multi-property optimization
tasks that require simultaneous improvement of at least 3 properties,
thereby representing complex
pharmacological trade-offs in lead optimization.
%
%
%
% \item
\textbf{(3) Real-world relevance:}
The tasks in \MOptData
are carefully constructed to
represent %\st{common optimization scenarios} \xia{
realistic challenges in lead optimization
%}
%\st{by combining key molecular properties.}
%\xia{
by combining molecular properties key to drug development.
%}
For instance, one of the tasks aims to improve
intestinal adsorption,
toxicity and BBBP -- key
properties for optimizing orally bioavailable CNS drugs.
%
% Moreover, the task construction process is flexible,
% allowing the formulation
% of diverse tasks with realistic constraints. 
%%
% \xiao{xiao: for concept 'real-world scenarios', I used the same words in section 2, where I try to say multi-objective, dynamic preferences optimization. I feel like in this section, it is more like drug-development relevance? I think we need to unify those words and concepts.}
% \item
% \textbf{Rigorous construction:}
% \MOptData consists of carefully and rigorously curated molecule pairs, 
% where the optimized molecules exhibit substantial
% property improvements over the source molecules,
% while preserving structural similarity.  
% Such selection of pairs for each task
% ensures that the optimization tasks are non-trivial.
%\xiao{I think we can remove 'realistic' here, it is hard to say why is realistic with substantial
%property improvements}
%\xia{what is rigorous here?}
%
%
\textbf{(4) Diverse instructions:}
\MOptData provides diverse natural language instructions,
each describing the optimization task using
different phrasings.
%
This prevents LLMs instruction-tuned on \MOptData
from overfitting to a specific phrasing
and thus, enables them to generalize to unseen instructions, 
%\xia{
which is crucial in practice to allow different descriptions on optimization tasks.
%}. 
%
% \st{This generalizability is crucial in practice as a user 
% (e.g., a chemist) may
% describe the optimization task uniquely.}

% \end{enumerate}
%%
% By incorporating these design principles, 
% we emphasize that \MOptData
% can be used to benchmark models
% on diverse and realistic pairwise molecule optimization tasks.
% \xiao{xiao: I think the meaningful, and the realistic here both refer to drug-development related right? So maybe we need to use the same word for these concepts, like use word 'meaningful' and 'realistic' in bullet 3 to let readers know}



\input{tables/task_summary}
%\ref{tbl:task_summary}

%========================================%
\subsection{Overview of \MOptData Tasks}
\label{sec:data:overview}
%========================================%

% \vishal{Overview: introduce all the tasks considered in the experiments and their significance in real life scenario.}

{\MOptData comprises 63 tasks, 
with 42 tasks aiming to improve at least 3 properties simultaneously, 
out of which 10 tasks are further divided into IND and OOD tasks (Section~\ref{sec:data:ind_ood}).
}
%designed for multi-property molecule optimization. 
%
All tasks in \MOptData are systematically designed
by considering combinations from 6
molecular properties:
%that influence a drug candidate's success during lead optimization:
%
\textbf{(i) Penalized LogP (plogP)} representing lipophilicity,
balancing permeability, 
%aqueous solubility,
solubility,
and metabolic stability
-- higher plogP is generally desired
in drug development;
%
\textbf{(ii) Quantitative Estimate of Drug-Likeness (QED)}
assessing overall drug-likeness,
incorporating multiple molecular attributes
related to molecular weight, lipophilicity, and solubility
-- higher QED indicates better drug-likeliness;
%
\textbf{(iii) Blood-Brain Barrier Permeability (BBBP)} which
refers to the ability of a drug to cross the blood-brain barrier
-- higher BBBP is desired for CNS drug candidates; 
%whereas,lower BBBP is desired if the drug target is not in the CNS;
%
\textbf{(iv) Mutagenicity (Mutag)} indicating the likelihood of a drug
causing genetic mutations --
lower Mutag scores are desired to reduce toxicity;
%
\textbf{(v) Human Intestinal Absorption (HIA)}
which reflects a drug's ability to be absorbed
through the gastrointestinal tract --
higher HIA is desired for orally administered drugs;
%
and
\textbf{(vi) Dopamine Receptor D2 (DRD2) binding affinity}
representing the ability of drugs to target dopaminergic pathways
-- higher DRD2 scores are desired for drugs
targeting the DRD2 receptor.
%

% Among these properties, 
% higher plogP, QED and HIA are generally desirable 
% to improve lipophilicity, drug-likeness, and absorption, respectively,
% whereas
% lower Mutag is desirable to minimize toxicity.
% %
% For drug candidates targeting the CNS,
% higher BBBP and DRD2 are desirable to ensure brain penetration and 
% effective receptor binding.
% %
%%
We focus on these properties because they 
are critical in influencing the
pharmacokinetics and safety profiles of molecules,
thereby contributing to the development of successful drug candidates.
%
Additionally, these properties are well-studied in the literature
and are used in existing benchmarks~\cite{gao2022sample}.
%
% By strategically combining these properties,
% we construct %10 multi-property 
% optimization tasks in \MOptData
% to reflect \emph{in vitro} lead optimization challenges.
% %
% These tasks can be used to benchmark LLMs 
% fine-tuned for molecule optimization.
% Success in such tasks provides insights on
% whether LLMs can handle
% realistic molecule optimization tasks.
%
%
{\MOptData provides 10 evaluation tasks 
%relevant to \emph{in vitro} optimization.
%
%These tasks %are used to evaluate models in this work and 
which are 
summarized in Table~\ref{tbl:task_summary}
with details in Appendix~\ref{sec:app:task}.}
These tasks can be categorized into two groups based on
their significance:
%%
\textbf{(1) General Drug-Likeness and Toxicity Optimization (\GTO):}
%%
%These tasks 
Focuses on widely studied molecular properties related to 
drug-likeness, absorption, and toxicity,
that are general to any successful drug candidates.
% Success in such tasks indicates the model's ability to improve 
% general molecular properties such as QED, plogP and Mutag,
% that are critical in lead optimization. 
%regardless of a specific biological target.
%
% Tasks include 
% simultaneous optimization of \underline{M}utag, \underline{p}logP and \underline{Q}ED denoted as \MPQ, 
% \underline{H}IA, \underline{M}utag, \underline{p}logP and \underline{Q}ED denoted as \HMPQ.
%%
%%
\textbf{(2) Context-specific Optimization (\CSO):}
%
%These tasks 
Includes properties 
relevant to specific disease contexts and therapeutic requirements.
% such as improving BBBP for CNS-targeting drug candidates
% with other general molecular properties, such as plogP and QED.
%
% Tasks include
% simultaneous improvement of 
% \underline{B}BBP, \underline{D}RD2 and \underline{Q}ED (\BDQ), 
% \underline{B}BBP, \underline{D}RD2 and \underline{p}logP (\BDP),
% \underline{B}BBP, \underline{p}logP and \underline{Q}ED (\BPQ), 
% \underline{D}RD2, \underline{p}logP and \underline{Q}ED (\DPQ),
% \underline{B}BBP, \underline{D}RD2, \underline{p}logP and \underline{Q}ED (\BDPQ), 
% \underline{B}BBP, \underline{H}IA, \underline{M}utag and \underline{Q}ED (\BHMQ),
% \underline{B}BBP, \underline{D}RD2, \underline{M}utag and \underline{Q}ED (\BDMQ)
% and 
% \underline{B}BBP, \underline{M}utag, \underline{p}logP and \underline{Q}ED (\BMPQ).

% \xia{the following categorization looks weird; I don't recommend this categorization}

%%
% \begin{enumerate}

% \item \textbf{General Drug-Likeness and Toxicity Optimization:}
% %
% This category reflects the 
% general challenges in lead optimization, 
% where improving drug-likeness must be carefully balanced against absorption and toxicity risks. 
% %
% It serves as a testbed for evaluating LLMs’ ability to optimize broadly applicable molecular properties.
% %
% The selected properties -- plogP, QED, mutagenicity, and HIA
% -- are widely used in lead optimization and 
% are not specific to any single target or therapeutic area. 
% %
% Tasks in this category include 
% simultaneous improvement of \underline{M}utag, \underline{p}logP and \underline{Q}ED denoted as \MPQ, 
% \underline{H}IA, \underline{M}utag, \underline{p}logP and \underline{Q}ED denoted as \HMPQ.
% %


% \item \textbf{Target-Specific Drug Optimization:}
% %
% This category focuses on optimization guided by properties 
% relevant to a particular pharmacological mechanism or biological target. 
% %
% We consider BBBP and DRD2 binding
% -- which are relevant for central nervous system (CNS)-targeting drugs
% -- as an example of how LLMs can handle target-specific constraints. 
% %
% However, our framework is generalizable to other targets 
% and therapeutic domains
% by substituting those properties with domain-relevant ones.
% %
% % In this study, we focus on drugs designed for 
% % central nervous system (CNS), 
% % incorporating BBBP and DRD2 as the therapeutic-relevant properties.
% %
% % Designing CNS-active drugs requires optimizing BBBP to ensure that
% % drug molecules can cross the blood-brain barrier while maintaining
% % high receptor binding affinity (e.g., DRD2) and overall drug-likeness (QED).
% % %
% % However, drugs with high mutagenicity can increase toxicity risks,
% % which requires a delicate balance and thus,
% % simultaneous optimization of these properties.
% %
% Tasks in this category include
% simultaneous improvement of 
% \underline{B}BBP, \underline{D}RD2 and \underline{Q}ED (\BDQ), 
% \underline{B}BBP, \underline{D}RD2 and \underline{p}logP (\BDP),
% \underline{B}BBP, \underline{p}logP and \underline{Q}ED (\BPQ), 
% \underline{D}RD2, \underline{p}logP and \underline{Q}ED (\DPQ),
% \underline{B}BBP, \underline{D}RD2, \underline{p}logP and \underline{Q}ED (\BDPQ), 
% \underline{B}BBP, \underline{H}IA, \underline{M}utag and \underline{Q}ED (\BHMQ),
% \underline{B}BBP, \underline{D}RD2, \underline{M}utag and \underline{Q}ED (\BDMQ)
% and 
% \underline{B}BBP, \underline{M}utag, \underline{p}logP and \underline{Q}ED (\BMPQ).

% % \item \textbf{Oral Bioavailability Optimization:}
% % For an oral drug to be viable, it must be efficiently 
% % absorbed in the intestines 
% % while maintaining a favorable safety and developability profile. 
% % Poor absorption or excessive toxicity significantly limits a molecule’s clinical potential.
% % Tasks in this category include
% % HIA + Mutagenicity + plogP + QED,
% % BBBP + HIA + Mutagenicity + QED.



% \end{enumerate}

% Table~\ref{tbl:task_summary} summarizes the
% tasks considered for evaluation.
% A detailed breakdown of each task and its real-world significance
% are provided in Appendix~\ref{}.

%
% We construct a comprehensive dataset encompassing molecule pairs for six multi-objective optimization tasks and their constituent subtasks. \xiao{TO-DO: Explain the practical meaning of each task, and the data collection.}
% IND Tasks:
% \begin{enumerate}
% %\item Task 1: plogP + QED + Intestinal absorption + Mutagenicity: Optimizing oral drug candidates
% \item BBBP + DRD2 + QED
% \item BBBP + plogP + QED
% \item DRD2 + plogP + QED
% \item BBBP + DRD2 + plogP
% \item BBBP + DRD2 + plogP + QED
% \end{enumerate}

% OOD Tasks:
% \begin{enumerate}
% %
% \item Mutagenicity + plogP + QED:
% Focuses on optimizing drug-likeliness and toxicity.
% %
% \item Human Intestinal Absorption + Mutagenicity + plogP + QED: optimizing orally bioavailable drug without potential genetic damage
% %
% \item BBBP + Mutagenicity + plogP + QED: optimizing CNS drugs without potential genetic damage 
% %
% \item BBBP + DRD2 + Mutagenicity + QED: developing CNS drugs with better BBB penetration ability while reducing potential genetic damage and adverse effects.
% %
% \item BBBP + Human Intestinal Absorption + Mutagenicity + QED:
% optimizing orally available CNS drugs while ensuring BBB penetration and minimizing adverse effects.
% %
% \end{enumerate}



% %========================================%
% \subsection{\MOptData Pairs and Instructions}
% \label{sec:data:pairs}
% %========================================%

% \MOptData comprises a total of X pairs across 10 molecule optimization tasks
% involving X unique molecules.




%========================================%
\subsection{Creation of Task-Specific Training Pairs}
\label{sec:data:pairs:collect}
%========================================%


% We construct a high-quality dataset for multi-property molecule optimization
% by generating structurally similar molecule pairs $(M_x, M_y)$ 
% such that $M_x \prec_\delta M_y$ indicating that
% the optimized molecule
% $M_y$ has a substantial improvement (of at least $\delta$)
% in each property relative to $M_x$. 
% %
% We extract such pairs from the dataset provided by Chen et al.\cite{chen2021deep},
% where each pair differs at only one disconnection site 
% (i.e., $M_x$ can be optimized to $M_y$ by modifying one fragment
% at a single disconnection site).
% %
% Such pairs satisfying substantial property improvement constraints will
% enable models to learn non-trivial optimization reflecting
% realistic lead optimization tasks.



%
% that satisfy task-specific property constraints.
% % %
% % Such pairs satisfying substantial property improvement constraints will
% % enable models to learn non-trivial optimization reflecting
% % realistic lead optimization tasks.
We construct task-specific training pairs,
where each pair 
$(M_x, M_y)$ is sourced
from the dataset provided by \citet{chen2021deep},
which consists of 
255K molecule pairs dervied from 331K molecules.
% sourced from
% two widely-used molecule databases, 
% ZINC\cite{} and ChEMBL\cite{}.
%
Each pair
differs at only one disconnection site,
meaning $M_x$ can be transformed to $M_y$ by modifying exactly one fragment.
%
% Such pairs are well-suited for modeling
% realistic optimization scenarios
% because a single fragment modification
% retains the core scaffold -- a key requirement for 
% \emph{in vitro} optimization.
%
%In total, we collected
%which provide a diverse and representative chemical space.
%
%All pairs satisfy the similarity constraint with $\epsilon=0.6$.
%Furthermore, e
%%%
%%
{Among these molecule pairs,
we select those
that satisfy all $P$
property constraints for a given task optimizing $P$ properties
(i.e., $(M_x \prec_{\Delta_p} M_y)_{\forall p \in P}$).
This ensures that
the hit molecule $M_x$ in each pair
requires substantial optimizations, making
the selected pairs
suitable to model realistic optimization tasks.
}
%
% For each task aiming to improve multiple properties,
% a selected pair must simultaneously satisfy the property constraint for 
% each property.
%(i.e., $M_x \prec_\delta M_y$).
%
% We set property constraint ($\delta$) to 
% 1.0 for plogP, 0.1 for QED, HIA and Mutag,
% and 0.2 for BBBP and DRD2.
%
% \xiao{xiao: do we need to justify these $\delta$? Why those numbers are realistic?}
% For instance, if the optimization task
% aims to simultaneously improve plogP and mutagenicity,
% a pair $(M_x, M_y)$ is selected into the training set if 
% $M_y$'s plogP is at least $\Delta_\text{plogP}$ 
% higher than $M_x$
% and $M_y$'s Mutag is at least $\Delta_\text{Mutag}$
% lower than $M_x$.
% $\text{plogP}(M_y) - \text{plogP}(M_x) \ge 1.0$
% and $\text{Mutag}(M_x) - \text{Mutag}(M_y) \ge 0.1$.
%
% The threshold ($\Delta_p$) used for each property
% constraint is shown
% in Table~\ref{tbl:task_summary}.
% These constraints ensure that 
% the hit molecule $M_x$
% requires substantial optimizations,
% and thus
% the selected pairs 
% can be used to model realistic optimization tasks.
%
% Table~\ref{tbl:task_prop} presents training set statistics
% and characteristics.

%
% Specifically,
% %%
% $M_x \prec_{\Delta_t} M_y$ iff
% $\{p_i(M_y) - p_i(M_x) >= \delta_i ~~~\forall ~p_i \in \mathcal{P}^+\}$
% and $\{p_j(M_x) - p_j(M_y) >= \delta_j ~~~\forall ~p_j \in \mathcal{P}^-\}$,
%%
% where 
% $\mathcal{P}^+$ denotes the set of properties in $t$ that
% needs to be increased, while
% $\mathcal{P}^-$ denotes the set of properties in $t$ that
% needs to be decreased during optimization.
% %%

% %
% For example, for a task involving only BBBP,
% we consider $M_y \prec_{\Delta} M_x$ if the BBBP score of $M_y$
% is at least $\delta = 0.2$ higher than that of $M_x$ (i.e., $\text{BBBP}(M_y) - \text{BBBP}(M_x) >= 0.2$).
% On the other hand, for a task involving BBBP and mutagenicity,
% we consider $M_y \prec_{\Delta} M_x$ if $M_y$ simultaneously
% has a mutagenicity score of at least $\delta = 0.1$ lesser than $M_x$
% (i.e., $\text{Mutag}(M_x) - \text{Mutag}(M_y) >= 0.1$)
% and has a BBBP score that is at least 0.2 higher than $M_x$ as above.
%
% To construct tasks in \MOptData,
% $\delta$ is set to 
% 1.0 for plogP, 0.1 for QED, HIA and Mutag,
% and 0.2 for BBBP and DRD2.
% %
% These thresholds are chosen to eliminate trivial modifications,
% ensuring that selected pairs reflect realistic optimization challenges.

% To ensure pharmacological relevance, 
% we enforce directional constraints for each property
% where
% higher values of BBBP, DRD2, plogP, and QED
% and lower values of mutagenicity are preferable.
%
% In essence,
% we identify molecule pairs $(M_x, M_y)$
% such that $M_x$ is structurally modified to $M_y$ at only one disconnection site,
% and $M_y$ exhibits an improved property over $M_x$ while
% maintaining a high similarity.


% Following prior work\cite{}, 
% we set the Tanimoto similarity threshold to 0.6, 
% ensuring that the structural modifications 
% do not drastically alter the molecular scaffold
% -- a key requirement in practical molecular optimization.
%
% Additionally,
% we impose minimum property improvement thresholds for each property to 
% retain only substantive optimizations. 
% %
% Specifically, a candidate molecule pair is retained 
% if the optimized molecule exhibits an increased value 
% of at least 0.2 each for BBBP and DRD2, 
% 1.0 for plogP, 0.1 for QED and HIA, 
% while a decrease of at least 0.1 for mutagenicity. 
%

%

% In summary, for each multi-property optimization task, we extract 
% molecule pairs that simultaneously satisfy these 
% improvement criteria across all involved properties in that task. 
%
% This results in a well-curated dataset of pairs
% that serve as an effective training resource for instruction-tuned LLMs.
%
% Table~\ref{tbl:task_prop} and \ref{tbl:task_prop_all} present an overview
% of the tasks considered for evaluation and for training generalist models,
% respectively.


%========================================%
\subsection{Creation of Task-Specific Test Set}
\label{sec:data:test}
%========================================%

%To ensure a rigorous evaluation,
We construct a test set 
% containing 
% at most 500 hit molecules for each task.
%
%To achieve this, we 
by randomly sampling an initial pool of 
250K molecules from the ZINC database~\cite{sterling2015zinc}
-- a collection of commercially available drug-like molecules --
that are not included in the training set.
% This ensures that the test set includes
% molecules that have drug-like characteristics.
% %
Out of this pool,
we select a molecule into the test set of a task
which has a property worse than 
the median among
all $M_x$ in the training pairs 
(i.e., median property scores of $M_x$ denoted as \MPSTr)
for each desired property.
% only if it satisfies the property threshold ($\theta$)
% for each property in the task. 
% \vishal{Specifically, for properties where higher (or lower) values are desirable, a test molecule must have a value below (or above) $\theta$.
% %
% For each task,
% we set $\theta$ to the median value of each property among
% all hit molecules $M_x$ in the training set of that task.
%
This %median-based criterion
%denoted as {\MPS} in Table~\ref{tbl:task_prop},
provides a task-specific, data-driven selection criteria
that is robust to outliers.
%
Additional criteria to exclude outliers are detailed in Appendix~\ref{sec:app:filter}.
%
After applying these steps to the initial pool of 250K molecules,
we randomly select at most 500 molecules into the test set for each task, with possible overlap across tasks.
Table~\ref{tbl:task_prop} presents the task-specific data set characteristics.

% This median-based thresholding
% ensures that the test set represents \emph{in silico}
% optimization scenarios while excluding extreme cases.
% }
%that require substantial optimization.
%thus making the optimization task non-trivial and realistic.
%
%


%
% In addition, the right-skewed distribution of DRD2 values
% results in a median close to zero.
% To address this, 
% we set $\theta$ to 0.1.
%

%



%========================================%
\subsection{Quality Control}
%========================================%
%%
We implement multiple quality control measures as detailed in
Appendix~\ref{sec:app:quality}.
%
We remove duplicate molecules based on
canonicalized SMILES strings.
For each molecule, we compute
empirical property scores using
well-established tools:
ADMET-AI~\cite{swanson2024admet}
and the official implementation provided by \citet{you2018graph}.
%
Additionally, we provide 6 distinctly phrased (i.e., diverse) instructions for each task (Appendix~\ref{sec:app:instr}).
%
To evaluate LLMs' instruction understanding and generalizability
to unseen instructions, 
we hold out one instruction for each task during training.
% Additional details on quality control are provided in Appendix~\ref{sec:app:quality}.




%========================================%
\subsection{IND and OOD Tasks}
\label{sec:data:ind_ood}
%========================================%
%%
To distinctly assess the capabilities of instruction-tuned LLMs 
on both familiar
and novel optimization tasks,
we categorize our tasks into two groups:
%

\paragraph{In-Distribution (IND) tasks:}
IND tasks only have property combinations that
are used during training.
%
Success in these tasks provides insights
on the models' ability to handle optimizations
they are specifically trained on.
%
% Each IND task has a training set of about 2k-4k pairs 
% (except for \BDPQ, which has 624 pairs),
% and a test set of 500 molecules.
% IND tasks serve as a baseline for measuring the model’s ability to generate optimized molecules when it has access to relevant property interactions during training. 
%%

\paragraph{Out-of-Distribution (OOD) tasks:}
%
OOD tasks 
include novel property combinations
that are not used during training.
%
Note that OOD is defined in terms of property combinations
and not individual properties,
that is, each property
is included
as part of training tasks.
%
OOD tasks evaluate the models' ability to generalize
to novel optimization tasks without task-specific retraining.
This generalizability
is crucial in real-world lead optimization, 
where new therapeutic requirements frequently arise.
%
%
% By evaluating models on both IND and OOD tasks, 
% we quantify their robustness, identifying whether LLMs 
% can effectively generalize to familiar and novel tasks
% in molecule optimization. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\mollm Models}
\label{sec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Traditional approaches, such as genetic algorithm-based molecule generation methods 
% or gradient-guided search, 
% often struggle with multi-objective trade-offs, scaffold retention, and generalization to diverse optimization tasks. 
% %
% To overcome these challenges,
We introduce \mollm, a series of general-purpose LLMs
instruction-tuned over \MOptData.
% Each training instance to \mollm consists of a molecule pair,
% along with a natural language instruction specifying the optimization task. 
% by following a well-curated dataset of an input molecule, a textual instruction specifying the desired optimization objective, and the corresponding optimized molecule.
%
% Unlike prior methods that directly generate novel molecules, 
% \mollm learns to edit a given molecule,
% formulating optimization as a guided transformation task.
%
% Through instruction tuning, \mollm learns to 
% map the task specification to
% effective structural modifications required
% to improve desired properties
% while preserving structural similarity.
%
Through instruction tuning, 
\mollm implicitly learns chemical semantics,
structure-property relationships (SPR)~\cite{hansch1969quantitative}
and associations between 
structural differences expressed in molecule pairs and
the desired property improvement
expressed via natural language instruction.
%
\mollm applies this knowledge
to perform structural modifications on a given molecule
and generate
better molecules with improved
properties. 
%
% This
% enables the model to reason about molecular optimization in a way that generalizes across 
% different property combinations and thus, to novel tasks.
%
% Instruction-tuning 
% enables \mollm to implicitly learn chemical semantics,
% %
% structure-property relationships (SPR)\cite{hansch1969quantitative}
% %
% %This allows tuned LLMs to make 
% and structural modifications to improve multiple properties simultaneously.
%
% By representing molecules as SMILES strings and including them within task-specific natural language instructions, 
% \mollm enables the LLM to interpret chemical structures and
% optimization task in a unified manner.
%%

We develop both task-specific and generalist {\mollm}s. 
%models to evaluate the trade-off between specialization and generalization.
%
Task-specific models
are trained on a single optimization task, 
and thus benefit from dedicated training tailored to that specific task. 
%often demonstrating strong performance. 
%
In contrast, generalist models are trained across 
multiple optimization tasks simultaneously.
%
This multi-task training enables cross-task knowledge transfer,
allowing the generalist \mollm to leverage shared
chemical knowledge on SPR
and multi-property trade-offs across all possible property combinations.
%
Thus, the generalist \mollm
represents a step toward a foundational model for molecule optimization,
capable of handling diverse tasks without task-specific retraining.
% 

%
% In particular, \mollmSixGen leverages all six properties but excludes the 5 OOD tasks 
% (Table~\ref{tbl:task_summary}) during training.
% %
% This allows us to evaluate how well 
% \mollmSixGen generalizes to novel property combinations
% (i.e., OOD tasks),
% thereby quantifying its robustness and generalizability.

%%
We develop a series of generalist {\mollm}s
trained on the power sets of 3, 4, and 6 properties,
denoted as \mollmTripleGen, \mollmQuadGen, and \mollmSixGen,
respectively.
%
To train these models, we fine-tune 2 general-purpose LLMs: 
Mistral-7B-Instruct-v0.3~\cite{mistral2023mistral} and Llama3.1-8B-Instruct~\cite{grattafiori2024llama3herdmodels}
by applying LoRA~\cite{hu2022lora}
adapters to all projection layers and the language modeling head. 
% %
% leveraging the Huggingface Transformers library~\cite{wolf2020transformers}. 
% %
% We fine-tune all models with a learning rate of 1e-4 and a batch size of 128, using a cosine learning rate scheduler with a 5\% warm-up period. 
% %
% We fine-tune task-specific {\mollm}s 
% and generalist {\mollm}s for 10 and 3 epochs, respectively,
% to balance efficiency and overfitting. 
% %
% We set LoRA parameters with $\alpha=16$ and a rank of 16,
% and apply LoRA adapters to all projection layers and the language modeling head. 
We perform 0-shot evaluations
(i.e., without in-context examples) for all {\mollm}s 
in all tasks.
For each test molecule, we generate 20 molecules 
via beam search decoding, with the number of beams set to 20.
Training details are provided
in Appendix~\ref{sec:app:reproducibility}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Setup}
\label{sec:expt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%========================================%
\subsection{Baselines}
\label{sec:expt:baselines}
%========================================%

We compare {\mollm}s against 3 categories of baseline models: 
\textbf{(1)} general-purpose LLMs: Mistral-7B Instruct-v0.3~\cite{mistral2023mistral}, 
Llama-3.1 8B-Instruct~\cite{touvron2023llama}, 
and Claude-3.5;
\textbf{(2)} foundational LLMs for chemistry: \LlaSMol tuned on Mistral-7B, denoted as \LlaSMolM~\cite{yu2024llasmol},
and \textbf{(3)} task-specific non-LLMs: \PMol~\cite{wu2024leveraging}.
%, with detailed setup provided in Appendidx~\ref{sec:app:pmol}.
%
Similarly to {\mollm}s, we generate 20 molecules for each input molecule for all baselines. 
%
For LLM baselines, we use the same generation strategy
as for {\mollm}s.
%
Experimental setups are detailed in Appendix~\ref{sec:app:expts_setup:baselines}.
Prompt templates for LLMs are in Appendix~\ref{sec:app:prompt}.
%
Discussion on \PMol and DeepSeek-R1 are in Appendix~\ref{sec:app:baselines} and \ref{sec:app:deepseek}, respectively.

%========================================%
\subsection{Evaluation Metrics}
\label{sec:expt:eval}
%========================================%

We employ multiple evaluation metrics 
(detailed in Appendix~\ref{sec:app:eval})
for a holistic comparison.
For brevity and clarity,
%in the following sections,
we present the results only in terms of:
%%
\textbf{(1) Success Rate} ({\SR})
which is the proportion of input molecules that
are successfully optimized with improvement in all
desired properties;
%%
\textbf{(2) Similarity with input} ({\Sim})
which denotes the average Tanimoto similarity~\cite{Bajusz2015why} between the
optimized and the corresponding input molecule;
and 
%%
\textbf{(3) Relative Improvement} ({\RI})
representing the average improvement in each desired property
relative to its initial value in the input molecule.
%averaged across all properties.
Higher {\SR}, {\Sim}, and {\RI} are desirable,
indicating more successful optimizations.
% with minimal structural modifications and substantial property improvements.
%
% Details on these metrics are provided in Appendix~\ref{sec:app:eval}.


\input{tables/main_ind.tex}
%\ref{tbl:main_ind}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%
\paragraph{Main Findings:} 
%%
Our experiments reveal the following findings:
%%
\textbf{(1)} Both task-specific and generalist 
{\mollm}s consistently outperform 
general-purpose LLMs, foundational LLMs for chemistry, 
and task-specific non-LLMs across all IND (Section~\ref{sec:results:ind}) and OOD tasks (Section~\ref{sec:results:ood}), 
significantly improving {\SR} by as much as 186.6\% over the best baseline.
%
%The best-performing {\mollm} achieves average X\% improvement on {{\SR}} over the best-performing baseline on the ten tasks. 
%
% Justification: This consistent superiority validates the effectiveness of our instruction tuning on pairs in enhancing molecular optimization capabilities beyond existing methods.
%
%%
\textbf{(2)} Compared to task-specific {\mollm}s, 
generalist {\mollm}s excel on 3 out of 5 IND tasks and
demonstrate competitive performance on the other 2 tasks,
with remarkable gains of 91.3\% in
{\SR} on more complex tasks such as \BDPQ 
(Section~\ref{sec:results:ind}).
% on more complex four-property tasks than three-property tasks (exhibiting almost a 2x improvement in {\SR} over the best-performing baseline). 
%%
{\textbf{(3)} 
% By training on diverse tasks
% optimizing combinations of all 6 properties,
Generalist \mollmSixGen models
exhibit strong 0-shot generalization to unseen (OOD) tasks and unseen instructions, 
significantly outperforming powerful general-purpose LLMs (Section~\ref{sec:results:ood}).}
%%
%%
\textbf{(4)} All {\mollm}s substantially outperform
the best general-purpose LLM, Claude-3.5 (5-shot) and foundational
LLM for chemistry, \LlaSMolM,
across all IND and OOD tasks.
%%
% \textbf{(5)} Generalist {\mollm}s demonstrate 
% strong generalizability to unseen instructions (Section~\ref{sec:results:uninst}), 
% with only a negligible drop in performance, still outperforming all baselines
% by a large margin.
%%
% optimization success and molecular similarity (average {{\SR}}=X\% and {{\Sim}}=X\% across ten tasks).
% Justification: in stark contrast to Mistral (0-shot) which maintains high similarity (X\%) but achieves very low success rates (X\%). This suggests that {\mollm} effectively balances property optimization while decently preserving molecular structure, which is xxx.
%
%
% \textbf{(4)} {\mollm} demonstrates superior property improvement, 
% achieving an average {{\RI}} of X\% across X\% successfully optimized molecules on ten tasks. 
% Justfication: This contrasts significantly with \PMol, which shows high property improvements ({\RI}=X\%) but only in a small fraction (X\%) of successful cases, indicating that {\mollm} can consistently achieve substantial property improvements across a broader range of molecules, which is a more reliable optimizer for usage.
%
%

%This enhanced performance on more complex tasks demonstrates that generalist {\mollm} is particularly effective for multi-objective optimization, where existing methods often more struggle.
%
%
% \textbf{(5)} Generalist {\mollm}s maintain competitive or even superior performance compared to task-specific {\mollm}s (section~\ref{sec:results:ind}). 
%This suggests that a single foundation model can be generalized across diverse molecular optimization tasks. This capability is ideal for real-world applications, where having one flexible yet reliable model is preferable to training and maintaining multiple task-specific models.
%%
%

%This zero-shot capability is crucial for practical applications where new property combinations frequently emerge, and generalist {\mollm}s can provide reliable optimization without requiring additional training.


%\begin{enumerate}
%\item First: As shown in Table~\ref{tbl:main_ind} and Table~\ref{tbl:main_ood}, {\mollm}s demonstrate superior performance, consistently outperforming both general-purpose LLMs, foundational LLM for chemistry, and domain-specific non-LLMs across all IND and OOD tasks in terms of {{\SR}}, with the average {{\SR}} improvement over the second best-performing baseline on IND tasks, and X\% on OOD tasks.
%
%This consistent superiority validates the effectiveness of our instruction tuning on pairs in enhancing molecular optimization capabilities beyond existing methods.
%
%Compard to the baselines, {\mollm}s strike a decent balance with achieving high {{\SR}} while retaining decent {{\Sim}} and {{\RI}}, these results suggest {\mollm}s 

%
%\item Second: Generalist {\mollm}s achieve even better performance improvement on four-property tasks than three-property tasks (exhibiting almost a 2x improvement in {\SR}). This enhanced performance on more complex tasks demonstrates that generalist {\mollm} is particularly effective for multi-objective optimization, where existing methods often struggle.
%
%\item Third: Compared to task-specific models, generalist {\mollm}s maintain competitive or superior performance, suggesting that a single foundation model can generalize across diverse molecular optimization tasks. This capability is ideal for real-world applications, where having one flexible yet reliable model is preferable to training and maintaining multiple task-specific models.
%
%\item Fourth: Generalist {\mollm}s exhibit strong zero-shot generalization to unseen property combinations in OOD tasks, significantly outperforming few-shot baselines and demonstrating greater generalizability and robustness. This zero-shot capability is crucial for practical applications where new property combinations frequently emerge, and generalist {\mollm}s can provide reliable optimization without requiring additional training.
%\end{enumerate}


%
%\xiao{Move specific comparison to the subsections}
%\item With extremely low {{\SR}} and {{\Sim}}, \PMol achieves the highest {{\APS}},
%which indicates that even though the very few optimized molecules have very good properties, 
%they are structurally almost entirely different from the input molecule.
%On the other hand, {\mollm}s 
%achieve meaningful property improvements while ensuring more realistic structural modifications rather than excessive scaffold alterations.


%\item {\mollm}s significantly outperform few-shot open-source 
%general-purpose LLMs, 
%which often suffer from memorization (low {\Nov}). 
%Prompted models frequently replicate exemplar molecules rather 
%than generating novel and diverse optimized ones.


%\end{enumerate}

%========================================%
\subsection{IND Evaluation}
\label{sec:results:ind}
%========================================%




Table~\ref{tbl:main_ind} shows the overall performance
of {\mollm}s and baselines across all 5 IND tasks.
Detailed results for each task are in Appendix~\ref{sec:app:results:ind}.


%---------------------------------------------------%
\paragraph{Overall Comparison:} 
%---------------------------------------------------%
%%
%Overall, 
%our instruction-tuned {\mollm}s,
Both task-specific and generalist {\mollm}s
significantly outperform all baselines across all IND tasks.
%
Specifically, the generalist {\mollm}s,
\mollmQuadGenM and \mollmSixGenM,
achieve an average {\SR} of 76.8\% and 76.1\%, respectively, across all 5 tasks --
outperforming the best baseline by 113.2\% and 108.8\% on average.
%
This is due to the ability of generalist {\mollm}s 
to leverage knowledge synergistically by optimizing different property combinations, 
thereby effectively capturing shared chemical principles and property trade-offs.
%
% Task-specific {\mollm}s, specifically \mollmTripleTaskL, achieve the best performance
% on 2 IND tasks (\BDQ, \BDP) optimizing highly specialized properties such as DRD2.
% This demonstrates the task-specific {\mollm}s' ability to learn precise and targeted optimization
% strategies.
%
On the most challenging task, \BDPQ with 4 properties,
the generalist {\mollm} outperforms all baselines, task-specific {\mollm}s
and non-LLMs
by as much as 176\% in terms of {\SR}, 
showcasing its strong ability to tackle complex tasks 
with limited training data.
%
Detailed comparison between {\mollm}s and task-specific non-LLMs
are provided in Appendix~\ref{sec:app:results:ind}.


%
% The generalist \mollm demonstrates very strong performance,
% surpassing the best baselines across three property tasks with X\% improvement. 
% This advantage becomes more pronounced in the four-property task (\BDPQ), where the performance gap widens to X\%.
%
% These results validate {\mollm}'s effectiveness in multi-property optimization tasks 
% through instruction tuning. 
% Notably, in the challenging scenarios where the number of target properties increases and training pairs become limited, the generalist model's advantages become particularly evident due to its ability to transfer knowledge from related tasks.
%
%Compared to all the general-purpose LLMs, foundational LLM for chemistry, and task-specific non-LLM models, \mollm generates more valid modifications with property improvements to achieve superior optimization outcomes by leveraging specialized training on molecular transformation pairs.

%


%---------------------------------------------------%
\paragraph{Comparison between task-specific and generalist {\mollm}s:} 
%---------------------------------------------------%
%%
As shown in Table~\ref{tbl:main_ind},
generalist {\mollm}s 
outperform task-specific {\mollm}s on 3 out of 5 IND tasks,
particularly with remarkable gains on more complex task \BDPQ.
%
On \BDPQ 
%(624 training pairs),
the generalist {\mollmQuadGenM} achieves a substantial
improvement of 91.3\% in {\SR} over the best task-specific 
{\mollmQuadTaskM}.
% This is likely due to the fact that \BDPQ 
% is a more complicated task that requires
% %improving four, potentially conflicting properties.
% improving and balancing four properties.
This is likely due to {\BDPQ}'s complexity in balancing four properties.
%
By leveraging data from other tasks, the generalist {\mollm}s
can better capture the nuanced property trade-offs
that task-specific {\mollm}s 
%will struggle to capture 
can not with limited task-specific training data.
%(e.g., 624 pairs in \BDPQ).


%
On the other hand, task-specific {\mollm}s outperform
generalist ones on 2 IND tasks
(\BDP and \BDQ),
where
the properties BBBP and DRD2 are positively correlated
(Pearson correlation of 0.6),
and there is sufficient training data (2,064 pairs in \BDP and 4,472 in \BDQ).
%
Notably, DRD2 targets the inhibition of a specific receptor, representing a more specialized therapeutic objective than fundamental molecular properties like pLogP or QED.
%
Therefore, by focusing exclusively on such highly correlated
properties and specific therapeutic requirements,
task-specific {\mollm}s learn
more targeted structural modifications for each task.
Nonetheless, this only leads to modest improvements of 4.6\% and 4.9\%
over the generalist {\mollmSixGenM}.
%
These results collectively highlight 
the complementary strengths of generalist and task-specific {\mollm}s, 
with the generalist \mollmSixGenM offering a more scalable and foundational model for diverse optimization tasks.


%---------------------------------------------------%
\paragraph{Comparison between {\mollm}s and general-purpose LLMs:} 
%---------------------------------------------------%
%%

All {\mollm}s substantially
outperform general-purpose LLMs by a large margin.
For example, across 5 IND tasks,
{\mollmQuadGenM} and {\mollmSixGenM}
achieve a significant average improvement of 
128.1\% and 124.0\% in {{\SR}}
over the best general-purpose LLM baseline, Claude-3.5 (5-shot).
%
This remarkable performance can be attributed to
the instruction tuning of \mollm on molecule pairs, 
enabling it to learn
modification strategies that general-purpose LLMs fail to acquire through in-context learning.
%
General-purpose LLMs,
particularly with 0-shot prompting, 
exhibit extremely low {\SR} and relatively high {\Sim},
{meaning} that the very few optimized
molecules are highly similar to the input.
%
In contrast, {\mollm}s achieve substantially higher {\SR} and {\RI},
demonstrating more successful optimizations
while maintaining reasonable similarity 
(e.g., {\Sim} in [0.5, 0.6]).





\paragraph{Comparison between {\mollm}s and foundational LLM for chemistry:} 
%
All {\mollm}s demonstrate significant improvement over
the state-of-the-art foundational LLM for chemistry, \LlaSMolM, on all IND tasks.
%
For example, \mollmSixGenM outperforms \LlaSMolM significantly by
186.6\% on \BDQ, %9.3\% on \BPQ 
and 99.1\% on \BDP,
%
with up to a 5-fold relative improvement 
(e.g., {\RI} of 508.6 on \BDQ)
in desired properties.
%
Note that \LlaSMol was not instruction-tuned on 
molecule optimization tasks.
%
Thus, the performance gap 
suggests that the pre-trained chemistry knowledge in foundational LLMs, such as \LlaSMol,
is not sufficient to solve tasks as specialized as molecule optimization.
% 
This highlights the importance of instruction tuning on \MOptData 
for acquiring more specialized knowledge.
% These results indicate \mollm's superior ability in making targeted and controlled molecular modifications for property improvement, highlighting the advantage of specialized instruction tuning for molecular optimization over general chemical task instruction tuning.

%
%


% % \vishal{A table only showing success rates with and without similarity constraint for each IND task. Put the entire table as in the slides.}


\input{tables/main_ood}
%\ref{tbl:main_ood}



%========================================%
\subsection{OOD Evaluation}
\label{sec:results:ood}
%========================================%


Table~\ref{tbl:main_ood} presents the overall performance of {\mollm}s
and baselines
across all 5 OOD tasks
(with detailed results in Appendix~\ref{sec:app:results:ood}).
%
Note that OOD tasks involve novel property combinations excluded from training, making task-specific models and comparisons with
\mollmTripleGen and \mollmQuadGen
inapplicable.
%
%
%
Generalist {\mollm}s demonstrate robust 0-shot generalization to OOD tasks, 
significantly outperforming all baselines. 
%%
For instance, both \mollmSixGenM and \mollmSixGenL
achieve very high {\SR} of 88.7\% and 90.8\%, respectively,
on average across all 5 tasks --
outperforming strong baselines such as Claude-3.5 (5-shot) and \LlaSMolM
by as much as 159.9\% on task \BDMQ.
%
% Such superior generalization of \mollmSixGen arises from
% its ability to transfer knowledge from diverse training tasks and 
% to leverage potential correlations between molecular properties.
% %
%
%
% For example, although \mollmSixGen was not explicitly tuned to
% improve BBBP, HIA, Mutag and QED simultaneously (i.e., on \BHMQ task),
% it can leverage knowledge from other training tasks such as
% \DHMQ
% since BBBP and DRD2 are highly correlated.
%
By learning optimization strategies and property trade-offs
across diverse tasks during training, 
generalist {\mollm}s develop a flexible understanding of modification strategies that can generalize to novel optimization tasks.
% %
This generalizability is crucial in practice, where 
the dynamic nature of therapeutic requirements requires one unified foundational model capable of handling novel and diverse optimization tasks
without task-specific retraining.

%
% \paragraph{Comparison between \mollm and chemical LLMs:} 
% Furthermore, generalist {\mollm}s consistently outperform 
% \LlaSMolM in {{\SR}} and {{\RI}} across all OOD tasks. 
% %
% While \LlaSMolM maintains slightly higher {{\Sim}} on 4 out of 5 OOD tasks, 
% \mollm achieves substantially better property improvements with controlled structural modifications, demonstrating a favorable trade-off between similarity and optimization effectiveness.
 
%

% \paragraph{Comparison between \mollm and general-purpose LLMs:} 
% While Claude3.5 (5-shot) performs best among general-purpose LLMs, generalist \mollm consistently achieves better {{\SR}} and {{\Val}}. 
% %
% 
% %
% On \BMPQ, \HMPQ, and \BHMQ tasks, \mollm maintains better structural similarity while showing generally comparable property improvements, with moderate trade-offs in specific properties.



% %----------------------------------------%
% \subsubsection{OOD-C: Unseen property combination}
% %----------------------------------------%
%     \begin{itemize}
%         \item Compare generalist and task-specific instruction-tuned LLM with Prompt-MolOpt
%         \item Prompt-MolOpt can run in this setting
%     \end{itemize}

% %----------------------------------------%
% \subsubsection{An unseen combination of preferences for a seen property combination}
% %----------------------------------------%
%     \begin{itemize}
%         \item Compare generalist and task-specific instruction-tuned LLMs with all baselines
%         \item For domain-specific models, an unseen preference means changing the weights in the aggregated objective or changing the Pareto front, so no need to retrain.
%     \end{itemize}



% %========================================%
% \subsection{Comparison with the Domain-specific models}
% %========================================%
% \begin{itemize}
%     \item IND tasks: Performing well on SR but low Similarity -> Performing worse when adding some Similarty constraints -> Related this issue to the practical usage of optimizing molecules (requires the generating molecules to be similar)
%     \item OOD tasks: Not able to perform or low performance -> Highly rely on the training data, not flexible when practical usage (generalist model is more needed).
%     \item Modeling-wise difficulty
% \end{itemize}

% %========================================%
% \subsection{Comparison with the General-purpose LLMs}
% %========================================%
% \begin{itemize}
%     \item Performance gain, why? Any claims?
% \end{itemize}
% %========================================%
% \subsection{Comparison with the Chemical LLMs}
% \begin{itemize}
%     \item Performance gain, why? Any claims?
% \end{itemize}
% %========================================%

% %========================================%
% \subsection{Generalist vs Task-specific \mollm Models}
% %========================================%

% A table with all the task-specific and generalist \mollm models.
% Describe the major trends and put the detailed results in the Appendix.


%========================================%
\subsection{Generalizability to Unseen Instructions}
\label{sec:results:uninst}
%========================================%
% \begin{itemize}
%     %\item For IND tasks, compare how a \mollm model instruction tuned on 1 instruction compare with \mollm model tuned on diverse instructions
%     \item Test \mollm models on a hold-out instruction and with a different property name unseen during training to compare how they generalize to unseen prompts
% \end{itemize}

\input{tables/main_uninst}
%\ref{tbl:main_uninst}



Table~\ref{tbl:main_uninst} presents the performance of task-specific {\mollm}s
and generalist model, {\mollmSixGen}, when prompted
with a hold-out instruction and unseen property names (Appendix~\ref{sec:app:instr}).
%
Overall, task-specific {\mollm}s retain their performance across all tasks,
while, generalist {\mollm}s exhibit a slight
drop of 7\% in {\SR} on average.
%
This minor drop is expected, 
since generalist {\mollm}s trained on more property combinations, 
encounter the same property names more frequently during instruction-tuning.
This may lead to subtle overfitting to specific names.
%
Importantly, even with this minor performance drop, 
generalist {\mollm}s still outperform all baselines by a large margin, 
(Section~\ref{sec:results:ind}), 
highlighting their overall superiority.
%
% Such generalizability to unseen instructions and property names 
% is crucial in practice, 
% where the same task can be often 
% described with diverse terminologies across users and contexts.
Detailed results are provided in Appendix~\ref{sec:app:results:uninst}.





%========================================%
\subsection{Case Studies}
\label{sec:results:case}
%========================================%

%%
\input{figures/BHMQ_case.tex}
%%


%We conduct case studies to investigate \mollm in greater detail.
%
Figure~\ref{fig:case_BHMQ} shows a
successful optimization for the OOD task \BHMQ,
where \mollmSixGenM improves all desired properties by 
replacing the sugar moiety in $M_x$ 
with a nitrogen-containing heterocycle in $M_y$ (highlighted fragments).
%resembling pyridine. 
%
%In $M_x$, 
The sugar moiety, with multiple hydroxyl (-OH) groups,
increases polarity and hydrogen bonding, 
limiting passive permeability and leading to low BBBP and HIA~\cite{goetz2017relationship, mullard2018re}.
%
Replacing this fragment with a nitrogen heterocycle
%in $M_y$
reduces polarity and hydrogen bonding,
leading to improved BBBP (+0.74) and HIA (+0.56).
%
Moreover, hydroxyl-rich sugars in $M_x$ are 
prone to oxidation and glycation,
compromising stability and bioavailability~\cite{twarda2022advanced, chen2024advanced}. 
%
In contrast, the nitrogen heterocycle in $M_y$ 
is a well-known motif for improving metabolic stability and bioavailability~\cite{kerru2020review, ebenezer2022overview},
leading to significant improvements in mutagenicity (-0.30) and QED (+0.37).
%
In contrast, \LlaSMolM retains the sugar moiety and
instead removes a phenol group (Figure~\ref{fig:case_BHMQ_appendix}), 
%leaving polarity and hydrogen bonding largely unaffected, 
resulting in limited improvements.
Additional cases are in Appendix~\ref{sec:app:results:additional_cases}.
%\input{figures/BHMQ_case.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work, we introduced \MOptData, the first high-quality 
instruction-tuning dataset specifically focused on challenging
multi-property optimization tasks.
%
Leveraging \MOptData, 
%we developed 
%{\mollm}s
%that achieved 
{\mollm}s achieve state-of-the-art performance across all IND and OOD tasks,
%outperforming strong general-purpose LLMs and foundational LLMs for chemistry by as much as XX\% in {\SR}.
notably outperforming strong general-purpose LLMs and foundational LLMs for chemistry.
%
Generalist {\mollm}s demonstrated remarkable generalization to unseen tasks with an average {\SR} of 90.9\%, making them
promising candidates for foundational models in molecule optimization.
%
This highlights the potential of {\mollm}s
to adapt to diverse optimization tasks mirroring dynamic
therapeutic requirements.
%



%\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Despite the strong performance of {\mollm}s as demonstrated in our work,
we acknowledge several limitations.
\textbf{(1)}
We did not explore scenarios 
where users specify precise property-specific 
improvement thresholds during inference, 
which could enhance the applicability of {\mollm}s 
for highly customized therapeutic needs.
%
\textbf{(2)}
Our evaluations are limited to single-step optimization.
We did not explore iterative refinement of generated molecules that could yield even better lead molecules over multiple steps.
%
\textbf{(3)}
\MOptData leverages empirical property scores that
are not experimentally validated,
which may impact the accuracy of optimization outcomes.
%
\textbf{(4)}
\MOptData encompasses 6 molecular properties 
that play a critical role in successful drug design. 
However, real-world lead optimization often involves additional,
more specialized properties and complex trade-offs
depending on specific therapeutic requirements.
%
Addressing these limitations in future work could enhance
{\mollm}s' applicability in practice.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact Statement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%

Our work introduces the first large-scale, high-quality
instruction-tuning dataset, \MOptData, specifically focused on
molecule optimization tasks improving at least 3 properties simultaneously.
%
By leveraging \MOptData, 
we developed a series of instruction-tuned LLMs ({\mollm}s).
These models
significantly outperform strong closed-source LLMs 
such as Claude-3.5 as well as foundational LLMs for chemistry
on complex multi-property optimization tasks.
%
To the best of our knowledge, our work is the first to 
introduce a generalist model training framework and a foundational model for molecule optimization.
%
Notably, the robust zero-shot performance of our generalist {\mollm}s demonstrates their potential
as foundational models for molecule optimization, 
offering scalability and adaptability to diverse optimization scenarios. 
%


%%%
\paragraph{Broader Impacts:}
%%
The introduction of foundational models 
capable of handling diverse optimization tasks holds
tremendous potential to accelerate drug discovery pipelines.
%
These models offer unparalleled flexibility and scalability, enabling practitioners
to adapt them to a wide range of therapeutic requirements
without requiring resource-intensive training.
%
By relying solely on an efficient inference process,
such models democratize access to advanced optimization capabilities to a broader range of practitioners. 
%
This advancement could streamline the identification of 
novel drug candidates, 
significantly reducing the cost and time required to develop a new drug. 
%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ethics Statement}
\label{sec:ethics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While \MOptData has been carefully curated
to include drug-like, commercially accessible molecules, 
we cannot guarantee that the dataset is entirely free from inaccuracies
or harmful content.
%
We also cannot eliminate
the potential of our tuned {\mollm}s to generate undesirable or harmful content (e.g., lethal drugs).
%
We should emphasize that our models are specifically tuned to 
improve widely used molecule properties aligned with 
general drug discovery goals,
and are not intended for generating toxic or lethal molecules. 
%

% Specifically, the properties we optimize 
% are widely used in drug discovery and aim to improve the
% pharmacological profile of drug candidates. 
%
The only property in \MOptData that is related to toxicity
is mutagenicity, which measures the risk of DNA mutations. 
% pertains specifically to pharmacological side effects,
% such as mutagenicity,
% which involves minimizing the risk of DNA mutations.
% %
% This is different from lethal toxicity, which directly causes fatal harm. 
Importantly, our models are tuned explicitly to reduce mutagenicity,
and not to increase it.
%
% Additionally, our training data consists of drug-like,
% commercially accessible molecules.
Furthermore, \mollm models are tuned exclusively on drug-like molecules and optimization objectives aimed at reducing mutagenicity.
%
As a result, they are unlikely to generate molecules with increased
toxicity or molecules that can be lethal under a normal dosage.
%


However, if such molecules can be generated with adversarial prompts, 
this could potentially arise from the pretrained knowledge of the base models, 
which includes broader chemical information outside the scope of \MOptData and our instruction-tuning.
%
To mitigate such risks, safeguards such as usage monitoring, and integration with toxicity prediction pipelines should be implemented when deploying these models. 
%
Users of our dataset and models are expected to uphold the 
highest ethical standards and
incorporate robust validation pipelines
to prevent misuse. 
%


% \paragraph{Ethical Considerations:}
% %%
% While this work holds promise for advancing drug discovery, 
% there are important ethical implications to consider. 
% %
% One significant concern is the potential misuse of the proposed framework, 
% such as generating lethal molecules or toxic compounds with harmful intent. 
% %
% % We emphasize that
% % our work focuses on improving widely used molecule properties,
% % with each task aiming to improve the
% % pharmacological profile (efficacy, bioavailability, toxicity). 
% % %
% % The properties we optimize 
% % are widely used in drug discovery and aim to improve the
% % pharmacological profile of drug candidates. 
% %
% We emphasize that 
% the toxicity considered in this work 
% pertains specifically to pharmacological side effects,
% such as mutagenicity,
% which involves minimizing the risk of DNA mutations.
% %
% This is different from lethal toxicity, which directly causes fatal harm. 
% Importantly, our models are tuned explicitly to reduce mutagenicity,
% and not to increase it.
% %
% Additionally, our training data consists of drug-like,
% commercially accessible molecules.
% As a result, {\mollm}s are unlikely to generate lethal molecules, 
% as they were not trained on such data or optimization objectives.

% %
% However, if such molecules can be generated with adversarial prompts, 
% this could potentially arise from the pretrained knowledge of the base models, 
% which includes broader chemical information outside the scope of \MOptData and our instruction-tuning.
% %
% To mitigate such risks, safeguards such as usage monitoring, and integration with toxicity prediction pipelines should be implemented when deploying these models. 
% %
% Moreover, collaboration with regulatory bodies and ethical committees can ensure responsible use and prevent misuse.
% %
% Ensuring equitable access to these tools is crucial to support under-resourced research communities in drug discovery.


% For instance, the generated molecules could inadvertently 
% include compounds with high toxicity or other undesirable properties if not carefully evaluated in downstream experimental validation. 
% %
% To mitigate such risks, users must employ robust post-generation filtering and validation pipelines. 
% %
% Additionally, ensuring equitable access to these tools is vital to prevent disparities in the availability of computational resources for drug discovery, particularly for under-resourced research communities.

% \section{Introduction}

% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

% \section{Engines}

% To produce a PDF file, pdf\LaTeX{} is strongly recommended (over original \LaTeX{} plus dvips+ps2pdf or dvipdf). Xe\LaTeX{} also produces PDF files, and is especially suitable for text in non-Latin scripts.

% \section{Preamble}

% The first line of the file must be
% \begin{quote}
% \begin{verbatim}
% \documentclass[11pt]{article}
% \end{verbatim}
% \end{quote}

% To load the style file in the review version:
% \begin{quote}
% \begin{verbatim}
% \usepackage[review]{acl}
% \end{verbatim}
% \end{quote}
% For the final version, omit the \verb|review| option:
% \begin{quote}
% \begin{verbatim}
% \usepackage{acl}
% \end{verbatim}
% \end{quote}

% To use Times Roman, put the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \usepackage{times}
% \end{verbatim}
% \end{quote}
% (Alternatives like txfonts or newtx are also acceptable.)

% Please see the \LaTeX{} source of this document for comments on other packages that may be useful.

% Set the title and author using \verb|\title| and \verb|\author|. Within the author list, format multiple authors using \verb|\and| and \verb|\And| and \verb|\AND|; please see the \LaTeX{} source for examples.

% By default, the box containing the title and author names is set to the minimum of 5 cm. If you need more space, include the following in the preamble:
% \begin{quote}
% \begin{verbatim}
% \setlength\titlebox{<dim>}
% \end{verbatim}
% \end{quote}
% where \verb|<dim>| is replaced with a length. Do not set this length smaller than 5 cm.

% \section{Document Body}

% \subsection{Footnotes}

% Footnotes are inserted with the \verb|\footnote| command.\footnote{This is a footnote.}

% \subsection{Tables and figures}

% See Table~\ref{tab:accents} for an example of a table and its caption.
% \textbf{Do not override the default caption sizes.}

% \begin{table}
%   \centering
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\"a}|     & {\"a}           \\
%     \verb|{\^e}|     & {\^e}           \\
%     \verb|{\`i}|     & {\`i}           \\
%     \verb|{\.I}|     & {\.I}           \\
%     \verb|{\o}|      & {\o}            \\
%     \verb|{\'u}|     & {\'u}           \\
%     \verb|{\aa}|     & {\aa}           \\\hline
%   \end{tabular}
%   \begin{tabular}{lc}
%     \hline
%     \textbf{Command} & \textbf{Output} \\
%     \hline
%     \verb|{\c c}|    & {\c c}          \\
%     \verb|{\u g}|    & {\u g}          \\
%     \verb|{\l}|      & {\l}            \\
%     \verb|{\~n}|     & {\~n}           \\
%     \verb|{\H o}|    & {\H o}          \\
%     \verb|{\v r}|    & {\v r}          \\
%     \verb|{\ss}|     & {\ss}           \\
%     \hline
%   \end{tabular}
%   \caption{Example commands for accented characters, to be used in, \emph{e.g.}, Bib\TeX{} entries.}
%   \label{tab:accents}
% \end{table}

% As much as possible, fonts in figures should conform
% to the document fonts. See Figure~\ref{fig:experiments} for an example of a figure and its caption.

% Using the \verb|graphicx| package graphics files can be included within figure
% environment at an appropriate point within the text.
% The \verb|graphicx| package supports various optional arguments to control the
% appearance of the figure.
% You must include it explicitly in the \LaTeX{} preamble (after the
% \verb|\documentclass| declaration and before \verb|\begin{document}|) using
% \verb|\usepackage{graphicx}|.

% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{example-image-golden}
%   \caption{A figure with a caption that runs for more than one line.
%     Example image is usually available through the \texttt{mwe} package
%     without even mentioning it in the preamble.}
%   \label{fig:experiments}
% \end{figure}

% \begin{figure*}[t]
%   \includegraphics[width=0.48\linewidth]{example-image-a} \hfill
%   \includegraphics[width=0.48\linewidth]{example-image-b}
%   \caption {A minimal working example to demonstrate how to place
%     two images side-by-side.}
% \end{figure*}

% \subsection{Hyperlinks}

% Users of older versions of \LaTeX{} may encounter the following error during compilation:
% \begin{quote}
% \verb|\pdfendlink| ended up in different nesting level than \verb|\pdfstartlink|.
% \end{quote}
% This happens when pdf\LaTeX{} is used and a citation splits across a page boundary. The best way to fix this is to upgrade \LaTeX{} to 2018-12-01 or later.

% \subsection{Citations}

% \begin{table*}
%   \centering
%   \begin{tabular}{lll}
%     \hline
%     \textbf{Output}           & \textbf{natbib command} & \textbf{ACL only command} \\
%     \hline
%     \citep{Gusfield:97}       & \verb|\citep|           &                           \\
%     \citealp{Gusfield:97}     & \verb|\citealp|         &                           \\
%     \citet{Gusfield:97}       & \verb|\citet|           &                           \\
%     \citeyearpar{Gusfield:97} & \verb|\citeyearpar|     &                           \\
%     \citeposs{Gusfield:97}    &                         & \verb|\citeposs|          \\
%     \hline
%   \end{tabular}
%   \caption{\label{citation-guide}
%     Citation commands supported by the style file.
%     The style is based on the natbib package and supports all natbib citation commands.
%     It also supports commands defined in previous ACL style files for compatibility.
%   }
% \end{table*}

% Table~\ref{citation-guide} shows the syntax supported by the style files.
% We encourage you to use the natbib styles.
% You can use the command \verb|\citet| (cite in text) to get ``author (year)'' citations, like this citation to a paper by \citet{Gusfield:97}.
% You can use the command \verb|\citep| (cite in parentheses) to get ``(author, year)'' citations \citep{Gusfield:97}.
% You can use the command \verb|\citealp| (alternative cite without parentheses) to get ``author, year'' citations, which is useful for using citations within parentheses (e.g. \citealp{Gusfield:97}).

% A possessive citation can be made with the command \verb|\citeposs|.
% This is not a standard natbib command, so it is generally not compatible
% with other style files.

% \subsection{References}

% \nocite{Ando2005,andrew2007scalable,rasooli-tetrault-2015}

% The \LaTeX{} and Bib\TeX{} style files provided roughly follow the American Psychological Association format.
% If your own bib file is named \texttt{custom.bib}, then placing the following before any appendices in your \LaTeX{} file will generate the references section for you:
% \begin{quote}
% \begin{verbatim}
% \bibliography{custom}
% \end{verbatim}
% \end{quote}

% You can obtain the complete ACL Anthology as a Bib\TeX{} file from \url{https://aclweb.org/anthology/anthology.bib.gz}.
% To include both the Anthology and your own .bib file, use the following instead of the above.
% \begin{quote}
% \begin{verbatim}
% \bibliography{anthology,custom}
% \end{verbatim}
% \end{quote}

% Please see Section~\ref{sec:bibtex} for information on preparing Bib\TeX{} files.

% \subsection{Equations}

% An example equation is shown below:
% \begin{equation}
%   \label{eq:example}
%   A = \pi r^2
% \end{equation}

% Labels for equation numbers, sections, subsections, figures and tables
% are all defined with the \verb|\label{label}| command and cross references
% to them are made with the \verb|\ref{label}| command.

% This an example cross-reference to Equation~\ref{eq:example}.

% \subsection{Appendices}

% Use \verb|\appendix| before any appendix section to switch the section numbering over to letters. See Appendix~\ref{sec:appendix} for an example.

% \section{Bib\TeX{} Files}
% \label{sec:bibtex}

% Unicode cannot be used in Bib\TeX{} entries, and some ways of typing special characters can disrupt Bib\TeX's alphabetization. The recommended way of typing special characters is shown in Table~\ref{tab:accents}.

% Please ensure that Bib\TeX{} records contain DOIs or URLs when possible, and for all the ACL materials that you reference.
% Use the \verb|doi| field for DOIs and the \verb|url| field for URLs.
% If a Bib\TeX{} entry has a URL or DOI field, the paper title in the references section will appear as a hyperlink to the paper, using the hyperref \LaTeX{} package.

% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only

\bibliography{paper}




\clearpage

\appendix


\renewcommand{\thefigure}{A\arabic{figure}} % Change figure numbering format
\setcounter{figure}{0} % Reset figure counter

\renewcommand{\thetable}{A\arabic{table}} % Change figure numbering format
\setcounter{table}{0} % Reset figure counter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Details on \MOptData}
\label{sec:app:task}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{tables/relwork_dataset}

Table~\ref{tbl:relwork_data}
highlights the differences between our curated dataset,
\MOptData and the only other existing instruction-tuning
dataset for molecule optimization, \DrugMOpt.

\input{tables/task_prop}
%\ref{tbl:task_prop}


%=================================%
\subsection{Details on Evaluation Tasks}
\label{sec:app:task:ind}
%=================================%

In this section, we provide descriptions of 
10 tasks in \MOptData used for evaluation.


%=================================%
\subsubsection{IND tasks}
\label{sec:app:task:ind}
%=================================%

Below are the 5 IND tasks:
\begin{enumerate}[leftmargin=*]

\item 
\BDP:
This task optimizes molecules to improve BBBP, DRD2 receptor inhibition, and lipophilicity (plogP). 
These properties are critical for central nervous system (CNS) drugs, where molecules must penetrate the blood-brain barrier, bind effectively to the DRD2 receptor (a common target for neurological disorders), and maintain sufficient lipophilicity for stability and membrane permeability.


\item \BDQ:
This task optimizes molecules to increase BBBP, DRD2 binding affinity, and improve QED. By balancing brain permeability, receptor activity, and drug-likeness, this task captures realistic trade-offs required in CNS drug development.


\item \BPQ:
This task aims to improve BBBP, plogP, and QED, prioritizing brain permeability and appropriate lipophilicity while ensuring the optimized molecules retain favorable drug-like properties.
% High QED ensures that the optimized molecule retains drug-like properties, enhancing its potential for bioavailability and development as a viable therapeutic agent.

\item \DPQ:
This task focuses on improving DRD2, plogP, and QED. It targets receptor binding potency while optimizing lipophilicity and maintaining overall drug-likeness, representing key requirements for receptor-specific drug design.
% This ensures that the molecule achieves strong receptor binding 
% without compromising simplicity, synthetic accessibility, or drug-like characteristics.

\item \BDPQ:
This task jointly optimizes BBBP, DRD2 activity, plogP, and QED, reflecting a challenging and comprehensive scenario for developing CNS drug candidates with high permeability, receptor activity, and drug-like characteristics.

\end{enumerate}




%=================================%
\subsubsection{OOD tasks}
\label{sec:app:task:ood}
%=================================%

Below are the 5 tasks used for evaluating out-of-domain generalizability:

\begin{enumerate}[leftmargin=*]
\item {\MPQ:}
This task focuses on reducing mutagenicity, improving plogP, and enhancing drug-likeness (QED). 
%The goal is to eliminate toxic structural alerts while ensuring adequate lipophilicity for absorption and metabolic stability, alongside maintaining favorable overall drug-like characteristics. 
This task represents an early-stage lead optimization scenario
to reduce genotoxic risks while ensuring adequate lipophilicity and drug-like properties.

\item {\BDMQ:}
This task optimizes BBBP, DRD2 inhibition, mutagenicity, and QED. 
It reflects CNS drug development by balancing domapine receptor activity, brain permeability, and safety while ensuring overall drug-likeness.

\item {\BHMQ:}
This task focuses on increasing BBBP and HIA, reducing mutagenicity, and improving QED. It is particularly relevant for orally administered CNS drugs, where both brain and intestinal absorption are critical.


\item {\BMPQ:}
This task optimizes BBBP, mutagenicity, plogP, and QED. It reflects CNS drug design by balancing adequate lipophilicity, reduced toxicity, and favorable drug-like properties, simulating realistic requirements for CNS-active drugs.

\item {\HMPQ:}
This task enhances HIA, reduces mutagenicity, and improves plogP and QED. It represents optimization for orally administered drugs, focusing on absorption, genotoxic risk reduction, and overall drug-like quality.

\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Additional Filtering in Test Set}
\label{sec:app:filter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
Out of the initial pool of 250K molecules sampled from ZINC,
we select a molecule into the test set of a task
which has a property worse than 
the median \MPSTr.
%
Additionally, for properties with highly skewed distributions, we exclude
molecules %whose property values 
falling below the 10th percentile of properties in training hit molecules,
thereby eliminating extreme cases (e.g., a molecule with a plogP of -30) that are rarely encountered as hits.
%
After applying these steps to the initial pool of 250K molecules,
we randomly select at most 500 molecules into the test set for each task, with possible overlap across tasks.
%
Table~\ref{tbl:task_prop} presents the property characteristics
of training pairs and test molecules
%{\MPSTr}
%and average property values of the test set ({\APS}Ts)
in all 10 tasks.
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quality Control}
\label{sec:app:quality}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We implement multiple quality control measures to ensure
dataset integrity.
%
% To ensure dataset integrity, 
% we enforce strict uniqueness checks
% to remove duplicate molecules.
%
In \MOptData, molecules are represented as 
Simplified Molecular Input Line Entry System (SMILES)~\cite{Weininger1988smiles} strings
that are canonicalized and deduplicated.
% Additionally to prevent data leakage,
% all test molecules are sampled such that they are unseen during training.
%
For each molecule,
empirical property scores are computed using
well-established tools:
ADMET-AI~\cite{swanson2024admet}
for BBBP, HIA, Mutag and QED, 
and the official implementation provided by \citet{you2018graph} for DRD2 and plogP.
%
%
While these property scores are not experimentally validated, 
they provide reliable and computationally efficient estimates, 
making them well-suited for large-scale dataset construction like ours.
%
% These tools are widely used in \emph{in silico} drug discovery 
% due to their ability to predict a broad range of molecular properties with reasonable accuracy\cite{}.
%All the tools used in our work are presented in Table~\ref{}.

We also ensure instruction diversity
to enhance the generalizability of instruction-tuned LLMs~\cite{xu2024wizardlm}.
%
We provide a manually written, clear and concise
seed instruction
into GPT-4~\cite{openai2024gpt4technicalreport}
to construct multiple distinctly phrased (i.e., diverse) instructions.
%
We select into \MOptData 5 diverse instructions 
synonymous
with the seed instruction.
%
%
To evaluate LLMs' instruction understanding and generalizability
to unseen instructions, 
we hold out one instruction for each task during training. 
%
Thus, each task in \MOptData has 5 diverse instructions
for instruction tuning, 
and 1 unseen instruction for testing.
All instructions are presented in Appendix~\ref{sec:app:instr}.

% Furthermore, we do not explicitly provide numerical property constraints
% within the instruction.
% The goal is to enable the model to implicitly learn
% the structure-activity relationships
% and the interaction between multiple properties from molecule pairs
% and corresponding natural language instructions.
% %
% This approach reinforces the model to interpret
% multi-property optimization tasks
% without relying on provided numerical constraints,
% improving its generalizability to novel tasks during inference.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diverse Instructions}
\label{sec:app:instr}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{fig:instr} presents the prompt
template used for instruction-tuning.
%%
\begin{figure*}[h!]
\begin{tcolorbox}[
  colback=lightergray, 
  colframe=black, 
  sharp corners, 
  boxrule=0.5pt, 
  width=\textwidth,
  left=1mm, % Reduce left margin
  right=1mm, % Reduce right margin
  top=1mm, % Reduce top margin
  bottom=1mm % Reduce bottom margin
]
\begin{lstlisting}[
  language=, 
  basicstyle=\ttfamily\footnotesize, 
  breaklines=true, 
  breakindent=0pt, % Disable indentation for wrapped lines
  showstringspaces=false, % Remove visible spaces
  xleftmargin=0pt, % Remove left margin
  xrightmargin=0pt, % Remove right margin
  aboveskip=0pt,belowskip=0pt
]
[INST]
{instruction}

%%% Input : <SMILES> {source-smiles} </SMILES>
%%% Adjust: {change_i} {property_i}, ..., {change_k} {property_k}
[/INST]

%%% Response: {target-smiles}
\end{lstlisting}
\end{tcolorbox}
\caption{Prompt template used for instruction-tuning {\mollm}s}
\label{fig:instr}
\end{figure*}
%%

The `\{instruction\}' will be replaced with one of 6
diverse instructions.
5 diverse instructions are used in training,
and 1 is held out for testing in the unseen instruction setting.
%
Below are the six diverse instructions, where the first one
is manually written, and the rest are generated by GPT-4o.
The last one is the hold-out instruction.

\begin{enumerate}[leftmargin=*]

\item 
``Your task is to modify the given molecule to adjust specific molecular properties while keeping structural changes as minimal as possible. Your response should only contain a valid SMILES representation of the modified molecule enclosed with <SMILES> </SMILES> tag."
    
    \item    ``Modify the given molecule to adjust the specified molecular properties by substituting functional groups while keeping changes to the core structure minimal. Output only the SMILES of the modified molecule, wrapped in <SMILES> </SMILES> tags."
    
    \item  ``Your goal is to fine-tune the specified molecular properties of the given compound with minimal structural changes. Make the necessary adjustments and return the modified molecule in a SMILES format enclosed in <SMILES> </SMILES> tags."

    \item
    ``Adjust the structure of the given molecule to target the specified adjustments in molecular properties. Retain the core structure as much as possible. Respond with only the SMILES of the modified molecule enclosed in <SMILES> </SMILES> tags."

    \item 
    ``Alter the given molecule to meet the desired property changes with the least structural alteration possible. Output only the adjusted molecule in SMILES format, using <SMILES> </SMILES> tags."
        
    \item
    ``Modify the given molecular structure to target specific property changes, aiming to keep structural adjustments minimal. Respond solely with the SMILES notation for the adjusted molecule, enclosed within <SMILES> </SMILES> tags."
    
\end{enumerate}




\paragraph{Property Names:}

We used the following names for each property 
where the former is used during instruction-tuning
and the latter is used for evaluation in
the unseen instruction setting.
For other evaluation settings, we used the same
property name as used in tuning.

\begin{enumerate}[leftmargin=*]
    \item BBBP: ``BBB permeability", ``Blood-brain barrier permeability (BBBP)"
    \item DRD2: ``DRD2 inhibition", ``inhibition probability of Dopamine receptor D2"
    \item HIA: ``Intestinal adsorption", ``human intestinal adsorption ability"
    \item Mutag: ``Mutagenicity",  ``probability to induce genetic alterations (mutagenicity)"
    \item plogP: ``Penalized octanol-water partition coefficient (penalized logP)", 
    ``Penalized logP which is logP penalized by synthetic accessibility score and number of large rings"
    \item QED: ``QED", ``drug-likeness quantified by QED score"
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Details on Experimental Setup}
\label{sec:app:expts_setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{{\mollm}s}
\label{sec:app:reproducibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
We develop a series of generalist {\mollm}s
which are trained on the power sets of 3, 4, and 6 properties,
denoted as \mollmTripleGen, \mollmQuadGen, and \mollmSixGen,
respectively.
%
To train these models, we fine-tune 2 general-purpose LLMs: 
Mistral-7B-Instruct-v0.3~\cite{mistral2023mistral} and Llama3.1-8B-Instruct~\cite{grattafiori2024llama3herdmodels}
using LoRA~\cite{hu2022lora},
%
leveraging the Huggingface Transformers library~\cite{wolf2020transformers}. 
%
We fine-tune all models with a learning rate of 1e-4 and a batch size of 128, using a cosine learning rate scheduler with a 5\% warm-up period. 
%
We fine-tune task-specific {\mollm}s 
and generalist {\mollm}s for 10 and 3 epochs, respectively,
to balance efficiency and overfitting. 
%
We set LoRA parameters with $\alpha=16$, dropout of 0.05, and a rank of 16,
and apply LoRA adapters to all projection layers and the language modeling head. 
We perform 0-shot evaluations
(i.e., without in-context examples) for all \mollm models 
in all tasks.
For each test molecule, we generate 20 molecules 
using beam search decoding, with the number of beams set to 20.

The number of trainable parameters varies from 42 million for Mistral-7B-Instruct-v0.3
to 44 million for Llama3.1-8B-Instruct.
Task-specific {\mollm}s need up to 1 hour on average on a NVIDIA H100 (Hopper) GPU for 10 epochs.
Generalist {\mollm}s take from 8 to 24 hours on average on the same GPU for 3 epochs,
depending on the number of tasks (property combinations).
In total, we spent about 120 GPU hours on an NVIDIA H100 GPU with 96 GB HBM2e memory.


%====================================%
\subsection{Baselines}
\label{sec:app:expts_setup:baselines}
%====================================%
In this section, we present the baselines considered and selected for our comparison. Table~\ref{tbl:baseline_LLMs} details the licenses and sources for both the datasets and models (i.e., artifacts).
%
We ensured that all artifacts used in this work were employed in a manner consistent with their intended use as specified by the original authors or licensors. 
%
For the models we developed, we identified ethical considerations
which are discussed in Section~\ref{sec:ethics}.


%----------------------------------------%
\paragraph{General-purpose LLMs:}
%----------------------------------------%

We evaluate 3 general-purpose LLMs:
Mistral-7B Instruct-v0.3~\cite{mistral2023mistral}, 
Llama-3.1 8B-Instruct~\cite{touvron2023llama}, 
and Claude-3.5 to assess the performance of such LLMs
in molecule optimization.
%
% For all the general-purpose
% LLMs, we use the officially released checkpoints. 
%
For Mistral-7B Instruct-v0.3 and Llama-3.1 8B-Instruct,
we use the officially released checkpoints provided in Huggingface.
For Claude-3.5,
we access the Sonnet checkpoints using the official API. 
%
We conduct both 0-shot and 5-shot inference (i.e., with 0 and 5 in-context examples, respectively) 
on all general-purpose LLMs
using the same prompt templates (Appendix~\ref{sec:app:prompt:glm}).
%\begin{itemize}
%    \item Could have high flexibility in task formulation and natural language interaction. Have the ability to incorporate diverse types of constraints and objectives.
%    \item Limited chemical domain knowledge leading to suboptimal property optimization.
%\end{itemize}

%----------------------------------------%
\paragraph{Foundational LLMs for Chemistry:}
%----------------------------------------%
We use \LlaSMolM (i.e., \LlaSMol tuned over the base model Mistral-7B) as the foundational LLM for chemistry since
it demonstrated state-of-the-art performance over
others such as MolInst~\cite{fang2024molinstructions} and
ChemLLM~\cite{zhang2024chemllm}
on a wide range
of molecular property prediction tasks.
% For \LlaSMol, we use the Mistral-based checkpoint 
% released by its developers, 
% as it demonstrated the best performance on their reported tasks and yielded superior results in our preliminary experiments. 
%
We conduct only zero-shot inference
since we did not observe
any improvement with in-context examples in our preliminary experiments. 
%
We use a simpler prompt template 
(Appendix~\ref{sec:app:prompt:llasmol}) for inference
since \LlaSMol struggles to follow instructions in more
detailed prompts.

% While \LlaSMol incorporates chemistry-domain knowledge from 
% training tasks such as property prediction and retrosynthesis via instruction tuning,
% it exhibits reduced natural language understanding ability compared to general-purpose LLMs with in-context examples (i.e., few-shot prompting).
% %
% Therefore, we only report zero-shot prompting results.

% Details on training/prompting baselines are provided in Appendix~\ref{sec:app:baselines}.
%----------------------------------------%
\paragraph{Task-specific non-LLM:}
%----------------------------------------%
%\vishal{Only \PMol}
% \vishal{no need to discuss what \PMol does, just describe how you trained and tested. Also provide some justification on why not other non-LLM baselines were used.} 
%\PMol employs a specialized sequence-to-sequence transformer for molecule optimization. It combines a pre-trained GNN for generating task-specific atom embeddings with a transformer that translates source SMILES to optimized SMILES. The task-atom embeddings, which incorporate property-specific information, will be added to the atom's token embeddings to guide the optimization process.
%
%While this property-guided embedding approach enables effective optimization for specific tasks, it inherently limits \PMol to task-specific settings since the GNN embeddings are trained with fixed property combinations.
%

%
We use \PMol as our task-specific non-LLM baseline since 
%It employs pairwise optimization during training
%that aligns with our experimental setting, 
it demonstrated better performance over other methods such as Modof on single- and double-property optimization tasks. 
%
%Non-pairwise optimization methods such as MolLeo are not considered as their experimental settings differ from our framework.
%
%implementation of 
During inference, we leverage its 
embedding generator and
transformer modules
which are separately trained for each task. 
% \PMol first trains the task-specific atom embedding generator, then trains a sequence-to-sequence transformer on paired molecules.
%represented with SMILES strings. 
%
We discuss the training details and 
limitations of \PMol in
Appendix~\ref{sec:app:pmol} and \ref{sec:app:pmol:discuss}, respectively.

\input{tables/baseline_LLMs.tex}
%, where we also discuss some issues in \PMol's method.
%During inference, we input the hit molecule %$M_x$ 
%and its task-specific atom embeddings
%into the transformer to generate 20 molecules. 
%

    
    
    %Introduces more flexibility by using seq2seq framework and prompt-based learning but still maintains significant constraints:
    %\begin{itemize}
    %    \item Primary focus on single/double property optimization. Shows promising results but is still constrained to double-property optimization scenarios.
    %    \item Implements zero-shot learning capability (training on a single property, testing on double).
    %    \item Requires pre-training of property-specific embedding generators, limiting flexibility for unseen properties.
    %    \item Unable to handle property name variations (e.g., "BBBP" vs. "BBB permeability") -> Rigid input format, no natural language processing ability (even using prompts).
    %\end{itemize}

%These existing approaches exhibit several critical limitations:
%(1) \textbf{Restricted Property Space}: By focusing only on up-to-two properties optimization, they fail to capture the complexity of real-world molecular design problems, where multiple properties must be balanced simultaneously. \xiao{Current results can support.}
%(2) \textbf{Absence of OOD Considerations}: Neither framework addresses out-of-distribution scenarios, which are crucial for practical applications where target properties may not included in the training data.
%\xiao{Experiments TO DO.}
%(3) \textbf{Rigid Frameworks}: Inflexible architectures and input requirements restrict adaptation to diverse practical usage.
%\xiao{Related these claims to the corresponding experiment results}




%========================================%
\subsection{Evaluation Metrics}
\label{sec:app:eval}
%========================================%

We use the following evaluation metrics for a holistic comparison.

\begin{enumerate}[leftmargin=*]

\item \textbf{Success rate ({\SR}):} 
Success rate is the proportion
of test molecules for which at least one of 20 generated molecules has improvements in all desired properties.
%
If multiple generated molecules have improved properties, the one achieving the highest improvement across all properties is selected for evaluation.
Higher {\SR} demonstrates the models' ability to 
successfully optimize most hit molecules.


\item \textbf{Validity ({\Val}):}
Validity is the proportion of test hit molecules for which
at least one of 20 generated molecules is chemically valid.
A molecule is considered valid if it can be successfully
parsed by RDKit.
Higher validity indicates more test cases have valid generations.

    
\item \textbf{Similarity {({\Sim})}:}
{{\Sim}} denotes the average Tanimoto similarity
between successfully optimized molecules and the
corresponding test molecules.
The Tanimoto similarity is computed using binary Morgan fingerprints with a dimension of 2,048 and a radius of 2.
Higher {\Sim} indicates minimal structural modifications, 
which is desirable for retaining the core scaffold
-- a key requirement in lead optimization.


\item \textbf{Novelty ({\Nov}):} Novelty is defined as the 
percentage of optimized molecules that are unseen during training.
Higher {\Nov} indicates the models' ability to generate novel molecules, which is important for
ensuring chemical diversity and finding new lead molecules.


\item \textbf{Synthetic Accessibility Score ({\SAS}):}
SAS estimates how easily a molecule can be synthesized based on its structural complexity and the presence of uncommon fragments.
SAS generally ranges from 1 (easy to synthesize)
to 10 (challenging to synthesize~\cite{Ertl2009}.
% {\SAS} in the range of [2, 3.5] are typically considered desirable,
% suggesting easily synthesizable molecules.

\item \textbf{Relative Improvement ({\RI}):}
RI measures the average improvement in each property
relative to its initial score in the input hit molecule.
%averaged across all desired properties.
Higher {\RI} implies significant improvements in desired properties on average.

\item \textbf{Average Property Score ({\APS}):}
{\APS} represents the average property score
of the generated optimized molecules in each property.
Higher {\APS} indicates that the model consistently generates molecules with better properties.
%\item \textbf{Average (Percentage) improvement in properties}

%\item \textbf{Computational cost}
\end{enumerate}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prompt Templates}
\label{sec:app:prompt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following prompts are used to evaluate general-purpose LLMs
and \LlaSMol.



%========================================%
\subsection{Prompt Template for General-purpose LLMs}
\label{sec:app:prompt:glm}
%========================================%

We use a detailed prompt template
which is clearly structured with a system prompt,
task instruction, the input hit molecule, and task-specific optimization goals.
Figure~\ref{fig:prompt_glm} shows the prompt template with an example
task.

\begin{figure*}[!t]
\begin{tcolorbox}[
  colback=lightergray, 
  colframe=black, 
  sharp corners, 
  boxrule=0.5pt, 
  width=\textwidth,
  left=1mm, % Reduce left margin
  right=1mm, % Reduce right margin
  top=1mm, % Reduce top margin
  bottom=1mm % Reduce bottom margin
]
\begin{lstlisting}[
  language=, 
  basicstyle=\ttfamily\footnotesize, 
  breaklines=true, 
  breakindent=0pt, % Disable indentation for wrapped lines
  showstringspaces=false, % Remove visible spaces
  xleftmargin=0pt, % Remove left margin
  xrightmargin=0pt, % Remove right margin
  aboveskip=0pt,belowskip=0pt
]
<<SYS>>
You are an expert medicinal chemist specializing in molecular optimization. You understand how structural modifications affect key ADMET properties and inhibitions of common receptor targets like DRD2.
<</SYS>>

[INST]
Your task is to modify the given molecule to adjust specific molecular properties while keeping structural changes as minimal as possible. Use the examples (if provided) as a guide. Your response should only contain a valid  SMILES representation of the modified molecule enclosed with <SMILES> </SMILES> tag.

Examples:
%%% Input : <SMILES> COCC(=O)Nc1ccc(C(N)=O)cc1 </SMILES>
%%% Adjust: decrease Mutagenicity, increase Penalized octanol-water partition coefficient (penalized logP) and increase QED
%%% Response: <SMILES> COCC(=O)Nc1ccc(Br)cc1 </SMILES>

Task:
%%% Input : <SMILES> COC1COCCN(C(=O)c2ccno2)C1 </SMILES>
%%% Adjust: decrease Mutagenicity, increase Penalized octanol-water partition coefficient (penalized logP) and increase QED
[/INST]

%%% Response:
\end{lstlisting}
\end{tcolorbox}
\vspace{-10pt}
\caption{An example of a prompt used for general-purpose LLMs}
\label{fig:prompt_glm}
\end{figure*}



%========================================%
\subsection{Prompt Template for \LlaSMol}
\label{sec:app:prompt:llasmol}
%========================================%

Unlike general-purpose LLMs, \LlaSMol was instruction-tuned 
on multiple chemistry tasks
with a specific prompt template. 
We observed that \LlaSMol struggled with following instructions 
using the prompt template for general-purpose LLMs,
resulting in poor performance. 
Hence, we used a simpler prompt template without a system prompt
and without a clear separation of task instruction, input, and response.
Moreover, we only conduct 0-shot inference for \LlaSMol.
%
Figure~\ref{fig:prompt_llasmol} 
shows the prompt template with the same task
used above but without in-context examples:
%%
%%
\begin{figure*}
\begin{tcolorbox}[
  colback=lightergray, 
  colframe=black, 
  sharp corners, 
  boxrule=0.5pt, 
  width=\textwidth,
  left=1mm, % Reduce left margin
  right=1mm, % Reduce right margin
  top=1mm, % Reduce top margin
  bottom=1mm % Reduce bottom margin
]
\begin{lstlisting}[
  language=, 
  basicstyle=\ttfamily\footnotesize, 
  breaklines=true, 
  breakindent=0pt, % Disable indentation for wrapped lines
  showstringspaces=false, % Remove visible spaces
  xleftmargin=0pt, % Remove left margin
  xrightmargin=0pt, % Remove right margin
  aboveskip=0pt,belowskip=0pt
]
Modify the molecule <SMILES> COC1COCCN(C(=O)c2ccno2)C1 </SMILES> to decrease its Mutagenicity, increase its Penalized octanol-water partition coefficient (penalized logP) value, and increase its QED value. Keep the modifications to the molecule structure as minimal as possible.
%%% Response:
\end{lstlisting}
\end{tcolorbox}
\vspace{-10pt}
\caption{An example of a prompt used for \LlaSMol}
\label{fig:prompt_llasmol}
\end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Complete Experimental Results}
\label{sec:app:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%========================================%
\subsection{IND Evaluation}
\label{sec:app:results:ind}
%========================================%

Tables~\ref{tbl:bdp_ind}, \ref{tbl:bdq_ind}, \ref{tbl:bpq_ind}, \ref{tbl:dpq_ind} and \ref{tbl:bdpq_ind} present the performance comparison of {\mollm}s
with baselines on all 5 IND tasks.

%\input{tables/bdp_ind}
\input{tables/bdp_ind}
\input{tables/bdq_ind}
\input{tables/bpq_ind}
\input{tables/dpq_ind}
\input{tables/bdpq_ind}



\paragraph{Comparison between \mollm and \PMol:} 

%Both generalist and task-specific 
{\mollm}s consistently outperform \PMol across all IND tasks,
improving {\SR} by as much as 770.0\% on \BDPQ task.
%
%
This gain likely arises due to instruction-tuning which enables {\mollm}s 
%Via instruction-tuning, \mollm 
to effectively learn modification strategies
while leveraging their pre-trained general-purpose and chemical knowledge.
%
In contrast, \PMol learns both
chemical knowledge and task-specific knowledge from scratch (Appendix~\ref{sec:app:pmol:discuss}),
making it heavily reliant on limited task-specific training data
and thus resulting in extremely low {\SR}.
%
Notably, the very few optimized cases from \PMol exhibit high {\RI} but low {\Sim}, indicating substantial property improvements with drastic structural changes. 
%
This suggests that \PMol tends to generate entirely new molecules, thus failing to retain the core scaffold -- a key requirement in lead optimization.
%\xiao{how to explain?}
%
%For example, on task \BDQ (Table~\ref{tbl:bbbp+drd2+qed}), \mollmTripleTaskL achieves 90\% {\SR}, 99.4\% {\Val}, and 0.46\% {\Sim}, substantially exceeding \PMol's 23.2\% {\SR}, 36.4\% {\Val}, and 0.10\% {\Sim}. This demonstrates \mollm's superior ability to generate valid molecules while making targeted modifications.


% %
% Conversely, task-specific {\mollm}s excel in \BDQ and \BDP, where positive property correlation 
% (e.g., a correlation of 0.6 between BBBP and DRD2) 
% and sufficient training data enable stronger performance.
% These results collectively highlight 
% the complementary strengths of generalist and task-specific {\mollm}s, 
% %
% with the generalist {\mollm} offering a scalable foundational model for diverse optimization tasks without task-specific retraining.
% For \BDQ and \BDP tasks, task-specific {\mollm}s show better performance with improvements of 6.1\% and 6.4\% with 4,472 and 2,064 data respectively.
%

% The trend aligns with the training data availability and task complexity. 
% %
% Four-property optimization (\BDPQ) is inherently more challenging as it requires balancing multiple objectives, and with very limited 624 training examples, generalist {\mollm} benefits from knowledge transfer across tasks. 
%
% Three-property tasks exhibit different patterns based on chemical compatibility. 
% %\BPQ and \DPQ tasks are more challenging as PlogP optimization often demands large structural modifications that can affect other properties. 
% \BDQ and \BDP tasks benefit from natural property alignment, where BBBP and DRD2 optimization share complementary structural requirements for CNS drugs, and sufficient training data enables task-specific {\mollm} to excel.
% Furthermore, we observed a high correlation of 0.6 between BBBP and DRD2
% in our dataset, indicating molecules with higher BBBP also tend to have high 
% probability of DRD2 inhibition, and vice-versa.
% \xiao{Use literatures and data analysis to support the complexity of these tasks}
% %

% Despite the difference, these results collectively demonstrate 
% a single generalist model can be used as a foundational model
% for diverse molecular optimization tasks without task-specific retraining.

%\xiao{In terms of {\APS}, task-specific \mollm still differs to generalist \mollm, why?}

%

% %%%
\paragraph{Comparison between \mollmTripleTask and \mollmQuadTask:} 
%%%
\mollmTripleTask is consistently better than \mollmQuadTask in terms of {{\SR}} across all 4 IND tasks with 3 properties. 
%
This performance gap can be attributed to \mollmQuadTask's more constrained training setup, 
with fewer training pairs (e.g., 624 in \BDPQ) and the added complexity of an additional property constraint. 
As a result, when evaluated on tasks with 3 properties,
\mollmQuadTask must adapt its knowledge learned
from improving 4 properties, which can limit its effectiveness.
%
In contrast, 
\mollmTripleTask benefits from more focused task-specific training with larger datasets (e.g., 4,472 in \BDQ),
enabling better performance.
%
% Nonetheless, \mollmQuadTask significantly surpasses
% task-specific non-LLM baseline like \PMol, highlighting the effectiveness of instruction tuning in enabling strong performance despite limited training data and more complex optimization tasks.
% Despite this performance difference, \mollmQuadTask still significantly outperforms other baselines, demonstrating that instruction tuning on LLMs enables effective learning and knowledge transfer, yielding strong results even with less training data and different optimization objectives.



%========================================%
\subsection{OOD Evaluation}
\label{sec:app:results:ood}
%========================================%

Tables~\ref{tbl:mpq_ood}, \ref{tbl:bdmq_ood}, \ref{tbl:bhmq_ood}, \ref{tbl:bmpq_ood} and \ref{tbl:hmpq_ood} present the performance comparison of {\mollm}s
with baselines on all 5 OOD tasks.

Since OOD tasks represent novel property combinations excluded from the training data,
task-specific models are not applicable in this setting.
%
Additionally, several properties in these tasks are not used in training
generalist models \mollmTripleGen and \mollmQuadGen,
making comparison with these models infeasible.

\input{tables/mpq_ood}
\input{tables/bdmq_ood}
\input{tables/bhmq_ood}
\input{tables/bmpq_ood}
\input{tables/hmpq_ood}




%========================================%
\subsection{Generalizability to Unseen Instructions}
\label{sec:app:results:uninst}
%========================================%

Tables~\ref{tbl:bdp_uninst}, \ref{tbl:bdq_uninst}, \ref{tbl:bpq_uninst}, \ref{tbl:dpq_uninst} and \ref{tbl:bdpq_uninst} present the performance comparison of {\mollm}s
with baselines on all 5 IND tasks when prompted with unseen instructions and unseen property names.
%
than those used during instruction-tuning. 
%
%
This evaluation is meaningful as it mimics real-world scenarios where users may describe optimization tasks using varying terminologies, requiring models to understand the underlying semantics of the task rather than relying on exact token matching.




\input{tables/bdp_uninst}
\input{tables/bdq_uninst}
\input{tables/bpq_uninst}
\input{tables/dpq_uninst}
\input{tables/bdpq_uninst}

%========================================%
\subsection{Additional Case Studies}
\label{sec:app:results:additional_cases}
%========================================%
In this section, we provide two additional cases from IND task \BDQ.
%
As shown in Figure~\ref{fig:BDQ_case1_appendix}, \mollmSixGenM improves molecular properties by removing a nitro group (–NO$_{2}$) from the aromatic ring and replacing it with a chlorine atom (–Cl), while \LlaSMolM replaces the nitro group with two morpholine rings (highlighted structures).  
%
Removing the nitro group reduces polarity and eliminates a structural alert associated with toxicity and poor pharmacokinetics \cite{nepali2018nitro}, and the chlorine substitution enhances lipophilicity and promotes passive diffusion across the BBB \cite{plattard2021overview, rosa2024identifying}.  
%
As a result, \mollmSixGenM achieves a notable increase in BBBP (+0.31), despite the hit molecule already having a relatively good BBBP value of 0.48, making further optimization more challenging.  
%
Additionally, the nitro group removal improves drug-likeness \cite{nepali2018nitro, noriega2022diverse}, and the minimal fragment substitution preserves the molecular scaffold, resulting in higher structural similarity (0.72 vs. 0.62).  
%
In contrast, \LlaSMolM’s morpholine rings increase polarity and hydrogen bonding capacity \cite{lenci2021occurrence}, which shows limited BBBP gains (+0.08).  
%

In another case shown in Figure~\ref{fig:BDQ_case2_appendix}, \mollmSixGenM improves the properties by replacing the hydrophobic naphthalene group in the hit molecule with a nitrogen-containing heteroaromatic ring resembling pyrazine (highlighted structures).
%
The naphthalene group increases hydrophobicity~\cite{bouiahya2020hydrophobic} but may disrupt the balance between lipophilicity and polarity required for efficient BBB penetration~\cite{jimenez2024molecular, cornelissen2023explaining}, while also posing potential metabolic stability concerns~\cite{kiani2019lipophilic}. 
%
Replacing it with a nitrogen-containing heterocyclic ring fine-tunes this lipophilicity-polarity balance, a well-established medicinal chemistry strategy to improve permeability and metabolic stability~\cite{kerru2020review}.  
%
This modification enables \mollmSixGenM to achieve notable improvements in BBBP (+0.41), DRD2 (+0.41), and QED (+0.11). Notably, the hit molecule’s BBBP is 0.51, a decent but not maximal value, making further optimization a nontrivial task.  
%
In contrast, \LlaSMolM’s adjustment is a minor local change to a single bond near the core structure, which neither meaningfully shifts polarity and lipophilicity nor reduces structural complexity, resulting in only negligible property improvements.
%
\input{figures/cases_appendix.tex}
%Overall, \mollmSixGenM achieves a more effective optimization by enhancing BBB permeability, improving drug-likeness, and retaining structural similarity, demonstrating its advantage over the baseline approach in this challenging case. 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion on \PMol}
\label{sec:app:baselines}
\input{tables/pmol_training}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%======================================%
\subsection{Training Details}
\label{sec:app:pmol}
%======================================%
%%
We use the official implementation of \PMol, 
and follow their two-stage training process.
%
First, we train the multitask graph attention network (MGA) for property predictions within each task (where a task refers to multi-property optimization). This MGA is later used to generate task-specific atom embeddings for optimization guidance.
%
%In the task-specific atom embedding generator GNN training, 
%
Specifically, we extract the unique molecules 
from each IND task's training and validation sets for the MGA training.
%, maintaining the 8:1:1 split ratio as in \PMol's paper. 
%
Secondly, we use task-specific molecule pairs, with atom embeddings provided generated by the trained MGA, to train the sequence-to-sequence transformer.
%
The task-specific atom embeddings are aggregated with token embeddings through summation for transformer input.
%
In total, \PMol has approximately 25 million parameters.
%
Training for each task with early stopping typically takes 1.5 hours on a single NVIDIA V100 GPU with 16GB memory,
hence totalling 7.5 GPU hours for 5 IND tasks.
%
Training data statistics and 
best hyper-parameters are presented in Table~\ref{tbl:pmol_training}. 
%


%======================================%
\subsection{Limitations}
\label{sec:app:pmol:discuss}
%======================================%

First, in \PMol's paper, the atom embedding produced by the MGA is claimed to be ``property-specific", implying that the embedding is trained independently for each property prediction (that is, only contains the specific property's information).
%
However, we find that during MGA training, this atom embedding is actually shared across all properties and is not differentiated for individual property prediction. 
%
This design introduces a coupling effect, where the atom embedding actually encodes information across multiple properties simultaneously.
%
As a result, when these embeddings are later used to guide molecular optimization towards improving a particular property, the encoded information from other properties will bias the property-specific optimization. 
%
Therefore, we argue that these embeddings are more accurately described as "task-specific", meaning they are only appropriate for guiding optimization under the same property combination (i.e., task) used during the transformer training.
%

Second, \PMol aggregates the task-specific atom embeddings and token embeddings through direct summation. 
%
However, this approach is problematic because the atom embeddings generated by MGA and the token embeddings reside in different latent spaces. 
Combining representations from different spaces 
directly through summation
is an ill-considered fusion strategy. 
%
A more principled approach would involve introducing a projection layer to align both embeddings into a common space before aggregation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussions on DeepSeek-R1}
\label{sec:app:deepseek}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

DeepSeek-R1~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability} is a recently open-sourced,
reasoning-focused LLM
trained via large-scale reinforcement learning
without relying on large amounts of supervised fine-tuning data.
%
Experiments demonstrated that DeepSeek-R1
has strong reasoning capabilities comparable to OpenAI-o1-1217
on tasks such as logical inference, mathematics, and coding.
%
Their experiments also highlight the effectiveness of distillation,
where smaller distilled models, such as Qwen2.5-14B,
outperform the larger base model QwQ-32B-Preview by a significant margin.
%
Given the relevance of reasoning capabilities in multi-property molecule optimization,
we chose to evaluate the distilled version of 
Llama-3.1-8B, DeepSeek-Distill-R1-Llama-8B, 
as it is the only version directly comparable to the Llama-based models in our experiments.
%


Following the recommendation of DeepSeek's authors in their paper,
we avoided using system prompts and few-shot prompting,
as such settings have been shown to degrade the model's performance.
%
Additionally, using few-shot prompts would significantly increase the cost and resource requirements, 
as it resulted in considerably longer response generation times in our preliminary demonstrations. 
%
Thus, we employed a zero-shot setting,
balancing efficiency and adherence to best practices.


Initially, we experimented with the same instruction template used in our general-purpose LLM evaluations.
%
However, DeepSeek-R1-Distill-Llama-8B 
consistently failed to optimize or generate modified molecules. 
%
Instead, it simply echoed the input molecule as its response, even after increasing the token limit. 
This behavior is likely because the prompt asked the model to only generate SMILES strings without explicitly mentioning step-by-step reasoning (i.e., chain-of-thought). 
%
Figure~\ref{fig:deepseek_1} provides an example of such a failure case.
%


%%
%%
\begin{figure*}
\begin{tcolorbox}[
  colback=lightergray, 
  colframe=black, 
  sharp corners, 
  boxrule=0.5pt, 
  width=\textwidth,
  left=1mm, % Reduce left margin
  right=1mm, % Reduce right margin
  top=1mm, % Reduce top margin
  bottom=1mm % Reduce bottom margin
]
\begin{lstlisting}[
  language=, 
  basicstyle=\ttfamily\footnotesize, 
  breaklines=true, 
  breakindent=0pt, % Disable indentation for wrapped lines
  showstringspaces=false, % Remove visible spaces
  xleftmargin=0pt, % Remove left margin
  xrightmargin=0pt, % Remove right margin
  aboveskip=0pt,belowskip=0pt
]
[INST]
Your task is to modify the given molecule to adjust specific molecular properties while keeping structural changes as minimal as possible. Use the examples (if provided) as a guide. Your response should only contain a valid SMILES representation of the modified molecule enclosed with <SMILES> </SMILES> tag.

Task:
%%% Input : <SMILES> CC(=O)OCC1=C(C(=O)O)N2C(=O)[C@@H](NC(=O)CCC[C@H](N)C(=O)O)[C@H]2SC1 </SMILES>
%%% Adjust: increase BBB permeability, increase DRD2 inhibition, increase Penalized octanol-water partition coefficient (penalized logP) and increase QED

[/INST]
%%% Response:
<SMILES>CC(=O)OCC1=C(C(=O)O)N2C(=O)[C@@H](NC(=O)CCC[C@H](N)C(=O)O)[C@H]2SC1</SMILES>
</br>
</br>
</br>
...
\end{lstlisting}
\end{tcolorbox}
\vspace{-10pt}
\caption{A failure case of DeepSeek-R1-Distill-Llama-8B on task \BDPQ}
\label{fig:deepseek_1}
\end{figure*}




%
We then modified the prompt structure, 
removing the clear separation between task instructions, input, and output, and explicitly instructing the model to "think step by step".
%
As shown in Figure~\ref{fig:deepseek_2}, 
this adjustment enabled DeepSeek-R1-Distill-Llama-8B to generate longer reasoning processes.
%
Interestingly, the model demonstrated self-reflection and explored multiple reasoning trajectories, akin to an "aha moment" expressed in Table 3 in the original paper~\cite{deepseekai2025deepseekr1incentivizingreasoningcapability}.
%
Despite this, the final molecule was still not optimized for the desired properties,
%
and the generation required significantly more tokens
-- up to 20K tokens more than those used in our instruction-tuned LLMs. 
%

This token inefficiency makes large-scale evaluation of DeepSeek-R1 using such prompts infeasible.
%
Due to these limitations, we decided not to pursue
further experiments, such as instruction-tuning on
top of DeepSeek-R1-Distill-Llama-8B or other distilled models. 
%
% While we observed promising reasoning behaviors and self-reflective capabilities in DeepSeek-R1-Distill-Llama-8B, it still faces several challenges of prompt sensitivity, high token requirements, high cost and time, 
% and the lack of successful molecular optimization.
% %
While the reasoning capabilities of DeepSeek-R1 are impressive, its current limitations in token efficiency, prompt sensitivity, and molecular optimization restrict its applicability in this domain.
We plan to explore this further in future work.
%%
%%
\begin{figure*}
\begin{tcolorbox}[
  colback=lightergray, 
  colframe=black, 
  sharp corners, 
  boxrule=0.5pt, 
  width=\textwidth,
  left=1mm, % Reduce left margin
  right=1mm, % Reduce right margin
  top=1mm, % Reduce top margin
  bottom=1mm % Reduce bottom margin
]
\begin{lstlisting}[
  language=, 
  basicstyle=\ttfamily\footnotesize, 
  breaklines=true, 
  breakindent=0pt, % Disable indentation for wrapped lines
  showstringspaces=false, % Remove visible spaces
  xleftmargin=0pt, % Remove left margin
  xrightmargin=0pt, % Remove right margin
  aboveskip=0pt,belowskip=0pt,
  escapeinside={(*@}{@*)}
]
Modify the molecule <SMILES> CC(=O)OCC1=C(C(=O)O)N2C(=O)[C@@H](NC(=O)CCC[C@H](N)C(=O)O)[C@H]2SC1 </SMILES> to increase its BBB permeability value, and increase its QED value. Keep the modifications to the molecule structure as minimal as possible. Think step by step and then output the modified molecule. 

%%% Response:
Okay, so I need to modify the given molecule to increase both its BBB permeability and QED value, while keeping the changes minimal. Let me start by understanding what BBB and QED mean.
...
I notice it's a cyclic structure with some substituents. Let me try to visualize it. It seems to have a central ring, maybe a thiazole or similar, with various substituents including ester groups, amides, and a sulfur atom.
...
(*@\textcolor{blue}{Another idea is to reduce the number of amide bonds.}@*) Amides are hydrophilic, but they can make the molecule bulkier and more polar, which might not always be good for BBB permeability. 

(*@\textcolor{red}{Wait, the chain is NC(=O)CCC[C@H](N)C(=O)O. That's an amide group connected...}@*)
...
(*@\textcolor{blue}{One approach could be to remove some of the ester or amide groups}@*)
...
Alternatively, maybe removing one of the carbonyl groups. 
...
(*@\textcolor{red}{Wait, but replacing a carbonyl with a CH2 would change the structure significantly. I need to make sure that the molecule still retains its functional groups necessary for its activity.}@*)
...
(*@\textcolor{blue}{Another thought: the molecule has multiple ester and amide groups,}@*)
...
So, the final modified molecule would have the NC(=O) chain shortened from four carbons to three carbons.
</think>

The molecule has been modified by shortening the NC(=O)CCC[C@H](N)C(=O)O chain to NC(=O)CC[C@H](N)C(=O)O, reducing the carbon count from four to three. This change decreases the molecular weight and increases hydrophilicity, enhancing both BBB permeability and QED.

SMILES: CC(=O)OCC1=C(C(=O)O)N2C(=O)[C@@H](NC(=O)CC[C@H](N)C(=O)O)[C@H]2SC1

\end{lstlisting}
\end{tcolorbox}
\vspace{-10pt}
\caption{"aha moment" even in the distilled models, DeepSeek-R1-Distill-Llama-8B, where it does \textcolor{red}{self-reflection} on its own and even \textcolor{blue}{explores
multiple reasoning} trajectories or chain-of-thoughts. Still fails to optimize on the same case as in Figure~\ref{fig:deepseek_1}.}
\label{fig:deepseek_2}
\end{figure*}



%\section{Case Study}
%Comparison of optimization results on some typical molecule-tasks.


%This is an appendix.

\end{document}
