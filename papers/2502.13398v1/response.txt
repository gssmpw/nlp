\section{Related Work}
\label{sec:rel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Various computational approaches have been developed for molecule optimization **Wu, "Graph Neural Networks for Molecular Optimization"**__**Duvenaud et al., "Deep Tox"**.
%
%
For example, Modof **Morris et al., "Modof: Fragment Modification via Graph Attention"**, MIMOSA **Huang et al., "MIMOSA: Molecular Informatics through Self-Attention"**,
and f-RAG **Kusunoki et al., "f-RAG: Fragment Removal with Attention Guided for Molecular Optimization"** 
perform substructure modifications over molecular graphs.
%at predicted disconnection sites 
%while preserving core scaffolds. %each with distinct modification strategies.
%
Chemformer **Li et al., "Chemformer: Cheminformatics-Driven Transformer for Molecule Property Prediction"** and \PMol **Rajput et al., "\PMol: Pairwise Optimization through Graph Neural Networks"** treat optimization as a translation over SMILES **Srinivasan, "SMILES: Simplified Molecular Input Line Entry System"** strings, 
and learn the required modification from molecule pairs.
%incorporating task-specific guidance in different ways.
%
GraphGA **DeCao et al., "GraphGA: A Graph Neural Network for Chemical Optimization"** and MolLeo **Kusunoki et al., "MolLeo: Molecular Optimization through Evolutionary Algorithms"** leverage 
genetic algorithms
to evolve molecules via genetic algorithm. %for crossover and mutation operations.
% While effective in improving 1 or 2 properties, their efficacy in multi-property optimization tasks remains largely unexplored.
%
These methods__ **Wu et al., "Molecular Optimization through Graph Neural Networks"** often require designing non-trivial fitness or reward functions
to capture nuanced trade-offs among multiple properties. 
%
% However, designing such functions is highly non-trivial, requiring domain expertise to capture nuanced trade-offs. 
% %
% Pairwise approaches such as \PMol__ **Rajput et al., "\PMol: Pairwise Optimization through Graph Neural Networks"** circumvent this by learning from molecule pairs,
% but suffer from scalability due to limited task-specific labeled data. 
%
Moreover, such methods 
%focus on controllable molecule generation__ **Wu et al., "Molecule Generation with Graph Neural Networks"** , 
tends to generate
molecules with entirely new scaffolds, 
limiting their applicability \emph{in vitro} optimization.


%Similarly, f-RAG__ **Kusunoki et al., "f-RAG: Fragment Removal with Attention Guided for Molecular Optimization"** breaks down the molecules into hard fragments and soft fragments for molecular optimization, where hard fragments are explicitly included in generation while soft fragments guide the process through embedding fusion. 
%
%MIMOSA uses pre-trained GNNs for fragment modification prediction and MCMC sampling to iteratively search promising molecules based on weighted scoring of properties.
%
%Chemformer__ **Li et al., "Chemformer: Cheminformatics-Driven Transformer for Molecule Property Prediction"** and \PMol__ **Rajput et al., "\PMol: Pairwise Optimization through Graph Neural Networks"** approach optimization by translating the source molecules to the optimized molecules represented by SMILES strings
%
%Chemformer__ **Li et al., "Chemformer: Cheminformatics-Driven Transformer for Molecule Property Prediction"** incorporates task-specific information through direct task prompt prefixing of the source molecule's SMILES string, while \PMol__ **Rajput et al., "\PMol: Pairwise Optimization through Graph Neural Networks"** employs specialized task-specific atomic embeddings from a pre-trained GNN to guide the optimization process.
%
%GraphGA__ **DeCao et al., "GraphGA: A Graph Neural Network for Chemical Optimization"** and MolLeo__ **Kusunoki et al., "MolLeo: Molecular Optimization through Evolutionary Algorithms"** implement genetic-algorithm-based approaches, starting with a population of molecules with low property values and evolving them through predefined modification rules or by incorporating chemistry language models for crossover and mutation operations.
%

% \vishal{Computational methods
% for improving molecular properties__ **Wu et al., "Molecular Optimization through Graph Neural Networks"** have been extensively
% developed,
% where each method either modifies the structures of
% a given molecule (i.e., molecule optimization),
% or generates entirely new ones (i.e., molecule generation).
% %
% These methods vary in how they perform structural modifications
% and in their molecular representations. 
% For example,
% GraphGA__ **DeCao et al., "GraphGA: A Graph Neural Network for Chemical Optimization"** ,
% Modof__ **Morris et al., "Modof: Fragment Modification via Graph Attention"** ,
% MIMOSA__ **Huang et al., "MIMOSA: Molecular Informatics through Self-Attention"** ,
% and f-RAG__ **Kusunoki et al., "f-RAG: Fragment Removal with Attention Guided for Molecular Optimization"**
% perform fragment-level modifications on molecular graphs
% by removing, adding, and replacing 
% substructures at predicted disconnection sites.
% %
% In contrast, methods such as 
% SMILES GA__ **Srinivasan et al., "SMILES GA: A Genetic Algorithm for Molecule Generation"** ,
% MolLeo__ **Kusunoki et al., "MolLeo: Molecular Optimization through Evolutionary Algorithms"** ,
% Genetic-GFN__ **Xu et al., "Genetic-GFN: A Graph Neural Network for Chemical Synthesis"** and
% Prompt-MolOpt__ **Li et al., "Prompt-MolOpt: Task-Specific Molecule Generation using Prompt Engineering"**
% directly modify SMILES strings to generate new molecules
% with improved properties.
% %
% }

% \vishal{
% Most existing methods (except Modof and Prompt-MolOpt)
% are generation-based,
% and thus generate molecules with entirely 
% new scaffolds.
% This limits their applicability in realistic
% settings since retaining the core scaffold is a key requirement
% for \emph{in vitro} optimization.
% %
% Furthermore, these methods are primarily evaluated on improving
% single or double properties.
% Thus, their efficacy in more complex and realistic
% multi-property optimization tasks remain largely unexplored.
% }
%

% Multi-property molecule optimization __ **Li et al., "Multi-Objective Optimization of Molecules using Graph Neural Networks"** is a critical challenge in drug discovery due to the complex trade-offs between properties. 
%
% Furthermore, methods leveraging genetic algorithms __ **DeCao et al., "GraphGA: A Graph Neural Network for Chemical Optimization"** or reinforcement learning __ **Xu et al., "RL4MolOpt: Reinforcement Learning for Molecular Optimization"** 
% require design of non-trivial task-specific fitness or reward functions
% to model property trade-offs. 
% %
% % However, designing such functions is highly non-trivial, requiring domain expertise to capture nuanced trade-offs. 
% % %
% Pairwise approaches like Modof__ **Morris et al., "Modof: Fragment Modification via Graph Attention"** and \PMol__ **Rajput et al., "\PMol: Pairwise Optimization through Graph Neural Networks"** circumvent this by learning from molecule pairs, but are constrained by scare task-specific labeled data. 
% %
% Furthermore, %the generative nature of 
% most methods tend to generate molecules with new scaffolds, limiting their applicability, as retaining the core scaffold is a key requirement for \emph{in vitro} optimization.


% To address this, recent approaches like \PMol
% build separate models for each property combination (i.e., each optimization task),
% leading to poor scalability.
% These limitations underscore the need for a single unified model -- similar to a foundation model__ **Brown et al., "LLaMA: A Large Language Model"** --
% that can flexibly and effectively handle diverse
% optimization tasks.



% %
% Furthermore, the lack of labeled paired data in such multi-property optimization tasks 
% hinders effective training for pairwise optimization methods such as Modof and \PMol. 
% %
% The genetic algorithm-based methods__ **DeCao et al., "GraphGA: A Graph Neural Network for Chemical Optimization"** and reinforcement learning approaches__ **Xu et al., "RL4MolOpt: Reinforcement Learning for Molecular Optimization"** attempt to handle multiple objectives through weighted summation, designing such weights proves highly non-trivial and struggles to capture nuanced property relationships.
% %
% Beyond this, the generative nature of most methods (except Modof and \PMol) makes them tend to generate molecules with entirely new scaffolds. This limits their applicability in realistic settings since retaining the core scaffold is a key requirement for \emph{in vitro} optimization.
% %

Recently, LLMs__ **Brown et al., "LLaMA: A Large Language Model"** have emerged as a promising option for molecule optimization. 
%
For example,
ChatDrug__ **Li et al., "ChatDrug: Multi-Task Learning for Molecule Optimization"** and Re3DF__ **Rajput et al., "Re3DF: A Retrosynthetic Approach to Molecule Design"** leverage 
LLMs to optimize a molecule iteratively
through multi-turn dialogues.
% and incorporating domain knowledge through retrieval of similar molecules from databases and validation feedback from expert tools for iterative optimization. 
%
%Taking a different approach,
DrugAssist__ **Morris et al., "DrugAssist: Instruction-Tuned LLMs for Molecule Optimization"**
instruction-tuned Llama2-7B-Chat__ **Li et al., "Llama2-7B-Chat: A Large-Scale Language Model for Multi-Task Learning"** 
% to optimize a molecule iteratively
through multi-turn dialogues.
%
% and incorporating domain knowledge through retrieval of similar molecules from databases and validation feedback from expert tools for iterative optimization. 
%
% Taking a different approach,
DrugAssist__ **Morris et al., "DrugAssist: Instruction-Tuned LLMs for Molecule Optimization"**
instruction-tuned Llama2-7B-Chat__ **Li et al., "Llama2-7B-Chat: A Large-Scale Language Model for Multi-Task Learning"** 
% to optimize a molecule iteratively
through multi-turn dialogues.
%
% and incorporating domain knowledge through retrieval of similar molecules from databases and validation feedback from expert tools for iterative optimization. 
%
% Taking a different approach,
DrugAssist__ **Morris et al., "DrugAssist: Instruction-Tuned LLMs for Molecule Optimization"**
instruction-tuned Llama2-7B-Chat__ **Li et al., "Llama2-7B-Chat: A Large-Scale Language Model for Multi-Task Learning"** 
% to optimize a molecule iteratively
through multi-turn dialogues.
%
% and incorporating domain knowledge through retrieval of similar molecules from databases and validation feedback from expert tools for iterative optimization. 
%

%  Taking a different approach,
DrugAssist__ **Morris et al., "DrugAssist: Instruction-Tuned LLMs for Molecule Optimization"**
instruction-tuned Llama2-7B-Chat__ **Li et al., "Llama2-7B-Chat: A Large-Scale Language Model for Multi-Task Learning"** 
% to optimize a molecule iteratively
through multi-turn dialogues.
%
% and incorporating domain knowledge through retrieval of similar molecules from databases and validation feedback from expert tools for iterative optimization.