\section{Background} \label{section:LLM}

% \subsection{Large Language Model (LLM)}   

Figure~\ref{fig:LLaMA_model}(a) shows that a decoder-only LLM initially processes a user prompt in the “prefill” stage and subsequently generates tokens sequentially during the “decoding” stage.
Both stages contain an input embedding layer, multiple decoder transformer blocks, an output embedding layer, and a sampling layer.
Figure~\ref{fig:LLaMA_model}(b) demonstrates that the decoder transformer blocks consist of a self attention and a feed-forward network (FFN) layer, each paired with residual connection and normalization layers. 

% Differentiate between encoder/decoder, explain why operation intensity is low, explain the different parts of a transformer block. Discuss Table II here. 

% Explain the architecture with Llama2-70B.

% \begin{table}[thb]
% \renewcommand\arraystretch{1.05}
% \centering
% % \vspace{-5mm}
%     \caption{ML Model Parameter Size and Operational Intensity}
%     \vspace{-2mm}
%     \small
%     \label{tab:ML Model Parameter Size and Operational Intensity}    
%     \scalebox{0.95}{
%         \begin{tabular}{|c|c|c|c|c|}
%             \hline
%             & Llama2 & BLOOM & BERT & ResNet \\
%             Model & (70B) & (176B) & & 152 \\
%             \hline
%             Parameter Size (GB) & 140 & 352 & 0.17 & 0.16 \\
%             \hline
%             Op Intensity (Ops/Byte) & 1 & 1 & 282 & 346 \\
%             \hline
%           \end{tabular}
%     }
% \vspace{-3mm}
% \end{table}

% {\fontsize{8pt}{11pt}\selectfont 8pt font size test Memory Requirement}

\begin{figure}[t]
    \centering
    \includegraphics[width=8cm]{Figure/LLaMA_model_new_new.pdf}
    \caption{(a) Prefill stage encodes prompt tokens in parallel. Decoding stage generates output tokens sequentially.
    (b) LLM contains N$\times$ decoder transformer blocks. 
    (c) Llama2 model architecture.}
    \label{fig:LLaMA_model}
\end{figure}

Figure~\ref{fig:LLaMA_model}(c) demonstrates the Llama2~\cite{touvron2023llama} model architecture as a representative LLM.
% The self attention layer requires three GEMVs\footnote{GEMVs in multi-head attention~\cite{attention}, narrow GEMMs in grouped-query attention~\cite{gqa}.} to generate query, key and value vectors.
In the self-attention layer, query, key and value vectors are generated by multiplying input vector to corresponding weight matrices.
These matrices are segmented into multiple heads, representing different semantic dimensions.
The query and key vectors go though Rotary Positional Embedding (RoPE) to encode the relative positional information~\cite{rope-paper}.
Within each head, the generated key and value vectors are appended to their caches.
The query vector is multiplied by the key cache to produce a score vector.
After the Softmax operation, the score vector is multiplied by the value cache to yield the output vector.
The output vectors from all heads are concatenated and multiplied by output weight matrix, resulting in a vector that undergoes residual connection and Root Mean Square layer Normalization (RMSNorm)~\cite{rmsnorm-paper}.
The residual connection adds up the input and output vectors of a layer to avoid vanishing gradient~\cite{he2016deep}.
The FFN layer begins with two parallel fully connections, followed by a Sigmoid Linear Unit (SiLU), and ends with another fully connection.