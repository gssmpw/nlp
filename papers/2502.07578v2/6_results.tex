\begin{figure}[!b]
% \begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{Figure/Latency_Throughput_NEW.pdf}
    \caption{\att{} speedup over GPU baselines.}
    \label{fig:Main_results}
\end{figure}


% \begin{figure*}[b]
\begin{figure*}[t]
	\centering
  	% \includegraphics[width=\textwidth]{Figure/Lat_Breakdown_NEW.pdf}
  	% \includegraphics[width=17cm, trim={2mm 1mm 1mm 1mm}, clip]{Figure/Lat_Breakdown_NEW.pdf}
  	\includegraphics[width=17cm]{Figure/Lat_Breakdown_NEW.pdf}
	\caption{Analysis on Llama2-70B. 
 (a) \att{} achieves higher decoding throughputs with long context windows and 3584 decoding sizes (Section~\ref{section:methodology}). 
 (b) QoS analysis: \att{} provides lower query latency while achieving similar throughput as GPU. 
 (c) \att{} latency breakdown with different parallelism strategies.
 (d) Prefill (In) and decoding (Out) latency comparison with different In/Out sizes, at maximum supported batch size for both GPU and \att{}.
 }
\label{fig:Combo_Main_Lat_Breakdown}
\end{figure*}


\section{Results}

\subsection{\att{} versus GPU Baseline}~\label{subsec:main_results}

Figure~\ref{fig:Main_results} compares the performance of \att{} and our GPU baseline under two scenarios:
(a) \textit{Latency Critical:} We use a batch of 1 query (\att{}'s tensor parallel mapping).
In this case, \att{} reduces the end-to-end latency by 4.6$\times$ compared to GPUs.
This speedup is due to the higher internal memory bandwidth of PIM.
(b) \textit{Throughput Critical:} We use the maximum batch size of 128 for GPU experiments, as explained in Section~\ref{section:methodology}.
On the other hand, \att{} utilizes pipeline parallelism to enable batches of 32/40/80 queries for the three models (batch size = pipeline stages).
Using this configuration, \att{} achieves a geomean of 2.3$\times$ higher end-to-end throughput across three models.
\att{} demonstrates 1.2$\times$ speedup on Llama2-70B because this model applies the grouped-query attention technique~\cite{gqa}, improving the operational intensity of the attention layers.
Figure~\ref{fig:Main_results}(c) shows that \att{} processes 5.2$\times$ higher tokens per dollar than GPU, which attributes to \att{}'s higher throughput and 2.5$\times$ cheaper TCO (Table~\ref{tab:Hardware configurations}).





\begin{figure*}[t]
    \centering
    % \includegraphics[width=\linewidth]{Figure/CENT_GPU_Power_NEW.pdf}
    \includegraphics[width=17cm]{Figure/CENT_GPU_Power_NEW.pdf}
  	% \vspace{-2mm}
    \caption{(a) Power consumption of \att{} and GPU (b) GPU SM frequency and board power, and (c) energy efficiency (Tokens per Joule) of \att{} and GPU
    % for different stages of Llama2 models~\cite{touvron2023llama} 
    using the maximum batch size, 512 prefill tokens and 3584 decoding tokens.}
	\label{fig:Energy_Power}
\end{figure*}

% \red{
Figure~\ref{fig:Main_results}(b) compares throughput in the prefill and decoding stages.
GPU achieves 2.5$\times$ higher throughput in the compute-intensive prefill stage than \att{} due to GPU's 2.0$\times$ higher peak compute throughput.
Conversely, \att{} outperforms GPU in the memory-intensive decoding stage by 2.5$\times$ due to PIM's higher internal memory bandwidth.
Notably, the prefill stage accounts for only 2$\%$ of the total GPU end-to-end processing time, so the overall LLM inference throughput closely aligns with that of the decoding stage.
% }

% \att{}'s speed-up is less than the batch of 1 due to the higher utilization of GPUs in larger batches.



\begin{figure*}[t]
	\centering
  	% \includegraphics[width=\textwidth]{Figure/Comparison_related_works.pdf}
  	\includegraphics[width=17cm]{Figure/Comparison_related_works.pdf}
  	% \vspace{-3mm}
	\caption{(a) \att{} employs vector units near PIM modules and utilizes a CXL switch to interconnect PIM devices with novel CXL communication primitives. (b) CXL-PNM~\cite{cxl-pnm} applies a processing-near-memory solution \emph{without} integrating compute logic into DRAM chips. (c-d) AttAcc~\cite{AttAcc} and NeuPIM~\cite{NeuPIM} are heterogeneous systems comprising GPUs and PIM devices.
    % with peer-to-peer interconnection between the PIM device and GPU. NeuPIM integrates a TPUv4-like NPU near PIM modules.
    }
\label{fig:Comparison_related_works}
% \vspace{-6mm}
\end{figure*}

\textbf{\att{} performs better than GPU in long context scenarios.}
The results in Figure~\ref{fig:Main_results} use a 4K context window.
However, \sota{} LLMs support longer contexts, ranging from 128K to 1M tokens~\cite{gpt4-turbo, gemini-pro}.
As discussed in Section~\ref{GPU Performance Characterization}, with longer contexts, the GPU system saturates at smaller batch sizes, from batch=128 at 4K context to batch=16 at 32K context.
On Llama2-70B, \att{} achieves higher speedup than GPUs as context length increases, attaining up to 3.3$\times$ speedup in decoding throughput for a context length of 32K, as shown in Figure~\ref{fig:Combo_Main_Lat_Breakdown}(a). 

\textbf{\att{} has lower query latency than GPU at similar throughput.}
Figure~\ref{fig:Combo_Main_Lat_Breakdown}(b) illustrates our QoS comparison on Llama2-70B.
These results are collected with different batch sizes on GPUs and different TP/PP mapping strategies on \att{}.
\att{} provides 3.4-7.6$\times$ lower query latency while achieving similar throughput to the baseline GPU.

% \textbf{Latency breakdown of the \att{} architecture} with different TP/PP mapping strategies is shown in Figure~\ref{fig:Combo_Main_Lat_Breakdown}(c). 
\textbf{Latency Breakdown.} Figure~\ref{fig:Combo_Main_Lat_Breakdown}(c) shows \att{}'s latency breakdown with different TP/PP mapping strategies.
PIM latency always dominates because most of the operations are mapped to PIM channels. As TP increases (from top to bottom), PIM latency reduces.
This is because more PIM channels are allocated to a single transformer block.
Yet, CXL communication latency increases with higher TP, because distributing a transformer block across more CXL devices necessitates more broadcast and gather transactions. 
% \red{
Figure~\ref{fig:Combo_Main_Lat_Breakdown}(d) depicts the latency comparison between \att{} and GPU at maximum supported batch sizes.  
Compared to GPU, \att{} shows 1.4$\times$ higher latency in the prefill stage and 1.7-2.0$\times$ lower latency in the decoding stage. Decoding latency dominates the end-to-end latency.
% }
% The host CPU latency is negligible.




\subsection{Power and Energy Consumption Analysis}\label{subsec:power_results}

% \red{
We developed an \textit{activity-based} power model for \att{}. 
% The maximum power of a CXL device is 63.7W, with PIM units accounting for 79\%.
% However, the PIM units do not consume energy during the DRAM activation and precharge periods, resulting in an average power of 32.4W per CXL device.
When deploying the Llama2-70B model on 32 CXL devices with the pipeline parallel model mapping, 27 devices are used. Among 80 transformer blocks (80 pipeline stages), 3 of them are mapped to each device, resulting in an average power of 32.4W per device. PIM operations and activation/precharge commands consume 54.5\% and 30.2\% of power, respectively.
% }

% \red{
Similarly, we used \texttt{nvidia-smi} to measure GPU power during the prefill and decoding stages in 100ms intervals. Figure~\ref{fig:Energy_Power}(a) illustrates the average power consumption of \att{} versus Nvidia A100 80GB GPUs. 
% We use 8/20/32 \att{} devices and 1/2/4 GPUs for Llama2 7B/13B/70B models~\cite{touvron2023llama}.
% This figure shows that CENT and GPU systems consume similar average paper. 
\textit{One} A100 GPU consumes $\approx8\times$ higher power than \textit{one} \att{} device. Modern GPUs consume significantly higher power as they support general-purpose PTX ISA~\cite{PTX-ISA}, a large number of Streaming Multiprocessors (108 SMs in A100), multithreading with fast context switching, and a multi-level cache hierarchy ($\approx$ 60 MB in A100~\cite{a100}). In contrast, \att{} is a custom architecture with minimal silicon used for near-bank compute units. 
% Based on these activity-based power measurements, we utilize 8 \att{} devices for each A100 GPU.
% }

% \att{} consumes $11.3\%$ less power in the prefill stage than decoding.
% This is because [?].
% However, 




% \red{
GPUs operate near their thermal design power (TDP) of 300W~\cite{a100} during both the prefill and decoding stages when processing a large batch size of 128 queries.
Figure~\ref{fig:Energy_Power}(b) illustrates this by showing the GPU’s SM clock frequency and board power consumption for the Llama2-7B model.
During vLLM~\cite{vLLM} initialization, the clock frequency is maximized at 1410 MHz due to low compute throughput and memory bandwidth utilization.
In the prefill stage, high SM utilization signals the GPU’s power manager to throttle the clock frequency, maintaining power consumption within the TDP.
During the decoding stage, reduced SM utilization allows for an increase in clock frequency. A higher clock rate and memory bandwidth usage keep power near the TDP.
% }



% \red{
Figure~\ref{fig:Energy_Power}(c) shows that \att{} processes 2.3$\times$ more \emph{tokens per Joule} than GPU, on average.
In the compute-bound prefill stage, GPU is 2.3$\times$ more energy efficient, as it achieves efficient data reuse in the on-chip SRAM.
% , as opposed to \att{}, which accesses data directly from DRAM banks.
In the memory-bound decoding stage, \att{} achieves 2.5$\times$ higher energy efficiency, while GPU cannot efficiently reuse data in the SRAM because of the low operational intensity.
% However, GPU fetches KV-caches from the off-chip DRAM to on-chip buffers without any reuse.
Our evaluation shows that \att{} consumes 0.6 pJ/bit on \texttt{MAC\_ABK} operations, making it $6.6\times$ more energy efficient than even \textit{only} the HBM2 memory read accesses of GPU, which consumes 3.97 pJ/bit~\cite{o2017fine}.
% }
% Our evaluation shows that \att{} consumes 1314 pJ per \texttt{MAC\_ABK} operation (0.16 pJ/b), making it $3.5\times$ more energy efficient than just the HBM2e read accesses of GPU that consume 0.56 pJ/b~\cite{hbm2e_energy}.}



\subsection{\att{} versus PIM/PNM Baselines}


We compare \att{} with the \sota{} CXL-PNM~\cite{cxl-pnm} and heterogeneous GPU-PIM baselines~\cite{AttAcc, NeuPIM}. Figure~\ref{fig:Comparison_related_works} provides an architectural overview of these systems. 






% \textbf{\att{} versus CXL-PNM.} Figure~\ref{fig:Comparison_related_works}(b) shows that CXL-PNM~\cite{cxl-pnm, samsung_pimpnm} is a processing-near-memory (PNM) platform that leverages a CXL controller to manage eight LPDDR5X packages within a single device. The CXL controller deploys matrix and vector units to perform computations near commodity LPDDR5X chips. This configuration allows for memory expansion up to 512GB per device.
% CXL-PNM is a near-memory solution that does not modify the LPDDR5X DRAM chips. 
% When compared to \att{}, CXL-PNM provides larger memory capacity but limited compute throughput and memory bandwidth. 

\textbf{\att{} versus CXL-PNM.} Figure~\ref{fig:Comparison_related_works}(b) shows that CXL-PNM~\cite{cxl-pnm, samsung_pimpnm} is a processing-near-memory (PNM) platform that leverages a CXL controller to manage eight LPDDR5X packages within a single device. The CXL controller deploys matrix and vector units to perform computations \emph{near} commodity LPDDR5X chips. 
% This configuration allows for memory expansion up to 512GB per device.
In contrast, Figure~\ref{fig:Comparison_related_works}(a) depicts \att{}, which utilizes processing-in-memory (PIM) technology to place compute logic adjacent to DRAM banks \emph{within} DRAM chips. Figure~\ref{fig:CXL_PNM_OPT}(b) shows that compared to CXL-PNM, \att{} provides significantly higher compute throughput (TFLOPs) and memory bandwidth (TB/s), at the cost of less memory capacity (GB).
Figure~\ref{fig:CXL_PNM_OPT}(a) illustrates that \att{}'s higher compute and memory bandwidth results in 4.5$\times$ higher throughput than CXL-PNM, at the maximum supported batch sizes for each system.

\begin{figure}[h]
	\centering
  	\includegraphics[width=8cm]{Figure/CXL_PNM_OPT.pdf}
    \caption{\att{} and CXL-PNM baseline comparison on OPT-66B~\cite{opt} with prefill=64 and decoding=1024.}
	\label{fig:CXL_PNM_OPT}
\end{figure}

% In contrast to CXL-PNM, \att{} utilizes processing-in-memory (PIM) technology, to place compute logic directly adjacent to DRAM banks in each DRAM chip(Figure~\ref{fig:Comparison_related_works}a). Figure~\ref{fig:CXL_PNM_OPT}(b) illustrates \att{}'s advantage in terms of compute capacity (TFLOPs) and internal memory bandwidth (TB/s). At maximum supported batch sizes for each system, \att{} delivers 4.5$\times$ higher throughput than CXL-PNM~\cite{cxl-pnm, samsung_pimpnm} as shown in Figure~\ref{fig:CXL_PNM_OPT}(a). 
% Each CXL-PNM device has an average power consumption of 77W, while \att{} device has 33W.
% }

% \red{
% Additionally, \att{} introduces novel CXL communication primitives specifically designed for inter-device communication via a CXL switch. These primitives allow independent execution of LLM inference on CXL devices without host intervention. 
%CXL-PNM relies on the standard CXL memory expansion protocol for intra-device communication and uses the PCIe protocol via the host for inter-device communication. In comparison,
% }


% \red{
\textbf{\att{} versus GPU-PIM.} AttAcc~\cite{AttAcc} and NeuPIM~\cite{NeuPIM} are heterogeneous systems consisting of GPUs and PIM devices as shown in Figure~\ref{fig:Comparison_related_works}(c) and (d). The AttAcc system consists of 8 A100 GPUs with HBM3 memory~\cite{dgx-a100} and 8 HBM-PIM devices. Each HBM-PIM device consumes 116W and has a memory capacity of 80GB. The NeuPIM device integrates a TPUv4-like NPU~\cite{tpu} architecture near PIM modules and extends PIM with dual row buffers, enabling concurrent PIM-NPU memory access.  The evaluated NeuPIM platform comprises 8 A100 GPUs and 8 NeuPIM devices. 
% }

% \ali{"CENT has 3.5× and 2.6× lower TCO than AttAcc and NeuPIM (Section 6)." Sec 7 is all about CENT and GPU TCO. How did we calculate AttAcc and NeuPIM TCO? Shouldn't we mention it?}



% \red{
Distinct from these systems, \att{} introduces a GPU-free inference server, providing an alternative cost-effective solution and eliminating the need for expensive GPUs. In GPU-PIM systems, the prefill stage is mapped to GPUs while the remaining computation is mapped to the PIM subsystem. \att{} does \emph{not} employ GPUs for the prefill stage for various reasons. \textit{First}, end-to-end LLM inference performance is primarily constrained by the decoding phase rather than the prefill phase; only 2\% of the total GPU's inference time is taken by the prefill stage across Llama2 models, on average (Section~\ref{subsec:main_results}). \textit{Second}, CENT’s compute throughput is not much worse than GPU ($\approx$49\%, Table~\ref{tab:Hardware configurations}). \textit{Third}, using expensive GPUs solely to support the prefill stage is a costly option. 
Using the methodology from Section~\ref{TCO}, we find that the TCO of AttAcc and NeuPIM is 3.5$\times$ and 2.6$\times$ higher than \att{}, respectively. The cost of HBM-PIM is estimated at 10$\times$ the price of HBM~\cite{HBM-price}, while the NPU cost is modeled based on die, 2.5D packaging, and NRE costs~\cite{tpu, palesko2014cost, moonwalk}.



% \red{
Figure~\ref{fig:AttAcc_NeuPIM} shows the performance of \att{} versus AttAcc and NeuPIM.
For a power-neutral evaluation, we assume 12 \att{} devices per GPU-PIM node. Across different sequence lengths and batch sizes, the blue bars show that \att{} processes 1.8-3.7$\times$ and 1.8-5.3$\times$ more tokens per dollar than AttAcc and NeuPIM systems, respectively. The orange dots show that \att{}'s raw throughput (Tokens/s) is 0.5-1.1$\times$ and 0.7-2.1$\times$ the throughput of AttAcc and NeuPIM, respectively. 
%The \att{} system described above achieves approximately 60\% of the peak compute throughput compared to GPUs (NPUs), with 1536 TFLOPS compared to 2496 (2200) TFLOPS. 
In scenarios with short sequence lengths, query batching enhances operational intensity in FC layers, improving performance on GPUs (or NPUs) with more TFLOPs. However, in cases with long sequence lengths that limit batch sizes, \att{} maintains higher raw throughput than the GPU-PIM baselines. Latest LLM models typically support 128K context windows~\cite{gpt4-turbo}. With these extended context lengths, we expect \att{} to provide even higher performance.
% }
%It should be noted that the AttAcc evaluation is restricted to a 4K sequence length, and NeuPIM uses the ShareGPT dataset, featuring an average input size of 80 and output size of 296. However, latest LLM models typically support 128K context windows. With these extended context lengths, attention layers with low operational intensity tend to dominate query latency. Additionally, the increased KV cache sizes limit the number of batches that can be supported due to memory constraints, which further reduces the operational intensity in FC layers. Under these conditions, \att{} is anticipated to exhibit even higher performance than GPU-PIM baselines.


\begin{figure}[t]
	\centering
  	\includegraphics[width=\columnwidth]{Figure/AttAcc_NeuPIM.pdf}
  	% \includegraphics[width=8cm]{Figure/AttAcc_NeuPIM.pdf}
  	% \vspace{-3mm}
	\caption{\att{} versus GPU-PIM (a) \att{} and AttAcc systems are evaluated on the GPT3-175B model across various input and output sizes, tested at the maximum supported batch sizes. 
 (b) The \att{} and NeuPIM systems are evaluated on GPT3-175B with data-parallel mapping (DP=4) and pipeline-parallel mapping (PP=4), respectively, using the ShareGPT dataset~\cite{sharegpt}. NeuPIM uses different batch sizes while \att{} uses the maximum supported batch size 96.}
\label{fig:AttAcc_NeuPIM}
% \vspace{-6mm}
\end{figure}

%The quantitative comparisons between these systems are detailed as follow.



 %The AttAcc system consists of 8 A100 GPUs equipped with HBM3 memory and 8 PIM devices. Each GPU and PIM device consumes $\sim$300W and 116W, respectively.   TPUv4 has an average power consumption of 197W when running BERT~\cite{tpu}. Additionally, the dual row buffer architecture introduces a 1.8$\times$ power overhead. Considering that each \att{} device consumes an average of 33W, we compare these systems with 12 \att{} devices for a power-neutral evaluation. In GPU-PIM systems, the prefill stage is mapped to GPUs, while \att{} maps both prefill and generation to PIM devices.

 
 




% CENT achieves 1.14-4.20$\times$ higher throughput than NeuPIM with different batch sizes on the ShareGPT dataset~\cite{sharegpt}. 
% \att{} has 2$\times$ near-bank processing units (PU) frequency because of different memory types (HBM2 and GDDR6), adding up to 8$\times$ PIM performance in attention layers by 8 devices. In FC layers, \att{} shows 8$\times$

% to align with the average power of 1 A100 GPU, the baseline of NeuPIM.
% Figure~\ref{fig:PIM_baselines}(a) shows that \att{} achieves 1.14-4.20$\times$ higher throughput than NeuPIM with different batch sizes on the ShareGPT dataset~\cite{sharegpt}. 

% \begin{figure}[htbp]
% % \vspace{-4mm}
% \centering
%     \includegraphics[width=\columnwidth]{Figure/NeuPIM_GPT3_TP8_PP4.pdf}
%   	% \vspace{-3mm}
%     \caption{[TODO]PIM baseline comparison on GPT3-175B~\cite{brown2020gpt3}. }
% 	\label{fig:NeuPIM_GPT3_TP8_PP4}
% % \vspace{-3mm}
% \end{figure}


% \begin{figure}[htbp]
% % \vspace{-4mm}
% \centering
%     \includegraphics[width=\columnwidth]{Figure/AttAcc_GPT3.pdf}
%   	% \vspace{-3mm}
%     \caption{[TODO]PIM baseline comparison on GPT3-175B~\cite{brown2020gpt3}. }
% 	\label{fig:AttAcc_GPT3}
% % \vspace{-3mm}
% \end{figure}


% We compare \att{} against two HBM-PIM solutions for LLM inference: NeuPIM~\cite{NeuPIM} and AttAcc~\cite{AttAcc}.
% Both works build heterogeneous systems, mapping fully connected layers to GPU/NPU and attention layers to PIM.
% In contrast, we use the cheaper GDDR6-PIM and use a hierarchical PIM-PNM architecture to support the entire transformer block on the CXL device, eliminating the need for expensive GPUs.



% AttAcc system consists of 8 A100 GPUs and 640GB HBM3-PIM.
% Considering that \att{} has a similar average power to 4 A100 GPUs, for this comparision, we use a host machine to drive 3 \att{} systems listed in Table~\ref{tab:Hardware configurations}.
% As a GPU-free system, \att{} has 5.3$\times$ cheaper TCO than AttAcc.
% Figure~\ref{fig:PIM_baselines}(b) shows that compared to AttAcc, \att{}  generates 2.7-5.9$\times$ more tokens per dollar while showing 0.5-1.1$\times$ raw throughput.

% In longer sequences and lower batch sizes, the performance of GPUs in the AttAcc and NeuPIM systems drop for the fully connected layers.
% So, \att{} outperforms for LLMs with extensive context lengths due to a lower batch size.


\subsection{Design Space Exploration}

% \ali{shouldn't we explain why utilization drops or throughput remains the same? I mean we do not split a block between multiple devices. In addition, I think we should say that we use PP=\#blocks and TP=1 for the highest throughput.}

% \red{
\att{} can interconnect a flexible number of CXL devices, allowing for scalable system configurations. Figure~\ref{fig:Scalability} shows the scalability of \att{} on Llama2-70B from 16 to 128 devices, with throughput increasing from 0.68 K tokens/s to 5.7 K tokens/s. We start with pipeline-parallel (PP) mapping and then apply various levels of data-parallel (DP) mapping to further boost the throughput as the \att{} system scales up. 
As the number of devices increases, the throughput reaches intermittent plateaus at certain points. This is due to the inefficiency of distributing transformer blocks across CXL devices. For example, 80 transformer blocks in the Llama2-70B model can be allocated to 40 devices, with two blocks per device. Expanding from 40 to 44 devices results in a distribution of 1.8 blocks per device. Yet, dividing a single block across multiple CXL devices introduces substantial inter-device communication overhead, ultimately reducing performance. To mitigate this, we maintain the same block distribution with 44 devices as with 40, leaving the remaining 4 devices idle. 
% This approach results in a plateau in throughput and a decline in device utilization. 
%Applying data-parallel (DP) mapping helps reduce this mapping fragmentation, further boosting throughput and maintaining high device utilization.
% though it does not entirely eliminate the inefficiencies noted.
% }

% We use pipeline parallel mapping (PP=80) to support the maximum batch size.
% % Pipeline parallel (PP) and data parallel (DP) mappings demonstrate optimal performance with small and large number of devices, respectively.
% With device number increasing, PP mappings exhibit lower resource utilization due to the inefficiency of splitting transformer blocks across CXL-PIM devices.
% % Transformer blocks are not split across CXL devices to reduce inter-device communication overhead, which leads to a throughput plateau and a decrease in device utilization. 
% Applying DP mappings on top of PP can further boost throughput and maintain high device utilization.
% % As the number of devices increases, data parallel (DP) mapping becomes essential to maintain high device utilization, given the challenges of efficiently distributing transformer blocks across CXL devices. 
% However, DP mapping requires the model parameter duplication, making it infeasible for configurations with a small number of devices.
% The \att{} system with 32 devices studied in prior sections is marked by stars.
% % The configurations highlighted with black circles represent the preferred setups for optimal performance.

% Data parallel (DP) and pipeline parallel (PP) mappings demonstrate optimal performance with smaller and larger numbers of devices, respectively. With device number increasing, PP mapping exhibits lower resource utilization due to the inefficiency of splitting transformer blocks across CXL-PIM devices. DP mapping requires multiple copies of model parameters, making it infeasible to a small number of devices.

\begin{figure}[h]
% \vspace{-3mm}
	\centering
  	\includegraphics[width=8cm]{Figure/Scalability.pdf}
  	% \includegraphics[width=\columnwidth]{Figure/Scalability.pdf}
  	% \vspace{-3mm}
    \caption{\att{} scalability study on Llama2-70B.}
	\label{fig:Scalability}
% \vspace{-3mm}
\end{figure}

The scalability of CXL devices is constrained by two primary factors: (1) The number of lanes and ports provided by a CXL switch. For example, a commercial PCIe 5.0 switch can accommodate up to 144 lanes and 72 ports~\cite{pcie5-switch}. (2) The maximum power supply available for the server, such as the DGX A100's peak input power of 6.5 kW~\cite{dgx-a100}. 
Due to these constraints, the \att{} system with a single switch can support up to 64 devices per server.
A larger number of devices can be driven by multi-socket CPUs or a memory pool implementation facilitated by two levels of CXL switches.

\subsection{Generality}

LLMs exhibit similar architectures but differ in their specific implementations of activation functions and positional encodings. \att{} is designed to support a variety of activation functions, including GeLU~\cite{GeLU}, Swish~\cite{Swish}, and their GLU variants~\cite{SwiGLU}. This versatility is achieved by decomposing these functions into fundamental non-linear operations, such as \texttt{sigmoid} and \texttt{tanh}, which are supported through lookup tables, as well as through basic PIM and RISC-V operations. Moreover, \att{} is capable of accommodating different types of positional embeddings, including both absolute~\cite{alibi} and relative~\cite{rope-paper} implementations. The integration of general-purpose RISC-V cores within the \att{} system opens up possibilities for further enhancements and optimizations of LLMs in the future.

% To demonstrate \att{}'s generality for LLMs, we deploy BLOOM~\cite{scao2022bloom} on \att{} with 32 CXL devices and compare it with a setup of 8 NVIDIA A100 80GB GPUs~\cite{bloom-fast}. \att{} achieves a 6.5$\times$ speedup in the latency-optimized mapping over the GPU baseline with a batch size of 1. While in the throughput prioritized mapping, \att{} generates 1.6$\times$ more tokens per second than GPUs with a batch size of 32. \att{} shows less throughput speedup on BLOOM than Llama2 because the BLOOM baseline uses 8 GPUs with a memory capacity of 640 GB, enabling a batch size up to 32. In contrast, \att{} only has a 512 GB memory capacity with 32 devices. We discuss the scalability of \att{} in the next section. 

% LLM inference includes both prompt summarization and token generation. \att{} only supports the latter stage, while the one-time summarization process is performed by host.

