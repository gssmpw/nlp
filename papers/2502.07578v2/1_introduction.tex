\section{Introduction}

Generative Artificial Intelligence (GenAI) has become pivotal in transforming a myriad of sectors. In the realm of content creation, Large Language Models (LLMs)~\cite{openai2023gpt4, claude, gemini-pro, touvron2023llama} provide assistance in writing, summarizing, and translating across diverse languages, revolutionizing the way textual content is produced.
LLMs are reshaping various fields in daily life, such as generating creative arts~\cite{dalle3, sora}, customer services through chatbots, generating code and debugging assistance in software development~\cite{kasneci2023chatgpt}. 
% The widespread integration of LLMs has profound influence across diverse domains. 
However, harnessing the power of LLMs presents substantial economic challenges, underlined by their significant resource requirements. A business cost model indicates that running ChatGPT inference tasks requires $\sim$3617 HGX A100~\cite{hgxa100} servers and costs $\sim$\$694,444 per day~\cite{chatgpt-cost}. 
Therefore, efficient and cost-effective server farms play a critical role in the broader adoption and practical application of LLMs. 


Decoder-only LLMs have witnessed exponentially larger parameter sizes. 
At the same time, LLMs use key-value (KV) caches to store context information, and modern LLMs support context windows from 128K to 1M to generate versatile texts, audios, and videos~\cite{gpt4-turbo, gemini-pro}.
Both the model parameters and KV caches require a large memory capacity. To meet this demand, advanced GPU stations feature multiple GPUs. However, the computational resources of multi-GPU systems are often underutilized in LLM inference tasks. Unlike earlier ML models, LLMs exhibit lower operational intensity characteristics, necessitating high memory bandwidth, primarily due to the sequential token generation and the lack of inherent parameter reuse. Although batching strategies could mitigate this issue, KV caches specific to each user require large memory capacity, limiting the feasibility of high batch sizes.
Hence, the expensive compute throughput of GPUs and custom ML accelerators is significantly under-utilized for LLM inference because of the limited external memory bandwidth. 
As a result, \emph{users pay for expensive computing resources for memory-bound LLM inference tasks.}

\label{PIM}
The high cost and low compute utilization of GPU systems motivate an alternative solution for LLM inference tasks. Processing-In-Memory (PIM) architectures~\cite{aim1, aim2, aim3, aim4, fimdram, upmem, axdimm, to-pim, floatpim, sky, ultra, impala, aquabolt, sparsep, binary} place processing units (PU) adjacent to DRAM banks within memory chips, facilitating a significantly higher internal bandwidth.
However, near-bank PUs, fabricated in the DRAM process, impose a high area overhead that reduces the memory density.
A lower memory density is especially detrimental to LLMs with large memory requirements.
On the other hand, Processing-Near-Memory (PNM) architectures~\cite{cxl-pnm, samsung_pimpnm, noise, recnmp, enmc, HB-PNM, nda, trim, gomez2023evaluating, oliveira2022accelerating, gomez2022benchmarking} employ compute units near memory chips, \textit{e.g.,} in memory controllers.
PNM units are manufactured using CMOS process, offering more area-efficient compute capability at the cost of lower memory bandwidth compared to PIM.

To address these challenges, \att{} exploits Compute eXpress Link (CXL)~\cite{CXL} based memory expansion to provide the requisite memory capacity for LLMs.
\att{} establishes a practical CXL network to interconnect CXL devices.
Each CXL device consists of 16 memory chips, with each chip containing two GDDR6-PIM channels, and compute units near these memory chips (PNM).
% These devices are composed of compute units both near memory banks (PIM) and near memory chips (PNM).
This hierarchical PIM-PNM design supports the entire transformer block computation, eliminating the need for expensive GPUs.

\att{} uses a CXL switch to connect multiple CXL devices, that are driven by a host CPU.
% The \textit{inter-device communication} happens across CXL devices with CXL transactions via CXL ports.
The \textit{inter-device communication} is enabled by CXL transactions~\cite{CXL}.
The \textit{intra-device communication} between PIM chips and PNM units is supported through a \rf{}.
Using these protocols, \att{} provides peer-to-peer and collective communication primitives such as \textit{send/receive}, \textit{broadcast}, \textit{multicast} and \textit{gather}.
These primitives enable various parallelism strategies, efficiently distributing LLMs across CXL devices.
In \textit{Pipeline Parallel (PP)}~\cite{gpipe} mapping, we assign each transformer block to multiple memory channels within a single CXL device, facilitating the concurrent processing of multiple prompts on different pipeline stages.
PP prioritizes inference throughput to accommodate a large user base.
In \textit{Tensor Parallel (TP)}~\cite{alpa, megatron} mapping, we distribute a transformer block across all CXL devices.
TP focuses on reducing latency for real-time applications, providing smooth user experiences~\cite{fowers2018configurable}. 
We also explore hybrid TP-PP mappings to strike a balance between the latency and throughput.

Within a CXL device, we introduce the detailed mapping of a transformer block onto the hierachical PIM-PNM architecture. 
In PIM chips, near-bank PUs incorporate Multiply-Accumulate (MAC) units, which support more than 99\% of the arithmetic operations within a transformer block. 
The PNM units are composed of accelerators and RISC-V cores to perform other special and complex operations, such as Softmax, square root, and division. The integration of RISC-V cores allows for the flexible support of a wide range of LLMs.


In summary, this paper makes the following contributions:
\begin{itemize}
\item We propose \att{}, a GPU-free system that uses CXL memory expansion to accommodate the considerable memory capacity requirements of LLMs. We design a hierarchical PIM-PNM architecture to support the entire transformer block computation, eliminating the need for expensive GPUs.

\item We introduce a scalable CXL network to support collective and peer-to-peer communication primitives.
We describe the mapping of LLM parallelization strategies across CXL devices based on the CXL communication primitives.

\item We evaluate \att{} on Llama2~\cite{touvron2023llama} models. 
Compared to state-of-the-art GPUs with maximum supported batch sizes and similar average power, \att{} achieves 2.3$\times$ higher throughput and consumes 2.3$\times$ less energy. 
\att{} exhibits a lower Total Cost of Ownership (TCO), generating 5.2$\times$ more tokens per dollar than GPUs\footnote{Open-source \att{} simulator \href{https://github.com/Yufeng98/CENT/}{https://github.com/Yufeng98/CENT/}}.
\end{itemize}

\att{} is evaluated on Llama2 70B with a context length of up to 32K, but it can show higher benefits for larger model sizes and extended context lengths. As model sizes scale up, such as Grok 314B~\cite{grok}, Llama3 405B~\cite{llama3}, and DeepSeek-V3 671B~\cite{deepseek-v3}, inference serving demands significantly more hardware resources. In such cases, \att{} offers greater cost-efficiency compared to GPUs.


In reasoning tasks~\cite{O1-reasoning, deepseek-r1} and video generation~\cite{sora, polyak2024movie, gemini-pro}, where context length can range from tens of thousands to 1 million tokens, \att{} achieves higher throughput speedup due to its high memory bandwidth, which enhances memory-bound attention computations. Notably, GPUs can still benefit from long-text and video understanding tasks, as the prefill stage exhibits high operational intensity. In these scenarios, prefill and decoding processes can be disaggregated between GPUs and \att{}, respectively~\cite{zhong2024distserve, patel2024splitwise}.