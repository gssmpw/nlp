\section{Motivation}~\label{sec:motivation}

% LLMs are characterized by limited operational intensity, rendering them memory-bound and leading to suboptimal GPU utilization.
% At the same time, the exponential growth in LLM parameters necessitates many GPUs to accommodate the requisite memory capacity.
% Hence, LLM service providers pay significant costs for substantial computational throughput of multiple GPUs, which remains largely under-utilized.

% \red{
The exponential growth of LLM parameters requires multi-GPU systems to accommodate the requisite memory capacity. However, LLMs exhibit limited operational intensity, making them memory-bound and resulting in suboptimal GPU utilization. Consequently, LLM service providers are paying significant costs for substantial computational throughput of multiple GPUs, which remains largely under-utilized.
% }

% LLMs are large (context length)
\textbf{High Memory Capacity Requirement.} LLM parameter size has witnessed an exponential increase from Billion to Trillion magnitudes, far surpassing previous Machine Learning (ML) models.
In addition, the context windows that modern LLMs support range from 128K to 1M~\cite{gpt4-turbo, gemini-pro}, enabling them to understand and generate longer contents.
% The long context window results in a large KV cache. Further, the KV cache is unique for each user, which requires large memory capacity when scaling up the inference batch size.
% \red{
The long context window results in large KV caches, requiring substantial memory capacity.
These KV caches are unique to each user, further limiting the ability to scale up the inference batch size due to the memory capacity requirement.
% }

\textbf{Low Operational Intensity.} LLM inference has two stages:
(a) The \textit{prefill} stage concurrently encodes input tokens within a prompt using matrix-matrix multiply (GEMM) operations.
(b) The \textit{decoding} stage decodes output tokens sequentially with matrix-vector multiply (GEMV) operations.
The operational intensity of GEMV is substantially lower than GEMM.
To mitigate this, several techniques are applied.
Batching strategies combine GEMV operations across multiple queries of a batch into GEMM operations.
This technique improves the operational intensity non-linearly because attention calculations rely on unique KV caches of each prompt.
Grouped-query attention~\cite{gqa} merges multiple GEMV operations into narrow GEMM, but its operational intensity still remains less than the GPU capabilities.

\label{GPU Performance Characterization}
% GPUs have limited utilization
\textbf{GPU Performance Characterization.} We use vLLM~\cite{vLLM}, the \sota{} inference serving framework, to study the effect of batch size and context length on 4 Nvidia A100 80GB GPUs running the Llama2-70B model~\cite{touvron2023llama, longlora}.
Figure~\ref{fig:Context_Length} shows that inference throughput improves with larger batch sizes but reaches a plateau once the memory requirement exceeds the GPU memory size.
As context length increases, inference throughput saturates with even smaller batch sizes, from batch=128 at 4K context length to batch=8 at 32K context length.
Moreover, Figure~\ref{fig:GPU_utilization}(a) shows that LLM inference query latency increases with larger batch sizes and longer contexts, violating a realistic query latency Service Level Agreements (SLA) constraint~\cite{mlperf-sla}.

\begin{figure}[h]
\centering
    \includegraphics[width=\columnwidth]{Figure/Context_Length_New.pdf}
    \caption{Llama2-70B~\cite{touvron2023llama, longlora} inference throughput and memory requirement on 4 A100 80GB GPUs.}
    \label{fig:Context_Length}
\end{figure}

Figure~\ref{fig:GPU_utilization}(b) compares the GPU compute utilization of an LLM (Llama2-70B~\cite{touvron2023llama, longlora}) with an encoder-only transformer model (BERT~\cite{devlin2018bert}) and a Convolutional Neural Network (ResNet-152~\cite{he2016deep}).
BERT and ResNet-152 models predominantly consist of GEMM operations with high operational intensity, effectively utilizing GPU compute throughput.
Conversely, LLama2-70B exhibits limited operational intensity, resulting in a mere 21\% utilization of the available GPU compute throughput.
Finally, decoding an output token in the decoding stage takes $3.4\times$ longer than encoding a prompt token in the prefill stage due to the significant lower operational intensity of GEMV operations.


\begin{figure}[h]
% \vspace{-3mm}
\centering
    \includegraphics[width=8cm]{Figure/GPU_utilization.pdf}
    % \includegraphics[width=\columnwidth]{Figure/GPU_utilization.pdf}
    % \vspace{-3mm}
    \caption{(a) Llama2-70B inference query latency increases with larger batches on 4 A100 80GB GPUs,  Prompt size=512, Decoding size=3584. (b) GPU compute utilization, measured by Nvidia Nsight Compute profiler on 4 GPUs for Llama2-70B and 1 GPU for the other two models.}
    \label{fig:GPU_utilization}
    % \vspace{-6mm}
\end{figure}

\label{PIM-prototype}
\textbf{PIM Provides Higher Memory Bandwidth}. 
Table~\ref{tab:hardware_comparison} compares various manufactured industrial PIM prototypes and GPU. 
PIM enables the compute units to utilize the internal memory bandwidth, which significantly exceeds the external memory bandwidth of high-end GPUs with high bandwidth memories (HBM). For example, GDDR6-based AiM~\cite{aim1, aim2} achieves $16~TB/s$ internal memory bandwidth compared to $2~TB/s$ external bandwidth of an A100 GPU with five HBM2E memory stacks. This large internal bandwidth coupled with a lower operational intensity makes PIM architectures a suitable alternative for expensive GPUs to perform LLM inference tasks. 

\begin{table}[h]
    % \setlength{\tabcolsep} {1.5pt}
    \footnotesize
    \centering
    \caption{Hardware System Comparison}
    \label{tab:hardware_comparison}    
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Type & \multicolumn{3}{|c|}{PIM} & GPU   \\
        \hline
        Name & UPMEM & AiM & FIMDRAM & A100 \\
        \hline
        \hline
        Mem. Units & 8 DIMMs & 32 channels & 5 stacks & 5 stacks \\
        \hline
        Ex. BW (TB/s) & 0.15 & 1 & 1.5 & 2 \\
        \hline
        In. BW (TB/s) & 1 & 16 & 12.3 & - \\
        \hline
        Capacity (GB) & 64 & 16 & 30 & 80 \\
        \hline
        TFLOPS & 0.5 TOPS\footnotemark & 16 & 6.2 & 312 \\
        \hline
        Ops/Byte & 0.5 & 1 & 0.5 & 156 \\
        \hline
        Mem. Density & 25\%-50\% & 75\% & 75\% & - \\
        \hline
    \end{tabular}
\end{table}


\textbf{Low Memory Density of PIM.}
PIM suffers from a lower memory density due to the near-bank processing units that are fabricated in the DRAM process.
For instance, DDR4-based UPMEM R-DIMM and GDDR6-based AiM reduce the memory capacity to $25\%{-}50\%$ and $75\%$ compared to conventional DDR4 R-DIMMs and GDDR6 channels, respectively~\cite{upmem, aim2}.
An HBM2-based FIMDRAM cube consists of 4 PIM-enabled DRAM dies with $50\%$ memory density and 4 conventional dies, lowering the memory capacity by $25\%$ on average~\cite{fimdram}.
Given the lower memory density of PIM technologies and the substantial memory demands of LLMs, leveraging PIM as a scalable solution for LLMs presents significant challenges.

\footnotetext{UPMEM supports only integer precision, so unit is TOPs.}

\textbf{Scalable Network of PIM.}
% To scale the memory capacity of PIM-enabled memories, a scalable interconnect, collective communication primitives, and a mapping of the LLM based on various parallelization strategies to the CXL devices are needed.
Scaling the memory capacity of PIM-enabled memories requires a scalable interconnect, efficient collective communication primitives, and parallelization strategies to optimally map LLMs to PIM devices.
We utilize CXL 3.0~\cite{CXL} as a low-latency interconnect protocol, built on top of the PCIe physical layer.
CXL 3.0 supports inter-device communication through a CXL switch.
Compared to network-based RDMA, CXL.mem offers ${\sim}8\times$ lower latency~\cite{gouk2022direct}.
The CXL 3.0 protocol can support up to 4,096 nodes, exhibiting better scalability than NVLink~\cite{nvlink}.
NVLink provides higher bandwidth (at a higher cost), which is critical for LLM training.
However, we show that the lower bandwidth of CXL is not a bottleneck for LLM inference due to the limited volume of data transfers in various parallelization strategies.

To distribute the LLMs, we detail the mapping of the transformer blocks to the CXL devices based on the Pipeline Parallel (PP)~\cite{megatron, alpa} and Tensor Parallel (TP)~\cite{gpipe} strategies.
For PP, we provide peer-to-peer \textit{send} and \textit{receive} primitives for the transmission of the embedding vector between the pipeline stages across CXL devices.
For TP, we implement \textit{gather} and \textit{broadcast} collective communication primitives to transfer partial results.
To balance the throughput and latency of the network, we study the hybrid TP-PP parallelization strategy using the \textit{multicast} primitive.

\textbf{Hierarchical PIM-PNM Architecture.}
In addition to GEMV, a transformer block contains different layers, such RMSNorm~\cite{rmsnorm-paper}, Rotary Embedding~\cite{rope-paper}, and SiLU~\cite{silu-paper}.
For the end-to-end execution of transformer blocks as an alternative to costly GPUs, there are two options:
(a) Perform all operations near-bank using a general-purpose PU similar to UPMEM~\cite{upmem} architecture.
(b) Perform MAC operations of GEMVs in domain-specific near-bank PUs similar to AiM~\cite{aim2}, and assign other operations to the PNM units, shared by multiple PIM chips.
We use the second approach and propose a hierarchical PIM-PNM solution because of two primary reasons:
First, a general-purpose near-bank PU incurs more overhead on memory density and yields lower compute throughput compared to domain-specific alternatives.
Second, MAC operations constitute over $99\%$ of arithmetic operations within a transformer block, rendering general-purpose near-bank PUs over-provisioned for other infrequent arithmetic operations.