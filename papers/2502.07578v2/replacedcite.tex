\section{Related Work}
Various ML accelerators and HW/SW co-designs have recently been proposed____. CXL memory expansion techniques are also widely explored____. Sections~\ref{PIM} and ~\ref{PIM-prototype} already discuss PIM and PNM related works.

\noindent{\textbf{Transformer Accelerators.}} A variety of transformer accelerators____ have been developed to enhance this prevalent ML architecture.
TransPIM____ accelerates inference of transformer encoders like BERT____ by reducing data loading time with an efficient token-based dataflow.
However, decoder-only LLM's inference tasks present a unique challenge due to their lower operational intensities, which have been less investigated. 
% Moreover, prior works are not scalable to support larger LLMs enabled by CXL memory expansion.
Approaches like Sprint____, OliVe____, FABNet____, and SpAtten____ employ quantization, approximation, and pruning strategies, respectively, aimed at reducing computations within the transformer blocks, which are orthogonal to \att{}.

\noindent{\textbf{CXL-Based NDP Accelerators.}}  Samsung's CXL-PNM platform____ integrates an LLM inference accelerator in the CXL controller. \att{} also integrates PIM memory chips with PUs adjacent to DRAM banks, providing both higher internal memory bandwidth and compute throughput than CXL-PNM.
% The RISC-V cores of \att{} provide additional flexibility. 
Beacon____ explores near-data processing in both DIMMs and CXL switches, with customized processing units for accelerating genome sequencing analysis.

% \yufeng{Could Sumanth and Ning add other CXL related works?}

%However, Beacon deploys accelerators outside DRAM chips, while \att{} incorporates PUs adjacent to memory banks to provides higher internal memory bandwidth.
% Beacon, being an NDP approach is however limited to CXL bandwidth while \att{} utilizes near-bank processing within DRAM chips to exploit much higher internal bandwidth.