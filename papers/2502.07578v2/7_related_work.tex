\section{Related Work}

Various ML accelerators and HW/SW co-designs have recently been proposed~\cite{eyeriss, in-switch, maeri}. CXL memory expansion techniques are also widely explored~\cite{CXL-DL, CXL-DB, Demystify-CXL, cxl-memory-pool, jang2023cxl, gouk2022direct}. Sections~\ref{PIM} and ~\ref{PIM-prototype} already discuss PIM and PNM related works.

\noindent{\textbf{Transformer Accelerators.}} A variety of transformer accelerators~\cite{elsa, dota, sanger, FACT} have been developed to enhance this prevalent ML architecture.
TransPIM~\cite{transpim} accelerates inference of transformer encoders like BERT~\cite{bert} by reducing data loading time with an efficient token-based dataflow.
However, decoder-only LLM's inference tasks present a unique challenge due to their lower operational intensities, which have been less investigated. 
% Moreover, prior works are not scalable to support larger LLMs enabled by CXL memory expansion.
Approaches like Sprint~\cite{sprint}, OliVe~\cite{olive}, FABNet~\cite{FABNet}, and SpAtten~\cite{spatten} employ quantization, approximation, and pruning strategies, respectively, aimed at reducing computations within the transformer blocks, which are orthogonal to \att{}.

\noindent{\textbf{CXL-Based NDP Accelerators.}}  Samsung's CXL-PNM platform~\cite{cxl-pnm, samsung_pimpnm} integrates an LLM inference accelerator in the CXL controller. \att{} also integrates PIM memory chips with PUs adjacent to DRAM banks, providing both higher internal memory bandwidth and compute throughput than CXL-PNM.
% The RISC-V cores of \att{} provide additional flexibility. 
Beacon~\cite{huangfu2022beacon} explores near-data processing in both DIMMs and CXL switches, with customized processing units for accelerating genome sequencing analysis.

% \yufeng{Could Sumanth and Ning add other CXL related works?}

%However, Beacon deploys accelerators outside DRAM chips, while \att{} incorporates PUs adjacent to memory banks to provides higher internal memory bandwidth.
% Beacon, being an NDP approach is however limited to CXL bandwidth while \att{} utilizes near-bank processing within DRAM chips to exploit much higher internal bandwidth. 

