@inproceedings{vLLM,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}



@inproceedings{tpu,
author = {Jouppi, Norm and Kurian, George and Li, Sheng and Ma, Peter and Nagarajan, Rahul and Nai, Lifeng and Patil, Nishant and Subramanian, Suvinay and Swing, Andy and Towles, Brian and Young, Clifford and Zhou, Xiang and Zhou, Zongwei and Patterson, David A},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
isbn = {9798400700958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579371.3589350},
doi = {10.1145/3579371.3589350},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
articleno = {82},
numpages = {14},
keywords = {machine learning, domain specific architecture, TPU, GPU, IPU, supercomputer, optical interconnect, reconfigurable, embeddings, large language model, power usage effectiveness, warehouse scale computer, carbon emissions, energy, CO2 equivalent emissions},
location = {Orlando, FL, USA},
series = {ISCA '23}
}


@article{longlora,
  title={Longlora: Efficient fine-tuning of long-context large language models},
  author={Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
  journal={arXiv preprint arXiv:2309.12307},
  year={2023}
}

@article{fowers2018configurable,
  title={{A configurable cloud-scale DNN processor for real-time AI. In 2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)}},
  author={Fowers, Jeremy and Ovtcharov, Kalin and Papamichael, Michael and Massengill, Todd and Liu, Ming and Lo, Daniel and Alkalay, Shlomi and Haselman, Michael and Adams, Logan and Ghandi, Mahdi and Heil, Stephen and Patel, Prerak and Sapek, Adam and Weisz, Gabriel and Woods, Lisa and Lanka, Sitaram and Reinhardt, Steven K. and Caulfield, Adrian M. and Chung, Eric S. and Burger, Doug},
  url={https://doi.org/10.1109/isca},
  year={2018}
}

@inproceedings{NeuPIM,
author = {Heo, Guseul and Lee, Sangyeop and Cho, Jaehong and Choi, Hyunmin and Lee, Sanghyeon and Ham, Hyungkyu and Kim, Gwangsun and Mahajan, Divya and Park, Jongse},
title = {NeuPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651380},
doi = {10.1145/3620666.3651380},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {722–737},
numpages = {16},
keywords = {processing-in-memory (PIM), neural processing unit (NPU), heterogeneous system, large language model (LLM), inference serving, transformer-based generative model (GPT)},
location = {<conf-loc>, <city>La Jolla</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ASPLOS '24}
}

@inproceedings{AttAcc,
author = {Park, Jaehyun and Choi, Jaewan and Kyung, Kwanhee and Kim, Michael Jaemin and Kwon, Yongsuk and Kim, Nam Sung and Ahn, Jung Ho},
title = {AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640422},
doi = {10.1145/3620665.3640422},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {103–119},
numpages = {17},
keywords = {processing-in-memory, transformer-based generative model, DRAM},
location = {, La Jolla, CA, USA, },
series = {ASPLOS '24}
}

@inproceedings{fimdram,
  title={{25.4 a 20nm 6gb function-in-memory DRAM, based on HBM2 with a 1.2 tflops programmable computing unit using bank-level parallelism, for machine learning applications}},
  author={Kwon, Young-Cheon and Lee, Suk Han and Lee, Jaehoon and Kwon, Sang-Hyuk and Ryu, Je Min and Son, Jong-Pil and Seongil, O and Yu, Hak-Soo and Lee, Haesuk and Kim, Soo Young and Cho, Youngmin and Kim, Jin Guk and Choi, Jongyoon and Shin, Hyun-Sung and Kim, Jin and Phuah, BengSeng and Kim, HyoungMin and Song, Myeong Jun and Choi, Ahn and Kim, Daeho and Kim, SooYoung and Kim, Eun-Bong and Wang, David and Kang, Shinhaeng and Ro, Yuhwan and Seo, Seungwoo and Song, JoonHo and Youn, Jaeyoun and Sohn, Kyomin and Kim, Nam Sung},
  booktitle={2021 IEEE International Solid-State Circuits Conference (ISSCC)},
  volume={64},
  pages={350--352},
  year={2021},
  organization={IEEE}
}

@inproceedings{aim1,
  title={A 1ynm 1.25 v 8gb, 16gb/s/pin gddr6-based accelerator-in-memory supporting 1tflops mac operation and various activation functions for deep-learning applications},
  author={Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and Jeon, Junyeol and Kim, Nahsung and Kwon, Yongkee and Vladimir, Kornijcuk and Shin, Woojae and Won, Jongsoon and Lee, Minkyu and Joo, Hyunha and Choi, Haerang and Lee, Jaewook and Ko, Donguc and Jun, Younggun and Cho, Keewon and Kim, Ilwoong and Song, Choungki and Jeong, Chunseok and Kwon, Daehan and Jang, Jieun and Park, Il and Chun, Junhyun and Cho, Joohwan},
  booktitle={2022 IEEE International Solid-State Circuits Conference (ISSCC)},
  volume={65},
  pages={1--3},
  year={2022},
  organization={IEEE}
}
@inproceedings{aim2,
  title={{System architecture and software stack for GDDR6-AiM}},
  author={Kwon, Yongkee and Vladimir, Kornijcuk and Kim, Nahsung and Shin, Woojae and Won, Jongsoon and Lee, Minkyu and Joo, Hyunha and Choi, Haerang and Kim, Guhyun and An, Byeongju and Kim, Jeongbin and Lee, Jaewook and Kim, Ilkon and Park, Jaehan and Park, Chanwook and Song, Yosub and Yang, Byeongsu and Lee, Hyungdeok and Kim, Seho and Kwon, Daehan and Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gimoon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and Jeon, Junyeol and Lee, Myeongjun and Shin, Minyoung and Shin, Minhwan and Cha, Jaekyung and Jung, Changson and Chang, Kijoon and Jeong, Chunseok and Lim, Euicheol and Park, Il and Chun, Junhyun},
  booktitle={2022 IEEE Hot Chips 34 Symposium (HCS)},
  pages={1--25},
  year={2022},
  organization={IEEE}
}
@INPROCEEDINGS{aim3,
  author={Kwon, Yongkee and Kim, Guhyun and Kim, Nahsung and Shin, Woojae and Won, Jongsoon and Joo, Hyunha and Choi, Haerang and An, Byeongju and Shin, Gyeongcheol and Yun, Dayeon and Kim, Jeongbin and Kim, Changhyun and Kim, Ilkon and Park, Jaehan and Park, Chanwook and Song, Yosub and Yang, Byeongsu and Lee, Hyeongdeok and Park, Seungyeong and Lee, Wonjun and Lee, Seongju and Kim, Kyuyoung and Kwon, Daehan and Jeong, Chunseok and Kim, John and Lim, Euicheol and Chun, Junhyun},
  booktitle={2023 IEEE Hot Chips 35 Symposium (HCS)}, 
  title={Memory-Centric Computing with SK Hynix's Domain-Specific Memory}, 
  year={2023},
  volume={},
  number={},
  pages={1-26},
  keywords={Costs;Artificial intelligence;Memory management;Transformers;Training;Computational modeling;Bandwidth},
  doi={10.1109/HCS59251.2023.10254717}}

@ARTICLE{aim4,
  author={Kwon, Daehan and Lee, Seongju and Kim, Kyuyoung and Oh, Sanghoon and Park, Joonhong and Hong, Gi-Moon and Ka, Dongyoon and Hwang, Kyudong and Park, Jeongje and Kang, Kyeongpil and Kim, Jungyeon and Jeon, Junyeol and Kim, Nahsung and Kwon, Yongkee and Kornijcuk, Vladimir and Shin, Woojae and Won, Jongsoon and Lee, Minkyu and Joo, Hyunha and Choi, Haerang and Kim, Guhyun and An, Byeongju and Lee, Jaewook and Ko, Donguc and Jun, Younggun and Kim, Ilwoong and Song, Choungki and Kim, Ilkon and Park, Chanwook and Kim, Seho and Jeong, Chunseok and Lim, Euicheol and Kim, Dongkyun and Jang, Jieun and Park, Il and Chun, Junhyun and Cho, Joohwan},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={A 1ynm 1.25V 8Gb 16Gb/s/Pin GDDR6-Based Accelerator-in-Memory Supporting 1TFLOPS MAC Operation and Various Activation Functions for Deep Learning Application}, 
  year={2023},
  volume={58},
  number={1},
  pages={291-302},
  keywords={Random access memory;Adders;Microprocessors;Deep learning;Costs;Bandwidth;Throughput;Accelerator-in-memory (AiM);activation function (AF);adder tree;bank-wide mantissa shift (BWMS);dynamic random access memory (DRAM);GDDR6;multiply-accumulate (MAC);processing-in-memory (PIM)},
  doi={10.1109/JSSC.2022.3200718}}


@inproceedings{jang2023cxl,
  title={{CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and Computation for Billion-Scale Approximate Nearest Neighbor Search}},
  author={Jang, Junhyeok and Choi, Hanjin and Bae, Hanyeoreum and Lee, Seungjun and Kwon, Miryeong and Jung, Myoungsoo},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={585--600},
  year={2023}
}

@inproceedings{gouk2022direct,
  title={{Direct access, High-Performance memory disaggregation with DirectCXL}},
  author={Gouk, Donghyun and Lee, Sangwon and Kwon, Miryeong and Jung, Myoungsoo},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={287--294},
  year={2022}
}

@article{cxl-memory-pool,
  title={Memory pooling with cxl},
  author={Gouk, Donghyun and Kwon, Miryeong and Bae, Hanyeoreum and Lee, Sangwon and Jung, Myoungsoo},
  journal={IEEE Micro},
  volume={43},
  number={2},
  pages={48--57},
  year={2023},
  publisher={IEEE}
}

@inproceedings{samsung_pimpnm,
  title={{Samsung PIM/PNM for Transfmer Based AI: Energy Efficiency on PIM/PNM Cluster}},
  author={Kim, Jin Hyun and Ro, Yuhwan and So, Jinin and Lee, Sukhan and Kang, Shin-haeng and Cho, YeonGon and Kim, Hyeonsu and Kim, Byeongho and Kim, Kyungsoo and Park, Sangsoo and Kim, Jin-Seong and Cha, Sanghoon and Lee, Won-Jo and Jung, Jin and Lee, Jong-Geon and Lee, Jieun and Song, JoonHo and Lee, Seungwon and Cho, Jeonghyeon and Yu, Jaehoon and Sohn, Kyomin},
  booktitle={2023 IEEE Hot Chips 35 Symposium (HCS)},
  pages={1--31},
  year={2023},
  organization={IEEE Computer Society}
}

@inproceedings{upmem,
  title={The true processing in memory accelerator},
  author={Devaux, Fabrice},
  booktitle={2019 IEEE Hot Chips 31 Symposium (HCS)},
  pages={1--24},
  year={2019},
  organization={IEEE Computer Society}
}

@online{mlperf-sla,
  author       = {Thomas Atta-fosu},
  title        = {Llama 2 70B: An MLPerf Inference Benchmark for Large Language Models},
  url          = {https://mlcommons.org/2024/03/mlperf-llama2-70b/},
}

@online{grok,
  author       = {XAI},
  title        = {Open Release of Grok-1},
  url          = {https://x.ai/blog/grok-os},
}

@online{O1-reasoning,
  author       = {OpenAI},
  title        = {Learning to reason with LLMs
},
  url          = {https://openai.com/index/learning-to-reason-with-llms/},
}

@inproceedings{patel2024splitwise,
  title={Splitwise: Efficient generative llm inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={118--132},
  year={2024},
  organization={IEEE}
}

@article{zhong2024distserve,
  title={Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={arXiv preprint arXiv:2401.09670},
  year={2024}
}

@article{polyak2024movie,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{deepseek-r1,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{deepseek-v3,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@online{chatgpt-cost,
  author       = {Dylan Patel, Afzal Ahmad},
  title        = {The Inference Cost Of Search Disruption – Large Language Model Cost Analysis},
  url          = {https://www.semianalysis.com/p/the-inference-cost-of-search-disruption},
}

@online{carbon-dioxide,
  author       = {Kathryn Tso},
  title        = {How much is a ton of carbon dioxide?},
  url          = {https://climate.mit.edu/ask-mit/how-much-ton-carbon-dioxide},
}

@online{aws-instance,
  author       = {AWS},
  title        = {Amazon EC2 Instance Types},
  url          = {https://aws.amazon.com/ec2/instance-types/},
}

@online{CPU-price,
  author       = {Intel},
  title        = {Intel® Xeon® Gold 6430 Processor},
  url          = {https://www.intel.com/content/www/us/en/products/sku/231737/intel-xeon-gold-6430-processor-60m-cache-2-10-ghz/specifications.html},
}


@online{switch-price,
  author       = {Mouser Electronics},
  title        = {PCI Interface IC},
  url          = {https://www.mouser.com/c/semiconductors/interface-ics/pci-interface-ic/},
}

@online{aws-price,
  title        = {Amazon EC2 Prices},
  url          = {https://ec2pricing.net/},
}

@online{gpu-price,
  title        = {NVIDIA Tesla A100 80GB GPU SXM4 Deep Learning Computing Graphics Card OEM},
author = {ebay},
  url          = {https://www.ebay.com/itm/126596600113?chn=ps&mkevt=1&mkcid=28&srsltid=AfmBOop8-DCL9WiHC15MU05ZikXFveIxl95uEuqd55d5LHBrMjRXNMiwSTg},
}

@online{azure-price,
  title        = {Azure Pricing Calculator},
  url          = {https://azure.microsoft.com/en-us/pricing/calculator/},
}

@online{electricity-price,
  title        = {Average energy prices for the United States, regions, census divisions, and selected metropolitan areas},
  author       = {U.S. Bureau of Labor Statistics},
  url          = {https://www.bls.gov/regions/midwest/data/averageenergyprices_selectedareas_table.htm}
}

@online{CXL,
  title        = {Specification},
  Author       = {Compute Express Link™},
  url          = {https://www.computeexpresslink.org/},
}

@online{copilot,
  title        = {About GitHub Copilot Individual},
  url          = {https://docs.github.com/en/copilot/overview-of-github-copilot/about-github-copilot-individual},
}

@online{HBM-price,
  title        = {Generative AI Winds in Memory Semiconductors, Total Demand for Server DRAMs is Declining},
    author = {Kiwoom Securities},
  url          = {https://www.businesspost.co.kr/BP?command=article_view&num=316574},
}

@online{GPU-volume,
  title        = {Nvidia shipped 3.76M data center GPUs in 2023},
author = {Jowi Morales},
  url          = {https://www.tomshardware.com/tech-industry/nvidia-shipped-376m-data-center-gpus-in-2023-dominates-business-with-98-revenue-share},
}

@inproceedings{o2017fine,
  title={Fine-grained DRAM: Energy-efficient DRAM for extreme bandwidth systems},
  author={O'Connor, Mike and Chatterjee, Niladrish and Lee, Donghyuk and Wilson, John and Agrawal, Aditya and Keckler, Stephen W and Dally, William J},
  booktitle={Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={41--54},
  year={2017}
}

@online{micron-gddr6,
  title        = {GDDR6: The Next-Generation Graphics DRAM},
  url          = {https://www.mouser.com/pdfDocs/tned03_gddr6-4.pdf},
}


@online{packaging-cost,
  title        = {Using Machine Learning To Increase Yield And Lower Packaging Costs},
author = {Melvin Lee},
  url          = {https://semiengineering.com/using-machine-learning-to-increase-yield-and-lower-packaging-costs/},
}

@online{TU104,
  title        = {Die shot of the TU104 GPU used in RTX 2080 cards},
  author       = {Wikipedia},
  url          = {https://en.wikipedia.org/wiki/Turing_(microarchitecture)#/media/File:Nvidia@12nm@Turing@TU104@GeForce_RTX_2080@S_TAIWAN_1841A1_PKYN44.000_TU104-400-A1_DSCx7_poly@5xExt.jpg},
}

@online{xAI-H200,
  title        = {Elon Musk set up 100,000 Nvidia H200 GPUs in 19 days - Jensen says process normally takes 4 years},
  url          = {https://www.tomshardware.com/pc-components/gpus/elon-musk-took-19-days-to-set-up-100-000-nvidia-h200-gpus-process-normally-takes-4-years},
}

@article{binary,
  title={{A Binary-activation, Multi-level Weight RNN and Training Algorithm for ADC-/DAC-free and Noise-resilient Processing-in-memory Inference with eNVM}},
  author={Ma, Siming and Brooks, David and Wei, Gu-Yeon},
  journal={IEEE Transactions on Emerging Topics in Computing},
  year={2023},
  publisher={IEEE}
}

@inproceedings{ning2023supply,
  title={Supply chain aware computer architecture},
  author={Ning, August and Tziantzioulis, Georgios and Wentzlaff, David},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@article{moonwalk,
  title={Moonwalk: Nre optimization in asic clouds},
  author={Khazraee, Moein and Zhang, Lu and Vega, Luis and Taylor, Michael Bedford},
  journal={ACM SIGARCH Computer Architecture News},
  volume={45},
  number={1},
  pages={511--526},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@inproceedings{recnmp,
  title={Recnmp: Accelerating personalized recommendation with near-memory processing},
  author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz,  Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole-Jean and Hempstead, Mark and Zhang, Xuan},
  booktitle={2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages={790--803},
  year={2020},
  organization={IEEE}
}

@article{maeri,
  title={{Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects}},
  author={Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar},
  journal={ACM SIGPLAN Notices},
  volume={53},
  number={2},
  pages={461--475},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{in-switch,
  title={Accelerating distributed reinforcement learning with in-switch computing},
  author={Li, Youjie and Liu, Iou-Jen and Yuan, Yifan and Chen, Deming and Schwing, Alexander and Huang, Jian},
  booktitle={Proceedings of the 46th International Symposium on Computer Architecture},
  pages={279--291},
  year={2019}
}

@article{eyeriss,
  title={Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks},
  author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne},
  journal={IEEE journal of solid-state circuits},
  volume={52},
  number={1},
  pages={127--138},
  year={2016},
  publisher={IEEE}
}

@article{Demystify-CXL,
  title={{Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices}},
  author={Sun, Yan and Yuan, Yifan and Yu, Zeduo and Kuper, Reese and Jeong, Ipoom and Wang, Ren and Kim, Nam Sung},
  journal={arXiv preprint arXiv:2303.15375},
  year={2023}
}

@inproceedings{CXL-DB,
  title={Enabling CXL memory expansion for in-memory database management systems},
  author={Ahn, Minseon and Chang, Andrew and Lee, Donghun and Gim, Jongmin and Kim, Jungmin and Jung, Jaemin and Rebholz, Oliver and Pham, Vincent and Malladi, Krishna and Ki, Yang Seok},
  booktitle={Proceedings of the 18th International Workshop on Data Management on New Hardware},
  pages={1--5},
  year={2022}
}

@inproceedings{CXL-DL,
  title={{Exploiting CXL-based memory for distributed deep learning}},
  author={Arif, Moiz and Assogba, Kevin and Rafique, M Mustafa and Vazhkudai, Sudharshan},
  booktitle={Proceedings of the 51st International Conference on Parallel Processing},
  pages={1--11},
  year={2022}
}

@inproceedings{trim,
  title={Trim: Enhancing processor-memory interfaces with scalable tensor reduction in memory},
  author={Park, Jaehyun and Kim, Byeongho and Yun, Sungmin and Lee, Eojin and Rhu, Minsoo and Ahn, Jung Ho},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={268--281},
  year={2021}
}

@article{sparsep,
  title={Sparsep: Towards efficient sparse matrix vector multiplication on real processing-in-memory architectures},
  author={Giannoula, Christina and Fernandez, Ivan and Luna, Juan G{\'o}mez and Koziris, Nectarios and Goumas, Georgios and Mutlu, Onur},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={6},
  number={1},
  pages={1--49},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@article{aquabolt,
  title={{Aquabolt-XL HBM2-PIM, LPDDR5-PIM with in-memory processing, and AXDIMM with acceleration buffer}},
  author={Kim, Jin Hyun and Kang, Shin-Haeng and Lee, Sukhan and Kim, Hyeonsu and Ro, Yuhwan and Lee, Seungwon and Wang, David and Choi, Jihyun and So, Jinin and Cho, YeonGon and Sohn, Kyomin and Kim, Nam Sung},
  journal={IEEE Micro},
  volume={42},
  number={3},
  pages={20--30},
  year={2022},
  publisher={IEEE}
}

@inproceedings{nda,
  title={{NDA: Near-DRAM acceleration architecture leveraging commodity DRAM devices and standard memory modules}},
  author={Farmahini-Farahani, Amin and Ahn, Jung Ho and Morrow, Katherine and Kim, Nam Sung},
  booktitle={2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)},
  pages={283--295},
  year={2015},
  organization={IEEE}
}

@article{near-data,
  title={Near-memory data services},
  author={Falsafi, Babak and Stan, Mircea and Skadron, Kevin and Jayasena, Nuwan and Chen, Yunji and Tao, Jinhua and Nair, Ravi and Moreno, Jaime and Muralimanohar, Naveen and Sankaralingam, Karthikeyan and Estan, Cristian},
  journal={IEEE Micro},
  volume={36},
  number={1},
  pages={6--13},
  year={2016},
  publisher={IEEE}
}

@inproceedings{impala,
  title={Impala: Algorithm/architecture co-design for in-memory multi-stride pattern matching},
  author={Sadredini, Elaheh and Rahimi, Reza and Lenjani, Marzieh and Stan, Mircea and Skadron, Kevin},
  booktitle={2020 IEEE international symposium on high performance computer architecture (HPCA)},
  pages={86--98},
  year={2020},
  organization={IEEE}
}

@inproceedings{ultra,
  title={Ultra-efficient processing in-memory for data intensive applications},
  author={Imani, Mohsen and Gupta, Saransh and Rosing, Tajana},
  booktitle={Proceedings of the 54th Annual Design Automation Conference 2017},
  pages={1--6},
  year={2017}
}

@article{sky,
  title={{Sky-Sorter: A Processing-in-Memory Architecture for Large-Scale Sorting}},
  author={Zokaee, Farzaneh and Chen, Fan and Sun, Guangyu and Jiang, Lei},
  journal={IEEE Transactions on Computers},
  volume={72},
  number={2},
  pages={480--493},
  year={2022},
  publisher={IEEE}
}

@inproceedings{enmc,
  title={{Enmc: Extreme near-memory classification via approximate screening}},
  author={Liu, Liu and Lin, Jilan and Qu, Zheng and Ding, Yufei and Xie, Yuan},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={1309--1322},
  year={2021}
}

@inproceedings{gomez2023evaluating,
  title={Evaluating machine learningworkloads on memory-centric computing systems},
  author={G{\'o}mez-Luna, Juan and Guo, Yuxin and Brocard, Sylvan and Legriel, Julien and Cimadomo, Remy and Oliveira, Geraldo F and Singh, Gagandeep and Mutlu, Onur},
  booktitle={2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  pages={35--49},
  year={2023},
  organization={IEEE}
}

@article{oliveira2022accelerating,
  title={Accelerating neural network inference with processing-in-dram: From the edge to the cloud},
  author={Oliveira, Geraldo F and G{\'o}mez-Luna, Juan and Ghose, Saugata and Boroumand, Amirali and Mutlu, Onur},
  journal={IEEE Micro},
  volume={42},
  number={6},
  pages={25--38},
  year={2022},
  publisher={IEEE}
}

@article{gomez2022benchmarking,
  title={Benchmarking a new paradigm: Experimental analysis and characterization of a real processing-in-memory system},
  author={G{\'o}mez-Luna, Juan and El Hajj, Izzat and Fernandez, Ivan and Giannoula, Christina and Oliveira, Geraldo F and Mutlu, Onur},
  journal={IEEE Access},
  volume={10},
  pages={52565--52608},
  year={2022},
  publisher={IEEE}
}

@inproceedings{floatpim,
  title={Floatpim: In-memory acceleration of deep neural network training with high precision},
  author={Imani, Mohsen and Gupta, Saransh and Kim, Yeseong and Rosing, Tajana},
  booktitle={Proceedings of the 46th International Symposium on Computer Architecture},
  pages={802--815},
  year={2019}
}

@inproceedings{to-pim,
  title={{To pim or not for emerging general purpose processing in ddr memory systems}},
  author={Devic, Alexandar and Rai, Siddhartha Balakrishna and Sivasubramaniam, Anand and Akel, Ameen and Eilert, Sean and Eno, Justin},
  booktitle={Proceedings of the 49th Annual International Symposium on Computer Architecture},
  pages={231--244},
  year={2022}
}

@article{noise,
  title={{Noise-resilient DNN: Tolerating noise in PCM-based AI accelerators via noise-aware training}},
  author={Kariyappa, Sanjay and Tsai, Hsinyu and Spoon, Katie and Ambrogio, Stefano and Narayanan, Pritish and Mackin, Charles and Chen, An and Qureshi, Moinuddin and Burr, Geoffrey W},
  journal={IEEE Transactions on Electron Devices},
  volume={68},
  number={9},
  pages={4356--4362},
  year={2021},
  publisher={IEEE}
}

@article{prime,
  title={Prime: A novel processing-in-memory architecture for neural network computation in reram-based main memory},
  author={Chi, Ping and Li, Shuangchen and Xu, Cong and Zhang, Tao and Zhao, Jishen and Liu, Yongpan and Wang, Yu and Xie, Yuan},
  journal={ACM SIGARCH Computer Architecture News},
  volume={44},
  number={3},
  pages={27--39},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@online{claude,
  title        = {Introducing the next generation of Claude},
  author       = {Anthropic},
  url          = {https://www.anthropic.com/news/claude-3-family},
}

@online{PTX-ISA,
  title        = {PTX ISA},
  author       = {NVIDIA},
  url          = {https://docs.nvidia.com/cuda/pdf/ptx_isa_8.5.pdf},
}

@online{gemini-pro,
  title        = {Our next-generation model: Gemini 1.5},
  author       = {Google},
  url          = {https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/},
}

@online{gemini-models,
  title        = {Gemini API models},
  author       = {Google},
  url          = {https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models},
}

@online{a100,
  title        = {NVIDIA A100 TENSOR CORE GPU},
  author       = {NVIDIA},
  url          = {https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf},
}

@online{bloom-fast,
  title        = {Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate},
  author       = {Stas Bekman, Sylvain Gugger},
  url          = {https://huggingface.co/blog/bloom-inference-pytorch-scripts},
}



@online{boom-pdf,
  title        = {The	Berkeley Out-of-Order Machine (BOOM): An Open-source Industry-Competitive, Synthesizable, Parameterized RISC-V Processor},
author = {Christopher	Celio and	Krste	Asanovic and	David	Patterson},
  url          = {https://riscv.org/wp-content/uploads/2016/01/Wed1345-RISCV-Workshop-3-BOOM.pdf},
}

@online{bf16,
  title        = {The bfloat16 numerical format},
  url          = {https://cloud.google.com/tpu/docs/bfloat16},
}

@article{rope-paper,
  title={{RoFormer: Enhanced Transformer with Rotary Position Embedding}},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@article{Swish,
  title={Searching for activation functions},
  author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1710.05941},
  year={2017}
}

@article{alibi,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{SwiGLU,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{GeLU,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{du2022glam,
  title={{Glam: Efficient scaling of language models with mixture-of-experts}},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}

@inproceedings{transpim,
  title={Transpim: A memory-based acceleration via software-hardware co-design for transformer},
  author={Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana},
  booktitle={2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={1071--1085},
  year={2022},
  organization={IEEE}
}

@inproceedings{FACT,
  title={{FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction}},
  author={Qin, Yubin and Wang, Yang and Deng, Dazheng and Zhao, Zhiren and Yang, Xiaolong and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--14},
  year={2023}
}

@inproceedings{optimstore,
  title={{OptimStore: In-Storage Optimization of Large Scale DNNs with On-Die Processing}},
  author={Kim, Junkyum and Kang, Myeonggu and Han, Yunki and Kim, Yang-Gon and Kim, Lee-Sup},
  booktitle={2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={611--623},
  year={2023},
  organization={IEEE}
}

@inproceedings{dota,
  title={Dota: detect and omit weak attentions for scalable transformer acceleration},
  author={Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={14--26},
  year={2022}
}

@inproceedings{sanger,
  title={Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture},
  author={Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={977--991},
  year={2021}
}

@inproceedings{elsa,
  title={{ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks}},
  author={Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={692--705},
  year={2021},
  organization={IEEE}
}

@inproceedings{spatten,
  title={Spatten: Efficient sparse attention architecture with cascade token and head pruning},
  author={Wang, Hanrui and Zhang, Zhekai and Han, Song},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={97--110},
  year={2021},
  organization={IEEE}
}

@inproceedings{FABNet,
  title={{Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design}},
  author={Fan, Hongxiang and Chau, Thomas and Venieris, Stylianos I and Lee, Royson and Kouris, Alexandros and Luk, Wayne and Lane, Nicholas D and Abdelfattah, Mohamed S},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={599--615},
  year={2022},
  organization={IEEE}
}

@inproceedings{sprint,
  title={Sparse attention acceleration with synergistic in-memory pruning and on-chip recomputation},
  author={Yazdanbakhsh, Amir and Moradifirouzabadi, Ashkan and Li, Zheng and Kang, Mingu},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={744--762},
  year={2022},
  organization={IEEE}
}

@inproceedings{olive,
  title={{OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization}},
  author={Guo, Cong and Tang, Jiaming and Hu, Weiming and Leng, Jingwen and Zhang, Chen and Yang, Fan and Liu, Yunxin and Guo, Minyi and Zhu, Yuhao},
  booktitle={Proceedings of the 50th Annual International Symposium on Computer Architecture},
  pages={1--15},
  year={2023}
}

@article{rmsnorm-paper,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{brown2020gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@misc{openai2023gpt4,
      title={{GPT-4 Technical Report}}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{chowdhery2022palm,
  title={{Palm: Scaling language modeling with pathways}},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{agostinelli2023musiclm,
  title={{Musiclm: Generating music from text}},
  author={Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{copet2023simple,
  title={{Simple and Controllable Music Generation}},
  author={Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\'e}fossez, Alexandre},
  journal={arXiv preprint arXiv:2306.05284},
  year={2023}
}

@misc{ramesh2022hierarchical,
      title={{Hierarchical Text-Conditional Image Generation with CLIP Latents}}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@online{sora,
      title={{Video generation models as world simulators}}, 
author = {OpenAI},
      url          = {https://openai.com/research/video-generation-models-as-world-simulators},
}


@article{dalle3,
  title={Improving image generation with better captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and  Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
  journal={Computer Science.},
  url={https://cdn.openai.com/papers/dall-e-3.pdf},
  volume={2},
  number={3},
  pages={8},
  year={2023}
}

@article{thoppilan2022lamda,
  title={{Lamda: Language models for dialog applications}},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{scao2022bloom,
  title={{Bloom: A 176b-parameter open-access multilingual language model}},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{luo2023ramulator,
  title={{Ramulator 2.0: A Modern, Modular, and Extensible DRAM Simulator}},
  author={Luo, Haocong and Tu{\u{g}}rul, Yahya Can and Bostanc{\i}, F and Olgun, Ataberk and Ya{\u{g}}l{\i}k{\c{c}}{\i}, A Giray and Mutlu, Onur},
  journal={arXiv preprint arXiv:2308.11030},
  year={2023}
}

@inproceedings{HB-PNM,
  title={{184QPS/W 64Mb/mm 2 3D logic-to-DRAM hybrid bonding with process-near-memory engine for recommendation system}},
  author={Niu, Dimin and Li, Shuangchen and Wang, Yuhao and Han, Wei and Zhang, Zhe and Guan, Yijin and Guan, Tianchan and Sun, Fei and Xue, Fei and Duan, Lide and Fang, Yuanwei and Zheng, Hongzhong and Jiang, Xiping and Wang, Song and Zuo, Fengguo and Wang, Yubing and Yu, Bing and Ren, Qiwei and Xie, Yuan},
  booktitle={2022 IEEE International Solid-State Circuits Conference (ISSCC)},
  volume={65},
  pages={1--3},
  year={2022},
  organization={IEEE}
}

@inproceedings{axdimm,
  title={{Improving in-memory database operations with acceleration DIMM (AxDIMM)}},
  author={Lee, Donghun and So, Jinin and Ahn, Minseon and Lee, Jong-Geon and Kim, Jungmin and Cho, Jeonghyeon and Oliver, Rebholz and Thummala, Vishnu Charan and JV, Ravi shankar and Upadhya, Sachin Suresh and Lee, Donghun and So, Jinin and Ahn, Minseon and Lee, Jong-Geon and Kim, Jungmin and Cho, Jeonghyeon and Oliver, Rebholz and Thummala, Vishnu Charan and JV, Ravi shankar and Upadhya, Sachin Suresh and Khan, Mohammed Ibrahim and Kim, Jin Hyun},
  booktitle={Proceedings of the 18th International Workshop on Data Management on New Hardware},
  pages={1--9},
  year={2022}
}

@inproceedings{fimdram-isca,
  title={{Hardware architecture and software stack for PIM based on commercial DRAM technology: Industrial product}},
  author={Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Lee, Sukhan and Kang, Shin-haeng and Lee, Jaehoon and Kim, Hyeonsu and Lee, Eojin and Seo, Seungwoo and Yoon, Hosang and Lee, Seungwon and Lim, Kyounghwan and Shin, Hyunsung and Kim, Jinhyun and Seongil, O and Iyer, Anand and Wang, David and Sohn, Kyomin and Kim, Nam Sung},
  booktitle={2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)},
  pages={43--56},
  year={2021},
  organization={IEEE}
}

@article{kasneci2023chatgpt,
  title={{ChatGPT for good? On opportunities and challenges of large language models for education}},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and Krusche, Stephan and  Kutyniok, Gitta and  Michaeli, Tilman and  Nerdel, Claudia and Pfeffer, J{\"u}rgen and Poquet, Oleksandra and  Sailer, Michael and Schmidt, Albrecht and Seidel, Tina and Stadler, Matthias and Weller, Jochen and Kuhn, Jochen and  Kasneci, Gjergji},
  journal={Learning and individual differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

@article{touvron2023llama,
  title={{Llama 2: Open foundation and fine-tuned chat models}},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{devlin2018bert,
  title={{Bert: Pre-training of deep bidirectional transformers for language understanding}},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{yang2023harnessing,
  title={{Harnessing the power of LLMs in practice: A survey on chatgpt and beyond}},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
  journal={arXiv preprint arXiv:2304.13712},
  year={2023}
}

@article{sharma2022compute,
  title={{Compute Express Link (CXL): Enabling Heterogeneous Data-Centric Computing With Heterogeneous Memory Hierarchy}},
  author={Sharma, Debendra Das},
  journal={IEEE Micro},
  volume={43},
  number={2},
  pages={99--109},
  year={2022},
  publisher={IEEE}
}

@inproceedings{li2023pond,
  title={{Pond: CXL-based memory pooling systems for cloud platforms}},
  author={Li, Huaicheng and Berger, Daniel S and Hsu, Lisa and Ernst, Daniel and Zardoshti, Pantea and Novakovic, Stanko and Shah, Monish and Rajadnya, Samir and Lee, Scott and Agarwal, Ishwar and Li, Huaicheng and Berger, Daniel S. and Hsu, Lisa and Ernst, Daniel and Zardoshti, Pantea and Novakovic, Stanko and Shah, Monish and Rajadnya, Samir and Lee, Scott and Agarwal, Ishwar and Hill, Mark D. and Fontoura, Marcus and Bianchini, Ricardo},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={574--587},
  year={2023}
}
@inproceedings{Jung2022Storage,
author = {Jung, Myoungsoo},
title = {Hello Bytes, Bye Blocks: PCIe Storage Meets Compute Express Link for Memory Expansion (CXL-SSD)},
year = {2022},
booktitle = {Proceedings of the 14th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {45–51},
numpages = {7},
series = {HotStorage '22}
}

@online{pcie5-switch,
  author       = {Broadcom},
  title        = {144-lane, 72-port, PCI Express Gen 5.0 PEX89144 ExpressFabric Platform},
  url          = {https://www.broadcom.com/products/pcie-switches-bridges/expressfabric/gen5/pex89144},
}

@online{dgx-a100,
  author       = {NVIDIA},
  title        = {Introduction to the NVIDIA DGX A100 System},
  url          = {https://docs.nvidia.com/dgx/dgxa100-user-guide/introduction-to-dgxa100.html#power-specifications},
}

@online{dgx-h100,
  author       = {NVIDIA},
  title        = {Introduction to the NVIDIA DGX H100 System},
  url          = {https://docs.nvidia.com/dgx/dgxh100-user-guide/introduction-to-dgxh100.html#power-specifications},
}

@online{pcie4-programmable-switch,
  author       = {Microchip},
  title        = {100-lane, 48-port, PSX 100xG4 Gen 4 Programmable PCIe® Switch },
  url          = {https://www.broadcom.com/products/pcie-switches-bridges/expressfabric/gen5/pex89144},
}

@article{opt,
  title={{Opt: Open pre-trained transformer language models}},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{cxl-pnm,
  title={{An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models}},
  author={Park, Sang-Soo and Kim, KyungSoo and So, Jinin and Jung, Jin and Lee, Jonggeon and Woo, Kyoungwan and Kim, Nayeon and Lee, Younghyun and Kim, Hyungyo and Kwon, Yongsuk and Kim, Jinhyun and Lee, Jieun and Cho, YeonGon and Tai, Yongmin and Cho, Jeonghyeon and Song, Hoyoung and Ahn, Jung Ho and Kim, Nam Sung},
  booktitle={2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={970--982},
  year={2024},
  organization={IEEE}
}

@online{cxl-2-switch,
  author       = {XConn},
  title        = {256 lane CXl 2.0 Switch},
  url          = {https://www.xconn-tech.com/product},
}

@online{gpt4-turbo,
  author       = {OpenAI},
  title        = {GPT-4 Turbo and GPT-4},
  url          = {https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4},
}

@online{pim-price,
  author       = {UPMEM},
  title        = {Accelerating Compute By Cramming It Into DRAM Memory},
  url          = {https://www.upmem.com/nextplatform-com-2019-10-03-accelerating-compute-by-cramming-it-into-dram/},
}

@inproceedings{palesko2014cost,
  title={Cost and yield analysis of multi-die packaging using 2.5 D technology compared to fan-out wafer level packaging},
  author={Palesko, Chet and Palesko, Amy and Vardaman, E Jan},
  booktitle={Proceedings of the 5th Electronics System-integration Technology Conference (ESTC)},
  pages={1--5},
  year={2014},
  organization={IEEE}
}

@online{dram-price,
  author       = {dramexchange},
  title        = {DRAM Spot Price},
  url          = {https://www.dramexchange.com/},
}

@online{micron-power-calculator,
  author       = {Micron},
  title        = {DRAM power calculator},
  url          = {https://www.micron.com/support/tools-and-utilities/power-calc},
}

@online{samsung-8gb-gddr6,
  author       = {Samsung},
  title        = {8Gb GDDR6 SGRAM C-die},
  url          = {https://datasheet.lcsc.com/lcsc/2204251615_Samsung-K4Z80325BC-HC14_C2920181.pdf},
}

@online{micron-interface-power,
  author       = {Micron},
  title        = {Technical Note. GDDR6: The Next-Generation Graphics DRAM},
  url          = {https://media-www.micron.com/-/media/client/global/documents/products/technical-note/dram/tned03_gddr6.pdf},
}

@online{A100-die-shot,
  title        = {Nvidia Ada Lovelace Leaked Specifications, Die Sizes, Architecture, Cost, And Performance Analysis},
author = {Dylan Patel},
  url          = {https://www.semianalysis.com/p/nvidia-ada-lovelace-leaked-specifications},
}

@techreport{BOOM,
    Author = {Celio, Christopher and Chiu, Pi-Feng and Nikolic, Borivoje and Patterson, David A. and Asanović, Krste},
    Title = {{BOOM v2: an open-source out-of-order RISC-V core}},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2017},
    Month = {Sep},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-157.html},
    Number = {UCB/EECS-2017-157},
}


@INPROCEEDINGS{pcie-power,
  author={Zand, Bahram and Bichan, Mike and Mahmoodi, Alireza and Shashaani, Mansour and Wang, Jing and Shulyzki, Ruslana and Guthrie, James and Tyshchenko, Katya and Zhao, Junhong and Liu, Eric and Soltani, Nima and Freeman, Al and Anand, Rishi and Rubab, Syed and Khela, Ranjit and Sharifian, Shaham and Herterich, Karl},
  booktitle={2022 IEEE International Solid-State Circuits Conference (ISSCC)}, 
  title={{A 1-58.125Gb/s, 5-33dB IL Multi-Protocol Ethernet-Compliant Analog PAM-4 Receiver with 16 DFE Taps in 10nm}}, 
  year={2022},
  volume={65},
  number={},
  pages={1-3},
  doi={10.1109/ISSCC42614.2022.9731768}
}

@techreport{dram-controller-power,
  title={Memory systems and interconnects for scale-out servers},
  author={Volos, Stavros},
  year={2015},
  institution={EPFL}
}


@article{scaling-technology,
    title = {Scaling equations for the accurate prediction of CMOS device performance from 180nm to 7nm},
    journal = {Integration},
    volume = {58},
    pages = {74-81},
    year = {2017},
    issn = {0167-9260},
    doi = {https://doi.org/10.1016/j.vlsi.2017.02.002},
    url = {https://www.sciencedirect.com/science/article/pii/S0167926017300755},
    author = {Aaron Stillmaker and Bevan Baas},
    keywords = {Transistor scaling, Deep submicron performance, VLSI design, CMOS device},
}

@INPROCEEDINGS{newton,
  author={He, Mingxuan and Song, Choungki and Kim, Ilkon and Jeong, Chunseok and Kim, Seho and Park, Il and Thottethodi, Mithuna and Vijaykumar, T. N.},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)}, 
  title={{Newton: A DRAM-maker’s Accelerator-in-Memory (AiM) Architecture for Machine Learning}}, 
  year={2020},
  volume={},
  number={},
  pages={372-385},
  doi={10.1109/MICRO50266.2020.00040}}

@inproceedings{huangfu2022beacon,
  title={{BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support}},
  author={Huangfu, Wenqin and Malladi, Krishna T and Chang, Andrew and Xie, Yuan},
  booktitle={2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={727--743},
  year={2022},
  organization={IEEE}
}

@article{resnet,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  journal      = {CoRR},
  volume       = {abs/1512.03385},
  year         = {2015},
  url          = {http://arxiv.org/abs/1512.03385},
  eprinttype    = {arXiv},
  eprint       = {1512.03385},
  timestamp    = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/1810.04805},
  year         = {2018},
  url          = {http://arxiv.org/abs/1810.04805},
  eprinttype    = {arXiv},
  eprint       = {1810.04805},
  timestamp    = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inbook{gpipe,
author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
title = {GPipe: efficient training of giant neural networks using pipeline parallelism},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {10},
numpages = {10}
}

@inproceedings {alpa,
author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica},
title = {Alpa: Automating Inter- and {Intra-Operator} Parallelism for Distributed Deep Learning},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {559--578},
url = {https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin},
publisher = {USENIX Association},
month = jul
}

@article{megatron,
  title={{Megatron-lm: Training multi-billion parameter language models using model parallelism}},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@online{nvlink,
  title        = {NVLink and NVLink Switch},
  author       = {Nvidia},
  url          = {https://www.nvidia.com/en-us/data-center/nvlink/},
}

@online{sharegpt,
  title        = {ShareGPT},
author = {ShareGPT Team},
  url          = {https://sharegpt.com/},
}

@inproceedings{hbm2e_energy,
  title={A 3.2 Gbps/pin HBM2E PHY with low power I/O and enhanced training scheme for 2.5 D system-in-package solution},
  author={Hwang, SY and Business, Foundry},
  booktitle={Proc. IEEE Hot Chips Symp},
  pages={1--12},
  year={2020}
}

@online{hgxa100,
  title        = {NVIDIA HGX A100, THE MOST POWERFUL END-TO-END
AI SUPERCOMPUTING PLATFORM},
  author = {Nvidia},
  url          = {https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/HGX/a100-80gb-hgx-a100-datasheet-us-nvidia-1485640-r6-web.pdf},
}

@misc{gqa,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245}, 
}

@misc{silu-paper,
      title={Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning}, 
      author={Stefan Elfwing and Eiji Uchibe and Kenji Doya},
      year={2017},
      eprint={1702.03118},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1702.03118}, 
}

@inproceedings{attention,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
title = {Attention is all you need},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6000–6010},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@online{intelxeon,
  title        = {Intel Xeon Gold 6430 Processor, 60M Cache, 2.10 GHz},
  author        = {Intel},
  url          = {https://www.intel.com/content/www/us/en/products/sku/231737/intel-xeon-gold-6430-processor-60m-cache-2-10-ghz/specifications.html},
}

@online{synopsis_dc,
  title        = {Design Compiler. Concurrent Timing, Area, Power, and Test Optimization.},
  author        = {Synopsys},
  url          = {https://www.synopsys.com/implementation-and-signoff/rtl-synthesis-test/dc-ultra.html},
}

@article{hbm3,
  title={High bandwidth memory DRAM (HBM3)},
  author={JEDEC Solid State Technology Association and others},
  journal={JESD238},
  year={2022}
}