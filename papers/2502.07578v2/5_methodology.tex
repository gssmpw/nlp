\section{Methodology}
\label{section:methodology}

Table~\ref{tab:Hardware configurations} lists the system configurations of \att{} and our GPU baseline.
The GPU system contains 4 NVIDIA A100 80GB GPUs equipped with the NVLink 3.0 interconnect.
\att{} has 32 CXL devices, resulting in a similar average power to the GPU system, as further explained in Section~\ref{subsec:power_results}. 

\begin{table}[h]
\footnotesize
\renewcommand\arraystretch{1.1}
\centering
    \caption{Evaluated system configurations}
    \label{tab:Hardware configurations}    
    \scalebox{1}{
        \begin{tabular}{|c||c|c|}
            \hline
            \textbf{System} & \textbf{\att{}} & \textbf{GPU} \\
            \hline
            \hline
            Hardware & 32 CXL devices & 4 NVIDIA A100 \\
            \hline
            Process & 1Y nm (14-16nm) & 7nm \\
            \hline
            Memory & 512GB, GDDR6 & 320GB, HBM2E \\
            \hline
            \multirow{2}{*}{\shortstack{Compute \\ Throughput}} & 512 TFLOPS (PIM) & \multirow{2}{*}{\shortstack{1248 TFLOPS}} \\
            \cline{2-2}
            & 96 TFLOPS (PNM) & \\
            \hline
            Peak Bandwidth & 512 TB/s (Internal) & 8 TB/s (External) \\
            \hline
            3-Year Owned TCO & 0.73\$/hour & 1.76\$/hour \\ 
            \hline
            3-Year Rental TCO & 1.05\$/hour & 5.45\$/hour \\ 
            \hline
            \hline
            GDDR6-PIM & \multicolumn{2}{|c|}{$t_{RCDRD}$=18ns, $t_{RAS}$=27ns, $t_{CL}$=25ns} \\
            Timing Constraints & \multicolumn{2}{|c|}{$t_{RCDWR}$=14ns, $t_{CCDS}$=1ns, $t_{RP}$=16ns} \\
            \hline
        \end{tabular}
    }
\end{table}

We benchmark Llama2 7B, 13B, and 70B models~\cite{touvron2023llama}.
Each evaluated query contains 512 tokens in the prefill stage and 3584 tokens in the decoding stage, adding up to a context length of 4K, \textit{i.e.,} the maximum supported by the Llama2 models.
For a fair comparison between \att{} and the GPU baseline, we deploy these models using different configurations for different parameter sizes: 1, 2, and 4 GPUs, and 8, 20 and 32 CXL devices.
We use vLLM~\cite{vLLM}, the \sota{} inference serving framework on GPUs with a batch size of 128, where the inference throughput saturates (Figure~\ref{fig:Context_Length}).



We generate \att{} instruction traces for a single block and verify the correctness using a functional simulator.
We modify Ramulator2~\cite{luo2023ramulator} to model a CXL device containing 32 GDDR6-PIM memory channels with timing constraints in Table~\ref{tab:Hardware configurations}. 
The inter-device communication through the CXL 3.0 protocol is modeled by an analytical model based on the CXL latency~\cite{li2023pond} and PCIe 6.0 bandwidth. 
To model a CXL switch supporting multicast, we use half of the bandwidth and double the latency of the baseline switch. 
We use Intel Xeon Gold 6430L CPU~\cite{intelxeon} as the host machine in \att{}.

We use Micron DRAM Power Calculator~\cite{micron-power-calculator} to evaluate DRAM core power using current and voltage specifications of Samsung's 8Gb GDDR6 SGRAM C-die~\cite{samsung-8gb-gddr6}.
The MAC operation power is modeled assuming 3$\times$ more current than a typical gapless read~\cite{aim2}.
We assume that each GDDR6 memory controller for two channels consumes 314.6 mW~\cite{dram-controller-power} and each BOOM RISC-V core consumes 250 mW~\cite{boom-pdf}.
We implement the RTL of the remaining components in the CXL controller and synthesize it using a TSMC 28nm technology library and the Synopsys Design Compiler~\cite{synopsis_dc}.
We find the critical path delay as 1ns at 28nm and project the CXL controller clock frequency to be 2.0 GHz at 7nm~\cite{scaling-technology}.

We estimate the die area of CXL controller in two parts. First, we synthesize the custom logic in 28nm~(See Table~\ref{tab:Area_and_power}) and scale it down to 7nm~\cite{scaling-technology}. Then, we add measurements of the memory controller, PCIe controller, and PHY from the NVIDIA GPU die shots~\cite{TU104, A100-die-shot}, which are also scaled down to 7nm. 
This results in an estimated area of 19.0$mm^2$ in 7nm.


\begin{table}[h]
\footnotesize
\centering
\caption{CXL Controller Custom Logic Area\&Power in 28nm}
    \label{tab:Area_and_power}    
    \scalebox{1}{
        \begin{tabular}{|c||c|c|c|}
            \hline
            \multicolumn{2}{|c|}{Components} & Area (mm$^2$) & Power (W) \\
            \hline
            \hline
            \multirow{2}{*}{SRAM} & Instruction Buffer & 3.33 & 0.61 \\
            \cline{2-4}
            % \hline
            & Shared Buffer & 0.11 & 0.03 \\
            \hline
            \multirow{3}{*}{Logics} & Accelerators & 1.34 & 0.18 \\
            \cline{2-4}
            % \hline
            & RISC-V Cores & 2.94 & 0.19 \\
            \cline{2-4}
            % \hline
            & Others & 0.12 & 0.05 \\
            \hline
            \hline
            \multicolumn{2}{|c|}{\textbf{Total}} & \textbf{7.85} & \textbf{1.06} \\
            \hline
        \end{tabular}
    }
\end{table}





\label{TCO}

Table~\ref{tab:Hardware configurations} presents the 3-year Total Cost of Ownership (TCO) for both owned and rental hardware. (a) \textit{Own TCO:} We model a local server by accounting for hardware and operational costs. (b) \textit{Rental TCO:} The cost for host CPU in \att{} and GPU are estimated based on the Microsoft Azure prices ~\cite{azure-price}. The CXL devices in \att{} are evaluated using the owned TCO methodology, as there are no available references for rental costs. To calculate operational cost, we use \$$0.139/KWh$~\cite{electricity-price} and average power consumption. Hardware costs are listed in Table~\ref{tab:hardware_cost}. While the lowest available price for A100 80GB is close to \$20,000, we instead use only \$10,000 by conservatively deducting 50\% margin~\cite{gpu-price}. The PIM module cost is estimated as 10$\times$ the cost of standard DRAM modules~\cite{pim-price, dram-price}.

\begin{table}[h]
    \footnotesize
    \centering
    \caption{Hardware Costs}
    \begin{tabular}{|c||c|c|}
        \hline
        \textbf{System} & \textbf{Hardware} & \textbf{Cost (\$)} \\
        \hline
        \hline
        \multirow{3}{*}{GPU} & Xeon Gold 6430 CPU~\cite{CPU-price} & 2,128 \\
        & 4 NVIDIA A100 80GB GPU~\cite{gpu-price} & 40,000 \\
        \cline{2-3}
        & \textbf{Total Cost} & \textbf{42,128}  \\
        \hline
        \hline
        \multirow{5}{*}{\shortstack{\att{} \\ 32 devices}} & Xeon Gold 6430 CPU~\cite{CPU-price} & 2,128 \\
        & 512GB GDDR6-PIM~\cite{pim-price, dram-price} & 11,873 \\
        & 32 CXL Controllers & 381.3 \\
        & 96-lane 48-port switch~\cite{switch-price} & 490 \\
        \cline{2-3}
        & \textbf{Total Cost} & \textbf{14,873} \\
        \hline
    \end{tabular}
    \label{tab:hardware_cost}
\end{table}

Figure~\ref{fig:TCO} illustrates the breakdown of CXL controller cost per \att{} CXL device (Figure~\ref{fig:CXL_device}). The CXL controller costs are broken down into die, packaging and Non Recurring Engineering (NRE) cost components~\cite{ning2023supply, moonwalk}. Die cost is derived from the wafer cost, considering the CXL controller die area (19.0$mm^2$ in 7nm) and yield rate. A 300mm diameter 7nm wafer costs \$9,346 with a defect density of 0.0015 per $mm^2$~\cite{ning2023supply}. Cost of 2D packaging is assumed to be 29\% of chip cost~\cite{packaging-cost}, while the 2.5D packaging cost is calculated based on interposer, die placement and substrate assembly~\cite{palesko2014cost}.
NRE cost is influenced by chip production volumes, which we estimate at 3 million units based on the following assumptions.
NVIDIA shipped $3.76M$ datacenter GPUs in 2023~\cite{GPU-volume}.
We assume that $10\%$ of datacenter GPUs (around $370K$) are used for LLM inference.
Since each GPU consumes ${\sim}8\times$ more power compared to a CENT device (explained in Section~\ref{subsec:power_results}), we project ${\sim}3M$ volume for \att{} devices.

\begin{figure}[t]
    \centering
    \includegraphics[width=8cm]{Figure/TCO_NEW.pdf}
    \caption{CXL Controller Cost Breakdown}
    \label{fig:TCO}
\end{figure}






