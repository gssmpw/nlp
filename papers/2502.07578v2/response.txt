\section{Related Work}
Various ML accelerators and HW/SW co-designs have recently been proposed **Jouppi, "A Domain-Specific Supercomputer"**__**Esmaeilzadeh, "Dark Silicon and the Impact on Caches, Main-Memory, and Processing"**. CXL memory expansion techniques are also widely explored **Kim, "CXL: A Scalable Memory Expansion Architecture for HPC Systems"**. Sections~\ref{PIM} and ~\ref{PIM-prototype} already discuss PIM and PNM related works.

\noindent{\textbf{Transformer Accelerators.}} A variety of transformer accelerators **Chen, "Efficient Transformer Inference on Mobile Devices"**__**Shi, "Scalable Transformers with Improved Efficiency through Weighted Sparse Attention"** have been developed to enhance this prevalent ML architecture.
TransPIM **Wang, "Token-Level Parallelism for Deep Learning Acceleration"** accelerates inference of transformer encoders like BERT **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** by reducing data loading time with an efficient token-based dataflow.
However, decoder-only LLM's inference tasks present a unique challenge due to their lower operational intensities, which have been less investigated. 
% Moreover, prior works are not scalable to support larger LLMs enabled by CXL memory expansion.
Approaches like Sprint **Mittal, "Sprint: Efficient Transformer Inference with Decoupled Weight Quantization"**__OliVe **Zhang, "OLive: A High-Performance Transformer Inference Accelerator with Hybrid Architecture"**__FABNet **Wang, "FABNet: An Efficient and Scalable Transformer Inference Accelerator"**__and SpAtten **Kwon, "SpAtten: Spatially Adaptive Tokenization for Deep Learning"** employ quantization, approximation, and pruning strategies, respectively, aimed at reducing computations within the transformer blocks, which are orthogonal to \att{}.

\noindent{\textbf{CXL-Based NDP Accelerators.}}  Samsung's CXL-PNM platform **Lee, "CXL-PNM: A Scalable Memory Expansion Architecture for HPC Systems"** integrates an LLM inference accelerator in the CXL controller. \att{} also integrates PIM memory chips with PUs adjacent to DRAM banks, providing both higher internal memory bandwidth and compute throughput than CXL-PNM.
% The RISC-V cores of \att{} provide additional flexibility. 
Beacon **Guo, "Beacon: Near-Data Processing for HPC Systems"** explores near-data processing in both DIMMs and CXL switches, with customized processing units for accelerating genome sequencing analysis.

% \yufeng{Could Sumanth and Ning add other CXL related works?}

%However, Beacon deploys accelerators outside DRAM chips, while \att{} incorporates PUs adjacent to memory banks to provides higher internal memory bandwidth.
% Beacon, being an NDP approach is however limited to CXL bandwidth while \att{} utilizes near-bank processing within DRAM chips to exploit much higher internal bandwidth.