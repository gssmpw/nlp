[
  {
    "index": 0,
    "papers": [
      {
        "key": "eyeriss",
        "author": "Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and Sze, Vivienne",
        "title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks"
      },
      {
        "key": "in-switch",
        "author": "Li, Youjie and Liu, Iou-Jen and Yuan, Yifan and Chen, Deming and Schwing, Alexander and Huang, Jian",
        "title": "Accelerating distributed reinforcement learning with in-switch computing"
      },
      {
        "key": "maeri",
        "author": "Kwon, Hyoukjun and Samajdar, Ananda and Krishna, Tushar",
        "title": "{Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "CXL-DL",
        "author": "Arif, Moiz and Assogba, Kevin and Rafique, M Mustafa and Vazhkudai, Sudharshan",
        "title": "{Exploiting CXL-based memory for distributed deep learning}"
      },
      {
        "key": "CXL-DB",
        "author": "Ahn, Minseon and Chang, Andrew and Lee, Donghun and Gim, Jongmin and Kim, Jungmin and Jung, Jaemin and Rebholz, Oliver and Pham, Vincent and Malladi, Krishna and Ki, Yang Seok",
        "title": "Enabling CXL memory expansion for in-memory database management systems"
      },
      {
        "key": "Demystify-CXL",
        "author": "Sun, Yan and Yuan, Yifan and Yu, Zeduo and Kuper, Reese and Jeong, Ipoom and Wang, Ren and Kim, Nam Sung",
        "title": "{Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices}"
      },
      {
        "key": "cxl-memory-pool",
        "author": "Gouk, Donghyun and Kwon, Miryeong and Bae, Hanyeoreum and Lee, Sangwon and Jung, Myoungsoo",
        "title": "Memory pooling with cxl"
      },
      {
        "key": "jang2023cxl",
        "author": "Jang, Junhyeok and Choi, Hanjin and Bae, Hanyeoreum and Lee, Seungjun and Kwon, Miryeong and Jung, Myoungsoo",
        "title": "{CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and Computation for Billion-Scale Approximate Nearest Neighbor Search}"
      },
      {
        "key": "gouk2022direct",
        "author": "Gouk, Donghyun and Lee, Sangwon and Kwon, Miryeong and Jung, Myoungsoo",
        "title": "{Direct access, High-Performance memory disaggregation with DirectCXL}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "elsa",
        "author": "Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W",
        "title": "{ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks}"
      },
      {
        "key": "dota",
        "author": "Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan",
        "title": "Dota: detect and omit weak attentions for scalable transformer acceleration"
      },
      {
        "key": "sanger",
        "author": "Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun",
        "title": "Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture"
      },
      {
        "key": "FACT",
        "author": "Qin, Yubin and Wang, Yang and Deng, Dazheng and Zhao, Zhiren and Yang, Xiaolong and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi",
        "title": "{FACT: FFN-Attention Co-optimized Transformer Architecture with Eager Correlation Prediction}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "transpim",
        "author": "Zhou, Minxuan and Xu, Weihong and Kang, Jaeyoung and Rosing, Tajana",
        "title": "Transpim: A memory-based acceleration via software-hardware co-design for transformer"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "bert",
        "author": "Jacob Devlin and\nMing{-}Wei Chang and\nKenton Lee and\nKristina Toutanova",
        "title": "{BERT:} Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "sprint",
        "author": "Yazdanbakhsh, Amir and Moradifirouzabadi, Ashkan and Li, Zheng and Kang, Mingu",
        "title": "Sparse attention acceleration with synergistic in-memory pruning and on-chip recomputation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "olive",
        "author": "Guo, Cong and Tang, Jiaming and Hu, Weiming and Leng, Jingwen and Zhang, Chen and Yang, Fan and Liu, Yunxin and Guo, Minyi and Zhu, Yuhao",
        "title": "{OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization}"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "FABNet",
        "author": "Fan, Hongxiang and Chau, Thomas and Venieris, Stylianos I and Lee, Royson and Kouris, Alexandros and Luk, Wayne and Lane, Nicholas D and Abdelfattah, Mohamed S",
        "title": "{Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "spatten",
        "author": "Wang, Hanrui and Zhang, Zhekai and Han, Song",
        "title": "Spatten: Efficient sparse attention architecture with cascade token and head pruning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "cxl-pnm",
        "author": "Park, Sang-Soo and Kim, KyungSoo and So, Jinin and Jung, Jin and Lee, Jonggeon and Woo, Kyoungwan and Kim, Nayeon and Lee, Younghyun and Kim, Hyungyo and Kwon, Yongsuk and Kim, Jinhyun and Lee, Jieun and Cho, YeonGon and Tai, Yongmin and Cho, Jeonghyeon and Song, Hoyoung and Ahn, Jung Ho and Kim, Nam Sung",
        "title": "{An LPDDR-based CXL-PNM Platform for TCO-efficient Inference of Transformer-based Large Language Models}"
      },
      {
        "key": "samsung_pimpnm",
        "author": "Kim, Jin Hyun and Ro, Yuhwan and So, Jinin and Lee, Sukhan and Kang, Shin-haeng and Cho, YeonGon and Kim, Hyeonsu and Kim, Byeongho and Kim, Kyungsoo and Park, Sangsoo and Kim, Jin-Seong and Cha, Sanghoon and Lee, Won-Jo and Jung, Jin and Lee, Jong-Geon and Lee, Jieun and Song, JoonHo and Lee, Seungwon and Cho, Jeonghyeon and Yu, Jaehoon and Sohn, Kyomin",
        "title": "{Samsung PIM/PNM for Transfmer Based AI: Energy Efficiency on PIM/PNM Cluster}"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "huangfu2022beacon",
        "author": "Huangfu, Wenqin and Malladi, Krishna T and Chang, Andrew and Xie, Yuan",
        "title": "{BEACON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support}"
      }
    ]
  }
]