\section{Motivation}

The key motivation driving our work is the \textbf{large memory capacity} requirements and the \textbf{low GPU compute utilization} in LLM inference. The increased demand for memory capacity limits inference batch sizes, leading to low compute utilization. Further, low compute utilization is caused by the low operational intensity of decoder-only LLMs and insufficient memory bandwidth in existing GPU systems.

% LLMs are large
LLM parameter size has witnessed an exponential increase from Billion to Trillion magnitudes, far surpassing previous machine learning (ML) models. In addition, the context windows that modern LLMs support range from 128K to 1M~\cite{gpt4-turbo, gemini-pro}, enabling the ability to understand and generate longer contents.
% , as shown in Table~\ref{tab:context_wiindow}. 
The long context window results in a large KV cache. Further, the KV cache is unique for each user, which requires large memory capacity when scaling up the inference batch size. 

% \begin{table}[htbp]
%     \vspace{-4mm}
%     \centering
%     \caption{Modern LLM Context Window Size}
%     \vspace{-2mm}
%     \scriptsize
%     \label{tab:context_wiindow}    
%     % \setlength{\tabcolsep}{2.5pt} % Default value: 6pt
%     \renewcommand{\arraystretch}{1.1}
%     \scalebox{1.15} {
%     \begin{tabular}{|c|c|c|c|c|c|}
%         \hline
%         \multirow{3}{*}{Model} & \multirow{3}{*}{\shortstack{Meta \\ Llama 2}} & \multirow{3}{*}{\shortstack{Google \\ Gemini \\ 1.0 Pro}} & \multirow{3}{*}{\shortstack{OpenAI \\ GPT-4 \\ Turbo}} & \multirow{3}{*}{\shortstack{Anthropic \\ Claude 3}} & \multirow{3}{*}{\shortstack{Google \\ Gemini \\ 1.5 Pro}}  \\
%          & & & & & \\
%          & & & & & \\
%         \hline
%         \hline
%         \multirow{2}{*}{\shortstack{Context \\ Window}} & \multirow{2}{*}{\shortstack{4K \\ ~\cite{touvron2023llama}}} & \multirow{2}{*}{\shortstack{32K \\ ~\cite{gemini-models}}} & \multirow{2}{*}{\shortstack{128K \\ ~\cite{gpt4-turbo}}} & \multirow{2}{*}{\shortstack{200K \\ ~\cite{claude}}} & \multirow{2}{*}{\shortstack{1M \\ ~\cite{gemini-pro}}} \\
%          & & & & & \\
%         \hline
%     \end{tabular}
%     }
    
%     \vspace{-2mm}
% \end{table}

Figure~\ref{fig:Context_Length} studies the memory capacity requirements, and its adverse impact on batch size using vLLM~\cite{vLLM}, the \sota{} inference serving framework for GPUs. vLLM 
% employs the PagedAttention algorithm to 
dynamically swaps memory blocks between the host CPU and GPUs, enabling flexible batch sizes that can exceed the physical memory capacity of the GPUs. 
% We study the open-source Llama2-70B model and its. While Llama2-70B only supports 4K context length, its fine-tuned versions support up to 32K~\cite{longlora}. 

% {\fontsize{8pt}{11pt}\selectfont 8pt font size test Memory Requirement}
% \todo{Remove batch size = 1 to increase the font in the figure?}

\begin{figure}[htbp]
% \vspace{-3mm}
\centering
    % \includegraphics[width=8.5cm]{Figure/Context_Length.pdf}
    \includegraphics[width=\columnwidth]{Figure/Context_Length.pdf}
    % \vspace{-3mm}
    \caption{Llama2-70B~\cite{touvron2023llama, longlora} on 4 A100 80GB GPUs}
    \label{fig:Context_Length}
    % \vspace{-3mm}
\end{figure}

In Figure~\ref{fig:Context_Length}, the memory requirements (right y-axis, blue bars) increase with larger batch sizes (x-axis). The inference throughput (left y-axis, dot line) improves with larger batch sizes but reaches a plateau once the memory requirement exceeds the GPU memory size. With the context length increasing, the inference throughput saturates with even smaller batch sizes, from batch=128 at 4K context length to batch=8 at 32K context length. Moreover, LLM inference query latency increases with larger batch sizes, as depicted in Figure~\ref{fig:GPU_utilization}(a). Thus realistic query latency service level agreements (SLA) constraints~\cite{mlperf-sla} further prohibit the use of large batch sizes, especially for longer context lengths.


\begin{figure}[t]
% \vspace{-3mm}
\centering
    % \includegraphics[width=8.5cm]{Figure/GPU_utilization.pdf}
    \includegraphics[width=\columnwidth]{Figure/GPU_utilization.pdf}
    % \vspace{-3mm}
    \caption{(a) Llama2-70B inference query latency increases with larger batches on 4 A100 80GB GPUs,  Prompt size=512, Decoding size=3584. (b) GPU compute utilization is measured by Nvidia Nsight Compute profiler on 4 GPUs for Llama2-70B and 1 GPU for the other two models.}
    \label{fig:GPU_utilization}
    % \vspace{-6mm}
\end{figure}


To accommodate the large memory capacity requirements of LLM inference tasks, multiple GPUs are connected together, which leaves the individual GPU underutilized and significantly drives up the hardware cost. Figure~\ref{fig:GPU_utilization}(b) depicts the GPU compute utilization of three ML models. 
% The decoder-only LLMs' operational intensity remains lower compared to earlier ML models such as BERT~\cite{devlin2018bert} and ResNet~\cite{he2016deep}. 
Encoder-only transformer models such as BERT~\cite{devlin2018bert} process input tokens within a prompt concurrently, forming an input matrix to multiply with the weight matrix (GEMM).
Convolutional Neural Networks (CNNs) such as ResNet~\cite{he2016deep} implement parameter reuse in convolution operations, further increasing their operational intensity.

In decoder-only LLMs, prompt processing (prefill stage) is similar to the encoding process in BERT, however, output tokens are generated sequentially during the decoding stage.
This process results in an input vector that multiplies with the weight matrix (GEMV).
The operational intensity of GEMV is substantially lower than GEMM, underutilizing the massive compute throughput of GPUs.
Furthermore, the operational intensity in LLMs does not scale linearly with batch size because the attention calculations rely on KV caches, which is unique to each batch.
Although 128 batch size and group query attention coalesce GEMVs into GEMM, Llama2-70B only utilizes \textcolor{myblue}{21\% of the GPU's compute throughput.}
% still exhibits half GPU utilization when compared to BERT.
\textcolor{myblue}{Due to the significant lower operational intensity, decoding an output token takes $3.4\times$ longer than encoding a prompt token in the prefill stage.
Hence, this work focuses on the decoding stage of the decoder-only LLMs.}


% Note that the operational intensity of Llama2-70B does not scale linearly with batch size because the attention calculations rely on KV caches. The KV caches are unique to each batch, which prevents GEMVs coalescing into GEMM with batching. 

\todo{Do we need this paragraph? It looks redundant:}
Modern GPUs are equipped with substantial computational resources, offering a high peak compute throughput with a comparably limited memory bandwidth. This makes them over-provisioned for LLM inference with relatively lower operational intensity, leading to low GPU utilization.

% \subsection{PIM Provides High Internal Bandwidth}

% Many PIM architectures place processing units (PU) adjacent to memory banks, enhancing internal bandwidth between the bank and PU. %This surpasses the external bandwidth capabilities of conventional DRAM. 
\textbf{PIM Provides Higher Internal Memory Bandwidth}. 
% PIM has been manufactured in various industrial prototypes.
Table~\ref{tab:hardware_comparison} compares various manufactured industrial PIM prototypes and GPU. 
% with Bfloat16/FP16 compute throughput. 
The internal bandwidth provided by PIM significantly exceeds that of high bandwidth memories (HBM) used in high-end GPUs. For example, GDDR6-based AiM~\cite{aim1, aim2} achieves $16~TB/s$ internal memory bandwidth compared to $2~TB/s$ A100 GPU with five HBM2E memory stacks. This large internal bandwidth coupled with lower operational intensity makes PIM architectures more suitable than GPUs to perform LLM inference tasks. 

%Table~\ref{tab:hardware_comparison} shows that the A100 GPU provides a theoretical maximum operational intensity of $156~Ops/byte$,
%Llama2-70B model has an operational intensity of $1~Op/byte$ in a single batch and $59~Op/byte$ with batch=128, contrasting with BERT and ResNet-152, with operational intensity of $282$ and $346~Ops/byte$, respectively. 

\begin{table}[t]
% \small
\footnotesize
    \centering
    \caption{Hardware System Comparison}
    \label{tab:hardware_comparison}    
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Type & \multicolumn{3}{|c|}{PIM} & GPU   \\
        \hline
        % Name & UPMEM~\cite{upmem} & AiM~\cite{aim1} & FIMDRAM~\cite{fimdram} & A100 \\
        Name & UPMEM & AiM & FIMDRAM & A100 \\
        
        % \multirow{2}{*}{Name} & UPMEM & AiM & FIMDRAM & \multirow{2}{*}{A100} \\
        % & ~\cite{upmem} & ~\cite{aim1} & ~\cite{fimdram} & \\
        \hline
        Mem. Units & 8 channels & 32 ch. & 5 stacks & 5 st. \\
        \hline
        Ex. BW (TB/s) & 0.15 & 1 & 1.5 & 2 \\
        \hline
        In. BW (TB/s) & 1 & 16 & 12.3 & - \\
        \hline
        Capacity (GB) & 64 & 16 & 30 & 80 \\
        \hline
        TFLOPS & 0.5 TOPS~\footnotemark & 16 & 6.2 & 312 \\
        \hline
        Ops/Byte & 0.5 & 1 & 0.5 & 156 \\
        \hline
    \end{tabular}
\end{table}

% \begin{table}[t]
% \footnotesize
%     % \vspace{-4mm}
%     \centering
%     \caption{Hardware System Comparison}
%     % \vspace{-1mm}
%     \label{tab:hardware_comparison}    
%     \setlength{\tabcolsep}{2.8pt} % Default value: 6pt
%     \renewcommand{\arraystretch}{1.2}
%     \begin{tabular}{|c|c|c|c|c|c|c|c|}
%         \hline
%         \multirow{3}{*}{Type} & \multirow{3}{*}{Name} & \multirow{3}{*}{\shortstack{Mem. \\ Units}} & \multirow{3}{*}{\shortstack{Ext. \\ BW \\ (TB/s)}} & \multirow{3}{*}{\shortstack{Int. \\ BW \\ (TB/s)}} & \multirow{3}{*}{\shortstack{Memory \\ Capacity \\ (GB)}} & \multirow{3}{*}{\shortstack{Compute \\ Throughput \\ (TFLOPS)}} & \multirow{3}{*}{\shortstack{Operational \\ Intensity \\ (Ops/Byte)}} \\
%          & & & & & & & \\
%          & & & & & & & \\
        
%         \hline 
%         \hline 
%         % \ignore{
%         % & DDR4 & 8 ch & 0.15 & - & 512 & - & - \\
%         % \cline{2-8}
%         % DRAM & GDDR6~\cite{micron-gddr6} & 32 ch & 1 & - & 64 & - & - \\
%         % \cline{2-8}
%         % & HBM2E & 5 st & 2  & - & 80 & - & - \\
%         % \hline
%         % }
%         & UPMEM~\cite{upmem} & 8 ch & 0.15 & 1  & 64 & 0.5 TOPS\footnotemark 
%         & 0.5  \\
%         \cline{2-8}
%         PIM & AiM~\cite{aim1} & 32 ch & 1  & 16 & 16 & 16 & 1  \\
%         \cline{2-8}
%         & FIMDRAM~\cite{fimdram} & 5 st & 1.5 & 12.3 & 30 & 6.2  & 0.5  \\
%         \hline
%         GPU &  A100 & 5 st & 2 & - & 80 & 312  & 156 \\
%         \hline
%         % CPU &  Xeon Max & 8 ch & 0.31 & - & 512  & $\approx$ 10  & 32.5  \\
%         % \hline
%     \end{tabular}
    
%     \vspace{-5mm}
% \end{table}
% \vspace{-7mm}
\footnotetext{UPMEM supports only integer precision, so unit is TOPs.}


%Despite the low operational intensity, the aggregate operations needed for LLM inference are still large. Hence modern CPUs with limited acceleration features and memory bandwidth fall short of the compute resources.



% While PIM's large internal bandwidth aligns well with the requirements of inference tasks, PIM designs typically exhibit low operational intensity. AiM achieves an operational intensity of $1~Ops/Byte$ as only low-intensity compute logic can be fabricated with DRAM processes. Fortunately, these compute resources match the decoder-only LLM requirements.

% \subsection{Hierarchical PIM-PNM architures}
\todo{Avoid selecting thoughts}
UPMEM~\cite{upmem} and FIMDRAM~\cite{fimdram} supports general-purpose operations near memory banks. AiM~\cite{aim1, aim2} applies a domain specific design into near-bank PUs, providing higher compute throughput. Although a custom PU design achieves better performance, it cannot support the entire transformer block without resorting to expensive GPUs.
To balance this trade-off between generality and customization, we propose a \textbf{hierarchical PIM-PNM architecture.} In PIM channels, the near-bank PUs are composed of MAC units, which can efficiently support the GEMV operations in the transformer block. The PNM units are placed near memory channels, comprising accelerators and RISC-V cores to perform infrequent special operations. RISC-V cores provides flexibility and support a diverse range of LLMs. 

% \subsection{CXL Features an Efficient and Scalable Network}

Additionally, an inherent trade-off with PIM is the increased area due to processing units and enabling internal memory access features. This consequently leads to a reduction in memory capacity when compared to equivalent DRAM technologies. For instance, 
DDR4-based UPMEM R-DIMM~\cite{upmem} has 4$\times$ lower memory capacity compared to conventional DDR4 R-DIMMs. 
%\reetu - Upmem uses RDIMM or registered DIMM which comes in smaller sizes than typical unbuffered DIMMs.
GDDR6 channels already have lower memory capacity than DDR4, and GDDR6-based AiM further incurs an area overhead of 25\% due to added computational circuit~\cite{aim2}. 
%4$\times$ compared to highest density GDDR6 chips~\cite{micron-gddr6} available for same I/O pin bandwidth of 16 Gb/s. 
This motivates our proposed architecture that compensates for the reduced memory capacity in PIM by utilizing CXL~\cite{sharma2022compute} based memory expansion. 

\textbf{CXL Features an Efficient and Scalable Network.}
As a low-latency interconnect protocol built on top of the PCIe physical layer, CXL supports peer-to-peer inter-device communication in the 3.0 version~\cite{CXL}. CXL.mem offers an effective solution to connect plenty of memory modules, achieving lower latency than network-based RDMA solutions. The CXL 3.0 protocol can support up to 4,096 nodes, exhibiting better scalability than NVLink. Although NVLink provides higher bandwidth, which is critical in LLM training, CXL network is capable of LLM inference with the mapping strategies introduced in Section~\ref{model mapping}.










% The parameter size of LLMs has witnessed an exponential increase, far surpassing previous machine learning (ML) models.
% However, the decoder-only LLMs' operational intensity remains lower compared to earlier ML models such as BERT~\cite{devlin2018bert} and ResNet~\cite{he2016deep}. In BERT, input tokens within a prompt are processed concurrently, forming an input matrix to multiply with the weight matrix. This parallel processing contrasts with decoder-only LLMs, where tokens are generated one after another in a sequential manner, resulting in an input vector that multiplies with the weight matrix. The operational intensity of vector-matrix multiplication (GEMV) is substantially lower than that of matrix-matrix multiplication (GEMM). Additionally, convolutional neural networks (CNNs), such as ResNet, not only engage in matrix-matrix multiplications but also implement kernel reuse strategies, which further increase the operational intensity. 
% % The comparison above is shown in Figure~\ref{fig:gpu_op_density} (a). 
% With a single batch, Llama2-70B model has an operational intensity of $1~Op/byte$ contrasting with BERT and ResNet-152, with operational intensity of $282$ and $346~Ops/byte$, respectively.


% \begin{figure*}[ht]
% 	\centering
%   	\includegraphics[width=17.5cm]{Figure/Combo_Motivation.pdf}
%   	\vspace{-4mm}
% 	\caption{(a) Llama2-70B uses 4 NVIDIA A100 80GB GPUs while the other two models use a single GPU. Llama2-70B is measured with 128 batches and 4K context length. (b) With larger inference batch sizes (x-axis), memory requirements (right y-axis) increase because KV Caches of each batch are unique. The inference throughput (left y-axis) improves with increasing batch sizes but reaches a plateau once the memory requirement exceeding the GPU memory size. While Llama2-70B only supports 4K context length, its fine-tuned versions support up to 32K context length~\cite{longlora}. With longer context length, the inference throughput saturates with smaller batch sizes. (c) Inference query latency exhibits a linear increase with respect to both batch size and context length.}
% \label{fig:Combo_Motivation}
% \vspace{-6mm}
% \end{figure*}

% Modern GPUs enjoy expansive compute resources, offering a high peak operation throughput at a comparably limited memory bandwidth. Table~\ref{tab:hardware_comparison} shows that A100 GPUs offer $156~Ops/byte$. Thus GPU's compute resources are over-provisioned for LLMs which require a low operational intensity.
% Further, to accommodate large parameter sizes, multiple GPUs are connected together to provide a high aggregate memory capacity, which significantly drives up the cost of inference hardware. In addition, the large KV cache unique to each user requires extra memory capacity (Table~\ref{tab:context_wiindow}), limiting the inference batch size.
% Therefore, we observe that GPUs suffer from significant underutilization for decoder-only LLMs. To illustrate this phenomenon, we measure the GPU utilization using the maximum supported batch size of three models, as shown in Figure~\ref{fig:Combo_Motivation}(a).
% % To illustrate this phenomenon, we measure the utilization of the instruction issue throughput of the NVIDIA A100 GPUs using maximum supported batch size of three machine learning models.
% % Figure~\ref{fig:Combo_Motivation}(a) shows that while BERT and ResNet-152 highly utilize a GPU ($43\%$ and $80\%$), Llama2-70B only utilizes $21\%$ compute throughput of 4 GPUs. 


% Llama2-70B GPU utilization is measured using vLLM~\cite{vLLM}, a state-of-the-art LLM inference serving framework.
% While vLLM uses the PageAttention technique to swap memory blocks between the host CPU and GPUs to support flexible batch sizes, the inference throughput reaches a plateau once the memory requirement exceeds the GPU memory size, as shown in Figure~\ref{fig:Combo_Motivation}(b). 
% % We measure Llama2 70B with 128 batches and 4K context length, where the inference throughput stops increasing.
% Note that with longer context lengths, the inference throughput saturates with smaller batch sizes. 
% % LLMs use key-value caches to store context information.
% The context windows that modern LLMs support are increasing from thousands of tokens to 1 million tokens to understand and generate longer contents, as shown in Table~\ref{tab:context_wiindow}. Figure~\ref{fig:Combo_Motivation}(c) shows that the inference query latency increases with larger batch sizes, detrimentally affecting user experiences. Both longer context length and quality of service (QoS) requirements limit inference batch sizes, making the GPU utilization worse in real world LLM deployments.



% Despite the low operational intensity, the aggregate operations needed for LLM inference are still large. Hence modern CPUs with limited acceleration features and memory bandwidth fall short of the compute resources.

% Many PIM architectures place processing units (PU) adjacent to each memory bank, enhancing internal bandwidth between the bank and PU. %This surpasses the external bandwidth capabilities of conventional DRAM. 
% Table~\ref{tab:hardware_comparison} compares different PIM technologies with their Bfloat16/FP16 compute throughput. The internal bandwidth provided by PIM significantly exceeds that of High Bandwidth Memories (HBM) used in high-end GPUs. For example, GDDR6-based AiM~\cite{aim1, aim2} achieves $32~TB/s$ internal memory bandwidth compared to $2~TB/s$ A100 GPU with five HBM2e memory stacks. While PIM's large internal bandwidth aligns well with the requirements of inference tasks, PIM designs typically exhibit low operational intensity. AiM achieves an operational intensity of $1~Ops/Byte$ as only low-intensity compute logic can be fabricated with DRAM processes. Fortunately, these compute resources match the decoder-only LLM requirements.


% However, an inherent trade-off with PIM is the increased area due to processing units and enabling PIM memory access features. This consequently leads to a reduction in memory capacity when compared to equivalent DRAM technologies. For instance, 
% DDR4-based UPMEM R-DIMM~\cite{upmem} has 4$\times$ lower memory capacity compared to conventional DDR4 R-DIMMs. 
% %\reetu - Upmem uses RDIMM or registered DIMM which comes in smaller sizes than typical unbuffered DIMMs.
% GDDR6 channels already have lower memory capacity than DDR4, and GDDR6-based AiM further incurs an area overhead of 25\% due to added computational circuit~\cite{aim2}. 
% %4$\times$ compared to highest density GDDR6 chips~\cite{micron-gddr6} available for same I/O pin bandwidth of 16 Gb/s. 
% This motivates our proposed architecture that compensates for the reduced memory capacity in PIM by utilizing CXL~\cite{sharma2022compute} based memory expansion. 

% CXL is a low-latency interconnect protocol built on top of the PCIe physical layer, supporting peer-to-peer inter-device communication in the 3.0 version~\cite{CXL}. CXL offers an effective and scalable solution to connect plenty of memory modules, achieving lower latency than network-based RDMA solutions. 
% % Multi-sockets CPUs connected through the UPI protocol also support a large amount of memory modules but use more CPUs as control nodes than CXL-based solutions.

% % [TODO: not cache but inter-device] CXL is a low-latency cache-coherent interconnect built on top of the PCIe physical layer. CXL offers an effective and scalable solution for memory expansion as it supports byte-addressable and CPU cacheable accesses, even though it is built upon PCIe. This allows the host to access any CXL-connected remote memory devices seamlessly and even integrate PCIe storage into a cache-coherent memory space, significantly expanding the available memory~\cite{Jung2022Storage}. 
