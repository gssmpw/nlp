\section{Conclusion}

Given the challenges posed by the low operational intensity and substantial memory capacity requirements of decoder-only LLMs, we introduce \att{}, utilizing PIM technology to facilitate the high internal memory bandwidth and CXL memory expansion to ensure ample memory capacity. 
When compared to GPU baselines with the maximum supported batch sizes, \att{} achieves 2.3$\times$ higher throughput and consumes 2.3$\times$ less energy. \att{} also enables lower TCO and generates 5.2$\times$ more tokens per dollar than GPUs.

% % \att{} explores the parallel mapping strategies to balance the latency and throughput in LLM inference, 
% \att{} achieves 4.6$\times$ speedup over GPUs on latency-critical tasks and 2.5$\times$ more tokens per second on throughput-critical tasks, consuming 17.9$\times$ and 2.5$\times$ less energy to generate a token than GPUs in these scenarios, respectively.
% % \att{} consumes 16.5 $\times$ and 30.7 $\times$ less energy to generate a token than GPUs in these scenarios, respectively.
% \todo{Notably, in reference, the full, non-abbreviated first and last names of all co-authors of all citations must be specified (no “et al.”)}