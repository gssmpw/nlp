%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
% \usepackage{epsfig} % for postscript graphics files
% \usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{url}
\usepackage{array}
\usepackage{cite}
\usepackage[hidelinks]{hyperref}
\usepackage{tablefootnote}
\usepackage{booktabs}
\title{\LARGE \bf
Motion Control in Multi-Rotor Aerial Robots Using Deep Reinforcement Learning
}
\author{Gaurav Shetty$^{1,2}$, Mahya Ramezani$^{1}$, Hamed Habibi$^{1}$, Holger Voos$^{1}$, and Jose Luis Sanchez-Lopez$^{1}$% <-this % stops a space
\thanks{$^{1}$Authors are with the Automation and Robotics Research Group, Interdisciplinary Centre for Security, Reliability, and Trust (SnT), University of Luxembourg, Luxembourg. Holger Voos is also associated with the Faculty of Science, Technology and Medicine, University of Luxembourg, Luxembourg. \texttt{\{gaurav.shetty, mahya.ramezani\}@ext.uni.lu},
\texttt{\{hamed.habibi, holger.voos, joseluis.sanchezlopez\}@uni.lu}}%
\thanks{$^{2}$Gaurav Shetty is also with the Bonn-Rhein-Sieg University of Applied Sciences, Germany.}
}
% \author{Gaurav Shetty$^{1}$, Mahya Ramezani$^{1}$, Hamed Habibi$^{1}$, Jose Luis Sanchez-Lopez$^{1}$, and Holger Voos$^{1}$% <-this % stops a space
% \thanks{$^{1}$All authors are with the SnT Automation and Robotics Research Group, University of Luxembourg.}%
% \thanks{Emails: \texttt{gaurav.shetty@ext.uni.lu}, \texttt{mahya.ramezani@ext.uni.lu},\\
% \texttt{hamed.habibi@uni.lu}, \texttt{jose.sanchez@uni.lu}, \texttt{holger.voos@uni.lu}}%
% }
% \author{Albert Author$^{1}$ and Bernard D. Researcher$^{2}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%         University of Twente, 7500 AE Enschede, The Netherlands
%         {\tt\small albert.author@papercept.net}}%
% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small b.d.researcher@ieee.org}}%
% }


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This paper investigates the application of Deep Reinforcement Learning (DRL) to address motion control challenges in drones for additive manufacturing (AM). Drone-based additive manufacturing offers a flexible and autonomous solution for material deposition in large-scale or hazardous environments. However, achieving robust real-time control of a multi-rotor aerial robot under varying payloads and potential disturbances remains challenging. Traditional controllers like PID often require frequent parameter re-tuning, limiting their applicability in dynamic scenarios. We propose a DRL framework that learns adaptable control policies for multi-rotor drones performing waypoint navigation in AM tasks. We compare Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic Policy Gradient (TD3) within a curriculum learning scheme designed to handle increasing complexity. Our experiments show TD3 consistently balances training stability, accuracy and success, particularly when mass variability is introduced. These findings provide a scalable path toward robust, autonomous drone control in additive manufacturing.


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Unmanned Aerial Vehicles (UAVs) constitute a technology within autonomous systems, with applications in precision agriculture, infrastructure inspection, and logistics \cite{c1}. Recently, the integration of UAVs with Additive Manufacturing (AM) capabilities has been recognized as a promising development \cite{c2}. UAV-based AM combines the aerial mobility of drones with 3D printing technologies, enabling material deposition in complex or otherwise inaccessible environments \cite{c3}. This integration introduces new possibilities for construction and repair, particularly for large-scale structures that are difficult to access using traditional methods.

However, the deployment of UAVs for AM tasks presents control challenges \cite{c4}. As drones deposit materials, their mass distribution changes continuously, altering both the center of gravity and inertia. In addition, external disturbances further complicate control, making it difficult to maintain stability and accuracy in dynamic three-dimensional environments \cite{c5}. These challenges highlight the need for robust and adaptive control systems \cite{c27}.

Traditional control systems \cite{c6,c7_new}, have been widely utilized in UAV navigation. These methods rely on fixed parameters and predefined models, performing effectively in predictable environments \cite{c7,c9_new}. However, within the context of UAV-based AM, the continuous changes in mass distribution and the presence of external forces require frequent recalibration of these controllers. This necessity limits their suitability for real-time adaptive applications, where dynamic and unpredictable conditions are prevalent \cite{c8}.

Reinforcement Learning (RL) offers an alternative approach to UAV control by enabling systems to learn control policies through interaction with their environment, without depending on explicit system models. This model-free method allows RL algorithms to adapt dynamically to both internal variations and external disturbances  \cite{c9}. However, standard RL methods encounter challenges in high-dimensional environments like AM, where state and action spaces are extensive and the environment is highly unpredictable \cite{c10, c26}.

Deep Reinforcement Learning (DRL) builds on traditional reinforcement learning by using neural networks to approximate value functions and policies. This approach enables effective learning in high-dimensional state and action spaces. DRL proves effective in addressing the complex challenges associated with UAV-based AM \cite{c11}. 

A key consideration in applying DRL to UAV-based AM is the choice between deterministic and stochastic policy methods. Deterministic approaches, such as Deep Deterministic Policy Gradient (DDPG) \cite{c12}, produce specific actions for a given state. This ensures higher precision and consistency \cite{c13}, which are essential for accurate material deposition in AM. Stochastic methods sample actions from probability distributions, introducing variability that can reduce precision. Deterministic methods are better suited for UAV-based AM as they provide predictable and repeatable control. They also reduce computational overhead and work well in stable, low-noise environments \cite{c14}. However, DDPG faces challenges such as instability and overestimation bias \cite{c9}. Twin Delayed Deep Deterministic Policy Gradient (TD3) \cite{c15} addresses these issues. It uses twin critics to reduce overestimation and delayed policy updates to improve stability. These improvements make TD3 an effective choice for UAV-based AM tasks that require precise and reliable control under varying conditions \cite{c16}. 

Despite advancements, applying DRL to UAV-based AM faces several practical issues. These include improving policy stability during training, adapting to dynamic task complexity, and ensuring reliable performance under changing operational conditions \cite{c17}. Curriculum learning helps address these issues by using a gradual training strategy. It starts with simple navigation tasks and incrementally introduces dynamic waypoints, variable payloads, and external disturbances. This staged approach stabilizes training, improves robustness, and enhances real-world applicability \cite{c18}. 

Waypoint-based navigation simplifies the control problem by dividing it into discrete target points. It provides clear evaluation metrics, measuring accuracy through the average positional error and precision via the standard deviation of these errors. This approach enables systematic testing of control algorithms, facilitates benchmarking in dynamic environments, and ensures precise assessments of UAV control strategies \cite{c19}. 

This work focuses on controlling a UAV in 3D for AM using DDPG and TD3. Curriculum learning is integrated to improve training stability and performance under increasing task complexity. The study compares DDPG and TD3 to identify the most effective control strategy for UAV-based AM.

Key contributions include:
\begin{itemize}

\item The UAV-AM control problem is modeled as a Markov Decision Process (MDP) \cite{c20} with well-defined states, actions, rewards, and constraints. The state includes accelerations in the x, y, and z directions, to allow the agent to infer mass variations and adapt thrust commands.
\item The performance of DDPG and TD3 is evaluated using metrics such as average cumulative reward, positional error and precision. 
\item Curriculum learning is introduced to enable the agent to handle progressively complex tasks effectively. 
\item Waypoint-based navigation is used to simplify the control problem and facilitate evaluation and training. 

\end{itemize}

This study provides a framework for improving UAV control in AM, supporting advancements in autonomous construction and repair technologies. 


\section{Problem Formulation And Preliminaries}

This study focuses on developing a precise and stable UAV control system for AM in a structured environment. The UAV operates in a three-dimensional space governed by nonlinear dynamics, incorporating thrust forces, gravitational effects, and external disturbances. The mathematical formulation of the UAV’s motion dynamics follows standard multirotor modeling, as detailed in \cite{mellinger2012trajectory}. 

The control objective is to enable accurate waypoint navigation while adapting to system variations. To simplify the problem, waypoints are predefined in a fixed sequence, eliminating the need for path planning and allowing a focus on control adaptation. Material deposition is modeled by introducing a reaction force in the negative z-direction to simulate the UAV counteracting deposition forces. The force exerted on the UAV due to material deposition is given by  

% \begin{equation}
%     F = \dot{m} v_{\text{exit}},
% \end{equation}
\[
F = \dot{m} v_{\text{exit}},
\]

where \( \dot{m} \) is the mass flow rate of the extruded material and \( v_{\text{exit}} \) is the velocity of material leaving the nozzle. Based on the material properties reported in \cite{zhang2022aerial}, with a density of \( 1700 \) kg/m³, a nozzle diameter of \( 8 \) mm, and a flow velocity of \( 0.5 \) m/s, the calculated force is approximately  

% \begin{equation}
%     F = 0.02135 \text{ N}
% \end{equation}
\[
F = 2.135 \times 10^{-2}~\text{N}
\]

The corresponding acceleration imparted to the UAV is computed as  

% \begin{equation}
%     a = \frac{F}{m},
% \end{equation}
\[
a = \frac{F}{m},
\]


where \( m \) is the UAV mass. This additional acceleration is incorporated into the UAV’s motion model in the simulation to effectively account for the impact of material deposition. Additionally, random noise is introduced into sensor measurements to ensure robustness against real-world uncertainties. A uniform noise model was used, with only measurement noise (no process noise), and this noise was injected into the state space of the drone once we obtain it from the multirotor model in the simulation. The environment is assumed to be obstacle-free, allowing the study to focus solely on the effects of waypoint tracking, external disturbances, and system variations. The UAV is controlled through roll, pitch, and thrust, with yaw control omitted to reduce complexity while preserving stability. The rotational dynamics, including attitude control, are managed separately using the MATLAB UAV Toolbox multirotor model (version 2024a), which ensures stability by converting control inputs to PWM commands. This design choice is justified as AM applications primarily rely on precise lateral and vertical positioning rather than continuous yaw adjustments.  

To improve training efficiency and generalization, curriculum learning is introduced. The agent first learns basic waypoint navigation under controlled conditions before progressively handling more complex challenges, such as dynamic waypoints, mass variability, and external disturbances. This staged approach facilitates more stable learning and enhances adaptability. The UAV control problem is formulated as a MDP, The next section defines the MDP representation and details the integration of curriculum learning into the RL framework. 

% \subsection{Problem Modeling}
% The challenge addressed in this research involves developing a precise and stable control system for UAVs performing AM in an idealized environment. The UAV's mass changes at the start of each episode but remains constant throughout, simplifying the problem while still introducing variability. Several other key considerations are made to focus the study on control adaptation. First, waypoints are given in a predefined sequence, eliminating the need for path planning. Second, the effect of material deposition is modeled by applying a reaction force in the negative z-direction, facing upwards, simulating the UAV counteracting the force from material deposition. Third, sensor noise is introduced into the system by adding random noises to the state estimator outputs, ensuring the trained policy remains robust under realistic conditions. Additionally, the environment is assumed to be obstacle-free, allowing the study to focus solely on the challenges posed by waypoint navigation, mass changes, and external disturbances without additional complexities related to collision avoidance.
% % The challenge addressed in this research involves developing a robust and adaptive control system for UAVs tasked with AM in dynamic environments. These environments are characterized by continuous mass redistribution due to material deposition and external disturbances. The primary objective is to enable precise waypoint navigation while compensating for these dynamic internal and external factors.
% Waypoint-based navigation is adopted to divide the task into discrete target points. This approach simplifies control, allows systematic evaluation of algorithms, and ensures repeatability for comparative analysis. It is particularly effective in AM, where precise material deposition along predefined paths is critical

% The UAV is modeled as a multirotor aerial vehicle operating in three-dimensional space, governed by nonlinear equations of motion that incorporate thrust forces, gravitational effects, and external disturbances \cite{mellinger2012trajectory}. The UAV receives four control inputs—roll, pitch, yaw rate, and thrust. However, in this study, the RL agent optimizes only roll, pitch, and thrust, omitting yaw control to simplify learning while maintaining flight stability. This design choice reduces the complexity of the learning process while ensuring that the UAV remains maneuverable and capable of executing precise AM tasks.

% The consideration of waypoint navigation offers significant benefits, particularly in experimental setups where accurate path planning and execution are essential for evaluating UAV performance. Waypoint-based navigation enables the systematic assessment of a UAV's ability to adapt to dynamic conditions, ensuring that it follows predefined paths while compensating for disturbances and changes in mass distribution. Experimental studies, such as those in [1, 2], have demonstrated that incorporating waypoints into UAV control frameworks allows for rigorous validation of navigation algorithms, highlighting their ability to handle real-world complexities. Moreover, waypoint navigation ensures repeatability in experiments, facilitating comparisons across different control strategies and environmental conditions, as noted in [3]

% To address these challenges, the UAV control problem is formulated as a reinforcement learning (RL)-based optimization task. The UAV is modeled as a multirotor aerial vehicle operating in three-dimensional space, governed by nonlinear equations of motion incorporating thrust forces, gravitational effects, and external disturbances \cite{mellinger2012trajectory}. The UAV receives four control inputs—roll, pitch, yaw rate, and thrust. However, in this study, the RL agent optimizes only roll, pitch, and thrust, omitting yaw control to simplify learning while maintaining flight stability.

% Several key considerations are made to focus the study on control adaptation. First, waypoints are given in a predefined sequence, eliminating the need for path planning. Second, the effect of material deposition is modeled by applying a reaction force in the negative z-direction, facing upwards, simulating the UAV counteracting the force from material deposition. Third, sensor noise is introduced into the system by adding random noise to the state estimator outputs, ensuring the trained policy remains robust under realistic conditions. Additionally, the environment is assumed to be obstacle-free, allowing the study to focus solely on the challenges posed by waypoint navigation, mass variations and external disturbances without additional complexities related to collision avoidance.

% \subsection{Optimization Problem}

% The control problem for UAV-AM is formulated within the framework of a MDP, where the UAV’s state representation includes positional errors, velocity components, accelerations, and orientation parameters. The action space consists of roll, pitch, and thrust commands, with the objective of optimizing the control policy to ensure stable waypoint following.

% The optimization problem is defined as:

% \begin{equation}
% \max_{\pi} \mathbb{E}\left[ \sum_{t=0}^{T} \gamma^t R_t \right]
% \end{equation}

% where:
% \begin{itemize}
%     \item \( \pi \) represents the control policy,
%     \item \( \gamma \) is the discount factor,
%     \item \( R_t \) is the reward function at time step \( t \).
% \end{itemize}

% The optimization problem is formulated to maximize cumulative rewards, with the reward function providing higher positive rewards as the UAV approaches the next waypoint in the given sequence. The objective is to ensure stability and robustness under dynamic conditions, such as changing mass and external disturbances. By prioritizing movement toward the next waypoint, the UAV can effectively adapt to variations in system dynamics while maximizing the overall reward.

% To enhance training efficiency and robustness, curriculum learning is employed to progressively introduce complexity during training. This approach allows the UAV to first master simple navigation tasks before facing more challenging scenarios, such as dynamic waypoints, variable UAV mass, and external disturbances. The task complexity at time step \( t \), represented as \( C(t) \), transitions through predefined levels \( C_1, C_2, \dots, C_n \), enabling the agent to build foundational skills before addressing more advanced challenges.

% The curriculum learning structure is mathematically modeled by the complexity function \( C(t) \), which dictates the task difficulty throughout the agent's learning process:

% \begin{equation}
% C(t) \in \{C_1, C_2, \dots, C_n\}
% \end{equation}

% At each level \( C_i \), the reward function remains consistent, rewarding the agent for approaching the target waypoint, with the complexity level introducing additional challenges. The reward function can be detailed in later sections of the paper.

% The agent’s learning process is governed by the optimization of the cumulative reward, as the complexity level \( C(t) \) increases over time:

% \begin{equation}
% \max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t, C(t)) \right]
% \end{equation}

% where \( R(s_t, a_t, C(t)) \) represents the reward at time step \( t \), prioritizing the UAV's progress toward the target waypoint despite increasing complexity.

% This curriculum learning approach ensures a stable learning curve, as the agent gradually adapts to more complex tasks while maximizing the cumulative reward.

% We use DDPG and TD3 to solve this problem, comparing their effectiveness for UAV-based AM.


% The RL agent directly outputs roll, pitch, and thrust commands, which are applied to the multirotor model to compute the next UAV state. This formulation eliminates the need for an explicit state estimator within the learning loop, allowing the control policy to learn directly from state transitions. By integrating experimental considerations such as waypoint following, external disturbances, and material deposition effects, this research establishes a structured and adaptable framework for UAV control in AM applications. The proposed approach enables the UAV to follow waypoints accurately and maintain stable flight under dynamic conditions, addressing key challenges in real-world UAV-based AM operations.

\section{Proposed Approach}

The proposed approach develops a DRL-based control system for UAV navigation in AM applications. The methodology is organized into two main phases:

Two off-policy DRL algorithms, DDPG and TD3, are evaluated for their performance in continuous action spaces. DDPG, while effective for high-dimensional tasks, is prone to overestimation bias, which TD3 mitigates using twin critics, target policy smoothing, and delayed updates. Both algorithms are tested in a simplified 3D simulation, where the UAV navigates between fixed waypoints. Metrics such as accuracy, precision, and success ratio are used for performance evaluation.

Next, curriculum learning is used to gradually increase task complexity. The agent starts with simple waypoint navigation in static environments and progresses to dynamic waypoints, variable payloads, and external disturbances. This staged approach stabilizes training, accelerates convergence, and ensures the UAV learns robust and adaptive control strategies for AM applications.
% The proposed approach develops a DRL-based control system for UAV navigation in AM applications. The methodology is organized into two main phases.

% In the first phase, two off-policy DRL algorithms, DDPG and TD3, are evaluated for their performance in continuous action spaces. DDPG, while effective for high-dimensional tasks, is prone to overestimation bias, which TD3 mitigates using twin critics, target policy smoothing, and delayed updates. Both algorithms are tested in a simplified 3D simulation where the UAV navigates between fixed waypoints. Performance is evaluated using metrics such as average positional error, precision, and success ratio.

% The second phase builds upon the best-performing agent from the first phase and incorporates curriculum learning to gradually increase task complexity. Initially, the agent is trained on simple waypoint navigation in a static environment. As training progresses, the complexity increases with the introduction of dynamic waypoints, variable payloads, and external disturbances. This staged approach stabilizes training, accelerates convergence, and ensures that the UAV learns robust and adaptive control strategies for AM applications.

% The next section defines the MDP representation and details the integration of curriculum learning into the RL framework.

% Two off-policy DRL algorithms, DDPG and TD3, are evaluated for their performance in continuous action spaces. DDPG, while effective for high-dimensional tasks, is prone to overestimation bias, which TD3 mitigates using twin critics, target policy smoothing, and delayed updates. Both algorithms are tested in a simplified 3D simulation, where the UAV navigates between fixed waypoints. Metrics such as accuracy, precision, and success ratio are used for performance evaluation. 

% Next, curriculum learning is used to gradually increase task complexity. The agent starts with simple waypoint navigation in static environments and progresses to dynamic waypoints, variable payloads, and external disturbances. This staged approach stabilizes training, accelerates convergence, and ensures the UAV learns robust and adaptive control strategies for AM applications.

\subsection{System Architecture}

The environment is modeled as an MDP, characterized by state and action spaces. An MDP is defined by a tuple $(S,A,P,R,\gamma)$, where $S$ denotes the state space, $A$ represents the action space, $P: S \times A \times S \to [0,1]$ specifies the transition probability function $P(s'|s,a)$, indicating the likelihood of transitioning from the current state $s$ to a new state $s'$ upon executing action $a$; $\gamma$ is a discount factor within the range $(0,1)$, and $R: S \times A$ defines the reward function. The objective is to maximize cumulative rewards: 

\begin{equation}
\max_{\pi} \mathbb{E}\left[ \sum_{t=0}^{T} \gamma^t R_t \right],
\end{equation}

where \( \gamma \) is the discount factor, and \( R_t \) represents the reward at time \( t \), designed to ensure stability and precise trajectory tracking.

To enhance training efficiency and improve policy robustness, curriculum learning is implemented to gradually introduce complexity. The agent begins with basic navigation tasks and progressively encounters more challenging scenarios, including dynamic waypoints, variable mass distributions, and external disturbances. The task complexity at time step \( t \), represented as \( C(t) \), which evolves as \( C(t) \in \{C_1, C_2, \dots, C_n\} \). The curriculum-based objective is then defined as: 

\begin{equation}
\max_{\pi} \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t R(s_t, a_t, C(t)) \right],
\end{equation}

where \( C(t) \) adjusts task complexity throughout training, ensuring the UAV progressively learns to handle increasingly difficult conditions while maintaining stability. 

The state representation for the UAV in the proposed control framework is defined by a state vector $s_t$, which encapsulates key physical and positional parameters critical for navigation and control. This vector is expressed as: 
% \[
% s_t = [a_{x,t}, a_{y,t}, a_{z,t}, \Delta x_t, \Delta y_t, \Delta z_t, v_{x,t}, v_{y,t}, v_{z,t}, \theta_t, \phi_t, \psi_t, z_t],
% \]

\begin{equation}
s_t = \begin{bmatrix}
a_{x,t}, & a_{y,t}, & a_{z,t}, & \Delta x_t, & \Delta y_t, & \Delta z_t, \\
v_{x,t}, & v_{y,t}, & v_{z,t}, & \theta_t, & \phi_t, & \psi_t, & z_t
\end{bmatrix},
\end{equation}

where $a_{x,t}, a_{y,t}, a_{z,t}$ are the drone's acceleration along the $x$-, $y$-, and $z$-axes, respectively. $\Delta x_t, \Delta y_t, \Delta z_t$ are the positional differences between the drone's current position and the target position along the $x$-, $y$-, and $z$-axes. $v_{x,t}, v_{y,t}, v_{z,t}$ are the velocity components of the drone in the $x$-, $y$-, and $z$-axes. $\theta_t, \phi_t$, and $\psi_t$ are the roll, pitch, and yaw angles, representing the drone's orientation around the $x$-, $y$-, and $z$-axes. $z_t$ is the current height of the drone.

To ensure consistent scaling and enhance the stability of the RL agent’s policy updates, each observation variable in $s_t$ is normalized using Z-score normalization:
% \[
% s_t = \frac{s_t - \mu}{\sigma},
% \]
\begin{equation}
s_t = \frac{s_t - \mu}{\sigma},
\end{equation}

where $\mu$ is the mean of each state variable and $\sigma$ is the standard deviation of each state variable.

The RL agent controls the multirotor using three actions: roll ($\phi_a$), pitch ($\theta_a$), and thrust ($T$). Yaw control is omitted in this configuration. Excluding yaw control reduces the complexity of the problem and allows the RL agent to stabilize and control the roll and pitch axes effectively. These actions are normalized to ensure compatibility with neural network policies and facilitate robust learning. The rotational dynamics, including attitude control, are managed by a separate algorithm using the MATLAB UAV Toolbox multirotor model (version 2024a), which handles the attitude rate control and converts the control inputs to PWM commands for stability.

The reward function $r_p$ is designed to promote stable, precise navigation by rewarding proximity to the target position and discouraging positional deviations. Inspired by established reward-shaping techniques for positional control in robotics \cite{c21}, this convex reward function encourages the drone to progressively minimize its distance from the target. The reward function is defined as:
% \[
% r_p = w_p \cdot (1 \cdot \exp(-\|p - p_{\text{desired}}\|^2)),
% \]
\begin{equation}
r_p = w_p \cdot  \exp\left(-\|p - p_{\text{desired}}\|^2\right),
\end{equation}

where $p$ represents the current position vector of the drone, $p_{\text{desired}}$ denotes the target position vector, $\|p - p_{\text{desired}}\|^2$ is the $L_2$ norm (Euclidean distance) between the current and target positions, and $w_p$ is a scaling factor to adjust the reward magnitude. By applying a negative exponential to the Euclidean distance, the function generates a reward that decreases smoothly as the drone moves away from the target. Experimental results and prior literature indicate that exponential reward functions can yield smoother, more stable trajectories in drone navigation tasks \cite{c21}, as shown in Fig. \ref{fig:reward-func}, which visualizes the convex reward distribution. In Fig. \ref{fig:reward-func}, the visualization of the reward function is presented for positional deviations of the drone along the X and Y axes, while keeping the Z-axis (height) constant. In this figure, the X and Y axes represent the drone's position in the horizontal plane, and the Z-axis illustrates the corresponding reward value that decays as the distance increases.


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{reward_func_plot.png} % Renamed the file to avoid special characters
    \caption{3D Scatter Plot of Reward Function}
    \label{fig:reward-func}
\end{figure}

The transition function \( P(s_{t+1} \mid s_t, a_t) \) delineates how the drone transitions from its current state \( s_t \) to the next state \( s_{t+1} \) upon executing action \( a_t \) within the RL framework. In this study, the transition dynamics are modeled deterministically using MATLAB Simulink’s UAV Toolbox (version 2024a), which simulates the multi-rotor's physical behavior based on applied control inputs. The high-level actions are first scaled to match the UAV’s operational ranges and then translated into low-level Pulse-Width Modulation (PWM) \cite{c22} signals via a state estimator. The PWM commands directly influence the drone’s actuators as governed by the UAV’s dynamic model within Simulink. Consequently, the next state \( s_{t+1} \) is deterministically determined by the interplay between the current state, the scaled actions, and the inherent physical laws modeled in the simulation environment. 

The episode terminates when the drone reaches the target position with a velocity less than $0.1 m/s$ (near-zero velocity). Conversely, termination occurs under failure scenarios if the drone crashes by touching the ground (defined as a height less than 0.1 meters) or if it deviates more than 10 meters from the target position.

Two state-of-the-art off-policy DRL algorithms DDPG and TD3 are evaluated for their effectiveness in handling UAV navigation. Each algorithm is tested in simplified environments to determine baseline performance in waypoint navigation. For more information on the DDPG and TD3 algorithms, see \cite{c23, c24}.

\subsection{Curriculum Learning for Complexity Management}

To improve the robustness and adaptability of DRL agents, curriculum learning is employed to progressively increase task complexity. The task difficulty at time step $t$, represented as $C(t)$, transitions through predefined levels $C_1, C_2, \dots, C_n$, allowing the agent to build foundational skills before addressing more advanced challenges.

\textbf{Stages of curriculum learning include:}

\begin{itemize}
    \item \textbf{Basic Navigation ($C_1$):} The agent trains with static waypoints in a controlled environment, optimizing a reward function:
    % \[
    % R(s,a) = -\|p_{\text{target}} - p_{\text{current}}\|_2
    % \]
    \begin{equation}
    R(s, a) = -\|p_{\text{target}} - p_{\text{current}}\|_2,
    \end{equation}
    where \( p_{\text{target}} \) and \( p_{\text{current}} \) denote the target and current positions, respectively. % This formulation provides higher rewards for proximity to the target, ensuring smooth learning dynamics and efficient navigation. 
    \item \textbf{Dynamic Waypoints ($C_2$):} Randomized start and target positions simulate adaptive path planning, with an added reward for waypoint completion.

    \item \textbf{Mass Variability ($C_3$):} Variable drone mass emulates material deposition, testing the agent's adaptation to center-of-gravity shifts.

    \item \textbf{External Disturbances ($C_4$):} Wind and noise test resilience, with penalties for deviations due to disturbances.
\end{itemize}

\section{Experiments}

This paper employs MATLAB’s Simulink platform (version 2024a), along with the RL Toolbox and UAV Toolbox, to develop a simulation environment for a drone engaged in AM. The Simulink setup shown in Fig \ref{fig:simulink-model} includes essential blocks such as the RL agent, observation, reward, and termination function, designed to enable real-time adaptive learning for stable control.

The Simulink environment functions in a continuous loop. At each step, the RL agent processes the latest observations to generate control actions. These actions are scaled, with roll and pitch constrained between \(-\frac{\pi}{2}\) and \(\frac{\pi}{2}\), and thrust limited between \(0\) and \(10\). The scaled actions are then processed through the state estimator and translated into commands for the UAV model. The drone’s updated state is integrated to produce the next observation, guiding the agent’s decisions in the subsequent step.  


\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{simulink_diagram.png}
    \caption{Simulink Model}
    \label{fig:simulink-model}
\end{figure}

During the simulation, a dataset comprising state observations, actions, rewards, the subsequent state, and an episode termination flag is collected. This dataset forms the foundation for the RL agent’s learning process, enabling effective training through experience replay. Training is conducted using experience replay \cite{c25}, where the agent samples mini-batches from this dataset to update its policy and value function parameters. This approach mitigates the effects of temporal correlation between states, facilitating more stable learning. Experience replay also enhances training efficiency, allowing the agent to leverage past interactions to develop a robust policy capable of adaptive control in non-linear environments.

Table \ref{tab:hyperparameters} summarizes the hyperparameter settings for the DDPG and TD3 agents.

\begin{table}[t]
\caption{HYPERPARAMETER SETTINGS FOR DDPG AND TD3}
\centering
\renewcommand{\arraystretch}{1.4} % Adjust row separation for readability
\begin{tabular}{|p{4.0cm}|p{1.7cm}|p{1.7cm}|}
\hline
\textbf{Hyperparameter}               & \textbf{DDPG} & \textbf{TD3} \\
\hline
Optimizer                              & Adam          & Adam          \\
Learning Rate                          & $1 \times 10^{-3}$ & $1 \times 10^{-3}$ \\
Gradient Threshold                    & 1             & 1             \\
Sample Time                            & 0.01          & 0.01          \\
Discount Factor                        & 0.99          & 0.99          \\
Mini Batch Size                        & 256           & 256           \\
Experience Buffer Length               & $1 \times 10^{6}$ & $1 \times 10^{6}$ \\
Target Smooth Factor                   & $5 \times 10^{-3}$ & $5 \times 10^{-3}$ \\
Number of Epochs                       & 3             & 3             \\
Max Mini Batch per Epoch               & 100           & 100           \\
Learning Frequency                     & $-1$ (batch update) & $-1$ (batch update) \\
Policy Update Frequency                & 1             & 1             \\
Target Update Frequency                & 1             & 1             \\
Exploration Model                      & Gaussian Noise & Ornstein-Uhlenbeck Noise \\
\hline
\multicolumn{3}{|l|}{\textbf{Noise Parameters:}} \\
\hline
Mean Attraction Constant               & 1             & 1             \\
Noise Standard Deviation               & 0.1           & 0.1           \\
\hline
\multicolumn{3}{|l|}{\textbf{Target Policy Smooth Model:}} \\
\hline
Std. Dev. Min                            & -             & 0.05          \\
Std. Dev.                                & -             & 0.05          \\
Model Limits                           & -             & [$-0.5$, 0.5] \\
\hline
\end{tabular}
\label{tab:hyperparameters}
\end{table}

The neural network architectures of TD3 and DDPG share many similarities but are structured to align with their unique algorithmic needs. Both algorithms use an actor-critic framework with two fully connected layers in the actor network, consisting of 400 and 300 neurons. The primary distinction lies in the critic network. TD3 employs dual-critic networks, each with 400 and 300 neurons, to mitigate overestimation bias, while DDPG uses a single-critic network with the same structure.

For activation functions, both TD3 and DDPG utilize ReLU in the hidden layers and Tanh for the actor network's output to ensure smooth and bounded continuous action control.

In addition, each critic network is initialized using a weight scaling strategy \cite{datta2020survey}, where weights are sampled from a uniform distribution within the range 
% \[
% \left[ -\frac{2}{\sqrt{\text{fan\_in}}}, \frac{2}{\sqrt{\text{fan\_in}}} \right].
% \]
\begin{equation}
\left[ -\frac{2}{\sqrt{\text{fan\_in}}}, \frac{2}{\sqrt{\text{fan\_in}}} \right].
\end{equation}

Here, \(\text{fan\_in}\) refers to the number of input units to the layer. This initialization ensures stable variance of activations across layers, preventing issues such as gradient vanishing or explosion during training. Actor networks are initialized using the same scaling method to maintain consistent training dynamics and facilitate efficient gradient flow.

% \begin{figure}
%     \centering
%     \includegraphics[width=1.0\linewidth]{critic_network.png}
%     \caption{Critic Network}
%     \label{fig:critic-network}
% \end{figure}

% The actor network for both TD3 and DDPG agents transforms the state input into action outputs that define the agent’s policy. This network processes state inputs through two fully connected layers of 400 and 300 neurons, respectively, each followed by ReLU activation. The final layer employs a Tanh activation function to ensure the actions remain bounded within specified limits. Figure \ref{fig:actor} highlights the actor network’s flow from state inputs through hidden layers to the bounded action outputs.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.95\linewidth]{actornet.png}
%     \caption{Actor Network for DDPG and TD3 Agents}
%     \label{fig:actor}
% \end{figure}

% Table \ref{tab:network_comparison} compares the network structures and activation functions of TD3 and DDPG agents.
% \begin{table*}[t]
%     \centering
%     \footnotesize % Adjust font size to match the rest of the paper
%     \setlength{\tabcolsep}{10pt} % Adjust column separation for compactness
%     \renewcommand{\arraystretch}{1.4} % Adjust row separation for readability
%     \caption{Comparison of Network Structures and Activation Functions for TD3 and DDPG}
%     \label{tab:network_comparison}
%     \begin{tabular}{|l|c|c|}
%     \hline
%     \textbf{Feature} & \textbf{TD3} & \textbf{DDPG} \\
%     \hline
%     \textbf{Actor Network Structure} & Two fully connected layers with 400 and 300 neurons & Two fully connected layers with 400 and 300 neurons \\
%     \hline
%     \textbf{Critic Network Structure} & Dual-critic networks, each with 400 and 300 neurons & Single-critic network, with 400 and 300 neurons \\
%     \hline
%     \textbf{Activation Functions} & ReLU for hidden layers; Tanh for action output & ReLU for hidden layers; Tanh for action output \\
%     \hline
%     \end{tabular}
% \end{table*}


% \subsection{Training Methodology}

% Training is conducted using the experience replay mechanism, where experiences \( (\mathbf{s}_t, \mathbf{a}_t, r_t, \mathbf{s}_{t+1}) \) are stored in a replay buffer. The agent samples mini-batches of size 256 from this buffer to perform gradient updates on both the actor and critic networks. This decoupling of data sampling from data generation mitigates the effects of temporal correlations and promotes more stable and efficient learning.

% Curriculum Learning is employed to further enhance the training process. The complexity of tasks is gradually increased, allowing the agent to build foundational skills before tackling more challenging control scenarios. This structured progression accelerates convergence and reduces training variance, enabling the agent to develop robust policies capable of adaptive control in dynamic and non-linear environments.

% \subsection{Neural Network Initialization}

% Each critic network is initialized using a fan-in weight scaling strategy, which initializes weights by sampling from a uniform distribution scaled by \( \frac{2}{\sqrt{\text{fan-in}}} \). This initialization technique, applied independently for each layer, helps prevent gradient vanishing or explosion, maintaining balanced gradient flow and promoting more efficient training across all critic layers. The actor networks for TD3 and DDPG are similarly initialized to ensure consistent training dynamics.

% \subsection{Shared Enhancements}

% Both TD3 and DDPG incorporate several shared enhancements to support robust learning:

% \begin{itemize}
%     \item \textbf{Dual-Critic Min Q for Actor Updates:} Utilizes the minimum Q-value from the two critic networks to guide actor updates, mitigating overestimation biases.
%     \item \textbf{Clipped Noise for Target Policy Smoothing:} Introduces noise to target policy actions to smooth Q-function estimates and prevent spurious peaks.
%     \item \textbf{Delayed Actor Network Updates:} Updates the actor network less frequently than the critic networks to allow critics to stabilize, enhancing policy stability.

% \end{itemize}

% These architectural and methodological choices underpin the agents' ability to perform reliable and efficient drone navigation and control in additive manufacturing applications.

\section{Results}

We conducted a comparative analysis of two RL algorithms, TD3 and DDPG, by training each agent on a target-reaching task. The objective was to navigate a drone to a specified target position. Evaluation metrics included average cumulative reward, standard deviation of rewards, average positional error measured in meters, precision (standard deviation of positional error), and success ratio across 100 test trials.

In the test phase, the drone always started from a fixed, stationary position, and the waypoints it needed to navigate to changed with each trial. In contrast, during training, both the drone's starting position and the waypoints were initialized randomly to expose the agent to a more diverse set of conditions. This approach helped the agent to become more robust by training in a variety of environmental configurations, ensuring its generalization ability across different scenarios.

\subsection{Performance on Target-Reaching Task}

The performance of TD3 and DDPG was analyzed using their training progress, as shown in Fig. \ref{fig:test_td3_ddpg}, where Agent 1 represents TD3 and Agent 2 represents DDPG. The TD3 agent outperformed DDPG, achieving a higher average cumulative reward, lower variance, and greater consistency in learning trends. A summary of the results over 100 tests is presented in Table~\ref{tab:target_reaching}.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{td3_ddpg_result_compare_new.png}
    \caption{Training Comparison of TD3 and DDPG}
    \label{fig:test_td3_ddpg}
\end{figure}

\begin{table}[h!]
    \centering
    \renewcommand{\arraystretch}{1.4} % Adjust row separation for readability
    \caption{TEST RESULTS OF TD3 AND DDPG AGENTS}
    \label{tab:target_reaching}
    %\resizebox{\columnwidth}{!}{%
    \begin{tabular}{|p{1.0cm}|p{1.0cm}|p{1.0cm}|p{1.cm}|p{1cm}|p{1.0cm}|}
        \hline
        \textbf{Agent} & \textbf{Avg. Cum. Reward} & \textbf{Std. Dev.} & \textbf{Avg. Pos. Error} & \textbf{Precision} & \textbf{Success (\%)} \\ \hline
        TD3 & 703 & 133.213 & $1.804 \times 10^{-2}$ & $2.26 \times 10^{-4}$ & 96 \\ \hline
        DDPG & 604 & 140.603 & $5.643 \times 10^{-2}$ & $6.412 \times 10^{-3}$ & 82 \\ \hline
    \end{tabular}%
   % }
\end{table}

TD3 achieved an average positional error of $0.01804m$, significantly lower than DDPG’s $0.05643m$, with higher precision, confirming its superior navigation performance. These results demonstrate TD3’s ability for precise and consistent target-reaching, making it the optimal agent for the subsequent waypoint navigation task.

\subsection{Waypoint Navigation Task}

To evaluate performance in more complex scenarios, the TD3 agent was tested on a waypoint navigation task involving multiple sequential targets. Without curriculum learning, TD3 achieved limited success, with lower cumulative rewards and success ratios. However, introducing curriculum learning, which progressively increases task complexity, significantly improved the agent’s performance.

As shown in Table \ref{tab:waypoint_navigation}, the curriculum-trained TD3 agent outperformed the non-curriculum-trained agent, achieving a higher average cumulative reward (\(729\) vs. \(593\)) and a greater success ratio (\(87\%\) vs. \(66\%\)). Additionally, the curriculum-trained agent exhibited a lower average positional error (\(0.01581m\)) compared to the non-curriculum-trained agent (\(0.01994m\)).

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.4} % Adjust row separation for readability
    \caption{TRAINING RESULTS OF TD3 AGENT WITH AND WITHOUT CURRICULUM LEARNING}
    \label{tab:waypoint_navigation}
    %\resizebox{\columnwidth}{!}{%
    \begin{tabular}{|p{2.8cm}|p{1.4cm}|p{1.2cm}|p{1cm}|}
        \hline
        \textbf{Agent} & \textbf{Avg. Cum. Reward} & \textbf{Std. Dev.} & \textbf{Success (\%)} \\ \hline
        TD3 (No Curriculum) & 593 & 145.9763 & 66 \\ \hline
        TD3 (With Curriculum) & 729 & 148.5167 & 87 \\ \hline
    \end{tabular}%
    %}
\end{table}

The KDE (Kernel Density Estimate) plots of positional errors (Fig. \ref{fig:test-curr}) illustrate these improvements. The curriculum-trained agent’s error distribution is tightly clustered near zero, while the non-curriculum-trained agent exhibits a broader error spread, indicating greater variability. 
%These findings demonstrate that curriculum learning improves both precision and consistency in waypoint navigation.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{kde.png}
    \caption{Test Results of TD3 Agent Trained with Different Methods}
    \label{fig:test-curr}
\end{figure}

\subsection{Adaptation to Dynamic Mass Changes}

To address challenges arising from dynamic mass changes, such as those encountered during AM, the TD3 agent was adapted to account for variations in drone mass. Initially, the curriculum-trained agent achieved an average cumulative reward of $450$ and a success ratio of only $26\%$ under such conditions.

To enhance adaptability, the observation space was expanded to include the drone’s accelerations in the $x$, $y$ and $z$ directions. This modification allowed the agent to predict thrust commands corresponding to changes in mass, leveraging the relationship between thrust, mass and acceleration.

Retraining the agent with the expanded observation space led to significant improvements. The average cumulative reward increased from $454$ to $760$, and the success ratio rose from $29\%$ to $94\%$. Additionally, the average positional error was reduced from $0.2688 m$ to $0.09857 m$, and the precision (standard deviation of positional error), improved from $0.1459$ to $0.0626$.

Fig. \ref{fig:test-dynamic-mass} presents a histogram of Euclidean distances between the drone’s final and target positions, fitted with a gamma distribution, while Fig. \ref{fig:trajectory} illustrates the path of the drone as it successfully navigates six randomaly generated waypoints under varying mass conditions.

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{gamma.png}
    \caption{Test Results with Acceleration in Observation Space of TD3 Agent}
    \label{fig:test-dynamic-mass}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{trajectory.png}
    \caption{Final Demonstration of Drone Waypoint Navigation}
    \label{fig:trajectory}
\end{figure}



\subsection{Summary of Agent Performance}

Table~\ref{tab:agent_summary} summarizes the training and testing results for all agents, highlighting their performance across various tasks. The TD3 agent demonstrated clear superiority in baseline tasks and in complex scenarios involving curriculum learning and dynamic mass adaptation. This progression underscores TD3’s robustness for real-world applications, such as waypoint navigation in environments with operational complexities.

 \begin{table*}[h!]
    \centering
    \renewcommand{\arraystretch}{1.4} % Adjust row separation for readability
    \caption{PERFORMANCE METRICS FOR DIFFERENT AGENTS ACROSS VARIOUS TRAINING ENVIRONMENTS}
    \label{tab:agent_summary}
    \begin{tabular}{|p{2.8cm}|p{2.2cm}|p{2.2cm}|p{1.4cm}|p{1cm}|p{1.4cm}|p{1.6cm}|p{1cm}|}
    \hline
    \textbf{Agent} & \textbf{Task Agent Trained On} & \textbf{Complexity of Environment} & \textbf{Avg. Cum. Reward} & \textbf{Std. Dev. of Reward} & \textbf{Avg. Pos. Error (in meter)} & \textbf{Precision (Std. Dev. of Pos. Error)} & \textbf{Success Ratio (\%)} \\ \hline
    DDPG & Single Waypoint Navigation & No Noise Added; No Variable Mass & 604 & 140.6029 & $5.643 \times 10^{-2}$ & $6.412 \times 10^{-3}$ & 82 \\ \hline
    TD3 & Single Waypoint Navigation & No Noise Added; No Variable Mass & 703 & 133.2130 & $1.804 \times 10^{-2}$ & $2.26 \times 10^{-4}$ & 96 \\ \hline\hline
    TD3 (No Curriculum Learning) & Multiple Waypoint Navigation & Noise Added; No Variable Mass & 593 & 145.9763 & $1.994 \times 10^{-2}$ & $2.780 \times 10^{-3}$ & 66 \\ \hline
    TD3 (With Curriculum Learning) & Multiple Waypoint Navigation & Noise Added; No Variable Mass & 729 & 148.5167 & $1.581 \times 10^{-2}$ & $6.831 \times 10^{-3}$ & 87 \\ \hline\hline
    TD3 (With Acceleration in Observation Space) & Multiple Waypoint Navigation & Noise Added; Variable Mass & 760 & 152.5167 & $9.857 \times 10^{-2}$ & $6.260 \times 10^{-2}$ & 94 \\ \hline
    \end{tabular}
\end{table*}

% \footnotesize{DDPG and TD3 were first evaluated on single waypoint navigation without noise or variable mass, where TD3 outperformed DDPG. TD3 was then used for multiple waypoint navigation, first without and then with curriculum learning to assess its impact. Finally, acceleration was added to the observation space, and TD3 was evaluated in a more complex environment with noise and variable mass.}\\

\section{Conclusion}

This study evaluated the performance of two RL algorithms, DDPG and TD3, for UAV control in additive manufacturing tasks. Results showed that TD3 outperformed DDPG in key metrics, including cumulative reward, average positional error, precision, and success ratio, confirming its suitability for precise and consistent target-reaching tasks. Given these results, TD3 was selected for further evaluation in more complex tasks, as it showed better performance compared to DDPG in the initial tests.

The integration of curriculum learning further enhanced TD3’s performance, enabling the agent to handle complex waypoint navigation scenarios. Curriculum-trained TD3 achieved higher rewards, greater success ratios, and improved accuracy compared to its non-curriculum counterpart. Additionally, adapting the observation space to include acceleration data allowed TD3 to handle dynamic mass variations effectively, achieving a cumulative reward of 760 and a success ratio of 94\%.

Overall, TD3 demonstrated superior adaptability and robustness across all tested scenarios, including dynamic environments with noise and mass changes. These findings highlight its potential for real-world AM applications, providing a robust framework for UAV waypoint navigation under operational complexities. Future work will focus on real-world implementation, and expanding the agent’s capabilities for diverse AM scenarios.

% \subsection{Abbreviations and Acronyms} Define abbreviations and acronyms the first time they are used in the text, even after they have been defined in the abstract. Abbreviations such as IEEE, SI, MKS, CGS, sc, dc, and rms do not have to be defined. Do not use abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}

% \begin{itemize}

% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as Ò3.5-inch disk driveÓ.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ÒWb/m2Ó or Òwebers per square meterÓ, not Òwebers/m2Ó.  Spell out units when they appear in text: Ò. . . a few henriesÓ, not Ò. . . a few HÓ.
% \item Use a zero before decimal points: Ò0.25Ó, not Ò.25Ó. Use Òcm3Ó, not ÒccÓ. (bullet list)

% \end{itemize}


% \subsection{Equations}

% The equations are an exception to the prescribed specifications of this template. You will need to determine whether or not your equation should be typed using either the Times New Roman or the Symbol font (please no other font). To create multileveled equations, it may be necessary to treat the equation as a graphic and insert it into the text after your paper is styled. Number equations consecutively. Equation numbers, within parentheses, are to position flush right, as in (1), using a right tab stop. To make your equations more compact, you may use the solidus ( / ), the exp function, or appropriate exponents. Italicize Roman symbols for quantities and variables, but not Greek symbols. Use a long dash rather than a hyphen for a minus sign. Punctuate equations with commas or periods when they are part of a sentence, as in

% $$
% \alpha + \beta = \chi \eqno{(1)}
% $$

% Note that the equation is centered using a center tab stop. Be sure that the symbols in your equation have been defined before or immediately following the equation. Use Ò(1)Ó, not ÒEq. (1)Ó or Òequation (1)Ó, except at the beginning of a sentence: ÒEquation (1) is . . .Ó

% \subsection{Some Common Mistakes}
% \begin{itemize}


% \item The word ÒdataÓ is plural, not singular.
% \item The subscript for the permeability of vacuum ?0, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ÒoÓ.
% \item In American English, commas, semi-/colons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ÒinsetÓ, not an ÒinsertÓ. The word alternatively is preferred to the word ÒalternatelyÓ (unless you really mean something that alternates).
% \item Do not use the word ÒessentiallyÓ to mean ÒapproximatelyÓ or ÒeffectivelyÓ.
% \item In your paper title, if the words Òthat usesÓ can accurately replace the word ÒusingÓ, capitalize the ÒuÓ; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ÒaffectÓ and ÒeffectÓ, ÒcomplementÓ and ÒcomplimentÓ, ÒdiscreetÓ and ÒdiscreteÓ, ÒprincipalÓ and ÒprincipleÓ.
% \item Do not confuse ÒimplyÓ and ÒinferÓ.
% \item The prefix ÒnonÓ is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ÒetÓ in the Latin abbreviation Òet al.Ó.
% \item The abbreviation Òi.e.Ó means Òthat isÓ, and the abbreviation Òe.g.Ó means Òfor exampleÓ.

% \end{itemize}


% \section{USING THE TEMPLATE}

% Use this sample document as your LaTeX source file to create your document. Save this file as {\bf root.tex}. You have to make sure to use the cls file that came with this distribution. If you use a different style file, you cannot expect to get required margins. Note also that when you are creating your out PDF file, the source file is only part of the equation. {\it Your \TeX\ $\rightarrow$ PDF filter determines the output file size. Even if you make all the specifications to output a letter file in the source - if your filter is set to produce A4, you will only get A4 output. }

% It is impossible to account for all possible situation, one would encounter using \TeX. If you are using multiple \TeX\ files you must make sure that the ``MAIN`` source file is called root.tex - this is particularly important if your conference is using PaperPlaza's built in \TeX\ to PDF conversion tool.

% \subsection{Headings, etc}

% Text heads organize the topics on a relational, hierarchical basis. For example, the paper title is the primary text head because all subsequent material relates and elaborates on this one topic. If there are two or more sub-topics, the next level head (uppercase Roman numerals) should be used and, conversely, if there are not at least two sub-topics, then no subheads should be introduced. Styles named ÒHeading 1Ó, ÒHeading 2Ó, ÒHeading 3Ó, and ÒHeading 4Ó are prescribed.

% \subsection{Figures and Tables}

% Positioning Figures and Tables: Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Large figures and tables may span across both columns. Figure captions should be below the figures; table heads should appear above the tables. Insert figures and tables after they are cited in the text. Use the abbreviation ÒFig. 1Ó, even at the beginning of a sentence.

% \begin{table}[h]
% \caption{An Example of a Table}
% \label{table_example}
% \begin{center}
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}


%    \begin{figure}[thpb]
%       \centering
%       \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
% }}
%       %\includegraphics[scale=1.0]{figurefile}
%       \caption{Inductance of oscillation winding on amorphous
%        magnetic core versus DC bias magnetic field}
%       \label{figurelabel}
%    \end{figure}
   

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words rather than symbols or abbreviations when writing Figure axis labels to avoid confusing the reader. As an example, write the quantity ÒMagnetizationÓ, or ÒMagnetization, MÓ, not just ÒMÓ. If including units in the label, present them within parentheses. Do not label axes only with units. In the example, write ÒMagnetization (A/m)Ó or ÒMagnetization {A[m(1)]}Ó, not just ÒA/mÓ. Do not label axes with a ratio of quantities and units. For example, write ÒTemperature (K)Ó, not ÒTemperature/K.Ó

% \section{CONCLUSIONS}

% A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions. 

% \addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
%                                   % on the last page of the document manually. It shortens
%                                   % the textheight of the last page by a suitable amount.
%                                   % This command does not take effect until the next page
%                                   % so it should come on the page before the last. Make
%                                   % sure that you do not shorten the textheight too much.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.



\begin{thebibliography}{99}

% \bibitem{c1}
% D. C. Tsouros, S. Bibi, and P. G. Sarigiannidis, "A review on UAV-based applications for precision agriculture," 
% \textit{Information}, vol. 10, no. 11, article 349, 2019. [Online]. Available: \url{https://www.mdpi.com/2078-2489/10/11/349}. 
% DOI: 10.3390/info10110349.

% \bibitem{c2}
% Y. Li, M. Liu, and D. Jiang, “Application of unmanned aerial vehicles in logistics: A literature review,” \textit{Sustainability}, vol. 14, no. 21, p. 14473, Nov. 2022. [Online]. Available: \url{https://doi.org/10.3390/su142114473}.

% \bibitem{c3}
% J. Park, Y. Zhang, H. S. Lee, and S.-H. Choi, “Deep reinforcement learning for autonomous UAVs in construction: A review,” 
% \textit{Automation in Construction}, vol. 133, p. 104061, 2022. [Online]. Available: \url{https://doi.org/10.1016/j.autcon.2022.104061}.

% \bibitem{c4}
% P. Gonzalez-deSantos \textit{et al.}, “Challenges of using unmanned aerial vehicles (UAVs) in additive manufacturing: A review,” 
% \textit{Robotics and Autonomous Systems}, vol. 128, p. 103535, 2020. [Online]. Available: \url{https://doi.org/10.1016/j.robot.2020.103535}.

% \bibitem{c5}
% M. Maaruf, M. S. Mahmoud, and A. Ma'arif, "A survey of control methods for quadrotor UAV," \textit{Int. J. Robotics Control Syst.}, vol. 2, no. 4, pp. 652–665, 2022. [Online]. Available: \url{https://doi.org/10.31763/ijrcs.v2i4.743}.

% \bibitem{c6}
% M. Rinaldi, S. Primatesta, and G. Guglieri, “A comparative study for control of quadrotor UAVs,” \textit{Applied Sciences}, vol. 13, no. 6, p. 3464, 2023. [Online]. Available: \url{https://doi.org/10.3390/app13063464}.

% \bibitem{c7}
% “Reinforcement learning based quadcopter controller,” \textit{Stanford University}, 2019. [Online]. Available: \url{https://web.stanford.edu/class/aa228/reports/2019/final62.pdf}.

% \bibitem{c8}
% Z. Jiang and A. F. Lynch, “Quadrotor motion control using deep reinforcement learning,” \textit{Journal of Unmanned Vehicle Systems}, vol. 9, no. 4, pp. 234–251, 2021. [Online]. Available: \url{https://doi.org/10.1139/juvs-2021-0010}.

% \bibitem{c9}
% G. Matheron, L. Denoyer, and T. Serre, “The problem with DDPG: Understanding failures in deterministic environments,” \textit{arXiv preprint arXiv:1911.11679}, Nov. 2019. [Online]. Available: \url{https://arxiv.org/pdf/1911.11679}.

% \bibitem{c10}
% S. Fujimoto, H. van Hoof, and D. Meger, “Addressing function approximation error in actor-critic methods,” \textit{arXiv preprint arXiv:1802.09477}, Feb. 2018. [Online]. Available: \url{https://arxiv.org/pdf/1802.09477}.

% \bibitem{c11}
% S. Narvekar, P. S. Morgan, and P. Stone, “Curriculum learning for reinforcement learning domains: A framework and survey,” \textit{arXiv preprint arXiv:2003.04960}, Mar. 2020. [Online]. Available: \url{https://arxiv.org/abs/2003.04960}.

% \bibitem{Sabatino2015}
% F. Sabatino, \textit{Quadrotor Control: Modeling, Nonlinear Control Design, and Simulation}, Ph.D. dissertation, KTH Royal Institute of Technology, 2015.

% % \bibitem{c12} R. W. Lucky, ÒAutomatic equalization for digital communication,Ó Bell Syst. Tech. J., vol. 44, no. 4, pp. 547Ð588, Apr. 1965.
% % \bibitem{c13} S. P. Bingulac, ÒOn the compatibility of adaptive controllers (Published Conference Proceedings style),Ó in Proc. 4th Annu. Allerton Conf. Circuits and Systems Theory, New York, 1994, pp. 8Ð16.
% % \bibitem{c14} G. R. Faulhaber, ÒDesign of service systems with priority reservation,Ó in Conf. Rec. 1995 IEEE Int. Conf. Communications, pp. 3Ð8.
% % \bibitem{c15} W. D. Doyle, ÒMagnetization reversal in films with biaxial anisotropy,Ó in 1987 Proc. INTERMAG Conf., pp. 2.2-1Ð2.2-6.
% % \bibitem{c16} G. W. Juette and L. E. Zeffanella, ÒRadio noise currents n short sections on bundle conductors (Presented Conference Paper style),Ó presented at the IEEE Summer power Meeting, Dallas, TX, June 22Ð27, 1990, Paper 90 SM 690-0 PWRS.
% % \bibitem{c17} J. G. Kreifeldt, ÒAn analysis of surface-detected EMG as an amplitude-modulated noise,Ó presented at the 1989 Int. Conf. Medicine and Biological Engineering, Chicago, IL.
% % \bibitem{c18} J. Williams, ÒNarrow-band analyzer (Thesis or Dissertation style),Ó Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, 1993. 
% % \bibitem{c19} N. Kawasaki, ÒParametric study of thermal and chemical nonequilibrium nozzle flow,Ó M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
% % \bibitem{c20} J. P. Wilkinson, ÒNonlinear resonant circuit devices (Patent style),Ó U.S. Patent 3 624 12, July 16, 1990. 

% \bibitem{Bellman1957}
% R. Bellman, “A Markovian decision process,” \textit{Journal of Mathematics and Mechanics}, vol. 6, no. 5, pp. 679–684, 1957. [Online]. Available: \url{https://www.jstor.org/stable/24900506}.

% \bibitem{zahavy2021reward}
% T. Zahavy, D. Hren, and S. Mannor, "Reward is Enough for Convex MDPs," in \textit{arXiv preprint arXiv:2106.00661}, 2021. [Online]. Available: \url{https://arxiv.org/abs/2106.00661}.

% \bibitem{ddpg2015}
% V. Mnih, H. Blundell, D. Wierstra, T. P. Lillicrap, D. Silver, and D. Hassabis, "Continuous control with deep reinforcement learning," in \textit{Proc. of the 4th International Conference on Learning Representations (ICLR)}, 2016. [Online]. Available: \url{https://arxiv.org/abs/1509.02971}.


\bibitem{c1}
S. A. H. Mohsan, M. A. Khan, F. Noor, I. Ullah, and M. H. Alsharif, "Towards the unmanned aerial vehicles (UAVs): A comprehensive review," \textit{Drones}, vol. 6, no. 6, p. 147, 2022.

\bibitem{c2}
K. Aghaee, L. Li, A. Roshan, and P. Namakiaraghi, "Additive manufacturing evolution in construction: from individual terrestrial to collective, aerial, and extraterrestrial applications," \textit{Journal of Building Engineering}, p. 110389, 2024.

\bibitem{c3}
M.-N. Stamatopoulos, A. Banerjee, and G. Nikolakopoulos, "Collaborative Aerial 3D Printing: Leveraging UAV Flexibility and Mesh Decomposition for Aerial Swarm-Based Construction," in \textit{2024 International Conference on Unmanned Aircraft Systems (ICUAS)}, 2024: IEEE, pp. 45-52.






\bibitem{c4}
S. A. H. Mohsan, N. Q. H. Othman, Y. Li, M. H. Alsharif, and M. A. Khan, "Unmanned aerial vehicles (UAVs): Practical aspects, applications, open challenges, security issues, and future trends," \textit{Intelligent Service Robotics}, vol. 16, no. 1, pp. 109-137, 2023.


\bibitem{c5}
P. Chermprayong, "Enabling Technologies for Precise Aerial Manufacturing with Unmanned Aerial Vehicles," Imperial College London, 2019.


\bibitem{c27}
M. A. Alandihallaj, M. Ramezani, and A. M. Hein, "MBSE-Enhanced LSTM Framework for Satellite System Reliability and Failure Prediction," in \textit{Proceedings of the 12th International Conference on Model-Based Software and Systems Engineering (MODELSWARD)}, 2024, pp. 349–356. [Online]. Available: \url{https://www.scitepress.org/Papers/2024/126076/126076.pdf}

\bibitem{c6}
L. G. Kraft and D. P. Campagna, "A comparison between CMAC neural network control and two traditional adaptive control systems," \textit{IEEE Control Systems Magazine}, vol. 10, no. 3, pp. 36-43, 1990.

\bibitem{c7_new}
M.~A. Alandihallaj, N.~Assadian, and K.~Khorasani, ``Stochastic model predictive control-based countermeasure methodology for satellites against indirect kinetic cyber-attacks,'' \emph{International Journal of Control}, vol.~96, no.~7, pp. 1895--1908, 2023.

\bibitem{c7}
C. Patchett, "On the derivation and analysis of decision architectures for unmanned aircraft systems," 2013.

\bibitem{c9_new}
M.~A.~A. Hallaj and N.~Assadian, ``Sliding mode control of electromagnetic tethered satellite formation,'' \emph{Advances in Space Research}, vol.~58, no.~4, pp. 619--634, 2016.

\bibitem{c8}
M. Ramezani and M. Amiri Atashgah, "Energy-Aware Hierarchical Reinforcement Learning Based on the Predictive Energy Consumption Algorithm for Search and Rescue Aerial Robots in Unknown Environments," \textit{Drones}, vol. 8, no. 7, p. 283, 2024.

\bibitem{c9}
M. Ramezani, M. A. Alandihallaj, and A. M. Hein, "PPO-Based Dynamic Control of Uncertain Floating Platforms in Zero-G Environment," in \textit{2024 IEEE International Conference on Robotics and Automation (ICRA)}, 2024: IEEE, pp. 11730-11736.


\bibitem{c26}
M. Ramezani, M. A. Alandihallaj, and A. M. Hein, "Fuel-Efficient and Fault-Tolerant CubeSat Orbit Correction via Machine Learning-Based Adaptive Control," \textit{Aerospace}, vol. 11, no. 1, p. 1, 2024. [Online]. Available: \url{https://www.mdpi.com/2226-4310/11/1/1}

\bibitem{c10}
M. Ramezani, H. Habibi, J. L. Sanchez-Lopez, and H. Voos, "UAV path planning employing MPC-reinforcement learning method considering collision avoidance," in \textit{2023 International Conference on Unmanned Aircraft Systems (ICUAS)}, 2023: IEEE, pp. 507-514.

\bibitem{c11}
M. Ramezani, M. Amiri Atashgah, and A. Rezaee, "A Fault-Tolerant Multi-Agent Reinforcement Learning Framework for Unmanned Aerial Vehicles–Unmanned Ground Vehicle Coverage Path Planning," \textit{Drones}, vol. 8, no. 10, p. 537, 2024.

\bibitem{c12}
H. Tan, "Reinforcement learning with deep deterministic policy gradient," in \textit{2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)}, 2021: IEEE, pp. 82-85.

\bibitem{c13}
K. Teixeira, G. Miguel, H. S. Silva, and F. Madeiro, "A survey on applications of unmanned aerial vehicles using machine learning," \textit{IEEE Access}, 2023.

\bibitem{c14}
Z. Song et al., "From deterministic to stochastic: an interpretable stochastic model-free reinforcement learning framework for portfolio optimization," \textit{Applied Intelligence}, vol. 53, no. 12, pp. 15188-15203, 2023.

\bibitem{c15}
M.~Ramezani, M.~Atashgah, J.~L.~Sanchez-Lopez, and H.~Voos, ``Human-centric aware UAV trajectory planning in search and rescue missions employing multi-objective reinforcement learning with AHP and similarity-based experience replay,'' in \emph{2024 International Conference on Unmanned Aircraft Systems (ICUAS)}, 2024: IEEE, pp. 177--184.

\bibitem{c16}
Y.~Bai, H.~Zhao, X.~Zhang, Z.~Chang, R.~J{\"a}ntti, and K.~Yang, ``Towards autonomous multi-UAV wireless network: A survey of reinforcement learning-based approaches,'' \emph{IEEE Communications Surveys \& Tutorials}, 2023.

\bibitem{c17}
H.~Kurunathan, H.~Huang, K.~Li, W.~Ni, and E.~Hossain, ``Machine learning-aided operations and communications of unmanned aerial vehicles: A contemporary survey,'' \emph{IEEE Communications Surveys \& Tutorials}, 2023.

\bibitem{c18}
X.~Wang, Y.~Chen, and W.~Zhu, ``A survey on curriculum learning,'' \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol.~44, no.~9, pp. 4555--4576, 2021.

\bibitem{c19}
N.~M.~B.~Lakhal, L.~Adouane, O.~Nasri, and J.~B.~H.~Slama, ``Safe and adaptive autonomous navigation under uncertainty based on sequential waypoints and reachability analysis,'' \emph{Robotics and Autonomous Systems}, vol.~152, p.~104065, 2022.

\bibitem{c20}
M.~L.~Puterman, ``Markov decision processes,'' in \emph{Handbooks in Operations Research and Management Science}, vol.~2, pp. 331--434, 1990.

\bibitem{mellinger2012trajectory}  
Mellinger, Daniel, and Nathan Michael. "Trajectory Generation and Control for Precise Aggressive Maneuvers with Quadrotors." \textit{The International Journal of Robotics Research}, 2012, pp. 664–674.  

\bibitem{zhang2022aerial}  
K. Zhang, P. Chermprayong, F. Xiao, et al., ``Aerial additive manufacturing with multiple autonomous robots,'' *Nature*, vol. 609, pp. 709–717, 2022.  

\bibitem{c21}
M.~Mutti, R.~De~Santi, P.~De~Bartolomeis, and M.~Restelli, ``Challenging common assumptions in convex reinforcement learning,'' \emph{Advances in Neural Information Processing Systems}, vol.~35, pp. 4489--4502, 2022.

\bibitem{c22}
J.~Sun, ``Pulse-width modulation,'' in \emph{Dynamics and Control of Switched Electronic Systems: Advanced Perspectives for Modeling, Simulation and Control of Power Converters}, Springer, 2012, pp. 25--61.

% \bibitem{c23}
% T.~P. Lillicrap, J.~J. Hunt, A.~Godfrey, T.~Qiu, A.~A. Rusu, A.~Tamar, D.~Silver, and D.~Wierstra, ``Continuous control with deep reinforcement learning,'' \textit{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{c23}
Y.~Hou, L.~Liu, Q.~Wei, X.~Xu, and C.~Chen, ``A novel DDPG method with prioritized experience replay,'' in \emph{2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 2017: IEEE, pp. 316--321.

% \bibitem{c24}
% S.~Fujimoto, H.~van Hoof, and D.~Meger, ``Addressing function approximation error in actor-critic methods,'' in \textit{Proceedings of the 35th International Conference on Machine Learning (ICML)}, 2018. [Online]. Available: \url{http://proceedings.mlr.press/v80/fujimoto18a.html}

\bibitem{c24}
Y.~Ye, D.~Qiu, H.~Wang, Y.~Tang, and G.~Strbac, ``Real-time autonomous residential demand response management based on twin delayed deep deterministic policy gradient learning,'' \emph{Energies}, vol.~14, no.~3, p.~531, 2021.

\bibitem{c25}
L.-J. Lin, "Self-improving reactive agents based on reinforcement learning, planning, and teaching," \emph{Machine Learning}, vol. 8, no. 3-4, pp. 293-321, 1992.

\bibitem{datta2020survey}
Leonid Datta, \emph{A Survey on Activation Functions and their relation with Xavier and He Normal Initialization}, arXiv, 2020, \url{https://arxiv.org/abs/2004.06632}, doi: \href{https://doi.org/10.48550/ARXIV.2004.06632}{10.48550/ARXIV.2004.06632}.



\end{thebibliography}




\end{document}
