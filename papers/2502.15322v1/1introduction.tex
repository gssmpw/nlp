\section{Introduction}
% With the increasing popularity of social media platforms (such as Flickr, Instagram, and Twitter), more and more people tend to post images, short texts, and tags online to express their feelings and opinions. This growing variety of data on social platforms has brought a research hotspot to image sentiment analysis.

Image sentiment analysis aims to automatically predict the sentiment polarity expressed in images. It has a wide range of applications in many fields, such as opinion mining~\cite{al2024comprehensive} and recommendation system~\cite{zhan2023analyzing}. 
% With the increasing popularity of social media platforms such as Flickr, Instagram and Twitter, more and more people tend to post images with textual description or tags online to express their feelings and opinions.
With the increasing popularity of social media platforms such as Flickr, Instagram and Twitter, more and more people tend to post images online to express their feelings and opinions of their daily lives. 
% 第一段不用引入metadata！！！
% With the increasing popularity of social media platforms such as Flickr, Instagram, and Twitter, more and more people post images usually attached with textual description or tags to express their feelings and opinions. 
% Metadata associated with images (e.g., image description, tags and scene) may help improve image sentiment understanding, but have not been fully explored. 
% This brings a hot research topic and many new challenges for image sentiment analysis.
Therefore, this task has become a hot topic, and many significant efforts have been made in this field to help analyze image sentiment.

With the successful accomplishments of deep learning and computer vision, most recent studies focused on designing various networks to better extract image features for training sentiment polarity classifiers.
Early researchers~\cite{lu2012shape, zhao2014exploring, katsurai2016image} manually designed hand-crafted image features, such as color, shape, and texture, to explore the sentiment of images.
% liang2021cross(icassp)
More recently, with the rapid popularity of CNNs, some studies~\cite{liang2021cross, yang2018weakly} focused on designing various CNN-based networks to extract deep features from images for improved sentiment analysis. 
Rao et al.~\cite{rao2020learning} and Koromilas et al.~\cite{koromilas2023mmatr}
% Chen et al.~\cite{14} and Peng et al.~\cite{16}
used deep CNN networks for sentiment classification, and demonstrated the superior performance of deep features against hand-crafted features.
Rani et al.~\cite{rani2022efficient} combined CNN and LSTM to better integrate multi-level visual attributes for sentiment classification. 
With the popularity of attention mechanisms and transformer architectures, various neural network modules ~\cite{zhang2022image, 9102855, zhang2023learning, feng2024caption, ruan2021dae, truong2023concept} have been exploited by adaptively learning the attended image features.
% zhu2019joint, zhang2022image, huang2020attention, wu2020visual, kumar2020gated(icassp)
Zhang et al.~\cite{zhang2022image} proposed stacked multi-head self-attention modules to explore the relationship between semantic regions and sentiment labels.
Truong et al.~\cite{truong2023concept} designed a concept-oriented transformer to capture the correlation between image features and specific concepts.
With the exploration of a unified architecture for large models, more recent works (\textit{e.g.}, CLIP~\cite{radford2021learning}, BLIP~\cite{li2022blip}, ImageBind~\cite{girdhar2023imagebind}) have been successively proposed.
Radford et al.~\cite{radford2021learning} utilized contrastive learning to embed images and text into a shared semantic space, enabling the learning of a universal vision-language representation.
Liu et al.~\cite{liu2024visual} utilized instruction-following data to fine-tune an end-to-end large model for general-purpose visual and language understanding.
These methods demonstrate their capacity to maintain comprehensive understanding across diverse data, resulting in remarkable performance in various tasks.

% With the widespread application of Vision-Language Pre-training (VLP) models, some VLP models, like ViLBERT~\cite{lu2019vilbert} and LXMERT~\cite{tan2019lxmert}, improved visual representations for vision-language tasks from large-scale unsupervised datasets.


\begin{figure*}[t]
\centering
\includegraphics[width=.96\textwidth]{fig/fig_framwork6.pdf}
\caption{The overall architecture of~\shortname.}
% \caption{The overall architecture of~\fullname.}
\label{fig:fig2}
\vspace{-6mm}
\end{figure*}
% !!!

% 
Despite the significant progress, metadata, the data~(\textit{e.g.}, text descriptions and keyword tags) for describing the image, has not been fully explored in this task, which is highly helpful for image sentiment understanding.
% For example, when a person posts an image online, the sentiment state they want to express is different when they are in different scenes, such as an office or a park.
% For example, when a person posts an image of themselves driving a car, the sentiment they want to express is different when they are in different scenes, such as traveling or on their way to work.
% For example, when a person posts an image of themselves driving a car, the sentiment they want to express is different when they are in different scenes, such as traveling, stuck in traffic, or on their way to work.
% 
For example, considering a landscape photo with metadata of the scene tag, such as a beach or city, helps in understanding the image's context and sentiment. Knowing it was taken at a beach can suggest feelings of relaxation or joy, enhancing sentiment analysis accuracy.
% 
This brings a new perspective to image sentiment analysis, which is the main focus of this paper: how to incorporate metadata into a unified framework and make comprehensive utilization of it for better knowledge reasoning and integration.

To this end, we propose a novel metadata enhanced transformer for image sentiment analysis (SentiFormer) to integrate multiple metadata and images into a unified framework.
Specifically, we first obtain multiple metadata corresponding to the image and use prompt learning for input alignment. 
Then, we employ CLIP to generate unified initial representations for both images and text. Next, we design an adaptive relevance learning module to highlight more effective information while suppressing weaker ones. Finally, we develop a cross-modal fusion module to integrate the adaptively learned representations and make the final prediction.
% Moreover, we are the first to construct and publicly release metadata-enhanced image sentiment analysis datasets, which include scenes, tags, descriptions, and corresponding confidence scores, and all the data are available at this link for future research purposes.
% Moreover, we are the first to construct and publicly release metadata-enhanced image sentiment analysis datasets, which include scenes, tags, descriptions, and corresponding confidence scores. All the data is publicly available at this link\footnote{\scriptsize{https://drive.google.com/drive/folders/1d5Qwr7pur6t50p5KgJFk7aLArlFmrdHI}} for research purposes.
Moreover, we are the first to construct and publicly release metadata-enhanced image sentiment analysis datasets, which include text descriptions and keyword tags. 
% All the data and code are publicly available at this link\footnote{\fontsize{7.5pt}{0.6\baselineskip}\selectfont{https://drive.google.com/drive/folders/1d5Qwr7pur6t50p5KgJFk7aLArlFmrdHI}} for research purposes.
All the data and code are publicly available at this link\footnote{https://github.com/MET4ISA/SentiFormer} for research purposes.
% All the data and code are publicly available at https://github.com/zzmyrep/SentiFormer for research purposes.
% https://github.com/zzmyrep/SentiFormer
% Moreover, we are the first to construct and publicly release metadata-enhanced image sentiment analysis datasets, which include scenes, tags, descriptions, and corresponding confidence scores. All the data is publicly available at  https://drive.google.com/drive/folders/1d5Qwr7pur6t50p5KgJFk7aLArlFmrdHI for research purposes.
Extensive experiments on three benchmark datasets demonstrate the superiority and rationality of our proposed method.
% https://drive.google.com/drive/folders/1d5Qwr7pur6t50p5KgJFk7aLArlFmrdHI
% 
% Moreover, we are the first to construct and publicly release metadata-enhanced image sentiment analysis datasets. These datasets include scenes, tags, descriptions, and corresponding confidence scores, and are available at this link for future research purposes.
% \shortname~compared with baseline approaches.

% Our main contributions are summarized as follows:
% \begin{itemize}
%     \item We introduce the idea of metadata enhancement into image sentiment analysis, build and publicly release the first image sentiment analysis dataset with metadata.
%     \item We build a unified metadata-enhanced sentiment transformer network to ensure adaptive fusion and reasoning of various data.
%     \item We conduct extensive experiments on three publicly available datasets. Both qualitative and quantitative results illustrate the effectiveness and advancement of our proposed method.
% \end{itemize}




