\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% \usepackage{url}
% \def\UrlBreaks{\do\A\do\B\do\C\do\D\do\E\do\F\do\G\do\H\do\I\do\J
% \do\K\do\L\do\M\do\N\do\O\do\P\do\Q\do\R\do\S\do\T\do\U\do\V
% \do\W\do\X\do\Y\do\Z\do\[\do\\\do\]\do\^\do\_\do\`\do\a\do\b
% \do\c\do\d\do\e\do\f\do\g\do\h\do\i\do\j\do\k\do\l\do\m\do\n
% \do\o\do\p\do\q\do\r\do\s\do\t\do\u\do\v\do\w\do\x\do\y\do\z
% \do\.\do\@\do\\\do\/\do\!\do\_\do\|\do\;\do\>\do\]\do\)\do\,
% \do\?\do\'\do+\do\=\do\#} 

\renewcommand{\footnoterule}{%
    \vspace{-0.3cm} % 
    \hrule width 0.2\textwidth height 0.2pt % 
    \vspace{0.1cm} % 
}

% for table
\usepackage{tabularx}
\usepackage{svg}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{bm}
% \usepackage{picins}
\usepackage{multirow}%
\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}
\newcommand{\revise}[1]{\textcolor{purple}{#1}}
% \captionsetup[figure]{labelfont={normal},name={Fig.},labelsep=period}

\begin{document}


% 
\newcommand{\fullname}{Metadata Enhanced Transformer}
% \newcommand{\shortname}{MESN}
\newcommand{\shortname}{SentiFormer}

% 
% \title{SentiFormer: Metadata Enhanced Transformer for Image Sentiment Analysis
% % Conference Paper Title*\\
% % {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
% % \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\title{SentiFormer: Metadata Enhanced Transformer \\for Image Sentiment Analysis
% Conference Paper Title*\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}


% \author{\IEEEauthorblockN{Bin Feng}
% \IEEEauthorblockA{\textit{State Key Laboratory of Cognitive Intelligence} \\
% \textit{University of Science and Technology of China}\\
% Hefei, China \\
% fengbin1@mail.ustc.edu.cn}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

% \author{
% \textit{author$^{1}$, author$^{2}$, author$^{1}$, author$^{1}$, author$^{1}$, author$^{1*}$} \\
% $^1$xueyuan1, University of Science and Technology of China, Hefei, China \\
% $^2$xueyuan2,Tsinghua University, Shenzhen, China \\
% \{xx, xx, xx, xx\}@mail.ustc.edu.cn, \{xx, xx\}@ustc.edu.cn
% }

% \author{
% \textit{Bin Feng$^{1,2}$, Shulan Ruan$^{3}$, Mingzheng Yang$^{1,2}$, Dongxuan Han$^{1,2}$, Huijie Liu$^{1,2}$, Kai Zhang$^{1,2}$, Qi Liu$^{1,2*}$\thanks{*Corresponding author}} \\
% $^1$University of Science and Technology of China 
% $^2$State Key Laboratory of Cognitive Intelligence\\
% $^3$Shenzhen International Graduate School, Tsinghua University \\
% fengbin@mail.ustc.edu.cn, slruan@sz.tsinghua.edu.cn, qiliuql@ustc.edu.cn
% }
\author{
\textit{Bin Feng$^{1,2}$, Shulan Ruan$^{3}$, Mingzheng Yang$^{1,2}$, Dongxuan Han$^{1,2}$, Huijie Liu$^{1,2}$, Kai Zhang$^{1,2}$, Qi Liu$^{1,2*}$\thanks{*Corresponding author. This research was partially supported by grants from the National Natural Science Foundation of China (Grant No. 62337001), the Key Technologies R\&D Program of Anhui Province (No. 202423k09020039), the Fundamental Research Funds for the Central Universities, and the National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, Xi'an Jiaotong University (No. HMHAI-202410).}} \\
$^1$University of Science and Technology of China 
$^2$State Key Laboratory of Cognitive Intelligence\\
$^3$Shenzhen International Graduate School, Tsinghua University \\
fengbin@mail.ustc.edu.cn, slruan@sz.tsinghua.edu.cn, qiliuql@ustc.edu.cn
}

\maketitle

\newcommand{\fb}[1]{\textcolor{purple}{#1}}

% 
\begin{abstract}
% Image sentiment analysis has attracted more and more research attention in recent years.
% ：
As more and more internet users post images online to express their daily emotions, image sentiment analysis has attracted increasing attention.
Recently, researchers generally tend to design different neural networks to extract visual features from images for sentiment analysis. 
% Despite the significant progress, metadata, the data associated with the image (e.g., image description, object and scene tags), is still underexplored in this task.
% Despite the significant progress, metadata, the relevant/attached data associated with the image, has not been sufficiently explored in this task.
Despite the significant progress, metadata, the data~(\textit{e.g.}, text descriptions and keyword tags) for describing the image, has not been sufficiently explored in this task.
% 
In this paper, we propose a novel Metadata Enhanced Transformer for sentiment analysis (\shortname) 
to fuse multiple metadata and the corresponding image into a unified framework. 
Specifically, we first obtain multiple metadata of the image and unify the representations of diverse data. 
To adaptively learn the appropriate weights for each metadata, we then design an adaptive relevance learning module to highlight more effective information while suppressing weaker ones. 
Moreover, we further develop a cross-modal fusion module to fuse the adaptively learned representations and make the final prediction.
Extensive experiments on three publicly available datasets demonstrate the superiority and rationality of our proposed method.
\end{abstract}

%  ，
\begin{IEEEkeywords}
% image sentiment analysis, representation learning, multi-view embedding, adaptive attention, cross-modal fusion
image sentiment analysis, metadata, transformer model, adaptive attention, cross-modal fusion
\end{IEEEkeywords}
%   


\input{1introduction}
% \input{2related_work}
\input{3method}
\input{4experiment}
\input{5conclusion}

\bibliography{main}
\bibliographystyle{IEEEtran}



\end{document}
