\section{Model Structure}
\label{s:method}



As shown in Fig.~\ref{fig:fig2}, our proposed method consists of three modules:
the \textit{feature representation} module, the \textit{adaptive relevance learning} module, and the \textit{cross-modal fusion and prediction} module.
% The details will be introduced in the following part.

\subsection{Feature Representation}
% \subsection{Representation Learning}
\textbf{Metadata Representation:}
% \textbf{Data Construction:}
In social media, such as Flickr and Instagram, when users post an image to express their emotions, 
% they usually include metadata such as description, tags, and scene.
they usually attach metadata such as descriptions, tags, and so on.
Due to privacy reasons and copyright protection, some metadata are unprovided in many image sentiment analysis datasets. In order to get a full understanding of the impact of metadata on this task, for partial missing metadata, we adopt some methods to generate them with confidence.
%  for scientific purposes

Given an image $\bm{I}$, for the text description of the image, we utilize BLIP~\cite{li2022blip}, pre-trained on the COCO dataset, to generate textual caption $\bm{C}$.
% In this paper, keyword tags typically include object tags and scene tags, which are highly helpful for image sentiment analysis.
% In order to obtain the object tags, we apply Faster R-CNN~\cite{ren2015faster} to obtain top-k tags $\bm{T}_{obj}=\{\bm{obj}_{i}\}_{i=1}^{k}$ ranked by confidence score.
% To obtain the scene or background of the image, we employ Hiera~\cite{ryali2023hiera} to obtain the most likely one as $\bm{T}_{sce}$ among 365 categories in Place365~\cite{zhou2017places}.
Keyword tags typically include object tags and scene tags, which are highly helpful for image sentiment analysis.
In order to obtain objects contained in the image, we apply Faster R-CNN~\cite{ren2015faster} to obtain top-k object tags $\bm{T}_{obj}=\{\bm{obj}_{i}\}_{i=1}^{k}$ ranked by confidence score.
To obtain scene tag of the image, we employ Hiera~\cite{ryali2023hiera} to obtain the most likely one as $\bm{T}_{sce}$ among 365 categories in Place365~\cite{zhou2017places}.


For input alignment and better representation of multiple metadata,
we use prompt learning to represent keyword tags since they are word phrases rather than sentences. Specifically, we design a heuristic prompt $\bm{P} = $
\textit{`the scene or background of the image is $\bm{T}_{sce}$, and the image contains the following objects: $\bm{obj}_1$, $\bm{obj}_2$, ..., $\bm{obj}_k$'}.
% "The image has a scene or background that is sce, and it contains the following objects: tag1, tag2, ..., tagk."
% "The scene or background of the image is sce, containing the following objects: tag1, tag2, ..., tagk."
% 

\textbf{Image Representation:}
% \textbf{Unified Representation:}
% \textbf{Semantic Representation:}
CLIP~\cite{radford2021learning} has powerful capabilities of image and text representation. Therefore, in this paper, we employ CLIP, which uses a ViT-B/32 transformer architecture as an image encoder and a masked self-attention transformer as a text encoder, to unify and align the representations of images and metadata.
% 
After unified encoding, each output of $\bm{I}$, $\bm{C}$, and $\bm{P}$ above is a 512-dimensional vector, which can be represented as follows: 
% $\bm{I}$, $\bm{C}$, and $\bm{P}$ mentioned above are uniformly encoded, and each output is a 512-dimensional vector.
\begin{equation}
    \bm{E}_v, \bm{E}_c, \bm{E}_p = \text{CLIP}(\bm{I, C, P}),
\end{equation}
where $\bm{E}_i \in \mathbb{R}^{d_e} , i \in \{ v, c, p \}$, and $d_e = 512$.

% \subsection{Adaptive Attention Learning}
\subsection{Adaptive Relevance Learning}
\textbf{Unified Embedding:}
% \textbf{Unified Projection/Mapping/Space:}
First, we pass $\bm{E}_v, \bm{E}_c, \bm{E}_p$ through a fully connected (FC) layer and project them into a $d_h$-dimensional space.
% 
Then, we use three parallel transformer layers to unify the features of each part, respectively.
In order to better handle the $L$-classification task, we expand each feature to $L \times d_h$ dimensions in the transformer layer. Following Vision Transformer~\cite{dosovitskiy2020image}, the structure of each transformer layer includes layer normalization, multi-head self-attention, and a fully connected layer. In practice, we use 8-head attention and set the dimension of each head to 64. The process is formulated as follows:
% The process is formulated as follows:
\begin{eqnarray}
\begin{aligned}
    \bm{H}_i^0 &= \text{FC}(\bm{E}_i),\\
    \bm{H}_i^1 &= \text{Transformer}(({\bm{W}_i^0}{\bm{H}_i^0} + {\bm{b}_i^0})),
\end{aligned}
\end{eqnarray}
% 
% https://zhuanlan.zhihu.com/p/445122996
% \begin{eqnarray}
% \begin{aligned}
%     \bm{H}_v^0, \bm{H}_c^0, \bm{H}_p^0 &= MLP(\bm{E}_v, \bm{E}_c, \bm{E}_p),\\
%     \bm{H}_v^1, \bm{H}_c^1, \bm{H}_p^1 &= Transformer(\bm{H}_v^0, \bm{H}_c^0, \bm{H}_p^0),
% \end{aligned}
% \end{eqnarray}
where $\bm{H}_i^0 \in \mathbb{R}^{1 \times d_h}$ and $\bm{H}_i^1 \in \mathbb{R}^{L \times d_h}$, $i \in \{ v, c, p \}$, and $\bm{W}_i^0 \in \mathbb{R}^{L \times 1} $, $\bm{b}_i^0 \in \mathbb{R}^{L \times d_h} $ are trainable parameters.

% 
\textbf{Adaptive Learning:}
In image sentiment analysis, some metadata are more relevant to the image content, while others are irrelevant information or noise. In order to adaptively learn the appropriate weight for each metadata, we use the 
% cross-modal attention mechanism
multi-head attention (MHA) mechanism
to highlight more effective information while suppressing the weaker.
Meanwhile, in order to capture the low-level to high-level features of the image, we use multiple cascaded transformer layers to learn a sequence of visual features. 
% Formally, $\bm{H}_v^{j+1} = \text{Transformer}(\bm{H}_v^{j}), j = 1, ..., L-1$, and $L$ is the number of transformer layers.
\begin{eqnarray}
\begin{aligned}
% \begin{equation}
    \bm{H}_v^{j+1} &= \text{Transformer}(\bm{H}_v^{j}),
% \end{equation}
\end{aligned}
\end{eqnarray}
where $j = 1, ..., N$, and $N$ is the number of transformer layers.

% When images and metadata interact, we use the sequence of image features as queries and the representation of metadata as keys and values. We define the features of metadata as Hm,m = 1,2,4. Inspired by ResNet, we use residual blocks to gradually accumulate and learn the representation of metadata.

In the interaction between image and metadata, we take this sequence of image features as query, and metadata embedding as key and value. 
% We define the joint feature of metadata as $\bm{H}_m^j$. Inspired by ResNet, we use residuals to gradually accumulate to learn the representation of metadata.
Furthermore, in order to learn the joint representation of multiple metadata, we first randomly initialize a new token $\bm{H}_m^1 \in \mathbb{R}^{L \times d_h}$. Then, inspired by ResNet~\cite{he2016deep}, we introduce the residual block to gradually accumulate and update the learned metadata representation $\bm{H}_m^j$. 
The process is formulated as follows:
% In order to obtain the joint feature of metadata, inspired by ResNet~\cite{he2016deep}, we use residual blocks to gradually accumulate and learn the metadata representation $\bm{H}_m^j$.
\begin{eqnarray}
\begin{aligned}
\text{MHA}(\bm{Q}, \bm{K}, \bm{V}) &= \textrm{Softmax}\left(\frac{\bm{Q}\bm{W}_Q\bm{W}_K^T\bm{K}^T}{\sqrt{d_k}}\right)\bm{V}\bm{W}_V, \\
\bm{H}_c^{j+1} &= \text{MHA}\left(\bm{H}_v^j, \bm{H}_c^1, \bm{H}_c^1\right),\\
\bm{H}_p^{j+1} &= \text{MHA}\left(\bm{H}_v^j, \bm{H}_p^1, \bm{H}_p^1\right),\\
\bm{H}_m^{j+1} &= \bm{H}_m^{j}+(\bm{H}_c^{j+1}+\bm{H}_p^{j+1})\bm{W}_O,
\end{aligned}
\end{eqnarray}
where $j = 1, ..., N$, and ${d_k}$ means the dimension of each head of the multi-head attention. $\bm{W}_Q$, $\bm{W}_K$, $\bm{W}_V$, and $\bm{W}_O$ are trainable parameters.

% \begin{eqnarray}
% \begin{aligned}
% \text{Attention}(\bm{Q}, \bm{K}, \bm{V}) = \textrm{softmax}\left(\frac{\bm{Q}\bm{K}^T}{\sqrt{d_k}}\right)\bm{V}, \\
% \text{head}_i = \text{Attention}\left(\bm{Q}\bm{W}_i^q, \bm{K}\bm{W}_i^k, \bm{V}\bm{W}_i^v\right),\\
% c \tanh
% \end{aligned}
% \end{eqnarray}
% 多头注意力机制(可以拆分为3个公式)：https://imzhanghao.com/2021/09/15/self-attention-multi-head-attention/
% https://blog.csdn.net/weixin_50752408/article/details/129584954

% 
\subsection{Cross-modal Fusion and Prediction}
% \subsection{Dynamically Adjusted Sentiment Prediction}
\textbf{Cross-modal Fusion:}
When predicting different sentiment labels, not all visual information and metadata make the same contribution to the final prediction. Therefore, we adopt a cross-modal transformer to tackle this problem.
% , where the weights of cross-modal attention should be different.

We take $\bm{H}_v^{N+1}$ and $\bm{H}_m^{N+1}$ as input, and add an extra\_token $\bm{H}_e$ and position embedding.
$\bm{H}_e$ is a $d_h$-dimensional token, which is used to capture global information at the head of a sequence, and
position embedding is used to preserve sequential information of the input.
As shown in Fig.~\ref{fig:fig2}, the cross-modal transformer is a deep stacking of several cross-modal attention blocks, with a depth of $M$. In the cross-modal transformer, we first take $\bm{X}_t^0$ as the query and $\bm{X}_s^0$ as the key and value.
The cross-modal transformer can effectively capture and fuse information from diverse data to enrich feature representation, which can be formulated as follows.

% Let the source modality input be \( \bm{X}_s \in \mathbb{R}^{ n_s \times d} \) and the target modality input be \( \bm{X}_t \in \mathbb{R}^{ n_t \times d} \), where  \( n_s \) and \( n_t \) are the sequence lengths of the source and target modalities, and \( d_h \) is the feature dimension.

First, the extra token is concatenated to the input sequence, followed by adding positional embeddings:
\begin{eqnarray}
\begin{aligned}
\bm{X}_s^0 &= \bm{H}_e \oplus \bm{H}_v^{N+1} + \bm{P}_s,\\
\bm{X}_t^0 &= \bm{H}_e \oplus \bm{H}_m^{N+1} + \bm{P}_t,
\end{aligned}
\end{eqnarray}
where \( \oplus \) denotes the concatenation operation along the sequence dimension, and \( \bm{H}_e \in \mathbb{R}^{ 1 \times d_h} \) represents the added token.
The positional embeddings are defined as $\bm{P}_s \in \mathbb{R}^{(L + 1) \times d_h}$ and$\quad \bm{P}_t \in \mathbb{R}^{(L + 1) \times d_h}$.

% Then, the cross-attention mechanism computes the attention of the target modality on the source modality as follows:
Next, in each transformer block, the multi-head attention computes the attention from the target to the source as follows:
\begin{eqnarray}
\begin{aligned}
[\bm{Q}_t, \bm{K}_s, \bm{V}_s] &= [\bm{X}_t \bm{W}_{Q'}, \bm{X}_s \bm{W}_{K'}, \bm{X}_s \bm{W}_{V'}],\\
\bm{X}_{s2t} &= \text{Softmax}\left(\frac{\bm{Q}_t \bm{K}_s^T}{\sqrt{d_s}}\right) \bm{V}_s,
% 
\end{aligned}
\end{eqnarray}
where \( \bm{W}_{Q'}, \bm{W}_{K'}, \bm{W}_{V'}\) are trainable parameters, and \( d_s \) is the dimension of each attention head. 

Then, we obtain the final cross-modal fusion output $\bm{X}_{s2t}^M$ by repeating cross-modal transformer blocks $M$ times.
% \( \bm{X}_{s2t} \in \mathbb{R}^{ L \times d_h} \) means the output of the cross-modal transformer.
% , which contains the interaction information between the source and target modalities and can be passed to downstream tasks.

\textbf{Sentiment Prediction:}
For the image sentiment classification task, we select the first token of the sequence from the cross-modal fusion output. The token at the head of the sequence contains global information of the entire sequence. 
Then we construct a classification head to make the final prediction, which consists of a linear layer and a softmax layer.
\begin{eqnarray}
\begin{aligned}
\bm{p} &= \text{Softmax}(\bm{X}_{s2t}^M[:, 0] \bm{W}_f + \bm{b}_f),
\end{aligned}
\end{eqnarray}
where $\bm{W}_f \in \mathbb{R}^{d_h \times L}$ and $\bm{b}_f \in \mathbb{R}^{1 \times L}$ are trainable parameters.

For model learning, we employ the cross-entropy as the loss
function, which is calculated as follows:
% The loss function for the output $\bm{p}$ of the last layer is shown as follows:
\begin{eqnarray}
\begin{aligned}
L &= -\frac{1}{n} \sum_{i = 1}^{n} \boldsymbol{y}_{i} log P(\boldsymbol{p}_{i} \mid \boldsymbol{I}),
\end{aligned}
\end{eqnarray}
where $\boldsymbol{y}_i$ is the true answer label of the $i^{th}$ instance of the dataset, and $n$ represents the number of training instances.

