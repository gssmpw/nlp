\section{Related Work}
\paragraph{Free-text explanation generation and evaluation}
Self-rationalization has been a popular approach for generating free-text explanations____. ____ shows that joint learning of label prediction and explanation generation results in explanations more aligned with predicted labels. ____ addressed the scarcity of annotated explanation data by using prompt-based fine-tuning on a few examples, though their evaluation was limited to in-distribution datasets. Few works have studied how such models can generalize to OOD.____ studied how learning with few-shot instances with template-based explanations influences OOD generalization. Their OOD dataset (e-HANS) is limited with constructed templates based on the HANS dataset____.____ studied the effect of self-rationalization on reducing models' reliance on spurious cues in out-of-domain datasets, and they showed that self-rationalization improves models robustness when fine-tuning data size is small. ____ studied the setup where the target dataset has few annotated free-text explanations but abundant labels. Their approach is limited to target datasets in which free-text explanations exist. In contrast to the above OOD evaluations, we focus on the OOD evaluation of self-rationalization for 19 diverse datasets, and our evaluation does not rely on reference explanations.

Reliable evaluation is crucial for explanation generation. Traditional metrics that measure text overlap with references have shown low correlation with human judgments____, and reference explanations are not always available. Recent works, like TigerScore____, Auto-J____, and Themis____, use LLMs as evaluators. These metrics rely on detailed instructions specifying evaluation aspects (e.g., relevance, accuracy, coherence) and formatted inputs for the task. The trained metric then generates a rating along with a textual analysis.
To test their suitability for the explanation generated with self-rationalization, in this work, we study their correlations with human judgments.


\paragraph{Few-shot sample selection}
Recent studies show that fine-tuning with smaller, high-quality datasets can outperform larger datasets____.
____ proposed to use a relatively small language model to evaluate and select a few instances for instruction-tuning on larger models.
To select data to perform well in transfer learning, ____ proposed data selection for instruction-tuning on a target-specific domain. 
They show that training with 5\% of the data outperforms training with the full dataset. The main constraint is that the validation set needs to be from the target domains.
____ proposed to improve data quality by estimating their model's confidence, and for the low-quality data, they either filter or correct them.
Most methods for sample selection are designed to perform well on in-distribution or known target domains, and the goal is for better classification performance. In contrast, our work focuses on selecting data that should help OOD performance on both label prediction and explanation generation.