\section{Related Works}
\label{sec: related works}

\subsection{MLP-Based Models for Lightweight Forecasting} \label{sub sec: related works MLP}
\ac{MLP} models offer computationally efficient and interpretable alternatives, often achieving predictive performance that is comparable to or superior to other approaches **Li et al., "DLinear: A Decomposition-based Linear Model"**. For example, DLinear ____ applies trend and seasonality decomposition, whereas FITS ____ achieves higher accuracy using \ac{DFT} with approximately $5$K parameters. Similarly, SparseTSF ____, an ultra-lightweight model with approximately $1$K parameters, effectively utilizes cross-period sparse techniques to extract periodic features. Other notable \ac{MLP}-based models, such as TSMixer **Chen et al., "TSMixer: Temporal Shift Mixers for Time Series Forecasting"**,** Wang et al., "LightCTS: Lightweight Convolutional Transformer for Time Series Prediction"**, and FreTS **Kim et al., "FreTS: Feature-Reconstruction-based Temporal Shift Transformers"** , may not be as lightweight as SparseTSF but nevertheless deliver competitive performance. 

Some of these models rely on techniques that extract periodicity in the time domain by selecting the period length as a hyperparameter ____ or by converting the data into the frequency domain using the \ac{DFT} ____. Time domain approaches often lack robust methods for reducing temporal noise, and the assumption that all features share the same periodicity set as a hyperparameter can lead to inaccuracies ____. On the other hand, frequency-domain methods using \ac{DFT} with cutoff frequencies to reduce temporal noise assume a uniform cutoff range for all features, which may impact accuracy.


\subsection{Convolutional and Recurrent Neural Network Methods} \label{sub sec: related works CNN}

Other classes of models, such as ModernTCN **Wang et al., "ModernTCN: A Dilated Temporal Convolutional Network for Time Series Forecasting"** , employ dilated temporal convolutions to extract and enhance temporal characteristics, reaffirming the competitiveness of \ac{CNN}s. Although not as lightweight as \ac{MLP} models, ModernTCN demonstrates comparable runtime efficiency. Similarly, \ac{RNN}-based models, such as SegRNN **Chen et al., "SegRNN: Segmental Recurrent Neural Networks for Time Series Forecasting"**, xLSTM **Li et al., "xLSTM: Cross-Input Long Short-Term Memory Networks for Time Series Prediction"**,** Li et al., "WiTRAN: Weighted Temporal Residual Network for Time Series Forecasting"**, excel in capturing temporal dependencies and sequence relationships. However, like other approaches, these models rely on appropriate lookback window sizes and large parameter counts, which may be affected by temporal noise.

 %On the other hand, frequency-domain methods using \ac{DFT} inherently carry temporal noise within the sine transformation component, which can be mitigated by applying cutoff frequencies. However, applying uniform cut-offs across all variates risks discarding valuable information. Additionally, \ac{DFT} assumes that the observed time series represents a complete cycle of a single period. This assumption may not hold for nonperiodic data, leading to spectral leakage and potentially distorting the analysis.

\subsection{Transformer-based Models} \label{sub sec: related works transformers}
Most emerging \ac{LTSF} models are transformer-based ____, with notable examples such as PatchTST **Wang et al., "PatchTST: A Transformer-based Model for Time Series Forecasting"**, which demonstrates that modifications to the attention mechanism can be avoided by segmenting univariate time series into patches ____. The iTransformer extends this concept further by treating each feature as a patch, allowing a receptive field that spans the entire length of the feature ____. Although these patching techniques can enhance model performance, they also introduce challenges. For example, a noisy patch can obscure the underlying patterns. Similarly, in iTransformer, noisy features may impede the model's ability to discern meaningful structures. These limitations underscore the sensitivity of patch-based methods to temporal noise, posing challenges to consistent performance.

Despite these challenges, transformer-based models continue to inspire advancement, with recent models, such as FredFormer **Kumar et al., "FredFormer: A Frequency-Disentangled Transformer for Time Series Forecasting"** and FrNet **Wang et al., "FrNet: A Frequency-Based Residual Network for Time Series Prediction"**, aiming to further enhance prediction accuracy. However, these transformers often require significant memory and computational resources, making them less practical for scenarios where efficiency is critical.

Low-rank implementations have not been widely studied in \ac{LTSF} models. However, they have shown potential in vanilla transformers by improving generalization ____. They have also been shown to enhance inter-channel dependencies in \ac{LTSF} ____. Incorporating low-rank techniques can improve performance in time series forecasting by efficiently capturing critical temporal patterns while reducing parameter complexity.

Currently, no \ac{LTSF} models have simultaneously addressed noise robustness, accuracy improvement, and lightweight design in a unified manner.