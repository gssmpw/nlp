\section{Related Works}
\label{sec: related works}

\subsection{MLP-Based Models for Lightweight Forecasting} \label{sub sec: related works MLP}
\ac{MLP} models offer computationally efficient and interpretable alternatives, often achieving predictive performance that is comparable to or superior to other approaches \cite{zeng2023transformers,xu2023fits,ekambaram2023tsmixer}. For example, DLinear \cite{zeng2023transformers} applies trend and seasonality decomposition, whereas FITS \cite{xu2023fits} achieves higher accuracy using \ac{DFT} with approximately $5$K parameters. Similarly, SparseTSF \cite{lin2024sparsetsf}, an ultra-lightweight model with approximately $1$K parameters, effectively utilizes cross-period sparse techniques to extract periodic features. Other notable \ac{MLP}-based models, such as TSMixer \cite{ekambaram2023tsmixer}, LightCTS \cite{lai2023lightcts}, and FreTS \cite{yi2024frequency}, may not be as lightweight as SparseTSF but nevertheless deliver competitive performance. 

Some of these models rely on techniques that extract periodicity in the time domain by selecting the period length as a hyperparameter \cite{lin2024sparsetsf,yi2024frequency} or by converting the data into the frequency domain using the \ac{DFT} \cite{xu2023fits,yi2024frequency}. Time domain approaches often lack robust methods for reducing temporal noise, and the assumption that all features share the same periodicity set as a hyperparameter can lead to inaccuracies \cite{lin2024sparsetsf}. On the other hand, frequency-domain methods using \ac{DFT} with cutoff frequencies to reduce temporal noise assume a uniform cutoff range for all features, which may impact accuracy.


\subsection{Convolutional and Recurrent Neural Network Methods} \label{sub sec: related works CNN}

Other classes of models, such as ModernTCN \cite{luo2024moderntcn}, employ dilated temporal convolutions to extract and enhance temporal characteristics, reaffirming the competitiveness of \ac{CNN}s. Although not as lightweight as \ac{MLP} models, ModernTCN demonstrates comparable runtime efficiency. Similarly, \ac{RNN}-based models, such as SegRNN \cite{lin2023segrnn}, xLSTM \cite{alharthi2024xlstmtime}, and WiTRAN \cite{jia2024witran}, excel in capturing temporal dependencies and sequence relationships. However, like other approaches, these models rely on appropriate lookback window sizes and large parameter counts, which may be affected by temporal noise.

 %On the other hand, frequency-domain methods using \ac{DFT} inherently carry temporal noise within the sine transformation component, which can be mitigated by applying cutoff frequencies. However, applying uniform cut-offs across all variates risks discarding valuable information. Additionally, \ac{DFT} assumes that the observed time series represents a complete cycle of a single period. This assumption may not hold for nonperiodic data, leading to spectral leakage and potentially distorting the analysis.

\subsection{Transformer-based Models} \label{sub sec: related works transformers}
Most emerging \ac{LTSF} models are transformer-based \cite{liu2024deep}, with notable examples such as PatchTST, which demonstrates that modifications to the attention mechanism can be avoided by segmenting univariate time series into patches \cite{nie2022time}. The iTransformer extends this concept further by treating each feature as a patch, allowing a receptive field that spans the entire length of the feature \cite{liu2023itransformer}. Although these patching techniques can enhance model performance, they also introduce challenges. For example, a noisy patch can obscure the underlying patterns. Similarly, in iTransformer, noisy features may impede the model's ability to discern meaningful structures. These limitations underscore the sensitivity of patch-based methods to temporal noise, posing challenges to consistent performance.

Despite these challenges, transformer-based models continue to inspire advancement, with recent models, such as FredFormer \cite{piao2024fredformer} and FrNet \cite{zhang2024frnet}, aiming to further enhance prediction accuracy. However, these transformers often require significant memory and computational resources, making them less practical for scenarios where efficiency is critical.

Low-rank implementations have not been widely studied in \ac{LTSF} models. However, they have shown potential in vanilla transformers by improving generalization \cite{hu2021lora,zhang2023adalora,valipour2022dylora}. They have also been shown to enhance inter-channel dependencies in \ac{LTSF} \cite{nie2024channel}. Incorporating low-rank techniques can improve performance in time series forecasting by efficiently capturing critical temporal patterns while reducing parameter complexity.

Currently, no \ac{LTSF} models have simultaneously addressed noise robustness, accuracy improvement, and lightweight design in a unified manner.