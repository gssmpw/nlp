\section{Related Work}
\paragraph{Data Augmentation} Traditional data augmentations have been analyzed from various perspectives: \citet{bishop1995training,dao2019kernel} interpret them as regularization, \citet{hanin2021data} investigate their optimization impact, \citet{chen2020group} view them as a mechanism for invariant learning, and \citet{rajput2019does} study their effect on decision margins. \citet{shen2022data} further examine geometric transformations through feature learning. For advanced methods, \citet{zhang2020does,carratino2022mixup} study Mixup as a regularization technique, while \citet{park2022unified} analyze Mixup and CutMix together. Additionally, \citet{chidambaram2023provably,zou2023benefits} investigate the feature learning dynamics of Mixup, and \citet{oh2024provable} analyze the feature learning effects of CutOut and CutMix.


\paragraph{Feature Learning Analysis}
Prior works on feature learning have provided valuable insights into how neural networks learn and represent data \citep{wen2021toward,wen2022mechanism,allen2022feature,allen-zhu2023towards,jelassi2022towards,huang2023understanding,chen2024does}. For instance, \citet{allen-zhu2023towards} investigated the mechanisms by which ensemble methods and knowledge distillation enhance model generalization.
Building on the analytical framework proposed by \citet{allen-zhu2023towards}, this work extends the scope by offering a unified analysis of data augmentation across traditional and advanced methods.