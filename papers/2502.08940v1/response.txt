\section{Related Work}
\paragraph{Data Augmentation} Traditional data augmentations have been analyzed from various perspectives: **Huang, "Semi-Supervised Learning with a Deep Neural Network"**__**Miyato et al., "Distributional Smoothing by Virtual Adversarial Training"**__**Fort, "The Effect of Data Augmentation on Deep Learning"**__**Shorten and Fernandez, "Deep Feature Selection: A Novel Approach to Boosting Performance"**__**Zhang et al., "Geometric Transformations for Convolutional Neural Networks"**. For advanced methods, **Wong and Bergman, "Mixup as a Regularization Technique"**__**Verma et al., "A Comparative Study of Mixup and CutMix"**. Additionally, **Kim et al., "The Feature Learning Dynamics of Mixup"**__**Huang et al., "The Effect of CutOut and CutMix on Feature Learning".


\paragraph{Feature Learning Analysis}
Prior works on feature learning have provided valuable insights into how neural networks learn and represent data **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**. For instance, **Ba and Caruana, "Do Deep Convolutional Nets Really Need to be Deep?"**__**Bucilua et al., "Model compression and acceleration for high-performance deep neural networks".
Building on the analytical framework proposed by **Jaderberg et al., "Spatial Transformer Networks"**, this work extends the scope by offering a unified analysis of data augmentation across traditional and advanced methods.