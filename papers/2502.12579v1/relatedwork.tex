\section{Related Work}
\subsection{Human Preference Optimization}

Human preference optimization aims to improve the aesthetic quality of generated images. One line of research~\cite{image_reward,hpsv2,pickscore,ahf} focused on developing reward models to evaluate the alignment between generated images and their corresponding prompts. These reward models are subsequently used to finetune diffusion models, guiding them to better capture desirable characteristics. Another line of research explored direct finetuning on preferred images~\cite{raft,emu} or adopted reinforcement learning from human feedback (RLHF)~\cite{ddpo,DPOK,d3po,diffusion-dpo}. Specifically, DDPO~\cite{ddpo} formulated denoising as a multi-step decision-making problem. DPOK~\cite{DPOK} incorporated KL regularization as an implicit reward to stabilize the finetuning process. D3PO~\cite{d3po} and Diffusion-DPO~\cite{diffusion-dpo} extended the concept of DPO~\cite{llm_dpo} to diffusion models by optimizing the respective policies. Different from these methods, CHATS employs two distinct models to separately capture preferred and dispreferred information.


\subsection{Guided Sampling for Text-to-Image Generation}

Guided sampling enhances fidelity by steering generation towards specific directions determined by various strategies. CG~\cite{cg} utilized the gradients from a classifier as the signal to achieve class-conditional generation at the cost of diversity. CFG~\cite{cfg} extended this concept to the open-vocabulary domain by combining conditional and unconditional outputs, thereby biasing the noise towards areas with higher semantic density. Numerous methods~\cite{sag,uni_guide,readout,glide} were proposed to further refine CFG. In contrast to these approaches, as shown in Fig.~\ref{fig:network}, our CHATS enhances sampling quality by facilitating collaboration with DPO, enabling both processes to mutually reinforce each other and thus achieve superior results.