\section{Related Work}
\subsection{Human Preference Optimization}

Human preference optimization aims to improve the aesthetic quality of generated images. One line of research____ focused on developing reward models to evaluate the alignment between generated images and their corresponding prompts. These reward models are subsequently used to finetune diffusion models, guiding them to better capture desirable characteristics. Another line of research explored direct finetuning on preferred images____ or adopted reinforcement learning from human feedback (RLHF)____. Specifically, DDPO____ formulated denoising as a multi-step decision-making problem. DPOK____ incorporated KL regularization as an implicit reward to stabilize the finetuning process. D3PO____ and Diffusion-DPO____ extended the concept of DPO____ to diffusion models by optimizing the respective policies. Different from these methods, CHATS employs two distinct models to separately capture preferred and dispreferred information.


\subsection{Guided Sampling for Text-to-Image Generation}

Guided sampling enhances fidelity by steering generation towards specific directions determined by various strategies. CG____ utilized the gradients from a classifier as the signal to achieve class-conditional generation at the cost of diversity. CFG____ extended this concept to the open-vocabulary domain by combining conditional and unconditional outputs, thereby biasing the noise towards areas with higher semantic density. Numerous methods____ were proposed to further refine CFG. In contrast to these approaches, as shown in Fig.~\ref{fig:network}, our CHATS enhances sampling quality by facilitating collaboration with DPO, enabling both processes to mutually reinforce each other and thus achieve superior results.