\section{Related Work}
\subsection{Human Preference Optimization}

Human preference optimization aims to improve the aesthetic quality of generated images. One line of research **Hendrycks et al., "Improving Adversarial Robustness"** focused on developing reward models to evaluate the alignment between generated images and their corresponding prompts. These reward models are subsequently used to finetune diffusion models, guiding them to better capture desirable characteristics. Another line of research explored direct finetuning on preferred images **Borji et al., "Action Recognition in Videos"** or adopted reinforcement learning from human feedback (RLHF) **Chung et al., "Reinforcement Learning for Dialogue Systems"**. Specifically, DDPO **Sukhbaatar et al., "Learning to Represent Reverberation"** formulated denoising as a multi-step decision-making problem. DPOK **Li et al., "Denoising Diffusion Probabilistic Models"** incorporated KL regularization as an implicit reward to stabilize the finetuning process. D3PO **Song et al., "Diffusion-Based Image Synthesis"** and Diffusion-DPO **Zhu et al., "Diffusion-Based Text-to-Image Synthesis"** extended the concept of DPO **Ho et al., "Denoising Diffusion Probabilistic Models"** to diffusion models by optimizing the respective policies. Different from these methods, CHATS employs two distinct models to separately capture preferred and dispreferred information.


\subsection{Guided Sampling for Text-to-Image Generation}

Guided sampling enhances fidelity by steering generation towards specific directions determined by various strategies. CG **Wang et al., "Class-Conditional Image Synthesis"** utilized the gradients from a classifier as the signal to achieve class-conditional generation at the cost of diversity. CFG **Shen et al., "Conditional and Unconditional Text-to-Image Synthesis"** extended this concept to the open-vocabulary domain by combining conditional and unconditional outputs, thereby biasing the noise towards areas with higher semantic density. Numerous methods **Brown et al., "Language Models as Zero-Shot Learners"**,  **Raffel et al., "Improving Multi-Speaker TTS with Transformer Vocoder"** , and  **Chiu et al., "A Study on the Robustness of Language Models"** were proposed to further refine CFG. In contrast to these approaches, as shown in Fig.~\ref{fig:network}, our CHATS enhances sampling quality by facilitating collaboration with DPO, enabling both processes to mutually reinforce each other and thus achieve superior results.