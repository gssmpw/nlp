\section{Related Work}
\label{sec:Related Work}

\paragraph{Person-job fit systems}


% Early person-job fit systems that uses neural networks typically focus on architecture modification. These methods include **Zhang, "Deep Architectural Invariant Network"**, which explores
% architectures such as RNN, LSTM **Huang, "Long Short-Term Memory Recurrent Neural Networks for Natural Language Processing Tasks"** and CNN **Kim, "Convolutional Neural Networks for Sentence Classification"**.
% However, these are significantly outperformed by more recent transformer-based encoder models such as **Vaswani, "Attention Is All You Need"**.
% Recent deep learning methods include **Sang, "Deep Siamese Network for Resume/Job Embedding Learning"**, which uses deep siamese network to learn an embedding space for resume/jobs, **Wu, "Hierarchical Recurrent Neural Networks for Person-Job Fit Systems Domain Adaptation"** which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and **Mehdi, "Federated Learning for Privacy-Preserving Model Training in Person-Job Fit Systems"** which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
Early person-job fit systems that use neural networks typically focus on architecture designs. These methods include **Zhang, "Deep Architectural Invariant Network"**, which explores
architectures such as RNN, LSTM **Huang, "Long Short-Term Memory Recurrent Neural Networks for Natural Language Processing Tasks"** and CNN **Kim, "Convolutional Neural Networks for Sentence Classification"**.
However, these are significantly outperformed by more recent transformer-based methods such as **Vaswani, "Attention Is All You Need"**, which focuses on small architecture or loss modifications.
For example, MV-CoN **Zhou, "Co-Teaching Network for Person-Job Fit System"** uses a co-teaching network **Dai, "Gradual Co-Teaching for Model Updates with Confidence to Data Noises"** to perform gradient updates based on model's confidence to data noises; InEXIT **Lin, "Hierarchical Attention for Resume/Job Interactions in Person-Job Fit Systems"**, which uses hierarchical attention to model resume-job interactions; and DPGNN **Li, "Graph-Based Approach with BPR Loss for Resume/Job Ranking in Person-Job Fit Systems"** , which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% Recent deep learning methods include **Sang, "Deep Siamese Network for Resume/Job Embedding Learning"**, which uses deep siamese network to learn an embedding space for resume/jobs, **Wu, "Hierarchical Recurrent Neural Networks for Person-Job Fit Systems Domain Adaptation"** which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and **Mehdi, "Federated Learning for Privacy-Preserving Model Training in Person-Job Fit Systems"** which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
% Recent work with public implementations includes MV-CoN **Zhou, "Co-Teaching Network for Person-Job Fit System"** , which uses a co-teaching network **Dai, "Gradual Co-Teaching for Model Updates with Confidence to Data Noises"** to perform gradient updates based model's confidence to data noises; InEXIT **Lin, "Hierarchical Attention for Resume/Job Interactions in Person-Job Fit Systems"**, which uses hierarchical attention to model resume-job interactions; and DPGNN **Li, "Graph-Based Approach with BPR Loss for Resume/Job Ranking in Person-Job Fit Systems"** , which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
% However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% The former is unrealistic considering new job seekers/job positions frequently appear, and the latter is only applicable to specific recruitment platforms.
More recently, \confitold{} **Zhang, "Contrastive Learning for Person-Job Fit System"** focuses on applying dense retrieval techniques to person-job fit. Using contrastive learning with encoders such as E5 **Wang, "Encoder 5: A Pre-Trained Model for Passage Embedding"**, **Zhou, "Contrastive Learning for Passage Embedding with Encoder 5"** demonstrates its performance and flexibility by achieving the best scores in almost all ranking and classification tasks across two different person-job fit datasets.
In this work, we propose \HyReShort{} and \RunnerUpMiningShort{} to further enhance \framework{}, achieving an average absolute improvement of 13.8\% in recall and 17.5\% in nDCG across job ranking and resume-ranking tasks in two resume-job benchmarks.


\paragraph{Dense retrieval techniques} 
\framework{} benefits from dense retrieval techniques such as contrastive learning **Chen, "Contrastive Learning for Dense Text Retrieval"** and hard-negative mining.
Popular methods in text retrieval include BM25 **Robertson, "BM25: A Robust Model for Information Retrieval Tasks"**, a keyword-based approach used as the baseline in many text ranking tasks **Buckley, "An analysis of the effect of stemming on information retrieval systems"**, and dense retrieval methods such as **Huang, "Contrastive Learning with Encoder for Passage Embedding"**, which uses contrastive learning with an encoder to obtain high-quality passage embeddings, and typically performs top-k search based on inner product.

To further improve retrieval results, researchers considered methods such as query/document expansions **Wu, "Query/Document Expansion for Dense Text Retrieval"** and hard-negative mining **Zhou, "Hard Negative Mining with BM25 for Dense Text Retrieval"**.
Recent query/document expansion approaches include HyDE **Lin, "Hybrid Document Expansion for Passage Retrieval"** and HyQE **Xu, "Hierarchical Query/Document Expansion for Passage Retrieval"**, which prompts an LLM to augment the original query or the passage to retrieve and is typically used \emph{without} finetuning (the LLM or the encoder model). Common hard-negative mining strategies often use BM25 to select top-k unlabeled/incorrect samples as hard negatives **Chen, "BM25-based Hard Negative Mining for Dense Text Retrieval"**. Other methods include: ANCE **Dai, "Adversarial Negatives for Contrastive Learning"**, which samples negatives from the top retrieved documents asynchronously during training; SimANS **Wang, "Sampling Asymmetric Negatives with Probability Distribution"**, which samples hard negatives using a manually designed probability distribution; and more **Lin, "More Methods for Hard Negative Mining in Dense Text Retrieval"**. \confitold{} **Zhang, "Contrastive Learning for Person-Job Fit System"** presents the first successful attempt to use contrastive learning for person-job fit.
We extend \confitold{} and adapt recent dense retrieval techniques to person-job fit by 1) sampling hard-negative resumes/jobs from a ``runner-up'' \emph{percentile ranges} to mitigate selecting false-negatives; and 2) few-shot prompting an LLM to augment a job posted used for later encoder \emph{training}.


% \#TODO the following is copy pasted
% \framework{} benefits from contrastive learning techniques, which have seen wide applications in many dense text retrieval and representation learning tasks **Chen, "Contrastive Learning for Dense Text Retrieval"**.
% Given a query (e.g., user-generated question), an information retrieval system aims to find top-k relevant passages from a large reserve of candidate passages **Wang, "Passage Embedding with Contrastive Learning"**.
% Popular methods in information retrieval include BM25 **Robertson, "BM25: A Robust Model for Information Retrieval Tasks"**, a keyword-based approach used as the baseline in many text ranking tasks **Buckley, "An analysis of the effect of stemming on information retrieval systems"**, and dense retrieval methods such as **Huang, "Contrastive Learning with Encoder for Passage Embedding"**, which uses contrastive learning to obtain high-quality passage embeddings and typically performs top-k search based on inner product. To our knowledge, \framework{} is the first attempt to use contrastive learning for person-job fit, achieving the best performances in almost all person-job ranking tasks across two different person-job fit datasets.
% \paragraph{Hard Negative Mining} 
% Hard negatives are crucial for learning robust representations. Common strategies often begin with random sampling **Wu, "Random Sampling for Hard Negative Mining"** or in-batch negatives **Zhou, "In-Batch Negatives for Contrastive Learning"**. In person-job fit, \confitold{} used explicitly “rejected” resume-job pairs as difficult examples. Alternatively, BM25-based methods retrieve top-ranked candidates **Chen, "BM25-based Hard Negative Mining for Dense Text Retrieval"**, while PassageBM25 **Lin, "Passage-BM25: A Novel Approach to Hard Negative Mining"** refines this by comparing candidate passages to the positive passage rather than the query. Dynamic methods **Dai, "Dynamic Hard Negative Mining with Asynchronous Updates"** further refine the negative pool across training iterations. For example, ANCE **Wang, "Adversarial Negatives for Contrastive Learning"** leverages a pretrained dense retriever to asynchronously update top-scoring negatives.Instead of strictly choosing top-ranked candidates, SimANS **Zhang, "Sampling Asymmetric Negatives with Probability Distribution"** designs a probability distribution to sample difficult negatives near the boundary. Meanwhile, TriSampler **Wu, "Triangular Principle for Hard Negative Sampling in Dense Text Retrieval"** applies a “quasi-triangular principle” to locate especially informative negatives. Recent work has also explored graph-based negative sampling **Huang, "Graph-Based Hard Negative Sampling with Structural Information"**, leveraging structural information between items to improve sample selection. Methods like SyNeg **Lin, "Synthetic Negatives for Contrastive Learning in Dense Text Retrieval"** employ LLMs to generate contextually coherent but factually incorrect samples.


% By contrast, our method first trains a dense retriever with \confitold{}’s hard negative approach, then uses it to score all resume-job pairs. We focus on the top 3–4\% non-matching “runner-ups,” striking a careful balance between being sufficiently challenging and avoiding excessive false negatives. This strategy refines the model’s ability to handle label-sparse scenarios more effectively.