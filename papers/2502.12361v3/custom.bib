@inproceedings{DPGNN,
  author = {Chen Yang and Yupeng Hou and Yang Song and Tao Zhang and Ji-Rong Wen and Wayne Xin Zhao},
  title = {Modeling Two-Way Selection Preference for Person-Job Fit},
  booktitle = {{RecSys}},
  year = {2022}
}
@inproceedings{ListNet,
    author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
    title = {Learning to Rank: From Pairwise Approach to Listwise Approach},
    year = {2007},
    isbn = {9781595937933},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1273496.1273513},
    doi = {10.1145/1273496.1273513},
    booktitle = {Proceedings of the 24th International Conference on Machine Learning},
    pages = {129–136},
    numpages = {8},
    location = {Corvalis, Oregon, USA},
    series = {ICML '07}
}
@inproceedings{wei-zou-2019-eda,
    title = "{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks",
    author = "Wei, Jason  and
      Zou, Kai",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1670",
    pages = "6383--6389",
}
@inproceedings{bian-etal-2019-domain,
    title = "Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network",
    author = "Bian, Shuqing  and
      Zhao, Wayne Xin  and
      Song, Yang  and
      Zhang, Tao  and
      Wen, Ji-Rong",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1487",
    doi = "10.18653/v1/D19-1487",
    pages = "4810--4820",
}
@inproceedings{InEXIT,
  author    = {Taihua Shao and
               Chengyu Song and
               Jianming Zheng and
               Fei Cai and
               Honghui Chen},
  title     = {Exploring Internal and External Interactions for Semi-Structured
               Multivariate Attributes in Job-Resume Matching},
  booktitle = {International Journal of Intelligent Systems},
  doi       = {10.1155/2023/2994779},
  year      = {2023},
}
@inproceedings{APJFNN,
  author = {Qin, Chuan and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Jiang, Liang and Chen, Enhong and Xiong, Hui},
  title = {Enhancing Person-Job Fit for Talent Recruitment: An Ability-Aware Neural Network Approach},
  year = {2018},
  isbn = {9781450356572},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3209978.3210025},
  doi = {10.1145/3209978.3210025},
  booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages = {25–34},
  numpages = {10},
  keywords = {neural network, person-job fit, recruitment analysis},
  location = {Ann Arbor, MI, USA},
  series = {SIGIR '18}
}
@misc{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year={2019},
  eprint={1810.04805},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@misc{pjfnn,
  title={Person-Job Fit: Adapting the Right Talent for the Right Job with Joint Representation Learning}, 
  author={Chen Zhu and Hengshu Zhu and Hui Xiong and Chao Ma and Fang Xie and Pengliang Ding and Pan Li},
  year={2018},
  eprint={1810.04040},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
@misc{mvcon,
  title={Learning to Match Jobs with Resumes from Sparse Interaction Data using Multi-View Co-Teaching Network}, 
  author={Shuqing Bian and Xu Chen and Wayne Xin Zhao and Kun Zhou and Yupeng Hou and Yang Song and Tao Zhang and Ji-Rong Wen},
  year={2020},
  eprint={2009.13299},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@article{FAISS,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}
@misc{DPR,
  title={Dense Passage Retrieval for Open-Domain Question Answering}, 
  author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
  year={2020},
  eprint={2004.04906},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@misc{SimCLR,
      title={A Simple Framework for Contrastive Learning of Visual Representations}, 
      author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
      year={2020},
      eprint={2002.05709},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{simlm,
  title = "{S}im{LM}: Pre-training with Representation Bottleneck for Dense Passage Retrieval",
  author = "Wang, Liang  and
    Yang, Nan  and
    Huang, Xiaolong  and
    Jiao, Binxing  and
    Yang, Linjun  and
    Jiang, Daxin  and
    Majumder, Rangan  and
    Wei, Furu",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
  month = jul,
  year = "2023",
  address = "Toronto, Canada",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2023.acl-long.125",
  pages = "2244--2258",
}
@misc{izacard2021contriever,
  title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
  author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
  year={2021},
  url = {https://arxiv.org/abs/2112.09118},
  doi = {10.48550/ARXIV.2112.09118},
}
@misc{E5,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
  author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
  year={2022},
  eprint={2212.03533},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
@inproceedings{gao-etal-2021-simcse,
  title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
  author = "Gao, Tianyu  and
    Yao, Xingcheng  and
    Chen, Danqi",
  editor = "Moens, Marie-Francine  and
    Huang, Xuanjing  and
    Specia, Lucia  and
    Yih, Scott Wen-tau",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Online and Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.552",
  doi = "10.18653/v1/2021.emnlp-main.552",
  pages = "6894--6910",
}
@inproceedings{gillick-etal-2019-learning,
    title = "Learning Dense Representations for Entity Retrieval",
    author = "Gillick, Daniel  and
      Kulkarni, Sayali  and
      Lansing, Larry  and
      Presta, Alessandro  and
      Baldridge, Jason  and
      Ie, Eugene  and
      Garcia-Olano, Diego",
    editor = "Bansal, Mohit  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1049",
    doi = "10.18653/v1/K19-1049",
    pages = "528--537",
}
@inproceedings{bm25-all,
    author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
    title = {Improvements to BM25 and Language Models Examined},
    year = {2014},
    isbn = {9781450330008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2682862.2682863},
    doi = {10.1145/2682862.2682863},
    booktitle = {Proceedings of the 19th Australasian Document Computing Symposium},
    pages = {58–65},
    numpages = {8},
    keywords = {Document Retrieval, Relevance Ranking, Procrastination},
    location = {Melbourne, VIC, Australia},
    series = {ADCS '14}
}
@article{bm25,
    author = {Robertson, Stephen and Zaragoza, Hugo},
    title = {The Probabilistic Relevance Framework: BM25 and Beyond},
    year = {2009},
    issue_date = {April 2009},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
    volume = {3},
    number = {4},
    issn = {1554-0669},
    url = {https://doi.org/10.1561/1500000019},
    doi = {10.1561/1500000019},
    journal = {Found. Trends Inf. Retr.},
    month = {apr},
    pages = {333–389},
    numpages = {57}
}
@misc{chatgpt-paraphrase1,
      title={ChatGPT to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness}, 
      author={Jan Cegin and Jakub Simko and Peter Brusilovsky},
      year={2023},
      eprint={2305.12947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{dai2023auggpt,
      title={AugGPT: Leveraging ChatGPT for Text Data Augmentation}, 
      author={Haixing Dai and Zhengliang Liu and Wenxiong Liao and Xiaoke Huang and Yihan Cao and Zihao Wu and Lin Zhao and Shaochen Xu and Wei Liu and Ninghao Liu and Sheng Li and Dajiang Zhu and Hongmin Cai and Lichao Sun and Quanzheng Li and Dinggang Shen and Tianming Liu and Xiang Li},
      year={2023},
      eprint={2302.13007},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{chatgpt,
    author       = {OpenAI},
    howpublished = {},
    title        = {{OpenAI}: Introducing {ChatGPT}},
    year         = {2022},
    url          = {https://openai.com/blog/chatgpt}
}
@misc{4omini,
    author       = {OpenAI},
    howpublished = {},
    title        = {{GPT-4o-mini}: Advancing cost-effective intelligence},
    year         = {2024},
    url          = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}
}
@misc{text-ada,
    author       = {OpenAI},
    howpublished = {},
    title        = {New embedding models and API updates},
    year         = {2024},
    url          = {https://openai.com/index/new-embedding-models-and-api-updates/}
}
@inproceedings{XGBoost,
   series={KDD ’16},
   title={XGBoost: A Scalable Tree Boosting System},
   url={http://dx.doi.org/10.1145/2939672.2939785},
   DOI={10.1145/2939672.2939785},
   booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
   publisher={ACM},
   author={Chen, Tianqi and Guestrin, Carlos},
   year={2016},
   month=aug, collection={KDD ’16} 
}
@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{xlm-roberta,
  author    = {Alexis Conneau and
               Kartikay Khandelwal and
               Naman Goyal and
               Vishrav Chaudhary and
               Guillaume Wenzek and
               Francisco Guzm{\'{a}}n and
               Edouard Grave and
               Myle Ott and
               Luke Zettlemoyer and
               Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{han2018coteaching,
      title={Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels}, 
      author={Bo Han and Quanming Yao and Xingrui Yu and Gang Niu and Miao Xu and Weihua Hu and Ivor Tsang and Masashi Sugiyama},
      year={2018},
      eprint={1804.06872},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{rendle2012bpr,
      title={BPR: Bayesian Personalized Ranking from Implicit Feedback}, 
      author={Steffen Rendle and Christoph Freudenthaler and Zeno Gantner and Lars Schmidt-Thieme},
      year={2012},
      eprint={1205.2618},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{kamalloo2023beir-resources,
      title={Resources for Brewing BEIR: Reproducible Reference Models and an Official Leaderboard}, 
      author={Ehsan Kamalloo and Nandan Thakur and Carlos Lassance and Xueguang Ma and Jheng-Hong Yang and Jimmy Lin},
      year={2023},
      eprint={2306.07471},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@misc{thakur2021beir,
      title={BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models}, 
      author={Nandan Thakur and Nils Reimers and Andreas Rücklé and Abhishek Srivastava and Iryna Gurevych},
      year={2021},
      eprint={2104.08663},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}
@article{tsne,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}
@inproceedings{profiling-choice,
  author = {Yan, Rui and Le, Ran and Song, Yang and Zhang, Tao and Zhang, Xiangliang and Zhao, Dongyan},
  title = {Interview Choice Reveals Your Preference on the Market: To Improve Job-Resume Matching through Profiling Memories},
  year = {2019},
  isbn = {9781450362016},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3292500.3330963},
  doi = {10.1145/3292500.3330963},
  booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages = {914–922},
  numpages = {9},
  keywords = {job-resume matching, talent recruitment, profiling memory, neural networks},
  location = {Anchorage, AK, USA},
  series = {KDD '19}
}
@article{GUO2016169,
  title = {RésuMatcher: A personalized résumé-job matching system},
  journal = {Expert Systems with Applications},
  volume = {60},
  pages = {169-182},
  year = {2016},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2016.04.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417416301798},
  author = {Shiqiang Guo and Folami Alamudun and Tracy Hammond},
  keywords = {Recommender system, Ontology, Résumé, Job search, Cosine similarity},
}
@article{siamese,
  title={Matching Resumes to Jobs via Deep Siamese Network},
  author={Saket Maheshwary and Hemant Misra},
  journal={Companion Proceedings of the The Web Conference 2018},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:13807085}
}
@article{Zhang2023FedPJFFC,
  title={FedPJF: federated contrastive learning for privacy-preserving person-job fit},
  author={Yunchong Zhang and Baisong Liu and Jiangbo Qian},
  journal={Applied Intelligence},
  year={2023},
  volume={53},
  pages={27060 - 27071},
  url={https://api.semanticscholar.org/CorpusID:261454666}
}
@misc{jiang2020learning,
  title={Learning Effective Representations for Person-Job Fit by Feature Fusion}, 
  author={Junshu Jiang and Songyun Ye and Wei Wang and Jingran Xu and Xiaosheng Luo},
  year={2020},
  eprint={2006.07017},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}
@inproceedings{cnn-lstm-siamese,
  author = {Rezaeipourfarsangi, Sima and Milios, Evangelos E.},
  title = {AI-Powered Resume-Job Matching: A Document Ranking Approach Using Deep Neural Networks},
  year = {2023},
  isbn = {9798400700279},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3573128.3609347},
  doi = {10.1145/3573128.3609347},
  abstract = {This study focuses on the importance of well-designed online matching systems for job seekers and employers. We treat resumes and job descriptions as documents. Then, calculate their similarity to determine the suitability of applicants, and rank a set of resumes based on their similarity to a specific job description. We employ Siamese Neural Networks, comprised of identical sub-network components, to evaluate the semantic similarity between documents. Our novel architecture integrates various neural network architectures, where each sub-network incorporates multiple layers such as CNN, LSTM and attention layers to capture sequential, local and global patterns within the data. The LSTM and CNN components are applied concurrently and merged together. The resulting output is then fed into a multi-head attention layer. These layers extract features and capture document representations. The extracted features are then combined to form a unified representation of the document. We leverage pre-trained language models to obtain embeddings for each document, which serve as a lower-dimensional representation of our input data. The model is trained on a private dataset of 268,549 real resumes and 4,198 job descriptions from twelve industry sectors, resulting in a ranked list of matched resumes. We performed a comparative analysis involving our model, Siamese CNN (S-CNNs), Siamese LSTM with Manhattan distance, and a BERT-based sentence transformer model. By combining the power of language models and the novel Siamese architecture, this approach leverages both strengths to improve document ranking accuracy and enhance the matching process between job descriptions and resumes. Our experimental results demonstrate that our model outperforms other models in terms of performance.},
  booktitle = {Proceedings of the ACM Symposium on Document Engineering 2023},
  articleno = {22},
  numpages = {4},
  keywords = {Siamese neural network, document ranking, LSTM, CNN, language model, attention},
  location = {Limerick, Ireland},
  series = {DocEng '23}
}
@inproceedings{eliyas2022recommendation,
  title={Recommendation Systems: Content-Based Filtering vs Collaborative Filtering},
  author={Eliyas, Sherin and Ranjana, P},
  booktitle={2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE)},
  pages={1360--1365},
  year={2022},
  organization={IEEE}
}
@INPROCEEDINGS{10169716,
  author={Mhatre, Sonali and Dakhare, Bhawana and Ankolekar, Vaibhav and Chogale, Neha and Navghane, Rutuja and Gotarne, Pooja},
  booktitle={2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS)}, 
  title={Resume Screening and Ranking using Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={412-419},
  doi={10.1109/ICSCSS57650.2023.10169716}
}
@misc{malach2018decoupling,
  title={Decoupling "when to update" from "how to update"}, 
  author={Eran Malach and Shai Shalev-Shwartz},
  year={2018},
  eprint={1706.02613},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}
@misc{staudemeyer2019understanding,
      title={Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks}, 
      author={Ralf C. Staudemeyer and Eric Rothstein Morris},
      year={2019},
      eprint={1909.09586},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{oshea2015introduction,
      title={An Introduction to Convolutional Neural Networks}, 
      author={Keiron O'Shea and Ryan Nash},
      year={2015},
      eprint={1511.08458},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{ms-marco,
  author    = {Tri Nguyen and
               Mir Rosenberg and
               Xia Song and
               Jianfeng Gao and
               Saurabh Tiwary and
               Rangan Majumder and
               Li Deng},
  title     = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  journal   = {CoRR},
  volume    = {abs/1611.09268},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09268},
  archivePrefix = {arXiv},
  eprint    = {1611.09268},
  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{muennighoff2022mteb,
  doi = {10.48550/ARXIV.2210.07316},
  url = {https://arxiv.org/abs/2210.07316},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  title = {MTEB: Massive Text Embedding Benchmark},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2210.07316},  
  year = {2022}
}
@misc{joshi2017triviaqa,
      title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}, 
      author={Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
      year={2017},
      eprint={1705.03551},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{natural-questions,
  title   = {Natural Questions: a Benchmark for Question Answering Research},
  author  = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
  year    = {2019},
  journal = {Transactions of the Association of Computational Linguistics}
}
@misc{linkedin,
  author       = {Mansoor Iqbal},
  title        = {{LinkedIn} Usage and Revenue Statistics (2023)},
  howpublished = {online},
  year         = {2025},
  url          = {https://www.businessofapps.com/data/linkedin-statistics/},
  note         = {Accessed: 2025-02-02}
}
@misc{cheng2021fairfil,
      title={FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders}, 
      author={Pengyu Cheng and Weituo Hao and Siyang Yuan and Shijing Si and Lawrence Carin},
      year={2021},
      eprint={2103.06413},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{social-bias,
    title = "On Measuring Social Biases in Sentence Encoders",
    author = "May, Chandler  and
      Wang, Alex  and
      Bordia, Shikha  and
      Bowman, Samuel R.  and
      Rudinger, Rachel",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1063",
    doi = "10.18653/v1/N19-1063",
    pages = "622--628",
}
@inproceedings{gaci-etal-2022-debiasing,
    title = "Debiasing Pretrained Text Encoders by Paying Attention to Paying Attention",
    author = "Gaci, Yacine  and
      Benatallah, Boualem  and
      Casati, Fabio  and
      Benabdeslem, Khalid",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.651",
    doi = "10.18653/v1/2022.emnlp-main.651",
    pages = "9582--9602",
}
@inproceedings{jentzsch-turan-2022-gender,
    title = "Gender Bias in {BERT} - Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task",
    author = "Jentzsch, Sophie  and
      Turan, Cigdem",
    editor = "Hardmeier, Christian  and
      Basta, Christine  and
      Costa-juss{\`a}, Marta R.  and
      Stanovsky, Gabriel  and
      Gonen, Hila",
    booktitle = "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gebnlp-1.20",
    doi = "10.18653/v1/2022.gebnlp-1.20",
    pages = "184--199",
}

@InProceedings{pmlr-v97-brunet19a,
  title = 	 {Understanding the Origins of Bias in Word Embeddings},
  author =       {Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson, Ashton and Zemel, Richard},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {803--811},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/brunet19a/brunet19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/brunet19a.html},
}
@inproceedings{Caliskan_2022,
   series={AIES ’22},
   title={Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics},
   url={http://dx.doi.org/10.1145/3514094.3534162},
   DOI={10.1145/3514094.3534162},
   booktitle={Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
   publisher={ACM},
   author={Caliskan, Aylin and Ajay, Pimparkar Parth and Charlesworth, Tessa and Wolfe, Robert and Banaji, Mahzarin R.},
   year={2022},
   month=jul, collection={AIES ’22} 
}
@inproceedings{guo-etal-2022-auto,
    title = "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
    author = "Guo, Yue  and
      Yang, Yi  and
      Abbasi, Ahmed",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.72",
    doi = "10.18653/v1/2022.acl-long.72",
    pages = "1012--1023",
}
@misc{bolukbasi2016man,
      title={Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings}, 
      author={Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai},
      year={2016},
      eprint={1607.06520},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{schick-etal-2021-self,
    title = "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in {NLP}",
    author = {Schick, Timo  and
      Udupa, Sahana  and
      Sch{\"u}tze, Hinrich},
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.84",
    doi = "10.1162/tacl_a_00434",
    pages = "1408--1424",
}
@misc{adamw,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{Falcon_PyTorch_Lightning_2019,
  author = {Falcon, William and {The PyTorch Lightning team}},
  doi = {10.5281/zenodo.3828935},
  license = {Apache-2.0},
  month = mar,
  title = {{PyTorch Lightning}},
  url = {https://github.com/Lightning-AI/lightning},
  version = {1.4},
  year = {2019}
}
@misc{deepspeed,
    title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
    author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
    year={2020},
    eprint={1910.02054},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@inproceedings{bm25l,
    author = {Lv, Yuanhua and Zhai, ChengXiang},
    title = {When Documents Are Very Long, BM25 Fails!},
    year = {2011},
    isbn = {9781450307574},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2009916.2010070},
    doi = {10.1145/2009916.2010070},
    booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
    pages = {1103–1104},
    numpages = {2},
    keywords = {term frequency, very long documents, bm25, bm25l},
    location = {Beijing, China},
    series = {SIGIR '11}
}
@misc{rank_bm25,
  author = {Dorian Brown},
  title = {{Rank-BM25: A Collection of BM25 Algorithms in Python}},
  year = 2020,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.4520057},
  url = {https://doi.org/10.5281/zenodo.4520057}
}
@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{confit_v1,
    author={Xiao Yu and Jinzhong Zhang and Zhou Yu},
    title = {ConFit: Improving Resume-Job Matching using Data Augmentation and Contrastive Learning},
    year = {2024},
    isbn = {9798400705052},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3640457.3688108},
    doi = {10.1145/3640457.3688108},
    pages = {601–611},
    numpages = {11},
    keywords = {contrastive learning, data augmentation, person-job fit},
    location = {Bari, Italy},
    series = {RecSys '24}
}

@inproceedings{robinson2021contrastive,
  author    = {Joshua Robinson and Ching-Yao Chuang and Suvrit Sra and Stefanie Jegelka},
  title     = {Contrastive learning with hard negative samples},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021}
}

@inproceedings{kalantidis2020hard,
  author    = {Yannis Kalantidis and Mert Bulent Sariyildiz and Noe Pion and Philippe Weinzaepfel and Diane Larlus},
  title     = {Hard negative mixing for contrastive learning},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {21798--21809},
  year      = {2020}
}

@inproceedings{ge2021robust,
  author    = {Songwei Ge and Shlok Mishra and Chun-Liang Li and Haohan Wang and David Jacobs},
  title     = {Robust contrastive learning using negative samples with diminished semantics},
  booktitle = {Advances in Neural Information Processing Systems},
  volume    = {34},
  pages     = {27356--27368},
  year      = {2021}
}

@article{xia2021progcl,
  author    = {J. Xia and L. Wu and J. Chen and S. Z. Li},
  title     = {ProgCL: Rethinking hard negative mining in graph contrastive learning},
  journal   = {arXiv preprint arXiv:2110.02027},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.02027}
}

@inproceedings{gao-etal-2021-simcse,
    title = "{S}im{CSE}: Simple Contrastive Learning of Sentence Embeddings",
    author = "Gao, Tianyu  and
      Yao, Xingcheng  and
      Chen, Danqi",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.552",
    doi = "10.18653/v1/2021.emnlp-main.552",
    pages = "6894--6910",
    abstract = "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using {``}entailment{''} pairs as positives and {``}contradiction{''} pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3{\%} and 81.6{\%} Spearman{'}s correlation respectively, a 4.2{\%} and 2.2{\%} improvement compared to previous best results. We also show{---}both theoretically and empirically{---}that contrastive learning objective regularizes pre-trained embeddings{'} anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.",
}

@misc{günther2023jina,
      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}, 
      author={Michael Günther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},
      year={2023},
      eprint={2310.19923},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{hyde,
      title={Precise Zero-Shot Dense Retrieval without Relevance Labels}, 
      author={Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
      year={2022},
      eprint={2212.10496},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2212.10496}, 
}
@misc{hyqe,
      title={HyQE: Ranking Contexts with Hypothetical Query Embeddings}, 
      author={Weichao Zhou and Jiaxin Zhang and Hilaf Hasson and Anu Singh and Wenchao Li},
      year={2024},
      eprint={2410.15262},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.15262}, 
}
@article{mohr2024multi,
  title={Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings},
  author={Mohr, Isabelle and Krimmel, Markus and Sturua, Saba and Akram, Mohammad Kalim and Koukounas, Andreas and G{\"u}nther, Michael and Mastrapas, Georgios and Ravishankar, Vinit and Mart{\'\i}nez, Joan Fontanals and Wang, Feng and others},
  journal={arXiv preprint arXiv:2402.17016},
  year={2024}
}
@misc{yu2024teachinglanguagemodelsselfimprove,
      title={Teaching Language Models to Self-Improve through Interactive Demonstrations}, 
      author={Xiao Yu and Baolin Peng and Michel Galley and Jianfeng Gao and Zhou Yu},
      year={2024},
      eprint={2310.13522},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.13522}, 
}
@misc{rafailov2024directpreferenceoptimizationlanguage,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2024},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.18290}, 
}
@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}
@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}
@article{Zhao2024,
author = {Zhao, Wayne Xin and Liu, Jing and Ren, Ruiyang and Wen, Ji-Rong},
title = {Dense Text Retrieval Based on Pretrained Language Models: A Survey},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3637870},
doi = {10.1145/3637870},
abstract = {Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user’s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models&nbsp;(PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {89},
numpages = {60},
keywords = {Text retrieval, dense retrieval, pretrained language models}
}
@inproceedings{nguyen-etal-2023-passage,
    title = "Passage-based {BM}25 Hard Negatives: A Simple and Effective Negative Sampling Strategy For Dense Retrieval",
    author = "Nguyen, Thanh-Do  and
      Bui, Chi Minh  and
      Vuong, Thi-Hai-Yen  and
      Phan, Xuan-Hieu",
    editor = "Huang, Chu-Ren  and
      Harada, Yasunari  and
      Kim, Jong-Bok  and
      Chen, Si  and
      Hsu, Yu-Yin  and
      Chersoni, Emmanuele  and
      A, Pranav  and
      Zeng, Winnie Huiheng  and
      Peng, Bo  and
      Li, Yuxi  and
      Li, Junlin",
    booktitle = "Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation",
    month = dec,
    year = "2023",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.paclic-1.59/",
    pages = "591--599"
}

@inproceedings{yang2024trisampler,
  title={TriSampler: A Better Negative Sampling Principle for Dense Retrieval},
  author={Yang, Zhen and Shao, Zhou and Dong, Yuxiao and Tang, Jie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={8},
  pages={9269--9277},
  year={2024}
}

@inproceedings{zhou-etal-2022-simans,
    title = "{S}im{ANS}: Simple Ambiguous Negatives Sampling for Dense Text Retrieval",
    author = "Zhou, Kun  and
      Gong, Yeyun  and
      Liu, Xiao  and
      Zhao, Wayne Xin  and
      Shen, Yelong  and
      Dong, Anlei  and
      Lu, Jingwen  and
      Majumder, Rangan  and
      Wen, Ji-rong  and
      Duan, Nan",
    editor = "Li, Yunyao  and
      Lazaridou, Angeliki",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-industry.56/",
    doi = "10.18653/v1/2022.emnlp-industry.56",
    pages = "548--559",
    abstract = "Sampling proper negatives from a large document pool is vital to effectively train a dense retrieval model. However, existing negative sampling strategies suffer from the uninformative or false negative problem. In this work, we empirically show that according to the measured relevance scores, the negatives ranked around the positives are generally more informative and less likely to be false negatives. Intuitively, these negatives are not too hard (\textit{may be false negatives}) or too easy (\textit{uninformative}). They are the ambiguous negatives and need more attention during training.Thus, we propose a simple ambiguous negatives sampling method, SimANS, which incorporates a new sampling probability distribution to sample more ambiguous negatives.Extensive experiments on four public and one industry datasets show the effectiveness of our approach.We made the code and models publicly available in \url{https://github.com/microsoft/SimXNS}."
}

@misc{syneg,
      title={SyNeg: LLM-Driven Synthetic Hard-Negatives for Dense Retrieval}, 
      author={Xiaopeng Li and Xiangyang Li and Hao Zhang and Zhaocheng Du and Pengyue Jia and Yichao Wang and Xiangyu Zhao and Huifeng Guo and Ruiming Tang},
      year={2024},
      eprint={2412.17250},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2412.17250}, 
}

@inproceedings{
xiong2021approximate,
title={Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval},
author={Lee Xiong and Chenyan Xiong and Ye Li and Kwok-Fung Tang and Jialin Liu and Paul N. Bennett and Junaid Ahmed and Arnold Overwijk},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=zeFrfgyZln}
}
@inproceedings{GNNO,
author = {Fan, Lu and Pu, Jiashu and Zhang, Rongsheng and Wu, Xiao-Ming},
title = {Neighborhood-based Hard Negative Mining for Sequential Recommendation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591995},
doi = {10.1145/3539618.3591995},
abstract = {Negative sampling plays a crucial role in training successful sequential recommendation models. Instead of merely employing random negative sample selection, numerous strategies have been proposed to mine informative negative samples to enhance training and performance. However, few of these approaches utilize structural information. In this work, we observe that as training progresses, the distributions of node-pair similarities in different groups with varying degrees of neighborhood overlap change significantly, suggesting that item pairs in distinct groups may possess different negative relationships. Motivated by this observation, we propose a graph-based negative sampling approach based on neighborhood overlap (GNNO) to exploit structural information hidden in user behaviors for negative mining. GNNO first constructs a global weighted item transition graph using training sequences. Subsequently, it mines hard negative samples based on the degree of overlap with the target item on the graph. Furthermore, GNNO employs curriculum learning to control the hardness of negative samples, progressing from easy to difficult. Extensive experiments on three Amazon benchmarks demonstrate GNNO's effectiveness in consistently enhancing the performance of various state-of-the-art models and surpassing existing negative sampling strategies. The code will be released at https://github.com/floatSDSDS/GNNO.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2042–2046},
numpages = {5},
keywords = {graph mining, hard negative mining, sequential recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}
@inproceedings{huang-2020,
author = {Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
title = {Embedding-based Retrieval in Facebook Search},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403305},
doi = {10.1145/3394486.3403305},
abstract = {Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in web search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2553–2561},
numpages = {9},
keywords = {search, information retrieval, embedding, deep learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}
@inproceedings{zhan2021,
author = {Zhan, Jingtao and Mao, Jiaxin and Liu, Yiqun and Guo, Jiafeng and Zhang, Min and Ma, Shaoping},
title = {Optimizing Dense Retrieval Model Training with Hard Negatives},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462880},
doi = {10.1145/3404835.3462880},
abstract = {Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1503–1512},
numpages = {10},
keywords = {dense retrieval, neural ranking, representation learning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}
@article{query-expansion-survey,
author = {Carpineto, Claudio and Romano, Giovanni},
title = {A Survey of Automatic Query Expansion in Information Retrieval},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2071389.2071390},
doi = {10.1145/2071389.2071390},
abstract = {The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user’s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {1},
numpages = {50},
keywords = {word associations, search, query refinement, pseudo-relevance feedback, document ranking, Query expansion}
}
@inproceedings{llm-can-self-improve,
    title = "Large Language Models Can Self-Improve",
    author = "Huang, Jiaxin  and
      Gu, Shixiang  and
      Hou, Le  and
      Wu, Yuexin  and
      Wang, Xuezhi  and
      Yu, Hongkun  and
      Han, Jiawei",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.67",
    pages = "1051--1068",
}
@misc{moreira2024nvretrieverimprovingtextembedding,
      title={NV-Retriever: Improving text embedding models with effective hard-negative mining}, 
      author={Gabriel de Souza P. Moreira and Radek Osmulski and Mengyao Xu and Ronay Ak and Benedikt Schifferer and Even Oldridge},
      year={2024},
      eprint={2407.15831},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2407.15831}, 
}
@inproceedings{yang2018ensemble,
  title={Ensemble strategy for hard classifying samples in class-imbalanced data set},
  author={Yang, Yingze and Xiao, Pengcheng and Cheng, Yijun and Liu, Weirong and Huang, Zhiwu},
  booktitle={2018 IEEE International Conference on Big Data and Smart Computing (BigComp)},
  pages={170--175},
  year={2018},
  organization={IEEE}
}
@article{zhang2011robust,
  title={Robust ensemble learning for mining noisy data streams},
  author={Zhang, Peng and Zhu, Xingquan and Shi, Yong and Guo, Li and Wu, Xindong},
  journal={Decision Support Systems},
  volume={50},
  number={2},
  pages={469--479},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}