\section{Related Work}
\label{sec:Related Work}

\paragraph{Person-job fit systems}


% Early person-job fit systems that uses neural networks typically focus on architecture modification. These methods include ____, which explores
% architectures such as RNN, LSTM ____ and CNN ____.
% However, these are significantly outperformed by more recent transformer-based encoder models such as BERT ____.
% Recent deep learning methods include ____, which uses deep siamese network to learn an embedding space for resume/jobs, ____ which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and ____ which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
Early person-job fit systems that use neural networks typically focus on architecture designs. These methods include ____, which explores
architectures such as RNN, LSTM ____ and CNN ____.
However, these are significantly outperformed by more recent transformer-based methods such as ____, which focuses on small architecture or loss modifications.
For example, MV-CoN ____ uses a co-teaching network ____ to perform gradient updates based on model's confidence to data noises; InEXIT ____, which uses hierarchical attention to model resume-job interactions; and DPGNN ____, which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% Recent deep learning methods include ____, which uses deep siamese network to learn an embedding space for resume/jobs, ____ which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and ____ which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
% Recent work with public implementations includes MV-CoN ____, which uses a co-teaching network ____ to perform gradient updates based model's confidence to data noises; InEXIT ____, which uses hierarchical attention to model resume-job interactions; and DPGNN ____, which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
% However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% The former is unrealistic considering new job seekers/job positions frequently appear, and the latter is only applicable to specific recruitment platforms.
More recently, \confitold{} ____ focuses on applying dense retrieval techniques to person-job fit. Using contrastive learning with encoders such as E5 ____, ____ demonstrates its performance and flexibility by achieving the best scores in almost all ranking and classification tasks across two different person-job fit datasets.
In this work, we propose \HyReShort{} and \RunnerUpMiningShort{} to further enhance \framework{}, achieving an average absolute improvement of 13.8\% in recall and 17.5\% in nDCG across job ranking and resume-ranking tasks in two resume-job benchmarks.


\paragraph{Dense retrieval techniques} 
\framework{} benefits from dense retrieval techniques such as contrastive learning ____ and hard-negative mining.
Popular methods in text retrieval include BM25 ____, a keyword-based approach used as the baseline in many text ranking tasks ____, and dense retrieval methods such as ____, which uses contrastive learning with an encoder to obtain high-quality passage embeddings, and typically performs top-k search based on inner product.

To further improve retrieval results, researchers considered methods such as query/document expansions ____ and hard-negative mining ____.
Recent query/document expansion approaches include HyDE ____ and HyQE ____, which prompts an LLM to augment the original query or the passage to retrieve and is typically used \emph{without} finetuning (the LLM or the encoder model). Common hard-negative mining strategies often use BM25 to select top-k unlabeled/incorrect samples as hard negatives ____. Other methods include: ANCE ____ which samples negatives from the top retrieved documents asynchronously during training; SimANS ____ which samples hard negatives using a manually designed probability distribution; and more ____. \confitold{} ____ presents the first successful attempt to use contrastive learning for person-job fit.
We extend \confitold{} and adapt recent dense retrieval techniques to person-job fit by 1) sampling hard-negative resumes/jobs from a ``runner-up'' \emph{percentile ranges} to mitigate selecting false-negatives; and 2) few-shot prompting an LLM to augment a job posted used for later encoder \emph{training}.


% \#TODO the following is copy pasted
% \framework{} benefits from contrastive learning techniques, which have seen wide applications in many dense text retrieval and representation learning tasks ____.
% Given a query (e.g., user-generated question), an information retrieval system aims to find top-k relevant passages from a large reserve of candidate passages ____.
% Popular methods in information retrieval include BM25 ____, a keyword-based approach used as the baseline in many text ranking tasks ____, and dense retrieval methods such as ____, which uses contrastive learning to obtain high-quality passage embeddings and typically performs top-k search based on inner product. To our knowledge, \framework{} is the first attempt to use contrastive learning for person-job fit, achieving the best performances in almost all person-job ranking tasks across two different person-job fit datasets.
% \paragraph{Hard Negative Mining} 
% Hard negatives are crucial for learning robust representations. Common strategies often begin with random sampling ____ or in-batch negatives ____. In person-job fit, \confitold{} used explicitly “rejected” resume-job pairs as difficult examples. Alternatively, BM25-based methods retrieve top-ranked candidates____, while PassageBM25 ____ refines this by comparing candidate passages to the positive passage rather than the query. Dynamic methods ____further refine the negative pool across training iterations. For example, ANCE ____ leverages a pretrained dense retriever to asynchronously update top-scoring negatives.Instead of strictly choosing top-ranked candidates, SimANS ____ designs a probability distribution to sample difficult negatives near the boundary. Meanwhile, TriSampler ____ applies a “quasi-triangular principle” to locate especially informative negatives. Recent work has also explored graph-based negative sampling____, leveraging structural information between items to improve sample selection. Methods like SyNeg____ employ LLMs to generate contextually coherent but factually incorrect samples.


% By contrast, our method first trains a dense retriever with \confitold{}’s hard negative approach, then uses it to score all resume-job pairs. We focus on the top 3–4\% non-matching “runner-ups,” striking a careful balance between being sufficiently challenging and avoiding excessive false negatives. This strategy refines the model’s ability to handle label-sparse scenarios more effectively.