% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
% \pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[]{acl}
\usepackage{xcolor}
% \usepackage{minted}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{placeins}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% CUSTOM PACKAGES
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage[nameinlink]{cleveref}
\usepackage{multirow}
\usepackage{arydshln}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{CJKutf8}  % to allow Chinese
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{fdsymbol}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\framework}{\textsc{ConFit v2}}
\newcommand{\confitold}{\textsc{ConFit}}
\newcommand{\confitsimple}{\textsc{ConFit}$^{*}$}
\newcommand{\RunnerUpMining}{\textsc{Runner-Up Mining}}
\newcommand{\RunnerUpMiningShort}{\textsc{RUM}}
\newcommand{\HyRe}{\textsc{Hypothetical Resume Embedding}}
\newcommand{\second}[1]{\textcolor{gray}{\emph{#1}}}
\newcommand{\HyReShort}{\textsc{HyRe}}
\newcommand{\bestunderline}[1]{{\textbf{#1}}}
\newcommand{\chinese}[1]{\begin{CJK}{UTF8}{gbsn}#1\end{CJK}}


\title{\framework{}: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Xiao Yu\thanks{denotes equal contribution}$^{\spadesuit}$
        Ruize Xu$^{*\spadesuit}$
        Chengyuan Xue$^{*\vardiamondsuit}$
        Jinzhong Zhang$^{\clubsuit}$\label{author:intellipro}
        Xu Ma$^{\clubsuit}$\label{author:intellipro}
        Zhou Yu$^\spadesuit$~~~\\[3pt]
  $^\spadesuit$Columbia University~ %, New York, NY \\
  $^\vardiamondsuit$University of Toronto~
  $^\clubsuit$Intellipro Group Inc.\\[3pt] % \\
  \texttt{\{xy2437,zy2461\}@columbia.edu}\\
  \texttt{\{jinzhong\}@intelliprogroup.com}
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle

\begin{abstract}
  A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts.
  However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse.
  We introduce \framework{}, an improvement over \confitold{} to tackle this sparsity problem.
  We propose two techniques to enhance the encoder’s contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy.
  % We then train an encoder using contrastive learning, 
 %  We propose \framework{}, an improvement over \confitold{} to tackle this sparsity problem by proposing two techniques.
 % A novel hard-negative mining strategy to automatically extract high-quality hard negatives to reduce the label sparsity issue, and improve encoder training results. Also, a finetuned language model to generate hypothetical reference resumes to add synthesized positive data. This method also simplifies the representation space of the encoder.
 We evaluate \framework{} on two real-world datasets and demonstrate that it outperforms \confitold{} and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average \emph{absolute} improvement of 13.8\% in recall and 17.5\% in nDCG across job-ranking and resume-ranking tasks.
  % We evaluate \framework{} on two real-world datasets and show that it outperforms \confitold{} and other prior work (including BM25 and OpenAI text-embedding-003) by on average 14.5\% and 17.5\% \emph{absolute} in nDCG in recall, respectively, across ranking jobs and ranking resumes tasks from two real-life datasets.
 
  % Different from many prior work that use complex modeling techniques, we tackle this sparsity problem using data augmentations and a simple contrastive learning approach.
  % \framework{} first formulates resume-job datasets as a sparse bipartite graph, and creates an augmented dataset by paraphrasing specific sections in a resume or a job post.
  % Then, \framework{} finetunes pre-trained encoders with contrastive learning to further increase training samples from $B$ pairs per batch to $\mathcal{O}(B^2)$ per batch.
  % We evaluate \framework{} on two real-world datasets and find it outperforms prior methods (including BM25 and OpenAI text-ada-002) by up to 19\% and 31\% absolute in nDCG@10 for ranking jobs and ranking resumes, respectively. We believe \framework{}'s simple yet highly performant approach lays a strong foundation for future research in modeling person-job fit.
\end{abstract}


\section{Introduction}
Online recruitment platforms like LinkedIn serve over 990 million users and 65 million businesses, processing more than 100 million job applications each month \cite{linkedin}. As these platforms continue to grow, there is a rising need for \emph{efficient} and \emph{robust} resume-job matching systems.
A practical system that reliably identifies suitable candidates/jobs from large pools will save considerable effort for both employers and job seekers.


Since both resume and job posts are often stored as text data, there has been an increased interest in using transformer models to model resume-job fit (or referred to as ``person-job fit'').
Many prior works \cite{pjfnn,APJFNN,mvcon,DPGNN,InEXIT} focus on designing complex modeling techniques to model resume-job matching.
For example, APJFNN \cite{APJFNN} uses hierarchical recurrent neural networks to process the job and resume content, and DPGNN \cite{DPGNN} uses a dual-perspective graph neural network to model the relationship between resumes and jobs.
Unlike these manual model/feature engineering methods, more recently \citet{confit_v1} shows that dense retrieval techniques like contrastive learning achieve significant improvements without relying on any complex designs.
% In contrast to these manual model/feature engineering approaches, more recently \citet{confit_v1} shows that significant improvements can be achieved by using dense retrieval techniques such as contrastive learning, without the need for complex model architectures.
By introducing dense retrieval methods to resume-job matching, \confitold{} \cite{confit_v1} presented a simple yet effective method to rank thousands of resumes/jobs in milliseconds, by combining training neural encoders such as E5 \cite{E5} with inner-product search algorithms such as FAISS \cite{FAISS}.

\input{floats/confit_v1_vs_v2}
We propose \framework{}, an enhanced baseline that achieves an average absolute improvement of 13.8\% in recall and 17.5\% in nDCG on ranking resumes and jobs compared to \confitold{} (see \Cref{fig:confit_v1_vs_v2}).
We introduce two key improvements to enhance the neural encoder's training process: 
1) \underline{Hy}pothetical \underline{R}esume \underline{E}mbedding (\HyReShort{}) that leverages a large language model (LLM) to generate a hypothetical resume and augment the job post (see \Cref{fig:inference}), providing implicit details to reduce the burden on encoder training; and
% augment job post with 
% resume-to-job matching to \emph{resume-to-reference-resume} matching, where the reference resume is generated by the LLM (see \Cref{fig:inference}).
2) \underline{R}unner-\underline{U}p \underline{M}ining (\RunnerUpMiningShort{}) that mines a large set of high-quality hard negatives to help the encoder better discern positive with near-positive samples.
% leverages a fine-tuned LLM to simplify the representation space by transforming resume-to-job matching into resume-to-ideal-resume ranking, where the ideal resume is generated based on the job post.
% 2) Hypothetical Resume Embedding (\HyReShort{}), which utilizes a finetuned large language model (LLM) to simplify the encoder's representation space from resume-to-job to resume-to-ideal-resume matching, by ranking an ``ideal'' resume generated by the decoder given a job post (see Figure \#TODO).
% and 2) Hypothetical Resume Embedding (\HyReShort{}), which utilizes an additional (finetuned) decoder model to generate an ``ideal'' resume given a job post, and simplifies the representation learning process of the encoder from resume-to-job to resume-to-ideal-resume matching (Figure \#TODO).
We train \framework{} using \HyReShort{} and \RunnerUpMiningShort{} on two real-world datasets, and show that \framework{} achieves new state-of-the-art performance on ranking resumes and jobs, outperforming \confitold{} and other prior work (including BM25 and OpenAI text-embedding-003-large).
We will open-source our code and data (under license agreements) to provide a strong baseline for future research in empowering resume-job matching systems with dense retrieval techniques.
% In this paper, we propose ConFit v2, an enhanced framework that directly addresses these limitations. It introduces a RUM, a hard negative mining strategy that extracts a larger set of high-quality hard negatives, even when labels are sparse. ConFit v2 learns finer distinctions and further improves performance by focusing on challenging near-positive samples. Our experiments show notable gains on two real-world datasets, outperforms previous approaches(including ConFit v1) by up to $20\%$ in ranking metrics.
\input{floats/inferece}

\section{Background}
\label{sec:Background}
% symbols
A resume-job matching (or often called \emph{person-job fit}) system models the compatibility between a resume $R$ and a job post $J$, allowing the systems to recommend the most suitable candidates for a given job, or suggest the most relevant jobs for a given candidate \cite{mvcon,DPGNN,InEXIT,confit_v1}.
Since resumes and job posts are often stored as text data, many prior works consider using neural networks (e.g., encoders) to quantify the compatibility between resumes and jobs:
\[
\textrm{score}(R, J) = f_\theta(R, J) \to \mathbb{R},
\] 
where $f_\theta$ could be directly modeled by a neural network \cite{pjfnn,DPGNN,InEXIT}, or by computing inner product/cosine similarity \cite{confit_v1} between a resume embedding $f_\theta(R)$ and a job embedding $f_\theta(J)$.

% sparsity issue. How it limits hard negative samples. Representation issues, how it limits performance of resume - job matching.
Despite the rapid growth of online recruitment platforms and the increasing availability of job posts and resume data, there are \emph{very few interaction labels} between any resumes and job posts \cite{mvcon,confit_v1}.
This is because a candidate usually applies to only a few positions, and a job interviewer often reviews only a few resumes for a given job post.
Often, the resulting dataset $\mathcal{D}= \{ R_i, J_i, y_i \}$ has size $|\mathcal{D}| \ll n_{R} \times n_{J}$, where $n_{R}$ and $n_{J}$ are the total number of resumes and jobs respectively, and $y_{i} \in \{0,1\}$ is a \emph{binary} signal representing whether a resume $R_i$ is short listed for a job $J_i$.
For example, in both the Recruiting and AliYun datasets (\Cref{subsec:Dataset and Preprocessing}) used in this work, \textbf{less than 0.05\%} of the total possible (resume, job) pairs are annotated.
This label sparsity poses challenges in: 1) crafting high-quality training data/hard negatives for training a neural encoder $f_\theta$; and 2) learning a representation space that generalizes well across diverse resumes and job posts.

% \subsection{Hard Negative Mining}
% Hard negatives are non-matching examples that closely resemble a positive sample in semantic or representation space. They provide much richer information for contrastive learning than “easy” negatives, which are far apart from the positive samples. Many prior studies have shown that focusing on these challenging negatives significantly improves feature discrimination and accelerates model convergence.

% A common strategy is to pick the top-most similar negative samples as hard negatives. However, in resume-job matching under label sparsity, the most similar negatives might be false negatives. Candidates often do not apply to all matching positions, so a seemingly negative pair could actually be a good fit that never appeared in the dataset. This raises concerns about injecting noisy examples and degrading training quality. It calls for a refined approach to selecting hard negatives, one that balances difficulty with a lower risk of mislabeling.

% \subsection{Contrastive Learning}
% \label{subsec:Contrastive Learning}
% Contrastive learning maximizes the distinction between positive and negative examples. Contrastive learning frameworks can produce embedding spaces that preserve semantic closeness and yield strong discriminative power. This approach helps mitigate overfitting in supervised tasks, particularly when labeled data is limited.

% In resume-job matching, data sparsity is a major concern. As each candidate usually applies to only a few positions, the number of labeled resume-job pairs is relatively small. Contrastive learning transforms each batch of B labeled pairs into O(B²) pairs by treating all other resumes (or jobs) in the batch as negatives. Our prior work, ConFit v1, leveraged this mechanism to boost ranking performance under sparse labels. However, the performance of contrastive learning crucially depends on the quality and quantity of negative samples.


\section{Approach}
\label{sec:ConFit}

We propose \framework{}, a simple and general-purpose approach that improves \confitold{} \cite{confit_v1} to model resume-job compatibility.
Similar to \confitold{}, we use an encoder to produce an embedding of a given resume or job post, and model the matching score between an $\langle R,J \rangle$ pair as the cosine similarity of their representations.
Unlike \confitold{}, \framework{} uses a simpler encoder architecture, and substantially improves ranking performance by using 1) hypothetical reference resume generated by an LLM to help improve the encoder's job data understanding; and 2) a novel hard-negative mining method to enhance encoder contrastive learning.
We provide a high-level overview of \framework{} during inference in \Cref{fig:inference}.
We detail each modification below.

\subsection{Encoder Architecture}
\label{subsec:Model Architecture}
Many prior works on resume-job matching employ complex neural architectures to encode human-designed matching heuristics \cite{pjfnn,DPGNN}.
For example, \citet{InEXIT,confit_v1} uses a hierarchical attention mechanism to model interactions between each \emph{section} (e.g., Education, Experience, etc.) of a resume and a job post.
However, in practice, these modifications often rely on specific domain knowledge and require additional structural constraints on resume/job data, restricting their generalizability.

Different from these methods, we treat the resume/job post as \emph{a single sequence of text}, and directly use a transformer-based encoder to produce the embedding of the entire document.
This allows the encoder model itself to learn the most relevant features from the data, instead of relying on human-designed heuristics.
We illustrate this difference in \Cref{fig:archi_diff}.
In practice, we find this simpler design is more effective and robust across different datasets and backbones (\Cref{subsec:Ablation Studies}).
We denote this simplified encoder as \confitsimple{}.

% Different from \confitold{} that adopts interaction layers fusing extracted features of separate sections, we simply encode the entire documents for resume and jobs, on which we perform direct contrastive learning on top of advanced backbones with long-context abilities, such as Jina V2 \cite{günther2023jina}. We prove by experiments that our simpler model architectures allows learned representation to be more discriminative and improves the matching performance. 

% \subsection{Contrastive Learning}
% \label{subsec:Contrastive Learning}
% XXX

\subsection{\HyRe{}}
\label{subsec:Hypothetical Resume Embedding}
In traditional dense passage retrieval tasks, one challenge is the discrepancy between query and passage formats and content \cite{hyde,hyqe}.
To address this, \citet{hyde} shows that zero-shot passage ranking improves significantly with Hypothetical Document Embeddings (HyDe), where an LLM generates a hypothetical passage from a query, and the encoder ranks actual passages based on their cosine similarity to the hypothetical one.
% To address this, \citet{hyde} demonstrates that zero-shot passage ranking improves significantly using Hypothetical Document Embeddings (HyDe), where an LLM generates a hypothetical passage from a query, and the encoder ranks actual passages by their cosine similarity to the hypothetical one.
We observe a similar discrepancy in resume-job matching, and find a substantial performance gain when high-quality (real) resumes are used in place of jobs during ranking. See \Cref{sec:Additional Results on HyRe} for more details. 

% However, high-quality reference resumes are often not readily available for every job post, and that prompting an LLM \cite{hyde,hyqe} to generate hypothetical resumes can frequently result in poor-quality samples due to lack of domain specific knowledge.
% We thus finetuned the LLM to generate hypothetical reference resumes given a job post, and use these hypothetical resumes during both encoder training and testing.
Given a dataset of accepted/rejected resume-job pairs, one extension to systems like HyDe is to finetune an LLM such as LLaMA-3 \cite{grattafiori2024llama3herdmodels} or Qwen-2.5 \cite{qwen2.5} using SFT and DPO \cite{rafailov2024directpreferenceoptimizationlanguage}. However, in our prior study, we find fine-tuning these models yields inferior results (see \Cref{subsec:Finetuning HyRe}) to few-shot prompting powerful closed-source models such as GPT-4o-mini \cite{4omini}.
We believe this is because accepting a resume can be subjective, making it hard for an LLM to learn an ``ideal candidate'' from the dataset.
% Specifically,
We thus construct hypothetical references by \emph{augmenting the existing job posts}. Specifically, we 1) fix $N$ accepted resume-job pairs from the training set as few-shot examples; 2) prompt GPT-4o-mini to generate a reference resume given a job post; and 3) concatenate the generated resume with the real job post. We repeat this for all jobs used during training and testing.

\subsection{\RunnerUpMining{}}
\label{subsec:Hard Negative Mining}
To accurately model resume-job compatibility, the encoder needs to distinguish between matching and near-matching resume-job pairs.
% Given a job, \confitold{} \cite{confit_v1} achieves state-of-the-art performance by training the encoder with contrastive loss, using 1) random resumes as in-batch negatives and 2) rejected resumes as hard negatives.
\confitold{} \cite{confit_v1} achieves state-of-the-art performance by training the encoder with contrastive loss, where given a job the model uses 1) random resumes as in-batch negatives and 2) rejected resumes as hard negatives, and vice versa for resumes. However, due to label scarcity (\Cref{sec:Background}), the quantity of rejected resumes/hard negatives is highly limited.

% \confitold{} \cite{confit_v1} uses rejected resume-job pairs as hard negatives during contrastive learning and finds improvement in performance when more hard negatives are used during training.
% However, as resume-job annotations are naturally sparse (\Cref{subsec:Contrastive Learning}), the quantity of rejected pairs is still limited.

We propose \RunnerUpMining{} (\RunnerUpMiningShort{}), a new hard-negative mining method that selects “runner-up” resume-job pairs as hard negatives based on compatibility scores from an encoder model.
In dense text retrieval tasks, many prior hard-negative mining methods that treat \emph{unlabeled or incorrect top-k results} (e.g., using BM25) as hard \emph{negatives} \cite{xiong2021approximate,Zhao2024}.
However, in resume-job matching, we find many of these top-k results are \emph{positive} pairs that are unlabeled simply because the candidate did not apply to the job\footnote{For example, he/she may be overqualified or may have already accepted another offer.}.
To avoid these top-k false negatives, we instead take ``runner-up'' (e.g., top 3\%-4\%) samples based on the cosine similarity scores, and use them as hard negatives for training.
% This is because, unlike traditional text retrieval datasets where unlabeled top-k results (e.g., using BM25) are often considered as \emph{negatives}, in resume-job matching, some of these top-k results may be \emph{positive} pairs. These data are unlabeled simply because the candidate did not apply to the job, as he/she may be overqualified or may have already accepted another offer.
% To address this we instead take ``runner-up'' samples from the top $3\%-4\%$ based on the cosine similarity scores, and use them as hard negatives during training.


% Unlike traditional methods that simply pick the top-most similar negatives, we must also address the risk of false negatives under label sparsity. In resume-job matching, many “negative” pairs are unlabeled rather than truly mismatched, because candidates often do not apply to all positions they are qualified for. We consider samples with  top $3\%-4\%$ matching scores but negative labels as hard negative samples. Hard negatives push the model to learn finer distinctions, improving its ability to differentiate subtle data variations.

Specifically, \RunnerUpMiningShort{} begins by computing the cosine similarity scores between all possible resume-job pairs using an encoder model $f_\mu$ (e.g., \confitsimple{}).
Then, we rank these pairs by similarity score and randomly sample resumes and jobs from the top percentile \emph{ranges} (e.g., top 3\%-4\%) as challenging hard negatives.
Finally, during contrastive training, we replace hard negatives used in \confitold{} (i.e., rejected resumes/jobs) with these hard negatives mined by \RunnerUpMiningShort{}.

% Specifically, \RunnerUpMiningShort{} begins by computing the cosine similarity scores between all possible resume-job pairs within the embedding space generated by \confitold{}. \confitold{} model already captures key features of resumes and jobs, which helps us better distinguish truly negative examples from potentially matching—but unlabeled—pairs. We then rank these pairs according to their similarity scores and select hard negatives from the top percentile ranges (e.g., top $3\%-4\%$). This percentile range consistently strikes a balance, it targets challenging negative pairs without introducing excessive label noise. In practice, we find that focusing on this range yields robust performance gains across different backbones and datasets.
%We focus on this range rather than the absolute highest scores, providing a balance that pushes the model to learn subtle distinctions effectively. These high-similarity non-matching pairs act as difficult samples that challenge the model to distinguish subtle differences between matching and non-matching resumes and job postings.

\subsection{\framework{}}
\label{subsec:Confit_overall}
% In Figure X, we summarize \framework{} approach.
We summarize our contribution in \framework{}. First, we simplified the encoder architecture used by \confitold{}, so that complex dynamics between different resumes and job posts are learned directly from the data (i.e., \confitsimple{}, \Cref{subsec:Model Architecture}).
Then, we improve the encoder training process by using 1) \HyReShort{} to generate pivot hypothetical resumes to simplify the job post representation space during both training and testing; and 2) \RunnerUpMiningShort{} to mine high-quality hard negatives for more effective contrastive training.
We illustrate \framework{}'s inference process in \Cref{fig:inference}.
% We propose \framework{}, an improved resume-job matching system that uses 1) a simpler encoder architecture compared to \confitold, 2) \RunnerUpMiningShort{} to mine high-quality hard negatives for encoder contrastive training, and 3) \HyReShort{} to generate pivot hypothetical resumes to simplify the representation space of the encoder during training and testing.
Given a job post, \framework{} converts it into an embedding by first using an LLM to generate a hypothetical resume, and then using the encoder to produce a job embedding using the concatenation of the generated resume and the original job post. Given a resume, \framework{} directly uses the encoder to produce the resume embedding. The compatibility score between a resume and a job is then computed as the cosine similarity between their embeddings.

% To train \framework{} encoder, we first use \RunnerUpMiningShort{} to mine hard negative resumes/jobs (\Cref{subsec:Hard Negative Mining}) for later contrastive learning. Then, we use \HyReShort{} to augment all job posts with generated reference resumes \Cref{subsec:Hypothetical Resume Embedding}.
To train \framework{} encoder, we use \HyReShort{} to augment all job posts with generated reference resumes (\Cref{subsec:Hypothetical Resume Embedding}), and replace them with original job posts for later training and testing.
Then, we use \RunnerUpMiningShort{} to mine hard-negative resumes and (augmented) jobs (\Cref{subsec:Hard Negative Mining}).
Finally, we train the simplified \confitsimple{} using the modified contrastive learning loss $\mathcal{L}$ from \citet{confit_v1}:
\begin{align}
  &\qquad\qquad \mathcal{L} = \mathcal{L}_{R} + \mathcal{L}_{J} \label{eq:contrastive_loss} \\
  \mathcal{L}_{R} = &-\log \frac{e^{f_{\theta}(R_i^{+},J_i^{+})}}{e^{f_{\theta}(R_i^{+},J_i^{+})} + \sum_{j=1}^{l} e^{f_{\theta}(R_i^{+},J_{i,j}^{-})}} \nonumber \\
  \mathcal{L}_{J} = &-\log \frac{e^{f_{\theta}(R_i^{+},J_i^{+})}}{e^{f_{\theta}(R_i^{+},J_i^{+})} + \sum_{j=1}^{l} e^{f_{\theta}(R_{i,j}^{-},J_i^{+})}} \nonumber
\end{align}
where $\mathcal{L}_R$ and $\mathcal{L}_J$ is the resume/job contrastive loss, respectively; $(R^+, J^+)$ denotes accepted resume-job pairs; $(R^-, J^+)$ or $(R^+, J^-)$ denote negative resume-job pairs including both in-batch negatives and hard negatives; and $f_\theta$ is cosine similarity after embedding the resume-job pairs.
% We train all components of \framework{} independently. We first train the \HyReShort{} LLM using \#TODO (\Cref{subsec:Hypothetical Resume Embedding}). Then, we use \RunnerUpMiningShort{} to mine hard negative resumes/jobs (\Cref{subsec:Hard Negative Mining}). Finally, we train the encoder by converting all job posts (including the mined hard negatives) into hypothetical resumes using the LLM, and then training the encoder using contrastive learning.

% To address the label sparsity problem in person-job fit datasets, \framework{} XXXX

\input{floats/main_exp_small}
\input{floats/main_exp_big}

\section{Experiments}
\label{sec:Experiments}
We evaluate \framework{} on two real-world person-job fit datasets, and measure its performance on ranking resumes and ranking jobs.

\subsection{Dataset and Preprocessing}
\label{subsec:Dataset and Preprocessing}

\paragraph{AliYun Dataset} To our knowledge, the 2019 Alibaba job-resume intelligent matching competition\footnote{
\href{https://tianchi.aliyun.com/competition/entrance/231728/introduction}{https://tianchi.aliyun.com/competition/entrance/231728}
} \emph{provided} the only publicly available person-job fit dataset.
All resume and job posts were desensitized and were processed into a collection of text fields, such as ``Education'' and ``Work Experiences'' for a resume (see \Cref{sec:More Details on Dataset and Preprocessing} for more details). All resumes and jobs are in Chinese.

\paragraph{Recruiting Dataset} The resumes and job posts are provided by a hiring solution company. To protect the privacy of candidates, all records have been anonymized by removing sensitive identity information (see \Cref{sec:More Details on Dataset and Preprocessing}).
For each resume-job pair, we record whether the candidate is accepted ($y=1$) or rejected ($y=0$) for an interview.
Similar to the AliYun dataset, all resumes and jobs were available as a collection of sections/fields. Both English and Chinese resumes and jobs are included.

\subsection{Implementation Details}
\label{subsec:Implementation Details}
% We follow \confitold{} \cite{confit_v1} and experiment with two encoders architectures. We consider E5-base \cite{E5}, and Jina-v2-base\footnote{We used ``jinaai/jina-embeddings-v2-base-zh''.} 
We follow \confitold{} \cite{confit_v1} and experiment with two encoder architectures. We consider E5-base \cite{E5}, and Jina-v2-base.
\cite{mohr2024multi}. Both encoders are trained on large-scale Chinese-English bilingual text data, and were amongst the best embedding models according to benchmarks such as MTEB \cite{muennighoff2022mteb} at the time of the project.
% suitable for further finetuning on multilingual resumes and job posts.
For \HyReShort{}, we use GPT-4o-mini as the LLM for generating hypothetical resumes due to its cost-effectiveness.
We provide the prompts used in \Cref{subsec:HyRe Prompts}.
We then train \confitsimple{} with \HyReShort{} using contrastive learning (without hard negatives), and use to mine hard negatives with \RunnerUpMiningShort{}.
For \RunnerUpMiningShort{}, we use the top 3\%-4\% percentile to pick hard-negatvies \emph{across all experiments}.
% For \RunnerUpMining{}, we use \confitsimple{}+\hyre as the encoder model for mining hard negatives, and use the top 3\%-4\% of resumes as hard negatives \emph{across all experiments}.
% For \HyReShort{}, we use Qwen-2.5-7B \cite{qwen2.5} as the LLM for generating hypothetical resumes.
% We train all models using a single A100 GPU.
Finally, we train \confitsimple{} again, using 1) \HyReShort{} augmented job data; and 2) \RunnerUpMiningShort{} mined hard negatives. We use Eq \eqref{eq:contrastive_loss} with a batch size of 4 and 2 hard negatives per batch, and a learning rate of 1e-5.
This is the final encoder used for \framework{}.
% We adopt the AdamW optimizer \cite{adamw} and a weight decay of $1e-2$, a linear warm-up schedule for the first $5\%$ of steps.
% We train the LLM using a learning rate of \#TODO.
We run all experiments using one A100 80GB GPU.
To speed up cosine similarity computation across a large pool of resumes and jobs, we use FAISS \cite{FAISS} throughout all experiments.
% We use jina-embeddings-v2 as our main backbone. This model is a Chinese/English bilingual encoder based which allowing up to 8192 tokens. 

% For training, we set batch size = 4 and hard negatives = 2. We adopt the AdamW optimizer and apply a linear warm-up schedule for the first $5\%$ of steps. We also use a weight decay of 1e-2 for both datasets. To manage large inputs and memory usage, we run training under DeepSpeed ZeRO 2 in BF16 mixed precision. Each ConFit model is trained for 6 epochs, and we select the best checkpoint based on validation loss before final testing.

% We train each model from scratch whenever new hard negatives are selected by our Runner-Up Mining module, rather than continuing from a previously trained checkpoint. This helps avoid carrying over biases from an earlier iteration of negative selection and ensures that each new set of hard negatives provides a fresh training signal.



\subsection{Baselines}
\label{subsec:Baselines}
We compare \framework{} against both recent best person-job fit systems and strong baselines from information retrieval systems.

Prior person-job fit systems include \emph{MV-CoN} \cite{mvcon}, \emph{InEXIT} \cite{InEXIT}, and \emph{ConFit} \cite{confit_v1} and more \cite{APJFNN,pjfnn,DPGNN}.
MV-CoN considers a co-teaching network \cite{han2018coteaching} to learn from sparse, noisy person-job fit data, and InEXIT uses hierarchical attention to model interactions between the text fields of a resume-job pair.
\confitold{} focuses on dense retrieval techniques, and uses contrastive learning and LLM-based data augmentation to train an encoder.
Other older methods such as APJFNN \cite{APJFNN}, PJFNN \cite{pjfnn}, and DPGNN \cite{DPGNN} are omitted since they are already outperformed by these more recent methods.
% Other older methods such as APJFNN \cite{APJFNN} and PJFNN \cite{pjfnn} are omitted since they were already outperformed by these more recent methods \cite{InEXIT,mvcon,DPGNN}.

% Recent person-job fit systems can be grouped into two categories: classification-targeted and ranking-targeted. The \textbf{best} classification-targeted system include \emph{MV-CoN} \cite{mvcon} and \emph{InEXIT} \cite{InEXIT}.
% MV-CoN considers a co-teaching network \cite{han2018coteaching} to learn from sparse, noisy person-job fit data, and InEXIT uses hierarchical attention to model interactions between the text fields of a resume-job pair.
% MV-CoN considers a co-teaching network \cite{han2018coteaching} to learn from sparse, noisy person-job fit data, and InEXIT uses hierarchical attention to model interactions between the text fields of a resume-job pair\footnote{
%   In our experiment, we find that \emph{InEXIT} performs slightly worse than MV-CoN on the AliYun dataset \Cref{tbl:main_exp_bert}. We believe this is because \emph{InEXIT} sampled a test set where part of the resumes/job posts are \emph{seen} during training. In contrast, in our experiment, we explicitly left out all resumes/job posts used in the test and validation set from training.
% }.
% Other older methods such as APJFNN \cite{APJFNN} and PJFNN \cite{pjfnn} are omitted since they were already outperformed by these more recent methods \cite{InEXIT,mvcon,DPGNN}.

We also compare against methods from information retrieval systems such as: \emph{BM25} \cite{bm25,bm25-all} and \emph{RawEmbed}. BM25 is a strong baseline used for many text ranking tasks \cite{thakur2021beir,kamalloo2023beir-resources}, and RawEmbed is based on dense retrieval methods \cite{DPR,FAISS} that directly uses a pre-trained encoder to produce dense embeddings for inner product scoring.
Since RawEmbed does not need training, we consider both open-source encoders such as E5 \cite{E5} and Jina-v2 \cite{mohr2024multi} and commercial models such as OpenAI's text-embedding-003-large \cite{text-ada}.

% Unless otherwise indicated, \framework{} first uses data augmentation with both EDA and ChatGPT, each augmenting 500 resumes and 500 jobs for each dataset (\Cref{subsec:Data Augmentation}), followed by contrastive learning with $B=8$ and $B_{\mathrm{hard}}=8$ (\Cref{subsec:Contrastive Learning}). See \Cref{sec:Training Hyperparameters} for other hyperparameters used by \framework{}, and see \Cref{sec:more_details_on_baselines} for more implementation details of the baselines.

\input{floats/iterative}
\input{floats/ablation}
\vspace{-0.2cm}
\subsection{Main Results}
\label{subsec:Main Results}
\Cref{tbl:main_exp_bert} summarizes \framework{}'s performance in comparison to other baselines on the Recruiting and AliYun datasets when E5-base is used as the encoder.
We report two ranking metrics, Recall@K and nDCG@K.
In general, \confitold{} shows substantial improvement over many prior methods, such as MV-CoN, InEXIT, and BM25.
\framework{} further improves \confitold{} by 17.1\% in recall and 20.4\% in nDCG score on average, outperforming all other baselines, including OpenAI's text-embedding-003-large, on both datasets.

\Cref{tbl:main_exp_e5} summarizes each method's performance when a different encoder (Jina-v2-base) is used.
Similar to \Cref{tbl:main_exp_bert}, \confitold{} shows impressive performance compared to many baselines, but is exceeded by OpenAI's text-embedding-003-large.
However, \framework{} outperforms all other baselines, by up to 10.6\% in recall and 14.5\% in nDCG score for ranking resumes and jobs on both datasets\footnote{We believe this is due to, despite Jina-v2's recency, \citet{günther2023jina} find E5 to be more robust for retrieval tasks.}.
We believe these results underscore the effectiveness of our method across different encoders and datasets.



% \Cref{tbl:main_exp_bert} summarizes \framework{}'s performance in comparison to other baselines, XXX, on the Recruiting and AliYun datasets when using a smaller encoder (E5-base, ~278M parameters). We report two ranking metrics, Recall@K and nDCG@K. In general, the earlier approaches—MV-CoN and InEXIT—struggle with ranking performance. ConFit v1 already demonstrates a substantial gain over these methods. Building on that, ConFit v2 (w/ hard negatives) achieves further improvement of up to $~20\%$ nDCG score for ranking resume and up to  $~25\%$ for ranking job on both Intelipro and AliYun datasets. %We attribute these gains to our Runner-Up Mining strategy and change in model architecture, which focuses the model on distinguishing subtle differences in semantically similar resume-job pairs.

% \Cref{tbl:main_exp_e5} summarizes each method's performance when a larger backbone encoder ($\sim$560M parameters) is used. 

% Finally, \Cref{tbl:exp_hard_neg} focuses on the Recruiting dataset under two test-set configurations: a random subset of 100 jobs and 1000 resumes (“rand”) and a hard subset of equal size (“hard”) determined by top BM25 scores. We use nDCG@10 to measure ranking effectiveness due to smaller test-set size. As expected, all methods perform worse on the hard subset, since high BM25 similarity indicates more confusable pairs. However, ConFit v2 still achieves the highest nDCG in both conditions. ConFit v2 manages to consistently rank relevant resumes and jobs more accurately, validating the necessity of a targeted hard-negative approach.
%
% 

% 
% 


% 

% 
%
\input{floats/error_analysis}
\subsection{Iterative \RunnerUpMiningShort{}}
Since \RunnerUpMiningShort{} only requires an encoder model to mine hard negatives, we also experiment with using \RunnerUpMiningShort{} iteratively to improve the model's performance.
For each iteration, we 1) use the encoder trained from the previous iteration to mine hard negatives using \RunnerUpMiningShort{}; and 2) train the backbone encoder again using the newly acquired hard negatives\footnote{
  We also experimented with continuously training the model from the previous iteration, but found that it can more easily lead to overfitting.
}.
Throughout all iterations, we keep all hyperparameters the same as in \Cref{subsec:Implementation Details}. We focus on the ranking resume task in the Recruiting dataset since it is the most challenging. 

We present the results in \Cref{fig:iterative}. In general, we find that training with more than one iteration ($t>1$) improves resume ranking performance compared to a single iteration ($t=1$). However, as the number of iterations increases, improvement begins to drop.
We believe this is similar to many model self-improvement research \cite{llm-can-self-improve,yu2024teachinglanguagemodelsselfimprove}. While model-created data often helps improve performance, these data also contains noises that can compound over multiple iterations.
We believe techniques such as model ensembles \cite{zhang2011robust,moreira2024nvretrieverimprovingtextembedding} could help mitigate this issue, and we leave this for future work.

% We can mention the gap between the number of resume and job(10w compare to 1w7), and use this to address the result? The resume part still shows improvement. And mention the possible improvement in model architecture at last(use separate encoder).

% Since our method RUM only requires a (trained) model to mine hard negatives, we also experiment with there our method generalizes to multiple iteration. The core idea is to iteratively refine the model’s view of difficult examples.

% We begin by using a pretrained or previously trained version of the model to produce the cosine scores for all resume-job pairs. From these scores, we select the top $3\%-4\%$ high-similarity negatives according to RUM’s criteria.

% Instead of continuing from the model’s weights in the previous step, we re-initialize the backbone and retrain it from scratch using the newly acquired hard negatives alongside the standard contrastive learning setup. This avoids potential overfitting or bias carried over from earlier training phases.

% We then use the newly trained model to repeat the negative selection, creating an updated pool of runner-up samples. The model is once again reset and retrained with these refreshed negatives.

% In our experiments on the Recruiting dataset, two rounds of iterative mining suffice to yield noticeable gains(Table 5), while additional rounds offer diminishing returns. We hypothesize that by the second iteration, the model has learned most of the subtle distinctions captured by these runner-up samples, converging toward an upper performance bound.

\subsection{Ablation Studies}
\label{subsec:Ablation Studies}

\Cref{tbl:ablation} presents our ablation studies for each component of \framework{}.
First, we consider only simplifying the encoder architecture (denoted as \emph{ConFit$^{*}$}), and use rejected resumes/jobs as hard negatives used by \confitold{} \cite{confit_v1}.
We find that this simplification significantly improves ranking performance.
We believe this is due to the strong performance of recent embedding models, which can effectively learn complex interactions between resumes and job posts directly from data.
Next, we use \HyReShort{} to generate reference resumes, augment job posts, and train \confitsimple{} on the augmented data (denoted as \emph{+\HyReShort{}}).
We find this to improve 2.9\% absolute in recall and 3.1\% nDCG scores on average.
Finally, we use \RunnerUpMiningShort{} to mine hard resume/(augmented) job negatives (denoted as \emph{+RUM}), and replace them with the rejected resume/jobs used by \confitold{}.
In \Cref{tbl:ablation}, we find that \RunnerUpMiningShort{} further improved by, on average, 1.2\% absolute in recall scores, and 1.8\% absolute in nDCG across both datasets.
We find that all components are important for \framework{}.
% Next, we use \RunnerUpMiningShort{} to mine hard resume/job negatives (denoted as \emph{+RUM}), and replace t
% hem with rejected resumes/jobs used by \confitold{}.
% in \Cref{tbl:ablation}, we find that \RunnerUpMiningShort{} consistently improved upon \confitsimple{} by, on average, 4.63\% absolute in nDCG scores, and 4.35\% absolute in recall across both datasets.
% This indicates that these mined hard-negatives better, near-positive samples than prior method \cite{confit_v1}, which improved the model's ability to distinguish between matching and non-matching resume-job pairs.
% Finally, we use \HyReShort{}

% Additionally, we also 
For other results, such as comparing \RunnerUpMiningShort{} against methods that use BM25, with different percentile ranges other than the top 3\%-4\% as well as testing \RunnerUpMiningShort{} against hard negatives mined by BM25, please refer to \Cref{sec:Additional Results on RUM}.
% For other results, such as using \RunnerUpMiningShort{} with different percentile ranges other than the top 3\%-4\% as well as testing \RunnerUpMiningShort{} against hard-negatives mined by BM25, please refer to \Cref{sec:Additional Results on RUM}.

% \Cref{tbl:ablation} presents our ablation studies for each component of \framework{} training. We use Jina-base as backbone as it support long text and multilingual. 

% Next, We use the proposed RUM hard negatives instead of rejected pairs as hard negatives. We find CONFIT v2 with \RunnerUpMiningShort{} hard negatives can acheieve the best performance for job and resume ranking on both Recruiting and AliYun datasets. For results on running \RunnerUpMiningShort{} with other percentile ranges, please refer to \# TODO.

% This indicates that the newly discovered “runner-ups” hard negatives provide critical distinguishing signals, driving more accurate ranking than relying solely on explicitly rejected resumes or jobs.

\section{Analysis}
In this section, we provide both qualitative and quantitative analysis on the ranking output produced by \framework{}. We mainly focus on the Recruiting dataset as it is more challenging.

\subsection{Error Analysis}
To analyze the errors made by \framework{}, we manually inspect 30 \emph{negative} resume-job pairs from the ranking tasks that are \emph{incorrectly ranked at top 5\%} and are before at least one positive pair.
For each incorrectly ranked pair, we compare against other positive resume-job pairs from the dataset, and categorize the errors following the criteria from \citet{confit_v1}: \emph{unsuitable}, where some requirements in the job post are not satisfied by the resume; \emph{less competent}, where a resume satisfies all job requirements, but many {competing candidates} have a higher degree/more experience; \emph{out-of-scope}, where a resume satisfies all requirements, appears competitive compared to other candidates, but is still rejected due to other (e.g., subjective) reasons not presented in our resume/job data themselves; and \emph{potentially suitable}, where a resume from the ranking tasks satisfied the requirements and seemed competent, but had no label in the original dataset.

We present our analysis in \Cref{fig:error_analysis}. We find a significant portion of the errors are \emph{out-of-scope}, where it is hard to determine the reason for rejection based on information in the resume/job data alone.
The next most frequent error is \emph{less competent}, which is understandable since \framework{} produces a compatibility score for a resume-job pair independent of other candidates.
Lastly, we also find that 16.7\% of the errors are \emph{unsuitable}, with resumes not fulfilling certain requirements such as ``Backend SWEs with \emph{Ruby} experience''.
We believe \emph{unsuitable} errors can be mitigated by combining \framework{} with keyword-based approaches (e.g., BM25 or human-designed rules). We leave this for future work.

\input{floats/gender}
\input{floats/dset_gender_dist}
\subsection{Bias Analysis}
\label{subsec:bias analysis}
\framework{} relies on pretrained encoders such as E5 and Jina-v2 \cite{devlin2019bert,E5}, and it is well-known that many powerful encoders can contain biases \cite{pmlr-v97-brunet19a,social-bias,jentzsch-turan-2022-gender,Caliskan_2022} such as gender, age, demographics, etc.
As a case study, we examine whether ranking outputs from \framework{} contain gender bias.
For this experiment, we add gender information\footnote{Since gender information is voluntarily submitted by the candidates, this affects around 15\% of all resumes.} back to our desensitized resumes, and compare 1) whether training on gender-included data increases gender disparities in the ranking output; and 2) whether \RunnerUpMiningShort{} has any effect on gender bias.

We present our analysis in \Cref{tbl:gender}. Across all experiments, we find adding gender information back into the training data increases gender disparities in the ranking output. ``\confitsimple{}'' trained with gender information widened the gender gap in the ranking output by $\sim$10\% compared to training without gender, and ``\confitsimple{}+\RunnerUpMiningShort{}'' widened the gap by $\sim$6\%.
In general, we believe one major cause of this disparity is the inherent biases in the datasets.
In \Cref{fig:dset_gender_dist}, we find gender distributions are uneven across different industries, and that overall, 72\% of the (accepted or rejected) resumes used during training are from male candidates, and only 28\% are from female candidates.


In \Cref{tbl:gender}, we also find that using \RunnerUpMiningShort{} helps reduce gender disparities compared to \confitsimple{} alone.
We believe this indicates \RunnerUpMiningShort{} provided challenging hard negatives that do \emph{not} always rely on gender information, hence reducing gender disparity during training. In general, we advise researchers and practitioners to be cautious when using \framework{}, and to use explicit de-biasing techniques \cite{bolukbasi2016man,cheng2021fairfil,gaci-etal-2022-debiasing,guo-etal-2022-auto} \emph{in addition to} data de-sensitization used in this work. We do not condone the use of \framework{} for any ethically unjust purposes.
%  the model away from relying on demographic shortcuts by exposing it to more challenging text-based negatives.

% Real-world recruitment often exhibits inherent biases, and these biases can manifest in the underlying data. Training models on such data may inadvertently reinforce demographic disparities. We examine whether including gender in the resume fields leads to more skewed ranking outcomes and whether our Runner-Up Mining (RUM) can mitigate some of these effects.

% As shown in Table \Cref{tbl:bias}, adding gender information to ConFit v2 (row 2) improves ranking metrics such as Recall and nDCG compared to its counterpart without gender (row 1). This result suggests that the model exploits demographic cues as easy signals, yielding a short-term performance boost. However, when we introduce RUM (rows 3 and 4), the gap largely disappears or even reverses. In other words, RUM nudges the model away from relying on demographic shortcuts by exposing it to more challenging text-based negatives.

% To further understand how gender affects the returned candidates, we aggregate all top-10 resumes for each job and measure the proportion of male and female from resumes that explicitly include gender. Table \ref{tbl:gender} shows that V2 (w/ gender info) surfaces significantly more gender-identified resumes than V2 (w/o gender info), alongside a larger gap between male and female percentages. In contrast, adding RUM narrows this gap in both the with-gender and without-gender settings, suggesting that challenging negative examples can reduce the model’s reliance on sensitive attributes.

% Nevertheless, our findings indicate that RUM alone does not eliminate bias when the original dataset encodes uneven hiring outcomes. Merely removing or masking sensitive fields may not suffice to ensure fair outcomes. Although our RUM approach shows promise in limiting the model’s reliance on gender cues, inherent biases in hiring data can still persist. Additional techniques, such as adversarial debiasing, selective data augmentation, or post-hoc calibration, may be needed to address deeper structural disparity.

\section{Related Work}
\label{sec:Related Work}

\paragraph{Person-job fit systems}


% Early person-job fit systems that uses neural networks typically focus on architecture modification. These methods include \citet{APJFNN,pjfnn,cnn-lstm-siamese,jiang2020learning,10169716}, which explores
% architectures such as RNN, LSTM \cite{staudemeyer2019understanding} and CNN \cite{oshea2015introduction}.
% However, these are significantly outperformed by more recent transformer-based encoder models such as BERT \cite{devlin2019bert}.
% Recent deep learning methods include \citet{siamese,cnn-lstm-siamese}, which uses deep siamese network to learn an embedding space for resume/jobs, \citet{bian-etal-2019-domain} which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and \citet{Zhang2023FedPJFFC} which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
Early person-job fit systems that use neural networks typically focus on architecture designs. These methods include \citet{APJFNN,pjfnn,cnn-lstm-siamese,jiang2020learning,10169716}, which explores
architectures such as RNN, LSTM \cite{staudemeyer2019understanding} and CNN \cite{oshea2015introduction}.
However, these are significantly outperformed by more recent transformer-based methods such as \citet{siamese,cnn-lstm-siamese,bian-etal-2019-domain,Zhang2023FedPJFFC}, which focuses on small architecture or loss modifications.
For example, MV-CoN \cite{mvcon} uses a co-teaching network \cite{malach2018decoupling} to perform gradient updates based on model's confidence to data noises; InEXIT \cite{InEXIT}, which uses hierarchical attention to model resume-job interactions; and DPGNN \cite{DPGNN}, which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% Recent deep learning methods include \citet{siamese,cnn-lstm-siamese}, which uses deep siamese network to learn an embedding space for resume/jobs, \citet{bian-etal-2019-domain} which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and \citet{Zhang2023FedPJFFC} which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
% Recent work with public implementations includes MV-CoN \cite{mvcon}, which uses a co-teaching network \cite{malach2018decoupling} to perform gradient updates based model's confidence to data noises; InEXIT \cite{InEXIT}, which uses hierarchical attention to model resume-job interactions; and DPGNN \cite{DPGNN}, which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
% However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% The former is unrealistic considering new job seekers/job positions frequently appear, and the latter is only applicable to specific recruitment platforms.
More recently, \confitold{} \cite{confit_v1} focuses on applying dense retrieval techniques to person-job fit. Using contrastive learning with encoders such as E5 \cite{E5}, \citet{confit_v1} demonstrates its performance and flexibility by achieving the best scores in almost all ranking and classification tasks across two different person-job fit datasets.
In this work, we propose \HyReShort{} and \RunnerUpMiningShort{} to further enhance \framework{}, achieving an average absolute improvement of 13.8\% in recall and 17.5\% in nDCG across job ranking and resume-ranking tasks in two resume-job benchmarks.


\paragraph{Dense retrieval techniques} 
\framework{} benefits from dense retrieval techniques such as contrastive learning \cite{SimCLR,radford2021learning} and hard-negative mining.
Popular methods in text retrieval include BM25 \cite{bm25,bm25-all}, a keyword-based approach used as the baseline in many text ranking tasks \cite{ms-marco,thakur2021beir,muennighoff2022mteb}, and dense retrieval methods such as \citet{DPR,izacard2021contriever,E5,günther2023jina}, which uses contrastive learning with an encoder to obtain high-quality passage embeddings, and typically performs top-k search based on inner product.

To further improve retrieval results, researchers considered methods such as query/document expansions \cite{query-expansion-survey} and hard-negative mining \cite{robinson2021contrastive}.
Recent query/document expansion approaches include HyDE \cite{hyde} and HyQE \cite{hyqe}, which prompts an LLM to augment the original query or the passage to retrieve and is typically used \emph{without} finetuning (the LLM or the encoder model). Common hard-negative mining strategies often use BM25 to select top-k unlabeled/incorrect samples as hard negatives \cite{DPR,Zhao2024,nguyen-etal-2023-passage}. Other methods include: ANCE \cite{xiong2021approximate} which samples negatives from the top retrieved documents asynchronously during training; SimANS \cite{zhou-etal-2022-simans} which samples hard negatives using a manually designed probability distribution; and more \cite{zhan2021,syneg}. \confitold{} \cite{confit_v1} presents the first successful attempt to use contrastive learning for person-job fit.
We extend \confitold{} and adapt recent dense retrieval techniques to person-job fit by 1) sampling hard-negative resumes/jobs from a ``runner-up'' \emph{percentile ranges} to mitigate selecting false-negatives; and 2) few-shot prompting an LLM to augment a job posted used for later encoder \emph{training}.


% \#TODO the following is copy pasted
% \framework{} benefits from contrastive learning techniques, which have seen wide applications in many dense text retrieval and representation learning tasks \cite{SimCLR,radford2021learning}.
% Given a query (e.g., user-generated question), an information retrieval system aims to find top-k relevant passages from a large reserve of candidate passages \cite{joshi2017triviaqa,natural-questions}.
% Popular methods in information retrieval include BM25 \cite{bm25,bm25-all}, a keyword-based approach used as the baseline in many text ranking tasks \cite{ms-marco,thakur2021beir,muennighoff2022mteb}, and dense retrieval methods such as \citet{DPR,izacard2021contriever,E5}, which uses contrastive learning to obtain high-quality passage embeddings and typically performs top-k search based on inner product. To our knowledge, \framework{} is the first attempt to use contrastive learning for person-job fit, achieving the best performances in almost all person-job ranking tasks across two different person-job fit datasets.
% \paragraph{Hard Negative Mining} 
% Hard negatives are crucial for learning robust representations. Common strategies often begin with random sampling \cite{huang-2020} or in-batch negatives \cite{DPR}. In person-job fit, \confitold{} used explicitly “rejected” resume-job pairs as difficult examples. Alternatively, BM25-based methods retrieve top-ranked candidates\cite{Zhao2024}, while PassageBM25 \cite{nguyen-etal-2023-passage} refines this by comparing candidate passages to the positive passage rather than the query. Dynamic methods \cite{zhan2021}further refine the negative pool across training iterations. For example, ANCE \cite{xiong2021approximate} leverages a pretrained dense retriever to asynchronously update top-scoring negatives.Instead of strictly choosing top-ranked candidates, SimANS \cite{zhou-etal-2022-simans} designs a probability distribution to sample difficult negatives near the boundary. Meanwhile, TriSampler \cite{yang2024trisampler} applies a “quasi-triangular principle” to locate especially informative negatives. Recent work has also explored graph-based negative sampling\cite{GNNO}, leveraging structural information between items to improve sample selection. Methods like SyNeg\cite{syneg} employ LLMs to generate contextually coherent but factually incorrect samples.


% By contrast, our method first trains a dense retriever with \confitold{}’s hard negative approach, then uses it to score all resume-job pairs. We focus on the top 3–4\% non-matching “runner-ups,” striking a careful balance between being sufficiently challenging and avoiding excessive false negatives. This strategy refines the model’s ability to handle label-sparse scenarios more effectively.


\section{Conclusion}
\label{sec:Conclusion}
We propose \framework{}, an improvement of \confitold{} to model person-job fit. Similar to \confitold{}, we model person-job fit using dense retrieval techniques such as contrastive learning. Unlike \confitold{}, we first simplified the encoder architecture, and improved encoder training using 1) hypothetical reference resume generated by an LLM to augment a job post; and 2) a new hard-negative mining strategy that selects ``runner-up'' resume-job pairs to avoid false negatives used for encoder training.
We evaluate \framework{} on two real-world datasets, and demonstrate that it outperforms \confitold{} and prior methods, achieving an average \emph{absolute} improvement of 13.8\% in recall and 17.5\% in nDCG across both ranking resume and ranking job tasks.
We believe our work lays a strong foundation for future resume-job matching systems to leverage the latest advancements in dense text retrieval.

% \input{floats/bias}

\section{Limitations}

\paragraph{Data Sensitivity} To our knowledge, there is no standardized, public person-job fit dataset\footnote{The AliYun dataset used in this work is no longer publicly available as of 09-11-2023.} that can be used to compare performances of existing systems \cite{pjfnn,APJFNN,mvcon,DPGNN,InEXIT}.
This is due to the highly sensitive nature of resume and job post content, making large-scale person-job fit datasets largely proprietary.
We follow \citet{confit_v1} and provide the best effort to make \framework{} reproducible and extensible for future work.
We will open-source implementations of \framework{}, related baselines, data processing scripts, and dummy train/valid/test data files that can be used to test drive our system end-to-end.  We will also privately release our model weights and full datasets to researchers under appropriate license agreements, aiming to make future research in person-job fit more accessible.

\paragraph{Local \HyReShort{}} 

During \HyReShort{}, we use a commercial LLM such as GPT-4o-mini to generate hypothetical reference resumes to augment our job data for retrieval.
% \framework{} uses \HyReShort{} to augment job data using hypothetical reference resume generated after prompting an LLM such as GPT-4o-mini.
Ideally, one would prefer using an in-house, local model to better retrain data privacy and optimize for inference speed.
However, in our prior study, we find fine-tuning open-source LLMs (up to 32B in size) for this task to be challenging, due to the scarce and subjective nature of the person-job fit labels (\Cref{subsec:Finetuning HyRe}).
We believe designing data/training algorithms tailored for this task could benefit person-job fit systems such as \framework{}, which we leave for future work.
% The use of LLM costs money, and during inference it adds overhead of additionally querying an LLM.

% \paragraph{Recruiter/Job Seeker Preference}


\section{Ethical Considerations}

\framework{} uses and trains pretrained encoders such as E5 \cite{E5} and Jina-v2 \cite{günther2023jina}, and it is well-known that these models contain biases including but not limited to gender, race, demographics, and more \cite{pmlr-v97-brunet19a,social-bias,jentzsch-turan-2022-gender,Caliskan_2022}.
For practical person-job fit systems, we believe it is crucial to ensure that they do \textbf{not} exhibit these biases, such as preferring a certain gender for certain jobs.
In this work, we have followed best practices from prior work \cite{DPGNN,confit_v1} to remove sensitive information (e.g., gender) from our data.
However, our bias analysis (\Cref{subsec:bias analysis}) reveals that gender disparities in the trained model could stem from imbalances in the dataset.
We do \textbf{not} condone using \framework{} for real-world applications without using de-biasing techniques such as \citet{bolukbasi2016man,cheng2021fairfil,gaci-etal-2022-debiasing,guo-etal-2022-auto,schick-etal-2021-self}, and in general, we do \textbf{not} condone the use of \framework{} for any morally unjust purposes.
To our knowledge, there is little work on investigating or mitigating biases in existing person-job fit systems, and we believe this is a crucial direction for future person-job research.

\bibliography{custom}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{A\arabic{figure}}
\appendix
% 
\clearpage

\section{More Details on Model Architecture}
\label{sec:more_details_on_model_architecture}

\Cref{fig:archi_diff} displays encoder model architecture used in \confitold{} and \confitsimple{}. We remove all the additional layers used after the backbone encoder, including the attention layers that interact among features of each field, and the MLP layers for feature fusion. Instead, we directly employ mean pooling on the embeddings of all the tokens in the texts to obtain document-level representation \cite{E5,günther2023jina}.
Then, we perform contrastive learning on the document-level embeddings.


We implement this with both encoders such as E5 \cite{E5} and long-context encoders such as Jina-v2 \cite{günther2023jina}. E5 supports 512 input sequence length and Jina-v2 supports 8192. We add $L_2$ normalization to the features, and use temperature $T$ to rescale the cosine similarity to facilitate training. For E5 we use $T=0.01$, and for Jina-v2 we use $T=0.05$. In \Cref{tbl:main_exp_bert} and \Cref{tbl:main_exp_e5}, we find that \confitsimple{} significantly improve upon \confitold{} across all settings.
% Following the training settings of Jina-v2, we add $L_2$ normalization to the features, and use temperature $T=0.05$ to rescale the cosine similarity to facilitate training.


% This is implemented on top of long-context encoder Jina V2 \cite{günther2023jina}, which supports inputs of at most 8192 sequence length. We then perform contrastive learning on the document-level embeddings. Following the training settings of Jina V2, we add $L_2$ normalization to the features, and use temperature $T=0.05$ to rescale the cosine similarity to facilitate training.

% \Cref{tbl:main_exp_bert} and \Cref{tbl:main_exp_e5} shows the effectiveness of our model design. In Table \ref{tbl:main_exp_e5}, with the same encoder E5 in \confitold{} that only supports 512 sequence length, our simplified architecture also improves the performance by a large margin. It demonstrates that our simplification is highly performant, when used with both short and long-context encoders.

\section{Additional Results on \HyReShort{}}
\label{sec:Additional Results on HyRe}


\subsection{\HyReShort{} Prompts}
\label{subsec:HyRe Prompts}




We use the prompt in Table \ref{tbl:prompt} to perform few-shot prompting for hypothetical resume generation. In the prompt, \textit{\{\{\{example job\}\}\}}, \textit{\{\{\{example resume\}\}\}} are randomly sampled positive pairs from the training set, and the \textit{\{\{\{target job\}\}\}} is the target job to be augmented. For the ablation study in \ref{subsec: Hypothetical job}, we swap the order of template job and resume, send in a resume as target, and revise the prompt correspondingly. When concatenating job and hypothetical resume, we add \textit{["An Example Resume"]} in between for clarity.



\subsection{\HyReShort{} Upperbound}
\label{subsec:HyRe Upperbound}
% In this section, we show the performance gap between \HyReShort{} and using real resumes.
In this section, we estimate the upper bound performance of \HyReShort{} by using real resumes from the dataset to augment each job during both training and testing.
However, evaluation based on training the entire dataset is time-consuming, and thus we trained on a randomly sampled subset containing 20\% of the training data while keeping all other settings identical to those used for training on the full dataset.

% and train \confitsimple{} to evaluate the upperbound performance of \HyReShort. We train on a randomly sampled subset containing 20\% of the training data while keeping all other settings identical to those used for training on the full dataset. The results are shown in Table \ref{tbl:Upperbound}.

We experiment with three strategies of selecting ``ideal'' reference resumes for each job from the dataset.
For \emph{max-match (human)}, we select the highest-scoring resume for each job according to additional annotations provided by a hiring solution company. 
For \emph{max-match (Jina)}, we select the highest-scoring resume that achieves high human annotation score \textbf{and} a high cosine similarity score computed directly by Jina-v2 \cite{günther2023jina}.
% For \emph{max-match (Jina)}, we use the raw embedding of Jina-v2 to calculate the cosine similarity of each pair as the criterion instead of human labels.
For \emph{centroid}, we select the centroid resume of all accepted resumes given a job post.
% For centroid strategy, we aim to find the centroid of all the resumes related to a job.
Since computing the centroid resume across the full resume pool is time-consuming, we instead restrict the pool to all \emph{labeled} resumes given the job post.
% The candidate pool consists of both accepted and rejected resumes, as the rejected resumes can also match the job. 
Then, we pick the resume, which has the shortest average $L_2$ normalized distance (i.e. cosine similarity) to its job's accepted resumes, as the centroid resume.

We present the results in Table \ref{tbl:Upperbound}. Compared to \confitsimple{}, all three strategies show that \HyReShort{} yields a high potential performance if the hypothetical resumes can truly approximate the real selected ones.
This indicates that \HyReShort{}, if implemented well, could significantly improve ranking job or ranking resume performance.


\begin{table}[!t]
\centering
\scalebox{0.6}{
    \begin{tabular}{l cccc}
      \toprule
      & \multicolumn{4}{c}{\textbf{Recruiting Dataset}}\\
      & \multicolumn{2}{c}{Rank Resume} & \multicolumn{2}{c}{Rank Job} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
      \textbf{Method}
      &  Recall@100 & nDCG@100 & Recall@10 & nDCG@10 \\
      \midrule
      \confitsimple
       & 79.00 &44.21 &80.83&60.67
      \\
      \midrule
      Max-match (human)
      & 82.06 &73.14 &84.75 &70.11
      \\
      % \midrule
      Max-match (Jina)
      & 80.14 &65.95 &82.78 &66.34\\

      % \midrule
      Centroid
      & 74.82 &68.28 &87.50&69.29\\
    
      \bottomrule

    \end{tabular}
  }
\caption{Estimating \HyReShort{} upperbound using real resumes. To speed-up evaluation, we trained all models on a fixed subset of the data.}
\label{tbl:Upperbound}
\end{table}
\begin{table}[!t]
\centering
\scalebox{0.6}{
    \begin{tabular}{l cccc}
      \toprule
      & \multicolumn{4}{c}{\textbf{Recruiting Dataset}}\\
      & \multicolumn{2}{c}{Rank Resume} & \multicolumn{2}{c}{Rank Job} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
      \textbf{Method}
      &  Recall@100 & nDCG@100 & Recall@10 & nDCG@10 \\
      \midrule
      \confitsimple
       & 79.00 &44.21 &80.83&60.67
      \\
      \midrule
      Job2Resume
      & 80.92 &46.01 &85.00 &65.12
      \\
      % \midrule
      Resume2Job
      
      & 74.18 &42.30 &77.75&59.82\\
    
      \bottomrule

    \end{tabular}
  }
\caption{Comparison of generating hypothetical job posts instead of resume.}
\label{tbl:Upperbound_job}
\end{table}
\begin{table}[!t]
\centering
\scalebox{0.55}{
    \begin{tabular}{l cccc}
      \toprule
      & \multicolumn{4}{c}{\textbf{Recruiting Dataset}}\\
      & \multicolumn{2}{c}{Rank Resume} & \multicolumn{2}{c}{Rank Job} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
      \textbf{Method}
      &  Recall@100 & nDCG@100 & Recall@10 & nDCG@10 \\
      \midrule
      \confitsimple
       & 79.00 &44.21 &80.83&60.67
      \\
      \midrule
      GPT-4o-mini (Few-shot)
      & 80.92 &46.01 &85.00 &65.12
      \\
      % \cmidrule{2-5}
     Qwen (SFT)
      
      & 71.62 & 42.07&78.50&57.38\\
      Qwen (SFT+DPO)
      
      &77.17 & 45.07&84.75&63.53\\
    
      \bottomrule

    \end{tabular}
  }
\caption{Comparison of Few-shot Prompting and Finetuning \HyReShort{}\ trained on subset}
\label{tbl:finetune}
\end{table}
% 
% 
% 
\subsection{Hypothetical Job Embedding}
\label{subsec: Hypothetical job}

In \framework{}, we augment job data with hypothetical resumes generated by an LLM (\Cref{subsec:Hypothetical Resume Embedding}). Alternatively, one can also augment resume data with hypothetical jobs generated by an LLM, which we experiment in this section.

We follow the same setup as \Cref{subsec:HyRe Upperbound}, but instead generate a hypothetical job for each resume. We then compare this against generating hypothetical resumes (\HyReShort{}). We present the results in \Cref{tbl:Upperbound_job}.
We find, on average, augmenting job post with hypothetical resume is more effective.
We believe this is because job data are often short and contain less details compared to resumes, making it more beneficial for content expansion.
% as resumes are more finegrained and discriminative. Resume-to-Job loses more details than Job-to-Resume, and thus is detrimental to the performance.
% We use the same subset as \ref{subsec:HyRe Upperbound} but reversely generate a hypothetical job for each resume using GPT-4o-mini with the similar few-shot prompt. 
% The results show that converting jobs to resumes is more useful. This is reasonable as resumes are more finegrained and discriminative. Resume-to-Job loses more details than Job-to-Resume, and thus is detrimental to the performance.


 % \clearpage
\subsection{Finetuning HyRe}
\label{subsec:Finetuning HyRe}

In this section, we examine whether fine-tuned LLMs are capable of generating hypothetical resume to approxiate \Cref{subsec:HyRe Upperbound}. We perform Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) on Qwen-2.5 series models \cite{qwen2.5}. Specifically, we train Qwen-2.5-32B-Instruct with LoRA \cite{hu2022lora} on the Recruiting Dataset. All the experiments are implemented on 4 A100 80GB GPUs.

For SFT training, we select 6k high-quality resume-job pairs that have both a high human label score and a high cosine similarity score computed by Jina-v2' raw embeddings.
For DPO training, we select the resume with the highest human label score as the chosen sample, and randomly select a resume with lower label score as the rejected sample. This results in a ``preference'' dataset with around 13k pairs. We then train Qwen-2.5-32B-Instruct with SFT and DPO, and evaluate the checkpoints after each training stage.

% Then, we use the trained Qwen model to generate hypothetical resume to boost \confitsimple{} training. The following table shows \confitsimple{} performance trained with a converted subset same as \ref{subsec: Hypothetical job} and \ref{subsec:HyRe Upperbound}.

To speed up evaluation, we follow \Cref{subsec:HyRe Upperbound} and evaluate all methods using a subset of the Recruiting dataset. We present the result in \Cref{tbl:finetune}.
We find after SFT+DPO training. the Qwen model improves \confitsimple{}, but it does not overpass Few-shot prompting on GPT-4o-mini.
% The Table \ref{tbl:finetune} shows, with SFT+DPO, a Qwen model can also facilitate \confitsimple \ training, but it does not overpass Few-shot prompting on GPT-4o-mini.
We believe this is because accepting a resume is subjective, making it hard to learn an ``ideal candidate'' directly from the dataset itself.
% The limitations of existing datasets in diversity, noise, and formats also hinder finetuned LLMs from generating generalizable, long-context, high-quality resumes. Thus we leave this attempt to future study. 



% \clearpage
\input{floats/RUM_range}
\input{floats/dset_stats}
\input{floats/archi_diff}
\input{floats/hard_neg_ablation}

\section{Additional Results on \RunnerUpMiningShort{}}
\label{sec:Additional Results on RUM}


We investigate two aspects of \RunnerUpMiningShort{}: (1) how different percentile ranges affect performance, and (2) how RUM compares to previous BM25-based hard-negative mining method \cite{DPR,Zhao2024}. Specifically, we consider “BM25(top-10)”, which retrieves top-10 highest-scoring unlabeled or incorrect resume/job computed by BM25.

% \Cref{tbl:RUM} summarizes the results on the Recruiting dataset, where “BM25(top-10)” retrieves the top-10 highest-scoring negatives per query via BM25, while “RUM(K\%–L\%)” denotes selecting runner-up samples in the top K–L\% similarity range.

We present the result in \Cref{tbl:RUM}.
Overall, we find \RunnerUpMiningShort{} consistently surpasses BM25-based negatives in nDCG across both ranking tasks. 
% Although a narrow slice like 0\%–1\% can already outperform BM25 in some metrics, focusing on the 3\%–4\% band yields the best performance on recall and nDCG. 
When using \RunnerUpMiningShort{}, we find although ranges such as 0\%–1\% outperforms BM25 in some metrics, it significantly underperforms other ranges such as 2\%-3\%, and 3\%-4\%.
This indicates that many high-scoring unlabeled samples are likely \emph{positives} for person-job fit.
In this work, we picked 3\%-4\% due to its high average performance, and \emph{fixed it for all subsequent runs} (e.g., with different model architectures and datasets).
% Our experiments also confirm this improvement persists under different backbone encoders, reinforcing the conclusion that a small, high-similarity interval (around 3\%–4\%) hits an optimal balance between being challenging and avoiding excessive false negatives.

% \clearpage

\section{Details on Dataset and Preprocessing}
\label{sec:More Details on Dataset and Preprocessing}


\paragraph{Recruiting Dataset} The talent-job pairs are provided by a hiring solution company. The original resumes/job posts are parsed into text fields using techniques such as OCR. Some of the information is further corrected by humans. All sensitive information, such as names, contacts, college names, and company names, has been either removed or converted into numeric IDs. Example resume and job post are shown in \Cref{tbl:example_resume} and \Cref{tbl:example_job}, respectively.

\paragraph{AliYun Dataset} The 2019 Alibaba job-resume intelligent matching competition provided resume-job data that is already desensitized and parsed into a collection of text fields. There are 12 fields in a resume (\Cref{tbl:example_resume}) and 11 fields in a job post (\Cref{tbl:example_job}) used during training/validation/testing. Sensitive fields such as ``\chinese{居住城市}'' (living city) were already converted into numeric IDs. ``\chinese{工作经验}'' (work experience) was processed into a list of keywords. Overall, the average length of a resume or a job post in the AliYun dataset is much shorter than that of the Recruiting dataset (see \Cref{tbl:train_dset}).

We present the training and test dataset statistics in \Cref{tbl:train_dset} and \Cref{tbl:test_dset}, respectively.
\begin{table*}[!t]
  \centering
  \scalebox{1.0}{
    \begin{tabular}{p{0.95\linewidth}}
      \toprule
      \textbf{\HyReShort{} Prompt Template} \\
      \midrule
      Here is a template pair of matching resume and job:

[The start of the example job]

\textit{\{\{\{example job\}\}\}}

[The end of the example job]

[The start of the example resume]

% \textit{template["resume"]}
\textit{\{\{\{example resume\}\}\}}

[The end of the example resume]

You are a helpful assistant. Following the above example pair of job and resume, construct an ideal 
resume for the target job shown below. You should strictly follow the format of the given pairs, make sure the resume you give perfectly matches the 
target job, and directly return your answer in plain text.

[The start of the target job]

\textit{\{\{\{target job\}\}\}}

[The end of the target job]\\

      \bottomrule
    \end{tabular}
  }
  \caption{Few-shot prompt template for hypothetical reference resume generation. ```\textit{\{\{\{...\}\}\}}''' are placeholders to be programmatically inserted during inference. ``target job'' is the job post to be augmented.
  }
  \label{tbl:prompt}
\end{table*}
\input{floats/example_resume}
\input{floats/example_job}

\end{document}