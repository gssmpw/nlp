\section{Related Work}
\label{sec:Related Work}

\paragraph{Person-job fit systems}


% Early person-job fit systems that uses neural networks typically focus on architecture modification. These methods include \citet{APJFNN,pjfnn,cnn-lstm-siamese,jiang2020learning,10169716}, which explores
% architectures such as RNN, LSTM \cite{staudemeyer2019understanding} and CNN \cite{oshea2015introduction}.
% However, these are significantly outperformed by more recent transformer-based encoder models such as BERT \cite{devlin2019bert}.
% Recent deep learning methods include \citet{siamese,cnn-lstm-siamese}, which uses deep siamese network to learn an embedding space for resume/jobs, \citet{bian-etal-2019-domain} which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and \citet{Zhang2023FedPJFFC} which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
Early person-job fit systems that use neural networks typically focus on architecture designs. These methods include \citet{APJFNN,pjfnn,cnn-lstm-siamese,jiang2020learning,10169716}, which explores
architectures such as RNN, LSTM \cite{staudemeyer2019understanding} and CNN \cite{oshea2015introduction}.
However, these are significantly outperformed by more recent transformer-based methods such as \citet{siamese,cnn-lstm-siamese,bian-etal-2019-domain,Zhang2023FedPJFFC}, which focuses on small architecture or loss modifications.
For example, MV-CoN \cite{mvcon} uses a co-teaching network \cite{malach2018decoupling} to perform gradient updates based on model's confidence to data noises; InEXIT \cite{InEXIT}, which uses hierarchical attention to model resume-job interactions; and DPGNN \cite{DPGNN}, which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% Recent deep learning methods include \citet{siamese,cnn-lstm-siamese}, which uses deep siamese network to learn an embedding space for resume/jobs, \citet{bian-etal-2019-domain} which uses a hierarchical RNN to improve domain-adaptation of person-job fit systems, and \citet{Zhang2023FedPJFFC} which uses federated learning to perform model training while preserving user privacy. However, as person-job fit systems involve sensitive data, most work do not open-source datasets \emph{or implementations}, and are often optimized for one particular dataset.
% Recent work with public implementations includes MV-CoN \cite{mvcon}, which uses a co-teaching network \cite{malach2018decoupling} to perform gradient updates based model's confidence to data noises; InEXIT \cite{InEXIT}, which uses hierarchical attention to model resume-job interactions; and DPGNN \cite{DPGNN}, which uses a graph-based approach with a novel BPR loss to optimize for resume/job ranking.
% However, these methods tend to focus on task-specific modifications, such as assuming no unseen resumes/jobs at test time or assuming access to internal data such as whether the recruiter sent private messages to the job applicant.
% The former is unrealistic considering new job seekers/job positions frequently appear, and the latter is only applicable to specific recruitment platforms.
More recently, \confitold{} \cite{confit_v1} focuses on applying dense retrieval techniques to person-job fit. Using contrastive learning with encoders such as E5 \cite{E5}, \citet{confit_v1} demonstrates its performance and flexibility by achieving the best scores in almost all ranking and classification tasks across two different person-job fit datasets.
In this work, we propose \HyReShort{} and \RunnerUpMiningShort{} to further enhance \framework{}, achieving an average absolute improvement of 13.8\% in recall and 17.5\% in nDCG across job ranking and resume-ranking tasks in two resume-job benchmarks.


\paragraph{Dense retrieval techniques} 
\framework{} benefits from dense retrieval techniques such as contrastive learning \cite{SimCLR,radford2021learning} and hard-negative mining.
Popular methods in text retrieval include BM25 \cite{bm25,bm25-all}, a keyword-based approach used as the baseline in many text ranking tasks \cite{ms-marco,thakur2021beir,muennighoff2022mteb}, and dense retrieval methods such as \citet{DPR,izacard2021contriever,E5,günther2023jina}, which uses contrastive learning with an encoder to obtain high-quality passage embeddings, and typically performs top-k search based on inner product.

To further improve retrieval results, researchers considered methods such as query/document expansions \cite{query-expansion-survey} and hard-negative mining \cite{robinson2021contrastive}.
Recent query/document expansion approaches include HyDE \cite{hyde} and HyQE \cite{hyqe}, which prompts an LLM to augment the original query or the passage to retrieve and is typically used \emph{without} finetuning (the LLM or the encoder model). Common hard-negative mining strategies often use BM25 to select top-k unlabeled/incorrect samples as hard negatives \cite{DPR,Zhao2024,nguyen-etal-2023-passage}. Other methods include: ANCE \cite{xiong2021approximate} which samples negatives from the top retrieved documents asynchronously during training; SimANS \cite{zhou-etal-2022-simans} which samples hard negatives using a manually designed probability distribution; and more \cite{zhan2021,syneg}. \confitold{} \cite{confit_v1} presents the first successful attempt to use contrastive learning for person-job fit.
We extend \confitold{} and adapt recent dense retrieval techniques to person-job fit by 1) sampling hard-negative resumes/jobs from a ``runner-up'' \emph{percentile ranges} to mitigate selecting false-negatives; and 2) few-shot prompting an LLM to augment a job posted used for later encoder \emph{training}.


% \#TODO the following is copy pasted
% \framework{} benefits from contrastive learning techniques, which have seen wide applications in many dense text retrieval and representation learning tasks \cite{SimCLR,radford2021learning}.
% Given a query (e.g., user-generated question), an information retrieval system aims to find top-k relevant passages from a large reserve of candidate passages \cite{joshi2017triviaqa,natural-questions}.
% Popular methods in information retrieval include BM25 \cite{bm25,bm25-all}, a keyword-based approach used as the baseline in many text ranking tasks \cite{ms-marco,thakur2021beir,muennighoff2022mteb}, and dense retrieval methods such as \citet{DPR,izacard2021contriever,E5}, which uses contrastive learning to obtain high-quality passage embeddings and typically performs top-k search based on inner product. To our knowledge, \framework{} is the first attempt to use contrastive learning for person-job fit, achieving the best performances in almost all person-job ranking tasks across two different person-job fit datasets.
% \paragraph{Hard Negative Mining} 
% Hard negatives are crucial for learning robust representations. Common strategies often begin with random sampling \cite{huang-2020} or in-batch negatives \cite{DPR}. In person-job fit, \confitold{} used explicitly “rejected” resume-job pairs as difficult examples. Alternatively, BM25-based methods retrieve top-ranked candidates\cite{Zhao2024}, while PassageBM25 \cite{nguyen-etal-2023-passage} refines this by comparing candidate passages to the positive passage rather than the query. Dynamic methods \cite{zhan2021}further refine the negative pool across training iterations. For example, ANCE \cite{xiong2021approximate} leverages a pretrained dense retriever to asynchronously update top-scoring negatives.Instead of strictly choosing top-ranked candidates, SimANS \cite{zhou-etal-2022-simans} designs a probability distribution to sample difficult negatives near the boundary. Meanwhile, TriSampler \cite{yang2024trisampler} applies a “quasi-triangular principle” to locate especially informative negatives. Recent work has also explored graph-based negative sampling\cite{GNNO}, leveraging structural information between items to improve sample selection. Methods like SyNeg\cite{syneg} employ LLMs to generate contextually coherent but factually incorrect samples.


% By contrast, our method first trains a dense retriever with \confitold{}’s hard negative approach, then uses it to score all resume-job pairs. We focus on the top 3–4\% non-matching “runner-ups,” striking a careful balance between being sufficiently challenging and avoiding excessive false negatives. This strategy refines the model’s ability to handle label-sparse scenarios more effectively.