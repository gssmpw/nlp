@INPROCEEDINGS{10169716,
  author={Mhatre, Sonali and Dakhare, Bhawana and Ankolekar, Vaibhav and Chogale, Neha and Navghane, Rutuja and Gotarne, Pooja},
  booktitle={2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS)}, 
  title={Resume Screening and Ranking using Convolutional Neural Network}, 
  year={2023},
  volume={},
  number={},
  pages={412-419},
  doi={10.1109/ICSCSS57650.2023.10169716}
}

@inproceedings{APJFNN,
  author = {Qin, Chuan and Zhu, Hengshu and Xu, Tong and Zhu, Chen and Jiang, Liang and Chen, Enhong and Xiong, Hui},
  title = {Enhancing Person-Job Fit for Talent Recruitment: An Ability-Aware Neural Network Approach},
  year = {2018},
  isbn = {9781450356572},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3209978.3210025},
  doi = {10.1145/3209978.3210025},
  booktitle = {The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages = {25–34},
  numpages = {10},
  keywords = {neural network, person-job fit, recruitment analysis},
  location = {Ann Arbor, MI, USA},
  series = {SIGIR '18}
}

@inproceedings{DPGNN,
  author = {Chen Yang and Yupeng Hou and Yang Song and Tao Zhang and Ji-Rong Wen and Wayne Xin Zhao},
  title = {Modeling Two-Way Selection Preference for Person-Job Fit},
  booktitle = {{RecSys}},
  year = {2022}
}

@misc{DPR,
  title={Dense Passage Retrieval for Open-Domain Question Answering}, 
  author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
  year={2020},
  eprint={2004.04906},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{E5,
  title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
  author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
  year={2022},
  eprint={2212.03533},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@inproceedings{GNNO,
author = {Fan, Lu and Pu, Jiashu and Zhang, Rongsheng and Wu, Xiao-Ming},
title = {Neighborhood-based Hard Negative Mining for Sequential Recommendation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591995},
doi = {10.1145/3539618.3591995},
abstract = {Negative sampling plays a crucial role in training successful sequential recommendation models. Instead of merely employing random negative sample selection, numerous strategies have been proposed to mine informative negative samples to enhance training and performance. However, few of these approaches utilize structural information. In this work, we observe that as training progresses, the distributions of node-pair similarities in different groups with varying degrees of neighborhood overlap change significantly, suggesting that item pairs in distinct groups may possess different negative relationships. Motivated by this observation, we propose a graph-based negative sampling approach based on neighborhood overlap (GNNO) to exploit structural information hidden in user behaviors for negative mining. GNNO first constructs a global weighted item transition graph using training sequences. Subsequently, it mines hard negative samples based on the degree of overlap with the target item on the graph. Furthermore, GNNO employs curriculum learning to control the hardness of negative samples, progressing from easy to difficult. Extensive experiments on three Amazon benchmarks demonstrate GNNO's effectiveness in consistently enhancing the performance of various state-of-the-art models and surpassing existing negative sampling strategies. The code will be released at https://github.com/floatSDSDS/GNNO.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2042–2046},
numpages = {5},
keywords = {graph mining, hard negative mining, sequential recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{InEXIT,
  author    = {Taihua Shao and
               Chengyu Song and
               Jianming Zheng and
               Fei Cai and
               Honghui Chen},
  title     = {Exploring Internal and External Interactions for Semi-Structured
               Multivariate Attributes in Job-Resume Matching},
  booktitle = {International Journal of Intelligent Systems},
  doi       = {10.1155/2023/2994779},
  year      = {2023},
}

@misc{SimCLR,
      title={A Simple Framework for Contrastive Learning of Visual Representations}, 
      author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
      year={2020},
      eprint={2002.05709},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Zhang2023FedPJFFC,
  title={FedPJF: federated contrastive learning for privacy-preserving person-job fit},
  author={Yunchong Zhang and Baisong Liu and Jiangbo Qian},
  journal={Applied Intelligence},
  year={2023},
  volume={53},
  pages={27060 - 27071},
  url={https://api.semanticscholar.org/CorpusID:261454666}
}

@article{Zhao2024,
author = {Zhao, Wayne Xin and Liu, Jing and Ren, Ruiyang and Wen, Ji-Rong},
title = {Dense Text Retrieval Based on Pretrained Language Models: A Survey},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3637870},
doi = {10.1145/3637870},
abstract = {Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user’s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models&nbsp;(PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.},
journal = {ACM Trans. Inf. Syst.},
month = feb,
articleno = {89},
numpages = {60},
keywords = {Text retrieval, dense retrieval, pretrained language models}
}

@inproceedings{bian-etal-2019-domain,
    title = "Domain Adaptation for Person-Job Fit with Transferable Deep Global Match Network",
    author = "Bian, Shuqing  and
      Zhao, Wayne Xin  and
      Song, Yang  and
      Zhang, Tao  and
      Wen, Ji-Rong",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1487",
    doi = "10.18653/v1/D19-1487",
    pages = "4810--4820",
}

@article{bm25,
    author = {Robertson, Stephen and Zaragoza, Hugo},
    title = {The Probabilistic Relevance Framework: BM25 and Beyond},
    year = {2009},
    issue_date = {April 2009},
    publisher = {Now Publishers Inc.},
    address = {Hanover, MA, USA},
    volume = {3},
    number = {4},
    issn = {1554-0669},
    url = {https://doi.org/10.1561/1500000019},
    doi = {10.1561/1500000019},
    journal = {Found. Trends Inf. Retr.},
    month = {apr},
    pages = {333–389},
    numpages = {57}
}

@inproceedings{bm25-all,
    author = {Trotman, Andrew and Puurula, Antti and Burgess, Blake},
    title = {Improvements to BM25 and Language Models Examined},
    year = {2014},
    isbn = {9781450330008},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2682862.2682863},
    doi = {10.1145/2682862.2682863},
    booktitle = {Proceedings of the 19th Australasian Document Computing Symposium},
    pages = {58–65},
    numpages = {8},
    keywords = {Document Retrieval, Relevance Ranking, Procrastination},
    location = {Melbourne, VIC, Australia},
    series = {ADCS '14}
}

@inproceedings{cnn-lstm-siamese,
  author = {Rezaeipourfarsangi, Sima and Milios, Evangelos E.},
  title = {AI-Powered Resume-Job Matching: A Document Ranking Approach Using Deep Neural Networks},
  year = {2023},
  isbn = {9798400700279},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3573128.3609347},
  doi = {10.1145/3573128.3609347},
  abstract = {This study focuses on the importance of well-designed online matching systems for job seekers and employers. We treat resumes and job descriptions as documents. Then, calculate their similarity to determine the suitability of applicants, and rank a set of resumes based on their similarity to a specific job description. We employ Siamese Neural Networks, comprised of identical sub-network components, to evaluate the semantic similarity between documents. Our novel architecture integrates various neural network architectures, where each sub-network incorporates multiple layers such as CNN, LSTM and attention layers to capture sequential, local and global patterns within the data. The LSTM and CNN components are applied concurrently and merged together. The resulting output is then fed into a multi-head attention layer. These layers extract features and capture document representations. The extracted features are then combined to form a unified representation of the document. We leverage pre-trained language models to obtain embeddings for each document, which serve as a lower-dimensional representation of our input data. The model is trained on a private dataset of 268,549 real resumes and 4,198 job descriptions from twelve industry sectors, resulting in a ranked list of matched resumes. We performed a comparative analysis involving our model, Siamese CNN (S-CNNs), Siamese LSTM with Manhattan distance, and a BERT-based sentence transformer model. By combining the power of language models and the novel Siamese architecture, this approach leverages both strengths to improve document ranking accuracy and enhance the matching process between job descriptions and resumes. Our experimental results demonstrate that our model outperforms other models in terms of performance.},
  booktitle = {Proceedings of the ACM Symposium on Document Engineering 2023},
  articleno = {22},
  numpages = {4},
  keywords = {Siamese neural network, document ranking, LSTM, CNN, language model, attention},
  location = {Limerick, Ireland},
  series = {DocEng '23}
}

@inproceedings{confit_v1,
    author={Xiao Yu and Jinzhong Zhang and Zhou Yu},
    title = {ConFit: Improving Resume-Job Matching using Data Augmentation and Contrastive Learning},
    year = {2024},
    isbn = {9798400705052},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3640457.3688108},
    doi = {10.1145/3640457.3688108},
    pages = {601–611},
    numpages = {11},
    keywords = {contrastive learning, data augmentation, person-job fit},
    location = {Bari, Italy},
    series = {RecSys '24}
}

@misc{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  year={2019},
  eprint={1810.04805},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@misc{günther2023jina,
      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}, 
      author={Michael Günther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},
      year={2023},
      eprint={2310.19923},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{huang-2020,
author = {Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
title = {Embedding-based Retrieval in Facebook Search},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403305},
doi = {10.1145/3394486.3403305},
abstract = {Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher's context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval (EBR) has been applied in web search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying EBR to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including ANN parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated EBR on verticals for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
pages = {2553–2561},
numpages = {9},
keywords = {search, information retrieval, embedding, deep learning},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@misc{hyde,
      title={Precise Zero-Shot Dense Retrieval without Relevance Labels}, 
      author={Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
      year={2022},
      eprint={2212.10496},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2212.10496}, 
}

@misc{hyqe,
      title={HyQE: Ranking Contexts with Hypothetical Query Embeddings}, 
      author={Weichao Zhou and Jiaxin Zhang and Hilaf Hasson and Anu Singh and Wenchao Li},
      year={2024},
      eprint={2410.15262},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2410.15262}, 
}

@misc{izacard2021contriever,
  title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
  author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
  year={2021},
  url = {https://arxiv.org/abs/2112.09118},
  doi = {10.48550/ARXIV.2112.09118},
}

@misc{jiang2020learning,
  title={Learning Effective Representations for Person-Job Fit by Feature Fusion}, 
  author={Junshu Jiang and Songyun Ye and Wei Wang and Jingran Xu and Xiaosheng Luo},
  year={2020},
  eprint={2006.07017},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}

@misc{joshi2017triviaqa,
      title={TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}, 
      author={Mandar Joshi and Eunsol Choi and Daniel S. Weld and Luke Zettlemoyer},
      year={2017},
      eprint={1705.03551},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{malach2018decoupling,
  title={Decoupling "when to update" from "how to update"}, 
  author={Eran Malach and Shai Shalev-Shwartz},
  year={2018},
  eprint={1706.02613},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{ms-marco,
  author    = {Tri Nguyen and
               Mir Rosenberg and
               Xia Song and
               Jianfeng Gao and
               Saurabh Tiwary and
               Rangan Majumder and
               Li Deng},
  title     = {{MS} {MARCO:} {A} Human Generated MAchine Reading COmprehension Dataset},
  journal   = {CoRR},
  volume    = {abs/1611.09268},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09268},
  archivePrefix = {arXiv},
  eprint    = {1611.09268},
  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NguyenRSGTMD16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{muennighoff2022mteb,
  doi = {10.48550/ARXIV.2210.07316},
  url = {https://arxiv.org/abs/2210.07316},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  title = {MTEB: Massive Text Embedding Benchmark},
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2210.07316},  
  year = {2022}
}

@misc{mvcon,
  title={Learning to Match Jobs with Resumes from Sparse Interaction Data using Multi-View Co-Teaching Network}, 
  author={Shuqing Bian and Xu Chen and Wayne Xin Zhao and Kun Zhou and Yupeng Hou and Yang Song and Tao Zhang and Ji-Rong Wen},
  year={2020},
  eprint={2009.13299},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{natural-questions,
  title   = {Natural Questions: a Benchmark for Question Answering Research},
  author  = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
  year    = {2019},
  journal = {Transactions of the Association of Computational Linguistics}
}

@inproceedings{nguyen-etal-2023-passage,
    title = "Passage-based {BM}25 Hard Negatives: A Simple and Effective Negative Sampling Strategy For Dense Retrieval",
    author = "Nguyen, Thanh-Do  and
      Bui, Chi Minh  and
      Vuong, Thi-Hai-Yen  and
      Phan, Xuan-Hieu",
    editor = "Huang, Chu-Ren  and
      Harada, Yasunari  and
      Kim, Jong-Bok  and
      Chen, Si  and
      Hsu, Yu-Yin  and
      Chersoni, Emmanuele  and
      A, Pranav  and
      Zeng, Winnie Huiheng  and
      Peng, Bo  and
      Li, Yuxi  and
      Li, Junlin",
    booktitle = "Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation",
    month = dec,
    year = "2023",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.paclic-1.59/",
    pages = "591--599"
}

@misc{oshea2015introduction,
      title={An Introduction to Convolutional Neural Networks}, 
      author={Keiron O'Shea and Ryan Nash},
      year={2015},
      eprint={1511.08458},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{pjfnn,
  title={Person-Job Fit: Adapting the Right Talent for the Right Job with Joint Representation Learning}, 
  author={Chen Zhu and Hengshu Zhu and Hui Xiong and Chao Ma and Fang Xie and Pengliang Ding and Pan Li},
  year={2018},
  eprint={1810.04040},
  archivePrefix={arXiv},
  primaryClass={cs.IR}
}

@article{query-expansion-survey,
author = {Carpineto, Claudio and Romano, Giovanni},
title = {A Survey of Automatic Query Expansion in Information Retrieval},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2071389.2071390},
doi = {10.1145/2071389.2071390},
abstract = {The relative ineffectiveness of information retrieval systems is largely caused by the inaccuracy with which a query formed by a few keywords models the actual user information need. One well known method to overcome this limitation is automatic query expansion (AQE), whereby the user’s original query is augmented by new features with a similar meaning. AQE has a long history in the information retrieval community but it is only in the last years that it has reached a level of scientific and experimental maturity, especially in laboratory settings such as TREC. This survey presents a unified view of a large number of recent approaches to AQE that leverage various data sources and employ very different principles and techniques. The following questions are addressed. Why is query expansion so important to improve search effectiveness? What are the main steps involved in the design and implementation of an AQE component? What approaches to AQE are available and how do they compare? Which issues must still be resolved before AQE becomes a standard component of large operational information retrieval systems (e.g., search engines)?},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {1},
numpages = {50},
keywords = {word associations, search, query refinement, pseudo-relevance feedback, document ranking, Query expansion}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{robinson2021contrastive,
  author    = {Joshua Robinson and Ching-Yao Chuang and Suvrit Sra and Stefanie Jegelka},
  title     = {Contrastive learning with hard negative samples},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021}
}

@article{siamese,
  title={Matching Resumes to Jobs via Deep Siamese Network},
  author={Saket Maheshwary and Hemant Misra},
  journal={Companion Proceedings of the The Web Conference 2018},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:13807085}
}

@misc{staudemeyer2019understanding,
      title={Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks}, 
      author={Ralf C. Staudemeyer and Eric Rothstein Morris},
      year={2019},
      eprint={1909.09586},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{syneg,
      title={SyNeg: LLM-Driven Synthetic Hard-Negatives for Dense Retrieval}, 
      author={Xiaopeng Li and Xiangyang Li and Hao Zhang and Zhaocheng Du and Pengyue Jia and Yichao Wang and Xiangyu Zhao and Huifeng Guo and Ruiming Tang},
      year={2024},
      eprint={2412.17250},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2412.17250}, 
}

@misc{thakur2021beir,
      title={BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models}, 
      author={Nandan Thakur and Nils Reimers and Andreas Rücklé and Abhishek Srivastava and Iryna Gurevych},
      year={2021},
      eprint={2104.08663},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{yang2024trisampler,
  title={TriSampler: A Better Negative Sampling Principle for Dense Retrieval},
  author={Yang, Zhen and Shao, Zhou and Dong, Yuxiao and Tang, Jie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={8},
  pages={9269--9277},
  year={2024}
}

@inproceedings{zhan2021,
author = {Zhan, Jingtao and Mao, Jiaxin and Liu, Yiqun and Guo, Jiafeng and Zhang, Min and Ma, Shaoping},
title = {Optimizing Dense Retrieval Model Training with Hard Negatives},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462880},
doi = {10.1145/3404835.3462880},
abstract = {Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1503–1512},
numpages = {10},
keywords = {dense retrieval, neural ranking, representation learning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{zhou-etal-2022-simans,
    title = "{S}im{ANS}: Simple Ambiguous Negatives Sampling for Dense Text Retrieval",
    author = "Zhou, Kun  and
      Gong, Yeyun  and
      Liu, Xiao  and
      Zhao, Wayne Xin  and
      Shen, Yelong  and
      Dong, Anlei  and
      Lu, Jingwen  and
      Majumder, Rangan  and
      Wen, Ji-rong  and
      Duan, Nan",
    editor = "Li, Yunyao  and
      Lazaridou, Angeliki",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-industry.56/",
    doi = "10.18653/v1/2022.emnlp-industry.56",
    pages = "548--559",
    abstract = "Sampling proper negatives from a large document pool is vital to effectively train a dense retrieval model. However, existing negative sampling strategies suffer from the uninformative or false negative problem. In this work, we empirically show that according to the measured relevance scores, the negatives ranked around the positives are generally more informative and less likely to be false negatives. Intuitively, these negatives are not too hard (\textit{may be false negatives}) or too easy (\textit{uninformative}). They are the ambiguous negatives and need more attention during training.Thus, we propose a simple ambiguous negatives sampling method, SimANS, which incorporates a new sampling probability distribution to sample more ambiguous negatives.Extensive experiments on four public and one industry datasets show the effectiveness of our approach.We made the code and models publicly available in \url{https://github.com/microsoft/SimXNS}."
}

