\section{Previous work}
\label{sec:previous-work}

Estimating FRT accuracy and bias requires large, accurately labeled datasets to ensure tight confidence intervals. Furthermore, one needs diverse attributes representative of the general population to explore effects on all demographics.  A team with the U.S. National Institute of Standards and Technology (NIST)~\cite{grother2014face,grother2019face} has, over the past 20 years, developed state-of-the-art test datasets and testing practices. They test algorithms on six large ($\simsym 10$M images) datasets collected from visas, visa applications, border crossings, arrest mugshots, kiosk images, and images collected in the wild. Accurate identity annotations are achieved by combining trained government officials and identity documents. NIST publishes updated reports every few months on  NIST's  ``Face Recognition Vendor Test'' web page~\cite{nist_frvt}. A number of academic teams are also engaged in testing FRT~\cite{albiero2020analysis,albiero2020does,albiero2021gendered,krishnapriya2020issues}. They use public datasets that may have been included in the training sets of FRT vendors and whose identity labels are often not accurate. Thus, while valuable for science, academic tests may not be suitable for probing the accuracy of commercial systems.

Only governments and large tech companies have access to large, accurately labeled datasets. 
To democratize the testing of FRT algorithms, we need to reduce the cost of ground-truth identity annotation dramatically.
This has long been considered unlikely since accurate face identification using human annotators is very difficult~\cite{phillips2018face}, and benchmarking without an independently annotated ground truth might seem impossible. Semi-automated clean-up methods have been proposed to reduce the cost of improving label quality in training sets. The state-of-the-art, WebFace260M~\cite{webface260}, iteratively employs a face recognition (FR) model to clean the images and then re-trains that model with the new clean dataset to obtain an improved model. This dynamic has three shortcomings in our application: (i) Constructing a high-quality embedding requires millions of images, which is the case for training sets. We focus on test sets, which are a couple of orders of magnitudes smaller and yet require higher accuracy. (ii) The stable point of the method may suffer from partial mode collapse due to errors in the original data leaking into the model. This has been documented on small datasets~\cite{shumailov2024ai} and is difficult to verify on large datasets. (iii) Legislation in many countries prohibits the storage of biometric data that enables the identification of individuals. To address the first two problems, our method takes advantage of the ensemble of the embeddings of the systems being tested, which is high-quality and stable. To address the latter, our method does not store any identity labels or images, relying solely on the scores of FR systems.

Recent studies propose semi- and unsupervised methods for benchmarking algorithm accuracy~\cite{welinder2013lazy,ji2020can,chouldechova2022unsupervised}, which can estimate algorithms' accuracy even without access to an externally provided ground truth. These methods take advantage of statistical regularities of the confidence values of classification algorithms to estimate the underlying error statistics. We take inspiration from this work. We note that face recognition algorithms are highly accurate and thus will produce strongly bimodal distributions of confidence values when confronted with an (unlabeled) mix of same-ID and different-ID pairs of face images. We exploit this fact to estimate the ground truth image identity labels. 

Our method addresses the practical case of image collections obtained through web image searches, where face identities are typically correct in the 20-80\% range. Thus, our method is supervised in that identity (ID) labels are provided. However, crucially, it is designed to tolerate highly noisy face identity labels and does not require human supervision, such as hand-labeling, to correct such errors. In other words, we address the situation that lies in between the availability of exact ground truth (conventional benchmarks based on carefully annotated test sets) and the (quasi) complete absence of identity labels (previous unsupervised and semi-supervised methods~\cite{welinder2013lazy,chouldechova2022unsupervised}). 

 Like other benchmarks, our method uses {\em observational} test sets with demographic annotations (typically age, gender, ethnicity)~\cite{ricanek2006morph,liu2015deep,grother2019face,karkkainen2021fairface}. Recent literature indicates that observational methods are susceptible to bias from unmodeled confound variables and propose {\em experimental} approaches based on synthetic images \cite{balakrishnan2021towards,liang2023benchmarking}. We agree that there is merit in this concern. For the time being, we believe that both observational and experimental methods need to be used to assess the accuracy of FRT.