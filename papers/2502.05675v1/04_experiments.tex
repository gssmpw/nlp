\section{Experimental Setup}
\label{sec:experiments}

\subsection{Models}
We evaluate a range of closed-source LLMs including GPT-4-Turbo, GPT-4o, GPT-3.5-turbo, and open-weight models Llama-3-8B-Instruct, and Mistral-7B-v2-Instruct on the `\textit{Civ. Pro.}' dataset using the zero-shot CoT prompting setting
\citep{wei2023chainofthoughtpromptingelicitsreasoning, kojima2023largelanguagemodelszeroshot}. While we also conduct the few-shot CoT prompting by providing 1-3 exemplars for solving a legal scenario to LLMs, we find that the final accuracy (\textsection \ref{section: metrics}) is lower across LLMs when compared to the zero-shot CoT setting (refer Appendix \ref{section:few-shot}). NVIDIA A100 GPUs were used to conduct the inference of open-weight models with a batch size of 1. OpenAI API and Gemini API were used for obtaining inference results from the closed-source models. An example prompt used for these experiments is provided in Figure \ref{fig:legal-reasoning-complexity-part-1}.

\subsection{Metrics}
\label{section: metrics}
\paragraph{Accuracy} We use accuracy to demonstrate the capability of LLMs in solving legal scenarios based on their ability to predict the final answer (the conclusion). To calculate this metric, we use the LLM-generated final options chosen as answers and compare them with the available expert answer. 

\paragraph{Soundness Score} We create a step-wise soundness score metric to check the number of premises which is error-free in terms of the absence of errors as delineated by our established error taxonomy. We computed this score by taking its average across a single reasoning chain. The Soundness score (between 0 and 1) is calculated by:

\[
S = \frac{\text{Number of sound premises}}{\text{Total Number of premises}}
\]

\paragraph{Correctness Score} This metric is calculated to evaluate the condition where the reasoning chain must be both error-free at both premise and conclusion levels. Likewise in the soundness metric, the expert answers to extract the ground-truth options as conclusions. A reasoning chain must be both `sound' (with the Soundness score being equal to 1) and arrive at the correct final option as its conclusion. The Correctness score is calculated as:
\[
C =
\begin{cases} 
1, & \text{if (premises +  conclusion) correct} \\
0, & \text{for all other cases} 
\end{cases}
\]

The results of these metrics calculated for 120 reasoning chains annotated by humans are provided in Table \ref{table:metrics_humans} (Appendix \ref{section: metrics_results_humans}). Table \ref{table:metrics} and Figure \ref{fig:acc_correctness_results} show the metric results calculated from auto-evaluator annotations on the entire \textit{Civ. Pro.} dataset.

