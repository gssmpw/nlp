% \section*{Appendix}
% \section{The \textit{Civil Procedure }Dataset}
% For the compilation of this dataset, we obtained the necessary permissions to use the dataset provided in the research paper: 'The Legal Argument Reasoning Task in Civil Procedure'\citep{bongard2022legalargumentreasoningtask}. The dataset has also been featured as a task in SemEval-2024 (Task 5: Legal Argument Reasoning in Civil Procedure) \citep{held-habernal-2024-semeval}. We compiled the \textit{'Civil Procedure'} (\textit{Civ. Pro.} for short) dataset containing the relevant legal context, questions with multiple choice options, and the expert answer along with the correct explanations provided by the legal experts, by extracting them and converting into appropriate prompt-based format which can be used for LLM inference and generation of reasoning chains. To solve a given legal problem, a LLM will need to generate a set of statements $<$$A: s_1, s_2, ..., s_k, c$$>$, where $A$ represents the legal argument/rationale put forward to solve the problem, with $s_1, s_2...s_k$ being the '\textit{k}' number of intermediate steps generated to reason towards the final conclusion $c$, where an option is chosen to answer the question.
\section{Metrics Results from human evaluations}
\label{section: metrics_results_humans}
Table \ref{table:metrics_humans} shows the statistics of the metrics calculated on the reasoning-chains directly by humans. The results show a marked difference in the values of Accuracy and Correctness across all LLMs manually evaluated by human evaluators. The results in Table \ref{table:metrics}, calculated from the LLM-based auto-evaluator annotations, also reflect the same trends in this table.

\input{Tables/metric_scores_humans}

\section{Manual Evaluation results}
\label{section:manual-eval}
Tables \ref{table:manual-evaluation-premise} and \ref{table:manual-evaluation-conclusion} show the statistics of the errors found by human evaluators in the premise and conclusion levels based on the process described in \textsection \ref{section:manual-eval-sec} and in accordance to the proposed error taxonomy in \textsection \ref{section:error_taxonomy}.

\input{Tables/average_steps}
\input{Tables/manual-eval}

\input{Tables/manual-eval2}

\section{Percentage distribution of Premise-level errors}
\label{section: premise-level-error-dist}
Figure \ref{fig:prem-level-cat} represents the percentage distribution of premise-level errors across the reasoning chains of all
5 LLMs. Due to the lowest number of average steps in reasoning steps, Llama-3-8B-instruct has the highest proportion of errors in the reasoning chains ($\sim$65.4$\%$) containing premise-level errors in the reasoning steps.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{Images/premise_level_errors.pdf}
    \caption{The percentage distribution of the premise-level error categories across the reasoning chains of all 5 LLMs. The total number of steps generated by each model is provided inside the round brackets below the model names. Here ’NE’ denotes Correct Premise (No errors), ’M’ denotes Premise containing a Misinterpretation, ’FH’ denotes Factual Hallucination in the premise, ’IP’ denotes an Irrelevant Premise.}
    \label{fig:prem-level-cat}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{Images/conclusion_level_errors.pdf}
    \caption{The percentage distribution of the conclusion-level error categories across the reasoning chains of all 5 LLMs. The total number of steps generated by each model is provided inside the round brackets below the model names. Here `NE' denotes Correct Conclusion(CC) From Correct Premises(CP) (No errors), `CF' denotes CC from False Premises (FP), `CH' denotes CC with Hallucinated Content, `WF' denotes Wrong Conclusion(WC) from FP and `WI' denotes EC from Incomplete Premises.}
    \label{fig:conc-level-cat}
\end{figure}

\section{Percentage distribution of Conclusion-level errors}
\label{section: conclusion-level-error-dist}
Figure \ref{fig:conc-level-cat} represents the percentage distribution of conclusion-level errors across the reasoning chains of all
5 LLMs. Mistral-7B-v2-instruct ($\sim$86.8$\%$) and Llama-3-8B-instruct ($\sim$86.3$\%$) have the highest proportion of errors in the reasoning chains containing conclusion-level errors. An interesting observation is that only Llama-3-8B instruct and GPT-4o have non-zero percentages of errors in the category of Correct Conclusion with Hallucinated Content, which could possibly mean that these LLMs have been trained on very similar data to that in the `\textit{Civ. Pro.}' dataset which could be causing these LLMs to spuriously output modified content for the correct options. 

\section{Average number of steps generated by LLMs}
\label{section:avg_steps_generated}
The average number of steps (the premise-level steps) generated by each LLM to solve a legal scenario is provided in Table \ref{table:average_steps}. Llama-3-8B-instruct has the lowest average of number of steps which is a probable cause leading to lower soundness score as shown in Table \ref{table:metrics}. Overall, LLMs mostly try to complete the reasoning chain in around 4-6 steps to arrive at the conclusion. While this is good for not introducing redundancies and keeping the irrelevant premise and hallucination errors low, this could also potentially be an indicator that LLMs do not explicitly output tokens which could be crucial in outlining the reasoning process and making the rationale better in terms of interpretability and explainability.    

\section{Misinterpretations vs. Factual Hallucinations vs. Irrelevant Premises}
\label{section:error_disambiguation}
A premise is classified as containing a `Misinterpretation' error when the LLM reasoner is making wrong inferences based on the information it generates. There is a `Factual Hallucination' when the information generated by the LLM reasoner (by directly extracting from the provided content of the input legal context, questions, and options) is factually incorrect and can be easily verified while directly going through the input context. A key distinction between Misinterpretation and Factual Hallucination is illustrated in Figures \ref{fig:autoeval-single-call} and \ref{fig:autoeval-multi-call}. Detecting a `Misinterpretation' requires the expert answer to account for complex legal reasoning nuances, while detecting a `Factual Hallucination' does not depend on the expert answer for validation. An irrelevant premise is said to occur when a premise contains unnecessary or tangential information that does not contribute to reasoning toward the correct answer. 

Extensive discussions and iterations occurred to define the taxonomy and differentiate between the three premise-level errors. However, these errors can often overlap or appear together in the same premise, especially in cases where `Misinterpretations' are caused by `Factual Hallucinations'. In the cases of significant overlap, both the human and the auto-evaluator were instructed to annotate multiple errors for the same premise. Detecting multiple errors can be crucial for longer premises which creates inferences on a greater number of contextual factors and nuances and hence are vulnerable to more of errors occurring in them.  

\section{Human Annotation Guidelines and Process}
\label{section:annotation_guidelines}
A set of 11 annotation guidelines, as detailed in Figure \ref{fig:annotation-guideline}, was developed and provided to annotators for manual evaluation and annotation. Four annotators, two students from undergraduate and graduate level each respectively, participated in this process, and cross-evaluation was conducted at the final stage to resolve any discrepancies. The annotation guidelines in Figure \ref{fig:annotation-guideline} also served as the basis for creating system prompts used in the LLM-based auto-evaluator. 

As an additional study to check for inter-annotator agreement, three annotators separately annotated 10 reasoning chains from Mistral-7B-v2-Instruct and the calculated the Cohen's kappa coefficient ($\kappa$). The $\kappa$ values for the 3 annotator pairs (by selecting 2 unique annotators out of 3 everytime) came out to be 0.862, 0.783 and 0.813, making the average $\kappa$ value to be 0.819. This indicated there is almost perfect agreement between the annotators according to Cohen's kappa metric interpretation. 

\section{Human Annotation Examples}
\label{section:human_annot}
Initially, Mistral-7B-v2-Instruct was selected for human evaluation due to its unique position as the earliest and smallest parametric model among the LLMs tested. Its smaller size increases the likelihood of it producing a wider range of reasoning and contextual errors, making it an ideal candidate for error analysis. By starting with a model that has fewer parameters and is more prone to subtle reasoning gaps, we can thoroughly evaluate and better understand the types of errors that may occur. This approach maximized the chances of capturing diverse error types that could be missed in larger, more sophisticated models, which tend to exhibit fewer surface-level mistakes. Table 7-11 contain few examples of human analysis of complexity of legal reasoning and error annotations performed for the reasoning chains. The text written in \textcolor{blue}{blue} font in the tables represents the human analyses and annotations performed.


\section{The LLM-based `Auto-Evaluator'}
\label{section:auto-eval}

\paragraph{Rationale behind using LLM-based Auto-Evaluator:} The primary rationale for employing an LLM-based auto-evaluator is its scalability and efficiency compared to human annotation. While human evaluators required approximately 30–60 minutes per reasoning chain to accurately identify and categorize errors in the \textit{`Civ. Pro.'} dataset, the auto-evaluator can process large volumes of reasoning chains significantly faster and at a lower cost. This efficiency becomes even more critical in real-world legal contexts, where expert reviews demand extensive time and resources. Modern LLMs, trained on vast amounts of data (including legal texts), demonstrate state-of-the-art performance in natural language understanding and reasoning. Although our results and analyses show that LLM-based reasoning systems are not yet fully error-free, the rapid improvements in LLM reasoning capabilities and the decreasing costs of inference make them a promising solution for scalable, cost-effective error detection.

\paragraph{Implementation:}At the premise level, four separate prompt-based evaluation pipelines (LLM-based error detectors) have been implemented. Three of the pipelines utilize a single call to GPT-4o, where the information provided to the LLM is broken into three parts: a. The system prompt which contains the information about the error taxa \textbf{(the knowledge base)} b. \textbf{important instructions} provided to the LLM evaluator on how to evaluate and detect errors and c. We also provide an \textbf{in-context learning example} of how the human annotation was carried out for the model to follow and replicate the annotation format. 

The first pipeline (Figure \ref{fig:autoeval-system}) is designed to simply detect whether a premise contains an error, without assigning a specific label from the error taxonomy. The second pipeline detects whether a premise contains a `Misinterpretation' error. The third pipeline does the same for detecting an `Irrelevant Premise'. Along with this, the input prompt includes the step-by-step reasoning chain. The LLM is expected to classify the error and provide an explanation for the classification. The fourth pipeline \ref{fig:autoeval-multi-call}, which has been designed to detect 'Factual Hallucination' errors, consists of a `Multi-call' LLM system where primarily two calls to GPT-4o are made: 1. The first call to GPT-4o is used to create fact-verification questions about various aspects of a particular premise. 2. The second call to GPT-4o is used to answer these verification questions by referencing the legal context and content of the questions and the options. A premise is classified as containing factual hallucination(s) if the answers to its verification questions reveal a contradiction between its content and that of the provided input context. The premise-wise evaluation results of four pipelines are aggregated and summarized by a final `summarization' call to GPT-4o which summarizes and enumerates all possible errors detected in a single premise. The aggregated and summarized results are then sent to the conclusion-level error evaluator which performs a conditional mapping procedure (as shown in Figure \ref{fig:autoeval-conclusion-level} to assign the conclusion-level errors.

\begin{figure}[H]
    \centering     \includegraphics[width=0.90\linewidth]{Prompt-Strategies/plan-and-solve.drawio.pdf}
    \caption{Prompt used to implement 'Plan-and-Solve' technique.} 
    \label{fig:plan-and-solve}
\end{figure}

% \begin{figure}[h]
%     \centering     \includegraphics[width=0.90\linewidth]{Prompt-Strategies/self-correct.drawio.pdf}
%     \caption{Prompt used to implement `Self-Correct' technique.} 
%     \label{fig:self-correct}
% \end{figure}
% \section{The `Aggregator + Summarizer' LLM}
% \label{section: summ_agg_llm}

GPT-4o (Refer Figure \ref{fig:autoeval-system}) was used to aggregate and summarize all the errors detected by the separate premise-level error detectors and summarize them for each premise separately. This had a corrective effect as the most appropriate errors (or combination of multiple errors) were summarized for each premise. An example of this corrective effect is shown in Figure \ref{fig:auto-eval-annot-example}, where both the `Misinterpretation' and `Factual Hallucination' auto-evaluators flag a premise with their respective labels and explanations. However, the summarizer LLM correctly identifies `Misinterpretation' as the most accurate error classification for that premise. The LLM also filtered out unnecessary text, retaining only the premises that contained errors. This process ensured that only the premise steps flagged with errors by the auto-evaluator were forwarded as input to the conclusion-level error analyzer. By focusing on the erroneous premises, this approach streamlined the error analysis process, enabling more efficient and targeted evaluation of how these errors impact the final conclusion.

\begin{figure}[H]
    \centering     \includegraphics[width=0.97\linewidth]{Prompt-Strategies/self-discovery.drawio.pdf}
    \caption{Prompt used to implement `Self-Discovery' technique.} 
    \label{fig:self-discovery}
\end{figure}


\section{Auto-Evaluator Effectiveness}
\label{section:auto-eval-agree}
Based on the errors found in the reasoning chains by human evaluators (as shown in Tables \ref{table:manual-evaluation-premise} and \ref{table:manual-evaluation-conclusion}), the effectiveness of GPT-4o-based auto-evaluator was measured using agreement of step-level presence or absence of errors at both the premise and conclusion level (refer Tables \ref{table:auto-eval-agreement-premise} and \ref{table:auto-eval-agreement-conclusion}). Agreement occurs only when both correct error category and matching error description is generated by the auto-evaluator through semantic error explanation match (refer \textsection \ref{section:llm_aided_eval}). Recall metric was chosen as the appropriate measure for agreement as it is crucial to identify if the auto-evaluator correctly identified an error in a premise or conclusion level step. The recall percentage of detecting an error at the premise level across four LLMs ranged from 86.17$\%$ to 93.85$\%$. The recall percentage range for detecting an error-free premise step was from 83.87$\%$ to 90.6\% (refer Table \ref{table:auto-eval-agreement-premise}). Similarly, at conclusion level, the average recall percentages ranges on agreement on the presence and absence of errors are mentioned in Table \ref{table:auto-eval-agreement-conclusion}. The current auto-evaluator system cannot detect 'Correct Conclusion from Incomplete Premise(s)' (CCIP), as it cannot distinguish it from a `Correct Conclusion from Correct Premises' scenario. While this is a drawback, the `CCIP' error is very rare in its occurrences as an error category. 
\begin{figure}[H]
    \centering     \includegraphics[width=0.97\linewidth]{Images/Annotation_guideline.drawio.pdf}
    \caption{The 11-step guideline provided to the annotators for conducting manual evaluations of the LLM-generated rationale} 
    \label{fig:annotation-guideline}
\end{figure}

\begin{figure}[h]
    \centering     \includegraphics[width=0.97\linewidth]{Prompt-Strategies/plan-and-solve-feedback.drawio.pdf}
    \caption{Prompt used to implement `Plan-and-Solve with error feedback' technique. Error feedback can be added to other prompting strategies in the same way.} 
    \label{fig:plan-and-solve-with-feedback}
\end{figure}

\begin{figure}[h]
    \centering     \includegraphics[width=0.90\linewidth]{Prompt-Strategies/self-correct.drawio.pdf}
    \caption{Prompt used to implement `Self-Correct' technique.} 
    \label{fig:self-correct}
\end{figure}
\section{The `Aggregator + Summarizer' LLM}
\label{section: summ_agg_llm}

\input{Tables/llm-agreement}
\input{Tables/llm-agreement2}


Furthermore, we conducted another experiment, replacing GPT-4o with Gemini-1.5-Flash as the backbone LLM for the auto-evaluator system. The prompts provided to auto-evaluator system were unchanged. The recall rate of identifying an error in the reasoning step at the premise-level for Gemini-based auto-evaluator on the Mistral-7B-v2-Instruct generated reasoning chains, when compared with human annotations, was found out to be $\sim$78.1\%. The recall-rate for identifying correct premise-level reasoning steps highly decreased to $\sim$20.61\%. This indicates that Gemini-1.5-Flash, while comparable to GPT-4o in identifying errors, was less effective overall as an auto-evaluator due to a higher number of false-negative predictions (with larger number of error-free steps were incorrectly identified as containing errors). Although fine-tuning and testing a dedicated LLM for error detection was not conducted in this study, the authors acknowledge it as an interesting avenue for future improvements and research directions.

\section{Prompting-techniques for Error Mitigation}
\label{section:prompting_strategies}
We carried out several experiments on the legal reasoning dataset, employing widely used prompting techniques alongside the most frequently observed errors we found through \textsection \ref{section:error_taxonomy} with the aim to explore the possibility of enhancing the reasoning capabilities of both closed-source and open-source LLMs. Four prompting techniques were utilized: (1) Chain-of-Thought \citep{wei2022chainofthought} (2) Plan-and-Solve \citep{wang2023plan} (3) Self-Correct  \citep{zhang2024smalllanguagemodelsneed}, and (4) Self-Discovery  \citep{zhou2024selfdiscoverlargelanguagemodels}. We used the zero-shot CoT (Figure \ref{fig:auto-eval-annot-example}) method as the baseline method in which the LLM is prompted to provide the final answer along with step-by-step reasoning. 

Plan-and-Solve prompts the LLM first to generate a plan to solve the problem without solving it and after that the LLM carries out the self-suggested plan to get the final answer as shown in Figure \ref{fig:plan-and-solve}. Self-Correct uses self-verification and self-refining to improve the reasoning ability of the LLMs as shown in Figure \ref{fig:self-correct}. Self-Discover as shown in Figure \ref{fig:self-discovery} utilizes self-discover reasoning modules to create an explicit reasoning structure to follow to solve the problem. For running the above prompting strategies with error taxonomy as the feedback, we include a detailed description of the error taxonomy in the system prompt. For instance, Plan-and-Solve with error feedbacks is shown in Figure \ref{fig:plan-and-solve-with-feedback}.



\input{Tables/few-shot}

\section{Few-shot prompting on performance of LLMs}
\label{section:few-shot}

We evaluated the performance of several large language models (LLMs) on a set of legal reasoning questions using zero-shot and few-shot prompting, as well as chain-of-thought (CoT) prompting. For Llama models, we used one example and for other we used 3 examples in few-shot training. Our results indicate that LLMs perform best in the zero-shot setting, where no in-context examples are provided. We hypothesize that the diverse nature of legal reasoning questions limit the effectiveness of in-context learning, as it restricts the model's ability to generalize beyond the provided examples.


% \begin{table}
%     \centering
%     \begin{tabular}{ccc}
%         Model & Zero-shot & Few-shot \\
%         Llama-3-8B-Instruct & 53.71  & 52.87 \\
%         Llama-3-70B-Instruct & 76.74  & 71.51 \\
%         GPT-4o & 82.56  & 80.23 \\
%          &   & \\
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table}

% \input{Tables/few-shot}

\begin{figure*}[tp]
    \centering
    
    % First row
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/mistral-premise-conc-corr.pdf}
        %\caption{Mistral Heatmap}
    \end{subfigure}
    \hfill % Space between the subfigures
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/llama-premise-conc-corr.pdf}
        %\caption{Llama Heatmap}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/gpt-3.5t-conc-corr.pdf}
        %\caption{GPT-3.5t Heatmap}
    \end{subfigure}
    
    \vspace{1em} % Space between rows

    % Second row
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/gpt-4t-conc-corr.pdf}
        %\caption{GPT-4t Heatmap}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/gpt-4o-conc-corr.pdf}
        %\caption{GPT-4o Heatmap}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/all-llm-average-conc-corr.pdf}
        %\caption{New Bar Graph}
    \end{subfigure}
    \caption{The first five sub-figures in the preceding section display the error distribution for premise-level errors in instances where the conclusion contains an error, across five different LLMs. The dimension in the x-axis represents the categories of errors at the premise level. The dimension in the y-axis represents the errors at the conclusion level. Here `M' denotes `Misinterpretation', `FH' denotes Factual Hallucination, `IP' denotes Irrelevant Premise, `WCFP' denotes Wrong Conclusion from False Premise(s), and `CCFP' denotes Correct Conclusion from False Premises. Here the counts in the heatmap represent the presence of one (or more) premise-level error(s) (in the x-axis) in a reasoning chain containing the specified conclusion-level error (in the y-axis)}
    \label{fig:prem_conc_corr}
\end{figure*}

\begin{figure*}[ht]
    \centering     
    \includegraphics[width=0.6\linewidth]{Images/Auto-Eval-System-Prompt.drawio.pdf}
    \caption{Prompt structure for LLM evaluator 2 from Figure \ref{fig:autoeval-system}. Appropriate Instructions and Knowledge Base for detecting `Misinterpretation' errors are provided in the prompt. Similar prompt structures and appropriate knowledge bases have been provided to all other LLM-based evaluators.} 
    \label{fig:misinterpretation-prompt}
\end{figure*}

\begin{figure*}
    \centering     \includegraphics[width=1.0\linewidth]{Images/Auto-Eval-single-call.drawio.pdf}
    \caption{Schematic representation of the `Single-Call' LLM-based auto-evaluator for premise-level error detection. 'Inst + KB' includes instructions for error analysis and the knowledge base with error definitions, based on the proposed error taxonomy. The \textcolor{green}{green} check-mark indicates the absence of errors and the \textcolor{red}{red} cross-mark represents the presence of an error in a premise.}
    \label{fig:autoeval-single-call}
\end{figure*}

\begin{figure*}
    \centering     \includegraphics[width=1.0\linewidth]{Images/Auto-Eval-multi-call.drawio.pdf}
    \caption{Schematic representation of the `Multi-Call' LLM-based auto-evaluator for 'Factual-Hallucination' error detection. `Inst + KB' includes instructions for error analysis and the knowledge base with error definitions, based on the proposed error taxonomy. The \textcolor{green}{green} check-mark indicates the verification question being correctly answered and the \textcolor{red}{red} cross-mark represents the presence of a contradiction in the content of the premise with the provided context.}
    \label{fig:autoeval-multi-call}
\end{figure*}

\begin{figure*}[t]
    \centering     
    \includegraphics[width=0.965\linewidth]{Images/Auto-Eval-System-conclusion-level.pdf}
    \caption{Schematic representation of the  LLM-based auto-evaluator system for error detection at conclusion-level. The GPT-4o LLM represents the `LLM evaluator 5' in Figure \ref{fig:autoeval-system}. The dotted lines represent the conditional paths of which only one will be true and lead to respectively conclusion-level error being labelled to the conclusion, Here, `CCHC' represents `Correct conclusion with Hallucinated Content, `CCFP' represents `Correct Conclusion from False Premises', `WCIP' represents `Wrong Conclusion from Incomplete Premises' and `WCFP' represents `Wrong Conclusion from False Premises'.} 
    \label{fig:autoeval-conclusion-level}
\end{figure*}

\begin{figure*}[t]
    \centering     
    \includegraphics[width=1.0\linewidth]{Images/Auto-Eval-System.drawio.pdf}
    \caption{Schematic representation of the  LLM-based auto-evaluator system for error detection. `Inst + KB' with their respective numbers in brackets includes instructions for error analysis and the knowledge base with error definitions, based on the proposed error taxonomy. An example prompt structure (for LLM evaluator 2) has been shown in Figure \ref{fig:annotation-guideline}. Here, 'P-level' denotes the Premise-level, and `C-level' denotes the Conclusion-level. The schematics of the `Single-Call' and `Multi-call' GPT-4o LLM-based evaluators are represented in Figure 
 \ref{fig:autoeval-single-call} and \ref{fig:autoeval-multi-call} respectively.} 
    \label{fig:autoeval-system}
\end{figure*}


\clearpage

% First subfigure on its own page
\begin{figure*}[p]
  \centering
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Images/mullane1.drawio.pdf}
    \caption{Part 1: Contains the Legal Context, Questions and Options.}
    \label{fig:subfig1}
  \end{subfigure}
  \caption{A Legal scenario which showcases the complexity of legal reasoning involved. This is one of the cases where all the LLMs under evaluation provided wrong options as answer.}
  \label{fig:legal-reasoning-complexity-part-1}
\end{figure*}

\clearpage

% Second subfigure, continuing the same figure number
\begin{figure*}[p]\ContinuedFloat
  \centering
  \begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{Images/mullane2.drawio.pdf}
    \caption{Part 2: Contains the Expert Answer, LLM step-by-step response and Reasoning challenges.}
    \label{fig:subfig2}
  \end{subfigure}
  \caption{Continuation from the previous sub-part of Figure \ref{fig:legal-reasoning-complexity-part-1}. This sub-part contains the expert-answer to a legal scenario, a wrong response provided by GPT-4o and the challenges faced by a legal reasoner to solve this scenario}
  \label{fig:legal-reasoning-complexity-part-2}
\end{figure*}

\begin{figure*}[t]
    \small
    \centering     \includegraphics[width=0.96\linewidth]{Images/Auto_eval-annotation-example.drawio.pdf}
    \caption{Example LLM Auto-Evaluator Response: Top: Zero-shot CoT prompt and LLM response. Bottom: Outputs from the LLM-based auto-evaluator.} 
    \label{fig:auto-eval-annot-example}
\end{figure*}

\begin{table*}[h]
\footnotesize
\centering
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Legal Context:} Under federal diversity jurisdiction a citizen of one state may sue a citizen of another in federal court, even though her claim arises under state law if she has a colorable claim for more than \$75,000. The state citizenship of a person—as opposed to that of a corporation—is determined by her domicile, that is, the most recent state where she has (1) resided with (2) the intent to remain indefinitely. The ‘‘residence’’ requirement is easily satisfied. Staying overnight in a hotel or a tent will establish ‘‘residence’’ in a state. It is the intent-to-remain-indefinitely prong that gives students problems. A person intends to remain indefinitely in a state if she is residing in the state on an open-ended basis, without the intent to leave at a definite time or on the occurrence of a definite event. You don’t have to swear allegiance forever to a state to acquire domicile there; you only need to reside there ‘‘indefinitely,’’ that is, on an open-ended basis. If a party is living in a state without definite plans to leave, the domicile test regards that state as her ‘‘home.’’ She is there, not as a visitor, but as a citizen. She is, psychologically speaking, at home there, rather than passing through. She may choose to move on, as we all may, but at the moment she has no plans to do so. Remember that, until the two prongs coincide in a new state, your old domicile continues, whether you plan to return to that state or not. If Acari, from Hawaii, leaves for a one-year job acting in a play in California, planning to go to New York afterwards, he remains domiciled in Hawaii, even if he swears that he will never return to Hawaii. He hasn’t acquired a domicile in California, because he doesn’t plan to stay there indefinitely. He hasn’t acquired one in New York either, since he doesn’t reside there yet. Domicile doctrine abhors a vacuum, so it holds that Acari keeps his Hawaii domicile until the two prerequisites come together in another state. In analyzing the question below, assume that the court applies the reside-with-intent-to-remain-indefinitely test, and consider where Marla has established a residence with the requisite ‘‘indefinite’’ intent.                      \\ \midrule
\textbf{Question:} Marla, who grew up in Montana, moved to Colorado after high school to enter a two-year program for hair stylists at the Denver Beauty School. She wasn’t sure if she really wanted to be a stylist, but she was anxious to get away from home, and her parents agreed to foot the bill, so off she went. She figured she’d stay if she liked it, and get a job as a stylist afterwards, in Denver or elsewhere in the West (including Montana). Or she would leave the program if she didn’t like it and look for work, hopefully in Denver. She took an apartment on a six-month lease. After moving to Denver, Marla\\ \textbf{Options:}\\ A. remains domiciled in Montana, because the program is only for two years. \\ B. remains domiciled in Montana, because she may return there to work as a stylist. \\ C. remains domiciled in Montana, because she was domiciled there before she left for Denver. \\ D. is domiciled in Colorado, because she resides there with the intent to remain indefinitely.                                \\ \midrule
\textbf{Human Annotator Analysis of LLM generated Reasoning Chain:}\\ {[}1{]} The question asks about Marla's domicile based on her current residence in Denver, Colorado, and her intentions regarding her stay there. \textcolor{blue}{(Correct)}\\ \\ {[}2{]} According to the legal context, a person's domicile is determined by the state where they reside with the intent to remain indefinitely.\textcolor{blue}{(Correct)}\\ \\ {[}3{]} Marla moved to Denver, Colorado, to attend a two-year hair stylist program. She was unsure about her career choice and had plans to leave the program if she didn't like it. She also mentioned that she might look for work in Denver or other western states, including Montana.\textcolor{blue}{(Correct)}\\ \\ {[}4{]} Marla's lease in Denver is for six months, which indicates that she does not have an open-ended intention to remain in Colorado indefinitely. \\ \textcolor{blue}{(\textbf{Misinterpretation - Omission of parts of the provided context while reasoning}) - (Lease in Denver is for six months (A) -> indicates that she does not have an open-ended intention to remain in Colorado indefinitely (B)) - Here B can not be true just because A is true, there are other factors which need to be true for B to be actually true. (A alone is not sufficient to prove that B is true.) (Incomplete/Insufficient cause for an effect)}\\ \\ {[}5{]} The fact that Marla was domiciled in Montana before moving to Denver does not automatically make her domiciled in Montana again. Domicile is determined by the present intent to remain indefinitely in a state, not by past domicile. \textcolor{blue}{(Correct)} \\ \\ {[}6{]} Based on the information provided, Marla does not meet the requirement of residing in Colorado with the intent to remain indefinitely. \textcolor{blue}{(Wrong Conclusion from False Premise) - Wrong Conclusion as a consequence of {[}4{]}; because of point {[}4{]}, the LLM seems to have drawn a wrong conclusion, Error Propagation from {[}4{]} to {[}6{]})} \\ \\ {[}Final Answer{]} A. remains domiciled in Montana.\\ \bottomrule
\end{tabular}
\caption{Example of a Misinterpretation and Wrong Conclusion due to False Premise. The complexities of this scenario are interpreting Marla's intent to remain indefinitely in Colorado by assessing her open-ended plans and lack of definite departure; distinguishing between temporary residence and establishing a new domicile by considering factors like her enrollment in a two-year program and a six-month lease against her willingness to stay in Colorado long-term; and applying domicile principles amid uncertainty.}
\label{table:human_annotation_example_initial}
\end{table*}

\begin{table*}[h]
\footnotesize
\centering
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Legal Context:} The most fundamental point about arising-under jurisdiction is that many cases that involve federal issues do not ‘‘arise under’’ federal law. This is the essential message of that civil procedure chestnut, Louisville \& Nashville R.R. v. Mottley, 211 U.S. 149 (1908). In Mottley, the plaintiffs sued the railroad for breach of contract, and alleged in their complaint that the railroad had breached the contract because it believed that a federal statute prohibited it from renewing their passes for free travel. In fact, when it answered the complaint, the railroad did rely on the federal statute as their justification for denying the passes. The parties then litigated that federal question and the federal trial court held that the federal statute did not bar renewing the passes. The United States Supreme Court reversed and ordered the case dismissed for lack of subject matter jurisdiction. The Court held that a case only ‘‘arises under’’ federal law, within the meaning of the federal statute granting arising-under jurisdiction, if the plaintiff relies on federal law as the source of her right to relief. That wasn’t true in Mottley: the plaintiffs had sued for breach of contract, a state law claim...\textcolor{blue}{\textbf{...Truncated}}                      \\ \midrule
\textbf{Question:} Consolidated Corporation sues Garces, a former employee, for business libel, a tort claim. The suit is brought in federal court, and alleges that Garces falsely reported to a federal agency that Consolidated was using watered-down concrete on a federal construction project. Garces answers, admitting that he had made a report to the agency, but alleging that his report was protected by the First Amendment to the United States Constitution, which guarantees the right of free speech. He also files a counterclaim against Consolidated, based on the Federal Whistleblower Act, which authorizes damages for anyone who is dismissed or disciplined for reporting fraud on the federal government. After answering the complaint, Garces moves to dismiss for lack of subject matter jurisdiction. (Assume that there is no jurisdiction based on diversity.) The motion should be\\ \textbf{Options:}\\ A. granted, because the court lacks subject matter jurisdiction over the case. \\ B. denied, because the complaint alleges that Garces reported the violations to a federal agency. \\ C. denied, because the case involves a question arising under the Constitution of the United States. \\ D. denied, because Garces’s counterclaim arises under federal law. \\ E. denied, because Garces waived his objection to jurisdiction by answering the complaint and asserting a counterclaim against Consolidated.                                \\ \midrule
\textbf{Human Annotator Analysis of LLM generated Reasoning Chain:}\\ {[}1{]} Understanding the Mottley Rule: The Mottley rule establishes that a federal court has jurisdiction over a case only if the plaintiff's claim itself arises under federal law. This means the plaintiff must base their right to relief on federal law for the case to be heard in federal court. \textcolor{blue}{(Correct)}\\
\\{[}2{]} Analyzing the Plaintiff's Claim: In this scenario, Consolidated Corporation's claim against Garces is for business libel, which is a state tort claim. The claim does not inherently rely on federal law for the basis of the relief sought.\textcolor{blue}{(Correct)}\\
\\{[}3{]} Considering the Defendant's Response: Garces's answer and the assertion of a First Amendment defense introduce a federal issue. However, under the Mottley rule, the introduction of a federal issue in the defense does not confer federal question jurisdiction if the initial claim did not arise under federal law. \textcolor{blue}{(Correct)}\\
\\{[}4{]} Examining the Counterclaim: Garces files a counterclaim under the Federal Whistleblower Act, which is a federal statute. This counterclaim does indeed arise under federal law. \textcolor{blue}{(Correct)}\\
\\{[}5{]} Jurisdiction Based on Counterclaim: Federal courts can have supplemental jurisdiction over claims that are related to claims in the action which the court has original jurisdiction. Since Garces’s counterclaim arises under federal law, the court has original jurisdiction over the counterclaim and could potentially exercise supplemental jurisdiction over the related libel claim. \textcolor{blue}{(\textbf{Misinterpretation - Incompletely applying a legal rule}) - This step incorrectly applies supplemental jurisdiction principles. A counterclaim that arises under federal law does not retroactively establish original jurisdiction for the plaintiff's state law claim. Under Mottley, jurisdiction is assessed based on the plaintiff's claim, and the counterclaim cannot create subject matter jurisdiction for the law libel claim.} \\
\\{[}6{]} Assessing the Motion to Dismiss: Garces moves to dismiss for lack of subject matter jurisdiction. Given that his counterclaim arises under federal law, and considering the potential for supplemental jurisdiction over the libel claim, dismissing the case for lack of subject matter jurisdiction would be inappropriate. \textcolor{blue}{(\textbf{Misinterpretation - Misunderstanding the legal rules \& Misunderstanding the legal situation/issue at hand}) - This step misinterprets the legal rule and scenario. The federal counterclaim does not save the case from dismissal because the court’s jurisdiction is determined based on the plaintiff’s well-pleaded complaint.}\\ \\
{[}Final Answer with the correct option{]} \\
D. denied, because Garces’s counterclaim arises under federal law. \textcolor{blue}{(Wrong Conclusion from False Premise)}
\\\bottomrule
\end{tabular}
\caption{Example of another Misinterpretation and Wrong Conclusion from False Premise. This scenario involves interpreting federal jurisdiction rules, distinguishing between federal defenses and federal claims, and determining the impact of counterclaims on jurisdiction.}
\label{table: misinterpretation-2}
\end{table*}

\begin{table*}[h]
\footnotesize
\centering
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Legal Context:}\\ The basic standard for removal jurisdiction is simple and sensible: a defendant sued in state court may remove a case to federal court if it could have been filed originally in federal court. The rationale for removal is to give both parties access to federal court for cases within the federal court’s jurisdiction. So it makes sense that the standard for removing a case to federal court should mirror the standard for filing one in federal court to begin with. So, 28 U.S.C. §1441(a) provides that ‘‘any civil action brought in a State court of which the district courts of the United States have original jurisdiction’’ may be removed to federal court. Consequently, the first question to ask in analyzing a removal problem is whether the claim asserted by the plaintiff fits into some category of federal jurisdiction. Is it a diversity case, or an arising-under case? A case between a citizen and foreign citizen, or some other case provided for in Article III, §2? If not, it can’t be removed. If it is, then, with a few minor exceptions, it can be. Here’s a first example to illustrate this relation between original and removal jurisdiction.          \\\hline
\textbf{Question:}\\ Castor Chemical Company, a California company, sues Pollux, from California, in state court for breach of contract, claiming that Pollux agreed to manufacture a drug for Castor and then reneged. Pollux answers the complaint, admitting that he refused to deliver the drug, and offering as an affirmative defense that he learned, after making the contract, that the drug was covered by a patent, so that, under federal patent law, he could not manufacture it without a license to do so. After answering, Pollux removed the action to federal court.\\ \\ \textbf{Options:}\\ A. The action is properly removed, because at the time he was removed, Pollux had raised a federal law issue in his answer. \\ B. The action is properly removed, because the case ‘‘arises under’’ federal law within the meaning of that phrase in Article III, §2. \\ C. The action is not properly removed, because it is a federal question case, not a diversity case. \\ D. The action is not properly removed, because the case does not ‘‘arise under’’ federal law within the meaning of 28 U.S.C. §1331.  \\\hline
\textbf{Human Annotator Analysis of Reasoning Chain 2:}\\ 1. The first step in determining whether a case can be removed to federal court is to identify whether the claim fits into any category of federal jurisdiction. In this case, the claim is for breach of contract between two California companies. \textcolor{blue}{(Factual Hallucination) - The breach of contract is between a person and a company.}\\ 2. The federal question jurisdiction, as per 28 U.S.C.§1331, applies to civil actions arising under federal law. In this case, Pollux has raised a federal law issue in his answer by asserting that he could not manufacture the drug due to a patent. \textcolor{blue}{(Correct)} \\ 3. However, the removal jurisdiction, as per 28 U.S.C. §1441(a), applies to civil actions brought in state court that have original jurisdiction in federal courts. \textcolor{blue}{(Correct)}\\ 4. In this case, the claim is not originally brought in federal court, but rather in state court. Therefore, even though a federal law issue has been raised, the case cannot be removed based on federal question jurisdiction alone.\\ \textcolor{blue}{(\textbf{Misinterpretation - Incorrectly applying the legal rule}) - The actual reason for why the case can not be removed to the federal court is that the plaintiff has invoked a state law tort in the claim and hence a federal court has no jurisdiction on this case even though the defendant raises a federal law issue. The correct standard is whether the plaintiff's original claim arises under federal law, not whether a federal issue is raised in the defense.}\\ 5. The correct answer is D: The action is not properly removed, because the case does not arise under federal law within the meaning of 28 U.S.C.§1331. \textcolor{blue}{(Correct Conclusion from False Premises)}\\ \\ \textcolor{blue}{(Both entities in same state jurisdiction -\textgreater Here, defendant is raising the federal jurisdiction claim-\textgreater Motley’s ‘arise under’ won’t apply (it only applies when plaintiff does that) -\textgreater Case can not be removed to Federal court)}\\ \bottomrule
\end{tabular}
\caption{Example of Factual Hallucination and Misinterpretation. The complexities of this scenario would include: applying the well-pleaded complaint rule to determine whether a case arises under federal law; distinguishing between state law and federal law claims for removal purposes; and recognizing that federal defenses do not confer federal jurisdiction, as established in the Mottley rule.}
\label{table: misinterpretation-3}
\end{table*}

\begin{table*}[h]
\footnotesize
\centering
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Legal Context:}\\ Let’s start with the role of the United States Constitution in defining the limits of a court’s power to subject a defendant to jurisdiction. The Fourteenth Amendment bars a state from depriving a person of life, liberty or property ‘‘without due process of law,’’ that is, without a basically fair procedure. If it’s a court that’s doing the depriving—by entering a judgment against a person and forcing her to pay it—basic fairness requires that the defendant have some relationship to the state where the court sits that will make it fair to conduct the litigation there. In civil procedure terms, that means that the court must have ‘‘a basis to exercise personal jurisdiction’’ over the defendant. In the major personal jurisdiction cases, such as International Shoe, World-Wide Volkswagen, Daimler, Asahi, Burnham v. Superior Court, and Bristol-Myers Squibb, the United States Supreme Court has provided some guidance as to the types of relations to a state that will support the exercise of jurisdiction. Some relations that satisfy due process under the Fourteenth Amendment include domicile in a state,1 being ‘‘at home’’ in a state,2 minimum contacts that give rise to the claim,3 and service of process on an individual in the forum state. 4 That’s not an exhaustive list, and of course there are refinements and ambiguities, but the point is that the Supreme Court has upheld certain relations to a state as sufficient under the Fourteenth Amendment to support personal jurisdiction over a defendant there. However, it is not enough that a defendant has a contact with the state that is sufficient under the Due Process Clause to allow the state to exert jurisdiction over him... \textcolor{blue}{\textbf{...Truncated}} \\ \hline
\textbf{Question:}\\ Perrone, an Acadia resident, sues Margules, a Montanan, in an Acadia state court, for defamation. He claims that Margules called Acme Corporation, an Acadia company that was planning to hire Perrone, and made false and defamatory statements about his professional abilities that led Acme to decide not to hire him. Margules placed the call from her office in Montana. Assume that Acadia has a long-arm statute that authorizes personal jurisdiction over a defendant who ‘‘causes personal injury in this state by a tortious act or omission while physically present within this state.’’ Assume that this is the only statute that might authorize the Acadia court to exercise personal jurisdiction over Perrone for this claim.\\ \\ \textbf{Options:}\\ A. The Acadia statute authorizes the court to exercise jurisdiction over Margules in this case. However, it would be unconstitutional under the Fourteenth Amendment for the court to exercise jurisdiction over her on the facts of this case. \\ B. The Acadia statute does not authorize the court to exercise jurisdiction over Margules in this case. In addition, it would be unconstitutional under the Fourteenth Amendment for the court to exercise jurisdiction over her on the facts of this case. \\ C. The Acadia statute does not authorize the court to exercise jurisdiction over Margules in this case. However, it would be constitutional under the Fourteenth Amendment for the court to exercise jurisdiction over her on the facts of this case. \\ D. Although the Acadia statute does not authorize the court to exercise jurisdiction over Margules in this case, the court may do so because the claim arises out of her deliberate contacts with Acadia that gave rise to Perrone’s claim.                      \\ \hline
\textbf{Human Annotator Analysis of LLM generated Reasoning Chain:}\\ {[}1{]} The first step is to determine whether the Acadia statute authorizes the court to exercise jurisdiction over Margules in this case. The statute in question authorizes personal jurisdiction over a defendant who causes personal injury in Acadia by a tortious act or omission while physically present within the state. \textcolor{blue}{(Correct)}\\ \\ {[}2{]} In this case, Margules is not physically present in Acadia when she allegedly committed the tortious act. She made the defamatory statements from her office in Montana. \textcolor{blue}{(Correct)}\\ \\ {[}3{]} Therefore, based on the text of the statute, it does not appear to authorize the court to exercise jurisdiction over Margules in this case.\textcolor{blue}{(Correct)}\\ \\ {[}Final Answer: B. The Acadia statute does not authorize the court to exercise jurisdiction over Margules in this case.{]}\textcolor{blue}{(Wrong Conclusion from Incomplete Premises) - Making a decision based on an incomplete set of information, without considering all necessary legal aspects before jumping to the conclusion. Here the validity of the due process clause is not carefully examined and a premature conclusion is drawn.}\\ \\ However, it's important to note that even if the statute does not explicitly authorize jurisdiction in this case, a court may still exercise jurisdiction if the defendant has sufficient contacts with the forum state such that the exercise of jurisdiction comports with due process. This is known as the "minimum contacts" standard, which is a constitutional requirement separate from the statutory authorization. In this case, Margules' contact with Acadia through her defamatory statements may be sufficient to establish minimum contacts, but that analysis is beyond the scope of this question. \textcolor{blue}{(\textbf{Misinterpretation - Wrong assumptions derived from the provided context}) - No, that analysis is not outside the scope of this question!, in fact the expert has used “minimum” contact (instead of physical contact) to narrow the option to C. Margules' contact with Acadia through her defamatory statements is indeed be sufficient to establish minimum contacts and constitutionally}\\                 \\ \bottomrule
\end{tabular}
\caption{Example of Wrong Conclusion from Incomplete Premises. (The legal context has been truncated to save space.)}
\label{table: misinterpretation-4}
\end{table*}


\begin{table*}[h]
\footnotesize
\centering
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Legal Context:}\\ 
The Due Process Clause of the Fourteenth Amendment prohibits a state from depriving a person of life, liberty, or property without due process of law. When a court enters a civil judgment against a person, it begins the process of taking the person’s property (usually in the form of money). Consequently, the due process clause requires the court to use a fair procedure in entering judgment. Certainly, one component of a fair procedure is to tell the defendant that the court is going to adjudicate her rights. Hence, the Due Process Clause requires a court to use a constitutionally adequate means of notifying the defendant that a lawsuit has been commenced against her. Typically, it is the plaintiff who does the legwork of serving process on the defendant. The statutes or court rules in every state contain detailed provisions governing how this notice of a lawsuit is provided to the defendant. Court rules may authorize various means of serving process. The most obvious is to deliver the initiating papers in the case to the defendant in person, called ‘‘personal service of process.’’ Other methods are often authorized as well, though they are less certain to actually inform the defendant about the suit. The service rules may provide, for example, that the papers may be left with someone at the defendant’s home or place of business or slipped under the door. Some authorize service to the defendant by certified or ordinary mail. In some circumstances, court rules or statutes may authorize service by publication, by printing a notice of the suit in the legal notices in the local newspaper for several weeks. Such publication notice is sometimes referred to as ‘‘constructive service,’’ as opposed to actual delivery of the papers to the defendant. Which of these methods is constitutionally adequate under the Due Process Clause? Justice Jackson’s wise opinion in Mullane v. Central Hanover Bank \& Trust Co., 339 U.S. 306 (1950) does not provide a mechanical answer to that question, but instead establishes broad standards for constitutionally adequate notice. Mullane involved a device called a common trust fund, which allowed a bank to pool a group of small trusts into one common fund, which could be managed more efficiently. Income (or losses) from the trust would then be shared by each smaller trust in proportion to their contributions to the fund. Periodically, the trustee of the fund would file accounts with the court. If the court approved the accounts, its judgment would bar investors from suing the bank for mismanagement. Since this judicial accounting could deprive trust beneficiaries of property—the right to sue the bank for mismanagement—due process required adequate notice to them of the proceeding. The problem in Mullane was that it was difficult to ascertain who all the beneficiaries were. Beneficiaries currently entitled to receive income from the trust were known, and the bank had their addresses in its files. But most trusts have contingent beneficiaries, such as the children or heirs of a beneficiary if the primary beneficiary dies. The bank might not know who these beneficiaries were, and they could change periodically. Perhaps, with a large investment of time by lawyers and investigators, the bank could identify most them, but doing so would eat up the profits of the common trust fund.\textcolor{blue}{\textbf{...Truncated}} \\ \midrule
\textbf{Question:}\\ In Mullane v. Central Hanover Bank \& Trust Co., the Supreme Court held that                              \\ \midrule
\textbf{Human Annotator Analysis of LLM generated Reasoning Chain:}\\ {[}1{]} The case of Mullane v. Central Hanover Bank \& Trust Co. involved a common trust fund where the bank pooled multiple trusts into one fund for more efficient management. The trustee would file accounts with the court, and if approved, the judgment would bar investors from suing the bank for mismanagement, potentially depriving them of their property right to sue. The issue was who the bank had to notify about the suit and how. \textcolor{blue}{(Correct)}\\

{[}2{]} The Supreme Court held that due process requires notice reasonably calculated to apprise interested parties of the pendency of the action and afford them an opportunity to present their objections. What is reasonable in one circumstance may not be in another. In Mullane, several factors reduced the risk of error if notice failed to reach all beneficiaries: the appointment of a guardian to represent their interests, and the likelihood that many beneficiaries would receive notice and share the interests of all beneficiaries in the settlement of the accounts. \textcolor{blue}{(Correct)}\\

{[}3{]} Based on the information provided in the case, the Court did not require that each individual with an interest in the fund be given personal notice of the action. Instead, the Court focused on the reasonableness of the notice given under the circumstances.\textcolor{blue}{(Correct)}\\

{[}4{]} Therefore, the correct answer is D: due to the large number of persons whose interests might be affected by the proceedings, notice by publication was sufficient on the facts of the case. \textcolor{blue}{(Wrong Conclusion from Incomplete Premises) - the LLM assumed on its own that notice by publication is sufficient, whereas the court ruling in Mullane, even though focusing on the reasonableness of the notice given in such circumstances, never mentioned that notice by publication was the right way! The ruling went for representation by Guardianship and Common Interests representation)
Step [4] simplifies the Court's nuanced position into a broad and incorrect generalization. While the Court did acknowledge the use of publication notice as part of the process, it did not conclude that publication alone was sufficient under the circumstances of this specific case.}\\\\

[Final Answer: D] 
                                                            \\ \bottomrule
\end{tabular}
\caption{Example of Wrong Conclusion from Incomplete Premises.}
\end{table*}

\begin{table*}[h]
\footnotesize
\centering
\begin{tabular}{p{{0.9\linewidth}}}
\toprule
\textbf{Legal Context:}\\ The relation between statutory diversity jurisdiction and the constitutional grant Article III, §2 of the Constitution creates diversity jurisdiction in very general terms. It does not expressly state whether there must be ‘‘complete diversity,’’ or whether diversity might be satisfied if some plaintiffs are diverse from some defendants.  Strawbridge v. Curtiss,  of course, held that all defendants must be diverse from all plaintiffs, but for many years it was unclear whether  Strawbridge interpreted Article III or interpreted the statute by which Congress conveys diversity jurisdiction to the lower federal courts. If  Strawbridge interpreted the language ‘‘citizens of different states’’ in Article III, the complete diversity rule would be constitutionally required, and could not be changed  \textcolor{blue}{\textbf{...Truncated}}                  \\ \hline
\textbf{Question:}\\ A diversity case cannot be heard in federal court unless the amount in controversy is at least\\ \hline 
\textbf{Options:}\\ A.  \$75,000, counting interest and costs as well as the plaintiff’s damages.\\ B.  \$75,000, not counting interest and costs.\\ C.  \$75,000.01, counting interest and costs. \\ D.  \$75,000.01, exclusive of interest and costs.
\\ \hline
\textbf{Human Annotator Analysis of LLM generated Reasoning Chain:}\\ {[}1{]} The legal context explains that the amount-in-controversy requirement is a condition imposed by Congress on the exercise of diversity jurisdiction in federal courts, which is not present in the constitutional grant of diversity jurisdiction in Article III, §2. \textcolor{blue}{(Correct)}\\ \\ {[}2{]} The question asks for the minimum amount in controversy that must be present for a diversity case to be heard in federal court. \textcolor{blue}{(Correct)}\\ \\ {[}3{]} Based on the legal context, the constitutional grant of diversity jurisdiction in Article III, §2, does not contain any monetary restriction. Therefore, there is no minimum amount in controversy required for a case to be considered a diversity case under the Constitution. \textcolor{blue}{(Irrelevant Premise) - The question is about the statutory requirement, not the constitutional one.}\\ \\ {[}4{]} However, the federal courts derive their jurisdiction from Congress, and Congress has imposed an amount-in-controversy requirement as a condition for the exercise of diversity jurisdiction. \textcolor{blue}{(Correct)}\\ \\ {[}5{]} Therefore, the correct answer is [B]: \$75,000, not counting interest and costs. This is the minimum amount in controversy required for a diversity case to be heard in federal court under the statutory grant of jurisdiction \textcolor{blue}{(Wrong Conclusion from False Premises) -  Mistral doesn’t fully understand the terms of the legal context, which states “the amount in controversy exceeds the sum or value of \$75,000, exclusive of interest or costs” not equals or exceeds, meaning the value must be greater than, not equal to, \$75,000.}\\ \bottomrule
\end{tabular}
\caption{Example of an Irrelevant Premise.}
\label{table:human_annotation_example_final}
\end{table*}