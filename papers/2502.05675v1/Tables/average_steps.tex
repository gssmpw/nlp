\begin{table}[!htbp]
\small
\centering
\begin{tabular}{lc}
\toprule
LLM   & Average number of steps\\ 
\midrule
Mistral-7B (893)   & 5.1\\  
Llama-3-8B (642)   & 3.66\\
GPT-3.5-turbo (649)   & 3.70\\
GPT-4-turbo (811)     & 4.63\\ 
GPT-4o (974)   & 5.56\\
\bottomrule
\end{tabular}
\caption{The average number of steps(premises) generated by all LLMs in Zero-shot CoT setting. The numbers in brackets indicate the total number of steps generated by each LLM in the generated reasoning chains (excluding the final conclusion step) for the 175 sample \textit{Civ. Pro.} dataset}
\label{table:average_steps}
\end{table}