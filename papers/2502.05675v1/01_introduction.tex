\section{Introduction}
\begin{figure}[ht]
    \centering     \includegraphics[width=0.8\linewidth]{Images/first_teaser.pdf}
    \caption{An example of determining domicile in a legal context. A reasoner must discern whether the condition of `indefinite to stay in a place' is met. While many LLMs predict Marla is domiciled in Montana since her program is only for 2 years, legally, her ambiguous plans indicate an intent to remain in Denver indefinitely, making her domiciled in Denver, not Montana.}
    \label{fig:first_teaser}
\end{figure}

\begin{figure*}[t]
    \centering     \includegraphics[width=1.0\linewidth]{Images/final_teaser.pdf}
    \caption{Overview of the proposed pipeline for evaluating legal reasoning in LLMs. The process begins with converting \textit{Civ. Pro.} dataset (\textbf{top left}), followed by generating reasoning chains using LLMs in a zero-shot CoT setting (\textbf{bottom left}). These chains are manually analyzed for various error types (\textbf{top right}), based on the proposed error taxonomy. The pipeline is then automated using an LLM-based system (\textbf{bottom right}) to assess reasoning chains for errors such as misinterpretation, providing insights into the LLMs' reasoning accuracy.}
    \label{fig:teaser}
\end{figure*}

Legal reasoning is a complex process requiring the careful application of rules, and precedents 
% (often using analogies) 
while balancing deductive and analogical reasoning, for various legal scenarios \citep{walker2007discovering, hafner2002role}. 
These challenges are heightened by reasoning through uncertainties (Figure \ref{fig:first_teaser}) and ambiguous laws (Figure \ref{fig:legal-reasoning-complexity-part-2}).
In recent years, Large Language Models (LLMs) %\citep{brown2020language, touvron2023llama, achiam2023gpt} 
have emerged as the most dominant AI models to process and generate natural language. There has been widespread research to showcase the emergence of natural language understanding (NLU) and reasoning abilities \citep{zelikman2022star, zelikman2024quiet, hao2023reasoning, mondorf2024beyond, lanchantin2024learning} of LLMs. These findings have opened up avenues to utilize LLMs in complex domains like Law \citep{lai2023large}. Consequently, there has been significant research in evaluating the performance of LLMs' legal reasoning ability \citep{guha2023legalbench, blairstanek2023gpt3, kang2023can}. 
{\em In this paper, we focus on providing a detailed, fine-grained analysis of the errors that occur during step-by-step legal reasoning using LLMs.}
While earlier works exist on evaluating step-by-step reasoning of LLMs \citep{golovneva2023roscoesuitemetricsscoring,prasad2023recevalevaluatingreasoningchains}, they do not specifically cater to legal reasoning. 

%Although there are numerous works for evaluating  logical reasoning \citep{yu2020reclor, han2024folio, saparov2022language}, and step-by-step reasoning \citep{golovneva2023roscoesuitemetricsscoring, prasad2023recevalevaluatingreasoningchains}, {\em evaluating the reasoning abilities of LLM through analyzing step-by-step rationales to solve legal scenarios remain underexplored.}

As shown in Figure \ref{fig:first_teaser}, analyzing legal scenarios requires extensive consideration of critical analysis of prior context. Hence, beyond just evaluating final answers, it is crucial to analyze the step-by-step reasoning chains generated by LLMs and where they falter to gauge their reasoning capabilities to solve such tasks. Despite continued improvements, LLM outputs are affected by fundamental challenges such as hallucinations \citep{dahl2024large, varshney2024investigating}, and misunderstanding long-contexts \citep{lu2024insights}. Our work aims to investigate such errors on a fine-grained level for each step generated to showcase the ability of LLM to perform legal reasoning. To this end, we leveraged the dataset from \citet{bongard2022legalargumentreasoningtask} (referred to as `\textit{Civ. Pro.}' throughout the paper) in the form of MCQA and analyzed reasoning chains generated by LLMs. The dataset comprises of 175 legal scenarios, each providing a comprehensive legal context that includes relevant rules, precedents, and exceptions. Following this context, questions and options are presented to test the understanding and application of these legal principles. The primary goal of using this dataset is to assess how well the LLM reasons logically and contextually when provided with all necessary legal rules, without relying on its inherent knowledge to recall them. This approach highlights the LLM's ability to perform deductive and analogical reasoning in complex, context-rich legal scenarios. Some of the widely used LLMs, Mistral-7B-v2-Instruct, Llama-3-8B-Instruct, GPT-3.5-turbo, GPT-4-turbo, and GPT-4o, are evaluated in zero-shot-CoT setting \citep{wei2023chainofthoughtpromptingelicitsreasoning} (as shown in Figure \ref{fig:teaser}). 

Subsequently, human evaluation of the LLM-generated reasoning chains against the ground truth solutions is performed for a subset of the dataset to find the most common errors being committed in the reasoning steps. This resulted in the development of a detailed error taxonomy, as outlined in Tables \ref{table:premise_errors} and \ref{table:conclusion_errors}, which allowed for a deeper understanding of the underlying causes of LLMs' failures. Additionally, to extend this evaluation across the entire dataset, we develop a method employing `LLMs as auto-evaluators' (inspired by \citet{liu2023gevalnlgevaluationusing, chern2024largelanguagemodelstrusted}) that leverages our proposed error taxonomy. The auto-evaluator framework achieves a recall of $\sim$87.06$\%$ in terms of identifying errors similar to human annotation. To derive better qualitative observations, we utilize two metrics: soundness and correctness (details in \textsection \ref{section: metrics}) to further analyze the reasoning chains. These metrics provide us with interesting insights, the most prominent of them being the inability to generate error-free rationales due to misinterpretation of the contextual nuances.

In the end, we apply various prompting strategies (details in \textsection \ref{section:mitigation_discussion}) known to enhance LLM reasoning abilities with the zero-shot method. We also incorporated our error taxonomy as feedback to the prompting strategies to assess their effectiveness in mitigating errors in legal reasoning. The prompting strategies enhanced with error taxonomy feedback show improved accuracy (maximum of $\sim$4$\%$) which suggests that providing information about errors help in improving the performance of LLMs. We hope that this framework for automatically evaluating step-by-step reasoning in complex tasks will be helpful for future research. In summary, the main contributions of our paper are: 
\begin{enumerate}[noitemsep]
    \item We propose an error taxonomy to systematically identify the most commonly occurring errors in step-by-step legal reasoning.
    \item We develop an LLM-based pipeline to automatically detect errors, and introduce two key metrics—soundness and correctness—to evaluate step-by-step legal reasoning.
    
    \item We investigate the integration of error-taxonomy feedback into various LLM prompting strategies and find that it enhances the LLMs' legal reasoning capabilities.
\end{enumerate}

% with an average of $\sim$1.5$\%$ across the two tested LLMs.

\label{sec:intro}

% \mihir{Write this line differently saying that "we leverage dataset from \citet{bongard2022legalargumentreasoningtask} in form of MCQA and analyze reasoning chains generated by LLMs" (the dataset is referred as `\textit{Civ. Pro.}' throughout the paper)}

% \mihir{Add one line about what this dataset content like context, all laws, question, option, etc}

% \mihir{Divide this in two lines: one saying which prompting techniques we have used and other saying baseline is without feedback and zero-shot and we show prompting with feedback}.

% \mihir{add one line showing how figure 1 shows informal logic and other aspects}

% (\textbf{FUTURE WORK}) Finally, we employed various mitigation based on improved prompting and finetuning which can target the errors primarily occurring in the logical reasoning. Our findings indicate that enhancing the reasoning chains at the individual statement level contributed to improved final answer accuracy for the large language models under evaluation. Last but not least, the insights from the detailed analysis also show the potential to be extended to domains beyond legal reasoning, specifically other logical-reasoning domains focused on primarily on informal reasoning. The key observations and contributions of our research are outlined as follows:

% There has been widespread research ongoing to investigate the reasoning capabilities of Large Language Models \citep{huang2023reasoning,qiao2023reasoning}, with logical reasoning being one of the crucial areas \citep{yu2020reclor,han2024folio}. There have been numerous advances in prompting \citep{wei2022chainofthought, wang2023selfconsistency, wang2022iteratively}, training \citep{liu2023improving, havrilla2024teaching} and technique-based \citep{li2023making, yu2022retrieval, chen2023program, gao2023pal} strategies for LLMs which have been used to demonstrate and improve their reasoning abilities in various domains. While popular reasoning benchmarks (for e.g. \citep{clark2018thinksolvedquestionanswering,parmar2024logicbenchsystematicevaluationlogical} and many reasoning tasks involve applying formal logic, legal reasoning involves navigating the intricacies of natural language arguments, taking into account the context, ambiguities, analogies and broader implications. Therefore, emphasis on soundness and the relevance of each reasoning step for solving the given problem and arriving at the conclusion is crucial. 

% The ground truth solutions provide context-rich information on how to approach and solve the given problem and hence serve as expert references to compare the reasoning chains trying to solve the question.
% This analysis offers quantitative insights into the most common errors that occur in reasoning steps, which would be applicable across various reasoning domains.

% LLM outputs are affected by several challenges, including hallucinations\citep{xu2024hallucination,dahl2024large}, difficulties in understanding long-contexts \citep{li2024long}, a lack of long-term memory \citep{zhong2024memorybank}, and inherent biases \citep{gallegos2024bias}, among others. The core argument of this research is that identifying and correcting the flaws in step-by-step reasoning chains is crucial for evaluating the soundness of LLMs' reasoning abilities. Recent research in legal domains \citep{guha2023legalbench, nguyen2023enhancing, blairstanek2023gpt3} has also started making overtures towards evaluating the reasoning capabilities of LLMs. To investigate the arguments generated for solving an informal logic based reasoning task, we have compiled a legal reasoning dataset designed to elicit reasoning chains generated by LLMs , allowing for a detailed evaluation based on the quality of their logical reasoning as well as real-world contextual understanding. 


% The compiled dataset is well-suited for this task as it requires generation of detailed reasoning chains. The construction of our dataset began by obtaining permission to use data samples sourced in the form of multiple-choice questions sourced from the Glannon Guide To Civil Procedure \citep{glannon2013guide}. The legal context containing the appropriate laws, precedents, and exceptions was augmented to the prompt of every legal scenario posed as a data sample. The primary objective was to evaluate how effectively the LLM reasons, both logically and contextually, when supplied with all relevant legal rules and sufficient context to resolve a legal scenario, without the need to rely on its inherent parametric knowledge to recall laws, precedents, or exceptions. This approach highlights the LLM's ability to perform deductive reasoning in complex, context-rich scenarios.

% The investigation started with manual analysis of the reasoning chains generated by the LLMs against the ground truth solutions on an empirical and statistical basis. Human evaluation of the LLM-generated reasoning chains was performed for a subset of the dataset to find the most common errors being committed in the reasoning steps. This resulted in the development of a detailed error taxonomy, as outlined in Tables \ref{table:premise_errors} and \ref{table:premise_errors}, which allowed for a deeper understanding of the underlying causes of LLMs' failures. Additionally, to extend this evaluation across the entire dataset, we devised a methodology employing 'large language models as auto-evaluators' (inspired by research in works like \citep{liu2023gevalnlgevaluationusing, chern2024largelanguagemodelstrusted} that leveraged the predominant errors identified through human annotation to compare the reasoning chains against the ground truth solutions. This analysis offers both qualitative and quantitative insights into the most common errors that occur in reasoning steps, which would be applicable across various reasoning domains. Lastly, we implemented various prompting strategies with the augmented error-taxonomy to investigate the scope of mitigating the errors occurring in the reasoning chains.

% In summary, our work makes three key contributions. First, we developed an easy-to-interpret error taxonomy to identify common causes of reasoning errors within reasoning chains generated by large language models (LLMs) for a legal reasoning dataset. Second, we created an LLM-automated evaluation system capable of detecting these errors with a defined level of confidence in accurately identifying them. Finally, we utilized the error taxonomy to analyze the scope of mitigating errors within the reasoning chains in the complex domain of legal reasoning, and compared the effectiveness of our approach with other existing prompt-based techniques.