\section{Related Work}
\label{sec:related_work}
\paragraph{Legal Reasoning with LLMs}
LegalBench ____, LawBench ____, LEXGLUE ____ and LEGALSEMI ____ have introduced exhaustive benchmarks which cover distinct tasks to measure the legal reasoning abilities of LLMs. ____ addresses the problem of hallucinations in LLMs, particularly their behavior in generating information that lacks factual accuracy. While these works have evaluated legal reasoning of LLMs on labelled predictions or through expert manual evaluation, our work specifically focuses on evaluating natural language step-by-step rationales with the help of LLMs. We also introduce a task-specific fine-grained error taxonomy to assess the reliability of LLMs in producing error-free reasoning chains.

\paragraph{Evaluation and Verification of Step-by-Step Reasoning Chains through LLMs}
ROSCOE ____ offers a comprehensive suite of metrics to assess various aspects of reasoning quality, including correctness, informativeness, consistency, and coherence. ReCEval ____, on the other hand, specifically targets incorrect answer detection by specifically analyzing the `correctness' and `informativeness' of reasoning steps. LLM Reasoners ____ introduces a novel evaluation framework for a detailed analysis of large language models’ step-by-step reasoning abilities. ____ introduces Natural Program, a natural language-based deductive reasoning format that decomposes a reasoning verification process into a series of step-by-step subprocesses. ____ explores the limitations of LLMs in detecting reasoning errors and highlights their effectiveness in correcting errors when provided with specific locations. ____ presents the DIVERSE approach involving a three-stage process to identify and correct errors at each step of the reasoning chain. While prior works ____ introduce valuable evaluation frameworks, they are not optimized specifically for a legal reasoning task. Our work utilizes soundness and correctness metrics, which are simple yet effective in offering detailed insights into step-by-step legal reasoning errors. These metrics also make evaluation scalable and reduce the need for manual effort with our formulated LLM-based evaluation framework. 

% \mihir{Here, we should write that there is no metric for evaluating errors in the legal reasoning domain, hence we propose - something like this.} 

% \mihir{how are you building on prior work and if you are building on which specific prior works?}

% \mihir{one question - why we did not write about correctness and soundness here? We have to say why we propose them here.} 

% \mihir{Suggestion: rather than just listing down the work, we should say how we are different than them.}

% \mihir{how our work is different? you are just saying other works evaluate on a broad-scale, but that does not show any limitations}

% SocREval ____ framework evaluates the reasoning capabilities of large language models (LLMs) in a reference-free setting, inspired by the Socratic method

% In ____, a new benchmark  `R2PE' introduces Process Discernibility Score (PDS) as a novel metric to measure the consistency and contradictoriness of reasoning steps, demonstrating its effectiveness in identifying errors in a model’s output

% The expressiveness of natural language generated to reason about and solve legal scenarios makes LLMs suitable candidates to tackle legal reasoning.