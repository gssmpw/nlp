\section{Results and Analysis}
\label{sec:results}

\subsection{Objective Evaluation}

\paragraph{Soundness metrics are high but correctness scores are low}
Table \ref{table:metrics} shows that the majority of the premises are error-free (with the highest being GPT-4o having 78.4\% of the generated premises being error-free). In contrast, Figure \ref{fig:prem_conc_corr} reveals that an average of $\sim$96\% of reasoning chains leading to conclusions from false premises have one or more misinterpretation errors in the intermediate premises. This finding, aided by empirical human analysis, suggests that much of the LLM-generated reasoning chain re-iterates existing context, while most errors occur in the smaller portion where new `decision-making' inferences are generated. The similarity in correctness score in Mistral-7B-v2-Instruct and Llama-3-8B-Instruct in contrast to the higher accuracy of Llama-3-8B-Instruct could be attributed to the lesser number of steps (see Table \ref{table:average_steps} (Appendix \ref{section:avg_steps_generated})) on average in the reasoning chain of Llama-3-8B-Instruct when compared to Mistral-7B-v2-Instruct. 
\input{Tables/metric_scores}

\paragraph{Accuracy vs. Correctness Score} Table \ref{table:metrics} and Figure \ref{fig:acc_correctness_results} show a sharp decrease (an average of $\sim$27\%) in the scores of accuracy to correctness across all LLMs. The highest fall in percentage is observed in Llama-3-8B-Instuct (31.4\% decrease). This is significant as it shows that while LLMs can arrive at the correct conclusion, there are a lot of cases where the reasoning chain they generate is not entirely error-free. These results also suggest that LLMs often rely on superficial correlations and patterns, likely learned in the training stages, to arrive at correct conclusions, rather than through genuine reasoning. In high-stakes domains such as legal, financial, and medical fields, it is imperative that the reasoning generated by LLMs is completely error-free as even minor inaccuracies in these critical areas can lead to significant consequences. This also underscores the necessity for robust evaluation mechanisms to ensure the reliability and correctness of model outputs.   

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{Images/accuracy_and_correctness.pdf}
    \caption{Performance of 5 LLMs in terms of Accuracy vs. Correctness on the \textit{Civ. Pro.} dataset. Here, Mistral stands for Mistral-7B-v2-Instruct, Llama stands for Llama-3-8B-Instruct, GPT-3.5t and GPT-4t stand for GPT-3.5-turbo and GPT-4-turbo respectively.}
    \label{fig:acc_correctness_results}
\end{figure}

\input{Tables/mitigation_results}
\paragraph{Larger, proprietary models `reason' better than smaller, open-source models}
Figure \ref{fig:acc_correctness_results} and Table \ref{table:metrics} convey that proprietary models generate more error-free reasoning steps and arrive at the correct conclusion more often than the open-source LLMs. An exception is GPT-3.5-turbo, which performs comparably to Llama-3-8B-Instruct and Mistral-7B-v2-Instruct, suggesting that training data and methods might play a more significant role in enhancing reasoning than merely scaling model parameters. 

\subsection{Reasoning Chain Evaluation}
\paragraph{`Misinterpretations' are the dominant category of errors at premise-level}
Figure \ref{fig:prem-level-cat} (Appendix \ref{section: premise-level-error-dist}) and Figure \ref{fig:prem_conc_corr} reveal that `Misinterpretation' is the most dominant category of error which occurs in the reasoning chains at the premise-level. This highlights that  LLMs struggle to fully grasp the nuanced complexities of legal scenarios requiring the demonstration of critical analysis in zero-shot CoT settings.  

% This aptly goes on to show that LLMs, while trained on vast amounts of data, do not represent and effectively `understand' the complex nuances of legal scenarios (or any other complex context requiring critical analysis) when used in a zero-shot setting which is equivalent to an experienced human reasoner (a lawyer in the scenario of legal reasoning) being able to capture the nuances and correctly solve the problem with minimal supervision/guidance. The frequent occurrence of misinterpretations within reasoning chains, which lead to conclusion-level errors, suggests that they are a primary factor contributing to flaws or failures in the reasoning process.
\paragraph{`Wrong Conclusion from False Premises' is the dominant category of error at conclusion-level} The prevalence of `Wrong Conclusion from False Premises' (Figure \ref{fig:conc-level-cat} (Appendix \ref{section: conclusion-level-error-dist})) in conclusion-level errors results from premise-level mistakes leading to incorrect conclusions. However, in GPT-4-turbo and GPT-4o, the dominant error is `Correct Conclusion from False Premises,' suggesting these models may be relying on patterns of similar examples from their training.  

\subsection{Discussion on Error-Mitigation Strategies}
\label{section:mitigation_discussion}
We carry out several experiments on the \textit{Civ. Pro.} dataset, employing widely used prompting techniques alongside the most frequently observed errors we found through \textsection \ref{section:error_taxonomy} with the aim to explore the possibility of enhancing the reasoning capabilities of both closed-source and open-source LLMs. Four prompting techniques are utilized: (1) Chain-of-Thought \citep{wei2022chainofthought} (2) Plan-and-Solve \citep{wang2023plan} (3) Self-Correct  \citep{zhang2024smalllanguagemodelsneed} and (4) Self-Discovery  \citep{zhou2024selfdiscoverlargelanguagemodels}. These techniques are tested with and without incorporating error definitions as feedback, following the Feedback-Learning method \citep{tyagi2024stepbystepreasoningsolvegrid}. Detailed descriptions of the prompting strategies can be found in Appendix \ref{section:prompting_strategies}.

The error definitions are provided in three styles: generic, short, and long. The generic version uses the error definitions from the Feedback-Learning method, while the short and long versions are derived from the error taxonomy described in \textsection\ref{section:error_taxonomy}. All experiments are conducted in a zero-shot setting, and we evaluate each prompting technique based on the accuracy metric. We test one closed-source model, Gemini-1.5-Flash, and one open-source model, Llama-3-8B-Instruct.

As shown in Table \ref{table:prompt_accuracy}, adding the error definitions as feedback showed improvement in accuracy up to ~4\%. For Llama-3-8B-instruct, accuracy improved across all prompting techniques, whereas for Gemini-1.5-Flash, the accuracy increased only for the Chain-of-Thought and Plan-and-Solve methods. From our observations, the decrease in accuracy for these strategies with Gemini resulted due to self-doubting \citep{krishna2023intersectionselfcorrectiontrustlanguage} nature of LLMs. These findings suggest that while feedback on errors provides marginal improvements in LLM performance, there is a need to develop more effective frameworks beyond prompting, such as agent-based methods, that account for these errors and enhance the modelâ€™s legal reasoning capabilities.

% These findings show The minor improvements highlight the need for strategies beyond prompting to achieve further performance gains. 

% This finding points towards the possibility that while feedback of errors committed helps in marginally improving LLM performance, they are not strong supervision signals for the LLMs to rectify their reasoning traces.

% \mihir{Write 1-2 lines about what are the takeaways. Why these experiments add value to current study?} 

% We used the zero-shot CoT method as the baseline method in which the LLM is prompted to provide the final answer along with step-by-step reasoning. Plan-and-Solve prompts the LLM first to generate a plan to solve the problem without solving it and after that the LLM carries out the self-suggested plan to get the final answer. Self-Correct uses self-verification and self-refining to improve the reasoning ability of the LLMs. Self-Discover utilizes self-discover reasoning modules to create an explicit reasoning structure to follow to solve the problem. Lastly, we follow Feedback-Learning which provides the definitions and methods to navigate through the identified error taxonomy. The detailed prompt structure for each technique is illustrated in the Appendix. 

% and GPT-4o outperforms Llama3-70B in every prompting strategy. It is also should be noted that Self-Discovery is the most costly prompting technique due to the use of more tokens and the 4-step structure to get the final answer.

% \begin{table*}[!htbp]
% \small
% \centering
% \begin{tabular}{lcccccc}
% \toprule
% Model   & Baseline & Plan-and-Solve  & Self-Correct & Self-Discovery & Feedback-Learning & Self-Ranking \\ 
% \midrule
% GPT-4o   & 78.85  & 80.00 & 80.00 & 76.57 & 79.43 & 80.57 \\ 
% Llama-3-70B   & 50.29  & 52.00 & 51.43 & 48.00 & 52.00 & 57.71 \\ 
% \bottomrule
% \end{tabular}
% \caption{The results for accuracy for GPT-4o and Llama-3-70B on the legal reasoning dataset.}
% \label{table:accuracy}
% \end{table*}

% \st{sheds light on the finding} \mihir{do not use words like this}