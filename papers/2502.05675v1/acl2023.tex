% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl2023}
% review
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{cite} % Optional, for improved citation handling
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{tcolorbox}
\usepackage{stackengine}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor,colortbl}         % colors
\usepackage{times}
\usepackage{latexsym}
\usepackage{multicol}
\usepackage{blindtext}
\usepackage{tabu}
\usepackage{amsmath, bm}
\newcommand{\sub}{\textsubscript}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{subcaption}
\usepackage{caption}
\usepackage[normalem]{ulem}
\usepackage{soul}
\usepackage[shortlabels]{enumitem}
\usepackage{array}
\usepackage{pgffor}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{mwe}
\usepackage{float}
\newcommand{\xmark}{\ding{55}}
\definecolor{lightgreen}{rgb}{0.8, 0.95, 0.8}
\definecolor{lightred}{rgb}{0.95, 0.8, 0.8}
\definecolor{naplesyellow}{rgb}{0.98, 0.85, 0.37}
\definecolor{pastelyellow}{rgb}{0.99, 0.99, 0.59}

% \newcommand{\Venkatesh}[1]{\textbf{\small {\color{blue}[#1 -Venkatesh]}}}
\newcommand{\mihir}[1]{\textcolor{cyan}{[Mihir: #1]}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{Investigating the Shortcomings of LLMs in Step-by-Step Reasoning using Context-Rich Legal Scenarios}
\title{Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning}

% \title{Investigating the Shortcomings of Large Language Models in Step-by-Step Reasoning using Context-Rich Legal Scenarios}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{Venkatesh Mishra$^{1*}$ \quad Bimsara Pathiraja$^1$\thanks{\ \ Equal Contribution} \quad Mihir Parmar$^1$ \quad Sat Chidananda$^1$  \\ \textbf{Jayanth Srinivasa}$^2$\quad  \textbf{Gaowen Liu}$^2$ \quad \textbf{Ali Payani}$^2$ \quad \textbf{Chitta Baral}$^1$ \\\\ 
$^1$Arizona State University \quad $^2$Cisco Research\\
\small{\texttt{\{vmishr23, bpathir1, chitta\}@asu.edu}}
}
\begin{document}
\maketitle
\begin{abstract}

Reasoning abilities of LLMs have been a key focus in recent years. One challenging reasoning domain with interesting nuances is legal reasoning, which requires careful application of rules, and precedents while balancing deductive and analogical reasoning, and conflicts between rules.  Although there have been a few works on using LLMs for legal reasoning, their focus has been on overall accuracy. In this paper, we dig deeper to do a step-by-step analysis and figure out where they commit errors. We use the college-level Multiple Choice Question-Answering (MCQA) task from the \textit{Civil Procedure} dataset and propose a new error taxonomy derived from initial manual analysis of reasoning chains with respect to several LLMs, including two objective measures: soundness and correctness scores. We then develop an LLM-based automated evaluation framework to identify reasoning errors and evaluate the performance of LLMs. The computation of soundness and correctness on the dataset using the auto-evaluator framework reveals several interesting insights. Furthermore, we show that incorporating the error taxonomy as feedback in popular prompting techniques marginally increases LLM performance. Our work will also serve as an evaluation framework that can be used in detailed error analysis of reasoning chains for logic-intensive complex tasks\footnote{Data and source code are available at \url{https://github.com/VenkyMishra/legal_reasoning}}.


%With the advent of Large Language Models (LLMs) and their adoption in the legal domain, it is important to analyze their legal reasoning abilities. Since legal reasoning requires these models to reason logically, evaluating it in LLMs also provides valuable insight into their logical reasoning skills. To the best of authors' knowledge, most existing work evaluating LLMs' legal reasoning primarily focus on their ability to predict the correct final answer, and there have not been much attention paid to analyzing its step-by-step reasoning process and where they falter. Motivated by this, we systematically evaluate the reasoning processes of LLMs on college-level Multiple Choice Question-Answering (MCQA) task from the \textit{Civil Procedure} dataset. We propose a new error taxonomy derived from initial manual analysis of reasoning chains from LLMs including Mistral-7B, Llama-3-8B, GPT-3.5, GPT-4-Turbo, and GPT-4o. Leveraging this taxonomy, we develop an LLM-based automated evaluation framework to identify reasoning errors, akin to human error identification. Additionally, we evaluate the performance of LLMs using two objective measures, soundness and correctness scores. Furthermore, we propose that incorporating the error taxonomy as feedback in existing prompting methods enhances LLM reasoning abilities and improves their final performance. We believe that this work provides evaluation framework for future exploration of detailed error analysis of reasoning chains for more logic-intensive complex tasks\footnote{Data and source code are available at <anonymous link>}.     

% Legal reasoning requires highly critical multi-step analytical and logical reasoning while considering the subtle differences and nuances of different legal scenarios. Evaluating the legal reasoning capabilities of large language models (LLMs) can serve as a valuable measure of its overall reasoning abilities. While almost all of the existing work on legal reasoning evaluate the performance of LLMs based on their final answer, in-depth analysis of the most common errors made by LLMs in their intermediate reasoning steps is crucial to improve the reasoning abilities of LLMs. Thus, in this work, we critically analyze the reasoning capabilities of various LLMs on a context-rich multiple choice question answering dataset containing 175 college-level law problems from Civil Procedure encompassed in the US federal and state laws. The study aims to assess the correctness of these models' reasoning processes, highlighting their strengths and limitations in scenarios where they have access to all the necessary knowledge. Based on meticulous human evaluation, we have created an error taxonomy of the common errors committed by five LLMs (Mistral-7B-v2-instruct, Llama-3-8B-instruct, GPT-3.5-turbo, GPT-4-turbo, and GPT-4o) while generating the reasoning chains. Our proposed automated evaluator for reasoning step error identification helps scaling up the evaluation on the entire dataset. The prompt-based error mitigation strategies which leverage the proposed error taxonomy \textit{outperform}\footnote{Mitigation of errors investigation currently in progress} prompting methods which do not incorporate this framework. 
% We have proposed various mitigation strategies to tackle and prevent fallacies of these X categories from occurring during the logical reasoning process. These strategies demonstrate that LLMs can improve their reasoning abilities, even on challenging datasets such as LRBENCH and tasks beyond legal reasoning.

%Now, we utilize these manually annotated samples based on error categories as few-shot exemplars for GPT-4 and explore its capabilities to evaluate reasoning chains. On the puzzle-solving task, we show that our proposed method easily identifies various error types. Based on various error categories, we propose to utilize some base prompting methods to improve the performance of LLMs on puzzle-solving. We believe that our framework for error categorization can be useful to many reasoning tasks in improving performance beyond puzzle solving.

%annotate different errors in reasoning chains for puzzle-solving task. Based on that, we propose a guideline to 
\end{abstract}

\input{01_introduction}

\input{02_related_work}

\input{03_error_categories}

\input{04_experiments}

\input{05_results_and_analysis}

\input{06_conclusion}


% \section*{Ethics Statement}
% We have utilized AI assistants, specifically Grammarly and ChatGPT, to correct grammatical errors and rephrase sentences

% \section*{Acknowledgement}

% We extend our gratitude to the Research Computing (RC), and Enterprise Technology at ASU for providing computing resources, and access to the ChatGPT enterprise version for experiments. We acknowledge the support of a CISCO research grant.

% Entries for the entire Anthology, followed by custom entries
\bibliography{references}
\bibliographystyle{acl_natbib}

\clearpage

\appendix
\input{07_appendix}
\end{document}
