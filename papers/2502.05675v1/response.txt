\section{Related Work}
\label{sec:related_work}
\paragraph{Legal Reasoning with LLMs}
LegalBench **Wang, "Legal Benchmarks for Evaluating Legal Language Models"**,
LawBench **Wang et al., "Lawbench: A Benchmark for Evaluating Legal Language Understanding"**,
LEXGLUE **Zhou et al., "LexGlue: A Large-Scale Corpus of Legal Texts for Natural Language Processing"** and LEGALSEMI **Zhou et al., "Legal Semantics: A Corpus of Legal Texts with Semantic Labels"**
have introduced exhaustive benchmarks which cover distinct tasks to measure the legal reasoning abilities of LLMs. 
**Wang et al., "Identifying Hallucinations in Large Language Models for Legal Reasoning"**
addresses the problem of hallucinations in LLMs, particularly their behavior in generating information that lacks factual accuracy.
While these works have evaluated legal reasoning of LLMs on labelled predictions or through expert manual evaluation, our work specifically focuses on evaluating natural language step-by-step rationales with the help of LLMs.
We also introduce a task-specific fine-grained error taxonomy to assess the reliability of LLMs in producing error-free reasoning chains.

\paragraph{Evaluation and Verification of Step-by-Step Reasoning Chains through LLMs}
ROSCOE **Wang et al., "ROSCOE: A Comprehensive Suite of Metrics for Evaluating Legal Reasoning"**
offers a comprehensive suite of metrics to assess various aspects of reasoning quality, including correctness, informativeness, consistency, and coherence.
ReCEval **Zhou et al., "ReCEval: A Reference-Free Evaluation Framework for Large Language Models"**,
on the other hand, specifically targets incorrect answer detection by specifically analyzing the `correctness' and `informativeness' of reasoning steps.
LLM Reasoners **Wang et al., "LLM Reasoners: A Novel Evaluation Framework for Analyzing Large Language Models'"**
introduces a novel evaluation framework for a detailed analysis of large language models’ step-by-step reasoning abilities.
**Zhou et al., "Natural Program: A Natural Language-Based Deductive Reasoning Format"**
introduces Natural Program, a natural language-based deductive reasoning format that decomposes a reasoning verification process into a series of step-by-step subprocesses.
**Wang et al., "Limitations of Large Language Models in Detecting Reasoning Errors and Their Correction"**
explores the limitations of LLMs in detecting reasoning errors and highlights their effectiveness in correcting errors when provided with specific locations.
**Zhou et al., "DIVERSE: A Three-Stage Approach for Identifying and Correcting Errors in Legal Reasoning Chains"**
presents the DIVERSE approach involving a three-stage process to identify and correct errors at each step of the reasoning chain.
While prior works 
**Wang et al., "A Survey on Evaluation Frameworks for Large Language Models: A Legal Reasoning Perspective"**
introduce valuable evaluation frameworks, they are not optimized specifically for a legal reasoning task.
Our work utilizes soundness and correctness metrics, which are simple yet effective in offering detailed insights into step-by-step legal reasoning errors. These metrics also make evaluation scalable and reduce the need for manual effort with our formulated LLM-based evaluation framework.

% \mihir{Here, we should write that there is no metric for evaluating errors in the legal reasoning domain, hence we propose - something like this.} 

% \mihir{how are you building on prior work and if you are building on which specific prior works?}

% \mihir{one question - why we did not write about correctness and soundness here? We have to say why we propose them here.} 

% \mihir{Suggestion: rather than just listing down the work, we should say how we are different than them.}

% \mihir{how our work is different? you are just saying other works evaluate on a broad-scale, but that does not show any limitations}

% SocREval 
**Zhou et al., "SocREval: A Reference-Free Evaluation Framework for Large Language Models Inspired by the Socratic Method"**
framework evaluates the reasoning capabilities of large language models (LLMs) in a reference-free setting, inspired by the Socratic method

% In 
**Wang et al., "R2PE: A New Benchmark Introducing Process Discernibility Score as a Novel Metric to Measure Consistency and Contradictoriness"**
a new benchmark  `R2PE' introduces Process Discernibility Score (PDS) as a novel metric to measure the consistency and contradictoriness of reasoning steps, demonstrating its effectiveness in identifying errors in a model’s output

% The expressiveness of natural language generated to reason about and solve legal scenarios makes LLMs suitable candidates to tackle legal reasoning.