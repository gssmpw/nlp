\section{Conclusion}
Through our work, we assess the reasoning capabilities of LLMs by examining their performance on the `\textit{Civ. Pro.}' dataset designed to evaluate legal reasoning. The sequential workflow of manually evaluating LLM-generated reasoning chains on data samples, creating an error taxonomy based on the commonly occurring errors, automating the error evaluation through LLM-based pipelines lead us to some interesting insights. LLMs still struggle with producing error-free rationales while reasoning about legal scenarios despite being provided with necessary legal knowledge and context. Additionally, we present our findings on implementing various prompting techniques augmented with feedback from our error taxonomy. We hope that our work lays a solid foundation for developing a framework to critically evaluate complex reasoning tasks, such as legal reasoning, and is extendable to other domains in a similar manner.    

\section*{Limitations}
While the \textit{Civ. Pro.} dataset provides a valuable benchmark for assessing the legal reasoning capabilities of LLMs, real-world legal reasoning—particularly in the context of legal judgment prediction—is inherently ambiguous. Legal cases often evolve over time, with new information emerging throughout the litigation process. Consequently, evaluations based on static datasets that capture information at a single point in time may not fully reflect the dynamic nature of legal decision-making, limiting their effectiveness in assessing real-world legal reasoning. There are many legal scenarios in this dataset that are complex in terms of being ambiguous and tricking the reader/reasoner and would probably require legal expertise and experience to understand the nuances required to solve such scenarios. Although our study intends to capture the errors committed by natural-language-based reasoners, the systematic evaluation could benefit from converting natural language to formal language through auto-formalization. While our work mainly focuses on the soundness of the reasoning steps of legal reasoning chains, we would like to acknowledge that properties like consistency, coherence, completeness, and clarity are not directly measured in the current work. Furthermore, the current evaluation of reasoning is restricted to English, leaving room to expand this work into a multilingual context.

\section*{Ethics Statement}
 We obtained the necessary permissions to use the dataset provided by \citet{bongard2022legalargumentreasoningtask}. We have utilized AI assistants, specifically Grammarly and ChatGPT, to correct grammatical errors and rephrase sentences.

\section*{Acknowledgement}

We thank the anonymous reviewers for their constructive suggestions. We extend our gratitude to the Research Computing (RC), and Enterprise Technology at ASU for providing computing resources, and access to the ChatGPT enterprise version for experiments. This work was in part supported by a gift award from Cisco Research. We thank Nimeesh Mahajan, Alexander Carr and Ren Cheng for their help in human evaluations.