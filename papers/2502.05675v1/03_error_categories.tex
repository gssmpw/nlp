\input{Tables/premise-level-error-taxonomy}
\section{Evaluation of Reasoning Chains}
\label{sec:error_category}

\subsection{The \textit{Civil Procedure} Dataset}
The dataset has been sourced from MCQs present in the `The Glannon Guide To Civil Procedure' \citep{glannon2013guide}. We compile the \textit{Civ. Pro.} dataset with 175 samples of college-level law multiple-choice questions from the US Civil Procedure laws. The questions are primarily designed to evaluate the ability of university-level law students to reason about various legal scenarios about US Civil Procedure laws and provide their final judgment by choosing the most correct option as an answer. The dataset includes relevant legal context, multiple-choice questions, and expert answers with correct explanations provided by legal experts. These elements were extracted and converted into a prompt-based format suitable for LLM inference and the generation of reasoning chains. The \textit{Civ. Pro.} dataset consists of samples comprising of $\mathcal{D} = {<lc_{n}, q_{n}, o_{n}, e_{n}>}$, where $lc_n$, $q_{n}$, $o_{n}$ and $e_{n}$ denote the $n^{th}$ legal context, question, option-set and expert-answer respectively.  


 % The questions range from straightforward applications of relevant rules to find the answers to complex legal scenarios involving scenarios with a multitude of combination of statutes, precedents and exceptions
 % To find out the commonly occurring errors in the reasoning chains generated by the LLMs, we performed a detailed sentence-level analysis of the chains. We began by segmenting the chains into the individual statements which form part of the argument, comprising of set of premises and a conclusion. To avoid confusion and maintain simplicity, we specify all intermediate sentences in the reasoning chain to be premises and the final sentence which provides the final answer by choosing an option from given options as the conclusion. The premises may be single declarative sentences or combination of declarative sentences and inferences made from those declarations.
 
 % For sound and valid argument construction, the LLM must accurately deduce applicable laws, follow correct reasoning steps, and reach an appropriate conclusion. Similar to solving a deductive reasoning problem, our focus is on evaluating the soundness of arguments, particularly in the context of informal legal reasoning. 
\subsection{Manual Evaluation Of Reasoning Chains}
\label{section:manual-eval-sec}
Human evaluators are instructed to find flaws in a reasoning chain and explain the flaws in natural descriptive language. To solve a given legal question in \textit{Civ. Pro.}, an LLM generates a set of statements $<$$A: s_1, s_2, ..., s_k, c$$>$, where $A$ represents the legal argument/rationale put forward to solve the problem, with $s_1, s_2...s_k$ being the `\textit{k}' number of intermediate steps generated to reason towards the final conclusion $c$. Each step in the reasoning-chain, including the final conclusion, is separately evaluated for the presence/absence of errors. To create an error taxonomy, we adopt an exhaustive approach, continuously updating the taxonomy until no new errors are identified. Specifically, 120 reasoning chains containing approximately 537 reasoning steps are used for evaluation (generated as responses by four LLMs: Mistral-7B-v2-Instruct, Llama-3-8B-Instruct, GPT-3.5-turbo and GPT-4-turbo, to the same 30 data sample subset). This evaluation helped to solidify our proposed taxonomy as described in \textsection \ref{section:error_taxonomy}. Detailed statistics of the human-evaluations are provided in Tables \ref{table:manual-evaluation-premise} and \ref{table:manual-evaluation-conclusion} of Appendix \ref{section:manual-eval}. Further details regarding annotation guidelines and process, inter-annotator agreement statistics using Cohen's kappa coefficient \citep{cohen1960kappa} and annotation examples are provided in Appendix \ref{section:annotation_guidelines} and Appendix \ref{section:human_annot} (Tables \ref{table:human_annotation_example_initial}-\ref{table:human_annotation_example_final}).

%These initial annotations and analyses gave rise to the formulation of an error taxonomy on a fine-grained level.
% divided classifying the errors on two-broad levels: finding errors in 1. Premise-level, the intermediate step level of the rationale, and 2. Final Conclusion level, where an option is chosen as the answer to the legal MCQ. Futher details and examples of the process are provided in  

% Premise-level errors were divided into: 1. Misinterpretation 2. Factual Hallucinations and 3. Irrelevant premises. Conclusion-level errors were divided into: 1. Wrong conclusion from False Premises. 2. Wrong Conclusion from Incomplete Premises 3. Correct Conclusion from False Premises 4. Correct Conclusion from Incomplete Premises and 5. Correct Conclusion with Hallucinated Output 

\subsection{Proposed Error Taxonomy}
\label{section:error_taxonomy}
The error taxonomy is designed to mirror the types of errors humans make when reasoning about passage comprehension and constructing rational arguments. It classifies errors into two levels: 1. Premise-level and 2. Conclusion-level errors. Premise-level errors are based on `Errors of Law' and `Errors of Fact' grounded in the legal domain \citep{cornell2024mistake, cornell2024mistake_fact, oreilly2012errors, wilberg2023mistake}. While premise-level errors often influence errors at the conclusion level, many conclusion-level errors occur independently. Conclusion-level errors serve as indicators of the  overall decision-making ability of LLMs in generating the final answer to a legal question. 

\paragraph{Premise-level Errors}
These errors have occurred in one of the premises of the reasoning chain. They highlight the core issue with LLMs that ineffectively prioritize relevant parts of the prior context and incorrectly identify important information. We categorize these errors as shown in Table \ref{table:premise_errors}.
 
% \paragraph{Category 1: Misinterpretation}
% This is the dominant category of error which occurs in in the premise-level of the step-by-step rationale generated by LLMs. The underlying issue with LLMs is their inability to dynamically prioritize specific parts of prior context and 'intelligently' discern what information is relevant and what can be disregarded. 
% The following error instances fall under the taxon of misinterpretation: 1. Misunderstanding the legal rules. 2. Misunderstanding the legal situation/issue at hand. 3. Incorrectly applying the legal rule. 4. Incompletely applying a legal rule. 5. Omission of parts of the provided context while reasoning. 6. Wrong assumptions derived from the the provided context. 

% \paragraph{Category 2: Factual Hallucinations} This error category includes instances where the LLM, during its reasoning process, cites information that is either inconsistent with the facts of the given legal scenario or is entirely fabricated with no basis in reality. 

% \paragraph{Category 3: Irrelevant Premises}
% The error occurs when the LLM reasoner generates a premise which is not directly useful in reasoning towards an answer. These premises might be factually true and logically valid but they do not contribute towards proving the final conclusion. These errors can deviate the line of reasoning towards focusing on some aspect which would draw wrong conclusions.
\input{Tables/conclusion-level-errors-taxonomy}
\input{Tables/example_auto_evaluation}

\paragraph{Conclusion-level Errors} 
Conclusion-level errors indicate issues with deductive reasoning, reflecting the LLM's ability to follow premises to reach the correct conclusion. They also reveal how much the decision-making process is influenced by intermediate premises in choosing the final answer. We categorize these errors as shown in Table \ref{table:conclusion_errors}.  
% \paragraph{Category 1: Wrong Conclusion from False Premise(s)}
% This error primarily occurs when the step-by-step rationale generated includes premises that are logically invalid, factually incorrect, irrelevant to solving the question posed, or a combination of these issues.   
% These factors render the intermediate step as being 'False' and therefore lead the line of reasoning towards a wrong conclusion. 

% \paragraph{Category 2: Wrong Conclusion from Incomplete Premise(s)}
% This error occurs when the step-by-step generated rationale includes premises which, while correct and valid towards the answering question, fall short of providing the entire line of reasoning which could lead to the correct conclusion. As a result, the wrong conclusion gets selected as the answer. A special case of this error is the 'Wrong Conclusion from Correct Premises' where even though the premises generated are true and sufficient to find the correct conclusion, the LLM reasoner still spuriously chooses a wrong conclusion as its answer.

% \paragraph{Category 3: Correct Conclusion from False Premises}
% This error occurs when the LLM reasons to the correct option while providing a wrong argument. One or multiple premises contain errors which fall under one of the three premise-level error categories and yet lead to the reasoning path choosing the correct option as its final answer.  

\begin{figure*}[t]
    \centering     
    \includegraphics[width=0.95\linewidth]{Images/Autoeval.pdf}
    \caption{The overall schematic representation of the LLM-based error-detection and evaluation system and the calculation of the metrics. The reasoning chains are produced by 5 LLMs and the expert answer is referenced from the \textit{Civ. Pro.} dataset}
    \label{fig:autoeval}
\end{figure*}

\paragraph{Conclusion from Incomplete Premises \textit{vs.} Correct Premises}
We argue that a `Wrong Conclusion from Correct Premises' is essentially a `Wrong Conclusion from Incomplete Premises' because either the premises, though correct, are incomplete and lead to a wrong conclusion, or the LLM fails to explicitly generate a key premise. This poses a challenge for LLM-based auto-evaluators, as discussed in \textsection \ref{section:llm_aided_eval}, which struggle to assess whether the rationale is sufficient or inadequate.

\subsection{LLM-aided Automatic Evaluation}
\label{section:llm_aided_eval}
Manual analysis of reasoning chains provided a detailed categorization of errors; however, it was time-consuming and, therefore, challenging to scale for the entire dataset. Thus, we develop an alternate approach to leverage LLMs to evaluate the errors in the reasoning chains akin to human evaluation. Specifically, we use GPT-4o as the LLM backbone of the `auto-evaluator' system to identify and label the errors. The auto-evaluator assesses a total of 875 reasoning chains, encompassing approximately 4,844 individual reasoning steps, which include both premise-level and conclusion-level steps (refer Table \ref{table:average_steps}). The details of the implementation are described in Appendix \ref{section:auto-eval} and an example snippet of LLM-aided annotation is provided in Table \ref{table:example_annot}. We develop two approaches for error evaluation:     

\paragraph{Exact Error Label Match} In this approach, we task the `auto-evaluator' with identifying the exact error category labels which the human evaluators had labeled a particular premise/conclusion of a reasoning chain. Experiments revealed significant mislabeling between the auto-evaluator and human evaluators, with many `Misinterpretation' errors at the premise level being labeled as `Irrelevant Premises' or `Factual Hallucination' (Refer Appendix \ref{section:error_disambiguation}) by the auto-evaluator, and vice versa. Hence, we make changes to the auto-evaluators to include error explanations along with the labels.

Another significant challenge was the low error detection rate of factual hallucinations with the help of single-call LLM auto-evaluators. Motivated by \citet{varshney2023stitchtimesavesnine, dhuliawala2023chainofverificationreduceshallucinationlarge}, we develop a multi-call LLM system, consisting of two separate LLM calls,  in which one LLM call creates verification questions to probe various aspects of a premise and another LLM call answers them citing the provided legal context for factuality. A premise is considered to contain factual hallucination if the answers to any of the verification questions contradicts the content of the premise directly.  

\paragraph{Semantic Error Explanation Match} As an alternative approach to the above problems, we develop a multi-analyzer system consisting of three `single-call' and one `multi-call' LLM-based pipeline focused on providing explanation of errors at the premise-level. A `summarizer' LLM (Refer Appendix \ref{section: summ_agg_llm}) combines the individual analyses of all analyzers into a single error explanation for a premise. This enables the pipeline to detect and label multiple errors in a single premise (e.g., a premise containing both misinterpretation and factual hallucination).

To validate the effectiveness of the auto-evaluator, we sample 120 reasoning chains from the manually evaluated set of four LLMs (Mistral-7B-v2-Instruct, Llama-3-8B-Instruct, GPT-3.5-turbo and GPT-4-turbo). The human evaluators then compare their error category assignments as well as explanations to those provided by the auto-evaluator. The recall percentage of detecting an error at the premise level across four LLMs ranged from 83.87$\%$ to 90.6\%. The recall percentage range for detecting an error-free premise step ranged from 86.17$\%$ to 93.85$\%$. The details of the autoevaluator performance statistics are present in Tables \ref{table:auto-eval-agreement-premise} and \ref{table:auto-eval-agreement-conclusion} of Appendix \ref{section:auto-eval-agree}. Figure \ref{fig:autoeval-system} shows the pipeline of the error detection implemented using GPT-4o. 

% The agreement score (recall percentage) for detecting the presence of an error (through explanation match) in reasoning steps between the manual evaluation and the GPT-4o evaluation was $\sim$85.3$\%$. The agreement score (recall percentage) for detecting the absence of an error (through explanation match) in reasoning steps between the manual evaluation and the GPT-4o evaluation was $\sim$86$\%$