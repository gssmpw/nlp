\section{Related Work}

\noindent \textbf{LLM Security and Privacy.} 
Recent advancements in LLMs have raised significant concerns regarding their security and privacy. 
One of the most pressing security issues in LLMs is adversarial attacks~\cite{liu2024adversary,nicholas2023align,erfan2023surveyadversary}, where attackers manipulate inputs to deceive the model into producing incorrect outputs. These attacks exploit the modelâ€™s vulnerabilities by introducing subtle, often imperceptible, perturbations to the input data~\cite{andy2023universal}, causing the model to behave unexpectedly. Additionally, jailbreaking attacks~\cite{xu2024jailbreak}, a form of adversarial attack, aim to bypass the built-in restrictions or safety mechanisms of LLMs~\cite{alexander2023jailbroken}, enabling the model to perform harmful or unintended actions~\cite{shen2024dan}. 
Backdoor attacks~\cite{li2024backdoorllm,zhao2025surveybackdoor} represent another critical vulnerability in LLMs. Recent studies have highlighted backdoor vulnerabilities in various contexts, such as code completion LLMs~\cite{yan2024codebackdoor}, customized GPTs~\cite{zhang2024gptsbackdoor}, RAG systems~\cite{cheng2024trojanrag}, and LLM agents~\cite{wang2024badagent,yang2024agentbackdoor}.
While existing research has primarily concentrated on content security and vulnerabilities intrinsic to the models themselves, our work broadens the understanding of LLM-related vulnerabilities by focusing on their integration within real-world software systems.

\noindent \textbf{LLM System Vulnerabilities.}
Recent studies have highlighted a range of vulnerabilities associated with the integration of LLMs into software systems. 
One prominent category of vulnerabilities is prompt injection~\cite{sahar2023promptinjection,liu2023houyi}, which can lead to severe security risks such as RCE~\cite{liu2024llmrce} and SQL injections~\cite{pedro2025prompt2sql}. These vulnerabilities arise when attackers manipulate prompts to inject malicious code or SQL queries, allowing them to exploit LLM-integrated applications. 
Additionally, pre-trained models themselves have become targets for exploitation, including PyTorch~(pickle-based)~\cite{zhao2024malhug} and TensorFlow models~\cite{zhu2025tensorflow}. This highlights the potential for malicious actors to manipulate LLMs at the framework level, leveraging their capabilities to launch attacks. 
More recently, privacy and security risks in multi-tenant LLM environments have also been highlighted, particularly with KV-cache sharing vulnerabilities~\cite{song2024kvcache}. This underscores the need for robust isolation mechanisms in multi-tenant LLM deployments to prevent information leakage and ensure user privacy.
While these studies provide important insights into specific vulnerabilities, the overall security landscape of the LLM ecosystem remains largely unknown. Our work systematically investigates vulnerabilities across the entire LLM supply chain, thereby offering a more comprehensive understanding of the vulnerabilities within LLM-integrated systems.