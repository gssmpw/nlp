\section{RQ4: Unique Challenges}
Detecting vulnerabilities in the LLM supply chain presents unique challenges due to the intricate nature of LLMs and their integration into software systems. While many vulnerabilities share characteristics with traditional software flaws, such as injection attacks, improper access control, and insecure data handling, the unique aspects of LLM systems introduce additional layers of complexity in vulnerability detection. To better understand these challenges, we examine the three main layers of an LLM system: the data layer, model layer, and application layer—and identify the specific obstacles each presents for vulnerability detection.

\noindent \textbf{Data Layer.}  
The data layer, which involves data processing tasks such as indexing, embedding, and storage in vector databases, is critical to the functioning of LLM systems. A significant challenge in this layer arises from the use of high-performance languages like C++, Rust, and Golang to implement vector databases, with Python often being used as the interface for user interaction. The complexity of cross-language interactions between these systems introduces several vulnerabilities, particularly memory-related issues in vector databases. These vulnerabilities can include buffer overflows, memory corruption, and data leakage, which are difficult to detect and mitigate. The presence of these vulnerabilities is exacerbated by the reliance on fast, low-level memory operations in high-performance languages, which can result in hard-to-trace errors. Additionally, these vulnerabilities are not always easily observable in high-level programming languages like Python, further complicating detection efforts.

% \textit{Static Analysis Challenges.}  
% In the case of vector databases implemented in C++, Rust, or Golang, static analysis faces several challenges. First, the complexity of cross-language implementation makes it difficult for static analyzers to trace vulnerabilities across language boundaries, particularly when the database's internal implementation is in a lower-level language, while the user interacts with the system through a higher-level language like Python. Static analyzers often struggle to detect issues like buffer overflows or memory leaks in the underlying C++ or Rust code that might only manifest when the Python interface is invoked under certain conditions. Furthermore, the performance optimizations commonly used in these languages, such as manual memory management, make it harder to predict and validate memory usage patterns statically. This leads to a high rate of false negatives or missed vulnerabilities in static analysis results.

% \textit{Fuzz Testing Challenges.} 
% Fuzz testing for vector database faces unique challenges due to the complexity of cross-language interaction. The vector database, implemented in C++, Rust, or Golang, may accept inputs from Python, which requires generating inputs that correctly bridge the gap between these languages. Generating effective test cases that exercise both the high-level Python interface and the low-level database implementation is difficult, as the interaction between the languages may not be fully covered by random inputs alone. Additionally, fuzz testing may not fully simulate real-world usage scenarios in LLM systems, especially when dealing with large-scale data or specific data structures. Furthermore, the unpredictable behavior of memory-related vulnerabilities in low-level languages can make it challenging to identify and isolate the root cause of issues found during fuzz testing. These complexities increase the difficulty of using fuzz testing as a comprehensive vulnerability detection method in this layer.

\noindent \textbf{Model Layer.}  
A prominent characteristic of vulnerabilities in the model layer is related to the handling of model formats. In traditional software systems, untrusted inputs typically come from user-provided remote data; however, in large-scale model systems, remote models themselves should be treated as untrusted inputs. This is particularly evident in systems like PyTorch, where models are loaded via insecure formats such as Pickle, or TensorFlow, where models should be considered as executable code. In the case of \texttt{huggingface/transformers} (CVE-2023-6730), an insecure deserialization vulnerability was identified when \texttt{RagRetriever.from\_pretrained()} loads a model from an untrusted source, leading to RCE. Similarly, in the \texttt{parisneo/lollms-webui} project (CVE-2024-4897), a model chat template is rendered via Jinja2 from a remote source, also resulting in RCE. Despite the fact that the sinks in these vulnerabilities—such as \texttt{pickle.load} in CVE-2023-6730 and \texttt{jinja2.Environment} in CVE-2024-4897—are conceptually similar to traditional software vulnerabilities, detecting these issues via taint analysis remains a significant challenge.
The core issue with taint analysis in the model layer lies in the difficulty of formally defining the ``source'' of a vulnerability. In traditional web applications, taint sources are well-defined (e.g., user-submitted form data or HTTP headers), and the flow of tainted data can be traced using established models. However, in the context of remote model loading, the source of vulnerabilities often comes from model files fetched from various remote locations, making it difficult to apply existing taint specifications in a systematic way.

% These sources can vary significantly depending on the model format and how it is fetched from remote repositories. For instance, in CVE-2024-4897, the source of taint is the \texttt{pretrained\_model\_name\_or\_path} parameter. In the case of CVE-2024-4897, the vulnerability source originates from a \texttt{chat template} within the gguf model file. The variability in the form and nature of these remote inputs complicates the application of traditional taint analysis, which typically relies on a more uniform definition of sources and sinks.

\noindent \textbf{Application Layer.}   
The application layer is where many of the most prominent challenges in LLM vulnerability detection emerge. One of the core issues in this layer is the inherent uncertainty of model outputs. Due to the generative nature of LLMs, outputs can vary significantly depending on the input, making it difficult to fully trust the results without extensive validation. This uncertainty directly contributes to vulnerabilities like CWE-1426 (Improper Validation of Generative AI Output). Developers may overly rely on protection mechanisms such as input filtering or model alignment, assuming they are more effective than they actually are. This false sense of security can lead to prompt injection attacks, where malicious inputs manipulate the model to generate harmful or unintended outputs, such as SQL injection, command/code injection, or XSS attacks. Inadequate validation or filtering of these model-generated outputs allows attackers to exploit these vulnerabilities, potentially executing malicious code or exposing sensitive data.

% Another critical challenge is the assumption that LLMs are deployed in trusted environments. Many LLM-based applications, particularly in UI frameworks and apps, are designed with the assumption that they are running in a secure, local environment. This assumption leads to a lack of robust authentication and authorization mechanisms. As a result, systems become vulnerable to unauthorized access and privilege escalation. This oversight is particularly evident in projects like \texttt{gaizhenbiao/chuanhuchatgpt} and \texttt{stitionai/devika}, where the absence of proper access controls exposes the system to significant risks. Furthermore, the integration of external tools, plugins, and orchestration frameworks in the application layer introduces additional attack surfaces that require careful security management. The complexity of managing these diverse components increases the potential for vulnerabilities, necessitating a more comprehensive approach to security in LLM-based applications.
