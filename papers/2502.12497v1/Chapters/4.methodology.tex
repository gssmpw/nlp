\section{Approach}
\subsection{Study Overview}
To systematically examine the prevailing security vulnerabilities in the LLM supply chain, we designed a comprehensive methodology that integrates automated data collection, manual analysis, and multi-dimensional evaluation. 
The detailed methodology is illustrated in ~\autoref{fig:methodology}.
We began by identifying and collecting repositories and artifacts relevant to LLMs and their associated components from GitHub. For each repository, we crawled vulnerabilities from established databases (e.g., MITRE CVE, GitHub Advisory Database), security issues reported on bounty platforms (e.g., \texttt{huntr}), and security reports from prominent platforms (e.g., Protect AI, Oligo, HiddenLayer). These collected vulnerabilities form the candidate dataset for further analysis.

With the manually labeled vulnerabilities, we study 4 research questions~(RQs). First, for \textbf{RQ1}, we categorized vulnerabilities based on CWE classifications and examined their distribution across different stages in the lifecycle of LLM systems. This analysis provides insights into the most vulnerable phases and components of LLM workflows. For \textbf{RQ2}, we investigated the root causes of these vulnerabilities, creating a taxonomy that highlights the underlying issues across various LLM architectures and components. In \textbf{RQ3}, we analyzed fix patterns, summarizing common solutions and evaluating the potential side effects of vulnerability patches. Finally, \textbf{RQ4} focuses on comparing the identified vulnerabilities in LLM systems with those in traditional DL systems, highlighting the unique challenges posed by the LLM ecosystem.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{Figures/methodology.pdf}
    \caption{Approach Overview.}
    \label{fig:methodology}
\end{figure*}

\subsection{Data Collection and Pre-processing}

\noindent \textbf{Repository Identification.} Following existing works~\cite{quan2022jsdl,chen2023dlframework,shen2021compiler,chen2021faults,johirul2019bug}, we first use the GitHub search API~\cite{githubsearch} to collect repositories that are related to LLM tools and systems, in including the LLM frameworks~(e.g. Transformer, llama.cpp), third-party LLM tools~(e.g. langchain, RAGFlow), and web ui~(e.g. open-webui, LocalAI). We searched for repositories on GitHub using keywords such as ``LLM'', ``pre-trained models'', ``GPT'', and ``transformer''. We included popular repositories with high star counts~(more than 1 thousand), active maintenance~(updated within 1 year), and high developer engagement~(more than 30 issues). In total, we collected 619 candidate repositories. Furthermore, we manually check the remaining repositories to exclude irrelevant repositories that are not real LLM systems, e.g. some tutorials, books, or repositories that contain the keywords but do not actually use the LLM. Finally, 567 repositories are selected. The details of the repositories can be found on our website.

\noindent \textbf{Vulnerability Sources}. To ensure comprehensive coverage, we collected vulnerabilities from a wide range of sources, including official vulnerability databases such as MITRE~\cite{mitre} and GitHub Advisory~\cite{advisory}, bug bounty platforms like huntr~\cite{huntr}, and security reports from organizations specializing in AI security, such as Protect AI~\cite{protectai}. We crawled vulnerability lists from the MITRE CVE List and the GitHub Advisory Database, using repository names as the basis for our search. This resulted in the collection of 1729 vulnerabilities from MITRE and 342 from GitHub Advisory, which were further crawled to extract relevant advisories, patches, and other associated information.
For \texttt{huntr}, a platform specifically focused on AI/ML-related vulnerabilities, we collected all publicly disclosed vulnerability reports up to October 5, 2024, totaling 1497 vulnerabilities. Given the structured nature of these reports, we extracted key information such as vulnerability descriptions, proof-of-concept (PoC), impact, and occurrence details. Additionally, we crawled the comments of \texttt{huntr} reports, where discussions between bug hunters, platform administrators, and project maintainers often provided valuable insights into vulnerability fixes and potential discussions of similar issues, enriching our analysis.
To identify vulnerabilities directly disclosed by the community, we employed a targeted search strategy using repository names combined with keywords such as ``vulnerability'' on Google. This strategy led to the identification of four well-known companies related to LLM infrastructure security: Protect AI, JFrog, Hidden Layer, and Oligo. We subsequently collected vulnerability reports disclosed by these organizations, which resulted in the collection of 392 reports from Protect AI, 143 from JFrog, 47 from Hidden Layer, and 11 from Oligo.
These efforts allowed us to gather a diverse set of vulnerability reports, further enriching our dataset and providing a broad perspective on vulnerabilities in AI/ML security.

\begin{table}[ht]
\centering
\caption{Vulnerability Data Collection and Processing Summary. S1: Filtered by year (2023-2024); S2: Filtered by repository relevance; S3: Deduplication across sources.}
\fontsize{9}{13}\selectfont
\begin{tabular}{lccccc}
\hline
\textbf{Source}              & \textbf{Collected} & \textbf{S1} & \textbf{S2} & \textbf{S3} & \textbf{Final} \\ \hline
huntr                        & 1497        & -174         & -926         & /           & 397         \\ \hline
Github Advisory     & 342         & -69          & -38          & -235         & 92          \\
MITRE           & 1729        & -734         & -634         & -310         & 51          \\ \hline
Protect AI                   & 392         & /            & -58          & -326         & 8          \\
JFrog                        & 143         & /            & -125         & -6           & 12          \\
Hidden Layer                 & 47          & /            & /            & -13          & 34          \\
Oligo                        & 7           & /            & /            & -4           & 3           \\ \hline
\textbf{Total}                     & \textbf{4157}        & \textbf{-997}         & \textbf{-1781}        & \textbf{-894}         & \textbf{597}         \\ \hline
\end{tabular}
\label{tab:vuln_data_summary}
\end{table}

\noindent \textbf{Preprocessing}. To refine the dataset for manual analysis, we performed several preprocessing steps~(as shown in \autoref{tab:vuln_data_summary}). First, we filtered vulnerabilities by year, retaining only those reported in 2023 and 2024 to ensure relevance to the current landscape of LLM security. Second, we assessed the repository relevance by cross-checking each vulnerability to verify whether it was related to LLM infrastructure or belonged to relevant open-source repositories. Finally, we removed duplicates across sources to avoid overrepresentation of the same vulnerability. After these filtering and deduplication processes, we obtained a set of 597 unique vulnerabilities that were manually labeled and analyzed.

\subsection{Classification and Manual Labeling}

To systematically characterize vulnerabilities in the LLM ecosystem, we manually labeled the 597 vulnerabilities from six aspects: (1) \textit{relevance to LLM infrastructure}, (2) \textit{affected lifecycle stage}, (3) \textit{root cause}, (4) \textit{symptoms and impact}, (5) \textit{fixing status and patterns}, and (6) \textit{recurrence or existence of similar vulnerabilities}. The labeling process followed an iterative approach inspired by open coding procedure~\cite{opencoding} and prior empirical studies on software bugs and vulnerabilities~\cite{shen2021compiler,chen2023dlframework,chen2021faults,quan2022jsdl,lai2024dlvul}, ensuring both comprehensive coverage and high reliability. Below, we detail the methodology.

\noindent \textbf{Pilot Labeling.} We began with a pilot labeling phase, during which the first two authors, who have three and five years of experience in security research respectively, independently labeled a randomly selected subset (10\%) of vulnerabilities. Specifically, they follow the procedures described below. The two authors carefully read all vulnerability reports and analyzed all available Information, including titles, detailed descriptions, PoCs, impact statements, associated fix pull requests (PRs), and any developer discussions. For each vulnerability, they assigned short but descriptive phrases as initial labels to characterize its root cause and fix strategy. After reviewing the subset of vulnerabilities, the two authors independently constructed taxonomies for root causes and fix strategies. Specifically, they grouped similar root causes and fixes into categories, iteratively refining these groupings to ensure that they accurately represented the vulnerability data. This iterative process involved revisiting the reports and adjusting the taxonomy as new patterns or inconsistencies were identified. In cases where the two authors disagreed on the categorization of a vulnerability, the third author, who served as an arbitrator, intervened to facilitate discussions and resolve conflicts. This process continued until a full agreement was reached on all labels for the pilot set. 

\noindent \textbf{Labeling Consistency Evaluation.} To ensure the reliability and consistency of the labeling process, we measured the inter-rater agreement between the two authors using Cohenâ€™s Kappa coefficient~($\kappa$)~\cite{cohenkappa}, which is widely used in existing works~\cite{quan2022jsdl,chen2023dlframework,chen2021faults,liu2023distributed}. During the pilot labeling phase, the initial $\kappa$ score was calculated to be 0.65, indicating substantial agreement but leaving room for improvement. To address this, we conducted a training session to clarify the labeling guidelines, refine the taxonomy definitions, and resolve ambiguities in the classification process. Following the training session, the two authors independently labeled an additional 10\% of the dataset to re-evaluate the agreement. This time, the $\kappa$ score increased to 0.91, reflecting near-perfect agreement. Encouraged by this improvement, we proceeded to the full dataset labeling phase, maintaining the same process of independent labeling followed by conflict resolution through arbitration by the third author. As labeling progressed, the inter-rater agreement was periodically measured on subsequent batches of vulnerabilities. By the completion of the full labeling task, the $\kappa$ score consistently exceeded 0.8, demonstrating excellent agreement between the two authors. For vulnerabilities where disagreements arose, the authors revisited the original reports and discussed their interpretations with the third author until a consensus was reached. This process not only ensured that all vulnerabilities were labeled consistently but also allowed us to iteratively refine the taxonomy to better capture the nuances of LLM-specific vulnerabilities. The final agreement indicates the robustness and reliability of our code schema and procedure. In summary, among the 597 manually analyzed vulnerabilities, we identified 529 that meet the criteria, as they are indeed related to LLM infrastructure and contain sufficient detailed information for analysis.

\subsection{Research Questions}
To better understand the security vulnerabilities in the LLM supply chain and provide actionable insights for improving the robustness of these systems, we define the following RQs:

\textbf{RQ1~(Distribution):} \textit{What are the characteristics and lifecycle distributions of vulnerabilities in LLM systems?} 

\textbf{RQ2~(Root Cause):} \textit{What are the root causes of vulnerabilities in LLM systems?} 

\textbf{RQ3~(Fix Patterns):} \textit{How are vulnerabilities in LLM systems fixed, and what are the common fix patterns?} 

\textbf{RQ4~(Unique Challenges):} \textit{How do vulnerabilities in LLM systems compare to those in traditional DL systems?}

RQ1 is expected to investigate the distribution of vulnerabilities across different lifecycle stages (e.g., data preprocessing, training, deployment) and categorize them using CWE classifications to identify critical phases and components in LLM workflows. RQ2 focuses on identifying the root causes of vulnerabilities, constructing a taxonomy to uncover common patterns and inform secure LLM design. RQ3 analyzes the fix patterns of vulnerabilities, summarizing mitigation strategies, recurring patching methods, and their side effects to provide practical security guidance. Finally, RQ4 compares vulnerabilities in LLM systems with those in traditional DL systems, highlighting the unique challenges and risks of the LLM ecosystem and identifying areas requiring specialized security measures.
