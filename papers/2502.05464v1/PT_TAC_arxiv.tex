

%\documentclass{ieeeconf}
%\documentclass[journal,twosided,web]{ieeecolor}
%\usepackage{generic}
\documentclass[journal]{IEEEtran}



\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi

\IEEEoverridecommandlockouts                             
% This command is only needed 

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{dsfont}
\usepackage{color}
\usepackage{epstopdf}        
\usepackage{enumerate}
%\usepackage{enumitem}
\usepackage{lscape}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{calligra}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{framed} 
\usepackage{picins}
\usepackage{empheq}
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{amsfonts}  % assumes amsmath package installed
%\usepackage{subfig}
\usepackage{epstopdf}
\usepackage{enumerate}
\usepackage{graphicx} % graficos
\usepackage{xcolor}
\usepackage{comment} 

\usepackage{pst-all}
\usepackage{pstricks}

\usepackage{subfigure} %%I have added this package

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%
\usepackage{caption2} % to avoid the blue marine weird paragraphs after captions

\usepackage{etoolbox}

\usepackage{soul}% 
\usepackage{cancel}

\newcommand{\tcr}{\textcolor{red}}
\newcommand{\tcb}{\textcolor{black}}
\newcommand{\tcbb}{~}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\newcommand*{\QEDB}{\hfill\ensuremath{\square}}%
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

    
%\markboth{IEEE TRANSACTIONS ON AUTOMATIC CONTROL}
%{Espitia \MakeLowercase{\textit{et al.}}: IEEE TRANSACTIONS ON AUTOMATIC CONTROL}



\definecolor{Nicolas}{rgb}{0.017,0.59,0.55}
\newenvironment{SUBENVNicolas}{\color{Nicolas}}{\color{black}}
\newcommand{\Nicolas}[1]{\begin{SUBENVNicolas}\textbf{Nicolas's comment: #1}\end{SUBENVNicolas}}


\definecolor{Jorge}{rgb}{0.217,0.51,0.255}
\newenvironment{SUBENVJorge}{\color{Jorge}}{\color{black}}
\newcommand{\Jorge}[1]{\begin{SUBENVJorge}\textbf{Jorge's comment: #1}\end{SUBENVJorge}}

\definecolor{Miroslav}{rgb}{0.74,0.2,0.64}
\newenvironment{SUBENVMiroslav}{\color{Miroslav}}{\color{black}}
\newcommand{\Miroslav}[1]{\begin{SUBENVMiroslav}\textbf{Miroslav's comment:} #1\end{SUBENVMiroslav}}
%\newcommand{\Miroslav}{\textcolor{Miroslav}}




%
\begin{document}
\title{%Delay-Compensated  
\bf Prescribed-Time Newton Extremum Seeking\\ using Delays and Time-Periodic Gains}
%Prescribed-Time Newton Extremum Seeking \\ by Time-Varying Delayed Feedback\\ of Output of a Scalar Map with Delay}


\author{Nicolas Espitia,$^{1}$  Miroslav Krstic,$^{2}$ and Jorge I. Poveda$^{3}$    % <-this % stops a space
\thanks{$^{1}${\scriptsize CRIStAL UMR 9189 CNRS} - Centre de Recherche en Informatique Signal et Automatique de Lille - CNRS, Centrale Lille, Univ. Lille, F-59000 Lille, France. ({\tt\small e-mail: nicolas.espitia-hoyos@univ-lille.fr})}%
% \thanks{$^{2}$Centrale Lille, F-59000 Lille, France.}%
\thanks{$^{2}$Department of Mechanical and Aerospace Engineering, University of California, San Diego, USA.}
\thanks{$^{3}$Electrical and Computer Engineering Department,University of California, San Diego, USA.}
%
}

\maketitle


%
%
\begin{abstract}
We study prescribed-time extremum seeking (ES)  for scalar maps in the presence of time delay. The problem has been solved by Yilmaz and Krstic using chirpy probing and time-varying singular gains.  
To alleviate the gain singularity,  we present an alternative approach, employing delays with bounded time-periodic gains, for achieving prescribed-time convergence to the extremum. Our results are not extensions or refinements but a new methodological direction,  even in the absence of the delay on the map. The main result we present compensates the map's delay and uses  perturbation-based and the Newton (rather than gradient) approaches. The simultaneous presence of  perturbation period, and two delays --- a map delay and a seeking feedback delay --- whose values are different (feedback delay must be longer than map delay), makes for an intricate situation in the design and analysis. 
 ES can settle arbitrarily soon after four times the map delay.   In the absence of a map delay, the settling time is arbitrarily short, with feedback delay chosen as one quarter of the prescribed settling time, i.e., the search settles after four times any positive feedback delay.
In addition to removing the gain singularity of the Yilmaz-Krstic singular-gain prescribed-time ES, we go beyond that method's limitation to operating only up to the terminal time. With the help of averaging theorems in infinite dimension, we conduct a prescribed-time convergence analysis on a suitable perturbation-averaged \textit{target} ES system, which contains the time-periodic gains of the map and feedback delays. With numerical simulations, we illustrate the results. 
Since the notion of ``dead-beat'' Lyapunov stabilization by time-periodic delayed feedback originates from Hale and Verduyn-Lunel (analysis, 1993) and Karafyllis (feedback design, 2006), we refer to our approach to prescribed-time ES as the ``Karafyllis, Hale, Verduyn-Lunel" (KHV) PT-ES approach. 



 
\end{abstract}
%

%%%%%%%%%%%%%%%%%%
% \begin{IEEEkeywords}
% extremum seeking through delays, prescribed-time control, time-varying delayed feedback
% \end{IEEEkeywords}
%%%%%%%%%%%%%%%%%%%
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\vskip 1 cm

\subsection{Literature on extremum seeking (ES)}
% Since the emergence of the 



Extremum Seeking, initially introduced in  Leblanc's  paper in 1922
 \cite{Leblanc1922}, has established itself, over the years, as one of the most popular adaptive optimization algorithms attracting a considerable amount of interest \cite{AriyurKrstic2003,TanMoaseManzieNesicMareels2010} and resulting in many applications in several domains. This popularity is due to the algorithm's feature of systematically determining local optimums of static and dynamical nonlinear systems in real-time without the need for knowledge of these systems. 


Many pioneering contributions can be highlighted in this field of research including: \cite{KrsticWang2000} which presents the first complete stability analysis for an ES algorithm, based on averaging and singular perturbation theories, for a general nonlinear dynamical system; \cite{Krstic2000} which has improved the performance of the ES algorithm in  \cite{KrsticWang2000}  and overcame some of its limitations; \cite{ChoiKrsticAriyurLee2002} which has extended the ES method to discrete dynamical systems; \cite{WangKrstic2000} which has applied ES to the problem of limit cycle maximization; \cite{TanNesicMareels2006,TanNesicMareelsAstolfi2009} which have achieved semi-global convergence for ES algorithms; \cite{MoaseManzieBrear2009a,MoaseManzieBrear2009b,MoaseManzieBrear2010,GhaffariKrsticNesic2012} which have introduced for the first time Newton-based ES algorithms to overcome the shortcomings of the Gradient-based ones; \cite{StankovicStipanovic2010} which has dealt with measurement noise; \cite{LiuKrstic2012} which has used stochastic probing in ES; \cite{DurrStankovicEbenbauerJohansson2013,LabarGaroneKinnaertEbenbauer2019} which have employed Lie bracket approximation methods for ES systems; and finally \cite{FridmanZhang2020,ZhuFridmanOliveira2023} which have  employed a Time-Delay approach  in ES algorithms.
%\Nicolas{a recent survey to be included as well}
% Finite-time convergent gradient flows with applications to network consensus, Cortes \cite{Cortes2006} 

%\cite{Wang2012} Wang Teel Analysis for a class of singularly
%perturbed hybrid systems via averaging
%
% 



\subsection{Literature on ES with delays}


In all of these mentioned contributions, it is assumed that input/output delays are not present. However, in many processes feedback actions and output measurement may be subject to delays. These delays have to be taken into account in the ES algorithms, as they may induce instabilities or convergence property may no longer be guaranteed. The stabilization of ES systems is a challenging problem that requires using supplementary techniques to deal with the additional complexity introduced by the delay in both the design and the analysis. Only few results exist for this problem including \cite{OliveiraKrsticTsubakino2017} in which the problem of extremum-seeking for unknown static maps with constant pointwise delays, was formulated and solved for the first time using the so-called \textit{backstepping approach for PDEs} \cite{Krstic2009a} and employing averaging in infinite-dimension \cite{Hale1990}.  Recently, this result was extended to the case of distributed delays in \cite{TsubakinoOliveiraKrstic2023}. Since those pionering contributions, some relevant  extensions of ES with e.g., time-varying and state-dependent delays, and furthermore with  other  PDE dynamics,  have been proposed and can be found in  \cite{Oliveira_bookESDelays}.    Another recent improvement of \cite{OliveiraKrsticTsubakino2017} is found in  \cite{FerreiraOliveiraKrstic2023} in which not only the optimization of the quadratic maps with constant point-wise delays is achieved, but the transient towards the optimum itself is optimized. Other approaches include the use of sequential predictors \cite{MalisoffKrstic2021}.


Despite all this stream of results, most of them only consider asymptotic or exponential convergence guarantees towards the optimum, yet in several applications of ES (e.g.,   Air flow control \cite{KomatsuMiyamotoOhmoriSano2001,ChangMoura2009}, Antilock Braking Systems \cite{ZhangOrdonez2012}, Fusion power plant \cite{OuXuSchusterLuceFerronWalkerHumphreys2008},  process and reaction systems \cite{Dochain2011},  Multi-agent source localization \cite{KhongTanManzieNesic2014},   Nash Equilibrium Seeking and learning in non-cooperative games \cite{FrihaufKrsticBasar2011,StankovicJohanssonStipanovic2012}, Stock trading \cite{FormentinPrevidiMaroniCantaro2018}, etc.),   where   seeking  must occur within a finite given time, and delays need to be perfectly compensated, the \textit{finite-in-time} forms of convergence are strongly desired. 

\subsection{Literature on finite-in-time ES}


`Finite-in-time' convergence refers to several properties, with three standing out:  i) finite-time convergence \cite{BhatBernstein1995}, where the settling time is a finite but possibly depends on the system's initial conditions and parameters; ii) fixed-time convergence \cite{Polyakov2012},  where the settling time   is uniformly bounded by a constant that is independent of the initial conditions but may depend on system's parameters which can be tuned in some cases to achieve convergence in arbitrarily chosen finite time; and iii) prescribed-time (PT) convergence {\cite{SongWangHollowayKrstic2017}} where the settling time is independent of both the initial conditions and the system's parameters and assigned using time-varying singular gains. 

In addition to achieving PT convergence using time-varying singular gains, alternative approaches exist, including attaining PT convergence using artificial delays with time-periodic gains \cite{Karafyllis2006}, \cite{Insperger2006}  and \cite{ZhouMichielsChen2022}. Those contributions leverage a key result in \cite[Chapter 3, pp 87-88]{Hale1993}, which  has been the focus of subsequent research efforts on stabilization in fixed time by means of periodic time-varying feedback with notable extensions  to observer-based and dual/output  feedback designs,  robustness to external additive disturbance,  uncertainties \cite{Zhou_PDF_Automatica2024}, as well as the integration  with sliding mode controllers for disturbance rejection \cite{Deng_Moulay2024}.    
%\cite{RaffAllower2008}, \cite{Li2015} \cite{Mazenc_CDC_2021  
 


In the framework of `finite-in-time extremum seeking,' few results 
exist even in the absence of delays.   For instance finite-time ES has been pursued in \cite{Guay2021}.  In addition, the first result achieving  (semi-global practical) fixed-time convergence with predefined time of convergence (in light of \cite{Polyakov2012}) is given in \cite{PovedaKrstic2020a} for Gradient-based ES algorithms for Static maps. This result has been extended to Newton-based algorithm in \cite{PovedaKrstic2020b} and further extended to dynamical systems in \cite{PovedaKrstic2021}. These results have found application in  Nash Equilibrium Seeking in Time-Varying Networks \cite{Poveda2022}. Another notable result achieving prescribed-time source seeking, instead, for static nonlinear maps  is presented in \cite{TodorovskiKrstic2022,TodorovskiKrstic2023}. Recently, this result has been extended to the case of prescribed-time ES for the delay case in \cite[Section III]{CemalTugrul2024Tac} by relying on predictor-feedback whose design builds on the PDE backstpeping approach with time-varying backstepping Volterra transformation (as in \cite{EspitiaPerruquetti2022}); and by using  chirpy probing. A   novel 	averaging theorem in  infinite dimension  is proposed  in  \cite{CemalTugrul2024Tac}  to cope with the challenges of handling unbounded terms  due to the time-varying singular gain, and the lack of periodicity of the chirpy probing signals, among others.  




\subsection{Contributions}

In this paper, we provide a Newton-based extremum seeking algorithm achieving prescribed-time maximization for static maps subject to a delay in their formulation. In contrast to  \cite{CemalTugrul2024Tac}, we do not use time-varying singular gains nor chirpy probing (perturbation and demodulation signal with sinusoids whose frequency grows unbounded). Additionally, we do not rely on the backstepping method for PDEs. 
Instead, we build on the intentional use of artificial delays and periodic time-varying gains that do not grow unbounded. Combining these tools allows not only to compensate for the delay of the static map but also to achieve convergence in a prescribed time. Furthermore, this approach allows the extremum seeking algorithm to operate beyond terminal times; hence avoiding any possible issue of singularity and guaranteeing existence of solutions post-terminal times. It is worth mentioning that the intentional use of delays in control design has proved to be powerful in achieving objectives that usually  cannot be achieved using static feedback. Such objective include for instance finite time convergence \cite{Karafyllis2006}, \cite{ZhouMichielsChen2022}, or disturbance-robust adaptive control \cite{Karafyllis2023Adaptive}.

 To the best of our  knowledge, there are no proposed ES algorithms dealing with this problem by combining the aforementioned tools. The main idea of our approach is indeed to combine periodic time-varying feedback with delays. Furthermore,   the use of  Newton-based scheme is critical. We propose a ``Riccati-like" filter that can estimate the inverse of the Hessian in prescribed-time (in the average sense). This is instrumental in allowing both the learning dynamics and the estimator for the gradient to converge in a prescribed time, in spite of the presence of the output delay that needs to be compensated. 

Since the notion of ``dead-beat'' Lyapunov stabilization by time-periodic delayed feedback originates, 
%{\color{blue} to the best of our knowledge,} 
from Hale and Verduyn-Lunel \cite[Chapter 3, pp 87-88]{Hale1993} (the analysis; {1993}) and Karafyllis \cite{Karafyllis2006} (feedback design; 2006), we refer to our approach to prescribed-time ES as the ``Karafyllis, Hale, Verduyn-Lunel" (KHV) PT-ES approach. Insperger's \cite{Insperger2006} ``act-and-wait'' idea and \cite{Karafyllis2006} appeared in the same year. 
%{\color{Miroslav}but is without Lyapunov estimates, applicable only to linear systems, and does not constitute a mathematical triumph in the way that \cite{Karafyllis2006} does.} \Nicolas{If you don't mind, I would reformulate as follows:} {\color{blue} We acknowledge that Insperger's \cite{Insperger2006} "act-and-wait" concept was introduced in the same year as \cite{Karafyllis2006}. While 
While both works offer valuable, distinct contributions in their respective domains, \cite{Karafyllis2006} provides a broader framework, including Lyapunov estimates and applicability to nonlinear systems, and being, in fact, quite a mathematical triumph. 
%}
 We also acknowledge excellent subsequent efforts for linear systems, particularly \cite{ZhouMichielsChen2022}, and also note that the name suggested for the approach in \cite{ZhouMichielsChen2022} is a deft choice---``periodic delayed feedback'' (PDF). However, our preference is for terminology that evokes the innovators. Such terminology (e.g., Kravaris-Kazantsiz-Luenberger---KKL---observers) ends up being more memorable than precise (and dry) technical phrasing. Hence, we refer to our variety of PT-ES algorithms as {\em KHV-PT-ES}. 



\subsection{Organization}

This paper is organized as follows. In Section \ref{Section:Problem_Statement_ExTS}, we provide some preliminaries on delay-compensated extremum seeking, the motivation and  a key idea on prescribed-time stabilization by means of time-varying delayed feedbacks. In section \ref{PrT_ES} we present the prescribed-time Newton-based extremum seeking problem for a real-valued static map featuring an output delay, and the main result and its proof. In Section \ref{numerical_simulations},  we consider a numerical example to illustrate the main results. Finally, we give some conclusions and perspectives in Section \ref{Section:Conclusion_ExTS}.\\



\textbf{Notation:}

 $\mathbb{R}_+$ denotes the set of non-negative real numbers.
 For a vector $x \in \mathbb{R}^{n}$ we denote $\vert x \vert$ its Euclidean norm. $\lceil x\rceil$ denotes the ceiling function,  i.e., $\lceil x\rceil = \min\{n\in \mathbb{Z}: n \geq x \}$.
 We denote by the unit circle centered around the origin the set $\mathbb{S}^1=\left\{u=(u_1,u_2)^{\top} \in \mathbb{R}^2: u_1^2+u_2^2=1\right\}$. Let $I \subseteq \mathbb{R}$ be a non-empty interval and $\Omega \subset \mathbb{R}^n$ be a non-empty set. By $C^0(I;\Omega)$ we denote  the class of continuous functions on $I$, which takes values in $\Omega$.  For $x \in C^{0}([-r,0];\mathbb{R}^n)$, we define $\Vert x \Vert=\max_{-r \leq s \leq 0}(\vert x(s) \vert)$
 Let $x:[a-r,b) \rightarrow \mathbb{R}^n$ with $b>a \geq 0$ and $r >0$. By $x_t$ we denote the ``history" of $x$ from $t-r$ to $t$, i.e.,  $x_t(s)=x(t+s)$; $s\in[-r,0]$, for $t\in [a,b)$.

%A function $\alpha: \mathbb{R}_+ \rightarrow \mathbb{R}$ is said to be of class $\mathcal{K}_{\infty}$ if it is zero at zero, continuous, strictly increasing.  If in addition, $\alpha$ is unbounded with its argument then $\alpha$ is said to be a class-$\mathcal{K}_{\infty}$. % A function $\beta: \mathbb{R}_+ \times \mathbb{R}_+ \rightarrow \mathbb{R}$ is said to be of class $\mathcal{K L}$ if it is nondecreasing in its first argument, non-increasing in its second argument, $\lim _{r \rightarrow 0^{+}} \beta(r, s)=0$ for each $s \in \mathbb{R}_+$, and $\lim _{s \rightarrow \infty} \beta(r, s)=0$ for each $r \in \mathbb{R}_+$.
%A continuous function $\beta:\mathbb{R}_{+}\times\mathbb{R}_{+} \to \mathbb{R}_{+}$ is said to be a \textit{generalized class-$\mathcal{KL}$ function} (in short $\mathcal{GKL}-$function) iff 
%\begin{itemize}
%\item[$i)$] the mapping $r \mapsto \beta(r,0)$ is a class-$\mathcal{K}$ function;
%\item[$ii)$]for each fixed $r \geq 0$ the mapping $t \mapsto \beta(r,t)$ is continuous, decreases to zero and there exists some $\tilde{T}(r) \in [0,+\infty)$ such that $\beta(r,t)= 0$ for all $t \geq \,\tilde{T}(r)$;
%
%\end{itemize}


%} 
\section{Preliminaries and problem description }\label{Section:Problem_Statement_ExTS}


In this paper, we are interested in  maximizing  in a prescribed finite time   the following real-valued output:
%
\begin{equation}
y(t)=Q(\theta(t-D)),
\end{equation}
%
by varying the input $\theta \in\mathbb{R}$, where $Q(\cdot)$ is an unknown mapping, and $D\geq0$ is a nonnegative and known constant delay.     

For  maximum seeking purposes, we assume there exists an optimal input $\theta^\star \in \mathbb{R}$,  such that: 
%\begin{equation}\label{eq:Gradient_Hessian_equilibrium}
%\frac{\partial Q(\theta^\star)}{\partial \theta}=0,  \quad %\frac{\partial^2 Q(\theta^\star)}{\partial \theta^2}=H^\star <0,
%\end{equation}
\begin{equation}\label{eq:Gradient_Hessian_equilibrium}
 Q^{'}(\theta^\star)=0,  \quad  Q^{''}(\theta^\star)=H^\star <0,
\end{equation}
where  $\theta^\star$ and the Hessian of the static map $H^\star$ are unknown. The output $y$ is assumed to be at least locally quadratic. Therefore, it can be written as 
\begin{equation}\label{eq:static_map_without_perturbation}
y(t)=  y^\star + \tfrac{H^\star}{2}\left(\theta(t-D) - \theta^\star \right)^{2},
\end{equation}
where 
\begin{equation}
y^\star = Q(\theta^\star),
\end{equation}
is the maximum value attained by $y$. 


\subsection{Preliminaries on delay-compensated Newton ES with exponential convergence} \label{Preliminaries_delay_compensated_Tiago}

The actual input $\theta(t)=\hat{\theta}(t)+S_D(t)$ is a real estimate $\hat{\theta}(t)$ of $\theta^\star$ that is perturbed by $S_D(t)$ (to be specified below). Hence, \eqref{eq:static_map_without_perturbation} is expressed as follows:
\begin{equation}\label{eq:outout_function_static_maps}
y(t)=  y^\star + \tfrac{H^\star}{2}\left(\hat{\theta}(t-D) +S_D(t-D) - \theta^\star \right)^{2}. 
\end{equation}
The following delay-compensated Newton-based extremum seeking  algorithm for static maps was  introduced \cite{OliveiraKrsticTsubakino2017} (particularized here for the scalar case, for the sake of clarity in the presentation):
\begin{align}
&\dot{\hat{\theta}}(t)=  z(t),  \label{eq:Newton_ES_static_maps1} \\
&\dot{z}(t) = - cz(t) -cK\Gamma(t)M(t)y(t)  - cK \int_{t-D}^{t}z(s)ds,  \label{eq:Newton_ES_static_maps2} \\
&\dot{\Gamma}(t)=w_r\Gamma(t) - w_rN(t)y(t)\Gamma^2(t),  \label{eq:Newton_ES_static_maps3} 
\end{align}
where $\hat{\theta}(t)\in\mathbb{R}$ is the learning dynamics (an estimator of  $\theta^\star$), $z(t)\in\mathbb{R}$ is the state of a filter aiming at estimating the gradient, with $c>K>0$,  and $\Gamma(t)\in\mathbb{R}$ is the state of a Riccati filter (Riccati differential equation) aiming at estimating the inverse of the Hessian with $w_r>0$  a positive real tunable parameter.    The Dither signals $S_D(t)$, $M(t)$, and   $N(t)$ are given as follows:
\begin{align}
S_D(t)&=a\sin(\omega(t+D)), \label{eq:dither_signal_as_tiago_S}\\ 
M(t) &=\tfrac{2}{a}\sin(\omega t),\label{eq:dither_signal_as_tiago_M}\\ 
N(t)&=\tfrac{16}{a^2}\left(\sin^2(\omega t) - \tfrac{1}{2} \right). \label{eq:dither_signal_as_tiago_N}
\end{align} 
where $0 < a \ll 1$, and $\omega>0$. 
 Defining, for a given constant $y^\star \in \mathbb{R}$, $y_c(t,  \chi)=y^\star +\frac{H^\star}{2}\left(S_D(t-D) +  \chi \right)^{2}$ and $\Pi= \tfrac{2\pi}{\omega}$,  we recall  the following averaging   properties (see e.g. \cite{GhaffariKrsticNesic2012}, \cite{OliveiraKrsticTsubakino2017} or  \cite[Lemma 1]{MalisoffKrstic2021}):
\begin{align}
\tfrac{1}{\Pi}\int^{\Pi}_{0} M(s)y_c(s,\chi)ds &= H^\star  \chi,\label{property:averaging_get_gradient}\\
\tfrac{1}{\Pi}\int^{\Pi}_{0} N(s)y_c(s,\chi)ds &= H^\star. \label{property:averaging_get_Hessian}
\end{align}
The use of the Riccati filter in \eqref{eq:Newton_ES_static_maps1}-\eqref{eq:Newton_ES_static_maps3} is crucial and is the main feature of the Newton-based Extremum seeking algorithm that was originally introduced in \cite{Nesic2010}  and subsequently in \cite{GhaffariKrsticNesic2012} for the multi-variable case. Under this algorithm, one can obtain faster convergence  (with a convergence rate independent of the Hessian) compared to a traditional Gradient-based Extremum seeking algorithm \cite[Section IV]{OliveiraKrsticTsubakino2017}. In the multi-variable case, the use of the Riccati filter is particularly crucial to avoid having to invert directly the matrix of the Hessian estimate.

We recall the main ideas behind the delay-compensated Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1}-\eqref{eq:Newton_ES_static_maps3} introduced in \cite{OliveiraKrsticTsubakino2017}.  It  allows to compensate for the delay thanks to the distributed term in \eqref{eq:Newton_ES_static_maps2} i.e.,  using the history of the state of the filter $z(t)$ on a interval $[t-D,t]$. Averaging the error-dynamics,  reformulating the problem as an input delay control, and then applying the backstepping method for PDEs allow to come up with a predictor-based controller and, therefore, the updating law for the learning dynamics.  The stability analysis is carried out on the average error-dynamics, followed by  Lyapunov analysis. This methodology enables to invoke the averaging theorem in infinite-dimension (see \cite{Hale1990}).
The method carries out not only scalar static maps but also multi-variable maps with distinct delays. 
Using one-stage sub-predictors, an alternative approach to compensate for the delays was proposed in  \cite{MalisoffKrstic2021}. The algorithm does not involve distributed terms but point-wise delays (see \cite[Section 6]{MalisoffKrstic2021} for a Newton-Based extremum seeking).  The stability analysis of the average error-dynamics is performed using   Lyapunovâ€“Krasovskii techniques,  yielding a sufficient condition on the delay  and the convergence rate of the algorithm (slower convergence for larger delays).   The existing averaging theory for infinite dimensional systems applies as well.  

\subsection{Discussion of Newton ES  algorithm  \eqref{eq:Newton_ES_static_maps1}-\eqref{eq:Newton_ES_static_maps3}}\label{Discussion_on_Tiago_approach}
Notice that,  by means of the Fundamental Theorem of Calculus, the Newton-based Extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1}-\eqref{eq:Newton_ES_static_maps3}, with static map \eqref{eq:outout_function_static_maps} can be reformulated as follows:
\begin{align} 
&\dot{\hat{\theta}}(t)=  z(t), \label{eq:Newton_ES_static_maps1V2} \\
&\dot{z}(t) = - cz(t) -cK\Gamma(t)M(t)y(t)  - cK \hat{\theta}(t) + cK \hat{\theta}(t-D),  \label{eq:Newton_ES_static_maps2V2} \\
&\dot{\Gamma}(t)=w_r\Gamma(t) - w_rN(t)y(t)\Gamma^2(t),  \label{eq:Newton_ES_static_maps3V2}  
\end{align} 
where we can clearly see the presence of a ``state" delay of the learning dynamics in both \eqref{eq:Newton_ES_static_maps2V2} and the output \eqref{eq:outout_function_static_maps}. This reformulation may look similar to the algorithm using  one-stage sequential predictors in \cite{MalisoffKrstic2021}; with the difference, that in \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps3V2}  the delay is only being present in  the state of the learning dynamics  and one does not use approximations of the predictor, but an exact expression via Fundamental Theorem of Calculus.
If one applies averaging to the error-dynamics (with change of variables $\tilde{\theta}=\hat{\theta}- \theta^\star$,  $\tilde{\Gamma}=\Gamma- H^{\star-1}$)  by using properties \eqref{property:averaging_get_gradient}-\eqref{property:averaging_get_Hessian} in conjunction with the averaging operation in infinite dimensions \cite{Hale1990}, the resulting average error-dynamics  will read as follows:
\begin{align}
&\dot{\tilde{\theta}}_{\rm av}(t)=  z_{\rm av}(t),  \label{eq:Newton_ES_static_averaged1} \\
&\dot{z}_{\rm av}(t) = - cz_{\rm av}(t) -cK\tilde{\theta}_{\rm av}(t)  - cK \tilde{\Gamma}_{\rm av}(t)H^{\star} \tilde{\theta}_{\rm av}(t-D),  \label{eq:Newton_ES_static_averaged2} \\
&\dot{\tilde{\Gamma}}_{\rm av}(t)=-w_r\tilde{\Gamma}_{\rm av}(t) - w_rH^{\star}\tilde{\Gamma}_{\rm av}^2(t),  \label{eq:Newton_ES_static_averaged3} 
\end{align} 
where we can observe that \eqref{eq:Newton_ES_static_averaged1}-\eqref{eq:Newton_ES_static_averaged2} is cascaded by $\tilde{\Gamma}_{\rm av}(t)$.  In fact, the  multiplicative term $ \tilde{\Gamma}_{\rm av}(t)H^{\star} \tilde{\theta}_{\rm av}(t-D) $ in \eqref{eq:Newton_ES_static_averaged2} can be seen as a multiplicative perturbation that eventually goes to zero, since $\tilde{\Gamma}_{\rm av}(t) \rightarrow 0$ (fast enough depending on the choice of $w_r$) as  $t\rightarrow \infty$, and  as long as  $\vert  \tilde{\theta}_{\rm av} (t-D)\vert$ remains bounded. The stability result then would follow if,  instead of employing the PDE backstepping approach as in \cite{OliveiraKrsticTsubakino2017},   one were to  apply the classical theory of time-delay systems  as  demonstrated  in \cite{MalisoffKrstic2021} (possibly with similar trade-off of slow convergence rate for arbitrary long delays). Alternatively,   the singular perturbation theory (see e.g., \cite{PovedaKrstic2020b}) could be used where the multiplicative  disturbance term can be neglected or considered as a small external disturbance. Furthermore, both, \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps3V2} and  the averaged error-dynamics  \eqref{eq:Newton_ES_static_averaged1}-\eqref{eq:Newton_ES_static_averaged3} could be represented as Retarded Functional Differential Equations (RFDEs) with fast oscillations for which  classical theorems on averaging in infinite dimensions  are also applicable. These insights   may further justify, at intuitively level,  why  the original algorithm (in \cite{OliveiraKrsticTsubakino2017}) \eqref{eq:Newton_ES_static_maps1}-\eqref{eq:Newton_ES_static_maps3} is capable of actually estimating  the optimal point, hence maximizing the static map, even in the presence of delays. 


\subsection{Towards KHV approach to prescribed-time ES
%prescribed-time  extremum seeking 
under output delay
%: a new approach {\color{blue}(the KHL approach to Prescribed-time ES)} 
}

The  discussion in Subsection \ref{Discussion_on_Tiago_approach}, together with the observation leading to the ES algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps3V2} provides an intuitive understanding of how the algorithm operates. %{\color{red}It also   motivates the development of  new extremum seeking algorithms  capable to not only of compensating  for map delays, but also of accelerating  the convergence or even   achieving   finite-time convergence.}
 It also motivates the development of new extremum-seeking algorithms that not only compensate for map delays but also accelerate convergence or even achieve finite-time convergence.
  In particular, the  trade-off identified in in \cite{MalisoffKrstic2021} between slower convergence versus arbitrary large delays,   further underscores the seek for accelerated ES algorithms enabling to achieve finite-time convergence. More specifically, the seek for algorithms  enabling convergence within  prescribed finite time without relying necessarily on the use of chirpy probing and   time-varying singular gains (as in \cite{CemalTugrul2024Tac}), while ensuring solutions  exist beyond post-terminal times. 
Motivated by these considerations,  we propose in Section \ref{PrT_ES} a novel algorithm:  a   delay-compensated Newton-based  extremum seeking using periodic time-varying  point-wise delayed dynamics with a prescribed-time convergent Riccati-like filter (estimation of the inverse of the Hessian in prescribe time). Hence, under this ES algorithm  we can not only compensate for the output delay  but also to  achieve  to the optimal point and maximizing the output within prescribed times.  

 As we will see, the main idea is to use time-varying  periodic \textit{delayed} feedback whose time-varying gains are suitable chosen. To that end, let us first discuss the following key idea on prescribed-time stabilization  by means of  a time-varying \textit{delayed} feedback.  



\subsection{Key idea: PT stabilization of single integrator by time-periodic {\em delayed} feedback}\label{section_key_idea}


To illustrate the main idea of prescribed-time stabilization  using   a time-varying \textit{delayed} feedback, consider the   following  one-dimensional  system operating on $[0,3T]$: 
\begin{equation}\label{eq:single_integrator_in_coreIDEA}
\dot{x}(t)=u(t), \quad  \forall t \in [0, 3T], 
\end{equation}
 with arbitrary initial condition $x(s)=\phi(s)$, $\phi \in C^0([-T,0];\mathbb{R})$ and  the following $2 T$-periodic time-varying pointwise delayed feedback
\begin{equation}\label{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}
u(t)= -\mathcal{K}_{T}\left(t\right)x(t-T),
\end{equation} 
where  $T>0$ is a prescribed number,   and $\mathcal{K}_T$ is  a $2T$-periodic pulse signal, given as follows: 
\begin{equation}\label{eq:mathcalK_T_time_sequence_in_coreIDEA}
\mathcal{K}_T(t)= \begin{cases}0&  \text{if}  \quad 0 \leq t < T, \\
\tfrac{1}{T}\left( 1- \cos\left(\tfrac{2\pi t}{T} \right) \right) &\text{if}  \quad T \leq t \leq 2T, \\
0&   \quad 2T < t \leq 3T. \end{cases}
\end{equation}  
Notice that the controller \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA} features: 1) zero gain on $[0,T] $ and keeps constant the state  $x(t)$; 2)  \textit{mean} gain $\tfrac{1}{T}$  over the second $T$-period, acting on the constant state from the first period using a delay, and resulting in dead-beat convergence at $t=2T$. Indeed, on $[T,2T]$ the control is given  by
$$u(t)= -\tfrac{1}{T}x(0) + \tfrac{1}{T}\cos\left(\tfrac{2\pi t}{T} \right)x(0), $$
 and the resulting closed-loop solution is given by $$x(t)=x(0) - \tfrac{t-T}{T}x(0) + \tfrac{1}{2\pi}\sin\left(\tfrac{2\pi t}{T} \right)x(0), $$
  from which one gets  $x(2T)=0$.  Finally, notice that the control features zero  gain on  the interval $(2T,3T) $,  hence, $x(t)$ remains zero for all $t \geq 2T$.
\begin{figure*}[t]
\centering{
\subfigure{\includegraphics[width=0.9\columnwidth]{solution_scalar_eq_COREIDEAV2.eps} }
\subfigure{\includegraphics[width=0.9\columnwidth]{control_scalar_eq_COREIDEAV2.eps} }
\caption{On the left: Solution of \eqref{eq:single_integrator_in_coreIDEA} with the $2 T$ - periodic time-varying \textit{pointwise} delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA}  with $T=1s$. On the right: Profile of the $2 T$- periodic time-varying \textit{pointwise} delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA}.}
\label{closed-loop_solution_and_control_in_COREIDEA}
}
\end{figure*}
\begin{figure*}[t]
\centering{
\subfigure{\includegraphics[width=0.9\columnwidth]{solution_scalar_eq_COREIDEA_perturbed.eps} }
\subfigure{\includegraphics[width=0.9\columnwidth]{control_scalar_eq_COREIDEA_perturbed.eps} }
\caption{On the left: Solution of \eqref{eq:single_integrator_in_coreIDEA}    with the $2 T$ - periodic time-varying \textit{pointwise} delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA}  with $T=1s$, and which is subject  to an additive vanishing disturbance e.g., $v(t)=1$ for $t < 1s$, and $v(t)=0$ for $t \geq 1s$. On the right: Profile of the $2 T$- periodic time-varying \textit{pointwise} delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA}.}
\label{closed-loop_solution_and_control_in_COREIDEA_with_vanishing_distrubance}
}
\end{figure*}


Figure \ref{closed-loop_solution_and_control_in_COREIDEA} illustrates the dead-beat convergence property for the system \eqref{eq:single_integrator_in_coreIDEA},  with initial condition $x(0)=-1$ (and its preceding delay value), under the $2 T$-periodic time-varying pointwise delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA} with $T=1$s.   

 Suppose also that  system \eqref{eq:single_integrator_in_coreIDEA} is subject to an additive and bounded vanishing disturbance. Using the same controller \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA},  the  closed-loop system reads as  $\dot{x}(t)= -\mathcal{K}_{T}\left(t\right)x(t-T) + v(t)$, where $v(t) \in \mathbb{R}$, with $v(t) = 0$ for  $t\geq t^{'}$. In this scenario,  the dead-beat convergence is achieved at a multiple of $2T$.   
Figure \ref{closed-loop_solution_and_control_in_COREIDEA_with_vanishing_distrubance}  shows (on the left) the solution of the system \eqref{eq:single_integrator_in_coreIDEA},  with initial condition $x(0)=-1$ (and its preceding delay value), under the $2 T$-periodic time-varying pointwise delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA}-\eqref{eq:mathcalK_T_time_sequence_in_coreIDEA} with $T=1$s which is subject to a vanishing disturbance e.g., $v(t)=1$ for $t < 1s$, and $v(t)=0$ for $t \geq 1s$. As it can be observed,   
the dead-beat convergence is achieved at a multiple of $2T$ (in this case, at $4$s).
%

\begin{remark}\label{remark_historical_issues0}

It is important to recall that the dead-beat property discussed above for the same class of system, and time-varying gain (using the relation $\tfrac{2}{T}\sin^2\left(\tfrac{\pi t}{T} \right) = \tfrac{1}{T}\left( 1- \cos\left(\tfrac{2\pi t}{T} \right) \right)$),  can be found in, e.g.,  \cite[Chapter 3, pp 87-88]{Hale1993}; which  constitutes a key fundamental result  and that has been the focus of subsequent research efforts on stabilization in fixed time by means of periodic time-varying feedback systems for both linear \cite{Insperger2006,ZhouMichielsChen2022} and nonlinear systems \cite{Karafyllis2006}, \cite{Ding_stronPT-nonlinear2024}. Notable  extensions to observer-based and dual/output  feedback designs,  robustness to external additive disturbance,  uncertainties \cite{Zhou_PDF_Automatica2024}, as well as the integration  with sliding mode controllers for disturbance rejection \cite{Deng_Moulay2024}. 
\end{remark}











\section{Delay-Compensated Prescribed-Time Newton  Extremum Seeking }\label{PrT_ES}



%\subsection{Delay-Compensated Newton-based  extremum seeking by means of  point-wise delayed dynamics  }


As aforementioned, the discussion in Subsection \ref{Discussion_on_Tiago_approach}, together with the observation leading to the ES algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps3V2} motivate the search for novel delay-compensated extremum-seeking algorithms that are not only capable of converging within a prescribed finite time-i.e., ensuring that the learning dynamics $\hat{\theta}$ reach a neighborhood of the optimal input $\theta^\star$ within a specified timeframe-but also enabling operation beyond post-terminal times. 
 This objective  can be reframed as requiring  the averaged  error-dynamics   $\tilde{\theta}_{\rm av}(t)$ to converge to zero in a prescribed time. This goal is indeed attainable. To see this,  notice first that the averaged error-dynamics \eqref{eq:Newton_ES_static_averaged1}-\eqref{eq:Newton_ES_static_averaged2} is cascaded by $\tilde{\Gamma}_{\rm av}(t)$ and  that the   term $ \tilde{\Gamma}_{\rm av}(t)H^{\star} \tilde{\theta}_{\rm av}(t-D) $ appearing in \eqref{eq:Newton_ES_static_averaged2} can be seen as a perturbation. Suppose for the time being,  that  this perturbation  can be made to vanish in finite time e.g.,  by setting $\tilde{\Gamma}_{\rm av}(t) = 0$, for all $t\geq T^{'}$ (with $T^{'}>D$). Then the states of the resulting closed-loop average error-dynamics $(\tilde{\theta}_{\rm av}(t),z_{\rm av}(t))^{\top}$ would converge to zero exponentially, for all $t\geq T^{'}$. Nevertheless, our goal is to achieve  a more demanding type of convergence: prescribed-time convergence. Specifically, we want   $(\tilde{\theta}_{\rm av}(t),z_{\rm av}(t))^{\top}$ to reach zero in a prescribed-time and remain at zero thereafter.  This calls for a new approach that combines i) time-varying tools (still falling within the framework of \textit{prescribed-time} control), ii) the intentional use of delays; relying on the core idea presented in the previous section. \\



 Let us define first some signals of interest.

\subsection{Oscillators and time-periodic gains}


We consider the following  constrained linear oscillator  evolving on $\mathbb{S}^1$, for a given $T >0$:
\begin{equation}\label{eq:oscillator_system_periodic_time-varying_feedbacks}
\dot{\zeta}(t)= \mathcal{R}_{\tfrac{\pi}{T}} \zeta(t), \quad \zeta=(\zeta_1,\zeta_2)^\top \in \mathbb{S}^1,   
\end{equation}  
with 
\begin{equation}\label{eq:matrix_bloc_oscillator_pulse_signal_T}
\mathcal{R}_{\tfrac{\pi}{T}}=\left[\begin{array}{cc}
0 & \tfrac{\pi}{T} \\
-\tfrac{\pi}{T} & 0
\end{array}\right], \quad T >0,
\end{equation}
The explicit solutions to \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} are given as follows:
\begin{align}
\zeta_1(t)&=\zeta_{1}(0) \cos \left( \tfrac{\pi}{T}t\right)+\zeta_{2}(0) \sin \left(\tfrac{\pi}{T}t\right), \label{eq:explicit_solution_osicillator1}\\
\zeta_2(t)&=-\zeta_{1}(0) \sin \left(\tfrac{\pi}{T}t\right)+\zeta_{2}(0) \cos \left(\tfrac{\pi}{T}t\right), \label{eq:explicit_solution_osicillator2}
\end{align}
for any initial conditions $\zeta(0)=(\zeta_{1}(0),\zeta_{2}(0))^\top\in \mathbb{S}^1$. Moreover, \eqref{eq:explicit_solution_osicillator1}-\eqref{eq:explicit_solution_osicillator2} can further be expressed as
\begin{align}
\zeta_1(t)&= \sin \left(\tfrac{\pi}{T}\left( t + \delta_{\zeta} T \right)\right), \label{eq:explicit_solution_osicillator1_V2}\\
\zeta_2(t)&=\cos \left(\tfrac{\pi}{T}\left( t + \delta_{\zeta} T \right)\right), \label{eq:explicit_solution_osicillator2_V2}
\end{align}
where $\delta_{\zeta} \in (-1,1]$ is given by
\begin{equation}\label{eq:definition_of_delta-phase_of_oscillators}
\delta_{\zeta} = \Delta(\zeta_1(0),\zeta_2(0)),  
\end{equation}
 and the function $\Delta(s_1,s_2)$  is defined as follows\footnote{It can be seen as a variation of the  2-argument arctangent (``atan2") function.}: 
\begin{equation}\label{eq:definition_variation_atan2_function}
\Delta(s_1,s_2) = 
\begin{cases} 
\frac{1}{\pi} \arctan\left(\frac{s_1}{s_2}\right) & \text{if } s_2 > 0, \\
\frac{1}{\pi} \arctan\left(\frac{s_1}{s_2}\right) + 1 & \text{if } s_2 < 0 \text{ and } s_1 \geq 0, \\
\frac{1}{\pi} \arctan\left(\frac{s_1}{s_2}\right) - 1 & \text{if } s_2 < 0 \text{ and } s_1 < 0, \\
\frac{1}{2} & \text{if } s_2 = 0 \text{ and } s_1 =1 , \\
-\frac{1}{2} & \text{if } s_2 = 0 \text{ and } s_1 =-1, \\
0 & \text{if } s_2 =1 \text{ and } s_1 = 0, \\
1 & \text{if } s_2 =-1\text{ and }s_1 = 0.
\end{cases}
\end{equation}
Based on \eqref{eq:explicit_solution_osicillator1_V2}-\eqref{eq:explicit_solution_osicillator2_V2} we define the following $2T$-periodic pulse signal $\mathcal{K}_T(t)$  as a  smooth state-dependent  signal generated by the  constrained linear oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T}:
\begin{equation}\label{eq:mathcalK_T_V0}
\mathcal{K}_{T}(t)= \begin{cases}0&  \text{if}  \quad \zeta_1(t)>0, \\
\tfrac{2}{T}\zeta_1^2(t) &\text{if}  \quad \zeta_1(t)\leq 0.   \end{cases}
\end{equation} 
 which can, in turn, be written as follows:
\begin{equation}\label{eq:mathcalK_T}
\begin{split}
\mathcal{K}_{T}(t)= & \frac{2}{T}\max\big\{0, -\zeta_1(t) \vert\zeta_1(t) \vert \big\}.
\end{split} 
\end{equation}

\begin{remark}
By the definition of $\mathcal{K}_{T}(t)$ in  \eqref{eq:mathcalK_T_V0}, it can be noticed that the  following property holds:
\begin{equation}\label{eq:property_K_T_and delayed_K}
\mathcal{K}_{T}(t)\mathcal{K}_{T}(t-T)=0.
\end{equation}
\end{remark}
Moreover, we introduce another periodic pulse signal generated also by \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T}, that  corresponds to the time-derivative of $\mathcal{K}_{T}$. We denote it as $\mathcal{L}_{T}=\tfrac{d \mathcal{K}_T}{dt}$, which is given as follows:
%\begin{equation}\label{eq:mathcalL_T}
%\mathcal{L}_{T}(t)= \begin{cases}0&  \text{if}  \quad \zeta_1(t)>0 \\
%\tfrac{4}{T^2}\pi\zeta_1(t)\zeta_2(t) &\text{if}  \quad \zeta_1(t)\leq 0.   \end{cases}
%\end{equation} 
\begin{equation}\label{eq:mathcalL_T}
\mathcal{L}_{T}(t)=  -\frac{4}{T^2}\pi\max\big\{0, -\zeta_1(t)  \big\} \zeta_2(t).
\end{equation}
Figure \ref{Pulse_signals} depicts and example of the profiles of  the $2T$-periodic signals $\mathcal{K}_T$ (left - blue lines) and $\mathcal{L}_{T}$ (right - red lines) 
 with $T=1s$ and $T=2s$,   and initial conditions $\zeta(0)=(0,1)^{\top}\in \mathbb{S}^1$, which gives $\delta_{\zeta} = 0$, according to \eqref{eq:definition_of_delta-phase_of_oscillators}-\eqref{eq:definition_variation_atan2_function}.  
\begin{figure*}[t]
\centering{
\subfigure{\includegraphics[width=0.9\columnwidth]{pulse_signals_KT.eps} }
\subfigure{\includegraphics[width=0.9\columnwidth]{pulse_signals_LT.eps} }
\caption{On the left: Profile of the $2T$- periodic signal $\mathcal{K}_T(t)$ defined in  \eqref{eq:mathcalK_T}  generated by the linear constrained oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} with $T=1s$ (blue line) and $T=2s$ (blue dashed line),  and initial condition $\zeta(0)=(0,1)^{\top}\in \mathbb{S}^1$, which gives $\delta_{\zeta} = 0$, according to \eqref{eq:definition_of_delta-phase_of_oscillators}. On the right: Profile of the $2T$- periodic signal $\mathcal{L}_T(t)$ defined in  \eqref{eq:mathcalL_T}  generated by the linear constrained oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} with $T=1s$ (red line) and $T=2s$ (red dashed line),  and initial condition $\zeta(0)=(0,1)^{\top}\in \mathbb{S}^1$, which gives $\delta_{\zeta} = 0$, according to \eqref{eq:definition_of_delta-phase_of_oscillators}. }
\label{Pulse_signals}
}
\end{figure*}
Finally, we consider the following constrained linear  oscillator  evolving on $\mathbb{S}^1$: 
\begin{equation}\label{eq:oscillator_system_periodic_time-varying_feedbacks_tau}
\dot{\xi}(t)= \mathcal{R}_{\tfrac{\pi}{2T}} \xi(t), \quad \xi=(\xi_1,\xi_2)^\top \in \mathbb{S}^1,   
\end{equation} 
with
\begin{equation}\label{eq:matrix_bloc_oscillator_pulse_signal_tau}
\mathcal{R}_{\tfrac{\pi}{ 2T}}=\left[\begin{array}{cc}
0 & \tfrac{\pi}{ 2T} \\
-\tfrac{\pi}{ 2T} & 0
\end{array}\right], \quad T >0,
\end{equation}
where $T$ is the same as in \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T}.
Similarly, we can get
\begin{align}
\xi_1(t)&= \sin \left(\tfrac{\pi}{2T}\left( t + \delta_{\xi} 2T \right)\right), \label{eq:explicit_solution_osicillatortau_1_V2}\\
\xi_2(t)&=\cos \left(\tfrac{\pi}{2T}\left( t + \delta_{\xi} 2T \right)\right), \label{eq:explicit_solution_osicillatortau_2_V2}
\end{align}
where $\delta_{\xi} \in (-1,1]$ is characterized  as follows: 
\begin{equation}\label{eq:definition_of_delta-xi-phase_of_oscillators}
\delta_{\xi}=\Delta(\xi_1(0),\xi_2(0)),
\end{equation}
with $\Delta$ being defined in \eqref{eq:definition_variation_atan2_function}. 

Hence,    we finally define the following pulse signal of interest:
\begin{equation}\label{eq:mathcalK_tau}
\begin{split}
\mathcal{K}_{ 2T}(t)= & \frac{1}{ T}\max\big\{0, -\xi_1(t) \vert\xi_1(t) \vert \big\}.
\end{split} 
\end{equation} 

\begin{remark}\label{remk:forward_invariance_oscillators}
Let us remark first  that  by the structure of the oscillators  \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} and \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}, the set $\mathbb{S}^1$ is forward
invariant, i.e., if  $\zeta(0) \in \mathbb{S}^1$, and $\xi(0) \in \mathbb{S}^1$   then  $\zeta(t) \in \mathbb{S}^1$ and $\xi(t) \in \mathbb{S}^1$  for all $t \geq 0$.
Moreover, since in our formulation no solutions of \eqref{eq:oscillator_system_periodic_time-varying_feedbacks} and \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau} are defined outside the unit circle, the set $\mathbb{S}^1$ is trivially globally attractive. 
\end{remark}

\begin{remark}\label{remk:state-dependent-signals-time-invariantES}
Notice also that we have considered  the time-varying periodic signals \eqref{eq:mathcalK_T}, \eqref{eq:mathcalL_T} and \eqref{eq:mathcalK_tau}, as state-dependent signals - those generated by the oscillators  \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} and \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}, respectively. Those signals will be part of the dynamics of the Extremum Seeking algorithm. Hence,  the closed-loop systems  of the Extremum seeking algorithm, introduced in the  section \ref{KHV_ES_algorithm},  will not be treated  as a time-varying system, but rather  as time-invariant one. This has  important theoretical consequences:  we can handle a closed-loop averaged error-system  as a time-invariant  retarded functional differential equation with fast oscillations. This significantly facilitates the application of classical averaging theorems in infinite-dimensions.
\end{remark}


\subsection{PT input-to-state stabilization of single integrator by time-periodic {\em delayed} feedback}

In what follows, we exploit the  ideas in \eqref{section_key_idea} and formalize them in Lemma 1, which accounts for an  ISS-like property with respect to an external disturbance; in particular, a vanishing disturbance. Indeed, we study the effect of a vanishing disturbance and prove that the resulting settling-time  is shifted by a time characterized by $\delta_{\zeta}$ and by multiple of $2T$. The result in the lemma is instrumental for the design of a prescribed-time extremum seeking in Section \ref{KHV_ES_algorithm}. 

\begin{lemma}\label{Lemma_PrT_Pointwise_delay}
 Consider the   following  one-dimensional  system 
\begin{equation}\label{eq:single_integrator}
\dot{x}(t)=u(t) + v(t)
\end{equation}
where $u(t) \in \mathbb{R}$ is the control input and $v(t) \in \mathbb{R}$ is an external bounded disturbance. Let
 $T>0$ be a prescribed number. The solution of the  closed-loop system \eqref{eq:single_integrator} with  the following \textit{$2 T$-periodic} time-varying \textit{pointwise} delayed feedback
\begin{equation}\label{eq:time-varying_delayed-feedback_key_result}
u(t)= -\mathcal{K}_{T}\left(t\right)x(t-T),
\end{equation}
where $\mathcal{K}_{T}(t)$ is defined  by \eqref{eq:mathcalK_T},   with arbitrary initial condition  $x(s)=\phi(s)$, $\phi \in C^0([-T,0];\mathbb{R})$  satisfies 
 \begin{equation}\label{eq:bound_PrT_single_integrator}
\begin{split}
 \vert x(t) \vert \leq &\exp\left(4 \right)\sigma\left(t-(2T - \delta_{\zeta} T)  \right)\max_{-T \leq s\leq 0}(\vert \phi(s) \vert)  \\
 & \hskip 1cm + 6T \exp\left(4 \right) \sup_{\max\{0,t-(2T-\delta_{\zeta}  T) \}\leq s\leq t} (\vert v(s) \vert), 
\end{split} 
 \end{equation}
\begin{equation}\label{eq:bound_PrT_single_integrator-control}
\begin{split}
 \vert u(t) \vert \leq & \frac{2}{T}\exp\left(4 \right)\sigma\left(t-(2T - \delta_{\zeta} T)  \right)\max_{-T \leq s\leq 0}(\vert \phi(s) \vert) \\
 & \hskip 1cm + 12 \exp\left(4 \right) \sup_{\max\{0,t-(2T-\delta_{\zeta}  T) \}\leq s\leq t} (\vert v(s) \vert ),
\end{split}
\end{equation}
where  
\begin{equation}\label{eq:heaviside_function}
    \sigma(s) = \begin{cases}1& \mbox{if} \quad s < 0\\ 0
    & \mbox{if} \quad s \geq 0.   \end{cases}
\end{equation}
 and $\delta_{\zeta} \in (-1,1]$ is given by \eqref{eq:definition_of_delta-phase_of_oscillators}. 
Moreover,   if the bounded external disturbance $v(t)$ completely vanishes at $t^{'}\geq 0$,  i.e.,  $v(t) =0$ for  $t \geq t^{'} \geq 0$,  then, the following estimate holds:
\begin{equation}\label{eq:bound_PrT_single_integrator-control-vanishing_dist}
\begin{split}
 \vert x(t) \vert \leq &\exp\left(4 \right)\sigma\Big(t-\left(2T-\delta_{\zeta} T + 2T\Big\lceil \tfrac{t^{'}+\delta_{\zeta} T}{2T}\Big\rceil\right)  \Big)\\ & \times \Big[ \max_{-T \leq s\leq 0}(\vert \phi(s) \vert) + 6T \sup_{0\leq s\leq \min\{t,t^{'}\}} (\vert v(s)  \vert)\Big], 
\end{split} 
\end{equation}
which yields  $x(t)=0$ for all $t \geq   2 T - \delta_{\zeta} T+ 2T\Big \lceil \frac{t^{'}+\delta_{\zeta}  T}{2T}\Big\rceil$. 
\end{lemma}
\begin{proof}
See Appendix \ref{proof_of_lemma1}.
\end{proof}
It is important to understand the effect of the size of $T$ in the gain \eqref{eq:time-varying_delayed-feedback_key_result}, and also in the settling time $2T - \delta_{\zeta} T$, on the performance of the closed-loop system. The ISS gain from the disturbance $v$ to the state $x$ is proportional to $T$ and hence, as $T$ is decreased, and the gain in \eqref{eq:time-varying_delayed-feedback_key_result_in_coreIDEA} consequently increases, the gain from $v$ to $x$ improves. However, there is an adverse effect of increasing the gain $1/T$ on the control magnitude. While the ISS gain from $v$ to $u$ is independent of $T$, the finite-time transient portion of the bound on $u(t)$ is proportional to $1/T$, meaning that, for improved performance of $x(t)$, including shorter setting time $2T-\delta_{\zeta} T$, price is paid in the magnitude of $u(t)$, at least initially. 
\begin{figure*}[t!]
\centering
  \includegraphics[width=0.74\textwidth]{PT_ES_diagramV2.eps}
  \caption{Scheme of the Prescribed-time  Newton  Extremum Seeking \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} with  map delay $D$ and feedback delays $T$ and $2T$.}
  \label{PT_ES_diagram}
\end{figure*}
\begin{figure}[t!]
\centering
  \includegraphics[width=0.96\columnwidth]{KHV_filters_diagramV2.eps}
  \caption{
  Details of the three ``filters'' in Figure \ref{PT_ES_diagram}. Notation is abused copiously (simultaneous time domain and Laplace domain nomenclature) in order to convey intuition. 
  \ \ul{TOP:} the $z$-filter, given in \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise}, mirrors the structure of the exponential filter \eqref{eq:Newton_ES_static_maps2V2}. The $z$-filter is fed by $M(t)y(t)$, which is an estimate of the ``scalar gradient'' $ Q'(\theta(t-D))$. The estimate $My$ drives a predictor of the approximation of a ``Newton update'' $\Gamma Q'(\theta(t-D))$, cascaded into a low-pass filter of the KHV kind (prescribed time, employing time-periodic gains and delay feedback). The approximate Newton update $ \Gamma(t) M(t) y(t)\approx \Gamma(t) Q'(\theta(t-D)) $ is a product of the estimate $\Gamma$ of the inverse of the Hessian, $1/H^{\star}$, with the approximation $My$ of the scalar gradient 
  $ Q'(\hat\theta(t-D))= H^{\star} \tilde\theta(t-D)$, as in \eqref{eq:Newton_ES_static_maps2V2}. The role of the KHV low-pass filter is analogous to the role the exponential filter $c/(s+c)$ in \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise}. The low-pass filtering is not required for implementation---neither in \eqref{eq:Newton_ES_static_maps2V2} nor in \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise}. 
  It is simply that, without such a filter, the averaging theorem in \cite{Hale1990} is not applicable to the resulting closed-loop system, which is not a conventional differential equation with delays on the state only. So, the low-pass filter is included for the sake of theory. 
  \ \ul{MIDDLE:} The $\hat H$-filter, given in \eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}, is simply a low-pass filter of the KHV kind of the Hessian estimate $N(t)y(t)$.
  \ \ul{BOTTOM:} The $\Gamma$-filter, given in \eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati}, is a KHV version of the exponential Riccati-type $\Gamma$-filter in \eqref{eq:Newton_ES_static_maps3V2}. This KHV filter estimates in prescribed time the inverse of the Hessian, $1/H^{\star}$. Since $\hat H$ is a filtered estimate of $H^{\star}$, and $\Gamma$ is an estimate of $1/H^{\star}$, the product $\Gamma(t) \hat H(t-2T)$ should be regarded as $\approx 1$, in the sense of design inspiration. 
}
  \label{PT_ES_diagram_filters}
\end{figure}



% \begin{figure*}[t!]
% \centering{
% \includegraphics[width=1\textwidth]{PT_ES_diagram_with_filters_diagram.eps}
% \caption{{\color{blue}On the left: Scheme of the Prescribed-time  Newton  Extremum Seeking \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} with  map delay $D$ and feedback delays $T$ and $2T$. On the right: The three filters which are based on our KHV approach. The KHV low-pass filter within the diagram "$z$-filter" \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise}, as well as the KHV estimator of $\Gamma^{\star-1}$ within the diagram "$\Gamma$-filter" (with some abuse of notation, as we mix the time and frequency domains), can be seen as analogous to their counterparts in the filters \eqref{eq:Newton_ES_static_maps2V2} and \eqref{eq:Newton_ES_static_maps3V2} from the classical Newton ES with exponential convergence. }}
% \label{Static-map_and_Estimators_plot}
% }
% \end{figure*}


\begin{remark}\label{remark_on_historical_issues}
The one-dimensional control system \eqref{eq:single_integrator} with time-varying pointwise delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result}  is  a special case of the one-dimensional control  system  with time-varying \textit{distributed}  delayed feedback studied in \cite[Lemma 2.9]{Karafyllis2006}. Therefore, applying \cite[Lemma 2.9]{Karafyllis2006} in conjunction with the seminal result in \cite[Chapter 3, pp 87-88]{Hale1993}, the conclusion would follow: we could  obtain a similar bound to \eqref{eq:bound_PrT_single_integrator}, and the ISS prescribed-time stability result.  
Nevertheless, for the sake of completeness and conciseness, we adapt the ideas of the proof of  \cite[Lemma 2.9]{Karafyllis2006} to our particular case of time-varying point-wise delay feedback, with gain $\mathcal{K}_{T}$ defined as in \eqref{eq:mathcalK_T}, and which includes also the effect of a vanishing disturbance. The proof is given in Appendix \ref{proof_of_lemma1}.
\end{remark}



\subsection{KHV algorithm for PT extremum seeking}\label{KHV_ES_algorithm}
%Main Extremum Seeking  algorithm {\color{blue}-the { KHL-PT-ES}} }

 We are now in position to  propose the following Prescribed-time Newton-based  extremum seeking by means of time-varying point-wise delayed feedback.
\begin{align}
\dot{\hat{\theta}}(t)=&  z(t),  \label{eq:Newton_ES_static_maps1_prescribed-pointwise} \\
\dot{z}(t) =&-2\mathcal{K}_{T}(t)z(t-T)
\nonumber\\ &
+\mathcal{L}_{T}(t) \left[ \hat{\theta}(t-D) - \hat{\theta}(t-T)
%\nonumber \\  & 
-\Gamma(t)M(t)y(t)  \right]
% -\mathcal{L}_{T}(t) \hat{\theta}(t-T)
% \nonumber \\  & 
% -\mathcal{L}_{T}(t) \Gamma(t)M(t)y(t)  + \mathcal{L}_{T}(t) \hat{\theta}(t-D)
,  \label{eq:Newton_ES_static_maps2_prescribed-pointwise} \\
\dot{\Gamma}(t)=&\mathcal{K}_{ 2T}(t) \Gamma^2(t)\left[\hat{H}(t- 2T)  -  N(t)y(t)\right],  %\nonumber \\&  
\label{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati} \\
\dot{\hat{H }}(t)=&-\mathcal{K}_{ 2T}(t)\left[\hat{H}(t-  2T)  -N(t)y(t)\right], \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}
\end{align}   
where the static map $y(t)$ is given in \eqref{eq:outout_function_static_maps},  $\hat{\theta}(t)\in\mathbb{R}$ is the learning dynamics, $z(t)\in\mathbb{R}$ is the state of a filter-like aiming at estimating the gradient,  the time-varying gains $\mathcal{K}_T(t)$ and $\mathcal{L}_{T}(t)$, are defined, respectively  in \eqref{eq:mathcalK_T} and \eqref{eq:mathcalL_T} and generated by the oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T}. 
 The dither signals $S_D(t)$, $M(t)$, $N(t)$ are defined, respectively, in  \eqref{eq:dither_signal_as_tiago_S}, \eqref{eq:dither_signal_as_tiago_M} and \eqref{eq:dither_signal_as_tiago_N},  with $a\ll1$ and $\omega >0$ large.
 
  $\Gamma(t)\in\mathbb{R}$ is the state of a ``Riccati-like" filter  aiming at estimating the inverse of the Hessian. It involves, in addition,  the past values of $\hat{H}$ (which estimates the Hessian). % Both, the Hessian and its inverse are estimated in a prescribed time (that, as we will see later, it is given by {\color{blue}$ 4T-\delta_{\zeta} 2T$ units of time}). 
  The time-periodic gain $\mathcal{K}_{ 2T}(t)$ is defined in \eqref{eq:mathcalK_tau} and is generated by the oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}.
 We consider  initial conditions  $\phi:[- 2T,0]  \rightarrow \mathbb{R}^4$, $(\hat{\theta}^0,z^0,\Gamma^0,\hat{H}^0)^{\top}=\phi(s)$, with $\phi \in C^{0}([- 2T,0]; \mathbb{R}^{4})  $ for \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian},    $\zeta(0)=(\zeta_1(0),\zeta_2(0))^{\top} \in \mathbb{S}^1$ for  \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T}, and $\xi(0)=(\xi_1(0),\xi_2(0))^{\top} \in \mathbb{S}^1$ for \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}. 
  Figure \ref{PT_ES_diagram} depicts the scheme of the PT  ES \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} with  map delay $D$ and feedback delays $T$ and $2T$. Figure \ref{PT_ES_diagram_filters} shows the details of the three ``filters""  in  Figure \ref{PT_ES_diagram}.
  
 The different delays and ``time-periods" involved in the PT extremum seeking \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} are such that the following ordering holds: 
\begin{equation}\label{eq:ordering-time-scales}
\frac{2\pi}{\omega} \ll D <T.
\end{equation} 
In the sequel, we set $T^{\star}>0$ as the prescribed time of the prescribed-time  Newton-based ES algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}; and we establish the following characterization for $T$, in relation to the  map delay $D$, $\delta_{\zeta}$, $\delta_{\xi}$ (defined in \eqref{eq:definition_of_delta-phase_of_oscillators} and \eqref{eq:definition_of_delta-xi-phase_of_oscillators}, respectively), and the prescribed-time $T^{\star}$: For  given $D\geq 0$, $\delta_{\zeta} \in (-1,1]$, $\delta_{\xi} \in (-1,1]$, we pick any prescribed number  $T^{\star}>0$ such that
 \begin{equation}\label{eq:restriction_choice_T_star}
 T^{\star}>\left(2 - \delta_{\zeta}  + 2\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil\right)D.
 \end{equation}
  Then, we select
\begin{equation}\label{eq:characteriation_T}
T=
\frac{2D+T^{\star}}{4 - \delta _{\zeta} + 2\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil}.
\end{equation}
The restriction   \eqref{eq:restriction_choice_T_star} on $T^{\star}$ is justified by \eqref{eq:ordering-time-scales}. Replacing \eqref{eq:restriction_choice_T_star} in \eqref{eq:characteriation_T} yields $T>D$.  Suppose the  limiting scenario in which $T\leq D$. Consider for example  $T = \frac{D}{(4-2\delta_{\xi})}$. This implies   that the  Hessian and its inverse need to be estimated within at least   $4T-2\delta_{\xi} T=D$ units of time, using \eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} (as we will prove later based on the result in Lemma \ref{Lemma_PrT_Pointwise_delay}). In such scenario,  reliable estimation of the Hessian (consequently of its inverse, as well) cannot be ensured, as the demodulation process involving $N(t)y(t)$ would primarily rely on the initial data of $\hat{\theta}$ without sufficient new current information. Therefore, it is necessary for  $T$  to be large enough--- at least longer than the map delay, i.e., $T>D$ (as stated in  \eqref{eq:ordering-time-scales})  to allow sufficient time for the Hessian estimator to perform its task accurately.    This can be achieved by prescribing $T^{\star}$ according to \eqref{eq:restriction_choice_T_star}.






\subsection{Main results}
\begin{theorem}\label{theo:main_ES_algorithm_prescribed-pointwise}
 Let $D\geq 0$ be given.  Let $T^{\star}$ be the prescribed time that meets \eqref{eq:restriction_choice_T_star}.
Consider the prescribed-time  Newton extremum-seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}  with output map \eqref{eq:outout_function_static_maps}  and  time-periodic gains $\mathcal{K}_T(t)$,  $L_{T}(t)$ and $\mathcal{K}_{ 2T}(t)$  defined   in \eqref{eq:mathcalK_T}, \eqref{eq:mathcalL_T} and \eqref{eq:mathcalK_tau}, respectively,  and that are generated by the oscillators \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} and \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}, with arbitrary initial conditions in $\mathbb{S}^{1}$ yielding   $\delta_{\zeta}$ and $\delta_{\xi}$  as in  \eqref{eq:definition_of_delta-phase_of_oscillators} and \eqref{eq:definition_of_delta-xi-phase_of_oscillators}, respectively. Let $T$ be selected according to \eqref{eq:characteriation_T}.    Then, for any $\nu >0$, there exist $\bar{\omega} >0$  such that for each $\omega > \bar{\omega}$,   there exists a constant $\rho >0$ such that for each initial functions  $\phi:[- 2T,0]  \rightarrow \mathbb{R}^4$  that satisfy $\vert \phi(0) -\left(\theta^\star, 0,{H^\star}^{-1}, H^\star\right)^\top \vert \leq \rho $,   the solutions of the system  \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}   render  the learning dynamics and the output to satisfy, respectively,
\begin{equation}\label{eq:main_bound_learning_dynamics_in_theorem}
\left|\hat{\theta}(t)-\theta^\star\right| \leq \nu,\ \forall t \geq  T^{\star} +2D,    
\end{equation}
and
\begin{equation}\label{eq:main_bound_output_in_theorem}
\left|y(t)-y^\star\right| \leq H^\star(\nu^2+a^2),\ \forall t \geq   T^{\star}+3D.   
\end{equation}
In particular, with  $\mathcal{K}_T(t)$,  $L_{T}(t)$ generated by  the oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} initialized  with $\zeta(0)=(0,1)^{\top}$, and $\mathcal{K}_{ 2T}(t)$ generated by the oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau} initialized with $\xi(0)=(0,-1)^{\top}$,  meaning that  $\delta_{\zeta}=0$, $\delta_{\xi}=1$, and with 
\begin{equation}
  T^{\star}>2D \quad \mbox{and} \quad T=\frac{D}{2} + \frac{T^{\star}}{4}\,,  
\end{equation} the statements \eqref{eq:main_bound_learning_dynamics_in_theorem} and \eqref{eq:main_bound_output_in_theorem} hold. 
\end{theorem}
\vskip 0.4cm 

The rest of this section is devoted to the steps and the intermediate results that are instrumental to prove Theorem \ref{theo:main_ES_algorithm_prescribed-pointwise}.


\subsubsection{error-dynamics and averaging  }
Consider the following change of variables $\tilde{\theta}=\hat{\theta}- \theta^\star$, $\tilde{\Gamma}=\Gamma - H^{\star -1}$, $\tilde{H}=\hat{H}-H^\star$. From \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}, the error-dynamics reads as follows:
\begin{align}
\dot{\tilde{\theta}}(t)=&  z(t),  \label{eq:Newton_ES_static_maps1_prescribed-pointwise_error} \\
\dot{z}(t) =&- 2\mathcal{K}_{T}(t)z(t-T)    
\nonumber  \\ & 
+\mathcal{L}_{T}(t) 
\left[\tilde{\theta}(t-D)
- \tilde{\theta}(t-T) 
\right.\nonumber\\ & \left. 
- (\tilde{\Gamma}(t)+H^{\star-1})M(t)y(t) 
\right], 
% - \mathcal{L}_{T}(t)\tilde{\theta}(t-T) \nonumber  \\
% & - \mathcal{L}_{T}(t)  (\tilde{\Gamma}(t)+H^{\star-1})M(t)y(t)  +\mathcal{L}_{T}(t) \tilde{\theta}(t-D),
\label{eq:Newton_ES_static_maps2_prescribed-pointwise_error} 
\\
\dot{\tilde{\Gamma}}(t)=&
\mathcal{K}_{ 2T}(t)
(\tilde{\Gamma}(t)+H^{\star-1})^2 
\nonumber \\ & 
\times\left[\tilde{H}(t- 2T)+H^\star
 - N(t)y(t)\right], 
% \mathcal{K}_{ 2T}(t) (\tilde{H}(t- 2T)+H^\star)(\tilde{\Gamma}(t)+H^{\star-1})^2  
% \nonumber \\ & 
%  - \mathcal{K}_{ 2T}(t)(\tilde{\Gamma}(t)+H^{\star-1})^2 N(t)y(t),  
%\nonumber \\ &  
% \dot{\tilde{\Gamma}}(t)=&
% \mathcal{K}_{ 2T}(t) (\tilde{H}(t- 2T)+H^\star)(\tilde{\Gamma}(t)+H^{\star-1})^2  
% \nonumber \\ & 
%  %\hskip 2.2cm  
%  - \mathcal{K}_{ 2T}(t)(\tilde{\Gamma}(t)+H^{\star-1})^2 N(t)y(t), 
% %\nonumber \\ &  
\label{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati_error} \\
\dot{\tilde{H }}(t)=&-\mathcal{K}_{ 2T}(t)\left[\tilde{H}(t- 2T)+H^\star
 - N(t)y(t)\right]. \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error}
\end{align} 
with initial conditions $(\tilde{\theta}^0,z^0,\tilde{\Gamma}^0,\tilde{H}^0)^{\top}= \tilde{\phi}(s)$, $ \tilde{\phi} \in C^{0}([- 2T,0]; \mathbb{R}^{4})$, with $\tilde{\theta}^0=\hat{\theta}^0-\theta^{\star}$,  $\tilde{\Gamma}^0=\hat{\Gamma}^0 - H^{\star -1}$, and $\tilde{H}^0=\hat{H}^0-H^{\star}$.

As we have mentioned in  Remark \ref{remk:state-dependent-signals-time-invariantES}, $\mathcal{K}_T(t)=\mathcal{K}_T(\zeta_1(t))$, $\mathcal{L}_T(t)=\mathcal{L}_T(\zeta_1(t),\zeta_2(t))$ and $\mathcal{K}_{ 2T}(t)=\mathcal{K}_{ 2T}(\xi_1(t))$   are seen as state dependent signal generated by the oscillators.
 
The delays appearing in the above system can be  ordered  as $D < T < 2T$.
For \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error}, consider the following  Retarded Functional Differential Equation (RFDE) with fast oscillations:  
\begin{equation}\label{eq:Compact_form_error_system}
\dot{X}(t)= f(t/\epsilon,X_t),
\end{equation} 
with initial condition   $X(s)=\varphi(s) = (\tilde{\phi},\zeta(0),\xi(0))^{\top} \in C^{0}([- 2T,0];\mathbb{R}^4)\times \mathbb{S}^{1}\times \mathbb{S}^{1}$, and
  state variable    $X=(\tilde{\theta},z,\tilde{\Gamma},\tilde{H}, \zeta_1,\zeta_2,\xi_1,\xi_2)^{\top} \in \mathbb{R}^{8}$, $X_t \in C^{0}([- 2T,0];\mathbb{R}^{4})\times \mathbb{S}^{1}\times \mathbb{S}^{1}$, $X_t(s)=X(t+s)$ for all $s \in [- 2T,0]$. 
  Therefore we have $f=(f_1,f_2,f_3,f_4,f_5,f_6,f_7,f_8)^{\top}$  with $\epsilon=1/\omega$ and
\begin{align}
f_1(\omega t, X_t)&=X_2(t),  \label{eq:RFDE-f1}\\
f_2(\omega t,X_t)&= - 2\mathcal{K}_{T}(X_5(t))X_2(t-T) \nonumber \\
&   - \mathcal{L}_{T}(X_5(t),X_6(t))X_1(t-T)\nonumber \\ 
& +\mathcal{L}_{T}(X_5(t),X_6(t)) X_1(t-D) \nonumber \\  
 &  -\mathcal{L}_{T}(X_5(t),X_6(t))  [X_3(t)+H^{\star-1}] \nonumber \\
  &\times [\tfrac{2}{a}\sin(\omega t)][ y^\star + \tfrac{H^\star}{2}\left[X_1(t-D) +a\sin(\omega t) \right]^{2}], \label{eq:RFDE-f2} \\
f_3(\omega t,X_t)&=\mathcal{K}_{ 2T}(X_7(t)) [X_4(t- 2T)+H^\star][X_3(t)+H^{\star-1}]^2 \nonumber \\
&   - \mathcal{K}_{ 2T}(X_7(t))[X_3(t)+H^{\star-1}]^2[\tfrac{16}{a^2}\left[\sin^2(\omega t) - \tfrac{1}{2} \right]] \nonumber\\
& \times [ y^\star + \tfrac{H^\star}{2}\left[X_1(t-D) +a\sin(\omega t) \right]^{2}],  \label{eq:RFDE-f3}\\
f_4(\omega t,X_t)&=-\mathcal{K}_{ 2T}(X_7(t))[X_4(t-  2T) + H^\star] \nonumber \\ 
 &+ \mathcal{K}_{ 2T}(X_7(t))[\tfrac{16}{a^2}\left[\sin^2(\omega t) - \tfrac{1}{2} \right]] \nonumber\\
&\times [ y^\star + \tfrac{H^\star}{2}\left[X_1(t-D) +a\sin(\omega t) \right]^{2}], \label{eq:RFDE-f4}\\ 
f_5(\omega t,X_t)&=\tfrac{\pi}{T} X_6(t), \label{eq:RFDE-f5}\\ 
f_6(\omega t,X_t)&=-\tfrac{\pi}{T} X_5(t), \label{eq:RFDE-f6}\\
f_7(\omega t,X_t)&=\tfrac{\pi}{ 2T} X_8(t),\label{eq:RFDE-f7} \\
f_8(\omega t,X_t)&=-\tfrac{\pi}{ 2T} X_7(t). \label{eq:RFDE-f8}
\end{align}
 The compact formulation  $\dot{X}(t)= f(\omega t,X_t)$ will be helpful when invoking the averaging theorem in infinite dimension \cite{Hale1990}, that we will come back to in Section \ref{proof_of_main_result}. The prescribed-time convergence properties
of the closed-loop system \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error} can be investigated through the corresponding average system that we deduce next.
 
  Using the properties \eqref{property:averaging_get_gradient}-\eqref{property:averaging_get_Hessian} in conjunction with the averaging operation in infinite dimensions \cite{Hale1990},  
the average of the system $\dot{X}(t)=f(\omega t,X_t)$,   with $f$ stated above  over the period $\Pi=\tfrac{2\pi}{\omega}$, results in a closed-loop system of the form 
\begin{equation}\label{eq:Compact_form_Averaged_error_system}
\dot{X}^{\rm av}(t)=F^{\rm av}(X^{\rm av}_{t}),
\end{equation}
 with initial condition   $X^{\rm av}(s)=\varphi(s) = (\tilde{\phi},\zeta(0),\xi(0))^{\top} \in C^{0}([- 2T,0];\mathbb{R}^4)\times  \mathbb{S}^{1}\times \mathbb{S}^{1}$,  state variable    $X^{\rm av}=(\tilde{\theta}_{\rm av},z_{\rm av},\tilde{\Gamma}_{\rm av},\tilde{H}_{\rm av}, \zeta_{1},\zeta_{2}, \xi_{1},\xi_{2})^{\top}$,  $X_t^{\rm av}(\phi)=X^{\rm av}(t+s)$ for all $s \in [- 2T,0]$ and    $ F^{\rm av}(X_t)=\tfrac{1}{\Pi}\int_{0}^{\Pi}f(s,X_t)ds $.  
In particular, we display    
\begin{align}
\dot{\tilde{\theta}}_{\rm av}(t)=&  z_{\rm av}(t),  \label{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged} \\
\dot{z}_{\rm av}(t) =& - 2\mathcal{K}_{T}(t)z_{\rm av}(t-T)\nonumber  \\
 &   - \mathcal{L}_{T}(t)
 \left[\tilde{\theta}_{\rm av}(t-T)    
 +
 %-\mathcal{L}_{T}(t)
 \tilde{\Gamma}_{\rm av}(t)H^{\star}\tilde{\theta}_{\rm av}(t-D)\right], 
  \label{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged} \\
\dot{\tilde{\Gamma}}_{\rm av}(t)=&\mathcal{K}_{ 2T}(t)\tilde{H}_{\rm av}(t- 2T)(\tilde{\Gamma}_{\rm av}(t)+H^{\star-1})^2, \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati_error_averaged}  \\
\dot{\tilde{H }}_{\rm av}(t)=&-\mathcal{K}_{ 2T}(t)\tilde{H}_{\rm av}(t-  2T).   \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged}
\end{align} 
with initial conditions $(\tilde{\theta}_{\rm av}^0,z_{\rm av}^0,\tilde{\Gamma}_{\rm av}^0,\tilde{H}_{\rm av}^0)^{\top}=(\tilde{\theta}^0,z^0,\tilde{\Gamma}^0,\tilde{H}^0)^{\top}  \in C^{0}([- 2T,0]; \mathbb{R}^{4})$.

\subsubsection{Prescribed-time  convergence  of the averaged error-dynamics}
It is worth mentioning that since the dynamics of the oscillators $\zeta$, $\xi$ given in \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau} and \eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}, respectively,  render forward invariant  the set $\mathbb{S}^{1}$ (see Remark \ref{remk:forward_invariance_oscillators}), we focus mainly on the  convergence  properties of the states $(\tilde{\theta}_{\rm av},z_{\rm av},\tilde{\Gamma}_{\rm av},\tilde{H}_{\rm av})^{\top}$. Hence, we present next the prescribed-time convergence result for \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged}.


\begin{lemma}\label{eq:Prt_Target_system}
 Let $D\geq 0$ be given. Let us choose   the  prescribed time $T^{\star}$ according to condition \eqref{eq:restriction_choice_T_star}.     Consider the closed-loop system \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged},   with  gains $\mathcal{K}_T(t)$,  $L_{T}(t)$ and $\mathcal{K}_{ 2T}(t)$  defined   in \eqref{eq:mathcalK_T}, \eqref{eq:mathcalL_T} and \eqref{eq:mathcalK_tau}, respectively,   and that are generated by the oscillators \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} and \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}, with arbitrary initial conditions in $\mathbb{S}^{1}$ yielding   $\delta_{\zeta}$ and $\delta_{\xi}$  as in  \eqref{eq:definition_of_delta-phase_of_oscillators} and \eqref{eq:definition_of_delta-xi-phase_of_oscillators}, respectively. Let $T$ be selected according to \eqref{eq:characteriation_T}. The  solutions of the closed-loop system \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged}  are such that $\tilde{\Gamma}_{\rm av}(t)=0$ and $\tilde{H}_{\rm av}(t)=0$ for all $t \geq  4T - \delta_{\xi} 2T$; and $\tilde{\theta}_{\rm av}(t)=0$ and $z_{\rm av}(t)=0$   for all $t\geq T^{\star}+2D$.
% {\color{blue}Moreover, for every ..initial condition  $\in C^{0}(..)$.. there exists $\chi \in  \mathcal{K}_{\infty}$ such that
%\begin{equation}
%\begin{split}
%\vert (\tilde{\theta}_{\rm av}(t),z_{\rm av}(t))^{\top} \vert \leq  \chi\Big( & \Vert (\tilde{\theta}_{\rm av}^0,  z_{\rm av}^0)^{\top} \Vert_{\tau}  \\
%& +    {\color{red}\sigma(t- 2\tau) \sup_{0 \leq s \leq t} (\vert v(t) \vert) \Big)}, \forall t \geq 0.
%\end{split}
%\end{equation}
%  }
\end{lemma}
\begin{proof}
Consider the following  change of variables, inspired by the backstepping procedure, and specialized here for the double integrator system \footnote{
We recall that for higher-order systems with time-varying delayed feedbacks, the backstepping procedure can also be applied, see e.g.,   \cite[Chapter 6.7]{Iasson_book2011}  and  \cite{Ding_stronPT-nonlinear2024}.
} \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged}: 
\begin{align}
x_1(t)=&\tilde{\theta}_{\rm av}(t),\label{change_of_variable_targetsystem1} \\
x_2(t)=&\mathcal{K}_{T}(t)\tilde{\theta}_{\rm av}(t-T)+ z_{\rm av}(t) \label{change_of_variable_targetsystem2}.
\end{align}
 Using this change of variable together with   \eqref{eq:mathcalL_T} and  the fact $\mathcal{K}_{T}(t)\mathcal{K}_{T}(t-T) = 0$ (see \ref{eq:property_K_T_and delayed_K}), we get that the   
 system \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged} is transformed into the following \textit{target} system:
\begin{align}
\dot{x}_{1}(t)=& - \mathcal{K}_{T}(t)x_1(t-T) + x_2(t),  \label{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged_target} \\
\dot{x}_{2}(t) =&  - \mathcal{K}_{T}(t)x_2(t-T)   -\mathcal{L}_{T}(t)\tilde{\Gamma}_{\rm av}(t)H^{\star}x_{1}(t-D),  \label{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged_target}  \\
\dot{\tilde{\Gamma}}_{\rm av}(t)=&\mathcal{K}_{ 2T}(t)\tilde{H}_{\rm av}(t- 2T)(\tilde{\Gamma}_{\rm av}(t)+H^{\star-1})^2, \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati_error_averaged_target}  \\
\dot{\tilde{H }}_{\rm av}(t)=&-\mathcal{K}_{ 2T}(t)\tilde{H}_{\rm av}(t-  2T),   \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged_target}
\end{align}
with initial conditions  $(x_1^0,x_2^0,\tilde{\Gamma}_{\rm av}^0,\tilde{H}_{\rm av}^0)^{\top}  \in C^{0}([- 2T,0]; \mathbb{R}^{4})  $.
 By Lemma \ref{Lemma_PrT_Pointwise_delay}, we obtain the following estimate for the solution to \eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged_target}: 
\begin{equation}\label{eq:PrT_Hessian_estimator_average_in_proof}
\vert \tilde{H}_{\rm av}(t)  \vert  \leq \exp\left(4 \right)\sigma\left(t- (4T-\delta_{\xi} 2T) \right)\max_{- 2T \leq s\leq 0}(\vert \tilde{H}_{\rm av}^0(s) \vert).
\end{equation}
for $t\geq 0$ where  $\tilde{H}_{\rm av}(0)=\tilde{H}_{\rm av}^0(s)$, $s \in [- 2T,0]$ and
 $\sigma(\cdot)$ is defined in \eqref{eq:heaviside_function}. Therefore, $\tilde{H}_{\rm av}(t) = 0 $ for $t \geq 4T - \delta_{\xi} 2T$. Furthermore,  the following relation holds true: 
\begin{equation}
\tilde{\Gamma}_{\rm av}(t) + H^{\star-1} = \frac{1}{\tilde{H}_{\rm av}(t) + H^{\star}},
\end{equation}
which implies $\tilde{\Gamma}_{\rm av}(t) = \tfrac{-H^{\star-1} \tilde{H}_{\rm av}(t)}{\tilde{H}_{\rm av}(t)+H^{\star}}$. Consequently, using \eqref{eq:PrT_Hessian_estimator_average_in_proof}, we also obtain $\tilde{\Gamma}_{\rm av}(t) = 0 $ for $t \geq  4T  -\delta_{\xi} 2T$.

Next, notice that the time-delay system  \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged_target} is cascaded by $\tilde{\Gamma}_{\rm av}(t)$, solution of \eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati_error_averaged_target}, and that for all $t \geq  4T-\delta_{\xi} 2T$, $\tilde{\Gamma}_{\rm av}(t)=0$.  Notice also that the term $x_{1}(t-D)$ in \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged_target} remains bounded for all $t\in[0, 4T-\delta_{\xi} 2T]$ by virtue of Lemma \ref{Lemma_PrT_Pointwise_delay} applied to \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged_target} and \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged_target}.

Applying repeatedly Lemma \ref{Lemma_PrT_Pointwise_delay} (in particular \eqref{eq:bound_PrT_single_integrator-control-vanishing_dist}), with 
\begin{equation}\label{eq:vanishing_perturbation_v_in_proof}
 v(t)=-\mathcal{L}_{T}(t) \tilde{\Gamma}_{\rm av}(t)H^{\star}x_{1}(t-D),
 \end{equation}
and knowing that $v(t)=0$ for $t\geq  4T-\delta_{\xi} 2T$,   we have the following estimates for the solutions to \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged_target} and \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged_target}:
\begin{equation}\label{eq:estimate_x1_afterLemma1}
\begin{split}
\vert x_{1}(t)  \vert  \leq & \sigma \left(t-\left(4T - \delta_{\zeta} T + 2T\Bigg \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Bigg\rceil  \right)    \right)\\
&\times\Big(\exp\left(4 \right) \max_{- 2T \leq s\leq 0}(\vert  x_{1}^0(s) \vert) \\
& \quad + 6T\exp\left(8 \right) \max_{- 2T \leq s\leq 0}(\vert  x_{2}^0(s) \vert) \\
&  \quad  + 36T^2 \exp\left(8 \right)  \sup_{0\leq s\leq \min\{t,4T-\delta_{\xi} 2T\}} (\vert v(s)  \vert) \Big),
\end{split}
\end{equation}
 and
 \begin{equation}\label{eq:estimate_x2_afterLemma1}
\begin{split}
\vert x_{2}(t)  \vert   \leq &  \sigma \left(t-\left(2T - \delta_{\zeta} T + 2T\Bigg \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Bigg\rceil  \right)  \right)\\
&\times\Big(\exp\left(4 \right) \max_{- 2T \leq s\leq 0}(\vert  x_{2}^0(s) \vert) \\
& \quad \quad + 6T\exp\left(4 \right)   \sup_{0\leq s\leq \min\{t,4T-\delta_{\xi} 2T\}} (\vert v(s)  \vert) \Big),
\end{split}
\end{equation}
for all $t\geq 0$.  From \eqref{change_of_variable_targetsystem2}, notice that  $x_2(t)=0$, thus $z_{\rm av}(t)=-\mathcal{K}_{T}(t)\tilde{\theta}_{\rm av}(t-T)$, for $t \geq \left(2T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil  \right) $. It holds that $\mathcal{K}_{T}(t)=0$ for  $t \in [4T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil ,5T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil ]$ and $\tilde{\theta}_{\rm av}(t-T)=0$, for  $t\geq 5T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil$, yielding $z_{\rm av}(t)=0$, for all   $t \geq 4T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil$. Moreover, using  \eqref{change_of_variable_targetsystem1}-\eqref{change_of_variable_targetsystem2},  
 the triangle inequality and the definition of the norm, we  have on the one hand, 
\begin{equation}\label{eq:norm_equivalence_iniitalconditions1}
\Vert (x_1^0,x_2^0)^{\top} \Vert \leq q\left( \Vert (\tilde{\theta}_{\rm av}^0,z_{\rm av}^0)^{\top}   \Vert \right),
\end{equation}
and
\begin{equation}\label{eq:norm_equivalence_iniitalconditions2}
\Vert (\tilde{\theta}_{\rm av}^0,z_{\rm av}^0)^{\top} \Vert \leq q\left( \Vert   (x_1^0,x_2^0)^{\top}  \Vert \right),
\end{equation}
with $q(s)=(2/T +2)$. On the other hand,  using again  \eqref{change_of_variable_targetsystem1}-\eqref{change_of_variable_targetsystem2} together with facts  $\mathcal{K}_{T}(t)\sigma\left(t- \left(5T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil\right)\right)=\mathcal{K}_{T}(t)\sigma\left(t- \left(4T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil\right)\right)$ and $\sigma\left(t- \left(2T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil\right)\right) \leq \sigma\left(t- \left(4T - \delta_{\zeta} T + 2T\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil\right)\right) $, for $t \geq 0$, we   get,  from \eqref{eq:estimate_x1_afterLemma1}-\eqref{eq:estimate_x2_afterLemma1} and  \eqref{eq:norm_equivalence_iniitalconditions1}-\eqref{eq:norm_equivalence_iniitalconditions2}, the following estimates:
 \begin{equation}\label{eq:estimate_tilde_theta_av_afterLemma1}
\begin{split}
\vert \tilde{\theta}_{\rm av}(t)  \vert  \leq & \sigma\left(t- \left(4T - \delta_{\zeta} T + 2T\Bigg \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Bigg\rceil\right)\right)\\
&\times \Big((1+6T)\exp\left(8 \right)q\left( \Vert (\tilde{\theta}_{\rm av}^0,z_{\rm av}^0)_t^{\top}   \Vert \right)  \\
&  \quad \quad   + 36T^2 \exp\left(8 \right)  \sup_{0\leq s\leq \min\{t,4T-\delta_{\xi} 2T\}} (\vert v(s)  \vert) \Big),
\end{split}
\end{equation}
and
\begin{equation}\label{eq:estimate_z_av_afterLemma1}
\begin{split}
\vert z_{\rm av}(t)  \vert  \leq  &\sigma\left(t- \left(4T - \delta_{\zeta} T + 2T\Bigg \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Bigg\rceil\right)\right)\\
&\times \Big((2/T+13)\exp\left(8 \right)q\left( \Vert (\tilde{\theta}_{\rm av}^0,z_{\rm av}^0)_t^{\top}   \Vert \right)  \\
&  \quad  + 78T \exp\left(8 \right)  \sup_{0\leq s\leq \min\{t,4T-\delta_{\xi} 2T\}} (\vert v(s)  \vert) \Big),
\end{split}
\end{equation}
for all $t\geq 0$; where $v(t)$ is as in  \eqref{eq:vanishing_perturbation_v_in_proof} with $x_1=\tilde{\theta}_{\rm av}$. Using  \eqref{eq:characteriation_T}, it finally holds    $\tilde{\theta}_{\rm av}(t)=0$ and $z_{\rm av}(t) =0$ for all $t\geq T^{\star} +2D$. This concludes the proof.
 
 
 \end{proof}
We now proceed to prove the main result of this paper (Theorem \ref{theo:main_ES_algorithm_prescribed-pointwise}). The proof exploits the classical averaging theorem for retarded functional differential equations \cite[Section 5]{Hale1990}, specifically focusing on the closeness of the solutions of the error system to those of its averaged counterpart. Additionally, it utilizes the prescribed-time convergence property of the averaged error-learning dynamics. 
\subsection{Proof of Theorem \ref{theo:main_ES_algorithm_prescribed-pointwise}} \label{proof_of_main_result}
   The nonlinear function $f$ of the RFDE with fast oscillations \eqref{eq:Compact_form_error_system},  \eqref{eq:RFDE-f1}-\eqref{eq:RFDE-f8} is  continuous in its arguments and is continuously differentiable in $X_t$  (notice, in particular, that that  the terms $\mathcal{K}_T(X_5)$, $\mathcal{L}_T(X_5,X_6)$, $\mathcal{K}_{ 2T}(X_7)$ are bounded and differentiable with respect to their arguments). Moreover,  $f$ admits a continuous FrÃ©chet derivative with respect to $X_t$  (which implies $f$  is locally   Lipschitz in $X_t$)  and  also  verifies $f(\omega t + 2\pi,X_t)= f(\omega t ,X_t)$. Consequently, all conditions  on $f$ to apply classical averaging theory in infinite dimensions (see  \cite[Section 5]{Hale1990} and \cite[Section 2.3]{Lehman2002} are met.    In addition, by Lemma \ref{eq:Prt_Target_system} and invoking  the arguments in Remark \ref{remk:forward_invariance_oscillators}, the solution to \eqref{eq:Compact_form_Averaged_error_system} is bounded for $t \geq 0$. Therefore,   applying \cite[Corollary, 5.2]{Hale1990} and \cite[Theorem 4]{Lehman2002}),  we have that for any $\nu >0$ and any large $\bar{T}\geq 0$ (in particular $\bar{T} >> T^{\star}+ 2D$); there exists $\bar{\omega} >0$  such that for each $\omega > \bar{\omega}$, 
 \begin{equation}
 \vert  X(t;\varphi)  -  X^{\rm av}(t;\varphi)\vert \leq \nu,  
 \end{equation}
 for all $0 \leq t \leq \bar{T}$, where $X(t;\varphi)$ and  $X^{\rm av}(t;\varphi)$ denote the solution of \eqref{eq:Compact_form_error_system}, and \eqref{eq:Compact_form_Averaged_error_system}, respectively. This result holds true, in particular,  for the $\tilde{\theta}$-component of $X$ in  \eqref{eq:Compact_form_error_system}, and the $\tilde{\theta}_{\rm av}$-component of $X^{\rm av}$ in \eqref{eq:Compact_form_Averaged_error_system}. Thus,
  \begin{equation}
 \vert  \tilde{\theta}(t)  -  \tilde{\theta}_{\rm av}(t)\vert \leq \nu,  
 \end{equation}
for all $0 \leq t \leq \bar{T} $. This result, along with the triangle inequality, yield
\begin{equation}
\vert \tilde{\theta}(t) \vert \leq \nu + \vert \tilde{\theta}_{\rm av}(t) \vert.
\end{equation}
Using Lemma \ref{eq:Prt_Target_system}, the estimate \eqref{eq:estimate_tilde_theta_av_afterLemma1}, and the fact $\tilde{\theta}^0=\tilde{\theta}_{\rm av}^0$, $z^0=z_{\rm av}^0$, we get
\begin{equation}
\begin{split}
\vert \tilde{\theta}(t) \vert \leq \nu + &\sigma \left(t-T^{\star} - 2D  \right)\Big((1+6T)\exp\left(8 \right)q\left( \Vert (\tilde{\theta}^0,z^0)^{\top}   \Vert \right)  \\
&  \quad  + 36T^2 \exp\left(8 \right)  \sup_{0\leq s\leq \min\{t,4T-\delta_{\xi} 2T\}} (\vert v(s)  \vert) \Big),
\end{split}
\end{equation}
for $0\leq t\leq \bar{T}$,  with $v(t)$ being the  finite time vanishing disturbance,  as in \eqref{eq:vanishing_perturbation_v_in_proof}, i.e.,  $v(t)=-\mathcal{L}_{T}(t) \tilde{\Gamma}_{\rm av}(t)H^{\star}\tilde{\theta}_{\rm av}(t-D)$.  Therefore, we can conclude that 
\begin{equation}
\left|\hat{\theta}(t)-\theta^\star\right| \leq \nu,\quad \forall  t \geq T^{\star} +2D ,   
\end{equation}
Moreover, using the expression of $y(t)$ given in  \eqref{eq:outout_function_static_maps} and the  Young's inequality, we obtain:
\begin{align}
\left|y(t)-y^\star\right|&\leq \tfrac{H^\star}{2}\left(\vert \hat{\theta}(t-D)-\theta^\star \vert+\vert S_D(t-D)\vert \right)^2\\
&\leq H^\star\vert\hat{\theta}(t-D)-\theta^\star\vert^2 + H^\star\vert S_D(t-D)\vert^2\\
&\leq H^\star(\nu^2+a^2),\quad \forall t \geq T^{\star} +3D .
\end{align}
This concludes the proof.

 	   	
  
 
  


\subsection{PT Newton extremum Seeking  with delay-free  map }
%Prescribed-Time Newton Extremum Seeking  with delay-free output map }

\begin{corollary}\label{theo:delay-free_main_ES_algorithm_prescribed-pointwise}
  Let us choose $T^{\star} > 0$ the prescribed time.   
Consider the prescribed-time  Newton-based extremum-seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}  with output map \eqref{eq:outout_function_static_maps}, where $D=0$, and with  time-periodic gains $\mathcal{K}_T(t)$,  $L_{T}(t)$ and $\mathcal{K}_{ 2T}(t)$  defined   in \eqref{eq:mathcalK_T}, \eqref{eq:mathcalL_T} and \eqref{eq:mathcalK_tau}, respectively,  and that are generated by the oscillators \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} and \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau}, with arbitrary initial conditions in $\mathbb{S}^{1}$ yielding   $\delta_{\zeta}$ and $\delta_{\xi}$  as in  \eqref{eq:definition_of_delta-phase_of_oscillators} and \eqref{eq:definition_of_delta-xi-phase_of_oscillators}, respectively. Let $T$ be selected according to \eqref{eq:characteriation_T}  
with $D=0$, namely
\begin{equation}\label{eq:characteriation_T_for_D=0}
T=\frac{T^{\star}}{4 - \delta_{\zeta}  + 2\Big \lceil \frac{4-(2\delta_{\xi} - \delta_{\zeta}) }{2}\Big\rceil}.
\end{equation}
Then, for any $\nu >0$, there exist $\bar{\omega} >0$  such that for each $\omega > \bar{\omega}$,  there exists a constant $\rho >0$ such that for each initial functions  $\phi:[- 2T,0]  \rightarrow \mathbb{R}^4$  that satisfy $\vert \phi(0) -\left(\theta^\star, 0,{H^\star}^{-1}, H^\star\right)^\top \vert \leq \rho $,   the solutions of the system  \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}  render  the learning dynamics and the output to satisfy, respectively,
\begin{equation}\label{eq:main_bound_learning_dynamics_in_corollary}
\left|\hat{\theta}(t)-\theta^\star\right| \leq \nu,\ \forall t \geq  T^{\star},    
\end{equation}
and
\begin{equation}\label{eq:main_bound_output_in_corollary}
\left|y(t)-y^\star\right| \leq H^\star(\nu^2+a^2),\ \forall t \geq   T^{\star}.   
\end{equation}
In particular, with  $\mathcal{K}_T(t)$,  $L_{T}(t)$  and $\mathcal{K}_{ 2T}(t)$ with oscillators initialized with $\zeta(0)=(0,1)^{\top}$, $\zeta(0)=(0,-1)^{\top}$ as in Theorem \ref{theo:main_ES_algorithm_prescribed-pointwise}, and any  $T>0$, the statements  \eqref{eq:main_bound_learning_dynamics_in_corollary} and \eqref{eq:main_bound_output_in_corollary} hold with 
\begin{equation}
\label{eq-4T}
T^{\star}=4T\,.
\end{equation}
\end{corollary}
An alternative algorithm for the delay-free map ($D=0$) is
\begin{align}
\dot{\hat{\theta}}(t)=& -\mathcal{K}_{T}(t)\Gamma(t)M(t)y(t-T) \label{eq:Newton_ES_static_maps1_prescribed-pointwise_pure_map_delay}  \\
\dot{\Gamma}(t)=&\mathcal{K}_{ 4T}(t) \Gamma^2(t)\left[\hat{H}(t- 4T)  -  N(t)y(t-T)\right],  %\nonumber \\&  
\label{eq:Newton_ES_static_maps3_prescribed-pointwise_Riccati_pure_map_delay} \\
\dot{\hat{H }}(t)=&-\mathcal{K}_{ 4T}(t)\left[\hat{H}(t-  4T)  -N(t)y(t-T)\right], \label{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_pure_map_delay}
\end{align}   
with arbitrary $T>0$, and \eqref{eq:dither_signal_as_tiago_S}-\eqref{eq:dither_signal_as_tiago_N} (with $D$ replaced by $T$ in \eqref{eq:dither_signal_as_tiago_S}),
for which, with $\delta_{\zeta}=\delta_{\xi}=0$, the average parameter error $\tilde{\theta}_{\rm av}(t)$ settles to zero in $10T$. This is an interesting alternative, since it eliminates the $z$-filter \eqref{eq:Newton_ES_static_maps2_prescribed-pointwise} and the predictor, but $10T$ is more than twice \eqref{eq-4T}. 

If the map has a delay, $D>0$, the map delay can be leveraged, i.e., $T$ in the algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_pure_map_delay}--\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_pure_map_delay} taken as $T=D$, and $y(t-T)$ in \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_pure_map_delay} replaced by $y(t)$. 


\section{Numerical Simulations}\label{numerical_simulations}

We consider the following static
quadratic map with the delayed measurement (with $D=5s$):
\begin{align}
 Q(\theta)=&5-0.4(\theta - 2)^2, \\
y(t)=& Q(\theta(t-5)).
\end{align}
We  implement the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}. We use the dither signals given  by 
\eqref{eq:dither_signal_as_tiago_S}-\eqref{eq:dither_signal_as_tiago_N} whose parameters  are $a=0.5$, $\omega=1000$.   For simulations purposes the delayed dither signal $S_D(t-D)$  is initialized  as $S_D(t-D)=0$ for all $t \in [0,D]$.  We consider the oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_T} with initial conditions $\zeta(0)=(0,1)^{\top}$  and the oscillator \eqref{eq:oscillator_system_periodic_time-varying_feedbacks_tau}-\eqref{eq:matrix_bloc_oscillator_pulse_signal_tau} with initial conditions $\xi(0)=(0,1)^{\top}$, yielding in both cases, $\delta_{\zeta}=0$ and $\delta_{\xi}=0$ computed according to \eqref{eq:definition_of_delta-phase_of_oscillators}, and  \eqref{eq:definition_of_delta-xi-phase_of_oscillators}, respectively.  
We set the prescribed time $T^{\star}=40s$ (recall $T^{\star}>  6D$  according to \eqref{eq:restriction_choice_T_star}), and we select $T=6.25s$  according to  \eqref{eq:characteriation_T}. 
We set the initial state $\hat{\theta}(0)=-5$  of the estimator and its preceding delay values (i.e., $\hat{\theta}(s)=-5$ for all $s \in [-2T,0]$ knowing that $T> D$). We set initial state  of the filter $z$  as  $z(0)=0.1$ and its preceding delay values (i.e., $z(s)=0.1$ for all   $s \in  [-2T,0]$)  and we set  $\Gamma(0)=-0.9$ (thus $\hat{H}(0)=1.11$ and its preceding delay values (i.e., $\hat{H}(s)=1.11$ for all  $s \in [- 2T,0]$)). 

\begin{figure*}[t!]
\centering{
\subfigure{\includegraphics[width=0.9\columnwidth]{static_mapV2.eps} }
\subfigure{\includegraphics[width=0.9\columnwidth]{learning_dynamicsV2.eps} }
\caption{On the left: Time-evolution of the static map \eqref{eq:outout_function_static_maps}  under the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} (red line) and under the exponential  Newton-based extremu seeking algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps2V2} (black line). The prescribed time of convergence is dictated by  $T^{\star}+ 3D = 55s$.
On the right: Time evolution of the estimator $\hat{\theta}(t)$ under the prescribed-time seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}  (red line) and under the exponential  Newton-based extremu seeking algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps2V2} (black line). The prescribed-time   of convergence is dictated by   $T^{\star}+2D=50 s$.}
\label{Static-map_and_Estimators_plot}
}
\end{figure*}


\begin{figure}[t]
\centering{
%\includegraphics[width=1\columnwidth]{Riccati_plotV2.eps} 
\includegraphics[width=1\columnwidth]{Riccati_plotV2_modif.eps} 
\caption{On the top: Time-evolution of the Riccati-like filter estimating the inverse of the Hessian under the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} (red line). On the bottom: Time-evolution of the Riccati-like filter estimating the inverse of the Hessian  under the prescribed-time seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}  (red line) and under  the exponential  Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps2V2} (black line). The prescribed time of convergence for the Hessian and (its inverse) estimation is dictated by  $ 4T = 25s$.}
\label{Riccati_plot}
}
\end{figure}



\begin{figure}[t]
\centering{
\includegraphics[width=0.9\columnwidth]{plot_log_averaged_error_thetaav_and_zav.eps} 
\caption{Plot in logarithmic scale  of the norm of the average error-dynamics $\vert (\tilde{\theta}_{\rm av}(t),z_{\rm av})^{\top}\vert$ (see \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps2_prescribed-pointwise_error_averaged}) under the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} (red line) and under the exponential  Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps2V2} (black line).}
\label{Error_dynamics_plots_logaritmic_scale}
}
\end{figure}

Figure \ref{Static-map_and_Estimators_plot} shows on the left the evolution of the output $y(t)$ with delay $D=5s$  and on the right the  time-evolution of the estimator $\hat{\theta}(t)$. In both cases,  we use: i) the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} (red line)    and  ii)  the exponential  Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps2V2} (black line). For the latter ES algorithm, we have selected $w_r=0.02$, $c=0.1$ and $K=0.05$.

As expected, under the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian}, the time of convergence for the output to reach the maximum  is given by   $T^{\star}+3D=55s$ and the time of convergence for the learning dynamics $\hat{\theta}(t)$ to estimate $\theta^\star$ is given by  $T^{\star} +2D=50s$.  
As it can be observed in Figure  \ref{Static-map_and_Estimators_plot}  the convergence of the  output $y(t)$ (resp. the estimator $\hat{\theta}(t)$) to the optimum $y^\star$ (resp. to $\theta^\star$)   is faster with our Prescribed-time  Newton-based ES algorithm than with and Exponential ES algorithm. 


Figure \ref{Riccati_plot} shows the evolution of the $\Gamma(t)$  by using: i) the Prescribed-time Newton-based extremum seeking algorithm \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian} (red line) with a time of convergence $ 4T = 25s$;    and ii)  the exponential  Newton-based ES algorithm \eqref{eq:Newton_ES_static_maps1V2}-\eqref{eq:Newton_ES_static_maps2V2} (black line). 

Finally, Figure \ref{Error_dynamics_plots_logaritmic_scale}  shows the norm, in logarithmic scale,  of the solution of the averaged error-dynamics $\tilde{\theta}_{\rm av}(t)$ and $z_{\rm av}(t)$, in red lines, when using the  Prescribed-time Newton-based ES  (thus \eqref{eq:Newton_ES_static_maps1_prescribed-pointwise_error_averaged}-\eqref{eq:Newton_ES_static_maps3_prescribed-pointwise_Hessian_error_averaged}); and in black line, when using Exponential ES (thus \eqref{eq:Newton_ES_static_averaged1}-\eqref{eq:Newton_ES_static_averaged3}). One can appreciate the convergence to zero within a prescribed-time   $T^{\star}+2D=50 s$.

%\Nicolas{Simulations for the particular case $D=0$ can be included as well, if needed.}



\section{Conclusion}\label{Section:Conclusion_ExTS}
This paper presented a novel  Newton-like extremum seeking (ES) algorithm for static maps subject to time delay. The proposed algorithm achieves delay-compensation alongside convergence towards the optimum in a finite time  that can be prescribed in the design. It builds on time-periodic gains and delayed feedbacks and, as such, we refer to it as the {\em KHV approach to PT-ES}. The key advantage of this approach over existing methods is its ability to achieve PT convergence without relying on singular gains. 
%, ensuring that  ES closed-loop solutions remain well-defined for all time.

Future work will extend this result to Multi-variable extremum seeking with multiple delays. In addition, we will leverage this methodology to propose a delay-compensated prescribed-time  ES for dynamical systems with delays. 

%
\paragraph*{\bf Acknowledgment}This work was partially
supported by the Agence Nationale de la Recherche (ANR)
via grant PH-DIPSY ANR-24-CE48-1712, and by NSF grants CMMI-2228791 and ECCS-2210315. 


\appendix 
\subsection{Proof of Lemma \ref{Lemma_PrT_Pointwise_delay}}\label{proof_of_lemma1}
\begin{proof}
The one-dimensional control system \eqref{eq:single_integrator} with time-varying pointwise delayed feedback \eqref{eq:time-varying_delayed-feedback_key_result} reads as follows:
\begin{equation}\label{eq:single_integrator_closed-loop_in_proof}
\dot{x}(t)=-\mathcal{K}_{T}\left(t\right)x(t-T) + v(t).
\end{equation}
The solution to \eqref{eq:single_integrator_closed-loop_in_proof}, with initial condition $x_0=\phi(s) $, $\phi \in C^0([-T,0];\mathbb{R})$ is given  by $x(t)=\phi(t)$ for $t \in [-T,0]$; and
\begin{equation}\label{eq:direct_integration_solution_closed_loopsystem}
x(t)=\phi(0)  - \int_{0}^{t}\mathcal{K}_{T}(s)x(s-T)ds + \int_{0}^{t}v(s)ds,
\end{equation}
 for $t \geq 0$. 
 Hence, by  \cite[Chapter 6]{Hale1993}, we can obtain the following estimate for \eqref{eq:direct_integration_solution_closed_loopsystem}:
 \begin{equation}
 \vert x(t) \vert \leq \max_{-T \leq s\leq 0}(\vert \phi(s) \vert) + \int_{0}^{T}\vert \mathcal{K}_{T}(s)\vert \vert x(s-T) \vert ds + \int_{0}^{t}\vert v(s) \vert ds.
 \end{equation}
 Moreover, the following estimate holds:
 \begin{equation}
 \begin{split}
 \vert x(t) \vert \leq& \max_{-T \leq s\leq 0}(\vert \phi(s) \vert) + \int_{-T}^{0}\vert \mathcal{K}_{T}(r+T)\vert \vert \phi(r) \vert dr  \\
 &+ \int_{0}^{t}\vert \mathcal{K}_{T}(r+T)\vert \vert x(r) \vert dr + \int_{0}^{t}\vert v(s) \vert ds.
\end{split} 
 \end{equation}
 The definition of $\mathcal{K}_{T}$  ensures  that $\mathcal{K}_{T}(r +T)=0$ for $r  \in [-T,0]$, and that $\vert \mathcal{K}_{T}(r+T) \vert \leq \tfrac{2}{T}$ for $r  \in [0,t]$. Therefore we obtain, 
 \begin{equation}
 \vert x(t) \vert \leq \max_{-T \leq s\leq 0}(\vert \phi(s) \vert) + \int_{0}^{t}\tfrac{2}{T} \vert x(r) \vert dr + \int_{0}^{t}\vert v(s) \vert ds,
 \end{equation}
 for $t \geq 0$.  Using the GrÃ¶nwall-Bellman inequality, we further obtain
 \begin{equation}\label{eq:exponential_estimate_withGronwall}
 \vert x(t) \vert \leq \left( \max_{-T \leq s\leq 0}(\vert \phi(s) \vert) + t \sup_{0\leq s\leq t}  (\vert v(s) \vert \right) \exp\left( \tfrac{2}{T}t \right). 
 \end{equation}
 Consider now the time sequence  $\bar{t}_n=\max\{0,2Tn-\delta_{\zeta} T\}$, $\delta_{\zeta} \in (-1,1]$, $n\in\mathbb{Z}^{+}$. Then,  for $t \in [\bar{t}_n,\bar{t}_n +T]$, the solution to \eqref{eq:single_integrator_closed-loop_in_proof} is given as follows:
\begin{equation}\label{eq:solution_closed_loopsystem_interval_Ktequal_0}
x(t) = x(\bar{t}_n) + \int_{\bar{t}_n}^tv(\tau)d\tau,
\end{equation}
from which we can obtain the following estimate:
\begin{equation}\label{eq:first_estimate_interval}
\vert x(t) \vert \leq \vert x(\bar{t}_n)\vert  + T \sup_{\bar{t}_n\leq \tau \leq t}(\vert v(\tau) \vert).
\end{equation}
For  $t \in [\bar{t}_n + T,\bar{t}_{n+1}]$, the solution to \eqref{eq:single_integrator_closed-loop_in_proof} is given as follows:
\begin{equation}
\begin{split}
x(t) = & x(\bar{t}_n +T) + \int_{\bar{t}_n+T}^tv(\tau)d\tau - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)x(\tau-T)d\tau. \\
\end{split}
\end{equation}
Using \eqref{eq:solution_closed_loopsystem_interval_Ktequal_0}, we further obtain, for all $t \in [\bar{t}_n +T, \bar{t}_{n+1}]$
\begin{equation}
\begin{split}
x(t)=& x(\bar{t}_n) - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)x(\bar{t}_n)d\tau \\
&+ \int_{\bar{t}_n}^{\bar{t}_n+T}v(\tau)d\tau + \int_{\bar{t}_n+T}^tv(\tau)d\tau \\
&   - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)\left(\int_{\bar{t}_n}^{\tau-T} v(s)ds \right)d\tau. 
\end{split}
\end{equation}
Therefore, we get
\begin{equation}\label{eq:explicit_solution_on_interval_inproof}
\begin{split}
x(t)=& x(\bar{t}_n) \bar{K}_{T}(\bar{t}_n,t) \\
&+ \int_{\bar{t}_n}^{t}v(\tau)d\tau     - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)\left(\int_{\bar{t}_n}^{\tau-T} v(s)ds \right)d\tau. 
\end{split}
\end{equation}
with 
\begin{equation}\label{eq:K_T_bar_in-proof_lemma1}
\bar{K}_{T}(\bar{t}_n,t)=1 - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)d\tau.
\end{equation} 
Hence, from \eqref{eq:explicit_solution_on_interval_inproof}, we obtain the following estimate, for all $t \in [\bar{t}_n+T, \bar{t}_{n+1}]$:
\begin{equation}
\begin{split}
\vert x(t) \vert \leq & \vert x(\bar{t}_n) \vert \vert \bar{K}_{T}(\bar{t}_n,t) \vert + (t-\bar{t}_n)\sup_{\bar{t}_n \leq \tau \leq t} (\vert v(\tau) \vert) \\
& +(t-T -\bar{t}_n)\sup_{\bar{t}_n \leq \tau \leq t}(\vert v(\tau) \vert)\left( \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)d\tau \right). 
\end{split}
\end{equation}
Thus, 
\begin{equation}\label{eq:estimate_x(t)_in_proof_on_interval}
\vert x(t) \vert \leq  \vert x(\bar{t}_n) \vert \vert \bar{K}_{T}(\bar{t}_n,t) \vert + 3T\sup_{\bar{t}_n \leq \tau \leq t}(\vert v(\tau) \vert),
\end{equation}
where we have used $\int_{\bar{t}_n+T}^{t}\mathcal{K}_{T}(\tau)d\tau \leq \int_{\bar{t}_n+T}^{\bar{t}_{n+1}}\mathcal{K}_{T}(\tau)d\tau $ and  the property 
\begin{equation}\label{eq:Property_integral_K_T}
\int_{\bar{t}_n+T}^{\bar{t}_{n+1}}\mathcal{K}_{T}(\tau)d\tau = 1.
\end{equation}
This property can be verified by recalling the definition \eqref{eq:mathcalK_T_V0} in conjunction with \eqref{eq:explicit_solution_osicillator1_V2}. 
Notice that $\vert x(\bar{t}_n) \vert$ in  \eqref{eq:first_estimate_interval} or in  \eqref{eq:estimate_x(t)_in_proof_on_interval} can further be upper bounded using \eqref{eq:exponential_estimate_withGronwall}. 
Moreover, from \eqref{eq:estimate_x(t)_in_proof_on_interval}, at $t=\bar{t}_{n+1}$, and using the fact that $\bar{K}_{T}(\bar{t}_n,\bar{t}_{n+1})=0$ due to   \eqref{eq:K_T_bar_in-proof_lemma1} and \eqref{eq:Property_integral_K_T}, the following estimate holds:
\begin{equation}\label{eq:estimate_at_tnplus1}
\vert x(\bar{t}_{n+1}) \vert \leq 3T\sup_{\bar{t}_n \leq \tau \leq \bar{t}_{n+1}} (\vert v(\tau) \vert).
\end{equation}
Repeating the same reasoning on the interval $ [\bar{t}_{n+1}, \bar{t}_{n+2}]$ (the control features zero gain,  acting on the zero solution), we can get
\begin{equation}\label{eq:estimate_at_tnplus2}
\vert x(t) \vert \leq 6T \sup_{\bar{t}_n \leq \tau \leq t}  (\vert v(\tau) \vert).
\end{equation}
Hence, 
using the fact that $t-(2T-\delta_{\zeta} T) \leq \bar{t}_n$, for any $n \in \mathbb{Z}^{+}$
 and   combining \eqref{eq:exponential_estimate_withGronwall}, \eqref{eq:first_estimate_interval},  \eqref{eq:estimate_x(t)_in_proof_on_interval},  \eqref{eq:estimate_at_tnplus1} and \eqref{eq:estimate_at_tnplus2} we finally obtain
\begin{equation}\label{eq:final_estimate_in_proof}
\begin{split}
 \vert x(t) \vert \leq & \exp\left(4 \right) \sigma\left(t-2T \right)\max_{-T \leq s\leq 0}(\vert \phi(s) \vert )  \\
 & \hskip 1cm + 6T \exp\left(4 \right)\sup_{\max \{0,t-(2T-\delta_{\zeta} T) \}\leq s\leq t} (\vert v(s) \vert ). 
\end{split} 
 \end{equation}
 where $\sigma(\cdot)$ is defined in \eqref{eq:heaviside_function}.\\
  To show the  last part of the conclusion,    we follow  similar steps  as those  below \eqref{eq:exponential_estimate_withGronwall} and distinguish two cases:\\
 i) $v(t)=0$ for $t\geq t^{'}$ where $ \bar{t}_n \leq t^{'} \leq  \bar{t}_n +T$. We have that for   $t \in [\bar{t}_n,\bar{t}_n +T]$, the solution to \eqref{eq:single_integrator_closed-loop_in_proof} is given as follows:
\begin{equation}\label{eq:solution_closed_loopsystem_interval_Ktequal_0-distrubance}
x(t) = x(\bar{t}_n) + \bar{v}(\bar{t}_n,t),
\end{equation}
and, for   $t \in [\bar{t}_n + T,\bar{t}_{n+1}]$, the solution to \eqref{eq:single_integrator_closed-loop_in_proof} is given as follows:
\begin{equation}\label{eq:solution_closed_loopsystem_interval_Ktequal_1-distrubance}
\begin{split}
x(t) = & x(\bar{t}_n +T)  - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)x(\tau-T)d\tau \\
= &  x(\bar{t}_n)\bar{K}_{T}(\bar{t}_n,t) + \bar{v}(\bar{t}_n,\bar{t}_n+T) \\
& - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)\bar{v}(\bar{t}_n,\tau-T)d\tau ,
\end{split}
\end{equation}
where we have used \eqref{eq:K_T_bar_in-proof_lemma1}, and the following notation:
\begin{equation}
\bar{v}(\bar{t}_n,t)=\int_{\bar{t}_n}^tv(\tau)d\tau.
\end{equation}
ii) $v(t)=0$ for $t\geq t^{'}$ where $ \bar{t}_n +T  \leq t^{'} \leq  \bar{t}_{n+1} $.  A similar reasoning yields
\begin{equation}\label{eq:solution_closed_loopsystem_interval_Ktequal_2-distrubance}
\begin{split}
x(t) = &  x(\bar{t}_n)\bar{K}_{T}(\bar{t}_n,t) + \bar{v}(\bar{t}_n,\bar{t}_n+T) + \bar{v}(\bar{t}_n+T,t)  \\
& - \int_{\bar{t}_n +T}^t \mathcal{K}_{T}(\tau)\bar{v}(\bar{t}_n,\tau-T)d\tau ,
\end{split}
\end{equation}
where
\begin{equation}
\bar{v}(\bar{t}_n+T,t)=\int_{\bar{t}_n+T}^tv(\tau)d\tau.
\end{equation}
Thus, at $t=\bar{t}_{n+1}$ and by virtue of  \eqref{eq:Property_integral_K_T}, it holds from either  \eqref{eq:solution_closed_loopsystem_interval_Ktequal_1-distrubance}  or \eqref{eq:solution_closed_loopsystem_interval_Ktequal_2-distrubance} that $x(\bar{t}_{n+1}) \neq 0$, hence it becomes the new initial condition.
For the next interval of time $[\bar{t}_{n+1},\bar{t}_{n+1}+T]$, we have that the solution  to \eqref{eq:single_integrator_closed-loop_in_proof} is given as follows:
\begin{equation}\label{eq:solution_closed_loopsystem_interval_Ktequal_22-distrubance}
x(t)=x(\bar{t}_{n+1}),
\end{equation}
thus $x(\bar{t}_{n+1} +T)=x(\bar{t}_{n+1})$; and for $t \in [\bar{t}_{n+1}+T,\bar{t}_{n+2}]$, we obtain 
\begin{equation}\label{eq:solution_closed_loopsystem_interval_Ktequal_3-distrubance}
x(t)=x(\bar{t}_{n+1})\mathcal{K}_{T}(\bar{t}_{n+1},t),
\end{equation}
where $\bar{K}_{T}(\bar{t}_{n+1},t)$ is defined as follows (similarly to \eqref{eq:K_T_bar_in-proof_lemma1}): 
\begin{equation}\label{eq:K_T_bar_in-proof2_lemma1}
\bar{K}_{T}(\bar{t}_{n+1},t)=1 - \int_{\bar{t}_{n+1} +T}^t \mathcal{K}_{T}(\tau)d\tau,
\end{equation}
with the property $\bar{K}_{T}(\bar{t}_{n+1},\bar{t}_{n+2})=0$. Therefore, we finally obtain
\begin{equation}\label{eq:dead_beat_property_tn2}
x(\bar{t}_{n+2}) =0.
\end{equation}
Repeating the same reasoning on the interval  $ [\bar{t}_{n+2}, \bar{t}_{n+3}]$ (the control features zero gain,  acting on the zero solution), we conclude that $x(t)=0$ for all $t \geq  \bar{t}_{n+2} $.  
  Recalling that  $\bar{t}_{n+2}=2T(n+2) -\delta_{\zeta} T$,  the fact that $t^{'} \leq \bar{t}_{n+1} $,  and by the definition of the ceiling function, we have that $\frac{t^{'}+ \delta_{\zeta} T}{2T} \leq \Big \lceil \frac{t^{'}+ \delta_{\zeta} T}{2T}\Big\rceil = (n+1)$, hence the dead-beat property is achieved at $2T\lceil \frac{t^{'}+ \delta_{\zeta} T}{2T}\rceil + 2T-\delta_{\zeta} T$. By induction, it holds true for any interval,  i.e., $ \bar{t}_{n} \leq t^{'} \leq  \bar{t}_{n+1}$, for arbitrary  $n \in \mathbb{Z}^+$. Finally, estimate \eqref{eq:bound_PrT_single_integrator-control-vanishing_dist} can be established using  \eqref{eq:exponential_estimate_withGronwall}, \eqref{eq:solution_closed_loopsystem_interval_Ktequal_3-distrubance} and   \eqref{eq:dead_beat_property_tn2}.
This concludes the proof. 
 


  
 
\end{proof}




%\hspace*{\fill}~\QED\par\endtrivlist\unskip

%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
% on the last page of the document manually. It shortens
% the textheight of the last page by a suitable amount.
% This command does not take effect until the next page
% so it should come on the page before the last. Make
% sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\bibliographystyle{IEEEtranS}
\bibliography{biblio_Ectremum_seeking24}


% \hspace{-0.35cm}

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Espitia_Photo.eps}}]{Nicolas Espitia} received the B.S. degree in electronic engineering and the second B.S. degree in mathematics from the Escuela Colombiana de IngenierÃ­a Julio Garavito, Bogota, Colombia, in 2011 and 2012, respectively, the M.S. degree in systems, control, and information technology from UniversitÃ© Joseph Fourier, Grenoble, France, in 2014, and the Ph.D. degree in control systems from the GIPSA-Lab, Grenoble University, Grenoble, in 2017. From 2017 to 2019, he was a Postdoctoral Researcher with Inria Lille Nord-Europe. Since October 2019, he has been a CNRS Researcher with the Centre de Recherche en Informatique, Signal et Automatique de Lille, Lille, France. He serves as Associate Editor for European Journal of Control and Systems \& Control Letters. His research interests include event-triggered control and finite-/fixed-/prescribed-time stabilization and estimation of Distributed Parameter Systems.
% \end{IEEEbiography}
% \hspace{-0.5cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Krstic-red-tie.eps}}]{Miroslav Krstic} is Distinguished Professor and Senior Associate Vice Chancellor for Research at UC San Diego. Krstic is Fellow of IEEE, IFAC, ASME, SIAM, AAAS, IET (UK), and AIAA (Assoc. Fellow) - and member of the Serbian Academy of Sciences. He has received the Bode Lecture Prize, Bellman Award, SIAM Reid Prize, ASME Oldenburger Medal, Nyquist Lecture Prize, Paynter Outstanding Investigator Award, Ragazzini Education Award, IFAC Nonlinear Control Systems Award, IFAC Distributed Parameter Systems Award, IFAC Adaptive and Learning Systems Award, Chestnut textbook prize, Control Systems Society Distinguished Member Award, the PECASE, NSF Career, and ONR Young Investigator awards, the Schuck (96 and 19) and Axelby paper prizes. He serves as Editor-in-Chief of Systems \& Control Letters and has been serving as Senior Editor in Automatica and IEEE Transactions on Automatic Control.

% \end{IEEEbiography}
% \hspace{-0.5cm}


% \hspace{-0.5cm}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{Poveda.eps}}]{Jorge. I. Poveda } is Assistant Professor in the Department of Electrical and Computer Engineering at the University of California, San Diego, La Jolla, CA, USA. He received his M.Sc. and Ph.D. degrees in Electrical and Computer Engineering from the University of California, Santa Barbara in 2016 and 2018, respectively, followed by a Postdoctoral position at Harvard University. He was a Research Intern at the Mitsubishi Electric Research Laboratories in 2016 and 2017. He has received the NSF CRII and CAREER awards, the AFOSR and SHPE Young Investigator awards, the 2023 AACC Donald P. Eckman award, the UCSB-CCDC Outstanding Scholar Fellowship (2013) and Best Ph.D. Thesis award (2020), and the 2023 IEEE Transactions on Control of Network Systems Best Paper award. Furthermore, he serves as an advisor to students who were selected as finalists for the 2024 American Control Conference's Best Student Paper Award and winners of the Best Student Paper Award at the 2024 IFAC Conference on Analysis and Design of Hybrid Systems. He was a finalist for the Best Student Paper Award at the IEEE Conference on Decision and Control in 2017 (as a student) and in 2021 (as a co-author). He serves as Associate Editor for Automatica, IEEE Control Systems Letters, and  Nonlinear Analysis: Hybrid Systems. His research interests include hybrid control, adaptive and network systems, and model-free control and optimization.
% \end{IEEEbiography}




\end{document}
 %
%\appendices
