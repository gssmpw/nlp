% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{colortbl}
\usepackage{import}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
% Standard package includes
\usepackage{times}
\usepackage{markdown}
\usepackage{latexsym}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
% \usepackage[finalizecache,cachedir=./cache]{minted}
\usepackage[frozencache,cachedir=./cache]{minted}
\definecolor{promptColor1}{RGB}{245,245,220} % light beige 
\definecolor{promptColor2}{RGB}{220,235,255} % light blue 
\definecolor{promptColor3}{RGB}{232,245,219} % light green
\definecolor{promptColor4}{RGB}{255,243,224} % light orange 
\definecolor{promptColor5}{RGB}{253,236,234} % light pink 

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{subfigure}
\usepackage{subcaption}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{booktabs}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{comment}
\usepackage{color, colortbl}

% create a new command for the langue format
\newcommand{\lang}[1]{\texttt{#1}}
\newcommand{\dataset}[0]{\textsc{Injongo}}
\newcommand*{\clinc}{\textsc{Clinc} \xspace}
\newcommand*{\massive}{\textsc{Massive} \xspace}
\newcommand*{\afroxlmr}{AfroXLMR-76L \xspace}
\newcommand*{\gemini}{Gemini 1.5 Pro \xspace}
\newcommand*{\gpto}{GPT-4o\xspace}
\newcommand*{\yoruba}{Yor\`ub\'a\xspace}
%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{float}
\definecolor{Gray}{gray}{0.9}
\definecolor{LightCyan}{rgb}{0.88,1,1}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{listings}
\lstset{
  breaklines=true,
  breakatwhitespace=true,
  columns=fullflexible,
}
\title{\dataset: A Multicultural Intent Detection and Slot-filling Dataset for 16 African Languages}
\setlength\titlebox{7.0cm}


\author{
\normalsize 
Hao Yu$^{1,2}$, Jesujoba O. Alabi$^{3,*}$, Andiswa Bukula$^{4,*}$, Jian Yun Zhuang$^{5}$, En-Shiun Annie Lee$^{6}$,\\ 
%\textbf{\normalsize } \\ 
\textbf{\normalsize Tadesse Kebede Guge$^{*}$, Israel Abebe Azime$^{3}$, Happy Buzaaba$^{7,*}$, Blessing Kudzaishe Sibanda$^{*}$},\\ 
\textbf{\normalsize Godson K. Kalipe$^{*}$, Jonathan Mukiibi$^{8,*}$, Salomon Kabongo Kabenamualu$^{9,*}$, Mmasibidi Setaka$^{4}$},\\ 
\textbf{\normalsize Lolwethu Ndolela$^{*}$, Nkiruka Odu$^{*}$, Rooweither Mabuya$^{4,*}$, Shamsuddeen Hassan Muhammad$^{10,*}$},\\ 
\textbf{\normalsize Salomey Osei$^{11}$, Sokhar Samb$^{12}$, Juliet W. Murage$^{*}$, Dietrich Klakow$^{3}$, David Ifeoluwa Adelani$^{1,2,*}$} \\ 
\textbf{\normalsize } \\
\footnotesize
$^*$Masakhane NLP, 
$^{1}$McGill University, Canada, 
$^{2}$Mila, Quebec AI Institute, Canada, 
$^{3}$Saarland University, Germany,\\
\footnotesize
$^{4}$SADiLaR, South Africa, 
$^{5}$University of Toronto, Canada, 
$^{6}$ OntarioTech University, Canada,
$^{7}$Princeton University, USA,\\
\footnotesize
$^{8}$Makerere University, Uganda, 
$^{9}$L3S Research Center, Germany, 
$^{10}$Imperial College London, United Kingdom,\\
\footnotesize
$^{11}$Universidad de Deusto, Spain,
$^{12}$Dakar American University Of Science and Technology, Senegal.
\vspace{5em}
}
\begin{document}
\maketitle

\begin{abstract}
Slot-filling and intent detection are well-established tasks in Conversational AI. However, current large-scale benchmarks for these tasks often exclude evaluations of low-resource languages and rely on translations from English benchmarks, thereby predominantly reflecting Western-centric concepts. In this paper, we introduce \textsc{Injongo}---a multicultural, open-source benchmark dataset for 16 African languages with utterances generated by native speakers across diverse domains, including banking, travel, home, and dining. Through extensive experiments, we benchmark the fine-tuning multilingual transformer models and the prompting large language models (LLMs), and show the advantage of leveraging African-cultural utterances over Western-centric utterances for improving cross-lingual transfer from the English language. Experimental results reveal that current LLMs struggle with the slot-filling task, with GPT-4o achieving an average performance of 26 F1-score. In contrast, intent detection performance is notably better, with an average accuracy of 70.6\%, though it still falls behind the fine-tuning baselines. When compared to the English language, GPT-4o and fine-tuning baselines perform similarly on intent detection, achieving an accuracy of approximately 81\%. Our findings suggest that the performance of LLMs is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance. %These findings underscore the gap in LLM performance for many low-resource African languages, emphasizing the need for further research to enhance downstream performance in these languages.

\end{abstract}



\section{Introduction}
% TODO: Add more context and motivation

Intent detection and slot-filling are crucial components of the natural language understanding module in task-oriented dialogue systems~\citep{hemphill-etal-1990-atis, Coucke2018SnipsVP,gupta-etal-2018-semantic-parsing}. They map a user's request to a predefined semantic category recognized by the dialogue manager, along with extracting specific entities (known as slots). This process facilitates generating an appropriate response for the end user. Despite their importance, only a few languages have labeled datasets available for these tasks across multiple domains~\citep{Larson2022ASO}. 

Several efforts have been made to make datasets multilingual through human translation into other languages~\citep{xu-etal-2020-end,li-etal-2021-mtop,van-der-goot-etal-2021-masked,ruder-etal-2023-xtreme}. However, these efforts face two key challenges: (1) the translationese effect, which makes utterances sound less natural in the target languages~\citep{vanmassenhove-etal-2021-machine,bizzoni-etal-2020-human}, and (2) the creation of utterances that are less culturally relevant. The Massive dataset \cite{fitzgerald-etal-2023-massive}, which covers 51 languages, addresses the second challenge by encouraging translators to ``localize'', ``translate'', or ``keep the slot unchanged''. Despite improvements in the utterance generation process, \textsc{Massive} includes only three African languages (Amharic, Afrikaans and Swahili), and many utterances remain culturally irrelevant to the target language communities.

In this paper, we develop \dataset{}---the first large-scale \textit{multicultural} intent
detection and slot-filling dataset covering 16 African languages, and English language. We cover the following five domains: banking, home, travel, utility, and kitchen \& dining. The data construction process starts with providing an annotator with sentences from the \clinc dataset~\citep{Larson2022ASO} with a specified \textit{intent type}, and they are to come up with culturally-relevant similar sentences and relevant slot entities (see \autoref{fig:example}). The utterance generation process is followed by slots annotation. \dataset{} dataset covers 5 domains, 40 intents, 23 slots, and 3,200 instances per African language. 

\begin{table*}[t]
\small\centering
\resizebox{\textwidth}{!}{%
  \begin{tabular}{lrrrrrll}
  \toprule
  \textbf{Dataset} & \textbf{\# Domains} & \textbf{\# Intents}  & \textbf{\# Slots} & \textbf{\# utterances} & \textbf{\# Languages} & \textbf{\# African languages} & \textbf{Multi-cultural?} \\
  \midrule
CLINC~\citep{CLINC}& 10 & 150 & 0 & 23,700 & 1 & 0 & yes \\
Facebook~\citep{schuster-etal-2019-cross-lingual} & 3 & 12 & 11 & 57,000 & 3 & 0 & yes \\
MultiATIS~\citep{xu-etal-2020-end} & 11 & 26 & 140 & 44,943 & 9 & 0 & no \\
xSID~\cite{van-der-goot-etal-2021-masked} & 7 & 16 & 33 & 10,000 & 13 & 0 & no \\
MTOP~\cite{li-etal-2021-mtop} & 11 & 117 & 78 & 100,000 & 6 &  0 & no \\
MTOP++~\citep{ruder-etal-2023-xtreme} & 11 & 117 & 78 & 144,243 & 20 & 5 (amh, hau, yor, swa, zul) & no \\
MASSIVE~\citep{fitzgerald-etal-2023-massive} & 18 & 60 & 55 & 995,571 & 51 & 3 (afr, amh, swa) & partial\\
\midrule
\dataset~(Ours) & 5 & 40 & 23 & 52,979 & 17 & \textbf{16} & yes \\
  \bottomrule
  \end{tabular}
}
  \vspace{-2mm}
  \caption{\textbf{Overview of important related works that intent detection and slot-filling tasks}. We included the number of domains, intents, slots, languages, African languages and how multicultural are the utterances. }
  \label{tab:past_works}
\end{table*}


We performed several supervised fine-tuning experiments with multilingual encoders and prompting of Large Language Models (LLMs), both using \dataset{}. Our result shows that fine-tuning baselines could reach an accuracy of 93.7\% and F1-score of 85.6 for intent detection and slot-filling tasks respectively. While the best prompting of LLMs results (GPT-4o) drops by -28\% accuracy point and $-52.6$ F1 score. While slot-filling and named entity recognition tasks are often challenging for LLMs even for English~\citep{Yu2023OpenCO}, intent detection performance in English is similar performance whether we use fine-tuning baselines or prompt GPT-4o. Our findings suggest that LLMs performance is still behind for many low-resource African languages, and more work is needed to further improve their downstream performance.  For reproducibility, we open-source our code\footnote{\href{https://github.com/McGill-NLP/Injongo}{McGill-NLP/Injongo}} and dataset \footnote{\href{https://github.com/masakhane-io/masakhane-nlu/tree/main/InjongoIntent}{Masakhane-NLU}} on GitHub. Dataset is released under CC BY 4.0 license. Models will be released on the HuggingFace soon. 
 % and models  models not releasted
 
%Furthermore, we show that cross-lingual transfer learning is more effective from our English \dataset{} dataset that captures more African contexts than adapting from \clinc dataset with more Western contexts. This confirms the importance of developing culturally-relevant benchmarks for African languages for conversational AI and task-oriented dialogue systems.

%The development of NLP technologies has predominantly focused on high-resource languages, leaving a significant gap in support for low-resource languages, particularly those from Africa. Despite serving millions of speakers and representing rich cultural diversity, African languages remain underrepresented in mainstream NLP benchmarks. 

\section{Related Work}
% Happy is working on this section
% David focus on two aspects: African Benchmarks and Intent-detection benchmarks

\paragraph{African Benchmarks}
Limited available labeled datasets are one of the major challenges of AfricaNLP. Since 2021, there have been many grassroots efforts to create large-scale datasets for African languages covering several tasks such as machine translation~\citep{Alabi2025}, named entity recognition~\citep{adelani-etal-2021-masakhaner,adelani-etal-2022-masakhaner}, sentiment classification~\citep{muhammad-etal-2023-afrisenti}, hate speech~\citep{Muhammad2025AfriHateAM}, question answering~\citep{ogundepo-etal-2023-cross}, topic classification~\citep{adelani-etal-2023-masakhanews,AfroXLM-76L} covering 10 to 57 languages. %One of the largest natural language understanding benchmark in terms of language coverage is SIB-200~\cite{AfroXLM-76L} covering 200 languages including 57 African languages. 
The closest benchmark to our task of slot-filling is the MasakhaNER~\citep{adelani-etal-2021-masakhaner,adelani-etal-2022-masakhaner} that covers 20 African languages but they focus on four entity types  ``personal names'', ``organization'', ``location'', and ``dates'', which are not fine-grained and well adapted to several domains such as banking and travel %and kitchen \& diniing 
that we cover in \dataset{}. 


%Recent years have seen substantial growth in African language evaluation benchmarks. ChatGPT-MT~\cite{Robinson2023} provided extensive machine translation evaluation across 57 African languages, while MEGA~\cite{Ahuja2023} assessed 11 African languages on 10 fundamental NLP tasks. This coverage was further expanded by Megaverse~\cite{Ahuja2024} to 16 languages across 16 tasks and SIB-200~\cite{Adelani2024} which evaluated topic classification for 57 languages.

%The most comprehensive evaluation framework, AfroBench~\cite{AfroBench} spans 60 African languages across 15 distinct tasks, including text classification, question answering, and knowledge evaluation. However, despite this progress in general NLP evaluation, task-oriented dialogue systems---particularly intent detection and slot-filling---remain notably underexplored for African languages. This gap underscores the importance of our work in providing the first large-scale, culturally-authentic benchmark for these crucial dialogue system components.

\paragraph{Intent and Slot-filling Benchmarks} Most of the existing benchmarks for intent detection and slot-filling tasks are English-only. There are a few efforts to make them multilingual in two ways: (1) human generating the utterances in a particular domain, followed by intent and slot filling annotation. (2) through human translation of annotated data from English to other languages which introduces some cultural bias since Western entities are being propagated. While the first approach is the most ideal methodology, it is very cost-intensive when scaling to many languages. Facebook dataset~\citep{schuster-etal-2019-cross-lingual} followed the first approach by creating labeled data in three domains (alarm, reminder and weather) for three languages: English, Spanish and Thai. However, most other approaches make use of the second approach, where English data are translated to other languages~\citep{xu-etal-2020-end, van-der-goot-etal-2021-masked,li-etal-2021-mtop}, however, they often do not include African languages. XTREME-UP benchmark expanded the MTOP dataset~\citep{li-etal-2021-mtop} to five African languages (Amharic, Hausa, Yoruba, Swahili and Zulu), while \massive~\citep{fitzgerald-etal-2023-massive} perform human translation to 50 languages including three African languages (Afrikaans, Amharic, and Swahili). \massive benchmark partially addresses this Western cultural bias by encouraging translators to replace entities with more culturally relevant ones, but Western entities are still prevalent in the dataset. \autoref{tab:past_works} summarizes all existing related works. In our paper, we introduce \dataset{} which is the largest intent detection and slot-filling dataset covering 16 African languages, and we ensured that the slot entities are more culturally relevant in the respective countries the languages are from.



%Intent detection and slot filling are two fundamental tasks in task-oriented spoken language understanding (SLU). Intent detection aims to identify the user's purpose in a given utterance, while slot filling extracts relevant semantic entities from the input text.

%The development of multilingual intent detection and slot-filling benchmarks has evolved significantly. Facebook~\citep{schuster-etal-2019-cross-lingual} pioneered cross-lingual transfer methods with a dataset of 57k utterances across three languages. MultiATIS++~\citep{xu-etal-2020-end} advanced this through end-to-end slot alignment across nine languages, while xSID~\cite{van-der-goot-etal-2021-masked} introduced joint learning with auxiliary tasks across 13 languages from 6 language families.

%Scale became the next frontier, with MTOP~\cite{li-etal-2021-mtop} providing 100k annotated utterances across 11 domains. Recent efforts began incorporating African languages: MTOP++~\citep{ruder-etal-2023-xtreme} included five African languages in its user-centric benchmark, while MASSIVE~\citep{fitzgerald-etal-2023-massive} included three African languages among its 51 languages. However, these relied on translation rather than native generation.

%Studies addressing low-resource scenarios have explored various approaches. Recent work has adopted few-shot learning methods~\cite{chen2022contrastnet, dopierre2021protaugment, krone2020learning} and zero-shot techniques~\cite{parikh2023exploring} with parameter-efficient fine-tuning. A comprehensive review of these datasets and approaches is provided in~\cite{larson2022survey}.

%Beyond intent detection and slot-filling, broader evaluation efforts like IrokoBench~\cite{adelani2024irokobench} have highlighted substantial performance gaps between high-resource and African languages. These findings underscore the importance of developing native, culturally-relevant benchmarks for African languages, which our work directly addresses. The development of multilingual intent detection and slot-filling benchmarks has evolved significantly, though African languages remained notably underrepresented. 


% Facebook~\citep{schuster-etal-2019-cross-lingual} pioneered cross-lingual transfer methods by introducing a dataset of 57k utterances across three languages, evaluating translation-based transfer and cross-lingual embeddings. MultiATIS++~\citep{xu-etal-2020-end} advanced the field with end-to-end slot alignment across nine languages and four language families, while xSID~\cite{van-der-goot-etal-2021-masked} introduced joint learning with auxiliary tasks across 13 languages from 6 language families.
% MTOP~\cite{li-etal-2021-mtop} marked a significant expansion with 100k annotated utterances across 11 domains, addressing previous datasets' limitations in size and compositional query handling. However, these early benchmarks completely excluded African languages, focusing primarily on high-resource languages.
% Recent efforts have begun incorporating African languages, though limitations persist. MTOP++~\citep{ruder-etal-2023-xtreme} made a notable advancement by including five African languages (Amharic, Hausa, Yoruba, Swahili, Zulu) in its user-centric benchmark designed for scarce-data scenarios. MASSIVE~\citep{fitzgerald-etal-2023-massive}, while impressive in scale with 1M examples across 51 languages, included only three African languages (Afrikaans, Amharic, Swahili). Their "localize-translate-preserve" strategy attempted to maintain cultural relevance, but still relied on translation rather than native generation.

% Intent detection and slot filling are two fundamental tasks in task-oriented spoken language understanding (SLU). Intent detection aims to identify the user's purpose in a given utterance, while slot filling extracts relevant semantic entities from the input text. 

% Studies addressing slot and intent detection for low-resource languages have been conducted in recent years; we discuss some relevant work in this section. Existing studies have adopted few-shot learning methods to recognize intents~\cite{chen2022contrastnet, dopierre2021protaugment, krone2020learning}. In addition, the authors in~\cite{parikh2023exploring} explore intent filtering in a zero-shot setting and tune instruction-tuned language models with parameter-efficient fine-tuning in a few-shot setting.
% Another important direction in this domain is the development of diverse datasets for intent detection and slot-filling tasks. The paper "A Survey of intent detection and Slot-Filling Datasets for Task-Oriented Dialog"~\cite{larson2022survey} provides a comprehensive review of publicly available datasets used for intent detection and slot-filling in task-oriented dialogue systems. Beyond intent detection and slot-filling, recent work has emphasized the need for benchmarks that evaluate broader language model capabilities for low-resource African languages. The IrokoBench benchmark~\cite{adelani2024irokobench} introduces a human-translated evaluation dataset covering 17 typologically diverse African languages across multiple tasks, including natural language inference, mathematical reasoning, and knowledge-based question answering. Their findings reveal substantial performance gaps between high-resource and low-resource languages, particularly in comparison to proprietary models, highlighting the necessity of more inclusive evaluation benchmarks for African languages.


%including datasets, benchmarks, and models that aim to address this challenge.



\section{Introducing \dataset{} Dataset}

\begin{table}[t]
  \centering
  \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lclr}
    \toprule[1pt]
    % \multicolumn{4}{c|}{\textbf{Language Information}} \\
    \textbf{Language} & \textbf{Code} & \textbf{Language Family} & \textbf{No. of Speakers} \\ \midrule
    Amharic & amh & Afro-Asiatic/Semitic & 60M \\
    Ewe & ewe & Niger-Congo/Kwa & 7M \\
    Hausa & hau & Afro-Asiatic/Chadic & 63M \\
    Igbo & ibo & Niger-Congo/Volta-Niger & 27M \\
    Kinyarwanda & kin & Niger-Congo/Bantu & 10M \\
    Lingala & lin & Niger-Congo/Bantu & 41M \\
    Luganda & lug & Niger-Congo/Bantu & 7M \\
    Oromo & orm & Afro-Asiatic/Cushitic & 46M \\
    Shona & sna & Niger-Congo/Bantu & 12M \\
    Sesotho & sot & Niger-Congo/Bantu & 7M \\
    Swahili & swa & Niger-Congo/Bantu & 98M \\
    Twi & twi & Niger-Congo/Kwa & 9M \\
    Wolof & wol & Niger-Congo/Senegambia & 5M \\
    Xhosa & xho & Niger-Congo/Bantu & 9M \\
    Yoruba & yor & Niger-Congo/Volta-Niger & 42M \\
    Zulu & zul & Niger-Congo/Bantu & 27M \\
    \bottomrule[1pt]
    \end{tabular}%
  }
  \vspace{-2mm}
  \caption{\textbf{Overview of languages in the \dataset{} dataset}, including ISO 639-3 language codes, language families, and approximate number of speakers.}
  \label{tab:lang-overview}
\end{table}

\begin{figure*}[!ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/demo.png}
    \vspace{-5mm}
    \caption{\textbf{Task description for \dataset{} dataset}. An example from one of the five domains. It shows the semantic-similar sentences along with intent and slot-filling labels.}
    
    % Examples from one of the five \dataset{} domains. Each row shows a semantic instance in English (CLINC), paired with its corresponding Xhosa and English examples in \dataset{}, along with intent and slot-filling labels.
    \label{fig:example}
\end{figure*}



\dataset{}\footnote{\dataset{} means intent in isiXhosa language.} is a joint intent detection and slot-filling dataset (\textbf{ID-SF}) for typologically diverse Sub-Saharan African languages and English. The selected languages represent diverse linguistic families and are widely spoken in Africa. These languages come from the two dominant language families in Africa: 13 from Niger-Congo and three from Afro-Asiatic. The languages covered are spoken by a large population in Africa, ranging from Swahili with 98M speakers to Wolof with 5M speakers, making the dataset particularly valuable for over 400 million African population. ~\autoref{tab:lang-overview} shows the languages covered, their language family, and the number of speakers of the languages. 



\subsection{Data source and collection}
Typical ID-SF data collection often requires large crowd-sourcing efforts to collect utterances, with additional labeling of intents and slots in various domains. Developing such a large crowd-sourcing effort is time-consuming and costly for several low-resource languages. To simplify the process while making the dataset cultural, we provide each annotator with sample sentences from the \clinc dataset~\citep{Larson2022ASO} with a specified \textit{intent type}, say ``transfer''. Then, the dataset construction follows two stages: (1) \textbf{Utterance elicitation} in an African language and (2) \textbf{Slot-filling annotation} of the generated utterance. 

~\autoref{fig:example} shows an example of an English utterance from the \clinc dataset in the banking domain: ``\textit{please send ten dollars from bank of america to capital one}''. The corresponding intent label is ``\textit{transfer}'', and the entities of slot filling are the amount of [money] (\textit{ten dollars}), the source [bank] (\textit{bank of america}), and the destination [bank] (\textit{capital one}). A Xhosa annotator was asked to generate another utterance belonging to the same intent type but capturing the South African context where the language is spoken. Thus, the annotator used the R200 as \textit{``money''} with currency Rand (R), and more familiar South African banks such as ``FNB'' and ``Absa'' for \textit{``bank name''} slot. We provide more information about the two stages of data construction below. 


%The data construction process starts with providing an annotator with sentences from the \clinc dataset~\citep{Larson2022ASO} with a specified \textit{intent type}, and they are to come up with culturally-relevant similar sentences with more relevant slot entities (see \autoref{fig:example}). The utterance generation process is followed by slots annotation. \dataset{} dataset covers 5 domains, 40 intents and 23 slots, and 3,200 instances per African language. 




%Figure~\ref{fig:example} shows an example, the following English utterance from the \clinc dataset in the banking domain: ``\textit{please send ten dollars from bank of america to capital one}''. The corresponding intent label is ``\textit{exchange money}'', and the entities of slot filling are the amount of [money] (\textit{ten dollars}), the source [bank] (\textit{bank of america}), and the destination [bank] (\textit{capital one}).

% The \dataset{} dataset extends this task to 16 African languages, providing a comprehensive benchmark for evaluating the performance of multilingual models across diverse linguistic contexts.

% Domain
% banking [from this domain]
% home
% kitchen & dining
% travel
% utility

% eng (CLINC) please send ten dollars from bank of america to capital one
% xho (Injongo) Ndicela uthumele i-R200 uyisuse kwiakhawunti yakwaFNB uyise kwiakhawunti yakwaAbsa. 
% eng (Injongo) Could you please transfer R200 from FNB account to Absa account. 
% Intent Detection: exchange rate Slot Filling: money, bank name


%\subsection{Languages}
% Intention, why need cover these languages
% - Low-resource languages
% - Multicultural
% - Diverse
% - African languages

% The \dataset{} dataset is designed to address the lack of large-scale benchmarks for low-resource languages, particularly African languages.
% The dataset covers 16 diverse African languages spanning multiple language families, with a particular focus on Niger-Congo languages which are widely spoken across sub-Saharan Africa. As shown in Table \ref{tab:lang-overview}, these languages represent different linguistic groups including Afro-Asiatic (e.g., Amharic, Hausa), Niger-Congo/Bantu (e.g., Swahili, Zulu), and other Niger-Congo branches like Kwa (Ewe, Twi) and Volta-Niger (Igbo, Yoruba). The selected languages cover a significant population, with Swahili being the most widely spoken (98M speakers) and Wolof having the smallest speaker base (5M speakers).



%, the dataset includes representatives from Afro-Asiatic (e.g., Amharic, Hausa), Niger-Congo/Bantu (e.g., Swahili, Zulu), and other Niger-Congo branches like Kwa (Ewe, Twi) and Volta-Niger (Igbo, Yoruba). 


%Together, these languages serve a substantial population, ranging from Swahili with 98M speakers to Wolof with 5M speakers, making the dataset particularly valuable for developing practical NLP applications for African communities.





\paragraph{Utterance generation}
The source data for our multilingual benchmark is from the \clinc English dataset---an intent detection with 150 intent classes across 10 domains (but without slot annotation)~\footnote{The domains are: banking, work, meta, auto \& commute, travel, home, utility, kitchen \& dining, small talk, and credit cards}, we extracted 40 intents from five most suitable domains to the African contexts: \textbf{Banking} (e.g. ``transfer'',  ``pay bill''),  \textbf{Home} (e.g. ``play music'', ``calendar update''),  \textbf{Kitchen and Dining} (e.g. ``recipe'', ``confirm reservation''), \textbf{Travel} (e.g. ``exchange rate'', ``book flight'' ), and \textbf{Utility} (e.g. ``alarm'', ``make call'' ). Next, we conducted the tutorial on the utterance generation task and a \textbf{practice session} and asked every annotator to generate a sample English utterance per intent that culturally aligns with the African contexts (e.g. food type or language name). Per language, we recruited three annotators, and they generated 120 utterances (40 per annotator and intent). We aggregated the practice data as the \textit{\dataset{} English dataset}. %during the practice section
Finally, for the \textbf{full data collection}, we asked the same three annotators to generate \textit{80 utterances per intent}, given a sample sentence from \clinc. Each annotator worked on different intents. In total, we collected 3,200 utterances with a balanced number of intent types. Appendix~\ref{sec:intent-label} contains all the 40 intent types selected. 



\begin{table}[t]
  \centering
  \resizebox{0.48\textwidth}{!}{%
    \begin{tabular}{l|ccccc}
    \toprule[1pt]
    Lang. & Total & Avg. & Un. Fleiss'$\kappa$ & Fleiss'$\kappa$ & $\Delta$ \\ \midrule
    \lang{amh} & 10555 & 3.30 & 0.850 & 0.935 & +0.085 \\
    \lang{ewe} & 11181 & 3.49 & 0.875 & 1.000 & +0.125 \\
    \lang{hau} & 11491 & 3.59 & 0.892 & 0.997 & +0.105 \\
    \lang{ibo} & 12246 & 3.82 & 0.812 & 0.973 & +0.161 \\
    \lang{kin} & 10112 & 3.16 & 0.740 & 0.963 & +0.224 \\
    \lang{lin} & 11025 & 3.44 & 0.823 & 0.990 & +0.168 \\
    \lang{lug} & 11769 & 3.67 & 0.888 & 0.990 & +0.102 \\
    \lang{orm} & 11958 & 3.74 & 0.849 & 0.992 & +0.143 \\
    \lang{sna} & 15222 & 4.76 & 0.935 & 0.976 & +0.041 \\
    \lang{sot} & 6468 & 2.02 & 0.694 & 0.997 & +0.303 \\
    \lang{swa} & 14217 & 4.44 & 0.878 & 0.986 & +0.107 \\
    \lang{twi} & 14325 & 4.48 & 0.916 & 0.986 & +0.070 \\
    \lang{wol} & 10942 & 3.42 & 0.728 & 0.942 & +0.213 \\
    \lang{xho} & 12475 & 3.90 & 0.825 & 0.938 & +0.113 \\
    \lang{yor} & 13620 & 4.26 & 0.862 & 0.988 & +0.126 \\
    \lang{zul} & 11911 & 3.73 & 0.640 & 0.913 & +0.273 \\ 
    \bottomrule[1pt]
  \end{tabular}%
  }
  \vspace{-2mm}
  \caption{\textbf{Statistics of slot entity annotations across languages}. For each language, we show the total number of annotated entities, average entities per sentence, and inter-annotator agreement measured by Fleiss' Kappa ($\kappa$) before (Un.) and after review. $\Delta$ shows the improvement in agreement after the review process.}
  \label{tab:dataset-fk-merged}
\end{table}

\paragraph{Slot-filling annotation}
Similar to the utterance generation phase, we first conducted a practice session in English to train annotators followed by the full data annotation. We manually analyzed each generated utterance to come up with the most relevant slot entities (about 26). However, after the practice session, annotators recommended the addition of new slots such as ``airline'', ``airport name'', ``car type'', and ``supermarket name'', which we adopted. After the practice session, we gave detailed feedback on the issues with the annotation, and annotators discussed with their language coordinator how to resolve issues. Finally, we asked them to annotate the slot entities for the 3,200 utterances. Each utterance was annotated by three annotators so that we could check for agreement in the slot annotations. The annotation followed the named entity recognition setup on LabelStudio platform \footnote{\url{https://labelstud.io/}}. Appendix ~\ref{sec:slot-label} contains all the 34 intent types selected. 

For both utterance elicitation and slot-filling annotation, all recruited participants received an appropriate remuneration based on the per-country rate decided by our logistic company in Kenya.\footnote{Utterance elicitation rate ranges from \$1,555 to \$2,838 per language depending on country rate, and slot-filling annotation ranges from \$388 to \$709}



%The dataset was \textit{converted} into the 16 African languages with professional translators and native speakers to ensure high-quality translations and involve culture-based words inverting, such as ``\textit{Bank of America}'' in English and ``\textit{First National Bank}'' (\textit{FNB}, one of largest banks in Africa) in Xhosa (Figure~\ref{fig:example}). The current dataset with original intent labels from \clinc was further annotated for the slot filling task.

% With multiple rounds of review and processing, we ensured high-quality annotations for all languages.
% The intent classed and slot type can be found in the Appendix \ref{sec:label-lists}.


% \subsection{Slot Filling Annotating}
% Following the \cite{}, SF can be considered the Named Entity Recognition (NER) task. The following processes are similar to annotating the traditional NER task.
% Some NER annotating background and content
% High Level: 
% Firstly, we trained the annotators with  
% Annotation Process
% - Translation
% - Slot Filling
% - Quality Control
% - Label Merging
% - Data Analysis
% - Data Processing
% - Data Split
% - Data Statistics
% - Data Visualization


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/entity-distribution.png}
    \vspace{-3mm}
    \caption{\textbf{The distribution of slot entities appearances of all 16 African languages} with Unreviewed and Reviewed versions. The slot entities are sorted from left to right by frequency in descending order.}
    % 
    \label{fig:entity-dist}
\end{figure*}

\subsection{Quality Control for Slot-filling}
\label{sec:quality-control}
To ensure annotation quality and consistency, we follow a rigorous quality control process using a majority voting system with a minimum of three annotators per sentence to resolve disagreements. The annotation quality was evaluated using Fleiss' Kappa score \citep{Fleiss}, with scores presented in \autoref{tab:dataset-fk-merged} comparing agreement levels before and after the review process. 
Initial Fleiss' Kappa scores revealed substantial variation across languages, ranging from 0.618 (Zulu) to 0.934 (Shona), indicating significant inter-annotator disagreement. Following the review process, agreement scores improved markedly across all languages, reaching 0.912-1.00. Notable improvements were observed in Sesotho (+0.327) and Zulu (+0.294), with other languages showing average improvements of approximately 0.1 in their Fless' Kappa scores.


\subsection{Slot-filling label merging}
\label{sec:label-merge}
On completion of the final annotation, we found that some slot entities are rarely used. We performed an analysis of entity frequency distribution across all languages. \autoref{fig:entity-dist} shows the result of our analysis, we decided to exclude slot entities appearing less than 500 times across all languages (after \textit{MUSIC GENRE} in the figure).  Consequently, nine infrequent slots from \textit{NATIONALITY} through \textit{PLUG TYPE} were eliminated. Examination of annotator feedback and comparative analysis between unreviewed and reviewed versions indicated that ambiguous slot types significantly impacted annotation quality and introduced unnecessary complexity. To enhance annotation clarity and maintain consistency, the following merging strategy was implemented:
\begin{itemize}{\leftmargin=0.5mm}\setlength{\itemsep}{0em}\setlength{\parskip}{0em}
    \item \textit{Geographic entities}: \textit{STATE OR PROVINCE} and \textit{CITY NAME} were consolidated into a unified \textit{CITY OR PROVINCE} category to ensure consistent handling of geographic references.
    \item \textit{Food-Related Labels}: \textit{DISH NAME} and \textit{FOOD ITEM} were unified under \textit{DISH OR FOOD} to eliminate classification ambiguity.
\end{itemize}
This merging process resulted in a reduction from 34 to 23 slot types. The complete enumeration of original and consolidated labels, along with unmerged entity Fleiss' kappa scores, is provided in Appendix \ref{app:stat}.

\begin{table}[t]
  \centering
  \footnotesize
  \resizebox{\columnwidth}{!}{%
  %\scalebox{0.85}{
    \begin{tabular}{lcc|c}
    \toprule[1pt]
     & \multicolumn{2}{c}{\textbf{\dataset{}}} & \multicolumn{1}{c}{\textbf{\clinc}}  \\    
    \textbf{split} & \textbf{African} & \textbf{English} & \textbf{English} \\
    \midrule
    TRAIN & 2,240 (56 per intent)  & 1,047 & 4,000 (100 per intent) \\
    DEV & 320 (8 per intent) & 110 & 800 (20 per intent) \\
    TEST & 640 (16 per intent) & 622 & 1,200 (30 per intent) \\
    \bottomrule
    \end{tabular}%
  }
  \vspace{-2mm}
  \caption{\textbf{\dataset{} dataset split}. The African data have an equal number of samples per intent while the English samples per intent vary. }
  \label{tab:data_split}
\end{table}

\subsection{Data split}
%Through these rigorous steps of data collection and quality ensuring, the \dataset{} dataset encompasses total 17 languages (16 African languages plus English), comprising 40 intent classes and 23 slot-filling categories. 
Our final annotation resulted in 3,200 annotated utterances, with 80 utterances per intent for each of the 16 African languages. The dataset is partitioned following ratios of 70\%, 10\%, and 20\% for train, dev, and test splits respectively, stratified by intent for each language. Additionally, we aggregated the practice utterances generated and the practice slot annotations as the English dataset, leading to 17 annotated languages. In total, the English consist of 1779 utterances.~\footnote{Ideally, if each language completes 120 utterance generation, we ought to have 1920 utterances, however, some languages only did 80 in the practice, leading to a slightly lower English portion.} Finally, we sampled 4000 \clinc intent-only dataset to compare western-centric English dataset to our curated \dataset{} dataset that captures the African contexts. \autoref{tab:data_split} provides the comprehensive dataset statistics of the African languages and English splits. 




% \subsection{Dataset Overview}
% Languages except English
% Total: 40 intents * 80 samples (100%) / language
% Test split: 40 intent * 16 samples (20%)
% Dev split: 40 intent * 8 samples (10%)
% Train split: 40 intent * 56 samples (70%)

% English
% 40 intents * 3 people * 16 languages ~= 1779 (1920)
% Test split: 40 intent * 16 languages
% Dev split: 40 intent * ~ int(10% rand/intent) language
% Train split: remaining


% \subsection{Example Annotation}
% TODO: move to appendix
% \begin{lstlisting}[breaklines=true, frame=single]
% English: what do i have in my bank accounts right now
% Swahili: Je, nina shilingi ngapi katika akaunti yangu ya Akiba ya NBC benki?
% Intent: balance
% Slot entity:
%   CURRENCY: shilingi $$ ACCOUNT_TYPE: akaunti yangu ya Akiba $$ BANK_NAME: NBC
% \end{lstlisting}
% - 9:17:SL:CURRENCY, 31:53:SL:ACCOUNT_TYPE, 57:60:SL:BANK_NAME
% - [IN:balance [SL:CURRENCY shilingi] [SL:ACCOUNT_TYPE akaunti yangu ya Akiba] [SL:BANK_NAME NBC] ]

% \caption{Example annotation for slot filling task in Swahili.}
% \label{lst:annotation}
% Procedure
% Translate from English to low-resource languages
% Get Intent and Slot Entity
% Example:
% eng: what do i have in my bank accounts right now
% swa: Je, nina shilingi ngapi katika akaunti yangu ya Akiba ya NBC benki?
% intent: balance
% slot entity (3 format): 
% 9:17:SL:CURRENCY,31:53:SL:ACCOUNT_TYPE,57:60:SL:BANK_NAME
% [IN:balance [SL:CURRENCY shilingi] [SL:ACCOUNT_TYPE akaunti yangu ya Akiba] [SL:BANK_NAME NBC] ]
% CURRENCY: shilingi $$ ACCOUNT_TYPE: akaunti yangu ya Akiba $$ BANK_NAME: NBC

% Intent: directly collect from original dataset
% Slot entity:
% Step 0: setup label platform,  [Human Signal]
% Step 1: train annotator with ~40 (one per each intent) eng example


% Step 2: label the low-resource languages text
% Step 3: Find at least 3 annotators for each languages (16 languages) AND indecently annotator 
% Keep a snapshoot version, called not_reviewed.jsonl
% Step 4: Based on Agreement Score in HumanSignal, reviewers/annotators correct agreement score < ~80
% Keep another snapshoot version, called reviewed.jsonl

% Step 5: More Clean Up and Data Analysis
% Data Processing
% Primary Insight
% Merge Low Freq Entities and Combine Intent and Slot Entities
% Secondary Insight and Comparison
% English Dataset
% General Analysis

% \import{section}{exp_and_result}
\import{section}{exp}
\import{section}{result}
\import{section}{ending}


%\newpage
% \bibliographystyle{acl_natbib}
\bibliography{custom,peter_custom}

\newpage
\appendix
\import{section}{appendix}
\end{document}