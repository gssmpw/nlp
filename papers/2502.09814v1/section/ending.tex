\section{Conclusion}

We present \dataset{}, a new benchmark dataset for evaluating intent detection and slot-filling for 16 African languages. % and built by annotati native speakers, 
\dataset{} represents the first large-scale multicultural dataset focused on African language Conversation AI. Our experiments reveal that while fine-tuned multilingual models such as AfroXLMR-76L achieved strong performance LLMs still struggle with African languages, particularly in slot filling tasks. %where performance gaps reach up to 52.6 F1 points compared to English. 
We hope \dataset{} will accelerate the development of more effective and culturally-aware conversational AI systems for African languages.

\section*{Limitations}
The scope of \dataset{} is constrained by its coverage of only 5 domains and 40 intents, missing some other domains such as healthcare and education that are essential for real-world applications. Our language selection, while substantial, still represents only a fraction of Africa's linguistic diversity, particularly lacking representation from other language families such as Nilo-Saharan languages.
%Northern Africa and smaller language communities. 
The annotation process revealed inherent challenges in entity classification across languages, requiring two rounds of review to achieve consistent quality. %The dataset exhibits a notable urban bias, with many examples centered around banking, travel, and restaurant scenarios that may not resonate with rural communities. 
Although significant for low-resource languages, the dataset size of 3,200 examples per language remains modest compared to high-resource benchmarks, potentially limiting model performance. Additionally, the fixed distribution of examples across intents may not accurately reflect the natural frequency of these interactions in real-world conversations.


%\section*{Ethics Statement}

