
\begin{table*}[!htpb]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|cccccccccccccccccc}
\toprule
\textbf{Task} & \textbf{Model} & \lang{eng} & \lang{amh} & \lang{ewe} & \lang{hau} & \lang{ibo} & \lang{kin} & \lang{lin} & \lang{lug} & \lang{orm} & \lang{sna} & \lang{sot} & \lang{swa} & \lang{twi} & \lang{wol} & \lang{xho} & \lang{yor} & \lang{zul}& \textbf{AVG} \\ \midrule
\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}} \textsc{Intent} \\ \textsc{Detection}\end{tabular}} 
& Llama 3.1 8B  & 27.6 & 1.9 & 2.1 & 4.8 & 5.5 & 3.3 & 5.3 & 2.4 & 1.6 & 2.8 & 2.9 & 14.1 & 2.6 & 4.0 & 3.2 & 3.5 & 2.8 & 3.9$_{\pm2.4}$ \\
& Gemma 2 9B & 77.6 & 49.2 & 6.1 & 40.8 & 31.5 & 23.8 & 22.2 & 23.2 & 7.7 & 29.7 & 19.9 & 70.0 & 21.0 & 13.8 & 40.1 & 32.2 & 36.3 & 29.2$_{\pm8.7}$ \\ %\cmidrule{2-20}
& Aya-101 13B & 65.3 & 62.9 & 13.4 & 57.8 & 56.9 & 40.4 & 27.8 & 33.9 & 20.8 & 51.2 & 43.9 & 65.9 & 27.2 & 19.7 & 58.1 & 45.9 & 53.2 & 42.4$_{\pm9.1}$ \\
& Gemma 2 27B & 79.5 & 47.2 & 6.3 & 46.5 & 36.9 & 26.7 & 27.5 & 26.1 & 5.8 & 36.7 & 25.6 & 75.5 & 21.2 & 16.4 & 50.2 & 34.8 & 44.3 & 33.0$_{\pm9.6}$ \\
\rowcolor{Gray}
\cellcolor{white}
& Llama 3.3 70B  & 81.1 & 56.2 & 9.5 & 52.3 & 52.4 & 35.0 & 37.5 & 37.7 & 12.4 & 32.3 & 30.5 & 80.6 & 29.3 & 20.9 & 43.5 & 41.4 & 43.9 & 38.5$_{\pm9.5}$ \\
\cmidrule{2-20}
& Gemini 1.5 Pro & 81.8 & \textbf{77.9} & \textbf{24.3} & 74.8 & 65.4 & 61.5 & 54.6 & 59.3 & 39.3 & 68.6 & 51.6 & 83.2 & 47.2 & 25.6 & 76.2 & 66.8 & 68.7 & 59.1$_{\pm9.6}$ \\
\rowcolor{LightCyan}
\cellcolor{white}
& GPT-4o (Aug) & \textbf{80.9} & 76.0 & 15.1 & \textbf{80.7} & \textbf{71.8} & \textbf{64.7} & \textbf{56.4} & \textbf{68.2} & \textbf{59.3} & \textbf{75.5} & \textbf{59.7} & \textbf{84.5} & \textbf{58.6} & \textbf{43.7} & \textbf{79.6} & \textbf{77.0} & \textbf{71.2} & \textbf{65.1$_{\pm9.3}$} \\
\midrule
\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}} \textsc{Slot} \\ \textsc{Filling}\end{tabular}} 
& Llama 3.1 8B & 25.0 & 3.7 & 5.6 & 11.1 & 12.6 & 8.5 & 9.1 & 10.1 & 2.8 & 9.9 & 11.5 & 17.3 & 11.2 & 9.2 & 2.6 & 11.0 & 9.0 & 9.1$_{\pm2.2}$ \\
& Gemma 2 IT 9B & 34.1 & 4.5 & 0.3 & 7.4 & 10.6 & 5.0 & 6.0 & 5.6 & 0.1 & 7.3 & 10.8 & 21.2 & 2.4 & 2.6 & 2.2 & 5.2 & 8.2 & 6.2$_{\pm2.9}$ \\ %\cmidrule{2-20}
& Aya-101 13B & 21.4 & 8.2 & 7.9 & 11.8 & 14.6 & 12.2 & 9.4 & 15.5 & 3.6 & 15.0 & 17.0 & 16.2 & 13.8 & 14.0 & 2.8 & 9.6 & 10.6 & 11.4$_{\pm2.4}$ \\
& Gemma 2 IT 27B & 49.8 & 15.7 & 9.5 & 24.1 & 25.2 & 21.7 & 15.2 & 28.4 & 2.6 & 29.8 & 28.0 & 40.2 & 24.3 & 23.3 & 4.5 & 28.1 & 31.0 & 22.0$_{\pm5.8}$ \\
\rowcolor{Gray}
\cellcolor{white}
& Llama 3.3 70B Instruct & 52.6 & \textbf{26.3} & \textbf{22.0} & 29.5 & 35.0 & 31.4 & 25.0 & 30.4 & 9.3 & 29.5 & 36.4 & 40.7 & 35.6 & 36.4 & 6.9 & 34.2 & 31.9 & 28.8$_{\pm5.2}$ \\
\cmidrule{2-20}
& Gemini 1.5 Pro & 52.8 & 15.2 & 18.7 & 31.9 & 35.8 & 34.4 & \textbf{34.9} & 34.4 & 12.2 & 36.8 & \textbf{43.0} & 37.5 & 34.5 & 34.2 & 6.9 & 33.2 & \textbf{38.6} & 30.1$_{\pm6.1}$ \\
\rowcolor{LightCyan}
\cellcolor{white}
& GPT-4o (Aug) & \textbf{55.4} & 22.8 & 19.4 & \textbf{37.8} & \textbf{38.9} & \textbf{36.4} & 33.5 & \textbf{35.3} & \textbf{13.0} & \textbf{40.2} & 40.9 & \textbf{46.5} & \textbf{40.1} & \textbf{37.9} & \textbf{10.0} & \textbf{42.4} & 37.6 & \textbf{33.3$_{\pm6.0}$} \\

%& Gemma 2 IT 9B (\textit{SFT}) & 81.9 & 80.4 & 82.1 & 92.3 & 82.5 & 76.4 & 87.2 & 87.4 & 79.2 & 81.9 & 79.1 & 87.5 & 86.3 & 82.3 & 88.7 & 89.5 & 68.3 & \underline{83.2}$_{\pm5.8}$ \\
\bottomrule
\end{tabular}%
}
\vspace{-2mm}
\caption{\textbf{Zero-Shot performance of LLMs on Intent Detection (ID) and Slot Filling (SF)}. Evaluation is based on accuracy and F1-score for ID and SF tasks. Average computed on five templates, and on only African languages.}%, with the fully SFT Gemma 2 9B IT.}
\label{tab:prompt-results}
\end{table*}


\section{Results}
% We mainly evaluate model performance on \dataset{} with two categories of models: supervised fine-tuned SLMs and LLMs with zero/few-shot settings. We then further extend the scope to cross-lingual transfer learning and multicultural analysis of our datasets.


\subsection{Fine-tuned multilingual encoders}
\autoref{tab:sft-result} summarizes the results of the multilingual encoders fine-tuned \dataset{} dataset. Overall, AfroXLMR-76L achieves the best performance on both \textbf{ID-SF} tasks, with an average accuracy of 93.7\% and an F1 score of 85.6\%, respectively. We attribute the success of this model to the coverage of all languages in \dataset{} during its pre-training (see Appendix \autoref{tab:model_languages}). AfroXLMR, the earlier version of AfroXLMR-76L, follows closely with an average accuracy of 92.2\% and an F1 score of 85.2\%. However, this model was not pre-trained on some of the languages such as \texttt{ewe}, \texttt{twi}, \texttt{lin}, and \texttt{wol} leading to a significant drop in performance of $-5.8$, $-4.8$, $-1.3$, $-0.4$ for intent detection when compared to AfroXLMR-76L. This shows that multilingual encoders for African languages can significantly improve the performance over massively multilingual encoders covering more than 100 languages such as XLM-R and NLLB LLM2Vec. While NLLB LLM2Vec covers all languages in our dataset and is very effective for intent detection, it leads to $-3.5$ on slot-filling when compared to the performance of AfroXLMR-76L. In general, T5-based models such as mT5 and AfriTeVa V2 performed worse on both tasks compared to the BERT-based models, however, we still observe better performance of the African-centric T5-model, AfriTeVa V2 which gave decent results comparable to other models except AfroXLMR (-76L) models. 

Finally, we find that \textit{multilingual training} of AfroXLMR-76L over all languages gave better overall performance than \textit{in-language training} leading to $+0.7$ and $+1.7$ boost in performance on intent detection and slot-filling tasks respectively. This highlights the additional benefit of joint training of several languages, resulting in a \textit{single checkpoint} and better overall performance because they benefited from cross-lingual transfer learning among the languages. The languages that benefited the most are Oromo (\texttt{orm}) and English (\texttt{eng}) with $+4.6$ and $+4.5$ improvement respectively for intent detection. The large boost for English is because the training data is twice smaller than the remaining African languages ($1,047$ vs. $2,240$). Similarly, for slot-filling, the benefit of multilingual training is more obvious since all languages consistently improved in performance. We see that joint training benefited both high-resourced and low-resourced languages. 

%For the current low-resource languages, the specialized trained model and collected dataset are essential for improving the performance of tasks.

%For the generalized trained model, NLLB LLM2Vec also performs well with an average accuracy of 93.4\% for intent detection and an F1 score of 85.2\% for slot filling. On the opposite, the encoder-decoder models, such as AfriTeVa V2 Large and mT5-Large, rank lower than the encoder-only models. A closer examination reveals that performance variations are larger for slot filling ($\pm$5-7 F1) compared to intent detection ($\pm$4-5 F1), suggesting that token-level tasks remain more challenging across different linguistic contexts.

%Analysis of language-specific results shows particularly strong performance on Niger-Congo languages like \lang{hau} (98.7 F1 for intent, 94.3 F1 for slots) and \lang{swa} (98.1 F1, 90.2 F1), while some Nilotic and Cushitic languages see relatively lower scores. This pattern suggests that linguistic similarity to pretraining data influences fine-tuning effectiveness.




% suggesting that cross-lingual strategies can help share knowledge across related languages and benefit morphologically diverse tasks.

% Whereas encoder-decoder models rank slightly lower (e.g., AfriTeVa V2 Large at 91.7\%). These results highlight the effectiveness of large encoder-only architectures for classification tasks in African languages.

% For slot filling, AfroXLMR-large-76L again achieves the best results (85.6\% F1), with AfroXLMR-large closely behind (85.2\%). The performance gap between encoder-only and encoder-decoder approaches is more pronounced here, suggesting that token-level tasks may benefit from robust encoder modeling. Performance indicators also reveal higher variance across languages for slot filling (e.g., $\pm 5.8$) than for intent detection ($\pm 3.8$), demonstrating the relative complexity of structured prediction.

% \paragraph{Language-Specific Observations}
% The combined multilingual training yields 1â€“2\% overall improvement relative to monolingual models, particularly in Bantu and Nilotic languages.
%We observe that typologically similar languages (e.g., \lang{lug} and \lang{kin}) benefit from shared morphological features, while more distant languages also gain from cross-language subword overlaps. Moreover, data consolidation seems especially helpful for languages with limited single-language training samples, underscoring the utility of cross-lingual representation sharing.



\subsection{LLMs prompting results}
\autoref{tab:prompt-results} shows the zero-shot LLM evaluation of five open models and two closed models. Our key findings are below: 

\paragraph{Slot-filling task is difficult for all LLMs including on English} The highest average performance achieved by the LLMs is $33.3$ for GPT-4o, although much better than the open model at $28.8$. We attribute this to the difficulty of LLMs on the named entity recognition task as reported by other researchers~\cite{Yu2023OpenCO, Ojo2023HowGA}. In comparison to the best-finetuned model, there is a large drop in performance of $-53.2$. This shows that having training data is still relevant for this task even in the LLM era, especially for low-resource languages. 

\paragraph{Large gap in the performance of closed and open models} For intent detection, we find that all open models achieved below 50\% on the relatively easy task of intent detection. The poor performance may be attributed to either a lack of exposure to many African languages or the large label space (i.e. 40) for the classification task. The closed models result are better, with \gpto and \gemini achieving more than $+20$ points than the best open model, Llama 3.3 70B. However, if we compare the results in the English language, open models such as Gemma 2 27B and Llama 3.3 70B are competitive with closed models. This shows that open models are more biased toward high-resource languages. This implies that there is a continuous need to keep improving the capabilities of models for low-resource languages. 

\paragraph{Intent detection performance varies by languages} The performance of some African languages is often higher than others, this is probably connected to the amount of monolingual data available on the web. For example, Swahili (\texttt{swa}) with over 1 billion monolingual data~\citep{kudugunta2023madlad} has $80.6$ accuracy point that is comparable performance to English performance ($81.1$) with Llama 3.3 70B, while other languages have much lower performance. Similarly, \gpto has more than $70$ accuracy points for Amharic, Hausa, Igbo, Shona, Swahili, Xhosa, \yoruba, and Zulu. These languages also have larger monolingual data on the web than the ones with lower than $70\%$ accuracy. 


%As shown in Table~\ref{tab:prompt-results}, zero-shot performance of large language models exhibits high variance across languages. For intent detection, GPT-4o leads with 65.1\% accuracy ($\pm$16.7), followed by Gemini 1.5 Pro at 59.1\% accuracy ($\pm$17.4). However, performance drops significantly for slot filling, with best scores of only 33.3 F1 (GPT-4o) and 30.1 F1 (Gemini 1.5 Pro). This substantial gap between tasks suggests that structured prediction remains challenging for LLMs without task-specific training.

%The high standard deviations ($\pm$16-17 F1) in zero-shot performance reveal dramatic cross-lingual variations. For instance, GPT-4o achieves 80.9\% accuracy on \lang{amh} intent detection but only 15.1\% accuracy on \lang{hau}. Similar patterns emerge across models, with consistently strong performance on widely-used languages (\lang{amh}: 79.5-81.8 F1, \lang{twi}: 75.5-84.5 F1) but poor results on lower-resource languages (\lang{sna}: 5.8-59.3 F1). This disparity highlights the critical role of pretraining data diversity.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/crosslingual_merged.png}
    \vspace{-8mm}
    \caption{Performance of cross-lingual transfer across different shot settings and supervised fine-tuning (SFT) on the merged 17 languages \dataset{} dataset.}
    \label{fig:shot-sft-comparison}
    % \vspace{-1em}
\end{figure*}


\begin{comment}

\begin{table}[!htbp]
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|l|ccccc}
\toprule
\textbf{Task} & \textbf{Model} & \textbf{zero-shot} & \textbf{5-examples} & \textbf{one-shot} & \textbf{4-shots} & \textbf{SFT} \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Intent\\Detection\end{tabular}} 
& GPT-4o (2024-08-06) & 64.7$_{\pm 16.8}$ & 71.6$_{\pm 14.2}$ & 78.0$_{\pm 11.4}$ & 83.2$_{\pm 6.4}$ & - \\
& Gemini 1.5 Pro 002 & 59.1$_{\pm 17.1}$ & 73.4$_{\pm 10.1}$ & 80.3$_{\pm 5.9}$ & - & - \\
& Llama 3.3 70B & 39.4$_{\pm 16.8}$ & 52.4$_{\pm 17.7}$ & 68.5$_{\pm 12.2}$ & 76.8$_{\pm 8.1}$ & - \\
& Gemma 2 IT 27B & 34.9$_{\pm 17.5}$ & 56.7$_{\pm 18.5}$ & 66.2$_{\pm 14.3}$ & 75.3$_{\pm 7.9}$ & - \\
& Gemma 2 IT 9B & 31.3$_{\pm 16.2}$ & 47.0$_{\pm 17.0}$ & 50.9$_{\pm 16.9}$ & 68.3$_{\pm 9.7}$ & 85.7$_{\pm 3.7}$ \\
& AfroXLMR-large 76L & 51.6$_{\pm 16.5}$ & - & 53.4$_{\pm 16.1}$ & - & \textbf{94.4}$_{\pm 3.6}$ \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}l@{}}Slot\\Filling\end{tabular}}
& GPT-4o (2024-08-06) & 33.7$_{\pm 10.7}$ & 53.3$_{\pm 8.8}$ & 59.6$_{\pm 5.9}$ & 69.2$_{\pm 6.3}$ & - \\
& Gemini 1.5 Pro 002 & 30.1$_{\pm 10.3}$ & 55.4$_{\pm 8.5}$ & 60.7$_{\pm 7.0}$ & - & - \\
& Llama 3.3 70B & 28.8$_{\pm 8.7}$ & 33.3$_{\pm 13.5}$ & 42.0$_{\pm 10.4}$ & 40.3$_{\pm 12.1}$ & - \\
& Gemma 2 IT 27B & 31.6$_{\pm 11.6}$ & 43.3$_{\pm 11.3}$ & 48.7$_{\pm 11.6}$ & 56.3$_{\pm 9.7}$ & - \\
& Gemma 2 IT 9B & 2.4$_{\pm 3.3}$ & 35.5$_{\pm 10.4}$ & 41.6$_{\pm 9.3}$ & 49.0$_{\pm 8.9}$ & 83.2$_{\pm 5.8}$ \\
& AfroXLMR-large 76L & 40.2$_{\pm 13.6}$ & - & 40.3$_{\pm 12.9}$ & - & \textbf{87.3}$_{\pm 4.4}$ \\
\bottomrule
\end{tabular}
}
\caption{Performance comparison across different shot settings and supervised fine-tuning (SFT) on the merged 17 languages \dataset{} dataset.}
\label{tab:shot-sft-comparison}
\end{table}
\end{comment}


% \begin{table*}[htbp]
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{lc|c|cccccccccccccccc|c}
% \toprule
% \textbf{Train Split} & \textbf{Lang.} & \textbf{Size} & \lang{amh} & \lang{ewe} & \lang{hau} & \lang{ibo} & \lang{kin} & \lang{lin} & \lang{lug} & \lang{orm} & \lang{sna} & \lang{sot} & \lang{swa} & \lang{twi} & \lang{wol} & \lang{xho} & \lang{yor} & \lang{zul} & \textbf{AVG} \\
% \midrule
% eng & orignal & 1,047 & 77.4 & 25.8 & 73.7 & 55.5 & 51.4 & 56.9 & 43.2 & 31.0 & 54.9 & 46.5 & 81.3 & 40.9 & 28.9 & 60.8 & 41.1 & 56.1 & 51.6$_{\pm16.5}$ \\
% clinc & orignal & 1,080 & 81.5 & 26.9 & 73.8 & 57.0 & 51.5 & 60.0 & 41.6 & 32.4 & 56.4 & 46.5 & 80.6 & 41.4 & 30.4 & 63.1 & 39.8 & 57.6 & 52.5$_{\pm16.6}$ \\
% eng+clinc & orignal & 2,127 & 83.9 & 32.5 & 77.9 & 63.3 & 55.2 & 64.5 & 46.4 & 35.1 & 59.0 & 50.5 & 84.8 & 45.9 & 33.8 & 65.5 & 46.2 & 61.0 & 56.6$_{\pm16.4}$ \\
% clinc+extend & orignal & 2,160 & 83.9 & 36.0 & 78.7 & 60.8 & 55.6 & 63.8 & 46.3 & 37.8 & 60.2 & 50.0 & 83.4 & 47.6 & 35.3 & 65.8 & 48.5 & 61.7 & 57.2$_{\pm15.2}$ \\
% \midrule
% eng & eng & 1,047 & 80.3 & 58.8 & 85.7 & 78.1 & 66.2 & 74.5 & 73.1 & 32.0 & 78.0 & 68.5 & 84.7 & 68.5 & 55.1 & 83.1 & 78.0 & 69.7 & 70.9$_{\pm13.4}$ \\
% clinc & eng & 1,080 & 83.4 & 60.5 & 88.2 & 79.6 & 67.8 & 76.9 & 71.3 & 32.9 & 80.0 & 69.9 & 84.7 & 67.7 & 57.9 & 84.2 & 76.6 & 71.8 & 72.1$_{\pm13.3}$ \\
% eng+clinc & eng & 2,127 & 85.6 & 62.7 & 90.4 & 83.2 & 69.7 & 80.3 & 76.8 & 33.2 & 83.4 & 72.3 & 88.1 & 72.4 & 58.2 & 88.1 & 81.6 & 75.6 & \textbf{75.1}$_{\pm14.1}$ \\
% clinc+extend & eng & 2,160 & 85.7 & 61.7 & 90.7 & 81.9 & 69.1 & 79.7 & 73.8 & 33.3 & 83.0 & 71.8 & 86.6 & 70.4 & 58.2 & 87.3 & 79.4 & 74.6 & 74.2$_{\pm13.8}$ \\

% \midrule
% \midrule
% eng & orignal & 1,047 & 77.4 & 25.8 & 73.7 & 55.5 & 51.4 & 56.9 & 43.2 & 31.0 & 54.9 & 46.5 & 81.3 & 40.9 & 28.9 & 60.8 & 41.1 & 56.1 & 51.6$_{\pm16.5}$ \\
% clinc & orignal & 1,040 & 65.7 & 21.9 & 58.4 & 44.5 & 40.9 & 48.8 & 31.5 & 26.2 & 44.4 & 37.7 & 63.7 & 32.7 & 25.3 & 50.6 & 30.0 & 46.3 & 41.8$_{\pm24.7}$ \\
% eng+clinc & orignal & 2,087 & 84.6 & 36.7 & 79.5 & 65.8 & 56.8 & 65.8 & 49.2 & 37.6 & 63.3 & 50.3 & 87.0 & 48.5 & 34.8 & 67.4 & 51.4 & 63.3 & 58.9$_{\pm15.8}$ \\
% clinc+extend & orignal & 2,080 & 83.6 & 36.0 & 78.1 & 60.6 & 55.1 & 63.1 & 45.7 & 38.7 & 61.1 & 50.5 & 83.7 & 46.8 & 33.7 & 65.9 & 48.3 & 61.3 & 57.0$_{\pm15.3}$ \\
% \midrule
% eng & eng & 1,047 & 80.3 & 58.8 & 85.7 & 78.1 & 66.2 & 74.5 & 73.1 & 32.0 & 78.0 & 68.5 & 84.7 & 68.5 & 55.1 & 83.1 & 78.0 & 69.7 & 70.9$_{\pm13.4}$ \\
% clinc & eng & 1,040 & 44.4 & 31.6 & 45.5 & 41.5 & 34.8 & 40.4 & 37.3 & 17.4 & 41.1 & 36.7 & 43.4 & 34.6 & 31.0 & 42.7 & 40.0 & 37.4 & 37.5$_{\pm35.8}$ \\
% eng+clinc & eng & 2,087 & 86.6 & 62.5 & 91.4 & 83.9 & 70.7 & 81.5 & 77.8 & 33.8 & 84.5 & 72.8 & 89.8 & 73.1 & 58.4 & 88.6 & 82.2 & 76.4 & \textbf{75.9}$_{\pm14.2}$ \\
% clinc+extend & eng & 2,080 & 85.3 & 61.9 & 90.6 & 82.1 & 69.2 & 79.4 & 73.8 & 33.5 & 83.0 & 71.9 & 86.8 & 70.7 & 57.9 & 87.3 & 79.8 & 74.9 & 74.3$_{\pm13.8}$ \\
% \bottomrule
% \end{tabular}
% }
% \caption{Cross-lingual transfer learning in intent detection with AfroXLMR-large 76L across different sources of English training datasets. The first four rows show zero-shot transfer from English training data, while the bottom four rows show results when evaluating translated test sets in English with the NLLB-200 model.}
% \label{tab:multicultural}
% \end{table*}

\subsection{Few-shot performance}
\autoref{fig:shot-sft-comparison} shows the result of the various few-shot setups: 5-examples, 1-shot (40 examples, one from each intent type), and 4-shots (160 examples). Our result shows a big boost in performance with only 5-examples, especially for the \textbf{slot-filling task} and some LLMs: \gpto and \gemini improved the most by more than $+19$ points. Similarly, Gemma 2 9B improved from $2.4$ to $33.5$ matching the performance of Llama 3.3 70B (with 5-examples). Additional samples from 1-shot and 4-shot consistently improved performance for all models except Llama 3.3 70B. Similarly, for \textbf{intent detection}, there is consistent improvement in performance with more examples used for few-shot evaluations. We find Gemini 1.5 Pro, Gemma 2 9B and Gemma 2 27B to benefit the most from 5-examples, with an accuracy boost of $+14.3$, $+15.7$, and $+21.8$ respectively. Interestingly,  while the zero-shot performance of Gemini 1.5 Pro is worse than \gpto, the few-shot performance exceeds that of \gpto with $+1.8$ and $+2.3$ improvement in 5-examples and 1-shot.  Our result shows the effectiveness of LLMs in adapting quickly to a new task in low-resource settings. We provide the results of individual languages in Appendix \ref{app:llm-few-shot-results}.
% \autoref{tab:few-shot-results}. 

\paragraph{Would Few-shot performance match Supervised fine-tuning (SFT)?} While all LLMs improve performance with more shots, there is still a large gap with SFT. We performed SFT on Gemma 2 9B with all training samples and prompt templates, we found a large performance gap of $+17.4$ and $+34.2$ for intent detection and slot-filling respectively if we compare SFT (52k samples) to 4-shots (160 examples). However, for closed models, the gap of SFT on Gemma 2 9B to \gemini and \gpto is much smaller, especially for intent detection. In general, few-shots of LLMs are still worse than SFT but are very crucial and effective when training data are scarce. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/crosslingual_fewshot.png}
    \vspace{-7mm}
    \caption{Cross-lingual transfer results from \textsc{CLINC} and \textsc{Injongo} English data}
    \label{fig:cross-lingual-transfer}
    \vspace{-1em}
\end{figure}


\subsection{Cross-lingual Transfer results}
\autoref{fig:cross-lingual-transfer} shows our final experiments that compare cross-lingual transfer results from two English datasets: \textsc{Clinc} (Western-centric) and \dataset{} (African-centric) on the intent detection task. At 5-shots, in both i\textit{n-language} and \textit{translate-test} settings, the accuracy of all settings is quite similar, however as we increase the number of instances to 10-shots (400 examples), we find that the \dataset{} in-language performance was better than the \textsc{Clinc} ($16.1$ vs. $4.0$) that is more Western-centric. Similarly, in \textit{translate-test} setting, the gain in performance is much larger ($+29$)  which implies that in a low-resource setting, leveraging a multicultural dataset with the African context is effective. However, with more samples (25-shots and above), there is no significant difference in whether the samples are Western-centric or not, and training data size seems to be more important. 

%For observing the cross-lingual transfer learning, we evaluate the performance of AfroXLMR-large-76L on the combined \dataset{} dataset with all 17 languages. AfroXLMR-76L maintains its top performance, with an average accuracy of 94.4\% for intent detection and an F1 score of 87.3\% for slot filling. The performance on the combined multilingual dataset is generally 1-2\% higher than on the monolingual dataset, especially in Bantu and Nilotic languages.


    

%\subsection{Few-shot performance}


%\paragraph{With Additional Content}
% Few-shot learning demonstrates substantial improvements over zero-shot performance. As shown in Table~\ref{tab:shot-sft-comparison}, providing examples yields consistent gains across all models. The ``five-example'' setting, which includes one example from each domain, shows intermediate performance improvements. Notably, the ``one-shot'' setting, which provides one example per intent (40 examples) and slot type (23 examples), yields the best results, suggesting that coverage of all possible labels is crucial for optimal performance. However, even with these improvements, the few-shot results still fall short of the specialized encoder-decoder models shown in Table~\ref{tab:sft-result}, which achieve over 83\% accuracy without requiring exemplars, highlighting the continued advantage of task-specific architectures for this domain.

% Look more closely, for only ``five-example'' setting, shows incrediable improvements for all models. For example, GPT-4o improves xx\% in intent detection and xx\% in slot filling, and Gemini 1.5 Pro directly overpass GPT-4o in both tasks, even for Gemma 2 IT 9B, improve almost doulbe the performance in intent detction and 5 times better in slot filling and beats both GPT-4o and Gemini 1.5 Pro in zero-shot setting. This indicates that well-curated examples with diverse slot types and domains remain crucial in boosting zero-shot model coverage, reaffirming the value of guided examples for real-world African language tasks and formatted outputs for post-processing.

% The ``one-shot'' setting, which provides one example per intent and slot type, yields the best results witht the most context. But the performance gains diminish, suggesting a practical upper bound of cost-efficiency for LLM prompting, where continued additions of examples bring diminishing returns. However, the well-curated examples with diverse slot types and domains remain crucial in boosting zero-shot model coverage, reaffirming the value of guided examples for real-world African language tasks and formatted outputs for post-processing.

% ========================================================

\begin{comment}
    

Few-shot learning demonstrates substantial improvements over zero-shot performance. As shown in Table~\ref{tab:shot-sft-comparison}, providing examples yields consistent gains across all models. The ``five-example'' setting, which includes one example from each domain, shows steady performance improvements. With additional context, the ``one-shot'' setting, which provides one example per intent (40 examples) and slot type (23 examples), yields the best results.
% , suggesting that coverage of all possible labels is crucial for optimal performance. 
However, even with these improvements, the few-shot results still fall short of the finetuned baselines shown in Figure~\ref{fig:shot-sft-comparison}.
%  which achieve over 83\% accuracy without requiring exemplars, highlighting the continued advantage of task-specific architectures for this domain.

In the ``five-example'' setting alone, models show remarkable improvements. GPT-4o improves by 10\% in intent detection and 60\% in slot filling. Gemini 1.5 Pro demonstrates even stronger gains, improving by 23.8\% and 84\% respectively and beating GPT-4o in both tasks. Most notably, SFT Gemma 2 IT 9B shows dramatic improvements: 17.8 accuracy (from 29.2 to 47.0) in intent detection and 29.3 F1 score (from 6.2 to 35.5), in slot filling - representing a nearly 6-fold increase in slot filling performance and surpassing both GPT-4o and Gemini 1.5 Pro in zero-shot setting.

The ``one-shot'' setting yields further improvements, with GPT-4o reaching 78.0\% accuracy (+12.9 from zero-shot) in intent detection and 59.6\% F1 score (+26.3) in slot filling.
% Gemini 1.5 Pro achieves the highest few-shot performance at 80.3\% (+21.2 points) for intent detection and 60.7\% (+30.6 points) for slot filling. 
However, the incremental gains between five-example and one-shot (averaging 5-7 points) are smaller than those between zero-shot and five-example (averaging 15-20 points), suggesting diminishing returns from additional examples. This indicates that while well-curated few examples are crucial for model performance, there may be a practical limit to the benefits of adding more context examples in prompt-based approaches.


% (65.1\% and 59.1\%) to one-shot learning (78.0\% and 80.3\%), with gains of approximately 13-21 percentage points. The 5-example approach, which uses one example from each domain, shows intermediate performance gains. Notably, the one-shot setting, which provides one example per intent (40 examples) and slot type (23 examples), yields the best results, suggesting that coverage of all possible labels is crucial for optimal performance. However, even with these improvements, the few-shot results still fall short of the specialized encoder-decoder models shown in Table~\ref{tab:prompt-results}, which achieve over 83\% accuracy without requiring exemplars, highlighting the continued advantage of task-specific architectures for this domain.


% We further investigate performance when providing more in-context examples (e.g., random 5 examples from each doamin or one-shot setting). Though quality improves steadily, the rate of gain diminishes after about 5 examples. This suggests a practical upper bound of cost-efficiency for LLM prompting, where continued additions of examples bring diminishing returns. However, well-curated examples with diverse slot types and domains remain crucial in boosting zero-shot model coverage, reaffirming the value of guided examples for real-world African language tasks and formatted outputs for post-processing.

\paragraph{Supervised Fine-Tuning}
Even the ``one-shot'' setting, which provides one example per intent and slot type, yields the best results with the most context. However, the performance gains diminish, suggesting a practical upper bound of cost-efficiency for LLM prompting, where continued additions of examples bring diminishing returns. How well does the SFT model perform? Notably, models like Gemma 2 IT 9B substantially close the gap once they undergo SFT (85.7\% accuracy for intent detection, 83.2\% for slot filling) in combined datasets.
This underscores the importance of task-specific data and parameter level training to maximize cross-lingual coverage and yield more stable performance on structurally complex tasks.


% Performance generally improves with few-shot prompting, yet remains below specialized finetuned encoders.
% \subsection{Discussion}
% Generally, these results indicate that (1) large multilingual encoders excel at African language intent detection; (2) slot filling remains more demanding, likely due to token-level constraints; (3) scaled LLMs require explicit adaptation to match targeted performance, especially on languages with less pretraining coverage; and (4) cross-lingual strategies help share knowledge across related languages, benefiting morphologically diverse tasks.

\subsection{Multicultural Analysis}

The English-bridged transfer results in Table~\ref{tab:multicultural} reveal that combining English data with translated examples (eng+clinc) achieves the best cross-lingual transfer, with 75.9 F1 on translated evaluation sets. This outperforms both English-only (70.9 F1) and CLINC-only (37.5 F1) training, indicating the importance of diverse training data for robust cross-lingual generalization.
\end{comment}
