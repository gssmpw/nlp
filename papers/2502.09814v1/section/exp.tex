\section{Experiments Setup}

\subsection{Fine-tuning baselines}
We evaluate three categories of models: (1) \textbf{encoder-only models} such as XLM-RoBERTa Large \cite{XLM-R}, AfroXLMR~\cite{AfroXLM}, AfroXLMR-76L~\cite{AfroXLM-76L}, AfriBERTa V2~\cite{oladipo2024scaling}, (2)  \textbf{encoder-decoder models} such as mT5-Large \cite{mT5}, AfriTeVa V2 Large \cite{AfriTeVaV2}, and (3)  \textbf{a multilingual variant of LLM2Vec} model~\citep{llm2vec} i.e. NLLB-LLM2Vec \cite{NLLB-LLM2Vec} that stack NLLB-encoder~\citep{NLLB} model with LLaMa 3.1 8B~\citep{Llama3} to develop a multilingual sentence transformer model. These models are fine-tuned using the AdamW optimizer for 20 epochs with early stopping. All results are averaged over five runs. Learning rates are calibrated for each architecture and task as detailed in Appendix \ref{sec:lr-impact}. The languages covered in each pre-trained model are available in Appendix \ref{app:language}. 

\begin{table*}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|l|cccccccccccccccccc}
\toprule[1pt]
\multicolumn{1}{l|}{\textbf{Task}} & \textbf{Model} & \lang{eng} & \lang{amh} & \lang{ewe} & \lang{hau} & \lang{ibo} & \lang{kin} & \lang{lin} & \lang{lug} & \lang{orm} & \lang{sna} & \lang{sot} & \lang{swa} & \lang{twi} & \lang{wol} & \lang{xho} & \lang{yor} & \lang{zul} & \textbf{AVG} \\ \midrule
& \multicolumn{19}{l}{\textbf{\texttt{In-language training}}} \\ 
\multirow{10}{*}{\begin{tabular}[c]{@{}c@{}} \textsc{Intent} \\ \textsc{Detection}\end{tabular}} & mT5-Large & 80.5 & 91.5 & 77.3 & 94.6 & 92.9 & 83.7 & 91.3 & 83.3 & 73.3 & 92.6 & 80.2 & 95.8 & 85.3 & 91.6 & 95.8 & 90.9 & 82.4 & 87.7$_{\pm 4.1}$ \\
& AfriTeVa V2 (T5) & 81.6 & 93.2& 84.4& \textbf{98.9}& 95.7& 87.8& 91.6& 86.8& 86.6& 94.6& 85.7& 96.8& 87.1& 94.0& 97.3& 97.0& 89.2& 91.7$_{\pm 2.7}$ \\
\cmidrule{2-20}
& NLLB LLM2Vec& \textbf{88.4} & 94.2& 87.8& 98.3& \textbf{96.8}& 89.2& \textbf{95.2}& \textbf{93.2}& 86.2& \textbf{96.1}& 87.3& 97.4& 93.5& 95.6& \textbf{97.5}& 97.3& 89.1& 93.4$_{\pm 2.3}$ \\
& XLM-RoBERTa& 83.5 & 92.9& 77.9& 96.0& 88.8& 69.6& 90.5& 78.9& 75.0& 83.8& 76.0& 96.7& 79.5& 90.2& 89.6& 92.6& 74.7& 84.5$_{\pm 4.9}$ \\
& AfriBERTa V2& 74.2 & 91.2& 78.3& 98.2& 93.8& 83.1& 91.0& 83.8& 78.8& 89.5& 81.9& 96.0& 83.2& 92.3& 94.4& 95.0& 86.7& 88.6$_{\pm 3.5}$ \\
& AfroXLMR& 84.1 & 95.3& 84.6& 98.3& 96.0& 88.2& 93.3& 85.2& 88.3& 95.3& 85.5& 97.8& 88.8& 95.8& 97.3& 96.1& 89.0& 92.2$_{\pm 3.0}$ \\
\rowcolor{Gray}
\cellcolor{white} & AfroXLMR 76L& 84.5 & \textbf{95.5}& \textbf{90.4}& 98.7& 96.3& \textbf{89.4}& 94.6& 91.3& \textbf{88.3}& 95.1& \textbf{86.8}& \textbf{98.1}& \textbf{93.6}& \textbf{96.2}& 96.9& \textbf{97.7}& \textbf{89.8}& \textbf{93.7}$_{\pm 2.1}$ \\
\cmidrule{2-20}
& \multicolumn{19}{l}{\textbf{\texttt{Multi-lingual training}}} \\ \rowcolor{LightCyan}
\cellcolor{white} & \multicolumn{1}{l|}{AfroXLMR-76L}& 89.0 & 96.0& 92.6& 99.2& 96.6& 87.7& 95.9& 92.3& 92.9& 96.5& 87.6& 97.8& 94.2& 97.1& 97.3& 97.9& 89.2& \textbf{94.4$_{\pm 2.0}$} \\ \midrule
& \multicolumn{19}{l}{\textbf{\texttt{In-language training}}} \\ 
\multirow{8}{*}{{\begin{tabular}[c]{@{}c@{}}\textsc{Slot} \\\textsc{Filling}\end{tabular}}} & mT5-Large& 73.7 & 80.9& 71.6& 89.4& 80.5& 74.2& 82.6& 78.9& 72.1& 81.1& 74.7& 88.1& 79.0& 76.9& 88.4& 78.9& 68.3& 79.1$_{\pm 3.7}$ \\
& AfriTeVa V2 (T5) & 73.6 & 80.9& 74.5& 93.8& 79.9& 76.6& 87.1& 85.2& 79.0& 82.1& \textbf{77.5}& 88.9& 84.0& 79.0& 90.0& 87.2& 71.2& 82.3$_{\pm 3.3}$ \\
\cmidrule{2-20}
& NLLB LLM2Vec& 74.6 & 82.4& 80.5& 93.6& 78.1& 70.1& 84.8& 86.6& 80.8& 81.4& 74.8& 85.7& 85.7& 78.3& 88.0& 85.0& 78.3& 82.1$_{\pm 3.1}$ \\
& XLM-RoBERTa & 77.9 & 84.8& 79.9& 93.9& 76.6& 69.3& 86.3& 83.8& 83.8& 79.3& 71.7& 88.7& 84.2& 79.3& 89.1& 83.9& 79.4& 82.1$_{\pm 3.5}$ \\
& AfriBERTa V2 & 70.7 & 82.2& 77.9& 93.7& 78.3& 73.8& 84.4& 84.1& 81.0& 81.8& 73.5& 87.6& 81.9& 78.3& 88.5& 86.2& 79.6& 82.1$_{\pm 2.9}$ \\
& AfroXLMR& \textbf{79.0} & 86.2& 81.6& \textbf{95.1}& \textbf{82.0}& 76.3& 87.1& 88.5& 84.9& \textbf{84.9}& \textbf{77.5}& \textbf{90.2}& 85.5& \textbf{81.7}& \textbf{91.1}& 87.3& \textbf{82.5}& 85.2$_{\pm 2.7}$ \\
\rowcolor{Gray}
\cellcolor{white} & AfroXLMR 76L& 78.7 & \textbf{86.3}& \textbf{84.5}& 94.3& 81.9& \textbf{76.7}& \textbf{88.0}& \textbf{88.8}& \textbf{85.5}& \textbf{84.9}& 77.4& \textbf{90.2}& \textbf{89.8}& 81.3& 90.5& \textbf{88.1}& 81.3& \textbf{85.6$_{\pm 2.7}$} \\
\cmidrule{2-20}
& \multicolumn{19}{l}{\textbf{\texttt{Multi-lingual training}}} \\ 
\rowcolor{LightCyan}
\cellcolor{white} & \multicolumn{1}{l|}{AfroXLMR 76L}& 82.4 & 88.2& 87.0& 96.3& 84.0& 79.3& 90.3& 89.2& 87.2& 86.1& 80.4& 90.5& 90.3& 83.3& 91.8& 90.2& 83.3& \textbf{87.3$_{\pm 2.4}$} \\
\bottomrule[1pt]
\end{tabular}%
}
\vspace{-2mm}
\caption{\textbf{Intent detection and slot-filling results for supervised fine-tuned Small LMs on \dataset{}}. Models are ranked by \underline{accuracy} for \underline{intent detection} and \underline{F1-score} for \underline{slot-filling}. The average performance and standard deviation across 16 African languages are reported. The best models are highlighted in \textbf{Gray} and \textbf{Cyan} colours.}
\label{tab:sft-result}
\end{table*}

% AfriTeVa V2 Large	94.6	85.8	99.2	96.5	87.3	93.6	90.8	88.6	95.9	85.3	98.0	89.6	94.4	97.3	97.6	88.3	89.7	92.7$_{\pm 2.5}$
% AfroXLMR-large 76L	96.0	92.6	99.2	96.6	87.7	95.9	92.3	92.9	96.5	87.6	97.8	94.2	97.1	97.3	97.9	89.2	89.0	94.4$_{\pm 2.0}$
% AfroXLMR-large	96.1	90.3	99.3	96.5	86.8	94.2	91.6	92.2	96.0	87.1	97.9	91.6	96.1	96.9	97.4	88.6	89.7	93.7$_{\pm 2.2}$
% NLLB LLM2Vec	95.8	90.2	98.7	96.5	86.2	95.4	92.6	87.9	96.9	86.8	97.3	93.9	95.5	96.9	97.2	88.6	89.1	93.5$_{\pm 2.3}$
% AfriTeVa V2 Large	78.9	72.6	92.0	80.0	75.7	85.3	81.8	76.0	79.8	77.0	88.2	81.7	76.5	86.7	86.3	66.7	78.9	80.3$_{\pm 3.5}$
% AfroXLMR-large 76L	88.2	87.0	96.3	84.0	79.3	90.3	89.2	87.2	86.1	80.4	90.5	90.3	83.3	91.8	90.2	83.3	82.4	87.3$_{\pm 2.4}$
% AfroXLMR-large	87.9	84.0	96.4	83.6	80.4	89.5	88.4	88.2	87.0	82.0	91.5	87.7	81.9	91.7	90.4	84.2	82.8	87.2$_{\pm 2.3}$
% NLLB LLM2Vec	84.3	82.0	94.6	80.3	72.3	86.9	85.1	81.9	82.0	77.2	87.3	85.8	80.0	90.4	87.1	79.9	80.8	83.6$_{\pm 2.9}$

\subsection{LLM Prompting} 
First, we conduct \textbf{zero-shot prompting} using the following widely used LLMs for evaluation: GPT-4o,\footnote{\url{https://platform.openai.com/docs/models\#gpt-4o}} Gemini 1.5 Pro~\citep{Reid2024Gemini1U}%\footnote{\url{https://ai.google.dev/gemini-api/docs/models/gemini\#gemini-1.5-pro}}
, Gemma 2 9B/27B Instruct \cite{Gemma2}, Llama 3.1 8B/3.3 70B Instruct~\cite{Llama3}, and Aya-101~\cite{aya}. We make use of five different prompts for each LLM. Second, we perform \textbf{few-shot evaluation} using the best-performing zero-shot template for each task (see Appendix \ref{app:prompts}). We employ two few-shot strategies (1) \textit{5-examples}: prompting with any 5 samples from different domains (see \autoref{fig:example}) i.e. one intent type is covered by domain (2) \textit{One-shot} intent-type prompting i.e. one sample per intent type or 40 samples from different intent types. We used the same samples for both tasks. Finally, we extend to 4 shots ---acceptable maximum context length (CL) for Gemma 2, Aya-101 was excluded for small CL.

Finally, as an additional strong baseline for LLMs, we performed supervised fine-tuning (SFT) on the Gemma 2 9B for 5 epochs using learning rates of $2\times10^{-5}$/ $2.5\times10^{-5}$ for intent detection and slot filling. The dataset of SFT was obtained by aggregating all the training samples of the 17 languages in \dataset{} i.e. ``Combined \dataset{}'', with randomly sampled prompts from a pool of 5. The evaluations of LLMs use 5 different prompting templates and a temperature of 0.5. We provide all the prompts used in Appendix \ref{app:prompts}.

    
\subsection{Cross-lingual Transfer Analysis}
% To evaluate cross-lingual capabilities, w
To investigate how well our dataset facilitates cross-lingual learning and transfer capabilities across languages,
% For investigating the cross-lingual transfer, 
we tested two settings (1) \textbf{zero-shot transfer} from the English split of \dataset{}, and evaluated on African languages. (2) \textbf{Translate-Test} where we evaluate the best English model on the machine-translated sentence test sets from an African language to English. We leveraged the NLLB-200-3.3B~\cite{NLLB}  machine translation model for the Translate-test setting. We compare the results with LLM prompting. 


%we construct additional validation datasets shown in Table \ref{tab:validation-datasets}. We use the model, which achieved the best performance, training on ``Combined \dataset{}'' with the same training and evaluation setting of fine-tuning baselines.
% by merging training splits across all 17 languages. 

\begin{comment}
    

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{ll}
\toprule
\textbf{Dataset} & \textbf{Description} [Train Split] \\
\midrule
eng & 1,047 samples from Injongo English train split\\
clinc & 1,040 samples from CLINC, matching Injongo intents\\
eng+clinc & Combined training samples from ``eng'' and ``clinc''\\
clinc+extend & Extend ``clinc'' with additional 1,040 CLINC samples\\
\bottomrule
\end{tabular}
}
\caption{Additional English datasets from CLINC.}
\label{tab:validation-datasets}
\end{table}
\end{comment}

\paragraph{Hyper-parameters and Prompts used} %Performance is measured using accuracy for intent detection and micro-averaged  F1 scores for slot filling. 
Experiments of the baselines and cross-lingual transfer runs make use of five fixed random seeds. 
% The evaluation of LLMs makes use of 5 different prompt templates.
% \paragraph{} Training uses full precision (FP32) on NVIDIA L40S GPUs. 
Detailed experiments setup, training configuration and prompts are in Appendix \ref{sec:exp-more}.