\section{Related Work}
% Happy is working on this section
% David focus on two aspects: African Benchmarks and Intent-detection benchmarks

\paragraph{African Benchmarks}
Limited available labeled datasets are one of the major challenges of AfricaNLP. Since 2021, there have been many grassroots efforts to create large-scale datasets for African languages covering several tasks such as machine translation~\citep{Alabi2025}, named entity recognition~\citep{adelani-etal-2021-masakhaner,adelani-etal-2022-masakhaner}, sentiment classification~\citep{muhammad-etal-2023-afrisenti}, hate speech~\citep{Muhammad2025AfriHateAM}, question answering~\citep{ogundepo-etal-2023-cross}, topic classification~\citep{adelani-etal-2023-masakhanews,AfroXLM-76L} covering 10 to 57 languages. %One of the largest natural language understanding benchmark in terms of language coverage is SIB-200~\cite{AfroXLM-76L} covering 200 languages including 57 African languages. 
The closest benchmark to our task of slot-filling is the MasakhaNER~\citep{adelani-etal-2021-masakhaner,adelani-etal-2022-masakhaner} that covers 20 African languages but they focus on four entity types  ``personal names'', ``organization'', ``location'', and ``dates'', which are not fine-grained and well adapted to several domains such as banking and travel %and kitchen \& diniing 
that we cover in \dataset{}. 


%Recent years have seen substantial growth in African language evaluation benchmarks. ChatGPT-MT~\cite{Robinson2023} provided extensive machine translation evaluation across 57 African languages, while MEGA~\cite{Ahuja2023} assessed 11 African languages on 10 fundamental NLP tasks. This coverage was further expanded by Megaverse~\cite{Ahuja2024} to 16 languages across 16 tasks and SIB-200~\cite{Adelani2024} which evaluated topic classification for 57 languages.

%The most comprehensive evaluation framework, AfroBench~\cite{AfroBench} spans 60 African languages across 15 distinct tasks, including text classification, question answering, and knowledge evaluation. However, despite this progress in general NLP evaluation, task-oriented dialogue systems---particularly intent detection and slot-filling---remain notably underexplored for African languages. This gap underscores the importance of our work in providing the first large-scale, culturally-authentic benchmark for these crucial dialogue system components.

\paragraph{Intent and Slot-filling Benchmarks} Most of the existing benchmarks for intent detection and slot-filling tasks are English-only. There are a few efforts to make them multilingual in two ways: (1) human generating the utterances in a particular domain, followed by intent and slot filling annotation. (2) through human translation of annotated data from English to other languages which introduces some cultural bias since Western entities are being propagated. While the first approach is the most ideal methodology, it is very cost-intensive when scaling to many languages. Facebook dataset~\citep{schuster-etal-2019-cross-lingual} followed the first approach by creating labeled data in three domains (alarm, reminder and weather) for three languages: English, Spanish and Thai. However, most other approaches make use of the second approach, where English data are translated to other languages~\citep{xu-etal-2020-end, van-der-goot-etal-2021-masked,li-etal-2021-mtop}, however, they often do not include African languages. XTREME-UP benchmark expanded the MTOP dataset~\citep{li-etal-2021-mtop} to five African languages (Amharic, Hausa, Yoruba, Swahili and Zulu), while \massive~\citep{fitzgerald-etal-2023-massive} perform human translation to 50 languages including three African languages (Afrikaans, Amharic, and Swahili). \massive benchmark partially addresses this Western cultural bias by encouraging translators to replace entities with more culturally relevant ones, but Western entities are still prevalent in the dataset. \autoref{tab:past_works} summarizes all existing related works. In our paper, we introduce \dataset{} which is the largest intent detection and slot-filling dataset covering 16 African languages, and we ensured that the slot entities are more culturally relevant in the respective countries the languages are from.



%Intent detection and slot filling are two fundamental tasks in task-oriented spoken language understanding (SLU). Intent detection aims to identify the user's purpose in a given utterance, while slot filling extracts relevant semantic entities from the input text.

%The development of multilingual intent detection and slot-filling benchmarks has evolved significantly. Facebook~\citep{schuster-etal-2019-cross-lingual} pioneered cross-lingual transfer methods with a dataset of 57k utterances across three languages. MultiATIS++~\citep{xu-etal-2020-end} advanced this through end-to-end slot alignment across nine languages, while xSID~\cite{van-der-goot-etal-2021-masked} introduced joint learning with auxiliary tasks across 13 languages from 6 language families.

%Scale became the next frontier, with MTOP~\cite{li-etal-2021-mtop} providing 100k annotated utterances across 11 domains. Recent efforts began incorporating African languages: MTOP++~\citep{ruder-etal-2023-xtreme} included five African languages in its user-centric benchmark, while MASSIVE~\citep{fitzgerald-etal-2023-massive} included three African languages among its 51 languages. However, these relied on translation rather than native generation.

%Studies addressing low-resource scenarios have explored various approaches. Recent work has adopted few-shot learning methods~\cite{chen2022contrastnet, dopierre2021protaugment, krone2020learning} and zero-shot techniques~\cite{parikh2023exploring} with parameter-efficient fine-tuning. A comprehensive review of these datasets and approaches is provided in~\cite{larson2022survey}.

%Beyond intent detection and slot-filling, broader evaluation efforts like IrokoBench~\cite{adelani2024irokobench} have highlighted substantial performance gaps between high-resource and African languages. These findings underscore the importance of developing native, culturally-relevant benchmarks for African languages, which our work directly addresses. The development of multilingual intent detection and slot-filling benchmarks has evolved significantly, though African languages remained notably underrepresented. 


% Facebook~\citep{schuster-etal-2019-cross-lingual} pioneered cross-lingual transfer methods by introducing a dataset of 57k utterances across three languages, evaluating translation-based transfer and cross-lingual embeddings. MultiATIS++~\citep{xu-etal-2020-end} advanced the field with end-to-end slot alignment across nine languages and four language families, while xSID~\cite{van-der-goot-etal-2021-masked} introduced joint learning with auxiliary tasks across 13 languages from 6 language families.
% MTOP~\cite{li-etal-2021-mtop} marked a significant expansion with 100k annotated utterances across 11 domains, addressing previous datasets' limitations in size and compositional query handling. However, these early benchmarks completely excluded African languages, focusing primarily on high-resource languages.
% Recent efforts have begun incorporating African languages, though limitations persist. MTOP++~\citep{ruder-etal-2023-xtreme} made a notable advancement by including five African languages (Amharic, Hausa, Yoruba, Swahili, Zulu) in its user-centric benchmark designed for scarce-data scenarios. MASSIVE~\citep{fitzgerald-etal-2023-massive}, while impressive in scale with 1M examples across 51 languages, included only three African languages (Afrikaans, Amharic, Swahili). Their "localize-translate-preserve" strategy attempted to maintain cultural relevance, but still relied on translation rather than native generation.

% Intent detection and slot filling are two fundamental tasks in task-oriented spoken language understanding (SLU). Intent detection aims to identify the user's purpose in a given utterance, while slot filling extracts relevant semantic entities from the input text. 

% Studies addressing slot and intent detection for low-resource languages have been conducted in recent years; we discuss some relevant work in this section. Existing studies have adopted few-shot learning methods to recognize intents~\cite{chen2022contrastnet, dopierre2021protaugment, krone2020learning}. In addition, the authors in~\cite{parikh2023exploring} explore intent filtering in a zero-shot setting and tune instruction-tuned language models with parameter-efficient fine-tuning in a few-shot setting.
% Another important direction in this domain is the development of diverse datasets for intent detection and slot-filling tasks. The paper "A Survey of intent detection and Slot-Filling Datasets for Task-Oriented Dialog"~\cite{larson2022survey} provides a comprehensive review of publicly available datasets used for intent detection and slot-filling in task-oriented dialogue systems. Beyond intent detection and slot-filling, recent work has emphasized the need for benchmarks that evaluate broader language model capabilities for low-resource African languages. The IrokoBench benchmark~\cite{adelani2024irokobench} introduces a human-translated evaluation dataset covering 17 typologically diverse African languages across multiple tasks, including natural language inference, mathematical reasoning, and knowledge-based question answering. Their findings reveal substantial performance gaps between high-resource and low-resource languages, particularly in comparison to proprietary models, highlighting the necessity of more inclusive evaluation benchmarks for African languages.


%including datasets, benchmarks, and models that aim to address this challenge.