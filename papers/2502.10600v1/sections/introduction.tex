
Numerous problems arising in statistics and machine learning involve approximating a probability measure using a small set of points, a task usually referred to as quantization \cite{GrLu00}. Examples include clustering \cite{Llo82}, numerical integration \cite{RoCaCa99,Bac17} and teacher-student training of neural networks \cite{ChOyBa19,ArKoSaGr19}. Formally, the quantization problem consists of seeking an approximation to a probability measure $\pi$ using a mixture of Diracs $\sum_{i=1}^M \wi \delta_{\yi}$, where the $\yi$ are usually called nodes or centroids and the $\wi$ are weights. These nodes and weights are generally chosen to minimize $D(\pi, \sum_{i=1}^M \wi \delta_{\yi})$, where $D$ quantifies the discrepancy between measures. Lloydâ€™s algorithm is probably the most iconic example of quantization \cite{Llo82}. This algorithm aims to solve the optimization problem where $D$ is taken to be the 2-Wasserstein distance and the weights are constrained to the simplex. 



In this work, we focus on the case where $D$ is the maximum mean discrepancy (MMD), a statistical pseudo-distance between two measures defined as follows: given a measurable space $\mathcal{X}$ and a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$, the MMD between measures $\mu$ and $\nu$ is defined as 
\begin{equation*}
\mathrm{MMD}(\mu,\nu) := \sup_{\|f\|_{\mathcal{H}}\leq 1}\Big|\int_{\mathcal{X}} f(x) \mathrm{d} \mu(x) - \int_{\mathcal{X}} f(x) \mathrm{d} \nu(x)\Big|.
\end{equation*}

Compared to Wasserstein distances, MMD offers significant computational advantages, avoiding complex optimization problems and enabling efficient computation through kernel evaluations. Additionally,  MMD exhibits favorable statistical properties in high-dimensional domains that are crucial for numerical integration tasks \cite{MuFuSrSc17,WeBa19}. Initially introduced as a tool of statistical analysis and hypothesis testing \citep{GrFuTeScSm07, GrBoRaScSm12},  MMD has since found applications in diverse fields ranging from generative modeling \citep{DzRoGh15, LiChChYaPo17} to optimal transport \citep{ChNiRi24,PeCu19}.

More precisely, we consider the problem of MMD minimization under a cardinality constraint, formulated as
\begin{equation}\label{eq:constrained_MMD_minimization}
\min_{\mu \in \mathcal{M}_{M}(\mathcal{X}) }\MMD\left(\pi,\mu \right),
\end{equation}
where $\mathcal{M}_{M}(\mathcal{X})$ consists of mixtures of $M$ Diracs $\sum_{i=1}^{M} \wi \delta_{\yi}$ with $\yi \in \X$ and $\wi \in \mathbb{R}$. 
This minimization problem can be seen as a relaxation of 
classical optimal quantization, where the weights $\wi$ are constrained to the simplex.

As the quantization problem seeks to minimize a discrepancy between the target distribution $\pi$ and a finite set of particles, this work is naturally connected to the study of gradient flows. 
A gradient flow traces a path through the space of measures. In particular, it minimizes a given ``energy'' functional by following the local direction of steepest descent defined according to a chosen geometry.
This framework is amenable to numerical implementation, which in practice boils down to a discretization of the equations governing the gradient flow, often yielding an interacting particle system. Despite growing interest in the practice and theory of gradient flows \cite{JoKiOt98,Ott01,KoMoVo16,CrBe16,San17,GaMo17,CaCrPa19,ArKoSaGr19,ChGoLuMaRi20,SaKoLu20,GlArGr21,KoAuMaAb21,LuSlWa23,MaMa24,ChNiRi24,ZhMi24}, existing work largely focuses on the mean-field limit where the number of particles grows to infinity, conflicting with the motivation for quantization.

For this reason, we study~\eqref{eq:constrained_MMD_minimization} from two complementary perspectives. The first aligns naturally with  research on gradient flows \cite{ArKoSaGr19,GlDvMiZh24}, where we argue that the Wasserstein--Fisher--Rao (WFR) geometry is more flexible than the Wasserstein geometry for MMD minimization. From the second perspective, we exploit a novel connection between MMD minimization and the mean shift algorithm to derive a new analog of Lloyd's algorithm. 
















Our \textbf{main contributions} are then as follows:
\begin{itemize}
\item We \textit{unify} views of gradient flows, quantization, and mode-seeking methods for MMD minimization using a finite set of weighted particles.


\item We employ the \emph{Wasserstein--Fisher--Rao} (WFR) geometry for MMD minimization, which offers useful flexibility by allowing for both transport and the creation/destruction of mass. We represent the associated gradient flow with a mean-field ODE system that is efficient to solve using modern computational tools. 

\item We derive a new fixed-point iteration, named \emph{mean shift interacting particles} (MSIP), that \textit{directly} targets the steady-state solution of our WFR ODE. The weights of the proposed system naturally coincide with classical kernel-based quadrature constructions. 

\item We show that MSIP shares analytical and numerical similarities with both Lloyd's algorithm for clustering and the mean shift algorithm for mode-seeking. We prove that the particle system is a \textit{preconditioned gradient descent} of the function $$(y_1, \dots, y_M) \mapsto \min_{w_i \in \mathbb{R}} \MMD \left(\pi, \sum\limits_{i =1}^{M} w_{i} \delta_{y_i} \right)^2.$$ 


\item We demonstrate the computational \textit{efficiency} and \textit{robustness} of the proposed approaches for high-dimensional quantization problems, improving on current state-of-the-art methods. %
In particular, both MSIP and the WFR ODE yield \textit{near-optimal} quantizations under MMD, even with extremely adversarial initializations. 



\end{itemize}



This article is structured as follows. In \Cref{sec:related_work}, we review the literature on quantization, numerical integration in reproducing kernel Hilbert spaces (RKHS), the mean shift algorithm, and gradient flows. In \Cref{sec:algorithms} we present the main contributions of this work. In \Cref{sec:numerics}, we report on numerical experiments that illustrate the proposed algorithms. A short discussion concludes the article in \Cref{sec:discussion}.

\paragraph{Notation} We denote by $\mathcal{X}$ a subset of $\mathbb{R}^{d}$, and we use  $\mathcal{M}(\mathcal{X})$ and $\mathcal{P}(\mathcal{X})$ to refer to the set of measures and probability measures supported on $\mathcal{X}$, respectively. Moreover, given $M \in \mathbb{N}^{*}$, we denote by
$\mathcal{X}^{M}$ the $M$-fold Cartesian product of $\mathcal{X}$, and by $\mathcal{X}_{\neq}^{M} \subset \mathcal{X}^{M}$ the set
\begin{equation*}
  \mathcal{X}^{M}_{\neq}:= \{ (x_1, \dots,x_M) \in \mathcal{X}^{M}, \forall i\neq j \implies x_i \neq x_j \}.
\end{equation*}


We consider $\Y \in \mathcal{X}^{M}$ as a configuration of $M$ points $\yone, \dots, \ym \in \mathcal{X}$. Since $\mathcal{X} \subset \mathbb{R}^{d}$, we abuse notation and let $\Y$ refer to the $M \times d$ matrix whose rows are the vectors $\yone, \dots, \ym$. We also denote by $\Delta_{M-1}$ the simplex of dimension $M-1$ defined by
$\{ \bm{w} \in [0,1]^{M}| \:\: \sum_{i \in [M]} \wi = 1 \}$. Consider $\kappa:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ as the kernel associated to the RKHS $\mathcal{H}$. For a configuration $\Y$, we denote by $\bm{K}(\Y):= (\kappa(y_i,y_j))_{i,j \in [M]}$ the associated kernel matrix for $i,j \in [M]$. Additionally, for a given real-valued function $f: \mathcal{X} \rightarrow \RR$, we denote by $\bm{f}(\Y) := (f(y_i))_{i \in [M]}$ the vector of evaluations of $f$ on the elements of $\Y$. Finally, for a given vector-valued function $\bm{g}: \mathcal{X} \rightarrow \mathbb{R}^{d}$, we denote by $\bm{g}(\Y)$ the $M \times d$ matrix where each row is an evaluation of $\bm{g}$ on each $\yi$.
