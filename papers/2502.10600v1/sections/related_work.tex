\subsection{Numerical integration and quantization in RKHS}\label{sec:num_int_quantization_review}


 As it is at the core of many tasks in statistics and probability, we first review numerical methods to approximate integrals with a weighted sum of function evaluations at selected points \cite{RoCaCa99}.
Formally, for a measure $\pi \in \mathcal{P}(\mathcal{X})$ and a function $f: \mathcal{X} \to \mathbb{R}$, a quadrature rule seeks to approximate $\int_{\mathcal{X}} f(x)\dpix$ with $\sum_{i = 1}^{M} \wi f(\yi)$, where the points $\yi \in \mathcal{X}$ are the quadrature nodes and the scalars $\wi$ are the quadrature weights. To design a quadrature rule, we select representative nodes and weights to approximate the distribution $\pi$, essentially reducing the task to a quantization problem. Indeed, when the function $f$ belongs to the RKHS $\mathcal{H}$, the integration error $|\int_{\mathcal{X}}f(x) \mathrm{d}\pi(x) - \sum_{i = 1}^{M} \wi f(\yi) |$ is bounded by \cite{MuFuSrSc17} 
\begin{equation}\label{eq:quadrature_mmd_inequality}
     \|f\|_{\mathcal{H}} \mathrm{MMD}\Big(\pi, \sum\limits_{i=1}^M \wi \delta_{\yi}\Big).
\end{equation}
In other words, having an accurate quantization with respect to MMD leads to a highly accurate quadrature rule $\sum_{i =1}^{M} \wi \delta_{\yi}$ for functions living in the RKHS. Within this framework, the design of the quadrature rule remains independent of the function $f$, which is particularly advantageous in scenarios where evaluating $f$ is expensive. 



The \textit{optimal kernel-based quadrature} is a popular and well-studied family of quadrature rules: for a given configuration of nodes $\Y$, the weights $\hwone(\Y),\ldots,\hwm(\Y)$ satisfy 
\begin{equation}\label{eq:MMD_optimality_okq}
     \mathrm{MMD} \left( \pi, \sum\limits_{i=1}^M \hwi(\Y) \delta_{\yi}  \right) \leq \mathrm{MMD} \left( \pi, \sum\limits_{i=1}^M \wi \delta_{\yi} \right),
\end{equation}
for any $\wone, \dots, \wm \in \mathbb{R}$. These optimal weights can be expressed as the entries of the vector 
\begin{equation}\label{eq:optimal_w}
    \hat{\bm{w}}(\Y) = \bm{K}(\Y)^{-1} \vzerov(\Y),
\end{equation}
where $\vzero: \mathcal{X} \rightarrow \mathbb{R}$ is the \emph{kernel mean embedding} of the probability measure $\pi$ defined by 
\begin{equation}\label{eq:mke_def}
    \vzero(y):= \int_{\mathcal{X}} \kappa(x,y) \dpix.
\end{equation}
While the optimal weights $\hat{\bm{w}}(\Y)$ have an analytical formula for fixed configuration $\Y$, the function $\Y \mapsto \MMD^2\left(\pi, \sum_{i =1}^M \hwi(\Y) \delta_{\yi} \right)$
is daunting to minimize due to its non-convexity.
Recent work on optimal configurations often propose approaches tailored to specific classical probability measures (Gaussian measure, uniform measure on the hypercube, ...) \cite{KaSa18,KaSa19,EhGrOa19}. %
Although there are some methods designed to be universal, their numerical implementation is generally limited by difficulties in high-dimensional domains. Examples include ridge leverage score sampling \cite{Bac17}, determinantal point processes and volume sampling \cite{BeBaCh19, BeBaCh20,Bel21}, Fekete points \cite{KaSaTa21}, randomly pivoted Cholesky \cite{EpMo23}, or greedy selection algorithms \cite{De03,DeScWe05,SaHa16,Oet17,HuDu12,BrOaGiOs15,LaLiBa15}.

The function \eqref{eq:mke_def} is only practically tractable for some kernels and probability measures. One such case is when $\pi$ is an empirical measure. In this case, the quantization can be seen as a summarization of\footnote{Note that in general, the atoms of $\pi$ are not necessarily i.i.d. samples from a distribution.} $\pi$, notably relevant for Bayesian inference \cite{Owe17,RiChCoSwNiMaOa22}. In this regard, many approaches were proposed in the literature: recombination \cite{HaObLy22, HaObLy23}, kernel thinning \cite{DwMa21,DwMa24} and approximate ridge leverage score sampling \cite{ChScDeRo23}. This line of work is largely concerned with convergence rates in $M$, but the proposed methods are often outperformed in practice by simple algorithms (e.g., greedy methods) for small $M$.



























\subsection{Clustering}
Lloyd's algorithm is a standard method for quantization \cite{Llo82}, which aims to solve the problem
\begin{equation}\label{eq:w_2_quantization_problem}
    \min\limits_{\Y \in \mathcal{X}^{M}} \min\limits_{\bm{w} \in \Delta_{M-1}} W_{2}^{2}\Big(\pi, \sum\limits_{i = 1}^{M} \wi \delta_{\yi} \Big),
\end{equation}
where $W_{2}$ is the $2$-Wasserstein distance \cite{PeCu19}.
The solution of \eqref{eq:w_2_quantization_problem} is naturally expressed in terms of \emph{Voronoi tessellations} \cite{DuFaGu99}. The Voronoi tesselation of a configuration $\Y \in \mathcal{X}^{M}$ corresponds to the sets $\mathcal{V}_{1}(\Y),\, \dots \,, \,\mathcal{V}_{M}(\Y)$ defined by 
\begin{equation}
\mathcal{V}_{i}(Y) := \left\{\,x\in\mathcal{X}\ \Big|\ i = \arg\min_{m\in[M]} \|x - y_m\|\,\right\}.
\end{equation}
Given a mixture of Diracs $\sum_{i = 1}^{M} \wi \delta_{\yi}$, the optimal transport plan from $\pi$ to $\sum_{i=1}^M \wi \,\delta_{\yi}$ maps all mass in the region $\V_i(\Y)$ to the atom located at $\yi$, and the total transportation cost can be written as $\sum_{i=1}^M \int_{\mathcal{V}_i(\Y)}\|x- \yi\|^2 \dpix$. Lloyd's algorithm is a fixed-point iteration aiming to minimize this cost jointly over the configuration $\Y$ and the vector of weights $\bm{w}$ constrained to $\Delta_{M-1}$. More precisely, let $\yt$ be an iterate of the algorithm and $\bm{\Psi}_{\mathrm{Lloyd}}: \mathcal{X}^{M} \rightarrow \mathcal{X}^{M}$ be the \emph{Voronoi barycentric map} defined by 
\begin{equation}\label{eq:lloyd_map}
\forall i \in [M], \:\:  \left(\bm{\Psi}_{\mathrm{Lloyd}}(\Y)\right)_{i, :} := \frac{\int_{\V_{i}(\Y)}x \mathrm{d} \pi(x)}{\int_{\V_{i}(\Y)}\mathrm{d} \pi(x)} .
\end{equation}
The algorithm then consists of taking the fixed-point iteration $\ytplus = \bm{\Psi}_{\mathrm{Lloyd}}(\yt)$. When $\Y \notin \mathcal{X}_{\neq}^{M}$, the configuration of points is degenerate, i.e., the Voronoi tesselation is not uniquely defined, making the evaluation of $\bm{\Psi}_{\mathrm{Lloyd}}(\Y)$ ambiguous. However, $\bm{\Psi}_{\mathrm{Lloyd}}$ preserves nondegeneracy in the sense that, if $\Y \in \mathcal{X}_{\neq}^{M}$ then $\bm{\Psi}_{\mathrm{Lloyd}}(\Y) \in \mathcal{X}_{\neq}^{M}$, ensuring that the fixed-point iteration is closed \cite{EmJuRa08}. Interestingly, the fixed points of $\bm{\Psi}_{\mathrm{Lloyd}}$ are critical points of the function $G_M$, defined as \citep[Proposition 6.2.]{DuFaGu99}
\begin{equation}
  G_{M}: \Y \mapsto \min\limits_{\bm{w} \in \Delta_{M-1}} W_{2}^{2}\Big(\pi, \sum\limits_{i \in [M]} \wi \delta_{\yi} \Big).
\end{equation}
The authors prove that $G_{M}$ is differentiable in $\Y$ with the gradient given by
\begin{equation}\label{eq:gradient_G_M_lloyd}
    \nabla G_{M}(\Y) = \bm{D}(\Y) (\Y - \bm{\Psi}_{\mathrm{Lloyd}}(\Y)),
\end{equation}
where $\bm{D}(\Y) \in \mathbb{R}^{M \times M}$ is a diagonal matrix with $(\bm{D}(\Y))_{i,i} := \int_{\V_{i}(\Y)} \dpix$. Hence, the fixed-point iteration can be rewritten as a preconditioned gradient descent
\begin{equation}
    \ytplus = \yt - \bm{D}(\yt)^{-1} \nabla G_{M}(\yt).
\end{equation}




The theoretical analysis of Lloyd's algorithm has been the subject of extensive study \cite{Kie82,Wu92,DuFaGu99,DuEmJu06,EmJuRa08,PaYu16,PoCaPa24}.  




\subsection{Mean Shift}\label{sec:mean_shift}
Mean shift is a widely used algorithm for tasks such as clustering \cite{FuHo75,Car15} and image segmentation \cite{CoMe02}. The algorithm is a non-parametric method for locating the mode of a kernel density estimator (KDE) defined by 
\begin{equation}
\hat{f}_{\mathrm{KDE}}(y):= \frac{1}{N} \sum\limits_{\ell=1}^{N}\kappa(y,\xell) = \vzero(y),
\end{equation}
where $\vzero$ is defined by \eqref{eq:mke_def} taking $\pi$ to be the empirical measure associated to the data points $\xone, \dots, \xn  \in \mathcal{X}$. In this context, the kernel $\kappa$ must satisfy the following condition. 
\begin{assumption}\label{assumption:gradient_kappa} The kernel $\kappa$ is differentiable, and there exists a symmetric kernel $\bar{\kappa}: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ satisfying
\begin{equation}\label{eq:gradient_condition_msip}
\forall x,y \in \mathcal{X}, \quad \nabla_{2}\kappa(x,y) = (x-y) \bar{\kappa}(x,y).
\end{equation}
\end{assumption}
Under this assumption, a critical point $y_{\star}$ of the KDE satisfies $\sum_{\ell=1}^N (y_{\star}-\xell)\bar{\kappa}(\xell,y_{\star}) = 0 $, which implies
\begin{equation}\label{eq:ms_equation}
    y_{\star} = \bm{\Psi}_{\mathrm{MS}}(y_{\star}):= \frac{\sum_{\ell=1}^N \xell \, \bar{\kappa}(\xell,y_{\star})}{\sum_{\ell=1}^N \bar{\kappa}(\xell,y_{\star})}.
\end{equation}
The mean shift algorithm seeks the location of $y_{\star}$ using a fixed-point iteration on the map $\bm{\Psi}_{\mathrm{MS}}$. The convergence of the algorithm was extensively investigated in \cite{Che95,LiHuWu07,Car07,Gha13,Gha15,ArMaPe16,YaTa19,YaTa24}. \Cref{assumption:gradient_kappa} is satisfied by many distance-based kernels defined as $\kappa(x,y) = \varphi(\|x-y\|)$ for some function $\varphi:\mathbb{R}^+\to\mathbb{R}^+$ that is continuously differentiable almost everywhere. This includes the Gaussian kernel, the inverse multiquadric kernel (IMQ) and Mat√©rn kernels.

\subsection{Gradient flow methods for MMD minimization}


A gradient flow of a functional $\mathcal{F}$ on the space of positive measures $\mathcal{M}_{+}(\mathcal{X})$ is a trajectory $\mu_{t}$ satisfying the partial differential equation (PDE) 
\begin{equation}\label{eq:abstract_gradient_flow}
   \dmut = -\mathrm{grad}_d \mathcal{F}[\mut],
\end{equation}
where $\mathrm{grad}_d \mathcal{F}$ denotes the gradient of $ \mathcal{F}$ with respect to a metric $d: \mathcal{M}_{+}(\mathcal{X}) \times \mathcal{M}_{+}(\mathcal{X}) \rightarrow \mathbb{R}$%
; see \cite{Ott01,AmGiSa08} for details. In practice, the solution of \eqref{eq:abstract_gradient_flow} can be approximated using numerical schemes such as time-splitting \citep{JoKiOt98}. 

In the following, we review existing constructions of gradient flows for minimizing the MMD. We highlight how the choice of metric $d$, and thus the underlying geometry, differentiates these approaches for minimizing the functional 
\begin{equation}\label{def:mmd}
    \F(\mu) := \frac{1}{2} \mathrm{MMD}(\mu, \pi)^2.
\end{equation}
When the metric\footnote{Not to be confused with when $\F$ is the $\MMD$.} $d$ corresponds to the $\MMD$, the equation \eqref{eq:abstract_gradient_flow} simplifies to $\dmut = \mu_{t} - \pi$, whose solution is given by $\mut = e^{-t} \mu_{0} + (1-e^{-t}) \pi$, where $\mu_{0}$ is the initial condition \citep[Theorem 3.4]{GlDvMiZh24}. %
This solution is not suitable for the quantization problem we consider, as $\pi$ generally cannot be expressed as a mixture of $M$ Diracs.




    

\citet{ArKoSaGr19} studied the gradient flow of $\F$ under the 2-Wasserstein ($W_2$) geometry, corresponding to the solution of
\begin{equation}\label{eq:MMD_gf_equation}
    \dmut = \mathrm{div} \left( \mut\nabla \Ffv [\mut] \right),
\end{equation}
where $\Ffv[\mu]$ is the first variation of $\F$ at $\mu$ given by
\begin{equation}\label{eq:MMD_first_variation}
\Ffv[\mu](\cdot) = \int_{\X} \kappa(\cdot,x)\,\mathrm{d}\mu(x) - \int_{\X} \kappa(\cdot,x)\dpix.
\end{equation}

The authors propose various numerical solutions to \eqref{eq:MMD_gf_equation} based on discretizations in time and space. In particular, they replace the measures $\mut$ by empirical counterparts, yielding a system of interacting particles. Moreover, they derive conditions for the convergence of their numerical schemes, and study the importance of \emph{noise injection} as a regularization method.
However, the approaches deteriorate dramatically when initialized poorly, requiring a large number of particles to converge. Further, rigidly constraining particles to have uniform weights is problematic; this can be seen for $M=1$ when comparing gradient descent on the $\MMD$ to mean shift, which we discuss in \Cref{sec:msip}.

An alternative geometry called \emph{spherical interaction-force transport} (IFT) is proposed and studied in \cite{GlDvMiZh24,ZhMi24}. Inheriting properties of both the $W_2$ and MMD geometries, the IFT gradient flow satisfies
\begin{equation}\label{eq:ift_gf_equation}
     \dmut = \alpha\ \mathrm{div} \left( \mu_{t} \nabla \Ffv [\mut] \right) - \beta(\mut - \pi),
\end{equation}
where $\alpha, \beta\in\RR$ are positive. The authors prove that if $\mut$ solves \eqref{eq:ift_gf_equation}, then $\F[\mut]$ decays exponentially with $t$ \citep[Theorem 3.5]{GlDvMiZh24}.  While this is a remarkable theoretical result, it only holds in the mean-field limit $M \rightarrow +\infty$. They discretize  \eqref{eq:ift_gf_equation} using a system of weighted interacting particles; as we show in \Cref{sec:numerics}, many of these particles tend to drift far away from the modes of $\pi$, making the resulting quantization inefficient for, e.g., quadrature. %
Further, the proposed implementation is complicated by an inner $M$-dimensional constrained optimization problem at every iteration.













Finally, the Wasserstein-Fisher‚ÄìRao (WFR) geometry %
allows both mass transport and total mass variation. The PDE corresponding to the WFR metric is
\begin{equation}\label{eq:WFR_for_MMD}
    \dmut = \alpha\,\mathrm{div}(\mut \nabla \Ffv[\mut]) - \beta\,\Ffv[\mut] \mut.
\end{equation}
This corresponds to a gradient flow of the functional $\F$%
, extending the $W_2$ distance to measures with different total masses, i.e., \emph{unbalanced} optimal transport. See \cite{KoMoVo16,GaMo17,ChPeScVi18,LiMiSa18} for details about the WFR metric, its geometric properties, and the construction of WFR gradient flows. %
Additionally, \citep{LuLuNo19,LuSlWa23,YaWaRi24} provide theoretical and numerical results for the minimization of a kernelized KL divergence using the WFR geometry.

Compared to \eqref{eq:MMD_gf_equation}, equation \eqref{eq:WFR_for_MMD} introduces $\Ffv[\mu_t]\mu_t$ as a reaction term, which allows the total mass of $\mu_t$ to change as a function of $t$. %
Unbalanced transport allows for this creation or destruction of mass, which might be helpful when mass conservation is not a natural assumption. Despite this formulation, the numerical scheme proposed by \cite{GlDvMiZh24} to solve \eqref{eq:WFR_for_MMD} artificially conserves mass by projecting the weights onto the simplex; this is inconsistent with the PDE's theoretical properties.%
