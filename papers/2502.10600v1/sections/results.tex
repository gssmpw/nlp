In the following, we summarize the results of numerical simulations that validate the proposed algorithms.
In \Cref{sec:synthetic_basic_numerics} we conduct numerical experiments on synthetic Gaussian mixture datasets, while in \Cref{sec:real_datasets_basic_numerics}, we perform the experiments on MNIST. All experiments\footnote{Code used for experiments is freely available at \href{https://github.com/AyoubBelhadji/disruptive_quantization}{github.com/AyoubBelhadji/disruptive\_quantization}} were performed in Python using the Numba library~\citep{LaPiSe15} using MIT SuperCloud resources~\citep{ReKeBy18}.


\begin{figure}[H]
    \centering
     \begin{subfigure}[t]{0.78\linewidth}
         \centering
         \includegraphics[width=0.49\linewidth]{figs/gmm/wfr_init_data.png}
        \hfill
        \includegraphics[width=0.49\textwidth]{figs/gmm/wfr_init_far.png}
         \caption{WFR}
     \end{subfigure}
         
     
     \begin{subfigure}[t]{0.78\linewidth}
         \centering
         \includegraphics[width=0.49\textwidth]{figs/gmm/msip_init_data.png}
         \hfill
         \includegraphics[width=0.49\textwidth]{figs/gmm/msip_init_far.png}
         \caption{MSIP}
     \end{subfigure}

     \begin{subfigure}[t]{0.39\linewidth}
         \includegraphics[width=0.989\textwidth]{figs/gmm/iftflow_traj.png}
        \caption{IFTFlow}
     \end{subfigure}
     \begin{subfigure}[t]{0.39\linewidth}
         \includegraphics[width=0.989\textwidth]{figs/gmm/mmdgf_traj.png}
        \caption{MMDGF}
     \end{subfigure}
        \caption{Trajectories of four algorithms started at two different intializations (yellow and red symbols). Each marker is one iteration for a particle, and the lines show  particle paths. White markers are the final particle positions.}
    \label{fig:all_trajectories}
\end{figure}

\clearpage
\subsection{Experiment 1: a synthetic dataset}\label{sec:synthetic_basic_numerics}
In this section, we compare various algorithms of MMD minimization on synthetic datasets, for which the construction is detailed in \Cref{sec:settings_numerics_GMM}.

    



We start with a qualitative comparison of the following algorithms: i) Wasserstein-Fisher-Rao Flow for MMD (WFR) introduced in \Cref{sec:wfr}, ii) mean shift interacting particles (MSIP) introduced in \Cref{sec:msip}, iii) IFTflow introduced in \cite{GlDvMiZh24}, iv) MMD gradient flow (MMDGF) introduced in \cite{ArKoSaGr19}. \Cref{fig:all_trajectories} compares the dynamics of the four algorithms for the first $1000$ iterations. 
For WFR and MSIP, we display in separate plots the dynamics when matching three nodes to a mixture of three Gaussians with two different initializations: random elements of the dataset (left), and points far from the support of $\pi$ (right). For each of IFTFlow and MMDGF, we demonstrate both scenarios in the same plot. The lines indicate particle trajectories, where markers overlapping indicate slower dynamics. In particular, we see WFR performing well in both scenarios with sufficient compute, where MSIP is significantly faster; 
while MSIP produces erratic trajectories and might overshoot the support, it converges at the end.
In contrast, both MMDGF and IFTFlow succeed when starting from the support, but perform poorly otherwise.


When comparing the algorithms quantitatively, we evaluate six algorithms: i) $k$-means, ii) MMDGF, iii) IFTflow, iv) WFR, v) MSIP, vi) plain gradient descent on the function $F_{M}$ defined by \eqref{eq:opt_mmd_functional}, which we refer to as discrepancy minimizing gradient descent (DMGD). 

We present in \Cref{fig:mmd_comparison_gmm} the MMD as a function of the iteration for the measures obtained from running each algorithm on 100 initializations. %
For each algorithm iteration, we plot the highest and lowest $\MMD$ value across all 100 initializations. As a benchmark, we use the square root of the $(M+1)$-st eigenvalue of the integration operator associated with the kernel $\kappa$ and the probability measure $\pi$, divided by $\|\vzerov\|_{\mathcal{H}}$. The eigenvalues of this operator are classically used in measuring the complexity of numerical integration in an RKHS; see \cite{Pin12,Bac17}.




We observe that, even in the worst case for $d=2$, MSIP consistently outperforms other quantization algorithms, where it quickly converges to a near-optimal quantization under the $\MMD$. Similarly, WFR slowly descends in the MMD, surpassing $k$-means given enough iterations. In the best case, we see high performance from all the algorithms except for MMDGF. When $d=100$, we have a more difficult approximation problem, where MSIP is still robust to initialization and WFR performs comparably to $k$-means in the worst case and MSIP in the best case. Other algorithms struggle in this high-dimensional problem.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/gmm/gmm2_mmd.pdf}
    \includegraphics[width=0.98\linewidth]{figs/gmm100/gmm100_mmd.pdf}
    \caption{Comparison of different quantization algorithms on a GMM. (Top): dimension $d=2$, $M_0 = M = 3$. (Bottom): dimension $d=100$, $M_0=5$, $M=10$. We use bandwidth $\sigma=5$ for a squared-exponential kernel for all kernel-based algorithms and tried to provide hyperparameter tuning for each algorithm.}
    \label{fig:mmd_comparison_gmm}
\end{figure}






\begin{figure}[b!]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/mnist/main_paper_results/T_5000.pdf}
    





    

    \caption{Comparison of different algorithms on MNIST. }
    \label{fig:mnist_comparison}
\end{figure}






















\subsection{Experience 2: MNIST} \label{sec:real_datasets_basic_numerics}

We now illustrate using the MNIST dataset qualitatively; for a quantitative results, see \Cref{quant:mnist}. We compare MSIP, Lloyd's algorithm, WFR, IFTflow, MMDGF, DMGD, and mean shift without interaction (IIDMS). More precisely, IIDMS refers to the mean shift algorithm without interactions initialized with i.i.d. samples. 
When using Lloyd's algorithm with an empty Voronoi cell, we make the corresponding particle retain its position.

\Cref{fig:mnist_comparison} shows the results of $5000$ iterations for each algorithm, all initialized with the same set of $M$ i.i.d. samples drawn from the uniform distribution on the hypercube $[0,1]^{784}$. We use a Mat√©rn kernel with a bandwidth of $\sigma = 2.25$ and smoothness of $\nu = 1.5$. We see that MSIP and WFR recover recognizable digits. If we examine the  MSIP and WFR results, each output has two particles that look like ones: these image pairs correspond to different modes of MNIST with a separating distance larger than $\sigma$.
While IIDMS converges to a mode, it lacks the diversity of MSIP. On the other hand, Lloyd's algorithm has problems effectively using all centroids, as many particles have an empty Voronoi cell across all iterations\footnote{Lloyd's algorithm is typically initialized from the data distribution itself to mitigate this phenomenon.}; 
remaining particles are extended between many digits. The other three algorithms remain trapped far from the distribution's support. 
This experiment illustrates the
capacity of the proposed algorithms to detect diverse modes for a multi-modal distribution. Our algorithms show remarkable robustness to initialization, even in high-dimensional domains. 













