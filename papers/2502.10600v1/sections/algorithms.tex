


Based on the elements reviewed in \Cref{sec:related_work}, we contribute quantization schemes tailored for small values of $M$. This section is structured as follows: %
In \Cref{sec:wfr}, we propose simulating the WFR gradient flow to minimize the $\MMD$ using a weighted mixture of $M$ Diracs. In \Cref{sec:msip}, we use a sufficient condition for the WFR gradient flow's steady state to derive a fixed-point iteration. 
Moreover, we demonstrate that this iteration can be expressed as a preconditioned gradient descent for a function related to the $\MMD$. Further, the iteration is a natural extension of the mean shift algorithm.


\subsection{Simulating the WFR gradient flow using ODEs}\label{sec:wfr}
Without loss of generality on the time-scale, we consider \eqref{eq:WFR_for_MMD} with a reaction speed $\beta\equiv 1.$
We seek $\mut$ as a mixture of $M$ Diracs, i.e.,
\(
    \mut = \sum_{i = 1}^{M} \wit \delta_{\yit},
\)
where $\yonet, \dots, \ymt \in \mathcal{X} $ and $\wonet, \dots, \wmt \in \RR$. 


\begin{proposition}\label{prop:coupled_ode_WFR}
Define the system of differential equations
\begin{equation}\label{eq:WFR_particle_equation}
\begin{aligned}
    \dotyit &= - \alpha\nabla\Ffv[\mu_t](\yit) =-\alpha\left(\sum\limits_{m = 1}^{M} \wit \nabla_2 \kappa(\yjt, \yit) - \nabla v_{0}(\yit)\right)  \\
     \dotwit &= -  \wit\Ffv[\mut](\yit) =- \wit \left(\sum\limits_{m = 1}^{M} \wit \kappa(\yjt, \yit) - v_{0}(\yit) \right)
\end{aligned}
\end{equation}
where $i \in [M]$, and  $\vzero$ as defined in \eqref{eq:mke_def}.
If the trajectory $(\mut)_{t \geq 0}$ satisfies \eqref{eq:WFR_particle_equation}, then $(\mut)_{t \geq 0}$ weakly satisfies \eqref{eq:WFR_for_MMD}.
\end{proposition}


The proof and a more detailed account of \Cref{prop:coupled_ode_WFR} are given in \Cref{proof:coupled_ode_WFR}.
We can use standard ODE solvers to efficiently approximate $\mut$ provided the function $\vzero$ can be evaluated. For instance, when $\pi$ is an empirical measure of $N$ atoms in $\RR^d$, the system of equations \eqref{eq:WFR_particle_equation} can be implemented in 
$O(d(M+N)M)$ operations\footnote{This assumes that evaluating $\kappa$ is $O(1)$ and $\nabla_2\kappa$ is $O(d)$.}.


Informally, if $\wizero > 0$, we are ensured that $\wit \geq 0$ for sufficiently small $t$ %
by the structure of \eqref{eq:WFR_particle_equation}. To understand this, we consider the sign of the term $\Ffv[\mu_t](\yit)$: If this term is negative, then $\wit$ will increase locally in $t$, drifting away from $0$. However, if the term is positive, the weight $\wit$ locally decreases toward a steady state $\wit\equiv 0$. This argument only holds for operator $\Ffv$ that is continuously differentiable in $\mut$.













\subsection{A fixed-point scheme for steady-state solutions}\label{sec:msip}










While the solution to \eqref{eq:WFR_particle_equation} is an entire trajectory $(\mut)_{t\geq 0}$, we are primarily interested in $\mu_{\infty}$; if it exists, it should be a minimizer to the functional $\mathcal{F}$. To find $\mu_\infty$, we look for a steady state solution satisfying $\dotyit\equiv 0$ and $\dotwit\equiv 0$, which is ensured by the sufficient condition for all $i \in [M]$
\begin{align}
       \sum\limits_{m = 1}^{M} \wms \kappa(\yms,\yi) & = \int_{\mathcal{X}} \kappa(x,\yi) \dpix \label{eq:mixture_diracs_kbar_eq_1}\\
       \sum\limits_{m = 1}^{M} \wms \nabla_{2} \kappa(\yms,\yi) & = \int_{\mathcal{X}} \nabla_{2} \kappa(x,\yi) \dpix. \label{eq:MMD_gradient_mixture}
\end{align}

The condition~\eqref{eq:mixture_diracs_kbar_eq_1} is prevalent in the literature of kernel-based quadrature, and reflects that the quadrature rule defined by the measure $\mu = \sum_{m = 1 }^{M} \wms \delta_{\yms}$ is exact on the subspace spanned by the functions $\kappa(\cdot,\yi)$. Note that equation 
\eqref{eq:mixture_diracs_kbar_eq_1} enforces optimal kernel quadrature weights $    \hat{\bm{w}}(\Y) = \bm{K}(\Y)^{-1} \vzerov(\Y)$, as in~\eqref{eq:optimal_w}, and the resulting quadrature rule thus satisfies the optimality property \eqref{eq:MMD_optimality_okq}. 
Now, we explore the second condition~\eqref{eq:MMD_gradient_mixture}, less known in the literature, when making \Cref{assumption:gradient_kappa}.
\begin{proposition}\label{prop:steady_state_equation_on_y}
Let $\y \in \mathcal{X}^{M}$. Under \Cref{assumption:gradient_kappa} and assuming that the gradient of $\kappa$ is bounded on $\X$, the equations \eqref{eq:mixture_diracs_kbar_eq_1} and \eqref{eq:MMD_gradient_mixture} imply that 
\begin{equation}\label{eq:mixture_diracs_kbar_eq_2_reformulation}
     \bm{\bar{K}}(\y)\bm{W}(\y)\y  = \bvonem(\y) +  \bar{\bm{u}}(\y),
\end{equation}
where $\bm{\bar{K}}(\y) \in \RR^{M \times M}$ is the kernel matrix with the kernel $\bar{\kappa}$, diagonal matrix $\bm{W}(\y)$ has entries corresponding to those of $\hw(\y)$ given by \eqref{eq:optimal_w}, and
$\bvonem(\y), \bar{\bm{u}}(\y) \in \RR^{M \times d }$ are the matrices with rows defined as
\begin{align}\label{eq:u_vonebar_def}
(\bvonem(\y))_{i,:} &:=  \int_{\mathcal{X}} x \bar{\kappa}(x,\yi)\dpix,\\
(\bar{\bm{u}}(\y))_{i,:} &:= \Big(\sum\limits_{m=1}^M \wms\bar{\kappa}(\yms,\yi) - \int_{\mathcal{X}} \bar{\kappa}(x,\yi)\dpix\Big)\yi.\nonumber
\end{align}
\end{proposition}



The equation \eqref{eq:mixture_diracs_kbar_eq_2_reformulation} does not admit a tractable closed-form solution for $\y$. We propose a sequence $(\yt)_{t}$ satisfying
\begin{equation}\label{eq:msip_fixed_point_def}
     \bm{\bar{K}}(\yt) \bm{W}(\yt) \ytplus = \hvonem(\yt),
\end{equation}
where 
\begin{equation}\label{eq:v_hat_def}
    \hvonem(\y):=  \bvonem(\y) +  \bar{\bm{u}}(\y).
\end{equation}
Under the assumption that the kernel matrices $\bm{K}(\yt)$ and $\bm{\bar{K}}(\yt)$ are non-singular, the relationship \eqref{eq:msip_fixed_point_def} is expressed as a fixed-point iteration $\ytplus =  \bm{\Psi}_{\MSIP}(\yt)$, where
\begin{equation}\label{eq:fixed_point_general_case}
    \bm{\Psi}_{\MSIP}(\Y): = \bm{W}(\Y)^{-1} \bm{\bar{K}}(\Y)^{-1} \hvonem(\Y).
\end{equation}

We call $\bm{\Psi}_{\MSIP}$ the \textit{mean shift interacting particles} map. When $\kappa$ is the squared exponential kernel, there exists a scalar $\lambda \in \mathbb{R}$ such that $\bar{\kappa} = \lambda \kappa$; then, the condition~\eqref{eq:MMD_optimality_okq} implies $\bar{\bm{u}}(\y) = 0$, and \eqref{eq:fixed_point_general_case} simplifies to 
\begin{equation}
    \ytplus = \bm{W}(\yt)^{-1} \bm{K}(\yt)^{-1} \vonem(\yt),
\end{equation}
where $(\vonem(\y))_{i,:} :=  \int_{\mathcal{X}} x \kappa(\yi,x)\dpix$.
The proof of \Cref{prop:steady_state_equation_on_y} is given in \Cref{proof:steady_state_equation_on_y}.
 






\paragraph{Damped fixed-point iteration}
In the following, we prove that the fixed-point algorithm defined using \eqref{eq:fixed_point_general_case} can be seen as a preconditioned gradient descent iteration on the function 
$F_M: \mathcal{X}_{\neq}^{M} \rightarrow \mathbb{R}$ given by
\begin{equation}\label{eq:opt_mmd_functional}
        F_{M}(\yone, \dots, \ym) := \inf\limits_{\w \in \mathbb{R}^{M}} \F \left[ \sum\limits_{i=1}^{M}\wi \delta_{\yi} \right].
    \end{equation}
We start with the following result.
\begin{theorem}\label{thm:gradient_opt_mmd}
    Under \Cref{assumption:gradient_kappa}, the gradient of $F$, seen as an $M \times d$ matrix, is given by 
\begin{align}\label{eq:gradien_opt_mmd_under_assumption}
\nabla F_{M}(\y) & =  \bm{W}(\y)\left(\bm{\bar{K}}(\y)\bm{W}(\y)\y - \hat{\bm{v}}_1(\y) \right)\\
&= \bm{W}(\y) \bm{\bar{K}}(\y) \bm{W}(\y) \Big( \y - \bm{\Psi}_{\MMS}(\y) \Big), \nonumber
\end{align}
with $\bm{\Psi}_{\MMS}(\y)$ is defined in \eqref{eq:fixed_point_general_case}, where the latter equality holds when $\bm{W}(\y)\bm{\bar{K}}(\Y)\bm{W}(\y)$ is invertible.
In particular, for the Gaussian kernel, the expression simplifies to 
\begin{equation*}\label{eq:gradien_opt_mmd_gaussian_kernel}
  \nabla F_{M}(\y) =  \bm{W}(\y) \bm{K}(\y) \bm{W}(\y) \Big( \y - \bm{\Psi}_{\MMS}(\y) \Big).
\end{equation*}
\end{theorem}
The identity \eqref{eq:gradien_opt_mmd_under_assumption} proves that the fixed points of the function $\bm{\Psi}_{\MMS}$ are critical points of the function $F_M$. This result is reminiscent of \eqref{eq:gradient_G_M_lloyd} proved in \cite{DuFaGu99}, which gives a similar characterization of the fixed points of Lloyd's map $\bm{\Psi}_{\mathrm{Lloyd}}$. Following the same reasoning as in \cite{PoCaPa24}, 
we deduce from~\eqref{eq:gradien_opt_mmd_under_assumption} that the fixed point iteration defined by \eqref{eq:fixed_point_general_case} is a preconditioned gradient descent, with $\bm{M}(\y):=  \mathbf{W}(\y) \bm{\bar{K}}(\y) \mathbf{W}(\y)$ as the preconditioning matrix.
 Indeed, if the matrix $\bm{M}(\yt)$ is non-singular, combining \eqref{eq:fixed_point_general_case} and \eqref{eq:gradien_opt_mmd_under_assumption} yields
\begin{equation}\label{eq:fixed_point_as_precond_grad_descent}
  \ytplus   = \yt - \bm{M}(\yt)^{-1} \nabla F_{M}(\yt).
\end{equation}
Now, if we choose a step size $\eta$ for  \eqref{eq:fixed_point_as_precond_grad_descent}, we get 
\begin{equation}\label{eq:fixed_point_as_precond_grad_descent_alpha}
\begin{split}
  \ytplus &= \yt - \eta\bm{M}(\yt)^{-1} \nabla F_{M}(\yt)\\
  &= (1-\eta) \yt + \eta \bm{\Psi}_{\MMS}(\yt),
  \end{split}
\end{equation}
which is equivalent to a damped fixed-point iteration on $\bm{\Psi}_{\MMS}$ \cite{Ke18}. 

\paragraph{Mean shift through the lens of MMD minimization}
In the case $M = 1$, we see that the classical mean shift algorithm identifies critical points of the function $F_1$.
\begin{corollary}\label{cor:ms_as_MSIP} Under \Cref{assumption:gradient_kappa}, we have
\begin{equation}\label{eq:gradient_F_1}
    \nabla F_{1}(y) = \frac{v_{0}(y)}{\kappa(y,y)} \left(\bar{v}_{0}(y) y - \bar{v}_{1}(y) \right),
\end{equation}
where $\bar{v}_{0}(y):= \int_{\X} \bar{\kappa}(x,y) \dpix \in \mathbb{R}$. 
\end{corollary}
Thus, a critical point $y$ of $F_1$ is either a zero of $v_{0}$ or satisfies
\begin{equation}\label{eq:goq_for_K_1}
      y = \frac{\bar{v}_{1}(y)}{\bar{v}_{0}(y)} = \frac{\int_{\mathcal{X}} x \bar{\kappa}(x, y)  \dpix}{\int_{\mathcal{X}} \bar{\kappa}(x, y) \dpix},
\end{equation}
if the ratio is well-defined. 
In particular, when $\pi$ is taken to be an empirical measure $\pi = \frac{1}{N} \sum_{n=1}^{N} \delta_{\xns}$,
any point $y^\star$ that satisfies $\Psi_{\mathrm{MS}}(y^\star)\equiv y^\star$ as in \eqref{eq:ms_equation} is a critical point of $F_{1}$. Then, the mean shift algorithm becomes a special case of the fixed-point iteration defined in \Cref{sec:msip} when $M = 1$. 
The proof of \Cref{cor:ms_as_MSIP} is given in \Cref{proof:mean_shift_as_mmd_min} as a direct consequence of \Cref{thm:gradient_opt_mmd}.  

We now contrast \eqref{eq:goq_for_K_1} with an unweighted Wasserstein gradient flow for the MMD, as given in \citep{ArKoSaGr19}, when $M=1$. In particular, the path traced using the $W_2$ geometry will use the gradient of the first variation given by $\nabla \Ffv(y) = y \bar{v}_{0}(y) -  \bar{v}_{1}(y)$. When $y$ is far from the data, both $\bvzero(y)$ and $\bvonev(y)$ will vanish and thus the particle's speed vanishes along with them. On the other hand, mean shift ameliorates this through the inverse operation in \eqref{eq:goq_for_K_1}, magnifying the gradient considerably.





























