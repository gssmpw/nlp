






\section{Additional numerical results}\label{sec:settings_numerics}
In this section, we provide details on the numerical simulations presented in \Cref{sec:numerics}, along with additional simulations. 

\subsection{Implementation Details}
\begin{itemize}
    \item For WFR, we use a simple Runge-Kutta fourth order ODE solver for simulations. As this requires four evaluations of $\vzerov$ and $\bvonem$ for each iteration, we only run the ODE for one fourth of the iterations compared to the other algorithms. Outside of the experiment setting, we suggest that common adaptive ODE solvers should provide better results. We also choose the diffusion rate $\alpha$ according to na\"{i}ve hyperparameter tuning, where simple experiments provide good results for about $\alpha=25$. 
    \item For MSIP, we use the damped fixed-point with $\eta = 0.8$. Moreover, we use standard floating-point methods to ensure stability when working with terms that take extremely small values such as the functions $\vzerov$ and $\bvonem$, which we employ for the squared exponential kernel.  
    \item For MMDGF, we use a step-size of $10^{-1}$ and inject noise at each iteration via $\varepsilon_t\sim\mathcal{N}(0, \beta/\sqrt{t}),$ where $t$ is the iteration and $\beta=0.05$.
    \item For IFTFlow, we chose to implement the algorithm described in \citet[Appendix A]{GlDvMiZh24} with a regularization on the weight parameter of $10^{-3}$ and, similar to the authors, performing the $\MMD$ geometry minimization via a single step of projected gradient descent.
    \item We observed that the methods proposed in both \citet{ArKoSaGr19} and \citet{GlDvMiZh24} resample the target distributions; however, we assume a fixed target distribution in each simulation and thus assume that the same $N$ samples are available across all iterations of each algorithm.
\end{itemize}

\subsection{The generative model used in \texorpdfstring{\Cref{sec:synthetic_basic_numerics}}{Section \ref{sec:synthetic_basic_numerics}}}\label{sec:settings_numerics_GMM}
We consider the following setting, $\pi$ is the empirical measure associated to $N=1000$ i.i.d. samples from a mixture of Gaussians $\sum_{m =1}^{\overline{M}} \pi_{m} \mathcal{N}(c_{m}, \Sigma_{m}) $, where $\overline{M} \in \mathbb{N}^{*}$ is the number of components of the mixture, $c_1, \dots, c_{\overline{M}} \in \mathbb{R}^{d}$ are the means of the mixtures, and $\Sigma_{1}, \dots, \Sigma_{\overline{M}} \in \mathbb{S}^{++}_{d}$ are the covariance matrices, and $\pi_{1}, \dots, \pi_{\overline{M}}$ are the positive weights of the mixture which satisfy $\sum_{m = 1}^{\overline{M}} \pi_{m} =1$. For $d=2$ we take $\overline{M} = 3$, and for $d = 100$ we take $\overline{M} = 5$. \Cref{fig:gmm_marginals} illustrates some marginals of the $100$-dimensional distribution used in \Cref{sec:synthetic_basic_numerics}. The 100-dimensional distribution is marginally normalized to have covariance with ones on the diagonal.




\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\linewidth]{figs/gmm100/first_marginals_gmm100.pdf}

    \caption{First five marginals of the $100$-dimensional distribution used in \Cref{sec:synthetic_basic_numerics}}
    \label{fig:gmm_marginals}
\end{figure}







\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.35\linewidth]{figs/mnist/appendix_results/T_15000_2.pdf}
    





    

    \caption{Comparison of four algorithms on MNIST for the iteration $T=15000$. }
    \label{fig:mnist_comparison_2}
\end{figure}

\subsection{Additional results for the MNIST Simulation}
\subsubsection{The effect of additional iterations}
\Cref{fig:mnist_comparison_2} shows the results of the simulation detailed in \Cref{sec:real_datasets_basic_numerics} after $15000$ iterations for MSIP, WFR, Lloyd's algorithm, and IFTFlow. We observe that their recovered images are less blurry compared to the results after $5000$ iterations. Interestingly, one particle in the IFTFlow simulation reaches the support (and seems to visually correspond with the mode that IIDMS finds in \Cref{fig:mnist_comparison}). 

\subsubsection{Quantitative results for MNIST}\label{quant:mnist}
In \cref{fig:mnist_comparison_quant}, we calculate the $\MMD$ for each algorithm using the Mat\'{e}rn kernel with $\sigma= 2.25$ and $\nu = 1.5$. We use $10$ different initializations for the experiment and show the marginal median $\MMD$ at each iteration. Despite the fact that DMGD starts from a low level, the value of the MMD does not decrease over time for this algorithm. 
 MSIP and WFR are the only two algorithms that enjoy substantial dynamics in these simulations, shown by their decay over time.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/mnist/appendix_results/mnist_R10.pdf}
    \caption{Comparison of different algorithms on MNIST for the iteration $T=1000$. }
    \label{fig:mnist_comparison_quant}
\end{figure}




\clearpage
\section{Proofs}


\subsection{Proof of \texorpdfstring{\Cref{prop:coupled_ode_WFR}}{Theorem \ref{prop:coupled_ode_WFR}}} \label{proof:coupled_ode_WFR}







This is a ready extension to common proof techniques in, e.g., \citep{YaWaRi24} and \citep[Section 5.7.]{ChNiRi24}. Let $\varphi \in C_{c}^{\infty}$ be a test function. We have
\begin{align*}
    \frac{\mathrm{d}}{\mathrm{d}t} \int_{\mathbb{R}^{d}} \varphi(x) \mu_{t}(\mathrm{d}x) & = \frac{\mathrm{d}}{\mathrm{d}t} \left[ \sum\limits_{i = 1}^{M} \wit  \varphi(\yit) \right] \\
    & = \sum\limits_{i = 1}^{M} \frac{\mathrm{d}}{\mathrm{d}t} \wit  \varphi(\yit) + \sum\limits_{i = 1}^{M}\wit \frac{\mathrm{d}}{\mathrm{d}t} \varphi(\yit)   \\
    & = \sum\limits_{i = 1}^{M} \frac{\mathrm{d}}{\mathrm{d}t} \wit  \varphi(\yit) + \sum\limits_{i = 1}^{M}\wit \langle \nabla \varphi(\yit), \frac{\mathrm{d}}{\mathrm{d}t} \yit \rangle    \\
    & = - \sum\limits_{i = 1}^{M} \Ffv[\mu_t](\yit) \wit  \varphi(\yit) - \alpha\sum\limits_{i = 1}^{M} \wit \left\langle \nabla \varphi(\yit), \nabla \Ffv[\mu_t](\yit) \right\rangle    \\
    & = -  \int_{\mathbb{R}^{d}} \Ffv[\mu_t](x) \varphi(x)  \mu_{t}(\mathrm{d}x) -\alpha 
    \int_{
    \mathbb{R}^d} \left\langle \nabla \varphi(x), \nabla \Ffv[\mu_t](x) \right\rangle \mu_{t}(\mathrm{d}x)\\
    & = -  \int_{\mathbb{R}^{d}} \varphi(x) \Ffv[\mu_t](x)   \mu_{t}(\mathrm{d}x) + \alpha
    \int_{
    \mathbb{R}^d}  \varphi(x) \mathrm{div}(\mu_t \nabla \Ffv[\mu_t]  )(x) \mathrm{d}x.
\end{align*}

In other words, $(\mu_{t})_{t\geq 0}$ satisfies \eqref{eq:WFR_for_MMD} in the sense of distributions.



\subsection{Proof of \texorpdfstring{\Cref{prop:steady_state_equation_on_y}}{Theorem \ref{prop:steady_state_equation_on_y}}}\label{proof:steady_state_equation_on_y}

Under \Cref{assumption:gradient_kappa}, the equation \eqref{eq:MMD_gradient_mixture} writes for $j \in [M]$
\begin{equation}\label{eq:mixture_diracs_kbar_eq_2}
   \sum\limits_{i=1}^{M} w_{i} y_{i} \bar{\kappa}(y_i,y_j) = \Big(\sum\limits_{i=1}^{M} w_{i}  \bar{\kappa}(y_i,y_j) - \int_{\mathcal{X}} \bar{\kappa}(x,y_j) \mathrm{d}\pi(x) \Big)  y_j +    \int_{\mathcal{X}} x \bar{\kappa}(x,y_j) \mathrm{d}\pi(x),
\end{equation}
which can be expressed in matrix form as \eqref{eq:mixture_diracs_kbar_eq_2_reformulation}.



\subsection{Proof of \texorpdfstring{\Cref{thm:gradient_opt_mmd}}{Theorem \ref{thm:gradient_opt_mmd}}}\label{proof:thm_gradient_opt_mmd}

First, let $C_\pi := \int_{\X} \int_{\X} \kappa(x,x^{\prime}) \dpix \mathrm{d}\pi(x^{\prime})$. Then, observe that for $\Y\in \X_{\neq}^{M}$ we have
\begin{equation}
    F_{M}(\Y) = \frac{1}{2} \bigg( C_\pi + 2 \langle \what, \vzerov(\Y) \rangle + \langle \what, \Ky \what \rangle  \bigg),
\end{equation}
where $\hat{\bm{w}}(\Y)$ is defined by \eqref{eq:optimal_w}  respectively, and $\vzero$ is given by \eqref{eq:mke_def}.

Now, using \eqref{eq:optimal_w}, we prove that
\begin{equation}\label{eq:kernel_ids}
    \langle \hat{\bm{w}}(\Y), \vzerov(\Y) \rangle = \langle \vzerov(\Y), \bm{K}(\Y)^{-1} \vzerov(\Y) \rangle = \langle \hat{\bm{w}}(\Y), \bm{K}(\Y) \hat{\bm{w}}(\Y) \rangle.
\end{equation}
In particular, the expression of $F_{M}(\Y)$ simplifies further to 
\begin{align*}
    F_{M}(\Y) & = \frac{1}{2} \bigg(C_\pi - \langle \what, \vzerov(\Y) \rangle \bigg) = \frac{1}{2} \bigg( C_\pi - \langle \what, \bm{K}(\Y) \what \rangle \bigg).
\end{align*}


In the following, we proceed to the calculation of the gradient of $F_{M}$. First, for a configuration $Y\in\X^M$, we define $y_{ij}$ as the scalar in the $j$th dimension of node $\yi$. Now, we seek the gradient $\nabla F_{M}\in\RR^{M\times d}$. In particular, for $i \in [M]$ and $j \in [d]$, we use $\nabla_{ij} F_{M}(\Y)$ to denote the $(i,j)$ entry of $\nabla F_{M}(Y)$ corresponding to input $\yij$ evaluated at $\Y\in\mathbb{R}^{M \times d}$. 
Moreover, we define $\nabla_{ij} \Ky\in\RR^{M\times M}$  to be the differentiation of each element of matrix $\bm{K}(\Y)$ with respect to the $j$th coordinate of vector $\yi$. Similarly, for kernel mean embedding $\vzerov: \mathcal{X}_{\neq}^{M} \rightarrow \mathbb{R}^{M}$ we denote by $\nabla_{ij} \vzerov$ the vector obtained by differentiating each element of $\vzerov$ with respect to the $j$th coordinate of vector $\yi$. In other words, we have
\begin{equation}
    \Dij F_{M} = \frac{\partial}{\partial \yij} F_{M}(\Y),\quad \Dij \bm{K}(\Y) = \frac{\partial}{\partial \yij} \Ky , \quad \Dij \bm{v}(\Y) = \frac{\partial}{\partial \yij} \bm{v}(\Y)
\end{equation}



Using \eqref{eq:kernel_ids} and matrix calculus, we have 
\begin{equation}\label{eq:gradient_optimal_mmd_identity}
    2\Dij F_{M}(\Y) = 2\langle \what,\,\Ky\Dij\what\rangle + \langle \what,\,\Dij\Ky\,\what \rangle .
\end{equation}
Since $\what = \Ky^{-1}\vzerov(\Y) $, we get
\begin{gather}
\Dij \what = \Ky^{-1}\Dij \vzerov(\Y) -\Ky^{-1}\,\Dij\Ky\,\Ky^{-1}\vzerov(\Y),\\
\Ky\Dij\what = \Dij\vzerov(\Y) - \Dij\Ky\,\what
\end{gather}
so that
\begin{equation}
    \langle\what,\ \Ky \nabla_{ij} \what\rangle = \langle\what,\ \nabla_{ij} \vzerov(\Y)\rangle -\langle \what,\ (\nabla_{ij}\Ky)\what\rangle.
\end{equation}
Therefore, we have
\begin{gather}\label{eq:grad_F_pi_first}
\begin{split}
2\nabla_{ij} F_{M}(\Y) &= \langle \what,\, (\Dij\Ky)\,\what\rangle +\\
&\qquad +\,2\left(\,\langle\what,\, \Dij\bm{v}_0(\Y)\rangle - \langle\what,\,(\Dij\Ky)\,\what\rangle\,\right)\\
&= 2\langle \what,\,\Dij \bm{v}_0(\Y)\rangle - \langle\what,\,(\Dij\Ky)\,\what\rangle.
\end{split}
\end{gather}
Now recall that
\begin{equation*}
    [\Dij\Ky]_{m_1,m_2} = \Dij\kappa(y_{m_1}, y_{m_2}),
\end{equation*}
for all $m_1,\,m_2 \in [M]$. Therefore, under \Cref{assumption:gradient_kappa},
\begin{equation}
\nabla_{ij} \kappa(y_{m_1},y_{m_2}) = \delta_{i,m_1}(y_{m_2j} - \yij)\bar{\kappa}(\yi,y_{m_2}) + \delta_{i,m_2}(y_{m_1j} - \yij)\bar{\kappa}(y_{m_1}, \yi).
\end{equation}

First, we denote $[\bm{a}\otimes\bm{b}]_{ij} = a_ib_j$ as the outer product between vectors $\bm{a}$ and $\bm{b}$ with possibly different dimensions. We also denote $\bm{a}\odot\bm{b}$ as the elementwise (i.e., Hadamard) product between $\bm{a}$ and $\bm{b}$ with identical dimensions. Then, we have
\begin{equation*}
        \nabla_{ij} \Ky = \bij\otimes \bm{e}_i + \bm{e}_i\otimes \bij,
\end{equation*}
where $\bm{e}_i$ is the $i$th elementary vector, i.e., $[\bm{e}_i]_{m} = \delta_{im},$ and we define
\begin{equation*}
        \bij := (\Y_j - \yij\bm{1})\odot \Ki = \Y_{j}\odot \Ki - \yij \Ki,
\end{equation*}
with $\Y_{j} \in \RR^{M}$ is the vector containing the $j$-th entry of each $\yi$, identical to the $j$th column of the matrix $\Y$, and $\Ki = \Kmbar(\Y)\bm{e}_i$. Then,
\begin{align}\label{eq:w_hat_grad_K_w_hat_final}
\langle\what,\,(\Dij\Ky)\,\what\rangle &= \langle\what,\, (\bij\otimes\bm{e}_i + \bm{e}_i\otimes\bij)\what\rangle\nonumber\\
&= 2 \whati \langle \what,\,\bij\rangle \nonumber \\
&= 2 \whati \Big( \langle\what,\,\Y_{j}\odot \Ki - \yij \Ki\rangle\Big) \nonumber \\
&= 2 \whati \Big( \langle\what,\, \Y_{j}\odot \Ki\rangle - \yij\langle\what,\, \Ki\rangle \Big)
\end{align}
 For $m \in [M]$, we recall that $\nabla\kappa$ is bounded on $\X\times\X$ to find
 \begin{align*}
\Dij \vzerov(\Y) &= \left(\Dij \vzero(\yi)\right)\bm{e}_i\\
\Dij \vzero(\yi)&= \int_{\X} \Dij \kappa( x,\yi) \dpix \\
& = \int_{\X}  (\xj-\yij)\bar{\kappa}(x,\yi) \dpix\\
& = \int_{\X}    \xj\bar{\kappa}( x,\yi) \dpix -\yij  \bvzero(\yi),
 \end{align*}
where $\xj\in\RR$ is the $j$th entry of integration variable $x\in\RR^d$, and we recall that $\bar{v}_{0}(y)= \int_{\X} \bar{\kappa}(x,y) \dpix$. We then have
\begin{equation*}
    \Dij \vzerov(\Y) =  \left( [\bvonev(\yi)]_{j}  - \yij\bvzero(\yi)\right)\bm{e}_i.
\end{equation*}


Thus,
\begin{equation}\label{eq:w_hat_grad_v_final}
    \langle\what,\,\nabla_{ij} \bm{v}_0(\Y)\rangle = \whati\left( [\bvonev(\yi)]_{j}  - \yij\bvzero(\yi)\right).
\end{equation}
Combining~\eqref{eq:grad_F_pi_first}, \eqref{eq:w_hat_grad_K_w_hat_final} and \eqref{eq:w_hat_grad_v_final}, we obtain 
\begin{align}\label{eq:2times_gradient_FM}
    2\Dij F_{M}(\Y) &= -2\whati\left([\bvonev(\yi)]_{j}  - \yij\bvzero(\yi)- \langle\what,\, \Y_{j}\odot\Ki\rangle + \yij\langle\what,\,\Ki\rangle
    \right) \nonumber \\
    &= -2\whati\left( [\bvonev(\yi)]_{j} - \yij \bvzero(\yi) + \yij \langle\what,\,\Ki\rangle - \langle\what,\,\Y_{j}\odot\Ki\rangle 
    \right)
\end{align}
Now, 
\begin{align}\label{eq:ubar_i}
   - \yij \bvzero(\yi) + \yij \langle\what,\,\Ki\rangle &= \yij \left( \sum\limits_{m=1}^{M}\whatms\bar{\kappa}(\yi,\yms) - \int_{\X} \bar{\kappa}(x,\yi) \dpix \right)\nonumber\\
   &= [\bar{u}(\Y)]_{i} 
\end{align}
which matches the definition of $\bar{\bm{u}}(\Y)$ in \cref{eq:u_vonebar_def}. By the symmetry of $\bar\kappa$,%
\begin{align}\label{eq:what_yj_hadamard_K}
    \langle\what,\, \Y_{j}\odot \Ki\rangle &= \sum_{m=1}^M \whatms\ymsj\bar{\kappa}(\yms,\yi) \nonumber\\
    &= \sum_{m=1}^M \bar{\kappa}(\yi,\yms)\whatms\ymsj \nonumber\\
    &= \langle\bm{e}_i,\, \Kmbar(\Y)\bm{W}(\Y)\Y_j\rangle \nonumber\\
    &= \left[\,\Kmbar(\Y)\bm{W}(\Y)\Y\right]_{i,j}
\end{align}
where $\bm{W}(\Y)\in\mathbb{R}^{M\times M}$ satisfies $[\bm{W}(\Y)]_{i,m} = \whati\delta_{i,m}$.  Combining \eqref{eq:ubar_i}, \eqref{eq:what_yj_hadamard_K}, and the fact that $[\bvonev(\yi)]_j$ is the $(i,j)$-th entry of the matrix $\bvonem(\Y)$, we conclude that
\begin{equation*}
    \left([\bvonev(\yi)]_{j} - \yij\bvzero(\yi) + \yij \langle\what,\, \Ki\rangle - \langle\what,\, \Y_{j}\odot \Ki\rangle 
    \right)
\end{equation*}
is the $(i,j)$-entry of the matrix $\bvonem(\Y) + \bar{\bm{u}}(\Y) -\bm{\bar{K}}(\Y) \bmW \Y$. Thus, \eqref{eq:2times_gradient_FM} is
the $(i,j)$-entry of the matrix 
\begin{equation}
    2 \bmW \left( \bm{\bar{K}}(\Y) \bmW \Y  - \bvonem(\Y) - \bar{\bm{u}}(\Y)\right).
\end{equation}
In other words,
\begin{align}
    \nabla F_{M}(\Y) &= \bmW \left( \bm{\bar{K}}(\Y) \bmW \Y  - \bvonem(\Y) - \bar{\bm{u}}(\Y)\right)\nonumber\\
    &= \bmW \left( \bm{\bar{K}}(\Y) \bmW \Y  - \hvonem(\Y)\right),
\end{align}%
where $\hvonem(\Y) = \bvonem(\Y) + \bar{\bm{u}}(\Y)$ as in \eqref{eq:v_hat_def}. When the matrices $\bmW$ and $\bm{\bar{K}}(\Y)$ are non-singular, we have
\begin{equation}
    \nabla F_{M}(\Y) = \bmW \bm{\bar{K}}(\Y) \bmW\left( \Y - \bmW^{-1} \bm{\bar{K}}(\Y)^{-1} \hvonem(\Y) \right).
\end{equation}
Finally, observe that the function $\bm{\Psi}_{\MMS}$ defined by \eqref{eq:fixed_point_general_case} satisfies
\begin{equation}
\bm{\Psi}_{\MMS}(\Y) = \bmW^{-1} \bm{\bar{K}}(\Y)^{-1} \hvonem(\Y).
\end{equation}



\subsection{Proof of \texorpdfstring{\Cref{cor:ms_as_MSIP}}{Theorem \ref{cor:ms_as_MSIP}}}\label{proof:mean_shift_as_mmd_min}

We again define $C_\pi = \int_{\X} \int_{\X} \kappa(x,x^{\prime}) \dpix \mathrm{d}\pi(x^{\prime})$. First, for $w \in \mathbb{R}$ and $y \in \X$, we have
\begin{equation}
    \F(w\delta_{y}) = \frac{1}{2} \left( C_\pi- 2 w v_{0}(y) + w^2 \kappa(y,y) \right).
\end{equation}
Since this is a quadratic form on $w$, we get
\begin{equation}
    \argmin\limits_{w \in \mathbb{R}} \F(w\delta_{y}) = \frac{v_{0}(y)}{\kappa(y,y)}.
\end{equation}
Thus, by \eqref{eq:gradien_opt_mmd_under_assumption} in \Cref{thm:gradient_opt_mmd}, we get 
\begin{equation}\label{eq:proof_cor_eq_1}
    \nabla F_{1}(y)  =  \frac{v_{0}(y)}{\kappa(y,y)}\left(\bar{\kappa}(y,y) \frac{v_{0}(y)}{\kappa(y,y)} y - \bar{u}(y) - \bar{v}_{1}(y) \right).    
\end{equation}
By definition \eqref{eq:u_vonebar_def}, we have
\begin{equation}\label{eq:app_ubar_def}
    \bar{u}(y) = \left( \frac{v_{0}(y)}{\kappa(y,y)} \bar{\kappa}(y,y) - \bar{v}_{0}(y) \right) y.
\end{equation}
By rearranging \eqref{eq:app_ubar_def}, we see
\begin{equation}\label{eq:proof_cor_eq_2}
  \frac{v_{0}(y)}{\kappa(y,y)} \bar{\kappa}(y,y) y-   \bar{u}(y) = \bar{v}_{0}(y) y.
\end{equation}
Combining \eqref{eq:proof_cor_eq_1} and \eqref{eq:proof_cor_eq_2}
\begin{equation}
    \nabla F_{1}(y)  =  \frac{v_{0}(y)}{\kappa(y,y)}\Big(\bar{v}_{0}(y) y - \bar{v}_{1}(y) \Big).
\end{equation}









