%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\pdfoutput=1

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tcolorbox}
\usepackage{caption} 

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{algorithm}
\usepackage{algpseudocode}

% inline list
\usepackage{enumitem}
  \newlist{inlinelist}{enumerate*}{1}
  \setlist*[inlinelist,1]{%
          label=(\roman*),
      }

\usepackage{adjustbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs}

\begin{document}

\twocolumn[
\icmltitle{JoLT: Joint Probabilistic Predictions on Tabular Data Using LLMs}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aliaksandra Shysheya}{equal,cam}
\icmlauthor{John Bronskill}{equal,cam}
\icmlauthor{James Requiema}{tor,vec}
\icmlauthor{Shoaib Ahmed Siddiqui}{cam}
\icmlauthor{Javier González}{msft}
\icmlauthor{David Duvenaud}{tor,vec}
\icmlauthor{Richard E. Turner}{cam,ati}
\end{icmlauthorlist}

\icmlaffiliation{cam}{University of Cambridge}
\icmlaffiliation{tor}{University of Toronto}
\icmlaffiliation{ati}{The Alan Turing Institute}
\icmlaffiliation{vec}{Vector Institute}
\icmlaffiliation{msft}{Microsoft Research Cambridge}

\icmlcorrespondingauthor{John Bronskill}{jfb54@cam.ac.uk}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
%
We introduce a simple method for probabilistic predictions on tabular data based on Large Language Models (LLMs) called JoLT (Joint LLM Process for Tabular data). 
%
JoLT uses the in-context learning capabilities of LLMs to define joint distributions over tabular data conditioned on user-specified side information about the problem, exploiting the vast repository of latent problem-relevant knowledge encoded in LLMs.  
% 
JoLT defines joint distributions for multiple target variables with potentially heterogeneous data types without any data conversion, data preprocessing, special handling of missing data, or model training, making it accessible and efficient for practitioners. 
%
Our experiments show that JoLT outperforms competitive methods on low-shot single-target and multi-target tabular classification and regression tasks.
%
Furthermore, we show that JoLT can automatically handle missing data and perform data imputation by leveraging textual side information.
%
We argue that due to its simplicity and generality, JoLT is an effective approach for a wide variety of real prediction problems.
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}
%
Tabular data is ubiquitous in machine learning applications in finance, medicine, business, agriculture, and education \citep{sahakyan2021explainable,fang2024large}. Given their ubiquitous nature, \citet{van2024position} argue that tabular foundation models should rank higher as a research priority.
%
However, real-world tabular data rarely presents itself in an idealized form -- practitioners often encounter datasets with mixed data types, missing values, and complex dependencies between variables.
%
Traditional approaches typically require extensive preprocessing pipelines and specialized handling for different data types. %creating barriers to deployment. 
%
\input{tables/feature_comparison}
%
When modeling tabular data, incorporating side information and domain expertise is often a challenge. Additionally, many current approaches are inaccessible to lay users who lack advanced technical knowledge. 
%
For this reason, using Large Language Models (LLMs) to incorporate textual information for tabular data prediction \citep{fang2024large, lu2024large} is a natural approach. LLMs, trained on vast corpora of internet data, possess substantial implicit knowledge and provide an interface for expressing and incorporating side information in natural language. These attributes make LLMs particularly attractive for tabular prediction, offering the ability to integrate diverse data types naturally while circumventing the need for complex preprocessing pipelines.
%

Prediction using LLMs typically uses one of two approaches -- fine-tuning \citep{yosinski2014transferable} or inference-time in-context learning (ICL) \citep{brown2020language}. 
%
Fine-tuning involves modifying LLM's weights to adapt it for specific tasks. This approach has its own limitations, including high resource requirements, overfitting risks with small datasets, and potential privacy concerns when using sensitive data. 
%
In contrast, ICL is a simpler, more accessible alternative, enabling predictions without gradient updates to the model. 
%
This approach reduces the burden on users by eliminating the need for training or hyperparameter tuning, making it suitable for scenarios with limited data, such as personalization~\citep{massiceti2021orbit, ding2017collecting}.
%
\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\textwidth]{diagrams/table_to_prompt_diagram_no_legend_cropped.pdf}}
\vspace{-3mm}
\caption{\textbf{Mapping tabular data to a prompt $P$.} The diagram on the left depicts a tabular dataset where the first $N$ rows are training examples with $F$ features and $T$ targets fully observed. The last row represents a test example with $F$ observed features and unobserved targets. The prompt at the top right is formed by serializing the $N$ training examples and the test features into a single string. The prompt is input to a pretrained LLM that will generate the targets to complete the test example. See \cref{tab:nomenclature} for nomenclature.}
\label{fig:table_to_prompt}
\end{center}
\vspace{-8mm}
\end{figure*}
%

Well-calibrated uncertainty quantification is critical for decision-making processes that rely on tabular predictions \citep{sahakyan2021explainable,bishop2006pattern,kochenderfer2015decision}. 
% 
Additionally, joint probabilistic predictions have practical applications in sciences~\citep{kong2020deep} and healthcare, where they help predict multiple patient outcomes, such as survival, recovery time, and complication risk~\citep{martin2021clinical}.
%
Joint modeling can also exploit correlations between outputs to improve prediction accuracy \citep{MorenoArtesAlvarez18}.

\citet{requeima2024llm} introduced LLM Processes (LLMPs) which used pretrained LLMs and ICL to do probabilistic regression conditioned on textual side information. LLMPs effectively combine data and metadata, such as dataset descriptions and column headings, with the LLM’s latent knowledge. 
%
However, LLMPs are not currently suitable for tabular data as they do not support heterogeneous columns, including numerical and categorical types, or missing data, which many existing methods also fail to address comprehensively.
%
In this work, we build upon LLMPs and present JoLT (Joint LLMP for Tabular data), a method designed to address the challenges of heterogeneous tabular data. JoLT extends LLMPs beyond regression to make joint probabilistic predictions for multiple target variables, accommodating both numerical and categorical data types. 
%
Our approach eliminates the need for preprocessing, missing data imputation, model training, or hyperparameter tuning, making it highly accessible to non-experts in machine learning. 
%
Additionally, JoLT empowers users to incorporate side information or provide specific instructions in plain language or text, allowing for seamless integration of expert knowledge and contextual insights from the user and LLM into the modeling process. 
%
Our main contributions are:
%
\vspace{-3mm}
\begin{itemize}[leftmargin=*]
\setlength\itemsep{0pt}
    \item We present JoLT, a method that extends LLMPs beyond regression to make joint probabilistic predictions for multiple target variables with heterogeneous data types on tabular datasets.
    \item We demonstrate that JoLT outperforms competitive approaches on low-shot single and multiple target tabular classification and regression tasks.
    \item We show that JoLT implicitly handles missing tabular data and performs as well or better on downstream tasks when compared to imputing data as a preprocessing step.
    \item Finally, we show that JoLT can also effectively impute missing data by leveraging textual side information about the problem.
\end{itemize}
\vspace{-4mm}
%
Table \labelcref{tab:comparison} summarizes the key attributes of JoLT relative to competitive methods.
%
\section{Method}
\label{sec:method}
%
In this section, we describe how to make heterogeneous multiple target probabilistic predictions in the presence of missing data using JoLT.

\subsection{Prompt Engineering}
%
In a prediction setting, tabular datasets consist of multiple rows of examples or records.
%
Each row consists of one or more columns of features and one or more columns of targets to be predicted based on the feature values. 
%
Training examples have both observed features and observed targets, while test examples only have observed features.
\input{tables/nomenclature}
%
\cref{fig:table_to_prompt} shows how we design a prompt that contains the entire training set and the features for a test example that is fed to the LLM in order to generate a prediction.
%
For each test example, we serialize each row of training data, including both features and targets, plus the features of the test example to a string.
%
Each feature column may be of any data type (e.g. numerical - integer or floating point, categorical, date/time, addresses, unstructured text, etc.) given that the type is or can be converted to a string.
%
The target columns can also be of any type. However, we currently only support computing predictive distributions for numerical and categorical types.
%
We do not perform any preprocessing or scaling of the data as we want to retain the natural scale and units of the data such that related knowledge encoded in the LLM can be leveraged.
%
Furthermore, we do not modify the LLM weights via training or fine-tuning.
%
Next, we discuss how to make predictions and compute joint predictive distributions based on the prompt.
%
\subsection{Prediction}
Here we present two approaches to making predictions on multiple target, heterogeneous data -- rejection sampling and sampling from a full distribution.

\textbf{Rejection Sampling}
After feeding the prompt to the LLM, we use the autoregressive token prediction capability of the LLM to generate the targets for the test example.
%
Predicting most targets will require multiple steps of autoregressive token generation to represent numerical digits or categories.
%
Knowing the number and types of the targets, we can parse the generated output and ensure it conforms to the expected format.
%
If not, we reject the sample.
%
For numerical targets, we ensure the sample contains a valid number, and for categorical targets, we ensure that it contains a known category.
%
For a point estimate of a quantity, we can take a single top 1 sample from the LLM.
%
To obtain a point estimate and uncertainty for numerical targets, we can take a set of samples and use the median for the point estimate and compute a confidence interval over the range.
%
For categorical targets, the set of samples form a categorical distribution and the point estimate is the category with the highest number of samples.
%
To illustrate, \cref{fig:uncertainty} depicts median point estimates and the 95$\%$ confidence interval for JoLT predictions of alcohol percentage on the wine quality dataset \citep{wine_quality_186}.
%
In this example, the predictive medians are quite accurate and the confidence intervals well-calibrated.

\textbf{Full Distribution via LLM Logits}
%
We can compute the probability of a target value $s$ that is a member of the set of possible target values $\mathcal{S}$ conditioned on a prompt $P$ as:
\begin{equation}
\label{eq:target_probability}
    p(y = s | P, s \in S) = \frac{p(y = s | P)}{\sum_{s^\prime \in \mathcal{S}} p(y=s^\prime | P)}
\end{equation}
%
For categorical targets, we can obtain $p(y = s | P)$ for each category $s$ from the LLM logits using \cref{alg:categorical_logprobs} and then use \cref{eq:target_probability} to get the normalized probability for each category.
%
We can then predict the target category that has the highest probability or sample from the resulting full categorical distribution.
%
If the number of classes is relatively small, this approach is preferable to rejection sampling, as it enables direct access to the full predictive distribution.
%
However, for numerical targets or when the number of classes is large, the rejection sampling approach is preferred over the logits method. 
%
This is because the set $\mathcal{S}$ becomes large, and computing the full predictive distribution would require an excessive number of calls to the LLM to retrieve logits, making the logits method computationally impractical.
%
\subsection{Computing Joint Predictive Distributions}
Using the product rule, we can compute the joint probability of the ground truth target values for any test example as:
\begin{multline}
\label{eqn:joint_probability}
    p(y^*_1, y^*_2, \dots, y^*_T) =
    p(y^*_1 | ``P Y_1 \langle d \rangle")\\
    p(y^*_2 | ``P Y_1 \langle d \rangle y^*_1 \langle s \rangle Y_2 \langle d \rangle") \dots \\
    p(y^*_T | ``P Y_1 \langle d \rangle y^*_1 \langle s \rangle Y_2 \langle d \rangle y^*_2 \langle s \rangle \dots Y_T \langle d \rangle")
\end{multline}
%
where we have used the notation introduced in \cref{fig:table_to_prompt}. \cref{eqn:joint_probability} suggests that we can compute the joint likelihood of a multiple target test example by computing the product of the probability of each individual target conditioned on the text that precedes it using the logits of the LLM which hold token probabilities.
%
To introduce dependencies between multiple targets, \cref{eqn:joint_probability} adopts an autoregressive structure. 
%
This formulation enables the modeling of conditional relationships between the target variables, allowing the prediction of each target to depend on previously predicted ones.
%
One limitation of this approach is that the predictive distribution becomes dependent on the order of the target variables. As a result, it no longer guarantees a valid stochastic process and may fail to satisfy the Kolmogorov exchangeability condition \citep{oksendal2013stochastic}.

For a numerical target, we follow \citet{requeima2024llm} and approximate the probability of a ground truth value using \cref{alg:numerical_logprobs}.
%
To obtain a probability density, we convert the predicted probability mass by assuming a uniform distribution within each bin and normalizing by the bin width. 
%
The bin width is determined by the precision of the generated numerical values.
%
If the target is categorical, we can use \cref{alg:categorical_logprobs}, with $y$ set to the true target category.
%
In our experiments, for each test example, we compute the probability for each target, then compute the joint probability using \cref{eqn:joint_probability}, and report the negative log likelihood (NLL) as the mean of the joint NLLs over the test set.
%
\subsection{Missing Data Handling}
%
\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{diagrams/missing_data_diagram_cropped.pdf}}
\vspace{-3mm}
\caption{\textbf{Missing data handling.} The diagram depicts a tabular dataset with 4 rows of training examples. The last row represents a test example with 2 unobserved targets. Empty cells represent 40$\%$ (8 out of 20 feature cells) missing completely-at-random data. Note that both training and test examples are affected by missing data. The prompt is formed by simply omitting missing cells.}
\label{fig:missing_data_handling}
\end{center}
\vspace{-10mm}
\end{figure}

\label{sec:missing_data}
Our approach implicitly handles missing feature data in both training and test sets.
%
The strategy is to simply omit any missing feature data when building the prompt as shown in \cref{fig:missing_data_handling}.
%
In \cref{sec:missing_data_results}, we show that the \textit{omit} approach is either competitive or outperforms missing data imputation.
%
We posit that formatting the prompt so that each data value is preceded by descriptive column header text indicates to the LLM what data is present versus what data it needs to generate in order to make predictions for the targets.

%
\subsection{Missing Data Imputation}
\label{sec:imputation}
%
Our method can be extended to perform data imputation.
%
The approach involves imputing missing values row by row, while leveraging other incomplete rows of the table as in-context training data for the LLM. 
%
To impute missing values for a specific row, the columns of the table are first permuted so that the columns containing missing values for that row are positioned after those with non-missing values.
%
Following the same procedure described in \cref{sec:missing_data}, a separate prompt is constructed for each row, and the LLM predicts the missing values conditioned on the prompt. 
%
The procedure is illustrated in \cref{fig:imputation_diagram}.
%
\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{diagrams/imputation_diagram_cropped.pdf}}
\vspace{-3mm}
\caption{\textbf{JoLT imputation.} To impute values for a specific row (highlighted in pink), the features are reordered such that the features with existing values for the specific row are positioned first. The prompt for the LLM is constructed akin to \cref{fig:missing_data_handling}, where instead of predicting targets $Y_i$, the model predicts the missing values for a specific row (e.g., columns $X_1$ and $X_4$ in the diagram). }
\label{fig:imputation_diagram}
\end{center}
\vspace{-10mm}
\end{figure}
%
\section{Experiments}
%
In this section, we evaluate the performance of JoLT prediction on single- and multiple-target tabular prediction tasks.
%
Specifically, the experiments aim to answer the following questions:
%
\begin{inlinelist}
    \item knowing that LLMPs perform well on numerical regression tasks \citep{requeima2024llm}, can JoLT perform well on categorical classification tasks?
    \item how does JoLT compare to strong baseline methods when predicting multiple heterogeneous tabular targets?
    \item how does side information affect JoLT's predictive distribution? 
    \item can JoLT gracefully handle missing data? and
    \item can JoLT impute missing data as well as standard approaches? 
\end{inlinelist}

%
We use the term \textit{shots} to refer to the number of training examples.
%
Due to the context size limits on LLMs and the fact that processing time and GPU memory requirement grow roughly quadratically with the length of the prompt, we restrict our experiments to the low-shot setting.
%
We focus on open-source LLMs, as access to logits for computing probabilities is often unavailable for proprietary LLMs.
%
In addition, we only use LLMs that perform single-digit tokenization so that we can calculate the probabilities for numerical targets.
%
As a result, we utilize the Gemma-2 and Qwen2.5 LLMs for all of our experiments.
%
A key advantage of the JoLT approach is that it can produce probabilistic predictions.
%
Thus, we restrict the set of competitive approaches to those that can produce a distribution over outputs.
%
\subsection{Classification Setting}
\label{sec:classification_results}
%
\begin{figure*}[ht]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{diagrams/tabllm_plot.pdf}}
\vspace{-4mm}
\caption{Area Under the Receiver Operating Characteristic Curve (AUC) as a function of shot for JoLT using two different LLMs and three competitive methods. The solid line and dots indicate the mean over 5 seeds which affect the training shot selection and the shaded region shows a confidence interval of one $\sigma$. Competitive data from \citet{hegselmann2023tabllm}. Tabular results are in \cref{tab:classification_results}.}
\label{fig:classification_results}
\end{center}
\vspace{-8mm}
\end{figure*}
%

In this experiment, we compare JoLT low-shot classification performance to strong baselines with the same nine datasets used in \citet{hegselmann2023tabllm}.
%
To make a fair comparison, we use the serialized datasets from \citet{hegselmann2023github}.
%
We consider XGBoost \citep{chen2016xgboost}, TabPFN \citep{hollmann2025tabpfn}, and TabLLM \citep{hegselmann2023tabllm} as competitive baselines for comparison.
%
Like JoLT, TabLLM also relies on an LLM for prediction. However, TabLLM utilizes fine-tuning on the training data as opposed to ICL, which is the main focus of JoLT.

\cref{fig:classification_results} shows that on 7 of the 9 datasets (creditg and jungle being the exceptions), JoLT models outperform other competitive methods in the low-shot setting, often by a large margin.
%
However, expectedly, the gap shrinks with an increasing number of shots.
%

\textbf{Summary}: JoLT is able to outperform boosted decision trees (XGBoost), LLM fine-tuning (TabLLM), and TabPFN (which also uses ICL, but without an LLM)  in the low-shot classification setting by utilizing column header side information.
%
XGBoost and TabPFN cannot use side information and rely on larger amounts of training data to perform well.
%
TabLLM can leverage text information, but tends to overfit when fine-tuning on a small amount of training data.
%
\subsection{Multi-target Prediction}
\label{sec:multi_target_results}
%
Here, we evaluate the ability of JoLT to predict multiple heterogeneous targets and compute joint distributions on three different datasets.
%
\begin{figure*}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{diagrams/wine_multi_target_plot.pdf}}
\vspace{-4mm}
\caption{Results for predicting two target columns from the Wine Quality dataset \citep{wine_quality_186} as a function of shots when evaluating on 1000 test examples. The first target column is numerical (Alcohol $\%$) using the metric Mean Absolute Error (MAE) and the second target column is categorical (Quality on a scale of 1 to 10) using classification accuracy as the metric. The joint NLL is over both targets. The JoLT methods use the Gemma-2-27B LLM. JoLT (Text) utilized both prefix text $\langle prefix \rangle$ and text from the column headers $X_j, Y_j$, whereas JoLT (No Text) did not. The solid line and dots indicate the mean over 5 seeds which affect the training shot and test example selection and the shaded region shows a confidence interval of one $\sigma$. Tabular results are in \cref{tab:multi_column_results}.}
\label{fig:wine_multi_target}
\end{center}
\vskip -0.35in
\end{figure*}

\textbf{Wine Quality}
%
The first dataset is the Wine Quality dataset \citep{wine_quality_186} where there are two targets - one numerical (Alcohol $\%$) and one categorical (Wine Quality on the scale of 0 to 10).
%
This dataset is primarily comprised of numerical features, with the color feature column being the only categorical variable.
%
A sample JoLT prompt is shown in \cref{app:wine_quality_prompt}.
%
We compare JoLT to two competitive methods that can make probabilistic predictions - TabPFN \citep{hollmann2025tabpfn, hollmann2025github} and a Gaussian Process (GP).
%
To produce multiple targets with TabPFN and the GP, we query the models autoregressively with multiple passes.  
%
We use two variants of the LLM. One that uses both prefix text $\langle prefix \rangle$ and text from the column headers $X_j, Y_j$,
and a no text version that does not.

\cref{fig:wine_multi_target} indicates that JoLT without using textual information performs poorly across the board.
%
When using text, JoLT has the best classification accuracy, and the best NLL up to 40 shots, but loses out to TabPFN on MAE after 20 shots and to the GP at 30 shots and beyond.
%
In all cases, TabPFN and the GP improve rapidly with increasing number of shots.
%
When leveraging text, JoLT outperforms other models in the low-shot setting, but as the amount of training data increases, TabPFN dominates in terms of performance.
%
Given that this dataset is primarily numerical, it is unsurprising that the advantage of using an LLM-based method reduces as the number of training examples increases.

\input{tables/movies}
\textbf{Movies}
The second dataset is a subset of the Movies Box Office Dataset (2000-2024) \citep{jilla2024movies} where we predict the movie rating as a continuous value on a scale of 0 to 10, as well as eight binary categorical variables that indicate the movie genre, based on features that include the movie title and worldwide box office revenue.
%
We only used the 2024 data and split the dataset (89 train, 99 test examples) so that the movies in the test set have a release date after the release date of the Gemma-2 LLM. This was done to ensure that the LLM we use was not pretrained on the test data.
%
A sample JoLT prompt is shown in \cref{app:movies_prompt}.
%
The results are shown in \cref{tab:movies}.
%
When predicting the numerical rating, both JoLT and TabPFN have the same error.
%
However, JoLT outperforms TabPFN by a large margin in terms of AUC and NLL in predicting the genre attributes of the movies.
%
This is due to JoLT being able to use the text of the movie title to help predict the binary genre targets, whereas TabPFN is not able to use text columns and hence does only a little better than guessing these targets.

\input{tables/medals_multi_column}
\textbf{Medals}
%
The third dataset is the Olympic Games dataset collection \citep{ismail2024olympic} where we use the bronze, silver, and gold medal counts of 10 countries (USA, China, Japan, Australia, France, Netherlands, Great Britain, Italy, Germany, and Canada) from the 1996 to 2020 summer Olympics to train on.
%
The goal is to predict the silver and gold medal counts for the ten counties at the 2024 Olympics which were held after the release date of the Gemma-2 LLM that JoLT uses.
%
We treat medal counts as continuous variables for both prediction and NLL computation. 
%
The bin size for JoLT was selected to be comparable to the bin sizes predicted by TabPFN, ensuring a fair comparison between methods. 
%
A sample prompt is in \cref{app:medals_prompt}.
%
Results are shown in \cref{tab:multi_column_medals}.
%
JoLT has lower MAE and NLL when predicting the 2024 silver and gold medal counts for the 2024 Paris Olympics as it can use the name of the country as context, whereas TabPFN only gets a numerical country label.

\textbf{Summary}: JoLT performs best in low-shot settings due to the ability to utilize user-provided side information and latent knowledge in the LLM.
%
Performance is superior when dataset features contain rich text content that purely numerical approaches cannot exploit.
%
\subsection{Side Information Influence}
%
\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{diagrams/prompts_medals_narrow.pdf}}
\vspace{-4mm}
\caption{Effect of side information on JoLT's predictive distribution for the Medals Dataset. Each subplot corresponds to a different $\langle prefix \rangle$, shown in the title, and represents the distributions for the country mentioned in the $\langle prefix \rangle$. The baseline predictive distribution is in blue, while the distribution after adding side information is in orange. Shaded regions indicate the 25th and 75th percentiles, and markers represent the median values. }

\label{fig:prompt_influence}
\end{center}
\vskip -0.3in
\end{figure*}
In this section, we evaluate the effect of incorporating side textual information on JoLT's predictive distribution.
%
Using the Medals dataset from \cref{sec:multi_target_results}, we predict the counts for bronze, silver, and gold medals. 
%
In \cref{fig:prompt_influence}, we apply four different  $\langle prefix \rangle$ values that characterize a country's performance at the 2024 Olympics.
%
In all cases, the resulting distribution for the selected country shifts to better align with the provided textual information, demonstrating the effectiveness of textual side information in refining JoLT’s predictions.
%
\subsection{Handling Missing Data}
\label{sec:missing_data_results}
%
\begin{figure*}[t]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{diagrams/wine_multi_target_missing_plot.pdf}}
\vspace{-4mm}
\caption{Performance metrics for JoLT that uses the gemma-2-27B LLM and TabPFN as a function of shots and percentage of data missing completely-at-random (MCAR) on the multitarget Wine Quality dataset \citep{wine_quality_186}. 200 test examples were used. The solid line and dots indicate the mean over 3 seeds which affect the training shot and test example selection as well as the missing pattern and the shaded region shows a confidence interval of one $\sigma$. The first target column is numerical (Alcohol $\%$) using the metric Mean Absolute Error (MAE) and the second target column is categorical (Wine Quality on a scale of 1 to 10) using classification accuracy as the metric (ACC). The joint negative log-likelihood (NLL) is over both targets. Tabular version is in \cref{tab:wine_missing}.}
\label{fig:wine_missing}
\end{center}
\vskip -0.35in
\end{figure*}
%
\begin{figure*}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{diagrams/cars_missing_plot.pdf}}
\vspace{-4mm}
\caption{AUC for JoLT using the gemma-2-27B LLM and TabPFN as a function of shots and percentage of data missing completely at random (MCAR) on the Car classification dataset \citep{car_evaluation_19}. 200 test examples were used.  The solid line and dots indicate the mean over 3 seeds which affect the training shot and test example selection as well as the missing pattern and the shaded region shows a confidence interval of one $\sigma$. Tabular version is in \cref{tab:car_missing}}
\label{fig:cars_missing}
\end{center}
\vskip -0.3in
\end{figure*}
%
In these experiments, we compare the \textit{omit} approach to handling missing data as described in \cref{sec:missing_data} to imputing missing data as a preprocessing step.
%
To impute missing data, we replace missing numerical and categorical values with the column-wise mean and the column-wise mode from the training data, respectfully.
%
We use two different datasets -- the Wine Quality dataset as used in \cref{sec:multi_target_results} whose features are primarily numerical, and the Car dataset used in \cref{sec:classification_results} whose features are all categorical.

\cref{fig:wine_missing,fig:cars_missing} show the results for the two datasets where the amount of data missing completely-at-random (MCAR) varies from 10$\%$ to 40$\%$ in both the training and test features.
%
We evaluate four JoLT variants -- omit and impute with and without text, as well TabPFN.
%
As previously, the variants that do not use side text information perform the worst on both datasets.
%
Remarkably, for the JoLT variants that use side text information, the omit approach performs almost identically to imputing data on the Wine Quality dataset and exceeds it on the Car dataset.
%
Interestingly, the opposite is true when no side text is used --- impute performs better than omit.
%
This supports our argument that the heading text labels on each feature cell helps the LLM to do a better job of knowing what features are missing (or present) and compensate appropriately.
%
Compared to TabPFN, JoLT omit outperforms at low shot and performs similarly at higher shot.
%
The exception is at 40$\%$ missing using the Car dataset where JoLT omit outperforms TabPFN by a large margin.
%

\textbf{Summary}: JoLT implicitly handles missing data, obviating the need to impute as a preprocessing step, which greatly simplifies the data science workflow.
%
\subsection{Missing Data Imputation}
\label{sec:imputation_results}
While JoLT gracefully handles missing data while predicting, it is often required to recover missing feature values.
%
In this experiment, we demonstrate JoLT's ability to use contextual information and predict multiple values seamlessly to improve imputation performance.
%
We use the Paris 2024 data from the Olympic Games dataset collection \citep{ismail2024olympic} which lists medals won by each of the 91 nations participating in the event.
%
We choose this dataset as the event was held after the training cutoff data of the Gemma-2. Hence, it is impossible for the model to have seen this data during pretraining.
%
In particular, we use the following 4 columns: Country, Gold, Silver, and Bronze.
%
We then eliminate a fixed percentage of the data completely at random.
%
The country column cannot be used by existing imputation methods that only deal with numerical data, but can be leveraged by JoLT.
%
JoLT imputation implementation follows the algorithm described in \cref{sec:imputation}.
An example prompt is shown in \cref{app:imputation_prompt}.
%
The results are shown in \cref{fig:imputation} where JoLT has significantly lower error than conventional imputation techniques that include MICE~\citep{little2002statistical}, k-Nearest Neighbors~\citep{Troyanskaya2001MissingVE}, mean, and iterative application of a Bayesian Ridge regressor.
%

\textbf{Summary}: JoLT can outperform standard imputation techniques by leveraging text-based side information about the setting (i.e. the country name, the numerical columns contained medal counts for the 2024 Olympic games, etc.) that is not possible with numerical only methods.
%
\begin{figure}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\columnwidth]{diagrams/imputation_plot.pdf}}
\vspace{-5mm}
\caption{\textbf{JoLT imputation.} MAE as a function of $\%$ of missing data (MCAR) on the Paris 2024 Olympic Medals dataset for JoLT that uses the gemma-2-27B and four competive methods. The dots indicate the mean over 3 seeds which affect the missing pattern and the shaded shaded region is a confidence interval of one $\sigma$. Tabular results are in \cref{tab:imputation}.}
\label{fig:imputation}
\end{center}
\vspace{-10mm}
\end{figure}
%
\section{Related Work}
%
In this section, we survey related methods for tabular prediction and imputation.
%
For a more complete treatment, refer to \citet{borisov2022deep} for a survey of deep learning methods for tabular data and \citet{fang2024large,lu2024large} for applying LLMs to tabular data.

\textbf{Classification and Multi-target prediction}
%
TabLLM \citep{hegselmann2023tabllm} fine-tunes an LLM for single target classification only.
%
It can incorporate side information, but performance is limited in the few-shot setting.
%
TabPFN \citep{hollmann2025tabpfn} is an effective ICL method for both classification and regression and offers uncertainty in the form of quantiles.
%
However, it is restricted to handling numerical and categorical data, is unable to incorporate text or side information, cannot predict multiple targets at once, and does not perform well in the low-shot setting.
%
LLM Processes~\citep{requeima2024llm} support multi-target regression with uncertainty estimates and side information but do not handle classification or heterogeneous targets.
%
JoLT extends LLM Processes by enabling classification and supporting heterogeneous multiple targets in the tabular data setting.
%
Carte \citep{kim2024carte} uses a graph-attentional network pretrained across multiple tables, and subsequently fine-tuned on a downstream dataset.
%
It can incorporate side information while performing single-target regression or classification, but does not provide uncertainty.
%
Finally, GPs can make multiple target probabilistic predictions on heterogeneous data \citep{moreno2018heterogeneous}, but require training and cannot easily incorporate side information.

\textbf{Handling Missing Data and Data Imputation}
%
There exists a myriad of widely used imputation methods, including mean, median, and mode imputation, k-Nearest Neighbors~\citep{Troyanskaya2001MissingVE}, MICE~\citep{little2002statistical}, and tree-based methods like MissForest~\citep{Stekhoven2011MissForestN}.
%
However, they are restricted to using numerical or categorical data, and hence, are unable to exploit side information. Furthermore, they do not provide any estimates of uncertainty or distributions for the imputed values. 
%
In contrast, Bayesian methods, such as Gaussian Processes (GPs)~\citep{Rasmussen2006Gaussian} and Gaussian Copula~\citep{NelsenRogerB1999Aitc}, offer the advantage of providing uncertainty estimates. 
%
While Bayesian methods have been extended to handle missing data~\citep{zhaocopula2022,bahram2023gp}, their widespread adoption is hindered by scalability limitations and the challenges associated with incorporating side information effectively.
%
Recent work has also explored the use of LLMs for data imputation.
%
\citet{anonymous2024contextdriven} proposes a nearest-neighbor-based imputation method that operates in the embedding space of an LLM.
%
However, this method does not provide uncertainty estimates for the imputed values. 
%
\citet{Ding2024DataIU} and \citet{hayat2024claimdataenhancingimputation} fine-tune LLMs to handle missing data and report notable improvements on several downstream tasks. 
% 
To the best of our knowledge, there are no methods that utilize LLMs to perform data imputation in low-shot scenarios.
%
In contrast, JoLT handles missing data automatically, and if imputed values are required, JoLT can impute missing data with uncertainty, has the ability to incorporate side information, and operate well in the low-shot setting.
%
\section{Limitations}
Along with the flexibility of LLMs, JoLT inherits their drawbacks.
%
Maximum context sizes limit the size of tasks we can apply this method to and the amount of textual information we can condition on.
%
JoLT is significantly more computationally expensive compared to competitive tabular prediction methods.
%
In particular, both the computational complexity and the maximum context size of the LLM limit the number of training examples and the number of columns in the tabular dataset that can be reasonably processed.
%
All of the experiments were performed on readily available small to medium sized open source LLMs that have fewer parameters and are generally less capable compared to large open source and proprietary LLMs that are accessed through services.
%
We expect our results to improve and scale to larger tabular datasets with the use of proprietary LLMs.
%
% Furthermore, we can combine ICL and fine-tuning to generalize to even larger datasets in the future.

\section{Discussion}

In this paper, we introduce JoLT, a novel method for probabilistic predictions on tabular data that leverages LLMs to define joint distributions over heterogeneous data types. 
%
JoLT distinguishes itself from competitive methods by effectively utilizing side information, providing joint probabilities, and automatically handling missing data,  all without requiring additional training or data preprocessing.
%
Through extensive experiments, we demonstrate that JoLT excels in low-shot scenarios, particularly when rich text-based side information is available, showcasing its versatility and practicality in real-world applications.
%
Recently, \citet{agarwal2024many} demonstrated that in-context learning datasets with hundreds or thousands of shots yield performance benefits, indicating that scaling the number of shots might be an interesting direction for the future.
%
\section*{Impact Statement}
%
Our work has demonstrated a new and useful ICL approach for generating probabilistic predictions on tabular data.
%
It has the potential to allow practitioners from fields such as medical and ecological research to more easily employ probabilistic modeling and machine learning.
%
Like all machine learning technology, there is potential for abuse, and possible consequences from incorrect predictions made with JoLT.
%
Due to the black-box nature of the method, we do not know the biases in the underlying LLMs used and what effect they may have on JoLT output.
%
However, LLM researchers are striving to make LLMs more fair and equitable.
%
An open area of research is whether LLM biases propagate to JoLT predictions and whether de-biasing LLMs helps to fix such an issue.
%

\section*{Acknowledgements}
James Requeima and David Duvenaud acknowledge funding from the Data Sciences Institute at the University of Toronto and the Vector Institute.
%
Aliaksandra Shysheya, John Bronskill, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge.
%
Richard E. Turner is also supported by Google, Amazon, ARM, Improbable, and the EPSRC Probabilistic AI Hub (ProbAI, EP/Y028783/1).
%
\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}
\setcounter{algorithm}{0}
\renewcommand\thefigure{\thesection.\arabic{figure}} 
\renewcommand\thetable{\thesection.\arabic{table}}
\renewcommand\theequation{\thesection.\arabic{equation}}
\renewcommand\thealgorithm{\thesection.\arabic{algorithm}}

\section{Appendix}
%
\subsection{Algorithms}
%
\begin{algorithm*}
\caption{Computing the log probability distribution function of a categorical target $y$}
\label{alg:categorical_logprobs}
\begin{algorithmic}[1]
\Require $\mathcal{M}$: LLM model with vocabulary $\mathcal{V}$; $\mathcal{T}$: tokenizer
\Require $\mathcal{S}$: text string preceding $y$ to condition on; $\mathcal{Y} = \{y_1, \dots y_L\}$ set of possible classes
\State $\mathcal{S}^T = \mathcal{T}(\mathcal{S})$
\Comment{Tokenize $\mathcal{S}$}
\For{$i \gets 1$ to $L$} 
\State $y_i^T \gets \mathcal{T}(y_i)$ 
\Comment{Tokenize $y_i$}
\State $l_i \gets |y_i^T|$ 
\Comment Number of tokens in $y_i^T$
\State logits = $\mathcal{M}(\mathcal{S}^T + y_i^T)$ 
\Comment{Run the LLM model forward, the size of logits is $|\mathcal{S}^T + y_i^T| \times |\mathcal{V}|$}
\For{$j \gets 1$ to $l_i$}
\State y\_logits[j] $\gets$ logits[-$l_i$-2+j, $y_i^T$[j]]
\Comment Logits for $y_i^T$
\EndFor
\State class\_logits[i] $\gets$ y\_logits.sum
\Comment Logits corresponding to class $y_i$
\EndFor
\State y\_log\_pdf $\gets$ \texttt{CrossEntropy}(logits=class\_logits, target=$y$)
\end{algorithmic}
\end{algorithm*}
%
\begin{algorithm*}
\caption{Computing the log probability distribution function of a numerical target $y$ \citep{requeima2024llm}}
\label{alg:numerical_logprobs}
\begin{algorithmic}[1]
\Require $\mathcal{M}$: LLM model; $\mathcal{T}$: tokenizer
\Require $n$: number of digits after decimal point for $y$
\Require $\mathcal{A} = \{``0", ``1", \ldots, ``9", ``-", ``.", ``\langle t \rangle",  ``\langle s \rangle" \}$: set of allowed characters for $y$
\Require $\mathcal{S}$: text string preceding $y$ to condition on
\State non\_numeric\_mask $\gets$ all\_tokens $\notin \mathcal{T}(\mathcal{A})$ \Comment{Mask of non-numeric tokens}
\State $y^T \gets \mathcal{T}(\texttt{str}(y))$ \Comment{Tokenize $y$}
\State $l \gets |y^T|$ 
\Comment Number of tokens in $y^T$
\State logits = $\mathcal{M}(\mathcal{T}(\mathcal{S}) + y^T)$ \Comment{Run the LLM model forward}
\State y\_logits $\gets$ logits[-$l$-1:-1] \Comment{Logits corresponding to $y$}
\State y\_logits[non\_numeric\_mask] $\gets$ -100 \Comment{Mask out non-numeric tokens}
\State y\_log\_pmf $\gets$ \texttt{CrossEntropy}(logits=y\_logits, targets=$y^T$).sum   \Comment{Probability mass of bin that includes $y$}
\State y\_log\_pdf $\gets$ y\_logp\_mf + $n\log{10}$   \Comment{Convert probability mass to continuous likelihood}
\end{algorithmic}
\end{algorithm*}
%
\subsection{Sample Prompts}
\label{app:sample_prompts}
In this section, we provide sample prompts for various experiments.
%
\subsubsection{Wine Quality Experiment Sample Prompt}
\label{app:wine_quality_prompt}
A sample prompt with one training example, plus test example features, and $\langle d \rangle$ = ``:", $\langle s \rangle$ = ``;", and $\langle t \rangle$ = ``\textbackslash n" is:
%
\begin{tcolorbox}[colback=green!2,colframe=green!40!black]
``The data contains features that determine the quality of wine. Predict the alcohol content and the quality score of each wine based on the features.\textbackslash nfixed\_acidity: 6.2; volatile\_acidity: 0.23; citric\_acid: 0.35; residual\_sugar: 0.7; chlorides: 0.051; free\_sulfur\_dioxide: 24.0; total\_sulfur\_dioxide: 111.0; density: 0.992; pH: 3.37; sulphates: 0.43; 
color: white; alcohol: 11.0; quality: 3\textbackslash nfixed\_acidity: 9.9; volatile\_acidity: 0.49; citric\_acid: 0.23; residual\_sugar: 2.4; chlorides: 0.087; free\_sulfur\_dioxide: 19.0; total\_sulfur\_dioxide: 115.0; density: 0.995; pH: 2.77; sulphates: 0.44; color: white;"
\end{tcolorbox}
%
\subsubsection{Movies Box Office Sample Prompt}
\label{app:movies_prompt}
A sample prompt with one training example, plus test example features, and $\langle d \rangle$ = ``:", $\langle s \rangle$ = ``;", and $\langle t \rangle$ = ``\textbackslash n" is:
%
\begin{tcolorbox}[colback=green!2,colframe=green!40!black]
``Each example contains 10 columns: Movie Name, Revenue in Millions of Dollars, Rating, and 8 genre tags (Adventure, Comedy, Family, Action, Fantasy, Thriller, Drama, and Horror). Predict the movie rating and genre tags.\textbackslash nMovie Name:Kung Fu Panda 4;ID:36;Revenue in \$Millions:547.7;Rating:7.1;Adventure:No;Comedy:No;Family:Yes;Action:Yes;Fantasy:Yes;Thriller:No;Drama:No;Horror:\\No\textbackslash nMovie Name:Speak No Evil;ID:120;Revenue in \$Millions:76.8;"
\end{tcolorbox}
%
\subsubsection{Olympic Games Dataset Prompt}
\label{app:medals_prompt}
A sample prompt with one training example, plus test example features, and $\langle d \rangle$ = ``:", $\langle s \rangle$ = ``;", and $\langle t \rangle$ = ``\textbackslash n" is:
%
\begin{tcolorbox}[colback=green!2,colframe=green!40!black]
``Each example contains five columns: Olympic Year, Country, Bronze Medal Count, Silver Medal Count, and Gold Medal Count that describe what type and how many medals a country won at the Olympic games that year. Predict the number of silver and gold medals won by that country in that year.\textbackslash nOlympic Year:2020;Country:Netherlands;Bronze Medal Count:14;Silver Medal Count:12;Gold Medal Count:10\textbackslash nOlympic Year:2024;Country:USA;Bronze Medal Count:42;'"
\end{tcolorbox}
%
\subsubsection{Imputation Prompt}
\label{app:imputation_prompt}
A sample prompt with one training example, plus test example features, and $\langle d \rangle$ = ``:", $\langle s \rangle$ = ``;", and $\langle t \rangle$ = ``\textbackslash n" is:
%
\begin{tcolorbox}[colback=green!2,colframe=green!40!black]
``Each example contains four columns: Country, Silver Medal Count, Bronze Medal Count, and Gold Medal Count that describe what type and how many medals a country won at the Paris 2024 olympics.\textbackslash nCountry:Thailand;Silver Medal Count:3;Bronze Medal Count:2;Gold Medal Count:1\textbackslash nCountry:Slovakia;Silver Medal Count:0;Bronze Medal Count:1;"
\end{tcolorbox}
%
\subsection{Datasets}
\label{app:datasets}
%
\paragraph{Classification}

For the few-shot classification experiments, we utilize the nine datasets introduced in \citet{hegselmann2023tabllm}. The serialized versions of these datasets are obtained from \citet{hegselmann2023github}. These datasets are:
\begin{itemize}
    \item \textbf{Bank}~\citep{bank_marketing_222} contains records relevant to a direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe to a term deposit (binary classification). It consists of $45,211$ rows and $16$ features; $5,289$ labels are positive. 
    
    \item \textbf{Blood}~\citep{blood_transfusion_service_center_176} contains $748$ donor records from the Blood Transfusion Service Center in Taiwan. The classification task is to predict whether a donor returned for another blood donation.
    
    \item \textbf{Calhousing}~\citep{pace1997sparse} has entries of houses found in a given California district and some summary stats about them based on the 1990 U.S. census data. Following \citep{hegselmann2023github}, the goal is to predict whether the house value is below or above the median in its district.
 
    \item \textbf{Cars}~\citep{car_evaluation_19} contains records of various cars, characterized by six attributes. The task is a multiclass classification problem to evaluate the state of each car.
    
    \item \textbf{Creditg}~\citep{creditg_1994} contains $1000$ records, each described by $20$ attributes, and the task is to classify individuals as either good or bad credit risks. Of these records, $700$ are classified as good credit risks.
    
    \item \textbf{Diabetes}~\citep{smith1988diabetes}: originally from the National Institute of Diabetes and Digestive and Kidney Diseases, this dataset aims to diagnostically predict whether a patient has diabetes based on specific diagnostic measurements. Among the records, $268$ cases are positive for diabetes.
    
    \item \textbf{Heart}~\citep{detrano1989international} combines records from four hospitals in Cleveland, Hungary, Switzerland, and Long Beach V. The task is to predict the presence of heart disease in patients. Among the $918$ patients, $508$ are diagnosed as positive for heart disease.
    
    \item \textbf{Income}~\citep{income_1996} contains records of $48,842$ individuals with $12$ attributes collected in the 1994 U.S. Census. The classification task is to predict 
    whether annual income of an individual exceeds \$50K. The dataset has $11,687$ positive labels.
    
    \item \textbf{Jungle}~\citep{jungle_2014} consists of $44,819$ endgame positions from Jungle Chess. Each position is described by $6$ attributes, and the task is to predict whether the white player wins. Among the records, $23,062$ are positive outcomes for the white player.
\end{itemize}

\paragraph{Multi-target prediction}

For the multi-target prediction in \cref{sec:multi_target_results}, we use the following datasets: 

\begin{itemize}
    \item \textbf{Wine Quality}~\citep{wine_quality_186} contains records of Portuguese wines, each characterized by 11 physiochemical attributes. The original task is to predict wine quality on a scale from 0 to 10. To evaluate multi-target prediction capabilities, we modified the task to predict both the wine quality (categorical) and alcohol percentage (numerical).

    \item \textbf{Movies Box office Dataset (2000-2024)}~\citep{jilla2024movies} provides a comprehensive analysis of global box office performance from 2000 to 2024. Each row represents a movie and includes attributes such as release year, genres, production budget, worldwide gross, and additional descriptive features. The dataset contains $4955$ movies. 

    \item \textbf{Olympic Games (1994-2024)}~\citep{ismail2024olympic} is a database of medals awarded at the Summer and Winter Olympic Games from 1994 to 2024. Each table in the database corresponds to a specific Olympic Games, detailing medal counts by participating countries. Each row includes the country code and the number of gold, silver, and bronze medals won by the respective country.

\end{itemize}
%
In \cref{sec:missing_data_results}, we evaluate using the \textbf{Cars} and \textbf{Wine Quality} datasets, while in \cref{sec:imputation_results}, we use a subset of the \textbf{Olympic Games} dataset.

\subsection{Uncertainty Example}
%
\begin{figure}[h]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{diagrams/uncertainty_plot.pdf}}
\vspace{-2mm}
\caption{\textbf{JoLT predictions with uncertainty.} Median point estimates and 95$\%$ confidence intervals for predictions made by an JoLT using 100 samples with top-$p=$0.9 on the first 10 test examples on the wine quality dataset  \citep{wine_quality_186}.}
\label{fig:uncertainty}
\end{center}
\vskip -0.2in
\end{figure}
%

\subsection{Additional Experimental Results}
In this section, we provide tabular versions of the results that were presented as plots in the main paper.
%
    For any experiment with a regression target, we use \textit{top}-1 sampling. The one exception is in the Movies experiment, where we use 50 samples and take the median as the point estimate.
\input{tables/classification_results}
\input{tables/multi_column_results}
\input{tables/wine_missing}
\input{tables/cars_missing}
\input{tables/imputation_results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

