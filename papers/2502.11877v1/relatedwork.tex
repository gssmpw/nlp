\section{Related Work}
%
In this section, we survey related methods for tabular prediction and imputation.
%
For a more complete treatment, refer to \citet{borisov2022deep} for a survey of deep learning methods for tabular data and \citet{fang2024large,lu2024large} for applying LLMs to tabular data.

\textbf{Classification and Multi-target prediction}
%
TabLLM \citep{hegselmann2023tabllm} fine-tunes an LLM for single target classification only.
%
It can incorporate side information, but performance is limited in the few-shot setting.
%
TabPFN \citep{hollmann2025tabpfn} is an effective ICL method for both classification and regression and offers uncertainty in the form of quantiles.
%
However, it is restricted to handling numerical and categorical data, is unable to incorporate text or side information, cannot predict multiple targets at once, and does not perform well in the low-shot setting.
%
LLM Processes~\citep{requeima2024llm} support multi-target regression with uncertainty estimates and side information but do not handle classification or heterogeneous targets.
%
JoLT extends LLM Processes by enabling classification and supporting heterogeneous multiple targets in the tabular data setting.
%
Carte \citep{kim2024carte} uses a graph-attentional network pretrained across multiple tables, and subsequently fine-tuned on a downstream dataset.
%
It can incorporate side information while performing single-target regression or classification, but does not provide uncertainty.
%
Finally, GPs can make multiple target probabilistic predictions on heterogeneous data \citep{moreno2018heterogeneous}, but require training and cannot easily incorporate side information.

\textbf{Handling Missing Data and Data Imputation}
%
There exists a myriad of widely used imputation methods, including mean, median, and mode imputation, k-Nearest Neighbors~\citep{Troyanskaya2001MissingVE}, MICE~\citep{little2002statistical}, and tree-based methods like MissForest~\citep{Stekhoven2011MissForestN}.
%
However, they are restricted to using numerical or categorical data, and hence, are unable to exploit side information. Furthermore, they do not provide any estimates of uncertainty or distributions for the imputed values. 
%
In contrast, Bayesian methods, such as Gaussian Processes (GPs)~\citep{Rasmussen2006Gaussian} and Gaussian Copula~\citep{NelsenRogerB1999Aitc}, offer the advantage of providing uncertainty estimates. 
%
While Bayesian methods have been extended to handle missing data~\citep{zhaocopula2022,bahram2023gp}, their widespread adoption is hindered by scalability limitations and the challenges associated with incorporating side information effectively.
%
Recent work has also explored the use of LLMs for data imputation.
%
\citet{anonymous2024contextdriven} proposes a nearest-neighbor-based imputation method that operates in the embedding space of an LLM.
%
However, this method does not provide uncertainty estimates for the imputed values. 
%
\citet{Ding2024DataIU} and \citet{hayat2024claimdataenhancingimputation} fine-tune LLMs to handle missing data and report notable improvements on several downstream tasks. 
% 
To the best of our knowledge, there are no methods that utilize LLMs to perform data imputation in low-shot scenarios.
%
In contrast, JoLT handles missing data automatically, and if imputed values are required, JoLT can impute missing data with uncertainty, has the ability to incorporate side information, and operate well in the low-shot setting.
%