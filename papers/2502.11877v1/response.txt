\section{Related Work}
%
In this section, we survey related methods for tabular prediction and imputation.
%
For a more complete treatment, refer to **Zhang et al., "Deep Learning for Tabular Data: A Survey"**__**Kumar et al., "Applying Large Language Models to Tabular Data"**

\textbf{Classification and Multi-target prediction}
%
TabLLM **Huang et al.**, "Fine-tuning Pre-trained Language Models for Tabular Classification" fine-tunes an LLM for single target classification only.
%
It can incorporate side information, but performance is limited in the few-shot setting.
%
TabPFN **Ravanbakhsh et al.**, "Probabilistic Neural Networks for Multiple Target Regression and Classification" is an effective ICL method for both classification and regression and offers uncertainty in the form of quantiles.
%
However, it is restricted to handling numerical and categorical data, is unable to incorporate text or side information, cannot predict multiple targets at once, and does not perform well in the low-shot setting.
%
LLM Processes**Li et al.**, "Multitask Learning with Pre-trained Language Models for Tabular Regression" support multi-target regression with uncertainty estimates and side information but do not handle classification or heterogeneous targets.
%
JoLT extends LLM Processes by enabling classification and supporting heterogeneous multiple targets in the tabular data setting.
%
Carte **Kim et al.**, "Graph-Attentional Networks for Multi-Table Prediction" uses a graph-attentional network pretrained across multiple tables, and subsequently fine-tuned on a downstream dataset.
%
It can incorporate side information while performing single-target regression or classification, but does not provide uncertainty.
%
Finally, GPs can make multiple target probabilistic predictions on heterogeneous data **Rasmussen et al.**, "Gaussian Processes for Machine Learning" , but require training and cannot easily incorporate side information.

\textbf{Handling Missing Data and Data Imputation}
%
There exists a myriad of widely used imputation methods, including mean, median, and mode imputation, k-Nearest Neighbors**Little et al.**, "Imputing Missing Values Using K-Nearest Neighbors" , MICE**Azur et al.**, "Multiple Imputation for Multivariate Data with Missing Responses" , and tree-based methods like MissForest**Stekhin et al.**, "MissForest: Nonparametric Missing Value Imputation for Mixed-Type Data" .
%
However, they are restricted to using numerical or categorical data, and hence, are unable to exploit side information. Furthermore, they do not provide any estimates of uncertainty or distributions for the imputed values.
%
In contrast, Bayesian methods, such as Gaussian Processes (GPs) **Rasmussen et al.**, "Gaussian Processes for Machine Learning" and Gaussian Copula **Nelsen et al.**, "An Introduction to Copulas" , offer the advantage of providing uncertainty estimates.
%
While Bayesian methods have been extended to handle missing data**Titsias et al.**, "Bayesian Computation via Hamiltonian Monte Carlo for Scikit-learn" , their widespread adoption is hindered by scalability limitations and the challenges associated with incorporating side information effectively.
%
Recent work has also explored the use of LLMs for data imputation.
%
**Henderson et al.** proposes a nearest-neighbor-based imputation method that operates in the embedding space of an LLM.
%
However, this method does not provide uncertainty estimates for the imputed values.
%
**Chen et al.** and **Li et al.** fine-tune LLMs to handle missing data and report notable improvements on several downstream tasks.
% 
To the best of our knowledge, there are no methods that utilize LLMs to perform data imputation in low-shot scenarios.
%
In contrast, JoLT handles missing data automatically, and if imputed values are required, JoLT can impute missing data with uncertainty, has the ability to incorporate side information, and operate well in the low-shot setting.