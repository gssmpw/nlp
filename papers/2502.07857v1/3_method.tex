\section{SEQUENTIAL NON-ANCESTOR PRUNING}
\label{sec:method}

Causal effect estimation requires knowledge about the causal graph.
If causal relations are not known a priori, we can use causal discovery methods to learn them from data.
In many settings, we might have access to a large set of variables, but we are only interested in the causal effects between a small set of \emph{target variables}. Standard \emph{global} causal discovery on all variables can estimate the \emph{identifiable} causal effects between targets, as well as valid adjustment sets, but it might be computationally inefficient, since it might also learn parts of the graph that are uninformative for the causal effects of interest. 
Alternatively, focusing on the target variables and only learning the causal relations between them might lead to confounded causal effects and less identifiable causal relations than we would learn in global causal discovery.

Our goal is to estimate the identifiable causal effects between the targets as statistically efficiently as global causal discovery methods, but in a more computationally efficient way than estimating the complete graph.
We formalize this goal as a specific task as follows:

\begin{definition}%[Targeted causal effect estimation with an unknown graph]
\label{def:task}
Given a joint distribution $p$ over variables $\mathbf{V}$ and targets $\mathbf{T} \subseteq \mathbf{V}$,  we define \emph{targeted causal effect estimation with an unknown graph} as the task of estimating in a computationally and statistically efficient way the interventional distributions $P(T_i|do(T_j))$, for all possible pairs $T_i,T_j \in \mathbf{T}$.
\end{definition}

Local causal discovery methods, such as \citep{wang2014discovering, gupta2023local,maasch2024local}, address the computational efficiency by only learning a local structure in the neighborhood of a pair of target variables for which we know which is the cause and which is the effect a priori.
On the other hand, these methods are  not as statistically efficient as global causal discovery, since they cannot identify optimal adjustment sets. In particular, they either learn local adjustment sets relying on the parents of the treatment, which are suboptimal in terms of variance, or other valid adjustment sets based on groups of variables, which might not include the optimal adjustment sets. 

In this work, we address this task by progressively pruning definite non-ancestors of the target variables. 
We first show that definite non-ancestors of the targets are not needed to identify valid and statistically efficient adjustment sets. While \citet{guo2023variable} show that non-ancestors of the outcome are uninformative, our results show that they are also unnecessary to identify the causal structure between the targets and their (optimal) adjustment sets.
Then, we present \acf{SNAP}, a framework to progressively identify definite non-ancestors, inspired by low-order constraint-based causal discovery.
We provide all proofs for the following results in App.~\ref{app:proof}.

\subsection{Possible Ancestors Are All You Need}
\label{sec:pruning}
In this section, we show that definite non-ancestors of the target variables are not needed to identify statistically efficient adjustment sets for the causal effects between the targets. \cite{guo2023variable} show that non-ancestors of the target variables are not needed as part of the statistically efficient adjustment sets for the targets. Here, we do not know the non-ancestors of the target variables, since we learn the causal structure as a CPDAG $G$ and we can at best identify the set of possible ancestors of the targets $PossAn_G(\mathbf{T})$. Additionally, we show that definite non-ancestors of the targets will not have any effect on the orientations of the causal structure for the targets or their possible ancestors, and hence they can be safely ignored. 

In general, we might not be able to estimate the possible ancestors of the targets without reconstructing the whole CPDAG.
On the other hand, we can overestimate this set and consider a set $\mathbf{V}^*$ that contains  $PossAn(\mathbf{T})$. If $\mathbf{V}^*$ also contains all its possible ancestors, we then show that learning a CPDAG on it will provide the same results as learning a complete CPDAG and restricting it to $\mathbf{V}^*$. 
We formalize this results as follows:

\begin{restatable}[]{lemma}{lemmaancestors}
\label{thm:ancestors}
Let $G$ be a full \ac{CPDAG} over variables $\mathbf{V}$. 
Let $\mathbf{V}^* \subseteq \mathbf{V}$ be a possibly ancestral set of nodes, i.e. $PossAn_G(\mathbf{V}^*) \subseteq \mathbf{V}^*$.
Let $G(\mathbf{V})|_{\mathbf{V}^*}$ be the induced subgraph of $G$ over $\mathbf{V}^*$ and let $G(\mathbf{V}^*)$ be the CPDAG over variables $\mathbf{V}^*$. Then we have $G(\mathbf{V})|_{\mathbf{V}^*} = G(\mathbf{V}^*)$.
\end{restatable}

Lemma~\ref{thm:ancestors} implies that running a global causal discovery algorithm on a possibly ancestral set  that includes the possible ancestors of the targets, solves the task, allowing for the estimation of the causal effects between the targets in an as  statistically efficient way as learning the full CPDAG.
In particular, the restricted \ac{CPDAG} $G(\mathbf{V}^*)$ has the same canonical, parent, ancestor and optimal adjustment sets for pairs of targets $\mathbf{T}$ as the full \ac{CPDAG} $G(\mathbf{V})$. 
As discussed by \citet{guo2023variable} it also allows for more general optimal sets beyond valid adjustments.
Intuitively, the challenge is then how to identify as many definite non-ancestors as possible in a computationally efficient way.
In the next section, we propose two methods to address this challenge.

\subsection{The SNAP Framework}
\label{sec:snap}

If the possibly ancestral set $\mathbf{V}^*$, which contains the possible ancestors of the targets, is much smaller than the total number of nodes, then discovering only $G(\mathbf{V}^*)$, instead of the full CPDAG $G(\mathbf{V})$, can save considerable computational resources.
Furthermore, $G(\mathbf{V}^*)$ allows us to use statistically efficient adjustment sets.
Without background knowledge on a suitable $\mathbf{V}^*$, we approach the discovery of $G(\mathbf{V}^*)$ using an iterative framework, which we call \acl{SNAP}. We propose two implementations of this framework: SNAP$(k)$ (Alg.~\ref{alg:snap(k)}) and SNAP$(\infty)$ (Alg.~\ref{alg:snap(inf)}).

SNAP$(k)$ is a causal discovery algorithm that uses information about ancestry to progressively eliminate definite non-ancestors of targets, removing them from $\mathbf{V}^*$.
In particular, SNAP$(k)$ adapts the PC-style skeleton search, iteratively increasing conditioning set sizes of \ac{CI} tests.
At every iteration $i$, SNAP$(k)$ computes a partially oriented graph $\hat{G}^i$ by orienting v-structures according to the intermediate skeleton $\hat{U}^i$ and separating sets discovered so far.
Then, $\hat{G}^i$ is used to identify and eliminate a subset of the definite non-ancestors of the targets, and SNAP$(k)$ continues to the next iteration only on the remaining variables $\hat{V}^{i+1}$.
Thus, SNAP$(k)$ considers fewer and fewer variables as the size of the conditioning set increases.
In practice, we see that many non-ancestors are identified already by marginal tests, leading to significantly fewer higher order tests.

\begin{algorithm*}
\caption{Sequential Non-Ancestor Pruning - SNAP$(k)$}
\label{alg:snap(k)}
\footnotesize{
\begin{algorithmic}[1]
    \Require Vertices $\mathbf{V}$, targets $\mathbf{T} \subseteq \mathbf{V}$, Maximum order $k$
    \State $\hat{\mathbf{V}}^{0} \gets \mathbf{V}$, $\hat{U}^{-1} \gets$ fully connected undirected graph over $\mathbf{V}$
    \For{$i \in 0..k$}
        \State $\hat{U}^i \gets$ induced subgraph of $\hat{U}^{i-1}$ over $\hat{\mathbf{V}}^{i}$
        \For{$X \in \hat{\mathbf{V}}^{i}$, $Y \in Adj_{\hat{U}^i}(X)$} \Comment{Learn skeleton step at order $i$}
                \For{$\mathbf{S} \subseteq Adj_{\hat{U}^i}(X) \setminus \{Y\}$ s.t. $|\mathbf{S}| = i$}
                        \If{$X \perp\kern-6pt\perp Y | \mathbf{S}$}
                            \State Delete the edge $X - Y$ from ${\hat{U}^i}$
                            \State $sepset(X,Y) \gets sepset(Y,X) \gets \mathbf{S}$
                            \State \textbf{break}
                        \EndIf
                \EndFor
        \EndFor
        \If{$i < 2$} \Comment{Orient v-structures}
            \State$\hat{G}^i \gets$ OrientVstructPC($\hat{U}^i, sepset$) (Alg.~\ref{alg:pc-v})
        \Else
            \State$\hat{G}^i, sepset \gets$ OrientVstructRFCI($\hat{U}^i, sepset$) (Alg.~\ref{alg:rfci-v})
            \State $\hat{U}^i \gets$ skeleton of $\hat{G}^i$
        \EndIf
        \State $\hat{\mathbf{V}}^{i+1} \gets$ all $V \in \hat{\mathbf{V}}^{i}$ with a possibly directed path to any $T \in \mathbf{T}$ in $\hat{G}^i$ \Comment{Prune non-ancestors}
    \EndFor
\State $\hat{G}^k \gets$ induced subgraph of $\hat{G}^k$ over $\hat{\mathbf{V}}^{k+1}$    \State\Return{$\hat{\mathbf{V}}^{k+1}, \hat{G}^k$}
\end{algorithmic}
}
\end{algorithm*}

\begin{algorithm}
\caption{SNAP$(\infty)$ }
\footnotesize{
\label{alg:snap(inf)}
\begin{algorithmic}[1]
    \Require Vertices $\mathbf{V}$, targets $\mathbf{T} \subseteq \mathbf{V}$
    \State $\hat{\mathbf{V}}, \hat{G} \gets \text{SNAP}(\mathbf{V}, \mathbf{T}, |\mathbf{V}|-2)$
    \Repeat \Comment{Apply Meek rules}
        \If{$X \to Z - Y$ in $\hat{G}$ and $X \notin Adj_{\hat{G}}(Y)$}
            \State Orient $Z - Y$ as $Z \to Y$ in $\hat{G}$
        \EndIf
        \If{$X \to Z \to Y$, and $X - Y$ in $\hat{G}$}
            \State Orient $X - Y$ as $X \to Y$ in $\hat{G}$
        \EndIf
        \If{$X \to Z \gets Y, X - V - Y, V - Z$ in $\hat{G}$ and $X \notin Adj_{\hat{G}}(Y)$}
            \State Orient $V - Z$ as $V \to Z$ in $\hat{G}$
        \EndIf
    \Until{no edges can be oriented}
    \State $\hat{\mathbf{V}} \gets$ all $V \in \hat{\mathbf{V}}$ with a possibly directed path to any $T \in \mathbf{T}$ in $\hat{G}$
    \State $\hat{G} \gets$ induced subgraph of $\hat{G}$ over $\hat{\mathbf{V}}$
    \State\Return{$\hat{G}$}
\end{algorithmic}
}
\end{algorithm}

In \ac{SNAP} we orient v-structures in $\hat{G}^i$ using only conditional independence of a maximum order $i$ to identify definite non-ancestors.
This requires particular care, since in general \ac{CI} test results restricted to a maximum order can lead to conflicting v-structure orientations, which are oriented as bidirected edges, as shown by Fig.~\ref{fig:bidirected_example_main} for order $0$.
\citet{wienobst2020recovering} show that if \emph{all} \ac{CI} test results up to order $k$ are available, then these bidirected edges indicate that neither variable is the possible ancestor of the other.
Surprisingly, if we instead only perform a subset of \ac{CI} tests based on the skeleton search up to order $k$, as in anytime FCI \citep{pmlr-vR3-spirtes01a}, and then orient v-structures as in PC (Alg.~\ref{alg:pc-v}), this can lead to incorrect information about possible ancestry, as shown in this example:

\begin{figure}[t]
    \centering
    \begin{subfigure}{.25\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/bidirected_example.pdf}
        \caption{}
        \label{fig:bidirected_example_main}
    \end{subfigure}
    \hspace{.1\linewidth}
    \begin{subfigure}{.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rfci_example.pdf}
        \caption{}
        \label{fig:rfci_example}
    \end{subfigure}
    \caption{Given the DAG in Fig.~\ref{fig:bidirected_example_main} denoted by black edges, $A$ and $B$ are marginally dependent with conflicting v-structures $U \to A \gets B$ and $A \to B \gets V$ if we orient v-structures at order 0, as shown by the red bidirected edge.
    Given the DAG in Fig.~\ref{fig:rfci_example}, performing skeleton search up to order 3 and then orienting v-structures as in PC (Alg.~\ref{alg:pc-v}) leads to an incorrect bidirected edge $A \leftrightarrow B $.
    Details in App.~\ref{app:vstruct}.}
\end{figure}

\begin{example}
\label{example:rfci}
Consider the DAG in Fig~\ref{fig:rfci_example}.
At order 3, skeleton search may find $A \perp\kern-6pt\perp G | \{E,C,X\}$ and remove the edge $A-G$, before testing $A \perp\kern-6pt\perp X | \{G,U,V\}$.
Then, finding $X \perp\kern-6pt\perp B | \{G,U,V\}$ removes the edge $X - B$.
Orienting v-structures as in PC (Alg.~\ref{alg:pc-v}) at order 3 creates the conflicting v-structures $X \to A \gets B$ and $A \to B \gets V$, resulting in the bidirected edge $A \leftrightarrow B$, even though $A \to B$ is in the true DAG.
\end{example}

To overcome this, we show in App~\ref{app:proof} that orienting v-structures as in RFCI \citep{colombo2012learning} (Alg.~\ref{alg:rfci-v}) leads to correct information about possible ancestry at any order of the PC-style skeleton search, in the causally sufficient case.
Since the RFCI orientation involves performing additional CI tests, we show that it is only required above order 1, which greatly reduces its overhead.
Then, as Theorem~\ref{thm:snap_is_sound} states, SNAP$(k)$ only removes definite non-ancestors of targets, and the remaining nodes form a possibly ancestral set.

\begin{restatable}[]{theorem}{snapsound}
\label{thm:snap_is_sound}
Given oracle conditional independence tests, at each step $i=0, \dots, k$ of \ac{SNAP}$(k)$ (Alg.~\ref{alg:snap(k)}) $\hat{\mathbf{V}}^{i+1}$ contains $PossAn(\mathbf{T})$. Additionally, $\hat{\mathbf{V}}^{i+1}$ is a possibly ancestral set, i.e. $PossAn_G(\hat{\mathbf{V}}^i) \subseteq \hat{\mathbf{V}}^{i+1}$.
\end{restatable}

As shown by Corollary~\ref{cor:snap-prefiltering}, we can stop SNAP$(k)$ early at any maximum iteration $k$ to obtain the remaining variables $\hat{\mathbf{V}}^{k+1}$, and subsequently run a sound and complete global causal discovery algorithm of our choice only on $\hat{\mathbf{V}}^{k+1}$ to obtain a CPDAG that we can use for targeted causal effect estimation.
We refer to this approach as prefiltering with SNAP$(k)$.

\begin{restatable}[]{corollary}{snapkprefiltering}
\label{cor:snap-prefiltering}
Given oracle conditional independence tests, a sound and complete causal discovery algorithm on $\hat{\mathbf{V}}^{k+1}$, the output of \ac{SNAP}(k), will return a CPDAG on $\hat{\mathbf{V}}^{k+1}$, which is the same as the induced subgraph of the full CPDAG $G(\mathbf{V})$ restricted to $\hat{\mathbf{V}}^{k+1}$. Additionally, this CPDAG will contain all informative adjustment sets for estimating causal effects between the targets $\mathbf{T}$.
\end{restatable}

Similarly, local algorithms by \citep{wang2014discovering} and \citep{gupta2023local} output the same parents and undirected neighbors when ran only on $\hat{\mathbf{V}}^{k+1}$ instead of all nodes.
Thus, standard discovery methods on $\hat{\mathbf{V}}^{k+1}$ provide us with adjustment sets to solve the targeted causal effect estimation task with an unknown graph.

Furthermore, \ac{SNAP}$(k)$ can be extended to obtain a stand-alone causal discovery algorithm.
We call this algorithm \ac{SNAP}$(\infty)$ and show it in Alg.~\ref{alg:snap(inf)}.
\ac{SNAP}$(\infty)$ runs \ac{SNAP}$(k)$ \emph{until completion} with $k=|\mathbf{V}|-2$.
Then, it applies Meek's rules on the resulting partially oriented graph $\hat{G}^k$, identifies definite non-ancestors one more time, which in this case will provide exactly the set of possible ancestors of the targets, and returns the induced subgraph of $\hat{G}^k$ over the remaining variables.
Theorem~\ref{thm:snap_is_complete} states that \ac{SNAP}$(\infty)$ is sound and complete over the possible ancestors of the targets.

\begin{restatable}[]{theorem}{snapcomplete}
\label{thm:snap_is_complete}
Given oracle conditional independence tests, let $\hat{G}$ be the output of graph of \ac{SNAP}$(\infty)$ for targets $\mathbf{T}$. Then, \ac{SNAP}$(\infty)$ returns $\mathbf{\hat{V}} = PossAn(\mathbf{T})$. Additionally, \ac{SNAP}$(\infty)$ is sound and complete over the possible ancestors $\mathbf{T}$, i.e.
$
\hat{G} = G|_{PossAn(\mathbf{T})}.
$
\end{restatable}

\subsection{Complexity Analysis}

In this section, we show that the worst-case computational complexity of \ac{SNAP}$(\infty)$ in terms of \ac{CI} tests matches the complexity of PC
for graphs with maximum degree $d_{max} \geq 2$.
The worst-case complexity for PC is determined by its skeleton search, which is $\mathcal{O}(|\mathbf{V}|^{d_{\max}+2})$ \citep{spirtes2000causation}, where $|\mathbf{V}|$ is the number of nodes and $d_{\max}$ is the maximum degree.
The worst-case complexity of \ac{SNAP}$(\infty)$ is given by the complexity of the skeleton search and the RFCI orientation rules (Algorithm~\ref{alg:rfci-v}).
We present a lemma that states that the worst-case complexity of the RFCI orientation rules is polynomial in the number of nodes.

\begin{restatable}[]{lemma}{rfcicomplexity}
\label{lem:rfci_complexity}
The worst-case complexity of the RFCI orientation rules (Algorithm~\ref{alg:rfci-v}) in terms of \ac{CI} tests performed is at most $\mathcal{O}(|\mathbf{V}|^4)$ \ac{CI} tests, where $|\mathbf{V}|$ is the number of nodes.
\end{restatable}


We provide a formal proof in App.~\ref{app:complexity}.
Intuitively, Algorithm~\ref{alg:rfci-v} performs two \ac{CI} tests for each unshielded triple, leading to $\mathcal{O}(|\mathbf{V}|^3)$ \ac{CI} tests.
Then, for each $\mathcal{O}(|\mathbf{V}|^2)$ independences found, it finds a minimal separating set in $\mathcal{O}(|\mathbf{V}|^2)$ \ac{CI} tests.
This results in $\mathcal{O}(|\mathbf{V}|^3 + |\mathbf{V}|^2 \cdot |\mathbf{V}|^2) = \mathcal{O}(|\mathbf{V}|^4)$ \ac{CI} tests.
By combining the worst-case complexity of the PC-style skeleton search and the RFCI orientation rules, can then show that the worst-case computational complexity of SNAP($\infty$) is at most
$\mathcal{O}(|\mathbf{V}|^{d_{\max}+2} + |\mathbf{V}|^4)$. 

We now show that for graphs that allow a maximum degree $d_{max} \geq 2$, which include most realistic graphs, the complexity of the skeleton search dominates the complexity of the RFCI orientation rules, which means that on these graphs SNAP($\infty$) matches PC in terms of worst-case complexity.


\begin{restatable}[]{corollary}{snapcomplexity}
\label{col:snap_complexity}
For graphs with maximum degree $d_{max} \geq 2$, the worst-case computational complexity of SNAP($\infty$) in terms of \ac{CI} tests is $\mathcal{O}(|\mathbf{V}|^{d_{\max}+2})$, which matches the complexity of PC.
\end{restatable}

Intuitively this makes sense, since the RFCI orientation rules are dominated by the PC style skeleton search for dense enough graphs and in the worst case for SNAP, the ancestors of the targets include the whole graph. On the other hand, the average case complexity is intuitively substantially lower, as also shown empirically. Since this analysis can be quite complex, we only show a rough approximation of the expected number of possible ancestors in App.~\ref{app:expectedanc} as an indication for SNAP's average case complexity.
