\section{EXPERIMENTS}
\label{sec:experiments}

\paragraph{Baselines.} We evaluate \ac{SNAP}$(\infty)$, along with global algorithms PC \citep{spirtes2000causation}, MARVEL \citep{mokhtarian2021recursive} and FGES \citep{ramsey2017million}, the modified versions of local algorithms MB-by-MB \citep{wang2014discovering} and LDECC \citep{gupta2023local},  
and their combination with \ac{SNAP}$(0)$.
We choose \ac{SNAP}$(0)$ as it is the simplest prefiltering method, and it is already very effective in practice, as shown in App.~\ref{app:snapk}.
We do not compare to CBL \citep{watson2022causal}, since it requires that all non-target nodes are non-descendants of the targets.
Similarly, we do not compare to LDP \citep{maasch2024local}, since it requires known ancestral relationships between targets to estimate the groups of variables that can be used for adjustment.
We publish our code at \url{https://github.com/matyasch/snap}.

The local structure discovered by LDECC and MB-by-MB is sound regardless of the ancestral relationships between targets.
However, knowledge about the ancestral relationships is still needed for causal effect estimation.
Thus, we apply these methods on all targets separately and estimate if a target $X$ is an ancestor of another target $Y$ by testing their independence conditioned on the identified definite parents of $X$.

We also modify the publicly available implementation of local algorithms to speed up their execution time, as explained in App~\ref{sec:local_speed}.
We denote the modified methods as MB-by-MB* and LDECC*.
Following the authors, we use total conditioning \citep{pellet2008using} for Markov blanket discovery in MARVEL and  IAMB \citep{tsamardinos2003algorithms} in MB-by-MB* and LDECC*.
We follow Sec.~5.7.1 by \citet{mokhtarian2024recursive} to orient edges in MARVEL.

\begin{figure*}[t]
    \centering
    \includegraphics[width=.7\linewidth]{experiments/legend_small.pdf}
    \begin{subfigure}[b]{\linewidth}
        \begin{subfigure}[b]{0.23\linewidth}
            \centering
            \caption*{ $\quad$ d-separation tests}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
            \centering
            \caption*{$\quad$ Fisher-Z tests}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
            \centering
            \caption*{KCI tests}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\linewidth}
            \centering
            \caption*{$\chi^2$ tests}
        \end{subfigure}
        \includegraphics[width=\linewidth]{figures/test_per_node_unrest_std.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{\linewidth}
        \includegraphics[width=\linewidth]{figures/time_per_node_unrest_std.pdf}
    \end{subfigure}
    \caption{Number of \ac{CI} tests (top row) and computation time (bottom row) over number of nodes with $n_{\mathbf{T}}=4, \overline{d} = 3, d_{\max}=10$ and $n_{\mathbf{D}} = 1000$ data-points for different simulation settings in each column. The shadow area denotes the range of the standard deviation.
    SNAP $(\infty)$ is consistently one of the best methods.}
    \label{fig:test_and_time_per_node_unrest_std}
\end{figure*}

\paragraph{Metrics.} We compare the constraint-based algorithms in terms of the \ac{CI} tests they perform, and compare all algorithms, including FGES, in terms of computation time. To estimate the quality of the joint causal discovery and causal effect estimation, we report the intervention distance, i.e., the distance between the ground truth causal effect from the target $T$ to the target $T'$, and the predicted causal effect $\hat{\Theta}_{T, T'}$ for the same pair given an adjustment, which we define in App.~\ref{app:metrics}.
If the causal effect is identifiable from the CPDAG returned by a global algorithm or SNAP, then we estimate it using the optimal adjustment set \citep{henckel2022graphical} and $\hat{\Theta}_{T, T'}$ has a single value.

If the causal effect of $T$ on $T'$ is not identifiable, or the output is a local structure around $T$ by MB-by-MB* or LDECC*, then we estimate a set of possible causal effects $\hat{\Theta}_{T, T'}$ using the local structure around $T$ \citep{maathuis2009estimating}.
Following \citet{gradu2022valid}, we estimate causal effects using a separate set of samples after discovery.
Additionally, we also report the \Ac{SHD} between the ground truth and the estimated CPDAG, restricted to the possible ancestors of the target variables $\mathbf{T}$. For the subset of \emph{identifiable} causal effects, we  report the Adjustment identification distance \citep{henckel2024adjustment} for the parental, ancestor and optimal adjustment sets.

\paragraph{Synthetic data generation.}
We generate 100 random causal graphs with number of vertices $n_\mathbf{V} = 50, \dots, 300$, expected degree $\overline{d} = 2, \dots 4$ and maximum degree of $d_{\max} = 10$. For each graph, we generate $n_{\mathbf{D}} = 500, \dots, 10000$ number of data-points. For the linear Gaussian case, we sample edge weights from $[-3, -0.5] \cup [0.5, 3]$ with standard Gaussian noises.

For the binary data, we generate a conditional probability table according to the causal graph, where the  probabilities are sampled uniformly in  $[0,1]$.
For each graph, we sample randomly $n_\mathbf{T} = 2, 4, 6, 8$ targets.
To better show the general trends in the plots, we remove the top 5 and bottom 5 results for each method.

\subsection{Experimental Results}
We evaluate four settings: oracle \ac{CI} d-separation tests, Fisher-Z tests for the linear setting, KCI tests \citep{zhang2011kernel} for the linear setting, and $\chi^2$ test on binary data. For FGES, we use BIC \citep{schwarz1978estimating} for the linear setting and BDeu \citep{heckerman1995learning} for the binary setting. For KCI tests, we consider smaller graphs with $n_\mathbf{V} =5, \dots, 20$ nodes, because of computational constraints.
Fig.~\ref{fig:test_and_time_per_node_unrest_std} shows our results for $n_{\mathbf{T}}=4, \overline{d} = 3, d_{\max}=10$ and $n_{\mathbf{D}} = 1000$ 
in terms of the number of \ac{CI} tests performed and the execution time, over graphs with various number of nodes.

Under both metrics, SNAP$(\infty)$ performs consistently as one of the best methods across all domains, while the performance of other methods varies based on the setting. In particular, PC and LDECC* perform substantially more tests and are substantially slower for the oracle setting, while the gap is smaller for partial correlation and KCI tests. For the binary setting, LDECC* performs fewer tests, but is computationally comparable to SNAP$(\infty)$. On the contrary, MARVEL performs well with oracle tests, but requires more CI tests and is hence slower with linear Gaussian data. For KCI and $\chi^2$ tests, MARVEL performs comparably in the number of tests, but is substantially slower in  computation time. 
MB-by-MB* performs similarly to SNAP$(\infty)$ in terms of tests on most settings, except for KCI tests, but it is substantially slower for Fisher-Z and $\chi^2$ tests. FGES runs fastest when using the BIC score on linear data, but trails behind PC, LDECC* and SNAP$(\infty)$ on binary data using the BDeu score.

Fig.~\ref{fig:delta_time_per_node_unrest} shows the difference in computation time for the baselines methods and their versions combined with SNAP$(0)$, i.e., SNAP(k) with $k=0$, as a prefiltering method.
Prefiltering with SNAP$(0)$ can effectively improve the computation time in most settings.
Adding SNAP$(0)$ always improves the computation time of PC, MARVEL, and MB-by-MB*, and improves LDECC* on all domains except on binary data, while improving FGES on binary data. 
The corresponding figure for the number of CI tests and the computation times for each method with and without SNAP($0$) prefiltering (Fig.~\ref{fig:tests_and_time_per_node_unrest} in App.~\ref{app:additional_experiments}) shows the same trends.

In terms of causal effect estimation, our results in Fig.~\ref{fig:quality_per_node_unrest_std} and \ref{fig:quality_per_node_unrest} in App.~\ref{app:additional_experiments} show that all methods achieve comparable intervention distance, even though SNAP variants perform slightly worse in terms of \ac{SHD}. We show in
App~\ref{app:error_rates} that the difference is mostly due from pruning incorrectly some of the possible ancestors, and in App.~\ref{app:shd_opt} we demonstrate that this performance gap decreases substantially when we consider the \ac{SHD} of the induced CPDAG only over the targets and their oracle optimal adjustment sets.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{experiments/delta/legend_delta.pdf}
    \includegraphics[width=.9\linewidth]{figures/delta_time_per_node_unrest.pdf}
    \caption{Difference in computation time for each baseline  with and without SNAP$(0)$ for $n_{\mathbf{T}}=4, \overline{d} = 3, d_{\max}=10$ and $n_{\mathbf{D}} = 1000$ data-points.
    Prefiltering with SNAP($0$) improves the baselines in most settings.
    \label{fig:delta_time_per_node_unrest}
    }    
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{experiments/snapk/test_per_filter_order.pdf}
    \caption{Number of d-separation tests performed by SNAP($k$) during prefiltering for $k=0,1,2,3$ and SNAP($\infty$) (pink), to which we add the number of tests PC performs after prefiltering (blue) on graphs with $|\mathbf{V}| = 50, n_{\mathbf{T}}=4, \overline{d} = 5$ and $d_{\max}=10$.
    % PC without any prefiltering did not finish in two days.
    }
    \label{fig:test_per_filter_order}
\end{figure}

In App.~\ref{app:identifiable}, we provide results on a subset of graphs where all the targets have \emph{identifiable} causal effects, and each target has to be the ancestor or the descendant of at least on other target.
Fig.~\ref{fig:appendix_per_node_ident_std} and \ref{fig:appendix_per_node_ident} show that forcing targets to be identifiable is the most beneficial for LDECC* and MB-by-MB*, due to the identifiable targets being more likely to have identifiable parents.
However, SNAP variants remain competitive in terms of computational performance even in this setting.
Fig.~\ref{fig:aid_per_node_ident_std} and \ref{fig:aid_per_node_ident} show that SNAP achieves mostly comparable, but in some cases slightly higher Adjustment Identification Distances (AID) than other methods. AID counts how many times the learned adjustment sets for various criteria are invalid given the ground truth causal graph. An invalid adjustment set can be still useful for finite sample prediction of causal effects, which seems to be the case in our experiments, given the comparable intervention distance across methods.


In App.~\ref{app:over_targets}, we vary the number of targets and fix all other parameters.
Our results show that the performance of SNAP($\infty$) is constant in the number of targets, unlike local methods, for which the performances worsen substantially with more targets.
This highlights the advantage of considering all targets jointly rather than independently one-by-one.
In App~\ref{app:over_degrees}, we show that SNAP variants remain top performers even on denser graphs, while the performance of the baselines depends highly on the setting.
App.~\ref{app:over_samples} shows that more data points may improve the \ac{SHD} of global methods, but have little impact on intervention distance.


App.~\ref{app:snapk} shows that SNAP$(0)$ is already almost as effective as SNAP$(1)$ and SNAP$(2)$ on the graphs considered in Fig.~\ref{fig:test_and_time_per_node_unrest_std} and \ref{fig:delta_time_per_node_unrest}.
However, prefiltering with $k > 0$ can be beneficial on denser graphs, especially in combination with PC.
Fig.~\ref{fig:test_per_filter_order} shows the number of d-separation tests performed by SNAP($k$) during prefiltering and PC after prefiltering on graphs with 50 nodes and an expected degree of 5.
These results show that increasing $k$ from 0 to 1 reduces the total number of \ac{CI} tests substantially, and increasing it to 2 and 3 further reduces it noticeably in this case.


App.~\ref{app:order} highlights how SNAP variants can reduce the number of higher order CI tests. This is particularly advantageous for the $\chi^2$ test on discrete binary data, where larger conditioning sets require a greater fraction of the samples to be excluded, diminishing the testâ€™s power. Similarly, for KCI tests, larger conditioning set sizes are associated with a higher likelihood of type II errors \citep{zhang2011kernel}. By minimizing both the number and complexity of the tests, SNAP leverages these statistical advantages effectively

\paragraph{Real world data.}
We evaluate on the MAGIC-NIAB and Andes networks from bnlearn \citep{scutari2010bnlearn}, described in App.~\ref{app:real_networks}, as representative Gaussian and discrete models.
For both networks, we sample 100 times $n_{\mathbf{T}}=4$ identifiable targets and $n_{\mathbf{D}} = 1000$ number of data-points.
We report our results for MAGIC-NIAB in Tab.~\ref{tab:magic-niab}, which shows that SNAP reduces the number of CI tests and execution time compared to baselines. We show that the \ac{SHD} and intervention distance are comparable with most baselines in Tab.~\ref{tab:magic-niab_app} in App.~\ref{app:real-network-results}.  Tab.~\ref{tab:andes} shows that SNAP variants are also faster than baselines on the Andes data, while maintaining a comparable intervention distance.

\begin{table}
\centering
\footnotesize{
\begin{tabular}{lrrrr}
\hline
\multicolumn{1}{c}{} & \multicolumn{1}{c}{CI tests} & \multicolumn{1}{c}{Time} \\ \hline
PC                   & $12807 (\pm 2086)$           & $79.3 (\pm 24.7)$             \\
PC-SNAP($0$)           & $\mathbf{955 (\pm 10)}$       & $0.5 (\pm 0.1)$              \\ \hline
MARVEL               &{$8873 (\pm 3056)$}   & $27.3 (\pm 9.4)$                       \\
MARVEL-SNAP($0$)       & $960 (\pm 5)$                & $0.6 (\pm 0.1)$                       \\ \hline
LDECC*                & $18142 (\pm 2608)$           & $19.2 (\pm 4.0)$           \\
LDECC*-SNAP($0$)        & $981 (\pm 23)$               & $0.8 (\pm0.1)$        \\ \hline
MB-by-MB*             & $11464 (\pm 1995)$           & $25.7 (\pm 4.7)$        \\
MB-by-MB*-SNAP($0$)     & $972 (\pm 17)$               & $0.7 (\pm 0.2)$           \\ \hline
FGES                 & -                            & $0.7 (\pm 0.1)$             \\
FGES-SNAP($0$)         & -                            & $\mathbf{0.4 (\pm 0.1)}$     \\ \hline
SNAP($\infty$)       & $\mathbf{955 (\pm 10)}$       & $0.6 (\pm 0.2)$              \\ \hline
\end{tabular}
\caption{Results for the MAGIC-NIAB network. The best method is indicated in \textbf{bold}. Prefiltering with SNAP($0$) consistently improves most baselines.}
\label{tab:magic-niab}}
\end{table}
