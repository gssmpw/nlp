\section{RELATED WORK}
\label{sec:related_work}

Causal discovery is the task of identifying causal relations between variables from data \textbf{Spirtes, Glymour, and Scheines, "Causation, Prediction, and Search"}.
We limit our scope to observational data and assume causal sufficiency, meaning there are no unobserved confounders or selection bias.
Under these assumptions, there are multiple algorithms in the literature that can be used to learn an equivalence class of causal graphs. In this paper, we mostly focus on constraint-based algorithms, which use \ac{CI} tests to constrain the possible causal relations. PC \textbf{Spirtes et al., "Causal inference in policy and economics"} is one of the most famous constraint-based methods, but it also suffers from scalability issues and reliability.

Several methods have been proposed to reduce the number of CI tests needed, e.g., MARVEL \textbf{Kocaoglu and Zhang, "MARVEL: A computationally efficient causal discovery method"}, or to only consider \ac{CI} tests of low order, which are assumed to be more reliable. For example,  \textbf{Chickering et al., "Learning equivalence classes of Bayesian-Network structures"} extract all possible \acp{CPDAG} given marginal independence tests.
\textbf{Tsamardinos and Brown, "The graph-ordering oracle: A method for efficient causal discovery"} and \textbf{Tsamardinos et al., "Query-driven reverse-engineering of Bayesian networks from raw data"} expand on this by showing how to extract information from \ac{CI} tests up to order $k$, i.e., tests with maximum conditioning set size $k$.

These methods need to perform all possible \ac{CI} tests up to order $k$. We take inspiration from these approaches, as well as anytime FCI \textbf{Zhang and Hyttinen, "Anytime causal discovery"} , an extension of the more general causal discovery method FCI  \textbf{Spirtes et al., "Causal inference in policy and economics"}, that uses CI tests up to order $k$, to develop our incremental pruning of non-ancestors with increasing order.
As opposed to most of these works, we perform a small number of CI tests for each order.

Due to the complexity of discovering the structure over all variables, a large body of literature is concerned with collecting the set of variables that belong in some neighborhood of a single target variable, and estimating their causal effects on the target and vice versa. This task is often called  \emph{local causal discovery}.

Local causal discovery methods sequentially find parent-children-descendant sets \textbf{Tsamardinos et al., "Query-driven reverse-engineering of Bayesian networks from raw data"} or \acp{MB}, i.e., the set of parents, children, and the parents of the children of the variables \textbf{Mooij and Kappen, "From faces to graphs: Probabilistic inference in the presence of missing information"},   
until all edges around the target are oriented.
\textbf{Hauser and Bühlmann, "Characterization, computation, and evaluation of Bayesian score components"} extend PCD-by-PCD \textbf{Tsamardinos et al., "Query-driven reverse-engineering of Bayesian networks from raw data"}  to orient edges not only in the immediate neighborhood of the target, but within the \textit{depth $k$} neighborhood.
\textbf{Hauser and Bühlmann, "Characterization, computation, and evaluation of Bayesian score components"} perform \acl{LCD} on possibly cyclic models, assuming that they are linear and non-Gaussian.
\textbf{Tsamardinos et al., "Query-driven reverse-engineering of Bayesian networks from raw data"} and \textbf{Mooij and Kappen, "From faces to graphs: Probabilistic inference in the presence of missing information"} use both observational and experimental data to discovery the direct causes and direct effects of the target variable.
\textbf{Hauser et al., "A causal approach to active learning for structural interventions"} use experimental data to identify a subset of  target edges.

One of the most related local causal discovery methods is Local Discovery using Eager Collider Checks (LDECC) \textbf{Kocaoglu and Zhang, "Local Discovery Using Eager Collider Checks"}, which identifies the parents of the target by finding the MB of the target and then performing \ac{CI} tests similarly to PC.

Local Discovery by Partitioning (LDP)  \textbf{Tsamardinos et al., "Query-driven reverse-engineering of Bayesian networks from raw data"} overcomes this limitation by learning groups of nodes according to their ancestral relations to a target pair.
These groups can then be used as valid adjustment sets.
However, the groups can be identified only if the causal relationship between the targets is known a priori.
Furthermore, LDP might not recover the optimal adjustment set.

All previous methods consider only a single target pair, where one variable is known to be the treatment and the other the outcome.
Similarly to us, Confounder Blanket Learner (CBL)  \textbf{Jaber et al., "Confounder blanket learner: A method for causal discovery from observational data"}, recovers the causal order among multiple targets, but it assumes all other variables are non-descendants of the targets. Our approach does not require this assumption, and its output is complementary to this work.