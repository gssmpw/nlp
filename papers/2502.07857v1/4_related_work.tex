\section{RELATED WORK}
\label{sec:related_work}

Causal discovery is the task of identifying causal relations between variables from data \citep{glymour2019review}.
We limit our scope to observational data and assume causal sufficiency, meaning there are no unobserved confounders or selection bias.
Under these assumptions, there are multiple algorithms in the literature that can be used to learn an equivalence class of causal graphs. In this paper, we mostly focus on constraint-based algorithms, which use \ac{CI} tests to constrain the possible causal relations. PC \citep{spirtes2000causation} is one of the most famous constraint-based methods, but it also suffers from scalability issues and reliability. 

Several methods have been proposed to reduce the number of CI tests needed, e.g., MARVEL \citep{mokhtarian2021recursive}, or to only consider \ac{CI} tests of low order, which are assumed to be more reliable. For example, \citet{textor2015learning} extract all possible \acp{CPDAG} given marginal independence tests.
\citet{wienobst2020recovering} and \citet{kocaoglu2024characterization} expand on this by showing how to extract information from \ac{CI} tests up to order $k$, i.e., tests with maximum conditioning set size $k$.

These methods need to perform all possible \ac{CI} tests up to order $k$. We take inspiration from these approaches, as well as anytime FCI \citep{pmlr-vR3-spirtes01a}, an extension of the more general causal discovery method FCI \citep{spirtes2000causation} that uses CI tests up to order $k$, to develop our incremental pruning of non-ancestors with increasing order.
As opposed to most of these works, we perform a small number of CI tests for each order.

Due to the complexity of discovering the structure over all variables, a large body of literature is concerned with collecting the set of variables that belong in some neighborhood of a single target variable, and estimating their causal effects on the target and vice versa. This task is often called  \emph{local causal discovery}.

Local causal discovery methods sequentially find parent-children-descendant sets \citep{yin2008partial} or \acp{MB}, i.e., the set of parents, children, and the parents of the children of the variables \citep{wang2014discovering,gao2015local,ling2020using},   
until all edges around the target are oriented.
\citet{zhou2010discover} extend PCD-by-PCD \citep{yin2008partial}  to orient edges not only in the immediate neighborhood of the target, but within the \textit{depth $k$} neighborhood.
\citet{dai2024local} perform \acl{LCD} on possibly cyclic models, assuming that they are linear and non-Gaussian.
\citet{statnikov15ultra} and \citet{shiragur2023meek} use both observational and experimental data to discovery the direct causes and direct effects of the target variable.
\citet{choo2023subset} use experimental data to identify a subset of  target edges.

One of the most related local causal discovery methods is Local Discovery using Eager Collider Checks (LDECC) \citep{gupta2023local}, which identifies the parents of the target by finding the MB of the target and then performing \ac{CI} tests similarly to PC.
Since these algorithms recover only the local structure around a target, causal effect estimation is limited to using the parent adjustment set.

Local Discovery by Partitioning (LDP) \citep{maasch2024local} overcomes this limitation by learning groups of nodes according to their ancestral relations to a target pair.
These groups can then be used as valid adjustment sets.
However, the groups can be identified only if the causal relationship between the targets is known a priori.
Furthermore, LDP might not recover the optimal adjustment set.

All previous methods consider only a single target pair, where one variable is known to be the treatment and the other the outcome.
Similarly to us, Confounder Blanket Learner (CBL) \citep{watson2022causal} recovers the causal order among multiple targets, but it assumes all other variables are non-descendants of the targets. Our approach does not require this assumption, and its output is complementary to this work.
