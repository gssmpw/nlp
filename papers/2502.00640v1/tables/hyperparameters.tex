\begin{table}[h]
    \centering
    \caption{Hyperparameters for LoRA configuration, different stages of fine-tuning, and \name{}-specific fine-tuning.}
    \vspace{-10pt}
    \label{tab:hyper}
    \begin{minipage}{.3\linewidth}
        \centering
        \begin{tabular}{l|c}
            \toprule
            \multicolumn{2}{c}{\textbf{LoRA Configuration}} \\       
            \midrule
            Rank $r$ & 32 \\
            Scaling factor $\alpha$ & 16 \\
            Dropout & 0.1 \\
            Bias & False \\
            \bottomrule
        \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{.68\linewidth}
        \centering
        \begin{tabular}{l|cccc}
            \toprule
            \multicolumn{5}{c}{\textbf{Fine-Tuning Hyperparameters}} \\
            \midrule
            & SFT & Offline DPO & Online DPO & PPO \\
            \midrule
            Learning rate & 1e-5 & 5e-6 & 5e-6 & 2e-6 \\
            Total batch size & 64 & 64 & 32 & 64 \\
            Number of epochs & 3 & 8 & 1 & 5 \\
            \bottomrule
        \end{tabular}
    \end{minipage}

    \vspace{5pt}
    
    \begin{tabular}{l|ccc}
        \toprule
        \multicolumn{4}{c}{\textbf{\name{}-specific Hyperparameters}} \\
        \midrule
        & \doc & \code & \mathc \\
        \midrule
        Window size $w$ & 2 & 2 & 2 \\
        Sample size for MR & 3 & 3 & 3 \\
        Penalty $\lambda$ & 1e-4 & 5e-4 & 5e-4 \\
        \bottomrule
    \end{tabular}
    
\end{table}
