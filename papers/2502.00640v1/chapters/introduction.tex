\section{Introduction}

Modern Large Language Models (LLMs) excel at generating high-quality single-turn responses when given well-specified inputs. 
However, real-world users often do not fully articulate their intents and sometimes initiate conversations with an imprecise understanding of their own needs~\cite{taylor:1968}.
As a result, users routinely refine their requests post hoc through iterative corrections, which can increase frustration, hinder effective task completion, and reduce conversational efficiency~\cite{guidelines, johnny,understand_user_experience,dissatisfaction}. 
Therefore, an open problem is to train models that actively guide users in clarifying and refining their intents, and helps them achieve their goals.
This key challenge would improve user satisfaction and efficiency and streamline human-LLM interactions---especially as LLMs are being applied to real-world tasks that are increasingly complex and open-ended.

A notable limitation of established fine-tuning techniques, such as Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf}, is that they primarily reward LLMs for immediate, single-turn responses, reducing their incentive to seek clarification or assist users in refining their intents or preferences. 
As a result, commonly used LLMs tend to prioritize direct answers, even though seeking additional context would enhance task completion and increase user satisfaction~\cite{dissatisfaction}.

Here we introduce {\bf \name{}}, a novel and general training framework that improves the ability of LLMs to effectively collaborate with humans in multiturn scenarios~\citep{neural_approach, rethinking_conv_agent, clarify_survey}. 
The key innovation of \name{} is to promote LLMs' forward-looking behavior that leads to long-term collaboration gains (Figure~\ref{fig:overview}). 
We introduce a collaborative simulation module that samples future conversations with users to estimate the long-term impact of model responses across multiple turns, a measure we term the \textit{Multiturn-aware Reward (MR)}. 
The MR function evaluates responses by incorporating both extrinsic metrics, such as task-specific success, and intrinsic metrics, such as efficiency, to holistically assess collaboration quality (\cf Section \ref{sec:method}).
By fine-tuning with established RL algorithms~\citep{dpo, ppo} on MRs, \name{} promotes responses that lead to better task completion and efficiency in later conversation stages.
As shown in Figure~\ref{fig:examples}b, the fine-tuned model goes beyond simply responding to user requests in Figure~\ref{fig:examples}a---it actively collaborates by asking follow-up questions about the writing tone, generating targeted content about the role of optimism, and offering insightful suggestions such as adding anecdotes. 

We also introduce \textbf{three challenging multiturn tasks} for training and evaluation in simulated environments: \doct, \codet, and \mathct, which respectively encompass document creation, code generation, and multiturn question answering. 
On the three test sets, our approach improves task accuracy metrics by \taskimprov and interactivity by \itrimprov on average compared to our best baselines, according to LLM judges. Beyond the tasks that the \name{}s are fine-tuned on, we show \name{}s are highly generalizable to other data domains. 

Moreover, we perform a \textbf{large-scale and real-world user study} with \numturker{} Amazon Mechanical Turkers (MTurkers), who are asked to  write documents with the help of anonymous AI assistants, either \name{} or non-collaboratively trained LLMs. 
\name{} achieves impressive improvement with \realsatisfyimprov increase in user satisfaction and  yield user time savings of
\realtimeimprov{} on average. 
The qualitative analysis from MTurkers confirms our observations: non-collaboratively-trained LLMs passively agree with users, while \name{} actively provide insightful questions and suggestions to guide writing processes. 

