\begin{abstract}
Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. 
As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations.
To address these limitations, we introduce \mbox{\name{}}, a novel and general training framework that enhances multiturn human-LLM collaboration.
Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using  \textit{Multiturn-aware Rewards}.
By reinforcement fine-tuning these rewards, \name{} goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions---a key step towards more human-centered AI.
We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. 
\name{} significantly outperforms our baselines with averages of \taskimprov higher task performance and \itrimprov improved interactivity by LLM judges.
Finally, we conduct a large user study with \numturker{} judges, where \name{} increases user satisfaction by \realsatisfyimprov and reduces user spent time by \realtimeimprov{}.

\end{abstract}
\input{latexfig/intro_examples}
