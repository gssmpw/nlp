\section{Experimental Setup\protect\footnote{Dataset and training details in Appendix~\ref{app:dataset_n_train}; all prompts (\eg prompts of user simulator and LLM judges) in Appendix~\ref{app:prompts}.}}

\begin{comment}
We conducted extensive and large-scale experiments to answer the following research questions:
\begin{itemize}
    \item How \textit{effective} are the \objects in collaborating with users? (Sec.~\ref{sec:quantitative})
    \item How do \objects outperform? What component is \textit{essential} to their success? 
    (Sec.~\ref{sec:ablation})
    \item What are the \textit{insights} from the generation results from interactive-trained LLMs? (Sec.~\ref{sec:case})
    \item Can the capabilities of interactive-trained LLMs \textit{generalize} to untrained data domains? (Sec.~\ref{sec:extended})
\end{itemize}
\end{comment}




For fine-tuning and evaluation, we create three multiturn datasets using publicly available data across diverse domains~\citep{math, bigcodebench, medium}: collaborative document editing, coding problem assistance, and multiturn mathematics problem solving.

To build a multiturn environment (Figure~\ref{fig:evaluation}), we employ GPT-4o-mini as a user simulator LLM to role-play realistic user behaviors, given the target problem and conversation history. Our simulation-based evaluations are designed to closely mimic real-world interactions~\cite{simulate1000}. 
Unlike traditional single-turn tasks, our setup requires dynamic interactions over multiple turns to achieving a goal. 
The three interactive datasets are: 

\noindent \textbf{\doct}: Document editing requires iterative feedback and refinements across multiple turns to ensure coherence and alignment with user intent. We sample 100 Medium articles as goal documents, which are summarized into target problems to guide the user simulator. After each interaction, task performance is evaluated using the \textbf{BLEU} score, measuring similarity between the extracted document and the original articles.

\noindent \textbf{\codet}: Coding tasks inherently require multiturn interactions, such as clarifying requirements and debugging. We sample 600 coding problems from BigCodeBench~\citep{bigcodebench} as the target problems given to the user simulator. For evaluation, we compute the average \textbf{Pass Rate (PR)} of code at the end of the interactions.

\noindent \textbf{\mathct}: Math problem solving often requires addressing implicit assumptions, verifying intermediate steps, and clarifying reasoning. We sample 200 level-5 math problems from MATH~\citep{math} to prompt the user simulator, which interacts with the LLMs. Task success is measured by the \textbf{accuracy (ACC)} of the final solution, as evaluated by an LLM judge.

    

In addition to the above task-specific metrics, we incorporate two task-agnostic scores across all datasets: \textbf{1) Average Token Count}, which quantifies the average number of tokens generated by the LLM per conversation, reflecting interaction efficiency. \textbf{2) Interactivity (ITR)}, which evaluates engagement levels using an LLM judge (Claude-3.5-Sonnet), with scores rescaled to the range [0, 1]. 



\input{tables/main_results}
\input{latexfig/ablation}

\xhdr{Fine-tuning \name{}s}  \name{}s are based on \llama{}~\citep{metallama} with LoRA finetuning~\citep{lora}. We train four model variants: \textbf{1)~Offline models}: SFT and Offline DPO are fine-tuned on pre-generated multiturn conversational datasets guided by Multiturn-aware Rewards (MR) (\cf Section~\ref{sec:optimization}). \textbf{2) Online models}: PPO and Online DPO are further trained from the SFT and Offline DPO models, respectively. The model during online fine-tuning is involved in the collaborative simulation to compute MRs, which, in turn, dynamically adjust the model preference. 


\xhdr{Baselines} We compare \name{}s against (1) the pretrained \llama (\textit{Base}), (2) the base model with proactive prompt engineering (\textit{Proactive Base}), which encourages follow-up and clarification questions. 




\input{latexfig/case_study_example}
\input{latexfig/case_study_n_generalization}

\section{Results of Simulated Experiments}
\label{sec:quantitative}

We present the results in Table~\ref{tab:results} and the takeaways are:


\xhdr{Prompt engineering is helpful, but limited in terms of performance gains and flexibility}
Proactive Base improves base model performance by encouraging follow-up questions and clarifications. For example, it increases BLEU on \doc from 32.2\% to 35.0\% and reduces read tokens by 0.31k compared to the base model. However, these gains are modest and do not fully address the challenges of multiturn collaboration. We observe that prompting strategies remain rigid, relying on predefined instructions rather than adapting dynamically to user needs. For instance, the model sometimes asks clarification questions even when unnecessary, leading to redundant interactions that disrupt conversation flow.

\xhdr{\name{} increases task performance, conversational efficiency, and engagement}
\name{} achieves \taskimprov superior task-specific performance, \efficiencyimprov more efficient conversations, and \itrimprov enhanced interactivity compared to the best baselines.
We highlight that \name{} engage in more meaningful collaborations, with ITR shows substantial gains. For \doct, the Online DPO model increases ITR from 0.46 to 0.92. 
Moreover, our framework significantly improves conversational efficiency by minimizing the content users need to review to arrive at the final solution. For \mathct, Online DPO decreases token count per conversation by 1.03k compared to the base model.

\subsection{Ablations on Reward Mechanisms (Figure~\ref{fig:ablation})}
\label{sec:ablation}

To investigate how components contribute to \name{}'s superior performance, we conduct an ablation study focusing on the reward mechanisms used during fine-tuning. 
We evaluate the following reward mechanisms:
\begin{itemize}
    \item \textbf{Variants of Multiturn-aware Reward}: We vary the forward sampling window size $w=1,2,3$ to assess their ability to capture long-term conversational effects through simulated collaborations.
     \item \textbf{Immediate Rewards} evaluate the model's immediate response based on:
        \textit{1) Helpfulness}: Assessed by an LLM judge;
        \textit{2) Extrinsic Reward}: Focuses on task-specific metrics like BLEU while ignoring intrinsic factors such as efficiency;
        \textit{3) Extrinsic + Intrinsic Reward}: Combines task-specific metrics with efficiency and interactivity measures. This can be seen as a special case of the multiturn-aware reward function with $w=0$.
    
    
\end{itemize}

\input{latexfig/user_study}


We present results in Figure~\ref{fig:ablation}.
Interestingly,
expanding the forward sampling window $w$ within the range generally enhances performance and efficiency by better capturing future interactions. Notably, MR with $w=2$ balances the gains and additional costs to conduct forward sampling, making it well-suited for large-scale fine-tuning. In contrast, immediate rewards, even with extrinsic and intrinsic components, fall short as they ignore long-term impact. 
These findings validate the positive impact of the forward sampling strategy in MRs.


\subsection{Case Study (Figure~\ref{fig:coding} \& ~\ref{fig:reward_preference})}
\label{sec:case}
We now offer a deeper insight into \name{}'s behavior as shown in Figure~\ref{fig:coding}. In this example,
the user request to tokenize a text file is inherently open-ended due to unspecified factors, such as the NLTK environment, tokenizer selection, and optional preprocessing steps. The base LLM makes several arbitrary assumptions, applying lowercase conversion and stopword removal without user confirmation. The user simulator later corrects these assumptions, but the final solution remains incorrect due to missing stopwords.
In contrast, \name{} actively clarifies user intent by seeking confirmation on key decisions, ensuring an aligned final solution with a 100\% Pass Rate. This approach also reduces user effort with lower token usage.

In Figure \ref{fig:reward_preference}, we compare different reward mechanisms for responses A and B of Figure~\ref{fig:coding}, to confirm that these rewards work as intended. The helpfulness rewards favor response A due to its seemingly more well-round output. Extrinsic rewards assign zero scores to both, as A provides an incorrect solution and B defers answering. Extrinsic + Intrinsic rewards slightly favor B for efficiency and engagement. Interestingly, MR assigns significantly higher rewards to B, especially at $w=2$ and $w=3$, since the response obtains useful information and provide a precise answer within the future interaction window.


\begin{table*}[t]
\centering
\vspace{-5pt}
\caption{Representative Feedback from Human Participants.}
\vspace{-10pt}
\begin{tabularx}{\textwidth}{|p{1.6cm}|X|X|}
    \hline
    \footnotesize \textbf{Model} & \footnotesize \textbf{Strengths} & \footnotesize \textbf{Weaknesses} \\
    \hline
    Base & \textit{``Follows great instruction and does exactly what I'm asking it to do.'', ``It can create a nice form of an outline to work with.''} & \textit{``The AI just agreed with me on pretty much everything. There was no discussion'', ``I didn't really like that it kept coming up with different options''} \\
    \hline
    Proactive Base & \textit{``It is very organized and it actually asks you for feedback after writing the revision.''} & \textit{``The AI seemed to be very redundant and asked me the same questions over and over.''} \\
    \hline
    \namewithspace{} & \textit{``Asking questions and making you think of things you never thought of'', ``The AI really helped me with focusing on one part of the story at a time.'', ``It helped really well to navigate what to say and what information is needed''} & \textit{``The AI assistant was not up to date enough to help with this recent sporting event.  The AI assistant also asked me to repeat information I had already given it.''} \\
    \hline
\end{tabularx}
\vspace{-5pt}
\label{tab:user_study}
\end{table*}

\subsection{Model Generalization (Table~\ref{tab:abg_coqa})}
\label{sec:generalization}
Modern foundation models are expected to generalize across a diverse range of tasks beyond their training domain. A key question is whether collaborative behaviors learned by \name{} during fine-tuning transfer effectively to new tasks without additional adaptation. 

We assess \name{}, trained with online DPO on \code (the coding assistance task), on Abg-CoQA~\cite{abg_coqa}, a question-answering (QA) benchmark where questions are labeled as ambiguous or non-ambiguous (\cf Appendix~\ref{app:abg_coqa}).
We categorize the model’s responses into two actions—asking a clarifying question or providing a direct answer—and evaluate action-level accuracy within each question type. 
As shown in Table~\ref{tab:abg_coqa}, 
GPT-4o and \llama{} rarely ask clarifying questions regardless of ambiguity. 
In contrast, 
\name{} proactively asks questions about 50\% of the time while maintaining high accuracy on unambiguous inputs.
This behavior leads to the highest Macro Accuracy across both ambiguous and non-ambiguous sets and improves Macro F1 over the base model, while leaving room for further improvement against GPT-4o. These results suggest that \textbf{\name{} effectively generalizes its learned collaborative strategies beyond its training domain}.

\section{Real-world User Study}

\xhdr{Setup}
We conduct a large-scale user study using Amazon Mechanical Turk with \numturker{} participants. Each participant is assigned a document type---randomly selected to be either blog post, creative writing, or personal statement---and chooses a topic from a predefined set. To simulate real-world scenarios where users have only a rough idea of the task, they are first asked to provide brief responses to topic-related questions.
Participants then engage in at least eight turns of conversation with an anonymized AI assistant, which can be Base, Proactive Base, or \name{}. Every three turns, they provide an interaction rating based on their experience so far. After the conversation, participants rate the final document quality and overall interaction. All ratings are in a scale from 1 to 10. We also record the total interaction duration to assess efficiency.
The detailed user study setup is provided in Appendix~\ref{app:user_study}.

\xhdr{Quantitative Results (Figure~\ref{fig:user_study})} Across multiple metrics, \name{} consistently outperforms the baselines. It achieves an average document quality score of 8.50. Specifically, 91.4\% of participants rate \name{}'s \textbf{document quality} as ``good'' (score 8–9), and 56.9\% as ``very good'' (score 9–10), compared to 88.5\% and 39.3\% for Base (\llama{}), respectively. Similarly, 63.8\% of participants find \name{} \textbf{highly engaging}, while only 42.6\% report the same for \llama{}. 

Interestingly, for \textbf{multiturn interaction}, the Base model shows a declining trend in ratings from turns 6–9, indicating reduced user experience in longer conversations. In contrast, both \name{} and Proactive Base exhibit increasing ratings over time, with \name{} consistently achieving higher average ratings every three turns compared to Proactive Base. This suggests that \name{} maintains sustained engagement more effectively.  

Moreover, \name{} improves task efficiency, reducing \textbf{time spent} by \realtimeimprov{} compared to the Base model and by 15.6\% relative to Proactive Base. While Proactive Base is prompted to maintain conciseness, it frequently asks unnecessary questions, causing lower efficiency. In contrast, \name{} strikes a more streamlined user experience.


\xhdr{Qualitative Results (Table~\ref{tab:user_study})} We collected a total of 180 strengths and 180 weaknesses across the three models. Table~\ref{tab:user_study} presents representative feedback, while we summarize here the mddels' strengths and weaknesses:
The base model generates coherent content while effectively follow user instructions, but it sometimes struggles with maintaining context in long texts, and can be overly verbose or repetitive in its responses. 
Proactive Base excels in responsiveness and adapting to user input but struggles with memory retention, and could produce repetitive or overly structured content.
On the other hand, \name{} is highly engaging, effectively guiding users through writing, adapting seamlessly to feedback. However, users also point out that \name{} can occasionally feel bland, lack of up to date information, and require additional effort to personalize the output. 
Overall, \name{} enhances collaboration by guiding users through an interactive and iterative refinement process, yet future improvements should focus on increasing personalization, creativity, and real-time knowledge integration to further optimize human-LLM collaboration.
    
