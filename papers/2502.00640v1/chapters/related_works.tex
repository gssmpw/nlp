
\section{Related Work}
\label{app:related}

\input{tables/contribution}
\xhdr{Non-collaborative LLM training} 
Existing LLM training frameworks, including pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL)~\citep{dpo, ppo, rlhf, rlaif}, primarily optimize for next-turn response quality. Standard RL methods such as Proximal Policy Optimization (PPO)~\citep{ppo} apply rewards to individual model responses without accounting for their long-term impact on conversation trajectories. While effective for single-turn objectives, these approaches fail to capture how responses influence user intent discovery and long-term task success~\cite{guidelines, johnny,understand_user_experience,dissatisfaction,proactive_agent}.

\xhdr{Prompting techniques for multiturn interaction} 
Prior work has explored prompting strategies to enhance LLM interactivity, particularly for clarification questions~\citep{ask_more_informative_questions, clarifygpt, clarify_when_necessary, clarinet, tree_of_clarifications, rephrase_and_respond, multiturn_clarification} and mixed-initiative dialogues~\citep{proactive_cot, mixed_initiative_dialogue, proactive_agents}. For instance, \citet{clarifygpt} prompt LLMs to ask clarification questions when code generation requests are ambiguous. However, such prompting-based approaches are constrained by predefined interaction patterns, limiting adaptability across different tasks and conversation stages. Moreover, their reliance on fixed prompts reduces generalization, as demonstrated in our experiments where proactive prompting fails to match the effectiveness of our fine-tuned models.

\xhdr{Learning-based methods for multiturn interaction} 
\begin{itemize}
    \item \textbf{LLMs for generating clarification questions:} 
    Beyond prompting, prior studies have explored supervised fine-tuning~\citep{star_gate}, RL fine-tuning~\citep{learn_to_clarify, clarify_question_for_retrieval, circle}, and active learning~\citep{active_inquiry} to train models to ask clarification questions. For example, \citet{learn_to_clarify} use Direct Preference Optimization (DPO) to encourage models to identify ambiguous user queries and request clarifications. However, like prompting approaches, these methods primarily focus on clarification questions and do not generalize to broader multiturn collaboration strategies.
    \vspace{2pt}
    \item \textbf{Multiturn training for LLMs:} 
    Several studies extend RLHF to multiturn settings by optimizing trajectory-level rewards~\citep{multiturn_rlhf, archer, refuel, mt_preference}. Other works~\citep{baize, ppdpp} leverage self-chat or self-play to enhance model adaptation to dialogue. See \citet{mt_survey} for a comprehensive survey. However, these methods primarily rely on post-hoc trajectory-level data, learning from observed conversations rather than explicitly modeling the causal effect of individual responses on long-term task success (see Appendix~\ref{app:discussion} for further details). Additionally, they often overlook open-ended tasks such as document generation~\citep{interactive_text_gen, into_the_unknown}, where real-world user responses can be highly variable, and users may have limited capacity to read and refine lengthy model outputs.
    
\end{itemize}

In Table~\ref{tab:contribution}, we compare \name{} with related methods across four key dimensions. 
\name{} is a general, user-centric, and multiturn-aware framework that leverages more accurate reward estimation to better align with real-world objectives, enhancing user satisfaction and streamlining human-LLM interactions.

