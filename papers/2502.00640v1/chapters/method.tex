
\section{Problem Formulation}






\label{sec:formulation}
In contrast to many existing tasks that are single-turn and require no human involvement beyond the initial query, our problem formulation reflects a real-world setting in which a user's underlying (implicit) goal is defined as $g$ in a multiturn conversational task. The conversation unfolds over multiple turns $t_j = \{u_j, m_j\}$, where $u_j$  is the user input and $m_j$  is the model's response at each turn $j = 1, \dots, K$, where $K$ is the number of turns in the conversation. 

At the $j$-th turn, the model generates its response based on the previous conversation turns $t_{1:j-1} = \{t_1, \dots, t_{j-1}\}$ and the current user response $u_j$. For simplicity, we define historical conversation at $j$-th turn as $t^h_j = t_{1:j-1}\cup \{u_j\}$, therefore, $m_j = M(t^h_j)$. 
The objective is to generate a sequence of model responses $\{m_j\}_{j=1}^{K}$ that effectively and efficiently achieve for goal $g$, \eg answering a math question, where goal achievement is assessed based on user satisfaction or an external evaluation function, such as accuracy by LLM judge.
Formally, we define the objective as $R^*(t_{1:K} \mid g)$, where $R^*$ incorporate the achievement of task success and user experience factors such as time cost.




\section{Unified Collaborative LLM Training}
\label{sec:method}





\xhdr{Key Motivations} Established LLM training frameworks, such as Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf}, focus on maximizing immediate rewards for single-turn tasks. This cause a misalignment between their single-turn objective and real-world multiturn objective $R^*(t_{1:K} \mid g)$. 
Precisely, the model's accumulative single-turn reward $\sum_{j=1}^{j=K} R(m_j \mid t_j^h)$ may not imply a higher final reward $R^*(t_{1:K} \mid g)$. \textit{In fact, achieving high single-turn rewards at each turn may not imply a higher final reward}. For example, consider a task where the user’s goal $g$ is to write an engaging article. A model trained with traditional RLHF might generate isolated responses, like drafting an introduction or listing conclusions. While these responses are helpful in isolation, they fail to consider how the sections flow together, resulting in an article that might not be cohesive and aligned with the user’s goal.







\textit{Instead, effective multiturn collaboration requires model responses that optimally contribute to the final reward.} The model should aim to align its responses with the user’s goal $g$ by considering their impact on the entire conversation trajectory $t_{1:K}$. 
In the previous example, instead of generating a conclusion, asking, ``\textit{Should I maintain an engaging tone in the conclusion like the introduction?}'' offers better long-term alignment with the goal. 


\subsection{Multiturn-aware Rewards}
\label{sec:reward} 

In Figure~\ref{fig:overview}, our key insight is that effective multiturn collaboration relies on \textbf{forward-looking strategies}. Given a context \circled{\small 1}, the model should consider how its response \circled{\small 2} influences the subsequent turns of the conversation. To capture this, we design a \circled{\small 3} collaborative simulation module to estimate this impact. By \circled{\small 4} fine-tuning to distinguish between potential future conversations resulting from different responses, the model generates responses that align better with the overarching goal $g$. 

This high-level design naturally aligns with causal effect estimation~\citep{Pearl09a, pearl2016causal}, which evaluates the interventional effects of an action in sequential decision-making.
Appendix~\ref{app:discussion} provides further discussion on the connection between causal effect estimation and our approach.
More specifically, we define the Multiturn-aware Reward:



\textbf{Multiturn-aware Reward (MR):}  
\textit{The multiturn-aware reward for model response $m_j$ at the $j$-th turn is given by:}
\begin{equation}
\begin{aligned}
    &\text{\ourst}(m_j \mid t_j^h, g) \\=& 
    ~\mathbb{E}_{t_j^f \sim P(t_{j+1:K} \mid t_j^h \cup \{m_j\})} R^*(t_j^h \cup \{m_j\} \cup t_j^f \mid g) \\
    = &~\mathbb{E}_{t_j^f \sim P(t_j^f \mid t_{1:j})} R^*(t_{1:j} \cup t_j^f \mid g),
    \label{eq:main}
\end{aligned}
\end{equation}
\textit{where $t_{1:j}$ denotes the conversation history up to and including the $j$-th turn, and {\small $t_j^f = t_{j+1:K}$} represents the forward trajectory of turns following the $j$-th turn. The distribution {\small $P(t_j^f \mid t_{1:j})$} models the possible forward conversations conditioned on the prior conversation history.} 


However, computing Equation~\ref{eq:main} remains challenging as it requires the following components:
\textbf{(a) A conversation-level reward function}, $R^*(t \mid g)$, for evaluating an arbitrary multiturn conversation $t$, and  
\textbf{(b) a sampling strategy for obtaining forward conversations} $P(t_j^f \mid t_{1:j})$, which represents the forward conversation distribution.  We elaborate on the two components in Section~\ref{sec:conv} and \ref{sec:sample}.

\subsubsection{Conversation-level Reward Function}
\label{sec:conv}
We approximate the conversation-level reward $R^*(t \mid g)$ with a combination of extrinsic (goal-specific) and intrinsic (goal-agnostic) metrics:
\begin{equation}
R^*(t \mid g) \simeq R_{\text{ext}}(t, g) + R_{\text{int}}(t),
\end{equation}
where $R_{\text{ext}}(t, g)$ focuses on task-specific success, and $R_{\text{int}}(t)$ evaluates user experience including efficiency and engagement. 


\begin{itemize}
    \item \textbf{Extrinsic Reward} $R_{\text{ext}}(t, g)$ measures how well the conversation achieves the user’s goal $g$. Formally:
    \vspace{-2pt}
    \begin{equation}
        R_{\text{ext}}(t, g) = S(\operatorname{Extract}(t), y_g),
    \end{equation}
    where $\operatorname{Extract}(t)$ extracts the final solution or response from the conversation $t$, especially for tasks requiring revisions or multi-step answers. $y_g$ is the reference solution for the goal $g$, \eg the ground truth solution for a math problem. And $S(\cdot, \cdot)$ evaluates task-specific metrics like accuracy or similarity. This ensures the conversation contributes directly to achieving the desired goal.
    \vspace{0.5pt}
    \item \textbf{Intrinsic Reward} $R_{\text{int}}(t)$ prioritizes conversations that enhance user experience, defined as:
    \vspace{-2pt}
    \begin{equation}
        R_{\text{int}}(t) = - \min[\lambda \cdot \text{TokenCount}(t), 1] + R_{\text{LLM}}(t),
        \label{eq:intrinsic}
    \end{equation}
    where we encourage conversational efficiency by penalizing excessive tokens that users read and write, with $\lambda$ controlling the penalty severity. This efficiency measure is bounded by 1 to maintain balance with other metrics. The second term, $R_{\text{LLM}}(t)$, is assigned by an LLM-based judge~\citep{llm_as_judge} on a 0–1 scale, evaluating user-valued objectives such as engagement / interactivity. Notably, additional conversational aspects, such as clarity, can be further integrated into the objective.
\end{itemize}

The conversation-level reward incorporates task-specific and human-centered metrics, encouraging the model to balance goal achievement, efficiency, and engagement.
\begin{figure*}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/cropped/tasks.pdf}
    \vspace{-15pt}
    \caption{Simulated Multiturn Environment for Evaluation. Our evaluation pipeline simulates real-world collaborations by prompting an user simulator LLM to emulate diverse behaviors and personalities in multiturn conversations. 
    }
    \label{fig:evaluation}
\end{figure*}


\subsubsection{Forward Sampling}
\label{sec:sample}

To compute Eq.~\ref{eq:main}, we require samples from $P(t_j^f \mid t_{1:j})$, the distribution of forward conversation conditioned on the conversation history. 
A simple approach is to use Monte Carlo sampling, where the conversation is extended turn-by-turn until it concludes.  
However, this can be computationally expensive for computing reward for every model response.
For a scalable approximation, we introduce a window size $w$ as a hyperparameter to limit the maximum number of forward turns considered in $t_j^f$. This reduces the computational cost while maintaining sufficient context. 

More importantly, while real-world conversations could be gathered from human participants, sampling multiple forward conversations during training is costly and impractical. To further reduce cost and ensure scalability, we introduce a user simulator $U$.

\xhdr{User Simulator:}  
\textit{A user simulator $U: \mathcal{T} \rightarrow \mathcal{U}$ is a function that maps a given conversation history $t \in \mathcal{T}$ to a user response $u \in \mathcal{U}$. Specifically, $U$ generates a probabilistic distribution $P(u \mid t)$ over possible user responses conditioned on the conversation history $t$, simulating realistic user behavior. }

Specifically, we prompt an LLM to role-play as users,  explicitly asking the LLM to follow the same language style as the previous user turns, and injecting typical user behaviors. The user simulator operates with an implicit goal $g$, which it seeks to achieve over the course of the conversation. This design emulates real-world scenarios where users may have evolving needs, limited background knowledge, or require clarification, resulting in naturally unfolding multiturn conversations~\citep{simulate1000}.

\subsection{Optimization \& Synthetic Datasets}
\label{sec:optimization}
With the conversation-level reward function and forward sampling strategy, we can compute MR for any model response without requiring an additional reward model, which is often costly and slow to train. Unlike traditional single-turn reward approaches, MR explicitly accounts for the impact of a response on future conversations, promoting long-term collaboration.

Further, we employ reinforcement learning (RL) methods such as PPO~\citep{ppo} and DPO~\citep{dpo} to guide the model in navigating complex conversations. By optimizing for higher MR, the model learns to generate responses that enhance overall effectiveness and efficiency by the end of the conversation.




Moreover, \ours can generate \textbf{high-quality synthetic conversations} (\cf Figure~\ref{fig:flow} in Appendix~\ref{app:dataset_n_train}) for both supervised fine-tuning (SFT) and DPO. For SFT, it iteratively selects top-ranked responses to build realistic, goal-directed conversation histories. For DPO, it constructs pairwise comparisons by ranking responses at each turn, distinguishing ``chosen'' and ``rejected'' pairs based on \ours scores. The generated synthetic data aligns with multiturn objectives. 

Overall, \name{} enables scalable dataset generation and online RL training without human annotation, making it generalizable across diverse tasks. In Appendix~\ref{app:related}, we compare \name{} with related prompting- and training-based approaches, highlighting its contributions.





