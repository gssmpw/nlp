
\section{Dataset and Training Details}
\label{app:dataset_n_train}

\subsection{Dataset Generation for Offline Training}
\input{latexfig/data_generation}

The Multiturn-aware Reward (MR) function enables the generation of high-quality synthetic conversation datasets for training. Given a user query, multiple LLM responses are sampled and ranked based on their MR scores, with higher-ranked responses designated as \textit{Chosen} and lower-ranked as \textit{Rejected}. To simulate natural conversational flow, the first turn from the chosen response's forward interaction window is appended to the prompt for the next turn, iteratively extending the conversation until completion. Solid red arrows denote data collection for Supervised Fine-Tuning (SFT), while dashed blue arrows indicate preference data construction for Direct Preference Optimization (DPO). This approach systematically curates multiturn conversations that enhance both response quality and collaborative efficiency, both of which are explicitly captured by MR.

Given (1) a user simulator LLM, \eg GPT-4o-mini, (2) an assistant LLM, GPT-4o, and (3) arbitrary tasks with defined task-specific metric, we can simulated and generate high-quality conversations following Figure~\ref{fig:flow}. We create the following training datasets in this simulated environments. 
\input{tables/stats}


\subsection{Training Details}

We provide the hyperparameters for \name{} fine-tuning in Table~\ref{tab:hyper}. 

Notably, \name{} relies on a minimal set of hyperparameters, using the same window size and sample size for computing MRs across multiple datasets. The penalty factor on token count, $\lambda$, is set lower for \doc compared to \code and \mathc, as document lengths in \doc can vary significantly and may be easily bounded by 1 in Eq.~\ref{eq:intrinsic} if $\lambda$ is too large.
\input{tables/hyperparameters}