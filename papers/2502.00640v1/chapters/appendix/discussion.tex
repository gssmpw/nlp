
\section{Supplementary Discussion}
\label{app:discussion}

\subsection{Connection Between Multiturn-aware Reward and Causal Inference}

Our approach naturally aligns with causal inference principles, as it aims to quantify how a model's response influences the future trajectory of a conversation. This aligns with the fundamental goal of \textbf{causal effect estimation}, which seeks to isolate the impact of an intervention---in this case, a model response---on long-term outcomes.

From a causal perspective, given a conversation history \( t^h_j \) at turn \( j \), the \textbf{causal effect} of a model response \( m_j \) on the final conversation trajectory can be expressed using \textbf{front-door adjustment}~\citep{Pearl09a, pearl2016causal}:
\begin{equation}
    \sum R^*(t_{1:K} \mid g) P(t_{1:K} \mid t^h_j)P(t^h_j) = \sum R^*(t_{1:K} \mid g) P(t_{1:K} \mid t^h_j) = \mathbb{E}_{t_{1:K} \sim P(t_{1:K} \mid t^h_j)} R^*(t_{1:K} \mid g).
\end{equation}
This equation captures the expected long-term reward of a conversation conditioned on the modelâ€™s response at turn \( j \). It explicitly accounts for how \( m_j \) intervenes in the conversation, influencing future turns and, ultimately, task success.


\subsection{Distinction from Other Multiturn Training Frameworks}

Existing multiturn trajectory-based training frameworks~\citep{multiturn_rlhf, archer, refuel} primarily rely on learning from observed trajectory-level rewards. These methods estimate the utility of responses by assigning rewards post hoc to completed conversations, training models to imitate historically successful responses. However, this approach is fundamentally \textbf{correlational}---it captures statistical associations between responses and final outcomes but does not explicitly model how different responses causally influence future turns.

In contrast, our Multiturn-aware Reward (MR) framework \textbf{intervenes} on model responses and uses forward sampling to explicitly simulate alternative future trajectories. This enables the model to estimate the \textbf{counterfactual impact} of different responses and adjust its behavior accordingly. By leveraging causal effect estimation, MR training moves beyond passive imitation of high-reward conversations and instead actively optimizes responses to maximize long-term goal attainment.
This distinction is crucial in dynamic human-LLM interactions where user needs evolve throughout a conversation. 




