
\section*{Impact Statement}

This paper presents work aimed at making AI more user- and human-centric, which, in our view, yields a positive societal impact. Most current work on AI and its evaluation focuses on fully automated tasks, with no user involvement in solving the task or optimization for a collaborative experience with users. This has serious societal drawbacks, given issues such as AI hallucinations \cite{Huang:2025}, biases \cite{Gallegos:2024}, and unsafe language \cite{Shi:2024} that arise from a lack of human oversight. The common focus on having AI models autonomously complete tasks also ignores the reality that many scenarios have humans present regardless of the level of automation, and that not priming AI models to proactively seek human help, feedback, or clarifications misses an opportunity to make generative AI more accurate, effective, and safe.
This consideration would also help increase the adoption of AI in safety-critical scenarios, such as medical decision-making tasks \cite{Liu:2024}, in which we believe AI models should be inclined to seek confirmation or verification \cite{Gero:2023} from an expert in case of uncertainty---a behavior that is mostly absent in current state-of-the-art LLMs.

This work presents one of the first attempts to train LLMs in such human-centric environments. To promote future research in this societally beneficial direction, we plan to release all the code, models, data, benchmarks, and user simulators described in this work.

The data collected in our study involves human participants recruited through Mechanical Turk. We took several measures to ensure the privacy of these workers in the document creation tasks. First, we asked workers to confirm that they were willing to share the text they wrote as part of a public dataset. Second, we urged them not to include any personally identifiable information (PII) in their writings and to focus only on topics of public knowledge or fictitious stories. Third, we scanned the collected data to ensure that no PII was included. For the final version of the dataset, we will recruit additional workers to manually review each collected conversation to ensure that no PII or other safety issues (e.g., offensive language) exist in the data.
Mechanical Turk workers were paid \$10 per conversation. Given that conversations averaged 28.4 minutes, including break times, this means workers were paid more than \$20 per hour on averageâ€”above the minimum wage in the country where the data was collected.
