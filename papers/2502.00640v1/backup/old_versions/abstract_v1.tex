% \begin{strip}
%     \centering
%     % \includegraphics[width=0.9\linewidth]{figures/first_v3}
%     \includegraphics[width=0.8\linewidth]{figures/first_v3}
%     \captionof{figure}{Our interactive LLM training framework. Given a context, our framework trains the LLM to output responses that maximize long-term gains instead of intermediate rewards, encouraging the model to effectively collaborate with real users, \eg by proposing follow-up questions or alternative explorations.}
%     \label{fig:overview}
% \end{strip}
% \noindent\makebox[\textwidth]{%
%     \begin{minipage}{\textwidth}
%         \centering
%         % \includegraphics[width=0.9\linewidth]{figures/first_v3}
%         \includegraphics[width=0.8\linewidth]{figures/first_v3}
%         \captionof{figure}{Our interactive LLM training framework. Given a context, our framework trains the LLM to output responses that maximize long-term gains instead of intermediate rewards, encouraging the model to effectively collaborate with real users, \eg by proposing follow-up questions or alternative explorations.}
%         \label{fig:overview}
%     \end{minipage}%
% }

\begin{abstract}


Large language models (LLMs) support problem-solving across a wide range of tasks, yet their ability to collaborate effectively with humans in real-world scenarios remains limited.
Current LLMs tend to respond reactively to users rather than engaging actively toward long-term goals, as they are often trained to optimize immediate responses. 
To address this limitation, we introduce \name{}, a novel and general training framework that enhances LLMs' collaboration abilities.
At the core of \name{} is a collaborative simulation module that estimates a response's long-term contribution---\textit{Multiturn-aware Reward}---in a collaboration environment, which can be integrated with prevailing reinforcement fine-tuning frameworks.
Notably, we conduct a large-scale user study involving \numturker{} humans, where \name{} achieves a \realsatisfyimprov increase in user satisfaction and reduces time usage by \realtimeimprov{}.
Additionally, we devise a multiturn interaction benchmark in simulated environments with three challenging tasks, such as document creation. 
%Compared to our strongest baselines, 
\name{} significantly outperforms our strongest baselines with an average of \taskimprov higher task performance and \itrimprov improved interactivity by LLM judges.
Overall, \name{} advances human-centered AI by enabling more effective and efficient LLM-user collaboration.

% , and \efficiencyimprov greater conversational efficiency 
% : document creation, coding assistance, and multiturn question answering.
% we demonstrate the versatility of \name{} across three challenging tasks: document creation, coding assistance, and multiturn question answering.
% Notably, our approach significantly 

%% Original abstract:
%Large language models (LLMs) have been transforming problem-solving paradigms across diverse domains. However, their ability to effectively collaborate with humans in real-world tasks remains limited. Current LLMs tend to passively respond to user inputs and struggle to actively engage in the conversations toward long-term goals. To address the limitation, we propose a novel and general interactive training framework that empowers LLMs to act as active collaborators. By introducing a multiturn-aware reward mechanism, our framework optimizes responses for their long-term contribution to user objectives, which can be easily integrated into prevailing reinforcement learning frameworks. We demonstrate the versatility and impact of our framework across three challenging tasks including document editing, coding assistance, and multiturn question answering. To highlight, our approach significantly outperforms the best baselines, achieving an average of \taskimprov higher task-specific accuracy, \itrimprov improved interactivity, and \efficiencyimprov increased conversational efficiency, advancing the frontiers of human-centered LLMs.
    
\end{abstract}

\input{latexfig/intro_examples}