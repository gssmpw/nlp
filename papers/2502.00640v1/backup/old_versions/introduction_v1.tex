
--------------

% TODO: Real-world setting is collaborative, 
% But now is single-turn, 
% So there is this gap, 
% Now we want the training to be more close, 
% Distinguish o1. 


% WHAT IS THE GENERAL DOMAIN 
Interactive AI systems has received growing attention with the rise of Large Language Models (LLMs), which guide multiturn conversations or actions in human-machine interactions to achieve specific goals and improve task completion \citep{Wang24, proactive_survey}.
These interactive systems move beyond traditional response-generation models by emphasizing human-centered and collaborative problem solving \citep{rethinking_conv_agent, nerual_approach, clarify_survey}, spanning a wide range of practical tasks like question answering \citep{abg_coqa, pacific, m2lingual}, information-seeking \citep{inscit, clamber}, tutoring \citep{cima}, and code generation \citep{benchmark_code_generate, codegen}.
% , and agentic planning \citep{ask_before_plan, agentboard}.

% WHAT IS THE PROBLEM AND WHY IS IT IMPROTANT
Building on this progress, we aim to enhance the interactivity of LLMs toward a more human-centered and proactive manner. We envision that these LLMs would not merely respond to user inputs but actively collaborate with users by identifying missing or unclear information, anticipating user needs, and offering decisive guidance to resolve difficult situations. For example, if a user seeks help with a math problem but omits a key detail, such as whether the equation is linear or quadratic, the LLM could ask, “Could you clarify whether the equation is linear or quadratic? This will help me provide the correct solution.” In anticipating needs, if a user inquires about the weather in a particular city, the LLM might recognize that the user could be planning a trip and proactively offer assistance with booking local hotels. When editing documents, the LLM might detect that a document section is overly technical for a general audience and suggest, “This section appears quite technical — would you prefer simplifying it for broader readability, or keeping it for specialists?” By incorporating these capabilities, LLMs can significantly enhance the quality of human-machine interaction, facilitating more effective collaboration that supports efficient problem-solving and decision-making across a range of complex tasks.

% WHY DOES PREVIOUS WORKS FAIL
However, current LLMs, especially those trained using standard Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf} appear limited in achieving the desired level of interactivity. Although RLHF improves response quality, it often emphasizes short-term gains by rewarding models for immediate, turn-based replies. This tends to encourage reactive behavior, with models focusing on the next response rather than promoting proactive, long-term collaboration. As a result, many models struggle to anticipate user needs or guide conversations effectively, often missing opportunities to ask clarifying questions or offer alternative solutions.

% WHY DOES RECENT WORK FAIL
Recently, research efforts leveraging prompting and reinforcement learning have made progress in improving LLMs' interactivity, but they remain limited in scope. Prompting techniques \citep{ask_more_informative_questions, clarify_when_necessary, clarifygpt, rephrase_and_respond} often guide models to follow predefined templates or question structures, which can be rigid and fail to adapt to diverse or evolving conversational contexts. Similarly, reinforcement learning for improved interactivity~\citep{baize, clarify_question_for_retrieval, learn_to_clarify, star_gate, self_correct}, though effective in specific domains such as training LLMs to ask clarification questions or improve correctness, often falls short of acquiring upon more general interaction strategies. This constrained focus limits the broader potential of interactive systems, as models trained in this way often lack the flexibility to handle more open-ended and complex interactions. 

% WHAT IS OUR METHOD
To address these limitations, we propose a novel framework that trains LLMs in a simulated interactive environment transformed from any single-turn tasks. At the core of this framework, we design a general multiturn reward function, which can be integrated with RL algorithms such as DPO~\citep{dpo} and PPO~\citep{ppo}. Unlike traditional RLHF, our reward function encourages models to generate responses that not only resolve the immediate query but also facilitate richer, more informative interactions in subsequent turns. Through extensive experimentation on interactive question answering, code generation, and document editing tasks, our framework demonstrates significant improvements over baseline approaches. Additionally, the trained models exhibit greater interactivity and effectiveness in managing complex and wide-ranging interactions.

