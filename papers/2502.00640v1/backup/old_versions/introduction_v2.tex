\section{Introduction}
% WHAT IS THE GENERAL DOMAIN 
The real-world use of large language models (LLMs) has promoted collaborative problem-solving between humans and AI~\citep{rethinking_conv_agent, neural_approach, clarify_survey}. In these collaborations, an LLM and a human interact to achieve specific goals and complete a wide range of tasks, such as information seeking~\citep{inscit, clamber, cima,pacific}, code writing~\citep{benchmark_code_generate, codegen}, and document editing~\citep{interactive_text_gen}.

% WHAT IS THE PROBLEM AND WHY THE PROBLEM IS IMPORTANT
\xhdr{Challenges} However, there is a gap between how these LLMs are used in the real world and how they are trained or fine-tuned. 
Specifically, LLMs are generally trained in non-interactive environments without long-term collaboration with users, leading to a mismatch that limits their effectiveness and efficiency when collaborating with humans. 
Moreover, current LLMs trained using standard Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf} often emphasizes short-term gains by rewarding models for immediate, single-turn replies. This tends to encourage reactive behavior, with models focusing on the next response rather than promoting effective collaboration in multiturn interactions, which is more common and useful in the real-world usage. 
For example, in Figure~\ref{fig:examples} (a), the LLM provides plausible but untargeted information to the user instead of actively collaborating and leveraging the user provided information. 
This lack of interaction leads to inefficient problem-solving and fails to fully meet the real-world needs.

% WHY DOES RECENT WORK FAIL
While recent research utilizes prompting and reinforcement learning (RL) to enhance LLMs' collaborative problem-solving abilities, these approaches remain limited in scope. Prompting techniques \citep{ask_more_informative_questions, clarify_when_necessary, clarifygpt, rephrase_and_respond} often guide models to follow predefined instructions, such as asking follow-up questions, which can be rigid and fail to adapt to evolving conversational contexts. Similarly, RL methods~\citep{baize, clarify_question_for_retrieval, learn_to_clarify, star_gate, self_correct}, though effective in specific domains like training LLMs to ask clarification questions or improve accuracy, often fall short of adopting more general collaboration strategies. This limits broader applications, as models trained in this way frequently lack the flexibility needed to handle open-ended and complex interactions with humans.

% WHAT IS OUR APPROACH AND HOW DOES IT AOLVE THE PROBLEM (HIGH-LEVEL)
\xhdr{Our work: \Object (Figure~\ref{fig:overview})} To address these limitations, we propose a novel and general LLM training framework which enhances their collaborative problem-solving capabilities with humans across a broad range of scenarios~\citep{rethinking_conv_agent, neural_approach, clarify_survey}. 
The key insight is to promote forward-looking ability in LLMs through simulated interactions during the training phase. By "forward-looking," we mean that, rather than rewarding the immediate response generated by the LLM, we reward the model based on the expected long-term benefit of its response — referred to as a \textit{multiturn-aware reward} — which encourages responses that lead to more effective collaboration with humans in future conversations. To compute the multiturn-aware reward, we build an internal simulation pipeline that generates the interactive conversations following each model response during training. 

 
% HOW DOES IT SOLVE THE PROBLEM (LOW-LEVEL)
Our framework is plug-and-play and can be integrated into the existing RL algorithms, such as Direct Preference Optimization (DPO)~\citep{dpo} and Proximal Policy Optimization (PPO)~\citep{ppo}, to achieve a higher level of interactivity. After training, our \objects would not merely respond to user requests but actively collaborate with users to achieve long-term gains. In the example of Figure~\ref{fig:examples} (b), the \object actively asks follow-up questions and generate more targeted contents given the acquired information, ultimately leading to more effective collaboration. In general cases, our \objects could also identify unclear information to prevent future confusion, anticipate user needs to reach the ultimate goal more efficiently, and offer alternative directions to avoid dead ends. 

We design three challenging multiturn tasks: \doct, \codet, and \mathct, which cover document editing, code assistance, and question answering tasks. Compared to the best baselines, our approach improves the task-specific metric by \taskimprov and the interactivity metric from an LLM judge by \itrimprov on average. Furthermore, we find that \objects increase the task efficiency by \efficiencyimprov which significantly reduces the human effort and time. 
Beyond the task that the \objects are finetuned on, we show that their capabilities are highly generalizable to other data domains. 
% These improvements, achieved through the multiturn-aware reward inspired by the forward-looking strategy, underscore the improve collaborative ability of our LLMs. 
Our key contributions are:

\begin{itemize}[leftmargin=15pt]
    \item We introduce a novel and plug-and-play training framework for improving LLM interactivity in real-world collaboration with humans, which can be easily integrated into RL training frameworks like DPO.
    
    \item We propose a multiturn-aware reward mechanism that encourages forward-looking responses, enabling LLMs to guide conversations toward effective, long-term collaboration.
    
    \item We demonstrate the effectiveness of our approach on three multiturn tasks, achieving significant improvements in task-specific metrics, interactivity, and efficiency over tche baselines. Moreover, extensive ablation and case studies provide insights to understand the behaviors of \objectst.
\end{itemize}



% For example, if a user seeks help with a math problem but omits a key detail, such as whether the equation is linear or quadratic, the LLM could ask, “Could you clarify whether the equation is linear or quadratic? This will help me provide the correct solution.” In anticipating needs, if a user inquires about the weather in a particular city, the LLM might recognize that the user could be planning a trip and proactively offer assistance with booking local hotels. When editing documents, the LLM might detect that a document section is overly technical for a general audience and suggest, “This section appears quite technical — would you prefer simplifying it for broader readability, or keeping it for specialists?” 

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/overview_combined3}
%     \vspace{-15pt}
%     \caption{Comparison between (a) Non-interactive LLM training and (b) Interactive LLM training (Ours). The traditional LLM training overlooks the interactive collaboration between human and LLM which results in ineffective and inefficient conversation. While the \object asks follow-up questions and provides more targeted predictions, which results in effective collaboration.}
%     \label{fig:overview}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/previous_pipeline2.pdf}
%         \vspace{-10pt}
%         \caption*{(a) Non-interactive LLM training}
%         \label{fig:previous}
%     \end{minipage}%
%     \begin{minipage}{0.55\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ours_pipeline.pdf}
%         \vspace{-10pt}
%         \caption*{(b) Interactive LLM training}
%         \label{fig:ours}
%     \end{minipage}
%     \vspace{-5pt}
%     \caption{Comparison between non-interactive and interactive LLM training pipelines. }
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/bad_conv.pdf}
%         \vspace{-20pt}
%         \caption*{(a) Non-interactive LLM }
%         \label{fig:previous}
%     \end{minipage}%
%     \begin{minipage}{0.52\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/good_conv.pdf}
%         \vspace{-20pt}
%         \caption*{(b) Interactive LLM}
%         \label{fig:ours}
%     \end{minipage}
% \end{figure}
