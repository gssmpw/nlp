\section{Problem Formulation}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figures/multiturn2}
%     \caption{Overview of our work. At each model turn, we provide reward supervision for the model response by simulating interactive conversations and computing the expected multiturn-aware reward over the conversations.}
%     \label{fig:principles}
% \end{figure}

\xhdr{Previous focus: Single-turn generation} In a single-turn setting, given a prompt $x$ consisting of a complete problem description, an LLM $M$ generates an answer $m = M(x)$, where a general objective in reinforcement learning (RL) is to optimize the reward function\footnote{For simplicity, we consider reward maximization, although certain RL objectives, such as Direct Preference Optimization~\citep{dpo}, involve differentiating between rewards of winning and losing responses} $R(m \mid x)$. %, where $R$ is a reward function.

% \footnote{For simplicity, we consider reward maximization, although certain RL objectives, such as Direct Preference Optimization~\citep{dpo}, involve differentiating between $R(m_w \mid x)$ and $R(m_l \mid x)$, where $m_w$ and $m_l$ are the winning and losing responses, respectively.}

% produce an answer that approximates a ground truth answer $y$ or minimizing a loss function, to accurately addresses the user query.

\xhdr{Our focus: Goal-oriented multiturn interactive conversation}
\label{sec:formulation}
In a real-world setting, different from single-turn tasks without further human involvement, we denote a user's underlying (implicit) goal as $g$ in a multiturn conversational task. The conversation unfolds over multiple turns $t_j = \{u_j, m_j\}$ for $j = 1, \dots, K$, with user input or response $u_j$ and model response $m_j$ at each turn and $K$ is a variable denoting the number of conversational turns. 

At the $j$-th turn, the model generates its response based on the previous conversation turns $t_{1:j-1} = \{t_1, \dots, t_{j-1}\}$ and the current user response $u_j$. For simplicity, we define historical conversation at turn $j$ as $t^h_j = t_{1:j-1}\cup \{u_j\}$, therefore, $m_j = M(t^h_j)$. The objective is to collaboratively achieve goal $g$ through interactive conversations, and efficiently obtain results derived from the sequence of model responses $\{m_j\}_{j=1}^{K}$ to users' satisfaction. Formally, we define the objective as $R^*(t_{1:K} \mid g)$, where $R^*$ incorporate factors such as user experience and task performance, distinguished from the traditional reward function $R$.


The multiturn setting significantly increases task complexity, introducing challenges such as discovering user intents, ensuring mutual understanding, and minimizing user effort, which are discarded in the single-turn tasks. These aspects combined ultimately impact the model's ability to achieve the user's underlying goal. For example, the alignment between the implicit user goal and the estimated user goal / requirements inferred from the conversation history will influence the model's ability to drive the final answer.

% Generally talk about user, want an interactive env. 

% Ideally with training use real users but not applicable.

% Training environment and testing environment should be similar. We want the user simulator to approximate real users.
%\section{A Unified Interactive Training Approach}
\section{A Unified Interactive Training Framework}
%\section{Our Method: A Unified Training Framework for Interactive LLMs}

We train LLMs in an interactive environment that simulates real-world user interactions, enabling the model to learn effective strategies for multiturn conversations. We start with key principles on our framework design.

\subsection{Key Motivations for Interactive LLM Training}
%\subsection{Key Principles for Interactive LLM Training}
% Other options:
%Key Elements/Strategies/Components/Dimensions for Interactive LLM Training
% or Key Motivating Principles for ...
\label{sec:principle}

Traditional LLM training frameworks, such as Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf}, focus on maximizing immediate rewards for single-turn tasks. While effective for single-turn objectives, this approach overlooks the broader context of multiturn interactions and the long-term impacts of responses.

%\textbf{Principle 1:}  
\textbf{Motivation 1:}  
\textit{Traditional RLHF leads reactive model behaviors due to the misalignment between their single-turn objective $R(m \mid x)$ and real-world multiturn objective $R^*(t_{1:K} \mid g)$ (see notation definitions in Section~\ref{sec:formulation}).} 

Specifically, the model's accumulative reward $\sum_{j=1}^{j=K} R(m_j \mid t_j^h)$ may not imply a higher multiturn-aware reward $R^*(t_{1:K} \mid g)$. In fact, it is possible for the model to achieve high single-turn rewards at each turn while failing to achieve the overarching goal $g$. For example, consider a task where the user’s goal $g$ is to write an engaging article. A model trained with traditional RLHF might generate isolated responses, like drafting an introduction or listing conclusions. While these responses are helpful in isolation, they fail to consider how the sections flow together, resulting in an article that might not be cohesive and aligned with the user’s goal.

%\textbf{Principle 2:}  
\textbf{Motivation 2:}  
\textit{Effective multiturn collaboration requires model responses that optimally contribute to the expected value of $R^*(t_{1:K} \mid g)$.}

Instead of focusing only on maximizing immediate rewards or cumulative utility over future turns, the model should aim to align its responses with the user’s goal $g$ by considering their impact on the entire conversation trajectory $t_{1:K}$. This encourages responses that enhance interactivity and progress toward achieving $g$ while maintaining efficiency. In the previous example, instead of generating a conclusion, asking, ``Should I maintain an engaging tone in the conclusion like the introduction?'' offers better long-term alignment with the goal, avoids unnecessary revisions in the future, and ensures the final article is cohesive and satisfactory.

%\textbf{Principle 3: }  
\textbf{Motivation 3: }  
\textit{Effective multiturn collaboration relies on forward-looking strategies, where responses shape future interactions and align with the overarching goal $g$.}  

Rather than simply addressing the current user input, the model should consider how its responses influence the remaining turns of the conversation. By distinguishing between the possible trajectories that follow different responses, the model can anticipate user needs, resolve ambiguities, and select responses that are more likely to  lead to successful conversations.
%guide the conversation toward successful outcomes aligned with the overarching goal.


\xhdr{Connection between forward-looking strategies with causal effect}
This forward-looking strategy naturally aligns with causal effect estimation, as it requires understanding how a response affects the trajectory of future interactions. From a data-generating perspective on multiturn conversations, the causal effect of model response $m_j$  by front-door adjustment~\citep{Pearl09a, pearl2016causal} is 
$ \sum_{t_{j+1:K}} P(t_{j+1:K} \mid t_j^h\cup\ \{m_j\}) R^*(t_{1:K} \mid g)$. 
%  = \mathbb{E}_{t_{j+1:K}\sim P(t_{j+1:K} \mid t^h_j)} R^*(t_j^h\cup t_{j+1:K} \mid g)
% $$\sum R^*(t_{1:K} \mid g) P(t_{1:K} \mid t^h_j)P(t^h_j) = \sum R^*(t_{1:K} \mid g) P(t_{1:K} \mid t^h_j) = \mathbb{E}_{t_{1:K}\sim P(t_{1:K} \mid t^h_j)} R^*(t_{1:j} \mid g)$$
Essentially, this estimation isolates the impact of model response $m_j$ on the conversation trajectory. By evaluating this effect, the model can identify responses that maximize progress toward the user’s goal $g$. This distinguishes our approach from other training frameworks on multiturn trajectories~\citep{multiturn_rlhf, archer, refuel}, where our key insight is to capture the intervention effect rather than learning from observations over conversations and their trajectory-level rewards.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.00\linewidth]{figures/cropped/flow_v2}
    %\includegraphics[width=1.00\linewidth]{figures/flow_v2}
    \vspace{-20pt}
    \caption{Generating high-quality conversation data with Multiturn-aware Rewards (\ourst): Starting from a user query, LLM responses are sampled and ranked by \ours scores, with higher-ranked responses marked as ``Chosen'' and lower-ranked as ``Rejected''. The first turn from the chosen response's simulated interaction window is added to the prompt for the next turn, iteratively continuing until the conversation concludes. Solid orange and dashed blue arrows indicate SFT and DPO data collection, respectively.}
    \label{fig:flow}
\end{figure*}
\subsection{Our Objective: Maximizing Multiturn-aware Reward for Turn-wise Model Responses}
\label{sec:reward} 

Building on the principles outlined earlier, we define the Multiturn-aware Reward (MR) as follows.

%\xhdr{Definition 1 (Multiturn-aware Reward)}  
\xhdr{Multiturn-aware Reward:}  
\textit{The multiturn-aware reward for the model’s response at the $j$-th turn is given by:}
\begin{equation}
\begin{aligned}
    &\text{\ourst}(m_j \mid t_j^h, g) \\=& 
    ~\mathbb{E}_{t_j^f \sim P(t_{j+1:K} \mid t_j^h \cup \{m_j\})} R^*(t_j^h \cup \{m_j\} \cup t_j^f \mid g) \\
    = &~\mathbb{E}_{t_j^f \sim P(t_j^f \mid t_{1:j})} R^*(t_{1:j} \cup t_j^f \mid g),
    \label{eq:main}
\end{aligned}
\end{equation}
\textit{where $t_{1:j}$ denotes the conversation history up to and including turn $j$, and {\small $t_j^f = t_{j+1:K}$} represents the forward trajectory of turns following the $j$-th turn. The distribution {\small $P(t_j^f \mid t_{1:j})$} models the possible forward trajectories conditioned on the prior conversation history.} 

To compute Equation~\ref{eq:main}, two key components should be addressed:  
(a) approximation of $R^*(t \mid g)$, a reward function for evaluating an arbitrary multiturn conversation $t$, and  
(b) the sampling strategies for obtaining $P(t_j^f \mid t_{1:j})$, which represents the forward conversation distribution.  We elaborate on these components in the following sections.

\subsubsection{Conversation-level Reward Function}

To evaluate the quality of interactive conversations, we approximate the conversation-level reward $R^*(t \mid g)$ with a combination of extrinsic (goal-specific) and intrinsic (goal-agnostic) metrics:
\begin{equation}
R^*(t \mid g) \simeq R_{\text{ext}}(t, g) + R_{\text{int}}(t),
\end{equation}
where $R_{\text{ext}}(t, g)$ focuses on task-specific success, and $R_{\text{int}}(t)$ evaluates engagement and efficiency. Each component is defined below.


\begin{itemize}%[leftmargin=*]
    \item \textbf{Extrinsic Reward} ($R_{\text{ext}}(t, g)$): This term measures how well the conversation achieves the user’s goal $g$. Formally:
    \begin{equation}
        R_{\text{ext}}(t, g) = S(\operatorname{Extract}(t), y_g),
    \end{equation}
    where $\operatorname{Extract}(t)$ extracts the final solution or response from the conversation $t$, especially for tasks requiring revisions or multi-step answers. $y_g$ is the reference solution for the goal $g$, \eg the ground truth solution for a math problem. And $S(\cdot, \cdot)$ evaluates task-specific metrics like accuracy or similarity. This ensures the conversation contributes directly to achieving the desired goal.

    \item \textbf{Intrinsic Reward} ($R_{\text{int}}(t)$): This rewards interactive and efficient conversations, defined as
    \begin{equation}
        R_{\text{int}}(t) = R_{\text{LLM}}(t) - \min[\lambda \cdot \text{TokenCount}(t), 1],
    \end{equation}
    where $R_{\text{LLM}}(t)$ is the interactivity score assigned by the LLM-based judge~\citep{llm_as_judge} on a scale from 0 to 1. 
    Other conversational aspects such as clarity can be further incorporated into the objective. 
    Efficiency is encouraged in the second term by penalizing lengthy interactions. 
    $\lambda$ controls the penalty for excessive token usage. The token length penalty is bounded by $1$ to achieve balance with LLM-based metrics. 
\end{itemize}

To highlight, the conversation-level reward incorporates task-specific and human-centered metrics, encouraging the model to balance goal achievement, engagement, and efficiency along the conversation

\subsubsection{Forward Sampling}

To compute Equation~\ref{eq:main}, we require the distribution of forward conversation trajectories $t_j^f$ conditioned on the conversation history up to the current turn $t_{1:j}$. A straightforward approach is to use Monte Carlo sampling, where the conversation is extended turn-by-turn until it either concludes or reaches the maximum allowed turns $K$. However, this approach can be computationally expensive, especially for lengthy conversations. 

To address this, we introduce a window size $w$ as a hyperparameter to limit the maximum number of forward turns considered in $t_j^f$. This reduces the computational cost while maintaining sufficient context for meaningful forward sampling. Additionally, while real-world conversations could be gathered from human participants, this process is both costly and impractical when multiple forward samples are needed during training. To further reduce cost and ensure scalability, we introduce a user simulator $U$.

%\xhdr{Definition 2: User Simulator}  
\xhdr{User Simulator:}  
\textit{A user simulator $U: \mathcal{T} \rightarrow \mathcal{U}$ is a function that maps a given conversation history $t \in \mathcal{T}$ to a user response $u \in \mathcal{U}$. Specifically, $U$ generates a probabilistic distribution $P(u \mid t)$ over possible user responses conditioned on the conversation history $t$, simulating realistic user behavior. }

Specifically, we prompt an LLM to role-play as users,  explicitly asking the LLM to follow the same language style as the previous user turns, and injecting typical user behaviors (see Appendix \ref{app:user_simulator} for implementation details). The user simulator operates with an implicit goal $g$, which it seeks to achieve over the course of the conversation. This design emulates real-world scenarios where users may have evolving needs, limited background knowledge, or require clarification, resulting in naturally unfolding multiturn conversations~\citep{simulate1000}.

\subsubsection{Optimization with Synthetic Data}
%\subsubsection{Optimization Pipeline \& Synthetic Data Generation} 
\label{sec:optimization}
Given a conversation history ending with a user response, the multiturn-aware reward can be computed for any model response. This results in (context, response, reward) tuples, which can be utilized for reinforcement learning (RL) training methods such as PPO~\citep{ppo}. 
% or DPO (DPO)~\cite{dpo}. In the case of DPO, multiple pairs of responses and their corresponding rewards under the same prompt can be compared to create preference-based training data.

\xhdr{Benefits of Training w/ \ourst} To highlight, computing \ourst savoid the need to train an additional reward model, which could be a costing and slow process. Moreover, unlike traditional single-turn reward approaches, the \ours explicitly accounts for the impact of the current response on future conversation trajectories. By optimizing for \ourst s, the model learns to navigate complex, interactive conversations and produce responses that are more likely to lead to higher overall rewards by the end of the conversation.

% \subsection{Multiturn Synthetic Data Generation}
% \label{sec:generation}

Beyond its role in direct training, \ours can be used to generate high-quality synthetic conversations that serve as a robust data resource for supervised fine-tuning (SFT) and Direct Preference Optimization (DPO)~\citep{dpo}, as shown in Figure~\ref{fig:flow}. 

\xhdr{Synthetic SFT Data} Using \ourst, we iteratively select top-ranked responses to extend conversation histories. This approach generates synthetic conversations that reflect realistic and goal-directed interactions.

\xhdr{Synthetic DPO Data} \ours also facilitates the creation of pairwise comparisons for DPO training. By ranking responses at each turn based on their \ours scores, ``chosen'' and ``rejected'' pairs are constructed. These comparisons provide preference-based data that trains the model to produce responses balancing long-term objectives and interactivity.

\xhdr{Benefits of Training w/ \ourst-based Synthetic Data} 
% By leveraging \ours to evaluate and rank responses in simulated interactions, the generated data inherently aligns with multiturn objectives, promoting effective and efficient conversations. 
\ourst-based synthetic data aligns with multiturn objectives with preference on responses based on their long-term contribution to the conversation's overall success. The use of user simulators and forward sampling enables the efficient creation of large-scale datasets without requiring human annotations and is compatible with diverse task-specific objectives.

% This scalable and effective approach produces realistic multiturn conversations, complementing the direct use of \ours in (online) training and enhancing the model's ability to navigate complex interactions.

