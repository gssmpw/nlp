\section{Experiments}
We conducted extensive and large-scale experiments to answer the following research questions:
\begin{itemize}%[leftmargin=*]
    \item How \textit{effective} are the \objects in collaborating with users? (Sec.~\ref{sec:quantitative})
    %\vspace{-3pt}
    \item How do \objects outperform? What component is \textit{essential} to their success? 
    %\vspace{-3pt}
    (Sec.~\ref{sec:ablation})
    \item What are the \textit{insights} from the generation results from interactive-trained LLMs? (Sec.~\ref{sec:case})
    %\vspace{-3pt}
    \item Can the capabilities of interactive-trained LLMs \textit{generalize} to untrained data domains? (Sec.~\ref{sec:extended})
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/cropped/tasks.pdf}
    \vspace{-10pt}
    \caption{Multiturn Evaluation Pipeline (Demonstrations).
    We present a realistic evaluation pipeline to assess LLMs' multiturn collaboration capabilities. Unlike single-turn setups, where LLMs respond to complete, unambiguous, and static problems directly, our approach leverages target problems to inform user simulators that emulate real-world user behaviors and personalities. This setup evaluates both the quality of the final solution and the effectiveness of the entire conversation.}
    \label{fig:evaluation}
\end{figure*}

\subsection{Evaluation Setup: Multiturn Interactive Conversation for Goal Achievement}

Due to the lack of goal-oriented conversational datasets, we create three multiturn datasets using publicly available data across diverse domains~\citep{math, bigcodebench, medium}: multiturn mathematics problem solving, coding problem assistance, and collaborative document editing.

Unlike traditional single-turn tasks, these datasets simulate real-world scenarios where achieving a goal requires dynamic interactions over multiple turns. To enable this, we employ a user simulator to role-play realistic user behaviors and provide an interactive environment for evaluation. Our simulation-based evaluations are designed to closely mimic real-world interactions. This is supported by the findings of \citet{simulate1000}, which demonstrate that LLMs/LLM-based agents can effectively emulate the responses and behaviors of participants. It is important to note that, while we sample problems to inform the user simulator with realistic contexts, the evaluation is conducted at the conversational level, assessing the overall interaction's success rather than individual problem context. We construct the following evaluation datasets:
%
\begin{itemize}[leftmargin=*]
    \item \textbf{\doct}: Document editing tasks involve iterative feedback and incremental refinements across multiple turns to address diverse user inputs, ensure coherence, and achieve a polished final document, closely mirroring real-world collaborative writing processes. We sample 100 Medium articles\footnote{\url{https://huggingface.co/datasets/fabiochiu/medium-articles}} as goal documents to guide the user simulator. The user simulator and LLM collaborate to revise and improve the document toward the goal. The final aggregated document is extracted using an extractor LLM that consolidates updates from each turn. The document quality is evaluated using \textbf{{BLEU}} score, which measures similarity between the generated document and the target document.
    
    \item \textbf{\codet}: Coding tasks inherently require step-by-step interactions, such as clarifying requirements and debugging, making them well-suited to multiturn conversational settings. We sample 600 coding problems from BigCodeBench~\citep{bigcodebench} to define the user simulator's context. The evaluation is conducted at the conversational level, measuring the \textbf{{Pass Rate (PR)}} of the final code submissions produced after the interaction.
    \item \textbf{\mathct}: Solving math problems often involves addressing implicit assumptions, verifying intermediate steps, or clarifying reasoning, making it a relevant use case for multiturn interactions where iterative feedback ensures accuracy and alignment with the problem-solving process. We sample 200 math problems from the level-5 questions in MATH~\citep{math} to prompt the user simulator which collaborates with LLMs. The task success is measured by the \textbf{{accuracy (ACC})} of the final solution provided at the end of each conversation, as evaluated by an LLM-based judge.
    

\end{itemize}


Besides task-specific metrics, we include two task-agnostic metrics for all datasets: (1) \textbf{{Interactivity (ITR)}}, which measures the degree of engagement during the conversation using an LLM judge. We rescale the LLM judge's scores to a range of [0, 1]; and (2) \textbf{{Average Token Count}:} Quantifies the average number of tokens generated by the LLM assistant per conversation, which reflects the efficiency of the interaction.

\input{tables/stats}
\input{tables/main_results}

\xhdr{Baseline and Reference Models} We evaluate our finetuned models against (1) the pretrained \llama~\citep{metallama} (\textit{Base}), \ie base model, and (2) the base model with proactive prompting (\textit{Proactive Base}), where prompts encourage the model to ask follow-up and clarification questions (\cf Appendix~\ref{app:proact}). Moreover, we include single-turn results from the base model (\ie \textit{Reference (idealized)}) as a reference, where the model is provided with idealized, complete inputs and generates the solution directly, disregarding any interaction with users. 

\xhdr{Ours: Interactively-Trained Models} Our \objects include several variants: (1) SFT and Offline DPO models that are initialized with the \llama base model, and are further finetuned on pre-generated multiturn conversational datasets (Table~\ref{tab:stats}) guided by our multiturn-aware reward (as discussed in Section~\ref{sec:optimization}), and (2) PPO and Online DPO, which further trained upon the SFT and Offline DPO models, respectively, by further training in an online setting to refine interaction strategies based on the model's responses during conversations. We conduct LoRA finetuning~\citep{lora} for all our training experiments (\cf Appendix~\ref{app:train} for details).

\subsection{Main Results on Multiturn Conversational Datasets}
\label{sec:quantitative}

We present the results in Table~\ref{tab:results} and discuss the following takeaways:


\xhdr{Takeaway 1: Multiturn settings are generally more challenging compared to single-turn settings.}
For \mathct and \codet, LLMs often exhibit stronger domain knowledge than users, but their inability to ask clarifying questions or adapt to user needs leads to significant discrepancies in multiturn collaborations. For example, comparing the base model's performance in single-turn settings with idealized inputs, \ie Reference (idealized), to its performance in multiturn collaborative settings, \ie Base, reveals a sharp decline: on \mathct, accuracy drops from 19.5\% to 11.9\%, and on \codet, pass rate decreases from 14.8\% to 9.3\%.
This highlights the complexity of multiturn tasks in the real world compared to simplified single-turn settings with idealized and complete inputs.

\xhdr{Takeaway 2: Prompting is helpful, but limited in context-aware collaboration.}
Proactive prompting (\ie Proactive Base) improves the performance of base models in multiturn settings by encouraging follow-up questions and clarifications. 
For example, compared to the base model, proactive prompting increases BLEU score on \doc from 32.2\% to 35.0\% and improves interactivity. However, these gains are relatively modest and do not fully address the challenges of multiturn collaboration. 
Specifically, prompting strategies are constrained by their reliance on predefined instructions, limiting their adaptability to dynamic conversations with adaptive and context-aware collaboration strategies.

\xhdr{Takeaway 3: Multiturn-aware interactive training boosts task performance, interactivity, and conversational efficiency.}
Our interactive training framework achieves \taskimprov superior task-specific performance, \itrimprov enhanced interactivity, and \efficiencyimprov more efficient conversations compared to the best baselines.

To highlight, our \objects engage in more meaningful and context-aware collaborations, with interactivity scores showing substantial gains. On \doct, the ITR increases from 0.46 with the base model to 0.92 with the interactive Online DPO model. We believe the enhanced interactivity contributes to improved task performance by enabling models to better understand user needs and align their responses with long-term goals.

Moreover, our framework significantly improves conversational efficiency by minimizing the amount of content users need to review to arrive at the final solution. On \mathct, the token count per conversation decreases from 3.40 with the base model to 2.37 with Online DPO. Overall, these results demonstrate the ability of multiturn-aware interactive training to balance goal-oriented collaboration with concise and effective communication, ensuring superior performance across complex multiturn tasks.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/cropped/ablation.pdf}
    \vspace{-5pt}
    \caption{Ablation Study of Reward Mechanisms on \doct. This figure compares three immediate reward mechanisms with three multiturn-aware reward (MR) variants (\cf Section~\ref{sec:ablation}). The results demonstrate that MR consistently improves task-specific performance (BLEU), interactivity (ITR), and conversational efficiency (\# Tokens) in multiturn collaborations compared to immediate reward mechanisms.}
    \label{fig:ablation}
\end{figure*}

\subsection{Ablation Study: How Do Interactively-Trained LLMs Outperform?}
\label{sec:ablation}

To investigate the components contributing to the superior performance observed in Table~\ref{tab:results}, we conduct an ablation study focusing on the reward mechanisms used during fine-tuning. Specifically, we compare reward mechanisms that vary in their ability to account for future conversational context by adjusting the forward sampling window size $w$ from 0 to 3. Notably, $w=0$ represents a special case where no forward sampling is performed, and the reward is based solely on the immediate response.

We evaluate the following reward mechanisms:
\begin{itemize}[leftmargin=*]
     \item \textbf{Immediate Rewards}: These rewards evaluate the model's immediate response based on:
        1) \textit{Helpfulness}, which assesses response quality from an LLM judge (prompt available in Appendix~\ref{app:system_prompts}); 
        2) \textit{Extrinsic Reward}, which focuses on task-specific metrics like BLEU but ignores intrisic properties like efficiency; and 
        3) \textit{Extrinsic + Intrinsic Reward}, which combines task-specific metrics with interactivity and efficiency. This last mechanism can be viewed as a special case in our multiturn-aware reward function with $w=0$.
    
    \item \textbf{Multiturn-Aware Reward (MR) with Forward Sampling}: Our method which uses simulated interactions with a forward sampling window $w$ ($w=1,2,3$) to capture long-term response effects.
\end{itemize}

We use the \llama base model and apply online DPO fine-tuning with the specified reward mechanisms. Results for the \doc task are shown in Figure~\ref{fig:ablation}.

To highlight, the improvements achieved by MRs emphasize the importance of long-term conversational effects. Increasing the forward sampling window size $w$ enhances interactivity and reduces token counts; for example, MR with $w=3$ achieves the best interactivity and efficiency. Immediate rewards, even when combining extrinsic and intrinsic components, are less effective than multiturn-aware rewards, which better capture future conversational trajectories. MR with $w=2$ and $w=3$ strikes a balance between engagement and efficiency, making it suitable for real-world scenarios. This study validates multiturn-aware rewards with forward sampling, for fostering collaborative conversational strategies.


% Different window sizes -> curve

\subsection{Case Study: Responses from \Objects}
\label{sec:case}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/coding}
    \vspace{-10pt}
    \caption{Case study on \codet. The non-interactive LLM (left) generates a generic response without clarifying user-specific needs, resulting in unnecessary processing steps such as removing punctuation and stopwords that were falsely assumed. In contrast, the interactive LLM (right) engages with the user by asking clarifying questions about tokenizer preferences, error handling, and package installation. This efficient collaborative process leads to an implementation that aligns with the user’s intent. }
    \label{fig:coding}
\end{figure*}

\begin{figure}[t]
\centering
\begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/cropped/case_study.pdf}
    \caption{Reward comparison for response \texttt{A} and \texttt{B} (Fig.~\ref{fig:coding}) shows different preferences.}
    \label{fig:reward_preference}
\end{minipage}%
\hfill
\begin{minipage}{0.49\textwidth}
    \centering
    \resizebox{1.0\textwidth}{!}{
    \begin{tabular}{cccc}
        \toprule 
        & \multicolumn{2}{c}{Accuracy} & \multirow{2}{*}{Macro F1}\\
        & Ambiguous   & Non-Ambiguous \\
        \midrule
        \texttt{GPT-4o} & 15.44\% & 95.60\% & 56.62\% \\
        \midrule
        \texttt{\small non-Interactive} & 16.26\% & 90.40\% & 53.31\%\\
        \texttt{Interactive} & 52.84\% & 72.32\% & 55.08\%\\
        \bottomrule
    \end{tabular}
    }
    \captionof{table}{Zero-shot generalization results on \ambcoqa benchmark, containing both ambiguous and non-ambiguous questions. The online DPO model trained on \code is directly applied without any few-shot prompts or hints indicating question ambiguity.}
    \label{tab:abg_coqa}
\end{minipage}
\end{figure}
Studying model responses provides insights into the expected properties and behaviors of the model. We provide an example (Figure~\ref{fig:coding}) where a user requests coding assistance to tokenize a text file. Many aspects of the problem, such as the Python environment the user is using and the preferred tokenization method, remain unspecified. However, the non-interactive LLMs proceed with assumptions, such as lowercasing the text and removing stopwords. In contrast, the \object trained with our framework asks clarifying questions about the user's preferences, including the use of the NLTK tokenizer, exception handling, and data preprocessing steps. This approach delivers a solution that aligns precisely with the user’s needs and passes the unit tests.

Beyond differences in model responses, applying different reward mechanisms to each response (Figure~\ref{fig:reward_preference}) reveals the preferences of various reward functions. For instance, the helpfulness reward function assigns a significantly higher reward to Response A than Response B, favoring a detailed solution over follow-up questions. However, since our multiturn-aware reward function considers the impact on future turns, the reward evaluation adjusts, assigning a higher score to Response B than A. This adjustment reflects the positive impact Response B has on the overall conversation by saving the user effort and time otherwise spent on incorrect solutions and misunderstandings. Moreover, when the window size is set to 2 or 3, it provides a better estimation compared to a window size of 1 by capturing longer-term impacts of a response that may not be recognized in shorter or single-turn interactions. This observation is consistent with the ablation study in Section~\ref{sec:ablation}.

Additional examples are provided in Appendix~\ref{app:complex} due to space limitations. These examples demonstrate the \object's capabilities in various scenarios, such as ensuring users understand basic concepts before introducing complex solutions, adapting to cultural preferences when providing wedding advice, and asking users to specify their confusion to deliver more targeted explanations for coding problems.


\subsection{Extended Study: Do \Objects Generalize to Other Tasks?}
\label{sec:extended}
% Zero-shot application to clarification benchmark?
It is essential for LLMs to generalize their post-training capabilities to new inputs. While our \objects demonstrate capabilities far beyond asking clarification questions, we aim to test whether this ability generalizes to existing clarification benchmarks in entirely different domains.

Specifically, we evaluate our online DPO model trained on \codet, the code assistance task, on \ambcoqa~\cite{abg_coqa}, a clarification benchmark that assesses whether LLMs can ask clarification questions for ambiguous queries and provide direct answers for non-ambiguous ones. Each question in \ambcoqa is derived from a story provided in the context. We chose \codet because of the distinct nature of coding tasks compared to the reading comprehension tasks in \ambcoqat.

Unlike previous works~\citep{abg_coqa, learn_to_clarify}, we present the questions without few-shot prompts or any indication that they might be ambiguous (see question format and an example in Appendix~\ref{app:abg_coqa}). This approach represents a more realistic setting and poses a greater challenge for models to make open-ended decisions.

In Table~\ref{tab:abg_coqa}, we observe that the behavior of the non-interactive \llama model and the \texttt{GPT-4o} (\texttt{gpt-4o-2024-08-06}) model is highly similar: only about 16\% of ambiguous questions are followed up with clarifications. This finding indicates that some widely used LLMs often make false assumptions and rarely identify ambiguity. Interestingly, our \object improves accuracy on ambiguous questions to around 53\%, representing a 37\% improvement. However, the \object occasionally asks unnecessary questions, resulting in an 18\% decrease in accuracy for non-ambiguous questions compared to the non-interactive model. Nonetheless, in critical cases where important decisions are involved, double-checking the question through additional clarification may be less harmful than missing ambiguity and making unsafe assumptions.

