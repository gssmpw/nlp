% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=1.0\linewidth]{figures/examples_v3}
%     \vspace{-20pt}
%     \caption{Real examples from \name{} and non-collaborative LLM finetuing. (a) Non-collaborative LLM training relies single-turn rewards on immediate responses, which exhibits passive behaviors that follow the user's requests, leading to user frustration, less efficient process, and less satisfiable results. (b) \name{} incorporates Multiturn-aware Rewards from collaborative simulation, enabling forward-looking strategies. This results in more high-performing, efficient, and interactive conversations that anticipate future needs, propose timely clarification, and provide insightful suggestions.}
%     \label{fig:examples}
% \end{figure*}

\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%  WHAT IS THE GENERAL Setup  %%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Large language models (LLMs) are increasingly being adopted in real-world settings to facilitate collaborative problem-solving between humans and AI~\citep{rethinking_conv_agent, neural_approach, clarify_survey}. 
% In these collaborations, an LLM and a human interact for multiple turns to achieve specific goals together and complete a wide range of tasks, such as information seeking~\citep{inscit, clamber, cima,pacific}, code writing~\citep{benchmark_code_generate, codegen}, and document editing~\citep{interactive_text_gen}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%     WHAT IS THE PROBLEM    %%%%%%%%%%
%%%%%%%%%%  (GO SLOW FROM BEGINNING)  %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
However, there is often a disconnect between the collaborative use of LLMs in real-world scenarios and the non-collaborative environments in which they are typically trained. For example, standard Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf} typically rewards LLMs based on their immediate, single-turn responses, encouraging overly broad, ill-informed, and verbose outputs that may be advantageous in non-collaborative environments. Nevertheless, in real-world collaborations, such responses  (\eg Figure~\ref{fig:examples}a) often require  frequent user corrections and repeated generation attempts, which leads to user frustration, hinders effective task completion, and reduces conversational efficiency~\cite{}. 
Moreover, LLMs trained with single-turn rewards hardly take initiative to guide users in uncovering their ultimate intents, particularly when users are uncertain about or unable to fully articulate their needs, making them less effective collaborators in open-ended and exploratory tasks~\cite{understand_user_experience}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% PREVIOUS MOTIVATION %%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% However, there is often a disconnect between how these LLMs are being used in practical scenarios and how they are trained and evaluated.
% LLMs are typically trained and evaluated in non-interactive environments without long-term collaboration with users, 
% limiting their 
% ability to dynamically learn from user interactions.
% Moreover, current LLMs trained with standard Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf} tend to prioritize short-term rewards by optimizing for immediate, single-turn responses. As a result, these mcontributionodels exhibit reactive behavior, focusing on generating the next reply rather than fostering an engaging multiturn collaboration---which is essential for handling open-ended and complex tasks. 
% For example, our analysis shows that current LLMs often avoid asking clarification questions, instead often favoring ill-informed or vague responses.
% % This lack of interaction leads to inefficient problem-solving and fails to fully meet the real-world needs.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%     WHY IS IT HARD /     %%%%%%%%%%%%
%%%%%%%%%%  WHY DOES RECENT WORK FAIL  %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \xhdr{Limitations of previous works}  
Although recent research explored prompting and reinforcement learning (RL) to improve LLMs' collaborative problem-solving abilities, these approaches remain limited in scope. Prompting techniques \citep{ask_more_informative_questions, clarify_when_necessary, clarifygpt, rephrase_and_respond} typically rely on predefined instructions, making them rigid and less adaptable to evolving conversations. 
Similarly, some RL methods~\citep{baize, clarify_question_for_retrieval, learn_to_clarify, star_gate, self_correct} have been tailored to specific strategies (\eg clarification questions), hardly generalizing to broader collaboration strategies. 
% Another line of RL approaches aim to maximize trajectory-level rewards~\citep{multiturn_rlhf, archer, refuel}. 
See Appendix~\ref{app:related} for detailed discussion about related works.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%   WHAT IS OUR APPROACH &  %%%%%%%%%%%
%%%%%%%%%% HOW DOES IT AOLVE THE PROBLE %%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \xhdr{Our work: Unified collaborative training} 
To address these limitations, we introduce {\bf \name{}}, a novel and general training framework that enhances LLMs' ability to effectively collaborate with humans across diverse scenarios~\citep{rethinking_conv_agent, neural_approach, clarify_survey}. 
As shown in Figure~\ref{fig:overview},
the key insight is to promote LLMs' forward-looking behavior that anticipates long-term collaboration gains. 
In Figure~\ref{fig:overview}, we introduce a collaborative simulation module that simulates future conversations with users to estimate the long-term impact of model responses across multiple turns, a measure we term the \textit{Multiturn-aware Reward (MR)}. The MR function evaluates responses by incorporating both extrinsic metrics, such as task-specific success, and intrinsic metrics, such as efficiency, to holistically assess collaboration quality (\cf Section \ref{sec:method}).
By fine-tuning with prevailing RL algorithms~\citep{dpo, ppo} on MRs, \name{} promotes responses that lead to better task completion and efficiency in later conversation stages.
In Figure~\ref{fig:examples}b, the fine-tuned model goes beyond merely responding to user requests; it actively collaborates by asking follow-up questions about the writing tone, generating targeted content about the role of optimism, and offering insightful suggestions such as adding anecdotes. This results in an enhanced user experience, more satisfying documents, and highly efficient processes compared to non-collaboratively trained models.
% Here, \name{} is plug-and-play with existing RL algorithms such as Direct Preference Optimization (DPO)\citep{dpo} and Proximal Policy Optimization (PPO)\citep{ppo}.
% by incorporbased on theating collaborative simulation during training. 
% Specifically, we propose a \textit{Multiturn-aware Reward} function that promotes responses that lead to higher expected accuracies and more effective interactions the later stages of the conversation. .
% More generally, our \objects can also identify unclear information to prevent future misunderstandings, anticipate user needs to reach the ultimate goal more efficiently, and offer alternative directions to avoid stagnant conversations. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%  HOW WELL DOES IT PERFORM  %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \xhdr{Multiturn benchmarks} 
Due to a lack of automatic evaluation of LLMs in multiturn conversations, we contribute \textbf{three challenging multiturn tasks} for training and evaluation: \doct, \codet, and \mathct, which encompass document creation, code generation, and multiturn question answering. On the testing datasets, our approach improves task accuracy metrics by \taskimprov and interactivity by \itrimprov based on LLM judge scores on average, compared to the best baselines.
% Furthermore, we find that \name{} increases task efficiency by \efficiencyimprov{}\xspace, which significantly reduces human effort. 


% \xhdr{Real-world evaluation} 
Moreover, we perform a \textbf{large-scale and real-world user study} with \numturker{} Amazon Mechanical Turkers (MTurkers), who are asked to create writing contents with an anonymous AI assistant selected from non-collaboratively trained and our fine-tuned \name{}. \name{} achieves impressive improvement with \realsatisfyimprov increase in user satisfaction and reduces time usage per task by \realtimeimprov{}. The qualitative analysis from MTurkers confirms our observations: non-collaboratively-trained LLMs passively agree with users, while \name{} actively provide insightful questions and suggestions to enhance the writing process. 
Beyond the tasks that the \name{}s are fine-tuned on, we show \name{}s are highly generalizable to other data domains. 

\begin{comment}
To summarize our contributions:
%
\begin{itemize}
    \item We introduce a novel and plug-and-play training framework for improving LLM interactivity in real-world collaboration with humans, which can be easily integrated into RL training frameworks like DPO.
    
    \item We propose a multiturn-aware reward mechanism that encourages forward-looking responses, enabling LLMs to guide conversations toward effective, long-term collaboration.
    
    \item We demonstrate the effectiveness of our approach on three multiturn tasks, achieving significant improvements in task-specific metrics, interactivity, and efficiency over tche baselines. Moreover, extensive ablation and case studies provide insights to understand the behaviors of \objectst.
\end{itemize}
\end{comment}


% For example, if a user seeks help with a math problem but omits a key detail, such as whether the equation is linear or quadratic, the LLM could ask, “Could you clarify whether the equation is linear or quadratic? This will help me provide the correct solution.” In anticipating needs, if a user inquires about the weather in a particular city, the LLM might recognize that the user could be planning a trip and proactively offer assistance with booking local hotels. When editing documents, the LLM might detect that a document section is overly technical for a general audience and suggest, “This section appears quite technical — would you prefer simplifying it for broader readability, or keeping it for specialists?” 

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{figures/overview_combined3}
%     \vspace{-15pt}
%     \caption{Comparison between (a) Non-interactive LLM training and (b) Interactive LLM training (Ours). The traditional LLM training overlooks the interactive collaboration between human and LLM which results in ineffective and inefficient conversation. While the \object asks follow-up questions and provides more targeted predictions, which results in effective collaboration.}
%     \label{fig:overview}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/previous_pipeline2.pdf}
%         \vspace{-10pt}
%         \caption*{(a) Non-interactive LLM training}
%         \label{fig:previous}
%     \end{minipage}%
%     \begin{minipage}{0.55\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/ours_pipeline.pdf}
%         \vspace{-10pt}
%         \caption*{(b) Interactive LLM training}
%         \label{fig:ours}
%     \end{minipage}
%     \vspace{-5pt}
%     \caption{Comparison between non-interactive and interactive LLM training pipelines. }
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \begin{minipage}{0.48\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/bad_conv.pdf}
%         \vspace{-20pt}
%         \caption*{(a) Non-interactive LLM }
%         \label{fig:previous}
%     \end{minipage}%
%     \begin{minipage}{0.52\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{figures/good_conv.pdf}
%         \vspace{-20pt}
%         \caption*{(b) Interactive LLM}
%         \label{fig:ours}
%     \end{minipage}
% \end{figure}
