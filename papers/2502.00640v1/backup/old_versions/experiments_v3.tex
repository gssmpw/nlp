\section{Experimental Setup$^1$}
%\section{Experiments}

\begin{comment}
We conducted extensive and large-scale experiments to answer the following research questions:
\begin{itemize}%[leftmargin=*]
    \item How \textit{effective} are the \objects in collaborating with users? (Sec.~\ref{sec:quantitative})
    %\vspace{-3pt}
    \item How do \objects outperform? What component is \textit{essential} to their success? 
    %\vspace{-3pt}
    (Sec.~\ref{sec:ablation})
    \item What are the \textit{insights} from the generation results from interactive-trained LLMs? (Sec.~\ref{sec:case})
    %\vspace{-3pt}
    \item Can the capabilities of interactive-trained LLMs \textit{generalize} to untrained data domains? (Sec.~\ref{sec:extended})
\end{itemize}
\end{comment}

%\subsection{Evaluation Setup: Multiturn Interactive Conversation for Goal Achievement}

% Associate the asterisk with the desired footnote text


% Due to the lack of multiturn LLM evaluation benchmark, 
For fine-tuning, we create three multiturn datasets using publicly available data across diverse domains~\citep{math, bigcodebench, medium}: collaborative document editing, coding problem assistance, and multiturn mathematics problem solving.

To build a multiturn environment (Figure~\ref{fig:evaluation}), we employ GPT-4o-mini as a user simulator LLM to role-play realistic user behaviors, given the target problem and conversation history. Our simulation-based evaluations are designed to closely mimic real-world interactions~\cite{simulate1000}. 
% \james{Explicitly explain what LLM is used for simulator here and add more details of how the simulator is prompted in the Appendix.} 
Unlike traditional single-turn tasks, it requires dynamic interactions over multiple turns to achieving a goal. 
% these datasets simulate real-world scenarios where
% This is supported by the findings of \citet{simulate1000}, which demonstrate that LLMs/LLM-based agents can effectively emulate the responses and behaviors of participants. 
% It is important to note that, while we sample problems to inform the user simulator with realistic contexts, the evaluation is conducted at the conversational level, assessing the overall interaction's success rather than individual problem context. 
We describe the evaluation datasets as follows: 
% (\cf Appendix~\ref{app:dataset} for details):

%\footnote{\url{https://huggingface.co/datasets/fabiochiu/medium-articles}}
\noindent \textbf{\doct}: Document editing requires iterative feedback and refinements across multiple turns to ensure coherence and alignment with user intent. We sample 100 Medium articles as goal documents which are summarized into target problems to guide the user simulator. After interactions, the task performance is evaluated using the \textbf{BLEU} score, measuring similarity between the extracted document and the original articles.

\noindent \textbf{\codet}: Coding tasks inherently require multiturn interactions, such as clarifying requirements and debugging. We sample 600 coding problems from BigCodeBench~\citep{bigcodebench} as the target problems given to the user simulator. We compute the average \textbf{Pass Rate (PR)} of code at the end of the interactions.

\noindent \textbf{\mathct}: Math problem-solving often requires addressing implicit assumptions, verifying intermediate steps, and clarifying reasoning. We sample 200 level-5 math problems from MATH~\citep{math} to prompt the user simulator, which interacts with the LLMs. Task success is measured by the \textbf{accuracy (ACC)} of the final solution, as evaluated by an LLM judge.

% %
% \begin{itemize}%[leftmargin=*]
%     \item \textbf{\doct}: Document editing tasks involve iterative feedback and incremental refinements across multiple turns to address diverse user inputs, ensure coherence, and achieve a polished final document, closely mirroring real-world collaborative writing processes. We sample 100 Medium articles\footnote{\url{https://huggingface.co/datasets/fabiochiu/medium-articles}} as goal documents to guide the user simulator. The user simulator and LLM collaborate to revise and improve the document toward the goal. The final aggregated document is extracted using an extractor LLM that consolidates updates from each turn. The document quality is evaluated using \textbf{{BLEU}} score, which measures similarity between the generated document and the target document.
    
%     \item \textbf{\codet}: Coding tasks inherently require step-by-step interactions, such as clarifying requirements and debugging, making them well-suited to multiturn conversational settings. We sample 600 coding problems from BigCodeBench~\citep{bigcodebench} to define the user simulator's context. The evaluation is conducted at the conversational level, measuring the \textbf{{Pass Rate (PR)}} of the final code submissions produced after the interaction.
%     \item \textbf{\mathct}: Solving math problems often involves addressing implicit assumptions, verifying intermediate steps, or clarifying reasoning, making it a relevant use case for multiturn interactions where iterative feedback ensures accuracy and alignment with the problem-solving process. We sample 200 math problems from the level-5 questions in MATH~\citep{math} to prompt the user simulator which collaborates with LLMs. The task success is measured by the \textbf{{accuracy (ACC})} of the final solution provided at the end of each conversation, as evaluated by an LLM-based judge.
% \end{itemize}

Besides task-specific metrics, we incorporate two task-agnostic metrics across all datasets: \textbf{1) Average Token Count}, which quantifies the average number of tokens generated by the LLM per conversation, reflecting interaction efficiency. \textbf{2) Interactivity (ITR)}, which evaluates engagement levels using an LLM judge, with scores rescaled to the range [0, 1]. 


% Besides task-specific metrics, we include two task-agnostic metrics for all datasets: (1) \textbf{{Average Token Count}:} Quantifies the average number of tokens generated by the LLM assistant per conversation, which reflects the efficiency of the interaction. (2) \textbf{{Interactivity (ITR)}}, which measures the degree of engagement during the conversation using an LLM judge. We rescale the LLM judge's scores to a range of [0, 1]; and 

\input{tables/main_results}
\input{latexfig/ablation}

\xhdr{Fine-tuning \name{}s}  \name{}s are based on \llama{}~\citep{metallama} with LoRA finetuning~\citep{lora}. We provide four model variants: \textbf{1) Offline models}: SFT and Offline DPO are fine-tuned on pre-generated multiturn conversational datasets guided by Multiturn-aware Rewards (MR) (\cf Section~\ref{sec:optimization}). \textbf{2) Online models}: PPO and Online DPO are further trained upon the SFT and Offline DPO models, respectively. The model during online fine-tuning is involved in the collaborative simulation to compute MRs, which, in turn, dynamically adjust the model preference. 



% Our \objects include several variants: (1) SFT and Offline DPO models that are initialized with the \llama base model, and are further finetuned on pre-generated multiturn conversational datasets (Table~\ref{tab:stats}) guided by our multiturn-aware reward (as discussed in Section~\ref{sec:optimization}), and (2) PPO and Online DPO, which further trained upon the SFT and Offline DPO models, respectively, by further training in an online setting to refine interaction strategies based on the model's responses during conversations. We conduct LoRA finetuning~\citep{lora} for all our training experiments (\cf Appendix~\ref{app:train} for details).

\xhdr{Baselines} We compare \name{}s against (1) the pretrained \llama (\textit{Base}), (2) the base model with proactive prompting (\textit{Proactive Base}), which encourages follow-up and clarification questions. 
% , and (3) the single-turn performance of the base model (\textit{Reference (idealized)}), where the model is given idealized, complete inputs and generates the solution directly without user interaction.

\footnotetext{$^1$Dataset and training details in Appendix~\ref{app:dataset_n_train}; all related prompts in Appendix~\ref{app:prompts}}

% \xhdr{Baseline and Reference Models} We evaluate our fine-tuned models against (1) the pretrained \llama~\citep{metallama} (\textit{Base}), \ie base model, and (2) the base model with proactive prompting (\textit{Proactive Base}), where prompts encourage the model to ask follow-up and clarification questions (\cf Appendix~\ref{app:proact}). Moreover, we include single-turn results from the base model (\ie \textit{Reference (idealized)}) as a reference, where the model is provided with idealized, complete inputs and generates the solution directly, disregarding any interaction with users. 

% \xhdr{Ours: Interactively-Trained Models} Our \objects include several variants: (1) SFT and Offline DPO models that are initialized with the \llama base model, and are further finetuned on pre-generated multiturn conversational datasets (Table~\ref{tab:stats}) guided by our multiturn-aware reward (as discussed in Section~\ref{sec:optimization}), and (2) PPO and Online DPO, which further trained upon the SFT and Offline DPO models, respectively, by further training in an online setting to refine interaction strategies based on the model's responses during conversations. We conduct LoRA finetuning~\citep{lora} for all our training experiments (\cf Appendix~\ref{app:train} for details).

\input{latexfig/case_study_example}
\input{latexfig/case_study_n_generalization}

\section{Results of Simulated Experiments}
%\subsection{Main Results on Multiturn Conversational Datasets}
\label{sec:quantitative}

We present the results in Table~\ref{tab:results} and discuss the takeaways:

% \xhdr{Takeaway 1: Multiturn settings are generally more challenging compared to single-turn settings.}
% For \mathct and \codet, LLMs often exhibit stronger domain knowledge than users, but their inability to ask clarifying questions or adapt to user needs leads to significant discrepancies in multiturn collaborations. For example, comparing the base model's performance in single-turn settings with idealized inputs, \ie Reference (idealized), to its performance in multiturn collaborative settings, \ie Base, reveals a sharp decline: on \mathct, accuracy drops from 19.5\% to 11.9\%, and on \codet, pass rate decreases from 14.8\% to 9.3\%.
% This highlights the complexity of multiturn tasks in the real world compared to simplified single-turn settings with idealized and complete inputs.

\xhdr{Prompting is helpful, but limited in terms of performance gains and flexibility}
Proactive Base improves base model performance by encouraging follow-up questions and clarifications. For example, it increases BLEU on \doc from 32.2\% to 35.0\% and reduces read tokens by 0.31k compared to the base model. However, these gains are modest and do not fully address the challenges of multiturn collaboration. We observe that prompting strategies remain rigid, relying on predefined instructions rather than adapting dynamically to user needs. For instance, it could ask clarification questions even when unnecessary, leading to redundant interactions that disrupt conversation flow.
% Proactive prompting (\ie Proactive Base) improves base model performance by encouraging follow-up questions and clarifications. For example, it increases BLEU on \doc from 32.2\% to 35.0\% and reduces read tokens by 0.31k compared to the base model. However, these gains are modest and do not fully address the challenges of multiturn collaboration. Prompting strategies remain rigid, relying on predefined instructions rather than adapting dynamically to user needs. For instance, it tend to ask unnecessary clarification questions a, leading to redundant exchanges and inefficiencies rather than genuinely adaptive collaboration.

\xhdr{\name{} boosts task performance, conversational efficiency, and engagement}
\name{} achieves \taskimprov superior task-specific performance, \efficiencyimprov more efficient conversations, and \itrimprov enhanced interactivity compared to the best baselines.
To highlight, \name{} engage in more meaningful collaborations with ITR shows substantial gains. For \doct, the Online DPO model increases ITR from 0.46 to 0.92. 
% We believe the enhanced interactivity contributes to improved task performance by enabling models to better understand user needs and align their responses with long-term goals.
Moreover, our framework significantly improves conversational efficiency by minimizing the amount of content users need to review to arrive at the final solution. For \mathct, Online DPO decreases token count per conversation by 1.03k compared to the base model.
% Overall, these results demonstrate the ability of multiturn-aware collaborative training to balance goal-oriented collaboration with concise and effective communication, ensuring superior performance across complex multiturn tasks.

\subsection{Ablations on Reward Mechanisms (Figure~\ref{fig:ablation})}
%\subsection{Ablation Study: How Do Interactively-Trained LLMs Outperform?}
\label{sec:ablation}

To investigate the components contributing to the superior model performance, we conduct an ablation study focusing on the reward mechanisms used during fine-tuning. 
% Specifically, we compare reward mechanisms that vary in their ability to account for future conversational context by adjusting the forward sampling window size $w$ from 0 to 3. Notably, $w=0$ represents a special case where no forward sampling is performed, and the reward is based solely on the immediate response.
We evaluate the following reward mechanisms:
\begin{itemize}%[leftmargin=*]
    \item \textbf{Multiturn-aware Reward}: We compare reward mechanisms by varying the forward sampling window size $w=1,2,3$ to assess their ability to capture long-term conversational effects through simulated collaborations.
     \item \textbf{Immediate Rewards} evaluate the model's immediate response based on:
        \textit{1) Helpfulness}: Assessed by an LLM judge;
        \textit{2) Extrinsic Reward}: Focuses on task-specific metrics like BLEU while ignoring intrinsic factors such as efficiency;
        \textit{3) Extrinsic + Intrinsic Reward}: Combines task-specific metrics with efficiency and interactivity measures. This can be seen as a special case of the multiturn-aware reward function with $w=0$.
    
        % 1) \textit{Helpfulness}, which assesses response quality from an LLM judge;  % (prompt available in Appendix~\ref{app:system_prompts})
        % 2) \textit{Extrinsic Reward}, which focuses on task-specific metrics like BLEU but ignores intrisic properties like efficiency; and 
        % 3) \textit{Extrinsic + Intrinsic Reward}, which combines task-specific metrics with efficiency and interactivity measures. It can be viewed as a special case when applying MR to immediate responses. in our multiturn-aware reward function with $w=0$.
    
\end{itemize}

\input{latexfig/user_study}

% We use the \llama base model and apply online DPO fine-tuning with the specified reward mechanisms. 

We present results in Figure~\ref{fig:ablation}.
%To highlight, 
Interestingly,
%; for example, MR with $w=3$ achieves the best interactivity and efficiency. 
% the improvements from MRs highlight the importance of modeling long-term conversation effects. 
expanding the forward sampling window $w$ within the range generally enhances performance and efficiency by better capturing future interactions. Notably, MR with $w=2$ balances the gains and additional costs to conduct forward sampling, making it well-suited for large-scale fine-tuning. In contrast, immediate rewards, even with extrinsic and intrinsic components, fall short as they ignore long-term impact. 
These findings validate the positive impact of the forward sampling strategy in MRs.

% todo
% \TODO{Shirley}{Write summary of the qualitative analysis, after both qualitative and quantitative analyzes were moved to the appendix.}

% \subsection{Analysis}
\subsection{Case Study (Figure~\ref{fig:coding} \& ~\ref{fig:reward_preference})}
\label{sec:case}
We discuss a case study to illustrate the insights of \name{} in Figures~\ref{fig:coding}.
The user request on tokenizing a text file is inherently open-ended due to unspecified factors such as the NLTK environment, tokenizer selection, and optional preprocessing steps. The base LLM assumes defaults, applying lowercase conversion and stopword removal without user confirmation. The user simulator later corrects these assumptions, but the final solution remains incorrect due to missing stopwords.
In contrast, \name{} actively clarifies user intent by requesting confirmation on key decisions, ensuring a fully aligned final solution with a 100\% Pass Rate. This approach also reduces user effort, as reflected in lower token usage.

Further, in Figure \ref{fig:reward_preference}, we compare different reward mechanisms for responses A and B. Helpfulness rewards favor response A due to its seemingly more well-round output. Extrinsic rewards assign zero scores to both, as A provides an incorrect solution and B defers answering. Extrinsic + Intrinsic rewards slightly favor B for efficiency and engagement. Interestingly, MR assigns significantly higher rewards to B, especially at $w=2$ and $w=3$, since the response obtains useful information and provide a precise answer within the future interaction window.


\begin{table*}[t]
\centering
\begin{tabularx}{\textwidth}{|c|X|X|}
    \hline
    \footnotesize \textbf{Model} & \footnotesize \textbf{Strength} & \footnotesize \textbf{Weakness} \\
    \hline
    Base & \textit{``Follows great instruction and does exactly what I'm asking it to do.'', ``It can create a nice form of an outline to work with.''} & \textit{``The AI just agreed with me on pretty much everything. There was no discussion'', ``I didn't really like that it kept coming up with different options''} \\
    \hline
    Proactive Base & \textit{``It is very organized and it actually asks you for feedback after writing the revision!''} & \textit{``The AI seemed to be very redundant and asked me the same questions over and over.''} \\
    \hline
    \name{} & \textit{``Asking questions and making you think of things you never thought of'', ``The AI really helped me with focusing on one part of the story at a time.'', ``It helped really well to navigate what to say and what information is needed''} & \textit{``The AI assistant was not up to date enough to help with this recent sporting event.  The AI assistant also asked me to repeat information I had already given it.''} \\
    \hline
\end{tabularx}
\vspace{-10pt}
\caption{Representative Feedback from Human Participants.}
\vspace{-10pt}
\label{tab:user_study}
\end{table*}

\subsection{Model Generalization (Table~\ref{tab:abg_coqa})}
\label{sec:generalization}
Modern foundation models are expected to generalize across a diverse range of tasks beyond their training domain. A key question is whether collaborative behaviors learned by \name{} during fine-tuning transfer effectively to new tasks without additional adaptation. 

We assess \name{}, trained with online DPO on \code (the coding assistance task), on Abg-CoQA~\cite{abg_coqa}, a question-answering (QA) benchmark where questions are labeled as ambiguous or non-ambiguous.
We categorize the model’s responses into two actions—asking a clarifying question or providing a direct answer—and evaluate action-level accuracy within each question type. 
In Table~\ref{tab:abg_coqa}, \name{} exhibits a distinct response pattern compared to GPT-4o and \llama{}, which rarely ask clarifying questions regardless of ambiguity. In contrast, \name{} proactively asks questions about 50\% of the time while maintaining high accuracy on non-ambiguous inputs.

This behavior leads to the highest Macro Accuracy across both ambiguous and non-ambiguous sets and improves Macro F1 over the base model, while leaving room for further improvement against GPT-4o. These results suggest that \textbf{\name{} effectively generalizes its learned collaborative strategies beyond its training domain}.

\section{Real-world User Study }
\xhdr{Setup$^2$} We conduct a large-scale user study on Amazon Mechanical Turk with \numturker{} participants. Each participant is assigned a document type — randomly selected from blog post, creative writing, or personal statement — and chooses a topic from a predefined set. To simulate real-world scenarios where users have only a rough idea of the task, they first jot down brief responses to topic-related questions.
Participants then engage in at least eight turns of conversation with an anonymized AI assistant, randomly sampled from Base, Proactive Base, or \name{}. Every three turns, they provide an interaction rating based on their experience so far. After the conversation, they rate the final document quality, overall interaction. All ratings are in a scale from 1 to 10. We also record the total chat duration to assess efficiency.

\xhdr{Quantitative Results (Figure~\ref{fig:user_study})} Across multiple metrics, \name{} consistently outperforms the baselines. It achieves an average document quality score of 8.50. Specifically, 91.4\% of participants rate \name{}'s \textbf{document quality} as ``good'' (score 8–9), and 56.9\% as ``very good'' (score 9–10), compared to 88.5\% and 39.3\% for Base (\llama{}), respectively. Similarly, 63.8\% of participants find \name{} \textbf{highly engaging}, while only 42.6\% report the same for \llama{}. 

Interestingly, For \textbf{multiturn interaction}, the Base model shows a declining trend in ratings from turns 6–9, indicating reduced user experience in longer conversations. In contrast, both \name{} and Proactive Base exhibit increasing ratings over time, with \name{} consistently achieving higher average ratings every three turns compared to Proactive Base. This suggests that \name{} maintains sustained engagement more effectively.  

Moreover, \name{} improves task efficiency, reducing \textbf{time spent} by \realtimeimprov{} compared to the Base model and by 15.6\% relative to Proactive Base. While Proactive Base is prompted to maintain conciseness, it frequently asks unnecessary questions, inadvertently lowering efficiency. In contrast, \name{} strikes a more streamlined user experience.

% \input{tables/user_study}

\xhdr{Qualitative Results (Table~\ref{tab:user_study})} We collected a total of 180 strengths and 180 weaknesses across the three models. Table~\ref{tab:user_study} presents the most representative feedback, while we discuss the individual strengths and weaknesses.

% \begin{ex}{}
The base model generates coherent content while effectively follow user instructions, but it sometimes struggles with maintaining context in long texts, and can be overly verbose or repetitive in its responses. 
Proactive Base excels in responsiveness and adapting to user input but struggles with memory retention, and could produce repetitive or overly structured content.
On the other hand, \name{} is highly engaging, effectively guiding users through writing, adapting seamlessly to feedback. However, users also point out that \name{} can occasionally feel bland, lack of up to date information, and require additional effort to personalize the output. 
Overall, \name{} enhances collaboration by guiding users through an interactive and iterative refinement process, yet future improvements should focus on increasing personalization, creativity, and real-time knowledge integration to further optimize human-LLM collaboration.
    



\footnotetext{Detailed user study setup in Appendix~\ref{app:user_study}.}



