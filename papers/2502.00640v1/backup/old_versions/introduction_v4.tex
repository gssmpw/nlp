
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%  WHAT IS THE PROBLEM   %%%%%%%%%%%%%
% IN A VERY SPECIFIC WAY ABOUT REAL-WORLD USAGE %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Modern Large Language Models (LLMs) excel at generating high-quality single-turn responses when given well-specified inputs. 
%% MG: What about adding ref to Taylor here?
%However, real-world users may not often fully articulate or be certain about their exact requests. 
However, real-world users often do not fully articulate their intents and sometimes 
%unsure about their exact requests. 
initiate conversations with an imprecise understanding of their own needs~\cite{taylor:1968}.
%% MG: Comment this out? I think we can justify paragraphs 1+2 without examples:
%% MG: see my comments in chat why I commented. I commented this out for now, but we can bring this back later (ideally elsewhere once we have a better plan)
%For example, in Figure~\ref{fig:examples}a, when requesting an article on how optimism enhances well-being, an LLM may generate a generic response. Yet, the user’s intent may be under-specified---factors such as tone, target audience, and argumentative stance may not have been known, leading to a response misaligned with the user’s actual expectations.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% WHY IS THE PROBLEM IMPORTANT   %%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% MG: This claim about LLMs' weakness might be too strong without more context. I think the intro flows well without.
%Such behavior, where LLMs passively generate overly %broad, ill-informed, and verbose responses by making %assumptions when user intents are under-specified,can 
As a result,
users routinely refine their requests post hoc through iterative corrections, which can 
increase frustration, hinder effective task completion, and reduce conversational efficiency~\cite{guidelines, johnny,understand_user_experience}. Therefore, an open problem is to train models that actively guide users in clarifying and refining their intents, and helps them achieve their goals.
%% MG: Moving this later -> state that AI tasks become increasingly complex
%particularly for open-ended and exploratory tasks. Addressing this 
This key challenge would improve user satisfaction and efficiency
and streamline human-LLM interactions---especially as LLMs are being applied to real-world tasks that are increasingly complex and open-ended.
%% MG: moved "real-world" and "user-agent" above, and the rest might be too specific to information-seeking dialog:
%, and reducing the effort needed for users 
%to obtain precise information 
%in real-world human-LLM collaborations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% WHY DOES THE PREVIOUS METHOD FAIL   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
However,
a key observation is that established fine-tuning techniques such as Reinforcement Learning from Human Feedback (RLHF)~\citep{rlhf} primarily reward 
LLMs for immediate, single-turn responses, reducing the models' incentive to seek clarification or assist users in refining their intents. As a result, LLMs tend to prioritize direct answers even when additional context could improve task completion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% HOW DO WE ADDRESS IT   %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To address these challenges, we introduce {\bf \name{}}, a novel and general training framework that enhances LLMs' ability to effectively collaborate with humans in multiturn scenarios~\citep{neural_approach, rethinking_conv_agent, clarify_survey}. 
As shown in Figure~\ref{fig:overview},
the key innovation of \name{} is to promote LLMs' forward-looking behavior that %anticipates 
leads to long-term collaboration gains. 
%% MG: FIX unusual expression: What about "A key contribution is a ..." 
To highlight, 
%%
we introduce a collaborative simulation module that samples future conversations with users to estimate the long-term impact of model responses across multiple turns, a measure we term the \textit{Multiturn-aware Reward (MR)}. The MR function evaluates responses by incorporating both extrinsic metrics, such as task-specific success, and intrinsic metrics, such as efficiency, to holistically assess collaboration quality (\cf Section \ref{sec:method}).
By fine-tuning with established RL algorithms~\citep{dpo, ppo} on MRs, \name{} promotes responses that lead to better task completion and efficiency in later conversation stages.
As shown in Figure~\ref{fig:examples}b, the fine-tuned model goes beyond simply responding to user requests---it actively collaborates by asking follow-up questions about the writing tone, generating targeted content about the role of optimism, and offering insightful suggestions such as adding anecdotes. This results in an enhanced user experience, more satisfying documents, and highly efficient processes compared to non-collaboratively trained models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%  HOW WELL DOES IT PERFORM  %%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \xhdr{Multiturn benchmarks} 
% due to a lack of LLM benchmarks for multiturn conversations, 
We also introduce \textbf{three challenging multiturn tasks} for training and evaluation in simulated environments: \doct, \codet, and \mathct, which respectively encompass document creation, code generation, and multiturn question answering. On the test sets, our approach improves task accuracy metrics by \taskimprov and interactivity by \itrimprov based on LLM judge scores on average, compared to the best baselines.
% Furthermore, we find that \name{} increases task efficiency by \efficiencyimprov{}\xspace, which significantly reduces human effort. 


% \xhdr{Real-world evaluation} 
Moreover, we perform a \textbf{large-scale and real-world user study} with \numturker{} Amazon Mechanical Turkers (MTurkers), who are asked to create writing contents with an anonymous AI assistant selected from non-collaboratively trained and our fine-tuned \name{}. \name{} achieves impressive improvement with \realsatisfyimprov increase in user satisfaction and reduces user time per task by \realtimeimprov{}. The qualitative analysis from MTurkers confirms our observations: non-collaboratively-trained LLMs passively agree with users, while \name{} actively provide insightful questions and suggestions to enhance the writing process. 
Beyond the tasks that the \name{}s are fine-tuned on, we show \name{}s are highly generalizable to other data domains. 


% --------------

% \jure{Jure Intro}

% Present day LLMs take a user prompt as an input and generate the answer/response based on that prompt. But in practice, when users interact with LLMs, they are prone to frequent user corrections and repeated generation attempts. For example, consider a user writing a prompt to write a paragraph about how optimism improves human well-being. The LLM will immediately generate such a paragraph, but the task specified by the user might not be fully defined. The user forgot to think and specify what tone they want, who is the target audience, what kind of position they want to take, how long the paragraph should be. \jure{Give an example}.

% Such behavior, where LLM immediately acts and fills-in the blanks leads to user frustration, hinders effective task completion, and reduces conversational efficiency~\cite{}. \jure{Are there citations here? any evidence? can you cite HCI research}

% Thus it is an important open problem \jure{Let's invent a name for this problem} of how to design LLMs that don't just jump to the conclusion and immediate generate an (incomplete, wrong) answer, but rather seen further information (ask clarification questions) so that the answer they generate is what the user actually wants/needs. For example, many times the user's prompt might be incomplete because the user has simply not thought about all the dimensions and corner cases of the task they are asking the LLM to solve. The user might actually prefer to be asked clarification questions as these prompt the user to think more deeply about the task they want the LLM to solve.
% Providing a solution to the above problem is important because it would improve user satisfaction and interaction with the LLM. It would also shorten the time and amount of effort needed by the user to receive the answer they want. \jure{expand}

% Here we address the above problem and 

% Our key observation is that today's LLMs are trained with single-turn rewards hardly take initiative to guide users in uncovering their ultimate intents, particularly when users are uncertain about or unable to fully articulate their needs, making them less effective collaborators in open-ended and exploratory tasks~\cite{understand_user_experience}.


