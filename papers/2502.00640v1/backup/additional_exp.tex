
Studying model responses provides insights into the expected properties and behaviors of the model. We provide an example (Figure~\ref{fig:coding}) where a user requests coding assistance to tokenize a text file. Many aspects of the problem, such as the Python environment the user is using and the preferred tokenization method, remain unspecified. However, the non-interactive LLMs proceed with assumptions, such as lowercasing the text and removing stopwords. In contrast, the \object trained with our framework asks clarifying questions about the user's preferences, including the use of the NLTK tokenizer, exception handling, and data preprocessing steps. This approach delivers a solution that aligns precisely with the userâ€™s needs and passes the unit tests.

Beyond differences in model responses, applying different reward mechanisms to each response (Figure~\ref{fig:reward_preference}) reveals the preferences of various reward functions. For instance, the helpfulness reward function assigns a significantly higher reward to Response A than Response B, favoring a detailed solution over follow-up questions. However, since our multiturn-aware reward function considers the impact on future turns, the reward evaluation adjusts, assigning a higher score to Response B than A. This adjustment reflects the positive impact Response B has on the overall conversation by saving the user effort and time otherwise spent on incorrect solutions and misunderstandings. Moreover, when the window size is set to 2 or 3, it provides a better estimation compared to a window size of 1 by capturing longer-term impacts of a response that may not be recognized in shorter or single-turn interactions. This observation is consistent with the ablation study in Section~\ref{sec:ablation}.

Additional examples are provided in Appendix~\ref{app:complex} due to space limitations. These examples demonstrate the \object's capabilities in various scenarios, such as ensuring users understand basic concepts before introducing complex solutions, adapting to cultural preferences when providing wedding advice, and asking users to specify their confusion to deliver more targeted explanations for coding problems.


\subsection{Extended Study: Do \Objects Generalize to Other Tasks?}
\label{sec:extended}
It is essential for LLMs to generalize their post-training capabilities to new inputs. While our \objects demonstrate capabilities far beyond asking clarification questions, we aim to test whether this ability generalizes to existing clarification benchmarks in entirely different domains.

Specifically, we evaluate our online DPO model trained on \codet, the code assistance task, on \ambcoqa~\cite{abg_coqa}, a clarification benchmark that assesses whether LLMs can ask clarification questions for ambiguous queries and provide direct answers for non-ambiguous ones. Each question in \ambcoqa is derived from a story provided in the context. We chose \codet because of the distinct nature of coding tasks compared to the reading comprehension tasks in \ambcoqat.

Unlike previous works~\citep{abg_coqa, learn_to_clarify}, we present the questions without few-shot prompts or any indication that they might be ambiguous (see question format and an example in Appendix~\ref{app:abg_coqa}). This approach represents a more realistic setting and poses a greater challenge for models to make open-ended decisions.

In Table~\ref{tab:abg_coqa}, we observe that the behavior of the non-interactive \llama model and the \texttt{GPT-4o} (\texttt{gpt-4o-2024-08-06}) model is highly similar: only about 16\% of ambiguous questions are followed up with clarifications. This finding indicates that some widely used LLMs often make false assumptions and rarely identify ambiguity. Interestingly, our \object improves accuracy on ambiguous questions to around 53\%, representing a 37\% improvement. However, the \object occasionally asks unnecessary questions, resulting in an 18\% decrease in accuracy for non-ambiguous questions compared to the non-interactive model. Nonetheless, in critical cases where important decisions are involved, double-checking the question through additional clarification may be less harmful than missing ambiguity and making unsafe assumptions.
