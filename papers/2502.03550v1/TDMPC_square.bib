@techreport{PtolemyVol1:04,
   Author = {Brooks, C. and Lee, E. A. and Liu, X. and Neuendorffer, S. and Zhao, Y. and Zheng, H.},
   Title = {Heterogeneous Concurrent Modeling and Design in Java},
   Institution = {University of California},
   Number = {Technical Memorandum UCB/ERL M04/27},
   Month= {July 29},
   URL ={http://ptolemy.eecs.berkeley.edu/publications/papers/04/ptIIDesignIntro/},
   Year = {2004}
}

% Model-based RL and MPC
@inproceedings{williams2017MPPI,
  title={Information theoretic mpc for model-based reinforcement learning},
  author={Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M and Boots, Byron and Theodorou, Evangelos A},
  booktitle={2017 IEEE international conference on robotics and automation (ICRA)},
  pages={1714--1721},
  year={2017},
  organization={IEEE}
}
@article{PETS,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{PlaNet,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle={International conference on machine learning},
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}
@inproceedings{bhardwaj2020information,
  title={Information theoretic model predictive q-learning},
  author={Bhardwaj, Mohak and Handa, Ankur and Fox, Dieter and Boots, Byron},
  booktitle={Learning for Dynamics and Control},
  pages={840--850},
  year={2020},
  organization={PMLR}
}
@article{hansen2022TDMPC,
  title={Temporal difference learning for model predictive control},
  author={Hansen, Nicklas and Wang, Xiaolong and Su, Hao},
  journal={arXiv preprint arXiv:2203.04955},
  year={2022}
}
@article{hansen2023tdmpc2,
  title={Td-mpc2: Scalable, robust world models for continuous control},
  author={Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
  journal={arXiv preprint arXiv:2310.16828},
  year={2023}
}
@article{MBPO,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{LOOP,
  title={Learning off-policy with online planning},
  author={Sikchi, Harshit and Zhou, Wenxuan and Held, David},
  booktitle={Conference on Robot Learning},
  pages={1622--1633},
  year={2022},
  organization={PMLR}
}
@article{hansen2024hierarchical,
  title={Hierarchical World Models as Visual Whole-Body Humanoid Controllers},
  author={Hansen, Nicklas and SV, Jyothir and Sobal, Vlad and LeCun, Yann and Wang, Xiaolong and Su, Hao},
  journal={arXiv preprint arXiv:2405.18418},
  year={2024}
}
@article{deramerv2,
  title={Mastering atari with discrete world models},
  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  journal={arXiv preprint arXiv:2010.02193},
  year={2020}
}
@article{dreamerv3,
  title   = {Mastering Diverse Domains through World Models},
  author  = {Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2301.04104}
}
@article{moerland2023model,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={16},
  number={1},
  pages={1--118},
  year={2023},
  publisher={Now Publishers, Inc.}
}
@article{polydoros2017survey,
  title={Survey of model-based reinforcement learning: Applications on robotics},
  author={Polydoros, Athanasios S and Nalpantidis, Lazaros},
  journal={Journal of Intelligent \& Robotic Systems},
  volume={86},
  number={2},
  pages={153--173},
  year={2017},
  publisher={Springer}
}
@article{wang2019benchmarking,
  title={Benchmarking model-based reinforcement learning},
  author={Wang, Tingwu and Bao, Xuchan and Clavera, Ignasi and Hoang, Jerrick and Wen, Yeming and Langlois, Eric and Zhang, Shunshi and Zhang, Guodong and Abbeel, Pieter and Ba, Jimmy},
  journal={arXiv preprint arXiv:1907.02057},
  year={2019}
}
@article{sutton1991dyna,
  title={Dyna, an integrated architecture for learning, planning, and reacting},
  author={Sutton, Richard S},
  journal={ACM Sigart Bulletin},
  volume={2},
  number={4},
  pages={160--163},
  year={1991},
  publisher={ACM New York, NY, USA}
}
@article{katsigiannis2017dreamer,
  title={DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices},
  author={Katsigiannis, Stamos and Ramzan, Naeem},
  journal={IEEE journal of biomedical and health informatics},
  volume={22},
  number={1},
  pages={98--107},
  year={2017},
  publisher={IEEE}
}
@article{lowrey2018POLO,
  title={Plan online, learn offline: Efficient learning and exploration via model-based control},
  author={Lowrey, Kendall and Rajeswaran, Aravind and Kakade, Sham and Todorov, Emanuel and Mordatch, Igor},
  journal={arXiv preprint arXiv:1811.01848},
  year={2018}
}
@article{kabzan2019learning,
  title={Learning-based model predictive control for autonomous racing},
  author={Kabzan, Juraj and Hewing, Lukas and Liniger, Alexander and Zeilinger, Melanie N},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={4},
  pages={3363--3370},
  year={2019},
  publisher={IEEE}
}
@article{DMPC,
  title={Diffusion model predictive control},
  author={Zhou, Guangyao and Swaminathan, Sivaramakrishnan and Raju, Rajkumar Vasudeva and Guntupalli, J Swaroop and Lehrach, Wolfgang and Ortiz, Joseph and Dedieu, Antoine and L{\'a}zaro-Gredilla, Miguel and Murphy, Kevin},
  journal={arXiv preprint arXiv:2410.05364},
  year={2024}
}
@article{li2025robotic,
  title={Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics},
  author={Li, Chenhao and Krause, Andreas and Hutter, Marco},
  journal={arXiv preprint arXiv:2501.10100},
  year={2025}
}


% RL general
@misc{littman1996reinforcement,
  title={Reinforcement learning: A survey, journal of artificial intelligence research 4},
  author={Littman, M and Moore, A},
  year={1996},
  publisher={syf}
}
@inproceedings{thrun2014issues,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the 1993 connectionist models summer school},
  pages={255--263},
  year={2014},
  organization={Psychology Press}
}
@article{singh1994upper,
  title={An upper bound on the loss from approximate optimal-value functions},
  author={Singh, Satinder P and Yee, Richard C},
  journal={Machine Learning},
  volume={16},
  pages={227--233},
  year={1994},
  publisher={Springer}
}
@article{bertsekas1996neuro,
  title={Neuro-dynamic programming},
  author={Bertsekas, DP},
  journal={Athena Scientific},
  year={1996}
}
@inproceedings{munos2003error,
  title={Error bounds for approximate policy iteration},
  author={Munos, R{\'e}mi},
  booktitle={ICML},
  volume={3},
  pages={560--567},
  year={2003},
  organization={Citeseer}
}
@article{munos2007performance,
  title={Performance bounds in l\_p-norm for approximate value iteration},
  author={Munos, R{\'e}mi},
  journal={SIAM journal on control and optimization},
  volume={46},
  number={2},
  pages={541--561},
  year={2007},
  publisher={SIAM}
}
@article{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@article{agarwal2019RLtheory,
  title={Reinforcement learning: Theory and algorithms},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  journal={CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep},
  volume={32},
  pages={96},
  year={2019}
}
@inproceedings{TD3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@inproceedings{SAC,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
@article{chan2022greedification,
  title={Greedification operators for policy optimization: Investigating forward and reverse kl divergences},
  author={Chan, Alan and Silva, Hugo and Lim, Sungsu and Kozuno, Tadashi and Mahmood, A Rupam and White, Martha},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={253},
  pages={1--79},
  year={2022}
}
@article{BEAR,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{BCQ,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International conference on machine learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}
@article{AWR,
  title={Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}
@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}
@inproceedings{bc-sac,
  title={Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios},
  author={Lu, Yiren and Fu, Justin and Tucker, George and Pan, Xinlei and Bronstein, Eli and Roelofs, Rebecca and Sapp, Benjamin and White, Brandyn and Faust, Aleksandra and Whiteson, Shimon and others},
  booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={7553--7560},
  year={2023},
  organization={IEEE}
}
@article{CQL,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}
@article{cal-QL,
  title={Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning},
  author={Nakamoto, Mitsuhiko and Zhai, Simon and Singh, Anikait and Sobol Mark, Max and Ma, Yi and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{IQL,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}
@article{hansen2023idql,
  title={Idql: Implicit q-learning as an actor-critic method with diffusion policies},
  author={Hansen-Estruch, Philippe and Kostrikov, Ilya and Janner, Michael and Kuba, Jakub Grudzien and Levine, Sergey},
  journal={arXiv preprint arXiv:2304.10573},
  year={2023}
}
@article{td3bc,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}
@article{XQL,
  title={Extreme q-learning: Maxent rl without entropy},
  author={Garg, Divyansh and Hejna, Joey and Geist, Matthieu and Ermon, Stefano},
  journal={arXiv preprint arXiv:2301.02328},
  year={2023}
}
@article{fuguided,
  title={Guided Decoupled Exploration for Offline Reinforcement Learning Fine-tuning},
  author={Fu, Yuwei and Wu, Di and Boulet, Benoit}
}
@article{OOO,
  title={Offline RL for Online RL: Decoupled Policy Learning for Mitigating Exploration Bias},
  author={Mark, Max Sobol and Sharma, Archit and Tajwar, Fahim and Rafailov, Rafael and Levine, Sergey and Finn, Chelsea}
}
@article{zhou2024diffusion,
  title={Diffusion Model Predictive Control},
  author={Zhou, Guangyao and Swaminathan, Sivaramakrishnan and Raju, Rajkumar Vasudeva and Guntupalli, J Swaroop and Lehrach, Wolfgang and Ortiz, Joseph and Dedieu, Antoine and L{\'a}zaro-Gredilla, Miguel and Murphy, Kevin},
  journal={arXiv preprint arXiv:2410.05364},
  year={2024}
}


% Environments
@article{tassa2018deepmind,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}
@article{sferrazza2024humanoidbench,
  title={Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation},
  author={Sferrazza, Carmelo and Huang, Dun-Ming and Lin, Xingyu and Lee, Youngwoon and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2403.10506},
  year={2024}
}

% Diffusion
@article{pan2024MBD,
  title={Model-based diffusion for trajectory optimization},
  author={Pan, Chaoyi and Yi, Zeji and Shi, Guanya and Qu, Guannan},
  journal={arXiv preprint arXiv:2407.01573},
  year={2024}
}
@article{ho2020DDPM,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}
@article{akhound2024iDEM,
  title={Iterated denoising energy matching for sampling from Boltzmann densities},
  author={Akhound-Sadegh, Tara and Rector-Brooks, Jarrid and Bose, Avishek Joey and Mittal, Sarthak and Lemos, Pablo and Liu, Cheng-Hao and Sendera, Marcin and Ravanbakhsh, Siamak and Gidel, Gauthier and Bengio, Yoshua and others},
  journal={arXiv preprint arXiv:2402.06121},
  year={2024}
}

% Humanoid
@article{moro2019whole,
  title={Whole-body control of humanoid robots},
  author={Moro, Federico L and Sentis, Luis},
  journal={Humanoid robotics: a reference},
  pages={1161--1183},
  year={2019},
  publisher={Springer Dordrecht}
}

@article{MBOP,
  title={Model-based offline planning},
  author={Argenson, Arthur and Dulac-Arnold, Gabriel},
  journal={arXiv preprint arXiv:2008.05556},
  year={2020}
}

@article{TRPO,
  title={Trust Region Policy Optimization},
  author={Schulman, John},
  journal={arXiv preprint arXiv:1502.05477},
  year={2015}
}

@article{BERMEN,
  title={Deployment-efficient reinforcement learning via model-based offline optimization},
  author={Matsushima, Tatsuya and Furuta, Hiroki and Matsuo, Yutaka and Nachum, Ofir and Gu, Shixiang},
  journal={arXiv preprint arXiv:2006.03647},
  year={2020}
}
