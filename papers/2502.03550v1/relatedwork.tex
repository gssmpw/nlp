\section{Related Work}
\label{sec:Related Work}
\textbf{Model-based RL.} 
The core of model-based reinforcement learning is how to leverage the world model to recover a performant policy. Dyna-Q~\cite{sutton1991dyna} first introduced the idea of using simulated rollouts from a learned model to augment real-world experience for policy optimization. MBPO~\cite{MBPO} further provides a theoretical guarantee of monotonic policy improvement and promotes short model-generated rollouts. Dreamer \cite{katsigiannis2017dreamer, deramerv2, dreamerv3} optimizes policies entirely in imagination, leveraging latent world models for high-dimensional tasks like visual control. These methods are computationally efficient as they decouple model rollouts from online decisions, but they can suffer from model errors over long horizons.

Planning-based approaches use the world model for online decision-making by optimizing actions directly through simulated trajectories. PlaNet \cite{PlaNet} employs a latent dynamics model with trajectory optimization in latent space, while PETS \cite{PETS} utilizes an ensemble of probabilistic models and the Cross-Entropy Method (CEM) for sampling-based optimization. These methods are highly adaptive to online changes and precise for short-horizon tasks but face challenges in scaling to tasks with high-dimensional states or action spaces due to the computational cost of rollouts during execution.

\textbf{Temporal-Difference Model Predictive Control.}
Recent advances aim to balance scalability and adaptability by integrating strengths from both paradigms. \cite{bhardwaj2020information, LOOP, hansen2022TDMPC, DMPC} adopt a temporal-difference (TD) learning framework that combines with model predictive control, illustrating how a value-based learning signal can mitigate the need for hand-crafted cost functions and long-horizon planning. Building upon this idea, TD-MPC2~\cite{hansen2023tdmpc2} is able to learn scalable, robust world models tailored for continuous control tasks, effectively reducing compounding modeling errors and improving planning stability. These advances highlight how embedding temporal-difference learning within the MPC paradigm can significantly enhance control strategies' flexibility, sample efficiency, and robustness in high-dimensional continuous domains.

\textbf{Off-policy Learning with Policy Constraint}
Distributional mismatch is a long-standing challenge in off-policy learning. Standard off-policy algorithms are highly sensitive to distributional shifts, as bootstrapping errors can compound over time, leading to instability and poor generalization~\cite{BEAR}. Recent studies in offline RL have taken a huge leap in enabling policy learning from off-policy demonstrations. To enforce distributional constraints, \cite{BEAR, td3bc} incorporate policy regularization, while \cite{BCQ, AWR} mitigate OOD queries through importance sampling. Alternatively, \cite{IQL, XQL} adopt in-sample learning techniques to implicitly recover a policy from observed data, bypassing direct constraints on action selection.
The off-policy issue is also critical for planning-based MBRL that leverages a policy or value prior.
\cite{LOOP} introduced an approach that marries off-policy learning with online planning by actor regularization control, introducing conservatism into the planner. In comparison, our method addresses such constraints on the policy prior without compromising the planner. \cite{MBOP} achieves a similar planning process by learning a behavior cloning (BC) policy and corresponding value function. However, due to its pure offline nature, all the components can be considered to originate from the distribution of the behavior policy.

\iffalse
\begin{description}
  \item[$\bullet$]  Beginning, if it can be short yet detailed enough, or if it's critical to take a strong defensive stance about previous work right away. In this case Related Work can be either a subsection at the end of the Introduction, or its own Section 2.
  \item[$\bullet$]  End, if it can be summarized quickly early on (in the Introduction or Preliminaries), or if sufficient comparisons require the technical content of the paper. In this case Related Work should appear just before the Conclusions, possibly in a more general section ``Discussion and Related Work".
\end{description}
\fi
%===============================================================================