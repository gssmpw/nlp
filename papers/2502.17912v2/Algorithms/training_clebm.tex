\begin{algorithm}[!t]
\caption{Training algorithm of \shortname.}
\label{algo:training_clebm}
\begin{algorithmic}[1]

\REQUIRE graph $\mathcal{G} = (\mathbf{X}, \A)$, learning rate $\eta$, classification loss weight $\xi$, MCMC step size $\lambda$, MCMC noise variance $\sigma^2$, MCMC steps $K$, MCMC samples $M$, epoch number $E$. 
\ENSURE optimized models $\{ g_{\hat{\enc}}, D_{\hat{\disc}}, f_{\hat{\dec}}, I_{\hat{\cls}} \}$. 
\STATE Initialize weights $\{ \enc_0, \disc_0, \dec_0, \cls_0 \}$; 
\STATE Initialize MCMC replay buffer $\mathcal{B} \leftarrow \varnothing$; 

\FOR{$e \leftarrow 1$ {\bfseries to} $E$}

    \STATE Get node embeddings $\{\rvh_i\}_{i=1}^{N} \leftarrow \{ g_{\enc_{e-1}}(\rvx_i, \mathcal{N}(\rvx_i)) \}_{i=1}^{N}$; 

    \STATE
    \STATE \texttt{\textcolor{purple}{\# MCMC sampling}}
    \FOR{$\text{sample node } j = 1$ {\bfseries to} $M$}
        \STATE $\Tilde{\rvh}_j^{(0)} \sim \mathcal{B}$ with $95\%$ probability and $\mathcal{U}$ otherwise; 
        \FOR{$\text{sample step } k = 1$ {\bfseries to} $K$}
            \STATE $\Tilde{\rvh}_j^{(k)} = \Tilde{\rvh}_j^{(k-1)} - \lambda \nabla_{\Tilde{\rvh}}f_{\dec_{e-1}}(\Tilde{\rvh}_j^{(k-1)}) + \bm{\epsilon}^{(k)}$, $\bm{\epsilon}^{(k)} \sim \mathcal{N}(0, \sigma^2)$; 
        \ENDFOR
        \STATE $\Tilde{\rvh}_j \leftarrow \texttt{sg}[\Tilde{\rvh}_j^{(K)}]$; 
        \STATE $\mathcal{B} \leftarrow \mathcal{B} \cup \{\Tilde{\rvh}_j\}$; 
    \ENDFOR
    \STATE Sampled embeddings $\{\Tilde{\rvh}_j\}_{j=1}^{M}$; 

    \STATE
    \STATE \texttt{\textcolor{purple}{\# compute losses}}
    \STATE Compute $\loss_{\text{ebm}}$, $\loss_{\text{cl}}$, and $\loss_{\text{cls}}$ via recurrent update (\cref{sec:recur_update}); 
    % \STATE Get prediction probability $\{h_{\cls}(\rvh_i)\}_{i=1}^{N}$ and compute cross-entropy loss $\loss_{\text{cls}}$;  

    \STATE
    \STATE \texttt{\textcolor{purple}{\# update models}}
    \STATE $\enc_{e} \leftarrow \enc_{e-1} - \eta\nabla_{\enc}(\loss_{\text{cl}} +  \xi \loss_{\text{cls}})$; 
    \STATE $\disc_{e} \leftarrow \disc_{e-1} - \eta\nabla_{\disc}\loss_\text{cl}$; 
    \STATE $\dec_{e} \leftarrow \dec_{e-1} - \eta\nabla_{\dec}\loss_\text{ebm}$; 
    \STATE $\cls_{e} \leftarrow \cls_{e-1} - \eta\nabla_{\cls}(\xi \loss_\text{cls})$; 

\ENDFOR
\STATE $\hat{\enc} \leftarrow \enc_E$, $\hat{\disc} \leftarrow \disc_E$, $\hat{\dec} \leftarrow \dec_E$, $\hat{\cls} \leftarrow \cls_E$. 

\end{algorithmic}
\end{algorithm}