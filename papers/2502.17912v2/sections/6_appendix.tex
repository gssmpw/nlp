\clearpage
\appendix
\label{sec:appedix}

\section{Potential Broader Impact}
\label{app:broader_impact}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\section{Limitations}
\label{sec:limitation}
\shortname has achieved excellent performance, but the results are nearly maxed out on some datasets. We need better and more challenging benchmarks to evaluate performance. Additionally, \shortname shares the same drawback as EBMs trained with MLE: training requires MCMC sampling. However, it still shows superior performance. Besides, the cost of MCMC sampling can be reduced by cooperative learning~\citep{xie2018cooperative,coopvaebm,ecvae}.


\section{Additional Related Works}
\spara{Graph Contrastive Learning}
Contrastive learning (CL) stands as a widely applied self-supervised learning method, aiming to derive informative sample representation solely from feature information. 
The main idea of CL is to align the representations of similar samples in close proximity while driving apart the representations of dissimilar samples.
Witnessing the remarkable advancement of Graph Neural Networks (GNNs), a substantial amount of recent research has focused on Graph Contrastive Learning (GCL)~\citep{dgi,gmi,mvgrl,grace,sugrl}. 
DGI~\citep{dgi} learns by maximizing mutual information between node representations and corresponding high-level summaries of graphs. 
GRACE~\citep{grace} maximizes the agreement of corresponding node representations in two augmented views for a graph. 
SUGRL~\citep{sugrl} explores the complementary information from structural and neighbor information to maximize inter-class variation and minimize intra-class variation through triplet losses and an upper bound loss, while removing the need for data augmentation and discriminators. 
Our work builds upon the foundation of GCL, where we employ DGI as part of learning the graph encoder that extract graph topology information. We also establish the relationship between DGI and EBM: DGI can be understood as an EBM trained by noise contrastive estimation. To further enhance the capabilities of the proposed method, we design recurrent update to let the energy head and DGI promote each other in training (see \cref{sec:our_design}), and experiments show that this to be greatly beneficial for OOD detection (see \cref{sec:ablation}). 

\spara{Node Anomaly Detection (NAD)} NAD is a binary classification task, that directly categorizes nodes into two different categories: normal and anomalous. Some works~\citep{dongSpaceGNN,dong2024rayleigh,dong2025smoothgnn,zhao2021usingclassificationdatasetsevaluate,Gong2023BeyondHR} have been proposed to handle this problem. In contrast, node OOD detection requires balancing the ability to classify in-distribution nodes and detect out-of-distribution nodes.


\section{Deriviations}
\label{sec:deriviations}

\subsection{Training EBMs}
\label{sec:grad_ebm_nll}

Given a Boltzmann distribution $p_{\energy}(\rvx) = \frac{\exp (-E_{\energy}(\rvx))}{Z_{\energy}}$, 
its negative log-likelihood (NLL) $\loss_{\energy}$ is:
\begin{equation*}
\begin{aligned}
    \loss_{\energy} = -\E_{\rvx \sim p_d(\rvx)} \Big[ \log p_{\energy}(\rvx) \Big].
\end{aligned}
\end{equation*}
To minimize $\loss_{\energy}$, we first need to calculate the gradient of $\log p_{\energy}(\rvx)$ w.r.t. $\energy$:
\begin{equation*}
\begin{aligned}
    \nabla_\energy \log p_\energy(\rvx)
    &= \nabla_\energy \log \frac{\exp{(-E_\energy(\rvx))}} {Z_\energy} \\
    &= -\nabla_\energy E_\energy(\rvx) - \nabla_\energy \log Z_\energy \\
    &= -\nabla_\energy E_\energy(\rvx) - \nabla_\energy \log \sum_{\rvx} \exp{(-E_\energy(\rvx))} \\
    &= -\nabla_\energy E_\energy(\rvx) - \frac{1}{\sum_{\rvx'}\exp{(-E_\energy(\rvx'))}} \sum_{\rvx} \nabla_\energy \exp{(-E_\energy(\rvx))} \\
    &= -\nabla_\energy E_\energy(\rvx) + \sum_{\rvx} \frac{\exp{(-E_\energy(\rvx))}}{\sum_{\rvx'}\exp{(-E_\energy(\rvx'))}} \nabla_\energy E_\energy(\rvx) \\
    &= -\nabla_\energy E_\energy(\rvx) + \sum_{\rvx} p_{\energy}(\rvx) \nabla_\energy E_\energy(\rvx) \\
    &= -\nabla_\energy E_\energy(\rvx) + \E_{\Tilde{\rvx} \sim p_{\energy}(\Tilde{\rvx})} \Big[ \nabla_\energy E_\energy(\Tilde{\rvx}) \Big].
\end{aligned}
\end{equation*}
So, we have
\begin{equation*}
\begin{aligned}
    \nabla_\energy \loss_\energy = - \nabla_\energy \E_{\rvx\sim p_d(\rvx)} \Big[ \log p_\energy(\rvx) \Big] 
    = \E_{\rvx \sim p_d(\rvx)} \Big[ \nabla_\energy E_\energy(\rvx) \Big] - \E_{\Tilde{\rvx} \sim p_{\energy}(\Tilde{\rvx})} \Big[ \nabla_\energy E_\energy(\Tilde{\rvx}) \Big], 
\end{aligned}
\end{equation*}
Therefore, the EBM loss can be reformulated as:
\begin{equation*}
\begin{aligned}
        \loss_\energy
    = \E_{\rvx \sim p_d(\rvx)} \Big[ E_\energy(\rvx) \Big] - \E_{\Tilde{\rvx} \sim p_{\energy}(\Tilde{\rvx})} \Big[ E_\energy(\Tilde{\rvx}) \Big]
    \approx \frac{1}{N}\sum_{i=1}^N \energyfn(\rvx_i) - \frac{1}{M}\sum_{j=1}^M \energyfn(\Tilde{\rvx}_j).
\end{aligned}
\end{equation*}

Additionally, minimizing $\loss_{\energy}$ is equivalent to minimizing the KL divergence $\mathrm{KL} \Big( p_d(\rvx) \| p_{\energy}(\rvx) \Big)$:
\begin{equation*}
\begin{aligned}
    \nabla_{\energy} \mathrm{KL}\Big( p_d(\rvx) \| p_{\energy}(\rvx) \Big) 
    &= \nabla_{\energy} \sum_{\rvx} p_d(\rvx) \log \frac{p_d(\rvx)}{p_{\energy}(\rvx)} \\
    &= \nabla_{\energy} \sum_{\rvx} p_d(\rvx) \log p_d(\rvx) - \nabla_{\energy} \sum_{\rvx} p_d(\rvx) \log p_{\energy}(\rvx) \\
    &= \nabla_{\energy} \E_{\rvx\sim p_d(\rvx)}\Big[ \log p_d(\rvx) \Big] - \nabla_{\energy} \E_{\rvx\sim p_d(\rvx)}\Big[ \log p_{\energy}(\rvx) \Big] \\
    &= \bm{0} + \nabla_{\energy} \loss_{\energy} = \nabla_{\energy} \loss_{\energy}.
\end{aligned}
\end{equation*}


\subsection{Regularization for EBM Loss}
Additionally, we follow~\citep{du2019implicit} to adopt the $L_2$ regularization for energy magnitudes of both true data and sampled data when computing $\loss_{\text{ebm}}$ during training, 
as otherwise while the difference between true data and sampled data was preserved, the actual values would fluctuate to numerically unstable values.
For an energy function $E_{\energy}(\cdot)$, true data $\rvx_i\sim p_d(\rvx)$, and sampled data $\Tilde{\rvx}_j\sim p_{\energy}(\Tilde{\rvx})$:
\begin{equation*}
\begin{aligned}
    \loss_{\text{ebm}} = \frac{1}{N}\sum_{i=1}^{N} E_{\energy}(\rvx_i) - \frac{1}{M}\sum_{j=1}^{M} E_{\energy}(\Tilde{\rvx}_j) + c \left[ \frac{1}{N}\sum_{i=1}^{N} E_{\energy}(\rvx_i)^2 + \frac{1}{M}\sum_{j=1}^{M} E_{\energy}(\Tilde{\rvx}_j)^2 \right], 
\end{aligned}
\end{equation*}
where $c$ is a coefficient, we set $c = 1$ here. 

\subsection{Sampling from EBM}
\label{sec:sgld}
Given a Boltzmann distribution $p_{\energy}(\rvx) = \frac{\exp (-E_{\energy}(\rvx))}{Z_{\energy}}$, the gradient of $\log p_\energy(\rvx)$ w.r.t. $\rvx$ is:
\begin{equation*}
\begin{aligned}
    \nabla_\rvx \log p_\energy(\rvx)
    &= \nabla_\rvx \log \frac{\exp{(-E_\energy(\rvx))}}{Z_\energy} \\
    &= -\nabla_\rvx E_\energy(\rvx) - \nabla_\rvx \log Z_\energy \\
    &= -\nabla_\rvx E_\energy(\rvx), 
\end{aligned}
\end{equation*}
so we can first initialize a sample using uniform distribution $\rvx^{(0)} \sim \mathcal{U}$, 
and utilize multi steps stochastic gradient to make the sample follow distribution $p_\energy (\rvx)$:
\begin{equation*}
\begin{aligned}
    \rvx^{(k)} 
    &= \rvx^{(k-1)}+\lambda \nabla_\rvx \log p_\energy(\rvx^{(k-1)}) + \bm{\epsilon}^{(k)} \\
    &= \rvx^{(k-1)}-\lambda \nabla_\rvx E_\energy(\rvx^{(k-1)}) + \bm{\epsilon}^{(k)}, 
\end{aligned}
\end{equation*}
where $\bm{\epsilon}^{(k)} \sim \mathcal{N}(0, \sigma^2)$. After $K$ steps, $\Tilde{x}^{(K)}$ is the output of $K$-step MCMC sampling.

\input{Algorithms/training_clebm}

\section{Implementation Details}


\subsection{Training Algorithm}
\label{sec:training_clebm}
The detailed training algorithm is shown in \cref{algo:training_clebm}. 
We first obtain node embeddings (Line 4), and then sample nodes by a $K$-step MCMC sampling (Line 7-15). 
Then the recurrent update is conducted for computing losses $\loss_{\text{ebm}}$, $\loss_{\text{cl}}$, and $\loss_{\text{cls}}$ (Line 18). 
Finally, update the parameters (Line 21-24).

Furthermore, we follow~\citep{du2019implicit} to use sample \textbf{relay buffer}. 
The initialization of MCMC chain plays a crucial role in mixing time, but langevin dynamics does not place restrictions on sample initialization given sufficient sampling steps. 
Thus we use a sample replay buffer $\mathcal{B}$ in which we preserve previously generated samples and use either these samples or uniform noise for initialization. 



\subsection{Dataset and Splits}
\label{app:dataset_and_split}

For \texttt{Twitch} that has several subgraphs, we use \texttt{DE} as ID dataset and the other five subgraphs as OOD datasets. The subgraph \texttt{ENGB} is used as OOD exposure dataset for training the OOD Expo baselines. These subgraphs are different in size, edge densities, and degree distribution, thus can be regarded as samples from different distribution~\citep{wu2021handling}. 

For \texttt{Arxiv} that is in a single graph, we divide the nodes into three parts: the papers published before 2015 are used as ID data, those after 2017 are used as OOD data, and those between are used as OOD exposure data for training the OOD Expo baselines. 

For \texttt{Cora}, \texttt{Amazon}, \texttt{Chameleon}, \texttt{Actor}, and \texttt{Cornell} that have no clear domain information, we synthesize OOD data in three ways~\citep{wu2022energy}. 
Structure manipulation (S): use the original graph as ID data and adopt a stochastic block model to randomly generate a OOD graph. 
Feature interpolation (F): use random interpolation to create node features for OOD data and the original graph as ID data. 
Label leave-out (L): use nodes with partial classes as ID and leave out others for OOD.

We split the ID dataset as 10\%/10\%/80\% (train/valid/test), and use all the nodes in OOD dataset for evaluation. Additionally, an extra OOD dataset is also used in training for OOD exposure (\texttt{OE}, \texttt{Energy-FT}, and \texttt{GNNSafe++}).

\subsection{Search Space for Hyper-parameters}
\label{sec:search_space}
The detailed search space of hyper-parameters is shown in~\cref{tab:search_space}. We use Optuna~\citep{akiba2019optuna} to conduct random search for hyper-parameters.

\input{tables/search_space}


\section{Complexity Analysis}
Compared to other baselines, our additional computational complexity comes from the introduced GCL and MCMC sampling. 

Suppose that the GCN is used as the graph encoder in other baselines, with $m$ the edge numbers, $n$ the node numbers, $d_0$ the initial node dimension, $d_1$ the latent dimension of node representation, and $L$ the number of layers (propagation numbers). 
So other baselines have a complexity of $\mathcal{O}(L m d_0 d_1)$. 

In terms of our methods, the GCL processes both positive nodes and negative nodes, but the propagation for positive nodes is conducted in pre-process, with only the transformation remaining in training iterations. So the complexity for GCL is $\mathcal{O}(L n d_0 d_1 + L m d_0 d_1) \approx \mathcal{O}(L m d_0 d_1)$, since normally $m >> n$, where $n$ is the number of nodes. 
The MCMC sampling has a complexity of $\mathcal{O}((nd_1d_2+nd_2)K)=\mathcal{O}(nd_1d_2K)$, where $d_2$ is the hidden dimension of Energy Function ($2$-layer MLP), and $K$ is the number of steps of MCMC sampling. 

This actually suggests that our method has scalability comparable to GNNs, since the seemingly computationally intensive MCMC steps do not suffer from edges, with complexity only scaling linearly with the number of nodes.


\section{Time Consumption}
\label{sec:time_full}
The detailed time consumption of \shortname training is shown in~\cref{tab:time_full}. 
In the GCL algorithm, message-passing for positive samples is handled during preprocessing, and the augmentation step only involves shuffling the rows of the node feature matrix. This makes the entire GCL learning process highly efficient. 
Furthermore, MCMC sampling is conducted in the latent space, which is low-dimensional and topology-free, resulting in an acceptable time consumption. 
Overall, GCL learning and MCMC sampling account for only 6\% and 19\% of the total training time, respectively. 

\cref{tab:time_comparison} compares the training time consumption (s) of baseline models and our proposed \shortname.
Thanks to preprocessing node features and MCMC sampling in the latent space, \shortname requires only slightly more time during the training phase compared to other baselines. Specifically, training \shortname (32.18s) is just 7\% slower than GNNSafe (30.66s), while delivering significant performance improvements, particularly on heterophilic graphs.

\input{tables/time_full}
\input{tables/time_comparison}


\section{Evaluation on Synthetic Graphs}
We conduct additional experiments on varying the homophilic ratio using synthetic graphs~\citep{zhu2021graph}. 
We report the AUROC results on synthetic-Cora in~\cref{tab:homo_ratio}, and we use the Feature OOD setting. 

As the homophilic ratio decreases from 1.0 to 0.0, the performance of GNNSafe decreases significantly from 79.12 to 69.02. In contrast, \shortname consistently maintains a high performance across homophilic ratios, achieving around 99 in most cases. This indicates that GNNSafe struggles to handle heterophily, whereas our proposed method demonstrates strong performance on both homophilic and heterophilic graphs.

\input{tables/homo_ratio}



\section{Detailed Evaluation Results}
\label{sec:main_result_full}

The detailed results of OOD detection on homophilic and heterophilic graphs are shown in~\cref{tab:main_result_full_homo,tab:main_result_full_hetero}, respectively. 
\input{tables/main_results_full_homo}
\input{tables/main_results_full_hetero}

The detailed results of OOD detection on varying the available label ratio are shown in~\cref{tab:ablation_label_rate_full}.
\input{tables/ablation_label_rate_full}

The full results of the ablation study regarding the different components are shown in~\cref{tab:ablation_components_full}.
\input{tables/ablation_components_full}

The detailed results of OOD detection performance of different GCL algorithms are shown in~\cref{tab:gcl_algo_full}.
\input{tables/ablation_gcl_algorithms_full}


\clearpage
\thispagestyle{empty}