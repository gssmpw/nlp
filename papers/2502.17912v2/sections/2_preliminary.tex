\section{Preliminary}
\vspace{-3mm}
\spara{Notations}
We denote an undirected graph without self-loops as $\graph=\{ \mathbf{X}, \A \}$, 
where $\mathbf{X}=\{ \bm{x}_i \}_{i=1}^N\in\R^{N\times d_0}$ is the initial node feature matrix, $d_0$ is the feature dimension, 
and $\A\in\R^{N\times N}$ is the adjacency matrix. 
$\mathcal{N}(\rvx_i)$ is the feature set of neighbors of node $\rvx_i$. 
$\D$ is a diagonal matrix standing for the degree matrix such that $\D_{ii}=\sum_{j=1}^N\A_{ij}$. 
$\hid^{(\ell)}=\{\bm{h}_i^{(\ell)}\}_{i=1}^N \in \R^{N \times d}$ is the representation matrix in the $\ell$-th layer, where $d$ is the hidden dimension. 
We use $\mathbf{Y}=\{ \rvy_i \}_{i=1}^N\in\R^{N\times C}$ to denote the ground-truth node label matrix, where $C$ is the number of classes and $\bm{y}_i$ is a one-hot vector.


\spara{Graph Out-of-Distribution Node Detection}
OOD detection refers to identifying data samples that do not conform to the distribution of the training data, while keeping the classification capability of in-distribution (ID) data. Formally, an OOD detection score function $\mathcal{S}(\cdot,\cdot)$ should be defined to map the node and its neighbors to a scalar score, such that $\mathcal{S}(\rvx,\mathcal{N}(\rvx))$ yields a higher value for OOD nodes than for ID nodes. 
It can be seen that in the context of graph data, OOD detection becomes challenging due to the complex interplay between node features and graph topology. 
The OOD score for each node depends on itself and also the relational context provided by other nodes within the graph, which is distinct from OOD detection in the vision domain.

\spara{Heterophily Issue}
Most traditional GNNs are designed based on the homophily assumption~\citep{kipf2016classification,GAT}, where linked nodes tend to be similar. However, heterophilic graphs—where linked nodes often belong to different categories—are prevalent in real-world applications, on which the traditional GCNs perform poorly~\citep{pei2020geom}. 
While extensive works have been proposed to address the heterophily issue~\citep{abu2019mixhop,fagcn2021,zhu2020beyond,gprgnn,li2022finding,lsgnn,fgsam} in node classification tasks, it has not received sufficient attention in the context of node OOD detection, resulting in performance degradation when applying models designed for node OOD detection to heterophily graphs. 

\spara{Energy-based Model}
\label{sec:ebms}
A deep EBM models~\citep{du2019implicit} the data distribution $p_d(\rvx)$ using Boltzmann distribution $p_{\energy}(\rvx) = \frac{\exp{(-E_\energy(\rvx))}}{Z_\energy}$ with the energy function $E_{\energy}(\rvx)$, where $Z_\energy$ is the corresponding normalizing constant. 
EBM is trained by minimizing the negative log-likelihood~(NLL) $\loss_\energy$ of $p_{\energy}(\rvx)$, such that $\loss_\energy = -\E_{\rvx \sim p_d(\rvx)} \Big[ \log p_{\energy}(\rvx) \Big]$.
The EBM loss can be reformulated as follows:
\vspace{-10pt}
\begin{equation}
\label{eq:ebm_loss}
\begin{aligned}
    \loss_{\energy} = \E_{\rvx \sim p_d(\rvx)} \Big[ \energyfn(\rvx) \Big] - \E_{\Tilde{\rvx} \sim p_{\energy}(\Tilde{\rvx})} \Big[ \energyfn(\texttt{sg}[ \Tilde{\rvx} ]) \Big]
    \approx \frac{1}{N}\sum_{i=1}^N \energyfn(\rvx_i) - \frac{1}{M}\sum_{j=1}^M \energyfn( \texttt{sg}[\Tilde{\rvx}_j] ), 
\end{aligned}
\end{equation}
where $\texttt{sg}[\cdot]$ denotes the stop-gradient operation. 
Sampling $\Tilde{\rvx}$ from $p_\energy(\Tilde{\rvx})$ can be achieved by a $K$-step Markov chain Monte Carlo (MCMC) sampling:
\begin{equation*}
\begin{aligned}
    \Tilde{\rvx}^{(k)} 
    = \Tilde{\rvx}^{(k-1)} - \lambda \nabla_{\Tilde{\rvx}} E_\energy(\Tilde{\rvx}^{(k-1)}) + \bm{\epsilon}^{(k)},
\end{aligned}
\end{equation*}
where $\Tilde{\rvx}^{(0)}$ is given initial samples,  $\lambda$ is the step size, and $\bm{\epsilon}^{(k)} \sim \mathcal{N}(0, \sigma^2)$ with a given noise $\sigma^2$. 
More details of derivation can be found in the~\cref{sec:deriviations}.
