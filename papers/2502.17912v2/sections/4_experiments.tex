\vspace{-3mm}
\section{Experiments}
\vspace{-3mm}

    
\subsection{Setup}
\vspace{-3mm}
\spara{Dataset and Evaluation Metrics}
We evaluate \shortname on seven benchmark datasets for node classification tasks~\citep{yang2016revisiting,shchur2018pitfalls,rozemberczki2021multi,wang2020microsoft,pei2020geom}, including four \textit{homophily} datasets (\texttt{Cora}, \texttt{Amazon-Photo}, \texttt{Twitch}, and \texttt{ogbn-Arxiv}) and three \textit{heterophily} datasets (\texttt{Chameleon}, \texttt{Actor}, and \texttt{Cornell}). 
We mainly follow~\citep{wu2021handling,wu2023gnnsafe} to adopt two established methods to simulate OOD scenarios. In the multi-graph context, OOD samples stem from distinct graphs or subgraphs not linked to the training nodes. Conversely, in the single-graph setting, OOD samples are part of the same graph as the training data but remain unseen during training. For \texttt{Twitch}, we treat one subgraph as in-distribution (ID) and others as OOD, using one for OOD exposure during training. In \texttt{ogbn-Arxiv}, we split the nodes by publication year for ID, OOD, and OOD exposure sets. For \texttt{Cora}, \texttt{Amazon}, \texttt{Chameleon}, \texttt{Actor}, and \texttt{Cornell} that have no clear domain information, we synthesize OOD data in three ways: i) Structure manipulation (S); ii) Feature interpolation (F); iii) Label leave-out (L). See detailed settings and splits in \cref{app:dataset_and_split}.
For the assessment of OOD detection performance, we employ standard metrics: Area Under the Receiver Operating Characteristic curve (\texttt{AUC}), Area Under the Precision-Recall curve (\texttt{AUPR}), and the False Positive Rate at 95\% True Positive Rate (\texttt{FPR95}). In-distribution (ID) performance is quantified using the accuracy (\texttt{Acc}) metric on the testing nodes. In the following text, we use \texttt{AUC} to denote \texttt{AUC} with a little bit of abuse. Due to the space limits, we mainly present the results of \texttt{AUC} and \texttt{Acc}, the full results with \texttt{AUPR} and \texttt{FPR95} can be found in~\cref{sec:main_result_full}.


\spara{Baseline Comparisons}
Our model is benchmarked against two categories of baseline methods. The first category comprises models that handle i.i.d. inputs, which are predominantly used in computer vision. These include \texttt{MSP}~\citep{hendrycks2016baseline}, \texttt{ODIN}~\citep{liang2018enhancing}, \texttt{Mahalanobis}~\citep{lee2018simple}, \texttt{OE}~\citep{hendrycks2018deep}, and \texttt{Energy(-FT)}~\citep{liu2020energy}. We also include \texttt{ResidualFlow}~\citep{zisselman2020deep}, a density-based method capable of modeling data distribution. For a fair comparison, we substitute the CNN backbones in these models with a GCN encoder. The second category consists of methods tailored for graph-structured data, such as \texttt{GKDE}~\citep{zhao2020uncertainty}, \texttt{GPN}~\citep{stadler2021graph}, \texttt{OODGAT}~\citep{song2022learning} and \texttt{GNNSafe(++)}~\citep{wu2022energy}. 
It is noteworthy that \texttt{OE}, \texttt{Energy-FT} and \texttt{GNNSafe++} incorporate OOD samples during the training phase, i.e., trained with OOD exposure. 


\input{tables/main_results_homo}

\spara{Implemetation Details}
\label{sec:Implementation_details}
We implement our model by PyTorch and conduct experiments on 24GB RTX-3090ti. 
Epoch number $E=200$, MH layer number $L=5$, hidden dimension $d=512$, MCMC steps $K=20$.
We use Optuna~\citep{akiba2019optuna} to search hyper-parameters for our proposed model and baselines (see \cref{sec:search_space} for detailed search space).

\input{tables/main_results_hetero}

\vspace{-2mm}
\subsection{Evaluation Results}
\vspace{-2mm}
\label{sec:main_result}
We report the results of the proposed \shortname and competing baselines on different datasets in \cref{tab:small_res_homo,tab:small_res_hetero}. 
Our finding indicates that the \shortname consistently outperforms competing baselines without OOD exposure in terms of \texttt{AUC} and \texttt{Acc} across both homophilic and heterophilic datasets. 
Specifically, \shortname increases the average \texttt{AUC} by 4.26\% (resp. 20.63\%) on homophilic (resp. heterophilic) graphs and improves the average \texttt{Acc} for homophilic (resp. heterophilic) graphs by 2.37\% (resp. 13.95\%). 
On the contrary, although the baselines tailored for graph inputs surpass those designed for i.i.d. data on homophilic graphs, they show a lower performance on heterophilic graphs. 
Additionally, after adding the Energy Propagation technique to \texttt{Energy} (i.e., \texttt{GNNSafe}), the average \texttt{AUC} decreases significantly from 70.01\% to 55.23\% and 56.42\%, indicating that the Energy Propagation technique will cause severe performance decrease on heterophilic graphs. 

The high performance of \shortname is primarily attributed to the enhancement brought by the robust representation learned by the GCL algorithm and the classification loss, as well as the powerful data modeling capabilities of EBM trained via the MLE approach. 
Overall, these findings underscore the superiority of \shortname over baseline approaches, on both homophilic and heterophilic graphs.


\input{tables/ablation_label_rate}

\vspace{-2mm}
\subsection{Evaluation on Limited Labels}
\vspace{-2mm}
\label{sec:limited_labels}

\begin{wrapfigure}[10]{r}{0.46\textwidth}
\vspace{-8.5mm}
  \centering
  \includegraphics[width=0.95\linewidth]{Images/labelRatio_ablation_v3.png}
  \vspace{-12pt}
  \caption{Performance across labeled ratios.}
  \label{fig:label_ratio}
\end{wrapfigure}

Given the time-consuming and labor-intensive nature of obtaining node labels on graphs in the real world, an interesting evaluation task is to evaluate the OOD detection performance of a model with limited category labels. 
We follow the data splits used in the experiments of \cref{sec:main_result} as a foundation and progressively reduce the proportion of available labels in the training set: from 100\% to 50\%, and then to 10\%. 
To highlight the performance of our method, we select three baselines \texttt{OE}, \texttt{Energy-FT} and \texttt{GNNSafe++} which incorporate OOD samples during the training phase, i.e., trained with OOD exposure. 
Note that \textit{we do not reduce the number of nodes used for OOD exposure training}, but only decrease the number of category labels available within the distribution. 
We present the average results of each dataset in \cref{tab:ablation_label_rate}, and the average performance of each method in \cref{fig:label_ratio} for more intuitive comparison. We report the full results in~\cref{sec:main_result_full}. 
The results show that our proposed \shortname maintains high performance under different label proportions, with no significant decline; in contrast, all baselines exhibit a marked performance drop as the labels ratio decrease. 
In particular, when the available label rate is 100\%, our method still outperforms baselines, highlighting the superiority of our method.
These results demonstrate that our method, benefiting from its effective design, can consistently and effectively extract graph topology information and learn the data density well, thereby significantly outperforming previous state-of-the-art graph OOD detection methods in the more practical scenario of limited labels.


\vspace{-2mm}
\subsection{Ablation Study}
\vspace{-2mm}
\label{sec:ablation}
\input{tables/ablation_components}
\spara{The Impact of Proposed Techniques}
In what follows, we evaluate the impact of training the energy head via MLE (\texttt{MLE-Energy}) versus deriving node energy directly from classification logits (\texttt{Classify-Energy}).
Additionally, we assess the use of DGI algorithm during graph encoder training (\texttt{GCL}), and explore the effectiveness of the Energy-Propagation technique (\texttt{Eprop}), the Multi-Hop Graph Feature Encoder (\texttt{MH}), Conditional Energy (\texttt{CE}), and Energy Readout (\texttt{ERo}). 
We present the results in \cref{tab:ablation_components}. 
The baseline (Row 1) utilizes a GCN for node classification and obtains the node energy by the classification logits. 
Since \texttt{Eprop} is based on homophily assumption, the performance increases slightly (Avg +3.7\%) on homophilic graphs but shows a dramatic degradation (Avg -16\%) on heterophilic graphs (Row 2). 
Compared to the baseline (Avg 76.87\%), solely utilizing the MLE for training the energy head (Avg 72.24\%) or employing GCL algorithm for training the graph encoder (Avg 73.90\%) does not work. It is the combination of MLE and GCL algorithm that brings positive effects for OOD detection tasks (Avg 87.82\%). More detailed discussion can be found on \textbf{Observation 1--4} below. 
With \texttt{MH}, \shortname achieves a significant performance boost (Avg +11\%, Row 9), demonstrating that combining local and global information simultaneously enhances performance on both homophilic and heterophilic graphs. 
\texttt{CE} alone can improve the average performance (Avg +0.63\%, Row 10); however, using \texttt{ERo} on its own does not yield similar benefits (Avg -0.94\%, Row 11). 
This is because \texttt{CE} takes the global view into account, producing better node energy, which enables \texttt{ERo} to generate a more optimized readout summary. Therefore combining them together via \texttt{Recurrent Update} can bring more positive effects (Avg +1.33\%, Row 12).

\textbf{Observation 1: combining DGI with classification does not work.} 
As shown in \cref{tab:ablation_components}, although the \texttt{Classifier-Energy} variant shows 5.93\% improvement of AUC on heterophilic graphs after adding DGI loss (\texttt{GCL}) into the classification task (Row 3), its performance on homophilic graphs dramatically decrease from 77.22\% to 65.37\%. This indicates that the integration of DGI algorithm and \texttt{Classifier-Energy} has an overall negative impact on its performance.

\textbf{Observation 2: naively training Energy Head via MLE does not work.} 
As shown in \cref{tab:ablation_components}, when decoupling the model into two parts (a graph encoder and an energy head), if we train the energy head via MLE but only employ classification loss for training the graph encoder (Row 4), the model will show a worse performance compared to the \texttt{Classifier-Energy} variant (Row 1). 

\textbf{Observation 3: combining DGI and MLE works well} 
As shown in \cref{tab:ablation_components}, after adding DGI loss (\texttt{GCL}), the performance of \texttt{MLE-Energy} variant dramatically improves from 72.74\% to 87.82\% on average in terms of AUC, outperforming previous state-of-the-art method GNNSafe.

\textbf{Observation 4: combining other GCL and MLE not works well} 
We try to replace the DGI algorithm with other widely used GCL algorithms: GRACE~\citep{grace} and SUGRL~\citep{sugrl} (without \texttt{MH}, \texttt{CE}, and \texttt{ERo}). From~\cref{tab:gcl_algo}, DGI consistently outperforms its counterparties. 

From the above observation, we can conclude that the key reason behind the success of proposed \shortname is due to \textbf{the symbiosis between DGI and EBM}. We extensively evaluate the potential variants, however all variants show significantly worse performance. This also aligns with our theoretical analysis in \cref{sec:dgi_theory}, which said that the learning of DGI can be viewed as learning EBM via variational learning. The property enables us to use DGI for learning the graph encoder as part of the final constructed EBMs.




\input{tables/ablation_gcl_algorithms}
