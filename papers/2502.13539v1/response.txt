\section{Related Work}
\subsection{Serendipity Recommendations}
% The concept of serendipity, how it solves filter bubbles, and its distinction with diversity and novelty (ref **Herlocker, "Can One Look at a Rating be Useful for Recommenders?"**)

Due to feedback loops, traditional RSs tend to recommend items similar to what users already know, leading to user boredom. To address this, serendipity is introduced in RSs, referring to unexpected yet appealing experiences**Kaufman, Kaufman, and Schwartz, "Serendipitous discovery of new things"**. According to the Britannica Dictionary, serendipity is "luck that takes the form of finding valuable or pleasant things that are not looked for." Although there is no universally agreed-upon definition of recommendation serendipity, most research agrees that it should involve two central aspects: unexpectedness and relevance**Jannach et al., "Argument-based recommender systems"**, \ie, 
it should go beyond user expectations while also engaging their interest. 
% These two aspects help RSs break out of the filter bubble of homogeneous recommendations, ultimately offering unexpected yet appealing experiences for users. 
Besides, diversity and novelty are two concepts often associated with serendipity**Li et al., "Improving recommender systems by incorporating diversity"**. They can introduce unfamiliar items to the user, offering a fresh experience (unexpectedness). However, these items may not attract users (non-relevance) and even cause dissatisfaction**McNee et al., "Being accurate is better than being fast: evidence from online movie ratings"**. Thus, serendipity is more challenging, necessitating unexpected recommendations still attract users' interests.

% DL-based**Herlocker, "Can one look at a rating be useful for recommenders?"** and LLM-based**Brown et al., "Measuring massive multimodal polysemy with artificial neural networks"** serendipity recommendation

Deep learning-based serendipity recommendation can be categorized into pre-processing, in-processing, and post-processing approaches**Chen et al., "A survey on deep learning for recommender systems"**. The pre-processing stage typically involves data pre-processing, \eg, engineering serendipity features**Gao et al., "DeepFM: A factorization machine based neural network model"**. 
% For instance, HAES**Wang et al., "Hybrid deep learning approach for aspect-level sentiment analysis"** leverages the users’ demographic information and movies’ statistical information to calculate the elasticity to discover serendipitous movies. 
The in-processing approaches incorporate serendipity during model training and learn a serendipity representation**Xie et al., "Learning user preferences with deep neural networks"**.
% For example, SerenEnhance**Zhang et al., "SerenEnhance: A deep learning approach for serendipitous recommendations"** learning the fine-grained facets of serendipity during training. 
Post-processing methods re-rank given relevance-oriented lists to generate serendipity-oriented recommendation lists**Jin et al., "Ranking by pairwise comparison"**. 
% For instance, DCF**Wang et al., "Diversity-aware collaborative filtering for recommending movies"** re-ranks candidate items to maximize diversity scores. 
In-processing is generally the most prevalent and effective. However, due to the scarcity of serendipity data, which often relies on user annotations, they must adopt smaller models and depend on biased data for augmentation. This may reinforce feedback loops, making bursting filter bubbles and identifying serendipity challenging.


Recently, LLMs have been applied to serendipity recommendations**Chen et al., "Relevant but not interesting: On the relationship between relevance and serendipity in recommender systems"**. For example, in**Bao et al., "Zero-shot evaluation of text generation models"**, LLMs evaluate items' serendipity in a zero-shot manner. SerenPrompt**Xu et al., "SerenPrompt: A prompt engineering framework for evaluating the effectiveness of serendipitous recommendations"** designs various prompts for LLMs to assess serendipity. These preliminary attempts show the promising potential of LLMs in serendipity recommendations. Still, several unresolved challenges remain, such as aligning LLMs with humans in serendipity and their deployment in industrial RSs.


\subsection{LLM-based Recommendations}
The past few years have witnessed a growing interest in LLM-based RSs**Chen et al., "A survey on deep learning for recommender systems"**, which can be roughly classified into two categories: (1) LLMs as recommenders and (2) LLMs as components of traditional recommenders. The former is to adopt LLMs as recommenders to generate recommendations directly**Bao et al., "Zero-shot evaluation of text generation models"**. Early attempts primarily utilize zero-shot LLMs**Brown et al., "Measuring massive multimodal polysemy with artificial neural networks"**. 
% For instance, ChatRec**Wang et al., "ChatRec: A conversational recommender system based on deep learning"** and MemoCRS**Xu et al., "MemoCRS: A multi-round recommendation system using long short-term memory (LSTM)"** employ LLMs as recommender system interfaces for conversational multi-round recommendations. 
However, this falls behind SOTA recommendation models, so the focus of later work shifts to how to inject recommendation knowledge into LLMs, primarily through fine-tuning, such as TALLRec**Li et al., "Improving deep learning-based recommenders using transfer learning"** fine-tuned on LLaMA-7B**Kaplan et al., "Few-shot argumentation for language models"**. 
% ReLLa**Wang et al., "Retrieval-enhanced instruction tuning for natural language processing"** design retrieval-enhanced instruction tuning by adopting semantic user behavior retrieval as a data augmentation technique and finetunes Vicuna-13B**Zhang et al., "Vicuna: A unified framework for sequence generation tasks using pre-trained language models"**.
The latter integrates knowledge from LLMs into conventional RSs to enhance the recommendation performance and meanwhile avoid the online inference latency issue of LLMs**Chen et al., "A survey on deep learning for recommender systems"**. For example, KAR**Wang et al., "Knowledge-aware routing for efficient recommendation in large-scale networks"** and RLMRec**Li et al., "Improving deep learning-based recommenders using transfer learning"** integrate open-world knowledge from LLMs into RSs. 
% Some researchers propose S\&R Multi-Domain Foundation model**Kaplan et al., "Few-shot argumentation for language models"**, which finetunes LLMs on recommendation and search data to extract domain invariant features for promoting performance in cold-start scenarios. 

% There are two categories: LLMs as recommenders and LLMs as feature enhancers. The latter is more suitable for deployment.

The latter is more deployable for industrial RSs when integrating RSs and LLMs**Chen et al., "A survey on deep learning for recommender systems"**. The inference latency of LLMs is very high, and current industrial RSs cannot support the online inference of LLMs that the first approach requires. The latter only involves LLMs offline to generate features, which minimizes its impact on online inference latency. However, these deployable methods have only focused on improving recommendation accuracy and may struggle to adapt to data distribution changes. Therefore, we focus on serendipity recommendations and adopt nearline caching solutions to avoid LLMs' online inference while ensuring rapid updates to adapt to distribution changes.
% , efficiently integrating LLMs into industrial RSs. 
% To this end, our work follows the latter but shifts the focus to serendipitous recommendations, incorporating serendipitous items generated by LLMs with traditional RSs.