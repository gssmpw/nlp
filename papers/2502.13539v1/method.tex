\section{Preliminaries}\label{sect:network_structure}
\subsection{Task Formulation}
Given an item set $\mathcal{I}$ and user set $\mathcal{U}$, for each user $u\in\mathcal{U}$ with history $H_u=\{h_1,\ldots,h_m\}$ of length $m$, where $h_i, i=1,\ldots,m$ denotes the $i$-th item the user interacted recently, the goal of serendipity recommendation is to find an item which is serendipity to the user $u$. Previous approaches convert it into a matching problem~\cite{fu2023wisdom,fu2024art}, where each candidate item $i$ is scored based on $H_u$, \ie,
\begin{equation}
    y_{u,i} = matching(i, H_u),
\end{equation}
where $y_{u,i}$ denotes the predicted serendipity score and $matching(\cdot)$ is matching function.
However, it is highly inefficient for LLMs to score all items. Therefore, we can formulate it as a generation problem, generating the next serendipitous item $i$ based on $H_u$:
\begin{equation}
    i = \pi_{\theta}(H_u),
\end{equation}
where $i$ is the predicted serendipity item generated by LLM $\pi_{\theta}(\cdot)$.

\subsection{Serendipity in industrial RSs}
Identifying serendipity is usually a challenging issue. Previous serendipity datasets often relied on user surveys~\cite{kotkov2018investigating,chen2019serendipity}. In industrial scenarios, vast amounts of interaction data are generated daily, making user surveys impractical. Therefore, by combining the key aspects of serendipity, unexpectedness, and relevance, we propose an industrial and simple definition of serendipity: 
\newtheorem{defi}{Definition}
\begin{defi}
A serendipitous item is identified by two criteria: it must be clicked, and its atomic category should not have been present in the user's valid exposures over the last N days with visits.
\label{def:serend} 
\end{defi}

In e-commerce, item classifications typically follow a tree structure, and we refer to the indivisible subcategories at the leaf nodes as atomic categories. The unexposed category guarantees the user's sense of freshness and surprise, while the click indicates its relevance. Here, $N$ may vary with scenarios; in our scenario, we set $N=10$ and find that optimizing this metric significantly improves user experience. However, such serendipity data is still scarce, and maintaining accuracy is still the primary goal of RSs. Therefore, the main goal of serendipity in industrial RSs is to maintain the system's utility (such as click-through rate (CTR), revenue) while increasing the proportion of serendipity items (\ie, S-PVR in Section~\ref{sec:metrics}), thereby keeping users engaged and intrigued. 
% Therefore, we need to consider adapting the serendipity model to the accuracy-oriented RSs, and we will discuss this in Section~\ref{sec:downstream}. 

\begin{figure*}
    \centering
    \vspace{-10pt}
    \includegraphics[trim={0.3cm 0.3cm 0 0},clip,width=\textwidth]{imgs/fw-8.pdf}
    \vspace{-15pt}
    \caption{The overall framework of SERAL, with Cognitive Profile Generation to compresses long behavior sequences into concise profiles, SerenGPT Alignment to align LLMs' serendipity judgments with humans, and Nearline Adaptation to integrate serenGPT into industrial recommender systems efficiently.}
    \vspace{-10pt}
    \label{fig:fw}
\end{figure*}

\section{Methodology}
\subsection{Overview}
As illustrated in Figure~\ref{fig:fw}, the workflow of our proposed SERAL encompasses three stages. Firstly, \textbf{Cognitive Profile Creation} mirrors human cognitive processes and compresses the user's long behavior sequences into concise cognitive profiles, including static, short-term, and long-term profiles. Secondly, in \textbf{SerenGPT Alignment}, we devise an LLM-based serendipity recommender, SerenGPT, to predict next serendipitous items. Then, we train and align SerenGPT's judgments with humans via a preference alignment method IPO~\cite{azar2024general}. When curating training data, we incorporate Collaborative Data Intervention (CDI) from LLMs and humans, breaking the internal feedback loop of RSs. Finally, in \textbf{Nearline Adaptation}, we implement a serendipity channel with SerenGPT and integrate its recommendations into accuracy-oriented RSs via nearline cache. This solves LLMs' efficiency issues of online serving.


\subsection{Cognition Profile Generation}
User historical behavior is critical for modeling serendipity; however, LLMs struggle to leverage long user behavior sequences. Previous research has shown that long historical sequences, whose lengths often reach up to 10,000~\cite{pi2020search,hou2024cross,si2024twin,feng2024context} in large-scale RSs, contain a wealth of user preferences and could enhance recommendation~\cite{pi2020search,qin2020user}.  Serendipity relies heavily on capturing unique user preferences, surprises, and discoveries, which also requires a deep understanding of longer behavior sequences. Nevertheless, LLMs struggle to comprehend long user behaviors, with performance declining well before reaching the context window of LLMs~\cite{lin2024rella}, obstructing them from identifying serendipitous items. 
% Therefore, it is crucial to improve LLMs' ability to grasp user interests and intentions from long behavior sequences for serendipity recommendations.

To address this, we design cognition profile generation, which compresses long user behavior into precise profiles that LLMs can easily recognize. This process mimics human cognition by first understanding foundational information to form a static profile, then capturing short-term intent from recent behaviors, and finally summarizing short-term intent from long-term patterns, creating multi-layer cognitive profiles that LLMs can effectively utilize.

\subsubsection{Static Profile} This profile contains fundamental information, encompassing basic information and statistical characteristics. These features are generally stable and do not require further processing, as they can be directly retrieved from databases. For a given user $u$, we can convert these attributes into a textual profile $p_s^u$ with pre-defined templates.

% \subsubsection{Static Profile} This profile contains fundamental information, encompassing basic user information (\eg, age, occupation, gender, and location) and statistical characteristics (\eg, purchasing power and price sensitivity). These features are generally stable and do not require further processing or summarization, as they can be directly retrieved from databases. For a given user $u$, we can convert these attributes into a textual profile $p_s^u$ with pre-defined templates, such as \textit{``This user was born in \{\{year\}\}, is a \{\{gender\}\} working as a \{\{profession\}\}, resides in \{\{city\}\}, has a \{\{high/medium/low\}\} price sensitivity and a \{\{high/medium/low\}\} purchasing power.''}

\subsubsection{Short-term Profile} Short-term profile represents volatile, short-term intentions and interests extracted from the user's recent behaviors (\eg, search queries and titles of purchased products from the past 2-4 weeks). Many users' habits and interests are embedded within their behaviors. LLMs, with their strong reasoning abilities and extensive world knowledge~\cite{wei2022chain,llama}, can simulate human-like reasoning and extract users'  traits and interests from their behaviors. Thus, we first focus on refining profiles from recent behavior sequences via LLMs. Specifically, given recent behaviors $H_u$ of user $u$, LLMs infer candidate profile $p^u_{st}$ via Chain-of-Thought (CoT) reasoning~\cite{wei2022chain} based on prompt template $T_{st}$:
\begin{equation}
    p^u_{st} = f_{LLM}(H_u, T_{st}),
\label{eq:llm_recent}
\end{equation}
where $T_{st}$ possesses task description such as \textit{``Based on the user's purchasing history, infer their purchasing intent and generates diverse profile tags for the user.''} and format requirements like \textit{``The output format should be in JSON, with the user profile tag as key and the reasoning process as the value.''} To avoid irrelevant outputs and reduce hallucinations, we limit the user profile tags to preferred categories, brands, IPs, and attributes, possible interests, and possible usage scenarios. The full prompt can be found in Appendix~\ref{app:prompt}.

After the CoT reasoning of LLMs, we obtain several candidate profiles $p^u_{st}$. However, these profiles may be unreliable due to hallucinations. Therefore, an approval mechanism is necessary to calibrate a more accurate short-term profile. We maintain a tag pool for users to store credible tags. When new tags $p^u_{st}$ are generated, a historical prior check (whether it already exists in the tag pool) and a behavioral posterior check (based on the relevance between products' tags and profile tags) are employed to determine their credibility. Credible tags are then scored and ranked, with the higher-ranking tags selected as the short-term profile $\hat{p}^u_{st}$. 

% For example, we can determine whether a profile tag qualifies for the tag pool based on confidence or frequency and further score those profile tags via user behavior and tag priors, with the highest-ranked ones forming the user's recent profile $\hat{p}^u_{st}$.


% Next, LLMs infer the user's candidate intent based on the above keywords with chain-of-thought (COT) reasoning. 

% The admission criteria: The candidate intent is then scored and ranked after applying historical label priors, behavioral posteriors, and admission rules (confidence scoring).
\subsubsection{Long-term Profile}\label{sec:long-term} This refers to stable and long-term intentions derived from multiple short-term profiles. Compared to the short-term profile, the long-term profile focuses more on stability, accuracy, and long-term relevance, requiring a more rigorous reasoning and calibration process. To achieve this, we periodically mine the user's short-term profiles, with criteria like frequency, regularity, and long-term effectiveness as admission conditions, to get candidate long-term profiles $p^u_{lt}$. By combining these factors with the summarization and reasoning abilities of LLMs, we could refine the user's long-term profile $\hat{p}^u_{lt}$ as follows:
\begin{equation}
    \hat{p}^u_{lt} = f_{LLM}([p^u_{lt}, T_{lt}]),
\label{eq:llm_long}
\end{equation}
where $p^u_{lt}$ denotes candidate profiles that pass the admission criteria and $T_{lt}$ is a prompt template with task description like \textit{``Based on the given short-term profiles, summarize and refine the user's long-term profile.''} To ensure accuracy, long-term profiles also follow approval and ranking processes similar to short-term profiles.

Note that the LLMs employed for user profile generation are distinct from SerenGPT, the model discussed in section \ref{sec:align} for predicting serendipitous items. LLMs for cognitive profile generation are very powerful or highly specialized models for this task, \eg, advanced large-scale models at the
GPT-4 level or small LLMs fine-tuned on user profile data.

% LLMs are responsible for extracting and summarizing short-term profiles into candidate long-term intents.

% The admission criteria: frequency, periodicity, long-term effectiveness, and multi-dimensional verification. After confidence scoring, a decision is made on admitting the long-term intent.

\subsection{SerenGPT Alignment}\label{sec:align}
We now obtain cognition profiles, so LLMs can leverage profiles and other information to make serendipity recommendations. However, human perception of serendipity is nuanced and personalized, and zero-shot or even fine-tuned LLMs struggle to align with human perspectives on this~\cite{fu2024art,tokutake2024can}. To address this, we develop an LLM-based recommender, SerenGPT, trained with IPO~\cite{azar2024general}. In selecting data, we devise \textbf{Collaborative Data Intervention (CDI)}, which incorporates both intervention and annotations from LLMs and humans, along with the rich external knowledge of LLMs, to mitigate feedback loops and reduce the cost of human annotations.

\subsubsection{Serendipity Data Generation}\label{sec:data_gen}
% Diverse data sources: 

% - recommendation generated from relevance-based collaborative models. 

% - serendipity items directly recommended by LLMs (based on user behaviors, profiles, and external trends)

% the above data are filtered by specific rules 

% Combining GPT-4 with human annotation, we can identify serendipity items that can be inferred by profiles, history, and the current environment while excluding data that cannot be reasonably predicted, such as accidental clicks.

% A judgment model is trained on the above labeled data to select appropriate data for the upcoming SerenGPT training.

The filter bubble effect arises from RSs training on internal data biased by itself, leading to reinforced feedback loops. To break this cycle, the training of RSs should not solely rely on unmodified internal data; instead, it should integrate more external information and intervention. Therefore, we intervene in data collection by combining the world knowledge of LLMs and human annotations.
Our data sources primarily include:
\begin{itemize}
    \item Data from traditional accuracy-oriented recommender systems that align with the definition~\ref{def:serend} of serendipitous items.
    \item Data generated by serendipity recommender (\eg, SerenGPT or LLMs based on user profiles, historical behavior, and external trends) which also aligns with definition~\ref{def:serend}.
\end{itemize}


These two data sources may also contain some noise, such as accidental clicks. Thus, we design \textbf{Collaborative Data Intervention (CDI) Denoising}, employing powerful LLMs at GPT-4 level and human annotators to reduce noise. Specifically, the LLMs perform CoT reasoning~\cite{wei2022chain} based on user historical behavior and profiles to determine whether a target item is serendipitous for the user. Human annotators further assess the accuracy of the reasoning and conclusions from LLMs, generating more accurate serendipity labels. The annotated data, including user historical behaviors, profiles, target items, and serendipity labels, will be used to fine-tune a smaller-scale LLM (\eg, 7 billion parameters) to identify serendipity data for subsequent SerenGPT training. This allows for identifying serendipitous items with relatively low human annotation costs.
% The specific implementation of CDI here involves leveraging LLMs for annotations, followed by human evaluation to assess the rationality of annotations. 


\subsubsection{Supervised Fine-tuning (SFT)}\label{sec:sft}

Upon acquiring serendipity data, where each entry encompasses user $u$, historical behaviors $H_u$, and the next serendipitous item $i$, we proceed to fine-tune SerenGPT $\pi_\theta$. SerenGPT can be any causal language model that can analyze users' behaviors and profiles to generate the subsequent potential serendipitous item. First, the serendipity data are converted into a textual SFT dataset $D$, where each entry is a tuple $(x,y)$. Here, $y$ refers to the output, title of serendipitous item $i$, and $x$ represents the input prompt, \eg, ``\textit{Given the user's static profile $p^u_{s}$, short-term profile $\hat{p}^u_{st}$, long-term profile $\hat{p}^u_{lt}$, and recent historical behaviors $H_u$, generate the next serendipitous item that the user may be surprised by and enjoy.}'' To address delays in short-term profiles (which update every 15 days online), we also incorporate users' recent historical behaviors. Next, following Eq.~\eqref{eq:sft}, SerenGPT is supervised fine-tuned to augment its probability of generating desired responses.
\begin{equation}
    \begin{split}
    \mathcal{L}_{SFT}&=-E_{(x, y)\sim D}(\log \pi_\theta(y|x)) \\
    &=-E_{(x, y)\sim D}(\sum^{T}_{t=1}\log \pi_\theta(y_t|x,y_{1:t-1})).
\label{eq:sft}
\end{split}
\end{equation}


\subsubsection{Pair Generation and IPO Training}
In standard preference alignment frameworks, the LLM typically undergoes SFT, followed by preference alignment training on datasets comprising human-annotated preferences over pairs of model-generated responses. Techniques such as DPO~\cite{rafailov2024direct} are employed to align the model's outputs more closely with human preference. However, such methodologies encounter significant obstacles when applied to serendipity recommendations. Firstly, acquiring annotations directly from real users is impractical, while relying on other annotators may require substantial effort and could lead to inaccuracy. Secondly, our empirical findings indicate that LLMs after SFT tend to generate homogenized responses for serendipity recommendations, as shown in Appendix~\ref{app:diversity}. Even with ground truth recommendations as the preferred response, constructing a diverse preference dataset with discriminability between responses proves challenging.

To mitigate these limitations, we propose a \textbf{Collaborative Data Intervention (CDI) Pairing} mechanism, which synthesizes human expertise with teacher LLMs to curate high-quality preference pairs. Specifically, SerenGPT after SFT generates multiple recommendations by sampling, which are subsequently evaluated by teacher LLMs and assigned serendipity scores ranging from 1 to 6. Preference pairs are constructed by designating higher-scoring recommendations as preferred responses and lower-scoring ones as dis-preferred ones. For cases where scores are close, human annotations are performed to ensure accuracy. Here, the teacher LLMs employed in CDI are state-of-the-art, high-capability models at the GPT-4 level. These models enhance the quality and reliability of preference data, ensuring that the alignment training is rigorous and effective in capturing nuanced human preferences.

After obtaining the preference data, we train SerenGPT with the preference alignment algorithm IPO~\cite{azar2024general} as shown in Eq.~\eqref{eq:ipo}. The reason for choosing IPO over DPO~\cite{rafailov2024direct} is that during the training process, we observe that DPO tends to overfit, while the IPO algorithm helps alleviate this overfitting.

\begin{equation}
    \mathcal{L}_{IPO}=-E_{(x,y_w,y_l)\sim D'}\left[\left(\log\frac{\pi_\theta(y_w|x)\pi_{ref}(y_l|x)}{\pi_{ref}(y_w|x)\pi_{\theta}(y_l|x)} - \frac{\tau^{-1}}{2}\right)^2\right],
    \label{eq:ipo}
\end{equation}
where $D'$ denote the preference dataset,  $x$ denotes the input prompt, and $y_w$ and $y_l$ represent the preferred and dis-preferred responses. Here,  $\pi_{\theta}$ is the trainable LLM for preferences alignment, while $\pi_{ref}$ is a fixed reference policy, typically the original LLM. $\tau$ is the hyper-parameter of the regularization term, which balances learning human preferences and staying close to the reference policy, thereby preventing overfitting. In our experiments, we find that incorporating the SFT loss improves performance. Therefore, the final loss at this stage is:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{IPO} +\alpha \mathcal{L}_{SFT}.
    \label{eq:final_ipo}
\end{equation}

\subsection{Nearline Adaptation}\label{sec:downstream} Nearline computation lies between online serving and offline processing. This concept was first introduced by Netflix~\cite{amatriain2013big,amatriain2013system} and has been used in many recommender systems~\cite{ma2024simple,borisyuk2024lignn,li2021truncation}. Nearline computation does model calculations at periodic intervals, such as daily or hourly, and caches the results for online serving. When a user request is initiated, the system combines the user's real-time information with the nearline cached results to generate recommendations. This approach improves recommender systems' efficiency and response speed while maintaining high accuracy.

The high latency and low real-time dependency of LLM-based serendipity recommendations make it suitable for the nearline approach. Firstly, LLMs have significant inference latency, which makes it hard to meet the low-latency demands of online serving in RSs, usually in 100 ms~\cite{xi2023device}. Secondly, serendipity recommendations do not rely much on real-time user behaviors. Its objective is to provide users with unexpected but valuable content, prioritizing long-term user satisfaction over real-time needs. Over-reliance on real-time behavior could reduce serendipity. Therefore, we adapt SerenGPT to large-scale RSs via a nearline approach. Specifically, we design two channels: an online personalized channel and a nearline serendipity channel, as shown in Figure~\ref{fig:fw}. The serendipity channel consists of SerenGPT, downstream models, and a nearline cache, while personalized channel is personalized multi-stage RSs.

% why adapt it to conventional models and why adapt it on both recall and ranking stage 
\subsubsection{Serendipity Channel}
This channel is responsible for leveraging SerenGPT, trained in the previous stage, for generating serendipity candidates and then caching the results in a nearline cache for online serving. Specifically, recommendations generated by SerenGPT first undergo multi-channel retrieval, such as keyword-based and vector-based retrieval. A pre-ranking model then processes the aggregated results, filtering out fewer but higher-quality candidates. The optimization of retrieval and pre-ranking models also considers serendipity. For instance, their training data includes a higher proportion of serendipity samples and incorporates optimization for hard negative samples related to serendipitous items. Finally, the serendipity candidates from the pre-ranking model are stored in a nearline cache. This cache retains those candidates within a time window of T hours, typically set to 24 hours, after which the cached candidates expire and are recalculated.

\subsubsection{Integration with Personalized Channel}

The personalized channel refers to traditional multi-stage recommender systems, without consideration for serendipity. In our dual-channel system, when a user triggers a recommendation, two pathways are activated: 
\begin{itemize}
    \item It collects the user's real-time behaviors and contextual information to generate personalized candidate items through the retrieval and pre-ranking stages of the personalized channel.
    \item It checks the nearline cache for the user's serendipity candidate items. The cached candidates are used directly if they are valid and unexpired (within T hours). If not, the system triggers the whole serendipity channel to generate new candidates, which are then added to the nearline cache.
\end{itemize}
Subsequently, serendipity candidates and personalized candidates are combined in the ranking stage. Since ranking is a part of the personalized channel and does not include optimizations for serendipity, we adjust the weights of serendipity candidates to make them more prominent, ensuring they get sufficient exposure. After the fusion in ranking, the top results are recommended to the user.
