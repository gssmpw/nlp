\appendix
\section{Baselines}\label{app:baselines}
 For serendipity recommendations, we follow~\cite{fu2024art} and adopt three categories of baselines: sequential recommendation algorithms, deep learning-based serendipity recommendation algorithms, and LLM-based serendipity recommendation algorithms.

\textbf{Sequential recommendation} are trained on both serendipity and non-serendipity data. We selected two classic algorithms: 
\begin{itemize}
    \item \textbf{SASRec}~\cite{kang2018self} captures long-term semantics with a self-attention mechanism for sequential recommendation.
    \item \textbf{BERT4Rec}~\cite{sun2019bert4rec} introduces a Bidirectional Encoder Representation from Transformers for the sequential recommendation.
\end{itemize}
\textbf{Deep learning-based serendipity recommendation algorithms} are mainly trained on serendipity data, but due to the limited amount of such data, they leverage additional non-serendipity data for augmentation. In this category, we choose two state-of-the-art baselines: 
\begin{itemize}
    \item \textbf{SerenEhance}~\cite{fu2023wisdom} devises a self-enhanced module to learn fine-grained facets of serendipity for mitigating the data sparsity problem.
    % \item DESR~\cite{li2020directional} empolys serendipity vector to combine long-term preferences with short-term demands and generate serendipitous recommendations.
    \item \textbf{PURS}~\cite{li2020purs}  incorporates unexpectedness into recommendation by providing multi-cluster modeling of user interests and personalized unexpectedness via self-attention.
    % \item SNPR~\cite{zhang2021snpr} formulates serendipitous POI recommendation as a supervised multi-task learning problem and sloves it with Transformer.
\end{itemize}
\textbf{LLM-based serendipity recommendation algorithms} are trained only on serendipity data. This is a relatively new area, and we identify two baselines:
\begin{itemize}
    \item \textbf{LLM4Seren}~\cite{tokutake2024can} uses zero-shot LLMs to assess the serendipity of candidate items.
    \item \textbf{SerenPrompt}~\cite{fu2024art} employ various prompts and prompt tuning for LLMs to assess serendipity.
\end{itemize}
Our algorithm SerenGPT is also trained solely on serendipity data. While there are slight differences in training data, we ensure the use of the same test data for fair comparisons. Additionally, the LLM-based baselines utilize the same backbone LLM as SerenGPT and are trained on the same amount of training data.




\section{Diversity Analysis}\label{app:diversity} 
\begin{table}[ht]
\centering
% \vspace{-7pt}
\caption{Diversity comparison between different tuning strategies on serendipity recommendation. }
% \vspace{-5pt}
\scalebox{1}{
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{cccc}
\toprule
Method & avg\_title\_cnt & avg\_cate\_cnt & hit\_rate \\
\midrule
SFT & 29.2 & 83.0 & 0.5483 \\
KTO & 23.7 & 25.3 & 0.4315 \\
DPO & 25.5 & 56.7 & 0.4845 \\
\textbf{IPO (ours)} & \textbf{30.0} & \textbf{106.8} & \textbf{0.5532} \\
\bottomrule
\end{tabular}
}}
% \vspace{-5pt}
\label{tab:diversity}
\end{table}

In our online experiments, we find that the diversity of recommendation produced by different preference alignment algorithms varies widely, and the IPO~\cite{azar2024general} algorithm generates more diverse results while maintaining accuracy. Therefore, we ultimately choose IPO as the preference alignment algorithm.

During the nearline adaptation step, the predictions generated by SerenGPT are used to retrieve items from the candidate item pool. Specifically, we perform 30 sampling iterations for each user, generating 30 titles, which are then used to retrieve items across different categories. We evaluate the results generated by SFT, DPO~\cite{rafailov2024direct}, KTO~\cite{ethayarajh2024kto}, and IPO~\cite{azar2024general} on the test set by analyzing three metrics: \textit{avg\_title\_cnt} (number of unique titles among 30 samples), \textit{avg\_cate\_cnt} (the average number of categories retrieved), and \textit{hit\_rate} (whether the target serendipity item's category is included in the retrieved categories). 

From Table~\ref{tab:diversity}, we observe that IPO generates almost no duplicate titles, retrieves a wide variety of categories, and achieves a higher hit rate. This indicates that IPO can generate more diverse results while maintaining relevance, which is beneficial for producing serendipitous outcomes. We notice content homogeneity after the initial SFT stage, which hinders the generation of meaningful preference pairs. Initially, we bypass preference pair generation and directly adopt the pair-independent KTO approach, but this further amplifies the homogeneity (e.g., reduced number of titles and retrieved categories). To address this, we use CDI to generate higher-quality preference pairs for DPO, which alleviates the diversity issue but still underperforms SFT. Ultimately, IPO proves to be the best approach, achieving both higher accuracy and diversity. This improvement might be because SFT, KTO, and DPO tend to overfit when handling deterministic or near-deterministic data, whereas IPO effectively avoids such issues~\cite {azar2024general}.


\section{Efficiency Analysis}\label{app:efficiency}
To evaluate the inference efficiency of our model, we compare it with SOTA deep learning-based baseline SerenEnhance, two LLM-based models (LLM4Seren and SerenPrompt). We measure the average inference time each model takes to score a candidate set of 100 items, as shown in Table~\ref{tab:inference}. 

First, the deep learning-based model SerenEnhance has high inference efficiency, making it suitable for online serving. However, the LLM-based models have significantly longer inference times. To address this, we adopt a nearline caching approach to avoid high inference delays. Second, SerenGPT demonstrates substantially lower inference latency than the other LLM-based baselines. This is because SerenGPT is a generative model that directly predicts the next serendipitous item, bypassing the need to score each candidate individually. In contrast, LLM4Seren and SerenPrompt must score every candidate item, resulting in extremely long inference times. Additionally, their approach cannot support nearline caching since they require real-time scoring of candidate items, making it challenging to deploy them in production. This difference in efficiency highlights why we chose a generative approach with SerenGPT -- it reduces latency and enables practical deployment with nearline caching.

\begin{table}[ht]
\centering
\vspace{-5pt}
\caption{Comparison of average inference time. }
\vspace{-5pt}
\scalebox{0.9}{
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{ccccc}
\toprule
Method & {SerenEnhance} & {LLM4Seren} & {SerenPrompt} & {SerenGPT} \\
\midrule
Time (s) & {0.0204} & {35.65} & {35.36} & {4.48} \\
\bottomrule
\end{tabular}
}}
\vspace{-5pt}
\label{tab:inference}
\end{table}

% \section{Case Study}
% To explore the specific recommendations made of our model and the online baseline, we visualize the recommendations for the same user during the same recommendation session in Figure~\ref{fig:case_study}. The user's historical behavior primarily involves winter women's clothing. In this context, the baseline model mostly recommends winter women's clothing, with only a few items outside this category. In contrast, our model leverages the user's profile and reasoning on historical behaviors to generate more insightful recommendations. For instance, the user profile indicates they own pets and enjoy nail art, so our model recommended pet care products and nail art supplies. Additionally, the user's recent interactions involve winter clothing, such as sweaters, which are prone to pilling. Therefore, our model also recommended a fabric shaver. This demonstrates that our model can utilize powerful reasoning capabilities to uncover user needs and provide more novel and useful recommendations.

% \begin{figure}
%     \centering
%     \vspace{-10pt}
%     \includegraphics[trim={0 5cm
%     0 0},clip,width=0.5\textwidth]{imgs/case_study-2.pdf}
%     \vspace{-20pt}
%     \caption{Case Study.}
%     \vspace{-10pt}
%     \label{fig:case_study}
% \end{figure}

% \section{Long-term Impact of Serendipity}\label{app:long_ab}


\section{Full Ablation Study}\label{app:ablation}
Due to space constraints, we present a subset of ablation metrics in the main text, with the full set of metrics available in Table~\ref{tab:Ablation_full}.

\begin{table*}[h]

    % \vspace{-10pt}
    \caption{Ablation study of SerenGPT. The best result is given in bold, while the second-best value is underlined. }
    % \vspace{-8pt}
    \centering
    \scalebox{1}{
    \setlength{\tabcolsep}{1.5mm}
    {\begin{tabular}{ccccc|cccc|cccc}
\toprule
\multirow{2}{*}{Method} & \multicolumn{4}{c|}{MAP$_{seren}$} & \multicolumn{4}{c|}{NDCG$_{seren}$} & \multicolumn{4}{c}{HR$_{seren}$} \\
\cmidrule{2-13}
 & @1 & @3 & @5 & @10 & @1 & @3 & @5 & @10 & @1 & @3 & @5 & @10 \\
 \midrule
w/o CP & 0.2513 & 0.2933 & 0.3065 & 0.3195 & 0.2513 & 0.3031 & 0.3262 & 0.3558 & 0.2513 & 0.4148 & 0.5042 & 0.6450 \\
w/o CDI & 0.2445 & 0.2840 & 0.2960 & 0.3066 & 0.2445 & 0.2924 & 0.3141 & 0.3377 & 0.2445 & 0.4103 & 0.4996 & 0.6101 \\
w/o CDI denoise & 0.2619 & 0.2998 & 0.3110 & 0.3224 & 0.2619 & 0.3086 & 0.3284 & 0.3542 & 0.2619 & 0.4307 & 0.5193 & 0.6313 \\
w/o CDI pair & \underline{0.2860} & \underline{0.3223} & \underline{0.3317} & \underline{0.3397} & \underline{0.2860} & \underline{0.3303} & \underline{0.3470} & 0.3653 & \underline{0.2860} & 0.4534 & 0.5299 & 0.6101 \\
w/o IPO & 0.1771 & 0.2198 & 0.2327 & 0.2476 & 0.1771 & 0.2312 & 0.2530 & 0.2866 & 0.1771 & 0.3316 & 0.4148 & 0.5632 \\
\midrule
w/ $\alpha=0$ & 0.2521 & 0.2887 & 0.3009 & 0.3134 & 0.2521 & 0.2969 & 0.3176 & 0.3460 & 0.2521 & 0.4080 & 0.4943 & 0.6291 \\
w/ SFT & 0.2695 & 0.3117 & 0.3224 & 0.3335 & 0.2695 & 0.3216 & 0.3409 & \underline{0.3663} & 0.2695 & \textbf{0.4640} & \underline{0.5496} & \underline{0.6616} \\
w/ DPO & 0.2513 & 0.2926 & 0.3031 & 0.3144 & 0.2513 & 0.3027 & 0.3210 & 0.3459 & 0.2513 & 0.4239 & 0.5042 & 0.6192 \\
\midrule
\textbf{SerenGPT} & \textbf{0.2861} & \textbf{0.3260} & \textbf{0.3395} & \textbf{0.3509} & \textbf{0.2861} & \textbf{0.3353} & \textbf{0.3587} & \textbf{0.3847} & \textbf{0.2861} & \underline{0.4625} & \textbf{0.5556} & \textbf{0.6760} \\ 

\bottomrule
\end{tabular}}
    }

\label{tab:Ablation_full}
% \vspace{-5pt}
\end{table*}

\section{Prompts and Examples}\label{app:prompt}
The prompt for the short-term profile is long, so we provide only a brief description in the main text and place the complete prompt in Figure~\ref{fig:short-prompt}. The prompt for the long-term profile and SerenGPT is simpler, and we have described it in Section~\ref{sec:long-term} and~\ref{sec:sft}. 

% Here, we provide an example of SerenGPT's input (three types of profiles and user history) and its recommendations in Figure~\ref{fig:full-example}. To protect user privacy, we have withheld certain information in this example.
% The prompt for the short-term profile is long, so we provide only a brief description in the main text and place the complete prompt in Figure~\ref{fig:short-prompt}. The prompt for the long-term profile is simpler, and we have described it in Section~\ref{sec:long-term}. Here, we provide specific examples of three types of profiles: static, short-term, and long-term profiles, which can be found in Figure~\ref{fig:profile-example}. The prompt for SerenGPT is also described in Section~\ref{sec:sft} of the main text, and here we provide its recommendation example in Figure~\ref{fig:serenGPT-example}.

\begin{figure*}  
    \centering
    \includegraphics[width=\textwidth]{imgs/prompt-short.pdf}  
    % \vspace{-30pt}
    \caption{Prompt for short-term profile.}
    \label{fig:short-prompt}
\end{figure*}

% \begin{figure} 
%     \centering
%     % \vspace{-20pt}
%     \includegraphics[trim={0.5cm 0.6cm
%     0 0.9cm},clip,width=0.5\textwidth]{imgs/full-example_v2.pdf}  
%     \vspace{-20pt}
%     \caption{Example of SerenGPT's input and recommendation.}
%     \label{fig:full-example}
% \end{figure}

% \begin{figure} 
%     \centering
%     \includegraphics[width=0.5\textwidth]{imgs/profile-example.pdf}  
%     \vspace{-30pt}
%     \caption{Example of cognition profile.}
%     \label{fig:profile-example}
% \end{figure}

% \begin{figure} 
%     \centering
%     \includegraphics[width=0.5\textwidth]{imgs/SerenGPT-example.pdf}  
%     \vspace{-30pt}
%     \caption{Example of SerenGPT's recommendation.}
%     \label{fig:serenGPT-example}
% \end{figure}


\section{Another Task: Search Query Prediction}
\subsection{Setup} In addition to utilizing SerenGPT for item title generation, we also explore another task, \textbf{search query prediction}, to enhance the serendipity recommendation in the "Guess What You Like" column of the Taobao App homepage. This task predicts the next serendipitous search query based on the user's historical behavior and profile. Users' search queries can also contain rich serendipity information, such as new preferences and unmet needs in current recommendations. Therefore, we filter out search queries that lead to serendipitous clicks and train SerenGPT to predict these queries. In the online serving, the predicted search queries will enter the recall phase of the serendipity channel just like the predicted items, ultimately generating recommendations. For offline experiments, we filter out 26, 672 serendipity queries and divide them into a training set and a test set in a 9:1 ratio, focusing on comparing the accuracy of predicted queries.


While the field of \textbf{search query prediction} has not specifically focused on serendipity, predicting search queries that users may find serendipitous is still highly valuable in search. It can help capture user interest, improve retention, and enhance satisfaction. Since this field heavily relies on textual information, traditional serendipity recommendation algorithms based on IDs are difficult to adapt. Therefore, we design several LLM-based baseline algorithms inspired by SerenGPT, particularly exploring various DPO variants, which also serve as an ablation study for our method:
\begin{itemize}
    \item \textbf{ZSLLM} employs LLM to predict the next serendipitous search query in a zero-shot setting.
    \item \textbf{SFT} refers to LLM being fine-tuned in a supervised manner on the full training dataset.
    \item \textbf{DPO} replaces SerenGPT's IPO~\cite{azar2024general} with DPO~\cite{rafailov2024direct}, a classic preference optimization algorithm based on pairwise preferences.
    \item \textbf{KTO} replaces SerenGPT's IPO with KTO~\cite{ethayarajh2024kto}, which relies solely on binary feedback indicating whether an output is desirable or undesirable.
    \item \textbf{SimPO} replaces SerenGPT's IPO with SimPO~\cite{meng2024simpo}, which eliminates the need for a reference model.
\end{itemize}
All these baselines share the same backbone LLM and use the same prompt as SerenGPT. An example prompt structure is: "\textit{Based on the user's profile \{\{cognitive profile\}\} and recent behavior history \{\{history\}\}, predict the next search query that the user might find serendipitous: }" 
% \muyan{user's profile \{cognitive profile\} and recent behavior history \{history\} ?}

For evaluation, we use \textit{hit rate}, the proportion of test samples where the generated query matches the ground truth query. Different queries may have the same meaning, so we train an LLM-based relevance model to determine whether two queries are semantically aligned (matched). 

\subsection{Offline and Online Performance}
Since there is no existing serendipity model for the task of search query prediction, we compare several baselines that we design, which are based on the SerenGPT structure with modifications to the preference alignment approaches. These can also be seen as ablations of SerenGPT. The final results are presented in Table~\ref{tab:overall_search}, from which we can see that the SerenGPT trained with IPO performs the best. This highlights the importance of preference alignment and choosing the right alignment method. Some alignment methods, such as DPO, perform worse than SFT, which may be due to overfitting, as discussed in Appendix~\ref{app:diversity}, leading to the generation of overly homogeneous items.

\begin{table}[h]

    % \vspace{-8pt}
    \caption{Performance on search query prediction task.}
    % \caption{Overall performance on search query prediction task.  The best result is given in bold, while the second-best value is underlined. The symbol * indicates statistically significant improvement over the best baselines( t-test with $p < 0.05$).}
    \vspace{-8pt}
    \centering
    \scalebox{0.95}{
    \setlength{\tabcolsep}{1.5mm}
    {\begin{tabular}{c|cccccc}
\toprule
Method & ZSLLM & SFT & DPO & SimPO & KTO & \textbf{IPO(ours)} \\
\midrule
hit rate & 0.2842  & 0.3298 & 0.3191 & 0.3289 &  0.2181 & \textbf{0.3485*} \\
 \bottomrule
\end{tabular}}
}
\label{tab:overall_search}
\vspace{-8pt}
\end{table}

We also conduct an A/B test on the search query prediction task, comparing our approach to the online serendipity baseline. We select search queries that led to serendipitous clicks and train SerenGPT offline to predict these queries. During online serving, the predicted search queries, similar to predicted items, are incorporated into the serendipity channel’s recall stage, ultimately contributing to serendipitous recommendations. In the A/B test, we observe that the exposure, clicks, and transaction volume of serendipity-related items increased by 6.34\%, 18.69\%, and 30.20\%, respectively. Additionally, the CTR improves by 0.19 percentage points. However, UV3 shows a slight decline of 1.08\%.

