% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{arydshln}
\definecolor{LightCyan}{rgb}{0.93,0.95,1}
\newcolumntype{b}{>{\columncolor{LightCyan}}c}
\usepackage{minitoc} % Package for local table of contents

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research}

% Author information can be set in various styles:
% For several authors from the same institution:
\author{Junde Wu, Jiayuan Zhu, Yuyuan Liu \\
        University of Oxford }
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}


\begin{document}
\maketitle
\begin{abstract}
In this technical report, we introduce Agentic Reasoning, a framework\footnote{work in progress} that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: \url{https://github.com/theworldofagents/Agentic-Reasoning}.
%By enabling LLMs to reason more autonomously with external tools, Agentic Reasoning paves the way for AI systems that can tackle complex, knowledge-intensive challenges with greater depth and precision.
%Our code is released at: \url{https://github.com/MedicineToken/Medical-Graph-RAG}.
\end{abstract}
% through a comprehensive ablation study, comparing different methods for document chunking, graph construction, and information retrieval. 

\addtocontents{toc}{\protect\setcounter{tocdepth}{-10}} % Disable TOC for main sections

\section{Introduction}
Recently, large reasoning models, such as OpenAI’s o1 \cite{jaech2024openai}, Qwen-QwQ \cite{teamqwq}, and DeepSeek-R1 \cite{team2024deepseek}, have demonstrated impressive stepwise reasoning capabilities over long sequences through large-scale reinforcement learning. These advancements provide promising solutions to complex reasoning tasks \cite{wei2022chain, lewkowycz2022solving, noauthor_learning_nodate} and have inspired foundational efforts to replicate o1-like reasoning patterns across a broader range of models \cite{qin2024o1, huang2024o1, zhang2024llama}.

DeepSeek-R1, for example, relies exclusively on rule-based outcome rewards during training, such as evaluating whether a mathematical solution is correct or a piece of code executes successfully. While this approach has yielded remarkable reasoning capabilities, equaling o1’s performance in domains like math and code, it comes with notable trade-offs. As even the authors acknowledge, this type of training diminishes the model’s ability to articulate its reasoning process. DeepSeek-R1’s responses are often logical and accurate but lack detailed explanations of transitions between ideas or the finer connections between arguments.

Although current reasoning methods excel in structured domains like math and code—where outcomes are easily verifiable—applying these techniques to less structured or subjective tasks remains a significant challenge. Adapting these strategies to areas where answers are not inherently definitive is a key research gap. How can models be trained to handle tasks that require judgment, interpretation, or nuanced understanding rather than binary correctness?

Furthermore, not all problems benefit from formal reasoning approaches. Many fields, such as social sciences, ethics, or experiential disciplines, rely on abstract concepts, conventional wisdom, factual verification, understanding complex logical relationships, or moral reasoning. When models attempt to impose math- or coding-style reasoning onto such areas, they often produce flawed or overly rigid results. Developing approaches that account for these unique requirements is essential for advancing the applicability of reasoning model beyond their current domains.

Deep, thoughtful answers to open-ended questions often require extensive research, repeated verification, information retrieval, computational analysis, and the organization of complex logical relationships—steps fundamental to human reasoning. In this process, humans rely heavily on external tools, such as internet searches for gathering information, computational tools for quantitative analysis, or whiteboards and Mind Maps for organizing thoughts. This raises an intriguing question: can large language models similarly leverage external tools to enhance their reasoning and tackle intensive knowledge work across diverse domains?

Previous efforts have attempted to integrate search or retrieval-augmented generation (RAG) into the reasoning process \cite{shao2024assisting, khaliq2024ragar, islam2024open, li2025search}, with notable examples including Gemini's Deep Research. However, these models are closed, their exact methodologies remain undisclosed. In contrast, open-source models typically focus exclusively on retrieval or web-searching during reasoning, leaving a significant performance gap compared to their closed-source counterparts.

We introduce Agentic Reasoning, a framework that enhances the reasoning process by integrating external LLM-based agents as tools. This approach enables LLMs to perform multi-step reasoning and tackle complex problems more effectively by delegating specific tasks to these auxiliary agents. Through extensive experimentation with integrating various agents into the reasoning process, we identified three essential agents that prove highly effective for general reasoning across diverse problems. The web-search agent, which retrieves relevant information from the internet to supplement the model's knowledge. The  code agent, capable of performing computational analyses and coding tasks to support quantitative reasoning. Finally, the memory agent, which we call Mind Map, constructs knowledge graphs based on the reasoning context, enabling the organization of complex logical relationships in a manner similar to human mind mapping. Together, these agents enhance the model's ability to tackle complex problems with greater efficiency and precision.

When integrated into current reasoning LLMs, Agentic Reasoning transforms their problem-solving capabilities by enabling them to plan and execute multi-step strategies autonomously. These models can identify and retrieve the necessary data, adapt dynamically to real-time information, and perform quantitative analyses to generate precise outcomes. This framework also allows LLMs to deliver comprehensive reports comparable to those of a research analyst or provide solutions on par with PhD-level expertise.

% A standout feature of Agentic Reasoning is its knowledge-graph memory (KG-mem), which enhances the model’s ability to handle long-term reasoning tasks with complex logical relationships. This capability is particularly effective for solving intricate problems like “needle-in-a-haystack” scenarios and detective-style challenges, where maintaining clarity and coherence over extended reasoning chains is critical. By leveraging these tools and strategies, Agentic Reasoning empowers LLMs to achieve a level of precision, depth, and sophistication in knowledge-intensive tasks that was previously unattainable.

We evaluated our model on general knowledge-intensive benchmarks requiring complex reasoning capabilities, categorized into two key areas: (1) solving expert-level questions and (2) conducting deep research on real-world expert-level tasks.

For expert-level questions, we tested the model on the GPQA dataset, a PhD-level science multiple-choice QA benchmark with questions authored by domain experts in physics, chemistry, and biology. Our Agentic Reasoning framework achieved impressive accuracy rates: 58\% in chemistry, 88\% in physics, and 79\% in biology, closely rivals the best and newest closed reasoning model, OpenAI o1. For real-world expert-level tasks, Agentic Reasoning was evaluated by domain experts, who noted that it effectively automated several hours of challenging, manual investigation. This highlights its potential to streamline labor-intensive processes and enhance productivity in knowledge-intensive domains.

Additionally, we tested the model’s scalability in test-time reasoning using the agentic framework as a verifier. The results showed significant improvements in test-time computational efficiency, demonstrating the framework’s ability to optimize reasoning processes. This finding suggests that the agentic framework has strong potential to serve as a reward model for reinforcement learning, further advancing reasoning model training.

These results position Agentic Reasoning as a powerful and versatile framework, capable of tackling complex, domain-specific challenges with depth and precision. Its ability to perform in-depth research, navigate intricate logical structures, and synthesize information effectively highlights its potential for solving knowledge-intensive problems and driving advancements in deep analytical exploration.

\begin{figure*}[t]
    \begin{center}
    %\framebox[4.0in]{$\;$}
    \includegraphics[width=0.95\textwidth]{main_figure.png}
    \end{center}
    \caption{The overall workflow of Agentic Reasoning.}\label{fig1}
\end{figure*}

\section{Method}

\subsection{Preliminary}
We consider an expert-level task that requires multi-step complex reasoning. In the process of model reasoning, it can retrieve external tool usage, and structured memory of its previous reasoning. Our objective is to generate, for each query \( q \), both a logical reasoning chain \( r \) and a final answer \( a \). To achieve this, the reasoning model dynamically interacts with external tools \( e \), which are generally web search and python coding, and retrieves structured knowledge from an organized memory \( k \) throughout the reasoning process.

Formally, we identify four primary inputs in the problem-solving pipeline: task instruction \( o \), defining the overarching task objective, query \( q \), a complex question requiring multi-step reasoning, external tool outputs \( e \), dynamically retrieved content from tools such as web search or coding, reasoning memory \( k \), containing structured knowledge graph.

The goal is to integrate \( o, q, e, k \) to generate a coherent reasoning chain \( r \) and a final answer \( a \). This process can be expressed as the mapping:
\[
(o, q, e, k) \;\mapsto\; (r, a).
\]

We model the generation of \( r \) and \( a \) using the following joint probability formulation:
\[
\begin{aligned}
P(r, a \mid o, q, e, k) &= 
\underbrace{\prod_{t=1}^{T_r} P(r_t \mid r_{<t}, o, q, e_{\leq t}, k_{\leq t})}_{\text{Reasoning Process}} \\
&\quad \times
\underbrace{\prod_{t=1}^{T_a} P(a_t \mid a_{<t}, r, o, q, e, k)}_{\text{Answer Generation}}.
\end{aligned}
\]
where \( T_r \) and \( T_a \) represent the lengths (in tokens) of the reasoning chain \( r \) and the final answer \( a \), respectively. Here, \( r_t \) denotes the token at position \( t \) in the reasoning sequence, with \( r_{<t} \) representing all previous tokens. The terms \( e_{\leq t} \) and \( k_{\leq t} \) indicate all tool-generated outputs and knowledge-graph information retrieved up to step \( t \). Similarly, \( a_t \) is the token at position \( t \) in the final answer, and \( a_{<t} \) represents all previously generated answer tokens.

\subsection{Agentic Reasoning Pipeline}
Our core idea is to enhance the model reasoning by deploying external LLM-based agents during reasoning. The framework enables the reasoning LLM model interacts with external information in an agentic way. During its reasoning process, it could call the external tools to help solve the problem and also with a structured memory, called Mind Map, to store its reasoning context. At its core, an agentic mechanism empowers the model to determine, in real-time, when additional information is required. whenever the model identify the external information is needed during its reasoning, it will proactively embeds specialized  tokens into its reasoning tokens. These tokens can be generally categorized to web-search token, coding token, and mind-map calling token. Together with token, the reasoning model would also generate a precise query as a message to interact with these external agents, based on the reasoning context developed so far. 

Upon detecting such a token, the reasoning process temporarily halts to extract the query and its reasoning context. Those are then dispatched to external agents, such as search engines or Mind Map, to generate pertinent content. The generation would consider both the message received and the reasoning context to make sure returning the most relevant results. These results are then reintegrated into the reasoning chain, allowing the model to continue its inference with an updated and enriched knowledge.

This iterative retrieval-and-reasoning cycle continues as needed, enabling the model to dynamically refine its conclusions until it reaches a fully reasoned final answer.

\subsection{Mind Map Agent}

We construct a Mind Map to store and structure the real-time reasoning context of the reasoning model. This Mind Map is built by transforming raw reasoning chains into a structured knowledge graph. Specifically, we use a graph-construction LLM to extract entities from the reasoning chain and identify semantic relationships between related entities, following a process similar to that used in GraphRAG \cite{edge2024local}.

The Mind Map serves two primary functions. First, it clusters reasoning context into distinct groups and summarizes each theme. This is achieved by applying community clustering \cite{edge2024local} on the knowledge graph and using an LLM to generate concise summaries for each group. Second, the knowledge graph can be queried with specific questions, such as “Who was Jason's maternal great-grandfather?” Using standard retrieval-augmented generation (RAG) on the knowledge graph \cite{edge2024local}, we retrieve and return relevant information.

These functions integrate the Mind Map into various aspects of the Agentic Reasoning process. It provides contextual reasoning support to external tools, enabling them to generate more context-aware responses (as discussed in later sections). Additionally, when the reasoning model is uncertain about its claims or loses track in an extended reasoning process, it can query the Mind Map for relevant information, treating it as an external tool, and continue reasoning based on the retrieved answer.

\subsection{Web-search Agent}
A search agent is invoked to retrieve the most relevant documents from the web. Rather than incorporating the web pages in their raw form, they are temporarily held for further processing. This ensures that only the most pertinent information is extracted and integrated into the main reasoning chain, maintaining coherence and relevance.

Once the relevant web pages are retrieved by the search agent, we use LLM to extract a concise, rephrased summary of the content most relevant to the ongoing reasoning context. This agent processes the web pages in the context of both the user query and the reasoning context, distilling key insights that are directly applicable to the problem at hand. The format and length of the summary adapt dynamically based on the reasoning task, for example, for factual queries like “What is the population of the US in 2024? the result would be a simple numerical answer. For exploratory reasoning like finding a new perspective on a topic, the search agent would provide a summerized, detailed, nuanced viewpoint. For hypothesis validation like assessing supporting evidence for an assumption, the result would include the degree of support or contradiction found in the retrieved web-pages. This processed snippet is then integrated into the main reasoning process at the appropriate juncture, ensuring that external insights enhance rather than disrupt logical flow.

\subsection{Coding Agent}
Instead of prompting the reasoning model to generate code directly, we find it more efficient to delegate coding tasks to a specialized coding LLM. The reasoning model sends the relevant context and query message to the coding LLM, which then writes the required code, executes it via a compiler, and returns the results. This approach ensures that the reasoning model remains focused on its core reasoning process without being disrupted by coding tasks, allowing for longer and more coherent reasoning chains. Specifically, we format the coding request as follows:
"Write code to perform <code message from reasoning model> given the context <reasoning context from Mind Map> to answer the query <user query>." The coding LLM is instructed to always return its output in natural language, ensuring seamless integration with the reasoning model.


\subsection{Main Findings}
\textbf{Less is More}
Unlike general agentic frameworks that provide models with a large selection of external tools, we find that just two—web search and coding—are sufficient for most tasks, even those requiring expert-level proficiency. Adding more tools can degrade performance by increasing the risk of inappropriate tool selection. Moreover, inaccuracies in external tool outputs can negatively impact the overall response quality. While additional tools are not significantly beneficial for language-based reasoning, they can be crucial for processing non-text modalities such as financial data, medical images, and genetic data. Developing specialized tools for different data modalities could further enhance LLM reasoning capabilities, and we will explore related results in future updates.

\textbf{Delegating Tasks to LLM-Based Agents}
Distributing computational workloads across multiple LLM-based agents improves efficiency. Instead of having the main reasoning model handle all tool-related tasks (e.g., writing code or constructing a knowledge graph), or calling non-LLM tools like pure search engine or code compiler, we delegate these tasks to specialized LLM-Based Agents, like
a coding LLM generates code based on the query and context from the main reasoning model, or a knowledge-graph LLM constructs structured representations (e.g., a Mind Map) from the reasoning chain.
This approach offers two key advantages:1. Minimizing Disruptions. The main reasoning model can maintain longer, more coherent reasoning without being distracted by auxiliary tasks or exceeding token limits.
2. Leveraging Specialization. Different LLMs excel at different tasks—for instance, DeepSeek-R1 specializes in reasoning, while Claude-Sonnet excels at coding. By assigning tasks to models best suited for them, we achieve higher overall performance.

\textbf{Agentic Test-time Scaling?}
For a single question, we find reasoning chains that utilize more tool calls tend to yield better results. While across different questions, those requiring excessive tool usage often indicate inherent ambiguity or inaccuracy in the initial reasoning. This insight can be leveraged as a test-time reasoning verifier. By selecting the reasoning chain with the highest tool usage, we can implement best-of-N selection or beam search, which are techniques commonly used in mathematical and coding reasoning tasks as they can easily build a verifier, to open-domain, knowledge-intensive Q\&A, improving accuracy and robustness.
\begin{figure}[H]
    \begin{center}
    \vspace{-20pt}
    %\framebox[4.0in]{$\;$}
    \includegraphics[width=0.45\textwidth]{med_ex.png}
    \end{center}
    \vspace{-10pt}
    \caption{Case study on a complex medical decision-making problem.}\label{medex}
\end{figure}

\section{Experiments}
\begin{table}[ht]
\centering
\caption{Performance comparison on GPQA dataset across Physics, Chemistry, and Biology.}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Phy.} & \textbf{Chem.} & \textbf{Bio.}  \\
\hline
\multicolumn{4}{l}{\textit{Direct Reasoning}} \\
Qwen2.5-32B & 57.0 & 33.3 & 52.6  \\
Qwen2.5-Coder-32B & 37.2 & 25.8 & 57.9 \\
QwQ-32B & 75.6 & 39.8 & 68.4 \\
Qwen2.5-72B & 57.0 & 37.6 & 68.4  \\
Llama3.3-70B & 54.7 & 31.2 & 52.6 \\
GPT-4o$^\dagger$ & 59.5 & 40.2 & 61.6  \\
o1-preview$^\dagger$ & 89.4 & 59.9 & 65.9  \\
\hline
\multicolumn{4}{l}{\textit{Retrieve/Search in Reasoning}} \\
RAG-Qwen2.5-32B & 57.0 & 37.6 & 52.6  \\
RAG-QwQ-32B & 76.7 & 38.7 & 73.7  \\
RAgent-Qwen2.5-32B & 58.1 & 33.3 & 63.2 \\
RAgent-QwQ-32B & 76.7 & 46.2 & 68.4  \\
Search-o1 & 77.9 & 47.3 & 78.9 \\
\hline
\multicolumn{4}{l}{\textit{Agentic Reasoning}} \\
Ours & \textbf{88.1} & \textbf{58.3} & \textbf{79.6} \\
\hline
\end{tabular}\label{gpqa1}
\end{table}


\subsection{Solving Hard Problems}
We evaluate our Agentic Reasoning model on the GPQA dataset, a PhD-level multiple-choice science QA benchmark. The dataset consists of expert-authored questions spanning physics, chemistry, and biology. Our primary experiments focus on the high-quality Diamond Set, which contains 198 questions, while Table 2 presents results on the broader Extended Set of 546 questions, allowing for a direct comparison with human experts.

As shown in Table \ref{gpqa1}, our findings show that large reasoning models such as DeepSeek-R1-Lite and QwQ-32B-Preview significantly outperform traditional instruction-tuned LLMs. This demonstrates the effectiveness of chain-of-thought reasoning in solving complex, expert-level problems. Additionally, models like RAgent-QwQ-32B and Search-O1, which autonomously retrieve relevant information at reasoning, outperform non-reasoning models that simply utilize search tools. This confirms that calling tools is uniquely beneficial for enhancing reasoning accuracy.

Agentic Reasoning, which integrates external agents during reasoning, further improves performance over search-enhanced models. Our model achieves superior results on the GPQA dataset, demonstrating the power of tool-assisted reasoning in tackling expert-level challenges.

To illustrate the effectiveness of Agentic Reasoning, we also present a case study on a complex medical decision-making problem, as shown in Figure \ref{medex} The model autonomously executes code to compute the optimal $FiO_{2}$ (Fraction of Inspired Oxygen) for a patient, performs a web search to retrieve the most accurate PEEP (Positive End-Expiratory Pressure) value, and synthesizes both results to determine the best treatment plan. This example highlights how integrating coding and web search enhances the model’s ability to solve real-world medical challenges.

We further compare our model with human experts in physics, chemistry, and biology using the GPQA Extended Set. As shown in Table \ref{gpqa2}, our model surpasses human performance across all disciplines, achieving superior accuracy in all three subsets, and also outperforming human experts. These results highlight the model’s ability to handle specialized scientific reasoning tasks at an expert level.

\begin{table}[ht]
\centering
\caption{Performance comparison with human experts on the GPQA extended set.}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Phy.} & \textbf{Chem.} & \textbf{Bio.} \\
\hline
\multicolumn{4}{l}{\textit{Human Experts}} \\
Physicists & 57.9 & 31.6 & 42.0  \\
Chemists & 34.5 & 72.6 & 45.6\\
Biologists & 30.4 & 28.8 & 68.9 \\
\hline
\multicolumn{4}{l}{\textit{Reasoning Models}} \\
QwQ-32B & 61.7 & 36.9 & 61.0\\
RAG-QwQ-32B & 64.3 & 38.3 & 66.7  \\
Search-o1 & 68.7 & 40.7 & 69.5 \\
Agentic Reasoning  & 75.2 & 53.1 & 72.8 \\
\hline
\end{tabular}\label{gpqa2}
\end{table}


\subsection{Deep Research}
We conduct an evaluation of Agentic Reasoning for deep research in open-ended Q\&A tasks. A group of PhD-level experts in finance, medicine, and law were asked to formulate 15 to 30 professional research questions closely related to their respective fields. These questions were designed to require at least 20 minutes of in-depth research to answer comprehensively.

We assess the accuracy and reliability of reports generated by our Agentic Reasoning model, measuring the pass rate—the percentage of responses deemed satisfactory by domain experts. We compare this pass rate against Gemini Deep Research Service (experiments with OpenAI’s Deep Research are ongoing). As shown in Figure \ref{moretools}, our findings show that Agentic Reasoning outperforms Gemini Deep Research across all three domains, demonstrating the effectiveness of structured reasoning and tool-augmented frameworks in conducting deep research. 

\begin{figure}[h]
    \begin{center}
    %\framebox[4.0in]{$\;$}
    \includegraphics[width=0.45\textwidth]{moretools.png}
    \end{center}
    \vspace{-10pt}
    \caption{More calling for agentic tools, the better the model does. Red line denotes Gemini Deep Research}\label{moretools}
\end{figure}

\subsection{Analysis}

\subsubsection{Test-time Scaling}
In our deep research study, we find that increased tool usage improves performance on the same question. As shown in Figure \ref{moretools}, a higher number of tool calls by the reasoning model correlates with an increased pass rate in deep research tasks. However, when comparing different questions, those requiring excessive tool usage tend to indicate an inherently more challenging or ambiguous question, leading to lower accuracy. The questions with a higher number of tool calls within the same field ultimately achieve a lower pass rate.

Such observations provide a practical approach for test-time scaling. During inference-time search (running the same question multiple times), we can use the frequency of tool calls as a heuristic to select better responses. A simple implementation, such as best-of-N selection, can effectively filter out weaker outputs. This method even outperforms LLM-as-a-judge evaluation, which is more computationally expensive, time-consuming, and prone to instability.

These findings suggest a promising direction for reinforcement learning for reasoning model on knowledge-intensive fields. By leveraging agentic tool usage as an implicit reward signal, we can further optimize reasoning models for more effective tool utilization, ultimately enhancing their problem-solving capabilities.

 
\subsubsection{The Role of Mind Map}
\begin{figure}[h]
    \begin{center}
    %\framebox[4.0in]{$\;$}
    \includegraphics[width=0.48\textwidth]{tricky.png}
    \end{center}
    \vspace{-10pt}
    \caption{A tricky question that misleads most LLMs is correctly answered by us.}\label{medex}
\end{figure}

We find that Mind Maps are particularly effective in clarifying complex logical relationships, enabling the model to solve problems that often mislead traditional LLMs. We highlight two key cases where Mind Mapping maximizes its capabilities:

First, Mind Maps help correctly answer tricky logic-based questions that frequently fool LLMs. A well-known example is a modified riddle: "The surgeon, who is the boy's father, says 'I can't operate on this child, he's my son!' Who is the surgeon to the boy?" DeepSeek-R1 took 17 seconds to process this question but still produced the wrong answer, a failure also observed in models from the GPT and Gemini series models. These models often fall for a political-correct corpus contaminated response, failing to recognize the obvious logical structure. However, in our Agentic Reasoning framework, the use of a Mind Map allows the model to explicitly analyze the logical relationships between the entities [surgeon], [boy], and [father], leading to the correct answer.

Second, Mind Maps enhance deductive reasoning in strategic games. We test our approach in Werewolf, a classic social deduction game where players take on hidden roles as either villagers or werewolves. Villagers attempt to identify the werewolves, while werewolves deceive the group and eliminate players without being caught. The game alternates between "night", where werewolves secretly attack, and "day", where players debate and vote on eliminations. To evaluate our Agentic Reasoning model, we invited seven experienced Werewolf players (5+ years of experience) to play against it. The model achieved an impressive 72\% win rate, significantly exceeding both the expected statistical win rate and the performance of human players in our experiment.

We analyzed the Mind Maps generated by the Agentic Reasoning model over multiple rounds of play, as shown in Figure \ref{werewolf}. These visual structures helped the model track the relationships between different players based on their spoken arguments, allowing it to more accurately identify deception strategies, anticipate voting behaviors, and optimize its own disguise tactics. This result demonstrates that Mind Mapping is not just a tool for logic puzzles but also a powerful strategy enhancer in dynamic reasoning environments.

\begin{figure}[H]
    \begin{center}
    %\framebox[4.0in]{$\;$}
    \includegraphics[width=0.48\textwidth]{werewolf.png}
    \end{center}
    \vspace{-10pt}
    \caption{Mind Map in playing werewolf game. The first round and the second round.}\label{werewolf}
\end{figure}


\section{Conclusion}
We introduced Agentic Reasoning, a framework that enhances LLM reasoning by integrating external agents for structured memory (Mind Map), web search, and computational analysis. This approach improves logical coherence, factual accuracy, and deep research capabilities. Our evaluations show that Agentic Reasoning outperforms existing models on expert-level QA and real-world research tasks, demonstrating its ability to synthesize knowledge effectively. The structured use of external tools enables more interpretable and verifiable reasoning, paving the way for AI systems capable of expert-level problem-solving. Future work will explore extending this framework to multimodal data and real-time adaptability, further advancing AI’s ability to tackle complex, real-world challenges.


\bibliography{acl_latex}

\end{document}
