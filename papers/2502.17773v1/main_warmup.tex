\section{Warm-up: Simulation of Binary Responses}\label{sec-warmup}

To motivate our problem and methodology, we will start with a simple setting where an LLM simulates binary responses to a survey question. In \Cref{sec-general}, we will present the general problem setup and the general methodology.

\subsection{Motivating Example: Educational Test}\label{sec-example-education}

Suppose a school wants to estimate the proportion $\mu \in [0,1]$ of students that can answer a newly designed test question correctly. It will not only provide insights into student progress but also evaluate the question's effectiveness in differentiating among students with varying levels of understanding. Such information can guide the school in tailoring teaching strategies to better address student needs.

The most direct approach is to give the test to $n$ students and collect their results $y_1,...,y_n\in\{0,1\}$, where $y_i$ indicates whether student $i$ answers the question correctly. A point estimate for $\mean$ is the sample mean $\responsebar = \frac{1}{n} \sum_{i=1}^n \response_i$. Given $\alpha\in(0,1)$, we can construct a confidence interval for $\mean$:
\begin{equation}\label{eqn-CI-intro-standard}
\left[ \responsebar - \frac{\samplesd}{\sqrt{n}} \Phi^{-1}\left( 1- \frac{\alpha}{2} \right) , ~  \responsebar + \frac{\samplesd}{\sqrt{n}} \Phi^{-1}\left( 1 - \frac{\alpha}{2} \right) \right],
\end{equation}
where $\samplesd = \sqrt{\responsebar (1-\responsebar) }$ is the sample standard deviation, and $\Phi$ is the cumulative distribution function (CDF) of $\Normal(0,1)$. By the Central Limit Theorem (CLT), this interval has asymptotic coverage probability $1-\alpha$ as $n\to\infty$. As a different approach, one can also use Hoeffding's concentration inequality (e.g., Theorem 2.8 in \cite{BLM13}) to construct a finite-sample confidence interval
\begin{equation}
\left[ \responsebar - \sqrt{\frac{\log(2/\alpha)}{2n}}  , ~  \responsebar + \sqrt{\frac{\log(2/\alpha)}{2n}} \right],
\end{equation}
which has at least $(1-\alpha)$ coverage probability for every $n\in\ZZ_+$. For simplicity, we will stick to \eqref{eqn-CI-intro-standard} in this section.

Alternatively, the school may use an LLM to simulate students' responses to the question. Compared with directly testing on real students, this approach is more time-efficient and cost-saving. If we prompt the LLM $k$ times with random student profiles, then it generates $k$ synthetic responses, which leads to synthetic outcomes $\simresponse_1,...,\simresponse_k\in\{0,1\}$. We may also compute the sample mean $\simresponsebar_k = \frac{1}{k} \sum_{i=1}^k \simresponse_i$ and the CLT-based confidence interval
\begin{equation}\label{eqn-CI-intro-sim}
\simCI(k) =  \bigg[ \simresponsebar_k - \frac{c\cdot \simsamplesd_k}{\sqrt{k}} \Phi^{-1}\left( 1- \frac{\alpha}{2} \right) , 
~
\simresponsebar_k + \frac{c\cdot \simsamplesd_k}{\sqrt{k}} \Phi^{-1}\left( 1 - \frac{\alpha}{2} \right) \bigg],
\end{equation}
where $\simsamplesd_k = \sqrt{\simresponsebar_k (1-\simresponsebar_k)}$, and $c>1$ is a scaling parameter. Such a dilation by $c$ is necessary; without it, whenever the LLM-generated data deviates from the student population (even by the slightest amount), the interval $\simCI(k)$ may never achieve $(1-\alpha)$ coverage regardless of $k$. We give an example in \Cref{sec-impossibility-exact-CLT}.

Due to the misalignment between the LLM and students, the distribution of the synthetic data $\{\simresponse_i\}_{i=1}^k$ may be very different from the true response distribution. In this case, the sample mean $\simresponsebar$ can be a poor estimate of 
$\mean$, and $\simCI(k)$ is generally not a valid confidence interval for $\mean$. In particular, as $k\to\infty$, the interval concentrates tightly around the synthetic mean $\EE [\simresponse_1]$ and fails to cover the true mean $\mean$. When $k$ is small, the interval becomes too wide to be informative, even though it may cover $\mean$ with high probability. We provide an illustration in \Cref{fig-tradeoff}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.6]{figures/tradeoff.pdf}
\caption{The coverage-width trade-off for the simulation sample size $k$. The true distribution is $\Ber(0.4)$ and the synthetic distribution is $\Ber(0.6)$. The red dotted horizontal line plots the true mean $\mean=0.4$. The blue line plots the sample mean $\simresponsebar_k$ of the synthetic data, and the blue shaded region visualizes the confidence interval $\simCI(k)$, for $k\in[40]$. When $k$ is too small (say $k\le 6$), the interval $\simCI(k)$ is too wide. When $k$ is too large (say $k\ge 18$), interval $\simCI(k)$ fails to cover $\mean$.}\label{fig-tradeoff}
\end{figure}

\paragraph{Main insights.} Our goal in this work is to develop a principled approach for choosing a good simulation sample size $\widehat{k}$, so that $\simCI( \widehat{k} )$ is a valid confidence interval for $\mean$ while having a modest width. Solving this problem has the following important implications. 
\begin{enumerate}
\item The choice of $\widehat{k}$ offers valuable information for future simulation tasks on the appropriate number of synthetic samples to generate, so as to produce reliable confidence intervals. It also helps avoid generating excessive samples and improves computational efficiency.
\item The width of $\simCI( \widehat{k} )$ provides an assessment of the alignment between the LLM and the human population. A wide confidence interval indicates high uncertainty of its estimate of the true $\mean$, and thus a large gap between the synthetic data distribution and the true population.
\item The sample size $\widehat{k}$ also reflects the richness of the human population captured by the LLM. We make an analogy using the classical theory of parametric bootstrap. If a model is trained via maximum likelihood estimation over $k$ i.i.d.~human samples, then when performing bootstrap for uncertainty quantification, the bootstrap sample size is usually set to be $k$. In this regard, our approach can be thought of as uncovering the size of human population that the LLM can represent:
\begin{center}
	\emph{Using the LLM to simulate survey responses is roughly as accurate as surveying $\widehat{k}$ real people.}
\end{center}
We provide an illustration in \Cref{fig-LLMTurk}. 
The larger $\widehat{k}$ is, the more diversity that the LLM appears to capture. In contrast, a small $\widehat{k}$ could imply the peculiarity of the LLM compared to the major population. %Hence, $\widehat{k}$ gauges the alignment between the LLM and the human population.
\end{enumerate}

\begin{remark}[Comparison with existing works]
Existing works typically measure LLM misalignment using integral probability metrics and $f$-divergences \citep{SDL23, DHM24, DNL24}, which do not carry operational meanings themselves and at times can be hard to interpret. In contrast, our simulation sample size $\widehat{k}$ is scale-free and has a clear operational meaning.
\end{remark} 

\begin{figure}
\centering
\includegraphics[scale=0.15]{figures/LLMTurk}
\caption{An interpretation of the simulation sample size $k$ as the size of human population that an LLM can represent. Generating outputs from the LLM can be thought of as ``resampling'' from $k$ human samples that make up the LLM. The figure is generated by DALL-E \citep{RPG21}, and borrows ideas from the \emph{Mechanical Turk}, a chess-playing machine from the 18th century with a human player hidden inside.}\label{fig-LLMTurk}
\end{figure}


\subsection{Methodology for Selecting the Simulation Sample Size}\label{sec-method-1D}

We now introduce our method for choosing a good simulation sample size $\widehat{k}$. It makes use of similar test questions for which real students' results are available. If such data is available, we can compare LLM simulations with real students' results on these questions, and use it to guide the choice of $k$. 

Specifically, we assume access to $m$ test questions similar to the question of interest. For example, they can come from previous tests or a question bank. For $j\in[m]$, the $j$-th test question has been tested on $n_j$ real students, with test results $\dataset_j = \{ \response_{j,i} \}_{i=1}^{n_j}$. We also simulate LLM responses $\simdataset_j = \{ \simresponse_{j,i} \}_{i=1}^K$ to the $j$-th test question, and $\simdataset = \{ \simresponse_{i} \}_{i=1}^K$ to the new test question. Here $K\in\ZZ_+$ is the simulation budget.

For each question $j\in[m]$, we form confidence intervals similar to \eqref{eqn-CI-intro-sim} using the synthetic data $\simdataset_j$, aiming to cover the true proportion $\mean_j$ of students that can answer the $j$-th question correctly:
\begin{equation}\label{eqn-CI-intro-sim-calibrate}
\simCI_j(k) = \bigg[ \simresponsebar_{j,k} - \frac{c\cdot \simsamplesd_{j,k}}{\sqrt{k}} \Phi^{-1}\left( 1- \frac{\alpha}{2} \right) , 
~  \simresponsebar_{j,k} + \frac{c\cdot \simsamplesd_{j,k}}{\sqrt{k}} \Phi^{-1}\left( 1 - \frac{\alpha}{2} \right) \bigg], 
\end{equation}
where $\simresponsebar_{j,k} = \frac{1}{k} \sum_{i=1}^k \simresponse_{j,i}$ is the sample mean of the first $k$ samples in $\simdataset_j$, and $\simsamplesd_{j,k} = \sqrt{\simresponsebar_{j,k} ( 1 - \simresponsebar_{j,k} )}$ is the estimated standard deviation. We also set the convention $\simCI(0) = \simCI_j(0) = \RR$, as nothing can be said about the true parameter without data. We will pick $\widehat{k}\in\{0,1...,K\}$ such that $\simCI_j(\widehat{k})$ covers $\mean_j$ with high probability. We expect this choice of $\widehat{k}$ to be also good for $\simCI(k)$, as the test questions are similar.

Ideally, we would like to pick $k$ such that $(1-\alpha)$-coverage is achieved empirically over the $m$ test questions:
\begin{equation}\label{eqn-oracle-criterion-1D}
\frac{1}{m} \sum_{j=1}^m \ind \{ \mean_j \not\in \simCI_j(k) \} \le \alpha.
\end{equation}
As the true $\{\mean_j\}_{j=1}^m$ are not available, we use the real data $\{ \dataset_j \}_{j=1}^m$ to compute the sample means $\responsebar_j = \frac{1}{n_j} \sum_{i=1}^{n_j} \response_{j,i}$ as proxies for $\mean_j$. The empirical miscoverage can be approximated by
\begin{equation}\label{eqn-proxy-1D}
\coverage(k) = \frac{1}{m} \sum_{j=1}^m \ind \{ \responsebar_j \not\in \simCI_j(k) \}.
\end{equation}
Our criterion for selecting $k$ is given by
\begin{equation}\label{eqn-empirical-criterion-1D}
\widehat{k} = \max \left\{ 0\le k \le K : \coverage(i) \le \alpha/2 ~~\forall i\le k \right\}.
\end{equation}
Note that $\widehat{k}$ is well-defined because $\coverage(0) = 0$.

The choice of the threshold $\alpha/2$ in \eqref{eqn-empirical-criterion-1D} can be explained as follows. By CLT, when $n_j$ is large, $\PP( \mean_{j} \ge \responsebar_j ) \approx 1/2$. Suppose $\mean_{j} \not\in \simCI_j(k)$, then  $\mean_{j}$ is either on the left or the right of $\simCI_j(k)$. In the former case, $\responsebar_j$ is on the left of $\mean_{j}$ with probability around $1/2$, which implies $\responsebar_j \not\in \simCI_j(k)$. Similarly, in the latter case, $\responsebar_j$ is on the right of $\mean_{j}$ with probability around $1/2$, and then $\responsebar_j \not\in \simCI_j(k)$. Roughly speaking, the frequency of having $\responsebar_j \not\in \simCI_j(k)$ is at least half of the frequency of having $\mean_{j} \not\in \simCI_j(k)$. In other words, the lower bound
\begin{equation}\label{eqn-proxy-to-oracle-1D}
G(k) \geq \frac{1}{2} \cdot \frac{1}{m} \sum_{j=1}^m \ind \{ \mean_{j} \not\in \simCI_j(k) \} 
\end{equation}
approximately holds. Substituting \eqref{eqn-proxy-to-oracle-1D} into \eqref{eqn-oracle-criterion-1D} yields the threshold $\alpha/2$ for choosing $\widehat{k}$. 


\subsection{Theoretical Analysis}\label{sec-theory-1D}

In this section, we present a theoretical analysis of our proposed method. To do so, we first describe the setup in \Cref{sec-example-education} and \Cref{sec-method-1D} in mathematical terms.

The student population can be represented by a distribution $\distribution$ over a space $\profilespace$ of possible \emph{student profiles}, say, vectors of background information, classes taken, grades, etc. To simulate student responses from the LLM, synthetic student profiles are generated from a synthetic student population $\simdistribution$ over $\profilespace$, and then fed to the LLM.

We use $\testfunction$ and $\{ \testfunction_j \}_{j=1}^m$ to refer to the test question of interest and the $m$ similar ones, respectively. 
Students' performance on test questions are characterized by a \emph{performance function} $\performancefunction$: a student with profile $\profile\in\profilespace$ answers a question $\testfunction$ correctly with probability $\performancefunction ( \profile , \testfunction ) \in [0, 1]$. The average student performance on the test questions $\testfunction$ and $\{ \testfunction_j \}_{j=1}^m$ are then $\mean = \EE_{\profile \sim \distribution } \performancefunction ( \profile ,  \testfunction ) $ and $\mean_j = \EE_{\profile \sim \distribution } \performancefunction (  \profile, \testfunction_j ) $, respectively.
In addition, the LLM generates synthetic student performance from a \emph{synthetic performance function} $\simperformancefunction$: when prompted with a synthetic profile $\simprofile\in\profilespace$, the LLM answers a question $\testfunction$ correctly with probability $\simperformancefunction ( \simprofile , \testfunction ) \in [0, 1]$. 

The collection of the real dataset $\dataset_j = \{ \response_{j,i} \}_{i=1}^{n_j}$ can be thought of as drawing $n_j$ i.i.d.~student profiles $\{ \profile_{j,i} \}_{i=1}^{n_j} \sim \distribution$ and then sampling $\response_{j,i} \sim \Bernoulli (  \performancefunction ( \profile_{j,i} , \testfunction_j  ) )$ for each $i\in[n_j]$. Similarly, the generation of the synthetic dataset $\simdataset_j = \{ \simresponse_{j,i} \}_{i=1}^{K}$ can be thought of as drawing i.i.d.~synthetic profiles $\{ \simprofile_{j,i} \}_{i=1}^{K} \sim \simdistribution$ and then sampling $\simresponse_{j,i} \sim \Bernoulli ( 
\simperformancefunction
( \simprofile_{j,i} , \testfunction_j  ) )$ for each $i\in [K]$. For  $\simdataset = \{ \simresponse_i \}_{i=1}^K$, we adopt a similar notation $\{ \simprofile_i \}_{i=1}^K$ for the synthetic profiles. We note that when collecting real or synthetic samples, the performance functions never appear explicitly. They are introduced only to facilitate the problem formulation.

Finally, we assume that the test questions are drawn randomly from a question bank, and that the datasets are independent.

\begin{assumption}[Randomly sampled questions]\label{assumption-iid-test-1D}
The questions $\testfunction,\testfunction_1,...,\testfunction_m$ are independently sampled from a distribution over a space $\testfunctionfamily$.
\end{assumption}

\begin{assumption}[Independent data]\label{assumption-indep-data-1D}
For each $j\in[m]$, conditioned on $\testfunction_j$, the datasets $\dataset_j$ and $\simdataset_j$ are independent. Conditioned on $\testfunction_1,...,\testfunction_m$, the dataset tuples $(\dataset_1,\simdataset_1),...,(\dataset_m,\simdataset_m)$ are independent. Finally, $(\testfunction,\simdataset)$ is independent of $\big\{ (\testfunction_j,\dataset_j,\simdataset_j) \big\}_{j=1}^m$.
\end{assumption}

We are now ready to state the theoretical guarantee of our approach. Its proof is deferred to \Cref{sec-thm-coverage-1D-proof}. We note that the assumption $\PP( \mean_{j} \ge \responsebar_j \mid \testfunction_j ) = 1/2$ is a CLT approximation and is for mathematical convenience only.

\begin{theorem}[Coverage guarantee]\label{thm-coverage-1D}
Let Assumptions \ref{assumption-iid-test-1D} and \ref{assumption-indep-data-1D} hold. Assume that $\PP( \responsebar_j \leq \mean_{j} \mid \testfunction_j ) = 1/2$ for each $j\in[m]$. Fix $\alpha\in(0,1)$. Then the simulation sample size $\widehat{k}$ defined by \eqref{eqn-empirical-criterion-1D} satisfies
\[
\PP\Big( \mean \in \simCI(\widehat{k}) \Big) \ge 1-\alpha - \sqrt{\frac{2}{m}}.
\]
The probability is taken with respect to randomness of $\big\{ ( \testfunction_j , \dataset_j, \simdataset_j ) \big\}_{j=1}^m$, $\testfunction$ and $\simdataset$.
\end{theorem}

On average, the chosen simulation sample size $\widehat{k}$ leads to a confidence interval $\simCI(\widehat{k})$ that covers the true mean $\mean$ with probability at least $1-\alpha-O(\sqrt{1/m})$. As $m\to\infty$, the aforementioned lower bound converges to $1-\alpha$.


\subsection{Sharpness of Sample Size Selection}

We have seen that the chosen interval $\simCI(\widehat{k})$ has good coverage properties. In this section, we complement this result by showing that the interval is not overly conservative. To simplify computation, we slightly modify the setting.

\begin{example}[Gaussian performance score]\label{example-gaussian}
Consider the setting in \Cref{sec-theory-1D} with the following modifications. On a test question $\testfunction \in\testfunctionfamily$, the performance (e.g., score) of a real student with profile $\profile$ follows the distribution $N( \performancefunction ( \profile , \testfunction ),1)$ instead of $\Bernoulli ( F ( \profile , \testfunction ) )$; similarly, the performance of the LLM prompted with synthetic profile $\simprofile$ has distribution $N( \simperformancefunction (  \simprofile, \testfunction ),1)$. Moreover, the confidence intervals $\simCI(k)$ defined in \eqref{eqn-CI-intro-sim} and $\simCI_j(k)$ defined in \eqref{eqn-CI-intro-sim-calibrate} are changed to
\begin{align*}
& \simCI(k) = \left[ \simresponsebar_k - \frac{C}{\sqrt{k}},~ \simresponsebar_k + \frac{C}{\sqrt{k}} \right], \\[4pt]
& \simCI_j(k) = \left[ \simresponsebar_{j,k} - \frac{C}{\sqrt{k}},~ \simresponsebar_{j,k} + \frac{C}{\sqrt{k}} \right],
\end{align*}
respectively, where $C = 2 \Phi^{-1} (1 - \alpha / 4)$. For simplicity, we suppose that the real datasets have the same size: $n_j=n$ for all $j\in[m]$. Finally, we define
\[
\Delta = \sup_{\testfunction \in \testfunctionfamily} \big| \EE_{\profile \sim \distribution} \performancefunction ( \profile , \testfunction ) - \EE_{\simprofile \sim \simdistribution} \simperformancefunction ( \simprofile , \testfunction ) \big|.
\]
\end{example}

In \Cref{example-gaussian}, the quantity $\Delta$ measures the discrepancy between the distributions of the real students' performance and of the simulated students' performance. The following theorem presents a lower bound on the chosen simulation sample size $\widehat{k}$. Its proof can be found in \Cref{sec-thm-sharpness-1D-proof}.

\begin{theorem}[Sharpness of chosen sample size]\label{thm-sharpness-1D}
Consider the setting of \Cref{example-gaussian}. Let $\widehat{k}$ be chosen by the procedure \eqref{eqn-empirical-criterion-1D}. Choose $\delta \in (0, 1)$. There exists a constant $C'>0$ determined by $\alpha$ such that when $m  > C' \log(n / \delta)$, the following holds with probability at least $1 - \delta$:
\[
\widehat{k} \ge \min \left\{
\left(
\frac{C}{5 \Delta}
\right)^2,
n,  K
\right\}.
\]
When this happens, the selected confidence interval $\simCI(\widehat{k})$ has width $O\left( \max\{\Delta, n^{-1/2}, K^{-1/2} \} \right)$.
\end{theorem}

\Cref{thm-sharpness-1D} implies that the chosen $\widehat{k}$ is the largest possible, or equivalently, the interval $\simCI(\widehat{k})$ is the shortest possible. To see this, suppose that the simulation budget $K$ is large. When $\Delta$ is small, there is little distribution shift between the real and simulated student performance, so it is optimal to use as many synthetic samples as possible. \Cref{thm-sharpness-1D} shows that in this case, the procedure \eqref{eqn-empirical-criterion-1D} chooses $\widehat{k}=n$ with high probability. This is the maximum effective sample size, since the $n$ real data points can identify the true mean only up to an error of $O(n^{-1/2})$ and thus there is no statistical benefit in simulating more than $n$ samples. On the other hand, when the discrepancy $\Delta$ between the real and simulated student performance is large, any $\simCI(k)$ that covers the true mean with high probability must have width $\Omega(\Delta)$. Thus, the chosen confidence interval $\simCI(\widehat{k})$, which has width $O(\Delta)$ with high probability, is the shortest possible.

