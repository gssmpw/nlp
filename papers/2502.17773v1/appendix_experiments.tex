\section{Details of Numerical Experiments}\label{sec-appendix-experiments}


\subsection{The OpinionQA Dataset}\label{sec-opinion}

\paragraph{Selection of survey questions.} The original dataset is categorized into topics such as health, crime/security, and political issues. Ideally, we would want to consider questions from the same category to ensure that they are similar enough. However, the category with the most questions has fewer than 200 questions, which is not suited for demonstration. We thus consider all questions. In total, the dataset has 1,442 survey questions, which is too large for our computational resources. We thus selected a subset of questions. First, while the number of choices ranges from 2 to 19, most questions have 5 choices. To give a fair comparison and for simplicity, we therefore only consider questions with 5 choices. Second, not all questions have choices that can be clearly ordered in sentiments; the following question is an example:
\begin{quote}
\textit{Who do you think has the most responsibility to reduce the amount of made-up news and information? 1. The government, 2. Technology companies, 3. The public, 4. The news media, 5. None of these, 6. Refused.}
\end{quote}
In contrast, all example questions in the next section have choices that can be clearly ordered in sentiments. To streamline the process, we ask GPT-4o to determine if a question's choices can be ordered in sentiments and we keep those that have GPT-4o's affirmative answer. This leaves us with 546 questions. To compensate for the loss of similarity by pooling questions across various topics and to further reduce our computational cost, we selected 400 questions that are ``most similar'' to each other by embedding the question statements using OpenAI's \texttt{text-embedding-3-small}, calculating the mean, and selecting the 400 questions with the smallest Euclidean distance to the mean. Out of these 400 questions, 15 questions have various issues with their choices by manual inspection, so we exclude them. This leaves us with 385 questions. All these questions happen to have at least 400 responses.

\paragraph{Example questions.} The questions in the OpinionQA dataset span a wide range of topics, including health, crime/security, and political issues. Some example questions are as follows:
\begin{itemize}
\item \textit{How much, if at all, do you think wages and incomes are contributing to your opinion about how the economy is doing? 
\begin{center}
1.~A great deal \quad 2.~A fair amount \quad 3.~Not too much \quad 4.~Not at all \quad 5.~Refused
\end{center}}
\item \textit{Regardless of whether you would want to move, how likely is it that you will move to a different community at some point in the future?
\begin{center}
1.~Very likely \quad 2.~Somewhat likely \quad 3.~Not too likely \quad 4.~Not at all likely \quad 5.~Refused
\end{center}}
\item \textit{How much, if anything, would you be willing to change about how you live and work to help reduce the effects of global climate change? Would you be willing to make:
\begin{center}
1.~A lot of changes \,  2.~Some changes \,  3.~Only a few changes \, 4.~No changes at all \,  5.~Refused
\end{center}}
\end{itemize}

\paragraph{Profiles.} Excluding surveyees with missing information, each of the 385 questions we consider has at least 400 responses. Since there was no information on the surveyees' identification, by dropping repeated profiles we can only say that there are at least 32,864 surveyees. Each surveyee is described by 12 features. Their corresponding categories are listed in \Cref{tab:opinionprofilefeatures}.
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|}
\hline
Feature & Options \\
\hline
US citizenship &  `Yes', `No' \\
\hline
Region & `Northeast', `Midwest', `South', `West' \\
\hline
Sex & `Male', `Female' \\
\hline
Age & `18-29', `30-49', `50-64', `65+' \\
\hline
\makecell{Marital \\ status} & \makecell{`Married', `Divorced', `Separated', \\ `Widowed', `Never been married'} \\
\hline
Race & `White', `Black', `Asian', `Hispanic', `Other' \\
\hline
\makecell{Educational\\background} & \makecell{`Less than high school', `High school graduate', \\ `Some college, no degree', `Associate's degree', \\ `College graduate/some postgrad', `Postgraduate'}\\
\hline
Income & \makecell{`Less than \$30,000', `\$30,000-\$50,000', `\$50,000-\$75,000', \\ `\$75,000-\$100,000', `\$100,000 or more'} \\
\hline
\makecell{Religious\\ affiliation} & \makecell{`Protestant', `Roman Catholic', `Mormon', `Orthodox', \\ `Jewish', `Muslim', `Buddhist', `Hindu', \\ `Atheist', `Agnostic', `Other', `Nothing in particular'} \\
\hline
\makecell{Religious \\ attendance} & \makecell{`More than once a week', `Once a week', `Once or twice a month', \\ `A few times a year', `Seldom', `Never'} \\
\hline
Political party & `Republican', `Democrat', `Independent', `Other' \\
\hline
\makecell{Political \\ ideology} & \makecell{`Very conservative', `Conservative',\\ `Moderate', `Liberal', `Very liberal'} \\
\hline
\end{tabular}
\caption{Categories of surveyees' features in the OpinionQA dataset.}
\label{tab:opinionprofilefeatures}
\end{table}

\paragraph{Synthetic response generation.} We generate synthetic profiles by bootstrapping the 32,864 unique real profiles. We then generate synthetic answers by prompting LLMs to pretend that they are a surveyee with the synthetic profile and answer the question. An example prompt is as follows:
\begin{quote}
\textit{Pretend that you reside in the US and you are a US citizen from the West region of the country. You are female, your age is between 18 and 29, and you are single. In terms of race, you are white. In terms of education, you attended college but did not graduate. Your annual income is less than \$30,000. Religion-wise, you do not belong to any particular religion, and you never attend religious services. Politically, you are affiliated with a political party that is not Democratic or Republican, and you consider your political ideology to be liberal. Please answer the following question}: 

\textit{How much, if at all, do you think what happens to black people in the country overall affects what happens in your own life? [`1. A lot', `2. Some', `3. Not much', `4. Not at all', `5. Refused'].}

\textit{Please provide your answer choice (a single number from 1 to 5) in double square brackets.}
\end{quote}
While LLMs generally provide explanations for the math questions in EEDI dataset, they usually directly provide answers for the OpinionQA dataset; e.g., `[[2]]'.


\subsection{The EEDI Dataset}\label{sec-eedi}

\paragraph{Example questions.} Some example questions from the EEDI dataset are as follows:
\begin{itemize}
\item \textit{What number belongs in the box? $\square + 7 = 2$
\[
\text{A) 9} \quad \text{B) -5} \quad \text{C) -6} \quad \text{D) 5}
\]
}
\item \textit{If you multiply a square number by $9$, you get a square number. Is this statement:
\[
\text{A) always true} \quad \text{B) sometimes true} \quad \text{C) never true} \quad \text{D) impossible to say}
\]}
\item \textit{Which calculation is equal to $-20$?
\[
\text{A) } 2 \times (-2) - (-4) \times 4 \quad \text{B) } -28 - (-4) \times 2 \quad \text{C) } (-5)\top 2 + 5 \quad \text{D) } (-42) \div (-2) + 1
\]}
\end{itemize}

\paragraph{Profile distribution.} Excluding students with missing information which take up less than 10\% of the total population, there are 2,111 students who answered at least one of the 412 questions. Each student is described by three features: gender, age, and whether or not they are eligible for free school meals or premium pupil. Gender is represented by 1 or 2, where 1 corresponds to female and 2 corresponds to male. The students' ages are rounded to integers from 11 and 14. Whether or not a student is eligible for free school meals is represented by 0 or 1, where 0 corresponds to not eligible and 1 corresponds to eligible. The distribution of these students' features is presented in \Cref{tab:eediprofiledist}.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
 & min & max & mean & median & standard deviation \\
\hline
Gender & 1 & 2 & 1.4988 & 1 & 0.5001 \\
\hline
Age & 11 & 14 & 11.2776 & 11 & 0.4696 \\
\hline
Premium Pupil & 0 & 1 & 0.2842 & 0 & 0.4512 \\
\hline
\end{tabular}
\caption{Summary statistics of students' features in the EEDI dataset.}\label{tab:eediprofiledist}
\end{table}

\paragraph{Synthetic response generation.} For each question, we generate synthetic profiles by sampling with replacement from the real profiles. We then generate synthetic answers by prompting LLMs to pretend that they are a student with the synthetic profile and answer the question. We adapted the prompt from \cite{HMG24} with slight modifications to reduce computational cost. An example prompt featuring an 11-year-old boy who is not eligible for free school meals is as follows:
\begin{quote}
\textit{Pretend that you are an 11-year-old student. Your gender is male. You are not eligible for free school meals or pupil premium due to being relatively financially advantaged. Given your characteristics, is it likely that you would be able to solve the following problem?}

\textit{Problem: [Insert question here]}

\textit{If yes, put the final answer choice (a single letter) in double square brackets. If you are likely to struggle with this problem, put a plausible incorrect answer choice (a single letter) in double square brackets.}
\end{quote}
An example answer from GTP-4o when given the second example question above is as follows:
\begin{quote}
\textit{As an 11-year-old student, I might have learned about square numbers and multiplication in school. However, the problem may be a bit tricky if I haven't thought about how multiplying square numbers by other numbers can also result in square numbers. I might not immediately realize that 9 is actually a square number itself (3 squared), which makes this property more evident.}

\textit{Considering this, I could find the reasoning challenging and decide based on a misconception. I might go with a plausible incorrect answer choice like [[B]] because I might think that it's only sometimes possible without realizing the full mathematical principle involved.}
\end{quote}


\subsection{Visualization of Simulation Sample Size Selection}
In \Cref{fig:k-upcross-eg}, we visualize the process of selecting the simulation sample size $\widehat{k}$: it is the first $k$ at which $\{L(k)\}_{k=1}^K$ up-crosses the threshold $\gamma\alpha$.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{figures/k_upcross.pdf}
\caption{Visualization of simulation sample size selection for the OpinionQA dataset. Here $\alpha=0.1$, $\gamma=0.5$, $K=100$, $c = \sqrt{2}$ and $M=2$. The red dotted horizontal line is threshold $\gamma \alpha = 0.05$. The blue line is $\{L(k)\}_{k=1}^K$. Our approach chooses $\widehat{k}$ as the point at which the blue line first up-crosses the red threshold: $\widehat{k}=57$.}
\label{fig:k-upcross-eg}
\end{figure}


\section{Additional Experiment Results}\label{sec-appendix-experiments-results}

In this section, we provide additional experiment results for three performance metrics: the miscoverage probability proxy $\widetilde{L}(\widehat{k})$ as defined in \eqref{eqn:proxy}, the selected simulation sample size $\widehat{k}$, and the half-width of the synthetic confidence interval $\simCIalt(\widehat{k})$.


\subsection{Miscoverage Probability Proxy}\label{sec-experiments-miscoverage}

In \Cref{tab:opinion-miscoverage} and \Cref{tab:eedi-miscoverage}, we present the $p$-values for the hypothesis test $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ against $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] > \alpha$ for various LLMs and $\alpha$ over the OpinionQA and EEDI datasets, respectively. In \Cref{tab:opinion-miscoverage-mean-stderror} and \Cref{tab:eedi-miscoverage-mean-stderror}, we present the means and standard errors of $\widetilde{L}(\widehat{k})$ as defined by \eqref{eqn:proxy} over 100 random splits. They complement \Cref{fig:combined}.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
GPT-3.5-turbo & 0.0618 & 0.1293 & 0.1996 & 0.2245 & 0.6981 & 0.3460 & 0.3387 & 0.2591 & 0.7846 & 0.8430 \\
\hline
GPT-4o-mini & 0.1619 & 0.1642 & 0.0658 & 0.2352 & 0.4741 & 0.4363 & 0.2000 & 0.1150 & 0.5717 & 0.3812 \\
\hline
GPT-4o & 0.3679 & 0.3218 & 0.4910 & 0.2243 & 0.5837 & 0.4379 & 0.2329 & 0.3388 & 0.9686 & 0.9226 \\
\hline
Claude 3.5 Haiku & 0.3857 & 0.5889 & 0.5945 & 0.4558 & 0.7030 & 0.6029 & 0.7370 & 0.8853 & 0.9922 & 0.9791 \\
\hline
Llama-3-8B & 0.7571 & 0.4376 & 0.2609 & 0.6072 & 0.5689 & 0.4178 & 0.7018 & 0.6132 & 0.7391 & 0.5169 \\
\hline
Llama 3.3 70B & 0.5505 & 0.6519 & 0.7687 & 0.5936 & 0.7055 & 0.7207 & 0.6050 & 0.3530 & 0.6973 & 0.5274 \\
\hline
Mistral 7B & 0.6249 & 0.0505 & 0.0517 & 0.0333 & 0.6473 & 0.5988 & 0.5394 & 0.2119 & 0.3380 & 0.3426 \\
\hline
DeepSeek-V3 & 0.0686 & 0.0969 & 0.0841 & 0.1513 & 0.5075 & 0.2257 & 0.1678 & 0.2638 & 0.3457 & 0.3024 \\
\hline
Random & 0.7779 & 0.6214 & 0.7527 & 0.6586 & 0.9570 & 0.9903 & 0.9321 & 0.7984 & 0.9259 & 0.9783 \\
\hline
\end{tabular}
}
\caption{$p$-values for the hypothesis test $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ against $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] > \alpha$ for various LLMs and $\alpha$ over the OpinionQA dataset. Note that the $\alpha$ values specify the experiment configuration and are not the significance levels of the hypothesis test.}
\label{tab:opinion-miscoverage}
\end{table}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
GPT-3.5-turbo & 0.9988 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
\hline
GPT-4o-mini & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
\hline
GPT-4o & 0.1832 & 0.5820 & 0.9998 & 0.9732 & 0.9938 & 0.9807 & 0.9292 & 0.8259 & 0.9757 & 0.9922 \\
\hline
Claude 3.5 Haiku & 0.8656 & 0.9999 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
\hline
Llama 3.3 70B & 0.9575 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
\hline
Mistral 7B & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
\hline
DeepSeek-V3 & 0.9444 & 0.9789 & 0.9789 & 0.9981 & 1.0000 & 0.9992 & 1.0000 & 1.0000 & 1.0000 & 1.0000 \\
\hline
Random & 0.9800 & 1.0000 & 1.0000 & 0.9997 & 0.9954 & 0.9895 & 0.9979 & 0.9997 & 0.9920 & 0.9990 \\
\hline
\end{tabular}
}
\caption{$p$-values for the hypothesis test $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ against $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] > \alpha$ for various LLMs and $\alpha$ over the EEDI dataset. Note that the $\alpha$ values specify the experiment configuration and are not the significance levels of the hypothesis test.}
\label{tab:eedi-miscoverage}
\end{table}

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
\makecell{GPT-3.5\\-turbo} & \makecell{0.0547\\(0.003)} & \makecell{0.1051\\(0.0045)} & \makecell{0.1549\\(0.0059)} & \makecell{0.2051\\(0.0067)} & \makecell{0.2461\\(0.0075)} & \makecell{0.3032\\(0.0082)} & \makecell{0.3536\\(0.0087)} & \makecell{0.4056\\(0.0086)} & \makecell{0.4432\\(0.0086)} & \makecell{0.4909\\(0.009)} \\
\hline
\makecell{GPT-4o\\-mini} & \makecell{0.0535\\(0.0036)} & \makecell{0.1052\\(0.0053)} & \makecell{0.1588\\(0.0059)} & \makecell{0.2052\\(0.0072)} & \makecell{0.2505\\(0.008)} & \makecell{0.3014\\(0.0089)} & \makecell{0.3574\\(0.0088)} & \makecell{0.4104\\(0.0087)} & \makecell{0.4484\\(0.0086)} & \makecell{0.5027\\(0.009)} \\
\hline
GPT-4o & \makecell{0.051\\(0.0031)} & \makecell{0.1022\\(0.0048)} & \makecell{0.1501\\(0.0058)} & \makecell{0.2049\\(0.0065)} & \makecell{0.2486\\(0.0068)} & \makecell{0.3012\\(0.0075)} & \makecell{0.3555\\(0.0075)} & \makecell{0.4031\\(0.0075)} & \makecell{0.4361\\(0.0075)} & \makecell{0.489\\(0.0078)} \\
\hline
\makecell{Claude-3-5\\-Haiku} & \makecell{0.051\\(0.0036)} & \makecell{0.0988\\(0.0052)} & \makecell{0.1484\\(0.0065)} & \makecell{0.2008\\(0.007)} & \makecell{0.246\\(0.0076)} & \makecell{0.2979\\(0.008)} & \makecell{0.3448\\(0.0082)} & \makecell{0.3899\\(0.0084)} & \makecell{0.4284\\(0.0089)} & \makecell{0.4813\\(0.0092)} \\
\hline
\makecell{Llama-3-8B} & \makecell{0.0481\\(0.0028)} & \makecell{0.1006\\(0.0041)} & \makecell{0.1536\\(0.0057)} & \makecell{0.1982\\(0.0067)} & \makecell{0.2487\\(0.0075)} & \makecell{0.3017\\(0.0081)} & \makecell{0.3452\\(0.0091)} & \makecell{0.3974\\(0.009)} & \makecell{0.4442\\(0.0091)} & \makecell{0.4996\\(0.0092)} \\
\hline
\makecell{Llama-3.3\\-70B} & \makecell{0.0496\\(0.0031)} & \makecell{0.0983\\(0.0043)} & \makecell{0.1458\\(0.0057)} & \makecell{0.1984\\(0.0066)} & \makecell{0.2464\\(0.0067)} & \makecell{0.2955\\(0.0078)} & \makecell{0.3479\\(0.0078)} & \makecell{0.4032\\(0.0086)} & \makecell{0.4453\\(0.009)} & \makecell{0.4994\\(0.0094)} \\
\hline
\makecell{Mistral-7B} & \makecell{0.049\\(0.0033)} & \makecell{0.1075\\(0.0046)} & \makecell{0.1586\\(0.0053)} & \makecell{0.21\\(0.0055)} & \makecell{0.2478\\(0.0058)} & \makecell{0.2983\\(0.0067)} & \makecell{0.3492\\(0.0079)} & \makecell{0.4065\\(0.0081)} & \makecell{0.4536\\(0.0087)} & \makecell{0.5035\\(0.0087)} \\
\hline
\makecell{DeepSeek-V3} & \makecell{0.0549\\(0.0033)} & \makecell{0.106\\(0.0046)} & \makecell{0.1571\\(0.0052)} & \makecell{0.2062\\(0.006)} & \makecell{0.2499\\(0.0069)} & \makecell{0.3051\\(0.0067)} & \makecell{0.3579\\(0.0082)} & \makecell{0.4053\\(0.0084)} & \makecell{0.4535\\(0.0088)} & \makecell{0.5049\\(0.0095)} \\
\hline
\makecell{Random} & \makecell{0.0478\\(0.0029)} & \makecell{0.0986\\(0.0046)} & \makecell{0.1462\\(0.0055)} & \makecell{0.1975\\(0.006)} & \makecell{0.2403\\(0.0057)} & \makecell{0.2844\\(0.0067)} & \makecell{0.3412\\(0.0059)} & \makecell{0.3948\\(0.0062)} & \makecell{0.4401\\(0.0068)} & \makecell{0.4853\\(0.0073)} \\
\hline
\end{tabular}
}
\caption{Means and standard errors of $\widetilde{L}(\widehat{k})$ over 100 random splits for various LLMs and $\alpha$ over the OpinionQA dataset}
\label{tab:opinion-miscoverage-mean-stderror}
\end{table}

\begin{table}[ht]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
\makecell{GPT-3.5\\-turbo} & \makecell{0.0401\\(0.0033)} & \makecell{0.0882\\(0.0025)} & \makecell{0.1122\\(0.0057)} & \makecell{0.1401\\(0.0044)} & \makecell{0.2067\\(0.0038)} & \makecell{0.26\\(0.0051)} & \makecell{0.308\\(0.0088)} & \makecell{0.2806\\(0.0114)} & \makecell{0.3655\\(0.0116)} & \makecell{0.3925\\(0.0104)} \\
\hline
\makecell{GPT-4o\\-mini} & \makecell{0.035\\(0.0033)} & \makecell{0.0773\\(0.0026)} & \makecell{0.1155\\(0.0066)} & \makecell{0.1463\\(0.006)} & \makecell{0.2074\\(0.0055)} & \makecell{0.2425\\(0.0037)} & \makecell{0.2909\\(0.0043)} & \makecell{0.3395\\(0.0108)} & \makecell{0.3417\\(0.0117)} & \makecell{0.3747\\(0.0109)} \\
\hline
GPT-4o & \makecell{0.0528\\(0.0032)} & \makecell{0.099\\(0.0047)} & \makecell{0.1365\\(0.0039)} & \makecell{0.1891\\(0.0057)} & \makecell{0.2313\\(0.0075)} & \makecell{0.283\\(0.0082)} & \makecell{0.3383\\(0.008)} & \makecell{0.3931\\(0.0074)} & \makecell{0.4342\\(0.008)} & \makecell{0.4805\\(0.0081)} \\
\hline
\makecell{Claude-3-5\\-Haiku} & \makecell{0.0461\\(0.0036)} & \makecell{0.0833\\(0.0044)} & \makecell{0.1234\\(0.0046)} & \makecell{0.1618\\(0.0055)} & \makecell{0.1993\\(0.0072)} & \makecell{0.2577\\(0.008)} & \makecell{0.2772\\(0.008)} & \makecell{0.3326\\(0.0081)} & \makecell{0.3948\\(0.0089)} & \makecell{0.4327\\(0.0081)} \\
\hline
\makecell{Llama-3.3\\-70B} & \makecell{0.0444\\(0.0033)} & \makecell{0.0785\\(0.0031)} & \makecell{0.1261\\(0.0055)} & \makecell{0.1355\\(0.0059)} & \makecell{0.2096\\(0.0054)} & \makecell{0.2564\\(0.0051)} & \makecell{0.2998\\(0.0059)} & \makecell{0.348\\(0.0061)} & \makecell{0.3988\\(0.0085)} & \makecell{0.4235\\(0.0133)} \\
\hline
\makecell{Mistral-7B} & \makecell{0.0366\\(0.0033)} & \makecell{0.0829\\(0.0043)} & \makecell{0.1064\\(0.003)} & \makecell{0.1621\\(0.0031)} & \makecell{0.2074\\(0.008)} & \makecell{0.2447\\(0.0098)} & \makecell{0.2716\\(0.0108)} & \makecell{0.2858\\(0.0055)} & \makecell{0.3337\\(0.0043)} & \makecell{0.391\\(0.005)} \\
\hline
\makecell{DeepSeek-V3} & \makecell{0.045\\(0.0032)} & \makecell{0.0913\\(0.0043)} & \makecell{0.1399\\(0.005)} & \makecell{0.1835\\(0.0057)} & \makecell{0.2153\\(0.0068)} & \makecell{0.2777\\(0.0071)} & \makecell{0.3223\\(0.0067)} & \makecell{0.3667\\(0.0069)} & \makecell{0.4166\\(0.0075)} & \makecell{0.46\\(0.0073)} \\
\hline
\makecell{Random} & \makecell{0.0446\\(0.0026)} & \makecell{0.0645\\(0.0038)} & \makecell{0.1235\\(0.0064)} & \makecell{0.1812\\(0.0055)} & \makecell{0.2338\\(0.0062)} & \makecell{0.2851\\(0.0065)} & \makecell{0.3299\\(0.007)} & \makecell{0.3702\\(0.0086)} & \makecell{0.4302\\(0.0082)} & \makecell{0.4731\\(0.0087)} \\
\hline
\end{tabular}
}
\caption{Means and standard errors of $\widetilde{L}(\widehat{k})$ over 100 random splits for various LLMs and $\alpha$ over the EEDI dataset}
\label{tab:eedi-miscoverage-mean-stderror}
\end{table}


\subsection{Selected Simulation Sample Size}

In \Cref{tab:opinion-k-hat} and \Cref{tab:eedi-k-hat}, we present the detailed results for the selected simulation sample size $\widehat{k}$. They complement \Cref{fig:k-hat}.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
\makecell{GPT-3.5\\-turbo} & \makecell{36.65\\(1.064)} & \makecell{44.03\\(0.9373)} & \makecell{44.62\\(0.4879)} & \makecell{44.22\\(0.4263)} & \makecell{43.59\\(0.4794)} & \makecell{43.21\\(0.5823)} & \makecell{43.44\\(0.7403)} & \makecell{44.75\\(0.7367)} & \makecell{45.22\\(0.6492)} & \makecell{45.91\\(0.5296)} \\
\hline
\makecell{GPT-4o\\-mini} & \makecell{26.85\\(0.7517)} & \makecell{33.68\\(0.8189)} & \makecell{37.37\\(0.5871)} & \makecell{37.56\\(0.4058)} & \makecell{37.08\\(0.5025)} & \makecell{38.4\\(0.6345)} & \makecell{40.09\\(0.6149)} & \makecell{41.36\\(0.5745)} & \makecell{41.6\\(0.4671)} & \makecell{41.51\\(0.5328)} \\
\hline
GPT-4o & \textbf{\makecell{54.1\\(1.7862)}} & \textbf{\makecell{62.19\\(1.2286)}} & \textbf{\makecell{65.35\\(1.0516)}} & \textbf{\makecell{68.77\\(0.9885)}} & \textbf{\makecell{69.58\\(1.1095)}} & \textbf{\makecell{74.29\\(1.5015)}} & \textbf{\makecell{81.83\\(1.6431)}} & \textbf{\makecell{88.7\\(1.5013)}} & \textbf{\makecell{89.37\\(1.2831)}} & \textbf{\makecell{91.86\\(1.2268)}} \\
\hline
\makecell{Claude-3-5\\-Haiku} & \makecell{35.75\\(1.0428)} & \makecell{40.7\\(0.7842)} & \makecell{42.46\\(0.7336)} & \makecell{44.74\\(0.6872)} & \makecell{45.07\\(0.6561)} & \makecell{46.27\\(0.6464)} & \makecell{47.79\\(0.7881)} & \makecell{49.84\\(0.8488)} & \makecell{50.37\\(0.7415)} & \makecell{51.34\\(0.6819)} \\
\hline
\makecell{Llama-3-8B} & \makecell{28.81\\(0.6264)} & \makecell{31.93\\(0.7149)} & \makecell{34.35\\(0.4828)} & \makecell{34.83\\(0.5382)} & \makecell{35.13\\(0.6855)} & \makecell{37.3\\(0.5248)} & \makecell{38.1\\(0.5491)} & \makecell{38.9\\(0.6747)} & \makecell{40.5\\(0.8084)} & \makecell{44.25\\(0.8016)} \\
\hline
\makecell{Llama-3.3\\-70B} & \makecell{46.3\\(0.6921)} & \makecell{47.67\\(0.5339)} & \makecell{46.39\\(0.5331)} & \makecell{48.12\\(1.0276)} & \makecell{53.49\\(1.2524)} & \makecell{56.21\\(0.8797)} & \makecell{57.45\\(0.7294)} & \makecell{58.04\\(0.7947)} & \makecell{58.62\\(1.0285)} & \makecell{61.8\\(1.0627)} \\
\hline
\makecell{Mistral-7B} & \makecell{30.81\\(0.2566)} & \makecell{33.47\\(0.8928)} & \makecell{39.47\\(0.8815)} & \makecell{44.59\\(0.8935)} & \makecell{47.53\\(0.8622)} & \makecell{52.43\\(1.0825)} & \makecell{56.34\\(1.0113)} & \makecell{58.43\\(0.9535)} & \makecell{60.29\\(1.1178)} & \makecell{64.85\\(1.2481)} \\
\hline
DeepSeek-V3 & \makecell{37.59\\(1.4767)} & \makecell{46.75\\(0.79)} & \makecell{46.66\\(0.4771)} & \makecell{45.1\\(0.6201)} & \makecell{46.03\\(0.5657)} & \makecell{47.77\\(0.6135)} & \makecell{47.76\\(0.4606)} & \makecell{47.61\\(0.48)} & \makecell{47.92\\(0.6989)} & \makecell{49.51\\(0.7031)} \\
\hline
Random & \makecell{17.09\\(0.3761)} & \makecell{18.07\\(0.4402)} & \makecell{19.33\\(0.3361)} & \makecell{19.83\\(0.4046)} & \makecell{19.85\\(0.3261)} & \makecell{20.36\\(0.3227)} & \makecell{20.01\\(0.3191)} & \makecell{19.92\\(0.2849)} & \makecell{19.93\\(0.2662)} & \makecell{20.4\\(0.2252)} \\
\hline
\end{tabular}
}
\caption{Average $\widehat{k}$ (with 95\% margin of error in parentheses) for various LLMs and various $\alpha$ over the OpinionQA dataset. GPT-4o has the largest $\widehat{k}$ on average.}
\label{tab:opinion-k-hat}
\end{table}



\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
\makecell{GPT-3.5\\-turbo} & \makecell{5.68\\(0.1033)} & \makecell{5.89\\(0.0613)} & \makecell{5.19\\(0.0769)} & \makecell{5.03\\(0.0334)} & \makecell{4.99\\(0.0195)} & \makecell{4.94\\(0.0465)} & \makecell{4.71\\(0.0889)} & \makecell{4.17\\(0.0736)} & \makecell{4.29\\(0.0889)} & \makecell{4.13\\(0.0659)} \\
\hline
\makecell{GPT-4o\\-mini} & \makecell{6.26\\(0.086)} & \makecell{5.94\\(0.0465)} & \makecell{5.44\\(0.0973)} & \makecell{5.14\\(0.068)} & \makecell{5.06\\(0.0465)} & \makecell{5.0\\(0)} & \makecell{4.99\\(0.0195)} & \makecell{4.6\\(0.096)} & \makecell{4.25\\(0.0849)} & \makecell{4.15\\(0.07)} \\
\hline
GPT-4o & \makecell{8.04\\(0.1438)} & \makecell{7.82\\(0.1672)} & \makecell{8.31\\(0.1853)} & \makecell{8.1\\(0.116)} & \makecell{7.81\\(0.1609)} & \makecell{7.83\\(0.152)} & \makecell{7.84\\(0.1459)} & \makecell{7.93\\(0.1308)} & \makecell{7.9\\(0.1224)} & \makecell{8.17\\(0.1178)} \\
\hline
\makecell{Claude-3-5\\-Haiku} & \makecell{7.57\\(0.1337)} & \makecell{7.42\\(0.0967)} & \makecell{7.05\\(0.0802)} & \makecell{6.82\\(0.0849)} & \makecell{6.59\\(0.0964)} & \makecell{6.47\\(0.1017)} & \makecell{6.15\\(0.07)} & \makecell{6.15\\(0.07)} & \makecell{6.12\\(0.0889)} & \makecell{6.04\\(0.0674)} \\
\hline
\makecell{Llama-3.3\\-70B} & \makecell{6.19\\(0.0863)} & \makecell{5.94\\(0.0542)} & \makecell{5.6\\(0.096)} & \makecell{5.09\\(0.0561)} & \makecell{5.05\\(0.0427)} & \makecell{5.0\\(0.0277)} & \makecell{5.0\\(0.0392)} & \makecell{4.92\\(0.0532)} & \makecell{4.83\\(0.0736)} & \makecell{4.54\\(0.0977)} \\
\hline
Mistral-7B & \makecell{5.48\\(0.0979)} & \makecell{5.2\\(0.0877)} & \makecell{5.01\\(0.0195)} & \makecell{4.97\\(0.0334)} & \makecell{4.68\\(0.0914)} & \makecell{4.52\\(0.0979)} & \makecell{4.29\\(0.0889)} & \makecell{4.02\\(0.0274)} & \makecell{4.0\\(0)} & \makecell{4.0\\(0)} \\
\hline
DeepSeek-V3 & \textbf{\makecell{10.35\\(0.1695)}} & \textbf{\makecell{10.51\\(0.1628)}} & \textbf{\makecell{10.76\\(0.1841)}} & \textbf{\makecell{10.92\\(0.1954)}} & \textbf{\makecell{11.21\\(0.1551)}} & \textbf{\makecell{11.53\\(0.1504)}} & \textbf{\makecell{11.67\\(0.1594)}} & \textbf{\makecell{11.8\\(0.1518)}} & \textbf{\makecell{11.88\\(0.1337)}} & \textbf{\makecell{11.66\\(0.1364)}} \\
\hline
Random & \makecell{8.02\\(0.1239)} & \makecell{7.06\\(0.0465)} & \makecell{6.64\\(0.1127)} & \makecell{6.92\\(0.1198)} & \makecell{7.06\\(0.1435)} & \makecell{7.53\\(0.1223)} & \makecell{7.69\\(0.1405)} & \makecell{7.49\\(0.1828)} & \makecell{7.56\\(0.1475)} & \makecell{7.59\\(0.1361)} \\
\hline
\end{tabular}
}
\caption{Average $\widehat{k}$ (with 95\% margin of error in parentheses) for various LLMs and various $\alpha$ over the EEDI dataset. DeepSeek-V3 has the largest $\widehat{k}$ on average.}
\label{tab:eedi-k-hat}
\end{table}


\subsection{Half-Width of Confidence Interval}

In \Cref{fig:width}, \Cref{tab:opinion-synth-CI-width} and \Cref{tab:eedi-synth-CI-width}, we present the detailed experiment results for the half-width of the synthetic confidence interval $\simCIalt(\widehat{k})$.



\begin{figure}[H]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/width_OpinionQA.pdf}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/width_EEDI.pdf}
	\end{minipage}
	\caption{Half-widths of confidence intervals for various LLMs and $\alpha$'s over the OpinionQA (left) and EEDI (right) datasets.}
	\label{fig:width}
\end{figure}


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
\makecell{GPT-3.5\\-turbo} & \makecell{0.6401\\(0.0099)} & \makecell{0.5241\\(0.0058)} & \makecell{0.4825\\(0.0027)} & \makecell{0.4568\\(0.0021)} & \makecell{0.4373\\(0.0024)} & \makecell{0.4198\\(0.0027)} & \makecell{0.4017\\(0.0033)} & \makecell{0.3803\\(0.0031)} & \makecell{0.364\\(0.0026)} & \makecell{0.348\\(0.002)} \\
\hline
\makecell{GPT-4o\\-mini} & \makecell{0.7465\\(0.0097)} & \makecell{0.5999\\(0.0072)} & \makecell{0.5279\\(0.0043)} & \makecell{0.4958\\(0.0027)} & \makecell{0.4745\\(0.0032)} & \makecell{0.4457\\(0.0037)} & \makecell{0.418\\(0.0032)} & \makecell{0.3953\\(0.0028)} & \makecell{0.3792\\(0.0022)} & \makecell{0.3661\\(0.0024)} \\
\hline
GPT-4o & \textbf{\makecell{0.528\\(0.009)}} & \textbf{\makecell{0.4405\\(0.0041)}} & \textbf{\makecell{0.3991\\(0.0031)}} & \textbf{\makecell{0.3667\\(0.0025)}} & \textbf{\makecell{0.3466\\(0.0026)}} & \textbf{\makecell{0.3208\\(0.0031)}} & \textbf{\makecell{0.293\\(0.0029)}} & \textbf{\makecell{0.2702\\(0.0024)}} & \textbf{\makecell{0.2589\\(0.0018)}} & \textbf{\makecell{0.2461\\(0.0017)}} \\
\hline
\makecell{Claude-3-5\\-Haiku} & \makecell{0.6478\\(0.0095)} & \makecell{0.5446\\(0.0052)} & \makecell{0.4954\\(0.0041)} & \makecell{0.4548\\(0.0035)} & \makecell{0.4305\\(0.0031)} & \makecell{0.4057\\(0.0028)} & \makecell{0.383\\(0.0031)} & \makecell{0.3604\\(0.0031)} & \makecell{0.3449\\(0.0026)} & \makecell{0.3292\\(0.0023)} \\
\hline
\makecell{Llama-3-8B} & \makecell{0.7189\\(0.0078)} & \makecell{0.6156\\(0.0069)} & \makecell{0.5503\\(0.0038)} & \makecell{0.5154\\(0.0038)} & \makecell{0.4884\\(0.0047)} & \makecell{0.4519\\(0.0032)} & \makecell{0.4286\\(0.0028)} & \makecell{0.4079\\(0.0032)} & \makecell{0.3852\\(0.0036)} & \makecell{0.3552\\(0.0034)} \\
\hline
\makecell{Llama-3.3\\-70B} & \makecell{0.5658\\(0.0045)} & \makecell{0.502\\(0.0029)} & \makecell{0.4732\\(0.0026)} & \makecell{0.4393\\(0.0044)} & \makecell{0.3965\\(0.0048)} & \makecell{0.3683\\(0.0029)} & \makecell{0.3489\\(0.0023)} & \makecell{0.3336\\(0.0022)} & \makecell{0.32\\(0.0028)} & \makecell{0.3004\\(0.0025)} \\
\hline
Mistral-7B & \makecell{0.6925\\(0.0028)} & \makecell{0.6024\\(0.0079)} & \makecell{0.5149\\(0.0058)} & \makecell{0.4563\\(0.0047)} & \makecell{0.4196\\(0.0035)} & \makecell{0.3821\\(0.0041)} & \makecell{0.3529\\(0.0033)} & \makecell{0.3328\\(0.0026)} & \makecell{0.3156\\(0.0029)} & \makecell{0.2935\\(0.0028)} \\
\hline
DeepSeek-V3 & \makecell{0.6376\\(0.0143)} & \makecell{0.5078\\(0.0047)} & \makecell{0.4717\\(0.0025)} & \makecell{0.4527\\(0.0031)} & \makecell{0.4257\\(0.0026)} & \makecell{0.3992\\(0.0026)} & \makecell{0.3824\\(0.0019)} & \makecell{0.3681\\(0.0018)} & \makecell{0.3536\\(0.0025)} & \makecell{0.3353\\(0.0024)} \\
\hline
Random & \makecell{0.9337\\(0.0105)} & \makecell{0.8194\\(0.0106)} & \makecell{0.7341\\(0.006)} & \makecell{0.6842\\(0.0067)} & \makecell{0.649\\(0.0054)} & \makecell{0.612\\(0.005)} & \makecell{0.5918\\(0.0048)} & \makecell{0.5697\\(0.0042)} & \makecell{0.5481\\(0.0037)} & \makecell{0.522\\(0.0028)} \\
\hline
\end{tabular}
}
\caption{Average half-width (with 95\% margin of error in parentheses) of the synthetic confidence interval for various LLMs and $\alpha$ over the OpinionQA dataset. GPT-4o has the shortest confidence interval.}
\label{tab:opinion-synth-CI-width}
\end{table}


\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|}
\hline
$\alpha$ & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 & 0.30 & 0.35 & 0.40 & 0.45 & 0.50 \\
\hline\hline
\makecell{GPT-3.5\\-turbo} & \makecell{0.8085\\(0.0075)} & \makecell{0.714\\(0.0041)} & \makecell{0.7078\\(0.0048)} & \makecell{0.6768\\(0.002)} & \makecell{0.6457\\(0.0015)} & \makecell{0.6203\\(0.0034)} & \makecell{0.6106\\(0.0062)} & \makecell{0.6229\\(0.0049)} & \makecell{0.592\\(0.0057)} & \makecell{0.5806\\(0.0041)} \\
\hline
\makecell{GPT-4o\\-mini} & \makecell{0.769\\(0.005)} & \makecell{0.7107\\(0.0031)} & \makecell{0.6922\\(0.0061)} & \makecell{0.6703\\(0.004)} & \makecell{0.6415\\(0.0026)} & \makecell{0.616\\(0)} & \makecell{0.5911\\(0.0014)} & \makecell{0.5941\\(0.0064)} & \makecell{0.5945\\(0.0055)} & \makecell{0.5794\\(0.0043)} \\
\hline
GPT-4o & \makecell{0.6794\\(0.0059)} & \makecell{0.6215\\(0.0062)} & \makecell{0.5613\\(0.0068)} & \makecell{0.5343\\(0.0039)} & \makecell{0.5181\\(0.0052)} & \makecell{0.494\\(0.0047)} & \makecell{0.4731\\(0.0043)} & \makecell{0.4517\\(0.0036)} & \makecell{0.4355\\(0.0033)} & \makecell{0.4128\\(0.003)} \\
\hline
\makecell{Claude-3-5\\-Haiku} & \makecell{0.7001\\(0.0059)} & \makecell{0.6364\\(0.0041)} & \makecell{0.6069\\(0.0035)} & \makecell{0.582\\(0.0038)} & \makecell{0.5629\\(0.0042)} & \makecell{0.5428\\(0.0042)} & \makecell{0.533\\(0.0028)} & \makecell{0.5122\\(0.0027)} & \makecell{0.4947\\(0.0036)} & \makecell{0.4797\\(0.0027)} \\
\hline
\makecell{Llama-3.3\\-70B} & \makecell{0.7734\\(0.0052)} & \makecell{0.7108\\(0.0036)} & \makecell{0.6821\\(0.006)} & \makecell{0.6733\\(0.0033)} & \makecell{0.6421\\(0.0024)} & \makecell{0.6162\\(0.0018)} & \makecell{0.5908\\(0.0024)} & \makecell{0.5727\\(0.0036)} & \makecell{0.5572\\(0.0047)} & \makecell{0.5551\\(0.0061)} \\
\hline
Mistral-7B & \makecell{0.823\\(0.0073)} & \makecell{0.761\\(0.0062)} & \makecell{0.7191\\(0.0012)} & \makecell{0.681\\(0.0027)} & \makecell{0.6693\\(0.007)} & \makecell{0.6509\\(0.0071)} & \makecell{0.6399\\(0.0062)} & \makecell{0.633\\(0.0018)} & \makecell{0.6107\\(0)} & \makecell{0.5887\\(0)} \\
\hline
DeepSeek-V3 & \textbf{\makecell{0.5984\\(0.0045)}} & \textbf{\makecell{0.5351\\(0.004)}} & \textbf{\makecell{0.4921\\(0.0043)}} & \textbf{\makecell{0.4607\\(0.0042)}} & \textbf{\makecell{0.4315\\(0.0031)}} & \textbf{\makecell{0.4063\\(0.0027)}} & \textbf{\makecell{0.3872\\(0.0029)}} & \textbf{\makecell{0.3699\\(0.0025)}} & \textbf{\makecell{0.3548\\(0.002)}} & \textbf{\makecell{0.3453\\(0.0021)}} \\
\hline
Random & \makecell{0.6799\\(0.0056)} & \makecell{0.6517\\(0.002)} & \makecell{0.6263\\(0.0053)} & \makecell{0.5786\\(0.0052)} & \makecell{0.545\\(0.0058)} & \makecell{0.5032\\(0.0041)} & \makecell{0.4779\\(0.0049)} & \makecell{0.4665\\(0.0061)} & \makecell{0.4459\\(0.0045)} & \makecell{0.4287\\(0.0039)} \\
\hline
\end{tabular}
}
\caption{Average half-width (with 95\% margin of error in parentheses) of the synthetic confidence interval for various LLMs and $\alpha$ over the EEDI dataset. DeepSeek-V3 has the shortest confidence interval.}
\label{tab:eedi-synth-CI-width}
\end{table}

