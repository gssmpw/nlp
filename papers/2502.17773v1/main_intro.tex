\section{Introduction}

Large language models (LLMs) have demonstrated remarkable capabilities in mimicking human behaviors. Recent studies have leveraged LLMs to simulate human responses in various domains, including economic and social science experiments \citep{AAK23, Hor23, CLS23, BCD24, HYZ24, YLW24, ZHS24}, market research \citep{BIN23, GTo23, GSi24, WZZ24}, education \citep{ZMT23, LWa24}, and so on. Compared to traditional survey methods that recruit and query real people, LLM simulations offer significant advantages in terms of time and cost efficiency, enabling the generation of large-scale synthetic responses with minimal effort.

However, a growing body of evidence suggests that LLMs are not perfectly aligned with the human population, and in some cases, the misalignment can be substantial \citep{AAK23, SDL23}. This raises critical concerns about the reliability of insights derived from LLM-generated data. It remains a challenge how to properly simulate human responses using LLMs and how to account for their imperfections when using the simulated samples to make inference about the true human population.

We propose to address this challenge through the lens of \emph{uncertainty quantification}. Specifically, we seek to construct confidence sets for population statistics of human responses based on LLM-generated data. A central question in this process is:
\begin{center}
\emph{How many synthetic samples should be generated?}
\end{center}
On one hand, generating too many samples risks overfitting the synthetic distribution, which may deviate from the real human population. On the other hand, generating too few samples yields overly large and uninformative confidence sets. The optimal sample size depends on the discrepancy between the synthetic and real populations --- a quantity that is unknown in practice. This necessitates a data-driven approach to determine the appropriate number of simulated responses.


\paragraph{Main contributions.} In this paper, we develop a general framework to address these challenges. Our key contributions are as follows:

\begin{itemize}
\item (Formulation) We provide a rigorous mathematical framework for uncertainty quantification in LLM-based survey simulations.

\item (Methodology) We propose a flexible methodology that transforms simulated responses into valid confidence sets for population parameters of human responses. Our approach adaptively selects the simulation sample size based on the observed misalignment between the LLM and human populations. It is applicable to any LLM, regardless of its fidelity, and can be combined with any method for confidence set construction.
\end{itemize}


\paragraph{Related works.} 

Our work relates to research on assessing the fidelity of LLM simulations and measuring their alignment with real human populations. Prior studies have explored similarity metrics between synthetic and human distributions \citep{SDL23, HMG24, DHM24, DNL24, CRD25} and Turing-type tests \citep{ABFG23, MXY24} to evaluate LLM reliability. While these approaches provide valuable insights into LLM misalignment, they do not offer methods for leveraging imperfect LLM simulations to draw reliable conclusions about human populations. In contrast, our work provides a principled approach for constructing confidence sets that account for the inherent discrepancies between LLM-generated and human responses.

Additionally, our work connects to the line of work on model-free statistical inference \citep{SVo08, BAL21, ABF23}. At a high level, these methods use labeled data from the true distribution to calibrate imperfect point predictions from an arbitrary black-box model and then construct valid set estimates.
Our approach follows a similar spirit. The ``features'' and ``labels'' in our setting correspond to the survey questions and their population statistics of human responses, respectively. 
However, our labels are not directly observable. As a result, the labeled calibration data needed for these methods is not available.
Moreover, for every simulation sample size $k$, one can produce a point prediction of the label using $k$ synthetic responses generated by the LLM. As the optimal sample size is not known a priori, there are infinitely many candidate point predictions to choose from. This makes it difficult to apply existing methods.

\paragraph{Outline.} The rest of the paper is organized as follows. \Cref{sec-warmup} studies one-dimensional mean estimation as a warm-up example. \Cref{sec-general} presents the general problem setup and methodology. \Cref{sec-experiments} illustrates our proposed method on real datasets. \Cref{sec-discussions} concludes the paper.

\paragraph{Notation.} We use $\ZZ_+$ to denote the set of positive integers. For $n\in\ZZ_+$, define $[n]=\{1,2,...,n\}$. For $a,b\in\RR$, define $a\wedge b = \min\{a,b\}$ and $a\vee b = \max\{a,b\}$. For non-negative sequences $\{a_n\}_{n=1}^{\infty}$ and $\{b_n\}_{n=1}^{\infty}$, we write $a_n=O(b_n)$ if there exists $C>0$ such that for all $n$, it holds that $a_n \le C b_n$. We write $a_n = \Omega(b_n)$ if $b_n = O(a_n)$. We write $a_n = \Theta(b_n)$ if $a_n=O(b_n)$ and $a_n=\Omega(b_n)$. The notation $\Bernoulli(p)$ denotes the Bernoulli distribution with mean $p$. The notation $N(\mu,\sigma^2)$ denotes the normal distribution with mean $\mu$ and variance $\sigma^2$.