\section{Numerical Experiments}\label{sec-experiments}

In this section, we apply our general method in \Cref{sec-general} to LLMs over real datasets. The code and data are available at \url{https://github.com/yw3453/uq-llm-survey-simulation}.

\subsection{Experiment Setup}


\paragraph{LLMs.} We consider $8$ LLMs: GPT-3.5-Turbo (\texttt{gpt-3.5-turbo}), GTP-4o (\texttt{gpt-4o}), and GPT-4o-mini (\texttt{gpt-4o-mini}) \citep{GPT3.5, GPT4o, GPT4omini}; Claude 3.5 Haiku (\texttt{claude-3-5-haiku-20241022}) \cite{Ant24}; Llama 3.1 8B (\texttt{Llama-3-8B-Instruct-Turbo}) and Llama 3.3 70B (\texttt{Llama-3.3-70B-Instruct-Turbo}) \citep{DJP24}; Mistral 7B (\texttt{Mistral-7B-Instruct-v0.3}) \citep{JSM23}; DeepSeek-V3 (\texttt{DeepSeek-V3}) \citep{LFX24}.


\paragraph{Datasets.} We use two datasets for survey questions, each corresponding to one uncertainty quantification task. The first dataset is the OpinionQA dataset created by \cite{SDL23}. It was built from Pew Research's American Trends Panel\footnote{\url{https://www.pewresearch.org/the-american-trends-panel/}}, and contains the general US population's responses to survey questions spanning topics such as science, politics, and health. After pre-processing we have 385 unique questions and 1,476,868 responses to these questions from at least 32,864 people. These questions have $5$ choices corresponding to ordered sentiments which we map to sentiment scores $-1,-\frac{1}{3},0,\frac{1}{3},1$. Each question has at least $400$ responses. For each response, we have information on their political profile, religious affiliation, educational background, socio-economic status, etc. This information is used as their profiles to generate synthetic profiles. See \Cref{sec-opinion} for more information on this dataset, including example questions, profile features, and the generation of synthetic profiles and answers. We consider the task of constructing a confidence interval for the US population's average sentiment score for a survey question. This is the setup in \Cref{example-public-survey-1D}.


The second dataset is the EEDI dataset created by \cite{HMG24}, which was built upon the NeurIPS 2020 Education Challenge dataset \citep{WLS21}. It consists of students' responses to mathematics multiple-choice questions on the Eedi online educational platform\footnote{\url{https://eedi.com/}}. The dataset contains 573 unique questions and 443,433 responses to these questions from 2,287 students. All questions have four choices (A, B, C, D). Out of these questions, we use questions that have at least $100$ student responses. Excluding questions with graphs or diagrams, we are left with a total of $412$ questions. For each student, we have information on their gender, age, and socioeconomic status. This information is used as their profiles to generate synthetic profiles. See \Cref{sec-eedi} for more information on this dataset, including example questions, profile distribution, and the generation of synthetic profiles and answers. We consider the task of constructing a confidence interval for the probability of a student answering a question correctly. This is similar to the setup in \Cref{sec-warmup} and \Cref{example-education}.


\paragraph{Confidence set construction.} Our method can be built upon any arbitrary confidence set construction procedure $\setmap$. We use a construction procedure based on Hoeffding's concentration inequality (e.g., Theorem 2.8 in \cite{BLM13}), as it has valid coverage guarantee for any finite sample size. Given $\alpha\in(0,1)$ and responses $\{\simresponse_i\}_{i=1}^k$, we construct the confidence interval
\begin{equation}\label{eqn:synthCI}
\setmap \big(\{\simresponse_i\}_{i=1}^k\big)
= 
\bigg[ \simresponsebar_k - cM\sqrt{\frac{\log(2/\alpha)}{2k}}  ,  ~  \simresponsebar_k + cM\sqrt{\frac{\log(2/\alpha)}{2k}} \bigg],
\end{equation}
where $\simresponsebar_k = \frac{1}{k}\sum_{i=1}^k \simresponse_i$ is the sample mean, $M>0$ is an upper bound on the range of the responses, and $c>1$ is a scaling constant. 

\paragraph{Hyperparameters.} We consider $\alpha\in\{0.05\cdot\ell:\ell\in[10]\}$, $c=\sqrt{2}$ and $\gamma=0.5$. For the EEDI dataset, we set the simulation budget $K=50$ and take $M=1$ since the responses are binary. For the OpinionQA dataset, we set the simulation budget $K=100$ and take $M=2$ since the responses range within $[-1,1]$.


\subsection{Experiment Procedure}

We now describe our experiment procedure for applying the method in \Cref{sec-general} to each dataset. Denote the dataset by $\{ ( \testfunction_j , \dataset_j ) \}_{j=1}^{J}$, where $\testfunction_j$ is a survey question and $\dataset_j = \{ \response_{j,i} \}_{i=1}^{n_j}$ is a collection of human responses. For each $j\in[J]$, we simulate $K$ responses $\simdataset_j$ from an LLM. We then randomly split $ \datasetmeta = \{ ( \dataset_j, \simdataset_j ) \}_{j=1}^{J}$ into a training set $\datasetmeta^{\train} = \{ ( \dataset_j, \simdataset_j ) \}_{j\in\cJ_{\train}}$ and a testing set $\datasetmeta^{\test} = \{ ( \dataset_j, \simdataset_j ) \}_{j\in\cJ_{\test}}$, with $|\datasetmeta^{\train}| : |\datasetmeta^{\test}| = 3 : 2$.


\paragraph{Selection of simulation sample size.} We apply the approach \eqref{eqn-empirical-criterion} with the training set $\datasetmeta^{\train}$ to select a simulation sample size $\widehat{k}$. For the confidence set $\CIalt_j$ in \eqref{eqn-CI-calibrate} constructed from the real data $\dataset_j$, we use the standard CLT-based confidence interval:
\begin{equation}\label{eqn-CI-calibrate-CLT}
\CIalt_j = \bigg[ \responsebar_j - \frac{\samplesd_j}{\sqrt{n_j}} \Phi^{-1}\left( \frac{1+\gamma}{2} \right) ,
~  \responsebar_j + \frac{\samplesd_j}{\sqrt{n_j}} \Phi^{-1}\left( \frac{1+\gamma}{2} \right) \bigg],
\end{equation}
where $\responsebar_j = \frac{1}{n_j} \sum_{i=1}^{n_j} \response_{j,i}$ and $\samplesd_j = \sqrt{\responsebar_j (1-\responsebar_j) }$. Since $n_j$ is at least $100$, $\CIalt_j$ has approximately $\gamma$ coverage probability. In \Cref{fig:k-upcross-eg} of \Cref{sec-appendix-experiments-results}, we provide a visualization for the selection of $\widehat{k}$.

\paragraph{Evaluation of selected sample size.} We use $\datasetmeta^{\test}$ to evaluate the quality of the chosen simulation sample size $\widehat{k}$. As the true population mean $\statistic ( \testfunction )$ is unavailable, the true coverage probability $\PP\big( \statistic ( \testfunction ) \in \simCIalt(\widehat{k}) \big)$ cannot be computed. However, we can apply the same idea as \eqref{eqn-proxy} in \Cref{sec-general} to compute a proxy for the miscoverage level. For each survey question $j\in\cJ_2$, the selected sample size $\widehat{k}$ leads to the synthetic confidence set $\simCIalt_j(\widehat{k}) = \setmap\big( \{ \simresponse_{j,i} \}_{i=1}^{\widehat{k}} \big)$. We form the confidence set $\CIalt_j$ from real data $\dataset_j$ as in \eqref{eqn-CI-calibrate-CLT} and define
\begin{equation}\label{eqn:proxy}
\widetilde{\coveragealt}(\widehat{k}) = \frac{1}{\gamma} \cdot \frac{1}{|\cJ_{\test}|} \sum_{j\in\cJ_{\test}} \ind \{ \CIalt_j\not\subseteq \simCIalt_j(\widehat{k}) \}.
\end{equation}
The proof of \Cref{thm-coverage} shows that, for every $k\in[K]$ and survey question $j$,
\[
\gamma \cdot \PP\big( \statistic ( \testfunction ) \not\in \simCIalt(k) \big) \le \PP\big( \CIalt_j\not\subseteq \simCIalt_j(k) \big) = \gamma \cdot \EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] .
\]
Thus, if $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$, then $\PP\big( \statistic ( \testfunction ) \not\in \simCIalt(\widehat{k}) \big) \le \alpha$ must hold. To that end, we will test a hypothesis $H_0: \; \EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ against its alternative $H_1: \; \EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] > \alpha$.


\subsection{Experiment Results}\label{sec-experiments-results}

For both datasets, we evaluate three metrics as $\alpha$ varies: the miscoverage probability proxy \eqref{eqn:proxy}, the selected simulation sample size $\widehat{k}$, and the half-width of the synthetic confidence interval $\simCIalt(\widehat{k})$. We consider $100$ random train-test splits of the questions. For compactness, we present results on the miscoverage probability proxy and $\widehat{k}$, and defer the results on the half-width of the synthetic confidence interval as well as more experiment details to \Cref{sec-appendix-experiments-results}. We omit Llama 3.1 8B for the EEDI dataset experiment because it frequently failed to answer EEDI questions in required formats. As a baseline, we also include a na\"{i}ve response generator (\texttt{random}) that chooses an available answer uniformly at random. 

In \Cref{fig:combined}, we present histograms of $p$-values for the hypothesis test $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ against $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] > \alpha$ across various LLMs and $\alpha$'s over the OpinionQA and EEDI datasets. The $p$-values are computed using a one-sided $z$-test over the $100$ random splits. As can be seen from the histograms, all $p$-values are reasonably large, indicating that the hypothesis $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ cannot be rejected (e.g.,~at the 0.05 significance level) for any LLM and $\alpha$ across both datasets. These experiment results verify the theoretical guarantees in \Cref{sec-general}, showing that the miscoverage rate is effectively controlled by our method. 

\begin{figure}[h]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/pval_OpinionQA.pdf}
	\end{minipage}%\hspace{em}%\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/pval_EEDI.pdf}
	\end{minipage}
	\caption{Histograms of $p$-values for the hypothesis test $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] \le \alpha$ against $\EE \big[ \widetilde{\coveragealt}(\widehat{k}) \big] > \alpha$ across various LLMs and $\alpha$'s over the OpinionQA (left) and EEDI (right) datasets.}
	\label{fig:combined}
\end{figure}



In \Cref{fig:k-hat}, we plot the average $\widehat{k}$ over the $100$ random splits for various LLMs on the OpinionQA and EEDI datasets. The error bars represent 95\% confidence intervals.
In general, a larger $\widehat{k}$ means that the LLM has stronger simulation power. On the OpinionQA dataset, GPT-4o has the best performance. On the EEDI dataset, DeepSeek-V3 has the best performance. Interestingly, on the OpinionQA dataset all LLMs clearly outperform the random benchmark, while on the EEDI dataset only DeepSeek-V3 and GPT-4o seem to outperform the random benchmark. Moreover, LLMs exhibit uniformly higher $\widehat{k}$ on the OpinionQA dataset than on the EEDI dataset, suggesting higher fidelity in simulating subjective opinions than in simulating answers to mathematics questions.


\begin{figure}[h]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/k_hat_OpinionQA.pdf}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=0.8\linewidth]{figures/k_hat_EEDI.pdf}
	\end{minipage}
	\caption{Average $\widehat{k}$ for various LLMs and $\alpha$ over the OpinionQA (left) and EEDI (right) datasets.}
	\label{fig:k-hat}
\end{figure}



The experiment results demonstrate the importance of a disciplined approach to using synthetic samples. The ease of LLM-based simulation makes it tempting to generate a large number of responses per question. However, as can be seen from the figures, there is great heterogeneity in the simulation power of different LLMs over different datasets: the largest $\widehat{k}$ is below 100, while the smallest $\widehat{k}$ could be in the single digits. This means that there is real peril in using too many synthetic samples and being overly confident in the results.
