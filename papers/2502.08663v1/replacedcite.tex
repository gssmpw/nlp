\section{Related Work}
\label{sec:relworks}
Hallucinations are defined as ``a phenomenon in which the generated content appears non-sensical or unfaithful to the provided source content"____. They affect all LLMs____, due to the very architecture of LLMs. Common causes include vague prompts____ and gaps in the training data____, leading for instance to inaccuracies or perpetuation of societal biases____. The occurrence of hallucinations can significantly undermine the credibility of responses and complicate mitigation and detection efforts____.

\subsection{LLMs Hallucination Mitigation} \label{sub:mitigation} 
For what concerns mitigation, several approaches have been proposed. Some methods improve the prompt context by incorporating tags from reliable sources____ or using knowledge graphs____, while others use neural networks to predict plausible subsequent tokens and avoid unrelated outputs____. A different technique instead exploits self-evaluations, where models assess the validity of their own outputs____, and external evaluations, where a powerful LLM is used to evaluate content generated by other models____. In this way, the output is evaluated and corrected if necessary. Finally, LLMs can also use Retrieval Augmented Generation (RAG) to enhance response credibility---that is, accessing external data source for enhancing their knowledge and possibly reducing errors____.

\subsection{LLMs Hallucination Detection} \label{sub:detection}
Detecting hallucinations in large language models (LLMs) remains a significant challenge, with various techniques proposed to address this issue. Among the simplest approaches, which also serve mitigation purposes, are self-evaluation methods where the model assesses its own outputs____, or external evaluations performed by more advanced models, such as GPT-3.5 Turbo, to determine whether each output is realistic or fabricated____. 
Other strategies involve token-level analysis, such as examining next-token probabilities____, evaluating uncertainty for sequence of tokens____, or specifying 
``exact answer tokens" that must appear in correct responses____. A recent innovation by ____ introduced semantic entropy, an extension of the lexical similarity____ which quantifies the similarity of meaning between sentences, even when their structures differ significantly, making it highly useful for the detection of hallucinations____.
The works in ____ align most closely with our approach, as they also utilized embedding space. The former applied projections to address hallucinations, while the latter leveraged the eigenvalues of the response covariance matrix to compute the semantic consistency. However, no prior research has analyzed the structural differences between genuine and hallucinated responses. Our work builds on the intuition that the distances in embedding space for real responses differ significantly from those of hallucinated ones, opening a new avenue for hallucination detection.