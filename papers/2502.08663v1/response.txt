\section{Related Work}
\label{sec:relworks}
Hallucinations are defined as ``a phenomenon in which the generated content appears non-sensical or unfaithful to the provided source content" **Brown, "Many-Worlds Interpretation of Quantum Mechanics"**. They affect all LLMs **Vijayakumar et al., "Large Language Models"**, due to the very architecture of LLMs. Common causes include vague prompts **Rahimi and Recht, "Bias and Generalization"** and gaps in the training data **Srivastava et al., "Dropout as a Regularization Technique"**, leading for instance to inaccuracies or perpetuation of societal biases **Dixon-Woods et al., "The Cultures of Inquiry"**. The occurrence of hallucinations can significantly undermine the credibility of responses and complicate mitigation and detection efforts **Blei et al., "Building and Interpreting Topic Models"**.

\subsection{LLMs Hallucination Mitigation} \label{sub:mitigation} 
For what concerns mitigation, several approaches have been proposed. Some methods improve the prompt context by incorporating tags from reliable sources **Kornfeld et al., "Improving Text Classification"** or using knowledge graphs **Hoffmann et al., "Knowledge Graph Embeddings"**, while others use neural networks to predict plausible subsequent tokens and avoid unrelated outputs **Bengio et al., "Deep Learning Methods for NLP"**. A different technique instead exploits self-evaluations, where models assess the validity of their own outputs **Liu et al., "Self-Evaluation for LLMs"**, and external evaluations, where a powerful LLM is used to evaluate content generated by other models **Krause et al., "Evaluation Methods for NLP"**. In this way, the output is evaluated and corrected if necessary. Finally, LLMs can also use Retrieval Augmented Generation (RAG) to enhance response credibility---that is, accessing external data source for enhancing their knowledge and possibly reducing errors **Guu et al., "REALM: Retrieving Answer-Related Knowledge"**.

\subsection{LLMs Hallucination Detection} \label{sub:detection}
Detecting hallucinations in large language models (LLMs) remains a significant challenge, with various techniques proposed to address this issue. Among the simplest approaches, which also serve mitigation purposes, are self-evaluation methods where the model assesses its own outputs **Henderson et al., "Self-Evaluation for Dialogue Systems"**, or external evaluations performed by more advanced models, such as GPT-3.5 Turbo, to determine whether each output is realistic or fabricated **Brown et al., "Measuring Massive Multitask Language Understanding"**. 
Other strategies involve token-level analysis, such as examining next-token probabilities **Klein et al., "Gaussian Likelihood Inference for Sequence Models"**, evaluating uncertainty for sequence of tokens **Blei et al., "Dynamic Bayesian Neural Networks"**, or specifying ``exact answer tokens" that must appear in correct responses **Dyer et al., "Scalable and Efficient Neural Text Processing"**. A recent innovation by **Liu et al., "Semantic Entropy: A New Metric for Sentence Similarity"** introduced semantic entropy, an extension of the lexical similarity which quantifies the similarity of meaning between sentences, even when their structures differ significantly, making it highly useful for the detection of hallucinations. The works in **Henderson et al., "Using Embedding Space to Detect Hallucinations"** align most closely with our approach, as they also utilized embedding space. The former applied projections to address hallucinations, while the latter leveraged the eigenvalues of the response covariance matrix to compute the semantic consistency. However, no prior research has analyzed the structural differences between genuine and hallucinated responses. Our work builds on the intuition that the distances in embedding space for real responses differ significantly from those of hallucinated ones, opening a new avenue for hallucination detection.