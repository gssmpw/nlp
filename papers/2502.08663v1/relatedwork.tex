\section{Related Work}
\label{sec:relworks}
Hallucinations are defined as ``a phenomenon in which the generated content appears non-sensical or unfaithful to the provided source content"~\cite{filippova2020controlled, maynez2020faithfulness}. They affect all LLMs~\cite{huang2023survey}, due to the very architecture of LLMs. Common causes include vague prompts~\cite{ren2022out} and gaps in the training data~\cite{perkovic2024hallucinations}, leading for instance to inaccuracies or perpetuation of societal biases~\cite{ladhak2023pre, venkit2023nationality, giorgi2024human}. The occurrence of hallucinations can significantly undermine the credibility of responses and complicate mitigation and detection efforts~\cite{guerreiro2022looking, huang2025look}.

\subsection{LLMs Hallucination Mitigation} \label{sub:mitigation} 
For what concerns mitigation, several approaches have been proposed. Some methods improve the prompt context by incorporating tags from reliable sources~\cite{feldman2023trapping} or using knowledge graphs~\cite{martino2023knowledge}, while others use neural networks to predict plausible subsequent tokens and avoid unrelated outputs~\cite{verma2023reducing}. A different technique instead exploits self-evaluations, where models assess the validity of their own outputs~\cite{kadavath2022language, ji2023towards}, and external evaluations, where a powerful LLM is used to evaluate content generated by other models~\cite{hao2024quantifying}. In this way, the output is evaluated and corrected if necessary. Finally, LLMs can also use Retrieval Augmented Generation (RAG) to enhance response credibility---that is, accessing external data source for enhancing their knowledge and possibly reducing errors~\cite{fan2024survey,arslan2024survey}.

\subsection{LLMs Hallucination Detection} \label{sub:detection}
Detecting hallucinations in large language models (LLMs) remains a significant challenge, with various techniques proposed to address this issue. Among the simplest approaches, which also serve mitigation purposes, are self-evaluation methods where the model assesses its own outputs~\cite{kadavath2022language, lin2022teaching, manakul2023selfcheckgpt}, or external evaluations performed by more advanced models, such as GPT-3.5 Turbo, to determine whether each output is realistic or fabricated~\cite{friel2023chainpoll}. 
Other strategies involve token-level analysis, such as examining next-token probabilities~\cite{varshney2023stitch}, evaluating uncertainty for sequence of tokens~\cite{malinin2021uncertainty}, or specifying 
``exact answer tokens" that must appear in correct responses~\cite{orgad2024llms}. A recent innovation by \citet{kuhn2023semantic} introduced semantic entropy, an extension of the lexical similarity~\cite{lin2022truthfulqa} which quantifies the similarity of meaning between sentences, even when their structures differ significantly, making it highly useful for the detection of hallucinations~\cite{farquhar2024detecting}.
The works in \citet{du2024haloscope,chen2024inside} align most closely with our approach, as they also utilized embedding space. The former applied projections to address hallucinations, while the latter leveraged the eigenvalues of the response covariance matrix to compute the semantic consistency. However, no prior research has analyzed the structural differences between genuine and hallucinated responses. Our work builds on the intuition that the distances in embedding space for real responses differ significantly from those of hallucinated ones, opening a new avenue for hallucination detection.