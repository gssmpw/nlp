\section{Related Work}
\subsection{Diffusion Models}

Diffusion models have become a popular class of generative models due to their ability to model complex data distributions through a progressive denoising process. Recent works have explored various facets of diffusion models, ranging from their theoretical foundations to their practical applications in domains such as image generation, signal processing, and network analysis \cite{wang2024insectmamba}.

In the context of generative modeling, diffusion models were initially popularized for their capacity to generate high-quality images by simulating a reverse diffusion process that iteratively transforms noise into data samples. This approach contrasts with traditional models like GANs, which rely on adversarial training. Several studies have expanded on this idea by improving the efficiency and scalability of diffusion models. For example, the work in \cite{chen2024overview} offers a comprehensive survey on diffusion models, covering their applications in fields such as AI and computational biology \cite{wang2024diffusion}. The statistical properties and optimization challenges associated with these models are also discussed, highlighting the versatility and robustness of diffusion-based generative models.

Diffusion models have also been applied beyond image generation. For instance, \cite{li2023multivariate} examines their usage in multivariate time series data, proposing methods for optimizing the inference process to enhance sample quality. Additionally, the use of diffusion models in non-autoregressive text generation has garnered significant interest, as shown in \cite{li2023nonautoregressive}. This research shows how diffusion models can be adapted for sequential data generation, expanding their application scope to natural language processing tasks.

Further developments have sought to improve the practical implementation of diffusion models. Recent works have proposed methods for addressing indirect transmission in diffusion on dynamic networks \cite{shahzamal2019dynamic}, offering a more realistic model for diffusion processes in real-world systems like epidemiology and marketing. The integration of evolutionary algorithms with diffusion models is also explored in \cite{zhang2023diffusion}, demonstrating that these models can be treated as optimization algorithms in certain contexts.


\subsection{Large Language Models}

Large language models (LLMs) have become pivotal in advancing natural language processing (NLP \cite{zhou2022claret,zhou2024fine,zhou2022eventbert}), enabling state-of-the-art performance across a variety of tasks. Recent developments in LLMs have significantly improved their ability to handle different languages and domains. For instance, work on Cedille, a large autoregressive model trained specifically on French, highlights the significant performance improvements in zero-shot tasks for non-English languages \cite{cedille2022}. This is in line with the increasing focus on creating high-quality, monolingual models to enhance the accuracy of LLMs in low-resource settings, as evidenced by the Goldfish model, which introduces monolingual models for over 350 languages, offering better perplexity scores in specific languages when compared to multilingual models \cite{goldfish2024}.

In addition to language-specific models, there has been substantial interest in the application of LLMs to specific fields like vison generation \cite{zhou2024less}, vision understanding \cite{zhou2023style}, and reasoning \cite{zhou2021modeling}. The use of LLMs to improve accuracy in tasks such as genomic analysis has demonstrated their potential in specialized domains \cite{bioinformatics2024}. Furthermore, recent studies have explored the application of LLMs in psycholinguistics, suggesting that these models provide unique insights into the relationship between language and cognition \cite{psycholinguistics2023}.

LLMs have also been explored for multilingual and cross-lingual tasks \cite{zhou2021improving}. One such study examined the efficacy of commercial LLMs in handling various African languages, uncovering challenges in providing accurate translations and suggesting a need for better representation of African languages in future LLMs \cite{africanlanguages2023}. This concern has led to growing efforts in making LLMs more inclusive of underrepresented languages, especially in the context of non-English content analysis \cite{translation2023}. The exploration of LLMs in these contexts underscores the necessity of developing more adaptable models capable of handling diverse linguistic and cultural contexts.