\section{Related work}
\label{gen_inst}
\textbf{Music Foundation Model:} Music foundation models are pre-trained on large-scale music datasets, which are designed to gain a deeper understanding of underlying musical structures, genres, and instruments**Huang et al., "Music Foundation Models"**.
The existing music foundation models can be divided into two categories.
In the first category, the models are pre-trained with a single modality. This category includes **Baevski et al., "Wav2Vec 2.0"**, which is a self-supervised model that learns audio representations from raw audio through contrastive learning. Additionally, Li \textit{et al.}, **Li et al., "MERT: Music Encoder and Teacher for Music Understanding"** proposed MERT, a model for music understanding that takes advantage of Residual VQ-VAE (RVQ-VAE) and teacher models to extract musical features, facilitating pre-training based on mask language modeling (MLM). **Vandit et al., "Jukebox"** compresses raw audio into discrete codes using a multi-scale VQ-VAE and models them with an autoregressive Transformer, applied in**Li et al., "Audio-Visual Dance Generation"** to enhance previous hand-crafted audio feature extraction strategies for dance generation task.
In the second category, the models are pre-trained by multi-modal data, such as **Bertin et al., "CLAP: Conditional Latent Audio Process"** which leverages the latent space derived from both audio and text to develop continuous audio representations.
Building on this, **Dhariwal et al., "AudioLDM"** utilizes CLAP to train Latent Diffusion Models (LDMs) with audio embeddings, while text embeddings are used as conditions during sampling. 
**Wu et al., "Wav2CLIP: Audio-Visual Pre-training for Generalizable Audio Encoding"**, proposed Wav2CLIP based on **Radford et al., "CLIP: Learning Transferable Visual Representations from Natural Language Supervision"**, which projects audio into a shared embedding space along with images and text to pretrain the audio encoder.
However, the application of these models in music-driven dance generation remains under-explored. 
It is important to investigate how music foundation models can contribute to enhancing dance generation tasks.

\textbf{Music Driven Dance Generation:}
There has been extensive research exploring music-conditioned dance generation. 
Early studies**Huang et al., "Dance Generation using Music Conditional Variational Autoencoder"** formulate this task as a similarity-based retrieval problem due to limited training data, which substantially limits the diversity and creativity of the generated dance motions. 
With the development of deep learning, the mainstream approaches synthesize dance segments from scratch via motion prediction, with methods **Liu et al., "Convolutional Neural Network based Dance Motion Synthesis"**,**Li et al., "Recurrent Neural Network for Music-driven Dance Generation"**, and **Chen et al., "Transformer-based Dance Motion Prediction"**.
However, these frame-by-frame prediction models frequently face challenges like error accumulation and motion freezing**Huang et al., "Dance Generation using Music Conditional Variational Autoencoder"**.
Recent studies have focused on framing the task as a generative pipeline.
Built on VQ-VAE, **Li et al., "TM2D: Task-Agnostic Motion Generation via Text-Music-Video"** integrates both music and text instructions to generate dance movements that are consistent with the provided music and contain semantic information.
**Bailando: A Quantized Codebook for Dance Motion Synthesis using Reinforcement Learning** summarizes meaningful dance units into a quantized codebook and incorporates a reinforcement-learning-based evaluator to ensure alignment between beats and movement during dance generation. 
**EDGE: Diffusion-based Dance Generation with Physical Plausibility Evaluation** apply a diffusion-based dance generation model, and also introduce a novel evaluation method based on human physical plausibility. 
However, all these models are trained on datasets with 24 body joints and neglect the quality of the hand motions generated. To mitigate this issue, **Li et al., "FineNet: Fine-Grained Dance Motion Synthesis using Multi-Scale Feature Fusion"** proposed FineNet and introduced a new dataset with $52$ joints. 
Despite these developments, almost all the existing models rely on hand-crafted musical features, which may lack an understanding of the connection between music and dance movements.