\section{Related work}
\label{gen_inst}
\textbf{Music Foundation Model:} Music foundation models are pre-trained on large-scale music datasets, which are designed to gain a deeper understanding of underlying musical structures, genres, and instruments____.
The existing music foundation models can be divided into two categories.
In the first category, the models are pre-trained with a single modality. This category includes Wav2Vec 2.0____, which is a self-supervised model that learns audio representations from raw audio through contrastive learning. Additionally, Li \textit{et al.}____ proposed MERT, a model for music understanding that takes advantage of Residual VQ-VAE (RVQ-VAE) and teacher models to extract musical features, facilitating pre-training based on mask language modeling (MLM). Jukebox____ compresses raw audio into discrete codes using a multi-scale VQ-VAE and models them with an autoregressive Transformer, applied in____ to enhance previous hand-crafted audio feature extraction strategies for dance generation task.
In he second category, the models are pre-trained by multi-modal data, such as CLAP____ which leverages the latent space derived from both audio and text to develop continuous audio representations.
Building on this, AudioLDM____ utilizes CLAP to train Latent Diffusion Models (LDMs) with audio embeddings, while text embeddings are used as conditions during sampling. 
Wu \textit{et al.}____ proposed Wav2CLIP based on CLIP____, which projects audio into a shared embedding space along with images and text to pretrain the audio encoder.
However, the application of these models in music-driven dance generation remains under-explored. 
It is important to investigate how music foundation models can contribute to enhancing dance generation tasks.

\textbf{Music Driven Dance Generation:}
There has been extensive research exploring music-conditioned dance generation. 
Early studies____ formulate this task as a similarity-based retrieval problem due to limited training data, which substantially limits the diversity and creativity of the generated dance motions. 
With the development of deep learning, the mainstream approaches synthesize dance segments from scratch via motion prediction, with methods convolutional neural network (CNN)____, recurrent neural network (RNN)____, and Transformer____.
However, these frame-by-frame prediction models frequently face challenges like error accumulation and motion freezing____.
Recent studies have focused on framing the task as a generative pipeline.
Built on VQ-VAE, TM2D____ integrates both music and text instructions to generate dance movements that are consistent with the provided music and contain semantic information.
Bailando____ summarizes meaningful dance units into a quantized codebook and incorporates a reinforcement-learning-based evaluator to ensure alignment between beats and movement during dance generation. 
EDGE____ apply a diffusion-based dance generation model, and also introduce a novel evaluation method based on human physical plausibility. 
However, all these models are trained on datasets with 24 body joints and neglect the quality of the hand motions generated. To mitigate this issue, Li \textit{et al.}____ proposed FineNet and introduced a new dataset with $52$ joints. 
Despite these developments, almost all the existing models rely on hand-crafted musical features, which may lack an understanding of the connection between music and dance movements.