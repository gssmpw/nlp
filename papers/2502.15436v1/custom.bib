@misc{rslora,
      title={A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA}, 
      author={Damjan Kalajdzievski},
      year={2023},
      eprint={2312.03732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03732}, 
}

@inproceedings{mironov2017renyi,
  title={R{\'e}nyi differential privacy},
  author={Mironov, Ilya},
  booktitle={2017 IEEE 30th computer security foundations symposium (CSF)},
  pages={263--275},
  year={2017},
  organization={IEEE}
}

@inproceedings{moments,
  title={Subsampled r{\'e}nyi differential privacy and analytical moments accountant},
  author={Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
  booktitle={The 22nd international conference on artificial intelligence and statistics},
  pages={1226--1235},
  year={2019},
  organization={PMLR}
}

@inproceedings{bassily2014private,
  title={Private empirical risk minimization: Efficient algorithms and tight error bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={2014 IEEE 55th annual symposium on foundations of computer science},
  pages={464--473},
  year={2014},
  organization={IEEE}
}

@inproceedings{bun2014fingerprinting,
  title={Fingerprinting codes and the price of approximate differential privacy},
  author={Bun, Mark and Ullman, Jonathan and Vadhan, Salil},
  booktitle={Proceedings of the forty-sixth annual ACM symposium on Theory of computing},
  pages={1--10},
  year={2014}
}

@article{yu2021differentially,
  title={Differentially private fine-tuning of language models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and others},
  journal={arXiv preprint arXiv:2110.06500},
  year={2021}
}

@article{tang2024private,
  title={Private fine-tuning of large language models with zeroth-order optimization},
  author={Tang, Xinyu and Panda, Ashwinee and Nasr, Milad and Mahloujifar, Saeed and Mittal, Prateek},
  journal={arXiv preprint arXiv:2401.04343},
  year={2024}
}

@article{truong2021privacy,
  title={Privacy preservation in federated learning: An insightful survey from the GDPR perspective},
  author={Truong, Nguyen and Sun, Kai and Wang, Siyao and Guitton, Florian and Guo, YiKe},
  journal={Computers \& Security},
  volume={110},
  pages={102402},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{dgsgd,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@article{yousefpour2021opacus,
  title={Opacus: User-friendly differential privacy library in PyTorch},
  author={Yousefpour, Ashkan and Shilov, Igor and Sablayrolles, Alexandre and Testuggine, Davide and Prasad, Karthik and Malek, Mani and Nguyen, John and Ghosh, Sayan and Bharadwaj, Akash and Zhao, Jessica and others},
  journal={arXiv preprint arXiv:2109.12298},
  year={2021}
}

@article{bowman2015snli,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}

@article{wang2024flora,
  title={Flora: Federated fine-tuning large language models with heterogeneous low-rank adaptations},
  author={Wang, Ziyao and Shen, Zheyu and He, Yexiao and Sun, Guoheng and Wang, Hongyi and Lyu, Lingjuan and Li, Ang},
  journal={arXiv preprint arXiv:2409.05976},
  year={2024}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}
@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}
@article{zhu2024asymmetry,
  title={Asymmetry in low-rank adapters of foundation models},
  author={Zhu, Jiacheng and Greenewald, Kristjan and Nadjahi, Kimia and Borde, Haitz S{\'a}ez de Oc{\'a}riz and Gabrielsson, Rickard Br{\"u}el and Choshen, Leshem and Ghassemi, Marzyeh and Yurochkin, Mikhail and Solomon, Justin},
  journal={arXiv preprint arXiv:2402.16842},
  year={2024}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{cr-dataset,
      title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models}, 
      author={Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Ka-Wei Lee},
      year={2023},
      eprint={2304.01933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01933}, 
}

@misc{mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{metamathqa,
      title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}, 
      author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
      year={2024},
      eprint={2309.12284},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12284}, 
}

@misc{gsm8k,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{math,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.03874}, 
}

@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@inproceedings{vaswanietal,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama-2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@misc{zhang2023lorafamemoryefficientlowrankadaptation,
      title={LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning}, 
      author={Longteng Zhang and Lin Zhang and Shaohuai Shi and Xiaowen Chu and Bo Li},
      year={2023},
      eprint={2308.03303},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03303}, 
}

@misc{tian2024hydraloraasymmetricloraarchitecture,
      title={HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning}, 
      author={Chunlin Tian and Zhan Shi and Zhijiang Guo and Li Li and Chengzhong Xu},
      year={2024},
      eprint={2404.19245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.19245}, 
}

@article{tian2022fedbert,
  title={FedBERT: When federated learning meets pre-training},
  author={Tian, Yuanyishu and Wan, Yao and Lyu, Lingjuan and Yao, Dezhong and Jin, Hai and Sun, Lichao},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={13},
  number={4},
  pages={1--26},
  year={2022},
  publisher={ACM New York, NY}
}

@article{zhang2022federated,
  title={When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods},
  author={Zhang, Zhuo and Yang, Yuanhang and Dai, Yong and Qu, Lizhen and Xu, Zenglin},
  journal={arXiv preprint arXiv:2212.10025},
  year={2022}
}

@inproceedings{kuang2024federatedscope,
  title={Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning},
  author={Kuang, Weirui and Qian, Bingchen and Li, Zitao and Chen, Daoyuan and Gao, Dawei and Pan, Xuchen and Xie, Yuexiang and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5260--5271},
  year={2024}
}

@article{babakniya2023slora,
  title={SLoRA: Federated parameter efficient fine-tuning of language models},
  author={Babakniya, Sara and Elkordy, Ahmed Roushdy and Ezzeldin, Yahya H and Liu, Qingfeng and Song, Kee-Bong and El-Khamy, Mostafa and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2308.06522},
  year={2023}
}

@article{huang2022large,
  title={Are large pre-trained language models leaking your personal information?},
  author={Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2205.12628},
  year={2022}
}

@article{zhang2021survey,
  title={A survey on federated learning},
  author={Zhang, Chen and Xie, Yu and Bai, Hang and Yu, Bin and Li, Weihong and Gao, Yuan},
  journal={Knowledge-Based Systems},
  volume={216},
  pages={106775},
  year={2021},
  publisher={Elsevier}
}

@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}

@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@misc{wang2019gluemultitaskbenchmarkanalysis,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.07461}, 
}

@article{novikova2017e2e,
  title={The E2E dataset: New challenges for end-to-end generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  journal={arXiv preprint arXiv:1706.09254},
  year={2017}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@article{sun2024improving,
  title={Improving loRA in privacy-preserving federated learning},
  author={Sun, Youbang and Li, Zitao and Li, Yaliang and Ding, Bolin},
  journal={arXiv preprint arXiv:2403.12313},
  year={2024}
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{konečný2017federatedlearningstrategiesimproving,
      title={Federated Learning: Strategies for Improving Communication Efficiency}, 
      author={Jakub Konečný and H. Brendan McMahan and Felix X. Yu and Peter Richtárik and Ananda Theertha Suresh and Dave Bacon},
      year={2017},
      eprint={1610.05492},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1610.05492}, 
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@misc{bonawitz2019federatedlearningscaledesign,
      title={Towards Federated Learning at Scale: System Design}, 
      author={Keith Bonawitz and Hubert Eichner and Wolfgang Grieskamp and Dzmitry Huba and Alex Ingerman and Vladimir Ivanov and Chloe Kiddon and Jakub Konečný and Stefano Mazzocchi and H. Brendan McMahan and Timon Van Overveldt and David Petrou and Daniel Ramage and Jason Roselander},
      year={2019},
      eprint={1902.01046},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.01046}, 
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}
@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{FedIT,
  title={Towards building the federatedGPT: Federated instruction tuning},
  author={Zhang, Jianyi and Vahidian, Saeed and Kuo, Martin and Li, Chunyuan and Zhang, Ruiyi and Yu, Tong and Wang, Guoyin and Chen, Yiran},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6915--6919},
  year={2024},
  organization={IEEE}
}

@inproceedings{lai2022fedscale,
  title={Fedscale: Benchmarking model and system performance of federated learning at scale},
  author={Lai, Fan and Dai, Yinwei and Singapuram, Sanjay and Liu, Jiachen and Zhu, Xiangfeng and Madhyastha, Harsha and Chowdhury, Mosharaf},
  booktitle={International conference on machine learning},
  pages={11814--11827},
  year={2022},
  organization={PMLR}
}

@article{he2020fedml,
  title={Fedml: A research library and benchmark for federated machine learning},
  author={He, Chaoyang and Li, Songze and So, Jinhyun and Zeng, Xiao and Zhang, Mi and Wang, Hongyi and Wang, Xiaoyang and Vepakomma, Praneeth and Singh, Abhishek and Qiu, Hang and others},
  journal={arXiv preprint arXiv:2007.13518},
  year={2020}
}

@article{bai2024federated,
  title={Federated fine-tuning of large language models under heterogeneous language tasks and client resources},
  author={Bai, Jiamu and Chen, Daoyuan and Qian, Bingchen and Yao, Liuyi and Li, Yaliang},
  journal={arXiv preprint arXiv:2402.11505},
  year={2024}
}

@inproceedings{LLM_mult,
  title={Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts},
  author={Teng Wang and Zhenqi He and Wing-Yin Yu and Xiaojin Fu and Xiongwei Han},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:272694014}
}

@misc{gpt4_agi,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.12712}, 
}

@inproceedings{prefix_tuning,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    
}

@inproceedings{prompt_tuning,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    
}

@misc{adaptor_layer,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00751}, 
}


@misc{adalora,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

@inproceedings{qlora,
author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
title = {QLORA: efficient finetuning of quantized LLMs},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {441},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}
@misc{longlora,
      title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models}, 
      author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
      year={2024},
      eprint={2309.12307},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12307}, 
}
@misc{merge_peft,
      title={PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and Ensemble Techniques}, 
      author={Tzu-Han Lin and How-Shing Wang and Hao-Yung Weng and Kuang-Chen Peng and Zih-Ching Chen and Hung-yi Lee},
      year={2024},
      eprint={2401.02122},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.02122}, 
}

@misc{scaling_llm,
      title={When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method}, 
      author={Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
      year={2024},
      eprint={2402.17193},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17193}, 
}


@misc{hetero_lora,
      title={Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models}, 
      author={Yae Jee Cho and Luyang Liu and Zheng Xu and Aldi Fahrezi and Gauri Joshi},
      year={2024},
      eprint={2401.06432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.06432}, 
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third international workshop on paraphrasing (IWP2005)},
  year={2005}
}

@article{warstadt-etal-2019-neural,
    title = "Neural Network Acceptability Judgments",
    author = "Warstadt, Alex  and
      Singh, Amanpreet  and
      Bowman, Samuel R.",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1040",
    doi = "10.1162/tacl_a_00290",
    pages = "625--641",
    abstract = "This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.{'}s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.",
}

@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}

@article{cer2017semeval,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}

@article{eckart1936theorem1,
  title={The approximation of one matrix by another of lower rank},
  author={Eckart, Carl and Young, Gale},
  journal={Psychometrika},
  volume={1},
  number={3},
  pages={211--218},
  year={1936},
  publisher={Springer}
}

@article{singhal2024exact,
  title   = {FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models},
  author  = {Singhal, Raghav and Ponkshe, Kaustubh and Vepakomma, Praneeth},
  journal = {arXiv preprint arXiv:2410.09432},
  year    = {2025}
}


@article{10.14778/3503585.3503598,
author = {Li, Zitao and Ding, Bolin and Zhang, Ce and Li, Ninghui and Zhou, Jingren},
title = {Federated matrix factorization with privacy guarantee},
year = {2021},
issue_date = {December 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3503585.3503598},
doi = {10.14778/3503585.3503598},
abstract = {Matrix factorization (MF) approximates unobserved ratings in a rating matrix, whose rows correspond to users and columns correspond to items to be rated, and has been serving as a fundamental building block in recommendation systems. This paper comprehensively studies the problem of matrix factorization in different federated learning (FL) settings, where a set of parties want to cooperate in training but refuse to share data directly. We first propose a generic algorithmic framework for various settings of <u>f</u>ederated <u>m</u>atrix <u>f</u>actorization (FMF) and provide a theoretical convergence guarantee. We then systematically characterize privacy-leakage risks in data collection, training, and publishing stages for three different settings and introduce privacy notions to provide end-to-end privacy protections. The first one is vertical federated learning (VFL), where multiple parties have the ratings from the same set of users but on disjoint sets of items. The second one is horizontal federated learning (HFL), where parties have ratings from different sets of users but on the same set of items. The third setting is local federated learning (LFL), where the ratings of the users are only stored on their local devices. We introduce adapted versions of FMF with the privacy notions guaranteed in the three settings. In particular, a new private learning technique called embedding clipping is introduced and used in all the three settings to ensure differential privacy. For the LFL setting, we combine differential privacy with secure aggregation to protect the communication between user devices and the server with a strength similar to the local differential privacy model, but much better accuracy. We perform experiments to demonstrate the effectiveness of our approaches.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {900–913},
numpages = {14}
}

@inproceedings{Abadi_2016, series={CCS’16},
   title={Deep Learning with Differential Privacy},
   url={http://dx.doi.org/10.1145/2976749.2978318},
   DOI={10.1145/2976749.2978318},
   booktitle={Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
   publisher={ACM},
   author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
   year={2016},
   month=oct, collection={CCS’16} }

@misc{meng2024pissaprincipalsingularvalues,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02948}, 
}

@misc{kopiczko2024veravectorbasedrandommatrix,
      title={VeRA: Vector-based Random Matrix Adaptation}, 
      author={Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
      year={2024},
      eprint={2310.11454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11454}, 
}

@article{ponkshe2024initialization,
  title={Initialization using update approximation is a silver bullet for extremely efficient low-rank fine-tuning},
  author={Ponkshe, Kaustubh and Singhal, Raghav and Gorbunov, Eduard and Tumanov, Alexey and Horvath, Samuel and Vepakomma, Praneeth},
  journal={arXiv preprint arXiv:2411.19557},
  year={2024}
}

@misc{bałazy2024loraxslowrankadaptationextremely,
      title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters}, 
      author={Klaudia Bałazy and Mohammadreza Banaei and Karl Aberer and Jacek Tabor},
      year={2024},
      eprint={2405.17604},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17604}, 
}

@misc{zhang2024buildingfederatedgptfederated,
      title={Towards Building the Federated GPT: Federated Instruction Tuning}, 
      author={Jianyi Zhang and Saeed Vahidian and Martin Kuo and Chunyuan Li and Ruiyi Zhang and Tong Yu and Yufan Zhou and Guoyin Wang and Yiran Chen},
      year={2024},
      eprint={2305.05644},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.05644}, 
}


@misc{dettmers2023qloraefficientfinetuningquantized,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}

@misc{zhang2023adaloraadaptivebudgetallocation,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

@misc{liu2024doraweightdecomposedlowrankadaptation,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}


@inproceedings{dwork2006differential,
  title={Differential privacy},
  author={Dwork, Cynthia},
  booktitle={International colloquium on automata, languages, and programming},
  pages={1--12},
  year={2006},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@misc{mahla2025exploringgradientsubspacesaddressing,
      title={Exploring Gradient Subspaces: Addressing and Overcoming LoRA's Limitations in Federated Fine-Tuning of Large Language Models}, 
      author={Navyansh Mahla and Kshitij Sharad Jadhav and Ganesh Ramakrishnan},
      year={2025},
      eprint={2410.23111},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.23111}, 
}