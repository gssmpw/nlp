@article{10.14778/3503585.3503598,
author = {Li, Zitao and Ding, Bolin and Zhang, Ce and Li, Ninghui and Zhou, Jingren},
title = {Federated matrix factorization with privacy guarantee},
year = {2021},
issue_date = {December 2021},
publisher = {VLDB Endowment},
volume = {15},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3503585.3503598},
doi = {10.14778/3503585.3503598},
abstract = {Matrix factorization (MF) approximates unobserved ratings in a rating matrix, whose rows correspond to users and columns correspond to items to be rated, and has been serving as a fundamental building block in recommendation systems. This paper comprehensively studies the problem of matrix factorization in different federated learning (FL) settings, where a set of parties want to cooperate in training but refuse to share data directly. We first propose a generic algorithmic framework for various settings of <u>f</u>ederated <u>m</u>atrix <u>f</u>actorization (FMF) and provide a theoretical convergence guarantee. We then systematically characterize privacy-leakage risks in data collection, training, and publishing stages for three different settings and introduce privacy notions to provide end-to-end privacy protections. The first one is vertical federated learning (VFL), where multiple parties have the ratings from the same set of users but on disjoint sets of items. The second one is horizontal federated learning (HFL), where parties have ratings from different sets of users but on the same set of items. The third setting is local federated learning (LFL), where the ratings of the users are only stored on their local devices. We introduce adapted versions of FMF with the privacy notions guaranteed in the three settings. In particular, a new private learning technique called embedding clipping is introduced and used in all the three settings to ensure differential privacy. For the LFL setting, we combine differential privacy with secure aggregation to protect the communication between user devices and the server with a strength similar to the local differential privacy model, but much better accuracy. We perform experiments to demonstrate the effectiveness of our approaches.},
journal = {Proc. VLDB Endow.},
month = dec,
pages = {900–913},
numpages = {14}
}

@misc{adalora,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

@article{babakniya2023slora,
  title={SLoRA: Federated parameter efficient fine-tuning of language models},
  author={Babakniya, Sara and Elkordy, Ahmed Roushdy and Ezzeldin, Yahya H and Liu, Qingfeng and Song, Kee-Bong and El-Khamy, Mostafa and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2308.06522},
  year={2023}
}

@misc{bałazy2024loraxslowrankadaptationextremely,
      title={LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters}, 
      author={Klaudia Bałazy and Mohammadreza Banaei and Karl Aberer and Jacek Tabor},
      year={2024},
      eprint={2405.17604},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17604}, 
}

@inproceedings{dgsgd,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@inproceedings{dwork2006differential,
  title={Differential privacy},
  author={Dwork, Cynthia},
  booktitle={International colloquium on automata, languages, and programming},
  pages={1--12},
  year={2006},
  organization={Springer}
}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}

@misc{hetero_lora,
      title={Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models}, 
      author={Yae Jee Cho and Luyang Liu and Zheng Xu and Aldi Fahrezi and Gauri Joshi},
      year={2024},
      eprint={2401.06432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.06432}, 
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={Foundations and trends{\textregistered} in machine learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021},
  publisher={Now Publishers, Inc.}
}

@misc{kopiczko2024veravectorbasedrandommatrix,
      title={VeRA: Vector-based Random Matrix Adaptation}, 
      author={Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
      year={2024},
      eprint={2310.11454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11454}, 
}

@inproceedings{kuang2024federatedscope,
  title={Federatedscope-llm: A comprehensive package for fine-tuning large language models in federated learning},
  author={Kuang, Weirui and Qian, Bingchen and Li, Zitao and Chen, Daoyuan and Gao, Dawei and Pan, Xuchen and Xie, Yuexiang and Li, Yaliang and Ding, Bolin and Zhou, Jingren},
  booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={5260--5271},
  year={2024}
}

@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}

@misc{liu2024doraweightdecomposedlowrankadaptation,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}

@article{lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@misc{meng2024pissaprincipalsingularvalues,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02948}, 
}

@article{ponkshe2024initialization,
  title={Initialization using update approximation is a silver bullet for extremely efficient low-rank fine-tuning},
  author={Ponkshe, Kaustubh and Singhal, Raghav and Gorbunov, Eduard and Tumanov, Alexey and Horvath, Samuel and Vepakomma, Praneeth},
  journal={arXiv preprint arXiv:2411.19557},
  year={2024}
}

@inproceedings{qlora,
author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
title = {QLORA: efficient finetuning of quantized LLMs},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {441},
numpages = {28},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@misc{rslora,
      title={A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA}, 
      author={Damjan Kalajdzievski},
      year={2023},
      eprint={2312.03732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.03732}, 
}

@misc{scaling_llm,
      title={When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method}, 
      author={Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
      year={2024},
      eprint={2402.17193},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17193}, 
}

@article{singhal2024exact,
  title   = {FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models},
  author  = {Singhal, Raghav and Ponkshe, Kaustubh and Vepakomma, Praneeth},
  journal = {arXiv preprint arXiv:2410.09432},
  year    = {2025}
}

@article{sun2024improving,
  title={Improving loRA in privacy-preserving federated learning},
  author={Sun, Youbang and Li, Zitao and Li, Yaliang and Ding, Bolin},
  journal={arXiv preprint arXiv:2403.12313},
  year={2024}
}

@article{tian2022fedbert,
  title={FedBERT: When federated learning meets pre-training},
  author={Tian, Yuanyishu and Wan, Yao and Lyu, Lingjuan and Yao, Dezhong and Jin, Hai and Sun, Lichao},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={13},
  number={4},
  pages={1--26},
  year={2022},
  publisher={ACM New York, NY}
}

@article{wang2024flora,
  title={Flora: Federated fine-tuning large language models with heterogeneous low-rank adaptations},
  author={Wang, Ziyao and Shen, Zheyu and He, Yexiao and Sun, Guoheng and Wang, Hongyi and Lyu, Lingjuan and Li, Ang},
  journal={arXiv preprint arXiv:2409.05976},
  year={2024}
}

@article{zhang2022federated,
  title={When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods},
  author={Zhang, Zhuo and Yang, Yuanhang and Dai, Yong and Qu, Lizhen and Xu, Zenglin},
  journal={arXiv preprint arXiv:2212.10025},
  year={2022}
}

@misc{zhang2024buildingfederatedgptfederated,
      title={Towards Building the Federated GPT: Federated Instruction Tuning}, 
      author={Jianyi Zhang and Saeed Vahidian and Martin Kuo and Chunyuan Li and Ruiyi Zhang and Tong Yu and Yufan Zhou and Guoyin Wang and Yiran Chen},
      year={2024},
      eprint={2305.05644},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.05644}, 
}

@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}

