
\section{Experiments \& Results}\label{sec:experiments}


\begin{table*}[!h]
\centering
\setlength{\tabcolsep}{3.5pt}
\small
\begin{tabular}{lcc|ccccccccc}
\toprule
\multirow{2}{*}{\bf Method} & \multirow{2}{*}{\bf Rank} & \multirow{2}{*}{\bf \# Comm. (M) ($\downarrow$)} & \multicolumn{9}{c}{\textbf{Accuracy ($\uparrow$)}} \\
 &  &  & \textbf{BoolQ} & \textbf{PIQA} & \textbf{SIQA} & \textbf{HellaS.} & \textbf{WinoG.} & \textbf{ARC-e} & \textbf{ARC-c} & \textbf{OBQA} & \textbf{Avg} \\
\midrule
FedIT & $32$ & $48.63$ & $62.99$ & $81.50$ & $73.13$ & $76.83$ & $71.51$ & $84.89$ & $70.65$ & $70.62$ & $74.02$ \\
FFA-LoRA   & $32$ & $24.31$ & $62.87$ & $80.03$ & $68.53$ & $70.02$ & $65.56$ & $82.95$ & $66.38$ & $66.85$ & $70.40$ \\
FedEx-LoRA & $32$ & $243.15$ & $65.05$ & $82.81$ & $74.67$ & $81.84$ & $76.01$ & $86.32$ & $71.42$ & $73.81$ & $76.49$ \\
FLoRA & $32$ & $243.15$ & $65.05$ & $82.81$ & $74.67$ & $81.84$ & $76.01$ & $86.32$ & $71.42$ & $73.81$ & $76.49$ \\
\rowcolor{cyan!10} 
Fed-SB & $120$ & $2.83$  & $64.86$ & $81.66$ & $74.87$ & $81.67$ & $75.22$ & $86.03$ & $70.56$ & $72.25$ & $75.89$ \\
\rowcolor{cyan!10} 
Fed-SB & $160$ & $5.02$  & $65.57$ & $82.37$ & $76.15$ & $84.10$ & $77.98$ & $86.62$ & $72.10$ & $73.63$ & $77.32$ \\
\rowcolor{cyan!10} 
Fed-SB & $200$ & $7.85$  & $\mathbf{66.66}$ & $\mathbf{83.79}$ & $\mathbf{77.22}$ & $\mathbf{85.42}$ & $\mathbf{79.56}$ & $\mathbf{87.46}$ & $\mathbf{72.53}$ & $\mathbf{76.02}$ & $\mathbf{78.58}$ \\
\bottomrule
\end{tabular}
\caption{Federated fine-tuning performance of Llama-3.2 3B across eight commonsense reasoning datasets. \# Comm. denotes the number of parameters communicated per round. Best results are in \textbf{bold}.
}
\label{tab:commonsense}
\end{table*}

\begin{table*}[!h]
\centering
\setlength{\tabcolsep}{4.5pt}
\small
\begin{tabular}{llcc|cc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Rank}} & \multirow{2}{*}{\textbf{\# Comm. (M) ($\downarrow$)}} & \multicolumn{2}{c}{\textbf{Accuracy ($\uparrow$)}} \\
\cmidrule{5-6}
& & & & \textbf{GSM8K} & \textbf{MATH} \\
\midrule
\multirow{7}{*}{Mistral-7B} 
    & FedIT       & $32$  & $83.88$   & $52.91$ & $12.26$ \\
    & FFA-LoRA    & $32$  & $41.94$   & $53.67$ & $12.46$ \\
    & FedEx-LoRA  & $32$  & $2097.34$ & $54.28$ & $12.92$ \\
    & FLoRA       & $32$  & $2097.34$ & $54.28$ & $12.92$ \\
    & \cellcolor{cyan!10}Fed-SB       & \cellcolor{cyan!10}$120$  & \cellcolor{cyan!10}$3.22$    & \cellcolor{cyan!10}$54.44$ & \cellcolor{cyan!10}$\mathbf{14.06}$ \\
    & \cellcolor{cyan!10}Fed-SB       & \cellcolor{cyan!10}$160$  & \cellcolor{cyan!10}$5.73$    & \cellcolor{cyan!10}$54.81$ & \cellcolor{cyan!10}$13.74$ \\
    & \cellcolor{cyan!10}Fed-SB       & \cellcolor{cyan!10}$200$  & \cellcolor{cyan!10}$8.96$    & \cellcolor{cyan!10}$\mathbf{56.18}$ & \cellcolor{cyan!10}$13.76$ \\
\midrule
\multirow{7}{*}{Gemma-2 9B} 
    & FedIT       & $32$  & $108.04$  & $74.22$ & $36.30$ \\
    & FFA-LoRA    & $32$  & $54.02$   & $75.06$ & $35.44$ \\
    & FedEx-LoRA  & $32$  & $2701.12$ & $74.68$ & $36.70$ \\
    & FLoRA       & $32$  & $2701.12$ & $74.68$ & $36.70$ \\
    & \cellcolor{cyan!10}Fed-SB       & \cellcolor{cyan!10}$120$  & \cellcolor{cyan!10}$4.23$    & \cellcolor{cyan!10}$74.75$ & \cellcolor{cyan!10}$36.36$ \\
    & \cellcolor{cyan!10}Fed-SB       & \cellcolor{cyan!10}$160$  & \cellcolor{cyan!10}$7.53$    & \cellcolor{cyan!10}$76.88$ & \cellcolor{cyan!10}$36.94$ \\
    & \cellcolor{cyan!10}Fed-SB       & \cellcolor{cyan!10}$200$  & \cellcolor{cyan!10}$11.76$   & \cellcolor{cyan!10}$\mathbf{77.03}$ & \cellcolor{cyan!10}$\mathbf{37.56}$ \\
\bottomrule
\end{tabular}
\caption{Federated fine-tuning performance of Mistral-7B and Gemma-2 9B on arithmetic reasoning benchmarks GSM8K and MATH. \# Comm. denotes the number of parameters communicated per round. Best results are in \textbf{bold}.
}
\label{tab:arithmetic}
\end{table*}

\quad \textbf{Overview.}
We evaluate across three diverse NLP benchmarks, covering models that span from BERT-base (110M) to Gemma-2 (9B), thereby encompassing both masked and autoregressive architectures. 
Specifically, we fine-tune Mistral-7B \citep{mistral7b}, Gemma-2 9B \citep{gemma2}, Llama-3.2 3B \citep{llama3}, and BERT-base \citep{devlin2018bert}. 
% All implementations are built using PyTorch \citep{paszke2019pytorch}, utilizing the HuggingFace Transformers library \citep{wolf2020transformers}. 
Experiments are conducted on a \textbf{single NVIDIA A6000 GPU (48 GB)}, with results averaged over three random runs. 
Our experiments consider both performance and communication efficiency.
To optimize memory efficiency, all base models (except BERT) are loaded in \texttt{\textbf{torch.bfloat16}}. 
Detailed dataset specifications can be found in Appendix \ref{app:datasets}. 
For federated data distribution, we adopt a standard protocol where client datasets are randomly sampled, following established practice in FL \citep{sun2024improving, he2020fedml, lai2022fedscale}.
Detailed settings are provided in Appendix \ref{app:hyperparams}. 
\\

\textbf{Baselines.}
We evaluate Fed-SB against several SOTA federated FT approaches described previously, considering both private and non-private settings. 
Specifically, we compare it with \textbf{FedIT}, \textbf{FedEx-LoRA}, \textbf{FLoRA}, and \textbf{FFA-LoRA}. 
Where applicable, we also include comparisons with standard centralized \textbf{LoRA} \citep{lora}.
% \textbf{FedIT} \citep{FedIT} applies standard federated averaging (FedAvg, \citet{mcmahan2017communication}) to LoRA.  
% \textbf{FedEx-LoRA} \citep{singhal2024exact} and \textbf{FLoRA} \citep{wang2024flora} address inexact aggregation in LoRA by introducing adjustments to the error residual term, but differ in their re-initialization strategy at the end of each communication round.  
% \textbf{FFA-LoRA} \citep{sun2024improving} ensures exact aggregation by freezing $\mathbf{A}$ matrices while training only $\mathbf{B}$, which comes at the expense of losing the benefits of jointly optimizing $\mathbf{A}$.  


\subsection{Instruction Tuning}
\quad \textbf{Details.} 
We conduct experiments in the \textbf{federated non-private} setting across two reasoning tasks: commonsense reasoning and arithmetic reasoning. 
For \textbf{commonsense reasoning}, we fine-tune Llama-3.2 3B on \textsc{CommonSense170K}—a dataset aggregating eight commonsense reasoning corpora \citep{cr-dataset}—and evaluate its effectiveness across all constituent datasets. The experiments are performed in a cross-silo federated learning setup involving $5$ clients. 
For \textbf{arithmetic reasoning}, we fine-tune Mistral-7B \citep{mistral7b} and Gemma-2 9B \citep{gemma2} on 20K samples from the MetaMathQA dataset \citep{metamathqa} and assess their performance on the GSM8K \citep{gsm8k} and MATH \citep{math} benchmarks. In this setup, we distribute the federated training across $25$ clients.   
In both cases, we apply LoRA modules to the key, query, value, attention output, and all fully connected weights. 
\\

\textbf{Results} (Tables \ref{tab:commonsense}, \ref{tab:arithmetic}).
%Results for commonsense and arithmetic reasoning are presented in Tables \ref{tab:commonsense} and \ref{tab:arithmetic}, respectively. 
Our method \textbf{significantly advances the performance-communication cost Pareto frontier}, outperforming all previous baselines in both accuracy and communication efficiency \textbf{across all models and benchmarks}.
Figure \ref{fig:results-it} further illustrates this significant improvement.

\begin{table*}[!h]
\centering
\setlength{\tabcolsep}{4.5pt}
\small
\begin{tabular}{lcc|ccccc}
\toprule
\multirow{2}{*}{\bf Method} & \multirow{2}{*}{\bf Rank} & \multirow{2}{*}{\bf \# Params. (K) ($\downarrow$)} & \multicolumn{5}{c}{\textbf{Accuracy ($\uparrow$)}} \\
 &  &  & $\mathbf{\epsilon=1}$ & $\mathbf{\epsilon=3}$ & $\mathbf{\epsilon=5}$ & $\mathbf{\epsilon=7.5}$ & $\mathbf{\epsilon=10}$ \\
\midrule
Cent. LoRA & $32$ & $1181.96$ & $66.49$ & $67.79$ & $68.17$ & $70.78$ & $70.81$  \\
Cent. FFA-LoRA   & $32$ & $592.13$ &  $74.40$ & $75.02$ & $75.02$ & $76.14$ & $76.60$  \\
\rowcolor{cyan!10} 
Cent. Fed-SB & $32$ & $26.88$  & $73.99$ & $75.09$ & $74.45$ & $77.01$ & $76.24$  \\
\rowcolor{cyan!10} 
Cent. Fed-SB & $48$ & $57.59$  & $\mathbf{75.98}$ & $75.70$ & $76.58$ & $76.77$ & $77.96$ \\
\rowcolor{cyan!10} 
Cent. Fed-SB & $64$ & $100.61$ & $75.81$  & $\mathbf{77.07}$& $\mathbf{77.59}$ & $\mathbf{78.75}$ & $\mathbf{78.08}$\\
\bottomrule
\end{tabular}
\caption{Centralized (Cent.) private fine-tuning of BERT-base on SNLI for varying values of $\epsilon$. A smaller $\epsilon$ indicates a stricter privacy budget. \# Params. denotes the number of trainable parameters. Best results are in \textbf{bold}.
}
\label{tab:dp-central}
\end{table*}

\begin{table*}[!h]
\centering
\setlength{\tabcolsep}{4.5pt}
\small
\begin{tabular}{lcc|ccccc}
\toprule
\multirow{2}{*}{\bf Method} & \multirow{2}{*}{\bf Rank} & \multirow{2}{*}{\bf \# Comm. (K) ($\downarrow$)} & \multicolumn{5}{c}{\textbf{Accuracy ($\uparrow$)}} \\
  &  &  & $\mathbf{\epsilon=1}$ & $\mathbf{\epsilon=3}$ & $\mathbf{\epsilon=5}$ & $\mathbf{\epsilon=7.5}$ & $\mathbf{\epsilon=10}$ \\
\midrule
FedIT & $32$ & $1181.96$ & $49.57$ & $51.29$ & $48.53$ &  $55.63$ & $60.96$  \\
FFA-LoRA   & $32$ & $592.13$ & $70.11$ & $71.49$ & $72.69$ & $73.27$ & $74.02$  \\
FedEx-LoRA   & $32$ & $3541.26$ & $67.38$& $69.68$ & $72.92$& $71.89$ & $74.33$  \\
FLoRA   & $32$ & $3541.26$ & $67.38$ & $69.68$ & $72.92$& $71.89$ & $74.33$  \\
\rowcolor{cyan!10} 
Fed-SB & $32$ & $26.88$  & $70.33$ & $72.68$ & $73.57$& $73.62$ & $73.85$  \\
\rowcolor{cyan!10} 
Fed-SB & $48$ & $57.59$ & $73.7$ & $74.74$ & $73.66$& $74.75$ & $75.02$ \\
\rowcolor{cyan!10} 
Fed-SB & $64$ & $100.61$ & $\mathbf{73.83}$ & $\mathbf{74.88}$& $\mathbf{76.27}$ & $\mathbf{75.75}$ & $\mathbf{75.86}$\\
\bottomrule
\end{tabular}
\caption{Federated private fine-tuning of BERT-base on SNLI for varying values of $\epsilon$. A smaller $\epsilon$ indicates a stricter privacy budget. \# Comm. denotes the number of parameters communicated per round. Best results are in \textbf{bold}.
}
\label{tab:fed-central}
\end{table*}

\textbf{Commonsense Reasoning} (Table \ref{tab:commonsense}). 
Fed-SB ($r=200$) achieves an average accuracy improvement of 4.56\% over FedIT while requiring \textbf{6×} lower communication cost. 
Additionally, Fed-SB ($r=200$) surpasses the previous SOTA performance methods FedEx-LoRA/FLoRA by 2.09\%, while reducing communication cost by an impressive \textbf{31×}. 
Notably, while the communication cost of FedEx-LoRA/FLoRA scales linearly with the number of clients, our method maintains a constant, client-independent communication cost. 
These results are obtained with just $5$ clients, implying that the full extent of our method’s communication efficiency is not fully depicted in this experiment. 
As the number of clients increases, the relative advantage of Fed-SB over existing methods grows even further.
\\

\textbf{Arithmetic Reasoning} (Table \ref{tab:arithmetic}): For Mistral-7B, Fed-SB ($r=200$) outperforms FedEx-LoRA/FLoRA on GSM8K by 1.90\%, while achieving an impressive \textbf{234×} reduction in communication cost. 
Additionally, Fed-SB ($r=200$) surpasses FFA-LoRA on GSM8K by 2.51\%, with an approximately \textbf{5×} lower communication cost.
Similarly, for Gemma-2 9B, Fed-SB ($r=200$) outperforms FedEx-LoRA/FLoRA on MATH by 0.86\%, while reducing communication cost by \textbf{230×}.


\subsection{(Federated) Private Fine-Tuning}



\quad \textbf{Details.}
We fine-tune BERT-base \citep{devlin2018bert} on SNLI \citep{bowman2015snli}, a standard benchmark for natural language inference. 
Following \citet{lora}, we apply LoRA modules only to the self-attention layers.  
Our evaluation considers two DP settings: a \textbf{centralized private} setup and a \textbf{federated private} setup. 
To enforce DP guarantees during training, we use the Opacus library \citep{yousefpour2021opacus} with the DP-SGD optimizer \citep{dgsgd}.  
In the federated setting, training is conducted in a cross-silo setup with $3$ clients.
% In the centralized setup, we fine-tune for $3$ epochs. In the federated setting, training is conducted in a cross-silo setup with $3$ clients, where each client performs $6$ local epochs before a single aggregation round.
We conduct experiments across a range of privacy budgets, varying $\epsilon$ from $1$ to $10$. 
\\

\textbf{Results} (Tables \ref{tab:dp-central}, \ref{tab:fed-central}).
%Results for centralized private and federated private FT are presented in Tables \ref{tab:dp-central} and \ref{tab:fed-central}, respectively. 
Fed-SB consistently outperforms all prior baselines in both accuracy and communication/parameter efficiency across \textbf{all privacy budgets} in both settings. 
Figures \ref{fig:plots-dp-vary}, \ref{fig:plots-dp-central-eps}, and \ref{fig:plots-dp-fed-eps} further illustrate this significant improvement.
\\

\textbf{Centralized Private} (Table \ref{tab:dp-central}).
We first evaluate our method in the centralized setting against existing approaches. 
Fed-SB consistently outperforms previous methods while utilizing only a fraction of the parameters across all $\epsilon$ values. 
For instance, at $\epsilon=3$, Fed-SB ($r=64$) surpasses centralized LoRA and centralized FFA-LoRA by 9.28\% and 2.05\%, respectively, while using approximately \textbf{12x} and \textbf{6x} fewer parameters.
\\

\textbf{Federated Private} (Table \ref{tab:fed-central}).
Fed-SB consistently outperforms all previous methods across all values of $\epsilon$, while significantly reducing communication costs. 
For instance, at $\epsilon=1$, Fed-SB ($r=64$) outperforms FedIT, FedEx-LoRA/FLoRA, and FFA-LoRA by 24.26\%, 6.48\%, and 2.72\%, respectively, while reducing communication cost by approximately \textbf{12x}, \textbf{35x}, and \textbf{6x}.
FedIT performs significantly worse in the federated private setting compared to the federated non-private setting. 
We hypothesize that this is due to increased deviation in updates under DP constraints and added noise, leading to greater divergence from the ideal.



