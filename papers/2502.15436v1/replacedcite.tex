\section{Related Work}
\textbf{Parameter-Efficient Fine-Tuning (PEFT).}  
LoRA ____ has become ubiquitous for fine-tuning LLMs ____ by modeling weight updates as product of low-rank matrices. 
Several variants have been proposed to improve efficiency, stability, and adaptability.  
QLoRA ____ enables efficient fine-tuning through quantization strategies, reducing memory usage while maintaining performance.  
AdaLoRA ____ dynamically allocates a layer-specific rank budget by assigning importance scores to individual weight matrices.  
LoRA-XS ____ further reduces trainable parameters by inserting a trainable matrix between frozen LoRA matrices.  
VeRA ____ enhances parameter efficiency by learning shared adapters across layers.  
DoRA ____ decomposes the pre-trained matrix into two parts—\textit{magnitude} and \textit{direction}—and applies LoRA modules only to the \textit{direction} component.  
PiSSA ____ improves adaptation by initializing adapters using the singular value decomposition (SVD) of pre-trained weights.  
rsLoRA ____ introduces a rank-scaling factor to stabilize learning.  
LoRA-SB ____ provably approximates gradients optimally in low-rank spaces, achieving superior performance with significantly higher parameter efficiency.
\\

\textbf{Federated Fine-Tuning.}
Federated Learning (FL) consists of a centralized global model and multiple clients, each with its own local dataset and computational capacity. 
The global model is updated by aggregating client updates ____.  
FedBERT ____ focuses on federated pre-training, while other methods work on federated fine-tuning ____.  
Fed-IT ____ aggregates low-rank adapters across clients using standard federated averaging ____ before updating the global model.  
To address inexact aggregation, FedEx-LoRA ____ introduces an error matrix to correct residual errors, ensuring more precise updates.  
FLoRA ____ follows the same exact aggregation principle by stacking matrices and extends this approach to heterogeneous rank settings.  
FFA-LoRA ____ mitigates aggregation inexactness by freezing \( \mathbf{A} \) and updating only the trainable low-rank adapter, averaging the latter to compute the global update.  
In some scenarios, clients require heterogeneous LoRA ranks due to varying computational budgets ____.  
Methods like HetLoRA ____ enable rank heterogeneity through self-pruning and sparsity-aware aggregation strategies, but incur significant overhead.
\\

\textbf{Differential Privacy (DP) and FL.}
A common limitation of standard FL frameworks is their susceptibility to privacy attacks, as clients publicly share model updates with a central server. 
To address this issue, DP is incorporated into FL methods to ensure the privacy of client updates. 
This work follows the approximate DP framework ____, which provides formal privacy guarantees for model updates.
Privacy is enforced during training using the DP-SGD optimizer ____, which applies gradient clipping and noise injection to protect individual contributions. 
Since DP is preserved under composition and post-processing ____, the final global model update also retains DP guarantees.
Prior methods, such as Fed-IT and FedEx-LoRA, did not explicitly incorporate DP. 
This study extends these approaches to DP settings and benchmarks them alongside FFA-LoRA and the proposed method.