
\section{Method}\label{sec:method}

\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth]{latex/figures/method_updated.drawio.png}
    \caption{\textbf{Fed-SB}: Our method achieves optimal exact aggregation by averaging only the \( r\times r\) matrices \( \mathbf{R}_i \), significantly reducing communication costs.}
    \label{fig:method}
\end{figure}

\quad \textbf{LoRA-SB for Fine-Tuning.}
LoRA-SB \citep{ponkshe2024initialization} optimally approximates full FT gradients in low-rank spaces and demonstrates that its entire optimization trajectory aligns with the ideal low-rank projection of the full FT path. 
To achieve this, LoRA-SB fixes \( \mathbf{A} \) and \( \mathbf{B} \) while introducing a new trainable adapter \( \mathbf{R} \) of size \( r \times r \).  
Since \( \mathbf{R} \) has rank \( r \), it updates the pre-trained weight while maintaining rank \( r \), making it highly parameter efficient.  
As a result, LoRA-SB consistently outperforms LoRA (and its variants) across various benchmarks while using 45–90x fewer trainable parameters.
\\

\textbf{Fed-SB: A Silver bullet for (Private) Federated Fine-Tuning.} 
We propose \textbf{Fed-SB}, an extremely communication-efficient and high-performing federated adaptation of LoRA-SB. 
Instead of reparameterizing updates as a low-rank decomposition with learnable adapters, the server distributes frozen adapters \( \mathbf{B} \) and \( \mathbf{A} \), while clients train only a small matrix \( \mathbf{R} \) (Figure \ref{fig:method}). 
This enables exact aggregation, as the global update is simply the average of \( \mathbf{R} \) across clients.  
Formally, given a pre-trained weight \( \mathbf{W}_0 \) and data distributed across \( c \) clients, each client learns updates of the form:  
\begin{align}
    \Delta \mathbf{W}_i = \mathbf{B} \mathbf{R}_i \mathbf{A}.
\end{align}
The server then aggregates the updates by computing the global \( \mathbf{R} \) matrix:  
\begin{align}
    \mathbf{R}^{\text{agg}} = \frac{1}{c} \sum_{i=1}^{c} \mathbf{R}_i,
    \Delta \mathbf{W}^{\text{agg}} = \mathbf{B} \left(\frac{1}{c} \sum_{i=1}^{c} \mathbf{R}_i \right) \mathbf{A}.
\end{align}
We show that \textbf{Fed-SB} effectively resolves all challenges in (private) federated FT while achieving state-of-the-art communication efficiency and performance. 
Table \ref{tab:methods-comparison} highlights the advantages of Fed-SB over other methods.
\\

\textbf{Fed-SB: Exact Aggregation.}
Since only \( \mathbf{R} \) is trainable, simple averaging of \( \mathbf{R} \) across clients ensures exact aggregation without requiring updates to any other matrix. 
Further, the linearity of the global update with respect to the client-specific matrices \( \mathbf{R}_i \) guarantees that exact aggregation occurs within rank \( r \), preventing communication costs from scaling with the number of clients. 
This is because the server only needs to aggregate and transmit the \( \mathbf{R} \) matrix.  
This can be proven directly by computing the global update \( \Delta \mathbf{W}^{\text{agg}} \):  
\begin{gather}
     \Delta \mathbf{W}^{\text{agg}} = \mathbf{B} \left( \frac{1}{c} \sum_{i=1}^{c} \mathbf{R}_i \right) \mathbf{A},\\
    \quad \Delta \mathbf{W}^{\text{agg}} = \frac{1}{c} \sum_{i=1}^{c} \mathbf{B} \mathbf{R}_i \mathbf{A} =  \frac{1}{c} \sum_{i=1}^{c} \Delta \mathbf{W}_i  . 
\end{gather}
Since the global update is simply the average of the individual updates, the aggregation is exact. 
The key advantage here is that this exact aggregation does not incur additional communication overhead like FedEx-LoRA, nor does it compromise individual update quality like FFA-LoRA.
\\

\textbf{Fed-SB: Privacy.}  
Privacy-preserving FT with Fed-SB has two key advantages:  
1) Fed-SB avoids noise amplification, which is a common issue in LoRA-based methods.  
2) Since Fed-SB inherently requires fewer learnable parameters, the amount of noise added to enforce DP guarantees is significantly lower.
\\

\textbf{Avoids Noise Amplification.}
DP-SGD training in Fed-SB avoids second-order noise terms, as only \( \mathbf{R} \) is trainable. This prevents the introduction of cross terms, thereby eliminating noise amplification. The difference between the updates with and without private training is given by:  
\begin{multline}
     \Delta \mathbf{W}_{DP} - \Delta \mathbf{W} = \mathbf{B}\left(\mathbf{R} + \boldsymbol{\xi}_B\right)\mathbf{A} - \mathbf{B} \mathbf{R} \mathbf{A}  \\
     \implies \Delta \mathbf{W}_{DP} - \Delta \mathbf{W} =   \mathbf{B} \boldsymbol{\xi}_B \mathbf{A} .
\end{multline}
Since the private update remains linear in \( \mathbf{R} \), Fed-SB achieves the same benefits in private settings as FFA-LoRA, while avoiding its limitations.  
\\

\textbf{Fewer Learnable Parameters.}
The noise added to gradients for DP enforcement increases with the number of trainable parameters \citep{bassily2014private, dgsgd, bun2014fingerprinting}, potentially distorting learning and degrading performance.  
Reducing trainable parameters improves DP performance, provided the model retains sufficient task-specific expressivity.  
%LoRA-SB achieves competitive or superior performance to other low-rank adaptation methods while training only an $r \times r$ matrix, making it well-suited for private FT.

\begin{tcolorbox}[colback=cyan!10,colframe=black]
\begin{lemma} \label{lemma:privacy}
    Consider a model with \( d \) learnable parameters trained using DP-SGD. The privacy parameter \( \epsilon \) for \( \delta \)-approximate differential privacy, given \( T \) training steps and a batch size of \( q \), is expressed as:
\begin{align}
    \epsilon = O(q \sqrt{T d \log (1 / \delta)}) = O(\sqrt{d}).
\end{align}
\end{lemma}
\begin{proof}
    See Appendix \ref{app:proof_priv}.
\end{proof}
\end{tcolorbox}

Lemma \ref{lemma:privacy} establishes that reducing the number of learnable parameters enhances privacy guarantees under the same training setup. Specifically, achieving an equivalent level of privacy requires injecting less noise per parameter when fewer parameters are trained.  
Since LoRA-SB optimally approximates full fine-tuning gradients, its updates remain as effective as those in LoRA while benefiting from lower noise per update, resulting in a superior privacy-utility tradeoff.  
More generally, any reparameterization that reduces trainable parameters leads to a smaller accumulated privacy parameter \( \epsilon \), thereby improving privacy performance—provided the reduction does not compromise learning.
\\
%advanced composition case
%$\sigma \geq c_2 \frac{q \sqrt{T \log (1 / \delta)}}{\epsilon}$


%Renyi-DP equivalent (note eps,del to renyo dp is lossy, but with nicer composition)
%$\alpha(\lambda) \leq q^2 \lambda(\lambda+1) /(1-q) \sigma^2+O\left(q^3 / \sigma^3\right)$
%\\

\textbf{Fed-SB: Pushing the Pareto Frontier.}
Fed-SB has significantly less communication costs than other federated FT methods. This is due to two key reasons:  
1) LoRA-SB achieves performance comparable to or better than LoRA while requiring 45-90x fewer trainable parameters.  
2) Fed-SB aggregates only the \( r \times r \) trainable matrix \( \mathbf{R} \), ensuring exact aggregation without additional communication overhead.  
This efficiency allows Fed-SB to leverage higher-rank updates without increasing communication costs. 
LoRA-SB typically operates at ranks 2–4x higher than LoRA, enabling Fed-SB to capture richer model updates. 
Retaining high-rank information is crucial in FL \citep{mahla2025exploringgradientsubspacesaddressing} and is a key factor in the superior performance of FedEx-LoRA/FLoRA over FFA-LoRA and Fed-IT beyond just aggregation exactness.   
We empirically validate Fed-SB's advantages through extensive experiments.






% To solve this issue of noisy inexact aggregation caused by averaging A and B matrices separately, we propose a method.
% We then average product of BA
% however, as mentioned we cannot keep this entire matrix (or decomposition) trainable
% thus, we append the error between product of average of A and B versus average of product of A and B. this is a high rank matrix  
% Our solution includes appending this high-rank update matrix in the global frozen weight matrix. this high rank matrix is not trainable, we simply add the error residual term in each aggregation
% To tackle the problem of inexact aggregation arising from the independent averaging of the $\mathbf{A}$ and $\mathbf{B}$ matrices across clients, we introduce a novel method called FedEx-LoRA. Instead of separately averaging the low-rank adapter matrices $\mathbf{A}$ and $\mathbf{B}$, we compute the average of their product $\mathbf{BA}$ across all clients. However, as previously noted in Section \ref{sec:motivation}, we cannot keep this high-rank matrix or its lower-rank decomposition (with rank $(k\cdot r)$) trainable. Consequently, we append a high-rank error term that captures the discrepancy between the average of the products and the product of the averages. This error residual is incorporated into the global frozen weight matrix, ensuring its non-trainability. The update at the $j^{th}$ aggregation round can be expressed as follows:
% \begin{gather}
%     \mathbf{B}^{j+1}_i \leftarrow \frac{1}{k} \sum_{i=1}^k \mathbf{B}^{j}_i, \quad \mathbf{A}^{j+1}_i \leftarrow \frac{1}{k} \sum_{i=1}^k \mathbf{A}^{j}_i \label{eq:ass-ab} \\
%     \begin{gathered}
%     \mathbf{W_0}^{j+1} \leftarrow \mathbf{W_0}^{j} \\ 
%     + \underbrace{\frac{1}{k} \sum_{i=1}^k (\mathbf{B}^{j}_i \mathbf{A}^{j}_i) - \frac{1}{k} \sum_{i=1}^k \mathbf{B}^{j}_i \times \frac{1}{k} \sum_{i=1}^k \mathbf{A}^{j}_i}_{\text{Residual}} 
%     \end{gathered}
% \end{gather}
% We now demonstrate that our formulation results in exact aggregation for every client:
% \begin{gather}
%      \mathbf{W}_{global}^{j+1} = \mathbf{W_0}^{j} + \mathbf{B}_i^{j} \mathbf{A}_i^{j} \label{eq:prove-1}
% \end{gather}
% \begin{gather}
%      \begin{split}
%      \mathbf{W}_{global}^{j+1} = \mathbf{W_0}^{j} + \frac{1}{k} \sum_{i=1}^k (\mathbf{B}_i^{j} \mathbf{A}_i^{j})  \\ -\frac{1}{k} \sum_{i=1}^k \mathbf{B}_i^{j} \times \frac{1}{k} \sum_{i=1}^k \mathbf{A}_i^{j}
%      + \frac{1}{k} \sum_{i=1}^k \mathbf{B}_i^{j} \times \frac{1}{k} \sum_{i=1}^k \mathbf{A}_i^{j} \label{eq:prove-2}
%      \end{split}
%      \\
%      \mathbf{W}_{global}^{j+1} = \underbrace{\mathbf{W_0}^{j} + \frac{1}{k} \sum_{i=1}^k (\mathbf{B}_i^{j} \mathbf{A}_i^{j})}_{\text{Ideal aggregation}} \label{eq:prove-2}
% \end{gather}

% \subsection{FedEx-LoRA: Overall Pipeline}
% Initially, the server distributes the global pretrained model to all \( k \) clients and initializes the low-rank adapters \( \mathbf{A} \) and \( \mathbf{B} \) according to standard LoRA settings: \( \mathbf{B} \) is initialized to zero, while \( \mathbf{A} \) is initialized using a random Gaussian distribution.
% \begin{gather}
%         \mathbf{B}_i^{0} \leftarrow \mathbf{B}_{init}, \quad \mathbf{A}_i^{0} \leftarrow \mathbf{A}_{init} \quad \\
%         \mathbf{W}_0^{0} \leftarrow \mathbf{W}_{pretrained}
% \end{gather}
% Each client then independently trains their low-rank adapters \( \mathbf{A} \) and \( \mathbf{B} \) using their local data for a specified number of epochs (referred to as ``local epochs''). Upon completion of training, the clients send their updated low-rank adapters back to the server for aggregation. The server aggregates these low-rank adapters and incorporates the residual term into the global model:
% \begin{gather}
%  \mathbf{B}_{global}^{j} = \frac{1}{k} \sum_{i=1}^k \mathbf{B}_i^{j} \\ \quad\mathbf{A}_{global}^{j}= \frac{1}{k} \sum_{i=1}^k \mathbf{A}_i^{j} \\
%  \begin{split}
%     \mathbf{ \Delta W}_{res}^{j} = \frac{1}{k} \sum_{i=1}^k (\mathbf{B}_i^{j} \mathbf{A}_i^{j})\\
%     - \frac{1}{k} \sum_{i=1}^k \mathbf{B}_i^{j} \times \frac{1}{k} \sum_{i=1}^k \mathbf{A}_i^{j} 
%  \end{split}
% \end{gather}

% % \begin{gather}

% % \end{gather}
% The server then sends the aggregated matrices back to each client. After receiving these updates, the clients proceed to update their low-rank adapters \( \mathbf{A} \) and \( \mathbf{B} \), as well as the weight matrix:
% \begin{gather}
%         \mathbf{B}_i^{j+1} \leftarrow \mathbf{B}^{j}_{global}, \quad \mathbf{A}_i^{j+1} \leftarrow \mathbf{A}^{j}_{global} \\
%         \mathbf{W}_0^{j+1} \leftarrow \mathbf{W}_0^{j} + \mathbf{\Delta W}_{res}^{j}
% \end{gather}
% Following this, clients independently resume fine-tuning for a set number of local epochs. This process repeats across multiple aggregation rounds (also referred to as communication rounds).





