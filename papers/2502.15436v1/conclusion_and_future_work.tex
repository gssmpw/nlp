\section{Conclusion} \label{conclusion}
Existing LoRA-based federated FT methods either suffer from suboptimal updates or incur prohibitively high communication costs.  
We introduce Fed-SB, a federated adaptation of LoRA-SB that ensures exact aggregation while maintaining high communication efficiency.  
By training only a small \( r \times r \) matrix and leveraging direct averaging, Fed-SB eliminates high-rank update costs and achieves communication efficiency independent of the number of clients.  
Fed-SB is particularly well-suited for private FT, as its linearity prevents noise amplification, and its reduced parameter count minimizes noise required for enforcing DP guarantees. 
It consistently outperforms existing methods across all models and tasks while reducing communication costs by up to \textbf{230x}.  
These advantages establish Fed-SB as an efficient and scalable solution, setting a new Pareto frontier in (private) federated FT.

For future work, we plan to analyze the geometry of the \( R \) matrices, as it is important for interpretability and may provide insights into the learning process.  
We also aim to study how the \( R \) matrices change during training, which is key to understanding the fine-tuning process and characterizing its properties.


\section{Limitations}\label{limitations}

While our approach is easily adaptable to other architectures, such as vision language models (VLMs) and vision transformers (VITs), we have not evaluated on such architectures.  
We have not yet extended our work to rank-heterogeneous settings, where clients operate with different ranks and computational budgets.  
Furthermore, we do not evaluate Fed-SB in scenarios with extreme data heterogeneity or significant class imbalances.

