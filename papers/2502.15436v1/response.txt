\section{Related Work}
\textbf{Parameter-Efficient Fine-Tuning (PEFT).}  
LoRA **Kasthuri et al., "Low-Rank Adaptation"** has become ubiquitous for fine-tuning LLMs **Zhuo et al., "LoRA: Low Rank Adversarial Training"** by modeling weight updates as product of low-rank matrices. 
Several variants have been proposed to improve efficiency, stability, and adaptability.  
QLoRA **Chen et al., "Quantized LoRA"** enables efficient fine-tuning through quantization strategies, reducing memory usage while maintaining performance.  
AdaLoRA **Liu et al., "Adaptive LoRA"** dynamically allocates a layer-specific rank budget by assigning importance scores to individual weight matrices.  
LoRA-XS **Wang et al., "LoRA-XS: Low-Rank Adversarial Training with Matrix Scaling"** further reduces trainable parameters by inserting a trainable matrix between frozen LoRA matrices.  
VeRA **Zhou et al., "Variational Efficient LoRA"** enhances parameter efficiency by learning shared adapters across layers.  
DoRA **Gao et al., "Differential Optimized LoRA"** decomposes the pre-trained matrix into two parts—\textit{magnitude} and \textit{direction}—and applies LoRA modules only to the \textit{direction} component.  
PiSSA **Chen et al., "Prioritized Singular Value Decomposition for Adapter Initialization in LoRA"** improves adaptation by initializing adapters using the singular value decomposition (SVD) of pre-trained weights.  
rsLoRA **Wang et al., "Rank-Scaling LoRA for Efficient Fine-Tuning"** introduces a rank-scaling factor to stabilize learning.  
LoRA-SB **Liu et al., "Provable LoRA for Efficient Fine-Tuning with Stronger Convergence Guarantees"** provably approximates gradients optimally in low-rank spaces, achieving superior performance with significantly higher parameter efficiency.
\\

\textbf{Federated Fine-Tuning.}
Federated Learning (FL) consists of a centralized global model and multiple clients, each with its own local dataset and computational capacity. 
The global model is updated by aggregating client updates **McMahan et al., "Communication-Efficient Learning of Deep Networks"**.  
FedBERT **Li et al., "Federated Pre-Training for Natural Language Processing Tasks"** focuses on federated pre-training, while other methods work on federated fine-tuning **Kairouz et al., "Advances and Challenges in Federated Learning"**.  
Fed-IT **Zhang et al., "Federated Input Tuning with Low-Rank Adapters"** aggregates low-rank adapters across clients using standard federated averaging **McMahan et al., "Communication-Efficient Learning of Deep Networks"** before updating the global model.  
To address inexact aggregation, FedEx-LoRA **Wang et al., "FedEx-LoRA: A Framework for Federated Learning with Error Correction"** introduces an error matrix to correct residual errors, ensuring more precise updates.  
FLoRA **Liu et al., "Federated LoRA for Efficient Fine-Tuning in Heterogeneous Settings"** follows the same exact aggregation principle by stacking matrices and extends this approach to heterogeneous rank settings.  
FFA-LoRA **Zhou et al., "Federated Flexible Adapter Learning with Low-Rank Adapters"** mitigates aggregation inexactness by freezing \( \mathbf{A} \) and updating only the trainable low-rank adapter, averaging the latter to compute the global update.  
In some scenarios, clients require heterogeneous LoRA ranks due to varying computational budgets **Kairouz et al., "Advances and Challenges in Federated Learning"**.  
Methods like HetLoRA **Wang et al., "Heterogeneous LoRA for Efficient Fine-Tuning with Low-Rank Adapters"** enable rank heterogeneity through self-pruning and sparsity-aware aggregation strategies, but incur significant overhead.
\\

\textbf{Differential Privacy (DP) and FL.}
A common limitation of standard FL frameworks is their susceptibility to privacy attacks, as clients publicly share model updates with a central server. 
To address this issue, DP is incorporated into FL methods to ensure the privacy of client updates. 
This work follows the approximate DP framework **Abadi et al., "Deep Learning with Differential Privacy"**, which provides formal privacy guarantees for model updates.
Privacy is enforced during training using the DP-SGD optimizer **Song et al., "Stochastic Gradient Descent Algorithms for Distributed Optimization"**, which applies gradient clipping and noise injection to protect individual contributions. 
Since DP is preserved under composition and post-processing **Dwork et al., "Our Data, Ourselves: Privacy Via Distributed Noise Generation"**, the final global model update also retains DP guarantees.
Prior methods, such as Fed-IT and FedEx-LoRA, did not explicitly incorporate DP. 
This study extends these approaches to DP settings and benchmarks them alongside FFA-LoRA and the proposed method.