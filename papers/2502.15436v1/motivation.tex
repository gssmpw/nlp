\section{Preliminaries and Motivation} \label{sec:motivation}

\quad \textbf{Federated Fine-Tuning.}  
Given a pretrained weight matrix \( \mathbf{W} \in \mathbb{R}^{m \times n} \), the objective in FT is to learn an update \( \Delta \mathbf{W} \) for a given dataset.  
LoRA \citep{lora} remains the preferred method, where low-rank adapter matrices \( \mathbf{A} \in \mathbb{R}^{r \times n} \) and \( \mathbf{B} \in \mathbb{R}^{m \times r} \) are learned such that \( \Delta \mathbf{W} = \mathbf{B} \mathbf{A} \).  
In federated learning, the dataset is distributed across \( c \) clients, and the goal is to learn \( \Delta \mathbf{W} \) without sharing local data with a central server.  
To achieve this, each client learns its own adapter matrices \( \mathbf{A}_i \) and \( \mathbf{B}_i \).  
The server aggregates these updates to refine \( \mathbf{W} \), along with globally beneficial representations of \( \mathbf{A} \) and \( \mathbf{B} \), ultimately producing a shared aggregate model \( \mathbf{W}^{\text{agg}} \).
Next, each client continues the local FT process, followed by aggregation at the end of each round. This cycle repeats over multiple rounds.
We summarize some of the state-of-the-art federated FT methods below.

\textbf{Fed-IT} \citep{FedIT} updates the adapters \( \mathbf{A} \) and \( \mathbf{B} \) using the standard FedAvg \citep{mcmahan2017communication} algorithm:
\begin{align}
    \mathbf{A}^{\text{agg}} = \frac{1}{c} \sum_{i=1}^{c} \mathbf{A}_i, \quad 
    \mathbf{B}^{\text{agg}} = \frac{1}{c} \sum_{i=1}^{c} \mathbf{B}_i.
    \label{eq:fedit}
\end{align}
\quad \textbf{FedEx-LoRA} \citep{singhal2024exact} follows the same aggregation but introduces an additional error correction matrix \( \mathbf{W}_{\text{err}} \) of rank \( \min(c r, m, n) \):
\begin{align}
    \mathbf{W}_{\text{err}} = (\frac{1}{c}\sum_{i=1}^{c}\mathbf{A}_i \mathbf{B}_i)-(\frac{1}{c}\sum_{i=1}^{c}\mathbf{A}_i )(\frac{1}{c}\sum_{i=1}^{c} \mathbf{B}_i).
\end{align}
\quad \textbf{FLoRA} \citep{wang2024flora} follows the same principle as FedEx-LoRA but achieves it by stacking the adapter matrices, and reinitializes them randomly at the end of each communication round.

\textbf{FFA-LoRA} \citep{sun2024improving} keeps \( \mathbf{A} \) fixed while training (and aggregating) only \( \mathbf{B} \) matrices.
\begin{align}
    \mathbf{B}^{\text{agg}} = \frac{1}{c} \sum_{i=1}^{c} \mathbf{B}_i.
\end{align}
% 
\quad \textbf{(Approximate) Differential Privacy.} 
DP, introduced by \citet{dwork2006differential}, is a widely adopted mathematical framework for privacy preservation. A randomized mechanism \( \mathcal{M}: \mathcal{D} \rightarrow \mathcal{R} \), mapping a domain \( \mathcal{D} \) to a range \( \mathcal{R} \), satisfies \((\epsilon, \delta)\)-differential privacy if, for any two adjacent inputs \( d, d^{\prime} \in \mathcal{D} \) and any subset of outputs \( S \subseteq \mathcal{R} \), the following holds:
\begin{align}
    \operatorname{Pr}[\mathcal{M}(d) \in S] \leq e^{\epsilon} \operatorname{Pr}[\mathcal{M}(d^{\prime}) \in S] + \delta.
\end{align}
% This definition, known as approximate DP, allows for a small probability \( \delta \) of privacy leakage, making it a more flexible generalization of pure \(\epsilon\)-DP.
% DP introduced in \citet{dwork2006differential} is a popular mathematical notion of privacy. A randomized mechanism $\mathcal{M}: \mathcal{D} \rightarrow \mathcal{R}$ with domain $\mathcal{D}$ and range $\mathcal{R}$ satisfies $(\epsilon, \delta)$-differential privacy \cite{dwork2006differential} if for any two adjacent inputs $d, d^{\prime} \in \mathcal{D}$ and for any subset of outputs $S \subseteq \mathcal{R}$ it holds that 
% \begin{align}
%  \operatorname{Pr}[\mathcal{M}(d) \in S] \leq e^{\epsilon} \operatorname{Pr}\left[\mathcal{M}\left(d^{\prime}\right) \in S\right]+\delta   
% \end{align} 
% This form of DP is also referred to as approximate DP. 
%Typically randomization or noising is used in designing differentially private mechanisms to release outputs of queries operating on sensitive data.
\quad \textbf{DP-SGD.} 
DP-SGD \citep{dgsgd} is a privacy-preserving variant of stochastic gradient descent (SGD) designed to ensure DP during model training. 
It enforces privacy by clipping per-sample gradients to a fixed norm \( C \) to limit their sensitivity and then adding isotropic Gaussian noise \( \mathcal{N}\left(0, \sigma^2 C^2 \mathbf{I}\right) \), where \( \sigma \) controls the noise magnitude. 
The cumulative privacy loss over iterations is quantified using the moments accountant \citep{moments} and Rényi DP \citep{mironov2017renyi}, which offer a tight bound on the final privacy parameter \( \epsilon \).
% This framework aligns with Rényi differential privacy \citep{mironov2017renyi}, an alternative formulation that enables more precise privacy accounting.
% DP-SGD \cite{dgsgd} is a privacy preserving mechanism to perform standard stochastic gradient descent (SGD) for training a model. 
% It employs clipping of gradients at every optimization update in order to bound their variation to sensitive data followed by an addition of isotropic Gaussian noise $\mathcal{N}\left(0, \sigma^2 C^2 \mathbf{I}\right)$ with the parameters of $\sigma$ and $C$, calibrated to ensure differential privacy. 
% Repeat accesses to sensitive data (including data-dependent compositions), lead to a degradation in the privacy level. 
% The final privacy level $\epsilon$ obtained after the DP-SGD iterations is in fact calibrated through a budgeting procedure called the moments accountant \cite{moments}, that is motivated by an equivalent form of differential privacy called Renyi DP \cite{mironov2017renyi}. 
% This equivalent form comes along with simpler privacy budgeting for this purpose.
\\

\textbf{Exact Aggregation in Fed. LoRA: Tradeoff b/w Performance and Communication Costs.}  

Standard federated averaging of individual LoRA adapters (FedIT, \citet{FedIT}) introduces \textit{inexactness} in aggregation, as the ideal update should be the average of client updates.
\begin{multline}  \underbrace{\mathbf{W}_0 + \frac{1}{c} \sum_{i=1}^c \mathbf{B}_i \times \frac{1}{c} \sum_{i=1}^c \mathbf{A}_i}_{\text{Vanilla aggregation in LoRA (FedIT)}} \\ \neq \underbrace{\mathbf{W}_0 + \frac{1}{c} \sum_{i=1}^c (\mathbf{B}_i \mathbf{A}_i)}_{\text{Ideal aggregation}}.
    \label{eq:fedavg-inexact}  
\end{multline}
\quad The inexactness arises because the ideal averaged updates, given by \( \sum_{i=1}^{c} \mathbf{B}_i \mathbf{A}_i \), often exceed rank \( r \), violating the low-rank constraint imposed by LoRA.  
To address this, FedEx-LoRA and FLoRA introduce \( \mathbf{W}_{\text{err}} \) as a higher-rank correction term within the pre-trained weight matrix \( \mathbf{W}_0 \), which is inherently high-rank.  
This correction ensures exact aggregation, leading to consistently improved performance over FedIT.

This, however, comes at the cost of increased communication.  
Since the error matrix is high rank, it substantially increases the amount of data transmitted per round.  
The communication cost is determined by the number of parameters sent during aggregation, which, for an \( m \times n \) matrix, is proportional to its rank.  
As a result, in FedEx-LoRA and similar methods that enforce exact aggregation, communication cost scales linearly with the number of clients relative to Fed-IT.  
This becomes particularly concerning when the number of clients grows large, \textbf{potentially requiring the transmission of the entire model’s weights}.

FFA-LoRA addresses inexact aggregation by keeping only \( \mathbf{B} \) trainable while fixing \( \mathbf{A} \) uniformly across clients.  
However, this comes at the cost of reduced expressivity and limits the benefits of jointly optimizing \( \mathbf{A} \) and \( \mathbf{B} \).  
As a result, performance degrades, as demonstrated previously \citep{singhal2024exact}.  
This stems from two factors: suboptimal individual updates and the need for higher-rank adaptations.  
Freezing \( \mathbf{A} \) leads to suboptimal updates, even in centralized training, where FFA-LoRA underperforms compared to LoRA.  
Additionally, \citet{mahla2025exploringgradientsubspacesaddressing} show that models trained using FFA-LoRA progressively deviate from the optimal hypothesis.  
Empirical evidence shows that the advantages of exactness are outweighed by the degradation caused by these factors.
\\

\textbf{Private Fine-Tuning.} 
Pre-training on public data followed by FT on user-specific private data\footnote{Although pre-training data may be publicly available, it often contains sensitive or proprietary information, raising privacy concerns. However, any privacy loss from pre-training has already occurred upon the model’s release.} is a common approach for adapting models under privacy constraints \citep{yu2021differentially, tang2024private}.  
This two-stage process enhances performance in private learning while preserving user data privacy.  
FL naturally improves privacy by keeping data decentralized. 
However, even without direct data sharing, client-specific model updates can still leak sensitive information \citep{truong2021privacy}.  
Thus, developing privacy-preserving FT methods for FL is essential to ensure strong privacy guarantees while maintaining performance.

Training a model with DP-SGD introduces noise into the gradient, and consequently, into the model update itself. In the case of LoRA, this deviation from the ideal update is more pronounced than in full FT due to second-order noise terms.  
To illustrate this, let \( \mathbf{A} \) and \( \mathbf{B} \) represent the adapter updates learned without privacy. Under DP-SGD, these updates are perturbed by noise terms \( \boldsymbol{\xi}_A \) and \( \boldsymbol{\xi}_B \), respectively. The difference between the ideal update \( \Delta \mathbf{W} \) and the noisy update \( \Delta \mathbf{W}_{DP} \) is:
\begin{multline}
    \Delta \mathbf{W}_{DP} - \Delta \mathbf{W} 
    = \left(\mathbf{B} + \boldsymbol{\xi}_B\right)\left(\mathbf{A} + \boldsymbol{\xi}_A\right) - \mathbf{B} \mathbf{A}  
    \\
    = \boldsymbol{\xi}_B \mathbf{A} + \mathbf{B} \boldsymbol{\xi}_A + \boldsymbol{\xi}_B \boldsymbol{\xi}_A.
\end{multline}
The first-order noise term, \( \boldsymbol{\xi}_B \mathbf{A} + \mathbf{B} \boldsymbol{\xi}_A \), is expected and occurs even in full FT with DP-SGD. 
However, the second-order noise term, \( \boldsymbol{\xi}_B \boldsymbol{\xi}_A \), causes \textbf{noise amplification}, leading to further performance degradation in LoRA-based methods \citep{sun2024improving}.  
This issue is exacerbated in FL, as individual client updates deviate even further from the ideal global update. FFA-LoRA avoids this problem by freezing \( \mathbf{A} \), preventing the introduction of additional noise terms.
\\

\textbf{A Silver Bullet Indeed.}
The bilinear parameterization in LoRA introduces two key challenges: inexact aggregation and noise amplification. 
FedEx-LoRA/FLoRA addresses the inexactness issue by enabling exact aggregation, but at the cost of a communication overhead that scales prohibitively with the number of clients. 
FFA-LoRA mitigates inexact aggregation and excessive communication but sacrifices performance, as it operates in a low-rank space and has reduced expressivity.
An ideal method would efficiently learn higher-rank updates while inherently enabling exact aggregation without increasing communication costs. However, any LoRA-based formulation that attempts to resolve all these challenges must inevitably trade off expressivity, ultimately compromising performance. 
We prove that LoRA-SB provides an optimal reparameterization of the updates, effectively overcoming all limitations of LoRA in both non-private and privacy-preserving federated settings.
