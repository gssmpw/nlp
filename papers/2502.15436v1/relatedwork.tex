\section{Related Work}
\textbf{Parameter-Efficient Fine-Tuning (PEFT).}  
LoRA \citep{lora} has become ubiquitous for fine-tuning LLMs \citep{scaling_llm} by modeling weight updates as product of low-rank matrices. 
Several variants have been proposed to improve efficiency, stability, and adaptability.  
QLoRA \citep{qlora} enables efficient fine-tuning through quantization strategies, reducing memory usage while maintaining performance.  
AdaLoRA \citep{adalora} dynamically allocates a layer-specific rank budget by assigning importance scores to individual weight matrices.  
LoRA-XS \citep{bałazy2024loraxslowrankadaptationextremely} further reduces trainable parameters by inserting a trainable matrix between frozen LoRA matrices.  
VeRA \citep{kopiczko2024veravectorbasedrandommatrix} enhances parameter efficiency by learning shared adapters across layers.  
DoRA \citep{liu2024doraweightdecomposedlowrankadaptation} decomposes the pre-trained matrix into two parts—\textit{magnitude} and \textit{direction}—and applies LoRA modules only to the \textit{direction} component.  
PiSSA \citep{meng2024pissaprincipalsingularvalues} improves adaptation by initializing adapters using the singular value decomposition (SVD) of pre-trained weights.  
rsLoRA \citep{rslora} introduces a rank-scaling factor to stabilize learning.  
LoRA-SB \citep{ponkshe2024initialization} provably approximates gradients optimally in low-rank spaces, achieving superior performance with significantly higher parameter efficiency.
\\

\textbf{Federated Fine-Tuning.}
Federated Learning (FL) consists of a centralized global model and multiple clients, each with its own local dataset and computational capacity. 
The global model is updated by aggregating client updates \citep{kairouz2021advances}.  
FedBERT \citep{tian2022fedbert} focuses on federated pre-training, while other methods work on federated fine-tuning \citep{zhang2022federated, kuang2024federatedscope, babakniya2023slora}.  
Fed-IT \citep{zhang2024buildingfederatedgptfederated} aggregates low-rank adapters across clients using standard federated averaging \citep{mcmahan2017communication} before updating the global model.  
To address inexact aggregation, FedEx-LoRA \citep{singhal2024exact} introduces an error matrix to correct residual errors, ensuring more precise updates.  
FLoRA \citep{wang2024flora} follows the same exact aggregation principle by stacking matrices and extends this approach to heterogeneous rank settings.  
FFA-LoRA \citep{sun2024improving} mitigates aggregation inexactness by freezing \( \mathbf{A} \) and updating only the trainable low-rank adapter, averaging the latter to compute the global update.  
In some scenarios, clients require heterogeneous LoRA ranks due to varying computational budgets \citep{zhao2018federated, li2019convergence}.  
Methods like HetLoRA \citep{hetero_lora} enable rank heterogeneity through self-pruning and sparsity-aware aggregation strategies, but incur significant overhead.
\\

\textbf{Differential Privacy (DP) and FL.}
A common limitation of standard FL frameworks is their susceptibility to privacy attacks, as clients publicly share model updates with a central server. 
To address this issue, DP is incorporated into FL methods to ensure the privacy of client updates. 
This work follows the approximate DP framework \citep{dwork2006differential, dwork2014algorithmic}, which provides formal privacy guarantees for model updates.
Privacy is enforced during training using the DP-SGD optimizer \citep{dgsgd}, which applies gradient clipping and noise injection to protect individual contributions. 
Since DP is preserved under composition and post-processing \citep{dwork2006differential, 10.14778/3503585.3503598}, the final global model update also retains DP guarantees.
Prior methods, such as Fed-IT and FedEx-LoRA, did not explicitly incorporate DP. 
This study extends these approaches to DP settings and benchmarks them alongside FFA-LoRA and the proposed method.