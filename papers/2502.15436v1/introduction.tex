\section{Introduction}
\label{intro}
\quad Large language models (LLMs) have demonstrated remarkable generalization across a wide range of tasks \citep{achiam2023gpt, touvron2023llama-2, team2023gemini, raffel2020exploring}.
Fine-tuning (FT) remains the most effective approach for aligning LLMs to specific data distributions and reinforcing desired properties. 
However, as model sizes scale, full FT becomes increasingly prohibitive due to its substantial computational cost. 
To address this, parameter-efficient fine-tuning (PEFT) techniques, such as low-rank adaptation (LoRA, \citet{lora}), have emerged as viable alternatives, offering a favorable trade-off between computational efficiency and performance. 
Variants of LoRA, including QLoRA \citep{qlora}, DoRA \citep{liu2024doraweightdecomposedlowrankadaptation}, AdaLoRA \citep{adalora}, and LoRA-SB \citep{ponkshe2024initialization}, further refine this paradigm by optimizing memory efficiency, training dynamics, and generalization.

Federated learning (FL) is a popular method for training models in settings where data is siloed across multiple entities \citep{konečný2017federatedlearningstrategiesimproving, kairouz2021advances, bonawitz2019federatedlearningscaledesign}.
Federated FT extends this paradigm by enabling large models, pre-trained on public data, to be efficiently adapted to private, distributed datasets without requiring clients to share their local data. 
Existing methods predominantly rely on LoRA-based techniques to learn client-specific adaptations \citep{FedIT}. 
However, optimizing federated aggregation often involves tradeoffs between model performance \citep{sun2024improving} and communication efficiency \citep{wang2024flora, singhal2024exact}, necessitating careful design choices to balance these competing objectives.

\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{latex/figures/intro-results-fig-2.drawio.png}
    \caption{Performance vs. communicated parameter cost (log scale) for Fed-SB and other federated fine-tuning methods in both non-private and privacy-preserving federated settings. Fed-SB advances the performance-communication cost Pareto frontier across all models and tasks, achieving state-of-the-art accuracy while significantly reducing communication cost. Communicated parameters are in thousands for BERT and millions for other models.}
    \label{fig:intro-results}
\end{figure*}

LoRA-SB \citep{ponkshe2024initialization}, a state-of-the-art approach, optimally simulates full fine-tuning in low-rank spaces by learning an \( r \times r \) matrix between the low-rank adapters \( \mathbf{A} \) and \( \mathbf{B} \) while keeping other components fixed.  
This design reduces trainable parameters and enables better updates through its initialization strategy.  
Moreover, LoRA-SB demonstrates that this optimal approximation is not achievable with standard LoRA-based methods.
LoRA-SB learns higher-rank updates with 2–4x greater rank than LoRA while requiring \textbf{45-90x} fewer parameters.
We propose \textbf{Fed-SB}, a federated variant of LoRA-SB, providing an ideal framework for (private) federated FT. 
Fed-SB overcomes limitations in LoRA-based federated FT while being significantly more computation- and communication-efficient. 
Notably, it enables exact and optimal aggregation by simply averaging the learnable matrix \(\mathbf{R}\).

Differential privacy (DP) is a well-established framework for ensuring strong privacy guarantees \citep{dwork2006differential, dwork2014algorithmic}, which is particularly crucial in federated settings. 
DP-SGD is a widely used privacy-preserving optimization method \citep{dgsgd}, but its challenges are exacerbated in federated FT, where noise injected for privacy amplifies divergence across client models \citep{sun2024improving}. 
Learning in DP-SGD is more effective when the number of learnable parameters is reduced, as the magnitude of noise added for privacy guarantees scales with the parameter count. 
Fed-SB mitigates this issue to yield improved performance, since it inherently has fewer learnable parameters and thus less noise injection.
Furthermore, we show that Fed-SB avoids noise amplification introduced by other methods, further enhancing privacy-preserving learning.

Fed-SB pushes the performance vs communication cost Pareto frontier, offering an extremely efficient and scalable solution for both private and non-private federated FT, as shown in Figure \ref{fig:intro-results}. 
It consistently has superior performance while substantially reducing communication overhead than other methods. 
Our key contributions are summarized as follows.

\begin{itemize}
    \item We propose \textbf{Fed-SB}, a federated fine-tuning method that achieves exact and optimal aggregation in low-rank adaptation without incurring prohibitive communication costs or performance degradation.
    \item Fed-SB significantly reduces communication cost—by up to \textbf{230x} compared to state-of-the-art methods—by requiring only an \( r \times r \) matrix to be transmitted per aggregation.
    \item We demonstrate that Fed-SB is particularly well-suited for privacy-preserving federated fine-tuning, as it minimizes noise by reducing the number of learnable parameters and leveraging linearity in the aggregate update.
    \item Extensive experiments on four models across three diverse benchmarks show that Fed-SB consistently outperforms existing methods while drastically reducing communication overhead in both private and non-private federated settings, establishing a new Pareto frontier in federated fine-tuning.
\end{itemize}


\begin{table*}[ht]
\centering
\setlength{\tabcolsep}{2.58pt}
\small
\begin{tabular}{lccccc}
\toprule
 & \bf FedIT & \bf FLoRA & \bf FedEx-LoRA & \bf FFA-LoRA & \bf Fed-SB \\
\midrule
Exact aggregation &
  \cellcolor{pastelred}\xmark & 
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelgreen}\cmark \\
Learnable params. / client &
  \cellcolor{pastelyellow}$\mathcal{O}((m+n)r)$ &
  \cellcolor{pastelyellow}$\mathcal{O}((m+n)r)$ &
  \cellcolor{pastelyellow}$\mathcal{O}((m+n)r)$ &
  \cellcolor{pastelyellow}$\mathcal{O}(mr)$ &
  \cellcolor{pastelgreen}$\mathcal{O}(r^2)$ \\
Communication cost  &
  \cellcolor{pastelyellow}$\mathcal{O}((m+n)r)$ &
  \cellcolor{pastelred}$\mathcal{O}(min(c(m+n)r,mn))$ &
  \cellcolor{pastelred}$\mathcal{O}(min(c(m+n)r,mn))$ &
  \cellcolor{pastelyellow}$\mathcal{O}(mr)$ &
  \cellcolor{pastelgreen}$\mathcal{O}(r^2)$ \\
No noise amplification &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelgreen}\cmark \\
Privacy benefit (less params.) &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelgreen}\cmark \\
Optimal expressivity &
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelgreen}\cmark &
  \cellcolor{pastelred}\xmark &
  \cellcolor{pastelgreen}\cmark \\
\bottomrule
\end{tabular}
\caption{Advantages of Fed-SB over various state-of-the-art federated fine-tuning methods ($c$ clients).}
\label{tab:methods-comparison}
\end{table*}

% {\color{red}
% \begin{itemize}
%     \item General LLM - llm good, FT needed to get user-specific benefits, full fine FT good but v expensive, thats why use PEFT (LoRA). other methods that push lora boundary (dora, lora-sb, lora-ga, lora-pro)
%     \item Why FL - distributed client data, preserves privayc, use distrbuted client data to train global model without direct access
%     \item All papers focused on LoRA fed. but this has problems - to achieve exactness, they either prohibitbe communication cost or reduced performance, Can nwe use other lora-base methods which are a better fit for fed FT
%     \item LoRA-SB is a perfect fit. We get exact updates while maintianin constant communication cost. We get state-of-the-art performnce whilen having considerbaly less communication cost.
%     \item Private fine-tuniung is also important for maintainnig user data privacy. This is particaulry important in federated setting, where we dont want to share data information, which motivates use of DP fine-tuniung in fed,
%     \item We show that having lesser leranable params is direclty beneficial for DP fine-tuniung, and our method doesnt lead to additive noise as in other methods. we thus ew Pareto frontier in perfomance vs. communication cost, offering anm efficient and scalable solution for both private and non-private federated fine-tuning
% \end{itemize}

% }

% Large Language Models perform exceptionally well at  large variety of tasks, and are able to learn a new setting with just a few examples reasonably well. However, fine-tuning still remains as the best method for adapting these models to new data distributions, and tasks, or for reinforcing certain properties. Model size scaling continues to be the go-to method to keep improving the performance of these models. As the model size grows, the task of fine-tuning becomes increasingly computationally expensive. Fine-tuning all the weights of the models is unreasonable from a user perspective, due to the requirement of data and compute. Parameter-efficient-fine-tuning (or PEFT) techniques ensure optimal tradeoff between compute and performance. Decomposition based  methods, spearheaded by Low-rank Adapters (LoRA) reparameterize the updates efficiently, and adapt well, since this can be applied to any method with a high rank matrix. Recently however, variations of LoRA like DoRA, AdaLoRA, LoRA-SB, etc have garnered a lot of attention. These methods try to improve LoRA on one of two fronts: performance and efficiency.  

% The introduction of large language models (LLMs) has revolutionized natural language processing, enabling unprecedented performance across a wide range of tasks \citep{achiam2023gpt, touvron2023llama-2, team2023gemini, chang2024survey, raffel2020exploring, zeng2022glm}. While these models excel at transfer learning, their true potential is often unlocked through fine-tuning — a critical process that aligns these general-purpose models with specific tasks or domains. Moreover, the sheer size of these models presents significant challenges for fine-tuning and deployment, particularly in resource-constrained or distributed environments. To address these challenges, parameter-efficient fine-tuning (PEFT) methods have gained prominence, with Low-Rank Adaptation (LoRA) emerging as a particularly effective approach \citep{lora}. LoRA's success lies in its ability to adapt LLMs to new tasks by training only a small number of parameters, while freezing rest of the parameters. This significantly reduces computational and memory requirements without compromising performance. Although good progress in training of LLMs has been realized by entities equipped with massive computational resources, there is hoards of unreachable data in verticals such as healthcare, finance, law firms, social-media and logistics. Federated learning (FL) is a popular paradigm to learn a machine learning model in this setting with multiple distributed entities \citep{konečný2017federatedlearningstrategiesimproving, kairouz2021advances,bonawitz2019federatedlearningscaledesign} holding siloed data. 

% Federated Fine-Tuning (FFT) for foundation models addresses the challenge of leveraging distributed datasets while preserving data privacy. %Early approaches adapted Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) to federated settings. 
%  The current state-of-the-art, Federated Instruction Tuning (FedIT, \citet{FedIT}), uses conventional federated aggregation to average the low-rank matrices $\mathbf{A}$ and $\mathbf{B}$ individually. The resulting update matrix which is formed post aggregation is thus the product of the averaged matrices $\mathbf{A}$  and $\mathbf{B}$. However, the ideal update should be the average of the products of the low-rank adapters $\mathbf{A}$ and $\mathbf{B}$. The discrepancy results from the fact that \textit{"the average of the products is not equal to the product of the averages"}.
% A naive adhoc intervention of modifying the aggregation to directly average the client updates is not a viable solution, since the subsequently obtained weight matrix loses its low-rank structure. The low-rank structure provides the efficiency benefits of LoRA in the first place, making this approach computationally intractable.