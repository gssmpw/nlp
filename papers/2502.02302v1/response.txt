Traditional node representation learning methods mainly focus on mapping the original node representation vector (e.g., the adjacency vector) into the continuous low-dimension embedding space ____. 
Those works usually extend the classical machine learning models for graph-structured data. 
The early works like Locally Linear Embedding (LLE) ____ , Isomap ____ and Laplacian Eigenmaps ____ try to adopt the eigen-based methods, so as to keep the spectral properties of node when mapping into the lower space, which is time consuming and uneasy to be parallelized. 
Recently, inspired by word2vec ____, DeepWalk ____ adopts random walk to generate node sequences to imitate sentences in corpus, and then feeds them into the Skip-gram framework [26] to train node representations, which can to some extent learn both local and global structure information.
Node2vec ____ improves the DeepWalk model by introducing a bias mechanism into the random walk, which can get more efficient node sequences to enhance the model performance. 
Besides, Modularized Nonnegative Matrix Factorization (M-NMF) ____ focuses on extracting the mesoscopic community structure among nodes. 
Furthermore, Attributed Network Embedding with Micromeso structure (ANEM) ____ considers three kinds of information in node representation learning: attribute information, the local proximity structure, and mesoscopic community structure. 
As we can see, the above methods usually pay major attention to manually integrating special rules with shallow models.

Due to the great success of deep learning models in training representation, many attempts have been made to integrate existing deep learning models with graph-structured data. For example, Structural Deep Network Embedding (SDNE) [9] adopts deep auto-encoder to extract the lower dimensional node representation, and adds a pairwise loss to restrict the representation of the connected nodes to be close. Deep Neural networks for Graph Representations (DNGR) [10] further investigate the structure information by calculating the Positive Point Mutual Information (PPMI) matrix [28] for graph, and then feeding the matrix into the stacked denoising autoencoder model [29] to extract the node representation. ContextAware Network Embedding (CANE) [30] incorporates the mutual attention mechanism [31] with CNN [32] to learn the context-aware node representation. Self-Translation Network Embedding (STNE) [11] feeds node sequences obtained by a random walk into the bi-LSTM [33] model to extract the node representation, so as to keep the node order information. Deep Attributed Network Embedding (DANE) [12] assumes that node representations learned from the structure and feature information should be similar. Self-Paced Network Embedding (SPNE) [13] considers the process of the node receiving information from 1-hop neighbors to high-hop neighbors as learning knowledge from easy to hard, and thus adopts the self-paced learning mechanism to learn the node representation. Deep Network Embedding with Structural Balance Preservation (DNE-SBP) [34] adopts stacked auto-encoder and preserves the structural balance [35] for the signed networks. The Nodepair Information Preserving Network Embedding (NINE) [36] aims to preserve the information between a pair of nodes locally through adversarial networks. However, the traditional works mainly apply the existing models to train node representation, which are not targeted to the graph-structured data.

However, those models treat the representation learning of edge as the representation learning of node, without considering node interaction, and thus fail to learn the relation between the connected nodes. In this case, these models cannot properly integrate the edge representation learning into the message-passing framework.

This work relates to online learning and copula modeling, so we review prior studies in each category and discuss the differences and relations.
We note that, in concept-drift ____ or non-stationary online learning ____, the changeables are the statistical properties of variables or the underlying decision function, respectively, as data streaming in, but the number of features carried by each input is fixed in a priori.
Additionally, in the settings of streaming feature mining____, although the new features constantly emerge, the numbers of instances are known beforehand.
The dynamics of both the instance and feature spaces differentiate our learning problem from theirs.

Requiring an input sequence over wide time spans to be described by a fixed set of features is in general impractical.
In response, the pioneer work ____ considered a doubly-streaming setting where new inputs carry consistently more features.
Later studies extend this setting by allowing the pre-existing features to be missed out afterwards,
either by following a batch-by-batch regularity ____ or at purely random ____ .
This line of research shares the main idea of exploiting feature-wise relationship 
to anchor stationary information in a varied feature space.
Unfortunately, existing methods all consider a continuous  domain 
and thus do not scale up to the mixed data setting.
Our algorithm ____, filled the gap with Gaussian copula that 
maps and correlates arbitrary marginals across mixed variables
and hence is more general. 

To explore a continuous latent space, most of methods assume that the data are of the same digital type and make a prior guess of the underlying distribution, as directly modeling the multivariate joint distribution for mixed marginals is difficult. 
In this regard, copulas lend us a tool due to its modeling power. 
Prior studies ____ proposed Gaussian copula by combining the cumulative distribution function (CDF) of each feature, and separating marginal distribution from multivariate distribution, 
which is suitable for modeling with different marginal distributions, 
harmonizing continuousness and discreteness.
%cite copula'
Subsequent works include Bayesian copula with factorized models____,
impute missing entries in a mixed data matrix by extending Rank-PC____,
and solving the time-varying complexities and high-dimensionality  in estimating the 
covariance matrices ____ .
However, most studies focus on modeling offline mixed data,
with very few attempted online settings ____ .
None prior work has considered leveraging Gaussian copula
to deal with an arbitrarily varying feature space so as to learn effective online learners;
Our algorithm ___, strived to fill the gap.

Our research is also similar to missing feature imputation of mixed data.
For MNAR data, we can estimate by modeling missing data distribution, most of method estimating parameters in generalized linear models and maximum likelihood estimation ____ . However, considering the missing completely at random, make previous assumption for distribution or parameter is not reasonable.
Other common methods such as low rank matrix completion ____ , widely used in missing value filling and recommendation systems. But these method is base on the assumption that the data can be well approximated by low rank matrices. Fan, Jicong and Zhang et al ____ propose a high-rank matrix generation model that maps low-dimensional latent variables to high-dimensional environment space, and recover missing items by solving a polynomial matrix completion (PMC) problem. However these method has strictly assumption about dimensions of the matrix, and it is difficult to find a approximate matrix for mixed data.
Some of methods propose use graphs to model missing data. paper ____use Directed Acyclic Graph to model missing causal mechanisms, and demonstrated the feasibility of using the graph model to restore missing. paper ____propose, the missing data with observable data can be a Random Intersection Graphs, and can reconstruct the feature by the correlation of graphs. By contrast, refers to Zhao, Yuxuan et al ____ , Gaussian copula for missing feature reconstructing is more flexible and it can handle Sparse Data Stream in online learning.
These methods is mainly restore the original data, but we focus on reconstruct the correlation of feature through latent vector, so as to obtain a stable feature space.