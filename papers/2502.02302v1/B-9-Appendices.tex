\appendices
\section{}
% \subsection{Notations} \label{app:notation}
% Key notations used in the paper and their definitions are summarized in Table 4.

% \begin{table}[!h]
% 	\centering
% 	\small
% 	\caption{\alg~main notations and definitions.}
%         % \scalebox{.98}{
%     	\begin{tabular}{c|c}
%     		\toprule
%     		\midrule
%     		Notation    & Deiinitions  \\ %  & \# Avg deg.
%     		\midrule
%     		$\mathcal{G}$    & The undirected attributed network   \\
%                 $\mathcal{V}$    & The node set consisting of $n$ nodes  \\ 
%                 $\mathcal{E}$    & The edge set consisting of $m$ edges\\
%                 $\mathbf{F}$     & The node attribute matrix \\
%                 $\mathbf{f}_{i}$ & The attribute vector of node $v_{i}$ \\
%                 $\mathcal{N}_{i}$& The index set of neighbor nodes of $v_{i}$ \\
%                 $\mathbf{H}^{l}$ & The node representation matrix\\ % in the $l$-th layer
%                 $\mathbf{h}_{i}^{l}$  & The node representation vector of node $v_{i}$\\ % in the $l$-th layer 
%                 $\mathbf{Y}$ & The label indicator matrix\\
%                 % $y_{ij}$ & The label indicator denoting whether the $i$-th node has the $k$-th label\\
                
%     		\bottomrule
%     	\end{tabular}
%         % }
% 	\label{tab:dataset}
% \end{table}

% \subsection{Detailed Dataset Description}  \label{app:data}
% {\bf DBLP} dataset contains four types of nodes, i.e., author, paper, term and venue. We use the authors’ research field as a label for classification.

% {\bf ACM} dataset contains four types of nodes, i.e., paper, author, subject and reference relation. The domain of papers is considered as the class label.

% {\bf IMDB} dataset contains three types of nodes, i.e., movie, actor and director, and labels are genres of movies. Node features are given as bag-of-words representations of plots. 

% {\bf FreeBase} dataset contains eight types of nodes, i.e., book, film, location, music, person, sport, organization and business. 
% 
% {\bf LastFM} dataset contains three types of nodes. Each node in the LastFM dataset represents either a user, a song, or an artist. The edges in the dataset represent the listening history between users and songs, as well as the artist preferences of users. The node attributes are constructed using the tags associated with songs and artists, with each tag describing specific features or styles. Additionally, the class information assigned to each node indicates the music preferences or interests of the users.
% %
% {\bf Pubmed} dataset contains three types of nodes. Each node on the Pubmed dataset represents a scientific publication with each edge denoting the corresponding citation relation. The node attribute is constructed by the bag-of-word vector of abstract, and each class indicates the research field of the corresponding publication.
% %
% {\bf Amazon} dataset contains three types of nodes. Each node in the Amazon dataset represents either a customer, a product, or a product category. The edges in the dataset represent various relationships, such as customer-product interactions, product similarities, or customer reviews. The node attributes are constructed using product descriptions or customer reviews, often represented as bag-of-words or other text-based features. Additionally, each class assigned to the nodes indicates the product category or customer behavior.

% Node features are given as bag-of-words representations of plots. 

% \subsection{Detailed Experimental Settings}  \label{app:settings}
% % \subsubsection{Node Classifiction}

% {\bf GAT:} We set $d$ = 64, $n_h$ = 8 for all datasets. For DBLP, ACM and Freebase, we set $s$ = 0.05 and $L$ = 3. For IMDB, we set $s$ = 0.1 and $L$ = 5. We use feat = 2 for DBLP and Freebase, feat = 1 for ACM, and feat = 0 for IMDB.

% {\bf GCN:} We set $d$ = 64 for all datasets. We set $L$ = 3 for DBLP, ACM and Freebase, and $L$ = 4 for IMDB. We use feat = 2 for DBLP and Freebase, and feat = 0 for ACM and IMDB.

% {\bf HGT:} We use layer normalization in each layer, and set $d$ = 64, $n_h$ = 8 for all datasets. $L$ is set to 2, 3, 3, 5 for ACM, DBLP, Freebase and IMDB respectively. For input feature type, we use feat = 2 in Freebase, feat = 1 in IMDB and DBLP, and feat = 0 in ACM.

% {\bf GTN:} We use adaptive learning rate suggested in their paper for all datasets. We set $d$ = 64, number of GTN channels as 2. For DBLP and ACM, we set $L$ = 2. For IMDB dataset, we set $L$ = 3.

% {\bf HAN:} We set $d$ = 8, $d_a$ = 128, $n_h$ = 8 and $L$ = 2 for all datasets. For input feature type, we use feat = 2 in Freebase, and feat = 1 in other datasets. We have also tried larger $d$, but the variation of performance becomes very large. Therefore, we keep $d$ = 8 as suggested in HAN’s code.

% {\bf RGCN:} We set $L$ = 5 for all datasets. For ACM, we set $d$ = 16, feats = 2. For DBLP and Freebase, we set $d$ = 16, feats = 3. For IMDB, we set $d$ = 32, feats = 1.

% {\bf MAGNN:} We set d = 64, $d_a$ = 128 and $n_h$ = 8 in all cases. We use feat = 1 in all cases. For DBLP and ACM datasets, we set the batch size as 8, and the number of neighbor samples as 100. For IMDB dataset, we use full batch training.

% {\bf MH-GCN:} We set $d$ = 384, $L$ = 2, $n_h$ = 8 for DBLP and IMDB as suggested in MH-GCN's code.

% {\bf Simple-HGN:} We set $d$ = $d_e$ = 64, $n_h$ = 8, $\beta$ = 0.05 for all datasets. For DBLP, ACM and Freebase datasets, we set $L$ = 3, $s$ = 0.05. For IMDB dataset, we set $L$ = 6, $s$ = 0.1. We set feat = 0 for IMDB, feat = 1 for ACM, and feat = 2 for DBLP and Freebase.

% {\bf SeHGNN:} We set $L$ = 2 and $d$ = 512 for all datasets as suggested in their paper.

% {\bf \alg:} We set $\beta$ = 0.05 for all datasets. ForDBLP, ACM, we set $d$ = de = 128, $L$ = 3, $s$ = 0.05. For IMDB dataset, we set $d$ = de = 512, $L$ = 2. $s$ = 0.1. For Freebase dataset, we set $d$ = de = 128, $L$ = 1, $s$ = 0.05. We set feat = 0 for IMDB, feat = 1 for ACM, and feat = 2 for DBLP and Freebase

% \subsubsection{Node Clustering}
% \red{We set $k$ = 4 for DBLP dataset, $k$ = 3 for ACM, $k$ = 5 for IMDB, $k$ = 7 for Freebase in all clustering experiments.}
% {\bf GAT:} We set k = 64 for all datasets..

% {\bf GCN:} We set d = 64 for all datasets..

% {\bf HGT:} \red{Todo}

% {\bf HAN:} We set d = 64 for all datasets..

% {\bf RGCN:} We set d = 64 for all datasets..

% {\bf MAGNN:} We set d = 64 for all datasets..

% {\bf MH-GCN:} \red{Todo}

% {\bf Simple-HGN:} We set $d$ = $d_e$ = 64, $n_h$ = 8, $\beta$ = 0.05 for all datasets. For DBLP, ACM and Freebase datasets, we set $L$ = 3, $s$ = 0.05. For IMDB dataset, we set $L$ = 6, $s$ = 0.1. We set feat = 0 for IMDB, feat = 1 for ACM, and feat = 2 for DBLP and Freebase.

% {\bf SeHGNN:} \red{Todo}


% \subsection{Baselines}
% The publicly source codes of baselines can be available at the following URLs:
% \begin{itemize}
%     \item {\textbf{GCN} - \url{https://github.com/tkipf/gcn}}
%     \item {\textbf{GAT} - \url{https://github.com/PetarV-/GAT}}
%     \item {\textbf{GTN} - \url{https://github.com/seongjunyun/Graph_Transformer_Networks}}
%     \item {\textbf{HGT} - \url{https://github.com/UCLA-DM/pyHGT}}
%     \item {\textbf{HAN} - \url{https://github.com/Jhy1993/HAN}}
%     \item {\textbf{RGCN} - \url{https://github.com/JinheonBaek/RGCN}}
%     \item {\textbf{MAGNN} - \url{https://github.com/cynricfu/MAGNN}}
%     \item {\textbf{Simple-HGN} - \url{https://github.com/cathytung1999/SImple_HGN}}
%     \item {\textbf{SeHGNN} - \url{https://github.com/ICT-GIMLab/SeHGNN}}
%     \item {\textbf{MH-GCN} - \url{https://github.com/NSSSJSS/MHGCN}}
% \end{itemize}


% \subsection{Comparison Results of Node clustering}


