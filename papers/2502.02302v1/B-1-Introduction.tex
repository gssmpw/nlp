\section{Introduction}
\label{sec:intro}

\IEEEPARstart{G}{raphs} generally contain rich node features and edge features. 
However, in recent years, most advanced GNN models have mainly focused on enhancing the learning of node features while neglecting the simultaneous learning of edge features. 
Although the aggregation functions designed based on the Message Passing Neural Network (MPNN) framework can aggregate both node features and edge features and have shown good results in specific application scenarios, using predefined aggregation functions is akin to manual feature engineering and cannot be generalized to all cases. 
Therefore, we aim to develop a method that can iteratively learn multidimensional edge features and synchronize the updates of these multidimensional edge features during the node information aggregation process. 
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.41\textwidth]{Fig/EdgeFGL.pdf}
    \caption{
    Illustration of the difference between information preferences and attention mechanisms in a message passing framework.
    \textbf{(a)} The edge feature representation in ordinary GCN;
    \textbf{(b)} The edge feature representation in our proposed \alg.
    % Information preferences lead to richer information, whereas attention mechanisms have more limited information.
    }
    \label{fig:intro1}
\end{figure}
This approach fully leverages the edge features to assist in node representation learning. 
As shown in Fig.~\ref{fig:intro1}(a), edge features in traditional Graph Convolutional Networks (GCNs) are represented by an adjacency matrix, which can only be expressed with binary indicator variables or one-dimensional real values, failing to capture rich edge information. 
In contrast, Fig.~\ref{fig:intro1}(b) illustrates our proposed method for representing multidimensional edge features. 
Edge features are no longer represented by a single real value in the adjacency matrix but by learnable feature vectors, which can express rich edge information.

% Due to the strong expressiveness of topological relation, the graph has been widely adopted to organize complex data by treating every single entity as a node and each interaction relation between entities as an edge.
% Due to the expressive power of topological relations, graphs have become widely adopted for organizing complex data, where each entity is treated as a node, and interactions between entities are represented as edges.
% %
% For example, in a citation network, publications are treated as nodes, while the citation relations form edges. 
% In social networks, users are treated as nodes, while the following relations form edges.
% %
% Therefore, network analysis becomes increasingly important, which aims to find effective information from huge amounts of graph structure data \cite{cui2018survey}. 
% %
% Due to the enormous number of nodes and quite limited interactions, graph data are usually large and sparse, which causes the curse of dimensionality, intractable noise, and data sparsity problems for network analysis. 
% %
% In this case, before conducting network analysis tasks, it is better to obtaining low-dimensional and more effective node representation instead of directly using raw data (Fig. \ref{fig:intro111}).

Many methods have been developed in learning node representations that can better reflect the local and global structure in graph \cite{caos2015learninggraphrepresentations, du2021tabularnet, yu2020structured}. 
%
However, the above models mainly adopt traditional machine learning methods such as random walk and matrix factorization. 
%
By considering the remarkable success of deep learning models in learning representations, many attempts have been made to utilize neural network models for network embedding \cite{wang2016structural, cao2016deep, he2019content}. 
%
These approaches mainly aim to adapt graph-structure data to the traditional neural networks designed for specific tasks, which may not ‘understand’ the inherent characteristic of graph \cite{velivckovic2017graph}. 
%
For instance, the effectiveness of Convolutional Neural Network (CNN) is based on its grid-like structure of image, and the expressive power of Recurrent Neural Network (RNN) \cite{lipton2015critical} is related to the sequential information within the corpus. %the word order inside corpus. 
%
However, none of them consider the topological relation of graph.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.45\textwidth]{Fig/introduction-v4.pdf}
%     \caption{Illustration of the difference between information preferences and attention mechanisms in a message passing framework. Information preferences lead to richer information, whereas attention mechanisms have more limited information.}
%     % In this network, Paper I is cited by both Paper A and Paper E, where Paper A focuses on the recommender system field and Paper E pays more attention to the node representation learning field.}
%     \label{fig:intro111}
% \end{figure}


To solve this problem, Heterogeneous Graph Neural Network (HGNN) \cite{shi2022heterogeneous, wang2019heterogeneous} models have been developed. 
%
By considering each node as the information entity, and each edge as the propagation channel, many HGNN models apply a message-passing framework \cite{xu2022explicit, ding2021diffmg} to depict the training process, which is mainly composed of the \textbf{information propagation} phase and the \textbf{information aggregation} phase. 
%
In the information propagation phase, nodes send their information to their neighbors through the edge channel, where information may be modified according to the target node, and the information passed through edges is called message. 
In the information aggregation phase, nodes update their information by aggregating the received messages through a specifically designed function.
%
For instance, in Graph Convolutional Network (GCN) \cite{kipf2016semi, yu2022multiplex}, nodes first propagate their information to neighbors, and then aggregate the received information to update their representations by averaging. 
%
% Graph Attention Network (GAT) \cite{velivckovic2017graph} assumes that the node should distribute different weights to different neighbors, adding an attention mechanism to learn the adaptive weight value.
Essentially, the weights in Graph Attention Networks (GAT) \cite{velivckovic2017graph} are functions of node features, and the attention weight coefficients between two connected nodes are computed based on the feature vectors of these two nodes. This makes GAT more adaptable to the fusion of node features and structural features, leading to better performance. However, edges also contain rich information. Although existing GNN models have made significant progress in some aspects, they still have deficiencies in handling edge features.

Two challenges may be caused by the above phenomenon.
\emph{\textbf{First}}, as the link between a pair of nodes, the edge contains the interaction information between them, e.g., the similar feature interests between the connected nodes, and the translation relation between the start node and end node, which cannot be sufficiently expressed by a scalar value.
\emph{\textbf{Second}}, as the propagation channel, the edge should refine the sent information in a finer grain, which can significantly enhance the effectiveness of information exchange. From the perspective of the target node, only a part of the received information can be meaningful, while the remaining may have negative impacts, which can be considered as a kind of noise.
Therefore, we aim to develop a method that can fully leverage multidimensional edge features and synchronize the updates of these features during the node information aggregation process to further enhance the performance of GNN models. 
This approach will enable nodes to acquire more precise and comprehensive information while also learning edge embeddings to improve the representation of edge features. 
With this improvement, nodes will not only obtain richer feature information but also achieve better results in processing more complex and diverse graph data.

We propose using \emph{Edge-empowered Graph Feature Preference Learning} (\alg) to jointly embed edges and nodes to address the above challenges. 
In this paper, we abandon the traditional GCN method of using binary indicator variables or one-dimensional real values to represent edge features and instead introduce multidimensional edge embeddings to fully utilize edge information. 
We also propose a new message passing framework that integrates multidimensional edge features and node features, thereby fully leveraging both node and edge information.
In each training epoch of EdgeFGL, the process can be simply divided into the following steps. During the information propagation phase, each node sends its current representation to its neighbors through the connected edge channels. 
Specifically, each channel is represented as a relation vector, and when the node representation propagates through the channel, it is refined by a Hadamard product~\ref{horn1990hadamard}. 
Finally, each node receives the refined representation from the connected edges, and the node representation is updated through information aggregation. 
Integrating edge learning and node learning into the same convolutional layer greatly improves the model’s efficiency and reduces its complexity.

Experimental results show that our proposed method achieves state-of-the-art performance in both node classification and clustering tasks. 
The \textbf{contributions} of this paper are as follows:
\begin{itemize}[leftmargin=*]
    \item We propose a new message passing framework that can simultaneously aggregate multi- edge and node features. By introducing the concepts of residual connections and dense connections, we construct a deep graph convolutional neural network model to capture long-range dependencies and non-local structural features between nodes.
    \item We eliminate the limitation of traditional GNNs that only use binary variables or one-dimensional real values to represent edge features and propose a multidimensional edge feature representation method. Our approach uses edge embeddings to encode rich edge information, which can be iteratively updated across graph convolutional layers.
	\item We design a multidimensional edge feature matrix and construct multi-channel filters to filter node information, while introducing an identity mapping mechanism to prevent over-smoothing. Additionally, we conducted extensive experiments on four real-world datasets, verifying the superiority of our proposed model in node classification and clustering tasks compared to state-of-the-art baseline methods.
\end{itemize}


% To overcome the above challenges, we propose using a Edge-Enhanced Graph Feature Preference Learning (EdgeFGL) to jointly embed edges and nodes.

% the representation learning of edges is supposed to be properly integrated into the message-passing model, which can significantly enhance the effectiveness of node representation learning.
% %
% We propose a novel model, termed \emph{Heterogeneous Graph Feature Preference Network} (\alg), which aims to properly associate the node representation learning, the edge representation learning, and first-order neighbors edge representation learning with the message-passing framework, so as to enhance the performance of information exchange.
% %
% In each training epoch of \alg, the procedure can be briefly divided into the following steps. 
% %
% Each node sends its current representation to neighbors during the information propagation phase through the connected edge channels. 
% %
% Specifically, each channel is represented as a relation vector, when node representation propagates through the channel, the representation is refined through the Hardmard product. 
% %
% Finally, each node receives the refined representations from the connected edges, and the node representation will be updated through information aggregation.

% \par\smallskip\noindent
% {\bf Specific contributions of this paper are as follows:}
% %
% \begin{enumerate}
%     \item Our work aims to explore heterogeneous graph representation learning in the message-passing framework, leveraging vector incorporation to elevate the dimensionality and intricacy of data representation, thereby enriching information capture and patterns to enhance the model's expressiveness.

%     % \item This work is to explore the problem of heterogeneous graph representation learning in the message-passing framework that performance in dealing with the graph-structured data has been widely confirmed.    
%     % \item By using vectors instead of scalars, edge representations can be more informative, thereby enabling the extraction of more effective relationships between nodes and their neighbors.
    
%     \item Acting as the propagation channel, edge representations in \alg~ are aware of feature preference, which can refine the information in a finer granularity rather than just changing the information scale. Nodes obtain more helpful information during the aggregation process, which can alleviate the influence of noise and enhance the performance of node representations.

%     \item We conducted extensive experiments on four real-world datasets to verify the superiority of our proposed model in node classification and clustering tasks when competing against state-of-the-art baselines.
%     % Extensive experiments are carried out over four datasets, and the results demonstrate our proposal's viability, effectiveness, and superiority.
% \end{enumerate}



% The rest of this paper is organized as follows. 
The subsequent sections of this paper are structured as follows:
In Sec. \ref{sec:Literature}, we briefly review related work. 
In Sec. \ref{sec:preliminary}, we introduce the preliminary knowledge and describe the motivation of this work.
In Sec. \ref{sec:method}, we describe in detail the proposed \alg~ model. 
In Sec. \ref{sec:experiments}, the experimental results are reported. 
Finally, the conclusion is drawn in Sec. \ref{sec:Conclude}.


