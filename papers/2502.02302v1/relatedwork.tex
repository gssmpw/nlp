\section{Related Work}
\label{sec:Literature}

This work relates to node representation learning, graph structure learning, and graph attention 
% graph-structure learning and deep learning models
in training representation, so we review prior studies in each category and discuss the differences and relations.

\par\smallskip\noindent
\textbf{Node Representation Learning.}
Traditional node representation learning methods mainly focus on mapping the original node representation vector (e.g. the adjacency vector) into the continuous low-dimension embedding space \cite{goldberg2014word2vec}. 
%
Those works usually extend the classical machine learning models for graph-structured data. 
%
The early works like Locally Linear Embedding (LLE) \cite{roweis2000nonlinear}, Isomap \cite{tenenbaum2000global} and Laplacian Eigenmaps \cite{belkin2001laplacian} try to adopt the eigen-based methods, so as to keep the spectral properties of node when mapping into the lower space, which is time consuming and uneasy to be parallelized. 
%
Recently, inspired by word2vec \cite{mikolov2013distributed}, DeepWalk \cite{perozzi2014deepwalk} adopts random walk to generate node sequences to imitate sentences in corpus, and then feeds them into the Skip-gram framework [26] to train node representations, which can to some extent learn both local and global structure information.
%
Node2vec \cite{grover2016node2vec} improves the DeepWalk model by introducing a bias mechanism into the random walk, which can get more efficient node sequences to enhance the model performance. 
%
Besides, Modularized Nonnegative Matrix Factorization (M-NMF) \cite{wang2017community} focuses on extracting the mesoscopic community structure among nodes. 
%
Furthermore, Attributed Network Embedding with Micromeso structure (ANEM) \cite{li2021attributed} considers three kinds of information in node representation learning: attribute information, the local proximity structure, and mesoscopic community structure. 
%
As we can see, the above methods usually pay major attention to manually integrating special rules with shallow models.


\par\smallskip\noindent
\textbf{Graph Structure Learning.}
Due to the success of deep learning models in training representations, many attempts have been made to integrate them with graph-structured data. 
%
For example, Structural Deep Network Embedding (SDNE)~\cite{wang2016structural} uses deep auto-encoders to extract low-dimensional node representations and adds a pairwise loss to ensure connected nodes are close. 
%
Deep Neural Networks for Graph Representations (DNGR)~\cite{cao2016deep} calculates the Positive Point Mutual Information (PPMI)~\cite{bullinaria2007extracting} matrix for graphs and feeds it into a stacked denoising autoencoder to extract node representations. 
%
ContextAware Network Embedding (CANE)~\cite{tu2017cane} combines a mutual attention mechanism with CNNs to learn context-aware node representations. 
%
Self-Translation Network Embedding (STNE)~\cite{liu2018content} uses bi-LSTM on node sequences obtained by random walks to preserve node order information. 
%
Deep Attributed Network Embedding (DANE)~\cite{gao2018deep} assumes node representations from structure and feature information should be similar. 
%
Self-Paced Network Embedding (SPNE)~\cite{gao2018self} adopts a self-paced learning mechanism to learn node representations, considering information from neighbors of increasing hops. 
%
Deep Network Embedding with Structural Balance Preservation (DNE-SBP)~\cite{shen2018deep} uses stacked auto-encoders to preserve structural balance in signed networks. 
%
Nodepair Information Preserving Network Embedding (NINE)~\cite{wang2020node} uses adversarial networks to preserve local information between node pairs. 
%
However, traditional approaches often apply existing models to train node representations without specifically targeting graph-structured data.



% Due to the great success of deep learning models in training representation, many attempts have been made for integrating existing deep learning models with graph-structured data. 
% %
% For example, Structural Deep Network Embedding (SDNE)~\cite{wang2016structural} adopts deep auto-encoder to extract the lower dimensional node representation, and adds a pairwise loss to restrict the representation of the connected nodes to be close. 
% %
% Deep Neural networks for Graph Representations (DNGR)~\cite{cao2016deep} further investigates the structure information by calculating the Positive Point Mutual Information (PPMI) matrix~\cite{bullinaria2007extracting} for graph, and then feeds the matrix into the stacked denoising autoencoder model~\cite{vincent2010stacked} to extract the node representation. 
% %
% ContextAware Network Embedding (CANE)~\cite{tu2017cane} incorporates the mutual attention mechanism~\cite{vaswani2017attention} with CNN~\cite{lecun2015deep} to learn the context-aware node representation. 
% %
% Self-Translation Network Embedding (STNE)~\cite{liu2018content} feeds node sequence obtained by random walk into the bi-LSTM~\cite{sutskever2014sequence} model to extract the node representation, so as to keep the node order information. 
% %
% Deep Attributed Network Embedding (DANE)~\cite{gao2018deep} assumes that node representations learned from structure and feature information should be similar. 
% %
% Self-Paced Network Embedding (SPNE)~\cite{gao2018self} considers the process of node receiving information from 1-hop neighbors to high-hop neighbors as learning knowledge from easy to hard, and thus adopts the self-paced learning mechanism to learn the node representation. 
% %
% Deep Network Embedding with Structural Balance Preservation (DNE-SBP)~\cite{shen2018deep} adopts stacked auto-encoder and preserves the structural balance~\cite{davis1967clustering} for the signed networks. 
% %
% The Nodepair Information Preserving Network Embedding (NINE)~\cite{wang2020node} aims to preserve the information between a pair of nodes in a local way by means of adversarial networks. 
% %
% However, the traditional works mainly apply the existing models to train node representation, which are not targeted to the graph structured data.



% The above message-passing-based models tend to treat each edge as a scalar value, which will limit the representation capability of edges. 
% %
% For better utilizing the edge information, several models take edge representation learning into consideration. 
% %
% In CensNet \cite{pan2019learning}, two kinds of graphs are constructed to represent the adjacent relation of nodes and edges, and then node-based and edge-based layers are built to learn the corresponding representations, where the correlation between the representation of node and edge is constructed by sharing one trainable weight. 
% %
% In \cite{jiang2019censnet}, inspired by translation mechanisms \cite{tu2017transnet, lin2015learning}, edge representations are learned by assuming the transforming relation among the edge and two corresponding end nodes in latent space. 
% %
% Similarly, in \cite{wang2014knowledge}, Vectorized Relational Graph Convolutional Network (VR-GCN) deals with the multi-relation network alignment task [46] by combining the translation mechanism with GCN to train the representations of both nodes and the predefined relations in knowledge graph.
% \paragraph*{Heterogeneous graphs}



\par\smallskip\noindent
\textbf{Graph Attention Learning.} 
The message-passing framework, designed to fit the topological characteristics of graphs, has been widely adopted in various graph neural networks. 
In Graph Convolutional Networks (GCN) \cite{kipf2016semi}, nodes propagate their information to neighbors and then update their representations by averaging the received information. 
Graph Attention Networks (GAT) \cite{velivckovic2017graph} improve on this by using an attention mechanism to assign different weights to different neighbors. 
Building on GCN and GAT, several methods have been developed to enhance the message-passing framework: Hierarchical Graph Convolutional Networks (HGCN) \cite{hu2019hierarchical} use a coarsening-refining model to hierarchically aggregate node information; 
Relation-aware co-attentive Graph Convolutional Networks (RecoGCN) \cite{xu2019relation} integrate a co-attention mechanism to adapt GCN for recommender systems by bridging user, item, and agent relations; 
Dynamic Hypergraph Neural Networks (DHNN) \cite{jiang2019dynamic} introduce a hypergraph mechanism to capture group relations, where each hyperedge covers a group of nodes; 
and Graphsage \cite{hamilton2017inductive} extends GCN to inductive learning and explores various aggregation operations like max pooling and LSTM. 
However, these models often treat each edge as a scalar, limiting edge representation capability. 
To better utilize edge information, some models incorporate edge representation learning: 
CensNet \cite{pan2019learning} constructs two graphs to represent node and edge adjacency, using shared trainable weights to link node and edge representations. 
Inspired by translation mechanisms \cite{jiang2019censnet}, edge representations are learned by assuming a transforming relation between edges and their corresponding nodes. 
Vectorized Relational Graph Convolutional Network (VR-GCN) \cite{ye2019vectorized} combines translation mechanisms with GCN to train node and relation representations in multi-relation network alignment tasks.

However, those models treat the representation learning of edge as the representation learning of node, without considering node interaction, and thus fail to learn the relation between the connected nodes. 
In this case, these models cannot properly integrate the edge representation learning into the message-passing framework.


% The message-passing framework, which is proposed for the topological relation of graph, is adopted to better fit the intrinsic characteristic of graph. 
% %
% In Graph Convolutional Network (GCN) \cite{kipf2016semi}, nodes first propagate their information to neighbors, and then aggregate the received information to update their representations by averaging. 
% %
% Graph Attention Network (GAT) \cite{velivckovic2017graph} assumes that the node should distribute different weights to different neighbors, and hence utilizes the attention mechanism to learn the weight value. 
% %
% Based on GCN and GAT, several methods are proposed to further utilize the message-passing framework. 
% %
% For instance, Hierarchical Graph Convolutional Networks (HGCN) \cite{hu2019hierarchical} design a coarsening-refining model to hierarchically aggregate the information for each node. 
% %
% Relation-aware co-attentive Graph Convolutional Networks (RecoGCN) \cite{xu2019relation} integrate the co-attention mechanism into GCN, which bridges the relation between user, item, and and selling agent to adapt GCN to the recommender system. 
% %
% In order to capture the group relation, Dynamic Hypergraph Neural Network (DHNN) \cite{jiang2019dynamic} extends the GCN model by introducing the hypergraph mechanism, where each hyperedge can cover a group of nodes. 
% %
% Graphsage \cite{hamilton2017inductive} extends the GCN model to the inductive learning mode and investigates different aggregating operations (e.g. max pooling, LSTM).  
% %
% The above message-passing based models tend to treat each edge as a scalar value, which will limit the representation capability of edges. 
% %
% For better utilizing the edge information, several models take the edge representation learning into consideration.
% %
% In CensNet~\cite{pan2019learning}, two kinds of graphs are constructed to represent the adjacent relation of nodes and edges, and then node-based and edge-based layers are built to learn the corresponding representations, where the correlation between the representation of node and edge is constructed by sharing one trainable weight. 
% In~\cite{jiang2019censnet}, inspired by translation mechanisms~\cite{tu2017transnet, lin2015learning}, edge representations are learned by assuming the transforming relation among the edge and two corresponding end nodes in latent space. 
% Similarly, in~\cite{wang2014knowledge}, Vectorized Relational Graph Convolutional Network (VR-GCN) deals with the multi-relation network alignment task~\cite{ye2019vectorized} by combining the translation mechanism with GCN to train the representations of both nodes and the predefined relations in knowledge graph.


%
% Due to the great success of deep learning models in training representation, many attempts have been made to integrate existing deep learning models with graph-structured data. For example, Structural Deep Network Embedding (SDNE) [9] adopts deep auto-encoder to extract the lower dimensional node representation, and adds a pairwise loss to restrict the representation of the connected nodes to be close. Deep Neural networks for Graph Representations (DNGR) [10] further investigate the structure information by calculating the Positive Point Mutual Information (PPMI) matrix [28] for graph, and then feeding the matrix into the stacked denoising autoencoder model [29] to extract the node representation. ContextAware Network Embedding (CANE) [30] incorporates the mutual attention mechanism [31] with CNN [32] to learn the context-aware node representation. Self-Translation Network Embedding (STNE) [11] feeds node sequences obtained by a random walk into the bi-LSTM [33] model to extract the node representation, so as to keep the node order information. Deep Attributed Network Embedding (DANE) [12] assumes that node representations learned from the structure and feature information should be similar. Self-Paced Network Embedding (SPNE) [13] considers the process of the node receiving information from 1-hop neighbors to high-hop neighbors as learning knowledge from easy to hard, and thus adopts the self-paced learning mechanism to learn the node representation. Deep Network Embedding with Structural Balance Preservation (DNE-SBP) [34] adopts stacked auto-encoder and preserves the structural balance [35] for the signed networks. The Nodepair Information Preserving Network Embedding (NINE) [36] aims to preserve the information between a pair of nodes locally through adversarial networks. However, the traditional works mainly apply the existing models to train node representation, which are not targeted to the graph-structured data.



% \paragraph*{Graph Structure Learning}
% The message-passing framework, which is proposed for the topological relation of graph, is adopted to better fit the intrinsic characteristic of graph. 
% %
% In Graph Convolutional Network (GCN) \cite{kipf2016semi}, nodes first propagate their information to neighbors, and then aggregate the received information to update their representations by averaging. 
% %
% Graph Attention Network (GAT) \cite{velivckovic2017graph} assumes that the node should distribute different weights to different neighbors, and hence utilizes the attention mechanism to learn the weight value. 
% %
% Based on GCN and GAT, several methods are proposed to further utilize the message-passing framework. 
% %
% For instance, Hierarchical Graph Convolutional Networks (HGCN) \cite{hu2019hierarchical} design a coarsening-refining model to hierarchically aggregate the information for each node. 
% %
% Relation-aware co-attentive Graph Convolutional Networks (RecoGCN) \cite{xu2019relation} integrate the co-attention mechanism into GCN, which bridges the relation between user, item, and selling agent to adapt GCN to the recommender system. 
% %
% In order to capture the group relation, Dynamic Hypergraph Neural Network (DHNN) \cite{jiang2019dynamic} extends the GCN model by introducing the hypergraph mechanism, where each hyperedge can cover a group of nodes. 
% %
% Graphsage \cite{hamilton2017inductive} extends the GCN model to the inductive learning mode and investigates different aggregating operations (e.g. max pooling, LSTM).  
% %
% % The above message-passing-based models tend to treat each edge as a scalar value, which will limit the representation capability of edges. 
% % %
% % For better utilizing the edge information, several models take edge representation learning into consideration. 
% % %
% % In CensNet \cite{pan2019learning}, two kinds of graphs are constructed to represent the adjacent relation of nodes and edges, and then node-based and edge-based layers are built to learn the corresponding representations, where the correlation between the representation of node and edge is constructed by sharing one trainable weight. 
% % %
% % In \cite{jiang2019censnet}, inspired by translation mechanisms \cite{tu2017transnet, lin2015learning}, edge representations are learned by assuming the transforming relation among the edge and two corresponding end nodes in latent space. 
% % %
% % Similarly, in \cite{wang2014knowledge}, Vectorized Relational Graph Convolutional Network (VR-GCN) deals with the multi-relation network alignment task [46] by combining the translation mechanism with GCN to train the representations of both nodes and the predefined relations in knowledge graph.
% % \paragraph*{Heterogeneous graphs}

% \paragraph*{Graph Attention Learning} 
% Traditional node representation learning methods mainly focus on mapping the original node representation vector (e.g. the adjacency vector) into the continuous low-dimension embedding space \cite{goldberg2014word2vec}. 
% %
% Those works usually extend the classical machine learning models for graph-structured data. 
% %
% The early works like Locally Linear Embedding (LLE) \cite{roweis2000nonlinear}, Isomap \cite{tenenbaum2000global} and Laplacian Eigenmaps \cite{belkin2001laplacian} try to adopt the eigen-based methods, so as to keep the spectral properties of node when mapping into the lower space, which is time consuming and uneasy to be parallelized. 
% %
% Recently, inspired by word2vec \cite{mikolov2013distributed}, DeepWalk \cite{perozzi2014deepwalk} adopts random walk to generate node sequences to imitate sentences in corpus, and then feeds them into the Skip-gram framework [26] to train node representations, which can to some extent learn both local and global structure information.
% %
% Node2vec \cite{grover2016node2vec} improves the DeepWalk model by introducing a bias mechanism into the random walk, which can get more efficient node sequences to enhance the model performance. 
% %
% Besides, Modularized Nonnegative Matrix Factorization (M-NMF) \cite{wang2017community} focuses on extracting the mesoscopic community structure among nodes. 
% %
% Furthermore, Attributed Network Embedding with Micromeso structure (ANEM) \cite{li2021attributed} considers three kinds of information in node representation learning: attribute information, the local proximity structure, and mesoscopic community structure. 
% %
% As we can see, the above methods usually pay major attention to manually integrating special rules with shallow models.

% % Due to the great success of deep learning models in training representation, many attempts have been made to integrate existing deep learning models with graph-structured data. For example, Structural Deep Network Embedding (SDNE) [9] adopts deep auto-encoder to extract the lower dimensional node representation, and adds a pairwise loss to restrict the representation of the connected nodes to be close. Deep Neural networks for Graph Representations (DNGR) [10] further investigate the structure information by calculating the Positive Point Mutual Information (PPMI) matrix [28] for graph, and then feeding the matrix into the stacked denoising autoencoder model [29] to extract the node representation. ContextAware Network Embedding (CANE) [30] incorporates the mutual attention mechanism [31] with CNN [32] to learn the context-aware node representation. Self-Translation Network Embedding (STNE) [11] feeds node sequences obtained by a random walk into the bi-LSTM [33] model to extract the node representation, so as to keep the node order information. Deep Attributed Network Embedding (DANE) [12] assumes that node representations learned from the structure and feature information should be similar. Self-Paced Network Embedding (SPNE) [13] considers the process of the node receiving information from 1-hop neighbors to high-hop neighbors as learning knowledge from easy to hard, and thus adopts the self-paced learning mechanism to learn the node representation. Deep Network Embedding with Structural Balance Preservation (DNE-SBP) [34] adopts stacked auto-encoder and preserves the structural balance [35] for the signed networks. The Nodepair Information Preserving Network Embedding (NINE) [36] aims to preserve the information between a pair of nodes locally through adversarial networks. However, the traditional works mainly apply the existing models to train node representation, which are not targeted to the graph-structured data.

%  However, those models treat the representation learning of edge as the representation learning of node, without considering node interaction, and thus fail to learn the relation between the connected nodes. In this case, these models cannot properly integrate the edge representation learning into the message-passing framework.

% \paragraph*{Learning Topological Relations of Graph}


% This work relates to online learning and copula modeling, so we review prior studies in each category and discuss the differences and relations.
% %
% We note that, in concept-drift \cite{gama2014survey,lu2018learning} or non-stationary online learning \cite{zhang2020online}, the changeables are the statistical properties of variables or the underlying decision function, respectively, as data streaming in, but the number of features carried by each input is fixed in a priori.
% %
% Additionally, in the settings of streaming feature mining~\cite{wu2012online,wu2019online,wu2021latent}, although the new features constantly emerge, the numbers of instances are known beforehand.
% %
% The dynamics of both the instance and feature spaces differentiate our learning problem from theirs.

% \paragraph*{Online Learning in Variable Feature Space}
% %
% Requiring an input sequence over wide time spans to be described by a fixed set of features is in general impractical.
% %
% In response, the pioneer work~\cite{zhang2015towards,zhang2016online,hou2018safe,beyazit2018learning} considered a doubly-streaming setting where new inputs carry consistently more features.
% %
% Later studies extend this setting by allowing the pre-existing features to be missed out afterwards,
% either by following a batch-by-batch regularity~\cite{hou2017learning,hou2017one,zhang2020learning,hou2020storage,9406178} or at purely random~\cite{manzoor2018xstream,beyazit2019online,he2019online,he2020toward,he2021online}.
% %
% This line of research shares the main idea of exploiting feature-wise relationship 
% to anchor stationary information in a varied feature space.
% %
% Unfortunately, existing methods all consider a continuous  domain 
% and thus do not scale up to the mixed data setting.
% %
% Our \alg\ filled the gap with Gaussian copula that 
% maps and correlates arbitrary marginals across mixed variables
% and hence is more general. 

% \paragraph*{Copula Modeling with Mixed Data}
% %
% To explore a continuous latent space, most of methods assume that the data are of the same digital type and make a prior guess of the underlying distribution, as directly modeling the multivariate joint distribution for mixed marginals is difficult. 
% %
% In this regard, copulas lend us a tool due to its modeling power. 
% %
% Prior studies~\cite{durante2010copula,haugh2016introduction,lopez2013gaussian} proposed Gaussian copula by combining the cumulative distribution function (CDF) of each feature, and separating marginal distribution from multivariate distribution, 
% which is suitable for modeling with different marginal distributions, 
% harmonizing continuousness and discreteness.
% %
% %cite copula'
% %
% Subsequent works include Bayesian copula with factorized models~\cite{murray2013bayesian},
% impute missing entries in a mixed data matrix by extending Rank-PC~\cite{cui2017robust},
% and solving the time-varying complexities and high-dimensionality  in estimating the 
% covariance matrices~\cite{salinas2019high}.
% %
% However, most studies focus on modeling offline mixed data,
% with very few attempted online settings~\cite{zhao2020missing,landgrebe2020online}.
% %
% None prior work has considered leveraging Gaussian copula
% to deal with an arbitrarily varying feature space so as to learn effective online learners;
% Our \alg\ algorithm strived to fill the gap.


% Our research is also similar to missing feature imputation of mixed data.
% %
% For MNAR data, we can estimate by modeling missing data distribution, most of method estimating parameters in generalized linear models and maximum likelihood estimation\cite{ibrahim1999missing,morikawa2017semiparametric}. However, considering the missing completely at random, make previous assumption for distribution or parameter is not reasonable.
% %
% Other common methods such as low rank matrix completion\cite{udell2016generalized}, widely used in missing value filling and recommendation systems. But these method is base on the assumption that the data can be well approximated by low rank matrices. Fan, Jicong and Zhang \etc \cite{fan2020polynomial} propose a high-rank matrix generation model that maps low-dimensional latent variables to high-dimensional environment space, and recover missing items by solving a polynomial matrix completion (PMC) problem. However these method has strictly assumption about dimensions of the matrix, and it is difficult to find a approximate matrix for mixed data.
% %
% Some of methods propose use graphs to model missing data. paper\cite{burges2013graphical}use Directed Acyclic Graph to model missing causal mechanisms, and demonstrated the feasibility of using the graph model to restore missing. paper\cite{salti2020random}propose, the missing data with observable data can be a Random Intersection Graphs, and can reconstruct the feature by the correlation of graphs. By contrast, refers to Zhao, Yuxuan \etc\cite{zhao2020missing,zhao2020online}, Gaussian copula for missing feature reconstructing is more flexible and it can handle Sparse Data Stream in online learning.
% %
% These methods is mainly restore the original data, but we focus on reconstruct the correlation of feature through latent vector, so as to obtain a stable feature space.

  
%cite OLVF


% (such that it projects instances and classifier into a shared feature space).

% cite missing value imputation
  

%