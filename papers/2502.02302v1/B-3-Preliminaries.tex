\section{Background and moviation}
\label{sec:preliminary}

In this section, we begin by introducing the preliminaries that include the notations, preliminary (\emph{i.e.}, message-passing framework), and motivation. % in this paper

\subsection{Notations} \label{sec:notation}
Let $\mathcal{G} =\{\mathcal{V}, \mathcal{E}, \mathbf{F}, \phi, \psi \}$ denote an undirected attributed network, where $\mathcal{V} = \{ \upsilon_1, \upsilon_2, \cdots, \upsilon_{n} \}$ is a set of $n$ nodes, $\mathcal{E} = \{e_{ij}\ |\ i, j \in \text{nodes}, i \neq j\}$ is a set of $m$ edges, and a node attribute matrix $\mathbf{F} = [\mathbf{f}_{1};\ \cdots;\ \mathbf{f}_{n}] \in \mathbb{R}^{n \times d} $ with each row vector $\mathbf{f}_{i} \in \mathbb{R}^{n \times d} $ being the attribute vector of node $\upsilon_i$.
The $\phi(v)$ denotes the type of each node $\phi$, and $\psi(e)$ denotes the type of each edge $\psi$. 
Additional, each dimension being considered as a feature.
% Each dimension being considered as a feature; each node $\phi$ has a type $\phi(v)$; each edge $\psi$ has a type $\psi(v)$. 
%
% The sets of possible node types and edge types are denoted by $\mathbf{T}_v=\{\phi(v): \forall v \in \mathcal{V}\}$ and $\mathbf{T}_e=\{\phi(e): \forall e \in \mathcal{E}\}$, respectively. When $|\mathbf{T}_v| = |\mathbf{T}_e| = 1$, the graph degenerates into an ordinary homogeneous graph.
The collections of potential node types and edge types are symbolized as $\mathbf{T}_v=\{\phi(v): \forall v \in \mathcal{V}\}$ and $\mathbf{T}_e=\{\phi(e): \forall e \in \mathcal{E}\}$, correspondingly. 
When both $|\mathbf{T}_v|$ and $|\mathbf{T}_e|$ equal 1, the graph reverts to a conventional homogeneous graph.
%
Besides, each node $\upsilon_i \in \mathcal{V}$ has neighbor nodes, the index set of which is denoted as $\mathcal{N}_{i}$. 

We use $\mathbf{H}^{l} = [\mathbf{h}^{l}_{1}; \cdots; \mathbf{h}{^{l}_{n}}] \in \mathbb{R}^{n \times {d^{l}} }$ to denote the node representation matrix in the $l$-th layer of \alg~where the $i$-th row vector $\mathbf{h}^{l}_{i} \in \mathbb{R}^{n \times {d^{l}} }$ denotes the node representation vector of the $i$-th node in the $l$-th layer with dimensionality being $d^{l}$. 
% And there are a total of $L$ layers. 
Note that the initial node representation matrix is assigned as the node attribute matrix in the initial status, \emph{i.e.}, $\mathbf{H}^{l} = \mathbf{F}$ with $d^{0} = d$. 
%
The relation representation of each edge $e_{ij}$ in the $l$-th layer is denoted as $\mathbf{r}{^{l}_{ij}} \in \mathbb{R}^{n \times {d^{l}}}$. 
%
Besides, $\mathbf{Y} = [ y_{ik} ] \in \mathbb{R}^{n \times {c}}$ represents the label indicator matrix, where each element $y_{ik}$ indicates whether the $i$-th node has the $k$-th label.
%
Key notations used in the paper and their definitions are summarized in Table \ref{tab:notation}.
\begin{table}[!h]
	\centering
	\small
	\caption{Notations and Definitions.}
        % \scalebox{.98}{
    	\begin{tabular}{l|p{5.5cm}}              
    		\toprule
    		\midrule
    		Notation    & Definitions  \\ 
    		\midrule
    		$\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{F}, \phi, \psi\}$    & The undirected attributed network   \\
                \midrule
                $\mathcal{V}=\{v_1, \cdots, v_n\}$    & The node set consisting of $n$ nodes  \\ 
                \midrule
                $\mathcal{E}=\{e_{12}, \cdots, e_{ij}\}$    & The edge set consisting of $m$ edges\\
                \midrule
                $\mathbf{F}=[ \mathbf{f}_{1}, \cdots, \mathbf{f}_{n} ]$  & The node attribute matrix \\
                \midrule
                $\mathbf{f}_{i} \in \mathbb{R}^{1 \times d}$ & The attribute vector of node $v_{i}$ \\
                \midrule
                $\mathbf{T}_{v}, \mathbf{T}_{e}$ & The sets of the possible node and edge types \\
                \midrule
                % $\mathbf{T}_{e} $ & The sets of the possible edge types \\
                % \midrule
                $\mathcal{N}_{i}$& The index set of neighbor nodes of $v_{i}$ \\
                \midrule
                % $\mathbf{Z}$    & The probability matrix\\
                % \midrule
                $\sigma(\cdot)$ & The non-linear activation function. \\
                \midrule
                $\mathcal{S}(\cdot, \cdot)$ & The information propagation function.\\
                \midrule
                % $\text{att}(\cdot, \cdot)$ & The attention function \\
                % \midrule
                $\boldsymbol{o}_i$ & The output embedding of node $i$ \\
                \midrule
                $y_{ik}$ & The label indicator denoting whether the $i$-th node has the $k$-th label \\
                \midrule
                $\mathbf{H}^{l} = [h_{1}^{l}, \cdots, h_{n}^{l}]$ & The node representation matrix in the $l$-th layer\\
                \midrule
                $\mathbf{h}_{i}^{l} \in \mathbb{R}^{1 \times d}$  & The node representation vector of node $v_{i}$ in the $l$-th layer \\
                \midrule
                 $\mathbf{r}_{ij}^{l} \in \mathbb{R}^{1 \times d^{l}}$  & The relation representation of edge $e_{ij}$ in the $l$-th layer \\
                \midrule
                $\mathbf{Y}=[y_{ik}] \in \mathbb{R}^{n \times c}$ & The label indicator matrix\\
                \midrule
                $\mathbf{M}^{l}_{ij}\in \mathbb{R}^{1 \times d^{l}}$ & The representation of the sent message from node $v_i$ to node $v_j$ in the $l$-th layer\\
                \midrule
                $\mathbf{M}^{l}_{*i}\in \mathbb{R}^{1 \times d^{l}}$ & All of the information representations that node $v_i$ receives in the $l$-th layer\\
                \midrule
                $\mathbf{W}^{l} \in \mathbb{R}^{d^{1} \times d^{l+1}}$ & The trainable projection matrix in information propagation \\
                % \midrule
                % $\boldsymbol{o}_i$ & The output embedding of node $i$ \\
    		\bottomrule
    	\end{tabular}
        % }
	\label{tab:notation}
\end{table}

\subsection{Message-passing Framework} \label{sec:message}
The message-passing framework is composed of information propagation and information aggregation phases.
The information propagation phase is formalized as follows:
\begin{eqnarray}
    \mathbf{M}_{j i}^l=\mathcal{S}\left(\mathbf{h}_j^l, e_{j i}\right),
    \label{eq:1}
\end{eqnarray}
where $\mathbf{M}_{ji}^l \in \mathbb{R}^{1 \times d^{l}}$ denotes the representation of the sent message from node $\upsilon_{j}$ to node $\upsilon_{i}$ in the $l$-th layer; $\mathcal{S}(\cdot, \cdot)$ denotes the information propagation function.

The information aggregation phase is formalized as follows:
\begin{equation}
    \mathbf{h}_i^{l+1}=\mathcal{R}\left(\mathbf{h}_i^l, \mathbf{M}_{* i}^l\right),
    \label{eq:2}
\end{equation}
where $\mathbf{M}_{* i}^l \in \mathbb{R}^{n \times d^{l}}$ denotes all of the information representations that node $\upsilon_{i}$ receives in the $l$-th layer;
$\mathcal{R}(\cdot, \cdot)$ denotes the information aggregation function. 
The two famous GNN-based models can be formalized via the above message-passing framework, as will be elaborated below.

1) \emph{Graph Convolutional Network (GCN)}: GCN can be reconsidered in the message-passing framework.
In the information propagation phase, the information propagation of GCN is specified as follows:
\begin{equation}
    \mathbf{M}_{j i}^l=a_{j i} \cdot \mathbf{h}_j^l,
    \label{eq:3}
\end{equation}
where $a_{ij}$ is the averaging factor computed by $\frac{1}{\sqrt{\left|\mathcal{N}_i\right| \cdot\left|\mathcal{N}_j\right|}}$, which is inversely proportional to the number of neighbors.

In the information aggregation phase, the aggregation function is constructed as follows:
\begin{equation}
    \mathbf{h}_i^{l+1}=\sigma\left(\sum_{j \in \mathcal{N}_i \cup i} \mathbf{M}_{j i}^l \cdot \mathbf{W}^l\right),
    \label{eq:4}
\end{equation}
where $\mathbf{W}^l \in \mathbb{R}^{n \times {d^{l}}}$ is the trainable projection matrix from the $l$-th layer, and $\sigma(\cdot)$ is the non-linear activation function.

2) \emph{Graph Attention Network (GAT)}: GAT improves the GCN model by introducing the attention mechanism to compute adaptive weights for different nodes in a neighborhood during the information propagation phase. 
That is, the information propagation phase is formalized as:
\begin{equation}
    \mathbf{M}_{j i}^l= \text{att} (\mathbf{h}_j^l, \mathbf{h}_i^l ) \cdot \mathbf{h}_j^l,
    \label{eq:5}
\end{equation}
where $\text{att}(\cdot, \cdot)$ is the attention function. 
The aggregation phase of GAT is the same as that of GCN, which can also be formalized as Eq. (\ref{eq:4}).

\subsection{Motivation.}
The goal of the message-passing framework is to refine the information of each node by exchanging their information with neighbors, which efficiently utilizes the topological relation inside graph. 
In this case, the effectiveness of information exchange is vital in the message-passing framework. 
As we can see from Eqs. (\ref{eq:3}) and (\ref{eq:5}), both GCN and GAT pay much attention to modifying the scale of information,
but lack the consideration of the feature preference of each node. 
\textbf{1)} Nodes usually prefer to receive the part of related features instead of the whole information. 
\textbf{2)} The unrelated features inside the received information may act as noise to the target node.

In this work, we attempt to develop a method to enhance the effectiveness of the message-passing framework, which is aware of the node feature preference during the information exchange process.

\begin{definition}[Feature Preference Aware Message-passing, FPAMP]
    When node $v_{i}$ sends information to neighbor $v_{j}$, instead of modifying the scale of whole information, the connected edge $e_{ij}$ can magnify the related features while shrinking the unrelated features.
\end{definition}

Toward this end, we integrate the edge representation into the message-passing framework and propose the \emph{Edge-Enhanced Graph Feature Preference Learning} (\alg) model in the next section, which can effectively enhance the performance of information exchange.


% The goal of the message-passing framework is to refine the information of each node by exchanging their information with neighbors, which efficiently utilizes the topological relation inside graph. In this case, the effectiveness of information exchange is vital in the message-passing framework. As we can see from Eq. (3) and Eq. (5), both GCN and GAT pay much attention to modifying the scale of information,
% but lack the consideration of the feature preference of each node. For one thing, nodes usually prefer to receive the part of related features instead of the whole information. For another, the unrelated features inside the received information may act as noises to the target node.

% In this work, we attempt to develop a method to enhance the effectiveness of the message-passing framework, which is aware of the node feature preference during the information exchange process.

% To this end, we integrate the edge representation into the message-passing framework and propose the Graph Feature Preference Network (GFPN) model in the next section, which can effectively enhance the performance of information exchange.


