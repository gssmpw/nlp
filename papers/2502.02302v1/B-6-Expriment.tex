\section{{\bf Experiments}}
\label{sec:experiments}

% In this section, we deliver empirical evidences to substantiate that our HGFPN algorithm is a viable and effective solution to the problem of global representation learning and excels in four network datasets.
%
In this section, we evaluate the effectiveness of \alg~on four network datasets by comparing with eight state-of-the-art methods. 
%
Specifically, we conduct node classification experiment to evaluate the end-to-end learning performance of our model, node clustering experiment to evaluate whether the learned node representations can fit the correct node class distribution.
%
Besides, for observing the intuitive display of node representation, the node visualization is conducted as an auxiliary experiment.
%
Finally, for giving a time Analysis and parameter sensitivity of our model, we conducted an explanatory ablation study on time complexity and sensitivity analysis of model parameters. We chose the ACM and DBLP datasets and visualized the model's efficiency and performance across various parameters.

%
The experiments are designed to answer the following research questions ({\bf RQs}):
\begin{itemize}[leftmargin=*]
	\item \textbf{RQ1.} {\em Does our \alg\ outperform the state-of-the-arts in Node Classification?}
    \item \textbf{RQ2.} {\em How does the clustering and visualisation of nodes perform?}
	\item \textbf{RQ3.} {\em How effective is each component of our model?}
	\item \textbf{RQ4.} {\em How is the time consumption and sensitivity to important parameters of our \alg?}
\end{itemize}
Section~\ref{sec:datasets} introduces the studied datasets.
%
Sections~\ref{sec:protocol} elaborates the experiment setup.
%
Results and findings are given in Sections \ref{sec:node}, \ref{sec:ablation} and \ref{sec:time}.


\subsection{Datasets} \label{sec:datasets}
% Our evaluations are benchmarked on four datasets, with their statistics summarized in Table~\ref{tab:dataset}.
% %
% To validate the generalizability of our \alg\ algorithm, we select the datasets from a diverse range of applications including computer science, movie, wikipedia, and so on.
Our assessments are compared against four sets of data, and their metrics are condensed in Table~\ref{tab:dataset}. 
In order to verify the wide applicability of our \alg\ algorithm, we handpicked the datasets from a variety of domains such as computer science, films, wikipedia, and others.
%
% \subsubsection{Node Classification}
For node classification, a transductive approach is employed, wherein all edges are accessible during the training phase, and node labels are partitioned with 24\% for training, 6\% for validation, and 70\% for testing within each dataset.
% Node Classification adopts a transductive setting, where all edges are available during training and node labels are separated according to 24\% for training, 6\% for validation and 70\% for test in each dataset.
%
They are detailed descriptions are listed as follows:
\begin{itemize}[leftmargin=*] 
    \item {\bf DBLP\footnote{\url{{http://web.cs.ucla.edu/~yzsun/data/}}}}: This dataset captures a computer science bibliography website, employing a prevalent subset across four domains with nodes symbolizing authors, papers, terms, and venues.
    % This dataset is a bibliography website of computer science. We use a commonly used subset in 4 areas with nodes representing authors, papers, terms and venues. 
    \item {\bf ACM\footnote{\url{{https://libraries.acm.org/digital-library/acm-guide-to-computing-literature}}}}: This dataset concerns a movie-oriented website encompassing diverse genres like Action, Comedy, Drama, Romance, and Thriller.
    % This dataset is a website about movies and related information. A subset from Action, omedy, Drama, Romance and Thriller classes is used.
    \item {\bf IMDB\footnote{\url{{https://www.kaggle.com/karrrimba/movie- metadatacsv}}}}: 
    % This dataset is also a citation network. We use the subset hosted in HAN \cite{wang2019heterogeneous}, but preserve all edges including paper citations and references.
    This dataset also constitutes a citation network. We utilize the subset hosted within HAN, while retaining all edges, encompassing both paper citations and references.
    \item {\bf Freebase~}\cite{bollacker2008freebase}: 
    This dataset constitutes an extensive knowledge graph, extracted as a subgraph from 8 distinct genres: books, films, locations, music, individuals, sports, organizations, and businesses. The subgraph encompasses approximately 1,000,000 edges, following a method aligned with a prior survey \cite{yang2020heterogeneous}.
    % This dataset is a huge knowledge graph. We sample a subgraph of 8 genres (book, film, location, music, person, sport, organization and business) of entities with about 1,000,000 edges following the procedure of a previous survey \cite{yang2020heterogeneous}.
\end{itemize}

% \subsubsection{Link Prediction}
% Link prediction in HGB is formulated as a binary classification problem. The edges in the graph are split according to an 81\% for training, 9\% for validation, and 10\% for testing ratio. Then, the graph is reconstructed based only on the edges in the training set. For negative node pairs in the test set, we first attempted uniform sampling and found that most models could easily make nearly perfect predictions. Finally, we sampled 2-hop neighbors for negative node pairs in a 1:1 ratio with positive node pairs in the test set.
% They are detailed descriptions are listed as follows:
% \begin{itemize}
%     \item {\bf Amazon\footnote{\url{{http://jmcauley.ucsd.edu/data/amazon/}}}}: is an online purchasing platform. We used a preprocessed subset with electronics products from GATNE [5], which includes links representing shared browsing and common purchasing between the products. 
%     \item {\bf LastFM\footnote{\url{{http://ocelma.net/MusicRecommendationDataset/lastfm-360K.html}}}}: is an online music website. We utilized a subset published in HetRec\cite{cantador2011second}, and preprocessed the dataset by filtering out user-tag pairs with only one link.
%     \item {\bf PubMed\footnote{\url{{https://pubmed.ncbi.nlm.nih.gov}}}}: is a biomedical literature database. We employed a subset constructed by HNE\cite{yang2020heterogeneous}.
% \end{itemize}

\begin{table}[!t]
	\centering
	\small
        \setlength{\tabcolsep}{6pt}
	\caption{Characteristics of the studied datasets.}
        \scalebox{.95}{
	\begin{tabularx}{0.5\textwidth}{p{1.3cm}|cXcXc}
		\toprule
		\midrule
		\emph{Node classification}    & \#Nodes.  & \#Node Types.  & \#Edges.  & \#Edge Types.  & \#Classes. \\
		\midrule
		DBLP      & \num{26128}   & \num{4}      & \num{239566}  & 6 & \num{4}  \\
		ACM       & \num{10942}   & \num{4}      & \num{547872}  & 6 & \num{3}  \\
		IMDB      & \num{21420}   & \num{4}      & \num{86642}   & 8 & \num{5}  \\
		Freebase  & \num{180098}  & \num{8}      & \num{1057688} & 36 & \num{7}  \\
  %           \midrule
  %           \emph{Link Prediction} &  &  &  &   & \#Target \\
  %           \midrule
  %           Amazon   & \num{10099} & \num{1}  &\num{148659} & 2  & product-product \\
  %           Last.FM   & \num{20612} & \num{3}  &\num{141521} & 3  & user-artist \\
  %           PubMed   & \num{63109} & \num{4}  &\num{244986} & 10 & disease-disease \\
            \bottomrule
	\end{tabularx}
        }
        \label{tab:dataset}
\end{table}




% \begin{table*}[!t]
% 	\centering
% 	% \small
% 	\caption{Link prediction benchmark. Vacant positions (“-”) are due to lack of meta-paths on those datasets.} 
% 	\setlength{\tabcolsep}{4pt}
% 	\begin{tabular}{c|cc|cc|cc} % |cc
% 		\toprule
% 		\midrule
% 		   & \multicolumn {2}{c|}{Amazon}
% 		   & \multicolumn {2}{c|}{LastFM}
%               & \multicolumn {2}{c}{PubMed} \\
%               % & \multicolumn {2}{|c}{Freebase}\\
              
% 		& AUC(\%)    & MMR(\%)    & AUC(\%)    & MMR(\%) & AUC(\%)    & MMR(\%) \\
% 		\midrule
%     RCGN     & $89.76\pm0.33$
%             & $95.76\pm0.22$
%             & $81.90\pm0.29$
% 		  & $96.68\pm0.14$
%             & $88.32\pm0.08$
%             & $96.89\pm0.20$
% 		\\
%     GATNE     & $96.67\pm0.08$
%             & $98.68\pm0.06$
%             & $87.42\pm0.22$
% 		  & $96.35\pm0.24$
%             & $78.36\pm0.92$
%             & $90.64\pm0.49$
% 		\\
%     HetGNN     & $95.51\pm0.39$
%             & $97.91\pm0.08$
%             & $87.35\pm0.02$
% 		  & $96.15\pm0.18$    
%             & $84.14\pm0.01$
%             & $91.00\pm0.03$
% 		\\
%     MAGNN   & -  
%             & -  
%             & $76.50\pm0.21$
% 		  & $85.68\pm0.04$         
%             & -
%             & -
% 		\\
%     HGT     & $91.70\pm2.31$
%             & $96.07\pm0.68$
%             & $80.49\pm0.78$
% 		  & $95.48\pm0.38$
%             & $90.29\pm0.68$
%             & $97.31\pm0.09$
% 		\\
%     GCN     & $98.57\pm0.21$
%             & $99.77\pm0.02$
%             & $84.71\pm0.1$
% 		  & $96.60\pm0.12$
%             & $86.06\pm1.23$
%             & $98.80\pm0.56$
% 		\\
%     GAT     & $98.45\pm0.11$
%             & $99.61\pm0.22$
%             & $83.55\pm2.11$
% 		  & $91.45\pm5.66    $ 
%             & $87.57\pm1.23$
%             & $98.38\pm0.11$
% 		\\
%      HGB    & $98.74\pm0.25$
%             & $99.52\pm0.08$
%             & $91.04\pm0.22$
% 		  & $99.21\pm0.15  $        
%             & $91.40\pm0.30$
%             & $96.04\pm0.25$
% 		\\
%   \midrule
%   \alg      &   98.76
%             &   99.53
%             &   87.53
% 		  &   98.70       
%             &   89.53
%             &   96.79
%             \\
% 		\bottomrule
% 	\end{tabular}
% 	\label{tab:algorithm}
% \end{table*}

\subsection{Compared Methods} %Experiment Setup} 
\label{sec:protocol}
%
\par\smallskip\noindent
% {\bf Compared Methods.}
%
We take eight graph learning competitors to evaluate the effectiveness and generalizability of our \alg\ approach in various settings.
In the below, we describe their key ideas in a high-level, and discuss why they are chosen to benchmark the experiments.

\begin{itemize}[leftmargin=*]
    \item {\bf GCN}~\cite{kipf2016semi}: It entailed a multi-layer graph convolutional neural network structure, wherein each convolutional layer specifically addresses first-order neighborhood attributes. By layering multiple convolutional tiers, it enables the potential for multi-order neighborhood information propagation to be realized.
    % This involved a multi-layer graph convolutional neural network, where each convolutional layer exclusively handles first-order neighborhood details. By stacking multiple convolutional layers, the potential to attain multi-order neighborhood information transmission is realized.
    % It was a multi-layer graph convolutional neural network, where each convolutional layer processes only first-order neighbourhood information, and by stacking several convolutional layers it is possible to achieve multi-order neighbourhood information transfer.

    \item {\bf GAT}~\cite{velivckovic2017graph}: It was a graph neural network model based on an attention mechanism that assigns different weights to each node, and these weights affect the information transfer of the nodes in the graph.

    \item {\bf GTN}~\cite{yun2019graph}: It was a graph neural network model that leverages meta-path graphs to transform a heterogeneous graph and subsequently employs GCN for learning node embeddings. 
    
    \item {\bf HGT}~\cite{hu2020heterogeneous}: It was to combine the power of attention mechanisms and the Transformer architecture to handle and process complex heterogeneous graph data.

    \item {\bf HAN}~\cite{wang2019heterogeneous}: It was that different types of edges should have different weights, and different neighbouring nodes in the same type of edge have different weights, so it uses node level attention and semantic level attention.

    \item {\bf RGCN}~\cite{schlichtkrull2018modeling}: It considers the influence of different relations on nodes and proposes a method for fusing multiple relations in heterogeneous graphs, addressing the limitation that GCN does not consider node types.

    \item {\bf MAGNN}~\cite{fu2020magnn}: It captures structural and semantic information about the heterogeneous graph from neighbouring nodes and the meta-path contexts between them, and then performs inter-meta-path aggregation using an attention mechanism to fuse potential vectors obtained from multiple meta-paths into the final node embedding.

    \item {\bf Simple-HGN}~\cite{lv2021we}: It was based on the GAT and redesigned with three modules for learnable edge type embedding, residual connectivity, and $L_{2}$ normalization of the output embedding.

    \item {\bf SeHGNN}~\cite{yang2023simple}: It aimed to capture structural insights by pre-computing neighbor aggregation through a lightweight mean aggregator. Furthermore, it expands the receptive scope by implementing a single-layer structure that incorporates extended metapaths.
    % It was to capture structural information by pre-computing neighbor aggregation using a light-weight mean aggregator. It also extends the receptive field by adopting a single-layer structure with long metapaths.

    \item {\bf MH-GCN}~\cite{li2022multi}: It aims to tackle the challenge of capturing heterogeneous structural signals in multiplex networks and demonstrates significant superiority in various network analysis tasks.

\end{itemize}


% \par\smallskip\noindent
% {\bf Evaluation Protocol.}
\subsection{Evaluation Protocol}
%
To perform a fair comparison, the experiments are benchmarked in heterogeneous graph settings.

Micro-F1 and Macro-F1 are metrics used to evaluate classifier performance in multi-category classification problems.
%
% Micro-f1 which Harmonised the sum of precision and recall, namely, 
Micro-F1 is calculated as the harmonic mean of precision and recall over the entire dataset, namely, 
\begin{equation} \label{eq:micro}
    \textnormal{F}_{micro} =\frac{k \times P_{\text {sum }} \times R_{\text {sum }}}{k^2 \times P_{\text {sum }}+R_{\text {sum }}},
\end{equation}
where $P_{\text{sum}}$ and $R_{\text{sum}}$ represent the sum of precision and recall across all categories in the dataset, respectively, while $k$ denotes the total number of categories.

It is consistent with a similar sample size for each category.  
%
% Macro-f1 which Harmonised mean of precision and recall, 
Macro-F1 prioritizes the calculation of precision and recall for each category first, and then takes the average, namely, 
\begin{equation}\label{eq:macro}
    \textnormal{F}_{macro} =\frac{k \times P_{\text {ave}} \times R_{\text {ave}}}{k^2 \times P_{\text {ave }}+R_{\text {ave}}},
\end{equation}
where $P_{\text{ave}}$ and $ R_{\text{ave}} $ denote the average precision and recall per category, respectively.
% It is consistent with a similar sample size for each category. 
% The node classification performance is evaluated by Adjusted Rand index (ARI). And the node clustering performance is evaluated
% by both \emph{Adjusted Rand index} (ARI) and \emph{Normalized Mutual Information} (NMI). The \emph{Adjusted Rand index} (ARI) is computed as follows:
The evaluation of node classification performance employs the Adjusted Rand index (ARI). 
Additionally, the assessment of node clustering performance incorporates both the Adjusted Rand index (ARI) and the Normalized Mutual Information (NMI). 
The computation of the Adjusted Rand index (ARI) is outlined as follows:
\begin{equation}
% ACC\left(C_i^t, C_i^p\right)=\frac{\sum_i^k\left|C_i^t \cap C_i^p\right|}{n}
\textnormal{ARI}=\frac{\textnormal{RI}-E[\textnormal{RI}]}{\max (\textnormal{RI})-E[\textnormal{RI}]},
\end{equation}
\begin{equation}
    \textnormal{RI} =\frac{U + V}{C_2^{n_{\text {samples}}}},
\end{equation}
where $\textnormal{RI}$ is a metric used to assess the similarity between two clustering results, $U$ and $V$ denote the number of pairs of samples that are either in the same category or in different categories in the ground-truth and clustering result, respectively.
% 

\begin{table*}[!t]
	\centering
	% \small
	\caption{ Node classification performance evaluation. "OOM" denotes instances where the models exhausted memory resources. Higher values are indicative of better performance. The most optimal outcomes are highlighted in \textbf{bold}. \emph{w/o} FGL: no edge-to-node mapping or dot product; \emph{w/o} $L_{2}$: no $L_{2}$ normalization for node mapping; \emph{w/o} non-linear encoding (NLE): only linear transformation for nodes; \emph{w/o} edge information (EI): random edge initialization instead of type-based encoding.}
	\setlength{\tabcolsep}{4pt}
        \scalebox{.95}{
	\begin{tabular}{c|cc|cc|cc|cc}
		\toprule
		\midrule
		   & \multicolumn {2}{c|}{DBLP}
		   & \multicolumn {2}{c|}{ACM}
              & \multicolumn {2}{c|}{IMDB}
              & \multicolumn {2}{c}{Freebase}\\
		
		& micro-f1   & macro-f1   & micro-f1     & macro-f1 
            & micro-f1   & macro-f1   & micro-f1     & macro-f1\\
		\midrule
		% \\
       % $\mathbf{93.57 \pm 0.33}$\textcolor{white}{$\bullet$}    
         GCN       & $90.33 \pm 0.36$    
            & $89.68 \pm 0.43$    
            & $92.28 \pm 0.41$
		  & $92.30 \pm 0.43$                
            & $63.81 \pm 0.74$  
            & $56.32 \pm 1.51$
            & $59.51 \pm 0.34$
            & $31.01 \pm 0.66$
		\\
  GAT       & $92.26 \pm 0.64$    
            & $91.66 \pm 0.65$    
            & $91.01 \pm 1.28$
		  & $91.03 \pm 1.26$                
            & $65.32 \pm 1.08$  
            & $59.53 \pm 2.26$
            & $62.37 \pm 0.42$
            & $40.85 \pm 0.72$
		\\
    % \midrule
    GTN     & $93.97 \pm 0.54$    
            & $93.52 \pm 0.55$    
            & $91.20 \pm 0.71$
		  & $91.31 \pm 0.70$                
            & $65.14 \pm 0.45$  
            & $60.47 \pm 0.98$
            & OOM
            & OOM
		\\
    HGT     & $93.49 \pm 0.25$    
            & $93.01 \pm 0.23$    
            & $91.00 \pm 0.76$
		  & $91.12 \pm 0.76$                
            & $67.20 \pm 0.57$  
            & $63.00 \pm 1.19$
            & $60.51 \pm 1.16$
            & $29.28 \pm 2.52$
		\\
		HAN & $93.57 \pm 0.33$    
            & $93.09 \pm 0.34$    
            & $88.11 \pm 0.93$
		  & $88.06 \pm 1.02$                
            & $65.06 \pm 1.20$  
            & $58.54 \pm 2.39$
            & $56.26 \pm 0.87$
            & $21.66 \pm 2.29$
		\\
        RGCN & $92.15 \pm 1.22$    
            & $91.66 \pm 1.30$    
            & $90.54 \pm 0.89$
		  & $90.65 \pm 0.86$                
            & $61.73 \pm 0.94$  
            & $51.37 \pm 2.35$
            & $58.46 \pm 1.46$
            & $44.11 \pm 1.05$
		\\
  MAGNN     & $93.35 \pm 0.59$    
            & $92.85 \pm 0.66$    
            & $91.44 \pm 0.67$
		  & $91.50 \pm 0.68$                
            & $65.32 \pm 0.66$  
            & $57.22 \pm 1.59$
            & OOM
            & OOM
		\\
  
  MH-GCN    & $94.18 \pm 0.47$    
            & $93.84 \pm 0.48$    
            & OOM
		  & OOM
            & $60.11 \pm 1.32$  
            & $58.57 \pm 1.27$
            & OOM
            & OOM
		\\
  % SR-HGN    & $95.59 \pm 0.13$    
  %           & $95.29 \pm 0.15$    
  %           & $94.58 \pm 0.21$
  %           & $94.62 \pm 0.21$                
  %           & $67.67 \pm 0.22$  
  %           & $66.16 \pm 0.19$
  %           & $65.08 \pm 0.92$
  %           & $52.00 \pm 1.64$
		% \\
    Simple-HGN
            & $94.10 \pm 0.41$    
            & $93.65 \pm 0.42$    
            & $93.31 \pm 0.36$
		    & $93.40 \pm 0.36$                
            & $67.04 \pm 0.38$  
            & $64.14 \pm 0.49$
            & $\underline{65.52 \pm 0.55}$
            & $45.26 \pm 2.24$

		\\
  SeHGNN    & $\underline{95.42 \pm 0.17}$    
            & $\underline{95.06 \pm 0.17}$    
            & $\underline{93.98 \pm 0.36}$
		    & $\underline{94.05 \pm 0.35}$                
            & $\underline{69.17 \pm 0.43}$  
            & $\underline{67.11\pm0.25}$
            & $65.08 \pm 0.66$
            & $\underline{51.87 \pm 0.86}$
		\\
  \midrule
  \emph{w/o} FGL    
            & $92.82\pm0.69$ 
            & $92.30\pm0.73$  % \bullet
            & $92.51\pm0.21$
		    & $92.55\pm0.23$                
            & $67.47\pm0.54$  
            & $61.13\pm1.77$
            & $68.32\pm0.59$
            & $51.74\pm0.74$
		\\
    \emph{w/o} $L_{2}$    
            & $95.52\pm0.35$   
            & $95.18\pm0.38$    
            & $93.08\pm0.48$
		    & $93.10\pm0.47$                
            & $68.32\pm0.58$  
            & $63.13\pm1.15$
            & $67.19\pm0.61$
            & $47.03\pm2.49$
		\\
    \emph{w/o} NLE    
            & $95.56\pm0.12$
            & $95.20\pm0.11$
            & $93.83\pm0.29$
		    & $93.88\pm0.30$                
            & $68.06\pm0.42$  
            & $62.25\pm0.09$
            & $68.49\pm0.15$
            & $52.05\pm0.85$
		\\
    \emph{w/o} EI   
            & $92.63\pm0.48$
            & $92.12\pm0.52$
            & $91.82\pm0.76$
		    & $91.83\pm0.79$                
            & $65.31\pm1.13$  
            & $58.41\pm1.85$
            & $68.02\pm0.26$
            & $51.11\pm0.99$
		\\
  \midrule
    \alg      & $\mathbf{95.98\pm0.01} $
            & $\mathbf{95.68\pm0.01} $    
            & $\mathbf{94.42\pm0.01} $
		    & $\mathbf{94.47\pm0.01} $  
            & $\mathbf{69.36\pm0.04} $  
            & $\mathbf{67.29\pm0.02} $
            & $\mathbf{69.04\pm0.03} $
            & $\mathbf{53.18\pm0.09} $ \\
        \midrule
    \emph{p}-value
            & 0.012
            & 0.014
            & 0.018
            & 0.020
            & 0.025
            & 0.028
            & 0.015
            & 0.022 \\
    \emph{Improve}(\%)     
            & 0.59
            & 0.65
            & 0.47
		    & 0.45
            & 0.27
            & 0.27
            & 5.38
            & 2.53 \\
		\bottomrule
	\end{tabular}
        }
	\label{tab:algorithm}
\end{table*}

$C_i^t$ is the ground-truth label of each node, and $C_i^p$ is the label predicted by node embedding. And Normalized Mutual Information (NMI) is computed by:
\begin{equation}
\textnormal{NMI}\left(C_i^t, C_i^p\right)=\frac{\textnormal{I}\left(C_i^t, C_i^p\right)}{\left[\textnormal{H}\left(C_i^t\right)+\textnormal{H}\left(C_i^p\right)\right] / 2},
\end{equation}
where $\textnormal{I}(\cdot)$ and $\textnormal{H}(\cdot)$ denote the mutual information and entropy functions. 
The value of NMI ranges from 0 to 1, where 1 indicates perfect clustering results, and 0 signifies that the clustering outcome is indistinguishable from random assignment.
Their calculations are outlined as follows:
% where $I(\cdot), H(\cdot)$ denote the mutual information and entropy functions. They are respectively calculated as follows:
\begin{equation}
\textnormal{I}\left(C_i^t, C_i^p\right)=\sum_i \sum_j \frac{\left|C_i^t \cap C_i^p\right|}{n} \log \frac{N \cdot\left|C_i^t \cap C_i^p\right|}{\left|C_i^t\right|\left|C_i^p\right|} \\,
\end{equation}
\begin{equation}
    \textnormal{H}(C)=-\sum_i \frac{C_i}{N} \log \frac{C_i}{N}.
\end{equation}

Both of ACC and NMI can represent the correlation between the predicted label and the ground-truth label. 
%
And the higher value of them indicates the better performance of node clustering.
%
All the experiments are run on a computer server that has a 2.30 GHz Intel GlodXeon GD 5218 with 16 cores, and Quadro RTX6000 24GB * 1.


\subsection{Node Classification (\textbf{RQ1})} \label{sec:node}
We compare the performance of our model on the node classification assignment to that of current best practices. The outcomes are presented in Tables \ref{tab:algorithm} and \ref{tab:algorithm2}, with the finest highlighted in bold.The baseline includes homogeneous graph methods, heterogeneous graph methods, and graph representation learning methods.

As we can see, \alg\  exhibits outstanding efficiency on all tested networks. 
%
Regarding micro-F1 and macro-F1, our \alg\ outperforms the state-of-the-art HGNN model SeHGNN by an average of 1.51\% and 0.64\%, respectively, across all datasets. 
%
Given that the performance gain in node classification tasks described in some recent publications is typically around 0.5-1.0\%, our \alg's performance boost is noteworthy. 
%
Furthermore, we find that \alg\ outperforms rival approaches on large heterogeneous networks with multi-typed nodes (\emph{e.g.}, Freebase), attaining 5.3\% and 0.91\% improvement in micro-F1 and macro-F1 on the Freebase network, respectively. 
%
% The possible reason is that our \alg\  effectively learns node representations for classification by exploring the interactions between multiple relations with different importance (i.e., weights), which is ignored by heterogeneous network embedding methods based on scalar representation learning.
The potential explanation lies in the effectiveness of our \alg\ in acquiring node representations tailored for classification through the exploration of interactions among various relations carrying distinct levels of significance (\emph{i.e.}, weights). 
This aspect is often disregarded by heterogeneous network embedding techniques centered on scalar representation learning.

% \subsection{Link Prediction (\textbf{RQ2})} \label{sec:link}

\subsection{Node Clustering and Visualization (\textbf{RQ2})} \label{sec:link}
\subsubsection{Node Clustering}
In the clustering task, the output of the penultimate layer is preserved, which will be fed into the downstream clustering model. 
We set the number of clusters according to the number of labels, and the k-means algorithm is used as the downstream clustering method. 
For reducing the impact of the ground-truth information on the unsupervised learning task, we only use 3\% labels of each dataset to train the full-supervised model.

\begin{table}[!t]
	\centering
	\small
	\caption{Comparison Results in terms of Precision (\%) and Mean Average Precision (MAP) (\%) in the node clustering task on the four datasets. The most optimal results are highlighted in bold and the second optimal results is \underline{underlined}. "OOM" denotes instances where the models exhausted memory resources.} 
	\setlength{\tabcolsep}{4pt}
    \scalebox{1}{
	\begin{tabular}{c|cc|cc}
		\toprule
		\midrule
		   & \multicolumn {2}{c|}{DBLP}
		   & \multicolumn {2}{c}{ACM}\\
              
		& Precision(\%)    & MAP(\%)    & Precision(\%)    & MAP(\%)\\
		\midrule
    GCN     &   90.6338
            &   84.4697
            &   92.7289
		    &   88.5327
		\\
    GAT     &   93.3450
            &   88.8479
            &   92.4457
		    &   88.1196
		\\
    HAN     &  93.7676
            &  89.5615
            &  89.2823
		    &  83.4253
		\\
    RGCN    &  92.3239
            &  87.1350
            &  89.3295
		    &  83.8275
		\\
    MAGNN   &  93.1690
            &  88.4674
            &  91.9735
		    &  87.4646
		\\
    MH-GCN  &  94.0845
            &  90.0175
            &  OOM
		    &  OOM
		\\
 Simple-HGN &  94.7887
            &  91.2015
            &  93.2483
		    &  89.3293
		\\
  SeHGNN    &  \underline{95.6338}
            &  \underline{92.7124}
            &  \underline{94.5231}
		    &  \underline{91.2279}
		\\
  \midrule
  \alg      &   $\mathbf{95.9485}$
            &   $\mathbf{93.0509}$
            &   $\mathbf{94.5103}$
	        &   $\mathbf{94.4287}$
            \\
		\bottomrule
    & \multicolumn {2}{c|}{IMDB}
    & \multicolumn {2}{c}{Freebase}\\
              
    & Precision(\%)    & MAP(\%)    & Precision(\%)    & MAP(\%)\\
		\midrule
    GCN     &  45.5028
            &  55.4675
            &  59.4827
		    &  44.3824
		\\
    GAT     &  48.5634
            &  57.3882
            &  62.8592
		    &  49.2971
		\\
    HAN     &  78.3135
            &  57.0063
            &  57.4712
	        &  39.9496  
		\\
    RGCN    &  \underline{76.5646}
            &  54.8855
            &  58.6566
		    &  46.0053
		\\
    MAGNN   &  47.5015
            &  56.8387
            &  OOM
		    &  OOM
		\\
    MH-GCN  &  61.1570
            &  51.5787
            &  OOM
		    &  OOM
		\\
 Simple-HGN &  50.4060
            &  58.0322
            &  \underline{65.8944}
		    &  \underline{51.3584}
		\\
  SeHGNN    &  75.1530
            &  \underline{58.9580}
            &  64.1882
		    &  49.6740
		\\
  \midrule
  \alg      &   \textbf{78.2957}
            &   \textbf{59.4462}
            &   \textbf{68.2298}
		    &   \textbf{55.3684}    
            \\
		\bottomrule
	\end{tabular}
        }
	\label{tab:algorithm2}
\end{table}


As shown in Table \ref{tab:nodeclustering}, Models based on meta-path or graph attention mechanisms perform relatively poorly on DBLP, ACM and Freebase datasets, especially with respect to NMI values.
In attention-based models, the interactions between nodes are achieved through attention weights. However, excessive reliance on neighboring node information may lead to issues of information leakage and over-smoothing.
%
Path-based models may encounter two challenges in node clustering. 
%
Firstly, inappropriate selection of meta-paths can result in insufficient reflection of relationships between nodes, thus impacting the accuracy and expressive power of node aggregation. 
%
Secondly, different meta-paths may encompass diverse association information, leading to difficulties in effectively integrating their contributions during aggregation.

\begin{table*}[!t]
	\centering
	% \small
	\caption{Comparison Results in trems of ARI(\%) and NMI(\%) in the {\bf Node Clustering} task on the four datasets. The best result in trems of each evaluation measure is highlighted in bold.} 
	\setlength{\tabcolsep}{4pt}
        \scalebox{1}{
    	\begin{tabular}{c|cc|cc|cc|cc}
    		\toprule
    		\midrule
    		& \multicolumn {2}{c|}{DBLP}
    		  & \multicolumn {2}{c|}{ACM}
                & \multicolumn {2}{c|}{IMDB}
                & \multicolumn {2}{c}{Freebase} \\
                
    		& ARI(\%)    & NMI(\%)    & ARI(\%)    & NMI(\%)
                & ARI(\%)    & NMI(\%)    & ARI(\%)    & NMI(\%) \\
    		\midrule
        GCN     & $61.15 \pm 2.67$  
                & $55.55 \pm 2.05$    
                & $65.59 \pm 1.20$ 
                & $62.33 \pm 0.82$
    		    & $5.83 \pm 0.84$          
                & $8.55 \pm 0.44$  
                & $4.10 \pm 1.01$
                & $10.15 \pm 0.76$
    		\\
        GAT     & $53.70 \pm 3.54$
                & $52.26 \pm 2.46$    
                & $34.33 \pm 3.40$
                & $44.87 \pm 1.76$
                & $8.54 \pm 0.80$               
                & $11.09 \pm 0.81$  
                & $9.75 \pm 1.78$
                & $18.94 \pm 2.20$
    		\\
        HGT     & $74.86 \pm 1.48$
                & $69.19 \pm 1.77$    
                & $67.34 \pm 5.99$     
                & $63.93 \pm 3.53$
                & $3.20 \pm 2.33$              
                & $4.17 \pm 3.04$  
                & $4.52 \pm 2.80$
                & $8.06 \pm 1.12$
    		\\
        HAN     &  $76.71 \pm 1.25$
                &  $70.12 \pm 1.14$
                &  $43.50 \pm 1.28$
    		    &  $48.09 \pm 2.36$
                &  $4.18  \pm 0.34$
                &  $5.60  \pm 0.39$
                &  $4.02  \pm 1.09$
                &  $7.03  \pm 0.29$
    		\\
        RGCN    &  $0.28 \pm 0.18$
                &  $0.50 \pm 0.09$
                &  $23.17 \pm 0.74$
    		    &  $30.65 \pm 3.38$
                &  $0.20 \pm 0.00$
                &  $0.52 \pm 0.00$
                &  $5.77 \pm 1.24$
                &  $6.13 \pm 0.07$
    		\\
      MAGNN     &  $\underline{83.47 \pm 1.57}$
                &  $\underline{77.51 \pm 1.87}$
                &  $65.71 \pm 4.12$
                &  $62.13 \pm 4.16$
    		    &  $7.79 \pm 1.59$
                &  $9.13 \pm 0.84$
                &  OOM
                &  OOM
    		\\
      MH-GCN    &  $0.31 \pm 0.54$
                &  $2.42 \pm 1.17$
                &  OOM
                &  OOM
    		    &  $\underline{10.16 \pm 8.60}$
                &  $9.37 \pm 7.68$
                &  OOM
                &  OOM
    		\\
      Simple-HGN     
                &  $67.13 \pm 9.92$
                &  $64.32 \pm 5.19$
                &  $74.57 \pm 2.17$
                &  $70.20 \pm 1.82$
    		    &  $8.72 \pm 0.53$
                &  $\underline{11.42 \pm 0.57}$
                &  $13.30 \pm 1.64$
                &  $\underline{21.05 \pm 2.48}$
    		\\
      SeHGNN    &   $82.61 \pm 0.88$
                &   $77.20 \pm 0.75$
                &   $\underline{77.29 \pm 1.45}$
    		    &   $\underline{71.87 \pm 1.58}$
                &   $8.33  \pm 0.57$
                &   $10.31 \pm 0.72$
                &   $\underline{13.70 \pm 0.95}$
                &   $17.58 \pm 0.52$
    		\\
      \midrule
      \alg      & $\mathbf{84.46\pm1.36} $
                & $\mathbf{79.73\pm1.41} $
                & $\mathbf{77.70\pm0.07} $    
                & $\mathbf{72.42\pm0.88} $
    		    & $\mathbf{10.80\pm1.34} $
                & $\mathbf{12.39\pm0.77} $  
                & $\mathbf{14.81\pm0.84} $
                & $\mathbf{22.71\pm1.67} $ \\
        \midrule
        \emph{p}-value
            & 0.014
            & 0.018
            & 0.020
            & 0.022
            & 0.015
            & 0.018
            & 0.012
            & 0.016 \\
        \emph{Improve}(\%)     
            & 1.19
            & 2.86
            & 0.53
		    & 0.77
            & 6.30
            & 8.49
            & 8.11
            & 7.88 \\
    		\bottomrule
    	\end{tabular}
        }
	\label{tab:nodeclustering}
\end{table*}

\begin{figure*}[!t]
    \centering
    \begin{flushleft}
	\begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/HAN_ACM.pdf}
		\caption{HAN}
		% \label{fig:Micro_F1}
	\end{subfigure}
         \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/HGB_ACM.pdf}
		\caption{Simple-HGN}
	\end{subfigure}
	\begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/HGT_ACM.pdf}
		\caption{HGT}
		% \label{fig:Macro_F1}
	\end{subfigure}
        \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/MAGNN_ACM.pdf}
		\caption{MAGNN}
	\end{subfigure}
        \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/SeHGNN_ACM.pdf}
		\caption{SeHGNN}
	\end{subfigure}
        \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/cluster_ACM.pdf}
		\caption{\alg}
	\end{subfigure}
	\caption{Visualization of embedding on ACM. Nodes with different labels are differentiated by colors.}
	\label{fig:ablation}
    \end{flushleft}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \begin{flushleft}
	\begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/HAN_DBLP.pdf}
		\caption{HAN}
	\end{subfigure}
         \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/HGB_DBLP.pdf}
		\caption{Simple-HGN}
	\end{subfigure}
	\begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/HGT_DBLP.pdf}
		\caption{HGT}
	\end{subfigure}
        \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/MAGNN_DBLP.pdf}
		\caption{MAGNN}
	\end{subfigure}
        \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/SeHGNN_DBLP.pdf}
		\caption{SeHGNN}
	\end{subfigure}
        \begin{subfigure}[t]{0.16\linewidth}
		\includegraphics[width=\textwidth]{visual/cluster_DBLP.pdf}
		\caption{\alg}
	\end{subfigure}
	\caption{Visualization of embedding on DBLP. Nodes with different labels are differentiated by colors.}
	\label{fig:ablation2}
    \end{flushleft}
\end{figure*}

\subsubsection{Visualization} 
For a more intuitive comparison, we adopt T-SNE \cite{liao2018attributed} to obtain 2-dimensional node representations, and each node will be colored differently according to the ground-truth label.
%
For limited space, we choose to display the visualization results of ACM and DBLP instead of all datasets.
%
From Figs. \ref{fig:ablation} and \ref{fig:ablation2}, we can find \alg~provides the best visualization performance. 
The visualization result of HAN is a little bit messy. 
%
In the method, attention mechanisms are used to determine the importance of nodes and meta-paths. 
%
However, at higher levels, the attention weights may become more averaged, blurring the differences between nodes. 

\begin{figure}[!h]
    \centering
    \begin{flushleft}
	\begin{subfigure}[t]{0.95\linewidth}
		\includegraphics[width=\textwidth]{Fig/Micro.pdf}
		\caption{Micro-F1 score}
		% \label{fig:Micro_F1}
	\end{subfigure}
	\begin{subfigure}[t]{0.95\linewidth}
		\includegraphics[width=\textwidth]{Fig/Macro.pdf}
		\caption{Macro-F1 score}
		% \label{fig:Macro_F1}
	\end{subfigure}
	\caption{Experimental results of ablation study}
	\label{fig:ablation33}
    \end{flushleft}
\end{figure}

As a result, the distribution and clustering of nodes in the visualization are affected, leading to less accurate node representations.
% 
Simple-HGN is a graph-based model that generates node embeddings in the penultimate layer, which may suffer from poor visualization due to high dimensionality, insufficient feature richness, and limited learning capacity in capturing complex relationships. 
Addressing these issues through dimensionality reduction, feature enrichment, and model enhancement can improve the visualization quality.
%
As we can see, for HGT, MAGNN and SeHGNN, node representations with different labels are more distinct from each other, while those having the same label are more compact, which to a large extent demonstrates that the attention-based or mate-path-based models can integrate the network structure and node attribute information more rationally.
%
Specifically, \alg~can learn the best node distribution, which can distinctly separate nodes with different labels and thus provide convenience for both node classification and node clustering tasks.

\begin{figure*}[!h]
    \centering
    \begin{flushleft}
	\begin{subfigure}[t]{0.32\linewidth}
		\includegraphics[width=\textwidth]{Fig/layer_micro.pdf}
		\caption{Micro-F1 score \emph{w.r.t.} \#layers}
		\label{fig:layers}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\linewidth}
		\includegraphics[width=\textwidth]{Fig/dim_micro.pdf}
		\caption{Micro-F1 score \emph{w.r.t.} \#dimensions}
		\label{fig:dim}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\linewidth}
		\includegraphics[width=\textwidth]{Fig/epoch_micro.pdf}
		\caption{Micro-F1 score \emph{w.r.t.} \#rounds}
		\label{fig:rounds}
	\end{subfigure}
	\caption{
            % Micro-F1 result of parameter sensitivity of proposed method w.r.t. \#layers, \#dimension, and \#rounds.
            Micro-F1 outcomes depicting the parameter sensitivity of the proposed approach concerning the number of layers, dimensions, and training rounds.
            }
	\label{fig:micro-F11}
    \end{flushleft}
\end{figure*}

\begin{figure*}[!h]
    \centering
    \begin{flushleft}
	\begin{subfigure}[t]{0.32\linewidth}
		\includegraphics[width=\textwidth]{Fig/layer_macro.pdf}
		\caption{Macro-F1 score \emph{w.r.t.} \#layers}
		\label{fig:layers2}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\linewidth}
		\includegraphics[width=\textwidth]{Fig/dim_macro.pdf}
		\caption{Macro-F1 score \emph{w.r.t.} \#dimensions}
		\label{fig:dim2}
	\end{subfigure}
	\begin{subfigure}[t]{0.32\linewidth}
		\includegraphics[width=\textwidth]{Fig/epoch_macro.pdf}
		\caption{Macro-F1 score \emph{w.r.t.} \#rounds}
		\label{fig:rounds2}
	\end{subfigure}
	\caption{
        % Macro-F1 result of parameter sensitivity of proposed method w.r.t. \#layers, \#dimension, and \#rounds.
        Macro-F1 outcomes depicting the parameter sensitivity of the proposed approach concerning the number of layers, dimensions, and training rounds.
        }
	\label{fig:micro-f2}
    \end{flushleft}
\end{figure*}

\subsection{Ablation Study (\textbf{RQ3})} \label{sec:ablation}
% In order to verify the efficacy of each constituent within our model, we proceed to carry out experiments involving various iterations of HGFPN.
To substantiate the effectiveness of each element within our model, we proceed to conduct experiments involving different iterations of the \alg~framework.
% To validate the effectiveness of each component of our model, we further conduct experiments on different HGFPN variations. 
\begin{itemize}[leftmargin=*]
    \item \emph{w/o} FGL: without mapping the edge feature representations to the node feature space and performing the dot product in the same feature space; 
    \item \emph{w/o} $L_{2}$: without considering $L_{2}$ normalization of the output by encoding the final stage of uniform mapping of nodes; 
    \item \emph{w/o} non-linear encoding (NLE): not considering a non-linear encoding mapping of nodes, as it only employs a fully connected layer for linear transformation; 
    \item \emph{w/o} edge information (EI): use random initialization to encode edges instead of using initialized encoding based on different edge types;
\end{itemize}
% As we see that Fig. \ref{fig:ablation33} and Table \ref{tab:algorithm}, The \emph{w/o} FGL approach directly performs message passing by multiplying the initialized edge features of different types with node features, without mapping the edge feature representations to the node feature space and performing dot product in the same feature space;  
% \emph{w/o} $L_{2}$ method does not consider $L_{2}$ normalization of the output by encoding the final stage of uniform mapping of nodes;  
% \emph{w/o} NLE approach does not consider a non-linear encoding mapping of nodes, as it only employs a fully connected layer for linear transformation; 
% \emph{w/o} EI is to use random initialization to encode edges instead of using initialized encoding based on different edge types;
% In Figure 3, we present the results of an ablation study conducted on four datasets for node classification, where the performance on IMDB and Freebase corresponds to the right-hand axis.
In Fig.~\ref{fig:ablation33}, we present the outcomes obtained through a comprehensive breakdown analysis conducted on four distinct datasets, focusing on the task of node classification. 
The performance metrics for IMDB and Freebase are aligned with the right-hand axis for reference.

The results demonstrate that both essential components contribute to the performance enhancement of our. 
In particular, \emph{w/o} FGL and \emph{w/o} EI have a substantial effect on the effectiveness of our model~\alg. 
The comparisons between \emph{w/o} FGL and \alg~highlight the effectiveness of edge-unified mapping encoding. 
%
We observe that \emph{w/o} FGL performs worse than \alg~in terms of macro-F1 and micro-F1, showing a decrease of 3.53\% in macro-F1 score on the DBLP dataset. 
%
This indicates that the designed unified encoding module effectively captures the crucial role of different relationship importance aspects in graph representation learning. 
%
\alg~outperforms \emph{w/o} FGL by 3.66\% and 1.16\% in terms of macro-F1 on DBLP and Freebase, respectively. 
It is possible that edge features and node features are fundamentally heterogeneous, and if message passing is performed directly, the edge features, serving as the channels for message passing, cannot effectively scale and learn useful features from neighboring nodes because they exist in different data spaces.
%
The comparisons between \emph{w/o} $L_{2}$ and \alg~highlight the significance of $L_{2}$ normalization on the output embedding. 




Compared to \emph{w/o} NLE, \alg~improves by 0.53\% and 4.11\% in terms of micro-F1 on ACM and IMDB, respectively. 
%
Compared to \emph{w/o} EI, \alg~achieves an improvement of 3.86\% and 2.79\% in terms of macro-F1 on DBLP and ACM, respectively. 
%
For the initialization of the edge feature encoding, a random initialization was used instead of a type-based initialization for \emph{w/o} EI. 
This may lead to insufficient utilization of the type information of edges in the dataset during message passing, resulting in different types of edges receiving approximately distributed feature preferences. 
This can introduce noise when aggregating nodes, as unnecessary information is transmitted. 
For example, in a citation network, there is a citation relationship between papers, while there is a writing relationship between authors and papers. 
If paper obtains information about neighboring papers through encoding based on the writing action type, redundant feature information will inevitably be obtained.


\subsection{Time Analysis and Parameter Sensitivity (\textbf{RQ4})} \label{sec:time}
{\bf Time Analysis.}
We evaluated the time and memory requirements of all accessible algorithms for node classification on the DBLP dataset. 
As observed in Fig. \ref{fig:time}, our proposed EdgeFGL model stands out in terms of Micro-f1 score, achieving an impressive score close to 95.5, significantly outperforming all other compared models. 
This demonstrates that our EdgeFGL can provide higher accuracy and more reliable results. 
The SeHGNN and HAN models exhibit a balanced performance in terms of micro-f1 score and training time, with high scores and low memory consumption. 
The RGCN, MAGNN, and HGB models achieve moderate micro-f1 scores, while the GCN and GAT models have relatively lower scores but require less training time and memory. 
Overall, our EdgeFGL effectively balances time and memory costs to achieve superior model performance. 

% We evaluate the time and memory demands of all accessible \alg\ for the purpose of node classification on the DBLP dataset.
% Fig. \ref{fig:time} demonstrates the outcomes. 
% Notably, we exclusively measure the time needed for a single epoch for each model. However, the number of epochs necessary to achieve convergence can vary and might be challenging to anticipate.

{\bf Parameter Sensitivity.}
% We investigate the sensitivity of \alg\ with regard to the important parameters, such as the number of layers $l$, embedding dimension $d$, and the number of training rounds. 
We explore the \alg~sensitivity in relation to crucial parameters, including the number of layers $l$, embedding dimension $d$, and the quantity of training iterations.
%
Fig. \ref{fig:micro-F11} displays the macro-F1 score for node classification with various parameter settings across four datasets. 
%
Observe that the performance on FreeBase and IMDB corresponds to the rightmost ordinate.
%
As shown in Fig. \ref{fig:layers}, the performance of \alg~increases as $l$ increases, and then starts to decrease or level off when $l \geq $ 2. 
As the number of GCN layers increases, the representation of the nodes flattens out after multiple convolutions, resulting in a decrease in performance.
%
The results in Fig. \ref{fig:dim} show that the performance of \alg~gradually increases and then slightly decreases as the dimension $d$ increases, with DBLP and ACM reaching the best performance at embedding dimension $d$ = 128. 
%
% This is because when the dimension $d$ is small, the features of all nodes are compressed into a small embedding space, and therefore it is difficult to retain the feature proximity of all node pairs. 
This phenomenon arises due to the fact that when the dimension $d$ is small, the features of all nodes become compressed into a limited embedding space. Consequently, maintaining the feature proximity of all pairs of nodes becomes challenging.
%
Conversely, a higher dimensionality also tends to reduce the differentiation among all node embeddings. 
%larger dimensionality also flattens the distance between all node embeddings.
%
Figs. \ref{fig:rounds} and \ref{fig:rounds2} showcases the performance trend of our \alg~concerning the number of training rounds required for learning the model weights. 
It is evident that our \alg~can rapidly converge and consistently attain stable performance within 60 rounds across all evaluated datasets. 
% Fig. \ref{fig:rounds} illustrates the performance of our HGFPN in terms of the number of training rounds to learn the model weights. Our HGFPN can converge quickly and effectively achieve stable performance within 60 rounds on all tested datasets.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.45\textwidth]{Fig/Time-3.pdf}
    \caption{
    Contrast in time and memory consumption among HGNNs on the DBLP dataset. Circle sizes symbolize the (proportional) memory usage of the respective models.
    % Comparison of time and memory for HGNNs on DBLP. The area of the circles represents the (relative) amount of memory utilized by the corresponding models.
    }
    \label{fig:time}
\end{figure}



