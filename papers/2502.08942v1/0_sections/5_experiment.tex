\section{Experiment}

\input{tables/table_main_exp}

In this section, we empirically validate the effectiveness of the proposed TaTS framework through extensive experiments conducted on multiple datasets using a variety of existing time series models. Specifically, we demonstrate that TaTS can be seamlessly integrated with existing time series models to effectively handle time series with paired texts. In particular, we use GPT2 \cite{radford2019language} encoder to embed the paired texts. We also validate the performance of TaTS with other text encoders across different datasets in section \ref{subsec: ablation} and Appendix \ref{ap: full ablation of text encoder}.

\textbf{Datasets.} We evaluate our framework on 9 real-world datasets \cite{DBLP:journals/corr/abs-2406-08627}, spanning diverse domains such as environmental sciences, social systems, and infrastructure. The datasets have sample frequencies ranging from daily to weekly and monthly. Further details about the datasets can be found in Appendix \ref{ap: dataset details}.

\textbf{Time Series Models and Baselines.} To demonstrate the compatibility of TaTS with existing time series models, we integrate TaTS with 9 widely used models across different categories, including (i) Transformer-based models: iTransformer \cite{itransformer}, PatchTST \cite{patchtst}, Crossformer \cite{crossformer},   Autoformer \cite{autoformer}, Informer \cite{informer} and Transformer \cite{transformer}.
(ii) Linear models: DLinear \cite{dlinear}. (iii) Frequency-based models: FEDformer \cite{fedformer}, FiLM \cite{film}.

For each time series model, we compare our TaTS framework against two baselines: (i) Numerical-only uni-modal modeling, which ignores the paired texts and utilizes only the numerical time series with the given time series model. (ii) \multi \, \cite{DBLP:journals/corr/abs-2406-08627}, a recently proposed library for multimodal time series forecasting that performs a linear interpolation between the output of the time series model and the text embeddings, treating the texts as a bag-of-words.

\textbf{Metrics.} We evaluate the performance of multimodal time series modeling using MSE, MAE, RMSE, MAPE, and MSPE. Due to space constraints, we report only the MSE and MAE results in the main paper, while the results for the other metrics are provided in Appendix \ref{ap: full results}.



\subsection{Main Results}

\textbf{Settings.} Regardless of whether numerical-only models, MM-TSFLib, or TaTS are used, all models are trained to minimize the MSE loss as defined in Equation \ref{eq: training mse loss}. Implementation details are provided in Appendix \ref{ap: implementation details}.

Table \ref{tab: main forecasting} presents the performance results for the time series forecasting task. For each dataset and time series model, we report the average performance across four different prediction lengths, and the full results for each prediction length are provided in Appendix \ref{ap: full results}. For a dataset with relatively few samples, such as Economy, we perform short-term forecasting with prediction lengths of $\{6, 8, 10, 12\}$. In contrast, for a dataset with a larger number of samples, such as Environment, we perform long-term forecasting with prediction lengths of $\{48, 96, 192, 336\}$. 
From the results, compared to uni-modal modeling using only numerical values or the state-of-the-art baseline MM-TSFLib, our TaTS consistently achieves the best performance across all datasets. Notably, by plugging in TaTS to various existing time series models, it achieves an average performance improvement of over $5\%$ on 6 out of 9 datasets and delivers a remarkable performance boost of over $30\%$ on the Environment dataset, which contains the largest number of samples among all datasets. The results also demonstrate that TaTS is highly compatible with a wide range of existing time series forecasting models, including transformer-based models, linear models, and frequency-based models, consistently delivering performance improvements across all of them in both long-term forecasting and short-term forcasting tasks. We provide a visualization of the performance boost in Appendix \ref{ap: full radar}, showcasing the improvements achieved by different time series models on each dataset.

\input{tables/table_imputation_main}


We also evaluate the performance of our TaTS framework on the imputation task using the Climate, Economy, and Traffic datasets, each with an imputation length of 24. Since the original MM-TSFLib only supports forecasting tasks, we extend it by applying a similar linear interpolation approach to serve as a baseline. For this evaluation, we select one representative time series model from each category and present the results in Table \ref{tab: main imputation}. The results demonstrate that TaTS consistently enhances the imputation capabilities of existing time series models, achieving performance improvements of up to $30\%$ compared to baseline methods.

The above results show that leveraging the text modality provides significant benefits over numerical-only unimodal modeling when paired texts are available. Inspired by chronological textual resonance, our proposed TaTS framework effectively captures the temporal properties encoded in paired texts, achieving notable improvements over baseline methods that disregard the positional information inherent in the texts paired with time series.


\begin{figure*}[t]
\centering
\subfigure[]{
\includegraphics[width=0.23\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_SocialGood.png}
}
\subfigure[]{
\includegraphics[width=0.23\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Energy.png}
}
\subfigure[]{
\raisebox{1.8mm}{
\includegraphics[width=0.23\textwidth]{figures/llm_ablation_plots/ablation_study_Environment.png}
}
}
\subfigure[]{
\raisebox{2.3mm}{
\includegraphics[width=0.23\textwidth]{figures/num_params.png}
}
}
\vspace{-2mm}
\caption{Further analysis of our TaTS framework. (a) Learning rate sensitivity: TaTS maintains stable performance across different learning rates (full results in Appendix \ref{ap: full hyperparameter learning rate}).
(b) Text Projection Dimension sensitivity: TaTS remains robust across varying $d_{\text{mapped}}$ (full results in Appendix \ref{ap: full hyperparameter text embedding dimension}).
(c) Varying text encoder: TaTS consistently outperforms baselines across different text encoders (full results in Appendix \ref{ap: full ablation of text encoder}).
(d) Efficiency: TaTS introduces only a minor parameter increase ($\sim1\%$) but significantly improves the performance according to Table \ref{tab: main forecasting}.}
\label{fig: hyper and ablation}
\vspace{-3mm}
\end{figure*}


\subsection{Further Analysis}
\label{subsec: ablation}

\textbf{Hyperparameter Sensitivity.}
We perform hyperparameter studies to evaluate the impact of (i) the learning rate and (ii) $d_{\text{mapped}}$, the dimension to which high-dimensional text embeddings are projected by the MLP, as defined in Equation \ref{eq: mlp}. The results are presented in Figure \ref{fig: hyper and ablation}, subfigures (a) and (b), with full results available in Appendix \ref{ap: full hyperparameter learning rate} and Appendix \ref{ap: full hyperparameter text embedding dimension}. The findings indicate that TaTS maintains robust performance across different choices of the learning rate and the text projection dimension $d_{\text{mapped}}$.

\textbf{Ablation with Different Text Encoders in TaTS.}
While GPT-2 was used as the primary text encoder in our main experiments to demonstrate the effectiveness of TaTS, we further evaluate the performance of TaTS with different text encoders, including BERT \cite{DBLP:conf/naacl/DevlinCLT19}, GPT-2 \cite{radford2019language}, and LLaMA2 \cite{DBLP:journals/corr/abs-2307-09288}. We utilize the official implementations available on Hugging Face and present the results in Figure \ref{fig: hyper and ablation}, with full results provided in Appendix \ref{ap: full ablation of text encoder}. The results show that TaTS remains robust across different text encoders and consistently outperforms both the uni-modal and MM-TSFLib baselines. Notably, as the size of the language models used in TaTS increases from 110M (BERT) to 1.5B (GPT-2) and further to 7B (LLaMA2), we observe a slight improvement in performance. Investigating the relationship between the text encoder size and TaTS's effectiveness remains an open direction for the future research.


\begin{minipage}{0.5\linewidth}
  \includegraphics[width=\linewidth]{figures/efficiency_plots/efficiency_Environment_main.png}
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
  \captionof{figure}{Efficiency comparison on Environment dataset. While TaTS incurs a slight increase in training time due to the augmentation of auxiliary variables, it significantly enhances forecasting accuracy. Full results are in Appendix \ref{ap: full efficiency}.} 
  \label{fig: efficiency_main}
\end{minipage}


\textbf{Computational Overhead vs. Performance Gain.}
We also evaluate the efficiency of our proposed TaTS by measuring the training time per epoch and the total number of model parameters. Figure \ref{fig: hyper and ablation} (d) presents the total number of parameters for the best-performing models in our forecasting experiments. TaTS introduces only a lightweight three-layer MLP to project high-dimensional text embeddings into a lower-dimensional space, adding a minimal number of parameters compared to the original time series models. As a result, the overall parameter count increases by only about 1\%.
Due to the inclusion of augmented time series with auxiliary variables from paired texts, the training time per epoch increases slightly, as shown in Figure \ref{fig: efficiency_main}, with average performance of each framework indicated by cross markers. Full results for all datasets are provided in Appendix \ref{ap: full efficiency}. However, this marginal efficiency trade-off ($\sim1\%$ in terms of number of learnable parameters and $\sim8\%$ in terms of training time) leads to significant improvements ($\sim 14\%$) in forecasting performance, demonstrating the effectiveness of TaTS in enhancing time series modeling with paired texts.