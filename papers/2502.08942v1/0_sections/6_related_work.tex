\section{Related Work}

\textbf{Numerical-only Time Series Modeling.}
Recently, various deep learning models have been developed for time series analysis, which can be broadly categorized into three categories. (1) \textbf{Patch-based models.} PatchTST \cite{patchtst} segments time series into subseries-level patches to capture dependencies, while Crossformer \cite{crossformer} employs a two-stage attention mechanism to model both cross-time and cross-variable dependencies efficiently. Autoformer \cite{autoformer} introduces decomposition blocks to separate seasonal and trend-cyclical components. 
(2) \textbf{Global representation models.} iTransformer \cite{itransformer} utilizes attention over global series representations to capture multivariate correlations. Informer \cite{informer} reduces self-attention complexity using ProbSparse self-attention for improved efficiency. Dlinear \cite{dlinear} demonstrates that simple linear regression in the raw space can perform competitively on MTS tasks.
(3) \textbf{Frequency-aware models.} FEDformer \cite{fedformer} represents series through randomly selected Fourier components, while FiLM \cite{film} enhances representations with frequency-based layers to reduce noise and accelerate training. In this work, our proposed TaTS is compatible with all of the models listed above.

\textbf{Time Series with other data sources.}
Real-world data often coexists and coevolve with auxiliary information \cite{DBLP:conf/www/LiFH23, DBLP:conf/nips/BanZLQFKTH24}, such as various modalities \cite{DBLP:journals/corr/abs-2412-08174, DBLP:conf/kdd/FuFMTH22, DBLP:journals/corr/abs-2410-17576, DBLP:conf/sdm/ZhengZH23,DBLP:conf/sdm/ZhengCH19, DBLP:journals/corr/abs-2412-17336} and heterogeneous views \cite{DBLP:conf/kdd/ZhengJLTH24, DBLP:conf/kdd/ZhengXZH22, li2022novel, DBLP:conf/sigir/LiAH24}. In time series analysis, effectively utilizing this auxiliary information remains an emerging and nascent area.
In the financial domain, several early works have explored integrating time series with textual data, albeit not in a timestamp-aligned manner, or often leveraging general machine learning models rather than time series-specific architectures. For example, StockNet \cite{DBLP:conf/acl/CohenX18} uses a VAE-like model for chaotic stock-text data, while \cite{DBLP:journals/inffus/RodriguesMP19} fuses time series with a single event-related document. BoEC \cite{DBLP:conf/dsaa/FarimaniJFH21} applies a bag-of-economic-concepts approach, and Dandelion \cite{DBLP:conf/www/ZhouZZLH20} leverages multimodal attention for feature aggregation from multiple text sources. Some works also explored time series with vision information \cite{liu2012spatial, DBLP:conf/nips/GerardZS23, DBLP:journals/tgrs/LutjensLBCRMMRSPGRLN24} for spatio-temporal analysis. Recently, Time-MMD \cite{DBLP:journals/corr/abs-2406-08627} constructs a dataset of time series paired with parallel text, covering multiple domains, which we used in our main experiments.

\textbf{Large Language Models on Time Series.}
The rapid advancement of Large Language Models (LLMs) and pre-trained foundation models \cite{jing2024automated, DBLP:journals/corr/abs-2410-12126, DBLP:journals/corr/abs-2412-21151} has inspired a new line of research that transforms time series into natural language representations, enabling LLMs to perform downstream tasks \cite{DBLP:conf/emnlp/SuiZZHDH024, DBLP:conf/ijcai/0001C0S24}. While these approaches demonstrate strong generalization capabilities due to the power of LLMs \cite{DBLP:conf/nips/GruverFQW23, DBLP:conf/iclr/CaoJAPZY024, DBLP:conf/iclr/0005WMCZSCLLPW24, DBLP:journals/tkde/XueS24}, they also inherit limitations such as hallucination \cite{DBLP:journals/corr/abs-2311-05232, DBLP:conf/emnlp/ZouZLH024} and context length constraints \cite{DBLP:conf/ijcai/WangSORRE24, DBLP:journals/tacl/LiuLHPBPL24}. Notably, a recent work \cite{DBLP:journals/corr/abs-2406-16964} suggests that replacing complex LLM architectures with basic attention layers does not degrade the performance.



\section{Conclusion}
Real-world time series data often comes with textual descriptions, yet prior studies have largely overlooked this modality. We identify \textit{Chronological Textual Resonance}, where text embeddings exhibit periodic patterns similar to their paired time series.
To leverage this insight, we propose a plug-and-play framework that transforms text representations into auxiliary variables, seamlessly integrating them into existing time series models.
Extensive experiments demonstrate the state-of-the-art performance of our approach.