\section{Preliminary}

\begin{figure*}[t]
\centering
\subfigure[Economy]{
\includegraphics[width=0.32\textwidth]{figures/2_Economy_ts_amplitude_with_text_mark.png}
}
\subfigure[Social Good]{
\includegraphics[width=0.31\textwidth]{figures/2_SocialGood_ts_amplitude_with_text_mark.png}
}
\subfigure[Traffic]{
\includegraphics[width=0.313\textwidth]{figures/2_Traffic_ts_amplitude_with_text_mark.png}
}
\caption{By overlaying the top frequencies of paired texts (vertical dashed lines) onto the amplitude spectrum of the time series, it is observed that the time-series-paired texts exhibit similar periodic properties that closely mirror those of the original time series. We term this phenomenon \textit{Chronological Textual Resonance}. More Details are provided in Appendix \ref{ap: detailed frequency analysis}.}
\label{fig: fft results with text marks}
\end{figure*}

We use calligraphic letters (e.g., $\mathcal{A}$) for sets and bold capital letters for matrices (e.g., $\bm{A}$). For matrix indices, we use $\bm{A}[i, j]$ to denote the entry in the $i^{\textrm{th}}$ row and the $j^{\textrm{th}}$ 
column. For a vector $\bm{v}$, $v[i:j]$ 
represents the subvector consisting of elements from the $i^{\textrm{th}}$ to the $j^{\textrm{th}}$ position, inclusive. Additionally, $\bm{A}[i, :]$ returns the $i^{th}$ row in $\bm{A}$ and $\bm{A}[:i]$ returns the first $i$ rows of $\bm{A}$. In this paper, we focus on both time series forecasting and imputation.

\textbf{Time Series Forecasting.} 
A time series is denoted as $\mathbf{X} = \{ \vecx_1, \vecx_2, \dots, \vecx_N \} \in \mathbb{R}^{T \times N}$, where $T$ represents the number of time steps and $N$ denotes the number of variables. $\vecx_i$ is the time series sequence of the $i^{\textrm{th}}$ variable. When $N > 1$, the time series is referred to as a multivariate time series. Let $\mathbf{X}_{a:b}$ represent the time slice of the series from timestamp $a$ to $b$, i.e., $\mathbf{X}_{a:b} = \{\vecx_1[a:b], \vecx_2[a:b], \dots, \vecx_N[a:b]\}$.
The task of time series forecasting is to predict the future $H$ steps, which can be formulated as:
\begin{equation}
\widehat{\mathbf{X}}_{T+1: T+H}=\mathcal{F}\left(\mathbf{X}_{1: T} ; \theta_{\text{forecast}}\right) \in \mathbb{R}^{H \times N},
\end{equation}
where $\widehat{\mathbf X}_{T+1:T+H}$ denotes the forecasting results, $\mathcal{F}$ denotes the mapping function, and $\theta_{\text{forecast}}$ denotes the learnable parameters of $\mathcal{F}$.

\textbf{Time Series Imputation.} The goal of imputation is to estimate missing values in the observed time series $\mathbf{X}$, where the missing entries are denoted by a binary mask $\mathbf{M} \in \{0, 1\}^{T \times N}$. Specifically, $\mathbf{M}_{t,n}=1$ indicates that $\mathbf{X}_{t,n}$ is observed, while $\mathbf{M}_{t,n}=0$ indicates that $\mathbf{X}_{t,n}$ is missing. The imputation task can be formulated as:
\begin{equation}
\widehat{\mathbf{X}}^{\text{Imputed}} = \mathcal{G}\left(\mathbf{X} \odot \mathbf{M}, \mathbf{M}; \theta_{\text{impute}} \right) \in \mathbb{R}^{T \times N},
\end{equation}
where $\widehat{\mathbf{X}}^{\text{Imputed}}$ represents the imputed time series, $\mathcal{G}$ denotes the imputation function, $\theta_{\text{impute}}$ denotes its learnable parameters, and $\odot$ represents the element-wise multiplication. The imputation process aims to recover the missing entries such that $\widehat{\mathbf{X}}^{\text{Imputed}} \approx \mathbf{X}$ with respect to the observed $\mathbf{X}$ values.

\textbf{Extending Time Series with Paired Texts.} 
Real-world time series often evolves alongside data from other modalities.
%, enriching the context and interpretability of the observations. 
In this work, we focus on the scenario where the time series is accompanied by paired textual data for each timestamp. Specifically, in addition to the numerical time series $\bm{X} \in \mathbb{R}^{T \times N}$, the dataset $\mathcal{D} = \{\bm{X}, \bm{S}\}$ includes textual information $\bm{S} = \{s_1, s_2, \dots, s_T\}$, where each $s_t$ represents the text associated with timestamp $t$. Each $s_t$ is a string that can be tokenized into a sequence of tokens, i.e., $\text{Tokenize}(s_t) = \{w_{t,1}, w_{t,2}, \dots, w_{t,L_t}\}$, where $L_t$ denotes the number of tokens in the text at time $t$.
 The textual data can be transformed into numerical representations using a textual encoder $\mathcal{H}_{\text{text}}$, such that:
\begin{equation}
\bm{e}_t = \mathcal{H}_{\text{text}}(s_t; \theta_{\text{text}}) \in \mathbb{R}^{d_{\text{text}}},
\end{equation}
where $\bm{e}_t$ is the encoded representation of the text at time $t$, $d_{\text{text}}$ is the dimensionality of the textual embedding, and $\theta_{\text{text}}$ are the parameters of the encoder. In this work, we leverage pre-trained large language models to encode the texts by applying pooling over embeddings of individual tokens.
