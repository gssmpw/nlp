\begin{algorithm}[t]
    \caption{Texts as Time Series for forecasting task}
    \label{alg:main}
\begin{algorithmic}[1]
    \REQUIRE 
    Time series with concurrent texts embeddings $\mathcal{D} = \{\bm{X} = \{\vecx_1, \vecx_2, \dots, \vecx_N\}, \bm{E} = \{e_1, e_2, \dots, e_T\}\}$ in the input training dataset; prediction length $H$.
    \STATE \textbf{Initialize:} 
    Prepare training samples of sequence length $L$ and prediction length $H$ to be $\{\bm{X}^{(i)} = \bm{X}_{l_i+1:l_i + L}, \bm{E}^{(i)}=\bm{X}_{l_i+1:l_i + L}, \bm{Y}^{(i)}=\bm{X}_{l_i+L+1:l_i+L+H}\}_{i=1}^n$;  time series model $\cm{F}(\cdot; \theta_{\text{forecast}})$; projector $\text{MLP}(\cdot; \theta_{\text{MLP}})$ for reducing dimensionality. 
    \WHILE{not converged}
        \FOR{training sample $\{\bm{X}^{(i)}, \bm{E}^{(i)}, \bm{Y}^{(i)}\}$}
        \STATE Map $\bm{E}^{(i)}$ to $\bm{Z}^{(i)}$: $\bm{Z}^{(i)}[j] = \text{MLP}(\bm{E}^{(i)}[j]; \theta_{\text{MLP}})$
        \STATE Compute $\bm{U} = [\bm{X}^{(i)}; {(\bm{Z}^{(i)})}^{\intercal}]_{\text{dim=1}}$ as Eq (\ref{eq: compute u}).
        \STATE Forecast $\widehat{\mathbf{X}}^{(i)} = \mathcal{F}\left(\mathbf{U}; \theta_{\text{forecast}}\right)[:N]$
        \STATE \resizebox{0.83\columnwidth}{!}{
        $\argmin_{\Theta=\{\theta_{\text{forecast}}, \theta_{\text{MLP}}\}} \mathcal{L}_{\text{forecast}}(\bm{X}^{(i)}, \widehat{\mathbf{X}})^{(i)}$ as Eq. (\ref{eq: training mse loss})
        }
        \ENDFOR
    \ENDWHILE
    \STATE \textbf{Return:} TaTS model parameters $\Theta=\{\theta_{\text{forecast}}, \theta_{\text{MLP}}\}$
\end{algorithmic}
\end{algorithm}


\section{Texts as Time Series}
Guided by the above insights, this section introduces our proposed framework, \underline{T}exts \underline{a}s \underline{T}ime \underline{S}eries (TaTS). TaTS integrates concurrent texts by transforming them into auxiliary variables and seamlessly plugging the text-augmented time series into any existing time series model. Our approach effectively captures the temporal dynamics and interconnections between the time series and concurrent texts by unifying them within a single time series model. An overview of the proposed TaTS is illustrated in Figure \ref{fig: main}.

\subsection{Concurrent Texts are Secretly Auxiliary Variables}  
Previously, we elucidated three key reasons behind the CTR phenomenon, which serve as the core motivation for TaTS: mapping concurrent texts to auxiliary variables to augment the original time series. The properties of concurrent texts closely align with those of variables in a multivariate time series: similar to numerical variables, concurrent texts are influenced by shared external drivers and interact dynamically with the time series. Furthermore, mapping concurrent texts to structured variables enables capturing hidden variables embedded within the concurrent texts.






\subsection{Framework and Training Objectives}
Our proposed TaTS framework builds on the insight that concurrent texts behave similarly to additional variables and, therefore, can be treated in a comparable manner. As shown in Figure \ref{fig: main}.

\textbf{Transforming Concurrent Texts into Variables.}
Given the dataset $\mathcal{D} = \{\bm{X} = \{\vecx_1, \vecx_2, \dots, \vecx_N\}, \bm{S} = \{s_1, s_2, \dots, s_T\}\}$, we first embed the texts using a text encoder $\mathcal{H}_{\text{text}}$ to obtain text embeddings $\bm{E} = \{e_1, e_2, \dots, e_T\} \in \mathbb{R}^{d_{\text{text} \times T}}$. Since the text embedding dimension $d_{\text{text}}$ is typically much larger than the number of variables in the time series, we reduce the dimensionality of the text embeddings by applying a Multi-Layer Perceptron (MLP), mapping them into a lower-dimensional space.
\begin{equation}
\label{eq: mlp}
\bm{z}_t = \text{MLP}(\bm{e}_t; \theta_{\text{MLP}}) \in \mathbb{R}^{d_{\text{mapped}}},
\end{equation}
where $d_{\text{mapped}}$ is the reduced dimensionality.

\textbf{Unifying by Plugging-in a Time Series Model.}
The resulting mapped embeddings $\bm{Z} = \{\bm{z}_1, \bm{z}_2, \dots, \bm{z}_T\} \in \mathbb{R}^{d_{\text{mapped} \times T}}$ are then treated as auxiliary variables in the time series. Specifically, $\bm{Z}$ is concatenated with $\bm{X}$ to form a unified multimodal sequence:  
\begin{equation}
\label{eq: compute u}
\bm{U} = [\bm{X}; \bm{Z}^{\intercal}]_{\text{dim=1}} \in \mathbb{R}^{T \times (N + d_{\text{mapped}})},
\end{equation}
The unified sequence $\bm{U}$ is then passed into an existing time series model for downstream tasks. Here, we formulate the example of forecasting the next $H$ steps of the time series
\begin{equation}
    \widehat{\mathbf{X}}_{T+1: T+H} = \mathcal{F}\left(\mathbf{U}_{1: T}; \theta_{\text{forecast}}\right)[:N] \in \mathbb{R}^{H \times N},
\end{equation}
where $\mathcal{F}(\cdot; \theta_{\text{forecast}})$ denotes the time series forecasting model with parameters $\theta_{\text{forecast}}$, and $[:N]$ extracts the first $N$ variables corresponding to the original time series.

Finally, we joint train the time series model $\theta_{\text{forecast}}$ as well as the mapping MLP $\theta_{\text{MLP}}$ using the Mean Squared Error (MSE) loss.
\begin{equation}
\label{eq: training mse loss}
\mathcal{L}_{\text{forecast}}(\bm{X}, \widehat{\mathbf{X}}) = \frac{1}{H \cdot N} \sum_{t=T+1}^{T+H} \sum_{i=1}^{N} \left(\mathbf{X}_{t, i} - \widehat{\mathbf{X}}_{t, i}\right)^2,
\end{equation}
where $\mathbf{X}_{t, i}$ and $\widehat{\mathbf{X}}_{t, i}$ represent the ground truth and predicted values of the $i^{\textrm{th}}$ variable at time step $t$, respectively.
