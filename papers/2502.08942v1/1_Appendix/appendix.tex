% \begin{center}
    % {\Large\textbf{Appendix}}
% \end{center}

\textbf{\LARGE Appendix}

\textbf{Roadmap.} 
In this appendix, we provide a detailed overview of our methodology and experimental setup. Appendix \ref{ap: detailed frequency analysis} outlines the complete process of frequency analysis for both time series and paired texts. Appendix \ref{ap: exp details} includes details on datasets, hyperparameters, evaluation metrics, and additional implementation specifics. Due to space constraints in the main text, Appendix \ref{ap: full results} presents the full experimental results, including comprehensive forecasting and imputation outcomes, hyperparameter and ablation studies, efficiency evaluations, and visualizations. The table of contents is provided below for reference.

% Make appear only appendix sections in table of content
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

% Change title of table of contents
\renewcommand*\contentsname{\Large Table of Contents}

%\setstretch{1.5} % stretch for table of contents
\tableofcontents
%\noindent\hrulefill
%\setstretch{1} % unstretch for the rest
\clearpage

\section{Detailed Frequency Analysis Process of Time Series with Paired Texts}
\label{ap: detailed frequency analysis}

Here, we provide a detailed explanation of the frequency analysis process for both the time series and their paired texts.

\begin{proposition}
\label{proposition: lag similarity}
    The computation of lag similarity preserves the original periodicities of the data.
\end{proposition}

\begin{proof}
Let $\bm{S} = \{s_t\}_{t=1}^T$ represent the paired texts or data sequence, where each $s_t$ corresponds to a time step $t$. Define the lag similarity at lag $k$ as:
\begin{equation}
\text{LagSim}(k) = \frac{1}{T-k} \sum_{t=1}^{T-k} \text{sim}(s_t, s_{t+k}),
\end{equation}
where $\text{sim}(\cdot, \cdot)$ is a similarity measure (e.g., cosine similarity).

Now, consider the periodic component of the data sequence $\bm{S}$, which can be represented as:
\begin{equation}
s_t = A \cos\left(\frac{2\pi t}{P} + \phi\right),
\end{equation}
where $A$ is the amplitude, $P$ is the period, and $\phi$ is the phase.

For two points separated by lag $k$, the similarity $\text{sim}(s_t, s_{t+k})$ depends on the relative difference between their phases:
\begin{equation}
s_{t+k} = A \cos\left(\frac{2\pi (t+k)}{P} + \phi\right) = A \cos\left(\frac{2\pi t}{P} + \frac{2\pi k}{P} + \phi\right).
\end{equation}

The lag similarity is then computed as:
\begin{equation}
\text{LagSim}(k) = \frac{1}{T-k} \sum_{t=1}^{T-k} \text{sim}\left(A \cos\left(\frac{2\pi t}{P} + \phi\right), A \cos\left(\frac{2\pi t}{P} + \frac{2\pi k}{P} + \phi\right)\right).
\end{equation}

Since the cosine function is periodic with period $P$, the similarity $\text{sim}(s_t, s_{t+k})$ also inherits this periodicity. Therefore, the overall lag similarity $\text{LagSim}(k)$ retains the periodicities of the original sequence $\bm{S}$.

Thus, the computation of lag similarity preserves the original periodicities of the data.
\end{proof}



\begin{proposition}
\label{proposition: first-order differentiation}
    The stabilization of a data sequence using first-order differentiation preserves its original periodicities.
\end{proposition}

\begin{proof}
Let $\bm{S} = \{s_t\}_{t=1}^T$ represent a data sequence, where $s_t$ is the value at time step $t$. The first-order differentiation of the sequence is defined as:
\begin{equation}
\Delta s_t = s_{t+1} - s_t, \quad t = 1, 2, \ldots, T-1.
\end{equation}

Suppose the sequence $\bm{S}$ exhibits periodic behavior with period $P$ and can be represented as:
\begin{equation}
s_t = A \cos\left(\frac{2\pi t}{P} + \phi\right),
\end{equation}
where $A$ is the amplitude, $P$ is the period, and $\phi$ is the phase.

The first-order difference of $s_t$ is:
\begin{equation}
\Delta s_t = s_{t+1} - s_t = A \cos\left(\frac{2\pi (t+1)}{P} + \phi\right) - A \cos\left(\frac{2\pi t}{P} + \phi\right).
\end{equation}

Using the trigonometric identity for the difference of cosines:
\begin{equation}
\cos(x + y) - \cos(x) = -2 \sin\left(\frac{y}{2}\right) \sin\left(x + \frac{y}{2}\right),
\end{equation}
we set $x = \frac{2\pi t}{P} + \phi$ and $y = \frac{2\pi}{P}$, giving:
\begin{equation}
\Delta s_t = -2A \sin\left(\frac{\pi}{P}\right) \sin\left(\frac{2\pi t}{P} + \phi + \frac{\pi}{P}\right).
\end{equation}

The first term, $\sin\left(\frac{\pi}{P}\right)$, is a constant dependent on the period $P$. The second term, $\sin\left(\frac{2\pi t}{P} + \phi + \frac{\pi}{P}\right)$, retains the periodicity of $P$, as it is a sinusoidal function with the same frequency as the original sequence.

Thus, the first-order difference $\Delta s_t$ preserves the periodicity $P$ of the original data sequence.
\end{proof}




\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/mmts_frequency_flow_economy.png}
    % \vspace{-10pt}
    \caption{Illustration of the frequency analysis process for time series with paired texts in the Economy dataset. Step \textcircled{1}: Compute the average text similarity for each lag $k$. Step \textcircled{2}: Stabilize the time series using first-order differentiation, apply Fourier Transform, and perform Non-Maximum Suppression (NMS) to obtain the amplitude spectrum. Step \textcircled{3}: Visualize the lag similarity of paired texts. Step \textcircled{4}: Stabilize the paired texts, compute the Fourier Transform, and visualize the amplitude spectrum. Step \textcircled{5}: Overlay the top-$l$ (here $l$=4) frequencies of paired texts onto the time series amplitude spectrum to highlight shared periodic patterns. All the data transformation operations in this process are periodicity-preserving according to Proposition \ref{proposition: lag similarity} and Proposition \ref{proposition: first-order differentiation}.}
    \label{fig:frequency_flow_economy}
    % \vspace{-10pt}
\end{figure*}



The overall process of frequency analysis for the time series $\bm{X} = \{\vecx_1, \vecx_2, \dots, \vecx_N \} \in \mathbb{R}^{T \times N}$ and paired texts $\bm{S} = \{s_1, s_2, \dots, s_T\}$ in the Economy dataset $\cm{D}_{\text{Economy}} = \{\bm{X}, \bm{S}\}$ is illustrated in Figure \ref{fig:frequency_flow_economy}. In this process, starting from the original dataset (subfigure \textcircled{1}), the time series and paired texts are analyzed independently. \textbf{For the time series} $\bm{X} = \{\vecx_1, \vecx_2, \dots, \vecx_N \}$ \textbf{, we perform standard frequency analysis}, stabilizing the data through first-order differentiation.
\begin{equation}
\Delta \bm{X}_t = \bm{X}_{t+1} - \bm{X}_t, \quad \text{for } t = 1, 2, \dots, T-1,
\end{equation}
where $\Delta \bm{X}_t$ represents the first-order difference of the time series. This step removes long-term trends and ensures that the data is stationary, allowing for a more accurate analysis of its frequency components.

Then, we compute the Fourier Transform \cite{nussbaumer1982fast, sneddon1995fourier} of $\Delta\bm{X}$ to analyze its frequency components. The Fourier Transform of $\Delta\bm{X}$ is defined as:
\begin{equation}
\cm{F}_{\Delta\bm{X}}(f) = \sum_{t=1}^{T-1} \Delta\bm{X}_t e^{-i 2 \pi f t},
\end{equation}
where $f$ represents the frequency, $\Delta\bm{X}_t$ is the first-order difference of the time series at time $t$, and $i$ is the imaginary unit. The resulting $\cm{F}_{\Delta\bm{X}}(f)$ provides the amplitude and phase information of each frequency component present in the time series.

The magnitude spectrum, which represents the amplitude of each frequency component, is computed as:
\begin{equation}
|\cm{F}_{\Delta\bm{X}}(f)| = \sqrt{\text{Re}(\cm{F}_{\Delta\bm{X}}(f))^2 + \text{Im}(\cm{F}_{\Delta\bm{X}}(f))^2},
\end{equation}
where $\text{Re}(\cm{F}_{\Delta\bm{X}}(f))$ and $\text{Im}(\cm{F}_{\Delta\bm{X}}(f))$ are the real and imaginary parts of $\cm{F}_{\Delta\bm{X}}(f)$, respectively.

By analyzing $|\cm{F}_{\Delta\bm{X}}(f)|$, we identify the dominant frequencies in the time series, which reveal its periodic patterns. To further highlight the dominant frequencies, we apply Non-Maximum Suppression (NMS) to the magnitude spectrum $|\mathcal{F}_{\Delta\bm{X}}(f)|$. NMS ensures that only the most prominent frequencies are retained while suppressing nearby less significant frequencies. The NMS operation is defined as follows:
\begin{equation}
\mathcal{N}(f) =
\begin{cases}
|\mathcal{F}_{\Delta\bm{X}}(f)|, & \text{if } |\mathcal{F}_{\Delta\bm{X}}(f)| > |\mathcal{F}_{\Delta\bm{X}}(f')| \, \forall f' \in \mathcal{N}(f), \\
0, & \text{otherwise,}
\end{cases}
\end{equation}
where $\mathcal{N}(f)$ represents a local neighborhood around the frequency $f$. The operation compares the magnitude of $|\mathcal{F}_{\Delta\bm{X}}(f)|$ with those of neighboring frequencies and retains only the largest value within the neighborhood. Frequencies that do not satisfy the condition are set to zero.

After applying NMS, the remaining frequencies represent the dominant periodic components of the time series, making it easier to identify significant periodic patterns. This process eliminates noise and reduces the influence of minor frequency components, enhancing the interpretability of the spectrum. The final visualization of the amplitude spectrum of the time series is shown in Figure \ref{fig:frequency_flow_economy}, subfigure \textcircled{2}.


For the paired texts $\bm{S} = \{s_1, s_2, \dots, s_T\}$, we first embed each $s_t$ using the text encoder 
$\mathcal{H}_{\text{text}}$:
\begin{equation}
\bm{e}_t = \mathcal{H}_{\text{text}}(s_t; \theta_{\text{text}}) \in \mathbb{R}^{d_{\text{text}}},
\end{equation}
where $\theta_{\text{text}}$ represents the parameters of the text encoder, and $\bm{e}_t$ is the resulting text embedding at timestamp $t$.

Since the text embeddings are typically close in the embedding space, leading to similar cosine similarity values, we normalize the embeddings by centering them around their mean to improve numerical stability and enhance sensitivity to differences. Specifically, we compute the mean embedding:
\begin{equation}
\bm{e}_{\text{mean}} = \frac{1}{T} \sum_{t=1}^T \bm{e}_t,
\end{equation}
and shift all embeddings by subtracting the mean:
\begin{equation}
\bm{e}_t' = \bm{e}_t - \bm{e}_{\text{mean}},
\end{equation}
where $\bm{e}_t'$ represents the centered (shifted) embeddings.

Next, we compute the average text similarity for each lag $k \in \{1, T-1\}$ as:
\begin{equation}
\text{Sim}(k) = \frac{1}{T-k} \sum_{t=1}^{T-k} \text{sim}(\bm{e}_t', \bm{e}_{t+k}'),
\end{equation}
where $\text{sim}(\bm{e}_t', \bm{e}_{t+k}')$ denotes the similarity measure (e.g., cosine similarity) between the centered embeddings at time $t$ and $t+k$, defined as:
\begin{equation}
\text{sim}(\bm{e}_t', \bm{e}_{t+k}') = \frac{\bm{e}_t' \cdot \bm{e}_{t+k}'}{\|\bm{e}_t'\| \|\bm{e}_{t+k}'\|}.
\end{equation}
We visualize the lag similarity of paired texts, $\text{Sim}(k)$, in Figure \ref{fig:frequency_flow_economy}, subfigure \textcircled{3}. Subsequently, we stabilize the data by applying first-order differentiation and perform a Fourier Transform, following a similar process as previously described for the time series frequency analysis. The final visualization of the amplitude spectrum of the paired texts is presented in Figure \ref{fig:frequency_flow_economy}, subfigure \textcircled{4}.

Then, we compute the frequencies with the top-$l$ amplitudes from the lag similarity $\text{Sim}(k)$. We apply the Fourier Transform to $\text{Sim}(k)$:
\begin{equation}
\mathcal{F}_{\text{text}}(f) = \sum_{t=1}^{T-1} \text{Sim}(k) \, e^{-i 2 \pi f t},
\end{equation}
where $f$ is the frequency, and $\mathcal{F}_{\text{text}}(f)$ represents the complex Fourier coefficients corresponding to each frequency $f$.

Next, we compute the amplitude spectrum as:
\begin{equation}
|\mathcal{F}_{\text{text}}(f)| = \sqrt{\text{Re}(\mathcal{F}_{\text{text}}(f))^2 + \text{Im}(\mathcal{F}_{\text{text}}(f))^2},
\end{equation}
where $\text{Re}(\mathcal{F}_{\text{text}}(f))$ and $\text{Im}(\mathcal{F}_{\text{text}}(f))$ are the real and imaginary parts of $\mathcal{F}_{\text{text}}(f)$, respectively.

We then identify the top-$l$ dominant frequencies by selecting the $l$ frequencies corresponding to the largest amplitudes:
\begin{equation}
\mathcal{F}_{\text{top}} = \{f_i \mid |\mathcal{F}_{\text{text}}(f_i)| \text{ is among the top-} l \text{ largest amplitudes}\}.
\end{equation}
These top-$l$ frequencies represent the most significant periodic components of the paired texts, revealing their dominant temporal patterns. we overlay the top-$l$ (in the Economy dataset $l$=4) frequencies of the paired texts onto the amplitude spectrum of the time series, as illustrated in Figure \ref{fig:frequency_flow_economy}, subfigure \textcircled{5}.

We also visualize the frequency analysis process in the Social Good dataset and Traffic dataset respectively in Figure \ref{fig:frequency_flow_socialgood} and Figure \ref{fig:frequency_flow_traffic}.


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/mmts_frequency_flow_socialgood.png}
    % \vspace{-10pt}
    \caption{Illustration of the frequency analysis process for time series with paired texts in the Social Good dataset. In Step \textcircled{5}, we overlay the top-$9$ frequencies of paired texts onto the time series amplitude spectrum.}
    \label{fig:frequency_flow_socialgood}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/mmts_frequency_flow_traffic.png}
    % \vspace{-10pt}
    \caption{Illustration of the frequency analysis process for time series with paired texts in the Traffic dataset. In Step \textcircled{5}, we overlay the top-$7$ frequencies of paired texts onto the time series amplitude spectrum.}
    \label{fig:frequency_flow_traffic}
\end{figure*}




\section{Experiment Details}
\label{ap: exp details}

\subsection{Dataset Statistics and Details}
\label{ap: dataset details}

Table \ref{tab: dataset details} provides a summary of the statistics for the publicly available real-world datasets. Additionally, we visualize the numerical data for each multimodal time series dataset in Figure \ref{fig: dataset visualize}. For further details, please refer to the original work that introduced these datasets and benchmarks \cite{DBLP:journals/corr/abs-2406-08627}.



\begin{figure*}[t]
\centering
\vspace{-3mm}
% Row 1
\subfigure[Agriculture]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Agriculture_ts.png}
}
\subfigure[Climate]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Climate_ts.png}
}
\subfigure[Economy]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Economy_ts.png}
}
% Row 2
\subfigure[Energy]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Energy_ts.png}
}
\subfigure[Environment]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Environment_ts.png}
}
\subfigure[Health]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Health_ts.png}
}
% Row 3
\subfigure[Security]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Security_ts.png}
}
\subfigure[Social Good]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/SocialGood_ts.png}
}
\subfigure[Traffic]{
\includegraphics[width=0.31\textwidth]{figures/timeseries_plot/Traffic_ts.png}
}
\caption{Visualization of the numerical data in each multimodal time series dataset.}
\label{fig: dataset visualize}
\vspace{-3mm}
\end{figure*}





\begin{table*}[t]
\centering
\caption{Overview of the numerical data in the experimental datasets \cite{DBLP:journals/corr/abs-2406-08627}. Our experiments utilize 9 datasets spanning multiple domains. ``Prediction Length" refers to the number of future time points to be forecasted, with each dataset including four distinct prediction settings. refers to the number of variables (or variates) in each dataset.}
\label{tab: dataset details}
\vspace{3mm}
\resizebox{0.9\textwidth}{!}{
% \begin{tabular}{lllll}

\begin{tabular}{cccccc}
\toprule
Dataset Name/Domain & Prediction Length &Dimension & Frequency & Number of Samples & Timespan \\ \midrule
Agriculture & \{6, 8, 10, 12\}  &1  & Monthly & $496$ & 1983 - Present \\
Climate  & \{6, 8, 10, 12\} &5 & Monthly & $496$ & 1983 - Present \\
Economy & \{6, 8, 10, 12\} &3 & Monthly & $423$ & 1989 - Present \\
Energy & \{12, 24, 36, 48\} &9 & Weekly & $1479$ & 1996 - Present \\
Environment & \{48, 96, 192, 336\} &4 & Daily & $11102$ & 1982 - 2023 \\
Health & \{12, 24, 36, 48\} &11 & Weekly & $1389$ & 1997 - Present \\
Security & \{6, 8, 10, 12\} &1& Monthly & $297$ & 1999 - Present \\
Social Good & \{6, 8, 10, 12\} &1 & Monthly & $900$ & 1950 - Present \\ 
Traffic & \{6, 8, 10, 12\} &1& Monthly & $531$ &1980 - Present \\ \bottomrule
\end{tabular}
}
\end{table*}




\subsection{Hyperparameters}
We use Adam optimizer \cite{DBLP:journals/corr/KingmaB14} when training the neural networks. The default choices of hyperparameters in our code are provided in Table \ref{TB: hyper}. For LLM-based text encoders, we initialize them using the default configurations provided by Hugging Face\footnote{\url{https://huggingface.co/}}. Consistent with existing works \cite{DBLP:conf/iclr/WuHLZ0L23, itransformer}, we apply instance normalization to standardize the time series data within each dataset.

\begin{table}[t]
\caption{Default hyperparameters for the TaTS framework}
\label{TB: hyper}
\vspace{3mm}
\begin{center}
\begin{tabular}{llc}
\toprule
Hyperparameter & Description & Value or Choices \\
\midrule
batch\_size & The batch size for training & 32 \\
criterion & The criterion for calculating loss & Mean Square Error (MSE)  \\
learning\_rate & The learning rate for the optimizer  & \{0.0001, 0.00005, 0.00001\} \\
seq\_len & Input sequence length & 24 \\
label\_len & Start token length for prediction & 12 \\
prior\_weight & Weight for prior combination & \{0, 0.1, 0.2, 0.3, 0.5\} \\
train\_epochs & Number of training epochs & 50 \\
patience & Early stopping patience & 20 \\
text\_emb & Dimension of text embeddings & \{6, 12, 24\} \\
learning\_rate2 & Learning rate for MLP layers & \{0.005, 0.01, 0.02, 0.05\} \\
pool\_type & Pooling type for embeddings & ``avg" \\
init\_method & Initialization method for combined weights & ``normal" \\
dropout & dropout & 0.1 \\
use\_norm & whether to use normalize & True \\
\bottomrule
\end{tabular}
\end{center}
\end{table}




\subsection{Metrics}
Throughout this paper, we use the following metrics to evaluate performance:

MSE (Mean Squared Error): Measures the average squared difference between the predicted and actual values. It penalizes larger errors more heavily, making it sensitive to outliers.
\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2,
\end{equation}
where $y_i$ and $\hat{y}_i$ denote the ground truth and predicted values, respectively, and $n$ is the number of data points.

MAE (Mean Absolute Error): Represents the average absolute difference between the predicted and actual values, providing a more interpretable measure of average error magnitude.
\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^n \left| y_i - \hat{y}_i \right|.
\end{equation}

RMSE (Root Mean Squared Error): The square root of MSE, which provides an error measure in the same units as the original data. It is more sensitive to large deviations than MAE.
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2}.
\end{equation}

MAPE (Mean Absolute Percentage Error): Expresses errors as a percentage of the actual values, offering a scale-independent metric that facilitates comparisons across datasets.
\begin{equation}
\text{MAPE} = \frac{1}{n} \sum_{i=1}^n \left| \frac{y_i - \hat{y}_i}{y_i} \right| \times 100.
\end{equation}

MSPE (Mean Squared Percentage Error): Similar to MAPE but squares the percentage error, penalizing larger percentage deviations more heavily.
\begin{equation}
\text{MSPE} = \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \hat{y}_i}{y_i} \right)^2.
\end{equation}

These metrics collectively provide a comprehensive evaluation of model performance, capturing both absolute and relative errors as well as their sensitivity to outliers. For all metrics, \textbf{lower values indicate better performance}.



\subsection{Implementation Details}
\label{ap: implementation details}

\subsubsection{Code and Reproducibility}
The code for the experiments is included in the supplementary material, accompanied by a comprehensive README file. We provide detailed commands, scripts, and instructions to facilitate running the code. Additionally, the datasets used in the experiments are provided in the supplementary material as CSV files.


\subsubsection{Hardware and Environment}
We conducted all experiments on an Ubuntu 22.04 machine equipped with an Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz, 1.5TB of RAM, and a 32GB NVIDIA V100 GPU. The CUDA version used was 12.4. All algorithms were implemented in Python (version 3.11.11). To run our code, users must install several commonly used libraries, including pandas, scikit-learn, patool, tqdm, sktime, matplotlib, transformers, and others. Detailed installation instructions can be found in the README file within the code directory. We have optimized our code to ensure efficiency. Our tests confirmed that the CPU memory usage remains below 16 GB, while the GPU memory usage is under 20 GB. Additionally, the execution time for a single experiment is less than 10 minutes on our machine.










\section{Full Experiment Results}
\label{ap: full results}


\subsection{Full Forecasting Performance Comparison Visualization}
\label{ap: full radar}
To provide a comprehensive comparison of different frameworks for modeling time series with paired texts, we visualize the forecasting performance using radar plots in Figure \ref{ap: full radar}. Each subfigure corresponds to a dataset, with each axis representing a different time series model. The axes are inverted, where values closer to the center indicate worse performance, and larger areas signify better results. The results demonstrate that TaTS consistently outperforms both baselines across all datasets while maintaining compatibility with various time series models.


\begin{figure*}[t]
\centering
\vspace{-3mm}
% Row 1
\subfigure[Agriculture]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Agriculture.png}
}
\subfigure[Climate]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Climate.png}
}
\subfigure[Economy]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Economy.png}
}
% Row 2
\subfigure[Energy]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Energy.png}
}
\subfigure[Environment]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Environment.png}
}
\subfigure[Health]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Health.png}
}
% Row 3
\subfigure[Security]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Security.png}
}
\subfigure[Social Good]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Social_Good.png}
}
\subfigure[Traffic]{
\includegraphics[width=0.31\textwidth]{figures/radar_plots/radar_plot_Traffic.png}
}
\caption{Comparison of different frameworks for modeling time series with paired texts. Our TaTS achieves the best performance across all datasets and is compatible with various existing time series models.}
\label{fig: full radar}
\vspace{-3mm}
\end{figure*}



\subsection{Full Forecasting Results}
\label{ap: full forecasting}
Due to space limitations, we provide the full results of the time series forecasting task on paired time series and text in the appendix. We conduct extensive experiments across 9 datasets using 9 existing time series models, evaluating various prediction lengths as detailed in Table \ref{tab: dataset details}. The complete results are presented from Table \ref{tab: full forecasting ace ipc} to Table \ref{tab: full forecasting sst ait}. Overall, TaTS consistently achieves the best performance across all datasets, time series models, and prediction lengths. The averaged results across all prediction lengths are summarized in the main text (Table \ref{tab: main forecasting}). For better readability, we also visualize the performance of different frameworks using radar plots, as detailed in Appendix \ref{ap: full radar} and Figure \ref{fig: full radar}.

\input{tables/table_full_result_1_filled}
\input{tables/table_full_result_2_filled}
\input{tables/table_full_result_3_filled}

\subsection{Full Imputation Results}
\label{ap: full imputation}
\input{tables/table_imputation_appendix}






\clearpage
\subsection{Full Hyperparameter Study Results of Learning Rate}
\label{ap: full hyperparameter learning rate}

\begin{figure*}[h]
\centering
\vspace{-3mm}
% Row 1
\subfigure[Agriculture]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Agriculture.png}
}
\subfigure[Climate]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Climate.png}
}
\subfigure[Economy]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Economy.png}
}
% Row 2
\subfigure[Energy]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Energy.png}
}
\subfigure[Environment]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Environment.png}
}
\subfigure[Health]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Health.png}
}
% Row 3
\subfigure[Security]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Security.png}
}
\subfigure[Social Good]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_SocialGood.png}
}
\subfigure[Traffic]{
\includegraphics[width=0.31\textwidth]{figures/learning_rate_hyperparameter_plots/learning_rate_parameter_Traffic.png}
}
\caption{Parameter study on the learning rate. We evaluate the impact of varying the learning rate in $\{0.00005, 0.0001, 0.00015, 0.0002, 0.00025, 0.0003\}$ by reporting the mean squared error (MSE) of our TaTS framework across datasets. The results demonstrate that TaTS maintains stable performance across different learning rate choices.}
\label{fig: full hyperparameter learning rate}
\vspace{-3mm}
\end{figure*}







\clearpage
\subsection{Full Hyperparameter Study Results of Text Embedding Dimension}
\label{ap: full hyperparameter text embedding dimension}

\begin{figure*}[h]
\centering
\vspace{-3mm}
% Row 1
\subfigure[Agriculture]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Agriculture.png}
}
\subfigure[Climate]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Climate.png}
}
\subfigure[Economy]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Economy.png}
}
% Row 2
\subfigure[Energy]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Energy.png}
}
\subfigure[Environment]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Environment.png}
}
\subfigure[Health]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Health.png}
}
% Row 3
\subfigure[Security]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Security.png}
}
\subfigure[Social Good]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_SocialGood.png}
}
\subfigure[Traffic]{
\includegraphics[width=0.31\textwidth]{figures/text_emb_hyperparameter_plots/text_emb_parameter_Traffic.png}
}
\caption{Parameter study on the projection dimension of paired texts. We vary the text projection dimension in $\{6, 12, 18, 24\}$ and report the mean squared error (MSE) of our TaTS framework across datasets. The results indicate that TaTS maintains robust performance across different choices of text projection dimensions.}
\label{fig: full hyperparameter text embedding dimension}
\vspace{-3mm}
\end{figure*}






\clearpage
\subsection{Full Ablation Study Results Using Different Text Encoders}
\label{ap: full ablation of text encoder}
We conduct experiments to evaluate the performance of our TaTS with multiple language encoders. Specifically, we evaluation TaTS with BERT-110M\footnote{\url{https://huggingface.co/google-bert/bert-base-uncased}}, GPT2-1.5B\footnote{\url{https://huggingface.co/openai-community/gpt2}} and LLaMA2-7B\footnote{\url{https://huggingface.co/meta-llama/Llama-2-7b}} as the language encoders. The results, presented in Figure \ref{fig: full ablation text encoder}, demonstrate that TaTS remains robust across different text encoders and consistently outperforms the baselines.

% google-bert/bert-base-uncased, openai-community/gpt2 and huggyllama/llama-7b

\begin{figure*}[h]
\centering
\vspace{-3mm}
% Row 1
\subfigure[Agriculture]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Agriculture.png}
}
\subfigure[Climate]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Climate.png}
}
\subfigure[Economy]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Economy.png}
}
% Row 2
\subfigure[Energy]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Energy.png}
}
\subfigure[Environment]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Environment.png}
}
\subfigure[Health]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Health.png}
}
% Row 3
\subfigure[Security]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Security.png}
}
\subfigure[Social Good]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_SocialGood.png}
}
\subfigure[Traffic]{
\includegraphics[width=0.31\textwidth]{figures/llm_ablation_plots/ablation_study_Traffic.png}
}
\caption{Performance comparison of different text encoders within the TaTS framework. Specifically, we evaluate BERT-110M, GPT2-1.5B, and LLaMA2-7B across multiple datasets using PatchTST (transformer-based model), DLinear (linear-based model), and FiLM (frequency-based model). The results demonstrate that TaTS maintains relatively stable performance across various models and datasets.}
\label{fig: full ablation text encoder}
\vspace{-3mm}
\end{figure*}



\clearpage
\subsection{Full Efficiency Results: Computational Overhead vs. Performance Gain Trade-offs}
\label{ap: full efficiency}
We conduct experiments to analyze the efficiency of TaTS, with results presented in Figure \ref{fig: full efficiency}. Each subfigure visualizes the training time per epoch and the forecasting mean squared error (MSE) for different time series models, represented as transparent colored scatter points. The average performance is computed and marked with cross markers: the green cross represents the average performance of TaTS, while the red and blue crosses indicate the average performance of the baseline models. As TaTS introduces a lightweight MLP and augments the original time series with auxiliary variables projected from paired texts, it incurs a slight computational overhead, with an average increase of $\sim 8\%$. Yet this trade-off results in a $\sim 14\%$ average improvement of forecasting MSE.

\input{1_Appendix/full_efficiency_figure}