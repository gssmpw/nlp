\section{Related Works}
\label{sec:related-works}

\paragraph{Self-Supervised Learning for Pre-Training}
A large number of studies have reported that SSL pre-training is competitive with, or outperforms, supervised learning models\cite{caron2020unsupervised, misra2020self,chen2020simple,he2020momentum, he2022masked}.  Early SSL approaches involved solving various pretext tasks such as colorization \cite{zhang2016colorful}, relative patch prediction \cite{doersch2015unsupervised}, rotation prediction \cite{gidaris2018unsupervised}, jigsaw puzzle solving \cite{noroozi2016unsupervised}, and image inpainting \cite{pathak2016context}. State-of-the-art SSL generally adopt one of two general strategies: contrastive or generative.  In contrastive approaches, models are trained to discriminate between positive and negative pairs and learn distinct representations.  Example methods include SimCLR \cite{chen2020simple}, MoCo \cite{he2020momentum, chen2020mocov2}, BYOL \cite{grill2020bootstrap}, and SwAV \cite{caron2020unsupervised}.  We adopt SwAV for our experiments due to its state-of-the-art performance, extensive use in previous research \cite{isprs-archives-XLIII-B3-2022-1399-2022, hakizimana2024enhanced, calhoun2022self, guldenring2021self}, and good transfer performance \cite{ericsson2021well}. Generative approaches, on the other hand, learn meaningful representations by masking patches in the input images and reconstructing the missing pixels. Popular generative methods include SimMIM \cite{xie2022simmim}, BEiT \cite{bao2021beit}, iGPT \cite{chen2020generative}, and Masked Autoencoders (MAE) \cite{he2022masked}. We adopt MAE for our experiments due to its state-of-the-art performance, scalability to large datasets, fast training speed, and widespread adoption \cite{cong2022satmae, tang2024cross, reed2023scale, lin2023ss}.

\paragraph{SSL for Remote Sensing}
RS data exhibit unique properties compared to natural imagery, which has motivated the development of RSP-specific methods, as well as large RS datasets to support domain-aligned RSP. A large number of publications have focused on developing RSP SSL methods, e.g. Tile2Vec \cite{jean2019tile2vec}, Seasonal Contrast (SeCo) \cite{manas2021seasonal}, Geography-Aware Self-Supervised Learning \cite{ayush2021geography}, and SatMAE \cite{cong2022satmae}.  We refer readers to \cite{wang2022self, tao2023self} for a more detailed discussion of SSL methods for RS. While these models leverage the temporal and multi-spectral features of RS data, this work focuses on evaluating the benefits of domain-aligned \textit{data}: i.e., whether using RS data offers advantages over natural imagery, when applying the same SSL method. Therefore, we restrict our analysis to models suitable for RGB images which can be pretrained on both GeoNet and ImageNet.  Multiple datasets have been designed for both supervised \cite{cornebise2022open,christie2018functional,lam2018xview,demir2018deepglobe,gupta2019creating,helber2019eurosat,johnson2022opensentinelmap,tao2023tov,bastani2023satlaspretrain,xiong2024earthnets} and SSL pre-training on satellite imagery \cite{shen2023firerisk, li2021geographical}.  Our study focuses on Sentinel-2 optical imagery, as it is the highest resolution imagery available globally, with high revisit rates, and for free. To use Sentinel-2 imagery for SSL, researchers have developed different methods for dataset curation. The SeCo dataset \cite{manas2021seasonal} comprises 1 million images from 200K locations, with sampling focused around cities. The dataset includes 5 images per location, each taken 3 months apart. The SSL4EO dataset \cite{wang2023ssl4eo} improved upon SeCo by removing overlapping imagery, and emphasizing geographic diversity, with 250K locations with 4 images per location, also totaling 1 million optical images.  In this work, we constructed GeoNet, which comprises 1 million images, emphasizing geographic diversity more than prior datasets, with the intent of maximizing the geographic robustness of models trained on it. The details of GeoNet are presented in \cref{sec:geonet_description}.   


\paragraph{Effectiveness of Domain-Aligned Pre-Training}
The benefits of domain-aligned pre-training for RS vision tasks remains inconclusive due to mixed conclusions about it in prior studies, and inconsistent or improper comparisons with INP as a baseline. For scene classification tasks, \cite{dimitrovski2024domain} found that RSP consistently, albeit narrowly, outperforms INP across tasks and transfer learning methods, including linear probing and fine-tuning. \cite{manas2021seasonal, wang2023ssl4eo, ayush2021geography} reached similar conclusions, although they generally used supervised  training on ImageNet as a baseline rather than INP. In contrast, other research found that the benefits of RSP are less evident. The results in \cite{isprs-archives-XLIII-B3-2022-1399-2022} suggest that RS pre-training can be beneficial for scene classification, especially when the pre-training and downstream datasets share common classes. However, INP remained competitive, performing best in the fine-tuning scenario. They achieved state-of-the-art results by using domain-adaptive pre-training, a two-stage SSL process, pre-training first on ImageNet and then on RS data. \cite{zhang2022consecutive} expanded on this idea and proposed Consecutive PreTraining, which involves performing INP then RSP. They found that it outperformed both supervised ImageNet and supervised in-domain pre-training, but did not compare to either INP or RSP. \cite{corley2024revisiting} found that for classification tasks, INP remains a more competitive baseline than previously thought, particularly when using consistent preprocessing. \cite{wang2022empirical} found that while RSP can be beneficial, the performance improvements depend significantly on the task and level of detail required. Notably, most of these studies use high-resolution RS datasets (e.g. Million-AID) for RSP, and many recommend that future work should explore the development of a large RS dataset for SSL. In this work, we build upon these studies by evaluating the benefits of in-domain data for SSL pre-training (specifically using Sentinel-2 imagery), as opposed to out-of-domain SSL pre-training (i.e. ImageNet), and we do so across both classification and the less-studied segmentation tasks.