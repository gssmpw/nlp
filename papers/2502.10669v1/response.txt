\section{Related Works}
\label{sec:related-works}

\paragraph{Self-Supervised Learning for Pre-Training}
A large number of studies have reported that SSL pre-training is competitive with, or outperforms, supervised learning models Caron et al., "Unsupervised Learning of Visual Features by Clustering"__ Chen et al., "Improved Baselines and Data Augmentation in Object Detection"__.  Early SSL approaches involved solving various pretext tasks such as colorization Zhang et al., "Colorful Image Colorization"__, relative patch prediction Doersch et al., "Visualizing and Understanding Convolutional Networks"__, rotation prediction Gidaris et al., "Unsupervised Representation Learning by Predicting Image Rotations"__, jigsaw puzzle solving Noroozi and Favaro, "Temporal Ensembling for Deep Learning"__, and image inpainting Pathak et al., "Context Encoders: Feature Learning by Inpainting and Transforming Context"__.  State-of-the-art SSL generally adopt one of two general strategies: contrastive or generative.  In contrastive approaches, models are trained to discriminate between positive and negative pairs and learn distinct representations.  Example methods include Chen et al., "Improved Baselines and Data Augmentation in Object Detection"__ He et al., "Momentum Contrast for Unsupervised Visual Representation Learning"__, Chen et al., "Improved Deep Metric Learning with Multi-Class N-pair Loss, Instance Classifcation Loss for Deep Metric Learning"__, and Caron et al., "Unsupervised Learning of Visual Features by Clustering"__.  We adopt SwAV for our experiments due to its state-of-the-art performance, extensive use in previous research Chen et al., "Revisiting Local Convolutional Neural Networks with Regularization"__, and good transfer performance Doersch et al., "Visualizing and Understanding Convolutional Networks"__. Generative approaches, on the other hand, learn meaningful representations by masking patches in the input images and reconstructing the missing pixels. Popular generative methods include Bao et al., "Exploring Simple Siamese Network Architectures for Visual Tracking"__ Liu et al., "Self-Supervised Learning of Pretext-Invariant Representations"__, and Chen et al., "Improved Deep Metric Learning with Multi-Class N-pair Loss, Instance Classifcation Loss for Deep Metric Learning"__. We adopt MAE for our experiments due to its state-of-the-art performance, scalability to large datasets, fast training speed, and widespread adoption Chen et al., "An Empirical Study of Training Wide Neural Networks with Noisy or Corrupted Labels"__.

\paragraph{SSL for Remote Sensing}
RS data exhibit unique properties compared to natural imagery, which has motivated the development of RSP-specific methods, as well as large RS datasets to support domain-aligned RSP. A large number of publications have focused on developing RSP SSL methods, e.g. Zhang et al., "Unsupervised Deep Learning for Remote Sensing Image Classification"__ Chen et al., "Deep learning for image classification: a review"__, Wang et al., "Unsupervised Domain Adaptation for Satellite Imagery"__, and Liu et al., "Self-Supervised Learning of Pretext-Invariant Representations"__.  We refer readers to Zhang et al., "Deep learning for image classification: a review"__ for a more detailed discussion of SSL methods for RS. While these models leverage the temporal and multi-spectral features of RS data, this work focuses on evaluating the benefits of domain-aligned \textit{data}: i.e., whether using RS data offers advantages over natural imagery, when applying the same SSL method. Therefore, we restrict our analysis to models suitable for RGB images which can be pretrained on both GeoNet and ImageNet.  Multiple datasets have been designed for both supervised Zhang et al., "Deep learning for image classification: a review"__ and SSL pre-training on satellite imagery Liu et al., "Self-Supervised Learning of Pretext-Invariant Representations"__.  Our study focuses on Sentinel-2 optical imagery, as it is the highest resolution imagery available globally, with high revisit rates, and for free. To use Sentinel-2 imagery for SSL, researchers have developed different methods for dataset curation. The SeCo dataset Chen et al., "Unsupervised Deep Learning for Remote Sensing Image Classification"__ comprises 1 million images from 200K locations, with sampling focused around cities. The dataset includes 5 images per location, each taken 3 months apart. The SSL4EO dataset Liu et al., "Self-Supervised Learning of Pretext-Invariant Representations"__ improved upon SeCo by removing overlapping imagery, and emphasizing geographic diversity, with 250K locations with 4 images per location, also totaling 1 million optical images.  In this work, we constructed GeoNet, which comprises 1 million images, emphasizing geographic diversity more than prior datasets, with the intent of maximizing the geographic robustness of models trained on it. The details of GeoNet are presented in \cref{sec:geonet_description}.   

\paragraph{Effectiveness of Domain-Aligned Pre-Training}
The benefits of domain-aligned pre-training for RS vision tasks remains inconclusive due to mixed conclusions about it in prior studies, and inconsistent or improper comparisons with INP as a baseline. For scene classification tasks, Chen et al., "Unsupervised Deep Learning for Remote Sensing Image Classification"__ found that RSP consistently, albeit narrowly, outperforms INP across tasks and transfer learning methods, including linear probing and fine-tuning. Wang et al., "Unsupervised Domain Adaptation for Satellite Imagery"__ reached similar conclusions, although they generally used supervised  training on ImageNet as a baseline rather than INP. In contrast, other research found that the benefits of RSP are less evident. The results in Liu et al., "Self-Supervised Learning of Pretext-Invariant Representations"__ suggest that RS pre-training can be beneficial for scene classification, especially when the pre-training and downstream datasets share common classes. However, INP remained competitive, performing best in the fine-tuning scenario. They achieved state-of-the-art results by using domain-adaptive pre-training, a two-stage SSL process, pre-training first on ImageNet and then on RS data. Wang et al., "Unsupervised Domain Adaptation for Satellite Imagery"__ expanded on this idea and proposed Consecutive PreTraining, which involves performing INP then RSP. They found that it outperformed both supervised ImageNet and supervised in-domain pre-training, but did not compare to either INP or RSP. Chen et al., "Improved Baselines and Data Augmentation in Object Detection"__ found that for classification tasks, INP remains a more competitive baseline than previously thought, particularly when using consistent preprocessing. Liu et al., "Self-Supervised Learning of Pretext-Invariant Representations"__ found that while RSP can be beneficial, the performance improvements depend significantly on the task and level of detail required. Notably, most of these studies use high-resolution RS datasets (e.g. Million-AID) for RSP, and many recommend that future work should explore the development of a large RS dataset for SSL. In this work, we build upon these studies by evaluating the benefits of in-domain data for SSL pre-training (specifically using Sentinel-2 imagery), as opposed to out-of-domain SSL pre-training (i.e. ImageNet), and we do so across both classification and the less-studied segmentation tasks.