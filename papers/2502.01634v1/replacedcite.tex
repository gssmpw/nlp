\section{Related Work}
\vspace{-.02in}
\textbf{Incremental Learning} is a technique in machine learning that involves the gradual integration of new data into an existing model, continuously learning from the latest data to ensure performance on new data____. It has been a open problem in machine learning, and has been studied in convolutional neural network (CNN)____, DNN____, SVM____ and RF____. In gradient boosting, iGBDT offers incremental updates____, while other methods____ extend GB to online learning. However, these methods do not support removing data.


\textbf{Decremental Learning} allows for the removal of trained data and eliminates their influence on the model, which can be used to delete outdated or privacy-sensitive data____. It has been researched in various models, including CNN____, DNN____, SVM____, Naive Bayes____, K-means____, RF____, and GB____. In random forests, DaRE____ and a decremental learning algorithm____ are proposed for data removal with minimal retraining and latency.

However, in GBDT, trees in subsequent iterations rely on residuals from previous iterations, making decremental learning more complicated. DeltaBoost ____ simplified the dependency for data deletion by dividing the dataset into disjoint sub-datasets, while a recent study ____ proposed an efficient unlearning framework without simplification, utilizing auxiliary information to reduce unlearning time. Although effective, its performance on large datasets remains unsatisfactory.



\vspace{-.08in}