


%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxiv_template}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{multirow}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{caption}
\usepackage[hang, flushmargin]{footmisc}
\usepackage{comment}

\usepackage{titling}
\setlength{\droptitle}{-.35in}

\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}


\newcommand{\hlc}[2][yellow]{{%
    \colorlet{foo}{#1}%
    \sethlcolor{foo}\hl{#2}}%
}

%\usepackage{amssymb} %% <- for \square and \blacksuare

\newcommand*{\QEDA}{\null\nobreak\hfill\ensuremath{\blacksquare}}%
\newcommand*{\QEDB}{\null\nobreak\hfill\ensuremath{\square}}%

\newcounter{definitioncounter}
\newcommand\showdefinitioncounter{\refstepcounter{definitioncounter}\thedefinitioncounter}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\title{\textbf{Online Gradient Boosting Decision Tree: \\In-Place Updates for Efficient Adding/Deleting Data}}

\date{}

\author{
Huawei Lin $^{1}$\hspace{.2in}
Jun Woo Chung $^{1}$\hspace{.2in}
Yingjie Lao $^{2}$\hspace{.2in}
Weijie Zhao $^{1}$\\
$^1$\ Rochester Institute of Technology\hspace{.2in}
$^2$\ Tufts University\\
\texttt{\{hl3352, jc4303\}@rit.edu}\hspace{.2in}
\texttt{Yingjie.Lao@tufts.edu}\hspace{.2in}
\texttt{wjz@cs.rit.edu}
}

\begin{document}


\maketitle

\begin{abstract}
Gradient Boosting Decision Tree (GBDT) is one of the most popular machine learning models in various applications. However, in the traditional settings, all data should be simultaneously accessed in the training procedure: it does not allow to add or delete any data instances after training. In this paper, we propose an efficient online learning framework for GBDT supporting both incremental and decremental learning. To the best of our knowledge, this is the first work that considers an in-place unified incremental and decremental learning on GBDT. To reduce the learning cost, we present a collection of optimizations for our framework, so that it can add or delete a small fraction of data on the fly. We theoretically show the relationship between the hyper-parameters of the proposed optimizations, which enables trading off accuracy and cost on incremental and decremental learning. The backdoor attack results show that our framework can successfully inject and remove backdoor in a well-trained model using incremental and decremental learning, and the empirical results on public datasets confirm the effectiveness and efficiency of our proposed online learning framework and optimizations.
\end{abstract}

% \vspace{-.2in}
\section{Introduction}
\label{sec:intro}
Gradient Boosting Decision Tree (GBDT) has demonstrated outstanding performance across a wide range of applications~\cite{DBLP:journals/ml/BiauCR19, DBLP:journals/asc/RaoSRFXEYG19, DBLP:conf/iccv/LiuY07, DBLP:journals/corr/abs-2412-08637}. It outperforms deep learning models on many datasets in accuracy and provides interpretability for the trained models~\cite{DBLP:journals/gandc/SudakovBK19,  DBLP:journals/corr/abs-2108-08233, DBLP:journals/jmlr/WenLSLH020, DBLP:journals/corr/abs-1810-11363}. However, in traditional setting, all data is simultaneously accessed in training procedure, which makes its application scenarios limited. 

\textbf{Online Learning.} Online learning is a machine learning approach where data is sequentially available and used to update the predictor for the latest data~\cite{DBLP:journals/corr/Bertsekas15c, DBLP:journals/nn/ParisiKPKW19, DBLP:journals/ftopt/Hazan16, DBLP:conf/smc/Oza05}. Generally, online learning is expected to possess the capabilities of both incremental learning (adding training data) and decremental learning (removing a subset of training data). This allows the model to dynamically adapt to the latest data while removing outdated data. For example, recommender system can incrementally learn latest user behaviors and remove outdated behaviors without training from scratch~\cite{DBLP:conf/aaai/Wang0VT0HC23, DBLP:conf/cikm/Shi0X0F0024}.

\textbf{Incremental Learning.}
There are some challenges for incremental learning in GBDT due to its natural properties~\cite{Article:FHT_AS00, DBLP:journals/corr/abs-2205-10927, DBLP:journals/corr/abs-2207-08770}. Traditional GBDT trains over an entire dataset, and each node is trained on the data reaching it to achieve the best split for optimal accuracy. Adding unseen data may affect node splitting results, leading to catastrophic performance changes.


Moreover, training gradient boosting models involves creating trees for each iteration, with tree fitting based on the residual of previous iterations. More iterations create more trees, increasing model sizes and hurting inference throughput. This also prohibits tasks like fine-tuning or transfer learning without substantially increasing model sizes.

Recent studies have explored incremental learning on classic machine learning, such as support vector machine (SVM), random forest (RF), and gradient boosting (GB). \citet{DBLP:journals/tnn/ShiltonPRT05, DBLP:journals/jmlr/LaskovGKM06, DBLP:conf/nips/FineS01} proposed methods to maintain SVM optimality after adding a few training vectors. \citet{DBLP:conf/icip/WangWCL09} presented an incremental random forest for online learning with small streaming data. \citet{DBLP:conf/nips/BeygelzimerHKL15} extended gradient boosting theory for regression to online learning. \citet{DBLP:journals/npl/ZhangZSAFS19} proposed iGBDT for incremental learning by ``lazily'' updating, but it may require retraining many trees when the new data size is large. It is important to note that prior studies on online gradient boosting~\cite{DBLP:conf/nips/BeygelzimerHKL15, DBLP:conf/icml/ChenLL12, DBLP:conf/icml/BeygelzimerKL15} and incremental gradient boosting~\cite{DBLP:journals/npl/ZhangZSAFS19, DBLP:conf/aistats/HuSVHB17} do not support decremental learning.

\textbf{Decremental Learning.} Decremental learning is more complex and less studied than incremental learning. \citet{DBLP:conf/nips/CauwenberghsP00} presented an online recursive algorithm for training SVM with an efficient decremental learning method. \citet{DBLP:journals/cluster/ChenXXZ19} proposed online incremental and decremental learning algorithms based on variable SVM, leveraging pre-calculated results. \citet{DBLP:conf/icml/BrophyL21} and \citet{DBLP:journals/corr/abs-2009-05567} provided methods for data addition and removal in random forests. \citet{DBLP:conf/sigmod/SchelterGD21} proposed robust tree node split criteria and alternative splits for low-latency unlearning. Many works have also studied decremental learning in deep neural networks (DNN). \citet{DBLP:conf/sp/BourtouleCCJTZL21} introduced a framework that accelerates decremental learning by constraining individual data points' impact during training.

While online learning has emerged as a popular topic recently, it has been barely investigated on GBDT. \citet{DBLP:journals/pacmmod/WuZLH23, DBLP:conf/kdd/LinCL023} are among the latest studies in decremental learning for GBDT. \citet{DBLP:journals/pacmmod/WuZLH23} presented DeltaBoost, a GBDT-like model enabling data deletion. DeltaBoost divides the training dataset into several disjoint sub-datasets, training each iteration's tree on a different sub-dataset, reducing the inter-dependency of trees. However, this simplification may impact model performance. \citet{DBLP:conf/kdd/LinCL023} proposed an unlearning framework in GBDT without simplification, unlearning specific data using recorded auxiliary information from training. It optimizes to reduce unlearning time, making it faster than retraining from scratch, but introduces many hyper-parameters and performs poorly on extremely large datasets.


In this paper, we propose an efficient incremental and decremental learning framework for GBDT. To the best of our knowledge, this is the first work that considers in-place incremental and decremental learning at the same time on GBDT. Additionally, our incremental and decremental learning applies a unified notion, enabling convenient implementation.

% \textbf{Goals.} For incremental learning, our framework aims to: 1) leverage previously learned knowledge; 2) accumulate knowledge from each training step; 3) learn a small data fraction faster than retraining from scratch. For decremental learning, our goals are: 1) delete data and their effect on the model; 2) delete a small data fraction faster than retraining from scratch.

\textbf{Challenges.}
We identify three major challenges of in-place online learning for GBDT: (1) Unlike batch training of deep neural networks (DNN), more iterations in GBDT create more trees and parameters, leading to unbounded memory and computation costs in online learning. In-place learning on originally constructed trees is necessary for practicality. (2) Gradient-based methods in DNN add/subtract gradients for incremental and decremental learning, but GBDT is not differentiable. (3) GBDT depends on the residual of the previous tree, unlike independent iterations in random forests. Changing one tree requires modifying all subsequent trees, complicating incremental and decremental learning.

\textbf{Contributions.}
(1) We introduce an efficient in-place online learning framework for gradient boosting models supporting incremental and decremental learning, extensible to fine-tuning and transfer learning. (2) We present optimizations to reduce the cost of incremental and decremental learning, making adding or deleting a small data fraction substantially faster than retraining. (3) We theoretically show the relationship among optimization hyper-parameters, enabling trade-offs between accuracy and cost. (4) We experimentally evaluate our framework on public datasets, confirming its effectiveness and efficiency. (5) We release an open-source implementation of our framework\footnote{\href{https://github.com/huawei-lin/InplaceOnlineGBDT}{https://github.com/huawei-lin/InplaceOnlineGBDT}}.

\begin{figure}[t]
\vspace{-.11in}
\begin{algorithm}[H]
\footnotesize
\caption{Robust LogitBoost Algorithm.}
\label{alg:robust_LogitBoost}
\begin{algorithmic}[1]
    \STATE $F_{i,k} = 0$, $p_{i,k} = \frac{1}{K}$, $k = 0$ to  $K-1$, $i = 1$ to $N$
    \FOR{$m=0$ to $M-1$}
        \FOR{$k=0$ to $K-1$}
          \STATE $\hat{D_\textit{tr}} = \{r_{i,k} - p_{i,k}, \ \ \mathbf{x}_{i}\}_{i=1}^N$
          \STATE $w_{i,k}=p_{i,k}(1-p_{i,k})$
          \STATE  $\left\{R_{j,k,m}\right\}_{j=1}^J = J$-terminal node regression tree from
         $\hat{D_\textit{tr}}$,  with weights $w_{i,k}$, using the tree split gain formula Eq.~\eqref{eqn:logit_gain}.
          \STATE $\beta_{j,k,m} = \frac{K-1}{K}\frac{ \sum_{\mathbf{x}_i \in
          R_{j,k,m}} r_{i,k} - p_{i,k}}{ \sum_{\mathbf{x}_i\in
          R_{j,k,m}}\left(1-p_{i,k}\right)p_{i,k} }$ \label{line:predict_v}
          \STATE $f_{i,k} = \sum_{j=1}^J\beta_{j,k,m}1_{\mathbf{x}_i\in R_{j,k,m}}, \hspace{0.1in}F_{i,k} = F_{i,k} + \nu f_{i,k}$
        \ENDFOR
        \STATE $p_{i,k} = \exp(F_{i,k})/\sum_{s=1}^{K}\exp(F_{i,s})$\\
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-.2in}
\end{figure}


\begin{figure}[t]
\vspace{-.22in}
\begin{algorithm}[H]
\footnotesize
\caption{Online Learning in Gradient Boosting}
\label{alg:unlearning_model}
\begin{algorithmic}[1]
    \STATE $D' = D_{\textit{in}}$ \textbf{if} incremental learning \textbf{else} $D_{\textit{de}}$
    \FOR{$m=0$ to $M-1$}
        \FOR{$k=0$ to $K-1$}
            \STATE $\hat{D'} = \{r_{i,k} - p_{i,k}, \ \ \mathbf{x}_{i}\}_{i=1}^{|D'|}$
            \STATE Compute $w_{i,k}=p_{i,k}(1-p_{i,k})$ for $\hat{D'}$ using $F_{i,k}$ \label{line:recompute_hr}
            \STATE Compute $r_{i,k}$ for $\hat{D'}$ using $F_{i,k}$ \label{line:recompute_r}
            \IF{incremental learning}
                \STATE $\left\{\hat{R}_{j,k,m}\right\}_{j=1}^J$ = incr($\left\{R_{j,k,m}\right\}_{j=1}^J$, $\hat{D'}$, $w_{i,k}$, $r_{i,k}$) 
            \ELSE
                \STATE $\left\{\hat{R}_{j,k,m}\right\}_{j=1}^J$ = decr($\left\{R_{j,k,m}\right\}_{j=1}^J$, $\hat{D'}$, $w_{i,k}$, $r_{i,k}$) 
            \ENDIF
            \label{line:unlearn_tree}
            \STATE Update $F_{i,k}$ with $\left\{\hat{R}_{j,k,m}\right\}_{j=1}^J$
        \ENDFOR
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-.38in}
\end{figure}

\vspace{-.08in}
\section{Online GBDT Framework}

\subsection{GBDT Preliminary}

Gradient Boosting Decision Tree (GBDT) is an powerful ensemble technique that combines multiple decision tree to produce an accurate predictive model~\cite{Article:FHT_AS00,Article:Friedman_AS01}. Given a dataset $D_{\textit{tr}} = \{y_i,\mathbf{x}_i\}_{i=1}^N$, where $N$ is the size of training dataset, and $\mathbf{x}_i$ indicates the $i^{\textit{th}}$ data vector and $y_i \in \{0, 1, ..., K - 1\}$ denotes the label for the $i^{\textit{th}}$ data point. For a GBDT model with $M$ iteration, the probability $p_{i,k}$ for $i^{\textit{th}}$ data and class $k$ is:
% \vspace{-.1in}
{\footnotesize
\begin{align}\label{eqn_logit}
p_{i,k} = \mathbf{Pr}\left(y_i = k|\mathbf{x}_i\right) = \frac{e^{F_{i,k}(\mathbf{x_i})}}{\sum_{s=1}^{K} e^{F_{i,s}(\mathbf{x_i})}},\hspace{0.2in} i = 1, 2, ..., N
\end{align}
% \vspace{-.1in}
}
where $F$ is a combination of M terms:
{\footnotesize
\begin{align}
F^{(M)}(\mathbf{x}) = \sum_{m=0}^{M - 1} \rho_m h(\mathbf{x};\mathbf{a}_m)
\end{align}
\vspace{-.15in}
}

where $h(\mathbf{x};\mathbf{a}_m)$ is a regression tree, and $\rho_m$ and $\mathbf{a}_m$ denote the tree parameters that learned by minimizing the {\em negative log-likelihood}:

\vspace{-.2in}
{
\footnotesize
\begin{align}\label{eqn_loss}
L = \sum_{i=1}^N L_i, \hspace{0.4in} L_i = - \sum_{k=0}^{K - 1}r_{i,k}  \log p_{i,k}
\end{align}
\vspace{-.1in}
}

where $r_{i,k} = 1$ if $y_i = k$, otherwise, $r_{i,k} = 0$. The training procedures require calculating the derivatives of loss function $L$ with respect to $F_{i,k}$:
\vspace{-.1in}
{\footnotesize
\begin{align}\label{eqn:logit_d1d2}
&g_{i, k} = \frac{\partial L_i}{\partial F_{i,k}} = - \left(r_{i,k} - p_{i,k}\right),
\hspace{0.1in}
h_{i, k} = \frac{\partial^2 L_i}{\partial F_{i,k}^2} = p_{i,k}\left(1-p_{i,k}\right).
\end{align}
\vspace{-.18in}
}

In GBDT training, to solve numerical instability problem~\cite{Article:FHT_AS00,Article:Friedman_AS01,Article:FHT_JMLR08}, we apply \textbf{Robust LogitBoost} algorithm~\cite{Proc:ABC_UAI10} as shown in Algorithm~\ref{alg:robust_LogitBoost}, which has three parameters, the number of terminal nodes $J$, the shrinkage $\nu$ and the number of boosting iterations $M$. To find the optimal split for a decision tree node, we first sort the $N$ data by the feature values being considered for splitting.  We then iterate through each potential split index $s$, where $1 \leq s < N$, to find the best split that minimizes the weighted squared error (SE) between the predicted and true labels. Specifically, we aim to find an split $s$ to maximize the gain function:

\vspace{-.2in}
{\footnotesize
\begin{align}\label{eqn:logit_gain}
Gain(s) =&  \frac{\left(\sum_{i=1}^s g_{i,k} \right)^2}{\sum_{i=1}^s h_{i,k}}+\frac{\left(\sum_{i=s+1}^N g_{i,k} \right)^2}{\sum_{i=s+1}^{N} h_{i,k}}- \frac{\left(\sum_{i=1}^N g_{i,k} \right)^2}{\sum_{i=1}^N h_{i,k}}.
\end{align}
}

\vspace{-.20in}
\subsection{Problem Setting} \label{ssec:problem_setting}
For classic GBDT, all training data must be loaded during training, and adding/deleting instances is not allowed afterwards. This work proposes an online GBDT framework that enables in-place addition/deletion of specific data instances to/from a well-trained model through incremental and decremental learning.


\begin{figure*}[t!]
\centering
\includegraphics[width=.9\textwidth]{figs/OnlineGBDToverview.pdf}
\vspace{-.09in}
\caption{An example for the incremental learning and decremental learning procedure in the proposed framework. (a) For the node of \texttt{Loan < 31}, the current split is still the best after online learning. Thus, the split does not need to change. (b) An already well-trained tree in $D_\textit{tr}$. (c) For the node of \texttt{Auto < 57}, the best split has shifted after online learning. (d) Incremental update for derivatives -- only update the derivatives for those data reaching the changed terminal nodes.}\label{fig:overview}
\vspace{-.2in}
\end{figure*}

\textbf{Problem Statement.} Given a trained gradient boosting model $T(\theta)$ on training dataset $D_{\textit{tr}}$, where $\theta$ indicates the parameters of model $T$, an incremental learning dataset $D_{\textit{in}}$, and/or a decremental learning dataset $D_{\textit{de}}$ ($D_{\textit{de}} \subseteq D_{\textit{tr}}$), our goal is to find a tree model $T(\theta')$ that fits dataset $D_{\textit{tr}} \cup D_{\textit{in}} \setminus D_{\textit{de}}$, where $|\theta| = |\theta'|$ (the parameter size and the number of trees stay unchanged).

The most obvious way is to retrain the model from scratch on dataset $D_{\textit{tr}} \cup D_{\textit{in}} \setminus D_{\textit{de}}$. However, retraining is time-consuming and resource-intensive. Especially for online learning applications, rapid retraining is not practical. The key question of this problem is:
\textit{\textbf{Can we obtain the model $T(\theta')$ based on the learned knowledge of the original model $T(\theta)$ without retraining the entire model?}}

The proposed framework aims to find a tree model $T(\theta')$ as close to the model retraining from scratch as possible based on the learned knowledge of the model $T(\theta)$. In addition, this online learning algorithm is in a ``warm-start'' manner, because it learns a new dataset $D_\textit{in}$ or removes a learned sub-dataset $D_\textit{de} \subseteq D_{\textit{tr}}$ on a model that is already well-trained on training dataset $D_{\textit{tr}}$.

Let $\mathcal{A}$ denotes the initial GBDT learning algorithm , then we have $\mathcal{A}(D_{\textit{tr}}) \in \mathcal{H}$, where $\mathcal{H}$ is the hypothesis space. An online learning algorithm $\mathcal{L}$ for incremental learning or decremental learning can be used to learn dataset $D_\textit{in}$ or remove dataset $D_\textit{de} \subseteq D_{\textit{tr}}$. 

\vspace{-.08in}
\subsection{Framework Overview}
\vspace{-.02in}
The goal of this work is to propose an online learning framework for GBDT that supports incremental learning and decremental learning for any collection of data. 
%Here the decremental learning is the inverse process of incremental learning.




\textbf{Online Learning in GBDT.}
The Algorithm~\ref{alg:unlearning_model} shows the procedure of online learning in GBDT. At first, the GBDT model is a well-trained model on the training dataset $D_{\textit{tr}}$. Recall that the GBDT model is frozen and can not be changed after training---no training data modification. In this proposed framework, the user can do (1) incremental learning: update a new dataset $D_{\textit{in}}$ to the model, and (2) decremental learning: remove a learned dataset $D_\textit{de} \subseteq D_{\textit{tr}}$ and its effect on the model.

As shown in Algorithm~\ref{alg:unlearning_model}, it is similar to the learning process, but it only needs to compute $r_{i,k}$ and $p_{i,k}(1 - p_{i,k})$ for online dataset $D'$ without touching the training dataset $D_{\textit{tr}}$. Then, it will call the function of incremental learning or decremental learning to obtain {\small $\left\{\hat{R}_{j,k,m}\right\}_{j=1}^J$}. Finally, we update $F_{i,k}$ with new {\small $\left\{\hat{R}_{j,k,m}\right\}_{j=1}^J$}. Here we use the same notion to design the function of incremental learning and decremental learning -- decremental learning is the inverse process of incremental learning for dataset $D'$. Therefore, we describe them in the Algorithm~\ref{alg:unlearning_tree} at the same time.

\begin{figure}[htbp]
\vspace{-.15in}
\begin{algorithm}[H]
\footnotesize
\caption{Incr./Decr. Learning on One Tree}
\label{alg:unlearning_tree}
\begin{algorithmic}[1]
    \FOR{non-terminal $\textit{node}$ \textbf{in} $\left\{R_{j,k,m}\right\}_{j=1}^J$ with ascending depths} \label{line:traverse}
        \STATE $\hat{D'} = \{r_{i,k} - p_{i,k}, \ \ \mathbf{x}_{i}\}_{i=1}^{|D'|}$
        \STATE $s$ = current split of $\textit{node}$
        \STATE $s'$ = compute best gain with Eq.~\eqref{eqn:logit_gain} with $r_{i,k}$ and $w_{i,k}$ after adding/removing $\hat{D'}$
        \IF{$s' \not= s$}
            \STATE Retrain the subtree rooted at $\textit{node}$.
        \ENDIF
    \ENDFOR
    \STATE Update prediction value $\beta_{j,k,m}$ for all terminal nodes
\end{algorithmic}
\end{algorithm}
\vspace{-.4in}
\end{figure}


\textbf{Incremental \& Decremental Learning on One Tree.} Algorithm~\ref{alg:unlearning_tree} describes the detailed process for incremental and decremental learning, which are almost the same as decremental learning is the inverse of incremental learning for dataset $D'$. The main difference is at Line 3. First, we traverse all non-terminal nodes layer by layer from root to leaves. For each node, let $s$ denote the current split. We recompute the new best gain value with $r_{i,k}$ and $p_{i,k}(1 - p_{i,k})$ after adding $D'$ for incremental learning or removing $D'$ for decremental learning. If the current split $s$ matches the new best split $s'$ (after adding/removing $D'$), we keep the current split (Figure~\ref{fig:overview}(a)). Otherwise, if the current best split has changed ($s \not= s'$, Figure~\ref{fig:overview}(c)), we retrain the sub-tree rooted on this node and replace it with the new sub-tree. After testing all nodes, node splits remain on the best split. Finally, we recompute the prediction value on all terminal nodes. Appendix~\ref{apd:overview_example} provides a detailed explanation of Figure~\ref{fig:overview}.

\vspace{-.1in}
\section{Optimizing Learning Time}
\vspace{-.05in}

In this section, we introduce optimizations for our online learning framework to reduce computation overhead and costs. The key step is deciding whether a node should be kept or replaced: \textit{Can we design an algorithm to quickly test whether the node should be retained or retrained without touching the training data?} Our most important optimization is to avoid touching the full training dataset. We apply incremental update and split candidates sampling concepts from \cite{DBLP:conf/kdd/LinCL023}, extend them to support online learning, and provide evidence of the relationship between hyper-parameters of different optimizations, enabling trade-offs between accuracy and cost. Additionally, we design optimizations specific to online learning: 1) adaptive lazy update for residuals and hessians to substantially decrease online learning time; 2) adaptive split robustness tolerance to significantly reduce the number of retrained nodes.

\vspace{-.08in}
\subsection{Update without Touching Training Data}
\vspace{-.02in}

To reduce computation overhead and online learning time, we target to avoid touching the original training dataset $D$, and only focus on the online learning dataset $D'$. Following the study~\cite{DBLP:conf/kdd/LinCL023}, we extend the optimization of updating statistical information to the scenarios of online learning: (1) Maintain Best Split; (2) Recomputing Prediction Value; (3) Incremental Update for Derivatives, and the computation cost is reduced from $O(D \pm D')$ to $O(D')$ by these optimizations. The implementation of these optimizations are included in Appendix~\ref{apd:update_without_touching_training_data}.

\vspace{-.08in}
\subsection{Adaptive Lazy Update for Derivatives}\label{sec:lazy_update}
\vspace{-.02in}

Although incremental update can substantially reduce online learning time, we can take it a step further: if no retraining occurs, the changes to the derivatives will be very small. \textit{How can we effectively utilize the parameters already learned to reduce online learning time?}


Gradient Accumulation~\cite{DBLP:conf/kdd/LiZCS14, DBLP:journals/corr/GoyalDGNWKTJH17, DBLP:journals/corr/Ruder16, DBLP:conf/fgr/LinLLS23, DBLP:journals/access/QiLLC21} is widely used in DNN training. After computing the loss and gradients for each mini-batch, the system accumulates these gradients over multiple batches instead of updating the model parameters immediately. Inspired by Gradient Accumulation techniques, we introduce an adaptive lazy update for our online learning framework. Unlike \citet{DBLP:conf/kdd/LinCL023}, which perform updates after a fixed number of batches, we update the derivatives only when retraining occurs. This approach uses more outdated derivatives for gain computation but significantly reduces the cost of derivative updates.

\vspace{-.08in}
\subsection{Split Candidates Sampling}\label{ssec:split_sampling}
\vspace{-.02in}

From the above optimizations, if retraining is not required, we can keep the current best split. In this case, we only need to iterate over the online learning dataset $D'$ and update the prediction values to accomplish online learning, whether it involves adding or removing data.
However, if the sub-tree rooted in this node requires retraining, it is necessary to train the new sub-tree on the data from the dataset $D_\textit{tr} \pm D'$ that reaches this node.
It is clear that retraining incurs more resource consumption and takes a longer execution time.
In the worst case, if retraining is required in the root node, it has to retrain the entire new tree on full dataset $D_\textit{tr} \pm D'$.




To reduce time and resource consumption of online learning, a straightforward approach is to minimize retraining frequency. Therefore, we introduce split candidate sampling to reduce frequent retraining by limiting the number of splits, benefiting both training and online learning. All features are discretized into integers in ${0,1,2,\cdots,B-1}$, as shown in Appendix~\ref{apd:gbdt_fd}. The original training procedure enumerates all $B$ potential splits, then obtains the best split with the greatest gain value. In split candidates sampling, we randomly select $\lceil\alpha B\rceil$ potential splits as candidates and only perform gain computing on these candidates. As $\alpha$ decreases, the number of split candidates decreases, resulting in larger distances between split candidates. Consequently, the best split is less likely to change frequently.


%\vspace{.1in}
\noindent{\bf Definition \showdefinitioncounter} (Distance Robust) {\em Let $s$ be the best split, and $\frac{|D'|}{|D_{\textit{tr}}|} = \lambda$. $N_\Delta$ is the distance between $s$ and its nearest split $t$ with same feature, $N_\Delta=||t-s||$. $s$ is distance robust if}
\vspace{-.01in}
{\footnotesize
\begin{align}
N_\Delta > \frac{\lambda Gain(s)}{\frac{1}{N_{ls}}\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}} + \frac{1}{N_{rs}}\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}}}
\end{align}
\vspace{-.15in}
}

where $l$ represents the left child of split $s$, and it contains the samples belonging to this node, while $r$ represent the right child, $N_{ls}$ denotes $\left|l_s\right|$, and $N_{rs}$ denotes $\left|r_s\right|$. In this definition, $\mathbb{E}(N_\Delta) = 1/\alpha$, where $\alpha$ denotes the split sampling rate, we can observe that a smaller sampling rate will result in a more robust split, so we can reduce the number of retrain operations by reducing the sampling rate. Similarly, incremental learning can get the same result.


%\vspace{.1in}
\noindent{\bf Definition \showdefinitioncounter} (Robustness Split) {\em For a best split $s$ and an arbitrary split $t, t \neq s$, and online learning data rate $\frac{|D'|}{|D_\textit{tr}|} = \lambda$, the best split $s$ is robust split if}
\vspace{-.05in}
{\footnotesize
\begin{align}
Gain(s) > \frac{1}{1-\lambda}Gain(t)
\end{align}
\vspace{-.2in}
}

Robustness split shows that, as $\lambda = \frac{|D'|}{|D_\textit{tr}|}$ decreases, the splits are more robust, decreasing the frequency of retraining. In conclusion, decreasing either $\alpha$ or $\lambda$ makes the split more robust, reducing the change occurrence in the best split, and it can significantly reduce the online learning time. We provide the proof of \textit{Distance Robust} and \textit{Robustness Split} in Appendix~\ref{apd:split_sampling_app}.


\begin{figure}[htbp]
\vspace{-.05in}
\mbox{
\hspace{.15in}
\includegraphics[width=.2\textwidth]{figs/adult_incre_split_order.pdf}
\includegraphics[width=.2\textwidth]{figs/covtype_incre_split_order.pdf}
}
\mbox{
\hspace{.15in}
%\hspace{-.12in}
\includegraphics[width=.2\textwidth]{figs/letter_decre_split_order.pdf}
\includegraphics[width=.2\textwidth]{figs/SUSY_decre_split_order.pdf}
}
\vspace{-.13in}
\caption{Observation of distance of best split changes. The lines represents the average changes of best split distance, and the shaded region is the standard error.}
\label{fig:split_change_observation}
\vspace{-.1in}
\end{figure}


\input{tables/dataset_tbl}


\vspace{-.08in}
\subsection{Adaptive Split Robustness Tolerance}
\vspace{-.02in}

Recall the retraining condition for a node that we mentioned previously: we retrain the sub-tree rooted at a node if the best split changes.  Although the best split may have changed to another one, the gain value might only be slightly different from the original best split. We show the observation of the distance of best split changes (the changes in the ranking of the best split) in Figure~\ref{fig:split_change_observation}. The top row illustrates the distance of best split changes observed in the Adult and Covtype datasets for incremental learning, while the bottom row depicts same in Letter and SUSY datasets for decremental learning. Similar patterns are observed across various other datasets.
For adding or deleting a single data point, the best split does not change in most cases. As the $|D'|$ increases to $0.1\%$, $0.5\%$, and $1\%$, the best split in most cases switch to the second best.
If we only apply the optimal split, it will lead to frequent retraining during online learning.


The distance of the best split changes is usually small. Tolerating its variation within a certain range and continuing to use the original split significantly accelerates online learning. We propose adaptive split robustness tolerance: for a node with $\lceil\alpha B\rceil$ potential splits, if the current split is among the top $\lceil\sigma\alpha B\rceil$, we continue using it, where $\sigma\ (0 \leq \sigma \leq 1)$ is the robustness tolerance. $\sigma\ = 0$ selects only the best split, while $\sigma\ = 1$ avoids retraining. Higher $\sigma$ indicates greater tolerance, making the split more robust and less likely to retrain. We recommend setting $\sigma$ to approximately $0.1$.



\input{tables/time_mem_tbl} 


\vspace{-.08in}
\section{Experimental Evaluation}\label{sec:experimental_eval}
\vspace{-.02in}

In this section, we compare 1) our incremental learning with OnlineGB\footnote{\href{https://github.com/charliermarsh/online_boosting}{https://github.com/charliermarsh/online\_boosting}}~\cite{DBLP:conf/iccvw/LeistnerSRB09} and iGBDT~\cite{DBLP:journals/npl/ZhangZSAFS19}; 2) decremental learning with DeltaBoost~\cite{DBLP:journals/pacmmod/WuZLH23} and MUinGBDT~\cite{DBLP:conf/kdd/LinCL023}; 3) training cost with popular GBDT libraries XGBoost~\cite{DBLP:conf/kdd/ChenG16}, LightGBM~\cite{DBLP:conf/nips/KeMFWCMYL17}, CatBoost~\cite{DBLP:journals/corr/abs-1810-11363} and ThunderGBM~\cite{DBLP:journals/jmlr/WenLSLH020}.



\textbf{Implementation Details.} 
The details of environments and settings are included in Appendix~\ref{apd:experiment_setting}. We employ one thread for all experiments to have a fair comparison, and run ThunderGBM on a NVIDIA A100 40GB GPU, since it does not support only CPU\footnote{\href{https://github.com/Xtra-Computing/thundergbm/blob/master/docs/faq.md}{https://github.com/Xtra-Computing/thundergbm/blob/master/\\docs/faq.md}}. Unless explicitly stated otherwise, our default parameter settings are as follows: $\nu = 1$, $M=100$, $J=20$, $B=1024$, $|D'|=0.1\%\times |D_\textit{tr}|$, $\alpha=0.1$, and $\sigma=0.1$. 





\input{tables/time_speedup} 

\textbf{Datasets.}
We utilize 10 public datasets in the experiments. The specifications of these datasets are presented in Table~\ref{tbl:datasets}. The smallest dataset, Optdigits, consists of 3,822 training instances, while the largest dataset, HIGGS, contains a total of 11 million instances. The number of dimensions or features varies between 8 and 87 across the datasets.





\vspace{-.1in}
\subsection{Training Time and Memory Overhead}\label{ssec:time_mem}
\vspace{-.05in}

Since the proposed online learning framework stores statistical information during training, this may impact both the training time and memory usage. Table~\ref{tbl:training_time_memory} presents a detailed report of the total training time and memory overhead.


\input{wrapfigs/batch_addition_and_batch_removal}



\textbf{Training Time.} Table~\ref{tbl:training_time_memory} shows the total training time of our framework and baselines. Our online learning framework is substantially faster than OnlineGB, DeltaBoost, and XGBoost, and slightly slower than iGBDT. While slower on smaller datasets compared to LightGBM, it outperforms on larger datasets like SUSY and HIGGS, with training times similar to MUinGBDT. Overall, our framework offers significantly faster training than existing incr./decr. methods and is comparable to popular GBDT libraries.



\textbf{Memory Overhead.} Memory usage is crucial for practical applications. Most incremental and decremental learning methods store auxiliary information or learned knowledge during training, potentially occupying significant memory. As shown in Table~\ref{tbl:training_time_memory}, our framework's memory usage is significantly lower than OnlineGB, iGBDT, and DeltaBoost, while OnlineGB and DeltaBoost encountered OOM.



\vspace{-.1in}
\subsection{Online Learning Time}
\vspace{-.05in}

Retraining from scratch can be time-consuming, but in some cases, the cost of online learning outweighs the benefits compared to retraining from scratch, making online learning unnecessary or unjustified. Hence, evaluating the cost of online learning is crucial for practical applications. Table~\ref{tbl:time_speedup} shows the total online learning time (seconds) and speedup v.s. baselines, comparing OnlineGB and iGBDT for incremental learning, and DeltaBoost and MUinGBDT for decremental learning.



In incremental learning, compared to OnlineGB and iGBDT, which also support incremental learning, adding a single data instance can be up to 254.4x and 17x faster, respectively. Furthermore, compared to retraining from scratch on XGBoost, LightGBM, CatBoost, and ThunderGBM (GPU), it can achieve speedups of up to 974.3x, 58.2x, 64.9x, and 27.6x, respectively. 
In decremental learning, when deleting a data instance, our method offers a speedup of 1,619.9x and 6.4x over DeltaBoost and MUinGBDT, respectively, and is 1,254.7x, 74.9x, 90.2x, and 29.6x faster than XGBoost, LightGBM, CatBoost, and ThunderGBM (GPU).


Our method is substantially faster than other methods both in incremental and decremental learning, especially on large datasets. For example, in HIGGS dataset, the largest dataset in experiments, on removing (adding) $1\%$ data, we are 3.1x faster than MUinGBDT (2.6x faster than iGBDT), while OnlineGB and DeltaBoost encounter out of memory (OOM).



Interestingly, we observed that when $|D'|$ is small, decremental learning is faster than incremental learning. However, as $|D'|$ increases, incremental learning becomes faster than decremental learning. For decremental learning, the data to be removed has already been learned, and their derivatives have been stored from training. However, the deleted data often exists discretely in memory. On the other hand, for incremental learning, the data to be added are unseen, and derivatives need to be computed during the incremental learning process. Nevertheless, we append the added data at the end, ensuring that the added data are stored contiguously in memory. With a small $|D'|$, derivatives can be reused in decremental learning, whereas derivatives need to be computed in incremental learning. Therefore, decremental learning is faster. However, as $|D'|$ grows, continuous memory access in incremental learning is faster than decremental learning, making incremental learning faster.



\vspace{-.08in}
\subsection{Batch Addition \& Removal}\label{sec:batch_addition_removal}
\vspace{-.05in}

In the traditional setting, GBDT models must be trained in one step with access to all training data, and they cannot be modified after training -- data cannot be added or removed. In our proposed online learning framework, GBDT models support both incremental and decremental learning, allowing continual batch learning (data addition) and batch removal, similar to mini-batch learning in DNNs.

We conducted experiments on continual batch addition and removal by dividing the data into 20 equal parts, each with $5\%|D_\text{tr}|$. Figure~\ref{fig:batch_add_remove} (left) shows a GBDT model incrementally trained from $5\%$ to $100\%$ of the data, then decrementally reduced back to $5\%$. We retrained models for comparison. Figure~\ref{fig:batch_add_remove} (right) depicts a model decrementally reduced from $100\%$ to $5\%,$ then incrementally trained back to $100\%$. We also report the accuracy of XGBoost and LightGBM. The overlapping curves demonstrate the effectiveness of our online learning framework. Due to space limitations, results are shown for only three datasets.

\begin{table}[t]
\vspace{.05in}
\caption{\label{tbl:backdoor} {Accuracy for clean test dataset and attack successful rate for backdoor test dataset.}}
\vspace{-.12in}
\hspace{-.15in}
\resizebox{0.52\textwidth}{!}{
\begin{tabular}{c|cc|cc|cc|cc}
\toprule
\midrule
\multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{Train Clean} & \multicolumn{2}{c|}{Train Backdoor} & \multicolumn{2}{c|}{Add Backdoor} & \multicolumn{2}{c}{Remove Backdoor} \\
                         & Clean                & Backdoor             & Clean                 & Backdoor               & Clean                   & Backdoor                & Clean              & Backdoor            \\\midrule[0.8pt]
Optdigits                & 96.21\%              & 8.91\%               & 96.27\%               & 100\%                  & 95.94\%                 & 100\%                   & 95.82\%            & 9.69\%              \\
Pendigits                & 96.11\%              & 3.97\%               & 96.43\%               & 100\%                  & 96.48\%                 & 100\%                   & 96.51\%            & 5.55\%              \\
Letter                   & 93.9\%               & 1.38\%               & 94.08\%               & 100\%                  & 93.62\%                 & 100\%                   & 93.78\%            & 3.48\%              \\
Covtype                  & 78.4\%               & 47.83\%              & 78.32\%               & 100\%                & 78.38\%                 & 100\%                   & 78.38\%            & 51.71\%            \\
\midrule
\bottomrule
\end{tabular}
}
\vspace{-.3in}
\end{table}



\vspace{-.08in}
\subsection{Verifying by Backdoor Attacking}
\vspace{-.05in}


Backdoor attacks in machine learning refers to a type of malicious manipulation of a trained model, which is designed to modify the model's behavior or output when it encounters a specific, predefined trigger input pattern~\cite{DBLP:conf/eurosp/SalemWBMZ22, DBLP:conf/aaai/SahaSP20, DBLP:conf/acl/LinLX024}. In this evaluation, we shows that our framework can successfully inject and remove backdoor in a well-trained, clean GBDT model using incremental learning and decremental learning. The details of backdoor attack experiments are provided in Appendix~\ref{apd:backdoor}.


In this evaluation, we randomly selected a subset of the training dataset and injected triggers into it to create a backdoor training dataset, leaving the rest as the clean training dataset. The test dataset was similarly divided into backdoor and clean subsets. We report the accuracy for clean test dataset and attack successful rate (ASR) for backdoor test dataset in Table~\ref{tbl:backdoor}. Initially, we trained a model on the clean training data (``Train Clean''), which achieved high accuracy on the clean test dataset but low ASR on the backdoor test dataset. We then incrementally add the backdoor training data with triggers in to the model (``Add Backdoor''). After incremental learning, the model attained 100\% ASR on the backdoor test dataset, demonstrating effective learning of the backdoor data. For comparison, training a model on the combined clean and backdoor training datasets (``Train Backdoor'') yielded similar results to ``Add Backdoor''. Finally, we removed the backdoor data using decremental learning (``Remove Backdoor''), reducing the ASR to the level of the clean model and confirming the successful removal of backdoor data.




\vspace{-.08in}
\subsection{Additional Evaluations}
\vspace{-.02in}

To further validate our method's effectiveness and efficiency, we have included comprehensive additional evaluations in the Appendix due to page limitations:
\begin{itemize}[wide, labelwidth=!, labelindent=0pt, topsep=0pt, itemsep=-1ex,partopsep=0ex,parsep=1.8ex]
    \item \textbf{Time Complexity Analysis:} We analyze the computational complexity of our proposed framework compared to  retraining from scratch in Appendix~\ref{apd:time_complexity}.
    \item \textbf{Test Error Rate:} We compare the test error rate between our method and baselines in Appendix~\ref{apd:test_error_rate}.
    \item \textbf{Real-world Time Series Evaluation:} To confirm the performance of our methods on real-world datasets with varying data distributions, we conducted experiments on two time series datasets as included in Appendix~\ref{apd:real_world_time_series}.
    \item \textbf{Extremely High-dimensional Datasets:} To confirm the scalability of our framework, we report the experiments for two extremely high-dimensional datasets, RCV1 and News20, in Appendix~\ref{apd:high_dimensional}.
    \item \textbf{Model Functional Similarity:} We evaluate the similarity between the model learned by online learning and the one retrained from scratch in Appendix~\ref{apd:model_func}.
    \item \textbf{Approximation Error of Leaf Scores:} Since our framework might use the outdated derivatives in the gain computation, to assess the effect of these outdated derivatives, we report the approximation error of leaf scores between the model after addition/deletion and the one retrained from scratch in Appendix~\ref{apd:error_leaf_scores}.
    \item \textbf{Data Addition with More Classes:} Our framework supports incremental learning for previously unseen classes. Detailed results and analysis are provided in Appendix~\ref{apd:more_classes}.
    \item \textbf{Membership Inference Attack:} Additional to backdoor attack, we also confirm the effectiveness of our method on adding/deleting data by membership inference attack (MIA) in Appendix~\ref{apd:MIA}.
    \item \textbf{Ablation Study:} We report the ablation study for different hyper-parameter settings in Appendix~\ref{apd:ablation_study}.
\end{itemize}




\vspace{-.08in}
\section{Related Work}
\vspace{-.02in}
\textbf{Incremental Learning} is a technique in machine learning that involves the gradual integration of new data into an existing model, continuously learning from the latest data to ensure performance on new data~\cite{DBLP:journals/natmi/VenTT22}. It has been a open problem in machine learning, and has been studied in convolutional neural network (CNN)~\cite{DBLP:journals/tsmc/PolikarUUH01, DBLP:conf/cvpr/KuzborskijOC13, DBLP:conf/cvpr/0001WYMPZ22}, DNN~\cite{DBLP:journals/ieeemm/HussainLT23, dekhovich2023continual}, SVM~\cite{DBLP:journals/cluster/ChenXXZ19, DBLP:conf/nips/CauwenberghsP00} and RF~\cite{DBLP:conf/icip/WangWCL09, DBLP:journals/corr/abs-2009-05567}. In gradient boosting, iGBDT offers incremental updates~\cite{DBLP:journals/npl/ZhangZSAFS19}, while other methods~\cite{DBLP:conf/nips/BeygelzimerHKL15, DBLP:conf/iccvw/Babenko0B09} extend GB to online learning. However, these methods do not support removing data.


\textbf{Decremental Learning} allows for the removal of trained data and eliminates their influence on the model, which can be used to delete outdated or privacy-sensitive data~\cite{DBLP:conf/sp/BourtouleCCJTZL21, DBLP:journals/corr/abs-2209-02299, DBLP:conf/nips/SekhariAKS21, DBLP:journals/csur/XuZZZY24}. It has been researched in various models, including CNN~\cite{DBLP:journals/corr/abs-2304-02049, DBLP:journals/corr/abs-2111-08947}, DNN~\cite{chen2023boundary, DBLP:conf/uss/ThudiJSP22}, SVM~\cite{DBLP:conf/nips/KarasuyamaT09, DBLP:conf/nips/CauwenberghsP00}, Naive Bayes~\cite{DBLP:conf/sp/CaoY15}, K-means~\cite{DBLP:conf/nips/GinartGVZ19}, RF~\cite{DBLP:conf/sigmod/SchelterGD21, DBLP:conf/icml/BrophyL21}, and GB~\cite{DBLP:journals/pacmmod/WuZLH23, DBLP:journals/corr/abs-2311-13174}. In random forests, DaRE~\cite{DBLP:conf/icml/BrophyL21} and a decremental learning algorithm~\cite{DBLP:conf/sigmod/SchelterGD21} are proposed for data removal with minimal retraining and latency.

However, in GBDT, trees in subsequent iterations rely on residuals from previous iterations, making decremental learning more complicated. DeltaBoost \citet{DBLP:journals/pacmmod/WuZLH23} simplified the dependency for data deletion by dividing the dataset into disjoint sub-datasets, while a recent study \citet{DBLP:conf/kdd/LinCL023} proposed an efficient unlearning framework without simplification, utilizing auxiliary information to reduce unlearning time. Although effective, its performance on large datasets remains unsatisfactory.



\vspace{-.08in}
\section{Conclusion}
\vspace{-.02in}
In this paper, we propose an efficient in-place online learning framework for GBDT that support incremental and decremental learning: it enables us to dynamically add a new dataset to the model and delete a learned dataset from the model. It support continual batch addition/removal, and data additional with unseen classes. We present a collection of optimizations on our framework to reduce the cost of online learning. Adding or deleting a small fraction of data is substantially faster than retraining from scratch. Our extensive experimental results confirm the effectiveness and efficiency of our framework and optimizations -- successfully adding or deleting data while maintaining accuracy.



\clearpage


\section*{Impact Statement}\label{sec:limitations}
This paper introduces an online learning framework for GBDTs that enables both incremental and decremental learning. While the framework offers significant potential benefits, such as adapting models to the latest data and supporting data deletion requests, it also comes with some limitations and risks that would need to be carefully considered in practical applications. The ability to manipulate models through targeted addition or deletion of data introduces new security vulnerabilities. More analysis is needed to understand scalability limitations and to explore the practical implications, especially for sensitive use cases. Future work should further discuss the deployment considerations and develop strategies to mitigate the risks while realizing the benefits of online learning capabilities.

\bibliography{main_arxiv}
\bibliographystyle{icml2025}

\appendix
\onecolumn

\input{wrapfigs/tree_bin}

\section{Feature Discretization.}\label{apd:gbdt_fd}

The preprocessing step of feature discretization plays a crucial role in simplifying the implementation of Eq.~\eqref{eqn:logit_gain} and reducing the number of splits that need to be evaluated. This process involves sorting the data points based on their feature values and assigning them to bins, taking into account the distribution of the data, as shown in Figure~\ref{fig:tree-bin} and Algorithm~\ref{alg:quantile}. By starting with a small bin-width (e.g., $10^{-8}$) and a predetermined maximum number of bins $B$ (e.g., 1024). It assigns bin numbers to the data points from the smallest to the largest, carefully considering the presence of data points in each bin. This iterative process continues until the number of bins exceeds the specified maximum.




\begin{wrapfigure}[16]{r}{.5\textwidth}
\vspace{-.25in}
\begin{minipage}{.5\textwidth}
\begin{algorithm}[H]
\begin{algorithmic}[1]
\STATE $v_{\{1..N\}}$ = sorted feature values, $\textit{bin\_width} = 10^{-10}$
\WHILE{\textbf{true}}
    \STATE $\textit{cnt} = 0$, $\textit{curr\_idx} = 0$
    \FOR{$i = 1$ \textbf{to} $N$}
        \IF{$v_i - v_{\textit{curr\_idx}} > \textit{bin\_width}$}
            \STATE $\textit{cnt} = \textit{cnt} + 1$, $\textit{cur\_idx} = i$
            \IF{$\textit{cnt} > B$}
                \STATE $\textit{bin\_width} = \textit{bin\_width} * 2$
                \STATE \textbf{break}
            \ENDIF
        \ENDIF
        \STATE $v'_i = \textit{cnt}$
    \ENDFOR
    \STATE{\textbf{if} $\textit{cnt} <= B$} \textbf{then} \textbf{break}
\ENDWHILE
\STATE \textbf{return} $v'$ as discretized feature values
\end{algorithmic}
\caption{Discretize Feature}
\label{alg:quantile}
\end{algorithm}
\end{minipage}
\end{wrapfigure}
In cases where the number of required bins surpasses the maximum limit, the bin-width is doubled, and the entire process is repeated. This adaptive discretization approach proves particularly effective for boosted tree methods, ensuring that feature values are mapped to integers within a specific range. Consequently, after the discretization mapping is established, each feature value is assigned to the nearest bin. After this discretization preprocessing, all feature values are integers within $\{0,1,2,\cdots,B-1\}$.


The advantage of this discretization technique becomes evident during the gain searching step. Instead of iterating over all $N$ feature values, the algorithm only needs to consider a maximum of $B$ splits for each feature. This substantial reduction in the number of splits to evaluate leads to a significant decrease in the computational cost, transforming it from being dependent on the dataset size $N$ to a manageable constant $B$.

\section{Experiment Setting} \label{apd:experiment_setting}

The experiments are performed on a Linux computing node running Red Hat Enterprise Linux 7, utilizing kernel version 5.10.155-1.el7.x86\_64. The CPU employed was an Intel(R) Xeon(R) Gold 6150 CPU operating at a clock speed of 2.70GHz, featuring 18 cores and 36 threads. The system was equipped with a total memory capacity of 376 GB. We have built a prototype of our online learning framework using C++11. The code is compiled with g++-11.2.0, utilizing the ``O3'' optimization. Unless explicitly stated otherwise, our default parameter settings are as follows: $J=20$, $B=1024$, $|D'|=0.1\%\times |D_\textit{tr}|$, $\alpha=0.1$, and $\sigma=0.1$. We report the ablation study for different settings in Appendix~\ref{apd:ablation_study}.

\section{Framework Overview} \label{apd:overview_example}

Figure~\ref{fig:overview} is a visual example of incremental and decremental learning of our proposed framework. Figure~\ref{fig:overview}(b) is one tree of the GBDT model and has been well-trained on dataset $D_{\textit{tr}} = \{0,1,2,3...,19\}$. Every rectangle in the tree represents a node, and the labels inside indicate the splitting criteria. For instance, if the condition \texttt{Age < 42} is met, the left-child node is followed; otherwise, the right-child node is chosen. The numbers within the rectangles represent the prediction value of the terminal nodes. Please note that here the feature \texttt{42} is a discretized value, instead of the raw feature. Our online learning framework has the capability to not only incrementally learn a new dataset $D_{\textit{in}}$, but also decrementally delete a learned dataset $D_{\textit{de}} \subseteq D_\textit{tr}$.





\textbf{Example for Incremental Learning.} Here, we would like to add a new dataset $D' = D_\textit{in}=\{20, 21, 22, 23\}$ to the original model, so we will call the function of incremental learning. $|d|$ denotes how many data of the $D'$ reach this node. As shown in Algorithm~\ref{alg:unlearning_tree}, we traverse all non-terminal nodes (non-leaf nodes) in the tree at first. For example, we are going to test the node of \texttt{Loan < 31}. Its current best split is \texttt{Loan < 31}. One of the new data instances $\{22\}$ reaches this node. After adding this data and recomputing the gain value, \texttt{Loan < 31} is still best split with the greatest gain value of $26.937$, and meets $s = s'$, as shown in Figure~\ref{fig:overview}(a). Thus, we can keep this split and do not need to do any changes for this node. Then we are going to test the node of \texttt{Auto < 57} and the remaining three new data instances $\{20, 21, 23\}$ reach this node. As shown in the left side of Figure~\ref{fig:overview}(c), we recompute the gain value for this node, but the best split changes to \texttt{Income < 5}. Therefore, we retrain the pending sub-tree rooted on \texttt{Auto < 57} after adding new data instances to obtain a new sub-tree rooted on \texttt{Income < 5}. Then we replace the pending sub-tree with the new one. Finally, we update the prediction value on terminal nodes (leaf nodes). For example, $0.4322$ is updated to $0.2735$ because of adding data $\{22\}$; $-0.1252$ has no change because the data of this node are still the same.


\textbf{Example for Decremental Learning.} Similar to incremental learning, we would like to delete a learned dataset $D_\textit{de}=\{2, 7, 11, 13\}$ and its effect on the model. The best split of node \texttt{Loan < 31} does not change, so we keep the split. For \texttt{Auto < 57}, as shown in the right side of Figure~\ref{fig:overview}(c), after removing data instances $\{2, 11, 13\}$, the best split changes from \texttt{Auto < 57} to \texttt{Credit < 24}, so we retrain the pending sub-tree rooted on \texttt{Loan < 31} and then replace it with the new sub-tree. For terminal nodes (leaf nodes), the prediction value changes if any data reaching this node is removed.




\section{Split Candidates Sampling}\label{apd:split_sampling_app}

\vspace{.1in}
\noindent{\bf Definition 1} (Distance Robust) {\em Let $s$ be the best split, and $\frac{|D'|}{|D_{\textit{tr}}|} = \lambda$. $N_\Delta$ is the distance between $s$ and its nearest split $t$, $N_\Delta=||t-s||$. $s$ is distance robust if}
{
\begin{align}
N_\Delta > \frac{\lambda Gain(s)}{\frac{1}{N_{ls}}\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}} + \frac{1}{N_{rs}}\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}}}
\end{align}
}

\noindent{\em Proof.} In decremental learning, for a fixed $\lambda$, we have
{
\begin{align}
&\left(1 - \lambda\right)Gain(s) - Gain(s + N_\Delta)\\
\approx \ &(1 - \lambda)\left(\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}}
+\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}} 
- \frac{\left(\sum_{\mathbf{x}_i\in l_s\cup r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s\cup r_s} h_{i, k}}\right)\notag\\
&\hspace{0.3in}- \left(\left(1 - \frac{N_\Delta}{N_{ls}}\right)\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}}
+\left(1 - \frac{N_\Delta}{N_{rs}}\right)\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k}\right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}} \right.\notag\\ 
&\hspace{0.3in}\left.- \frac{\left(\sum_{\mathbf{x}_i\in l_s\cup r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s\cup r_s} h_{i, k}}\right)
\end{align}
}
where $l$ represents the left child of split $s$, and it contains the samples belonging to this node, while $r$ represent the right child, $N_{ls}$ denotes $\left|l_s\right|$, and $N_{rs}$ denotes $\left|r_s\right|$.

Let $(1 - \lambda)Gain(s) - Gain(s + N_\Delta) > 0$, we have 
{
\begin{align}
%&(1 - \lambda)Gain(s) - Gain(s + N_\Delta) > 0 \\
\overset{approx}{\Rightarrow}&(1-\lambda)Gain(s) -  \left(\left(1+\frac{N_\Delta}{N_{ls}}\right) \frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}} \right.\notag\\
&\hspace{.3in}+ \left.\left(1-\frac{N_\Delta}{N_{rs}}\right) \frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}} - \frac{\left(\sum_{\mathbf{x}_i\in l_s \cup r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s \cup r_s} h_{i, k}}\right)\\
\Rightarrow \ &\frac{N_\Delta}{N_{ls}}\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}}
+\frac{N_\Delta}{N_{rs}}\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}}
- \lambda Gain(s) > 0 \\
\Rightarrow \ &N_\Delta > \frac{\lambda Gain(s)}{\frac{1}{N_{ls}}\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}} + \frac{1}{N_{rs}}\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}}}
\end{align}
}
\QEDB

In the above definition, $\mathbb{E}(N_\Delta) = 1/\alpha$, where $\alpha$ denotes the split sampling rate, we can observe that a smaller sampling rate will result in a more robust split, so we can reduce the number of retrain operations by reducing the sampling rate. Similarly, incremental learning can get the same result.

\vspace{.1in}
\noindent{\bf Definition 2} (Robustness Split) {\em For a best split $s$ and an split $t$ with the same feature, $t \neq s$, and online learning data rate $\frac{|D'|}{|D_\textit{tr}|} = \lambda$, the best split $s$ is robust split if}
{
\begin{align}
Gain(s) > \frac{1}{1-\lambda}Gain(t)
\end{align}
}

\noindent{\em Proof.} Initially, we have
{
\begin{align}
Gain(s) =&  
\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}}
+\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}}
- \frac{\left(\sum_{\mathbf{x}_i\in l_s\cup r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s\cup r_s} h_{i, k}}
\end{align}
}

After decremental learning, we get
{
\begin{align}
\mspace{-10mu}
Gain'(s) =&  
\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} - \sum_{\mathbf{x}_i\in l_s \cap D'} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k} - \sum_{\mathbf{x}_i\in l_s \cap D'} h_{i, k}} +\frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} - \sum_{\mathbf{x}_i\in r_s \cap D'} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k} \sum_{\mathbf{x}_i\in r_s \cap D'} h_{i, k}}\\
&\hspace{0.3in}- \frac{\left(\sum_{\mathbf{x}_i\in l_s\cup r_s} g_{i, k}  - \sum_{\mathbf{x}_i\in (l_s\cup r_s) \cap D'} g_{i, k}\right)^2}{\sum_{\mathbf{x}_i\in l_s\cup r_s} h_{i, k} - \sum_{\mathbf{x}_i\in (l_s\cup r_s) \cap D'} h_{i, k}}\notag
\end{align}
}

For any possible split $t \ (t \neq s)$, the split $s$ is robust only and only if $Gain(s) > Gain(t)$ and $Gain'(s) > Gain'(t)$. First, let's analyze the first term of $Gain'(s)$. Suppose $\frac{\left|D'\right|}{\left|D_\textit{tr}\right|} = \lambda$, and $D'$ is randomly selected from $D$. Here we consider the leaf child $l_s$ of split $s$, and let the $\left|l_s \cap D'\right|$ to be $n_{ls}$, $\left|l_s\right|$ to be $N_{ls}$. Then we have
{
\begin{align}
\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} - \sum_{\mathbf{x}_i\in l_s \cap D'} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k} - \sum_{\mathbf{x}_i\in l_s \cap D'} h_{i, k}} \ &\overset{approx}{\Rightarrow} \ 
\frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} - n_{ls} \overline{g}_{ls} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k} - n_{ls} \overline{h}_{ls}}\\
&\hspace{-0.3in}\Rightarrow \ 
\left(1-\frac{n_{ls}}{N_{ls}}\right) \frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s} h_{i, k}}
\end{align}
}
where $\overline{g}$ and $\overline{h}$ denote the average of the $g_{i,k}$ and $h_{i, k}$ respectively.

Similarly, we can get all three terms for $Gain(s)$, $Gain'(s)$, $Gain(t)$, and $Gain'(t)$ in a similar form. For $Gain'(s) > Gain'(t)$, finally, we have $Gain(s) > Gain(t) + C$, where
{
\begin{align}
C &= \left(\frac{n_{ls}}{N_{ls}} \frac{\left(\sum_{\mathbf{x}_i\in l_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}} + 
\frac{n_{rs}}{N_{rs}} \frac{\left(\sum_{\mathbf{x}_i\in r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_s} h_{i, k}} \right.\notag\\
&\hspace{.4in}\left.- \frac{n_{ls} + n_{rs}}{N_{ls} + N_{rs}} \frac{\left(\sum_{\mathbf{x}_i\in l_s \cup r_s} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_s \cup r_s} h_{i, k}}\right) - \left(
\frac{n_{lt}}{N_{lt}} \frac{\left(\sum_{\mathbf{x}_i\in l_t} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_t} h_{i, k}} \right.\notag\\
&\hspace{.4in}\left.+ \frac{n_{rt}}{N_{rt}} \frac{\left(\sum_{\mathbf{x}_i\in r_t} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in r_t} h_{i, k}} - 
\frac{n_{lt} + n_{rt}}{N_{lt} + N_{rt}} \frac{\left(\sum_{\mathbf{x}_i\in l_t \cup r_t} g_{i, k} \right)^2}{\sum_{\mathbf{x}_i\in l_t \cup r_t} h_{i, k}} \right)
\end{align}
}

The upper bound of $C$ is $\lambda Gain(s)$. Further, we have
{
\begin{align}
Gain(s) > \frac{1}{1-\lambda}Gain(t)
\end{align}
}
\QEDB

The above definition shows that, as $\lambda = \frac{|D'|}{|D_\textit{tr}|}$ decreases, the splits are more robust, leading to a reduction in the frequency of retraining. In conclusion, decreasing either $\alpha$ or $\lambda$ makes the split more robust, reducing the change occurrence in the best split, and it can substantially reduce the online learning time.


\section{Update w/o Touching Training Data}\label{apd:update_without_touching_training_data}

\textbf{Maintain Best Split.}
The split gain is calculated by Eq.~\eqref{eqn:logit_gain}. There are three terms: the gain for the left-child, the gain for the right-child, and subtracting the gain before the split. Each gain is computed as the sum of the squared first derivatives $\left(\left[\sum_{i=1}^N \left(r_{i,k} - p_{i,k}\right) \right]^2\right)$ divided by the sum of the second derivatives $\left(\sum_{i=1}^N p_{i,k}(1-p_{i,k})\right)$ for all the data in the node. To compute these terms, it is necessary to iterate over all the data that reaches the current node. The most straightforward way for online learning to obtain the split gain is to directly compute these three terms for dataset $D_\textit{tr} \pm D'$. In the worst case, which is the root node, the computation cost for gain computing is $|D_{\textit{tr}}| + |D_{\textit{in}}|$ or $|D_{\textit{tr}}| - |D_{\textit{de}}|$ because the root node contains all the training data.

We calculate the split gain for $D_{\textit{tr}} \pm D'$ without touching the $D_{\textit{tr}}$. In this optimization, during the training process, we store the $S_{\textit{rp}}=\sum_{i=1}^N \left(r_{i,k} - p_{i,k}\right)$ and $S_{\textit{pp}}=\sum_{i=1}^N p_{i,k}(1-p_{i,k})$ for the training dataset $D_{\textit{tr}}$ for every potential split. In incremental learning process, we can only calculate the $S_{\textit{rp}}'$ and $S_{\textit{pp}}'$ for $D_{\textit{in}}$.  To obtain the new split gain based on Eq.~\eqref{eqn:logit_gain}, we add it to the stored $S_{\textit{rp}}$ and $S_{\textit{pp}}$.
Similarly, for decremental learning, we can only calculate the $S_{\textit{rp}}'$ and $S_{\textit{pp}}'$ for $D_{\textit{de}}$ to obtain the new split gain. In this manner, we successfully avoid the original training data for split gain computation and reduce the computation cost from $O(D_\textit{tr} \pm D')$ to $O(D')$.


\textbf{Recomputing Prediction Value.} For the terminal node (leaf node), if there are no data of $D'$ reaching this node, we can skip this node and do not need to change the prediction value. Otherwise, we have to calculate a prediction value $f$ as shown in line 5 of the Algorithm~\ref{alg:robust_LogitBoost}. Similar to split gain computing, it is required to iterate over all the data that reaches this terminal node. Here we store $S_{\textit{rp}}=\sum_{\mathbf{x}_i \in R_{j,k,m}} (r_{i,k} - p_{i,k})$ and $S_{\textit{pp}}=\sum_{\mathbf{x}_i\in R_{j,k,m}}\left(1-p_{i,k}\right)p_{i,k}$ for training dataset $D_\textit{tr}$ in training process. Thus, in online learning process, we only need to calculate $S_{\textit{rp}}'$ and $S_{\textit{pp}}'$ for online learning dataset $D'$.

\textbf{Incremental Update for Derivatives.} After conducting online learning on a tree, we need to update the derivatives and residuals for learning the next tree.
From the perspective of GBDT training, each tree in the ensemble is built using the residuals learned from the trees constructed in all previous iterations: 
Modifying one of the trees affects all the subsequent trees. 
A trivial method is to update the derivatives and residuals for all data instances of $D_{\textit{tr}} \pm D'$ in every tree, but it is time-consuming.

When performing online learning on a tree, not all terminal nodes will be changed---some terminal nodes remain unchanged because there is no data from $D'$ that reaches these terminal nodes. Note that our goal is to find a model close to the model
retraining from scratch. In the online learning scenario, all trees have already been well-trained on $D_\textit{tr}$. Intuitively, the derivative changes for data in those unchanged terminal nodes should be minimal. Therefore, as shown in Figure~\ref{fig:overview}(d), we only update the derivatives for those data reaching the changed terminal nodes. For example, the terminal node with a prediction value of $-0.1252$ does not meet any data in $D'$ in both incremental learning and decremental learning, so the prediction value of this node does not need to be changed. Therefore, we do not need to update the derivatives of the data $\{1, 6, 14, 16, 17\}$ reaching this terminal node.

\section{Time Complexity}\label{apd:time_complexity}
We compare the time complexity of retraining from scratch and our online learning approach in Table~\ref{tbl:time_complexity}. Training a tree involves three key steps: Derivatives Computing, Gain Computing \& Split Finding, and Prediction Computing. Let $B$ represent the number of bins, $J$ the number of leaves, $|D_{tr}|$ the number of training data points, and $|D'|$ the number of online learning data points ($|D'| \ll |D_{tr}|$).

\textbf{Derivatives Computing.} In retraining, each point is assigned to one of the $B$ bins, which take $O(\|D_{tr}\|)$ time. In our method, we optimize updates without touching training data, directly adding or subtracting derivatives for the online data points, which takes $O(\|D'\|)$ time.

\textbf{Gain Computing \& Split Finding.} In training, to identify the optimal split for each node, we compute the potential split gains for each bin. As a binary tree is constructed with $2J - 1$ nodes, the total computational complexity for split finding across the entire tree is $O(B(2J - 1)) = O(BJ)$. In our approach, Split Candidates Sampling reduces the number of split candidates from $B$ to $\alpha B$, where $\alpha$ denotes the split sample rate ($0 < \alpha \leq 1$). Additionally, let $P_\sigma$ represent the probability of a split change being within the robustness tolerance, indicating the likelihood that a node does not require retraining (with larger $\sigma$, $P_\sigma$ increases). If retraining is not required, the time complexity for checking a node is $O(|D'|)$. Conversely, if retraining is required, the complexity to retrain a node is $O(\alpha B)$. Consequently, the total time complexity for the entire tree is $O(J|D'| \cdot P_\sigma + J\alpha B \cdot (1-P_\sigma))$. For $P_\sigma \rightarrow 1$, no nodes require retraining, simplifying the complexity to $O(J|D'|)$. Conversely, for $P_\sigma \rightarrow 0$, all nodes require retraining, and the complexity becomes $O(J\alpha B)$.

\textbf{Predicted Value Computing.} During training, after the tree is built, the predicted value for each leaf is updated. This involves traversing the leaf for the data points that reach it, with the total number being equivalent to all training data points, resulting in a complexity of $O(|D_{tr}|)$. In our method, we update the predicted value only for leaves reached by at least one online data point, and adjust by adding/subtracting the impact of online data points, resulting in a complexity of $O(|D'|)$.

\begin{table}[thbp]
\centering
\caption{Time complexity comparison between retraining and online learning.}\label{tbl:time_complexity}
\vspace{-.1in}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llll}
\toprule
\midrule
Step                            & Training Time       & Optimization                                          & Online Learning Time  \\\midrule
Derivatives Computing           & $O(|D_{tr}|)$       & Update without Touching Training Data                 & $O(|D'|)$             \\
Gain Computing \& Split Finding & $O(BJ)$             & Split Candidates Sampling, Split Robustness Tolerance & $O(\alpha BJ \sigma)$ \\
Predition Computing             & $O(|D_{tr}|\log J)$ & Update without Touching Training Data                 & $O(|D'|)$         \\
\midrule
\bottomrule
\end{tabular}%
}
\end{table}




\section{Test Error Rate}\label{apd:test_error_rate}

Table~\ref{tbl:error_rate} presents the test error for different methods, defined as (1 - accuracy) for classification tasks and Mean Squared Error (MSE) for regression tasks. We have omitted the results for OnlineGB, as its excessively long learning time makes it relatively insignificant compared to the other methods. Three scenarios are considered: (1) Training, reporting the test error for models trained on the full dataset $D$; (2) Incremental Learning, performing incremental learning to add a randomly selected portion $D'$ into a model pre-trained on $D - D'$; and (3) Decremental Learning, conducting decremental learning to remove $D'$ from a model trained on the full dataset $D$. Our method achieved the best error rates in most cases.



\input{tables/test_error_rate}


\section{Real-world Time Series Evaluation}\label{apd:real_world_time_series}
To confirm the performance of our methods on real-world datasets with varying data distributions, we conducted experiments on two real-world time series datasets from Kaggle: 
\begin{itemize}[wide, labelwidth=!, labelindent=0pt, topsep=0pt, itemsep=-1ex,partopsep=0ex,parsep=1.8ex]
\item \textbf{GlobalTemperatures}\footnote{\href{https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data}{https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data}}: This dataset records the average land temperatures from 1750 to 2015.
\item \textbf{WebTraffic}\footnote{\href{https://www.kaggle.com/datasets/raminhuseyn/web-traffic-time-series-dataset}{https://www.kaggle.com/datasets/raminhuseyn/web-traffic-time-series-dataset}}: This dataset tracks hourly web requests to a single website over a span of five months.
\end{itemize}


\input{tables/time_series_tbl}

For this experiment, we constructed the input data $X$ using the time series values from the previous 15 time steps, with the goal of predicting the corresponding output value $y$. Initially, we randomly sample 10\% of the data as the test dataset, with the remaining 90\% used as the training dataset. Similar to Section~\ref{sec:batch_addition_removal}, we evenly divided the training data into 10 subsets, each containing 10\% of the training samples. It is important to note that we did not shuffle these time series datasets, meaning the 10 subsets were arranged sequentially from older to more recent data. We trained an initial model using the first subset, then incrementally added each subsequent subset one by one. After incorporating all training data, we sequentially removed each subset in reverse order. As expected, since the test dataset spans all time steps, the error rate decreases as more subsets are added to the model. This is because the model learns the updated distribution from the newly added subsets. After removing each subset, the error rate increases, reflecting the loss of information associated with the removed data and the model's adjustment to the remaining subsets. As shown in Table~\ref{tbl:time_series}, these results confirm the effectiveness of our method in adapting to changing data distributions.

\input{tables/functional_similarity_tbl}


\section{Model Functional Similarity}\label{apd:model_func}

As mentioned in Section~\ref{ssec:problem_setting}, the goal of the framework is to find a model close to the model retrained from scratch. The model functional similarity is a metric to evaluate how close the model learned by online learning and the one retrained from scratch. We show the model functional similarity for incremental learning and decremental learning in Table~\ref{tbl:functionality}. C2W refers to the ratio of testing instances that are correctly predicted during retraining but are wrongly predicted after decremental learning. Similarly, W2C represents the testing instances that are wrongly predicted during retraining but are correctly predicted after decremental learning. The W2W column indicates the cases where the two models have different wrong predictions. For binary labels, W2W is not applicable. 
In the $|D'|$ column, 1 indicates that only add/remove one instance, while 0.1\% corresponds to $|D'| = 0.1\% \times |D_{\textit{tr}}|$.
We present $\phi$ to evaluate the model functional similarity (adapted from the model functionality~\cite{DBLP:conf/uss/AdiBCPK18}), indicating the leakage of online learning:

%\vspace{.05in}
\noindent{\bf Definition \showdefinitioncounter} (Functional Similarity) {\em Given an input space $\mathcal{X}$, a model ${T}$, a model $\hat{{T}}$ online learned from ${T}$, and a dataset $D = \{{y_i, \mathbf{a}_i}\} \in \mathcal{X}$, the functional similarity $\phi$ between model ${T}$ and $\hat{{T}}$ is}:
%\begin{align}
$\phi = 1 - (r_\textit{w2w} + r_\textit{w2c} + r_\textit{c2w})$
%\end{align}
,{\em where $\phi$ is the leakage of learning.}
%\vspace{.1in}

% \vspace{.1in}
% \noindent{\em Proof.} For GBDT model ${T}$ and $\hat{{T}}$, we get
% \begin{align}
% p = {T}(D), \hspace{.3in} \hat{p} = \hat{{T}}(D)
% \end{align}
% where $p$ indicates the prediction. Then we have
% \begin{align}
% \phi&=\frac{||1_{p_i = \hat{p}_i}||}{|D|} \\
% &= 1 - (r_\textit{c2c} + r_\textit{w2c} + r_\textit{c2w})
% \end{align}
% \QEDB

Due to the size limitations of the table, we have omitted OnlineGB from this table because its learning duration is excessively long, making it relatively meaningless compared to other methods. We compared iGBDT in adding $1$ and $0.1\%$ data instances, and DeltaBoost and MUinGBDT in deleting data. As shown in Table~\ref{tbl:functionality}, we have a comparable model functionality in adding/deleting both $1$ and $0.1\%$. In most cases, our online learned model reaches $98\%$ similarity in both incremental learning and decremental learning.




\section{Backdoor Attacking}\label{apd:backdoor}


\textbf{Experimental Setup.} In this evaluation, we randomly select a subset of the training dataset, and set first a few features to a specific value (trigger, e.g. 0 or greatest feature value) on these data instances, and then set the label to a target label (e.g., 0). In the testing dataset, we set all labels to the target label to compose a backdoor test dataset. In this setting, if the model has correctly learned the trigger and target label, it should achieve a high accuracy on backdoor test dataset.


\section{Membership Inference Attack}\label{apd:MIA}

The membership inference attack (MIA) aims to predict whether a data sample is part of the training dataset~\cite{DBLP:conf/sp/ShokriSSS17,DBLP:journals/csur/HuSSDYZ22, DBLP:conf/icml/Choquette-ChooT21}. Therefore, the goal of this experiments is to determine if "deleted" data can still be identified as training data after decremental learning. However, in our experiment with default hyper-parameter setting, the predictions made by MIA are nearly random guesses.


\textbf{Experimental Setup.} Previous studies demonstrate that overfitting can make machine learning models more vulnerable to MIA~\cite{DBLP:conf/csfw/YeomGFJ18, DBLP:conf/aistats/BreugelSQS23, DBLP:journals/csur/HuSSDYZ22}. To further validate our approach, we apply a smaller model with the number of iterations $M=5$, which can be easily overfitted. For overfitting the model, we split each dataset into three subsets: base dataset $D_\text{base}$ ($49.9\%$), online dataset $D'$ ($0.1\%$), and test dataset $D_\text{test}$ ($50\%$). We first train a base model on $D_\text{base} + D'$. For this base model, the MIA should identify the data in $D'$ as part of the training dataset. Next, we perform decremental learning to delete $D'$ from the base model. After this process, the MIA should no longer identify the data in $D'$ as part of the training dataset, confirming that our approach effectively deletes the data from the model. Finally, we add $D'$ back to the model by incremental learning. Following this, the MIA should once again identify the data in $D'$ as part of the training dataset. These experiments are conducted on multi-class datasets: Optdigits, Pendigits, Letter, and Covtype.


\textbf{MIA Model.} By following the existing MIA methods~\cite{DBLP:journals/tdsc/YanLWZSHL23, DBLP:journals/corr/abs-2205-06469, DBLP:conf/sp/CarliniCN0TT22}, we train an MIA model (binary classification) on the prediction probabilities of each class. Since the GBDT model is overfitted, the probability distributions of the training data should substantially differ from those of the unseen data (test data). Therefore, the MIA model can predict whether a data sample is part of the training dataset based on its probability distribution. We sample 50\% of $D_\text{base}$ and 50\% of $D_\text{test}$ to train the MIA model. Then remaining 50\% of $D_\text{base}$, the entire $D'$ and 50\% of $D_\text{test}$ are used for evaluation.


\input{tables/MIA}

\textbf{Results.} Table~\ref{tbl:membership_inference_attack} presents the average probability of data samples being identified as part of the training dataset at different stages. For the base model, MIA identifies 100\% of the data in $D_\text{base}$ and $D'$ as part of the training dataset, while the data in $D_\text{test}$ has a low probability of being identified as part of the training dataset. After decremental learning, the probability for $D_\text{base}$ remains unchanged, while the probability for $D'$ drops to a level almost identical to $D_\text{test}$. This confirms that $D'$ has been effectively deleted from the base model. After incremental learning, the probability for $D'$ increases to 100\% again, indicating that the model has successfully relearned 
$D'$. The probability for $D_\text{test}$ in the incremental model remains almost the same as in the base model. This result confirms that our decremental/incremental learning approach can indeed delete/add data from/to the model.




\section{Extremely High-dimensional Datasets}\label{apd:high_dimensional}

We include two dataset with more features / high dimensional: RCV1 and News20, which have 47,236 and 1,355,191 features respectively. For News20 dataset, the substantial high dimension causes segmentation fault on CatBoost and GPU out of memory (OOM) on thunderGBM. We omit the results from the other incremental/decremental method because infeasible running time and massive occupied memory. Table \ref{rep_tab:high_time_mem} shows the comparison of the training time and memory usage for our methods and other popular methods. Table \ref{rep_tab:high_rate_time} illustrates the incremental and decremental learning time of our method for two high dimensional dataset.


\begin{figure}[h]
\begin{minipage}{.40\textwidth}

\begin{table}[H]
\vspace{-.3in}
\centering
\caption{Dataset specifications.}\label{rep_tbl:datasets}
\vspace{-.1in}
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrr}
\toprule
\midrule
Dataset     & \# Train & \# Test & \# Dim & \# Class \\ \midrule
{News20}   & {5,000}     & {14,996}    & {1,355,191}         & {2}      \\
{RCV1}   & {20,242}     & {677,399}    & {47,236}         & {2}      \\
\midrule
\bottomrule
\end{tabular}
}
\end{table}

\end{minipage}
\hspace{.1in}
\begin{minipage}{.59\textwidth}
\vspace{-.12in}

\begin{table}[H]
\centering
\caption{Comparison of the training time consumption and memory usage for RCV1 and News20.}
\vspace{-.1in}
\resizebox{\textwidth}{!}{%
\begin{tabular}{llccccc}
\toprule
\midrule
                                   & Dataset & XGBoost  & LightGBM & CatBoost   & \begin{tabular}[c]{@{}c@{}}ThunderGMB\\ (GPU)\end{tabular} & Ours       \\\midrule
\multirow{2}{*}{Training Time (s)} & RCV1    & 459.75   & 59.63    & 335.70     & 49.44                                                      & 295.43     \\
                                   & News20  & 637.02   & 28.42    & Seg. Fault & OOM                                                        & 225.73     \\\midrule
\multirow{2}{*}{Memory ($MB$)}     & RCV1    & 3,008.28 & 2,922.32 & 263.63     & 1,913.05                                                   & 185,851.72 \\
                                   & News20  & 3,061.99 & 2,509.29 & Seg. Fault & OOM                                                        & 128,131.43   \\
\midrule
\bottomrule
\end{tabular}%
}
\label{rep_tab:high_time_mem}
\end{table}


\end{minipage}
%\vspace{-.1in}
\end{figure}

\begin{table}[H]
\centering
\caption{The incremental/decremental learning time of the proposed method for RCV1 and News20. (ms, per tree, incre./decre.)}
\label{rep_tab:high_rate_time}
\vspace{-.1in}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lc|c|cccc|c|cccc}
\toprule
\midrule
\multirow{3}{*}{Dataset} & \multirow{3}{*}{$|D'|$} & \multicolumn{5}{c|}{Incremental Learning}                                                                                                                                     & \multicolumn{5}{c}{Decremental Learning}                                                                                                                                     \\
                         &                         & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Learning Time\\ (Ours)\end{tabular}} & \multicolumn{4}{c|}{Speedup v.s.}                                                           & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Learning Time\\ (Ours)\end{tabular}} & \multicolumn{4}{c}{Speedup v.s.}                                                           \\
                         &                         &     \multicolumn{1}{c|}{}       & XGBoost & LightGBM & CatBoost & \begin{tabular}[c]{@{}c@{}}ThunderGBM\\ (GPU)\end{tabular} &  \multicolumn{1}{c|}{}   & XGBoost & LightGBM & CatBoost & \begin{tabular}[c]{@{}c@{}}ThunderGBM\\ (GPU)\end{tabular} \\\midrule
\multirow{4}{*}{RCV1}    & 1                       & 21.431                                                                          & 214.5x  & 27.8x    & 156.6x   & 23.1x                                                      & 19.268                                                                          & 238.6x  & 30.9x    & 174.2x   & 25.7x                                                      \\
                         & 0.1\%                   & 37.707                                                                          & 121.9x  & 15.8x    & 89.0x    & 13.1x                                                      & 29.232                                                                          & 157.3x  & 20.4x    & 114.8x   & 16.9x                                                      \\
                         & 0.5\%                   & 39.428                                                                          & 116.6x  & 15.1x    & 85.1x    & 12.5x                                                      & 48.218                                                                          & 95.3x   & 12.4x    & 69.6x    & 10.3x                                                      \\
                         & 1\%                     & 43.901                                                                          & 104.7x  & 13.6x    & 76.5x    & 11.3x                                                      & 70.666                                                                          & 65.1x   & 8.4x     & 47.5x    & 7.0x                                                       \\\midrule
\multirow{4}{*}{News20}  & 1                       & 11.76                                                                           & 541.7x  & 24.2x    & -        & -                                                          & 7.718                                                                           & 825.4x  & 36.8x    & -        & -                                                          \\
                         & 0.1\%                   & 17.113                                                                          & 372.2x  & 16.6x    & -        & -                                                          & 12.363                                                                          & 515.3x  & 23.0x    & -        & -                                                          \\
                         & 0.5\%                   & 22.261                                                                          & 286.2x  & 12.8x    & -        & -                                                          & 30.076                                                                          & 211.8x  & 9.5x     & -        & -                                                          \\
                         & 1\%                     & 23.469                                                                                               & 271.4x  & 12.1x    & -        & -                                                          & 37.825                                                                                               & 168.4x  & 7.5x     & -        & -                                                       \\
\midrule
\toprule
\end{tabular}%
}
\vspace{-.1in}
\end{table}



\begin{table}[h]
\centering
\vspace{.2in}
\caption{The approximation error of leave's score between the model after addition/delection and the model retrained from scratch. $\text{Appr. Error} = \frac{\sum_{\text{all trees}}\sum_{\text{all leaves}}\text{abs}(p_{\text{add/del}} - p_{\text{retrain}})}{\sum_{\text{all trees}}\sum_{\text{all leaves}}\text{abs}(p_{\text{retrain}})}$, where $p_{\text{add/del}}$ is the leave's score after adding/deleting, $p_{\text{retrain}}$ is the leave's score of the model retraining from scratch.}
\label{tbl:approximate_error_leaf}
\vspace{-.1in}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lcccccccc}
\toprule
\midrule
          & Adult  & CreditInfo & SUSY   & HIGGS  & Optdigits & Pendigits & Letter  & Covtype \\\midrule
Add 1     & 2.42\% & 1.18\%     & 0.24\% & 0.00\% & 2.69\%    & 2.23\%    & 1.31\%  & 0.17\%  \\
Add 0.1\% & 4.59\% & 6.57\%     & 2.73\% & 1.63\% & 3.48\%    & 4.12\%    & 5.78\%  & 9.47\%  \\
Add 0.5\% & 5.10\% & 7.44\%     & 2.27\% & 3.05\% & 5.12\%    & 4.50\%    & 10.45\% & 11.68\% \\
Add 1\%   & 5.30\% & 7.43\%     & 3.07\% & 3.89\% & 5.92\%    & 4.70\%    & 11.75\% & 10.01\% \\
Add 10\%  & 4.25\% & 8.33\%     & 1.07\% & 1.73\% & 4.64\%    & 4.42\%    & 13.34\% & 4.96\%  \\
Add 50\%  & 3.55\% & 0.00\%     & 0.00\% & 1.51\% & 0.00\%    & 0.00\%    & 6.26\%  & 0.01\%  \\
Add 80\%  & 0.00\% & 0.00\%     & 0.00\% & 0.00\% & 0.00\%    & 0.00\%    & 0.00\%  & 0.00\%  \\\midrule
Del 1     & 1.21\% & 0.00\%     & 0.00\% & 0.00\% & 0.01\%    & 0.19\%    & 0.57\%  & 0.28\%  \\
Del 0.1\% & 3.63\% & 3.80\%     & 0.79\% & 0.72\% & 1.40\%    & 0.50\%    & 1.88\%  & 4.31\%  \\
Del 0.5\% & 3.58\% & 3.76\%     & 0.18\% & 0.56\% & 2.52\%    & 1.15\%    & 3.49\%  & 6.04\%  \\
Del 1\%   & 3.40\% & 3.16\%     & 0.15\% & 0.65\% & 3.07\%    & 1.73\%    & 3.74\%  & 4.48\%  \\
Del 10\%  & 0.27\% & 0.39\%     & 0.00\% & 0.16\% & 1.67\%    & 0.97\%    & 1.35\%  & 0.46\%  \\
Del 50\%  & 0.00\% & 0.00\%     & 0.00\% & 0.00\% & 0.00\%    & 0.00\%    & 0.00\%  & 0.00\%  \\
Del 80\%  & 0.00\% & 0.00\%     & 0.00\% & 0.00\% & 0.00\%    & 0.00\%    & 0.00\%  & 0.00\% \\
\midrule
\bottomrule
\end{tabular}%
}
\end{table}


\section{Data Addition with More Classes}\label{apd:more_classes}


Our framework can update data with unseen classes. We divide the dataset into sub-datasets based on labels (e.g., Optdigits has 10 labels, so we divide it into 10 sub-datasets). We train a model on the first sub-dataset and test it on two test datasets: 1) the original full test dataset with all labels, and 2) the partial test dataset  with only the learned labels. We fine-tune the model with a new sub-dataset through incremental learning until learning the full dataset, testing the model on both test datasets after each training. Figure~\ref{fig:finetune_classes} shows that the accuracy of incremental learning and retraining is nearly identical on both the full and partial datasets. Note that the decrease in accuracy on the partial dataset is likely due to the increasing complexity of the learned data, which leads to a decrease in accuracy.




\input{wrapfigs/finetune_classes}


\begin{figure*}[thbp]
\vspace{.2in}
\centering
\includegraphics[width=.95\textwidth]{figs/unids2acc_bar.pdf}
\caption{Different fine-tuning ratio.}
\vspace{-.2in}
\label{fig:train_tune_acc}
\end{figure*}




\section{Approximation Error of Leaf Scores}\label{apd:error_leaf_scores}

As mentioned in Section~\ref{sec:lazy_update}, outdated derivatives are used in gain computation to reduce the cost of updating derivatives. However, these outdated derivatives are only applied to nodes where the best split remains unchanged. When a sub-tree requires retraining, the derivatives are updated. Therefore, using outdated derivatives typically occurs when $|D'|$ is small, as fewer data modifications result in fewer changes to the best splits. Conversely, when more data is added or deleted, $|D'|$ becomes larger, increasing the likelihood of changes to the best splits in some nodes. As a result, the sub-trees are retrained, and the derivatives for the data reaching those nodes are updated.

To confirm the effect of using outdated derivatives during online learning, we report the result for the approximation error of leaf scores in Table~\ref{tbl:approximate_error_leaf}. $\text{Appr. Error} = \frac{\sum_{\text{all trees}}\sum_{\text{all leaves}}\text{abs}(p_{\text{add/del}} - p_{\text{retrain}})}{\sum_{\text{all trees}}\sum_{\text{all leaves}}\text{abs}(p_{\text{retrain}})}$, where $p_{\text{add/del}}$ is the leaf score after adding/deleting, and $p_{\text{retrain}}$ is the leaf score of the model retraining from scratch. Please note that the retrained model has the same structure and split in all nodes of all trees as the model after adding/deleting, and we only update the latest residual and hessian to calculate the latest leaf score. When the number of added/deleted data increases, the error will increase because our method uses outdated derivatives if the best splits remain unchanged. When the number of add/delete is large enough, almost all nodes in the model will be retrained because their best splits have changed, so the error becomes 0.


\begin{wrapfigure}[23]{r}{.5\textwidth}
\vspace{-.4in}
\mbox{
%\hspace{-.2in}
\includegraphics[width=.24\textwidth]{figs/pendigits_incre_split2time.pdf}
\includegraphics[width=.24\textwidth]{figs/pendigits_decre_split2time.pdf}
}
\mbox{
%\hspace{-.2in}
\includegraphics[width=.24\textwidth]{figs/pendigits_incre_sample2nodes.pdf}
\includegraphics[width=.24\textwidth]{figs/pendigits_decre_sample2nodes.pdf}
}
\mbox{
%\hspace{-.2in}
\includegraphics[width=.24\textwidth]{figs/pendigits_split2acc_bar_incre.pdf}
\includegraphics[width=.24\textwidth]{figs/pendigits_split2acc_bar_decre.pdf}
}
\caption{The impact of sampling rate on time, number of retrain nodes, and test accuracy during incremental/decremental learning.}
\label{fig:sampling_rate}
%\vspace{-.2in}
\end{wrapfigure}


\section{Ablation Study}\label{apd:ablation_study}
In this section, we discuss the impact of different hyper-parameter settings on the performance of our framework, e.g., time and accuracy.





\subsection{Size of Online Dataset $\boldsymbol{|D'|}$.}
Different sizes of online learning dataset $D'$ can have varying impacts on both the accuracy and time of the online learning process. Figure~\ref{fig:train_tune_acc} shows the impact of different data addition settings on test accuracy. Across all datasets, our framework achieved nearly the same test accuracy, which validates the effectiveness of our online learning framework. Decremental learning also has similar results.



Figure~\ref{fig:online_learning_time} shows the influence of $|D_\textit{in}|$ on incremental/decremental learning time. We only present the experiment on 2 datasets each for incremental/decremental learning, due to the results on other datasets show a similar trend. These results show that the online learning time increase when the size of $D_\textit{in}$ increase. The reason is straightforward: as the size of $D_\textit{in}$ increases, the model undergoes more significant changes, resulting in unstable splits. This leads to a greater number of sub-trees that require retraining, ultimately consuming more time. Figure~\ref{fig:tuneids2node} provides evidence to support this observation. It illustrates the accumulated number of retrained nodes -- how many nodes need to be retrained. As the size of $D_\textit{in}$ increases, the number of nodes that need to be retrained also increases. This leads to longer learning times.

\begin{figure}[t]
\vspace{-.1in}
\begin{minipage}{.49\textwidth}
\begin{figure}[H]
\centering
\mbox{
%\hspace{-.2in}
\includegraphics[width=.5\textwidth]{figs/adult_incre_unids2time.pdf}
\includegraphics[width=.5\textwidth]{figs/letter_incre_unids2time.pdf}
}
\mbox{
%\hspace{-.2in}
\includegraphics[width=.5\textwidth]{figs/covtype_decre_unids2time.pdf}
\includegraphics[width=.5\textwidth]{figs/SUSY_decre_unids2time.pdf}
}
\caption{The impact of $|D'|$ on average learning time in incremental/decremental learning (top/bottom row).}
\label{fig:online_learning_time}
\end{figure}
\end{minipage}
\hspace{.1in}
\begin{minipage}{.49\textwidth}
\begin{figure}[H]
\centering
\vspace{.1in}
\mbox{
%\hspace{-.2in}
\includegraphics[width=.5\textwidth]{figs/adult_incre_unids2nodes.pdf}
\includegraphics[width=.5\textwidth]{figs/letter_incre_unids2nodes.pdf}
}
\mbox{
%\hspace{-.2in}
\includegraphics[width=.5\textwidth]{figs/covtype_decre_unids2nodes.pdf}
\includegraphics[width=.5\textwidth]{figs/SUSY_decre_unids2nodes.pdf}
}
\vspace{-.1in}
\caption{The impact of $|D'|$ on the accumulated number of retrained nodes for each iteration in incr./decr. learning (top/bottom row).}
\label{fig:tuneids2node}
\end{figure}
\end{minipage}
\end{figure}


\begin{figure}[thbp]
%\vspace{-.2in}
% \vspace{-.1in}
\centering
\mbox{
%\hspace{-.22in}
\includegraphics[width=.25\textwidth]{figs/lazy2time_incre.pdf}
\hspace{.2in}
\includegraphics[width=.25\textwidth]{figs/lazy2acc_incre.pdf}
\hspace{.2in}
\includegraphics[width=.25\textwidth]{figs/lazy2func_incre.pdf}
}
% \vspace{-.1in}
\caption{The impact of split robustness tolerance on the learning time, test accuracy, and model functional similarity $\phi$ in incremental learning.}
\label{fig:tolerance}
% \vspace{-.9in}
\end{figure}




\begin{figure}[thbp]
\centering
% \vspace{.2in}
\mbox{
% \hspace{-.18in}
\includegraphics[width=.24\textwidth]{figs/bins2time_incre.pdf}
\includegraphics[width=.24\textwidth]{figs/bins2acc_incre.pdf}
\includegraphics[width=.24\textwidth]{figs/J2time_incre.pdf}
\includegraphics[width=.24\textwidth]{figs/J2acc_incre.pdf}
}
\vspace{-.1in}
\caption{The impact of the $\#$ bins and $\#$ leaves on the acceleration factor of incremental learning (adding $1$ data point).}
\label{fig:bins_leaves}
% \vspace{-3in}
\end{figure}




\subsection{Split Random Sampling}
Split random sampling is designed to reduce the frequency of retraining by limiting the number of splits. As mentioned in Section~\ref{ssec:split_sampling}, a smaller sampling rate leads to more stable splits, resulting in fewer nodes that require retraining and shorter online learning time. Figure~\ref{fig:sampling_rate} shows the impact of sampling rate $\alpha$ in split random sampling. 
The figures at the top demonstrate that when the sample rate is reduced, a smaller number of split candidates are taken into account, leading to an expected decrease in online learning time. However, there is no significant difference between $5\%$ and $10\%$ in the Pendigits dataset. The figures in the second row show the accumulated number of retrained nodes. It also shows that as the sample rate decreases, the splits become more stable, resulting in fewer nodes that require retraining. In Pendigits, since the number of nodes that require retraining is similar for $5\%$ and $10\%$, it results in a minimal difference in the online learning time, as mentioned above. However, interestingly, for example in $100\%$ sampling rate, although there are fewer retraining in incremental learning, it take more time during learning process, because incremental learning does not have derivatives of the data to be added. Therefore, more time is needed to calculate their derivatives. On the contrary, decremental learning can reuse the stored derivatives of the training process, resulting in less time.
The bottom row shows the impact of the sampling rate on the test accuracy. The test accuracy remains almost identical across all sampling rates. Similar results can be observed in other datasets.






\subsection{Split Robustness Tolerance}
Split robustness tolerance aims to enhance the robustness of a split in online learning. As the observation in Figure~\ref{fig:split_change_observation}, most best splits will be changed to second-best. Although the best split may change, we can avoid frequent retraining if we allow the split to vary within a certain range. For a node with $\lceil\alpha B\rceil$ potential splits, if the current split remains within the top $\lceil\sigma\alpha B\rceil$, we will continue using it. Here $\sigma$ $(0 \leq \sigma \leq 1)$ is the robustness tolerance. Figure~\ref{fig:tolerance} illustrates the impact of split robustness tolerance $\sigma$ on learning time, test accuracy, and functional similarity $\phi$ in incremental learning. 
To obtain more pronounced experimental results, in this experiment, we set $|D'| = 1\%\times |D_\textit{tr}|$.
The figure on the left shows that the learning time decreases as the tolerance level increases. Although test accuracy changes only slightly (middle figure), the functional similarity $\phi$ drops significantly (right figure). For example, in the Letter dataset, $\phi$ drops about $5\%$ from $\sigma=0$ to $\sigma=0.5$. This demonstrates that higher tolerance levels result in faster learning by avoiding retraining, but with a trade-off of decreased functional similarity. Therefore, we suggest $\sigma$ should not be greater than $0.15$. Similar results can be obtained on decremental learning.



\input{tables/error_rate_iterations_tbl}
\input{tables/time_iterations_tbl}
\input{tables/backdoor_iterations}


\subsection{Number of Bins and Leaves}
In online learning procedure, the number of bins and leaves also affects the online learning time. We report the impact of varying the number of bins $(128, 256, \cdots,$ $4096)$ and leaves $(4, 10, 20, 40, 60, \cdots, 200)$ on the acceleration factor of incremental learning (adding 1 data point) in Figure \ref{fig:bins_leaves}. The number of bins has few effect on both accuracy and the speed of online learning as shown in the top row of the figures. In terms of the number of leaves, when it exceeds 20, the accuracy tends to stabilize, except for Covtype, as shown in the bottom row of the figures.
For smaller datasets (Adult, Optdigits, Pendigits, Letter), the more the number of leaves, the lower the acceleration factor for incremental learning. However, for larger datasets (CreditInfo, SUSY, HIGGS, Covtype), the more the number of leaves, the greater the acceleration is. Especially for HIGGS, the largest dataset in our experiments, the acceleration can be more than 100x.



\subsection{Number of Iterations}
The number of base learners is important in practical applications. We provide additional results for different numbers of base learners in Tables~\ref{tbl:error_rate_iterations} and \ref{tbl:time_iterations}. Table~\ref{tbl:error_rate_iterations} reports the test error rate after training, adding, and deleting base learners in GBDT models with varying iterations, demonstrating that our method achieves a comparable error rate across different iterations. Table~\ref{tbl:time_iterations} shows the time consumption for incremental and decremental learning, illustrating that our methods are substantially faster than retraining a model from scratch, particularly in cases where a single data sample is added or deleted.

Additionally, to confirm that our method can effectively add and delete data samples across various iterations, we report results on backdoor attacks for different iterations, as shown in Table~\ref{tbl:backdoor_iterations}. These results confirm that our method successfully adds and removes data samples from the model across different numbers of iterations.


\end{document}