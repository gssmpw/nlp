\section{Related Work}
\vspace{-.02in}
\textbf{Incremental Learning} is a technique in machine learning that involves the gradual integration of new data into an existing model, continuously learning from the latest data to ensure performance on new data~\cite{DBLP:journals/natmi/VenTT22}. It has been a open problem in machine learning, and has been studied in convolutional neural network (CNN)~\cite{DBLP:journals/tsmc/PolikarUUH01, DBLP:conf/cvpr/KuzborskijOC13, DBLP:conf/cvpr/0001WYMPZ22}, DNN~\cite{DBLP:journals/ieeemm/HussainLT23, dekhovich2023continual}, SVM~\cite{DBLP:journals/cluster/ChenXXZ19, DBLP:conf/nips/CauwenberghsP00} and RF~\cite{DBLP:conf/icip/WangWCL09, DBLP:journals/corr/abs-2009-05567}. In gradient boosting, iGBDT offers incremental updates~\cite{DBLP:journals/npl/ZhangZSAFS19}, while other methods~\cite{DBLP:conf/nips/BeygelzimerHKL15, DBLP:conf/iccvw/Babenko0B09} extend GB to online learning. However, these methods do not support removing data.


\textbf{Decremental Learning} allows for the removal of trained data and eliminates their influence on the model, which can be used to delete outdated or privacy-sensitive data~\cite{DBLP:conf/sp/BourtouleCCJTZL21, DBLP:journals/corr/abs-2209-02299, DBLP:conf/nips/SekhariAKS21, DBLP:journals/csur/XuZZZY24}. It has been researched in various models, including CNN~\cite{DBLP:journals/corr/abs-2304-02049, DBLP:journals/corr/abs-2111-08947}, DNN~\cite{chen2023boundary, DBLP:conf/uss/ThudiJSP22}, SVM~\cite{DBLP:conf/nips/KarasuyamaT09, DBLP:conf/nips/CauwenberghsP00}, Naive Bayes~\cite{DBLP:conf/sp/CaoY15}, K-means~\cite{DBLP:conf/nips/GinartGVZ19}, RF~\cite{DBLP:conf/sigmod/SchelterGD21, DBLP:conf/icml/BrophyL21}, and GB~\cite{DBLP:journals/pacmmod/WuZLH23, DBLP:journals/corr/abs-2311-13174}. In random forests, DaRE~\cite{DBLP:conf/icml/BrophyL21} and a decremental learning algorithm~\cite{DBLP:conf/sigmod/SchelterGD21} are proposed for data removal with minimal retraining and latency.

However, in GBDT, trees in subsequent iterations rely on residuals from previous iterations, making decremental learning more complicated. DeltaBoost \citet{DBLP:journals/pacmmod/WuZLH23} simplified the dependency for data deletion by dividing the dataset into disjoint sub-datasets, while a recent study \citet{DBLP:conf/kdd/LinCL023} proposed an efficient unlearning framework without simplification, utilizing auxiliary information to reduce unlearning time. Although effective, its performance on large datasets remains unsatisfactory.



\vspace{-.08in}