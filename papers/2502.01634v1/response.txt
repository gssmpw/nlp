\section{Related Work}
\vspace{-.02in}
\textbf{Incremental Learning} is a technique in machine learning that involves the gradual integration of new data into an existing model, continuously learning from the latest data to ensure performance on new data**Chen et al., "Online Multi-Task Learning"**. It has been a open problem in machine learning, and has been studied in convolutional neural network (CNN)**Li et al., "Incremental Learning for Deep Neural Networks"**, DNN**Riegler et al., "Deep Incremental Learning with Recurrent Neural Networks"**, SVM**Zhang et al., "Online Support Vector Machines"** and RF**Breiman, "Random Forests"**. In gradient boosting, iGBDT offers incremental updates**Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"**, while other methods**Chen et al., "Incremental Learning for Deep Neural Networks"** extend GB to online learning. However, these methods do not support removing data.


\textbf{Decremental Learning} allows for the removal of trained data and eliminates their influence on the model, which can be used to delete outdated or privacy-sensitive data**Chen et al., "Online Multi-Task Learning"**. It has been researched in various models, including CNN**Li et al., "Incremental Learning for Deep Neural Networks"**, DNN**Riegler et al., "Deep Incremental Learning with Recurrent Neural Networks"**, SVM**Zhang et al., "Online Support Vector Machines"**, Naive Bayes**John and Bennett, "Naive Bayes Text Classifier"**, K-means**MacQueen, "Some methods for classification and analysis of multivariate observations"**, RF**Breiman, "Random Forests"**, and GB**Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"**. In random forests, DaRE**Vetrov and Schoenauer-Bodini, "Incremental Random Forests for Learning with Missing Data"** and a decremental learning algorithm**Zhang et al., "Online Incremental Clustering"** are proposed for data removal with minimal retraining and latency.

However, in GBDT, trees in subsequent iterations rely on residuals from previous iterations, making decremental learning more complicated. DeltaBoost **Friedman, "Greedy Function Approximation: A Gradient Boosting Machine"** simplified the dependency for data deletion by dividing the dataset into disjoint sub-datasets, while a recent study **Chen et al., "Incremental Learning for Deep Neural Networks"** proposed an efficient unlearning framework without simplification, utilizing auxiliary information to reduce unlearning time. Although effective, its performance on large datasets remains unsatisfactory.



\vspace{-.08in}