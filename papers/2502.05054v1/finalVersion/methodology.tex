\section{Methodology}
\label{meth}

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/MSR2025_Methodology.png}
  \caption{Description of the methodology}
  \label{fig:methodology}
    
\end{figure*}

\begin{table*}[h!]
\caption{Major Ethereum Events and Their Impact on Development Activity}
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l l l l }
\hline
\textbf{Event} & \textbf{Date} & \textbf{Significance} & \textbf{Impact} \\ \hline
\textit{Frontier Release} & July 30, 2015 & First official Ethereum release, allowing mining and dApp development. & Increased development activity and community engagement. \\ 
\textit{The DAO Creation and Hack} & Jun 17, 2016 & Major project built on Ethereum, hacked leading to a hard fork. & Led to Ethereum (ETH) and Ethereum Classic (ETC) split, impacting governance and developer focus. \\ 
\textit{Cryptocurrency Boom} & Dec 17, 2017 & Ethereum's price peak during the broader cryptocurrency surge. & Raised mainstream interest, shifting developer priorities. \\ 
\textit{Market Crash (COVID-19)} & Mar 13, 2020 & Ethereum's market value dropped with the global downturn. & Sparked growth in DeFi, shifting focus to decentralized finance projects. \\ 
\textit{Beacon Chain Launch} & Dec 1, 2020 & Introduction of the proof-of-stake consensus mechanism. & Changed development approaches and resource allocation. \\ 
\textit{London Hard Fork} & Aug 5, 2021 & Implemented EIP-1559, restructuring Ethereum's fee system. & Required developers to adapt to new fee structures. \\ 
\textit{Arrow Glacier Update} & Dec 9, 2021 & Delayed the difficulty bomb, encouraging the proof-of-stake transition. & Maintained network functionality, impacting development timelines. \\ 
\textit{Ropsten Testnet Merge} & Jun 8, 2022 & Key test for the mainnet transition to proof-of-stake. & Provided insights and preparation for the mainnet merge. \\ 
\textit{The Merge} & Sep 15, 2022 & Transition of Ethereum mainnet to proof-of-stake. & Triggered significant updates and adaptations for developers. \\ 
\textit{Shanghai Upgrade} & Apr 12, 2023 & Enabled staked ETH withdrawals for Ethereum 2.0. & Improved key network functionality, influencing developer activity. \\ \hline
\end{tabular}
}
\label{tab:events}
\end{table*}




Our systematic investigation of developer activity in the Ethereum ecosystem follows a structured approach, from detailed data collection through multiple layers of analysis, as illustrated in Fig. \ref{fig:methodology}. This methodology enables a thorough examination of how major events influence development patterns across key repositories.

Starting with data collection from GitHub, shown in the left section of Fig. \ref{fig:methodology},  we identified and extracted data from 10 central Ethereum repositories, focusing on core infrastructure and widely-used development tools. For each repository, we systematically collected three primary datasets: commit histories with metadata, issue tracking information (including creation and resolution timestamps), and developer comments.

The second phase is event selection. We analyzed Ethereum’s history to identify 10 major events that have shaped the ecosystem, listed in Table \ref{tab:events}. These events, spanning from Ethereum’s launch to recent protocol updates, offer varied contexts for examining developer responses to distinct challenges and changes.

The identification and classification of major events followed a three-phase protocol. First, we constructed an event pool by aggregating data from authoritative sources: Ethereum Foundation communications, network upgrade histories, market data, and security incident reports during 2014-2024. Two authors then independently evaluated each event using three classification criteria. \textbf{Infrastructure Impact} related to network-wide protocol changes and technical modifications (e.g., \textit{The Merge}, \textit{London Hard Fork}). \textbf{Market/Community Impact} includes significant market reactions and community-driven changes (e.g., \textit{COVID-19 crash}, \textit{The DAO hack}). \textbf{Development Trajectory Impact} covers events requiring coordinated development efforts or establishing new patterns (e.g., \textit{Arrow Glacier Update}, \textit{Frontier Release}). Events meeting criteria from at least two categories were classified as major, ensuring broad ecosystem impact rather than isolated effects. For diverging classifications, a third author conducted an independent review. Event validation involved examining Ethereum development community archives (GitHub discussions, Improvement Proposals, official announcements), yielding our final set of 10 major events.

The events span from the Frontier Release (July 30, 2015) to the Shanghai Upgrade (April 12, 2023), covering technical milestones like The Merge, market events such as the COVID-19 crash, and community-driven incidents like The DAO hack. The DAO Creation which took place on April 12, 2016 and the DAO Hack which took place on June 17, 2016 are two separate but very close events (less than 90 days apart). As explicated below, we avoided overlapping effects for streamlining our analysis; therefore, we took the date of the hack as the reference date. Table \ref{tab:events} provides details on each event’s significance and its impact on development activity.

The core of our analysis examines four distinct aspects of developer activity, each aligned with a specific research question.

To answer RQ1, we analyzed three key metrics across all repositories. The first metric, \textbf{Issue Resolution Time}, measures the time between issue creation and closure. The second metric, \textbf{Monthly Comment Count}, reflects the intensity of collaboration. Lastly, \textbf{Monthly Commit Count} serves as an indicator of development activity.

Based on initial observations and statistical analysis, we establish a \textbf{Pre-Event Window} covering the 90 days before each event to set baseline activity levels, and a \textbf{Post-Event Window} covering the 90 days following the event to track immediate and residual effects. For each metric, we conduct seasonal decomposition, partitioning the data into three components: \textbf{Trend}, which captures long-term movement patterns; \textbf{Seasonality}, which reflects regular, recurring fluctuations; and \textbf{Residuals}, representing remaining variation after accounting for trend and seasonality.

To validate these components, we use autocorrelation function (ACF) analysis, assessing correlations between time series values at different lags to ensure robustness in our decomposition and interpretation.

To answer RQ2, we conducted a detailed analysis of commit patterns. Our first step focused on data preparation and overview, where we normalized commit counts to enable fair comparisons across repositories with varying activity levels.
Next, we performed descriptive statistical analysis, calculating the means, medians, and standard deviations of commit activity for both pre-event and post-event periods. To understand the distribution characteristics of our data, we examined histograms, Q-Q plots, and conducted Shapiro-Wilk tests\cite{shapiro1965analysis}.

For our inferential statistical analysis, we examined each repository-event pair by testing the following hypothesis: \textbf{$H_0$:} \textit{The distribution of commit counts in the 90 days before the event is the same as the distribution in the 90 days after the event} (with \textbf{$H_A$} stating that the distribution differs). Given that each repository serves different purposes in the Ethereum ecosystem and operates independently, we treated them as separate experiment groups. For each repository, we conducted 10 tests (one per event). Since repositories are independent, for each repository we applied the Benjamini-Hochberg procedure \cite{Thissen2002Quick} to control the False Discovery Rate (FDR) at $\alpha = .05$ across its 10 events, which is suitable for balancing Type I and Type II errors \cite{Keselman2002Controlling} in studies akin to ours.

To quantify the magnitude of changes, we calculated effect sizes using the Z-score through the formula $r = Z / \sqrt{N}$, where $Z$ represents the standardized test statistic and $N$ is the number of observations. We interpreted these values using established thresholds: $>.10$ for small effects, $>.30$ for moderate effects, and $>.50$ for large effects.

Finally, to validate that our results were not due to random fluctuations, we performed a control analysis by testing against 100 randomly selected events and reshuffling time series.

For RQ3, We employed survival analysis techniques to examine issue management patterns. First, issue lifecycles are tracked by visualizing resolved issues at their closure points, while unresolved issues are observed until the study’s end. To account for both resolved and unresolved (censored) issues, Kaplan-Meier estimation is used, producing survival curves that represent resolution probabilities over time. Event impacts are then assessed through log-rank tests, comparing survival distributions before and after events (against the null hypothesis: \textbf{$H_0$:} \textit{H0: Events do not change how quickly issues get resolved}), with a significance level of $\alpha = .05$, and applying the Benjamini-Hochberg procedure to control the FDR for consistency with RQ2.


The final component of our methodology, for answering RQ4, examines collaboration networks. Developer interactions are represented by linking individuals who comment on the same issues, with the strength of each connection based on the number of shared issues. To observe network evolution, temporal analysis is performed over twelve-month periods segmented into three phases: a baseline phase from six to three months pre-event, an impact period spanning three months before and after the event, and a stabilization phase covering three to six months post-event.

Motif analysis identifies recurring interaction patterns, using the Configuration Model as a null model and Z-scores to quantify the significance of these patterns. Consistent 90-day windows established in RQ1 are applied throughout all analyses, ensuring comparable metrics across repositories. This methodology provides a multi-dimensional view of event impacts, examining individual contributions, issue resolution efficiency, and collaborative dynamics within the ecosystem.

Two events in our dataset—London Hard Fork/Arrow Glacier Update and Ropsten Testnet Merge/The Merge—have overlapping post-event and pre-event periods, where the three-month window of one event extends into the three-month period of another. We maintained a consistent 90-day analysis period for all events, based on the empirical findings from RQ1. Our analysis examines the relative changes in developer activity surrounding each event, with the validation against random events in RQ2 confirming the observed patterns.


