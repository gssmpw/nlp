\section{Related Work}
\label{sec:formatting}

\subsection{Traditional Person Re-ID}
Person re-identification (Re-ID) is a fundamental task in computer vision, which aims to match the same individual across different camera views based on visual features. Recent studies in person Re-ID carefully designed settings and developed models to tackle every specific scenario.
Standard person Re-ID**Zhao et al., "Deep Person Re-Identification"**, which aims to match individuals across cameras based on visual features. These methods distinguish pedestrian identities based on body posture and appearance.
Cloth-changing Re-ID (CC Re-ID) **Li et al., "Adversarial Learning for Cloth-Changing Person Re-identification"** is a more challenging variant where individuals change their clothing between camera views. It assists the model in extracting non-clothing information for identity determination. CSSC **Gao et al., "CSSC: Cross-View Synthesis-based Semantic Contextualization for Person Re-ID"** introduces a framework that leverages abundant semantics within pedestrian images to extract identity features. Visible-infrared person ReID (VI-ReID) methods **Hou et al., "Visible-Infrared Person Re-Identification with Multi-Spectral Feature Fusion"** extract pedestrian features under low-light environments. DDAG **Zhou et al., "Dense Discriminative Attention Graph for Person Re-IDentification"** improves performance by leveraging intra-modality and cross-modality contextual cues to enhance feature discriminability and robustness to noise. Text-to-image Re-ID **Liu et al., "Text-to-Image Person Re-Identification with Cross-Modal Contrastive Learning"** aims to identify pedestrians based on textual descriptions. It requires the model to understand and align linguistic descriptions with visual attributes. Zhao \textit{et al.},  **Zhao et al., "Multi-modal Uncertainty Modeling for Text-to-Image Person Re-IDentification"** proposes a novel method to model multi-modal uncertainty and semantic alignment using Gaussian distributions and a cross-modal circle loss. However, different settings within person Re-ID focus on distinct visual features, making it difficult to effectively integrate these settings into a single model. Consequently, we intend to develop a versatile ‘one-for-all’ framework to interactively ask the machine to help with the person retrieval task.


\subsection{VLM-driven Person Re-ID}
Vision-language models (VLMs) **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision"** have garnered significant attention in the AI community due to their impressive generalization capabilities. Recent studies have started investigating the incorporation of VLMs into the person Re-ID paradigm.
Tan \textit{et al.}.  **Tan et al., "Text-to-Image Person Re-IDentification with Multi-modal Large Language Models"** and Yang \textit{et al.}.  **Yang et al., "Unified Text-to-Image Person Re-IDentification with Instruction Templates"** primarily focuses on the text-to-image person Re-ID task. The former uses multi-modal large language models (MLLMs) to caption images according to various templates, thereby addressing issues related to the quantity and quality of textual descriptions. The latter proposes a common instruction template and uses features computed by MLLMs to train person Re-ID models. Instruct-ReID **Qian et al., "Instruct-ReID: Unified Person Re-identification with Instruction-based Model"** is the first work that unifies multiple person Re-ID settings within a single model, generating task-specific instructions and combining instruction encodings with visual encodings for Re-ID training.
Despite significant progress in integrating VLMs into person Re-ID, existing methods face key limitations. Firstly, they fail to fully utilize VLMs' perception and instruction-following abilities. Secondly, many approaches rely on rigid, template-based textual descriptions, limiting adaptability and scalability. Lastly, while some methods unify different Re-ID settings, their flexibility remains constrained, making it difficult to apply them to common scenarios. In this paper, we present a versatile `one-for-all' Re-ID framework that leverages VLMs for interactive, freeform person Re-ID.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{chatreid_iccv_3.pdf}
    \vspace{-0.2in}
\caption{Overview of the ChatReID framework. (a) shows the schematic of ChatReID. (b) shows the three-stage HPT strategy.
}
\vspace{-0.2in}
\label{fig:framework}
\end{figure*}



% ---------------------------------------------------
% ----------------    Methodology
% ---------------------------------------------------