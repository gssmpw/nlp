\section{Related Work}
\label{sec:formatting}

\subsection{Traditional Person Re-ID}
Person re-identification (Re-ID) is a fundamental task in computer vision, which aims to match the same individual across different camera views based on visual features. Recent studies in person Re-ID carefully designed settings and developed models to tackle every specific scenario.
Standard person Re-ID~\cite{zheng2017person,zheng2017discriminatively,ning2020feature,tan2021incomplete,hermans2017defense,yuan2020defense}, which aims to match individuals across cameras based on visual features. These methods distinguish pedestrian identities based on body posture and appearance.
Cloth-changing Re-ID (CC Re-ID)~\cite{qian2020long,bansal2022cloth,jin2022cloth,hong2021fine,guo2023semantic} is a more challenging variant where individuals change their clothing between camera views. It assists the model in extracting non-clothing information for identity determination. CSSC~\cite{wang2024content} introduces a framework that leverages abundant semantics within pedestrian images to extract identity features. Visible-infrared person ReID (VI-ReID) methods~\cite{feng2019learning,wang2019learning,huang2023deep8vc} extract pedestrian features under low-light environments. DDAG~\cite{ye2020dynamic} improves performance by leveraging intra-modality and cross-modality contextual cues to enhance feature discriminability and robustness to noise. Text-to-image Re-ID~\cite{shao2023unified,han2023text} aims to identify pedestrians based on textual descriptions. It requires the model to understand and align linguistic descriptions with visual attributes. Zhao \textit{et al.}~\cite{zhao2024unifying} proposes a novel method to model multi-modal uncertainty and semantic alignment using Gaussian distributions and a cross-modal circle loss. However, different settings within person Re-ID focus on distinct visual features, making it difficult to effectively integrate these settings into a single model. Consequently, we intend to develop a versatile ‘one-for-all’ framework to interactively ask the machine to help with the person retrieval task.


\subsection{VLM-driven Person Re-ID}
Vision-language models (VLMs)~\cite{bai2023qwen,wang2024qwen2,yang2023dawn,liu2024visual} have garnered significant attention in the AI community due to their impressive generalization capabilities. Recent studies have started investigating the incorporation of VLMs into the person Re-ID paradigm.
Tan \textit{et al.}.~\cite{tan2024harnessing} and Yang \textit{et al.}.~\cite{yang2024mllmreid} primarily focuses on the text-to-image person Re-ID task. The former uses multi-modal large language models (MLLMs) to caption images according to various templates, thereby addressing issues related to the quantity and quality of textual descriptions. The latter proposes a common instruction template and uses features computed by MLLMs to train person Re-ID models. Instruct-ReID~\cite{he2024instruct} is the first work that unifies multiple person Re-ID settings within a single model, generating task-specific instructions and combining instruction encodings with visual encodings for Re-ID training.
Despite significant progress in integrating VLMs into person Re-ID, existing methods face key limitations. Firstly, they fail to fully utilize VLMs' perception and instruction-following abilities. Secondly, many approaches rely on rigid, template-based textual descriptions, limiting adaptability and scalability. Lastly, while some methods unify different Re-ID settings, their flexibility remains constrained, making it difficult to apply them to common scenarios. In this paper, we present a versatile `one-for-all' Re-ID framework that leverages VLMs for interactive, freeform person Re-ID.


\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{chatreid_iccv_3.pdf}
    \vspace{-0.2in}
\caption{Overview of the ChatReID framework. (a) shows the schematic of ChatReID. (b) shows the three-stage HPT strategy.
}
\vspace{-0.2in}
\label{fig:framework}
\end{figure*}



% ---------------------------------------------------
% ----------------    Methodology
% ---------------------------------------------------