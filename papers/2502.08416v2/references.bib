
@inproceedings{ramesh_gatsbi_2021,
	title = {{GATSBI}: {Generative} {Adversarial} {Training} for {Simulation}-{Based} {Inference}},
	shorttitle = {{GATSBI}},
	url = {https://openreview.net/forum?id=kR1hC6j48Tp},
	abstract = {Simulation-based inference (SBI) refers to statistical inference on stochastic models for which we can generate samples, but not compute likelihoods. Like SBI algorithms, generative adversarial networks (GANs) do not require explicit likelihoods. We study the relationship between SBI and GANs, and introduce GATSBI, an adversarial approach to SBI. GATSBI reformulates the variational objective in an adversarial setting to learn implicit posterior distributions. Inference with GATSBI is amortised across observations, works in high-dimensional posterior spaces and supports implicit priors. We evaluate GATSBI on two common SBI benchmark problems and on two high-dimensional simulators. On a model for wave propagation on the surface of a shallow water body, we show that GATSBI can return well-calibrated posterior estimates even in high dimensions. On a model of camera optics, it infers a high-dimensional posterior given an implicit prior, and performs better than a state-of-the-art SBI approach. We also show how GATSBI can be extended to perform sequential posterior estimation to focus on individual observations. Overall, GATSBI opens up opportunities for leveraging advances in GANs to perform Bayesian inference on high-dimensional simulation-based models.},
	language = {en},
	urldate = {2025-01-27},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ramesh, Poornima and Lueckmann, Jan-Matthis and Boelts, Jan and Tejero-Cantero, Alvaro and Greenberg, David S. and Goncalves, Pedro J. and Macke, Jakob H.},
	month = oct,
	year = {2021},
}

@inproceedings{panigrahi_survey_2021,
	address = {Singapore},
	title = {A {Survey} on {Transfer} {Learning}},
	isbn = {9789811559716},
	doi = {10.1007/978-981-15-5971-6_83},
	abstract = {To facilitate learning in a target domain, transfer learning borrows knowledge from a source domain. What and how to transfer are two main issues that need to be addressed in transferring learning. Different transfer learning algorithms result in different knowledge transferred between them for a couple of domains. To find the optimal transfer learning algorithm that maximizes learning efficiency in the target domain, scientists need to investigate all current computationally intractable transfer learning algorithms exhaustively. A sub-optimal algorithm is selected as a trade-off, which in an ad hoc way requires considerable expertise. In instructional psychology, meanwhile, it is commonly recognized that people enhance the transfer of teaching abilities to decide what to transfer. This paper discusses what is transfer learning, the different transfer learning techniques, future scope, and applications of it.},
	language = {en},
	booktitle = {Intelligent and {Cloud} {Computing}},
	publisher = {Springer},
	author = {Panigrahi, Santisudha and Nanda, Anuja and Swarnkar, Tripti},
	editor = {Mishra, Debahuti and Buyya, Rajkumar and Mohapatra, Prasant and Patnaik, Srikanta},
	year = {2021},
	keywords = {Computer vision, Deep learning, Domain adaptation, Machine learning, Transfer learning},
	pages = {781--789},
}

@inproceedings{lopez-paz_revisiting_2017,
	address = {Toulon, France},
	title = {Revisiting {Classifier} {Two}-{Sample} {Tests}},
	doi = {10.48550/arXiv.1610.06545},
	abstract = {The goal of two-sample tests is to assess whether two samples, \$S\_P {\textbackslash}sim P{\textasciicircum}n\$ and \$S\_Q {\textbackslash}sim Q{\textasciicircum}m\$, are drawn from the same distribution. Perhaps intriguingly, one relatively unexplored method to build two-sample tests is the use of binary classifiers. In particular, construct a dataset by pairing the \$n\$ examples in \$S\_P\$ with a positive label, and by pairing the \$m\$ examples in \$S\_Q\$ with a negative label. If the null hypothesis "\$P = Q\$" is true, then the classification accuracy of a binary classifier on a held-out subset of this dataset should remain near chance-level. As we will show, such Classifier Two-Sample Tests (C2ST) learn a suitable representation of the data on the fly, return test statistics in interpretable units, have a simple null distribution, and their predictive uncertainty allow to interpret where \$P\$ and \$Q\$ differ. The goal of this paper is to establish the properties, performance, and uses of C2ST. First, we analyze their main theoretical properties. Second, we compare their performance against a variety of state-of-the-art alternatives. Third, we propose their use to evaluate the sample quality of generative models with intractable likelihoods, such as Generative Adversarial Networks (GANs). Fourth, we showcase the novel application of GANs together with C2ST for causal discovery.},
	urldate = {2024-02-22},
	booktitle = {International {Conference} on {Learning} {Representations}},
	publisher = {International Conference on Learning Representations},
	author = {Lopez-Paz, David and Oquab, Maxime},
	year = {2017},
	keywords = {Statistics - Machine Learning},
}

@misc{noauthor_39025799_nodate,
	title = {39025799 Â· {Sample}-efficient {Simulation}-based {Inference} for {Urban} {Travel} {Demand} {Calibration}},
	url = {https://slideslive.com/embed/presentation/39025799?js_embed_version=3&embed_init_token=eyJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE3MzgzMTY3NDksImV4cCI6MTczODQ0NjM0OSwidSI6eyJ1dWlkIjoiZDEyYTRjMmEtMGViYy00OWYzLWE1OWYtMGI0YmI2MmU4NTlhIiwiaSI6bnVsbCwiZSI6bnVsbCwibSI6ZmFsc2V9LCJkIjoibmV1cmlwcy5jYyJ9.KHnOxsrSpTrwiyN8o56sh_K2qAlr7RuWAswYtL5ciV4&embed_parent_url=https%3A%2F%2Fneurips.cc%2Fvirtual%2F2024%2Fposter%2F94187&embed_origin=https%3A%2F%2Fneurips.cc&embed_container_id=presentation-embed-39025799&auto_load=true&auto_play=false&zoom_ratio=&disable_fullscreen=false&locale=en&vertical_enabled=true&vertical_enabled_on_mobile=false&allow_hidden_controls_when_paused=true&fit_to_viewport=true&custom_user_id=&user_uuid=d12a4c2a-0ebc-49f3-a59f-0b4bb62e859a},
	abstract = {Professional Conference Recording},
	language = {en-US},
	urldate = {2025-01-31},
	journal = {SlidesLive},
}

@inproceedings{griesemer_active_2024,
	title = {Active {Sequential} {Posterior} {Estimation} for {Sample}-{Efficient} {Simulation}-{Based} {Inference}},
	url = {https://openreview.net/forum?id=fkuseU0nJs&noteId=AzIKRHiQD4},
	abstract = {Computer simulations have long presented the exciting possibility of scientific insight into complex real-world processes. Despite the power of modern computing, however, it remains challenging to systematically perform inference under simulation models. This has led to the rise of simulation-based inference (SBI), a class of machine learning-enabled techniques for approaching inverse problems with stochastic simulators. Many such methods, however, require large numbers of simulation samples and face difficulty scaling to high-dimensional settings, often making inference prohibitive under resource-intensive simulators. To mitigate these drawbacks, we introduce active sequential neural posterior estimation (ASNPE). ASNPE brings an active learning scheme into the inference loop to estimate the utility of simulation parameter candidates to the underlying probabilistic model. The proposed acquisition scheme is easily integrated into existing posterior estimation pipelines, allowing for improved sample efficiency with low computational overhead. We further demonstrate the effectiveness of the proposed method in the travel demand calibration setting, a high-dimensional inverse problem commonly requiring computationally expensive traffic simulators. Our method outperforms well-tuned benchmarks and state-of-the-art posterior estimation methods on a large-scale real-world traffic network, as well as demonstrates a performance advantage over non-active counterparts on a suite of SBI benchmark environments.},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {Neural {Information} {Processing} {Systems}},
	author = {Griesemer, Sam and Cao, Defu and Cui, Zijun and Osorio, Carolina and Liu, Yan},
	month = nov,
	year = {2024},
}

@inproceedings{falola_rapid_2023,
	address = {Abu Dhabi, UAE},
	title = {Rapid {High}-{Fidelity} {Forecasting} for {Geological} {Carbon} {Storage} {Using} {Neural} {Operator} and {Transfer} {Learning}},
	shorttitle = {Rapid {High}-{Fidelity} {Forecasting} for {Geological} {Carbon} {Storage} {Using} {Neural} {Operator} and {Transfer} {Learning}},
	doi = {10.2118/216135-MS},
	abstract = {Abstract. Carbon sequestration is a promising technique to minimize the emission of CO2 to the atmosphere. However, the computational time required for CO2 forecasting using commercial numerical simulators can be prohibitive for complex problems. In this work, we propose the use of transfer learning to rapidly forecast the CO2 pressure plume and saturation distribution under uncertain geological and operational conditions, specifically for variations in injector locations and injector rates. We first train a Fourier Neural Operator (FNO)-based machine learning (ML) model on a limited set of simple scenarios. Then, we use transfer learning to fine-tune the FNO model on a larger set of complex scenarios. Most importantly, the CMG forecasting time for one scenario requires approximately 40 to 50 minutes, which was drastically reduced to 12 seconds by using Fourier Neural Operator and then reduced further to 8 seconds by implementing transfer learning on the Fourier neural operator. The mean relative errors of the neural operator predictions of pressure and saturation were 1.42\% and 7.9\%, respectively. These errors get slightly higher when transfer learning is implemented on neural operator to learn complex task with less amount of data and low training time. Our results show that transfer learning can significantly reduce the computational time required for CO2 forecasting. The data generation and model training times were reduced by 50\% and 75\%, respectively, by using transfer learning on the Fourier neural operator. Additionally, the total number of trainable parameters was reduced by 99.9\%. Our results demonstrate the potential of transfer learning for rapid forecasting of CO2 pressure plume and saturation distribution. This technique can be used to improve the efficiency of CO2 forecasting and to help mitigate the risks associated with CO2 leakage.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {{ADIPEC}},
	publisher = {OnePetro},
	author = {Falola, Yusuf and Misra, Siddharth and Nunez, Andres Calvo},
	month = oct,
	year = {2023},
}

@inproceedings{lueckmann_likelihood-free_2019,
	title = {Likelihood-free inference with emulator networks},
	url = {https://proceedings.mlr.press/v96/lueckmann19a.html},
	abstract = {Approximate Bayesian Computation (ABC) provides methods for Bayesian inference in simulation-based models which do not permit tractable likelihoods. We present a new ABC method which uses probabilistic neural emulator networks to learn synthetic likelihoods on simulated data - both âlocalâ emulators which approximate the likelihood for specific observed data, as well as âglobalâ ones which are applicable to a range of data. Simulations are chosen adaptively using an acquisition function which takes into account uncertainty about either the posterior distribution of interest, or the parameters of the emulator. Our approach does not rely on user-defined rejection thresholds or distance functions. We illustrate inference with emulator networks on synthetic examples and on a biophysical neuron model, and show that emulators allow accurate and efficient inference even on problems which are challenging for conventional ABC approaches.},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {Proceedings of {The} 1st {Symposium} on {Advances} in {Approximate} {Bayesian} {Inference}},
	publisher = {PMLR},
	author = {Lueckmann, Jan-Matthis and Bassetto, Giacomo and Karaletsos, Theofanis and Macke, Jakob H.},
	month = jan,
	year = {2019},
	pages = {32--53},
}

@article{fadikar_calibrating_2018,
	title = {Calibrating a {Stochastic}, {Agent}-{Based} {Model} {Using} {Quantile}-{Based} {Emulation}},
	volume = {6},
	url = {https://epubs.siam.org/doi/10.1137/17M1161233},
	doi = {10.1137/17M1161233},
	abstract = {We develop a statistical approach for characterizing uncertainty in predictions that are made with the aid of a computer simulation model. Typically, the computer simulation code models a physical system and requires a set of inputs---some known and specified, others unknown. A limited amount of field data from the true physical system is available to inform us about the unknown inputs and also to inform us about the uncertainty that is associated with a simulation-based prediction. The approach given here allows for the following:uncertainty regarding model inputs (i.e., calibration); accounting for uncertainty due to limitations on the number of simulations that can be carried out; discrepancy between the simulation code and the actual physical system; uncertainty in the observation process that yields the actual field data on the true physical system. The resulting analysis yields predictions and their associated uncertainties while accounting for multiple sources of uncertainty. We use a Bayesian formulation and rely on Gaussian process models to model unknown functions of the model inputs. The estimation is carried out using a Markov chain Monte Carlo method. This methodology is applied to two examples: a charged particle accelerator and a spot welding process.},
	number = {4},
	urldate = {2025-01-29},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Fadikar, Arindam and Higdon, Dave and Chen, Jiangzhuo and Lewis, Bryan and Venkatramanan, Srinivasan and Marathe, Madhav},
	month = jan,
	year = {2018},
	pages = {1685--1706},
}

@article{hay_models_2011,
	title = {Models of neocortical layer 5b pyramidal cells capturing a wide range of dendritic and perisomatic active properties},
	volume = {7},
	issn = {1553-7358},
	doi = {10.1371/journal.pcbi.1002107},
	abstract = {The thick-tufted layer 5b pyramidal cell extends its dendritic tree to all six layers of the mammalian neocortex and serves as a major building block for the cortical column. L5b pyramidal cells have been the subject of extensive experimental and modeling studies, yet conductance-based models of these cells that faithfully reproduce both their perisomatic Na(+)-spiking behavior as well as key dendritic active properties, including Ca(2+) spikes and back-propagating action potentials, are still lacking. Based on a large body of experimental recordings from both the soma and dendrites of L5b pyramidal cells in adult rats, we characterized key features of the somatic and dendritic firing and quantified their statistics. We used these features to constrain the density of a set of ion channels over the soma and dendritic surface via multi-objective optimization with an evolutionary algorithm, thus generating a set of detailed conductance-based models that faithfully replicate the back-propagating action potential activated Ca(2+) spike firing and the perisomatic firing response to current steps, as well as the experimental variability of the properties. Furthermore, we show a useful way to analyze model parameters with our sets of models, which enabled us to identify some of the mechanisms responsible for the dynamic properties of L5b pyramidal cells as well as mechanisms that are sensitive to morphological changes. This automated framework can be used to develop a database of faithful models for other neuron types. The models we present provide several experimentally-testable predictions and can serve as a powerful tool for theoretical investigations of the contribution of single-cell dynamics to network activity and its computational capabilities.},
	language = {eng},
	number = {7},
	journal = {PLoS computational biology},
	author = {Hay, Etay and Hill, Sean and SchÃ¼rmann, Felix and Markram, Henry and Segev, Idan},
	month = jul,
	year = {2011},
	keywords = {Action Potentials, Action potentials, Algorithms, Animals, Behavior, Computational Biology, Dendrites, Evolutionary algorithms, Ion Channels, Ion channels, Models, Neurological, Neocortex, Neuronal dendrites, Neurons, Optimization, Pyramidal Cells, Pyramidal cells, Rats, Rats, Wistar, Single-Cell Analysis},
	pages = {e1002107},
}

@article{frazier_synthetic_2024,
	title = {Synthetic {Likelihood} in {Misspecified} {Models}},
	volume = {0},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2024.2370594},
	doi = {10.1080/01621459.2024.2370594},
	abstract = {Bayesian synthetic likelihood is a widely used approach for conducting Bayesian analysis in complex models where evaluation of the likelihood is infeasible but simulation from the assumed model is tractable. We analyze the behavior of the Bayesian synthetic likelihood posterior when the assumed model differs from the actual data generating process. We demonstrate that the Bayesian synthetic likelihood posterior can display a wide range of nonstandard behaviors depending on the level of model misspecification, including multimodality and asymptotic non-Gaussianity. Our results suggest that likelihood tempering, a common approach for robust Bayesian inference, fails for synthetic likelihood whilst recently proposed robust synthetic likelihood approaches can ameliorate this behavior and deliver reliable posterior inference under model misspecification. All results are illustrated using a simple running example. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
	number = {0},
	urldate = {2025-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Frazier, David T. and Nott, David J. and Drovandi, Christopher},
	year = {2024},
	keywords = {Approximate Bayesian computation, Likelihood tempering, Model misspecification, Synthetic likelihood},
	pages = {1--12},
}

@inproceedings{lueckmann_benchmarking_2021,
	title = {Benchmarking {Simulation}-{Based} {Inference}},
	url = {https://proceedings.mlr.press/v130/lueckmann21a.html},
	abstract = {Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such âlikelihood-freeâ algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
	month = mar,
	year = {2021},
	pages = {343--351},
}

@inproceedings{lueckmann_flexible_2017,
	title = {Flexible statistical inference for mechanistic models of neural dynamics},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html},
	abstract = {Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.},
	urldate = {2025-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lueckmann, Jan-Matthis and Goncalves, Pedro J and Bassetto, Giacomo and Ãcal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H},
	year = {2017},
}

@misc{wang_local_2024,
	title = {Local transfer learning {Gaussian} process modeling, with applications to surrogate modeling of expensive computer simulators},
	url = {http://arxiv.org/abs/2410.12690},
	doi = {10.48550/arXiv.2410.12690},
	abstract = {A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Wang, Xinming and Mak, Simon and Miller, John and Wu, Jianguo},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@article{maurais_multifidelity_2023,
	title = {Multifidelity {Covariance} {Estimation} via {Regression} on the {Manifold} of {Symmetric} {Positive} {Definite} {Matrices}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2307.12438},
	doi = {10.48550/ARXIV.2307.12438},
	abstract = {We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties enabling practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that the MRMF estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.},
	urldate = {2025-01-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Maurais, Aimee and Alsup, Terrence and Peherstorfer, Benjamin and Marzouk, Youssef},
	year = {2023},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Mathematics, Machine Learning (cs.LG), Numerical Analysis (math.NA)},
}

@article{held_gap_2005,
	title = {The {Gap} between {Simulation} and {Understanding} in {Climate} {Modeling}},
	url = {https://journals.ametsoc.org/view/journals/bams/86/11/bams-86-11-1609.xml},
	doi = {10.1175/BAMS-86-11-1609},
	abstract = {The problem of creating truly convincing numerical simulations of our Earth's climate will remain a challenge for the next generation of climate scientists. Hopefully, the ever increasing power of computers will make this task somewhat less frustrating than it is at present. But, increasing computational power also raises issues as to how we would like to see climate modeling and the study of climate dynamics evolve in the twenty-first century. One of the key issues we will need to address is the widening gap between simulation and understanding.},
	language = {en},
	urldate = {2025-01-14},
	journal = {ametsoc},
	author = {Held, Isaac M.},
	month = nov,
	year = {2005},
}

@techreport{friedman_multivariate_2004,
	title = {On {Multivariate} {Goodness}-of-{Fit} and {Two}-{Sample} {Testing}},
	url = {http://www.osti.gov/servlets/purl/826696/},
	language = {en},
	number = {SLAC-PUB-10325, 826696},
	urldate = {2025-01-28},
	institution = {Stanford},
	author = {Friedman, J},
	month = jan,
	year = {2004},
	doi = {10.2172/826696},
	pages = {SLAC--PUB--10325, 826696},
}

@misc{boelts_sbi_2024,
	title = {sbi reloaded: a toolkit for simulation-based inference workflows},
	shorttitle = {sbi reloaded},
	url = {http://arxiv.org/abs/2411.17337},
	doi = {10.48550/arXiv.2411.17337},
	abstract = {Scientists and engineers use simulators to model empirically observed phenomena. However, tuning the parameters of a simulator to ensure its outputs match observed data presents a significant challenge. Simulation-based inference (SBI) addresses this by enabling Bayesian inference for simulators, identifying parameters that match observed data and align with prior knowledge. Unlike traditional Bayesian inference, SBI only needs access to simulations from the model and does not require evaluations of the likelihood-function. In addition, SBI algorithms do not require gradients through the simulator, allow for massive parallelization of simulations, and can perform inference for different observations without further simulations or training, thereby amortizing inference. Over the past years, we have developed, maintained, and extended \${\textbackslash}texttt\{sbi\}\$, a PyTorch-based package that implements Bayesian SBI algorithms based on neural networks. The \${\textbackslash}texttt\{sbi\}\$ toolkit implements a wide range of inference methods, neural network architectures, sampling methods, and diagnostic tools. In addition, it provides well-tested default settings but also offers flexibility to fully customize every step of the simulation-based inference workflow. Taken together, the \${\textbackslash}texttt\{sbi\}\$ toolkit enables scientists and engineers to apply state-of-the-art SBI methods to black-box simulators, opening up new possibilities for aligning simulations with empirically observed data.},
	urldate = {2024-12-05},
	publisher = {arXiv},
	author = {Boelts, Jan and Deistler, Michael and Gloeckler, Manuel and Tejero-Cantero, Alvaro and Lueckmann, Jan-Matthis and Moss, Guy and Steinbach, Peter and Moreau, Thomas and Muratore, Fabio and Linhart, Julia and Durkan, Conor and Vetter, Julius and Miller, Benjamin Kurt and Herold, Maternus and Ziaeemehr, Abolfazl and Pals, Matthijs and Gruner, Theo and Bischoff, Sebastian and Krouglova, Nastya and Gao, Richard and Lappalainen, Janne K. and MucsÃ¡nyi, BÃ¡lint and Pei, Felix and Schulz, Auguste and Stefanidi, Zinovia and Rodrigues, Pedro and SchrÃ¶der, Cornelius and Zaid, Faried Abu and Beck, Jonas and Kapoor, Jaivardhan and Greenberg, David S. and GonÃ§alves, Pedro J. and Macke, Jakob H.},
	month = nov,
	year = {2024},
	keywords = {Computer Science - Machine Learning},
}

@misc{deistler_differentiable_2024,
	title = {Differentiable simulation enables large-scale training of detailed biophysical models of neural dynamics},
	copyright = {Â© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.08.21.608979v1},
	doi = {10.1101/2024.08.21.608979},
	abstract = {Biophysical neuron models provide insights into cellular mechanisms underlying neural computations. However, a central challenge has been the question of how to identify the parameters of detailed biophysical models such that they match physiological measurements at scale or such that they perform computational tasks. Here, we describe a framework for simulation of detailed biophysical models in neuroscienceâJaxleyâwhich addresses this challenge. By making use of automatic differentiation and GPU acceleration, Jaxley opens up the possibility to efficiently optimize large-scale biophysical models with gradient descent. We show that Jaxley can learn parameters of biophysical neuron models with several hundreds of parameters to match voltage or two photon calcium recordings, sometimes orders of magnitude more efficiently than previous methods. We then demonstrate that Jaxley makes it possible to train biophysical neuron models to perform computational tasks. We train a recurrent neural network to perform working memory tasks, and a feedforward network of morphologically detailed neurons with 100,000 parameters to solve a computer vision task. Our analyses show that Jaxley dramatically improves the ability to build large-scale data- or task-constrained biophysical models, creating unprecedented opportunities for investigating the mechanisms underlying neural computations across multiple scales.},
	language = {en},
	urldate = {2024-10-18},
	publisher = {bioRxiv},
	author = {Deistler, Michael and Kadhim, Kyra L. and Pals, Matthijs and Beck, Jonas and Huang, Ziwei and Gloeckler, Manuel and Lappalainen, Janne K. and SchrÃ¶der, Cornelius and Berens, Philipp and GonÃ§alves, Pedro J. and Macke, Jakob H.},
	month = aug,
	year = {2024},
}

@article{catanach_bayesian_2020,
	title = {Bayesian inference of stochastic reaction networks using multifidelity sequential tempered {Markov} {Chain} {Monte} {Carlo}},
	volume = {10},
	issn = {2152-5080},
	doi = {10.1615/int.j.uncertaintyquantification.2020033241},
	abstract = {Stochastic reaction network models are often used to explain and predict the dynamics of gene regulation in single cells. These models usually involve several parameters, such as the kinetic rates of chemical reactions, that are not directly measurable and must be inferred from experimental data. Bayesian inference provides a rigorous probabilistic framework for identifying these parameters by finding a posterior parameter distribution that captures their uncertainty. Traditional computational methods for solving inference problems such as Markov Chain Monte Carlo methods based on classical Metropolis-Hastings algorithm involve numerous serial evaluations of the likelihood function, which in turn requires expensive forward solutions of the chemical master equation (CME). We propose an alternate approach based on a multifidelity extension of the Sequential Tempered Markov Chain Monte Carlo (ST-MCMC) sampler. This algorithm is built upon Sequential Monte Carlo and solves the Bayesian inference problem by decomposing it into a sequence of efficiently solved subproblems that gradually increase both model fidelity and the influence of the observed data. We reformulate the finite state projection (FSP) algorithm, a well-known method for solving the CME, to produce a hierarchy of surrogate master equations to be used in this multifidelity scheme. To determine the appropriate fidelity, we introduce a novel information-theoretic criteria that seeks to extract the most information about the ultimate Bayesian posterior from each model in the hierarchy without inducing significant bias. This novel sampling scheme is tested with high performance computing resources using biologically relevant problems.},
	language = {eng},
	number = {6},
	journal = {International Journal for Uncertainty Quantification},
	author = {Catanach, Thomas A. and Vo, Huy D. and Munsky, Brian},
	year = {2020},
	pmid = {34007522},
	pmcid = {PMC8127724},
	keywords = {Bayesian inference, MCMC, SMC, Stochastic modeling, Systems Biology, UQ},
	pages = {515--542},
}

@article{vo_bayesian_2019,
	title = {Bayesian {Estimation} for {Stochastic} {Gene} {Expression} {Using} {Multifidelity} {Models}},
	volume = {123},
	issn = {1520-5207},
	doi = {10.1021/acs.jpcb.8b10946},
	abstract = {The finite state projection (FSP) approach to solving the chemical master equation has enabled successful inference of discrete stochastic models to predict single-cell gene regulation dynamics. Unfortunately, the FSP approach is highly computationally intensive for all but the simplest models, an issue that is highly problematic when parameter inference and uncertainty quantification takes enormous numbers of parameter evaluations. To address this issue, we propose two new computational methods for the Bayesian inference of stochastic gene expression parameters given single-cell experiments. We formulate and verify an adaptive delayed acceptance Metropolis-Hastings (ADAMH) algorithm to utilize with reduced Krylov-basis projections of the FSP. We then introduce an extension of the ADAMH into a hybrid scheme that consists of an initial phase to construct a reduced model and a faster second phase to sample from the approximate posterior distribution determined by the constructed model. We test and compare both algorithms to an adaptive Metropolis algorithm with full FSP-based likelihood evaluations on three example models and simulated data to show that the new ADAMH variants achieve substantial speedup in comparison to the full FSP approach. By reducing the computational costs of parameter estimation, we expect the ADAMH approach to enable efficient data-driven estimation for more complex gene regulation models.},
	language = {eng},
	number = {10},
	journal = {The Journal of Physical Chemistry. B},
	author = {Vo, Huy D. and Fox, Zachary and Baetica, Ania and Munsky, Brian},
	month = mar,
	year = {2019},
	pmid = {30777763},
	pmcid = {PMC6697484},
	keywords = {Algorithms, Bayes Theorem, Biochemical Phenomena, Computer Simulation, Gene Expression, Models, Biological, Models, Chemical, Stochastic Processes},
	pages = {2217--2234},
}

@article{prescott_multifidelity_2020,
	title = {Multifidelity {Approximate} {Bayesian} {Computation}},
	volume = {8},
	url = {https://epubs.siam.org/doi/10.1137/18M1229742},
	doi = {10.1137/18M1229742},
	abstract = {Multifidelity approximate Bayesian computation (MF-ABC) is a likelihood-free technique for parameter inference that exploits model approximations to significantly increase the speed of ABC algorithms [T. P. Prescott and R. E. Baker, SIAM/ASA J. Uncertain. Quantif., 8 (2020), pp. 114--138]. Previous work has considered MF-ABC only in the context of rejection sampling, which does not explore parameter space particularly efficiently. In this work, we integrate the multifidelity approach with the ABC sequential Monte Carlo (ABC-SMC) algorithm into a new MF-ABC-SMC algorithm. We show that the improvements generated by each of ABC-SMC and MF-ABC to the efficiency of generating Monte Carlo samples and estimates from the ABC posterior are amplified when the two techniques are used together.},
	number = {1},
	urldate = {2025-01-30},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Prescott, Thomas P. and Baker, Ruth E.},
	month = jan,
	year = {2020},
	pages = {114--138},
}

@article{warne_multifidelity_2022,
	title = {Multifidelity multilevel {Monte} {Carlo} to accelerate approximate {Bayesian} parameter inference for partially observed stochastic processes},
	volume = {469},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999122006052},
	doi = {10.1016/j.jcp.2022.111543},
	abstract = {Semantic Scholar extracted view of "Multifidelity multilevel Monte Carlo to accelerate approximate Bayesian parameter inference for partially observed stochastic processes" by D. Warne et al.},
	language = {en},
	urldate = {2025-01-30},
	journal = {Journal of Computational Physics},
	author = {Warne, David J. and Prescott, Thomas P. and Baker, Ruth E. and Simpson, Matthew J.},
	month = nov,
	year = {2022},
	pages = {111543},
}

@misc{noauthor_connected_nodate,
	title = {Connected {Papers} {\textbar} {Find} and explore academic papers},
	url = {https://www.connectedpapers.com/main/acb2e23e1e1b9c67d0e5f71fcf0214386f10c8cc/Multifidelity-Approximate-Bayesian-Computation-with-Sequential-Monte-Carlo-Parameter-Sampling/graph},
	abstract = {A unique, visual tool to help researchers and applied scientists find and explore papers relevant to their field of work.},
	language = {en},
	urldate = {2025-01-30},
}

@article{lenormand_adaptive_2013,
	title = {Adaptive approximate {Bayesian} computation for complex models},
	volume = {28},
	issn = {1613-9658},
	url = {https://doi.org/10.1007/s00180-013-0428-3},
	doi = {10.1007/s00180-013-0428-3},
	abstract = {We propose a new approximate Bayesian computation (ABC) algorithm that aims at minimizing the number of model runs for reaching a given quality of the posterior approximation. This algorithm automatically determines its sequence of tolerance levels and makes use of an easily interpretable stopping criterion. Moreover, it avoids the problem of particle duplication found when using a MCMC kernel. When applied to a toy example and to a complex social model, our algorithm is 2â8 times faster than the three main sequential ABC algorithms currently available.},
	language = {en},
	number = {6},
	urldate = {2025-01-30},
	journal = {Computational Statistics},
	author = {Lenormand, Maxime and Jabot, Franck and Deffuant, Guillaume},
	month = dec,
	year = {2013},
	keywords = {ABC, Population Monte Carlo, Sequential Monte Carlo},
	pages = {2777--2796},
}

@article{prescott_efficient_2024,
	title = {Efficient multifidelity likelihood-free {Bayesian} inference with adaptive computational resource allocation},
	volume = {496},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999123006721},
	doi = {10.1016/j.jcp.2023.112577},
	abstract = {Likelihood-free Bayesian inference algorithms are popular methods for inferring the parameters of complex stochastic models with intractable likelihoods. These algorithms characteristically rely heavily on repeated model simulations. However, whenever the computational cost of simulation is even moderately expensive, the significant burden incurred by likelihood-free algorithms leaves them infeasible for many practical applications. The multifidelity approach has been introduced in the context of approximate Bayesian computation to reduce the simulation burden of likelihood-free inference without loss of accuracy, by using the information provided by simulating computationally cheap, approximate models in place of the model of interest. In this work we demonstrate that multifidelity techniques can be applied in the general likelihood-free Bayesian inference setting. Analytical results on the optimal allocation of computational resources to simulations at different levels of fidelity are derived, and subsequently implemented practically. We provide an adaptive multifidelity likelihood-free inference algorithm that learns the relationships between models at different fidelities and adapts resource allocation accordingly, and demonstrate that this algorithm produces posterior estimates with near-optimal efficiency.},
	urldate = {2025-01-13},
	journal = {Journal of Computational Physics},
	author = {Prescott, Thomas P. and Warne, David J. and Baker, Ruth E.},
	month = jan,
	year = {2024},
	keywords = {Likelihood-free Bayesian inference, Multifidelity, Multifidelity approaches, SBI},
	pages = {112577},
}

@inproceedings{pillow_fully_2012,
	title = {Fully {Bayesian} inference for neural models with negative-binomial spiking},
	volume = {25},
	url = {https://papers.nips.cc/paper_files/paper/2012/hash/b55ec28c52d5f6205684a473a2193564-Abstract.html},
	abstract = {Characterizing the information carried by neural populations in the brain requires accurate statistical models of neural spike responses.  The negative-binomial distribution provides a convenient model for over-dispersed spike counts, that is, responses with greater-than-Poisson variability.  Here we describe a powerful data-augmentation framework for fully Bayesian inference in neural models with negative-binomial spiking. Our approach relies on a recently described latent-variable representation of the negative-binomial distribution, which equates it to a Polya-gamma mixture of normals.  This framework provides a tractable, conditionally Gaussian representation of the posterior that can be used to design efficient EM and Gibbs sampling based algorithms for inference in regression and dynamic factor models.  We apply the model to neural data from primate retina and show that it substantially outperforms Poisson regression on held-out data, and reveals latent structure underlying spike count correlations in simultaneously recorded spike trains.},
	urldate = {2025-01-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Pillow, Jonathan and Scott, James},
	year = {2012},
}

@article{barbers_exploring_2024,
	title = {Exploring the effects of cell-to-cell variability on battery aging through stochastic simulation techniques},
	volume = {84},
	issn = {2352-152X},
	url = {https://www.sciencedirect.com/science/article/pii/S2352152X24004353},
	doi = {10.1016/j.est.2024.110851},
	abstract = {This work introduces a comprehensive modeling framework designed to simulate the electrical, thermal, and aging behavior of battery cells connected in various parallel and series configurations. By utilizing Monte Carlo simulation techniques, the framework is used to investigate the inherent variability in cell attributes, including initial capacity, aging rate, and application profiles. Besides the estimation of expected battery life, this simulation environment enables the detailed investigation of failure distributions across different cell configurations and intensities of parameter variations. Results obtained from these simulations can be used, as an example, in the context of the automotive industry, where the insights of simulation in understanding the inherent variability of the aging process are particularly vital. As electric vehicles become more prevalent, understanding the performance and longevity of battery packs under various conditions is essential for effective design and management strategies, optimizing vehicle range, safety, and cost-effectiveness also on a fleet-level. Moreover, the ability to investigate failure distributions provides invaluable information for improving battery reliability and safety, key factors in the consumer acceptance of electric vehicles. Ultimately, the simulation environment provides a powerful tool for designing and optimizing efficient and durable battery technologies, with a focus on failure distribution analysis.},
	urldate = {2025-01-29},
	journal = {Journal of Energy Storage},
	author = {Barbers, Elias and Hust, Friedrich Emanuel and Hildenbrand, Felix Emil Arthur and Frie, Fabian and Quade, Katharina Lilith and Bihn, Stephan and Sauer, Dirk Uwe and Dechent, Philipp},
	month = apr,
	year = {2024},
	keywords = {Aging simulation, Battery modeling, Lithium-ion battery, Monte Carlo, Parameter variation, Stochastic simulation},
	pages = {110851},
}

@book{nelson_foundations_2021,
	address = {Cham},
	series = {International {Series} in {Operations} {Research} \& {Management} {Science}},
	title = {Foundations and {Methods} of {Stochastic} {Simulation}: {A} {First} {Course}},
	volume = {316},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-3-030-86193-3 978-3-030-86194-0},
	shorttitle = {Foundations and {Methods} of {Stochastic} {Simulation}},
	url = {https://link.springer.com/10.1007/978-3-030-86194-0},
	language = {en},
	urldate = {2025-01-29},
	publisher = {Springer International Publishing},
	author = {Nelson, Barry L. and Pei, Linda},
	year = {2021},
	doi = {10.1007/978-3-030-86194-0},
}

@article{mckeague_statistical_2005,
	title = {Statistical inversion of {South} {Atlantic} circulation in an abyssal neutral density layer},
	volume = {63},
	url = {https://elischolar.library.yale.edu/journal_of_marine_research/97},
	number = {4},
	journal = {Journal of Marine Research},
	author = {McKeague, Ian and Nicholls, Geoff and Speer, Kevin and Herbei, Radu},
	month = jan,
	year = {2005},
}

@misc{papamakarios_masked_2018,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	url = {http://arxiv.org/abs/1705.07057},
	doi = {10.48550/arXiv.1705.07057},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{papamakarios_normalizing_2021,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/19-1028.html},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	number = {57},
	urldate = {2024-10-23},
	journal = {Journal of Machine Learning Research},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	year = {2021},
	pages = {1--64},
}

@article{huang_learning_2023,
	title = {Learning {Robust} {Statistics} for {Simulation}-based {Inference} under {Model} {Misspecification}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/16c5b4102a6b6eb061e502ce6736ad8a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-16},
	journal = {Advances in Neural Information Processing Systems},
	author = {Huang, Daolang and Bharti, Ayush and Souza, Amauri and Acerbi, Luigi and Kaski, Samuel},
	month = dec,
	year = {2023},
	pages = {7289--7310},
}

@misc{gao_generalized_2023,
	title = {Generalized {Bayesian} {Inference} for {Scientific} {Simulators} via {Amortized} {Cost} {Estimation}},
	url = {http://arxiv.org/abs/2305.15208},
	doi = {10.48550/arXiv.2305.15208},
	abstract = {Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a parameter and observed data. The trained network can then be used with MCMC to infer GBI posteriors for any observation without running additional simulations. We show that, on several benchmark tasks, ACE accurately predicts cost and provides predictive simulations that are closer to synthetic observations than other SBI methods, especially for misspecified simulators. Finally, we apply ACE to infer parameters of the Hodgkin-Huxley model given real intracellular recordings from the Allen Cell Types Database. ACE identifies better data-matching parameters while being an order of magnitude more simulation-efficient than a standard SBI method. In summary, ACE combines the strengths of SBI methods and GBI to perform robust and simulation-amortized inference for scientific simulators.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Gao, Richard and Deistler, Michael and Macke, Jakob H.},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ward_robust_2022,
	title = {Robust {Neural} {Posterior} {Estimation} and {Statistical} {Model} {Criticism}},
	url = {http://arxiv.org/abs/2210.06564},
	doi = {10.48550/arXiv.2210.06564},
	abstract = {Computer simulations have proven a valuable tool for understanding complex phenomena across the sciences. However, the utility of simulators for modelling and forecasting purposes is often restricted by low data quality, as well as practical limits to model fidelity. In order to circumvent these difficulties, we argue that modellers must treat simulators as idealistic representations of the true data generating process, and consequently should thoughtfully consider the risk of model misspecification. In this work we revisit neural posterior estimation (NPE), a class of algorithms that enable black-box parameter inference in simulation models, and consider the implication of a simulation-to-reality gap. While recent works have demonstrated reliable performance of these methods, the analyses have been performed using synthetic data generated by the simulator model itself, and have therefore only addressed the well-specified case. In this paper, we find that the presence of misspecification, in contrast, leads to unreliable inference when NPE is used naively. As a remedy we argue that principled scientific inquiry with simulators should incorporate a model criticism component, to facilitate interpretable identification of misspecification and a robust inference component, to fit 'wrong but useful' models. We propose robust neural posterior estimation (RNPE), an extension of NPE to simultaneously achieve both these aims, through explicitly modelling the discrepancies between simulations and the observed data. We assess the approach on a range of artificially misspecified examples, and find RNPE performs well across the tasks, whereas naively using NPE leads to misleading and erratic posteriors.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Ward, Daniel and Cannon, Patrick and Beaumont, Mark and Fasiolo, Matteo and Schmon, Sebastian M.},
	month = oct,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{noauthor_230515208_nodate,
	title = {[2305.15208] {Generalized} {Bayesian} {Inference} for {Scientific} {Simulators} via {Amortized} {Cost} {Estimation}},
	url = {https://arxiv.org/abs/2305.15208},
	urldate = {2025-01-29},
}

@misc{cannon_investigating_2022,
	title = {Investigating the {Impact} of {Model} {Misspecification} in {Neural} {Simulation}-based {Inference}},
	url = {http://arxiv.org/abs/2209.01845},
	doi = {10.48550/arXiv.2209.01845},
	abstract = {Aided by advances in neural density estimation, considerable progress has been made in recent years towards a suite of simulation-based inference (SBI) methods capable of performing flexible, black-box, approximate Bayesian inference for stochastic simulation models. While it has been demonstrated that neural SBI methods can provide accurate posterior approximations, the simulation studies establishing these results have considered only well-specified problems -- that is, where the model and the data generating process coincide exactly. However, the behaviour of such algorithms in the case of model misspecification has received little attention. In this work, we provide the first comprehensive study of the behaviour of neural SBI algorithms in the presence of various forms of model misspecification. We find that misspecification can have a profoundly deleterious effect on performance. Some mitigation strategies are explored, but no approach tested prevents failure in all cases. We conclude that new approaches are required to address model misspecification if neural SBI algorithms are to be relied upon to derive accurate scientific conclusions.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Cannon, Patrick and Ward, Daniel and Schmon, Sebastian M.},
	month = sep,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{han_improving_2013,
	title = {Improving variable-fidelity surrogate modeling via gradient-enhanced kriging and a generalized hybrid bridge function},
	volume = {25},
	issn = {1270-9638},
	url = {https://www.sciencedirect.com/science/article/pii/S127096381200017X},
	doi = {10.1016/j.ast.2012.01.006},
	abstract = {Variable-fidelity surrogate modeling offers an efficient way to generate aerodynamic data for aero-loads prediction based on a set of CFD methods with varying degree of fidelity and computational expense. In this paper, direct Gradient-Enhanced Kriging (GEK) and a newly developed Generalized Hybrid Bridge Function (GHBF) have been combined in order to improve the efficiency and accuracy of the existing Variable-Fidelity Modeling (VFM) approach. The new algorithms and features are demonstrated and evaluated for analytical functions and are subsequently used to construct a global surrogate model for the aerodynamic coefficients and drag polar of an RAE 2822 airfoil. It is shown that the gradient-enhanced GHBF proposed in this paper is very promising and can be used to significantly improve the efficiency, accuracy and robustness of VFM in the context of aero-loads prediction.},
	number = {1},
	urldate = {2025-01-29},
	journal = {Aerospace Science and Technology},
	author = {Han, Zhong-Hua and GÃ¶rtz, Stefan and Zimmermann, Ralf},
	month = mar,
	year = {2013},
	keywords = {Computational fluid dynamics, Kriging model, Surrogate model, Variable-fidelity model},
	pages = {177--189},
}

@article{deistler_truncated_2022,
	title = {Truncated proposals for scalable and hassle-free simulation-based inference},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9278abf072b58caf21d48dd670b4c721-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-29},
	journal = {Advances in Neural Information Processing Systems},
	author = {Deistler, Michael and Goncalves, Pedro J. and Macke, Jakob H.},
	month = dec,
	year = {2022},
	keywords = {SBI},
	pages = {23135--23149},
}

@misc{chen_neural_2021,
	title = {Neural {Approximate} {Sufficient} {Statistics} for {Implicit} {Models}},
	url = {http://arxiv.org/abs/2010.10079},
	abstract = {We consider the fundamental problem of how to automatically construct summary statistics for implicit generative models where the evaluation of the likelihood function is intractable, but sampling data from the model is possible. The idea is to frame the task of constructing sufficient statistics as learning mutual information maximizing representations of the data with the help of deep neural networks. The infomax learning procedure does not need to estimate any density or density ratio. We apply our approach to both traditional approximate Bayesian computation and recent neural likelihood methods, boosting their performance on a range of tasks.},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Chen, Yanzhi and Zhang, Dinghuai and Gutmann, Michael and Courville, Aaron and Zhu, Zhanxing},
	month = mar,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
}

@article{druckmann_novel_2007,
	title = {A novel multiple objective optimization framework for constraining conductance-based neuron models by experimental data},
	volume = {1},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/neuro.01.1.1.001.2007},
	urldate = {2024-02-21},
	journal = {Frontiers in Neuroscience},
	author = {Druckmann, Shaul and Banitt, Yoav and Gidon, Albert and SchÃ¼rmann, Felix and Markram, Henry and Segev, Idan},
	year = {2007},
}

@misc{wehenkel_addressing_2024,
	title = {Addressing {Misspecification} in {Simulation}-based {Inference} through {Data}-driven {Calibration}},
	url = {http://arxiv.org/abs/2405.08719},
	doi = {10.48550/arXiv.2405.08719},
	abstract = {Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability. This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations. Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator. Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Wehenkel, Antoine and Gamella, Juan L. and Sener, Ozan and Behrmann, Jens and Sapiro, Guillermo and Cuturi, Marco and Jacobsen, JÃ¶rn-Henrik},
	month = may,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Multifidelity, SBI, Statistics - Machine Learning, Statistics - Methodology, important},
}

@article{peherstorfer_combining_2017,
	title = {Combining multiple surrogate models to accelerate failure probability estimation with expensive high-fidelity models},
	volume = {341},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999117302838},
	doi = {10.1016/j.jcp.2017.04.012},
	language = {en},
	urldate = {2024-12-17},
	journal = {Journal of Computational Physics},
	author = {Peherstorfer, Benjamin and Kramer, Boris and Willcox, Karen},
	month = jul,
	year = {2017},
	pages = {61--75},
}

@article{hodassman_efficient_2022,
	title = {Efficient dendritic learning as an alternative to synaptic plasticity hypothesis},
	volume = {12},
	copyright = {2022 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-10466-8},
	doi = {10.1038/s41598-022-10466-8},
	abstract = {Synaptic plasticity is a long-lasting core hypothesis of brain learning that suggests local adaptation between two connecting neurons and forms the foundation of machine learning. The main complexity of synaptic plasticity is that synapses and dendrites connect neurons in series and existing experiments cannot pinpoint the significant imprinted adaptation location. We showed efficient backpropagation and Hebbian learning on dendritic trees, inspired by experimental-based evidence, for sub-dendritic adaptation and its nonlinear amplification. It has proven to achieve success rates approaching unity for handwritten digits recognition, indicating realization of deep learning even by a single dendrite or neuron. Additionally, dendritic amplification practically generates an exponential number of input crosses, higher-order interactions, with the number of inputs, which enhance success rates. However, direct implementation of a large number of the cross weights and their exhaustive manipulation independently is beyond existing and anticipated computational power. Hence, a new type of nonlinear adaptive dendritic hardware for imitating dendritic learning and estimating the computational capability of the brain must be built.},
	language = {en},
	number = {1},
	urldate = {2023-12-05},
	journal = {Scientific Reports},
	author = {Hodassman, Shiri and Vardi, Roni and Tugendhaft, Yael and Goldental, Amir and Kanter, Ido},
	month = apr,
	year = {2022},
	keywords = {Biophysics, Computational biology and bioinformatics, metalearning},
	pages = {6571},
}

@article{rojas-carulla_invariant_nodate,
	title = {Invariant {Models} for {Causal} {Transfer} {Learning}},
	abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the ï¬eld of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are suï¬ciently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
	language = {en},
	author = {Rojas-Carulla, Mateo and Scholkopf, Bernhard and Turner, Richard and Peters, Jonas},
}

@article{pospischil_minimal_2008,
	title = {Minimal {Hodgkin}-{Huxley} type models for different classes of cortical and thalamic neurons},
	volume = {99},
	issn = {1432-0770},
	doi = {10.1007/s00422-008-0263-8},
	abstract = {We review here the development of Hodgkin-Huxley (HH) type models of cerebral cortex and thalamic neurons for network simulations. The intrinsic electrophysiological properties of cortical neurons were analyzed from several preparations, and we selected the four most prominent electrophysiological classes of neurons. These four classes are "fast spiking", "regular spiking", "intrinsically bursting" and "low-threshold spike" cells. For each class, we fit "minimal" HH type models to experimental data. The models contain the minimal set of voltage-dependent currents to account for the data. To obtain models as generic as possible, we used data from different preparations in vivo and in vitro, such as rat somatosensory cortex and thalamus, guinea-pig visual and frontal cortex, ferret visual cortex, cat visual cortex and cat association cortex. For two cell classes, we used automatic fitting procedures applied to several cells, which revealed substantial cell-to-cell variability within each class. The selection of such cellular models constitutes a necessary step towards building network simulations of the thalamocortical system with realistic cellular dynamical properties.},
	language = {eng},
	number = {4-5},
	journal = {Biological Cybernetics},
	author = {Pospischil, Martin and Toledo-Rodriguez, Maria and Monier, Cyril and Piwkowska, Zuzanna and Bal, Thierry and FrÃ©gnac, Yves and Markram, Henry and Destexhe, Alain},
	month = nov,
	year = {2008},
	pmid = {19011929},
	keywords = {Action Potentials, Animals, Cerebral Cortex, Models, Neurological, Neurons, Patch-Clamp Techniques, Thalamus},
	pages = {427--441},
}

@article{diggle_monte_1984,
	title = {Monte {Carlo} {Methods} of {Inference} for {Implicit} {Statistical} {Models}},
	volume = {46},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2345504},
	abstract = {A prescribed statistical model is a parametric specification of the distribution of a random vector, whilst an implicit statistical model is one defined at a more fundamental level in terms of a generating stochastic mechanism. This paper develops methods of inference which can be used for implicit statistical models whose distribution theory is intractable. The kernel method of probability density estimation is advocated for estimating a log-likelihood from simulations of such a model. The development and testing of an algorithm for maximizing this estimated log-likelihood function is described. An illustrative example involving a stochastic model for quantal response assays is given. Possible applications of the maximization algorithm to ad hoc methods of parameter estimation are noted briefly, and illustrated by an example involving a model for the spatial pattern of displaced amacrine cells in the retina of a rabbit.},
	number = {2},
	urldate = {2023-10-19},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Diggle, Peter J. and Gratton, Richard J.},
	year = {1984},
	keywords = {History, SBI},
	pages = {193--227},
}

@article{mozzachiodi_more_2010,
	title = {More than synaptic plasticity: role of nonsynaptic plasticity in learning and memory},
	volume = {33},
	issn = {0166-2236, 1878-108X},
	shorttitle = {More than synaptic plasticity},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(09)00170-2},
	doi = {10.1016/j.tins.2009.10.001},
	language = {English},
	number = {1},
	urldate = {2023-12-06},
	journal = {Trends in Neurosciences},
	author = {Mozzachiodi, Riccardo and Byrne, John H.},
	month = jan,
	year = {2010},
	pmid = {19889466},
	keywords = {metalearning},
	pages = {17--26},
}

@misc{noauthor_multifidelitysbi_nodate,
	title = {{multifidelitySBI}},
	url = {https://www.overleaf.com/project/674d8ed2dbc85b2c40d1e9db},
	abstract = {An online LaTeX editor thatâs easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2025-01-21},
}

@article{randi_neural_2023,
	title = {Neural signal propagation atlas of {Caenorhabditis} elegans},
	volume = {623},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06683-4},
	doi = {10.1038/s41586-023-06683-4},
	abstract = {Establishing how neural function emerges from network properties is a fundamental problem in neuroscience1. Here, to better understand the relationship between the structure and the function of a nervous system, we systematically measure signal propagation in 23,433 pairs of neurons across the head of the nematode Caenorhabditis elegans by direct optogenetic activation and simultaneous whole-brain calcium imaging. We measure the sign (excitatory or inhibitory), strength, temporal properties and causal direction of signal propagation between these neurons to create a functional atlas. We find that signal propagation differs from model predictions that are based on anatomy. Using mutants, we show that extrasynaptic signalling not visible from anatomy contributes to this difference. We identify many instances of dense-core-vesicle-dependent signalling,Â including on timescales of less than aÂ second, that evoke acute calcium transientsâoften where no direct wired connection exists but where relevant neuropeptides and receptors are expressed. We propose that, in such cases, extrasynaptically released neuropeptides serve a similar function to that of classical neurotransmitters. Finally, our measured signal propagation atlas better predicts the neural dynamics of spontaneous activity than do models based on anatomy. We conclude that both synaptic and extrasynaptic signalling drive neural dynamics on short timescales, and that measurements of evoked signal propagation are crucial for interpreting neural function.},
	language = {en},
	number = {7986},
	urldate = {2024-02-26},
	journal = {Nature},
	author = {Randi, Francesco and Sharma, Anuj K. and Dvali, Sophie and Leifer, Andrew M.},
	month = nov,
	year = {2023},
	keywords = {Biological physics, Biophysical models, Fluorescence imaging, Neural circuits, Optogenetics},
	pages = {406--414},
}

@article{smith_new_2020,
	series = {Cellular {Neuroscience}},
	title = {New light on cortical neuropeptides and synaptic network plasticity},
	volume = {63},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438820300817},
	doi = {10.1016/j.conb.2020.04.002},
	abstract = {Neuropeptides, members of a large and evolutionarily ancient family of proteinaceous cellâcell signaling molecules, are widely recognized as extremely potent regulators of brain function and behavior. At the cellular level, neuropeptides are known to act mainly via modulation of ion channel and synapse function, but functional impacts emerging at the level of complex cortical synaptic networks have resisted mechanistic analysis. New findings from single-cell RNA-seq transcriptomics now illuminate intricate patterns of cortical neuropeptide signaling gene expression and new tools now offer powerful molecular access to cortical neuropeptide signaling. Here we highlight some of these new findings and tools, focusing especially on prospects for experimental and theoretical exploration of peptidergic and synaptic networks interactions underlying cortical function and plasticity.},
	urldate = {2024-01-28},
	journal = {Current Opinion in Neurobiology},
	author = {Smith, Stephen J and Hawrylycz, Michael and Rossier, Jean and SÃ¼mbÃ¼l, Uygar},
	month = aug,
	year = {2020},
	pages = {176--188},
}

@article{kobyzev_normalizing_2021,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Normalizing {Flows}},
	url = {http://arxiv.org/abs/1908.09257},
	doi = {10.1109/TPAMI.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	number = {11},
	urldate = {2024-01-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	month = nov,
	year = {2021},
	keywords = {Computational modeling, Computer Science - Machine Learning, Context modeling, Estimation, Generative models, Jacobian matrices, Mathematical model, Random variables, Statistics - Machine Learning, Training, density estimation, invertible neural networks, normalizing flows, variational inference},
	pages = {3964--3979},
}

@article{dekkers_plasticity_2021,
	title = {Plasticity in gustatory and nociceptive neurons controls decision making in {C}. elegans salt navigation},
	volume = {4},
	issn = {2399-3642},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8429449/},
	doi = {10.1038/s42003-021-02561-9},
	abstract = {A conventional understanding of perception assigns sensory organs the role of capturing the environment. Better sensors result in more accurate encoding of stimuli, allowing for cognitive processing downstream. Here we show that plasticity in sensory neurons mediates a behavioral switch in C. elegans between attraction to NaCl in naÃ¯ve animals and avoidance of NaCl in preconditioned animals, called gustatory plasticity. Ca2+ imaging in ASE and ASH NaCl sensing neurons reveals multiple cell-autonomous and distributed circuit adaptation mechanisms. A computational model quantitatively accounts for observed behaviors and reveals roles for sensory neurons in the control and modulation of motor behaviors, decision making and navigational strategy. Sensory adaptation dynamically alters the encoding of the environment. Rather than encoding the stimulus directly, therefore, we propose that these C. elegans sensors dynamically encode a context-dependent value of the stimulus. Our results demonstrate how adaptive sensory computation can directly control an animalâs behavioral state., Martijn Dekkers and Felix Salfelder et al. combine experimental approaches and mathematical modeling to determine the contribution of the two main NaCl sensory neurons (termed ASEL and ASER) and the nociceptive neurons (termed ASH) in C. elegans to the context-dependent switching between NaCl attraction and avoidance. Their results show that regulated sensitivity of these sensory neurons to NaCl allows the animal to dynamically modulate its behavioral response and suggest a role for sensory modulation in balancing exploration and exploitation during foraging.},
	urldate = {2024-02-26},
	journal = {Communications Biology},
	author = {Dekkers, Martijn P. J. and Salfelder, Felix and Sanders, Tom and Umuerri, Oluwatoroti and Cohen, Netta and Jansen, Gert},
	month = sep,
	year = {2021},
	pmid = {34504291},
	pmcid = {PMC8429449},
	pages = {1053},
}

@article{yanez-guerra_premetazoan_2022,
	title = {Premetazoan {Origin} of {Neuropeptide} {Signaling}},
	volume = {39},
	issn = {1537-1719},
	url = {https://doi.org/10.1093/molbev/msac051},
	doi = {10.1093/molbev/msac051},
	abstract = {Neuropeptides are a diverse class of signaling molecules in metazoans. They occur in all animals with a nervous system and also in neuron-less placozoans. However, their origin has remained unclear because no neuropeptide shows deep homology across lineages, and none have been found in sponges. Here, we identify two neuropeptide precursors, phoenixin (PNX) and nesfatin, with broad evolutionary conservation. By database searches, sequence alignments, and gene-structure comparisons, we show that both precursors are present in bilaterians, cnidarians, ctenophores, and sponges. We also found PNX and a secreted nesfatin precursor homolog in the choanoflagellate Salpingoeca rosetta. PNX, in particular, is highly conserved, including its cleavage sites, suggesting that prohormone processing occurs also in choanoflagellates. In addition, based on phyletic patterns and negative pharmacological assays, we question the originally proposed GPR-173 (SREB3) as a PNX receptor. Our findings revealed that secreted neuropeptide homologs derived from longer precursors have premetazoan origins and thus evolved before neurons.},
	number = {4},
	urldate = {2024-02-23},
	journal = {Molecular Biology and Evolution},
	author = {YaÃ±ez-Guerra, Luis Alfonso and Thiel, Daniel and JÃ©kely, GÃ¡spÃ¡r},
	month = apr,
	year = {2022},
	keywords = {Animals, Biological Evolution, Choanoflagellata, Ctenophora, Nervous System, Neuropeptides, choanoflagellate, ctenophore, nesfatin, neuropeptide, phoenixin, sponge},
	pages = {msac051},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	volume = {32},
	shorttitle = {{PyTorch}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.
We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
	urldate = {2024-12-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = {2019},
}

@article{pritz_principles_2023,
	title = {Principles for coding associative memories in a compact neural network},
	volume = {12},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.74434},
	doi = {10.7554/eLife.74434},
	abstract = {A major goal in neuroscience is to elucidate the principles by which memories are stored in a neural network. Here, we have systematically studied how four types of associative memories (short- and long-term memories, each as positive and negative associations) are encoded within the compact neural network of Caenorhabditis elegans worms. Interestingly, sensory neurons were primarily involved in coding short-term, but not long-term, memories, and individual sensory neurons could be assigned to coding either the conditioned stimulus or the experience valence (or both). Moreover, when considering the collective activity of the sensory neurons, the specific training experiences could be decoded. Interneurons integrated the modulated sensory inputs and a simple linear combination model identified the experience-specific modulated communication routes. The widely distributed memory suggests that integrated network plasticity, rather than changes to individual neurons, underlies the fine behavioral plasticity. This comprehensive study reveals basic memory-coding principles and highlights the central roles of sensory neurons in memory formation.},
	urldate = {2024-02-27},
	journal = {eLife},
	author = {Pritz, Christian and Itskovits, Eyal and Bokman, Eduard and Ruach, Rotem and Gritsenko, Vladimir and Nelken, Tal and Menasherof, Mai and Azulay, Aharon and Zaslaver, Alon},
	editor = {Iino, Yuichi and Behrens, Timothy E},
	month = may,
	year = {2023},
	keywords = {calcium imaging, memory coding, neural netwrok},
	pages = {e74434},
}

@article{dax_real-time_2021,
	title = {Real-{Time} {Gravitational} {Wave} {Science} with {Neural} {Posterior} {Estimation}},
	volume = {127},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.127.241103},
	doi = {10.1103/PhysRevLett.127.241103},
	abstract = {We demonstrate unprecedented accuracy for rapid gravitational wave parameter estimation with deep learning. Using neural networks as surrogates for Bayesian posterior distributions, we analyze eight gravitational wave events from the first LIGO-Virgo Gravitational-Wave Transient Catalog and find very close quantitative agreement with standard inference codes, but with inference times reduced from O(day) to 20 s per event. Our networks are trained using simulated data, including an estimate of the detector noise characteristics near the event. This encodes the signal and noise models within millions of neural-network parameters and enables inference for any observed data consistent with the training distribution, accounting for noise nonstationarity from event to event. Our algorithmâcalled âDINGOââsets a new standard in fast and accurate inference of physical parameters of detected gravitational wave events, which should enable real-time data analysis without sacrificing accuracy.},
	number = {24},
	urldate = {2024-01-16},
	journal = {Physical Review Letters},
	author = {Dax, Maximilian and Green, Stephen R. and Gair, Jonathan and Macke, Jakob H. and Buonanno, Alessandra and SchÃ¶lkopf, Bernhard},
	month = dec,
	year = {2021},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, General Relativity and Quantum Cosmology},
	pages = {241103},
}

@inproceedings{papamakarios_sequential_2019,
	title = {Sequential {Neural} {Likelihood}: {Fast} {Likelihood}-free {Inference} with {Autoregressive} {Flows}},
	shorttitle = {Sequential {Neural} {Likelihood}},
	url = {https://proceedings.mlr.press/v89/papamakarios19a.html},
	abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
	language = {en},
	urldate = {2023-12-11},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Papamakarios, George and Sterratt, David and Murray, Iain},
	month = apr,
	year = {2019},
	pages = {837--848},
}

@article{boelts_simulation-based_2023,
	title = {Simulation-based inference for efficient identification of generative models in computational connectomics},
	volume = {19},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011406},
	doi = {10.1371/journal.pcbi.1011406},
	abstract = {Recent advances in connectomics research enable the acquisition of increasing amounts of data about the connectivity patterns of neurons. How can we use this wealth of data to efficiently derive and test hypotheses about the principles underlying these patterns? A common approach is to simulate neuronal networks using a hypothesized wiring rule in a generative model and to compare the resulting synthetic data with empirical data. However, most wiring rules have at least some free parameters, and identifying parameters that reproduce empirical data can be challenging as it often requires manual parameter tuning. Here, we propose to use simulation-based Bayesian inference (SBI) to address this challenge. Rather than optimizing a fixed wiring rule to fit the empirical data, SBI considers many parametrizations of a rule and performs Bayesian inference to identify the parameters that are compatible with the data. It uses simulated data from multiple candidate wiring rule parameters and relies on machine learning methods to estimate a probability distribution (the âposterior distribution over parameters conditioned on the dataâ) that characterizes all data-compatible parameters. We demonstrate how to apply SBI in computational connectomics by inferring the parameters of wiring rules in an in silico model of the rat barrel cortex, given in vivo connectivity measurements. SBI identifies a wide range of wiring rule parameters that reproduce the measurements. We show how access to the posterior distribution over all data-compatible parameters allows us to analyze their relationship, revealing biologically plausible parameter interactions and enabling experimentally testable predictions. We further show how SBI can be applied to wiring rules at different spatial scales to quantitatively rule out invalid wiring hypotheses. Our approach is applicable to a wide range of generative models used in connectomics, providing a quantitative and efficient way to constrain model parameters with empirical connectivity data.},
	language = {en},
	number = {9},
	urldate = {2024-01-16},
	journal = {PLOS Computational Biology},
	author = {Boelts, Jan and Harth, Philipp and Gao, Richard and Udvary, Daniel and YÃ¡Ã±ez, Felipe and Baum, Daniel and Hege, Hans-Christian and Oberlaender, Marcel and Macke, Jakob H.},
	month = sep,
	year = {2023},
	keywords = {Artificial neural networks, Connectomics, Dendritic structure, Neuronal dendrites, Neurons, Probability distribution, Simulation and modeling, Synapses},
	pages = {e1011406},
}

@article{berkooz_proper_1993,
	title = {The {Proper} {Orthogonal} {Decomposition} in the {Analysis} of {Turbulent} {Flows}},
	volume = {25},
	issn = {0066-4189, 1545-4479},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.fl.25.010193.002543},
	doi = {10.1146/annurev.fl.25.010193.002543},
	language = {en},
	number = {Volume 25, 1993},
	urldate = {2025-01-13},
	journal = {Annual Review of Fluid Mechanics},
	author = {Berkooz, G. and Holmes, P. and Lumley, J. L.},
	month = jan,
	year = {1993},
	pages = {539--575},
}

@misc{avila_symmetries_2024,
	title = {Symmetries and synchronization from whole-neural activity in \{{\textbackslash}it {C}. elegans\} connectome: {Integration} of functional and structural networks},
	shorttitle = {Symmetries and synchronization from whole-neural activity in \{{\textbackslash}it {C}. elegans\} connectome},
	url = {http://arxiv.org/abs/2409.02682},
	doi = {10.48550/arXiv.2409.02682},
	abstract = {Understanding the dynamical behavior of complex systems from their underlying network architectures is a long-standing question in complexity theory. Therefore, many metrics have been devised to extract network features like motifs, centrality, and modularity measures. It has previously been proposed that network symmetries are of particular importance since they are expected to underly the synchronization of a system's units, which is ubiquitously observed in nervous system activity patterns. However, perfectly symmetrical structures are difficult to assess in noisy measurements of biological systems, like neuronal connectomes. Here, we devise a principled method to infer network symmetries from combined connectome and neuronal activity data. Using nervous system-wide population activity recordings of the {\textbackslash}textit\{C.elegans\} backward locomotor system, we infer structures in the connectome called fibration symmetries, which can explain which group of neurons synchronize their activity. Our analysis suggests functional building blocks in the animal's motor periphery, providing new testable hypotheses on how descending interneuron circuits communicate with the motor periphery to control behavior. Our approach opens a new door to exploring the structure-function relations in other complex systems, like the nervous systems of larger animals.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Avila, Bryant and Augusto, Pedro and Phillips, David and Gili, Tommaso and Zimmer, Manuel and Makse, HernÃ¡n A.},
	month = sep,
	year = {2024},
	keywords = {Physics - Applied Physics, Quantitative Biology - Neurons and Cognition},
}

@article{titley_toward_2017,
	title = {Toward a {Neurocentric} {View} of {Learning}},
	volume = {95},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627317304592},
	doi = {10.1016/j.neuron.2017.05.021},
	abstract = {Synaptic plasticity (e.g., long-term potentiation [LTP]) is considered the cellular correlate of learning. Recent optogenetic studies on memory engram formation assign a critical role in learning to suprathreshold activation of neurons and their integration into active engrams (âengram cellsâ). Here we review evidence that ensemble integration may result from LTP but also from cell-autonomous changes in membrane excitability. We propose that synaptic plasticity determines synaptic connectivity maps, whereas intrinsic plasticityâpossibly separated in timeâamplifies neuronal responsiveness and acutely drives engram integration. Our proposal marks a move away from an exclusively synaptocentric toward a non-exclusive, neurocentric view of learning.},
	number = {1},
	urldate = {2023-12-06},
	journal = {Neuron},
	author = {Titley, Heather K. and Brunel, Nicolas and Hansel, Christian},
	month = jul,
	year = {2017},
	keywords = {Purkinje cell, cerebellum, ensemble, hippocampus, intrinsic, memory engram, metalearning, neocortex, plasticity, pyramidal cell, synaptic},
	pages = {19--32},
}

@misc{gambardella_transflow_2019,
	title = {Transflow {Learning}: {Repurposing} {Flow} {Models} {Without} {Retraining}},
	shorttitle = {Transflow {Learning}},
	url = {http://arxiv.org/abs/1911.13270},
	doi = {10.48550/arXiv.1911.13270},
	abstract = {It is well known that deep generative models have a rich latent space, and that it is possible to smoothly manipulate their outputs by traversing this latent space. Recently, architectures have emerged that allow for more complex manipulations, such as making an image look as though it were from a different class, or painted in a certain style. These methods typically require large amounts of training in order to learn a single class of manipulations. We present Transflow Learning, a method for transforming a pre-trained generative model so that its outputs more closely resemble data that we provide afterwards. In contrast to previous methods, Transflow Learning does not require any training at all, and instead warps the probability distribution from which we sample latent vectors using Bayesian inference. Transflow Learning can be used to solve a wide variety of tasks, such as neural style transfer and few-shot classification.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Gambardella, Andrew and Baydin, AtÄ±lÄ±m GÃ¼neÅ and Torr, Philip H. S.},
	month = dec,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{talts_validating_2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	doi = {10.48550/arXiv.1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	keywords = {Statistics - Methodology},
}

@incollection{martino_acceptreject_2018,
	address = {Cham},
	title = {Acceptâ{Reject} {Methods}},
	isbn = {978-3-319-72634-2},
	url = {https://doi.org/10.1007/978-3-319-72634-2_3},
	abstract = {The accept/reject method, also known as rejection sampling (RS), was suggested by John von Neumann in 1951. It is a classical Monte Carlo technique for universal sampling that can be used to generate samples virtually from any target density po(x) by drawing from a simpler proposal density Ï(x). The sample is either accepted or rejected by an adequate test of the ratio of the two pdfs, and it can be proved that accepted samples are actually distributed according to the target distribution. Specifically, the RS algorithm can be viewed as choosing a subsequence of i.i.d. realizations from the proposal density Ï(x) in such a way that the elements of the subsequence have density po(x).},
	language = {en},
	urldate = {2025-01-28},
	booktitle = {Independent {Random} {Sampling} {Methods}},
	publisher = {Springer International Publishing},
	author = {Martino, Luca and Luengo, David and MÃ­guez, JoaquÃ­n},
	editor = {Martino, Luca and Luengo, David and MÃ­guez, JoaquÃ­n},
	year = {2018},
	doi = {10.1007/978-3-319-72634-2_3},
	pages = {65--113},
}

@article{jarvenpaa_efficient_2019,
	title = {Efficient {Acquisition} {Rules} for {Model}-{Based} {Approximate} {Bayesian} {Computation}},
	volume = {14},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Efficient-Acquisition-Rules-for-Model-Based-Approximate-Bayesian-Computation/10.1214/18-BA1121.full},
	doi = {10.1214/18-BA1121},
	abstract = {Approximate Bayesian computation (ABC) is a method for Bayesian inference when the likelihood is unavailable but simulating from the model is possible. However, many ABC algorithms require a large number of simulations, which can be costly. To reduce the computational cost, Bayesian optimisation (BO) and surrogate models such as Gaussian processes have been proposed. Bayesian optimisation enables one to intelligently decide where to evaluate the model next but common BO strategies are not designed for the goal of estimating the posterior distribution. Our paper addresses this gap in the literature. We propose to compute the uncertainty in the ABC posterior density, which is due to a lack of simulations to estimate this quantity accurately, and define a loss function that measures this uncertainty. We then propose to select the next evaluation location to minimise the expected loss. Experiments show that the proposed method often produces the most accurate approximations as compared to common BO strategies.},
	number = {2},
	urldate = {2025-01-28},
	journal = {Bayesian Analysis},
	author = {JÃ¤rvenpÃ¤Ã¤, Marko and Gutmann, Michael U. and Pleska, Arijus and Vehtari, Aki and Marttinen, Pekka},
	month = jun,
	year = {2019},
	keywords = {Approximate Bayesian Computation, Bayesian optimisation, Gaussian processes, intractable likelihood, sequential experiment design},
	pages = {595--622},
}

@misc{lakshminarayanan_simple_2017,
	title = {Simple and {Scalable} {Predictive} {Uncertainty} {Estimation} using {Deep} {Ensembles}},
	url = {http://arxiv.org/abs/1612.01474},
	doi = {10.48550/arXiv.1612.01474},
	abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	month = nov,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schmitt_detecting_2024,
	title = {Detecting {Model} {Misspecification} in {Amortized} {Bayesian} {Inference} with {Neural} {Networks}: {An} {Extended} {Investigation}},
	shorttitle = {Detecting {Model} {Misspecification} in {Amortized} {Bayesian} {Inference} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/2406.03154},
	doi = {10.48550/arXiv.2406.03154},
	abstract = {Recent advances in probabilistic deep learning enable efficient amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program (simulation-based inference; SBI). But how faithful is such inference if the simulation represents reality somewhat inaccurately, that is, if the true system behavior at test time deviates from the one seen during training? We conceptualize the types of such model misspecification arising in SBI and systematically investigate how the performance of neural posterior approximators gradually deteriorates as a consequence, making inference results less and less trustworthy. To notify users about this problem, we propose a new misspecification measure that can be trained in an unsupervised fashion (i.e., without training data from the true distribution) and reliably detects model misspecification at test time. Our experiments clearly demonstrate the utility of our new measure both on toy examples with an analytical ground-truth and on representative scientific tasks in cell biology, cognitive decision making, disease outbreak dynamics, and computer vision. We show how the proposed misspecification test warns users about suspicious outputs, raises an alarm when predictions are not trustworthy, and guides model designers in their search for better simulators.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Schmitt, Marvin and BÃ¼rkner, Paul-Christian and KÃ¶the, Ullrich and Radev, Stefan T.},
	month = jun,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{dong_multi-fidelity_2015,
	title = {Multi-fidelity information fusion based on prediction of kriging},
	volume = {51},
	issn = {1615-1488},
	url = {https://doi.org/10.1007/s00158-014-1213-9},
	doi = {10.1007/s00158-014-1213-9},
	abstract = {In this paper, a novel kriging-based multi-fidelity method is proposed. Firstly, the model uncertainty of low-fidelity and high-fidelity models is quantified. On the other hand, the prediction uncertainty of kriging-based surrogate models(SM) is confirmed by its mean square error. After that, the integral uncertainty is acquired by math modeling. Meanwhile, the SMs are constructed through data from low-fidelity and high-fidelity models. Eventually, the low-fidelity (LF) and high-fidelity (HF) SMs with integral uncertainty are obtained and a proposed fusion algorithm is implemented. The fusion algorithm refers to the Kalman filterâs idea of optimal estimation to utilize the independent information from different models synthetically. Through several mathematical examples implemented, the fused SM is certified that its variance is decreased and the fused results tend to the true value. In addition, an engineering example about autonomous underwater vehiclesâ hull design is provided to prove the feasibility of this proposed multi-fidelity method in practice. In the future, it will be a helpful tool to deal with reliability optimization of black-box problems and potentially applied in multidisciplinary design optimization.},
	language = {en},
	number = {6},
	urldate = {2025-01-26},
	journal = {Structural and Multidisciplinary Optimization},
	author = {Dong, Huachao and Song, Baowei and Wang, Peng and Huang, Shuai},
	month = jun,
	year = {2015},
	keywords = {Information fusion, Kriging-based model, Model uncertainty, Multi-fidelity, Surrogate model},
	pages = {1267--1280},
}

@article{stern_simulation-based_1997,
	title = {Simulation-{Based} {Estimation}},
	volume = {35},
	issn = {0022-0515},
	url = {https://www.jstor.org/stable/2729885},
	number = {4},
	urldate = {2025-01-26},
	journal = {Journal of Economic Literature},
	author = {Stern, Steven},
	year = {1997},
	pages = {2006--2039},
}

@article{hoppe_dream_2021,
	title = {{DREAM}: {A} fluid-kinetic framework for tokamak disruption runaway electron simulations},
	volume = {268},
	issn = {0010-4655},
	shorttitle = {{DREAM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0010465521002101},
	doi = {10.1016/j.cpc.2021.108098},
	abstract = {Avoidance of the harmful effects of runaway electrons (REs) in plasma-terminating disruptions is pivotal in the design of safety systems for magnetic fusion devices. Here, we describe a computationally efficient numerical tool, that allows for self-consistent simulations of plasma cooling and associated RE dynamics during disruptions. It solves flux-surface averaged transport equations for the plasma density, temperature and poloidal flux, using a bounce-averaged kinetic equation to self-consistently provide the electron current, heat, density and RE evolution, as well as the electron distribution function. As an example, we consider disruption scenarios with material injection and compare the electron dynamics resolved with different levels of complexity, from fully kinetic to fluid modes.
Program summary
Program Title: Dream CPC Library link to program files: https://doi.org/10.17632/vs3yvnrzg6.1 Developer's repository link: https://github.com/chalmersplasmatheory/DREAM Licensing provisions: MIT Programming language: C++, Python Nature of problem: Self-consistently simulates the plasma evolution in a tokamak disruption, with specific emphasis on runaway electron dynamics. The runaway electrons can be simulated either as a fluid, fully kinetically, or as a mix of the two. Plasma temperature, current density, electric field, ion density and charge states are all evolved self-consistently, where kinetic non-thermal contributions are captured using an orbit-averaged relativistic electron Fokker-Planck equation, which couples to the plasma evolution. In the typical use case, the electrons are represented by two distinct populations: a cold fluid population and a kinetic superthermal population. Solution method: The system of equations is solved using a standard multidimensional Newton's method. Partial differential equationsâmost prominently the bounce-averaged FokkerâPlanck and current diffusion equationsâare discretized using a high-resolution finite volume scheme that preserves density and positivity.},
	urldate = {2025-01-26},
	journal = {Computer Physics Communications},
	author = {Hoppe, Mathias and Embreus, Ola and FÃ¼lÃ¶p, TÃ¼nde},
	month = nov,
	year = {2021},
	keywords = {Fokker-Planck, Runaway electrons, Tokamak disruptions},
	pages = {108098},
}

@article{behrens_new_2015,
	title = {New computational methods in tsunami science},
	volume = {373},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2014.0382},
	doi = {10.1098/rsta.2014.0382},
	abstract = {Tsunamis are rare events with severe consequences. This generates a high demand on accurate simulation results for planning and risk assessment purposes because of the low availability of actual data from historic events. On the other hand, validation of simulation tools becomes very difficult with such a low amount of real-world data. Tsunami phenomena involve a large span of spatial and temporal scalesâfrom ocean basin scales of  to local coastal wave interactions of  or even , or from resonating wave phenomena with durations of  to rupture with time periods of . The scale gap of five orders of magnitude in each dimension makes accurate modelling very demanding, with a number of approaches being taken to work around the impossibility of direct numerical simulations. Along with the mentioned multi-scale characteristic, the tsunami wave has a multitude of different phases, corresponding to different wave regimes and associated equation sets. While in the deep ocean, wave propagation can be approximated relatively accurately by linear shallow-water theory, the transition to a bore or solitary wave train in shelf areas and then into a breaking wave in coastal regions demands appropriate mathematical and numerical treatments. The short duration and unpredictability of tsunami events pose another challenging requirement to tsunami simulation approaches. An accurate forecast is sought within seconds with very limited data available. Thus, efficiency in numerical solution processes and at the same time the consideration of uncertainty play a big role in tsunami modelling applied for forecasting purposes.},
	number = {2053},
	urldate = {2025-01-26},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Behrens, J. and Dias, F.},
	month = oct,
	year = {2015},
	keywords = {Galerkin methods, finite-volume methods, numerical methods, statistical emulator, tsunamis},
	pages = {20140382},
}

@article{brodland_how_2015,
	series = {Coding and non-coding {RNAs} \& {Mammalian} development},
	title = {How computational models can help unlock biological systems},
	volume = {47-48},
	issn = {1084-9521},
	url = {https://www.sciencedirect.com/science/article/pii/S1084952115001287},
	doi = {10.1016/j.semcdb.2015.07.001},
	abstract = {With computation models playing an ever increasing role in the advancement of science, it is important that researchers understand what it means to model something; recognize the implications of the conceptual, mathematical and algorithmic steps of model construction; and comprehend what models can and cannot do. Here, we use examples to show that models can serve a wide variety of roles, including hypothesis testing, generating new insights, deepening understanding, suggesting and interpreting experiments, tracing chains of causation, doing sensitivity analyses, integrating knowledge, and inspiring new approaches. We show that models can bring together information of different kinds and do so across a range of length scales, as they do in multi-scale, multi-faceted embryogenesis models, some of which connect gene expression, the cytoskeleton, cell properties, tissue mechanics, morphogenetic movements and phenotypes. Models cannot replace experiments nor can they prove that particular mechanisms are at work in a given situation. But they can demonstrate whether or not a proposed mechanism is sufficient to produce an observed phenomenon. Although the examples in this article are taken primarily from the field of embryo mechanics, most of the arguments and discussion are applicable to any form of computational modelling.},
	urldate = {2025-01-26},
	journal = {Seminars in Cell \& Developmental Biology},
	author = {Brodland, G. Wayne},
	month = dec,
	year = {2015},
	keywords = {Biological systems, Cell mechanics, Computational modelling, Developmental mechanisms, Embryo mechanics, Embryogenesis, Models, Morphogenetic movements, Review, Tissue mechanics},
	pages = {62--73},
}

@article{calder_computational_2018,
	title = {Computational modelling for decision-making: where, why, what, who and how},
	volume = {5},
	shorttitle = {Computational modelling for decision-making},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.172096},
	doi = {10.1098/rsos.172096},
	abstract = {In order to deal with an increasingly complex world, we need ever more sophisticated computational models that can help us make decisions wisely and understand the potential consequences of choices. But creating a model requires far more than just raw data and technical skills: it requires a close collaboration between model commissioners, developers, users and reviewers. Good modelling requires its users and commissioners to understand more about the whole process, including the different kinds of purpose a model can have and the different technical bases. This paper offers a guide to the process of commissioning, developing and deploying models across a wide range of domains from public policy to science and engineering. It provides two checklists to help potential modellers, commissioners and users ensure they have considered the most significant factors that will determine success. We conclude there is a need to reinforce modelling as a discipline, so that misconstruction is less likely; to increase understanding of modelling in all domains, so that the misuse of models is reduced; and to bring commissioners closer to modelling, so that the results are more useful.},
	number = {6},
	urldate = {2025-01-26},
	journal = {Royal Society Open Science},
	author = {Calder, Muffy and Craig, Claire and Culley, Dave and de Cani, Richard and Donnelly, Christl A. and Douglas, Rowan and Edmonds, Bruce and Gascoigne, Jonathon and Gilbert, Nigel and Hargrove, Caroline and Hinds, Derwen and Lane, David C. and Mitchell, Dervilla and Pavey, Giles and Robertson, David and Rosewell, Bridget and Sherwin, Spencer and Walport, Mark and Wilson, Alan},
	month = jun,
	year = {2018},
	keywords = {communication, complexity, data, decision-making, modelling, uncertainty},
	pages = {172096},
}

@inproceedings{kandasamy_multi-fidelity_2017,
	title = {Multi-fidelity {Bayesian} {Optimisation} with {Continuous} {Approximations}},
	url = {https://proceedings.mlr.press/v70/kandasamy17a.html},
	abstract = {Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, multi-fidelity methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multi-fidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-fidelity methods assume only a finite number of approximations. On the other hand, in many practical applications, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data \$N\$ and/or few training iterations \$T\$. Here, the approximations are best viewed as arising out of a continuous two dimensional space \$(N,T)\$. In this work, we develop a Bayesian optimisation method, BOCA, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. BOCA outperforms several other baselines in synthetic and real experiments.},
	language = {en},
	urldate = {2025-01-26},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Schneider, Jeff and PÃ³czos, BarnabÃ¡s},
	month = jul,
	year = {2017},
	pages = {1799--1808},
}

@misc{noauthor_handbook_nodate,
	title = {âª{Handbook} of {LHC} {Higgs} cross sections: 4. {Deciphering} the nature of the {Higgs} sectorâ¬},
	shorttitle = {âª{Handbook} of {LHC} {Higgs} cross sections},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=de&user=ZdUMvCsAAAAJ&citation_for_view=ZdUMvCsAAAAJ:ULOm3_A8WrAC},
	abstract = {âªD de Florian, C Grojean, F Maltoni, C Mariotti, A Nikitenko, M Pieri, P Savard, M Schumacher, R Tanaka, R Aggletonâ¦, 2016â¬ - âª2.071-mal zitiertâ¬},
	urldate = {2025-01-24},
}

@article{dingeldein_simulation-based_2023,
	title = {Simulation-based inference of single-molecule force spectroscopy},
	volume = {4},
	issn = {2632-2153},
	url = {http://arxiv.org/abs/2209.10392},
	doi = {10.1088/2632-2153/acc8b8},
	abstract = {Single-molecule force spectroscopy (smFS) is a powerful approach to studying molecular self-organization. However, the coupling of the molecule with the ever-present experimental device introduces artifacts, that complicates the interpretation of these experiments. Performing statistical inference to learn hidden molecular properties is challenging because these measurements produce non-Markovian time-series, and even minimal models lead to intractable likelihoods. To overcome these challenges, we developed a computational framework built on novel statistical methods called simulation-based inference (SBI). SBI enabled us to directly estimate the Bayesian posterior, and extract reduced quantitative models from smFS, by encoding a mechanistic model into a simulator in combination with probabilistic deep learning. Using synthetic data, we could systematically disentangle the measurement of hidden molecular properties from experimental artifacts. The integration of physical models with machine learning density estimation is general, transparent, easy to use, and broadly applicable to other types of biophysical experiments.},
	number = {2},
	urldate = {2025-01-24},
	journal = {Machine Learning: Science and Technology},
	author = {Dingeldein, Lars and Cossio, Pilar and Covino, Roberto},
	month = jun,
	year = {2023},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Chemical Physics, Physics - Computational Physics, Quantitative Biology - Biomolecules},
	pages = {025009},
}

@misc{bharti_cost-aware_2024,
	title = {Cost-aware {Simulation}-based {Inference}},
	url = {http://arxiv.org/abs/2410.07930},
	doi = {10.48550/arXiv.2410.07930},
	abstract = {Simulation-based inference (SBI) is the preferred framework for estimating parameters of intractable models in science and engineering. A significant challenge in this context is the large computational cost of simulating data from complex models, and the fact that this cost often depends on parameter values. We therefore propose {\textbackslash}textit\{cost-aware SBI methods\} which can significantly reduce the cost of existing sampling-based SBI methods, such as neural SBI and approximate Bayesian computation. This is achieved through a combination of rejection and self-normalised importance sampling, which significantly reduces the number of expensive simulations needed. Our approach is studied extensively on models from epidemiology to telecommunications engineering, where we obtain significant reductions in the overall cost of inference.},
	urldate = {2025-01-20},
	publisher = {arXiv},
	author = {Bharti, Ayush and Huang, Daolang and Kaski, Samuel and Briol, FranÃ§ois-Xavier},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{razavi_review_2012,
	title = {Review of surrogate modeling in water resources},
	volume = {48},
	copyright = {Â©2012. American Geophysical Union. All Rights Reserved.},
	issn = {1944-7973},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2011WR011527},
	doi = {10.1029/2011WR011527},
	abstract = {Surrogate modeling, also called metamodeling, has evolved and been extensively used over the past decades. A wide variety of methods and tools have been introduced for surrogate modeling aiming to develop and utilize computationally more efficient surrogates of high-fidelity models mostly in optimization frameworks. This paper reviews, analyzes, and categorizes research efforts on surrogate modeling and applications with an emphasis on the research accomplished in the water resources field. The review analyzes 48 references on surrogate modeling arising from water resources and also screens out more than 100 references from the broader research community. Two broad families of surrogates namely response surface surrogates, which are statistical or empirical data-driven models emulating the high-fidelity model responses, and lower-fidelity physically based surrogates, which are simplified models of the original system, are detailed in this paper. Taxonomies on surrogate modeling frameworks, practical details, advances, challenges, and limitations are outlined. Important observations and some guidance for surrogate modeling decisions are provided along with a list of important future research directions that would benefit the common sampling and search (optimization) analyses found in water resources.},
	language = {en},
	number = {7},
	urldate = {2025-01-17},
	journal = {Water Resources Research},
	author = {Razavi, Saman and Tolson, Bryan A. and Burn, Donald H.},
	year = {2012},
	keywords = {computationally intensive simulation models, function approximation, low-fidelity models, metamodeling, optimization, surrogate modeling},
}

@article{li_-line_2022,
	title = {On-line transfer learning for multi-fidelity data fusion with ensemble of deep neural networks},
	volume = {53},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034622001495},
	doi = {10.1016/j.aei.2022.101689},
	abstract = {Deep Neural Network (DNN) is widely used in engineering applications for its ability to handle problems with almost any nonlinearities. However, it is generally difficult to obtain sufficient high-fidelity (HF) sample points for expensive optimization tasks, which may affect the generalization performance of DNN and result in inaccurate predictions. To solve this problem and improve the prediction accuracy of DNN, this paper proposes an on-line transfer learning based multi-fidelity data fusion (OTL-MFDF) method including two parts. In the first part, the ensemble of DNNs is established. Firstly, a large number of low-fidelity sample points and a few HF sample points are generated, which are used as the source dataset and target dataset, respectively. Then, the Bayesian Optimization (BO) is utilized to obtain several groups of hyperparameters, based on which DNNs are pre-trained using the source dataset. Next, these pre-trained DNNs are re-trained by fine-tuning on the target dataset, and the ensemble of DNNs is established by assigning different weights to each pre-trained DNN. In the second part, the on-line learning system is developed for adaptive updating of the ensemble of DNNs. To evaluate the uncertainty error of the predicted values of DNN and determine the location of the updated HF sample point, the query-by-committee strategy based on the ensemble of DNNs is developed. The Covariance Matrix Adaptation Evolutionary Strategies is employed as the optimizer to find out the location where the maximal disagreement is achieved by the ensemble of DNNs. The design space is partitioned by the Voronoi diagram method, and then the selected point is moved to its nearest Voronoi cell boundary to avoid clustering between the updated point and the existing sample points. Three different types of test problems and an engineering example are adopted to illustrate the effectiveness of the OTL-MFDF method. Results verify the outstanding efficiency, global prediction accuracy and applicability of the OTL-MFDF method.},
	urldate = {2025-01-16},
	journal = {Advanced Engineering Informatics},
	author = {Li, Zengcong and Zhang, Shu and Li, Hongqing and Tian, Kuo and Cheng, Zhizhong and Chen, Yan and Wang, Bo},
	month = aug,
	year = {2022},
	keywords = {Data fusion, Deep neural network, Ensemble of surrogates, Multi-fidelity model, Transfer learning},
	pages = {101689},
}

@article{taverniers_accelerated_2020,
	title = {Accelerated {Multilevel} {Monte} {Carlo} {With} {Kernel}-{Based} {Smoothing} and {Latinized} {Stratification}},
	volume = {56},
	copyright = {Â©2020. American Geophysical Union. All Rights Reserved.},
	issn = {1944-7973},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019WR026984},
	doi = {10.1029/2019WR026984},
	abstract = {Heterogeneity and a paucity of measurements of key material properties undermine the veracity of quantitative predictions of subsurface flow and transport. For such model forecasts to be useful as a management tool, they must be accompanied by computationally expensive uncertainty quantification, which yields confidence intervals, probability of exceedance, and so forth. We design and implement novel multilevel Monte Carlo (MLMC) algorithms that accelerate estimation of the cumulative distribution functions (CDFs) of quantities of interest, for example, water breakthrough time or oil production rate. Compared to standard non-smoothed MLMC, the new estimators achieve a significant variance reduction at each discretization level by smoothing the indicator function with a Gaussian kernel or replacing standard Monte Carlo (MC) with the recently developed hierarchical Latinized stratified sampling (HLSS). After validating the kernel-smoothed MLMC and HLSS-enhanced MLMC methods on a single-phase flow test bed, we demonstrate that they are orders of magnitude faster than standard MC for estimating the CDF of breakthrough times in multiphase flow problems.},
	language = {en},
	number = {9},
	urldate = {2025-01-16},
	journal = {Water Resources Research},
	author = {Taverniers, SÃ¸ren and Bosma, Sebastian B. M. and Tartakovsky, Daniel M.},
	year = {2020},
	keywords = {multiphase flow, probability, random, stochastic, uncertainty},
	pages = {e2019WR026984},
}

@inproceedings{jiang_face_2017,
	title = {Face {Detection} with the {Faster} {R}-{CNN}},
	url = {https://ieeexplore.ieee.org/abstract/document/7961803?casa_token=-q-090iAPz0AAAAA:ROfs0JGSrP6z-qHmRotNbJje6XSoXG0ph7yXylyZysmENWRdH28weBC03dXpJ7mMzWuh22nsqQ},
	doi = {10.1109/FG.2017.82},
	abstract = {While deep learning based methods for generic object detection have improved rapidly in the last two years, most approaches to face detection are still based on the R-CNN framework [11], leading to limited accuracy and processing speed. In this paper, we investigate applying the Faster RCNN [26], which has recently demonstrated impressive results on various object detection benchmarks, to face detection. By training a Faster R-CNN model on the large scale WIDER face dataset [34], we report state-of-the-art results on the WIDER test set as well as two other widely used face detection benchmarks, FDDB and the recently released IJB-A.},
	urldate = {2025-01-16},
	booktitle = {2017 12th {IEEE} {International} {Conference} on {Automatic} {Face} \& {Gesture} {Recognition} ({FG} 2017)},
	author = {Jiang, Huaizu and Learned-Miller, Erik},
	month = may,
	year = {2017},
	keywords = {Benchmark testing, Face, Face detection, Feature extraction, Object detection, Proposals, Training},
	pages = {650--657},
}

@article{haghighat_physics-informed_2021,
	title = {A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics},
	volume = {379},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521000773},
	doi = {10.1016/j.cma.2021.113741},
	abstract = {We present the application of a class of deep learning, known as Physics Informed Neural Networks (PINN), to inversion and surrogate modeling in solid mechanics. We explain how to incorporate the momentum balance and constitutive relations into PINN, and explore in detail the application to linear elasticity, and illustrate its extension to nonlinear problems through an example that showcases vonÂ Mises elastoplasticity. While common PINN algorithms are based on training one deep neural network (DNN), we propose a multi-network model that results in more accurate representation of the field variables. To validate the model, we test the framework on synthetic data generated from analytical and numerical reference solutions. We study convergence of the PINN model, and show that Isogeometric Analysis (IGA) results in superior accuracy and convergence characteristics compared with classic low-order Finite Element Method (FEM). We also show the applicability of the framework for transfer learning, and find vastly accelerated convergence during network re-training. Finally, we find that honoring the physics leads to improved robustness: when trained only on a few parameters, we find that the PINN model can accurately predict the solution for a wide range of parameters new to the networkâthus pointing to an important application of this framework to sensitivity analysis and surrogate modeling.},
	urldate = {2025-01-16},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
	month = jun,
	year = {2021},
	keywords = {Artificial neural network, Elastoplasticity, Inversion, Linear elasticity, Physics-informed deep learning, Transfer learning},
	pages = {113741},
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1517384113},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	number = {15},
	urldate = {2025-01-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pages = {3932--3937},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks â neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit RungeâKutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reactionâdiffusion systems, and the propagation of nonlinear shallow-water waves.},
	urldate = {2025-01-16},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, RungeâKutta methods},
	pages = {686--707},
}

@misc{song_transfer_2021,
	title = {Transfer {Learning} on {Multi}-{Fidelity} {Data}},
	url = {http://arxiv.org/abs/2105.00856},
	doi = {10.48550/arXiv.2105.00856},
	abstract = {Neural networks (NNs) are often used as surrogates or emulators of partial differential equations (PDEs) that describe the dynamics of complex systems. A virtually negligible computational cost of such surrogates renders them an attractive tool for ensemble-based computation, which requires a large number of repeated PDE solves. Since the latter are also needed to generate sufficient data for NN training, the usefulness of NN-based surrogates hinges on the balance between the training cost and the computational gain stemming from their deployment. We rely on multi-fidelity simulations to reduce the cost of data generation for subsequent training of a deep convolutional NN (CNN) using transfer learning. High- and low-fidelity images are generated by solving PDEs on fine and coarse meshes, respectively. We use theoretical results for multilevel Monte Carlo to guide our choice of the numbers of images of each kind. We demonstrate the performance of this multi-fidelity training strategy on the problem of estimation of the distribution of a quantity of interest, whose dynamics is governed by a system of nonlinear PDEs (parabolic PDEs of multi-phase flow in heterogeneous porous media) with uncertain/random parameters. Our numerical experiments demonstrate that a mixture of a comparatively large number of low-fidelity data and smaller numbers of high- and low-fidelity data provides an optimal balance of computational speed-up and prediction accuracy. The former is reported relative to both CNN training on high-fidelity images only and Monte Carlo solution of the PDEs. The latter is expressed in terms of both the Wasserstein distance and the Kullback-Leibler divergence.},
	urldate = {2025-01-16},
	publisher = {arXiv},
	author = {Song, Dong H. and Tartakovsky, Daniel M.},
	month = apr,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
}

@article{chakraborty_transfer_2021,
	title = {Transfer learning based multi-fidelity physics informed deep neural network},
	volume = {426},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120307166},
	doi = {10.1016/j.jcp.2020.109942},
	abstract = {For many systems in science and engineering, the governing differential equation is either not known or known in an approximate sense. Analyses and design of such systems are governed by data collected from the field and/or laboratory experiments. This challenging scenario is further worsened when data-collection is expensive and time-consuming. To address this issue, this paper presents a novel multi-fidelity physics informed deep neural network (MF-PIDNN). The framework proposed is particularly suitable when the physics of the problem is known in an approximate sense (low-fidelity physics) and only a few high-fidelity data are available. MF-PIDNN blends physics informed and data-driven deep learning techniques by using the concept of transfer learning. The approximate governing equation is first used to train a low-fidelity physics informed deep neural network. This is followed by transfer learning where the low-fidelity model is updated by using the available high-fidelity data. MF-PIDNN is able to encode useful information on the physics of the problem from the approximate governing differential equation and hence, provides accurate prediction even in zones with no data. Additionally, no low-fidelity data is required for training this model. Two examples involving function approximations with linear and nonlinear correlation are presented to illustrate the effectiveness of transfer learning in solving multi-fidelity problems. Applicability and utility of MF-PIDNN are illustrated in solving four benchmark reliability analysis problems. Case studies presented illustrate interesting features of the proposed approach.},
	urldate = {2025-01-16},
	journal = {Journal of Computational Physics},
	author = {Chakraborty, Souvik},
	month = feb,
	year = {2021},
	keywords = {Deep learning, Multi-fidelity, Physics-informed, Reliability, Transfer learning},
	pages = {109942},
}

@book{bellman_adaptive_2015,
	title = {Adaptive {Control} {Processes}: {A} {Guided} {Tour}},
	copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
	isbn = {978-1-4008-7466-8},
	shorttitle = {Adaptive {Control} {Processes}},
	url = {https://www.degruyter.com/document/doi/10.1515/9781400874668/html},
	abstract = {The aim of this work is to present a unified approach to the modern field of control theory and to provide a technique for making problems involving deterministic, stochastic, and adaptive processes of both linear and nonlinear type amenable to machine solution. Mr. Bellman has used the theory of dynamic programming to formulate, analyze, and prepare these processes for numerical treatment by digital computers. The unique concept of the book is that of a single problem stretching from recognition and formulation to analytic treatment and computational solution. Due to the emphasis upon ideas and concepts, this book is equally suited for the pure and applied mathematician, and for control engineers in all fields. Originally published in 1961. The Princeton Legacy Library uses the latest print-on-demand technology to again make available previously out-of-print books from the distinguished backlist of Princeton University Press. These editions preserve the original texts of these important books while presenting them in durable paperback and hardcover editions. The goal of the Princeton Legacy Library is to vastly increase access to the rich scholarly heritage found in the thousands of books published by Princeton University Press since its founding in 1905.},
	language = {en},
	urldate = {2025-01-16},
	publisher = {Princeton University Press},
	author = {Bellman, Richard E.},
	month = dec,
	year = {2015},
	doi = {10.1515/9781400874668},
	keywords = {A priori estimate, Accuracy and precision, Adaptive control, Algorithm, Analytic proof, Approximation, Bessel function, Big O notation, Boundary value problem, Calculation, Calculus of variations, Closed-form expression, Computation, Curse of dimensionality, Decision-making, Differential equation, Dynamic programming, Equation, Euler equations (fluid dynamics), Expected value, Extrapolation, Forcing function (differential equations), Functional equation, Hyperbolic partial differential equation, Idealization, Information theory, Ingenuity, Initial condition, Initial value problem, Iteration, Iterative method, Lagrange multiplier, Linear differential equation, Linear equation, Linear programming, Lipschitz continuity, Markov chain, Mathematical optimization, Mathematical physics, Mathematical problem, Mathematics, Monotonic function, Multistage, Newton's method, Nonlinear system, Notation, Numerical analysis, Optimization problem, Parameter, Partial derivative, Partial differential equation, Perturbation theory (quantum mechanics), Phase space, Probability, Probability distribution, Probability theory, Quantity, Recurrence relation, Result, Sensitivity analysis, Special case, State variable, Steady state, Stochastic, Stochastic approximation, Stochastic control, Stochastic process, Theory, Uncertainty, Variable (mathematics)},
}

@article{vogelsberger_cosmological_2020,
	title = {Cosmological simulations of galaxy formation},
	volume = {2},
	url = {https://ui.adsabs.harvard.edu/abs/2020NatRP...2...42V},
	doi = {10.1038/s42254-019-0127-2},
	abstract = {Over recent decades, cosmological simulations of galaxy formation have been instrumental in advancing our understanding of structure and galaxy formation in the Universe. These simulations follow the nonlinear evolution of galaxies, modelling a variety of physical processes over an enormous range of time and length scales. A better understanding of the relevant physical processes, improved numerical methods and increased computing power have led to simulations that can reproduce a large number of the observed galaxy properties. Modern simulations model dark matter, dark energy and ordinary matter in an expanding space-time starting from well-defined initial conditions. The modelling of ordinary matter is most challenging due to the large array of physical processes affecting this component. Cosmological simulations have also proven useful to study alternative cosmological models and their impact on the galaxy population. This Technical Review presents a concise overview of the methodology of cosmological simulations of galaxy formation and their different applications.},
	urldate = {2025-01-16},
	journal = {Nature Reviews Physics},
	author = {Vogelsberger, Mark and Marinacci, Federico and Torrey, Paul and Puchwein, Ewald},
	month = jan,
	year = {2020},
	keywords = {Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics},
	pages = {42--66},
}

@article{puniya_perspectives_2024,
	title = {Perspectives on computational modeling of biological systems and the significance of the {SysMod} community},
	volume = {4},
	issn = {2635-0041},
	url = {https://doi.org/10.1093/bioadv/vbae090},
	doi = {10.1093/bioadv/vbae090},
	abstract = {In recent years, applying computational modeling to systems biology has caused a substantial surge in both discovery and practical applications and a significant shift in our understanding of the complexity inherent in biological systems.In this perspective article, we briefly overview computational modeling in biology, highlighting recent advancements such as multi-scale modeling due to the omics revolution, single-cell technology, and integration of artificial intelligence and machine learning approaches. We also discuss the primary challenges faced: integration, standardization, model complexity, scalability, and interdisciplinary collaboration. Lastly, we highlight the contribution made by the Computational Modeling of Biological Systems (SysMod) Community of Special Interest (COSI) associated with the International Society of Computational Biology (ISCB) in driving progress within this rapidly evolving field through community engagement (via both in person and virtual meetings, social media interactions), webinars, and conferences.Additional information about SysMod is available at https://sysmod.info.},
	number = {1},
	urldate = {2025-01-16},
	journal = {Bioinformatics Advances},
	author = {Puniya, Bhanwar Lal and Verma, Meghna and Damiani, Chiara and Bakr, Shaimaa and DrÃ¤ger, Andreas},
	month = jan,
	year = {2024},
	pages = {vbae090},
}

@article{larsen-freeman_transfer_2013,
	title = {Transfer of {Learning} {Transformed}},
	volume = {63},
	copyright = {Â© 2013 Language Learning Research Club, University of Michigan},
	issn = {1467-9922},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9922.2012.00740.x},
	doi = {10.1111/j.1467-9922.2012.00740.x},
	abstract = {Instruction is motivated by the assumption that students can transfer their learning, or apply what they have learned in school to another setting. A common problem arises when the expected transfer does not take place, what has been referred to as the inert knowledge problem. More than an academic inconvenience, the failure to transfer is a major problem, exacting individual and social costs. In this article, I trace the evolution of research on the transfer of learning, in general, and on language learning, in particular. Then, a different view of learning transfer is advanced. Rather than learners being seen to âexportâ what they have learned from one situation to the next, it is proposed that learners transform their learning. The article concludes by offering some suggestions for how to mitigate the inert knowledge problem from this perspective.},
	language = {en},
	number = {s1},
	urldate = {2025-01-15},
	journal = {Language Learning},
	author = {Larsen-Freeman, Diane},
	year = {2013},
	keywords = {adaptation, affordances, complexity theory, iteration, levels of processing framework, second language learning, transfer appropriate processing, transfer of learning},
	pages = {107--129},
}

@inproceedings{hussain_study_2019,
	address = {Cham},
	title = {A {Study} on {CNN} {Transfer} {Learning} for {Image} {Classification}},
	isbn = {978-3-319-97982-3},
	doi = {10.1007/978-3-319-97982-3_16},
	abstract = {Many image classification models have been introduced to help tackle the foremost issue of recognition accuracy. Image classification is one of the core problems in Computer Vision field with a large variety of practical applications. Examples include: object recognition for robotic manipulation, pedestrian or obstacle detection for autonomous vehicles, among others. A lot of attention has been associated with Machine Learning, specifically neural networks such as the Convolutional Neural Network (CNN) winning image classification competitions. This work proposes the study and investigation of such a CNN architecture model (i.e. Inception-v3) to establish whether it would work best in terms of accuracy and efficiency with new image datasets via Transfer Learning. The retrained model is evaluated, and the results are compared to some state-of-the-art approaches.},
	language = {en},
	booktitle = {Advances in {Computational} {Intelligence} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hussain, Mahbub and Bird, Jordan J. and Faria, Diego R.},
	editor = {Lotfi, Ahmad and Bouchachia, Hamid and Gegov, Alexander and Langensiepen, Caroline and McGinnity, Martin},
	year = {2019},
	keywords = {Caltech Face, Convolutional Neural Network (CNN), Pre-trained Model, Rectified Linear Unit (ReLU), Transfer Learning},
	pages = {191--202},
}

@article{gupta_deep_2022,
	title = {Deep {Learning} ({CNN}) and {Transfer} {Learning}: {A} {Review}},
	volume = {2273},
	issn = {1742-6596},
	shorttitle = {Deep {Learning} ({CNN}) and {Transfer} {Learning}},
	url = {https://dx.doi.org/10.1088/1742-6596/2273/1/012029},
	doi = {10.1088/1742-6596/2273/1/012029},
	abstract = {Deep Learning is a machine learning area that has recently been used in a variety of industries. Unsupervised, semi-supervised, and supervised-learning are only a few of the strategies that have been developed to accommodate different types of learning. A number of experiments showed that deep learning systems fared better than traditional ones when it came to image processing, computer vision, and pattern recognition. Several real-world applications and hierarchical systems have utilised transfer learning and deep learning algorithms for pattern recognition and classification tasks. Real-world machine learning settings, on the other hand, often do not support this assumption since training data can be difficult or expensive to get, and there is a constant need to generate high-performance beginners who can work with data from a variety of sources. The objective of this paper is using deep learning to uncover higher-level representational features, to clearly explain transfer learning, to provide current solutions and evaluate applications in diverse areas of transfer learning as well as deep learning.},
	language = {en},
	number = {1},
	urldate = {2025-01-15},
	journal = {Journal of Physics: Conference Series},
	author = {Gupta, Jaya and Pathak, Sunil and Kumar, Gireesh},
	month = may,
	year = {2022},
	pages = {012029},
}

@article{bengio_deep_2021,
	title = {Deep learning for {AI}},
	volume = {64},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3448250},
	doi = {10.1145/3448250},
	abstract = {How can neural networks learn the rich internal representations required for difficult tasks such as recognizing objects or understanding language?},
	number = {7},
	urldate = {2025-01-15},
	journal = {Commun. ACM},
	author = {Bengio, Yoshua and Lecun, Yann and Hinton, Geoffrey},
	month = jun,
	year = {2021},
	pages = {58--65},
}

@inproceedings{pratt_direct_1991,
	title = {Direct {Transfer} of {Learned} {Information} {Among} {Neural} {Networks}},
	url = {https://www.semanticscholar.org/paper/Direct-Transfer-of-Learned-Information-Among-Neural-Pratt-Mostow/607e20aa228f9f8f0f9e96e841f2b0ec75726728},
	abstract = {A touted advantage of symbolic representations is the ease of transferring learned information from one intelligent agent to another. This paper investigates an analogous problem: how to use information from one neural network to help a second network learn a related task. Rather than translate such information into symbolic form (in which it may not be readily expressible), we investigate the direct transfer of information encoded as weights. 
 
Here, we focus on how transfer can be used to address the important problem of improving neural network learning speed. First we present an exploratory study of the somewhat surprising effects of pre-setting network weights on subsequent learning. Guided by hypotheses from this study, we sped up back-propagation learning for two speech recognition tasks. By transferring weights from smaller networks trained on subtasks, we achieved speedups of up to an order of magnitude compared with training starting with random weights, even taking into account the time to train the smaller networks. We include results on how transfer scales to a large phoneme recognition problem.},
	urldate = {2025-01-15},
	author = {Pratt, L. and Mostow, Jack and Kamm, C.},
	month = jul,
	year = {1991},
}

@article{zamir_taskonomy_nodate,
	title = {Taskonomy: {Disentangling} {Task} {Transfer} {Learning}},
	abstract = {Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.},
	language = {en},
	author = {Zamir, Amir R and Sax, Alexander and Shen, William and Guibas, Leonidas J and Malik, Jitendra and Savarese, Silvio},
}

@article{sisson_sequential_2007,
	title = {Sequential {Monte} {Carlo} without likelihoods},
	volume = {104},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.0607208104},
	doi = {10.1073/pnas.0607208104},
	abstract = {Recent new methods in Bayesian simulation have provided ways of evaluating posterior distributions in the presence of analytically or computationally intractable likelihood functions. Despite representing a substantial methodological advance, existing methods based on rejection sampling or Markov chain Monte Carlo can be highly inefficient and accordingly require far more iterations than may be practical to implement. Here we propose a sequential Monte Carlo sampler that convincingly overcomes these inefficiencies. We demonstrate its implementation through an epidemiological study of the transmission rate of tuberculosis.},
	number = {6},
	urldate = {2025-01-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sisson, S. A. and Fan, Y. and Tanaka, Mark M.},
	month = feb,
	year = {2007},
	pages = {1760--1765},
}

@book{sisson_handbook_2018,
	address = {New York},
	title = {Handbook of {Approximate} {Bayesian} {Computation}},
	isbn = {978-1-315-11719-5},
	abstract = {As the world becomes increasingly complex, so do the statistical models required to analyse the challenging problems ahead. For the very first time in a single volume, the Handbook of Approximate Bayesian Computation (ABC) presents an extensive overview of the theory, practice and application of ABC methods. These simple, but powerful statistical techniques, take Bayesian statistics beyond the need to specify overly simplified models, to the setting where the model is defined only as a process that generates data. This process can be arbitrarily complex, to the point where standard Bayesian techniques based on working with tractable likelihood functions would not be viable. ABC methods finesse the problem of model complexity within the Bayesian framework by exploiting modern computational power, thereby permitting approximate Bayesian analyses of models that would otherwise be impossible to implement.
The Handbook of ABC provides illuminating insight into the world of Bayesian modelling for intractable models for both experts and newcomers alike. It is an essential reference book for anyone interested in learning about and implementing ABC techniques to analyse complex models in the modern world.},
	publisher = {Chapman and Hall/CRC},
	editor = {Sisson, Scott A. and Fan, Yanan and Beaumont, Mark},
	month = sep,
	year = {2018},
	doi = {10.1201/9781315117195},
}

@article{marjoram_markov_2003,
	title = {Markov chain {Monte} {Carlo} without likelihoods},
	volume = {100},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0306899100},
	doi = {10.1073/pnas.0306899100},
	abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
	number = {26},
	urldate = {2025-01-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and TavarÃ©, Simon},
	month = dec,
	year = {2003},
	pages = {15324--15328},
}

@techreport{bishop_mixture_1994,
	type = {Monograph},
	title = {Mixture density networks},
	url = {https://publications.aston.ac.uk/id/eprint/373/},
	abstract = {Minimization of a sum-of-squares or cross-entropy error function leads to network outputs which approximate the conditional averages of the target data, conditioned on the input vector. For classifications problems, with a suitably chosen target coding scheme, these averages represent the posterior probabilities of class membership, and so can be regarded as optimal. For problems involving the prediction of continuous variables, however, the conditional averages provide only a very limited description of the properties of the target variables. This is particularly true for problems in which the mapping to be learned is multi-valued, as often arises in the solution of inverse problems, since the average of several correct target values is not necessarily itself a correct value. In order to obtain a complete description of the data, for the purposes of predicting the outputs corresponding to new input vectors, we must model the conditional probability distribution of the target data, again conditioned on the input vector. In this paper we introduce a new class of network models obtained by combining a conventional neural network with a mixture density model. The complete system is called a Mixture Density Network, and can in principle represent arbitrary conditional probability distributions in the same way that a conventional neural network can represent arbitrary functions. We demonstrate the effectiveness of Mixture Density Networks using both a toy problem and a problem involving robot inverse kinematics.},
	language = {en-GB},
	urldate = {2025-01-15},
	author = {Bishop, Christopher M.},
	year = {1994},
}

@article{schmid_dynamic_2010,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	issn = {1469-7645, 0022-1120},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/dynamic-mode-decomposition-of-numerical-and-experimental-data/AA4C763B525515AD4521A6CC5E10DBD4},
	doi = {10.1017/S0022112010001217},
	abstract = {The description of coherent features of fluid flow is essential to our understanding of fluid-dynamical and transport processes. A method is introduced that is able to extract dynamic information from flow fields that are either generated by a (direct) numerical simulation or visualized/measured in a physical experiment. The extracted dynamic modes, which can be interpreted as a generalization of global stability modes, can be used to describe the underlying physical mechanisms captured in the data sequence or to project large-scale problems onto a dynamical system of significantly fewer degrees of freedom. The concentration on subdomains of the flow field where relevant dynamics is expected allows the dissection of a complex flow into regions of localized instability phenomena and further illustrates the flexibility of the method, as does the description of the dynamics within a spatial framework. Demonstrations of the method are presented consisting of a plane channel flow, flow over a two-dimensional cavity, wake flow behind a flexible membrane and a jet passing between two cylinders.},
	language = {en},
	urldate = {2025-01-14},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	month = aug,
	year = {2010},
	pages = {5--28},
}

@article{majda_quantifying_2010,
	title = {Quantifying uncertainty in climate change science through empirical information theory},
	volume = {107},
	issn = {0027-8424},
	url = {https://www.jstor.org/stable/27862175},
	abstract = {Quantifying the uncertainty for the present climate and the predictions of climate change in the suite of imperfect Atmosphere Ocean Science (AOS) computer models is a central issue in climate change science. Here, a systematic approach to these issues with firm mathematical underpinning is developed through empirical information theory. An information metric to quantify AOS model errors in the climate is proposed here which incorporates both coarse-grained mean model errors as well as covariance ratios in a transformation invariant fashion. The subtle behavior of model errors with this information metric is quantified in an instructive statistically exactly solvable test model with direct relevance to climate change science including the prototype behavior of tracer gases such as CO $_{\textrm{2}}$ . Formulas for identifying the most sensitive climate change directions using statistics of the present climate or an AOS model approximation are developed here; these formulas just involve finding the eigenvector associated with the largest eigenvalue of a quadratic form computed through suitable unperturbed climate statistics. These climate change concepts are illustrated on a statistically exactly solvable one-dimensional stochastic model with relevance for low frequency variability of the atmosphere. Viable algorithms for implementation of these concepts are discussed throughout the paper.},
	number = {34},
	urldate = {2025-01-14},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Majda, Andrew J. and Gershgorin, Boris},
	year = {2010},
	pages = {14958--14963},
}

@article{forrester_recent_2009,
	title = {Recent advances in surrogate-based optimization},
	volume = {45},
	issn = {0376-0421},
	url = {https://www.sciencedirect.com/science/article/pii/S0376042108000766},
	doi = {10.1016/j.paerosci.2008.11.001},
	abstract = {The evaluation of aerospace designs is synonymous with the use of long running and computationally intensive simulations. This fuels the desire to harness the efficiency of surrogate-based methods in aerospace design optimization. Recent advances in surrogate-based design methodology bring the promise of efficient global optimization closer to reality. We review the present state of the art of constructing surrogate models and their use in optimization strategies. We make extensive use of pictorial examples and, since no method is truly universal, give guidance as to each method's strengths and weaknesses.},
	number = {1},
	urldate = {2025-01-13},
	journal = {Progress in Aerospace Sciences},
	author = {Forrester, Alexander I. J. and Keane, Andy J.},
	month = jan,
	year = {2009},
	pages = {50--79},
}

@article{fernandez-godino_review_2023,
	title = {Review of multi-fidelity models},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/acse.2023015},
	doi = {10.3934/acse.2023015},
	abstract = {Multi-fidelity models provide a framework for integrating computational models of varying complexity, allowing for accurate predictions while optimizing computational resources. These models are especially beneficial when acquiring high-accuracy data is costly or computationally intensive. This review offers a comprehensive analysis of multi-fidelity models, focusing on their applications in scientific and engineering fields, particularly in optimization and uncertainty quantification. It classifies publications on multi-fidelity modeling according to several criteria, including application area, surrogate model selection, types of fidelity, combination methods and year of publication. The study investigates techniques for combining different fidelity levels, with an emphasis on multi-fidelity surrogate models. This work discusses reproducibility, open-sourcing methodologies and benchmarking procedures to promote transparency. The manuscript also includes educational toy problems to enhance understanding. Additionally, this paper outlines best practices for presenting multi-fidelity-related savings in a standardized, succinct and yet thorough manner. The review concludes by examining current trends in multi-fidelity modeling, including emerging techniques, recent advancements, and promising research directions.},
	language = {en},
	number = {4},
	urldate = {2025-01-13},
	journal = {Advances in Computational Science and Engineering},
	author = {FernÃ¡ndez-Godino, M. Giselle},
	month = dec,
	year = {2023},
	pages = {351--400},
}

@article{west_multifidelity_2020,
	title = {Multifidelity {Uncertainty} {Quantification} of a {Commercial} {Supersonic} {Transport}},
	volume = {57},
	issn = {0021-8669},
	url = {https://arc.aiaa.org/doi/10.2514/1.C035496},
	doi = {10.2514/1.C035496},
	abstract = {The objective of this work was to develop a multifidelity uncertainty quantification approach for efficient analysis of a commercial supersonic transport concept. An approach based on point-collocation, nonintrusive polynomial chaos was formulated in which a low-fidelity model could be corrected using multiple higher-fidelity models. The formulation and methodology also allows for the addition of uncertainty sources not present in the lower-fidelity models. To demonstrate the applicability and potential computational savings of the multifidelity polynomial chaos approach, two model problems were explored. The first was a supersonic airfoil with three levels of modeling fidelity, each capturing a gradual increase in modeling of the underlying flow physics. As much as 50\% computational cost reduction was observed using the multifidelity approach, while predicting nearly the same amount of uncertainty in drag. The second problem was a commercial supersonic transport. This model had three levels of fidelity that included two different modeling approaches and the addition of physics between the fidelity levels. Results of this analysis yielded nearly a 70\% computational savings to predict a comparable amount of uncertainty in ground noise. Both problems illustrate the applicability and significant computational savings of the multifidelity method for efficient and accurate uncertainty quantification.},
	number = {3},
	urldate = {2025-01-12},
	journal = {Journal of Aircraft},
	author = {West, Thomas K. and Phillips, Ben D.},
	month = may,
	year = {2020},
	keywords = {Aerodynamic Shape Optimization, Angle of Attack, CFD Analysis, Collocation Method, Cost Reduction, Shock Wave Interaction, Supersonic Aircraft, Supersonic Airfoils, Supersonic Transports, Turbulence Models},
	pages = {491--500},
}

@incollection{oberkampf_mathematical_nodate,
	title = {Mathematical representation of uncertainty},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2001-1645},
	urldate = {2025-01-12},
	booktitle = {19th {AIAA} {Applied} {Aerodynamics} {Conference}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Oberkampf, William and Helton, Jon and Sentz, Kari},
	doi = {10.2514/6.2001-1645},
}

@misc{brehmer_simulation-based_2020,
	title = {Simulation-based inference methods for particle physics},
	url = {http://arxiv.org/abs/2010.06439},
	doi = {10.48550/arXiv.2010.06439},
	abstract = {Our predictions for particle physics processes are realized in a chain of complex simulators. They allow us to generate high-fidelity simulated data, but they are not well-suited for inference on the theory parameters with observed data. We explain why the likelihood function of high-dimensional LHC data cannot be explicitly evaluated, why this matters for data analysis, and reframe what the field has traditionally done to circumvent this problem. We then review new simulation-based inference methods that let us directly analyze high-dimensional data by combining machine learning techniques and information from the simulator. Initial studies indicate that these techniques have the potential to substantially improve the precision of LHC measurements. Finally, we discuss probabilistic programming, an emerging paradigm that lets us extend inference to the latent process of the simulator.},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Brehmer, Johann and Cranmer, Kyle},
	month = nov,
	year = {2020},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@article{zenke_limits_2014,
	title = {Limits to high-speed simulations of spiking neural networks using general-purpose computers},
	volume = {8},
	issn = {1662-5196},
	doi = {10.3389/fninf.2014.00076},
	abstract = {To understand how the central nervous system performs computations using recurrent neuronal circuitry, simulations have become an indispensable tool for theoretical neuroscience. To study neuronal circuits and their ability to self-organize, increasing attention has been directed toward synaptic plasticity. In particular spike-timing-dependent plasticity (STDP) creates specific demands for simulations of spiking neural networks. On the one hand a high temporal resolution is required to capture the millisecond timescale of typical STDP windows. On the other hand network simulations have to evolve over hours up to days, to capture the timescale of long-term plasticity. To do this efficiently, fast simulation speed is the crucial ingredient rather than large neuron numbers. Using different medium-sized network models consisting of several thousands of neurons and off-the-shelf hardware, we compare the simulation speed of the simulators: Brian, NEST and Neuron as well as our own simulator Auryn. Our results show that real-time simulations of different plastic network models are possible in parallel simulations in which numerical precision is not a primary concern. Even so, the speed-up margin of parallelism is limited and boosting simulation speeds beyond one tenth of real-time is difficult. By profiling simulation code we show that the run times of typical plastic network simulations encounter a hard boundary. This limit is partly due to latencies in the inter-process communications and thus cannot be overcome by increased parallelism. Overall, these results show that to study plasticity in medium-sized spiking neural networks, adequate simulation tools are readily available which run efficiently on small clusters. However, to run simulations substantially faster than real-time, special hardware is a prerequisite.},
	language = {eng},
	journal = {Frontiers in Neuroinformatics},
	author = {Zenke, Friedemann and Gerstner, Wulfram},
	year = {2014},
	pmid = {25309418},
	pmcid = {PMC4160969},
	keywords = {STDP, computational neuroscience, network simulator, parallel computing, spiking neural networks, synaptic plasticity},
	pages = {76},
}

@inproceedings{ribani_survey_2019,
	title = {A {Survey} of {Transfer} {Learning} for {Convolutional} {Neural} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8920338},
	doi = {10.1109/SIBGRAPI-T.2019.00010},
	abstract = {Transfer learning is an emerging topic that may drive the success of machine learning in research and industry. The lack of data on specific tasks is one of the main reasons to use it, since collecting and labeling data can be very expensive and can take time, and recent concerns with privacy make difficult to use real data from users. The use of transfer learning helps to fast prototype new machine learning models using pre-trained models from a source task since training on millions of images can take time and requires expensive GPUs. In this survey, we review the concepts and definitions related to transfer learning and we list the different terms used in the literature. We bring the point of view from different authors of prior surveys, adding some more recent findings in order to give a clear vision of directions for future work in this field of research.},
	urldate = {2025-01-02},
	booktitle = {2019 32nd {SIBGRAPI} {Conference} on {Graphics}, {Patterns} and {Images} {Tutorials} ({SIBGRAPI}-{T})},
	author = {Ribani, Ricardo and Marengoni, Mauricio},
	month = oct,
	year = {2019},
	keywords = {Convolutional Neural Networks, Data models, Deep Learning, Machine learning, Probability distribution, Supervised learning, Task analysis, Training, Transfer Learning},
	pages = {47--57},
}

@article{vogels_inhibitory_2011,
	title = {Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks},
	volume = {334},
	issn = {1095-9203},
	doi = {10.1126/science.1211095},
	abstract = {Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
	language = {eng},
	number = {6062},
	journal = {Science (New York, N.Y.)},
	author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
	month = dec,
	year = {2011},
	pmid = {22075724},
	keywords = {Afferent Pathways, Memory, Models, Neurological, Nerve Net, Neural Inhibition, Neuronal Plasticity, Neurons, Synaptic Transmission},
	pages = {1569--1573},
}

@book{gerstner_neuronal_2014,
	address = {Cambridge},
	title = {Neuronal {Dynamics}: {From} {Single} {Neurons} to {Networks} and {Models} of {Cognition}},
	isbn = {978-1-107-06083-8},
	shorttitle = {Neuronal {Dynamics}},
	url = {https://www.cambridge.org/core/books/neuronal-dynamics/75375090046733765596191E23B2959D},
	abstract = {What happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code? This textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the fields of computational and theoretical neuroscience. It covers classical topics, including the HodgkinâHuxley equations and Hopfield model, as well as modern developments in the field such as generalized linear models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and are richly illustrated by figures and worked-out examples. End-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.},
	urldate = {2025-01-01},
	publisher = {Cambridge University Press},
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	year = {2014},
	doi = {10.1017/CBO9781107447615},
}

@article{gerstner_mathematical_2002,
	title = {Mathematical formulations of {Hebbian} learning},
	volume = {87},
	issn = {1432-0770},
	url = {https://doi.org/10.1007/s00422-002-0353-y},
	doi = {10.1007/s00422-002-0353-y},
	abstract = {Several formulations of correlation-based Hebbian learning are reviewed. On the presynaptic side, activity is described either by a firing rate or by presynaptic spike arrival. The state of the postsynaptic neuron can be described by its membrane potential, its firing rate, or the timing of backpropagating action potentials (BPAPs). It is shown that all of the above formulations can be derived from the point of view of an expansion. In the absence of BPAPs, it is natural to correlate presynaptic spikes with the postsynaptic membrane potential. Time windows of spike-time-dependent plasticity arise naturally if the timing of postsynaptic spikes is available at the site of the synapse, as is the case in the presence of BPAPs. With an appropriate choice of parameters, Hebbian synaptic plasticity has intrinsic normalization properties that stabilizes postsynaptic firing rates and leads to subtractive weight normalization.},
	language = {en},
	number = {5},
	urldate = {2025-01-01},
	journal = {Biological Cybernetics},
	author = {Gerstner, Wulfram and Kistler, Werner M.},
	month = dec,
	year = {2002},
	keywords = {Firing Rate, Mathematical Formulation, Membrane Potential, Synaptic Plasticity, Time Window},
	pages = {404--415},
}

@article{zenke_diverse_2015,
	title = {Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks},
	volume = {6},
	copyright = {2015 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms7922},
	doi = {10.1038/ncomms7922},
	abstract = {Synaptic plasticity, the putative basis of learning and memory formation, manifests in various forms and across different timescales. Here we show that the interaction of Hebbian homosynaptic plasticity with rapid non-Hebbian heterosynaptic plasticity is, when complemented with slower homeostatic changes and consolidation, sufficient for assembly formation and memory recall in a spiking recurrent network model of excitatory and inhibitory neurons. In the model, assemblies were formed during repeated sensory stimulation and characterized by strong recurrent excitatory connections. Even days after formation, and despite ongoing network activity and synaptic plasticity, memories could be recalled through selective delay activity following the brief stimulation of a subset of assembly neurons. Blocking any component of plasticity prevented stable functioning as a memory network. Our modelling results suggest that the diversity of plasticity phenomena in the brain is orchestrated towards achieving common functional goals.},
	language = {en},
	number = {1},
	urldate = {2025-01-01},
	journal = {Nature Communications},
	author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
	month = apr,
	year = {2015},
	keywords = {Learning and memory, Neural circuits, Synaptic plasticity},
	pages = {6922},
}

@article{brette_what_2015,
	title = {What {Is} the {Most} {Realistic} {Single}-{Compartment} {Model} of {Spike} {Initiation}?},
	volume = {11},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4391789/},
	doi = {10.1371/journal.pcbi.1004114},
	abstract = {A large variety of neuron models are used in theoretical and computational neuroscience, and among these, single-compartment models are a popular kind. These models do not explicitly include the dendrites or the axon, and range from the Hodgkin-Huxley (HH) model to various flavors of integrate-and-fire (IF) models. The main classes of models differ in the way spikes are initiated. Which one is the most realistic? Starting with some general epistemological considerations, I show that the notion of realism comes in two dimensions: empirical content (the sort of predictions that a model can produce) and empirical accuracy (whether these predictions are correct). I then examine the realism of the main classes of single-compartment models along these two dimensions, in light of recent experimental evidence.},
	number = {4},
	urldate = {2024-12-30},
	journal = {PLoS Computational Biology},
	author = {Brette, Romain},
	month = apr,
	year = {2015},
	pmid = {25856629},
	pmcid = {PMC4391789},
	pages = {e1004114},
}

@article{kou_multiresolution_2012,
	title = {A {Multiresolution} {Method} for {Parameter} {Estimation} of {Diffusion} {Processes}},
	volume = {107},
	doi = {10.1080/01621459.2012.720899},
	abstract = {Diffusion process models are widely used in science, engineering and finance. Most diffusion processes are described by stochastic differential equations in continuous time. In practice, however, data is typically observed only at discrete time points. Except for a few very special cases, no analytic form exists for the likelihood of such discretely observed data. For this reason, parametric inference is often achieved by using discrete-time approximations, with accuracy controlled through the introduction of missing data. We present a new multiresolution Bayesian framework to address the inference difficulty. The methodology relies on the use of multiple approximations and extrapolation, and is significantly faster and more accurate than known strategies based on Gibbs sampling. We apply the multiresolution approach to three data-driven inference problems â one in biophysics and two in finance â one of which features a multivariate diffusion model with an entirely unobserved component.},
	journal = {Journal of The American Statistical Association - J AMER STATIST ASSN},
	author = {Kou, Supeng and Olding, Benjamin and Lysy, Martin and Liu, Jun},
	month = dec,
	year = {2012},
	keywords = {ornstein-uhlenbeck},
	pages = {4},
}

@article{zeng_multifidelity_2023,
	title = {Multifidelity uncertainty quantification with models based on dissimilar parameters},
	volume = {415},
	issn = {00457825},
	url = {http://arxiv.org/abs/2304.08644},
	doi = {10.1016/j.cma.2023.116205},
	abstract = {Multifidelity uncertainty quantification (MF UQ) sampling approaches have been shown to significantly reduce the variance of statistical estimators while preserving the bias of the highest-fidelity model, provided that the low-fidelity models are well correlated. However, maintaining a high level of correlation can be challenging, especially when models depend on different input uncertain parameters, which drastically reduces the correlation. Existing MF UQ approaches do not adequately address this issue. In this work, we propose a new sampling strategy that exploits a shared space to improve the correlation among models with dissimilar parametrization. We achieve this by transforming the original coordinates onto an auxiliary manifold using the adaptive basis (AB) method{\textasciitilde}{\textbackslash}cite\{Tipireddy2014\}. The AB method has two main benefits: (1) it provides an effective tool to identify the low-dimensional manifold on which each model can be represented, and (2) it enables easy transformation of polynomial chaos representations from high- to low-dimensional spaces. This latter feature is used to identify a shared manifold among models without requiring additional evaluations. We present two algorithmic flavors of the new estimator to cover different analysis scenarios, including those with legacy and non-legacy high-fidelity data. We provide numerical results for analytical examples, a direct field acoustic test, and a finite element model of a nuclear fuel assembly. For all examples, we compare the proposed strategy against both single-fidelity and MF estimators based on the original model parametrization.},
	urldate = {2024-12-17},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Zeng, Xiaoshu and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D. and Gorodetsky, Alex A. and Ghanem, Roger},
	month = oct,
	year = {2023},
	keywords = {Physics - Data Analysis, Statistics and Probability},
	pages = {116205},
}

@article{tsilifis_multifidelity_2022,
	title = {Multifidelity {Model} {Calibration} in {Structural} {Dynamics} {Using} {Stochastic} {Variational} {Inference} on {Manifolds}},
	volume = {24},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/24/9/1291},
	doi = {10.3390/e24091291},
	abstract = {Bayesian techniques for engineering problems, which rely on Gaussian process (GP) regression, are known for their ability to quantify epistemic and aleatory uncertainties and for being data efficient. The mathematical elegance of applying these methods usually comes at a high computational cost when compared to deterministic and empirical Bayesian methods. Furthermore, using these methods becomes practically infeasible in scenarios characterized by a large number of inputs and thousands of training data. The focus of this work is on enhancing Gaussian process based metamodeling and model calibration tasks, when the size of the training datasets is significantly large. To achieve this goal, we employ a stochastic variational inference algorithm that enables rapid statistical learning of the calibration parameters and hyperparameter tuning, while retaining the rigor of Bayesian inference. The numerical performance of the algorithm is demonstrated on multiple metamodeling and model calibration problems with thousands of training data.},
	language = {en},
	number = {9},
	urldate = {2024-12-17},
	journal = {Entropy},
	author = {Tsilifis, Panagiotis and Pandita, Piyush and Ghosh, Sayan and Wang, Liping},
	month = sep,
	year = {2022},
	keywords = {Gaussian processes, manifold gradient ascent, multifidelity modeling, stochastic variational inference, structural dynamics, vibration torsion},
	pages = {1291},
}

@article{nobile_multi_2015,
	title = {A {Multi} {Level} {Monte} {Carlo} method with control variate for elliptic {PDEs} with log-normal coefficients},
	volume = {3},
	issn = {2194-041X},
	url = {https://doi.org/10.1007/s40072-015-0055-9},
	doi = {10.1007/s40072-015-0055-9},
	abstract = {We consider the numerical approximation of the stochastic Darcy problem with log-normal permeability field and propose a novel Multi Level Monte Carlo (MLMC) approach with a control variate variance reduction technique on each level. We model the log-permeability as a stationary Gaussian random field with a covariance function belonging to the so called MatÃ©rn family, which includes both fields with very limited and very high spatial regularity. The control variate is obtained starting from the solution of an auxiliary problem with smoothed permeability coefficient and its expected value is effectively computed with a Stochastic Collocation method on the finest level in which the control variate is applied. We analyze the variance reduction induced by the control variate, and the total mean square error of the new estimator. To conclude we present some numerical examples and a comparison with the standard MLMC method, which shows the effectiveness of the proposed method.},
	language = {en},
	number = {3},
	urldate = {2024-12-17},
	journal = {Stochastic Partial Differential Equations: Analysis and Computations},
	author = {Nobile, Fabio and Tesei, Francesco},
	month = sep,
	year = {2015},
	keywords = {35R60, 60H35, 65C05, 65N15, 65N30, Control variate, Log-normal random-fields, MatÃ©rn covariance, Multi Level Monte Carlo, Stochastic Collocation, Stochastic Darcy Problem},
	pages = {398--444},
}

@article{giles_multilevel_2008,
	title = {Multilevel {Monte} {Carlo} {Path} {Simulation}},
	volume = {56},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0496},
	doi = {10.1287/opre.1070.0496},
	abstract = {We show that multigrid ideas can be used to reduce the computational complexity of estimating an expected value arising from a stochastic differential equation using Monte Carlo path simulations. In the simplest case of a Lipschitz payoff and a Euler discretisation, the computational cost to achieve an accuracy of O(Ïµ) is reduced from O(Ïµâ3) to O(Ïµâ2 (log Ïµ)2). The analysis is supported by numerical results showing significant computational savings.},
	number = {3},
	urldate = {2024-12-17},
	journal = {Operations Research},
	author = {Giles, Michael B.},
	month = jun,
	year = {2008},
	keywords = {analysis of algorithms, computational complexity, efficiency, finance, simulation},
	pages = {607--617},
}

@incollection{geraci_multifidelity_nodate,
	title = {A multifidelity multilevel {Monte} {Carlo} method for uncertainty propagation in aerospace applications},
	url = {https://arc.aiaa.org/doi/abs/10.2514/6.2017-1951},
	urldate = {2024-12-17},
	booktitle = {19th {AIAA} {Non}-{Deterministic} {Approaches} {Conference}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Geraci, Gianluca and Eldred, Michael S. and Iaccarino, Gianluca},
	doi = {10.2514/6.2017-1951},
}

@article{peherstorfer_optimal_2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/15M1046472},
	doi = {10.1137/15M1046472},
	abstract = {Variance-based sensitivity analysis provides a quantitative measure of how uncertainty in a model input contributes to uncertainty in the model output. Such sensitivity analyses arise in a wide variety of applications and are typically computed using Monte Carlo estimation, but the many samples required for Monte Carlo to be sufficiently accurate can make these analyses intractable when the model is expensive. This work presents a multifidelity approach for estimating sensitivity indices that leverages cheaper low-fidelity models to reduce the cost of sensitivity analysis while retaining accuracy guarantees via recourse to the original, expensive model. This paper develops new multifidelity estimators for variance and for the Sobol' main and total effect sensitivity indices. We discuss strategies for dividing limited computational resources among models and specify a recommended strategy. Results are presented for the Ishigami function and a convection-diffusion-reaction model that demonstrate up to \$10{\textbackslash}times\$ speedups for fixed convergence levels. For the problems tested, the multifidelity approach allows inputs to be definitively ranked in importance when Monte Carlo alone fails to do so.},
	number = {5},
	urldate = {2024-12-17},
	journal = {SIAM Journal on Scientific Computing},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	month = jan,
	year = {2016},
	pages = {A3163--A3194},
}

@inproceedings{durkan_contrastive_2020,
	title = {On {Contrastive} {Learning} for {Likelihood}-free {Inference}},
	url = {https://proceedings.mlr.press/v119/durkan20a.html},
	abstract = {Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classifier to distinguish between pairs of parameter-observation samples generated using the simulator and pairs sampled from some reference distribution, which implicitly learns a density ratio proportional to the likelihood. Another popular class of methods fits a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of flexible neural density estimators for this task. In this work, we show that both of these approaches can be unified under a general contrastive learning scheme, and clarify how they should be run and compared.},
	language = {en},
	urldate = {2024-12-10},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Durkan, Conor and Murray, Iain and Papamakarios, George},
	month = nov,
	year = {2020},
	pages = {2771--2781},
}

@inproceedings{greenberg_automatic_2019,
	title = {Automatic {Posterior} {Transformation} for {Likelihood}-{Free} {Inference}},
	url = {https://proceedings.mlr.press/v97/greenberg19a.html},
	abstract = {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.},
	language = {en},
	urldate = {2024-12-10},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
	month = may,
	year = {2019},
	pages = {2404--2414},
}

@inproceedings{papamakarios_fast_2016,
	title = {Fast {\textbackslash}epsilon -free {Inference} of {Simulation} {Models} with {Bayesian} {Conditional} {Density} {Estimation}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/6aca97005c68f1206823815f66102863-Abstract.html},
	abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an Îµ-ball around the observed data, which is only correct in the limit Îµâ0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as Îµâ0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
	urldate = {2024-12-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Papamakarios, George and Murray, Iain},
	year = {2016},
}

@article{peherstorfer_survey_2018,
	title = {Survey of {Multifidelity} {Methods} in {Uncertainty} {Propagation}, {Inference}, and {Optimization}},
	volume = {60},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/16M1082469},
	doi = {10.1137/16M1082469},
	abstract = {This work presents an optimal model management strategy that exploits multifidelity surrogate models to accelerate the estimation of statistics of outputs of computationally expensive high-fidelity models. Existing acceleration methods typically exploit a multilevel hierarchy of surrogate models that follow a known rate of error decay and computational costs; however, a general collection of surrogate models, which may include projection-based reduced models, data-fit models, support vector machines, and simplified-physics models, does not necessarily give rise to such a hierarchy. Our multifidelity approach provides a framework to combine an arbitrary number of surrogate models of any type. Instead of relying on error and cost rates, an optimization problem balances the number of model evaluations across the high-fidelity and surrogate models with respect to error and costs. We show that a unique analytic solution of the model management optimization problem exists under mild conditions on the models. Our multifidelity method makes occasional recourse to the high-fidelity model; in doing so it provides an unbiased estimator of the statistics of the high-fidelity model, even in the absence of error bounds and error estimators for the surrogate models. Numerical experiments with linear and nonlinear examples show that speedups by orders of magnitude are obtained compared to Monte Carlo estimation that invokes a single model only.},
	number = {3},
	urldate = {2024-02-11},
	journal = {SIAM Review},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	month = jan,
	year = {2018},
	keywords = {Multifidelity},
	pages = {550--591},
}

@article{confavreux_meta-learning_2023,
	title = {Meta-learning families of plasticity rules in recurrent spiking networks using simulation-based inference},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/2bdc2267c3d7d01523e2e17ac0a754f3-Abstract-Conference.html},
	language = {en},
	urldate = {2024-12-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Confavreux, Basile and Ramesh, Poornima and Goncalves, Pedro J. and Macke, Jakob H. and Vogels, Tim},
	month = dec,
	year = {2023},
	pages = {13545--13558},
}

@misc{roset_zuko_2024,
	title = {Zuko - {Normalizing} flows in {PyTorch}},
	copyright = {MIT},
	url = {https://github.com/probabilists/zuko},
	abstract = {Zuko - Normalizing flows in PyTorch},
	urldate = {2024-12-05},
	publisher = {The Probabilists},
	author = {Roset, Francois},
	month = nov,
	year = {2024},
	keywords = {deep-learning, density-estimation, generative-model, normalizing-flows, probability, torch},
}

@inproceedings{gloeckler_adversarial_2023,
	title = {Adversarial robustness of amortized {Bayesian} inference},
	url = {https://proceedings.mlr.press/v202/gloeckler23a.html},
	abstract = {Bayesian inference usually requires running potentially costly inference procedures separately for every new observation. In contrast, the idea of amortized Bayesian inference is to initially invest computational cost in training an inference network on simulated data, which can subsequently be used to rapidly perform inference (i.e., to return estimates of posterior distributions) for new observations. This approach has been applied to many real-world models in the sciences and engineering, but it is unclear how robust the approach is to adversarial perturbations in the observed data. Here, we study the adversarial robustness of amortized Bayesian inference, focusing on simulation-based estimation of multi-dimensional posterior distributions. We show that almost unrecognizable, targeted perturbations of the observations can lead to drastic changes in the predicted posterior and highly unrealistic posterior predictive samples, across several benchmark tasks and a real-world example from neuroscience. We propose a computationally efficient regularization scheme based on penalizing the Fisher information of the conditional density estimator, and show how it improves the adversarial robustness of amortized Bayesian inference.},
	language = {en},
	urldate = {2024-12-02},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Gloeckler, Manuel and Deistler, Michael and Macke, Jakob H.},
	month = jul,
	year = {2023},
	pages = {11493--11524},
}

@inproceedings{geffner_compositional_2023,
	title = {Compositional {Score} {Modeling} for {Simulation}-{Based} {Inference}},
	url = {https://proceedings.mlr.press/v202/geffner23a.html},
	abstract = {Neural Posterior Estimation methods for simulation-based inference can be ill-suited for dealing with posterior distributions obtained by conditioning on multiple observations, as they tend to require a large number of simulator calls to learn accurate approximations. In contrast, Neural Likelihood Estimation methods can handle multiple observations at inference time after learning from individual observations, but they rely on standard inference methods, such as MCMC or variational inference, which come with certain performance drawbacks. We introduce a new method based on conditional score modeling that enjoys the benefits of both approaches. We model the scores of the (diffused) posterior distributions induced by individual observations, and introduce a way of combining the learned scores to approximately sample from the target posterior distribution. Our approach is sample-efficient, can naturally aggregate multiple observations at inference time, and avoids the drawbacks of standard inference methods.},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Geffner, Tomas and Papamakarios, George and Mnih, Andriy},
	month = jul,
	year = {2023},
	pages = {11098--11116},
}

@inproceedings{papamakarios_fast_2018,
	title = {Fast \$Îµ\$-free {Inference} of {Simulation} {Models} with {Bayesian} {Conditional} {Density} {Estimation}},
	url = {http://arxiv.org/abs/1605.06376},
	abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an \${\textbackslash}epsilon\$-ball around the observed data, which is only correct in the limit \${\textbackslash}epsilon{\textbackslash}!{\textbackslash}rightarrow{\textbackslash}!0\$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as \${\textbackslash}epsilon{\textbackslash}!{\textbackslash}rightarrow{\textbackslash}!0\$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
	urldate = {2024-11-18},
	author = {Papamakarios, George and Murray, Iain},
	month = apr,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@misc{linhart_l-c2st_2023,
	title = {L-{C2ST}: {Local} {Diagnostics} for {Posterior} {Approximations} in {Simulation}-{Based} {Inference}},
	shorttitle = {L-{C2ST}},
	url = {http://arxiv.org/abs/2306.03580},
	abstract = {Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret -- e.g. graphical -- diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficient. On standard SBI benchmarks, L-C2ST provides comparable results to C2ST and outperforms alternative local approaches such as coverage tests based on highest predictive density (HPD). We further highlight the importance of local evaluation and the benefit of interpretability of L-C2ST on a challenging application from computational neuroscience.},
	urldate = {2024-10-24},
	publisher = {arXiv},
	author = {Linhart, Julia and Gramfort, Alexandre and Rodrigues, Pedro L. C.},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
}

@inproceedings{durkan_neural_2019,
	title = {Neural {Spline} {Flows}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/7ac71d433f282034e088473244df8c02-Abstract.html},
	abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
	urldate = {2024-10-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
	year = {2019},
}

@misc{creamer_bridging_2024,
	title = {Bridging the gap between the connectome and whole-brain activity in {C}. elegans},
	copyright = {Â© 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.09.22.614271v1},
	doi = {10.1101/2024.09.22.614271},
	abstract = {A fundamental goal of neuroscience is to understand how anatomy determines the functional properties of the nervous system. However, it has been challenging to relate large-scale functional measurements of the C. elegans nervous system to the wormâs known anatomical connectome1â3. Here, we address this apparent discrepancy using a connectome-constrained model of the nematode brain fit to neural recordings with optogenetic perturbations2. Our model consists of a noisy linear dynamical system with a sparse synaptic weight matrix with non-zero entries only where there are synapses in the C. elegans connectome. We evaluated the model by perturbing neurons in silico and measuring the perturbation response of all other neurons in the network. We compared these responses to those measured in held-out animals and found that this model captured the perturbation-triggered responses of individual neurons 92\% as well as the reproducibility of the perturbation responses themselves. This includes perturbation responses of neurons that were not anatomically connected, which the model explains in terms of signal propagation over multiple neurons. In addition to capturing perturbation responses, the model also accurately predicts the activity of held-out neurons using the observed activity of other neurons. Strikingly, alternative models with equivalent levels of sparsity but a shuffled connectome constraint achieved much lower performance. Finally, we demonstrate that adding connections beyond those found in the connectome did not improve the modelâs prediction of the perturbation measurements. The model described here provides the strongest link yet between the connectivity of the C. elegans nervous system and its causal and correlative functional properties.},
	language = {en},
	urldate = {2024-10-21},
	publisher = {bioRxiv},
	author = {Creamer, Matthew S. and Leifer, Andrew M. and Pillow, Jonathan W.},
	month = sep,
	year = {2024},
}

@article{ascoli_neuromorphoorg_2007,
	title = {{NeuroMorpho}.{Org}: {A} {Central} {Resource} for {Neuronal} {Morphologies}},
	volume = {27},
	copyright = {Copyright Â© 2007 Society for Neuroscience 0270-6474/07/279247-05\$15.00/0},
	issn = {0270-6474, 1529-2401},
	shorttitle = {{NeuroMorpho}.{Org}},
	url = {https://www.jneurosci.org/content/27/35/9247},
	doi = {10.1523/JNEUROSCI.2055-07.2007},
	abstract = {The structure of dendrites and axons plays fundamental roles in synaptic integration and network connectivity. Synergistic advances in neurobiology (e.g., intracellular injections, fluorescent protein expression), microscopy (e.g., multiphoton laser scanning, computer controllers), and imaging},
	language = {en},
	number = {35},
	urldate = {2024-10-18},
	journal = {Journal of Neuroscience},
	author = {Ascoli, Giorgio A. and Donohue, Duncan E. and Halavi, Maryam},
	month = aug,
	year = {2007},
	pmid = {17728438},
	pages = {9247--9251},
}

@article{pyapali_dendritic_1998,
	title = {Dendritic properties of hippocampal {CA1} pyramidal neurons in the rat: {Intracellular} staining in vivo and in vitro},
	volume = {391},
	copyright = {Copyright Â© 1998 Wiley-Liss, Inc.},
	issn = {1096-9861},
	shorttitle = {Dendritic properties of hippocampal {CA1} pyramidal neurons in the rat},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-9861%2819980216%29391%3A3%3C335%3A%3AAID-CNE4%3E3.0.CO%3B2-2},
	doi = {10.1002/(SICI)1096-9861(19980216)391:3<335::AID-CNE4>3.0.CO;2-2},
	abstract = {Dendritic morphology and passive cable properties determine many aspects of synaptic integration in complex neurons, together with voltage-dependent membrane conductances. We investigated dendritic properties of CA1 pyramidal neurons intracellularly labeled during in vivo and in vitro physiologic recordings, by using similar intracellular staining and three-dimensional reconstruction techniques. Total dendritic length of the in vivo neurons was similar to that of the in vitro cells. After correction for shrinkage, cell extent in three-dimensional representation was not different between the two groups. Both in vivo and in vitro neurons demonstrated a variable degree of symmetry, with some neurons showing more cylindrical symmetry around the main apical axis, whereas other neurons were more elliptical, with the variation likely due to preparation and preservation conditions. Branch order analysis revealed no difference in the number of branch orders or dendritic complexity. Passive conduction of dendritic signals to the soma in these neurons shows considerable attenuation, particularly with higher frequency signals (such as synaptic potentials compared with steady-state signals), despite a relatively short electrotonic length. Essential aspects of morphometric appearance and complex dendritic integration critical to CA1 pyramidal cell functioning are preserved across neurons defined from the two different hippocampal preparations used in this study. J. Comp. Neurol. 391:335â352, 1998. Â© 1998 Wiley-Liss, Inc.},
	language = {en},
	number = {3},
	urldate = {2024-10-18},
	journal = {Journal of Comparative Neurology},
	author = {Pyapali, G.k. and Sik, A. and Penttonen, M. and Buzsaki, G. and Turner, D.a.},
	year = {1998},
	keywords = {CA1 pyramidal cells, dendritic function., electrotonic modeling, neuronal reconstructions, synaptic integration},
	pages = {335--352},
}

@misc{tahir_features_2024,
	title = {Features are fate: a theory of transfer learning in high-dimensional regression},
	shorttitle = {Features are fate},
	url = {http://arxiv.org/abs/2410.08194},
	abstract = {With the emergence of large-scale pre-trained neural networks, methods to adapt such "foundation" models to data-limited downstream tasks have become a necessity. Fine-tuning, preference optimization, and transfer learning have all been successfully employed for these purposes when the target task closely resembles the source task, but a precise theoretical understanding of "task similarity" is still lacking. While conventional wisdom suggests that simple measures of similarity between source and target distributions, such as \${\textbackslash}phi\$-divergences or integral probability metrics, can directly predict the success of transfer, we prove the surprising fact that, in general, this is not the case. We adopt, instead, a feature-centric viewpoint on transfer learning and establish a number of theoretical results that demonstrate that when the target task is well represented by the feature space of the pre-trained model, transfer learning outperforms training from scratch. We study deep linear networks as a minimal model of transfer learning in which we can analytically characterize the transferability phase diagram as a function of the target dataset size and the feature space overlap. For this model, we establish rigorously that when the feature space overlap between the source and target tasks is sufficiently strong, both linear transfer and fine-tuning improve performance, especially in the low data limit. These results build on an emerging understanding of feature learning dynamics in deep linear networks, and we demonstrate numerically that the rigorous results we derive for the linear case also apply to nonlinear networks.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Tahir, Javan and Ganguli, Surya and Rotskoff, Grant M.},
	month = oct,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{huys_efficient_2006,
	title = {Efficient {Estimation} of {Detailed} {Single}-{Neuron} {Models}},
	volume = {96},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.00079.2006},
	doi = {10.1152/jn.00079.2006},
	abstract = {Biophysically accurate multicompartmental models of individual neurons have significantly advanced our understanding of the inputâoutput function of single cells. These models depend on a large number of parameters that are difficult to estimate. In practice, they are often hand-tuned to match measured physiological behaviors, thus raising questions of identifiability and interpretability. We propose a statistical approach to the automatic estimation of various biologically relevant parameters, including 1) the distribution of channel densities, 2) the spatiotemporal pattern of synaptic input, and 3) axial resistances across extended dendrites. Recent experimental advances, notably in voltage-sensitive imaging, motivate us to assume access to: i) the spatiotemporal voltage signal in the dendrite and ii) an approximate description of the channel kinetics of interest. We show here that, given i and ii, parameters 1â3 can be inferred simultaneously by nonnegative linear regression; that this optimization problem possesses a unique solution and is guaranteed to converge despite the large number of parameters and their complex nonlinear interaction; and that standard optimization algorithms efficiently reach this optimum with modest computational and data requirements. We demonstrate that the method leads to accurate estimations on a wide variety of challenging model data sets that include up to about 104 parameters (roughly two orders of magnitude more than previously feasible) and describe how the method gives insights into the functional interaction of groups of channels.},
	number = {2},
	urldate = {2024-09-30},
	journal = {Journal of Neurophysiology},
	author = {Huys, Quentin J. M. and Ahrens, Misha B. and Paninski, Liam},
	month = aug,
	year = {2006},
	pages = {872--890},
}

@misc{menon_personalized_2024,
	title = {Personalized and uncertainty-aware coronary hemodynamics simulations: {From} {Bayesian} estimation to improved multi-fidelity uncertainty quantification},
	shorttitle = {Personalized and uncertainty-aware coronary hemodynamics simulations},
	url = {http://arxiv.org/abs/2409.02247},
	doi = {10.48550/arXiv.2409.02247},
	abstract = {Simulations of coronary hemodynamics have improved non-invasive clinical risk stratification and treatment outcomes for coronary artery disease, compared to relying on anatomical imaging alone. However, simulations typically use empirical approaches to distribute total coronary flow amongst the arteries in the coronary tree. This ignores patient variability, the presence of disease, and other clinical factors. Further, uncertainty in the clinical data often remains unaccounted for in the modeling pipeline. We present an end-to-end uncertainty-aware pipeline to (1) personalize coronary flow simulations by incorporating branch-specific coronary flows as well as cardiac function; and (2) predict clinical and biomechanical quantities of interest with improved precision, while accounting for uncertainty in the clinical data. We assimilate patient-specific measurements of myocardial blood flow from CT myocardial perfusion imaging to estimate branch-specific coronary flows. We use adaptive Markov Chain Monte Carlo sampling to estimate the joint posterior distributions of model parameters with simulated noise in the clinical data. Additionally, we determine the posterior predictive distribution for relevant quantities of interest using a new approach combining multi-fidelity Monte Carlo estimation with non-linear, data-driven dimensionality reduction. Our framework recapitulates clinically measured cardiac function as well as branch-specific coronary flows under measurement uncertainty. We substantially shrink the confidence intervals for estimated quantities of interest compared to single-fidelity and state-of-the-art multi-fidelity Monte Carlo methods. This is especially true for quantities that showed limited correlation between the low- and high-fidelity model predictions. Moreover, the proposed estimators are significantly cheaper to compute for a specified confidence level or variance.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Menon, Karthik and Zanoni, Andrea and Khan, Owais and Geraci, Gianluca and Nieman, Koen and Schiavazzi, Daniele E. and Marsden, Alison L.},
	month = sep,
	year = {2024},
	keywords = {Computer Science - Computational Engineering, Finance, and Science, Mathematics - Statistics Theory, Multifidelity, Physics - Computational Physics, Physics - Fluid Dynamics, Physics - Medical Physics},
}

@article{alicea_raising_2020,
	title = {Raising the {Connectome}: {The} {Emergence} of {Neuronal} {Activity} and {Behavior} in {Caenorhabditis} elegans},
	volume = {14},
	issn = {1662-5102},
	shorttitle = {Raising the {Connectome}},
	url = {https://www.frontiersin.org/articles/10.3389/fncel.2020.524791},
	abstract = {The differentiation of neurons and formation of connections between cells is the basis of both the adult phenotype and behaviors tied to cognition, perception, reproduction, and survival. Such behaviors are associated with local (circuits) and global (connectome) brain networks. A solid understanding of how these networks emerge is critical. This opinion piece features a guided tour of early developmental events in the emerging connectome, which is crucial to a new view on the connectogenetic process. Connectogenesis includes associating cell identities with broader functional and developmental relationships. During this process, the transition from developmental cells to terminally differentiated cells is defined by an accumulation of traits that ultimately results in neuronal-driven behavior. The well-characterized developmental and cell biology of Caenorhabditis elegans will be used to build a synthesis of developmental events that result in a functioning connectome. Specifically, our view of connectogenesis enables a first-mover model of synaptic connectivity to be demonstrated using data representing larval synaptogenesis. In a first-mover model of Stackelberg competition, potential pre- and postsynaptic relationships are shown to yield various strategies for establishing various types of synaptic connections. By comparing these results to what is known regarding principles for establishing complex network connectivity, these strategies are generalizable to other species and developmental systems. In conclusion, we will discuss the broader implications of this approach, as what is presented here informs an understanding of behavioral emergence and the ability to simulate related biological phenomena.},
	urldate = {2023-12-04},
	journal = {Frontiers in Cellular Neuroscience},
	author = {Alicea, Bradly},
	year = {2020},
	keywords = {C. elegans},
}

@article{ardiel_elegant_2010,
	title = {An elegant mind: {Learning} and memory in \textit{{Caenorhabditis} elegans}},
	volume = {17},
	issn = {1072-0502, 1549-5485},
	shorttitle = {An elegant mind},
	url = {http://learnmem.cshlp.org/lookup/doi/10.1101/lm.960510},
	doi = {10.1101/lm.960510},
	abstract = {This article reviews the literature on learning and memory in the soil-dwelling nematode
              Caenorhabditis elegans
              . Paradigms include nonassociative learning, associative learning, and imprinting, as worms have been shown to habituate to mechanical and chemical stimuli, as well as learn the smells, tastes, temperatures, and oxygen levels that predict aversive chemicals or the presence or absence of food. In each case, the neural circuit underlying the behavior has been at least partially described, and forward and reverse genetics are being used to elucidate the underlying cellular and molecular mechanisms. Several genes have been identified with no known role other than mediating behavior plasticity.},
	language = {en},
	number = {4},
	urldate = {2024-01-16},
	journal = {Learning \& Memory},
	author = {Ardiel, Evan L. and Rankin, Catharine H.},
	month = apr,
	year = {2010},
	keywords = {C. elegans},
	pages = {191--201},
}

@misc{haspel_time_2024,
	title = {The time is ripe to reverse engineer an entire nervous system: simulating behavior from neural interactions},
	shorttitle = {The time is ripe to reverse engineer an entire nervous system},
	url = {http://arxiv.org/abs/2308.06578},
	doi = {10.48550/arXiv.2308.06578},
	abstract = {Just like electrical engineers understand how microprocessors execute programs in terms of how transistor currents are affected by their inputs, neuroscientists want to understand behavior production in terms of how neuronal outputs are affected by their inputs and internal states. This dependency of neuronal outputs on inputs can be described by a state-dependent input-output (IO)-function. However, to reliably identify these IO-functions, we need to perturb each input and combination of inputs while observing all the outputs. Here, we argue that such completeness is possible in C. elegans; a complete description that goes all the way from the activity of every neuron to predict behavior. The established and growing toolkit of optophysiology can non-invasively capture and control every neuron's activity and scale to countless experiments. The information from many such experiments can be pooled while capturing the inter-individual variability because neuronal identity and function are largely conserved across individuals. Just like electrical engineers use transistor IO-functions to simulate program execution, we argue that neuronal IO-functions could be used to simulate the impressive breadth of brain states and behaviors of C. elegans.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Haspel, Gal and Baker, Ben and Beets, Isabel and Boyden, Edward S. and Brown, Jeffrey and Church, George and Cohen, Netta and Colon-Ramos, Daniel and Dyer, Eva and Fang-Yen, Christopher and Flavell, Steven and Goodman, Miriam B. and Hart, Anne C. and Izquierdo, Eduardo J. and Kagias, Konstantinos and Lockery, Shawn and Lu, Yangning and Marblestone, Adam and Matelsky, Jordan and Mensh, Brett and Pereira, Talmo D. and Pfister, Hanspeter and Rajan, Kanaka and Rotstein, Horacio G. and Scholz, Monika and Shaevitz, Joshua W. and Shlizerman, Eli and Simeon, Quilee and Skuhersky, Michael A. and Tiruvadi, Vineet and Venkatachalam, Vivek and Wei, Donglai and Wester, Brock and Yang, Guangyu Robert and Yemini, Eviatar and Zimmer, Manuel and Kording, Konrad P.},
	month = sep,
	year = {2024},
	keywords = {C. elegans, Quantitative Biology - Neurons and Cognition},
}

@article{doya_modulators_2008,
	title = {Modulators of decision making},
	volume = {11},
	copyright = {2008 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn2077},
	doi = {10.1038/nn2077},
	abstract = {Human and animal decisions are modulated by a variety of environmental and intrinsic contexts. Here I consider computational factors that can affect decision making and review anatomical structures and neurochemical systems that are related to contextual modulation of decision making. Expectation of a high reward can motivate a subject to go for an action despite a large cost, a decision that is influenced by dopamine in the anterior cingulate cortex. Uncertainty of action outcomes can promote risk taking and exploratory choices, in which norepinephrine and the orbitofrontal cortex appear to be involved. Predictable environments should facilitate consideration of longer-delayed rewards, which depends on serotonin in the dorsal striatum and dorsal prefrontal cortex. This article aims to sort out factors that affect the process of decision making from the viewpoint of reinforcement learning theory and to bridge between such computational needs and their neurophysiological substrates.},
	language = {en},
	number = {4},
	urldate = {2024-08-28},
	journal = {Nature Neuroscience},
	author = {Doya, Kenji},
	month = apr,
	year = {2008},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, Neurobiology, Neurosciences, general},
	pages = {410--416},
}

@article{soh_computational_2018,
	title = {A computational model of internal representations of chemical gradients in environments for chemotaxis of {Caenorhabditis} elegans},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-35157-1},
	doi = {10.1038/s41598-018-35157-1},
	abstract = {The small roundworm Caenorhabditis elegans employs two strategies, termed pirouette and weathervane, which are closely related to the internal representation of chemical gradients parallel and perpendicular to the travelling direction, respectively, to perform chemotaxis. These gradients must be calculated from the chemical information obtained at a single point, because the sensory neurons are located close to each other at the nose tip. To formulate the relationship between this sensory input and internal representations of the chemical gradient, this study proposes a simple computational model derived from the directional decomposition of the chemical concentration at the nose tip that can generate internal representations of the chemical gradient. The ability of the computational model was verified by using a chemotaxis simulator that can simulate the body motions of pirouette and weathervane, which confirmed that the computational model enables the conversion of the sensory input and head-bending angles into both types of gradients with high correlations of approximately râ{\textgreater}â0.90 (pâ{\textless}â0.01) with the true gradients. In addition, the chemotaxis index of the model was 0.64, which is slightly higher than that in the actual animal (0.57). In addition, simulation using a connectome-based neural network model confirmed that the proposed computational model is implementable in the actual network structure.},
	language = {en},
	number = {1},
	urldate = {2024-08-21},
	journal = {Scientific Reports},
	author = {Soh, Zu and Sakamoto, Kazuma and Suzuki, Michiyo and Iino, Yuichi and Tsuji, Toshio},
	month = nov,
	year = {2018},
	keywords = {Behavioural methods, Computational models, Network models},
	pages = {17190},
}

@misc{richter_building_2024,
	title = {Building a small brain with a simple stochastic generative model},
	copyright = {Â© 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2024.07.01.601562v1},
	doi = {10.1101/2024.07.01.601562},
	abstract = {The architectures of biological neural networks result from developmental processes shaped by genetically encoded rules, biophysical constraints, stochasticity, and learning. Understanding these processes is crucial for comprehending neural circuitsâ structure and function. The ability to reconstruct neural circuits, and even entire nervous systems, at the neuron and synapse level, facilitates the study of the design principles of neural systems and their developmental plan. Here, we investigate the developing connectome of C. elegans using statistical generative models based on simple biological features: neuronal cell type, neuron birth time, cell body distance, reciprocity, and synaptic pruning. Our models accurately predict synapse existence, degree profiles of individual neurons, and statistics of small network motifs. Importantly, these models require a surprisingly small number of neuronal cell types, which we infer and characterize. We further show that to replicate the experimentally-observed developmental path, multiple developmental epochs are necessary. Validation of our modelâs predictions of the synaptic connections using multiple reconstructions of adult worms suggests that our model identified the fundamental âbackboneâ of the connectivity graph. The accuracy of the generative statistical models we use here offers a general framework for studying how connectomes develop and the underlying principles of their design.},
	language = {en},
	urldate = {2024-08-20},
	publisher = {bioRxiv},
	author = {Richter, Oren and Schneidman, Elad},
	month = jul,
	year = {2024},
}

@misc{sorensen_probabilistic_2024,
	title = {A probabilistic framework for learning non-intrusive corrections to long-time climate simulations from short-time training data},
	url = {http://arxiv.org/abs/2408.02688},
	doi = {10.48550/arXiv.2408.02688},
	abstract = {Chaotic systems, such as turbulent flows, are ubiquitous in science and engineering. However, their study remains a challenge due to the large range scales, and the strong interaction with other, often not fully understood, physics. As a consequence, the spatiotemporal resolution required for accurate simulation of these systems is typically computationally infeasible, particularly for applications of long-term risk assessment, such as the quantification of extreme weather risk due to climate change. While data-driven modeling offers some promise of alleviating these obstacles, the scarcity of high-quality simulations results in limited available data to train such models, which is often compounded by the lack of stability for long-horizon simulations. As such, the computational, algorithmic, and data restrictions generally imply that the probability of rare extreme events is not accurately captured. In this work we present a general strategy for training neural network models to non-intrusively correct under-resolved long-time simulations of chaotic systems. The approach is based on training a post-processing correction operator on under-resolved simulations nudged towards a high-fidelity reference. This enables us to learn the dynamics of the underlying system directly, which allows us to use very little training data, even when the statistics thereof are far from converged. Additionally, through the use of probabilistic network architectures we are able to leverage the uncertainty due to the limited training data to further improve extrapolation capabilities. We apply our framework to severely under-resolved simulations of quasi-geostrophic flow and demonstrate its ability to accurately predict the anisotropic statistics over time horizons more than 30 times longer than the data seen in training.},
	urldate = {2024-08-20},
	publisher = {arXiv},
	author = {Sorensen, Benedikt Barthel and Zepeda-NÃºÃ±ez, Leonardo and Lopez-Gomez, Ignacio and Wan, Zhong Yi and Carver, Rob and Sha, Fei and Sapsis, Themistoklis},
	month = aug,
	year = {2024},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Physics - Atmospheric and Oceanic Physics, Physics - Fluid Dynamics},
}

@article{weilbach_graphically_nodate,
	title = {Graphically {Structured} {Diffusion} {Models}},
	abstract = {We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our modelâs performance, in terms of both training time and final accuracy. Our code can be found at https: //github.com/plai-group/gsdm.},
	language = {en},
	author = {Weilbach, Christian and Harvey, William and Wood, Frank},
}

@misc{gloeckler_all--one_2024,
	title = {All-in-one simulation-based inference},
	url = {http://arxiv.org/abs/2404.09636},
	doi = {10.48550/arXiv.2404.09636},
	abstract = {Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.},
	urldate = {2024-07-08},
	publisher = {arXiv},
	author = {Gloeckler, Manuel and Deistler, Michael and Weilbach, Christian and Wood, Frank and Macke, Jakob H.},
	month = may,
	year = {2024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{strey_estimation_2019,
	title = {Estimation of parameters from time traces originating from an {Ornstein}-{Uhlenbeck} process},
	volume = {100},
	issn = {2470-0045, 2470-0053},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.062142},
	doi = {10.1103/PhysRevE.100.062142},
	language = {en},
	number = {6},
	urldate = {2024-05-13},
	journal = {Physical Review E},
	author = {Strey, Helmut H.},
	month = dec,
	year = {2019},
	keywords = {ornstein-uhlenbeck},
	pages = {062142},
}

@misc{carter_parameter_2023,
	title = {Parameter estimation from an {Ornstein}-{Uhlenbeck} process with measurement noise},
	url = {http://arxiv.org/abs/2305.13498},
	abstract = {This article aims to investigate the impact of noise on parameter fitting for an Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and thermal noise on the accuracy of signal separation. To address these issues, we propose algorithms and methods that can effectively distinguish between thermal and multiplicative noise and improve the precision of parameter estimation for optimal data analysis. Specifically, we explore the impact of both multiplicative and thermal noise on the obfuscation of the actual signal and propose methods to resolve them. Firstly, we present an algorithm that can effectively separate thermal noise with comparable performance to Hamilton Monte Carlo (HMC) but with significantly improved speed. Subsequently, we analyze multiplicative noise and demonstrate that HMC is insufficient for isolating thermal and multiplicative noise. However, we show that, with additional knowledge of the ratio between thermal and multiplicative noise, we can accurately distinguish between the two types of noise when provided with a sufficiently large sampling rate or an amplitude of multiplicative noise smaller than thermal noise. This finding results in a situation that initially seems counterintuitive. When multiplicative noise dominates the noise spectrum, we can successfully estimate the parameters for such systems after adding additional white noise to shift the noise balance.},
	language = {en},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Carter, Simon and Strey, Helmut H.},
	month = aug,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning, ornstein-uhlenbeck},
}

@misc{hernandez-garcia_multi-fidelity_2023,
	title = {Multi-{Fidelity} {Active} {Learning} with {GFlowNets}},
	url = {http://arxiv.org/abs/2306.11715},
	doi = {10.48550/arXiv.2306.11715},
	abstract = {In the last decades, the capacity to generate large amounts of data in science and engineering applications has been growing steadily. Meanwhile, the progress in machine learning has turned it into a suitable tool to process and utilise the available data. Nonetheless, many relevant scientific and engineering problems present challenges where current machine learning methods cannot yet efficiently leverage the available data and resources. For example, in scientific discovery, we are often faced with the problem of exploring very large, high-dimensional spaces, where querying a high fidelity, black-box objective function is very expensive. Progress in machine learning methods that can efficiently tackle such problems would help accelerate currently crucial areas such as drug and materials discovery. In this paper, we propose the use of GFlowNets for multi-fidelity active learning, where multiple approximations of the black-box function are available at lower fidelity and cost. GFlowNets are recently proposed methods for amortised probabilistic inference that have proven efficient for exploring large, high-dimensional spaces and can hence be practical in the multi-fidelity setting too. Here, we describe our algorithm for multi-fidelity active learning with GFlowNets and evaluate its performance in both well-studied synthetic tasks and practically relevant applications of molecular discovery. Our results show that multi-fidelity active learning with GFlowNets can efficiently leverage the availability of multiple oracles with different costs and fidelities to accelerate scientific discovery and engineering design.},
	urldate = {2024-04-25},
	publisher = {arXiv},
	author = {Hernandez-Garcia, Alex and Saxena, Nikita and Jain, Moksh and Liu, Cheng-Hao and Bengio, Yoshua},
	month = jun,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
}

@article{dhulipala_active_2022,
	title = {Active learning with multifidelity modeling for efficient rare event simulation},
	volume = {468},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S002199912200568X},
	doi = {10.1016/j.jcp.2022.111506},
	abstract = {While multifidelity modeling provides a cost-effective way to conduct uncertainty quantification with computationally expensive models, much greater efficiency can be achieved by adaptively deciding the number of required high-fidelity (HF) simulations, depending on the type and complexity of the problem and the desired accuracy in the results. We propose a framework for active learning with multifidelity modeling emphasizing the efficient estimation of rare events. Our framework works by fusing a low-fidelity (LF) prediction with an HF-inferred correction, filtering the corrected LF prediction to decide whether to call the high-fidelity model, and for enhanced subsequent accuracy, adapting the correction for the LF prediction after every HF model call. The framework does not make any assumptions as to the LF model type or its correlations with the HF model. In addition, for improved robustness when estimating smaller failure probabilities, we propose using dynamic active learning functions that decide when to call the HF model. We demonstrate our framework using several academic case studies (including some high-dimensional problems) and two finite element model case studies: estimating Navier-Stokes velocities using the Stokes approximation and estimating stresses in a transversely isotropic model subjected to displacements via a coarsely meshed isotropic model. Across these case studies, not only did the proposed framework estimate the failure probabilities accurately, but compared with either Monte Carlo or a standard variance reduction method, it also required only a small fraction of the calls to the HF model.},
	urldate = {2024-04-25},
	journal = {Journal of Computational Physics},
	author = {Dhulipala, Somayajulu L. N. and Shields, Michael D. and Spencer, Benjamin W. and Bolisetti, Chandrakanth and Slaughter, Andrew E. and LabourÃ©, Vincent M. and Chakroborty, Promit},
	month = nov,
	year = {2022},
	keywords = {Active learning, Monte Carlo, Multifidelity modeling, Reliability, Uncertainty quantification, Variance reduction},
	pages = {111506},
}

@misc{rodrigues_learning_2020,
	title = {Learning summary features of time series for likelihood free inference},
	url = {http://arxiv.org/abs/2012.02807},
	abstract = {There has been an increasing interest from the scientific community in using likelihood-free inference (LFI) to determine which parameters of a given simulator model could best describe a set of experimental data. Despite exciting recent results and a wide range of possible applications, an important bottleneck of LFI when applied to time series data is the necessity of defining a set of summary features, often hand-tailored based on domain knowledge. In this work, we present a data-driven strategy for automatically learning summary features from univariate time series and apply it to signals generated from autoregressive-moving-average (ARMA) models and the Van der Pol Oscillator. Our results indicate that learning summary features from data can compete and even outperform LFI methods based on hand-crafted values such as autocorrelation coefficients even in the linear case.},
	urldate = {2024-03-28},
	publisher = {arXiv},
	author = {Rodrigues, Pedro L. C. and Gramfort, Alexandre},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
}

@article{seung_how_1996,
	title = {How the brain keeps the eyesâstill},
	volume = {93},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.93.23.13339},
	doi = {10.1073/pnas.93.23.13339},
	abstract = {The brain can hold the eyes still because it stores a memory of eye position. The brainâs memory of horizontal eye position appears to be represented by persistent neural activity in a network known as the neural integrator, which is localized in the brainstem and cerebellum. Existing experimental data are reinterpreted as evidence for an âattractor hypothesisâ that the persistent patterns of activity observed in this network form an attractive line of fixed points in its state space. Line attractor dynamics can be produced in linear or nonlinear neural networks by learning mechanisms that precisely tune positive feedback.},
	number = {23},
	urldate = {2024-02-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Seung, H.âS.},
	month = nov,
	year = {1996},
	pages = {13339--13344},
}

@article{watteyne_neuromedin_2020,
	title = {Neuromedin {U} signaling regulates retrieval of learned salt avoidance in a {C}. elegans gustatory circuit},
	volume = {11},
	issn = {2041-1723},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7190830/},
	doi = {10.1038/s41467-020-15964-9},
	abstract = {Learning and memory are regulated by neuromodulatory pathways, but the contribution and temporal requirement of most neuromodulators in a learning circuit are unknown. Here we identify the evolutionarily conserved neuromedin U (NMU) neuropeptide family as a regulator of C. elegans gustatory aversive learning. The NMU homolog CAPA-1 and its receptor NMUR-1 are required for the retrieval of learned salt avoidance. Gustatory aversive learning requires the release of CAPA-1 neuropeptides from sensory ASG neurons that respond to salt stimuli in an experience-dependent manner. Optogenetic silencing of CAPA-1 neurons blocks the expression, but not the acquisition, of learned salt avoidance. CAPA-1 signals through NMUR-1 in AFD sensory neurons to modulate two navigational strategies for salt chemotaxis. Aversive conditioning thus recruits NMU signaling to modulate locomotor programs for expressing learned avoidance behavior. Because NMU signaling is conserved across bilaterian animals, our findings incite further research into its function in other learning circuits., Learning and memory are regulated by neuropeptides. Here, the authors show that the neuropeptide CAPA-1 and its receptor NMUR-1 are required to retrieve learned salt avoidance in C. elegans. CAPA-1/NMUR-1 activation in AFD sensory neurons modulates locomotor programs to express learned avoidance.},
	urldate = {2024-02-29},
	journal = {Nature Communications},
	author = {Watteyne, Jan and Peymen, Katleen and Van der Auwera, Petrus and Borghgraef, Charline and Vandewyer, Elke and Van Damme, Sara and Rutten, Iene and Lammertyn, Jeroen and Jelier, Rob and Schoofs, Liliane and Beets, Isabel},
	month = apr,
	year = {2020},
	pmid = {32350283},
	pmcid = {PMC7190830},
	pages = {2076},
}

@misc{noauthor_how_nodate,
	title = {How the brain keeps the eyesâstill {\textbar} {PNAS}},
	url = {https://www.pnas.org/doi/10.1073/pnas.93.23.13339},
	urldate = {2024-02-28},
}

@article{oka_high_2013,
	title = {High salt recruits aversive taste pathways},
	volume = {494},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3587117/},
	doi = {10.1038/nature11905},
	abstract = {In the tongue, distinct classes of taste receptor cells detect the five basic tastes, sweet, sour, bitter, sodium salt, and umami,. Among these qualities, bitter and sour stimuli are innately aversive, whereas sweet and umami are appetitive, and generally attractive to animals. In contrast, salty taste is unique in that increasing salt concentration fundamentally transforms an innately appetitive stimulus into a powerfully aversive oneâ. This appetitive-aversive balance helps maintain appropriate salt consumption,,,, and represents an important part of fluid and electrolyte homeostasis. We have previously shown that the appetitive responses to NaCl are mediated by taste receptor cells expressing the epithelial sodium channel, ENaC, while the cellular substrate for salt aversion was unknown. Here we explore the cellular and molecular basis for the rejection of high concentrations of salts ({\textgreater}300 mM NaCl or KCl). We now show that high-salt recruits the two primary aversive taste pathways by activating the sour and bitter taste-sensing cells. We also demonstrate that genetic silencing of these pathways abolishes behavioral aversion to concentrated salt, without impairing salt attraction. Notably, mice devoid of salt-aversion pathways now exhibit unimpeded, continuous attraction even to exceedingly high concentrations of NaCl. We propose that the âco-optingâ of sour and bitter neural pathways evolved as a means to ensure that high levels of salt reliably trigger robust behavioral rejection, thus preventing its potentially detrimental effects in health and well-being.},
	number = {7438},
	urldate = {2024-02-27},
	journal = {Nature},
	author = {Oka, Yuki and Butnaru, Matthew and von Buchholtz, Lars and Ryba, Nicholas J. P. and Zuker, Charles S.},
	month = feb,
	year = {2013},
	pmid = {23407495},
	pmcid = {PMC3587117},
	pages = {472--475},
}

@inproceedings{boelts_comparing_2019,
	title = {Comparing neural simulations by neural density estimation},
	url = {https://www.researchgate.net/profile/Jan-Boelts-2/publication/335434321_Comparing_neural_simulations_by_neural_density_estimation/links/6022acc9458515893992ea3b/Comparing-neural-simulations-by-neural-density-estimation.pdf},
	urldate = {2024-02-27},
	booktitle = {2019 {Conference} on {Cognitive} {Computational} {Neuroscience}},
	author = {Boelts, Jan and Lueckmann, Jan-Matthis and Goncalves, Pedro J. and Sprekeler, Henning and Macke, Jakob H.},
	year = {2019},
	pages = {1289--1299},
}

@article{yemini_neuropal_2021,
	title = {{NeuroPAL}: {A} {Multicolor} {Atlas} for {Whole}-{Brain} {Neuronal} {Identification} in \textit{{C}.Â elegans}},
	volume = {184},
	issn = {0092-8674},
	shorttitle = {{NeuroPAL}},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867420316822},
	doi = {10.1016/j.cell.2020.12.012},
	abstract = {Comprehensively resolving neuronal identities in whole-brain images is a major challenge. We achieve this in C.Â elegans by engineering a multicolor transgene called NeuroPAL (a neuronal polychromatic atlas of landmarks). NeuroPAL worms share a stereotypical multicolor fluorescence map for the entire hermaphrodite nervous system that resolves all neuronal identities. Neurons labeled with NeuroPAL do not exhibit fluorescence in the green, cyan, or yellow emission channels, allowing the transgene to be used with numerous reporters of gene expression or neuronal dynamics. We showcase three applications that leverage NeuroPAL for nervous-system-wide neuronal identification. First, we determine the brainwide expression patterns of all metabotropic receptors for acetylcholine, GABA, and glutamate, completing a map of this communication network. Second, we uncover changes in cell fate caused by transcription factor mutations. Third, we record brainwide activity in response to attractive and repulsive chemosensory cues, characterizing multimodal coding for these stimuli.},
	number = {1},
	urldate = {2024-02-27},
	journal = {Cell},
	author = {Yemini, Eviatar and Lin, Albert and Nejatbakhsh, Amin and Varol, Erdem and Sun, Ruoxi and Mena, Gonzalo E. and Samuel, Aravinthan D. T. and Paninski, Liam and Venkatachalam, Vivek and Hobert, Oliver},
	month = jan,
	year = {2021},
	keywords = {atlas, expression pattern, nervous system, whole nervous sytem imaging},
	pages = {272--288.e11},
}

@article{deistler_energy-efficient_2022,
	title = {Energy-efficient network activity from disparate circuit parameters},
	volume = {119},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2207632119},
	doi = {10.1073/pnas.2207632119},
	abstract = {Neural circuits can produce similar activity patterns from vastly different combinations of channel and synaptic conductances. These conductances are tuned for specific activity patterns but might also reflect additional constraints, such as metabolic cost or robustness to perturbations. How do such constraints influence the range of permissible conductances? Here we investigate how metabolic cost affects the parameters of neural circuits with similar activity in a model of the pyloric network of the crab Cancer borealis. We present a machine learning method that can identify a range of network models that generate activity patterns matching experimental data and find that neural circuits can consume largely different amounts of energy despite similar circuit activity. Furthermore, a reduced but still significant range of circuit parameters gives rise to energy-efficient circuits. We then examine the space of parameters of energy-efficient circuits and identify potential tuning strategies for low metabolic cost. Finally, we investigate the interaction between metabolic cost and temperature robustness. We show that metabolic cost can vary across temperatures but that robustness to temperature changes does not necessarily incur an increased metabolic cost. Our analyses show that despite metabolic efficiency and temperature robustness constraining circuit parameters, neural systems can generate functional, efficient, and robust network activity with widely disparate sets of conductances.},
	number = {44},
	urldate = {2024-02-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Deistler, Michael and Macke, Jakob H. and GonÃ§alves, Pedro J.},
	month = nov,
	year = {2022},
	pages = {e2207632119},
}

@misc{noauthor_energy-efficient_nodate,
	title = {Energy-efficient network activity from disparate circuit parameters {\textbar} {PNAS}},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2207632119},
	urldate = {2024-02-26},
}

@article{kuramochi_computational_2017,
	title = {A {Computational} {Model} {Based} on {Multi}-{Regional} {Calcium} {Imaging} {Represents} the {Spatio}-{Temporal} {Dynamics} in a {Caenorhabditis} elegans {Sensory} {Neuron}},
	volume = {12},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5224993/},
	doi = {10.1371/journal.pone.0168415},
	abstract = {Due to the huge number of neuronal cells in the brain and their complex circuit formation, computer simulation of neuronal activity is indispensable to understanding whole brain dynamics. Recently, various computational models have been developed based on whole-brain calcium imaging data. However, these analyses monitor only the activity of neuronal cell bodies and treat the cells as point unit. This point-neuron model is inexpensive in computational costs, but the model is unrealistically simplistic at representing intact neural activities in the brain. Here, we describe a novel three-unit Ordinary Differential Equation (ODE) model based on the neuronal responses derived from a Caenorhabditis elegans salt-sensing neuron. We recorded calcium responses in three regions of the ASER neuron using a simple downstep of NaCl concentration. Our simple ODE model generated from a single recording can adequately reproduce and predict the temporal responses of each part of the neuron to various types of NaCl concentration changes. Our strategy which combines a simple recording data and an ODE mathematical model may be extended to realistically understand whole brain dynamics by computational simulation.},
	number = {1},
	urldate = {2024-02-26},
	journal = {PLoS ONE},
	author = {Kuramochi, Masahiro and Doi, Motomichi},
	month = jan,
	year = {2017},
	pmid = {28072834},
	pmcid = {PMC5224993},
	pages = {e0168415},
}

@misc{noauthor_hierarchical_nodate,
	title = {Hierarchical sparse coding in the sensory system of {Caenorhabditis} elegans {\textbar} {PNAS}},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1423656112},
	urldate = {2024-02-26},
}

@article{beets_system-wide_2023,
	title = {System-wide mapping of peptide-{GPCR} interactions in {C}.Â elegans},
	volume = {42},
	issn = {2211-1247},
	doi = {10.1016/j.celrep.2023.113058},
	abstract = {Neuropeptides and peptide hormones are ancient, widespread signaling molecules that underpin almost all brain functions. They constitute a broad ligand-receptor network, mainly by binding to G protein-coupled receptors (GPCRs). However, the organization of the peptidergic network and roles of many peptides remain elusive, as our insight into peptide-receptor interactions is limited and many peptide GPCRs are still orphan receptors. Here we report a genome-wide peptide-GPCR interaction map in Caenorhabditis elegans. By reverse pharmacology screening of over 55,384 possible interactions, we identify 461 cognate peptide-GPCR couples that uncover a broad signaling network with specific and complex combinatorial interactions encoded across and within single peptidergic genes. These interactions provide insights into peptide functions and evolution. Combining our dataset with phylogenetic analysis supports peptide-receptor co-evolution and conservation of at least 14 bilaterian peptidergic systems in C.Â elegans. This resource lays a foundation for system-wide analysis of the peptidergic network.},
	language = {eng},
	number = {9},
	journal = {Cell Reports},
	author = {Beets, Isabel and Zels, Sven and Vandewyer, Elke and Demeulemeester, Jonas and Caers, Jelle and Baytemur, Esra and Courtney, Amy and Golinelli, Luca and HasakioÄullarÄ±, Ä°layda and Schafer, William R. and VÃ©rtes, Petra E. and Mirabeau, Olivier and Schoofs, Liliane},
	month = sep,
	year = {2023},
	pmid = {37656621},
	pmcid = {PMC7615250},
	keywords = {Animals, C.Â elegans, CP: Cell biology, CP: Neuroscience, Caenorhabditis elegans, GPCR, Neuropeptides, Peptide Hormones, Phylogeny, Receptors, G-Protein-Coupled, interactome, neuropeptide, reverse pharmacology},
	pages = {113058},
}

@inproceedings{deistler_training_2024,
	title = {Training networks of morphologically detailed biophysical neuron models with thousands of parameters},
	publisher = {Cosyne},
	author = {Deistler, Michael},
	year = {2024},
}

@article{van_geit_bluepyopt_2016,
	title = {{BluePyOpt}: {Leveraging} {Open} {Source} {Software} and {Cloud} {Infrastructure} to {Optimise} {Model} {Parameters} in {Neuroscience}},
	volume = {10},
	issn = {1662-5196},
	shorttitle = {{BluePyOpt}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2016.00017},
	abstract = {At many scales in neuroscience, appropriate mathematical models take the form of complex dynamical systems. Parameterizing such models to conform to the multitude of available experimental constraints is a global non-linear optimisation problem with a complex fitness landscape, requiring numerical techniques to find suitable approximate solutions. Stochastic optimisation approaches, such as evolutionary algorithms, have been shown to be effective, but often the setting up of such optimisations and the choice of a specific search algorithm and its parameters is non-trivial, requiring domain-specific expertise. Here we describe BluePyOpt, a Python package targeted at the broad neuroscience community to simplify this task. BluePyOpt is an extensible framework for data-driven model parameter optimisation that wraps and standardizes several existing open-source tools. It simplifies the task of creating and sharing these optimisations, and the associated techniques and knowledge. This is achieved by abstracting the optimisation and evaluation tasks into various reusable and flexible discrete elements according to established best-practices. Further, BluePyOpt provides methods for setting up both small- and large-scale optimisations on a variety of platforms, ranging from laptops to Linux clusters and cloud-based compute infrastructures. The versatility of the BluePyOpt framework is demonstrated by working through three representative neuroscience specific use cases.},
	urldate = {2024-02-21},
	journal = {Frontiers in Neuroinformatics},
	author = {Van Geit, Werner and Gevaert, Michael and Chindemi, Giuseppe and RÃ¶ssert, Christian and Courcol, Jean-Denis and Muller, Eilif B. and SchÃ¼rmann, Felix and Segev, Idan and Markram, Henry},
	year = {2016},
}

@article{toni_approximate_2009,
	title = {Approximate {Bayesian} computation scheme for parameter inference and model selection in dynamical systems},
	volume = {6},
	issn = {1742-5689},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2658655/},
	doi = {10.1098/rsif.2008.0172},
	abstract = {Approximate Bayesian computation (ABC) methods can be used to evaluate posterior distributions without having to calculate likelihoods. In this paper, we discuss and apply an ABC method based on sequential Monte Carlo (SMC) to estimate parameters of dynamical models. We show that ABC SMC provides information about the inferability of parameters and model sensitivity to changes in parameters, and tends to perform better than other ABC approaches. The algorithm is applied to several well-known biological systems, for which parameters and their credible intervals are inferred. Moreover, we develop ABC SMC as a tool for model selection; given a range of different mathematical descriptions, ABC SMC is able to choose the best model using the standard Bayesian model selection apparatus.},
	number = {31},
	urldate = {2024-02-21},
	journal = {Journal of the Royal Society Interface},
	author = {Toni, Tina and Welch, David and Strelkowa, Natalja and Ipsen, Andreas and Stumpf, Michael P.H.},
	month = feb,
	year = {2009},
	pmid = {19205079},
	pmcid = {PMC2658655},
	pages = {187--202},
}

@misc{lin_automatic_2024,
	title = {Automatic {Functional} {Differentiation} in {JAX}},
	url = {http://arxiv.org/abs/2311.18727},
	doi = {10.48550/arXiv.2311.18727},
	abstract = {We extend JAX with the capability to automatically differentiate higher-order functions (functionals and operators). By representing functions as a generalization of arrays, we seamlessly use JAX's existing primitive system to implement higher-order functions. We present a set of primitive operators that serve as foundational building blocks for constructing several key types of functionals. For every introduced primitive operator, we derive and implement both linearization and transposition rules, aligning with JAX's internal protocols for forward and reverse mode automatic differentiation. This enhancement allows for functional differentiation in the same syntax traditionally use for functions. The resulting functional gradients are themselves functions ready to be invoked in python. We showcase this tool's efficacy and simplicity through applications where functional derivatives are indispensable. The source code of this work is released at https://github.com/sail-sg/autofd .},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Lin, Min},
	month = jan,
	year = {2024},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Programming Languages},
}



@article{brehmer_mining_2020,
	title = {Mining gold from implicit models to improve likelihood-free inference},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1805.12244},
	doi = {10.1073/pnas.1915980117},
	abstract = {Simulators often provide the best description of real-world phenomena. However, they also lead to challenging inverse problems because the density they implicitly define is often intractable. We present a new suite of simulation-based inference techniques that go beyond the traditional Approximate Bayesian Computation approach, which struggles in a high-dimensional setting, and extend methods that use surrogate models based on neural networks. We show that additional information, such as the joint likelihood ratio and the joint score, can often be extracted from simulators and used to augment the training data for these surrogate models. Finally, we demonstrate that these new techniques are more sample efficient and provide higher-fidelity inference than traditional methods.},
	number = {10},
	urldate = {2024-02-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brehmer, Johann and Louppe, Gilles and Pavez, Juan and Cranmer, Kyle},
	month = mar,
	year = {2020},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	pages = {5242--5249},
}


@book{dayan_theoretical_2001,
	address = {Cambridge, Mass},
	series = {Computational neuroscience},
	title = {Theoretical neuroscience: computational and mathematical modeling of neural systems},
	isbn = {978-0-262-04199-7},
	shorttitle = {Theoretical neuroscience},
	language = {en},
	publisher = {Massachusetts Institute of Technology Press},
	author = {Dayan, Peter and Abbott, L. F.},
	year = {2001},
}

@article{langthaler_ion_2022,
	title = {Ion {Channel} {Modeling} beyond {State} of the {Art}: {A} {Comparison} with a {System} {Theory}-{Based} {Model} of the {Shaker}-{Related} {Voltage}-{Gated} {Potassium} {Channel} {Kv1}.1},
	volume = {11},
	issn = {2073-4409},
	shorttitle = {Ion {Channel} {Modeling} beyond {State} of the {Art}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8773569/},
	doi = {10.3390/cells11020239},
	abstract = {The mathematical modeling of ion channel kinetics is an important tool for studying the electrophysiological mechanisms of the nerves, heart, or cancer, from a single cell to an organ. Common approaches use either a HodgkinâHuxley (HH) or a hidden Markov model (HMM) description, depending on the level of detail of the functionality and structural changes of the underlying channel gating, and taking into account the computational effort for model simulations. Here, we introduce for the first time a novel system theory-based approach for ion channel modeling based on the concept of transfer function characterization, without a priori knowledge of the biological system, using patch clamp measurements. Using the shaker-related voltage-gated potassium channel Kv1.1 (KCNA1) as an example, we compare the established approaches, HH and HMM, with the system theory-based concept in terms of model accuracy, computational effort, the degree of electrophysiological interpretability, and methodological limitations. This highly data-driven modeling concept offers a new opportunity for the phenomenological kinetic modeling of ion channels, exhibiting exceptional accuracy and computational efficiency compared to the conventional methods. The method has a high potential to further improve the quality and computational performance of complex cell and organ model simulations, and could provide a valuable new tool in the field of next-generation in silico electrophysiology.},
	number = {2},
	urldate = {2024-02-18},
	journal = {Cells},
	author = {Langthaler, Sonja and LozanoviÄ Å ajiÄ, Jasmina and RienmÃ¼ller, Theresa and Weinberg, Seth H. and Baumgartner, Christian},
	month = jan,
	year = {2022},
	pmid = {35053355},
	pmcid = {PMC8773569},
	pages = {239},
}

@article{wang_deepneuropepred_2024,
	title = {{DeepNeuropePred}: {A} robust and universal tool to predict cleavage sites from neuropeptide precursors by protein language model},
	volume = {23},
	issn = {2001-0370},
	shorttitle = {{DeepNeuropePred}},
	url = {https://www.sciencedirect.com/science/article/pii/S2001037023004786},
	doi = {10.1016/j.csbj.2023.12.004},
	abstract = {Neuropeptides play critical roles in many biological processes such as growth, learning, memory, metabolism, and neuronal differentiation. A few approaches have been reported for predicting neuropeptides that are cleaved from precursor protein sequences. However, these models for cleavage site prediction of precursors were developed using a limited number of neuropeptide precursor datasets and simple precursors representation models. In addition, a universal method for predicting neuropeptide cleavage sites that can be applied to all species is still lacking. In this paper, we proposed a novel deep learning method called DeepNeuropePred, using a combination of pre-trained language model and Convolutional Neural Networks for feature extraction and predicting the neuropeptide cleavage sites from precursors. To demonstrate the modelâs effectiveness and robustness, we evaluated the performance of DeepNeuropePred and four models from the NeuroPred server in the independent dataset and our model achieved the highest AUC score (0.916), which are 6.9\%, 7.8\%, 8.8\%, and 10.9\% higher than Mammalian (0.857), insects (0.850), Mollusc (0.842) and Motif (0.826), respectively. For the convenience of researchers, we provide a web server (http://isyslab.info/NeuroPepV2/deepNeuropePred.jsp).},
	urldate = {2024-02-17},
	journal = {Computational and Structural Biotechnology Journal},
	author = {Wang, Lei and Zeng, Zilu and Xue, Zhidong and Wang, Yan},
	month = dec,
	year = {2024},
	keywords = {Deep learning, Neuropeptides, Protein language model, Webserver},
	pages = {309--315},
}

@article{vogel_dynamics_2020,
	title = {The {Dynamics} of the {Neuropeptide} {Y} {Receptor} {Type} 1 {Investigated} by {Solid}-{State} {NMR} and {Molecular} {Dynamics} {Simulation}},
	volume = {25},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1420-3049},
	url = {https://www.mdpi.com/1420-3049/25/23/5489},
	doi = {10.3390/molecules25235489},
	abstract = {We report data on the structural dynamics of the neuropeptide Y (NPY) G-protein-coupled receptor (GPCR) type 1 (Y1R), a typical representative of class A peptide ligand GPCRs, using a combination of solid-state NMR and molecular dynamics (MD) simulation. First, the equilibrium dynamics of Y1R were studied using 15N-NMR and quantitative determination of 1H-13C order parameters through the measurement of dipolar couplings in separated-local-field NMR experiments. Order parameters reporting the amplitudes of the molecular motions of the C-H bond vectors of Y1R in DMPC membranes are 0.57 for the CÎ± sites and lower in the side chains (0.37 for the CH2 and 0.18 for the CH3 groups). Different NMR excitation schemes identify relatively rigid and also dynamic segments of the molecule. In monounsaturated membranes composed of longer lipid chains, Y1R is more rigid, attributed to a higher hydrophobic thickness of the lipid membrane. The presence of an antagonist or NPY has little influence on the amplitude of motions, whereas the addition of agonist and arrestin led to a pronounced rigidization. To investigate Y1R dynamics with site resolution, we conducted extensive all-atom MD simulations of the apo and antagonist-bound state. In each state, three replicas with a length of 20 Î¼s (with one exception, where the trajectory length was 10 Î¼s) were conducted. In these simulations, order parameters of each residue were determined and showed high values in the transmembrane helices, whereas the loops and termini exhibit much lower order. The extracellular helix segments undergo larger amplitude motions than their intracellular counterparts, whereas the opposite is observed for the loops, Helix 8, and termini. Only minor differences in order were observed between the apo and antagonist-bound state, whereas the time scale of the motions is shorter for the apo state. Although these relatively fast motions occurring with correlation times of ns up to a few Âµs have no direct relevance for receptor activation, it is believed that they represent the prerequisite for larger conformational transitions in proteins.},
	language = {en},
	number = {23},
	urldate = {2024-02-17},
	journal = {Molecules},
	author = {Vogel, Alexander and Bosse, Mathias and Gauglitz, Marcel and Wistuba, Sarah and Schmidt, Peter and Kaiser, Anette and Gurevich, Vsevolod V. and Beck-Sickinger, Annette G. and Hildebrand, Peter W. and Huster, Daniel},
	month = jan,
	year = {2020},
	keywords = {GPCR, MD simulation, NMR spectroscopy, arrestin, molecular switch, structural dynamics},
	pages = {5489},
}

@article{marr_understanding_1976,
	title = {From {Understanding} {Computation} to {Understanding} {Neural} {Circuitry}},
	url = {https://dspace.mit.edu/handle/1721.1/5782},
	abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
	language = {en\_US},
	urldate = {2024-02-16},
	author = {Marr, D. and Poggio, T.},
	month = may,
	year = {1976},
}

@article{naudin_general_2023,
	title = {A general pattern of non-spiking neuron dynamics under the effect of potassium and calcium channel modifications},
	volume = {51},
	issn = {1573-6873},
	url = {https://doi.org/10.1007/s10827-022-00840-w},
	doi = {10.1007/s10827-022-00840-w},
	abstract = {Electrical activity of excitable cells results from ion exchanges through cell membranes, so that genetic or epigenetic changes in genes encoding ion channels are likely to affect neuronal electrical signaling throughout the brain. There is a large literature on the effect of variations in ion channels on the dynamics of spiking neurons that represent the main type of neurons found in the vertebrate nervous systems. Nevertheless, non-spiking neurons are also ubiquitous in many nervous tissues and play a critical role in the processing of some sensory systems. To our knowledge, however, how conductance variations affect the dynamics of non-spiking neurons has never been assessed. Based on experimental observations reported in the biological literature and on mathematical considerations, we first propose a phenotypic classification of non-spiking neurons. Then, we determine a general pattern of the phenotypic evolution of non-spiking neurons as a function of changes in calcium and potassium conductances. Furthermore, we study the homeostatic compensatory mechanisms of ion channels in a well-posed non-spiking retinal cone model. We show that there is a restricted range of ion conductance values for which the behavior and phenotype of the neuron are maintained. Finally, we discuss the implications of the phenotypic changes of individual cells at the level of neuronal network functioning of the C. elegans worm and the retina, which are two non-spiking nervous tissues composed of neurons with various phenotypes.},
	language = {en},
	number = {1},
	urldate = {2024-02-15},
	journal = {Journal of Computational Neuroscience},
	author = {Naudin, LoÃ¯s and Raison-Aubry, Laetitia and Buhry, Laure},
	month = feb,
	year = {2023},
	keywords = {Bifurcation, Caenorhabditis elegans, Conductance variations, Non-spiking neurons, Retina},
	pages = {173--186},
}

@article{li_neuropeptide_2015,
	title = {Neuropeptide {S} {Increases} locomotion activity through corticotropin-releasing factor receptor 1 in substantia nigra of mice},
	volume = {71},
	issn = {1873-5169},
	doi = {10.1016/j.peptides.2015.07.024},
	abstract = {Neuropeptide S (NPS), the endogenous ligand of NPS receptor (NPSR), was reported to be involved in the regulation of arousal, anxiety, locomotion, learning and memory. The basal ganglia play a crucial role in regulating of locomotion-related behavior. Here, we found that NPSR protein of mouse was distributed in the substantia nigra (SN) and globus pallidus (LGP) by immunohistochemical analysis. However, less is known about the direct locomotion-related effects of NPS in both SN and LGP. Therefore, we investigated the role of NPS in locomotion processes, using the open field test. The results showed that NPS infused into the SN (0.03, 0.1, 1nmol) or LGP (0.01, 0.03, 0.1nmol) dose-dependently increased the locomotor activity in mice. SHA 68 (50mg/kg), an antagonist of NPSR, blocked the locomotor stimulant effect of NPS in both nuleus. Meanwhile, these effects of NPS were also counteracted by the CRF1 receptor antagonist antalarmin (30mg/kg, i.p.). In addition, we found that the expression of c-Fos was significantly increased after NPS was delivered into SN. In conclusion, these results indicate that NPS-NPSR system may regulate locomotion together with the CRF1 system in SN.},
	language = {eng},
	journal = {Peptides},
	author = {Li, M. S. and Peng, Y. L. and Jiang, J. H. and Xue, H. X. and Wang, P. and Zhang, P. J. and Han, R. W. and Chang, M. and Wang, R.},
	month = sep,
	year = {2015},
	pmid = {26239581},
	keywords = {Animals, CRF1, Gene Expression Regulation, Locomotion, Male, Mice, NPS, NPSR, Neuropeptides, Proto-Oncogene Proteins c-fos, Pyrimidines, Pyrroles, Receptors, Corticotropin-Releasing Hormone, Substantia Nigra, Substantia nigra, c-Fos},
	pages = {196--201},
}

@article{miller_appetite_2017,
	title = {Appetite {Regulation}: {Hormones}, {Peptides}, and {Neurotransmitters} and {Their} {Role} in {Obesity}},
	volume = {13},
	issn = {1559-8276},
	shorttitle = {Appetite {Regulation}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6796227/},
	doi = {10.1177/1559827617716376},
	abstract = {Understanding body weight regulation will aid in the development of new
strategies to combat obesity. This review examines energy homeostasis and food
intake behaviors, specifically with regards to hormones, peptides, and
neurotransmitters in the periphery and central nervous system, and their
potential role in obesity. Dysfunction in feeding signals by the brain is a
factor in obesity. The hypothalamic (arcuate nucleus) and brainstem (nucleus
tractus solitaris) areas integrate behavioral, endocrine, and autonomic
responses via afferent and efferent pathways from and to the brainstem and
peripheral organs. Neurons present in the arcuate nucleus express
pro-opiomelanocortin, Neuropeptide Y, and Agouti Related Peptide, with the
former involved in lowering food intake, and the latter two acutely increasing
feeding behaviors. Action of peripheral hormones from the gut, pancreas,
adipose, and liver are also involved in energy homeostasis. Vagal afferent
neurons are also important in regulating energy homeostasis. Peripheral signals
respond to the level of stored and currently available fuel. By studying their
actions, new agents maybe developed that disable orexigenic responses and
enhance anorexigenic signals. Although there are relatively few medications
currently available for obesity treatment, a number of agents are in development
that work through these pathways.},
	number = {6},
	urldate = {2024-02-15},
	journal = {American Journal of Lifestyle Medicine},
	author = {Miller, Gary D.},
	month = jun,
	year = {2017},
	pmid = {31662725},
	pmcid = {PMC6796227},
	pages = {586--601},
}

@article{bazzari_neuromodulators_2019,
	title = {Neuromodulators and {Long}-{Term} {Synaptic} {Plasticity} in {Learning} and {Memory}: {A} {Steered}-{Glutamatergic} {Perspective}},
	volume = {9},
	issn = {2076-3425},
	shorttitle = {Neuromodulators and {Long}-{Term} {Synaptic} {Plasticity} in {Learning} and {Memory}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6896105/},
	doi = {10.3390/brainsci9110300},
	abstract = {The molecular pathways underlying the induction and maintenance of long-term synaptic plasticity have been extensively investigated revealing various mechanisms by which neurons control their synaptic strength. The dynamic nature of neuronal connections combined with plasticity-mediated long-lasting structural and functional alterations provide valuable insights into neuronal encoding processes as molecular substrates of not only learning and memory but potentially other sensory, motor and behavioural functions that reflect previous experience. However, one key element receiving little attention in the study of synaptic plasticity is the role of neuromodulators, which are known to orchestrate neuronal activity on brain-wide, network and synaptic scales. We aim to review current evidence on the mechanisms by which certain modulators, namely dopamine, acetylcholine, noradrenaline and serotonin, control synaptic plasticity induction through corresponding metabotropic receptors in a pathway-specific manner. Lastly, we propose that neuromodulators control plasticity outcomes through steering glutamatergic transmission, thereby gating its induction and maintenance.},
	number = {11},
	urldate = {2024-02-15},
	journal = {Brain Sciences},
	author = {Bazzari, Amjad H. and Parri, H. Rheinallt},
	month = oct,
	year = {2019},
	pmid = {31683595},
	pmcid = {PMC6896105},
	pages = {300},
}

@article{van_vreeswijk_chaos_1996,
	title = {Chaos in {Neuronal} {Networks} with {Balanced} {Excitatory} and {Inhibitory} {Activity}},
	volume = {274},
	url = {https://www.science.org/doi/10.1126/science.274.5293.1724},
	doi = {10.1126/science.274.5293.1724},
	abstract = {Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron.},
	number = {5293},
	urldate = {2024-02-13},
	journal = {Science},
	author = {van Vreeswijk, C. and Sompolinsky, H.},
	month = dec,
	year = {1996},
	pages = {1724--1726},
}

@article{goldwyn_what_2011,
	title = {The {What} and {Where} of {Adding} {Channel} {Noise} to the {Hodgkin}-{Huxley} {Equations}},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002247},
	doi = {10.1371/journal.pcbi.1002247},
	abstract = {Conductance-based equations for electrically active cells form one of the most widely studied mathematical frameworks in computational biology. This framework, as expressed through a set of differential equations by Hodgkin and Huxley, synthesizes the impact of ionic currents on a cell's voltageâand the highly nonlinear impact of that voltage back on the currents themselvesâinto the rapid push and pull of the action potential. Later studies confirmed that these cellular dynamics are orchestrated by individual ion channels, whose conformational changes regulate the conductance of each ionic current. Thus, kinetic equations familiar from physical chemistry are the natural setting for describing conductances; for small-to-moderate numbers of channels, these will predict fluctuations in conductances and stochasticity in the resulting action potentials. At first glance, the kinetic equations provide a far more complex (and higher-dimensional) description than the original Hodgkin-Huxley equations or their counterparts. This has prompted more than a decade of efforts to capture channel fluctuations with noise terms added to the equations of Hodgkin-Huxley type. Many of these approaches, while intuitively appealing, produce quantitative errors when compared to kinetic equations; others, as only very recently demonstrated, are both accurate and relatively simple. We review what works, what doesn't, and why, seeking to build a bridge to well-established results for the deterministic equations of Hodgkin-Huxley type as well as to more modern models of ion channel dynamics. As such, we hope that this review will speed emerging studies of how channel noise modulates electrophysiological dynamics and function. We supply user-friendly MATLAB simulation code of these stochastic versions of the Hodgkin-Huxley equations on the ModelDB website (accession number 138950) and http://www.amath.washington.edu/{\textasciitilde}etsb/tutorials.html.},
	language = {en},
	number = {11},
	urldate = {2024-02-13},
	journal = {PLOS Computational Biology},
	author = {Goldwyn, Joshua H. and Shea-Brown, Eric},
	month = nov,
	year = {2011},
	keywords = {Action potentials, Approximation methods, Cell membranes, Gaussian noise, Ion channels, Markov models, Simulation and modeling, White noise},
	pages = {e1002247},
}

@misc{noauthor_figure_nodate,
	title = {Figure 1. {Controlling} the {C}. elegans neural network. (a) {Schematic}...},
	url = {https://www.researchgate.net/figure/Controlling-the-C-elegans-neural-network-a-Schematic-neural-circuit-for-locomotor_fig1_320473561},
	abstract = {Download scientific diagram {\textbar} Controlling the C. elegans neural network. (a) Schematic neural circuit for locomotor response to gentle touch in C. elegans (adapted after Ref. 30; see SI Sec. III A). (b) Graphical representation of the proposed control framework. According to the principles illustrated in Fig. 2(a-d), if removal of a neuron disrupts controllability of the muscles, we designate it "essential" for locomotion; if not, we call it "non-essential". To make this assessment, we first mapped the C. elegans responsiveÂ  from publication: Network control principles predict neuron function in the Caenorhabditis elegans connectome {\textbar} Recent studies on the controllability of complex systems offer a powerful mathematical framework to systematically explore the structure-function relationship in biological, social, and technological networks. Despite theoretical advances, we lack direct experimental proof of... {\textbar} Connectome, Neuron and Caenorhabditis elegans {\textbar} ResearchGate, the professional network for scientists.},
	language = {en},
	urldate = {2024-02-13},
	journal = {ResearchGate},
}

@article{mei_informing_2022,
	title = {Informing deep neural networks by multiscale principles of neuromodulatory systems},
	volume = {45},
	issn = {0166-2236, 1878-108X},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(21)00256-3},
	doi = {10.1016/j.tins.2021.12.008},
	language = {English},
	number = {3},
	urldate = {2024-02-10},
	journal = {Trends in Neurosciences},
	author = {Mei, Jie and Muller, Eilif and Ramaswamy, Srikanth},
	month = mar,
	year = {2022},
	pmid = {35074219},
	keywords = {acetylcholine, adaptive learning, dopamine, multiscale organization, noradrenaline, serotonin},
	pages = {237--250},
}

@article{demaegd_neuropeptide_2021,
	title = {Neuropeptide {Modulation} {Increases} {Dendritic} {Electrical} {Spread} to {Restore} {Neuronal} {Activity} {Disrupted} by {Temperature}},
	volume = {41},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8425982/},
	doi = {10.1523/JNEUROSCI.0101-21.2021},
	abstract = {Peptide neuromodulation has been implicated to shield neuronal activity from acute temperature changes that can otherwise lead to loss of motor control or failure of vital behaviors. However, the cellular actions neuropeptides elicit to support temperature-robust activity remain unknown. Here, we find that peptide neuromodulation restores rhythmic bursting in temperature-compromised central pattern generator (CPG) neurons by counteracting membrane shunt and increasing dendritic electrical spread. We show that acutely rising temperatures reduced spike generation and interrupted ongoing rhythmic motor activity in the crustacean gastric mill CPG. Neuronal release and extrinsic application of Cancer borealis tachykinin-related peptide Ia (CabTRP Ia), a substance-P-related peptide, restored rhythmic activity. Warming led to a significant decrease in membrane resistance and a shunting of the dendritic signals in the main gastric mill CPG neuron. Using a combination of fluorescent calcium imaging and electrophysiology, we observed that postsynaptic potentials and antidromic action potentials propagated less far within the dendritic neuropil as the system warmed. In the presence of CabTRP Ia, membrane shunt decreased and both postsynaptic potentials and antidromic action potentials propagated farther. At elevated temperatures, CabTRP Ia restored dendritic electrical spread or extended it beyond that at cold temperatures. Selective introduction of the CabTRP Ia conductance using a dynamic clamp demonstrated that the CabTRP Ia voltage-dependent conductance was sufficient to restore rhythmic bursting. Our findings demonstrate that a substance-P-related neuropeptide can boost dendritic electrical spread to maintain neuronal activity when perturbed and reveals key neurophysiological components of neuropeptide actions that support pattern generation in temperature-compromised conditions., SIGNIFICANCE STATEMENT Changes in body temperature can have detrimental consequences for the well-being of an organism. Temperature-dependent changes in neuronal activity can be especially dangerous if they affect vital behaviors. Understanding how temperature changes disrupt neuronal activity and identifying how to ameliorate such effects is critically important. Our study of a crustacean circuit shows that warming disrupts rhythmic neuronal activity by increasing membrane shunt and reducing dendritic electrical spread in a key circuit neuron. Through the ionic conductance activated by it, substance-P-related peptide modulation restored electrical spread and counteracted the detrimental temperature effects on rhythmic activity. Because neuropeptides are commonly implicated in sustaining neuronal activity during perturbation, our results provide a promising mechanism to support temperature-robust activity.},
	number = {36},
	urldate = {2024-02-10},
	journal = {The Journal of Neuroscience},
	author = {DeMaegd, Margaret L. and Stein, Wolfgang},
	month = sep,
	year = {2021},
	pmid = {34321314},
	pmcid = {PMC8425982},
	pages = {7607--7622},
}

@article{piggott_neural_2011,
	title = {The neural circuits and synaptic mechanisms underlying motor initiation in {C}. elegans},
	volume = {147},
	issn = {1097-4172},
	doi = {10.1016/j.cell.2011.08.053},
	abstract = {C. elegans is widely used to dissect how neural circuits and genes generate behavior. During locomotion, worms initiate backward movement to change locomotion direction spontaneously or in response to sensory cues; however, the underlying neural circuits are not well defined. We applied a multidisciplinary approach to map neural circuits in freely behaving worms by integrating functional imaging, optogenetic interrogation, genetic manipulation, laser ablation, and electrophysiology. We found that a disinhibitory circuit and a stimulatory circuit together promote initiation of backward movement and that circuitry dynamics is differentially regulated by sensory cues. Both circuits require glutamatergic transmission but depend on distinct glutamate receptors. This dual mode of motor initiation control is found in mammals, suggesting that distantly related organisms with anatomically distinct nervous systems may adopt similar strategies for motor control. Additionally, our studies illustrate how a multidisciplinary approach facilitates dissection of circuit and synaptic mechanisms underlying behavior in a genetic model organism.},
	language = {eng},
	number = {4},
	journal = {Cell},
	author = {Piggott, Beverly J. and Liu, Jie and Feng, Zhaoyang and Wescott, Seth A. and Xu, X. Z. Shawn},
	month = nov,
	year = {2011},
	pmid = {22078887},
	pmcid = {PMC3233480},
	keywords = {Animals, Caenorhabditis elegans, Caenorhabditis elegans Proteins, Electrophysiology, Interneurons, Motor Activity, Mutation, Neural Pathways, Osmotic Pressure, Receptors, Glutamate, Synapses},
	pages = {922--933},
}

@article{heinrichs_neuropeptides_2008,
	title = {Neuropeptides and social behaviour: effects of oxytocin and vasopressin in humans},
	volume = {170},
	issn = {1875-7855},
	shorttitle = {Neuropeptides and social behaviour},
	doi = {10.1016/S0079-6123(08)00428-7},
	abstract = {The fundamental ability to form attachment is indispensable for human social relationships. Impairments in social behaviour are associated with decreased quality of life and psychopathological states. In non-human mammals, the neuropeptides oxytocin (OXT) and arginine vasopressin (AVP) are key mediators of complex social behaviours, including attachment, social recognition and aggression. In particular, OXT reduces behavioural and neuroendocrine responses to social stress and seems both to enable animals to overcome their natural avoidance of proximity and to inhibit defensive behaviour, thereby facilitating approach behaviour. AVP has primarily been implicated in male-typical social behaviours, including aggression and pair-bond formation, and mediates anxiogenic effects. Initial studies in humans suggest behavioural, neural, and endocrine effects of both neuropeptides, similar to those found in animal studies. This review focuses on advances made to date in the effort to understand the role of OXT and AVP in human social behaviour. First, the literature on OXT and AVP and their involvement in social stress and anxiety, social cognition, social approach, and aggression is reviewed. Second, we discuss clinical implications for mental disorders that are associated with social deficits (e.g. autism spectrum disorder, borderline personality disorder). Finally, a model of the interactions of anxiety and stress, social approach behaviour, and the oxytocinergic system is presented, which integrates the novel approach of a psychobiological therapy in psychopathological states.},
	language = {eng},
	journal = {Progress in Brain Research},
	author = {Heinrichs, Markus and Domes, Gregor},
	year = {2008},
	pmid = {18655894},
	keywords = {Aggression, Animals, Anxiety, Autistic Disorder, Borderline Personality Disorder, Cognition, Humans, Interpersonal Relations, Models, Animal, Neuropeptides, Oxytocin, Recognition, Psychology, Social Behavior, Stress, Psychological, Vasopressins},
	pages = {337--350},
}

@article{bhat_neuropeptides_2021,
	title = {Neuropeptides and {Behaviors}: {How} {Small} {Peptides} {Regulate} {Nervous} {System} {Function} and {Behavioral} {Outputs}},
	volume = {14},
	issn = {1662-5099},
	shorttitle = {Neuropeptides and {Behaviors}},
	url = {https://www.frontiersin.org/articles/10.3389/fnmol.2021.786471},
	abstract = {One of the reasons that most multicellular animals survive and thrive is because of the adaptable and plastic nature of their nervous systems. For an organism to survive, it is essential for the animal to respond and adapt to environmental changes. This is achieved by sensing external cues and translating them into behaviors through changes in synaptic activity. The nervous system plays a crucial role in constantly evaluating environmental cues and allowing for behavioral plasticity in the organism. Multiple neurotransmitters and neuropeptides have been implicated as key players for integrating sensory information to produce the desired output. Because of its simple nervous system and well-established neuronal connectome, C. elegans acts as an excellent model to understand the mechanisms underlying behavioral plasticity. Here, we critically review how neuropeptides modulate a wide range of behaviors by allowing for changes in neuronal and synaptic signaling. This review will have a specific focus on feeding, mating, sleep, addiction, learning and locomotory behaviors in C. elegans. With a view to understand evolutionary relationships, we explore the functions and associated pathophysiology of C. elegans neuropeptides that are conserved across different phyla. Further, we discuss the mechanisms of neuropeptidergic signaling and how these signals are regulated in different behaviors. Finally, we attempt to provide insight into developing potential therapeutics for neuropeptide-related disorders.},
	urldate = {2024-02-07},
	journal = {Frontiers in Molecular Neuroscience},
	author = {Bhat, Umer Saleem and Shahi, Navneet and Surendran, Siju and Babu, Kavita},
	year = {2021},
}

@article{sanchez-roige_genetics_2018,
	title = {The genetics of human personality},
	volume = {17},
	copyright = {Â© 2017 John Wiley \& Sons Ltd and International Behavioural and Neural Genetics Society},
	issn = {1601-183X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/gbb.12439},
	doi = {10.1111/gbb.12439},
	abstract = {Genome-wide association studies hits (log10 transformed) discovered as a function of sample size and personality trait.},
	language = {en},
	number = {3},
	urldate = {2024-02-07},
	journal = {Genes, Brain and Behavior},
	author = {Sanchez-Roige, S. and Gray, J. C. and MacKillop, J. and Chen, C.-H. and Palmer, A. A.},
	year = {2018},
	keywords = {EPQ, GWAS, NEO, TPQ, agreeableness, conscientiousness, extraversion, gene, neuroticism, openness, personality},
	pages = {e12439},
}

@article{stern_neuromodulatory_2017,
	title = {Neuromodulatory {Control} of {Long}-{Term} {Behavioral} {Patterns} and {Individuality} across {Development}},
	volume = {171},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(17)31267-9},
	doi = {10.1016/j.cell.2017.10.041},
	language = {English},
	number = {7},
	urldate = {2024-02-07},
	journal = {Cell},
	author = {Stern, Shay and Kirst, Christoph and Bargmann, Cornelia I.},
	month = dec,
	year = {2017},
	pmid = {29198526},
	pages = {1649--1662.e10},
}


@article{hiroki_molecular_2022,
	title = {Molecular encoding and synaptic decoding of context during salt chemotaxis in {C}. elegans},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-30279-7},
	doi = {10.1038/s41467-022-30279-7},
	abstract = {Animals navigate toward favorable locations using various environmental cues. However, the mechanism of how the goal information is encoded and decoded to generate migration toward the appropriate direction has not been clarified. Here, we describe the mechanism of migration towards a learned concentration of NaCl in Caenorhabditis elegans. In the salt-sensing neuron ASER, the difference between the experienced and currently perceived NaCl concentration is encoded as phosphorylation at Ser65 of UNC-64/Syntaxin 1âA through the protein kinase C(PKC-1) signaling pathway. The phosphorylation affects basal glutamate transmission from ASER, inducing the reversal of the postsynaptic response of reorientation-initiating neurons (i.e., from inhibitory to excitatory), guiding the animals toward the experienced concentration. This process, the decoding of the context, is achieved through the differential sensitivity of postsynaptic excitatory and inhibitory receptors. Our results reveal the mechanism of migration based on the synaptic plasticity that conceptually differs from the classical ones.},
	language = {en},
	number = {1},
	urldate = {2024-02-07},
	journal = {Nature Communications},
	author = {Hiroki, Shingo and Yoshitane, Hikari and Mitsui, Hinako and Sato, Hirofumi and Umatani, Chie and Kanda, Shinji and Fukada, Yoshitaka and Iino, Yuichi},
	month = may,
	year = {2022},
	keywords = {Learning and memory, Molecular neuroscience, Navigation, Synaptic plasticity},
	pages = {2928},
}

@article{adachi_reversal_2010,
	title = {Reversal of {Salt} {Preference} {Is} {Directed} by the {Insulin}/{PI3K} and {Gq}/{PKC} {Signaling} in {Caenorhabditis} elegans},
	volume = {186},
	issn = {0016-6731},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2998313/},
	doi = {10.1534/genetics.110.119768},
	abstract = {Animals search for foods and decide their behaviors according to previous experience. Caenorhabditis elegans detects chemicals with a limited number of sensory neurons, allowing us to dissect roles of each neuron for innate and learned behaviors. C. elegans is attracted to salt after exposure to the salt (NaCl) with food. In contrast, it learns to avoid the salt after exposure to the salt without food. In salt-attraction behavior, it is known that the ASE taste sensory neurons (ASEL and ASER) play a major role. However, little is known about mechanisms for learned salt avoidance. Here, through dissecting contributions of ASE neurons for salt chemotaxis, we show that both ASEL and ASER generate salt chemotaxis plasticity. In ASER, we have previously shown that the insulin/PI 3-kinase signaling acts for starvation-induced salt chemotaxis plasticity. This study shows that the PI 3-kinase signaling promotes aversive drive of ASER but not of ASEL. Furthermore, the Gq signaling pathway composed of GqÎ± EGL-30, diacylglycerol, and nPKC (novel protein kinase C) TTX-4 promotes attractive drive of ASER but not of ASEL. A putative salt receptor GCY-22 guanylyl cyclase is required in ASER for both salt attraction and avoidance. Our results suggest that ASEL and ASER use distinct molecular mechanisms to regulate salt chemotaxis plasticity.},
	number = {4},
	urldate = {2024-02-07},
	journal = {Genetics},
	author = {Adachi, Takeshi and Kunitomo, Hirofumi and Tomioka, Masahiro and Ohno, Hayao and Okochi, Yoshifumi and Mori, Ikue and Iino, Yuichi},
	month = dec,
	year = {2010},
	pmid = {20837997},
	pmcid = {PMC2998313},
	pages = {1309--1319},
}

@article{borbely_neuropeptides_2013,
	series = {Neuropeptides in {Mental} {Health} and {Behaviour}},
	title = {Neuropeptides in learning and memory},
	volume = {47},
	issn = {0143-4179},
	url = {https://www.sciencedirect.com/science/article/pii/S0143417913000784},
	doi = {10.1016/j.npep.2013.10.012},
	abstract = {Dementia conditions and memory deficits of different origins (vascular, metabolic and primary neurodegenerative such as Alzheimerâs and Parkinsonâs diseases) are getting more common and greater clinical problems recently in the aging population. Since the presently available cognitive enhancers have very limited therapeutical applications, there is an emerging need to elucidate the complex pathophysiological mechanisms, identify key mediators and novel targets for future drug development. Neuropeptides are widely distributed in brain regions responsible for learning and memory processes with special emphasis on the hippocampus, amygdala and the basal forebrain. They form networks with each other, and also have complex interactions with the cholinergic, glutamatergic, dopaminergic and GABA-ergic pathways. This review summarizes the extensive experimental data in the well-established rat and mouse models, as well as the few clinical results regarding the expression and the roles of the tachykinin system, somatostatin and the closely related cortistatin, vasoactive intestinal polypeptide (VIP) and pituitary adenylate-cyclase activating polypeptide (PACAP), calcitonin gene-related peptide (CGRP), neuropeptide Y (NPY), opioid peptides and galanin. Furthermore, the main receptorial targets, mechanisms and interactions are described in order to highlight the possible therapeutical potentials. Agents not only symptomatically improving the functional impairments, but also inhibiting the progression of the neurodegenerative processes would be breakthroughs in this area. The most promising mechanisms determined at the level of exploratory investigations in animal models of cognitive disfunctions are somatostatin sst4, NPY Y2, PACAPâVIP VPAC1, tachykinin NK3 and galanin GALR2 receptor agonisms, as well as delta opioid receptor antagonism. Potent and selective non-peptide ligands with good CNS penetration are needed for further characterization of these molecular pathways to complete the preclinical studies and decide if any of the above described targets could be appropriate for clinical investigations.},
	number = {6},
	urldate = {2024-02-06},
	journal = {Neuropeptides},
	author = {BorbÃ©ly, Ãva and Scheich, BÃ¡lint and Helyes, Zsuzsanna},
	month = dec,
	year = {2013},
	keywords = {Animal models, CGRP, Galanin, NPY, Opioid peptides, Somatostatin, Tachykinins, VIP/PACAP},
	pages = {439--450},
}

@article{gotzsche_role_2016,
	series = {Advances in {NPY} {Research}},
	title = {The role of {NPY} in learning and memory},
	volume = {55},
	issn = {0143-4179},
	url = {https://www.sciencedirect.com/science/article/pii/S0143417915000992},
	doi = {10.1016/j.npep.2015.09.010},
	abstract = {High levels of NPY expression in brain regions important for learning and memory together with its neuromodulatory and neurotrophic effects suggest a regulatory role for NPY in memory processes. Therefore it is not surprising that an increasing number of studies have provided evidence for NPY acting as a modulator of neuroplasticity, neurotransmission, and memory. Here these results are presented in relation to the types of memory affected by NPY and its receptors. NPY can exert both inhibitory and stimulatory effects on memory, depending on memory type and phase, dose applied, brain region, and NPY receptor subtypes. Thus NPY act as a resilience factor by impairing associative implicit memory after stressful and aversive events, as evident in models of fear conditioning, presumably via Y1 receptors in the amygdala and prefrontal cortex. In addition, NPY impairs acquisition but enhances consolidation and retention in models depending on spatial and discriminative types of associative explicit memory, presumably involving Y2 receptor-mediated regulations of hippocampal excitatory transmission. Moreover, spatial memory training leads to increased hippocampal NPY gene expression that together with Y1 receptor-mediated neurogenesis could constitute necessary steps in consolidation and long-term retention of spatial memory. Altogether, NPY-induced effects on learning and memory seem to be biphasic, anatomically and temporally differential, and in support of a modulatory role of NPY at keeping the system in balance. Obtaining further insight into memory-related effects of NPY could inspire the engineering of new therapeutics targeting diseases where impaired learning and memory are central elements.},
	urldate = {2024-02-06},
	journal = {Neuropeptides},
	author = {GÃ¸tzsche, C. R. and Woldbye, D. P. D.},
	month = feb,
	year = {2016},
	keywords = {Animal models, Explicit memory, Implicit memory, Learning, Memory, NPY},
	pages = {79--89},
}

@article{rakowski_optimal_2017,
	title = {Optimal synaptic signaling connectome for locomotory behavior in {Caenorhabditis} elegans: {Design} minimizing energy cost},
	volume = {13},
	issn = {1553-7358},
	shorttitle = {Optimal synaptic signaling connectome for locomotory behavior in {Caenorhabditis} elegans},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005834},
	doi = {10.1371/journal.pcbi.1005834},
	abstract = {The detailed knowledge of C. elegans connectome for 3 decades has not contributed dramatically to our understanding of wormâs behavior. One of main reasons for this situation has been the lack of data on the type of synaptic signaling between particular neurons in the wormâs connectome. The aim of this study was to determine synaptic polarities for each connection in a small pre-motor circuit controlling locomotion. Even in this compact network of just 7 neurons the space of all possible patterns of connection types (excitation vs. inhibition) is huge. To deal effectively with this combinatorial problem we devised a novel and relatively fast technique based on genetic algorithms and large-scale parallel computations, which we combined with detailed neurophysiological modeling of interneuron dynamics and compared the theory to the available behavioral data. As a result of these massive computations, we found that the optimal connectivity pattern that matches the best locomotory data is the one in which all interneuron connections are inhibitory, even those terminating on motor neurons. This finding is consistent with recent experimental data on cholinergic signaling in C. elegans, and it suggests that the system controlling locomotion is designed to save metabolic energy. Moreover, this result provides a solid basis for a more realistic modeling of neural control in these worms, and our novel powerful computational technique can in principle be applied (possibly with some modifications) to other small-scale functional circuits in C. elegans.},
	language = {en},
	number = {11},
	urldate = {2024-02-01},
	journal = {PLOS Computational Biology},
	author = {Rakowski, Franciszek and Karbowski, Jan},
	month = nov,
	year = {2017},
	keywords = {Animal behavior, Caenorhabditis elegans, Connectomics, Electrical circuits, Interneurons, Motor neurons, Neurons, Neurophysiology},
	pages = {e1005834},
}

@inproceedings{koyuncu_dddas-based_2007,
	title = {{DDDAS}-based multi-fidelity simulation for online preventive maintenance scheduling in semiconductor supply chain},
	url = {https://ieeexplore.ieee.org/document/4419819},
	doi = {10.1109/WSC.2007.4419819},
	abstract = {This research intends to augment the validity of simulation models in the most economic way using the DDDAS (Dynamic Data Driven Application Systems) paradigm. Implementation of DDDAS requires automated switching of model fidelity and incorporating selective, dynamic data into the executing simulation model. Comprehensive system architecture and methodologies are proposed, where the components include 1.) RT (Real Time) DDDAS simulation, 2.) grid computing modules, 3.) Web Service communication server, 4.) database, 5.) various sensors, and 6.) real system. Four algorithms are developed to facilitate integration of the various components in the DDDAS system. They are 1.) data filtering algorithm using control charts, 2.) preliminary fidelity selection algorithm using Bayesian belief network, 3.) fidelity assignment algorithm using integer programming and 4.) simulation model reconstruction algorithm using multiple linear regression. A prototype DDDAS simulation was successfully implemented for preventive maintenance scheduling in a semiconductor supply chain. The initial results look quite promising.},
	urldate = {2024-02-01},
	booktitle = {2007 {Winter} {Simulation} {Conference}},
	author = {Koyuncu, Nurcin and Lee, Seungho and Vasudevan, Karthik K. and Son, Young-Jun and Sarfare, Parag},
	month = dec,
	year = {2007},
	keywords = {Communication switching, Computational modeling, Computer architecture, Filtering algorithms, Grid computing, Preventive maintenance, Real time systems, Service oriented architecture, Supply chains, Web services},
	pages = {1915--1923},
}

@article{balabanov_multifidelity_1998,
	title = {Multifidelity {Response} {Surface} {Model} {For} {HSCT} {Wing} {Bending} {Material} {Weight}},
	doi = {10.2514/6.1998-4804},
	abstract = {Response surface techniques allow us to combine results from a large number of inexpensive low fidelity analyses with a small number of expensive high fidelity analyses for constructing inexpensive and accurate approximations. The paper demonstrates this approach by constructing approximations to wing bending material weight of a high speed civil transport (HSCT). The approximations employ a large number of structural optimizations of finite element models for a range of HSCT configurations. Thousands of structural optimizations of coarse finite element models are used to construct a quadratic response surface model. Then about a hundred structural optimizations of refined finite element models are used to construct linear correction response surface models. The usefulness of the approximations is demonstrated by performing aerodynamic optimizations of the HSCT while employing the response surface models to estimate wing bending material weight. The approximations for the final HSCT de...},
	author = {Balabanov, Vladimir and Haftka, Raphael and Grossman, Bernard and Mason, William and Watson, Layne},
	month = jun,
	year = {1998},
}

@article{barbulescu_learning_2023,
	title = {Learning the dynamics of realistic models of {C}. elegans nervous system with recurrent neural networks},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-25421-w},
	doi = {10.1038/s41598-022-25421-w},
	abstract = {Given the inherent complexity of the human nervous system, insight into the dynamics of brain activity can be gained from studying smaller and simpler organisms. While some of the potential target organisms are simple enough that their behavioural and structural biology might be well-known and understood, others might still lead to computationally intractable models that require extensive resources to simulate. Since such organisms are frequently only acting as proxies to further our understanding of underlying phenomena or functionality, often one is not interested in the detailed evolution of every single neuron in the system. Instead, it is sufficient to observe the subset of neurons that capture the effect that the profound nonlinearities of the neuronal system have in response to different stimuli. In this paper, we consider the well-known nematode Caenorhabditis elegans and seek to investigate the possibility of generating lower complexity models that capture the systemâs dynamics with low error using only measured or simulated input-output information. Such models are often termed black-box models. We show how the nervous system of C. elegans can be modelled and simulated with data-driven models using different neural network architectures. Specifically, we target the use of state-of-the-art recurrent neural network architectures such as Long Short-Term Memory and Gated Recurrent Units and compare these architectures in terms of their properties and their accuracy (Root Mean Square Error), as well as the complexity of the resulting models. We show that Gated Recurrent Unit models with a hidden layer size of 4 are able to accurately reproduce the system response to very different stimuli. We furthermore explore the relative importance of their inputs as well as scalability to more scenarios.},
	language = {en},
	number = {1},
	urldate = {2024-01-30},
	journal = {Scientific Reports},
	author = {Barbulescu, Ruxandra and Mestre, GonÃ§alo and Oliveira, Arlindo L. and Silveira, LuÃ­s Miguel},
	month = jan,
	year = {2023},
	keywords = {Computational neuroscience, Computational science, Computer science},
	pages = {467},
}

@article{varshney_structural_2011,
	title = {Structural {Properties} of the {Caenorhabditis} elegans {Neuronal} {Network}},
	volume = {7},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1001066},
	doi = {10.1371/journal.pcbi.1001066},
	abstract = {Despite recent interest in reconstructing neuronal networks, complete wiring diagrams on the level of individual synapses remain scarce and the insights into function they can provide remain unclear. Even for Caenorhabditis elegans, whose neuronal network is relatively small and stereotypical from animal to animal, published wiring diagrams are neither accurate nor complete and self-consistent. Using materials from White et al. and new electron micrographs we assemble whole, self-consistent gap junction and chemical synapse networks of hermaphrodite C. elegans. We propose a method to visualize the wiring diagram, which reflects network signal flow. We calculate statistical and topological properties of the network, such as degree distributions, synaptic multiplicities, and small-world properties, that help in understanding network signal propagation. We identify neurons that may play central roles in information processing, and network motifs that could serve as functional modules of the network. We explore propagation of neuronal activity in response to sensory or artificial stimulation using linear systems theory and find several activity patterns that could serve as substrates of previously described behaviors. Finally, we analyze the interaction between the gap junction and the chemical synapse networks. Since several statistical properties of the C. elegans network, such as multiplicity and motif distributions are similar to those found in mammalian neocortex, they likely point to general principles of neuronal networks. The wiring diagram reported here can help in understanding the mechanistic basis of behavior by generating predictions about future experiments involving genetic perturbations, laser ablations, or monitoring propagation of neuronal activity in response to stimulation.},
	language = {en},
	number = {2},
	urldate = {2024-01-30},
	journal = {PLOS Computational Biology},
	author = {Varshney, Lav R. and Chen, Beth L. and Paniagua, Eric and Hall, David H. and Chklovskii, Dmitri B.},
	month = feb,
	year = {2011},
	keywords = {Caenorhabditis elegans, Gap junctions, Interneurons, Motor neurons, Neural networks, Neurons, Synapses, Wiring diagrams},
	pages = {e1001066},
}

@book{carnevale_neuron_2006,
	address = {Cambridge},
	title = {The {NEURON} {Book}},
	isbn = {978-0-521-84321-8},
	url = {https://www.cambridge.org/core/books/neuron-book/7C8D9BD861D288E658BEB652F593F273},
	abstract = {The authoritative reference on NEURON, the simulation environment for modeling biological neurons and neural networks that enjoys wide use in the experimental and computational neuroscience communities. This book shows how to use NEURON to construct and apply empirically based models. Written primarily for neuroscience investigators, teachers, and students, it assumes no previous knowledge of computer programming or numerical methods. Readers with a background in the physical sciences or mathematics, who have some knowledge about brain cells and circuits and are interested in computational modeling, will also find it helpful. The NEURON Book covers material that ranges from the inner workings of this program, to practical considerations involved in specifying the anatomical and biophysical properties that are to be represented in models. It uses a problem-solving approach, with many working examples that readers can try for themselves.},
	urldate = {2024-01-30},
	publisher = {Cambridge University Press},
	author = {Carnevale, Nicholas T. and Hines, Michael L.},
	year = {2006},
	doi = {10.1017/CBO9780511541612},
}

@article{gourgou_caenorhabditis_2021,
	title = {Caenorhabditis elegans learning in a structured maze is a multisensory behavior},
	volume = {24},
	issn = {2589-0042},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8050377/},
	doi = {10.1016/j.isci.2021.102284},
	abstract = {We show that C.Â elegans nematodes learn to associate food with a combination of proprioceptive cues and information on the structure of their surroundings (maze), perceived through mechanosensation. By using the custom-made Worm-Maze platform, we demonstrate that C.Â elegans young adults can locate food in T-shaped mazes and, following that experience, learn to reach a specific maze arm. C.Â elegans learning inside the maze is possible after a single training session, it resembles working memory, and it prevails over conflicting environmental cues. We provide evidence that the observed learning is a food-triggered multisensory behavior, which requires mechanosensory and proprioceptive input, and utilizes cues about the structural features of nematodes' environment and their body actions. The CREB-like transcription factor and dopamine signaling are also involved in maze performance. Lastly, we show that the observed aging-driven decline of C.Â elegans learning ability in the maze can be reversed by starvation., 
          
            
              â¢
              C.Â elegans can be trained to reach a target arm in a T-shaped maze
            
            
              â¢
              Learning requires the contribution of tactile and proprioceptive cues
            
            
              â¢
              C.Â elegans follow a kind of response learning strategy in the maze environment
            
            
              â¢
              Learning is short-term and sensitive to distraction
            
          
        , Behavioral Neuroscience; Biological Sciences; Neuroscience},
	number = {4},
	urldate = {2024-01-29},
	journal = {iScience},
	author = {Gourgou, Eleni and Adiga, Kavya and Goettemoeller, Anne and Chen, Chieh and Hsu, Ao-Lin},
	month = mar,
	year = {2021},
	pmid = {33889812},
	pmcid = {PMC8050377},
	pages = {102284},
}

@misc{higuchi_high_2022,
	title = {High performance, large-scale multi-compartment {Hodgkin}-{Huxley} simulation of {Drosophila}âs whole-brain neural circuit model},
	copyright = {Â© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.01.512969v1},
	doi = {10.1101/2022.11.01.512969},
	abstract = {A major challenge in neurosciences is the elucidation of neural mechanisms in brains that are crucial for the processing of sensory information and the generation of adaptive behavior. In conjunction with the ever-growing body of experimental data, computational simulations have become crucial in integrating information and testing hypotheses, requiring fast large-scale simulators. We constructed a whole-brain neural circuit model of the fly Drosophila with biophysically detailed multi-compartment Hodgkin-Huxley models based on the morphologies of individual neurons published in open databases. Performance tuning of the simulator enabled near real-time simulation of the resting state of the Drosophila whole-brain model in the large-scale computational environment of the supercomputer Fugaku, for which we achieved in excess of 630 TFLOPS using 480k cores. In our whole-brain model, neural circuit dynamics related to a standard insect learning paradigm, the association of taste rewards with odors could be simulated.},
	language = {en},
	urldate = {2024-01-29},
	publisher = {bioRxiv},
	author = {Higuchi, Kaoruko and Kazawa, Tomoki and Sakai, Buntaro and Namiki, Shigehiro and Haupt, Stephan Shuichi and Kanzaki, Ryohei},
	month = nov,
	year = {2022},
}

@article{padamsey_imaging_2011,
	title = {Imaging synaptic plasticity},
	volume = {4},
	issn = {1756-6606},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3197482/},
	doi = {10.1186/1756-6606-4-36},
	abstract = {Over the past decade, the use and development of optical imaging techniques has advanced our understanding of synaptic plasticity by offering the spatial and temporal resolution necessary to examine long-term changes at individual synapses. Here, we review the use of these techniques in recent studies of synaptic plasticity and, in particular, long-term potentiation in the hippocampus.},
	urldate = {2024-01-28},
	journal = {Molecular Brain},
	author = {Padamsey, Zahid and Emptage, Nigel J},
	month = sep,
	year = {2011},
	pmid = {21958593},
	pmcid = {PMC3197482},
	pages = {36},
}

@article{feldman_spike-timing_2012,
	title = {The {Spike}-{Timing} {Dependence} of {Plasticity}},
	volume = {75},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627312007039},
	doi = {10.1016/j.neuron.2012.08.001},
	abstract = {Semantic Scholar extracted view of "The Spike-Timing Dependence of Plasticity" by D. Feldman},
	language = {en},
	number = {4},
	urldate = {2024-01-28},
	journal = {Neuron},
	author = {Feldman, DanielÂ E.},
	month = aug,
	year = {2012},
	pages = {556--571},
}

@article{markram_regulation_1997,
	title = {Regulation of synaptic efficacy by coincidence of postsynaptic {APs} and {EPSPs}},
	volume = {275},
	issn = {0036-8075},
	doi = {10.1126/science.275.5297.213},
	abstract = {Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials (APs) and unitary excitatory postsynaptic potentials (EPSPs) was found to induce changes in EPSPs. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic APs relative to EPSPs. These observations suggest that APs propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.},
	language = {eng},
	number = {5297},
	journal = {Science (New York, N.Y.)},
	author = {Markram, H. and LÃ¼bke, J. and Frotscher, M. and Sakmann, B.},
	month = jan,
	year = {1997},
	pmid = {8985014},
	keywords = {Action Potentials, Animals, Calcium, Cerebral Cortex, Dendrites, Down-Regulation, Electric Stimulation, In Vitro Techniques, Patch-Clamp Techniques, Pyramidal Cells, Rats, Rats, Wistar, Receptors, N-Methyl-D-Aspartate, Synapses, Synaptic Transmission, Time Factors, Up-Regulation},
	pages = {213--215},
}

@article{kandel_cellular_1968,
	title = {Cellular neurophysiological approaches in the study of learning.},
	volume = {48},
	issn = {0031-9333},
	url = {https://journals.physiology.org/doi/abs/10.1152/physrev.1968.48.1.65},
	doi = {10.1152/physrev.1968.48.1.65},
	number = {1},
	urldate = {2024-01-28},
	journal = {Physiological Reviews},
	author = {Kandel, E R and Spencer, W A},
	month = jan,
	year = {1968},
	pages = {65--134},
}

@article{kasai_structural_2010,
	title = {Structural dynamics of dendritic spines in memory and cognition},
	volume = {33},
	issn = {0166-2236, 1878-108X},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(10)00002-0},
	doi = {10.1016/j.tins.2010.01.001},
	language = {English},
	number = {3},
	urldate = {2024-01-28},
	journal = {Trends in Neurosciences},
	author = {Kasai, Haruo and Fukuda, Masahiro and Watanabe, Satoshi and Hayashi-Takagi, Akiko and Noguchi, Jun},
	month = mar,
	year = {2010},
	pmid = {20138375},
	pages = {121--129},
}

@book{hebb_organization_2002,
	address = {New York},
	title = {The {Organization} of {Behavior}: {A} {Neuropsychological} {Theory}},
	isbn = {978-1-4106-1240-3},
	shorttitle = {The {Organization} of {Behavior}},
	abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists--the inability to obtain one of the most cited publications in the field.   The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks.   D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development.   References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology--a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
	publisher = {Psychology Press},
	author = {Hebb, D. O.},
	month = may,
	year = {2002},
	doi = {10.4324/9781410612403},
}

@article{van_den_pol_neuropeptide_2012,
	title = {Neuropeptide {Transmission} in {Brain} {Circuits}},
	volume = {76},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627312008471},
	doi = {10.1016/j.neuron.2012.09.014},
	abstract = {Neuropeptides are found in many mammalian CNS neurons where they play key roles in modulating neuronal activity. In contrast to amino acid transmitter release at the synapse, neuropeptide release is not restricted to the synaptic specialization, and after release, a neuropeptide may diffuse some distance to exert its action through a G protein-coupled receptor. Some neuropeptides such as hypocretin/orexin are synthesized only in single regions of the brain, and the neurons releasing these peptides probably have similar functional roles. Other peptides such as neuropeptide Y (NPY) are synthesized throughout the brain, and neurons that synthesize the peptide in one region have no anatomical or functional connection with NPY neurons in other brain regions. Here, I review converging data revealing a complex interaction between slow-acting neuromodulator peptides and fast-acting amino acid transmitters in the control of energy homeostasis, drug addiction, mood and motivation, sleep-wake states, and neuroendocrine regulation.},
	number = {1},
	urldate = {2024-01-25},
	journal = {Neuron},
	author = {van den Pol, Anthony N.},
	month = oct,
	year = {2012},
	pages = {98--115},
}

@article{larhammar_evolution_1996,
	title = {Evolution of neuropeptide {Y}, peptide {YY} and pancreatic polypeptide},
	volume = {62},
	issn = {0167-0115},
	url = {https://www.sciencedirect.com/science/article/pii/0167011595001697},
	doi = {10.1016/0167-0115(95)00169-7},
	abstract = {The neuropeptide Y family of peptides consists of neuropeptide Y (NPY), which is expressed in the central and peripheral nervous systems, and peptide YY (PYY) and pancreatic polypeptide (PP) which are gut endocrine peptides. All three peptides are 36 amino acids long and act on G-protein-coupled receptors. NPY and PYY are present in all vertebrates, whereas PP probably arose as a copy of PYY in an early tetrapod ancestor. NPY is one of the most conserved peptides during evolution and no gnathostome (jawed) species differs from the ancestral gnathostome sequence at more than five positions. PYY is more variable, particularly in mammals which have nine differences to the gnathostome ancestor. PP may be the most rapidly evolving neuroendocrine peptide among tetrapods with only 50\% identity between mammals, birds, and amphibians. Ancestral gnathostome NPY and PYY seem to have differed at only four positions, suggesting that the gene duplication occurred shortly before the appearance of the gnathostomes. The two peptides differ from one another at 9â12 positions in tetrapod species and share at least two receptor subtypes in mammals. In bony and cartilaginous fishes, NPY and PYY have only 5â6 differences which, together with more extensive neuronal localization of PYY, indicate an even greater functional overlap between the two peptides in these animal groups. The emergence of sequence information for several receptor subtypes from various species will shed additional light on the evolution of the functions of the NPY-family peptides.},
	number = {1},
	urldate = {2024-01-24},
	journal = {Regulatory Peptides},
	author = {Larhammar, Dan},
	month = apr,
	year = {1996},
	keywords = {Gene duplication, Neuropeptide Y, Peptide YY},
	pages = {1--11},
}

@article{jiang_vgf_2018,
	title = {{VGF} function in depression and antidepressant efficacy},
	volume = {23},
	issn = {1359-4184},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5962361/},
	doi = {10.1038/mp.2017.233},
	abstract = {Brain-derived neurotrophic factor (BDNF) is a critical effector of depression-like behavior and antidepressant responses. Here, we show that VGF (non-acronymic), which is robustly regulated by BDNF/TrkB signaling, is downregulated in dorsal hippocampus (dHc) (male/female) and upregulated in nucleus accumbens (NAc) (male) in depressed human subjects and in mice subjected to chronic social defeat stress (CSDS). Adeno-associated virus (AAV)-Cre-mediated Vgf ablation in floxed VGF mice, in dHc or NAc, led to pro-depressant or antidepressant behaviors, respectively, while dHc or NAc AAV-VGF overexpression induced opposite outcomes. Mice with reduced VGF levels in the germline (Vgf+/â) or in dHc (AAV-Cre-injected floxed mice) showed increased susceptibility to CSDS and impaired responses to ketamine treatment in the forced swim test. Floxed mice with conditional pan-neuronal (Synapsin-Cre) but not those with forebrain (Î±CaMKII-Cre) Vgf ablation displayed increased susceptibility to subthreshold social defeat stress, suggesting that neuronal VGF, expressed in part in inhibitory interneurons, regulates depression-like behavior. Acute antibody-mediated sequestration of VGF-derived C-terminal peptides AQEE-30 and TLQP-62 in dHc induced pro-depressant effects. Conversely, dHc TLQP-62 infusion had rapid antidepressant efficacy, which was reduced in BDNF floxed mice injected in dHc with AAV-Cre, and in NBQX- and rapamycin-pretreated wildtype mice, these compounds blocking Î±-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor and mammalian target of rapamycin (mTOR) signaling, respectively. VGF is therefore a critical modulator of depression-like behaviors in dHc and NAc. In hippocampus, the antidepressant response to ketamine is associated with rapid VGF translation, is impaired by reduced VGF expression, and as previously reported, requires coincident, rapid BDNF translation and release.},
	number = {7},
	urldate = {2024-01-24},
	journal = {Molecular psychiatry},
	author = {Jiang, Cheng and Lin, Wei-Jye and Sadahiro, Masato and LabontÃ©, Benoit and Menard, Caroline and Pfau, Madeline L. and Tamminga, Carol A. and Turecki, Gustavo and Nestler, Eric J. and Russo, Scott J. and Salton, Stephen R.},
	month = jul,
	year = {2018},
	pmid = {29158577},
	pmcid = {PMC5962361},
	pages = {1632--1642},
}

@article{vollmer_neuropeptide_2016,
	title = {Neuropeptide {Y} {Impairs} {Retrieval} of {Extinguished} {Fear} and {Modulates} {Excitability} of {Neurons} in the {Infralimbic} {Prefrontal} {Cortex}},
	volume = {36},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6604823/},
	doi = {10.1523/JNEUROSCI.4955-13.2016},
	abstract = {Neuropeptide Y (NPY), a 36 aa peptide, regulates stress and emotional behaviors. Preclinical and clinical studies support an association of NPY with trauma-evoked syndromes such as posttraumatic stress disorder (PTSD), although the exact contribution of NPY is not clear. In the current study, we examined functional attributes of NPY in the infralimbic (IL) cortex, an area that regulates fear memories and is reported to be hypoactive in PTSD. Carriers of NPY gene polymorphism rs16147 have been reported to have elevated prefrontal NPY expression. Infusion of NPY into the IL cortex in rats significantly impaired fear extinction memory without affecting conditioned fear expression or acquisition of extinction. Neuroendocrine stress response, depression-like behavior, and working memory performance were not affected by NPY infusion into the IL. The NPY Y1 receptor antagonist BIBO3304 completely abolished NPY effects on fear extinction retrieval. Y1 receptor expression was localized on CaMKII-positive pyramidal projection neurons and GAD67-positive interneurons in the IL. Patch-clamp recordings revealed increased inhibitory synaptic transmission onto IL projection neurons in the presence of NPY. Thus, NPY dampens excitability of IL projection neurons and impairs retrieval of extinction memory by inhibiting consolidation of extinction. Of relevance to PTSD, elevation of prefrontal NPY attributable to the genetic polymorphism rs16147 may contribute to IL hypoactivity, resulting in impaired extinction memory and susceptibility to the disorder., SIGNIFICANCE STATEMENT Neuropeptide Y (NPY), a stress modulatory transmitter, is associated with posttraumatic stress disorder (PTSD). Contribution of NPY to PTSD symptomology is unclear. PTSD patients have reduced activity in the infralimbic (IL) subdivision of the medial prefrontal cortex (mPFC), associated with compromised extinction memory. No information exists on fear modulation by NPY in the IL cortex, although NPY and NPY receptors are abundant in these areas. This study shows that IL NPY inhibits consolidation of extinction, resulting in impaired retrieval of extinction memory and modulates excitability of IL projection neurons. In addition to providing a novel perspective on extinction memory modulation by NPY, our findings suggest that elevated mPFC NPY in gene polymorphism rs16147 carriers or after chronic stress could increase susceptibility to PTSD.},
	number = {4},
	urldate = {2024-01-24},
	journal = {The Journal of Neuroscience},
	author = {Vollmer, Lauren L. and Schmeltzer, Sarah and Schurdak, Jennifer and Ahlbrand, Rebecca and Rush, Jennifer and Dolgas, Charles M. and Baccei, Mark L. and Sah, Renu},
	month = jan,
	year = {2016},
	pmid = {26818517},
	pmcid = {PMC6604823},
	pages = {1306--1315},
}

@article{yan_mechanisms_2022,
	title = {Mechanisms of {Synaptic} {Transmission} {Dysregulation} in the {Prefrontal} {Cortex}: {Pathophysiological} {Implications}},
	volume = {27},
	issn = {1359-4184},
	shorttitle = {Mechanisms of {Synaptic} {Transmission} {Dysregulation} in the {Prefrontal} {Cortex}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8523584/},
	doi = {10.1038/s41380-021-01092-3},
	abstract = {Prefrontal cortex (PFC) serves as the chief executive officer of the brain, controlling the highest level cognitive and emotional processes. Its local circuits among glutamatergic principal neurons and GABAergic interneurons, as well as its long-range connections with other brain regions, have been functionally linked to specific behaviors, ranging from working memory to reward seeking. The efficacy of synaptic signaling in PFC network is profundedly influenced by monoaminergic inputs via the activation of dopamine, adrenergic or serotonin receptors. Stress hormones and neuropeptides also exert complex effects on the synaptic structure and function of PFC neurons. Dysregulation of PFC synaptic transmission is strongly linked to the social deficits, affective disturbance, and memory loss in brain disorders including autism, schizophrenia, depression and Alzheimerâs disease. Critical neural circuits, biological pathways, and molecular players that go awry in these mental illnesses have been revealed by integrated electrophysiological, optogenetic, biochemical, and transcriptomic studies of PFC. Novel epigenetic mechanism-based strategies are proposed as potential avenues of therapeutic intervention for PFC-involved diseases. This review provides an overview on PFC network organization and synaptic modulation, as well as the mechanisms linking PFC dysfunction to the pathophysiology of neurodevelopmental, neuropsychiatric and neurodegenerative diseases. Insights from the preclinical studies offer the potential for discovering new medical treatment of human patients with these brain disorders.},
	number = {1},
	urldate = {2024-01-24},
	journal = {Molecular psychiatry},
	author = {Yan, Zhen and Rein, Benjamin},
	month = jan,
	year = {2022},
	pmid = {33875802},
	pmcid = {PMC8523584},
	pages = {445--465},
}

@article{chen_is_2023,
	title = {Is {Learning} {Summary} {Statistics} {Necessary} for {Likelihood}-free {Inference}?},
	url = {https://openreview.net/forum?id=jjzJ768iV1},
	abstract = {Likelihood-free inference (LFI) is a set of techniques for inference in implicit statistical models. A longstanding question in LFI has been how to design or learn good summary statistics of data, but this might now seem unnecessary due to the advent of recent end-to-end (i.e. neural network-based) LFI methods. In this work, we rethink this question with a new method for learning summary statistics. We show that learning sufficient statistics may be easier than direct posterior inference, as the former problem can be reduced to a set of low-dimensional, easy-to-solve learning problems. This suggests us to explicitly decouple summary statistics learning from posterior inference in LFI. Experiments on diverse inference tasks with different data types validate our hypothesis.},
	language = {en},
	urldate = {2024-01-22},
	author = {Chen, Yanzhi and Gutmann, Michael U. and Weller, Adrian},
	month = jun,
	year = {2023},
}

@article{marjoram_approximation_2013,
	title = {Approximation {Bayesian} {Computation}},
	volume = {1},
	issn = {2054-197X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4297650/},
	abstract = {Approximation Bayesian computation [ABC] is an analysis approach that has arisen in response to the recent trend to collect data that is of a magnitude far higher than has been historically the case. This has led to many existing methods become intractable because of difficulties in calculating the likelihood function. ABC circumvents this issue by replacing calculation of the likelihood with a simulation step in which it is estimated in one way or another. In this review we give an overview of the ABC approach, giving examples of some of the more popular specific forms of ABC. We then discuss some of the areas of most active research and application in the field, specifically, choice of low-dimensional summaries of complex datasets and metrics for measuring similarity between observed and simulated data. Next, we consider the question of how to do model selection in an ABC context. Finally, we discuss an area of growing prominence in the ABC world, use of ABC methods in genetic pathway inference.},
	number = {3},
	urldate = {2024-01-20},
	journal = {OA genetics},
	author = {Marjoram, Paul},
	month = may,
	year = {2013},
	pmid = {25606346},
	pmcid = {PMC4297650},
	pages = {853},
}

@incollection{riddle_introduction_1997,
	title = {Introduction},
	url = {https://www.ncbi.nlm.nih.gov/books/NBK20180/},
	abstract = {Intercellular communication between neurons and their target cells is primarily accomplished by the regulated release of neurotransmitters at synapses. Extensive study of this vesicular-mediated process has revealed that many of the molecular components of the release apparatus are highly conserved in metazoans (Bennett and Scheller 1994; SÃ¼dhof 1995). In vertebrates, biochemical approaches have led to the identification of components that participate in the release process, whereas the study of synaptic transmission in Caenorhabditis and Drosophila has proceeded in large part using genetic approaches. Each of these approaches has yielded insights. An extensive array of biochemically defined protein-protein interactions has provided the framework for building mechanistic models of neurotransmitter release. The genetic approach has identified novel components that eluded biochemical characterization and has provided functional data to refine the models. Clearly, a complete analysis at the functional level will require the use of electrophysiological techniques, and although such methods are becoming available for C. elegans (Avery et al. 1995b; Avery and Thomas, this volume), they are more advanced in other organisms. In concert, the study of synaptic transmission in C. elegans, Drosophila, and vertebrates has begun to provide an outline of the molecular details of synaptic vesicle function and neurotransmitter release.},
	language = {en},
	urldate = {2024-01-17},
	booktitle = {C. elegans {II}. 2nd edition},
	publisher = {Cold Spring Harbor Laboratory Press},
	author = {Riddle, Donald L. and Blumenthal, Thomas and Meyer, Barbara J. and Priess, James R.},
	year = {1997},
}

@article{kusmierz_learning_2017,
	series = {Computational {Neuroscience}},
	title = {Learning with three factors: modulating {Hebbian} plasticity with errors},
	volume = {46},
	issn = {0959-4388},
	shorttitle = {Learning with three factors},
	url = {https://www.sciencedirect.com/science/article/pii/S0959438817300612},
	doi = {10.1016/j.conb.2017.08.020},
	abstract = {Synaptic plasticity is a central theme in neuroscience. A framework of three-factor learning rules provides a powerful abstraction, helping to navigate through the abundance of models of synaptic plasticity. It is well-known that the dopamine modulation of learning is related to reward, but theoretical models predict other functional roles of the modulatory third factor; it may encode errors for supervised learning, summary statistics of the population activity for unsupervised learning or attentional feedback. Specialized structures may be needed in order to generate and propagate third factors in the neural network.},
	urldate = {2024-01-16},
	journal = {Current Opinion in Neurobiology},
	author = {KuÅmierz, Åukasz and Isomura, Takuya and Toyoizumi, Taro},
	month = oct,
	year = {2017},
	pages = {170--177},
}

@article{skoulakis_dunces_2006,
	title = {Dunces and da {Vincis}: the genetics of learning and memory in {Drosophila}},
	volume = {63},
	issn = {1420-682X},
	shorttitle = {Dunces and da {Vincis}},
	doi = {10.1007/s00018-006-6023-9},
	abstract = {Progress towards amelioration and eventual cure of human cognitive disorders requires understanding the molecular signaling mechanisms that normally govern learning and memory. The fly Drosophila melanogaster has been instrumental in the identification of molecules and signaling pathways essential for learning and memory, because genetic screens have produced mutants in these processes and the system facilitates integrated genetic, molecular, histological and behavioral analyses. We discuss the behavioral paradigms available to assess associative learning and memory in the fly, the contributions learning and memory mutants have made to our understanding of the molecular mechanisms that govern learning and memory, and predictions stemming from the nature of the affected genes. Furthermore, we consider the multiple well-established behavioral assays available and the powerful molecular genetics of the fly with regard to development of models of human cognitive disorders and their pharmacological treatment.},
	language = {eng},
	number = {9},
	journal = {Cellular and molecular life sciences: CMLS},
	author = {Skoulakis, E. M. C. and Grammenoudi, S.},
	month = may,
	year = {2006},
	pmid = {16596334},
	keywords = {Animals, Cyclic AMP, Disease Models, Animal, Drosophila melanogaster, Learning, Learning Disabilities, Memory, Memory Disorders, Mutant Proteins, Signal Transduction},
	pages = {975--988},
}

@article{groschner_biophysical_2022,
	title = {A biophysical account of multiplication by a single neuron},
	volume = {603},
	copyright = {2022 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-04428-3},
	doi = {10.1038/s41586-022-04428-3},
	abstract = {Nonlinear, multiplication-like operations carried out by individual nerve cells greatly enhance the computational power of a neural system1â3, but our understanding of their biophysical implementation is scant. Here we pursue this problem in the Drosophila melanogaster ON motion vision circuit4,5, in which we record the membrane potentials of direction-selective T4 neurons and of their columnar input elements6,7 in response to visual and pharmacological stimuli in vivo. Our electrophysiological measurements and conductance-based simulations provide evidence for a passive supralinear interaction between two distinct types of synapse on T4 dendrites. We show that this multiplication-like nonlinearity arises from the coincidence of cholinergic excitation and release from glutamatergic inhibition. The latter depends on the expression of the glutamate-gated chloride channel GluClÎ±8,9 in T4 neurons, which sharpens the directional tuning of the cells and shapes the optomotor behaviour of the animals. Interacting pairs of shunting inhibitory and excitatory synapses have long been postulated as an analogue approximation of a multiplication, which is integral to theories of motion detection10,11, sound localization12 and sensorimotor control13.},
	language = {en},
	number = {7899},
	urldate = {2024-01-16},
	journal = {Nature},
	author = {Groschner, Lukas N. and Malis, Jonatan G. and Zuidinga, Birte and Borst, Alexander},
	month = mar,
	year = {2022},
	keywords = {Biophysical models, Motion detection, Neural circuits},
	pages = {119--123},
}

@article{oesterle_bayesian_2020,
	title = {Bayesian inference for biophysical neuron models enables stimulus optimization for retinal neuroprosthetics},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.54997},
	doi = {10.7554/eLife.54997},
	abstract = {While multicompartment models have long been used to study the biophysics of neurons, it is still challenging to infer the parameters of such models from data including uncertainty estimates. Here, we performed Bayesian inference for the parameters of detailed neuron models of a photoreceptor and an OFF- and an ON-cone bipolar cell from the mouse retina based on two-photon imaging data. We obtained multivariate posterior distributions specifying plausible parameter ranges consistent with the data and allowing to identify parameters poorly constrained by the data. To demonstrate the potential of such mechanistic data-driven neuron models, we created a simulation environment for external electrical stimulation of the retina and optimized stimulus waveforms to target OFF- and ON-cone bipolar cells, a current major problem of retinal neuroprosthetics.},
	urldate = {2024-01-16},
	journal = {eLife},
	author = {Oesterle, Jonathan and Behrens, Christian and SchrÃ¶der, Cornelius and Hermann, Thoralf and Euler, Thomas and Franke, Katrin and Smith, Robert G and Zeck, GÃ¼nther and Berens, Philipp},
	editor = {Borst, Alexander and Huguenard, John R and Borst, Alexander and Fairhall, Adrienne L},
	month = oct,
	year = {2020},
	keywords = {bayesian inference, biophysical model, bipolar cell, neuroprosthetics, retina, two-photon imaging},
	pages = {e54997},
}

@article{alsing_massive_2018,
	title = {Massive optimal data compression and density estimation for scalable, likelihood-free inference in cosmology},
	volume = {477},
	issn = {0035-8711},
	url = {https://doi.org/10.1093/mnras/sty819},
	doi = {10.1093/mnras/sty819},
	abstract = {Many statistical models in cosmology can be simulated forwards but have intractable likelihood functions. Likelihood-free inference methods allow us to perform Bayesian inference from these models using only forward simulations, free from any likelihood assumptions or approximations. Likelihood-free inference generically involves simulating mock data and comparing to the observed data; this comparison in data space suffers from the curse of dimensionality and requires compression of the data to a small number of summary statistics to be tractable. In this paper, we use massive asymptotically optimal data compression to reduce the dimensionality of the data space to just one number per parameter, providing a natural and optimal framework for summary statistic choice for likelihood-free inference. Secondly, we present the first cosmological application of Density Estimation Likelihood-Free Inference (delfi), which learns a parametrized model for joint distribution of data and parameters, yielding both the parameter posterior and the model evidence. This approach is conceptually simple, requires less tuning than traditional Approximate Bayesian Computation approaches to likelihood-free inference and can give high-fidelity posteriors from orders of magnitude fewer forward simulations. As an additional bonus, it enables parameter inference and Bayesian model comparison simultaneously. We demonstrate delfi with massive data compression on an analysis of the joint light-curve analysis supernova data, as a simple validation case study. We show that high-fidelity posterior inference is possible for full-scale cosmological data analyses with as few as â¼104 simulations, with substantial scope for further improvement, demonstrating the scalability of likelihood-free inference to large and complex cosmological data sets.},
	number = {3},
	urldate = {2024-01-16},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Alsing, Justin and Wandelt, Benjamin and Feeney, Stephen},
	month = jul,
	year = {2018},
	pages = {2874--2885},
}

@misc{marlier_simulation-based_2021,
	title = {Simulation-based {Bayesian} inference for multi-fingered robotic grasping},
	url = {http://arxiv.org/abs/2109.14275},
	doi = {10.48550/arXiv.2109.14275},
	abstract = {Multi-fingered robotic grasping is an undeniable stepping stone to universal picking and dexterous manipulation. Yet, multi-fingered grippers remain challenging to control because of their rich nonsmooth contact dynamics or because of sensor noise. In this work, we aim to plan hand configurations by performing Bayesian posterior inference through the full stochastic forward simulation of the robot in its environment, hence robustly accounting for many of the uncertainties in the system. While previous methods either relied on simplified surrogates of the likelihood function or attempted to learn to directly predict maximum likelihood estimates, we bring a novel simulation-based approach for full Bayesian inference based on a deep neural network surrogate of the likelihood-to-evidence ratio. Hand configurations are found by directly optimizing through the resulting amortized and differentiable expression for the posterior. The geometry of the configuration space is accounted for by proposing a Riemannian manifold optimization procedure through the neural posterior. Simulation and physical benchmarks demonstrate the high success rate of the procedure.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Marlier, Norman and BrÃ¼ls, Olivier and Louppe, Gilles},
	month = sep,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{schroder_simultaneous_2023,
	title = {Simultaneous identification of models and parameters of scientific simulators},
	url = {http://arxiv.org/abs/2305.15174},
	doi = {10.48550/arXiv.2305.15174},
	abstract = {Many scientific models are composed of multiple discrete components, and scien tists often make heuristic decisions about which components to include. Bayesian inference provides a mathematical framework for systematically selecting model components, but defining prior distributions over model components and developing associated inference schemes has been challenging. We approach this problem in an amortized simulation-based inference framework: We define implicit model priors over a fixed set of candidate components and train neural networks to infer joint probability distributions over both, model components and associated parameters from simulations. To represent distributions over model components, we introduce a conditional mixture of multivariate binary distributions in the Grassmann formalism. Our approach can be applied to any compositional stochastic simulator without requiring access to likelihood evaluations. We first illustrate our method on a simple time series model with redundant components and show that it can retrieve joint posterior distribution over a set of symbolic expressions and their parameters while accurately capturing redundancy with strongly correlated posteriors. We then apply our approach to drift-diffusion models, a commonly used model class in cognitive neuroscience. After validating the method on synthetic data, we show that our approach explains experimental data as well as previous methods, but that our fully probabilistic approach can help to discover multiple data-consistent model configurations, as well as reveal non-identifiable model components and parameters. Our method provides a powerful tool for data-driven scientific inquiry which will allow scientists to systematically identify essential model components and make uncertainty-informed modelling decisions.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {SchrÃ¶der, Cornelius and Macke, Jakob H.},
	month = may,
	year = {2023},
	keywords = {Computer Science - Machine Learning},
}

@article{furlong_modelling_2023,
	title = {Modelling neural probabilistic computation using vector symbolic architectures},
	issn = {1871-4099},
	url = {https://doi.org/10.1007/s11571-023-10031-7},
	doi = {10.1007/s11571-023-10031-7},
	abstract = {Distributed vector representations are a key bridging point between connectionist and symbolic representations in cognition. It is unclear how uncertainty should be modelled in systems using such representations. In this paper we discuss how bundles of symbols in certain Vector Symbolic Architectures (VSAs) can be understood as defining an object that has a relationship to a probability distribution, and how statements in VSAs can be understood as being analogous to probabilistic statements. The aim of this paper is to show how (spiking) neural implementations of VSAs can be used to implement probabilistic operations that are useful in building cognitive models. We show how similarity operators between continuous values represented as Spatial Semantic Pointers (SSPs), an example of a technique known as fractional binding, induces a quasi-kernel function that can be used in density estimation. Further, we sketch novel designs for networks that compute entropy and mutual information of VSA-represented distributions and demonstrate their performance when implemented as networks of spiking neurons. We also discuss the relationship between our technique and quantum probability, another technique proposed for modelling uncertainty in cognition. While we restrict ourselves to operators proposed for Holographic Reduced Representations, and for representing real-valued data. We suggest that the methods presented in this paper should translate to any VSA where the dot product between fractionally bound symbols induces a valid kernel.},
	language = {en},
	urldate = {2024-01-15},
	journal = {Cognitive Neurodynamics},
	author = {Furlong, P. Michael and Eliasmith, Chris},
	month = dec,
	year = {2023},
	keywords = {Bayesian modelling, Fractional binding, Probability, Spatial semantic pointers, Vector symbolic architecture, hyperdimensional computing HD/ASV},
}

@article{white_structure_1997,
	title = {The structure of the nervous system of the nematode {Caenorhabditis} elegans},
	volume = {314},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1986.0056},
	doi = {10.1098/rstb.1986.0056},
	abstract = {The structure and connectivity of the nervous system of the nematode Caenorhabditis elegans has been deduced from reconstructions of electron micrographs of serial sections. The hermaphrodite nervous system has a total complement of 302 neurons, which are arranged in an essentially invariant structure. Neurons with similar morphologies and connectivities have been grouped together into classes; there are 118 such classes. Neurons have simple morphologies with few, if any, branches. Processes from neurons run in defined positions within bundles of parallel processes, synaptic connections being made en passant. Process bundles are arranged longitudinally and circumferentially and are often adjacent to ridges of hypodermis. Neurons are generally highly locally connected, making synaptic connections with many of their neighbours. Muscle cells have arms that run out to process bundles containing motoneuron axons. Here they receive their synaptic input in defined regions along the surface of the bundles, where motoneuron axons reside. Most of the m orphologically identifiable synaptic connections in a typical animal are described. These consist of about 5000 chemical synapses, 2000 neuromuscular junctions and 600 gap junctions.},
	number = {1165},
	urldate = {2024-01-13},
	journal = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
	author = {White, John Graham and Southgate, Eileen and Thomson, J. N. and Brenner, Sydney},
	month = jan,
	year = {1997},
	pages = {1--340},
}

@article{cook_whole-animal_2019,
	title = {Whole-animal connectomes of both {Caenorhabditis} elegans sexes},
	volume = {571},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-019-1352-7},
	doi = {10.1038/s41586-019-1352-7},
	abstract = {Knowledge of connectivity in the nervous system is essential to understanding its function. Here we describe connectomes for both adult sexes of the nematode Caenorhabditis elegans, an important model organism for neuroscience research. We present quantitative connectivity matrices that encompass all connections from sensory input to end-organ output across the entire animal, information that is necessary to model behaviour. Serial electron microscopy reconstructions that are based on the analysis of both new and previously published electron micrographs update previous results and include data on the male head. The nervous system differs between sexes at multiple levels. Several sex-shared neurons that function in circuits for sexual behaviour are sexually dimorphic in structure and connectivity. Inputs from sex-specific circuitry to central circuitry reveal points at which sexual and non-sexual pathways converge. In sex-shared central pathways, a substantial number of connections differ in strength between the sexes.Â Quantitative connectomes that includeÂ all connections serveÂ as the basis for understanding how complex, adaptive behavior is generated.},
	language = {en},
	number = {7763},
	urldate = {2024-01-13},
	journal = {Nature},
	author = {Cook, Steven J. and Jarrell, Travis A. and Brittin, Christopher A. and Wang, Yi and Bloniarz, Adam E. and Yakovlev, Maksim A. and Nguyen, Ken C. Q. and Tang, Leo T.-H. and Bayer, Emily A. and Duerr, Janet S. and BÃ¼low, Hannes E. and Hobert, Oliver and Hall, David H. and Emmons, Scott W.},
	month = jul,
	year = {2019},
	keywords = {Network models, Neural circuits, Sexual dimorphism},
	pages = {63--71},
}

@article{ripoll-sanchez_neuropeptidergic_2023,
	title = {The neuropeptidergic connectome of {C}. elegans},
	volume = {111},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(23)00756-0},
	doi = {10.1016/j.neuron.2023.09.043},
	language = {English},
	number = {22},
	urldate = {2024-01-12},
	journal = {Neuron},
	author = {Ripoll-SÃ¡nchez, Lidia and Watteyne, Jan and Sun, HaoSheng and Fernandez, Robert and Taylor, Seth R. and Weinreb, Alexis and Bentley, Barry L. and Hammarlund, Marc and Miller, David M. and Hobert, Oliver and Beets, Isabel and VÃ©rtes, Petra E. and Schafer, William R.},
	month = nov,
	year = {2023},
	pmid = {37935195},
	keywords = {C. elegans, connectomics, networks, neuromodulation, neuropeptides},
	pages = {3570--3589.e5},
}


@article{li_neuropeptides_2008,
	title = {Neuropeptides},
	issn = {15518507},
	url = {http://www.wormbook.org/chapters/www_neuropeptides/neuropeptides.html},
	doi = {10.1895/wormbook.1.142.1},
	urldate = {2024-01-09},
	journal = {WormBook},
	author = {Li, Chris},
	month = sep,
	year = {2008},
	pages = {1--36},
}

@article{argiolas_neuropeptides_2013,
	title = {Neuropeptides and central control of sexual behaviour from the past to the present: a review},
	volume = {108},
	issn = {1873-5118},
	shorttitle = {Neuropeptides and central control of sexual behaviour from the past to the present},
	doi = {10.1016/j.pneurobio.2013.06.006},
	abstract = {Of the numerous neuropeptides identified in the central nervous system, only a few are involved in the control of sexual behaviour. Among these, the most studied are oxytocin, adrenocorticotropin, Î±-melanocyte stimulating hormone and opioid peptides. While opioid peptides inhibit sexual performance, the others facilitate sexual behaviour in most of the species studied so far (rats, mice, monkeys and humans). However, evidence for a sexual role of gonadotropin-releasing hormone, corticotropin releasing factor, neuropeptide Y, galanin and galanin-like peptide, cholecystokinin, substance P, vasoactive intestinal peptide, vasopressin, angiotensin II, hypocretins/orexins and VGF-derived peptides are also available. Corticotropin releasing factor, neuropeptide Y, cholecystokinin, vasopressin and angiotensin II inhibit, while substance P, vasoactive intestinal peptide, hypocretins/orexins and some VGF-derived peptide facilitate sexual behaviour. Neuropeptides influence sexual behaviour by acting mainly in the hypothalamic nuclei (i.e., lateral hypothalamus, paraventricular nucleus, ventromedial nucleus, arcuate nucleus), in the medial preoptic area and in the spinal cord. However, it is often unclear whether neuropeptides influence the anticipatory phase (sexual arousal and/or motivation) or the consummatory phase (performance) of sexual behaviour, except in a few cases (e.g., opioid peptides and oxytocin). Unfortunately, scarce information has been added in the last 15 years on the neural mechanisms by which neuropeptides influence sexual behaviour, most studied neuropeptides apart. This may be due to a decreased interest of researchers on neuropeptides and sexual behaviour or on sexual behaviour in general. Such a decrease may be related to the discovery of orally effective, locally acting type V phosphodiesterase inhibitors for the therapy of erectile dysfunction.},
	language = {eng},
	journal = {Progress in Neurobiology},
	author = {Argiolas, Antonio and Melis, Maria Rosaria},
	month = sep,
	year = {2013},
	pmid = {23851261},
	keywords = {ACTH, ACTH-MSH peptides, Adrenocorticotropic Hormone, Animals, CRF, Endorphins, GABA, GALP, GH, GnRH/LHRH, Gonadotropin-Releasing Hormone, Humans, LH, MSH, Melanocyte-Stimulating Hormones, NO, Neuropeptides, Opioid peptides, Oxytocin, PVN, Penile erection, SO, Sexual Behavior, Sexual Behavior, Animal, Sexual behaviour, VIP, adrenocorticotropin, corticotropin releasing factor, galanin-like peptide, gamma-amminobutirric acid, gonadotropin releasing hormone, growth hormone, luteinizing hormone, melanocyte stimulating hormone, nitric oxide, paraventricular nucleus of the hypothalamus, supraoptic nucleus, vasoactive intestinal peptide},
	pages = {80--107},
}

@article{zager_neuropeptides_1985,
	title = {Neuropeptides in human memory and learning processes},
	volume = {17},
	issn = {0148-396X},
	doi = {10.1227/00006123-198508000-00023},
	abstract = {The neuropeptides vasopressin, adrenocorticotropin (ACTH), and beta-endorphin seem to have important effects on memory and learning. Animal studies attempting to demonstrate these effects are difficult to interpret because of the complexity of behavior that is described as "learning" and the impossibility of assessing verbal learning in animals. This article therefore reviews some of the animal literature on neuropeptides and learning, but focuses primarily upon studies in humans, both in normal volunteers and in patients with neurological disorders. Vasopressin enhances learning under some conditions. Intranasal administration has been associated with improvement on psychometric tests in patients with mild Alzheimer's disease and Korsakoff's psychosis, although these findings are not uniform. It improves performance on memory tests in normal volunteers, but does not seem to improve the memory deficit after head trauma. Cerebrospinal fluid levels are low in patients with Alzheimer's disease. ACTH and melanocyte-stimulating hormone (MSH) are two peptides the primary behavioral effect of which seems to be on attention or goal-motivated behavior rather than on memory processes themselves. Visual discrimination and the ability to continue repetitive tasks are enhanced; in mentally retarded subjects, the administration of ACTH or MSH improves performance on a variety of neuropsychological tests. It does not, however, improve cognitive function in the elderly. Endogenous opioids including beta-endorphin and met-enkephalin seem to have primarily an amnesic effect in animal studies. Their role in human learning is still uncertain, although naloxone, which antagonizes their effects, has been associated with improved cognitive performance in patients with Alzheimer's disease. These data underscore the complexity of the processes associated with human memory and the rudimentary state of our present knowledge. Whatever the mechanisms, however, vasopressin, ACTH, and endogenous opioids seem to have important effects upon memory.},
	language = {eng},
	number = {2},
	journal = {Neurosurgery},
	author = {Zager, E. L. and Black, P. M.},
	month = aug,
	year = {1985},
	pmid = {2993944},
	keywords = {Adrenocorticotropic Hormone, Animals, Endorphins, Forecasting, Humans, Learning, Melanocyte-Stimulating Hormones, Memory, Memory Disorders, Nerve Tissue Proteins, Nervous System Diseases, Oxytocin, Pituitary Gland, Posterior, Vasopressins},
	pages = {355--369},
}

@misc{noauthor_fwoweb_nodate,
	title = {{FwoWeb}},
	url = {https://fwoweb.fwo.be/researcher/application-forms/122083?form-tab=project},
	urldate = {2023-12-25},
}

@inproceedings{papamakarios_masked_2017,
	title = {Masked {Autoregressive} {Flow} for {Density} {Estimation}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/6c1da886822c67822bcf3679d04369fa-Abstract.html},
	abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
	urldate = {2023-12-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	year = {2017},
}

@misc{winkler_learning_2023,
	title = {Learning {Likelihoods} with {Conditional} {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1912.00042},
	abstract = {Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y{\textbar}x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
	urldate = {2023-12-14},
	publisher = {arXiv},
	author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gerkin_towards_2018,
	title = {Towards systematic, data-driven validation of a collaborative, multi-scale model of {Caenorhabditis} elegans},
	volume = {373},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2017.0381},
	doi = {10.1098/rstb.2017.0381},
	abstract = {The OpenWorm Project is an international open-source collaboration to create a multi-scale model of the organism Caenorhabditis elegans. At each scale, including subcellular, cellular, network and behaviour, this project employs one or more computational models that aim to recapitulate the corresponding biological system at that scale. This requires that the simulated behaviour of each model be compared with experimental data both as the model is continuously refined and as new experimental data become available. Here we report the use of SciUnit, a software framework for model validation, to attempt to achieve these goals. During project development, each model is continuously subjected to data-driven âunit testsâ that quantitatively summarize model-data agreement, identifying modelling progress and highlighting particular aspects of each model that fail to adequately reproduce known features of the biological organism and its components. This workflow is publicly visible via both GitHub and a web application and accepts community contributions to ensure that modelling goals are transparent and well-informed.

This article is part of a discussion meeting issue âConnectome to behaviour: modelling C. elegans at cellular resolutionâ.},
	number = {1758},
	urldate = {2023-12-12},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Gerkin, Richard C. and Jarvis, Russell J. and Crook, Sharon M.},
	month = sep,
	year = {2018},
	keywords = {Python, informatics, modelling, unit-testing},
	pages = {20170381},
}

@misc{noauthor_credits_nodate,
	title = {Credits - sbi},
	url = {https://sbi-dev.github.io/sbi/credits/},
	urldate = {2023-12-11},
}

@misc{cui_multilevel_2023,
	title = {Multilevel {Dimension}-{Independent} {Likelihood}-{Informed} {MCMC} for {Large}-{Scale} {Inverse} {Problems}},
	url = {http://arxiv.org/abs/1910.12431},
	doi = {10.48550/arXiv.1910.12431},
	abstract = {We present a non-trivial integration of dimension-independent likelihood-informed (DILI) MCMC (Cui, Law, Marzouk, 2016) and the multilevel MCMC (Dodwell et al., 2015) to explore the hierarchy of posterior distributions. This integration offers several advantages: First, DILI-MCMC employs an intrinsic likelihood-informed subspace (LIS) (Cui et al., 2014) -- which involves a number of forward and adjoint model simulations -- to design accelerated operator-weighted proposals. By exploiting the multilevel structure of the discretised parameters and discretised forward models, we design a Rayleigh-Ritz procedure to significantly reduce the computational effort in building the LIS and operating with DILI proposals. Second, the resulting DILI-MCMC can drastically improve the sampling efficiency of MCMC at each level, and hence reduce the integration error of the multilevel algorithm for fixed CPU time. Numerical results confirm the improved computational efficiency of the multilevel DILI approach.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Cui, Tiangang and Detommaso, Gianluca and Scheichl, Robert},
	month = nov,
	year = {2023},
	keywords = {Mathematics - Numerical Analysis, Statistics - Computation, Statistics - Methodology},
}

@book{ferrand_non-synaptic_2023,
	title = {Non-synaptic plasticity enables memory-dependent local learning},
	abstract = {Synaptic plasticity is essential for memory formation and learning in the brain. In addition, recent results indicate that non-synaptic plasticity processes such as the regulation of neural membrane properties contribute to memory formation, its functional role in memory and learning has however remained elusive. Here, we propose that non-synaptic and synaptic plasticity are both essential components to enable memory-dependent processing in neuronal networks. While the former acts on a fast time scale for rapid information storage, the latter shapes network processing on a slower time scale to harness this memory as a functional component. We analyse this concept in a network model where pyramidal neurons regulate their apical trunk excitability in a Hebbian manner. We find that local synaptic plasticity rules can be derived for this model and show that the interplay between this synaptic plasticity and the non-synaptic trunk plasticity enables the model to successfully accommodate memory-dependent processing capabilities in a number of tasks, ranging from simple memory tests to question answering. The model can also explain contextual fear conditioning experiments, where freezing responses could be recovered by optogenetic reactivation of memory engrams under amnesia.

Author summary
How memory is organized in the brain in order to enable cognitive processing is a central open question in systems neuroscience. Traditionally, synaptic plasticity is considered the key mechanism for the establishment of memory in the brain. Recently however, this view has been questioned, and it was proposed that non-synaptic plasticity mechanisms play a more prominent role as previously considered. In this article, we propose that both, synaptic and non-synaptic plasticity are central components for the formation and utilization of memory in biological neuronal networks. Our results show that non-synaptic plasticity can act on a fast time-scale to store important information, while synaptic plasticity can adapt network function on a slow time scale in order to facilitate memory-dependent cognitive processing.},
	author = {Ferrand, Romain and Baronig, Maximilian and Unger, Florian and Legenstein, Robert},
	month = nov,
	year = {2023},
	doi = {10.1101/2023.11.14.567001},
}

@misc{noauthor_non-synaptic_nodate,
	title = {Non-synaptic plasticity enables memory-dependent local learning {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2023.11.14.567001v1.full},
	urldate = {2023-12-11},
}

@misc{miller_contrastive_2023,
	title = {Contrastive {Neural} {Ratio} {Estimation}},
	url = {http://arxiv.org/abs/2210.06170},
	abstract = {Likelihood-to-evidence ratio estimation is usually cast as either a binary (NRE-A) or a multiclass (NRE-B) classification task. In contrast to the binary classification framework, the current formulation of the multiclass version has an intrinsic and unknown bias term, making otherwise informative diagnostics unreliable. We propose a multiclass framework free from the bias inherent to NRE-B at optimum, leaving us in the position to run diagnostics that practitioners depend on. It also recovers NRE-A in one corner case and NRE-B in the limiting case. For fair comparison, we benchmark the behavior of all algorithms in both familiar and novel training regimes: when jointly drawn data is unlimited, when data is fixed but prior draws are unlimited, and in the commonplace fixed data and parameters setting. Our investigations reveal that the highest performing models are distant from the competitors (NRE-A, NRE-B) in hyperparameter space. We make a recommendation for hyperparameters distinct from the previous models. We suggest a bound on the mutual information as a performance metric for simulation-based inference methods, without the need for posterior samples, and provide experimental results.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {Miller, Benjamin Kurt and Weniger, Christoph and ForrÃ©, Patrick},
	month = jan,
	year = {2023},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, High Energy Physics - Phenomenology, Statistics - Machine Learning},
}

@inproceedings{hermans_likelihood-free_2020,
	title = {Likelihood-free {MCMC} with {Amortized} {Approximate} {Ratio} {Estimators}},
	url = {https://proceedings.mlr.press/v119/hermans20a.html},
	abstract = {Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to rely on approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in {\textbackslash}textsc\{mcmc\} samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicability.},
	language = {en},
	urldate = {2023-12-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
	month = nov,
	year = {2020},
	pages = {4239--4248},
}

@misc{glockler_variational_2022,
	title = {Variational methods for simulation-based inference},
	url = {http://arxiv.org/abs/2203.04176},
	doi = {10.48550/arXiv.2203.04176},
	abstract = {We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or likelihood-ratio-estimation) with variational inference to achieve a scalable simulation-based inference approach. SNVI maintains the flexibility of likelihood(-ratio) estimation to allow arbitrary proposals for simulations, while simultaneously providing a functional estimate of the posterior distribution without requiring MCMC sampling. We present several variants of SNVI and demonstrate that they are substantially more computationally efficient than previous algorithms, without loss of accuracy on benchmark tasks. We apply SNVI to a neuroscience model of the pyloric network in the crab and demonstrate that it can infer the posterior distribution with one order of magnitude fewer simulations than previously reported. SNVI vastly reduces the computational cost of simulation-based inference while maintaining accuracy and flexibility, making it possible to tackle problems that were previously inaccessible.},
	urldate = {2023-12-11},
	publisher = {arXiv},
	author = {GlÃ¶ckler, Manuel and Deistler, Michael and Macke, Jakob H.},
	month = oct,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yalcinkaya_personalized_2023,
	title = {Personalized virtual brains of {Alzheimer}âs {Disease} link dynamical biomarkers of {fMRI} with increased local excitability},
	copyright = {Â© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2023.01.11.23284438v1},
	doi = {10.1101/2023.01.11.23284438},
	abstract = {Alzheimerâs disease (AD) is a neurodegenerative disorder characterized by the accumulation of abnormal beta-amyloid (AÎ²) and hyperphosphorylated Tau (pTau). These proteinopathies disrupt neuronal activity, causing, among others, an excessive and hypersynchronous neuronal firing that promotes hyperexcitability and leads to brain network dysfunction and cognitive deficits. In this study, we used computational network modeling to build a causal inference framework to explain AD-related abnormal brain activity. We constructed personalized brain network models with a set of working points to enable maximum dynamical complexity for each brain. Structural brain topographies were combined, either with excitotoxicity, or postsynaptic depression, as two leading mechanisms of the AÎ² and pTau on neuronal activity. By applying various levels of these putative mechanisms to the limbic regions that typically present, with the earliest and largest protein burden, we found that the excitotoxicity is sufficient and necessary to reproduce empirical biomarkers two biometrics associated with AD pathology: homotopic dysconnectivity and a decrease in limbic network dynamical fluidity. This observation was shown not only in the clinical groups (aMCI and AD), but also in healthy subjects that were virtually-diseased with excitotoxicity as these abnormal proteins can accumulate before the appearance of any cognitive changes. The same findings were independently confirmed by a mechanistic deep learning inference framework. Taken together, our results show the crucial role of protein burden-induced hyperexcitability in altering macroscopic brain network dynamics, and offer a mechanistic link between structural and functional biomarkers of cognitive dysfunction due to AD.},
	language = {en},
	urldate = {2023-12-10},
	publisher = {medRxiv},
	author = {YalÃ§Ä±nkaya, Bahar Hazal and Ziaeemehr, Abolfazl and Fousek, Jan and Hashemi, Meysam and Lavanga, Mario and Solodkin, Ana and McIntosh, Anthony R. and Jirsa, Viktor K. and Petkoski, Spase},
	month = jan,
	year = {2023},
}

@article{markram_reconstruction_2015,
	title = {Reconstruction and {Simulation} of {Neocortical} {Microcircuitry}},
	volume = {163},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(15)01191-5},
	doi = {10.1016/j.cell.2015.09.029},
	language = {English},
	number = {2},
	urldate = {2023-12-10},
	journal = {Cell},
	author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, SÃ©bastien and LeÂ BÃ©, Jean-Vincent and MagalhÃ£es, Bruno R. C. and MerchÃ¡n-PÃ©rez, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and MuÃ±oz-CÃ©spedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and RodrÃ­guez, JosÃ©-Rodrigo and Riquelme, Juan Luis and RÃ¶ssert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and TrÃ¤nkler, Thomas and VanÂ Geit, Werner and DÃ­az, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and SchÃ¼rmann, Felix},
	month = oct,
	year = {2015},
	pmid = {26451489},
	pages = {456--492},
}

@article{perez-nieves_neural_2021,
	title = {Neural heterogeneity promotes robust learning},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-26022-3},
	doi = {10.1038/s41467-021-26022-3},
	abstract = {The brain is a hugely diverse, heterogeneous structure. Whether or not heterogeneity at the neural level plays a functional role remains unclear, and has been relatively little explored in models which are often highly homogeneous. We compared the performance of spiking neural networks trained to carry out tasks of real-world difficulty, with varying degrees of heterogeneity, and found that heterogeneity substantially improved task performance. Learning with heterogeneity was more stable and robust, particularly for tasks with a rich temporal structure. In addition, the distribution of neuronal parameters in the trained networks is similar to those observed experimentally. We suggest that the heterogeneity observed in the brain may be more than just the byproduct of noisy processes, but rather may serve an active and important role in allowing animals to learn in changing environments.},
	language = {en},
	number = {1},
	urldate = {2023-12-06},
	journal = {Nature Communications},
	author = {Perez-Nieves, Nicolas and Leung, Vincent C. H. and Dragotti, Pier Luigi and Goodman, Dan F. M.},
	month = oct,
	year = {2021},
	keywords = {Electrical and electronic engineering, Learning algorithms, Network models, metalearning},
	pages = {5791},
}

@article{kastellakis_dendritic_2023,
	title = {The dendritic engram},
	volume = {17},
	issn = {1662-5153},
	url = {https://www.frontiersin.org/articles/10.3389/fnbeh.2023.1212139},
	abstract = {Accumulating evidence from a wide range of studies, including behavioral, cellular, molecular and computational findings, support a key role of dendrites in the encoding and recall of new memories. Dendrites can integrate synaptic inputs in non-linear ways, provide the substrate for local protein synthesis and facilitate the orchestration of signaling pathways that regulate local synaptic plasticity. These capabilities allow them to act as a second layer of computation within the neuron and serve as the fundamental unit of plasticity. As such, dendrites are integral parts of the memory engram, namely the physical representation of memories in the brain and are increasingly studied during learning tasks. Here, we review experimental and computational studies that support a novel, dendritic view of the memory engram that is centered on non-linear dendritic branches as elementary memory units. We highlight the potential implications of dendritic engrams for the learning and memory field and discuss future research directions.},
	urldate = {2023-12-06},
	journal = {Frontiers in Behavioral Neuroscience},
	author = {Kastellakis, George and Tasciotti, Simone and Pandi, Ioanna and Poirazi, Panayiota},
	year = {2023},
	keywords = {metalearning},
}

@misc{romain_non-synaptic_2023,
	title = {Non-synaptic plasticity enables memory-dependent local learning},
	copyright = {Â© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.11.14.567001v1},
	doi = {10.1101/2023.11.14.567001},
	abstract = {Synaptic plasticity is essential for memory formation and learning in the brain. In addition, recent results indicate that non-synaptic plasticity processes such as the regulation of neural membrane properties contribute to memory formation, its functional role in memory and learning has however remained elusive. Here, we propose that non-synaptic and synaptic plasticity are both essential components to enable memory-dependent processing in neuronal networks. While the former acts on a fast time scale for rapid information storage, the latter shapes network processing on a slower time scale to harness this memory as a functional component. We analyse this concept in a network model where pyramidal neurons regulate their apical trunk excitability in a Hebbian manner. We find that local synaptic plasticity rules can be derived for this model and show that the interplay between this synaptic plasticity and the non-synaptic trunk plasticity enables the model to successfully accommodate memory-dependent processing capabilities in a number of tasks, ranging from simple memory tests to question answering. The model can also explain contextual fear conditioning experiments, where freezing responses could be recovered by optogenetic reactivation of memory engrams under amnesia.
Author summary How memory is organized in the brain in order to enable cognitive processing is a central open question in systems neuroscience. Traditionally, synaptic plasticity is considered the key mechanism for the establishment of memory in the brain. Recently however, this view has been questioned, and it was proposed that non-synaptic plasticity mechanisms play a more prominent role as previously considered. In this article, we propose that both, synaptic and non-synaptic plasticity are central components for the formation and utilization of memory in biological neuronal networks. Our results show that non-synaptic plasticity can act on a fast time-scale to store important information, while synaptic plasticity can adapt network function on a slow time scale in order to facilitate memory-dependent cognitive processing.},
	language = {en},
	urldate = {2023-12-06},
	publisher = {bioRxiv},
	author = {Romain, Ferrand and Maximilian, Baronig and Florian, Unger and Robert, Legenstein},
	month = nov,
	year = {2023},
	keywords = {metalearning},
}

@article{chib_understanding_1995,
	title = {Understanding the {Metropolis}-{Hastings} {Algorithm}},
	volume = {49},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/2684568},
	doi = {10.2307/2684568},
	abstract = {We provide a detailed, introductory exposition of the Metropolis-Hastings algorithm, a powerful Markov chain method to simulate multivariate distributions. A simple, intuitive derivation of this method is given along with guidance on implementation. Also discussed are two applications of the algorithm, one for implementing acceptance-rejection sampling when a blanketing function is not available and the other for implementing the algorithm with block-at-a-time scans. In the latter situation, many different algorithms, including the Gibbs sampler, are shown to be special cases of the Metropolis-Hastings algorithm. The methods are illustrated with examples.},
	number = {4},
	urldate = {2023-12-04},
	journal = {The American Statistician},
	author = {Chib, Siddhartha and Greenberg, Edward},
	year = {1995},
	pages = {327--335},
}

@article{hitchcock_history_2003,
	title = {A {History} of the {Metropolis}-{Hastings} {Algorithm}},
	volume = {57},
	issn = {0003-1305},
	url = {https://www.jstor.org/stable/30037292},
	abstract = {The Metropolis-Hastings algorithm is an extremely popular Markov chain Monte Carlo technique among statisticians. This article explores the history of the algorithm, highlighting key personalities and events in its development. We relate reasons for the delay in the acceptance of the algorithm and reasons for its recent popularity.},
	number = {4},
	urldate = {2023-12-04},
	journal = {The American Statistician},
	author = {Hitchcock, David B.},
	year = {2003},
	pages = {254--257},
}

@misc{bengio_gflownet_2023,
	title = {{GFlowNet} {Foundations}},
	url = {http://arxiv.org/abs/2111.09266},
	doi = {10.48550/arXiv.2111.09266},
	abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	urldate = {2023-12-04},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	month = jul,
	year = {2023},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goncalves_training_2020,
	title = {Training deep neural density estimators to identify mechanistic models of neural dynamics},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.56261},
	doi = {10.7554/eLife.56261},
	abstract = {Mechanistic modeling in neuroscience aims to explain observed phenomena in terms of underlying causes. However, determining which model parameters agree with complex and stochastic neural data presents a significant challenge. We address this challenge with a machine learning tool which uses deep neural density estimatorsâtrained using model simulationsâto carry out Bayesian inference and retrieve the full space of parameters compatible with raw data or selected data features. Our method is scalable in parameters and data features and can rapidly analyze new data after initial training. We demonstrate the power and flexibility of our approach on receptive fields, ion channels, and HodgkinâHuxley models. We also characterize the space of circuit configurations giving rise to rhythmic activity in the crustacean stomatogastric ganglion, and use these results to derive hypotheses for underlying compensation mechanisms. Our approach will help close the gap between data-driven and theory-driven models of neural dynamics.},
	urldate = {2023-12-04},
	journal = {eLife},
	author = {GonÃ§alves, Pedro J and Lueckmann, Jan-Matthis and Deistler, Michael and Nonnenmacher, Marcel and Ãcal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F and Haddad, Sara A and Vogels, Tim P and Greenberg, David S and Macke, Jakob H},
	editor = {Huguenard, John R and O'Leary, Timothy and Goldman, Mark S},
	month = sep,
	year = {2020},
	keywords = {bayesian inference, deep learning, mechanistic models, model identification, neural dynamics, stomatogastric ganglion},
	pages = {e56261},
}

@article{zhou_computational_2023,
	title = {Computational event-driven vision sensors for in-sensor spiking neural networks},
	volume = {6},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2520-1131},
	url = {https://www.nature.com/articles/s41928-023-01055-2},
	doi = {10.1038/s41928-023-01055-2},
	abstract = {Neuromorphic event-based image sensors capture only the dynamic motion in a scene, which is then transferred to computation units for motion recognition. This approach, however, leads to time latency and can be power consuming. Here we report computational event-driven vision sensors that capture and directly convert dynamic motion into programmable, sparse and informative spiking signals. The sensors can be used to form a spiking neural network for motion recognition. Each individual vision sensor consists of two parallel photodiodes with opposite polarities and has a temporal resolution of 5âÎ¼s. In response to changes in light intensity, the sensors generate spiking signals with different amplitudes and polarities by electrically programming their individual photoresponsivity. The non-volatile and multilevel photoresponsivity of the vision sensors can emulate synaptic weights and can be used to create an in-sensor spiking neural network. Our computational event-driven vision sensor approach eliminates redundant data during the sensing process, as well as the need for data transfer between sensors and computation units.},
	language = {en},
	number = {11},
	urldate = {2023-12-04},
	journal = {Nature Electronics},
	author = {Zhou, Yue and Fu, Jiawei and Chen, Zirui and Zhuge, Fuwei and Wang, Yasai and Yan, Jianmin and Ma, Sijie and Xu, Lin and Yuan, Huanmei and Chan, Mansun and Miao, Xiangshui and He, Yuhui and Chai, Yang},
	month = nov,
	year = {2023},
	keywords = {Electrical and electronic engineering, Electronic devices},
	pages = {870--878},
}

@article{luscher_synaptic_2000,
	title = {Synaptic plasticity and dynamic modulation of the postsynaptic membrane},
	volume = {3},
	copyright = {2000 Nature America Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn0600_545},
	doi = {10.1038/75714},
	abstract = {The biochemical composition of the postsynaptic membrane and the structure of dendritic spines may be rapidly modulated by synaptic activity. Here we review these findings, discuss their implications for long-term potentiation (LTP) and long-term depression (LTD) and propose a model of sequentially occurring expression mechanisms.},
	language = {en},
	number = {6},
	urldate = {2023-12-04},
	journal = {Nature Neuroscience},
	author = {LÃ¼scher, C. and Nicoll, R. A. and Malenka, R. C. and Muller, D.},
	month = jun,
	year = {2000},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, Neurobiology, Neurosciences, general},
	pages = {545--550},
}

@article{michael_neighborhood_2023,
	title = {Neighborhood poverty during childhood prospectively predicts adolescent functional brain network architecture},
	volume = {64},
	doi = {10.1016/j.dcn.2023.101316},
	abstract = {Family poverty has been associated with altered brain structure, function, and connectivity in youth. However, few studies have examined how disadvantage within the broader neighborhood may influence functional brain network organization. The present study leveraged a longitudinal community sample of 538 twins living in low-income neighborhoods to evaluate the prospective association between exposure to neighborhood poverty during childhood (6â10 y) with functional network architecture during adolescence (8â19 y). Using resting-state and task-based fMRI, we generated two latent measures that captured intrinsic brain organization across the whole-brain and network levels â network segregation and network segregation-integration balance. While age was positively associated with network segregation and network balance overall across the sample, these associations were moderated by exposure to neighborhood poverty. Specifically, these positive associations were observed only in youth from more, but not less, disadvantaged neighborhoods. Moreover, greater exposure to neighborhood poverty predicted reduced network segregation and network balance in early, but not middle or late, adolescence. These effects were detected both across the whole-brain system as well as specific functional networks, including fronto-parietal, default mode, salience, and subcortical systems. These findings indicate that where children live may exert long-reaching effects on the organization and development of the adolescent brain.},
	journal = {Developmental Cognitive Neuroscience},
	author = {Michael, Cleanthis and Tillem, Scott and Sripada, Chandra and Burt, S. Alexandra and Klump, Kelly and Hyde, Luke},
	month = oct,
	year = {2023},
	pages = {101316},
}

@article{bullmore_brain_2010,
	title = {Brain {Graphs}: {Graphical} {Models} of the {Human} {Brain} {Connectome}},
	volume = {7},
	shorttitle = {Brain {Graphs}},
	doi = {10.1146/annurev-clinpsy-040510-143934},
	abstract = {Brain graphs provide a relatively simple and increasingly popular way of modeling the human brain connectome, using graph theory to abstractly define a nervous system as a set of nodes (denoting anatomical regions or recording electrodes) and interconnecting edges (denoting structural or functional connections). Topological and geometrical properties of these graphs can be measured and compared to random graphs and to graphs derived from other neuroscience data or other (nonneural) complex systems. Both structural and functional human brain graphs have consistently demonstrated key topological properties such as small-worldness, modularity, and heterogeneous degree distributions. Brain graphs are also physically embedded so as to nearly minimize wiring cost, a key geometric property. Here we offer a conceptual review and methodological guide to graphical analysis of human neuroimaging data, with an emphasis on some of the key assumptions, issues, and trade-offs facing the investigator.},
	journal = {Annual review of clinical psychology},
	author = {Bullmore, Edward and Bassett, Danielle},
	month = apr,
	year = {2010},
	pages = {113--40},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC346238/},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	number = {8},
	urldate = {2023-12-01},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Hopfield, J J},
	month = apr,
	year = {1982},
	pmid = {6953413},
	pmcid = {PMC346238},
	pages = {2554--2558},
}

@article{betzel_multi-scale_2016,
	title = {Multi-scale brain networks},
	volume = {160},
	doi = {10.1016/j.neuroimage.2016.11.006},
	abstract = {The network architecture of the human brain has become a feature of increasing interest to the neuroscientific community, largely because of its potential to illuminate human cognition, its variation over development and aging, and its alteration in disease or injury. Traditional tools and approaches to study this architecture have largely focused on single scales -- of topology, time, and space. Expanding beyond this narrow view, we focus this review on pertinent questions and novel methodological advances for the multi-scale brain. We separate our exposition into content related to multi-scale topological structure, multi-scale temporal structure, and multi-scale spatial structure. In each case, we recount empirical evidence for such structures, survey network-based methodological approaches to reveal these structures, and outline current frontiers and open questions. Although predominantly peppered with examples from human neuroimaging, we hope that this account will offer an accessible guide to any neuroscientist aiming to measure, characterize, and understand the full richness of the brain's multiscale network structure -- irrespective of species, imaging modality, or spatial resolution.},
	journal = {NeuroImage},
	author = {Betzel, Richard and Bassett, Danielle},
	month = aug,
	year = {2016},
}

@article{brocke_multirate_2017,
	title = {Multirate method for co-simulation of electrical-chemical systems in multiscale modeling},
	volume = {42},
	issn = {1573-6873},
	url = {https://doi.org/10.1007/s10827-017-0639-7},
	doi = {10.1007/s10827-017-0639-7},
	abstract = {Multiscale modeling by means of co-simulation is a powerful tool to address many vital questions in neuroscience. It can for example be applied in the study of the process of learning and memory formation in the brain. At the same time the co-simulation technique makes it possible to take advantage of interoperability between existing tools and multi-physics models as well as distributed computing. However, the theoretical basis for multiscale modeling is not sufficiently understood. There is, for example, a need of efficient and accurate numerical methods for time integration. When time constants of model components are different by several orders of magnitude, individual dynamics and mathematical definitions of each component all together impose stability, accuracy and efficiency challenges for the time integrator. Following our numerical investigations in Brocke et al. (Frontiers in Computational Neuroscience, 10, 97, 2016), we present a new multirate algorithm that allows us to handle each component of a large system with a step size appropriate to its time scale. We take care of error estimates in a recursive manner allowing individual components to follow their discretization time course while keeping numerical error within acceptable bounds. The method is developed with an ultimate goal of minimizing the communication between the components. Thus it is especially suitable for co-simulations. Our preliminary results support our confidence that the multirate approach can be used in the class of problems we are interested in. We show that the dynamics ofa communication signal as well as an appropriate choice of the discretization order between system components may have a significant impact on the accuracy of the coupled simulation. Although, the ideas presented in the paper have only been tested on a single model, it is likely that they can be applied to other problems without loss of generality. We believe that this work may significantly contribute to the establishment of a firm theoretical basis and to the development of an efficient computational framework for multiscale modeling and simulations.},
	language = {en},
	number = {3},
	urldate = {2023-11-30},
	journal = {Journal of Computational Neuroscience},
	author = {Brocke, Ekaterina and Djurfeldt, Mikael and Bhalla, Upinder S. and Kotaleski, Jeanette Hellgren and Hanke, Michael},
	month = jun,
	year = {2017},
	keywords = {Adaptive time step integration, Backward differentiation formula, Co-simulation, Coupled integration, Coupled system, Multirate integration, Multiscale modeling, Multiscale simulation, Parallel numerical integration},
	pages = {245--256},
}

@misc{van_der_vlag_vast_2023,
	title = {Vast {TVB} parameter space exploration: {A} {Modular} {Framework} for {Accelerating} the {Multi}-{Scale} {Simulation} of {Human} {Brain} {Dynamics}},
	shorttitle = {Vast {TVB} parameter space exploration},
	url = {http://arxiv.org/abs/2311.13337},
	doi = {10.48550/arXiv.2311.13337},
	abstract = {Global neural dynamics emerge from multi-scale brain structures, with neurons communicating through synapses to form transiently communicating networks. Network activity arises from intercellular communication that depends on the structure of connectome tracts and local connection, intracellular signalling cascades, and the extracellular molecular milieu that regulate cellular properties. Multi-scale models of brain function have begun to directly link the emergence of global brain dynamics in conscious and unconscious brain states to microscopic changes at the level of cells. In particular, AdEx mean-field models representing statistical properties of local populations of neurons have been connected following human tractography data to represent multi-scale neural phenomena in simulations using The Virtual Brain (TVB). While mean-field models can be run on personal computers for short simulations, or in parallel on high-performance computing (HPC) architectures for longer simulations and parameter scans, the computational burden remains high and vast areas of the parameter space remain unexplored. In this work, we report that our TVB-HPC framework, a modular set of methods used here to implement the TVB-AdEx model for GPU and analyze emergent dynamics, notably accelerates simulations and substantially reduces computational resource requirements. The framework preserves the stability and robustness of the TVB-AdEx model, thus facilitating finer resolution exploration of vast parameter spaces as well as longer simulations previously near impossible to perform. Given that simulation and analysis toolkits are made public as open-source packages, our framework serves as a template onto which other models can be easily scripted and personalized datasets can be used for studies of inter-individual variability of parameters related to functional brain dynamics.},
	urldate = {2023-11-23},
	publisher = {arXiv},
	author = {van der Vlag, Michiel and Kusch, Lionel and Destexhe, Alain and Jirsa, Viktor and Diaz-Pier, Sandra and Goldman, Jennifer S.},
	month = nov,
	year = {2023},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Quantitative Biology - Neurons and Cognition},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2023-11-21},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
	pages = {859--877},
}

@incollection{bower_genesis_2003,
	title = {The {GENESIS} {Simulation} {System}},
	abstract = {GENESIS (the GEneral NEural Simulation System) was developed as a research tool to provide a standard and flexible means for
constructing structurally realistic models of biological neural systems. "Structurally realistic" simulations are computer-based implementations of models whose primary objective is to capture
what is known of the anatomical structure and physiological characteristics of the neural system of interest. The GENESIS project
is based on the belief that progress in understanding structure-function relationships in the nervous system specifically, or in biology
in general, will increasingly require the development and use
of structurally realistic models (Bower, 1995). It is our view that only through this type of modeling will general principles of neural or biological function emerge.},
	author = {Bower, James and Beeman, D. and Hucka, Michael},
	month = jan,
	year = {2003},
	pages = {475--478},
}

@article{taylor_how_2009,
	title = {How {Multiple} {Conductances} {Determine} {Electrophysiological} {Properties} in a {Multicompartment} {Model}},
	volume = {29},
	copyright = {Copyright Â© 2009 Society for Neuroscience 0270-6474/09/295573-14\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/29/17/5573},
	doi = {10.1523/JNEUROSCI.4438-08.2009},
	abstract = {Most neurons have large numbers of voltage- and time-dependent currents that contribute to their electrical firing patterns. Because these currents are nonlinear, it can be difficult to determine the role each current plays in determining how a neuron fires. The lateral pyloric (LP) neuron of the stomatogastric ganglion of decapod crustaceans has been studied extensively biophysically. We constructed â¼600,000 versions of a four-compartment model of the LP neuron and distributed 11 different currents into the compartments. From these, we selected â¼1300 models that match well the electrophysiological properties of the biological neuron. Interestingly, correlations that were seen in the expression of channel mRNA in biological studies were not found across the â¼1300 admissible LP neuron models, suggesting that the electrical phenotype does not require these correlations. We used cubic fits of the function from maximal conductances to a series of electrophysiological properties to ask which conductances predominantly influence input conductance, resting membrane potential, resting spike rate, phasing of activity in response to rhythmic inhibition, and several other properties. In all cases, multiple conductances contribute to the measured property, and the combinations of currents that strongly influence each property differ. These methods can be used to understand how multiple currents in any candidate neuron interact to determine the cell's electrophysiological behavior.},
	language = {en},
	number = {17},
	urldate = {2023-11-07},
	journal = {Journal of Neuroscience},
	author = {Taylor, Adam L. and Goaillard, Jean-Marc and Marder, Eve},
	month = apr,
	year = {2009},
	pmid = {19403824},
	pages = {5573--5586},
}

@article{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413/},
	number = {4},
	urldate = {2023-11-03},
	journal = {The Journal of Physiology},
	author = {Hodgkin, A. L. and Huxley, A. F.},
	month = aug,
	year = {1952},
	pmid = {12991237},
	pmcid = {PMC1392413},
	pages = {500--544},
}

@book{murphy_probabilistic_2023,
	title = {Probabilistic {Machine} {Learning}: {Advanced} {Topics}},
	url = {https://probml.github.io/pml-book/book2.html},
	urldate = {2023-10-24},
	publisher = {MIT Press},
	author = {Murphy, Kevin P},
	year = {2023},
}

@misc{falkiewicz_calibrating_2023,
	title = {Calibrating {Neural} {Simulation}-{Based} {Inference} with {Differentiable} {Coverage} {Probability}},
	url = {http://arxiv.org/abs/2310.13402},
	doi = {10.48550/arXiv.2310.13402},
	abstract = {Bayesian inference allows expressing the uncertainty of posterior belief under a probabilistic model given prior information and the likelihood of the evidence. Predominantly, the likelihood function is only implicitly established by a simulator posing the need for simulation-based inference (SBI). However, the existing algorithms can yield overconfident posteriors (Hermans *et al.*, 2022) defeating the whole purpose of credibility if the uncertainty quantification is inaccurate. We propose to include a calibration term directly into the training objective of the neural model in selected amortized SBI techniques. By introducing a relaxation of the classical formulation of calibration error we enable end-to-end backpropagation. The proposed method is not tied to any particular neural model and brings moderate computational overhead compared to the profits it introduces. It is directly applicable to existing computational pipelines allowing reliable black-box posterior inference. We empirically show on six benchmark problems that the proposed method achieves competitive or better results in terms of coverage and expected posterior density than the previously existing approaches.},
	urldate = {2023-10-24},
	publisher = {arXiv},
	author = {Falkiewicz, Maciej and Takeishi, Naoya and Shekhzadeh, Imahn and Wehenkel, Antoine and Delaunoy, Arnaud and Louppe, Gilles and Kalousis, Alexandros},
	month = oct,
	year = {2023},
	keywords = {Computer Science - Machine Learning, Method Improvement, SBI, Statistics - Machine Learning},
}

@phdthesis{luckmann_simulation-based_2022,
	type = {Dissertation},
	title = {Simulation-{Based} {Inference} for {Neuroscience} and {Beyond}},
	copyright = {http://tobias-lib.uni-tuebingen.de/doku/lic\_mit\_pod.php?la=de},
	url = {https://publikationen.uni-tuebingen.de/xmlui/handle/10900/131182},
	abstract = {Science makes extensive use of simulations to model the world. Statistical inference identifies which models are consistent with observed phenomena, thus bridging the gap between theory and reality. However, conventional statistical inference is often inapplicable to detailed simulation models because their associated likelihood functions are intractable. Simulation-based inference (SBI) addresses this problem: It allows statistical inference from simulations alone and can thus be used with implicit models, which lack evaluable likelihoods. This thesis consists of four publications that draw on advances in machine learning to contribute to the transition away from heuristic approaches towards principled statistical inference with SBI, which allows to identify data-consistent models. To this end, this thesis proposes new algorithms, applications to neuroscience, and the first unified benchmark for SBI. Overall, it shows the potential for fast and flexible likelihood-free algorithms to facilitate scientific discovery in neuroscience and beyond.  
 
The trade-off between models of neural dynamics that are statistically amenable or mechanistically plausible was the starting point for the work presented in this thesis. In the first publication, we introduce an SBI algorithm for sequential neural posterior estimation, which overcomes the drawbacks of an earlier method. We provide several extensions motivated by challenging problems in neuroscience, including end-to-end learning of summary statistics for high-dimensional time series data. In the second publication, we demonstrate its broad applicability to mechanistic models in neuroscienceâfrom the scale of ion channels, which are the basic building blocks of biophysical neuron models, to network models of neural dynamics. Our approach overcomes the limitations of heuristic alternatives and narrows the divide between statistical and mechanistic models. The third publication proposes a novel SBI algorithm that proceeds by learning an emulator of the simulator. This approach enables the use of active learning schemes to adaptively acquire new simulations, which allows scaling to problems that are computationally highly expensive. With rapid progress in SBI, the need for a unified benchmark became apparent: In the fourth publication, we propose the first benchmark for the field to transparently evaluate progress and to contribute to more efficient and reproducible science.},
	language = {en},
	urldate = {2023-10-23},
	school = {UniversitÃ¤t TÃ¼bingen},
	author = {LÃ¼ckmann, Jan-Matthis},
	month = aug,
	year = {2022},
	doi = {10.15496/publikation-72542},
	keywords = {SBI},
}

@article{cranmer_frontier_2020,
	title = {The frontier of simulation-based inference},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1912789117},
	doi = {10.1073/pnas.1912789117},
	abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.},
	number = {48},
	urldate = {2023-10-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
	month = dec,
	year = {2020},
	keywords = {SBI},
	pages = {30055--30062},
}

@article{prangle_lazy_2016,
	title = {Lazy {ABC}},
	volume = {26},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-014-9544-3},
	doi = {10.1007/s11222-014-9544-3},
	abstract = {Approximate Bayesian computation (ABC) performs statistical inference for otherwise intractable probability models by accepting parameter proposals when corresponding simulated datasets are sufficiently close to the observations. Producing the large quantity of simulations needed requires considerable computing time. However, it is often clear before a simulation ends that it is unpromising: it is likely to produce a poor match or require excessive time. This paper proposes lazy ABC, an ABC importance sampling algorithm which saves time by sometimes abandoning such simulations. This makes ABC more scalable to applications where simulation is expensive. By using a random stopping rule and appropriate reweighting step, the target distribution is unchanged from that of standard ABC. Theory and practical methods to tune lazy ABC are presented and illustrated on a simple epidemic model example. They are also demonstrated on the computationally demanding spatial extremes application of Erhardt and Smith (Comput Stat Data Anal 56:1468â1481, 2012), producing efficiency gains, in terms of effective sample size per unit CPU time, of roughly 3 times for a 20 location dataset, and 8 times for 35 locations.},
	language = {en},
	number = {1},
	urldate = {2023-10-23},
	journal = {Statistics and Computing},
	author = {Prangle, Dennis},
	month = jan,
	year = {2016},
	keywords = {ABC, Epidemics, Importance sampling, Multifidelity, SBI, Spatial extremes, Unbiased likelihood estimators},
	pages = {171--185},
}

@article{prescott_multifidelity_2021,
	title = {Multifidelity {Approximate} {Bayesian} {Computation} with {Sequential} {Monte} {Carlo} {Parameter} {Sampling}},
	volume = {9},
	url = {https://epubs.siam.org/doi/abs/10.1137/20M1316160},
	doi = {10.1137/20M1316160},
	abstract = {A vital stage in the mathematical modeling of real-world systems is to calibrate a model's parameters to observed data. Likelihood-free parameter inference methods, such as approximate Bayesian computation (ABC), build Monte Carlo samples of the uncertain parameter distribution by comparing the data with large numbers of model simulations. However, the computational expense of generating these simulations forms a significant bottleneck in the practical application of such methods. We identify how simulations of corresponding cheap, low-fidelity models have been used separately in two complementary ways to reduce the computational expense of building these samples, at the cost of introducing additional variance to the resulting parameter estimates. We explore how these approaches can be unified so that cost and benefit are optimally balanced, and we characterize the optimal choice of how often to simulate from cheap, low-fidelity models in place of expensive, high-fidelity models in Monte Carlo ABC algorithms. The resulting early accept/reject multifidelity ABC algorithm that we propose is shown to give improved performance over existing multifidelity and high-fidelity approaches.},
	number = {2},
	urldate = {2023-10-23},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Prescott, Thomas P. and Baker, Ruth E.},
	month = jan,
	year = {2021},
	keywords = {Multifidelity, SBI},
	pages = {788--817},
}

@article{pritchard_population_1999,
	title = {Population growth of human {Y} chromosomes: a study of {Y} chromosome microsatellites.},
	volume = {16},
	issn = {0737-4038},
	shorttitle = {Population growth of human {Y} chromosomes},
	url = {https://doi.org/10.1093/oxfordjournals.molbev.a026091},
	doi = {10.1093/oxfordjournals.molbev.a026091},
	abstract = {We use variation at a set of eight human Y chromosome microsatellite loci to investigate the demographic history of the Y chromosome. Instead of assuming a population of constant size, as in most of the previous work on the Y chromosome, we consider a model which permits a period of recent population growth. We show that for most of the populations in our sample this model fits the data far better than a model with no growth. We estimate the demographic parameters of this model for each population and also the time to the most recent common ancestor. Since there is some uncertainty about the details of the microsatellite mutation process, we consider several plausible mutation schemes and estimate the variance in mutation size simultaneously with the demographic parameters of interest. Our finding of a recent common ancestor (probably in the last 120,000 years), coupled with a strong signal of demographic expansion in all populations, suggests either a recent human expansion from a small ancestral population, or natural selection acting on the Y chromosome.},
	number = {12},
	urldate = {2023-10-19},
	journal = {Molecular Biology and Evolution},
	author = {Pritchard, J K and Seielstad, M T and Perez-Lezaun, A and Feldman, M W},
	month = dec,
	year = {1999},
	keywords = {History, SBI},
	pages = {1791--1798},
}

@article{tavare_inferring_1997,
	title = {Inferring coalescence times from {DNA} sequence data},
	volume = {145},
	issn = {0016-6731},
	doi = {10.1093/genetics/145.2.505},
	abstract = {The paper is concerned with methods for the estimation of the coalescence time (time since the most recent common ancestor) of a sample of intraspecies DNA sequences. The methods take advantage of prior knowledge of population demography, in addition to the molecular data. While some theoretical results are presented, a central focus is on computational methods. These methods are easy to implement, and, since explicit formulae tend to be either unavailable or unilluminating, they are also more useful and more informative in most applications. Extensions are presented that allow for the effects of uncertainty in our knowledge of population size and mutation rates, for variability in population sizes, for regions of different mutation rate, and for inference concerning the coalescence time of the entire population. The methods are illustrated using recent data from the human Y chromosome.},
	language = {eng},
	number = {2},
	journal = {Genetics},
	author = {TavarÃ©, S. and Balding, D. J. and Griffiths, R. C. and Donnelly, P.},
	month = feb,
	year = {1997},
	pmid = {9071603},
	pmcid = {PMC1207814},
	keywords = {Algorithms, DNA, Databases, Factual, History, Humans, SBI, Time Factors},
	pages = {505--518},
}

@article{rubin_bayesianly_1984,
	title = {Bayesianly {Justifiable} and {Relevant} {Frequency} {Calculations} for the {Applied} {Statistician}},
	volume = {12},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-12/issue-4/Bayesianly-Justifiable-and-Relevant-Frequency-Calculations-for-the-Applied-Statistician/10.1214/aos/1176346785.full},
	doi = {10.1214/aos/1176346785},
	abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
	number = {4},
	urldate = {2023-10-19},
	journal = {The Annals of Statistics},
	author = {Rubin, Donald B.},
	month = dec,
	year = {1984},
	keywords = {62-07, 62A15, 62F15, 62L10, 62P99, Calibration, Empirical Bayes, History, SBI, Stopping rules, history, inference, model monitoring, operating characteristics, posterior predictive checks},
	pages = {1151--1172},
}

@article{beaumont_approximate_2002,
	title = {Approximate {Bayesian} computation in population genetics.},
	volume = {162},
	issn = {0016-6731},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1462356/},
	abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
	number = {4},
	urldate = {2023-10-19},
	journal = {Genetics},
	author = {Beaumont, Mark A and Zhang, Wenyang and Balding, David J},
	month = dec,
	year = {2002},
	pmid = {12524368},
	pmcid = {PMC1462356},
	keywords = {History, SBI},
	pages = {2025--2035},
}

@article{durstewitz_reconstructing_2023,
	title = {Reconstructing computational system dynamics from neural data with recurrent neural networks},
	copyright = {2023 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-023-00740-7},
	doi = {10.1038/s41583-023-00740-7},
	abstract = {Computational models in neuroscience usually take the form of systems of differential equations. The behaviour of such systems is the subject of dynamical systems theory. Dynamical systemsÂ theory provides a powerful mathematical toolbox for analysing neurobiological processes and has been a mainstay of computational neuroscience for decades. Recently, recurrent neural networks (RNNs) have become a popular machine learning tool for studying the non-linear dynamics of neural and behavioural processes by emulating an underlying system of differential equations. RNNs have been routinely trained on similar behavioural tasks to those used for animal subjects to generate hypotheses about the underlying computational mechanisms. By contrast, RNNs can also be trained on the measured physiological and behavioural data, thereby directly inheriting their temporal and geometrical properties. In this way they become a formal surrogate for the experimentally probed system that can be further analysed, perturbed and simulated. This powerful approach is called dynamical system reconstruction. In this Perspective, we focus on recent trends in artificial intelligence and machine learning in this exciting and rapidly expanding field, which may be less well known in neuroscience. We discuss formal prerequisites, different model architectures and training approaches for RNN-based dynamical system reconstructions, ways to evaluate and validate model performance, how to interpret trained models in a neuroscience context, and current challenges.},
	language = {en},
	urldate = {2023-10-09},
	journal = {Nature Reviews Neuroscience},
	author = {Durstewitz, Daniel and Koppe, Georgia and Thurm, Max Ingo},
	month = oct,
	year = {2023},
	keywords = {Dynamical systems, Learning algorithms},
	pages = {1--18},
}
