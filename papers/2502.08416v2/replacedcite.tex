\section{Related work}
% \subsection{SBI via neural density estimation}

% The challenge of extending sampling-based methods ____ to high-dimensional problems has driven significant advancements in simulation-based inference, catalysed by developments in probabilistic deep learning ____. Modern SBI approaches aim for the direct estimation of the posterior distribution (e.g., Neural Posterior Estimation (NPE) ____), the likelihood function (e.g., Neural Likelihood Estimation (NLE) ____), or likelihood ratios (e.g., Neural Ratio Estimation (NRE) ____). Each class of these methods has sequential variants, enabling adaptive sampling to improve simulation efficiency given single empirical observations.

% Building on NPE, we introduce a multifidelity framework to extend its applicability to costly simulators. By incorporating multiple levels of fidelity in simulators, our approach enables the efficient and accurate estimation of posterior distributions even for computationally intensive scenarios.


\subsection{Multifidelity methods for inference} % in machine learning

%It is a common practice in science to develop models that vary in terms of accuracy and computational efficiency, in order to handle the complexity and computational cost of high-fidelity models ____. 


%Low-fidelity models are seen as simplifications of high-fidelity models, usually made possible through domain expertise ____, low-dimensional projection of the high-fidelity model ____, or interpolation or regression through surrogate models ____.
Multifidelity has been widely explored in the context of likelihood-based inference ____, from maximum likelihood estimation approaches ____ to Bayesian inference methods ____.

Currently, few studies tackle the scenario where the likelihood is not explicitly available ____. For instance, some multifidelity sampling-based methods have been proposed within the framework of ABC ____ or sequential Monte Carlo ABC ____. However, despite their potential for multifidelity inference, these methods inherit limitations of ABC approaches, particularly in high-dimensional parameter spaces, where neural density estimators offer more scalable alternatives to complex real-world problems ____.

% 1. multifidelity for models where you have a likelihood-based inference (bayesian and non-bayesian: e.g., max likelihood)
% bayesian methods have all likelihood
% and aiming for maximizing likelihood or...
% 2. Likelihood-free methods
% We don't have a likelihood. Only other example:
% The use of multiple models in likelihood-free bayesian inference has been applied only for for ABC : MF-ABC____.
% Inherits limitations of ABC approaches, namely scaling with dimensionality of parameter spaces
% How different from ours?

% To accelerate the outputs of computationally expensive (i.e., time-consuming) high-fidelity models, recent studies proposed approaches combining multifidelity and ABC ____. 

% proposed to accelerate the outputs of summary statistics.
%  When the cost of generating individual stochastic samples is high,

% Multifidelity Monte Carlo approaches incorporate typically coarse approximations as low-fidelity models, with controllable discretization parameters ____, 
% However, the results do typically not scale well with the dimensionality of the spaces. 

Here, we combine multifidelity approaches with neural density estimation, enabling its application to inference problems with large numbers of model parameters.

\subsection{Transfer learning and simulators} 

% approach here:
% (1) describe transfer learning in general very briefly, with key citations;
% (2) discuss previous approaches to transfer learning in the multifidelity setting (if any exist);
% (3) discuss previous approaches to transfer learning in neural density estimation (if any exist);
% (4) discuss previous approaches to transfer learning in the SBI setting (if any exist)

To facilitate learning in a target domain, transfer learning borrows knowledge from a source domain ____. This is often done when the target dataset is smaller than the source dataset ____ and has successfully been applied to a range of machine learning tasks, e.g., in computer vision ____. 

%This approach has been successfully applied to a range of tasks, e.g., in computer vision ____.
In the context of numerical simulators, transfer learning approaches have been used to lower the simulation budget, for instance, in CO$_2$ forecasting ____, surrogate modeling ____ and model inversion with physics-informed neural networks ____. To the best of our knowledge, the potential of transfer learning for computationally-efficient simulation-based inference has not been realized yet.

% We do not use the inverse properties of normalizing flow, removing the need to tune an additional hyperparameter.

% Point out: physics based models don't do inference, only fitting models to data


%(rather than inference), 
% Previous transfer learning approaches in a multifidelity setting include the work of ____, which explores the use of physics-informed deep neural networks ____. 

% Another direction explored by ____ uses an online transfer learning approach that fuses multifidelity data within an ensemble of deep neural networks and incorporates Bayesian Optimization (BO) to lower the cost of sampling points for expensive optimization tasks. 

% In the context of normalizing flows, ____ developed a method that does not require any additional training in the context of neural style transfer. The variance of the likelihood is tuned by hyperparameter $\lambda$, which is similar to the tolerance error in Approximate Bayesian Computation ____.

% We do not use the inverse properties of normalizing flow, removing the need to tune an additional hyperparameter.
% explored transfer learning in the context of normalizing flows. Leveraging its inverse properties, they

% This is different from our approach since we do not rely on additional hyperparameter tuning for the likelihood variance.


\subsection{Model misspecification} 

Our work has connections to the issue of model misspecification, a fundamental and widely studied problem in SBI ____. The issue arises when the probability over the true observation does not fall within the family of estimated probability distributions defined by a model.

A misspecified model can be seen as a form of low-fidelity model of the data-generating process (the high-fidelity model) since this will often rely on several simplifying assumptions.
% ____ proposed a misspecification measure that precludes the need for true observations, and ____ investigated the impact of misspecification through empirical tests. The problem has been addressed with a regularized loss function to penalize the statistics that increase the mismatch between the data and the model ____ 
With this perspective in mind, ROPE can be considered a multifidelity approach to SBI ____: model misspecification is addressed by providing a small set of real-world observations that re-calibrate through optimal transport the posterior estimates of the misspecified model. In contrast with our work, this scheme does not allow the active sampling of high-fidelity simulations. 

%It also does not allow posterior predictive checks.
% They rely on calibration set: as high-fdielity model (as if high-feilty simulator). Similar problem, but not simulator, but real empirical observation data. Imprtant is that we have an online method (they don't have). You get what you measure, and don't generate with high fidleit model (so no synthetic data). So no active scheme because we cannot sample for informative samples! active sampling but fixed there set of hf simulations. Also, we have the high-fidelity simulator, can do ppc and so on, they dont

% Here, the low-fidelity model is often seen as the misspecified model while relying on a small calibration set  ____.