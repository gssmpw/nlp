@article{adachi_reversal_2010,
	title = {Reversal of {Salt} {Preference} {Is} {Directed} by the {Insulin}/{PI3K} and {Gq}/{PKC} {Signaling} in {Caenorhabditis} elegans},
	volume = {186},
	issn = {0016-6731},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2998313/},
	doi = {10.1534/genetics.110.119768},
	abstract = {Animals search for foods and decide their behaviors according to previous experience. Caenorhabditis elegans detects chemicals with a limited number of sensory neurons, allowing us to dissect roles of each neuron for innate and learned behaviors. C. elegans is attracted to salt after exposure to the salt (NaCl) with food. In contrast, it learns to avoid the salt after exposure to the salt without food. In salt-attraction behavior, it is known that the ASE taste sensory neurons (ASEL and ASER) play a major role. However, little is known about mechanisms for learned salt avoidance. Here, through dissecting contributions of ASE neurons for salt chemotaxis, we show that both ASEL and ASER generate salt chemotaxis plasticity. In ASER, we have previously shown that the insulin/PI 3-kinase signaling acts for starvation-induced salt chemotaxis plasticity. This study shows that the PI 3-kinase signaling promotes aversive drive of ASER but not of ASEL. Furthermore, the Gq signaling pathway composed of Gqα EGL-30, diacylglycerol, and nPKC (novel protein kinase C) TTX-4 promotes attractive drive of ASER but not of ASEL. A putative salt receptor GCY-22 guanylyl cyclase is required in ASER for both salt attraction and avoidance. Our results suggest that ASEL and ASER use distinct molecular mechanisms to regulate salt chemotaxis plasticity.},
	number = {4},
	urldate = {2024-02-07},
	journal = {Genetics},
	author = {Adachi, Takeshi and Kunitomo, Hirofumi and Tomioka, Masahiro and Ohno, Hayao and Okochi, Yoshifumi and Mori, Ikue and Iino, Yuichi},
	month = dec,
	year = {2010},
	pmid = {20837997},
	pmcid = {PMC2998313},
	pages = {1309--1319},
}

@article{beaumont_approximate_2002,
	title = {Approximate {Bayesian} computation in population genetics.},
	volume = {162},
	issn = {0016-6731},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1462356/},
	abstract = {We propose a new method for approximate Bayesian statistical inference on the basis of summary statistics. The method is suited to complex problems that arise in population genetics, extending ideas developed in this setting by earlier authors. Properties of the posterior distribution of a parameter, such as its mean or density curve, are approximated without explicit likelihood calculations. This is achieved by fitting a local-linear regression of simulated parameter values on simulated summary statistics, and then substituting the observed summary statistics into the regression equation. The method combines many of the advantages of Bayesian statistical inference with the computational efficiency of methods based on summary statistics. A key advantage of the method is that the nuisance parameters are automatically integrated out in the simulation step, so that the large numbers of nuisance parameters that arise in population genetics problems can be handled without difficulty. Simulation results indicate computational and statistical efficiency that compares favorably with those of alternative methods previously proposed in the literature. We also compare the relative efficiency of inferences obtained using methods based on summary statistics with those obtained directly from the data using MCMC.},
	number = {4},
	urldate = {2023-10-19},
	journal = {Genetics},
	author = {Beaumont, Mark A and Zhang, Wenyang and Balding, David J},
	month = dec,
	year = {2002},
	pmid = {12524368},
	pmcid = {PMC1462356},
	keywords = {History, SBI},
	pages = {2025--2035},
}

@article{berkooz_proper_1993,
	title = {The {Proper} {Orthogonal} {Decomposition} in the {Analysis} of {Turbulent} {Flows}},
	volume = {25},
	issn = {0066-4189, 1545-4479},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.fl.25.010193.002543},
	doi = {10.1146/annurev.fl.25.010193.002543},
	language = {en},
	number = {Volume 25, 1993},
	urldate = {2025-01-13},
	journal = {Annual Review of Fluid Mechanics},
	author = {Berkooz, G. and Holmes, P. and Lumley, J. L.},
	month = jan,
	year = {1993},
	note = {Publisher: Annual Reviews},
	pages = {539--575},
}

@misc{cannon_investigating_2022,
	title = {Investigating the {Impact} of {Model} {Misspecification} in {Neural} {Simulation}-based {Inference}},
	url = {http://arxiv.org/abs/2209.01845},
	doi = {10.48550/arXiv.2209.01845},
	abstract = {Aided by advances in neural density estimation, considerable progress has been made in recent years towards a suite of simulation-based inference (SBI) methods capable of performing flexible, black-box, approximate Bayesian inference for stochastic simulation models. While it has been demonstrated that neural SBI methods can provide accurate posterior approximations, the simulation studies establishing these results have considered only well-specified problems -- that is, where the model and the data generating process coincide exactly. However, the behaviour of such algorithms in the case of model misspecification has received little attention. In this work, we provide the first comprehensive study of the behaviour of neural SBI algorithms in the presence of various forms of model misspecification. We find that misspecification can have a profoundly deleterious effect on performance. Some mitigation strategies are explored, but no approach tested prevents failure in all cases. We conclude that new approaches are required to address model misspecification if neural SBI algorithms are to be relied upon to derive accurate scientific conclusions.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Cannon, Patrick and Ward, Daniel and Schmon, Sebastian M.},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01845 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{catanach_bayesian_2020,
	title = {Bayesian inference of stochastic reaction networks using multifidelity sequential tempered {Markov} {Chain} {Monte} {Carlo}},
	volume = {10},
	issn = {2152-5080},
	doi = {10.1615/int.j.uncertaintyquantification.2020033241},
	abstract = {Stochastic reaction network models are often used to explain and predict the dynamics of gene regulation in single cells. These models usually involve several parameters, such as the kinetic rates of chemical reactions, that are not directly measurable and must be inferred from experimental data. Bayesian inference provides a rigorous probabilistic framework for identifying these parameters by finding a posterior parameter distribution that captures their uncertainty. Traditional computational methods for solving inference problems such as Markov Chain Monte Carlo methods based on classical Metropolis-Hastings algorithm involve numerous serial evaluations of the likelihood function, which in turn requires expensive forward solutions of the chemical master equation (CME). We propose an alternate approach based on a multifidelity extension of the Sequential Tempered Markov Chain Monte Carlo (ST-MCMC) sampler. This algorithm is built upon Sequential Monte Carlo and solves the Bayesian inference problem by decomposing it into a sequence of efficiently solved subproblems that gradually increase both model fidelity and the influence of the observed data. We reformulate the finite state projection (FSP) algorithm, a well-known method for solving the CME, to produce a hierarchy of surrogate master equations to be used in this multifidelity scheme. To determine the appropriate fidelity, we introduce a novel information-theoretic criteria that seeks to extract the most information about the ultimate Bayesian posterior from each model in the hierarchy without inducing significant bias. This novel sampling scheme is tested with high performance computing resources using biologically relevant problems.},
	language = {eng},
	number = {6},
	journal = {International Journal for Uncertainty Quantification},
	author = {Catanach, Thomas A. and Vo, Huy D. and Munsky, Brian},
	year = {2020},
	pmid = {34007522},
	pmcid = {PMC8127724},
	keywords = {Bayesian inference, MCMC, SMC, Stochastic modeling, Systems Biology, UQ},
	pages = {515--542},
}

@article{chakraborty_transfer_2021,
	title = {Transfer learning based multi-fidelity physics informed deep neural network},
	volume = {426},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120307166},
	doi = {10.1016/j.jcp.2020.109942},
	abstract = {For many systems in science and engineering, the governing differential equation is either not known or known in an approximate sense. Analyses and design of such systems are governed by data collected from the field and/or laboratory experiments. This challenging scenario is further worsened when data-collection is expensive and time-consuming. To address this issue, this paper presents a novel multi-fidelity physics informed deep neural network (MF-PIDNN). The framework proposed is particularly suitable when the physics of the problem is known in an approximate sense (low-fidelity physics) and only a few high-fidelity data are available. MF-PIDNN blends physics informed and data-driven deep learning techniques by using the concept of transfer learning. The approximate governing equation is first used to train a low-fidelity physics informed deep neural network. This is followed by transfer learning where the low-fidelity model is updated by using the available high-fidelity data. MF-PIDNN is able to encode useful information on the physics of the problem from the approximate governing differential equation and hence, provides accurate prediction even in zones with no data. Additionally, no low-fidelity data is required for training this model. Two examples involving function approximations with linear and nonlinear correlation are presented to illustrate the effectiveness of transfer learning in solving multi-fidelity problems. Applicability and utility of MF-PIDNN are illustrated in solving four benchmark reliability analysis problems. Case studies presented illustrate interesting features of the proposed approach.},
	urldate = {2025-01-16},
	journal = {Journal of Computational Physics},
	author = {Chakraborty, Souvik},
	month = feb,
	year = {2021},
	keywords = {Deep learning, Multi-fidelity, Physics-informed, Reliability, Transfer learning},
	pages = {109942},
}

@inproceedings{falola_rapid_2023,
	address = {Abu Dhabi, UAE},
	title = {Rapid {High}-{Fidelity} {Forecasting} for {Geological} {Carbon} {Storage} {Using} {Neural} {Operator} and {Transfer} {Learning}},
	shorttitle = {Rapid {High}-{Fidelity} {Forecasting} for {Geological} {Carbon} {Storage} {Using} {Neural} {Operator} and {Transfer} {Learning}},
	doi = {10.2118/216135-MS},
	abstract = {Abstract. Carbon sequestration is a promising technique to minimize the emission of CO2 to the atmosphere. However, the computational time required for CO2 forecasting using commercial numerical simulators can be prohibitive for complex problems. In this work, we propose the use of transfer learning to rapidly forecast the CO2 pressure plume and saturation distribution under uncertain geological and operational conditions, specifically for variations in injector locations and injector rates. We first train a Fourier Neural Operator (FNO)-based machine learning (ML) model on a limited set of simple scenarios. Then, we use transfer learning to fine-tune the FNO model on a larger set of complex scenarios. Most importantly, the CMG forecasting time for one scenario requires approximately 40 to 50 minutes, which was drastically reduced to 12 seconds by using Fourier Neural Operator and then reduced further to 8 seconds by implementing transfer learning on the Fourier neural operator. The mean relative errors of the neural operator predictions of pressure and saturation were 1.42\% and 7.9\%, respectively. These errors get slightly higher when transfer learning is implemented on neural operator to learn complex task with less amount of data and low training time. Our results show that transfer learning can significantly reduce the computational time required for CO2 forecasting. The data generation and model training times were reduced by 50\% and 75\%, respectively, by using transfer learning on the Fourier neural operator. Additionally, the total number of trainable parameters was reduced by 99.9\%. Our results demonstrate the potential of transfer learning for rapid forecasting of CO2 pressure plume and saturation distribution. This technique can be used to improve the efficiency of CO2 forecasting and to help mitigate the risks associated with CO2 leakage.},
	language = {en},
	urldate = {2025-01-30},
	booktitle = {{ADIPEC}},
	publisher = {OnePetro},
	author = {Falola, Yusuf and Misra, Siddharth and Nunez, Andres Calvo},
	month = oct,
	year = {2023},
}

@article{fernandez-godino_review_2023,
	title = {Review of multi-fidelity models},
	volume = {1},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/acse.2023015},
	doi = {10.3934/acse.2023015},
	abstract = {Multi-fidelity models provide a framework for integrating computational models of varying complexity, allowing for accurate predictions while optimizing computational resources. These models are especially beneficial when acquiring high-accuracy data is costly or computationally intensive. This review offers a comprehensive analysis of multi-fidelity models, focusing on their applications in scientific and engineering fields, particularly in optimization and uncertainty quantification. It classifies publications on multi-fidelity modeling according to several criteria, including application area, surrogate model selection, types of fidelity, combination methods and year of publication. The study investigates techniques for combining different fidelity levels, with an emphasis on multi-fidelity surrogate models. This work discusses reproducibility, open-sourcing methodologies and benchmarking procedures to promote transparency. The manuscript also includes educational toy problems to enhance understanding. Additionally, this paper outlines best practices for presenting multi-fidelity-related savings in a standardized, succinct and yet thorough manner. The review concludes by examining current trends in multi-fidelity modeling, including emerging techniques, recent advancements, and promising research directions.},
	language = {en},
	number = {4},
	urldate = {2025-01-13},
	journal = {Advances in Computational Science and Engineering},
	author = {Fernández-Godino, M. Giselle},
	month = dec,
	year = {2023},
	note = {Publisher: Advances in Computational Science and Engineering},
	pages = {351--400},
}

@article{forrester_recent_2009,
	title = {Recent advances in surrogate-based optimization},
	volume = {45},
	issn = {0376-0421},
	url = {https://www.sciencedirect.com/science/article/pii/S0376042108000766},
	doi = {10.1016/j.paerosci.2008.11.001},
	abstract = {The evaluation of aerospace designs is synonymous with the use of long running and computationally intensive simulations. This fuels the desire to harness the efficiency of surrogate-based methods in aerospace design optimization. Recent advances in surrogate-based design methodology bring the promise of efficient global optimization closer to reality. We review the present state of the art of constructing surrogate models and their use in optimization strategies. We make extensive use of pictorial examples and, since no method is truly universal, give guidance as to each method's strengths and weaknesses.},
	number = {1},
	urldate = {2025-01-13},
	journal = {Progress in Aerospace Sciences},
	author = {Forrester, Alexander I. J. and Keane, Andy J.},
	month = jan,
	year = {2009},
	pages = {50--79},
}

@article{frazier_synthetic_2024,
	title = {Synthetic {Likelihood} in {Misspecified} {Models}},
	volume = {0},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2024.2370594},
	doi = {10.1080/01621459.2024.2370594},
	abstract = {Bayesian synthetic likelihood is a widely used approach for conducting Bayesian analysis in complex models where evaluation of the likelihood is infeasible but simulation from the assumed model is tractable. We analyze the behavior of the Bayesian synthetic likelihood posterior when the assumed model differs from the actual data generating process. We demonstrate that the Bayesian synthetic likelihood posterior can display a wide range of nonstandard behaviors depending on the level of model misspecification, including multimodality and asymptotic non-Gaussianity. Our results suggest that likelihood tempering, a common approach for robust Bayesian inference, fails for synthetic likelihood whilst recently proposed robust synthetic likelihood approaches can ameliorate this behavior and deliver reliable posterior inference under model misspecification. All results are illustrated using a simple running example. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
	number = {0},
	urldate = {2025-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Frazier, David T. and Nott, David J. and Drovandi, Christopher},
	year = {2024},
	keywords = {Approximate Bayesian computation, Likelihood tempering, Model misspecification, Synthetic likelihood},
	pages = {1--12},
}

@misc{gambardella_transflow_2019,
	title = {Transflow {Learning}: {Repurposing} {Flow} {Models} {Without} {Retraining}},
	shorttitle = {Transflow {Learning}},
	url = {http://arxiv.org/abs/1911.13270},
	doi = {10.48550/arXiv.1911.13270},
	abstract = {It is well known that deep generative models have a rich latent space, and that it is possible to smoothly manipulate their outputs by traversing this latent space. Recently, architectures have emerged that allow for more complex manipulations, such as making an image look as though it were from a different class, or painted in a certain style. These methods typically require large amounts of training in order to learn a single class of manipulations. We present Transflow Learning, a method for transforming a pre-trained generative model so that its outputs more closely resemble data that we provide afterwards. In contrast to previous methods, Transflow Learning does not require any training at all, and instead warps the probability distribution from which we sample latent vectors using Bayesian inference. Transflow Learning can be used to solve a wide variety of tasks, such as neural style transfer and few-shot classification.},
	urldate = {2023-12-22},
	publisher = {arXiv},
	author = {Gambardella, Andrew and Baydin, Atılım Güneş and Torr, Philip H. S.},
	month = dec,
	year = {2019},
	note = {arXiv:1911.13270 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gao_generalized_2023,
	title = {Generalized {Bayesian} {Inference} for {Scientific} {Simulators} via {Amortized} {Cost} {Estimation}},
	url = {http://arxiv.org/abs/2305.15208},
	doi = {10.48550/arXiv.2305.15208},
	abstract = {Simulation-based inference (SBI) enables amortized Bayesian inference for simulators with implicit likelihoods. But when we are primarily interested in the quality of predictive simulations, or when the model cannot exactly reproduce the observed data (i.e., is misspecified), targeting the Bayesian posterior may be overly restrictive. Generalized Bayesian Inference (GBI) aims to robustify inference for (misspecified) simulator models, replacing the likelihood-function with a cost function that evaluates the goodness of parameters relative to data. However, GBI methods generally require running multiple simulations to estimate the cost function at each parameter value during inference, making the approach computationally infeasible for even moderately complex simulators. Here, we propose amortized cost estimation (ACE) for GBI to address this challenge: We train a neural network to approximate the cost function, which we define as the expected distance between simulations produced by a parameter and observed data. The trained network can then be used with MCMC to infer GBI posteriors for any observation without running additional simulations. We show that, on several benchmark tasks, ACE accurately predicts cost and provides predictive simulations that are closer to synthetic observations than other SBI methods, especially for misspecified simulators. Finally, we apply ACE to infer parameters of the Hodgkin-Huxley model given real intracellular recordings from the Allen Cell Types Database. ACE identifies better data-matching parameters while being an order of magnitude more simulation-efficient than a standard SBI method. In summary, ACE combines the strengths of SBI methods and GBI to perform robust and simulation-amortized inference for scientific simulators.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Gao, Richard and Deistler, Michael and Macke, Jakob H.},
	month = nov,
	year = {2023},
	note = {arXiv:2305.15208 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{giles_multilevel_2008,
	title = {Multilevel {Monte} {Carlo} {Path} {Simulation}},
	volume = {56},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.1070.0496},
	doi = {10.1287/opre.1070.0496},
	abstract = {We show that multigrid ideas can be used to reduce the computational complexity of estimating an expected value arising from a stochastic differential equation using Monte Carlo path simulations. In the simplest case of a Lipschitz payoff and a Euler discretisation, the computational cost to achieve an accuracy of O(ϵ) is reduced from O(ϵ−3) to O(ϵ−2 (log ϵ)2). The analysis is supported by numerical results showing significant computational savings.},
	number = {3},
	urldate = {2024-12-17},
	journal = {Operations Research},
	author = {Giles, Michael B.},
	month = jun,
	year = {2008},
	note = {Publisher: INFORMS},
	keywords = {analysis of algorithms, computational complexity, efficiency, finance, simulation},
	pages = {607--617},
}

@inproceedings{greenberg_automatic_2019,
	title = {Automatic {Posterior} {Transformation} for {Likelihood}-{Free} {Inference}},
	url = {https://proceedings.mlr.press/v97/greenberg19a.html},
	abstract = {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.},
	language = {en},
	urldate = {2024-12-10},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2404--2414},
}

@article{haghighat_physics-informed_2021,
	title = {A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics},
	volume = {379},
	issn = {0045-7825},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782521000773},
	doi = {10.1016/j.cma.2021.113741},
	abstract = {We present the application of a class of deep learning, known as Physics Informed Neural Networks (PINN), to inversion and surrogate modeling in solid mechanics. We explain how to incorporate the momentum balance and constitutive relations into PINN, and explore in detail the application to linear elasticity, and illustrate its extension to nonlinear problems through an example that showcases von Mises elastoplasticity. While common PINN algorithms are based on training one deep neural network (DNN), we propose a multi-network model that results in more accurate representation of the field variables. To validate the model, we test the framework on synthetic data generated from analytical and numerical reference solutions. We study convergence of the PINN model, and show that Isogeometric Analysis (IGA) results in superior accuracy and convergence characteristics compared with classic low-order Finite Element Method (FEM). We also show the applicability of the framework for transfer learning, and find vastly accelerated convergence during network re-training. Finally, we find that honoring the physics leads to improved robustness: when trained only on a few parameters, we find that the PINN model can accurately predict the solution for a wide range of parameters new to the network—thus pointing to an important application of this framework to sensitivity analysis and surrogate modeling.},
	urldate = {2025-01-16},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
	month = jun,
	year = {2021},
	keywords = {Artificial neural network, Elastoplasticity, Inversion, Linear elasticity, Physics-informed deep learning, Transfer learning},
	pages = {113741},
}

@article{han_improving_2013,
	title = {Improving variable-fidelity surrogate modeling via gradient-enhanced kriging and a generalized hybrid bridge function},
	volume = {25},
	issn = {1270-9638},
	url = {https://www.sciencedirect.com/science/article/pii/S127096381200017X},
	doi = {10.1016/j.ast.2012.01.006},
	abstract = {Variable-fidelity surrogate modeling offers an efficient way to generate aerodynamic data for aero-loads prediction based on a set of CFD methods with varying degree of fidelity and computational expense. In this paper, direct Gradient-Enhanced Kriging (GEK) and a newly developed Generalized Hybrid Bridge Function (GHBF) have been combined in order to improve the efficiency and accuracy of the existing Variable-Fidelity Modeling (VFM) approach. The new algorithms and features are demonstrated and evaluated for analytical functions and are subsequently used to construct a global surrogate model for the aerodynamic coefficients and drag polar of an RAE 2822 airfoil. It is shown that the gradient-enhanced GHBF proposed in this paper is very promising and can be used to significantly improve the efficiency, accuracy and robustness of VFM in the context of aero-loads prediction.},
	number = {1},
	urldate = {2025-01-29},
	journal = {Aerospace Science and Technology},
	author = {Han, Zhong-Hua and Görtz, Stefan and Zimmermann, Ralf},
	month = mar,
	year = {2013},
	keywords = {Computational fluid dynamics, Kriging model, Surrogate model, Variable-fidelity model},
	pages = {177--189},
}

@inproceedings{hermans_likelihood-free_2020,
	title = {Likelihood-free {MCMC} with {Amortized} {Approximate} {Ratio} {Estimators}},
	url = {https://proceedings.mlr.press/v119/hermans20a.html},
	abstract = {Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to rely on approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in {\textbackslash}textsc\{mcmc\} samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicability.},
	language = {en},
	urldate = {2023-12-11},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hermans, Joeri and Begy, Volodimir and Louppe, Gilles},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4239--4248},
}

@article{huang_learning_2023,
	title = {Learning {Robust} {Statistics} for {Simulation}-based {Inference} under {Model} {Misspecification}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/16c5b4102a6b6eb061e502ce6736ad8a-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-16},
	journal = {Advances in Neural Information Processing Systems},
	author = {Huang, Daolang and Bharti, Ayush and Souza, Amauri and Acerbi, Luigi and Kaski, Samuel},
	month = dec,
	year = {2023},
	pages = {7289--7310},
}

@inproceedings{hussain_study_2019,
	address = {Cham},
	title = {A {Study} on {CNN} {Transfer} {Learning} for {Image} {Classification}},
	isbn = {978-3-319-97982-3},
	doi = {10.1007/978-3-319-97982-3_16},
	abstract = {Many image classification models have been introduced to help tackle the foremost issue of recognition accuracy. Image classification is one of the core problems in Computer Vision field with a large variety of practical applications. Examples include: object recognition for robotic manipulation, pedestrian or obstacle detection for autonomous vehicles, among others. A lot of attention has been associated with Machine Learning, specifically neural networks such as the Convolutional Neural Network (CNN) winning image classification competitions. This work proposes the study and investigation of such a CNN architecture model (i.e. Inception-v3) to establish whether it would work best in terms of accuracy and efficiency with new image datasets via Transfer Learning. The retrained model is evaluated, and the results are compared to some state-of-the-art approaches.},
	language = {en},
	booktitle = {Advances in {Computational} {Intelligence} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hussain, Mahbub and Bird, Jordan J. and Faria, Diego R.},
	editor = {Lotfi, Ahmad and Bouchachia, Hamid and Gegov, Alexander and Langensiepen, Caroline and McGinnity, Martin},
	year = {2019},
	keywords = {Caltech Face, Convolutional Neural Network (CNN), Pre-trained Model, Rectified Linear Unit (ReLU), Transfer Learning},
	pages = {191--202},
}

@inproceedings{jiang_face_2017,
	title = {Face {Detection} with the {Faster} {R}-{CNN}},
	url = {https://ieeexplore.ieee.org/abstract/document/7961803?casa_token=-q-090iAPz0AAAAA:ROfs0JGSrP6z-qHmRotNbJje6XSoXG0ph7yXylyZysmENWRdH28weBC03dXpJ7mMzWuh22nsqQ},
	doi = {10.1109/FG.2017.82},
	abstract = {While deep learning based methods for generic object detection have improved rapidly in the last two years, most approaches to face detection are still based on the R-CNN framework [11], leading to limited accuracy and processing speed. In this paper, we investigate applying the Faster RCNN [26], which has recently demonstrated impressive results on various object detection benchmarks, to face detection. By training a Faster R-CNN model on the large scale WIDER face dataset [34], we report state-of-the-art results on the WIDER test set as well as two other widely used face detection benchmarks, FDDB and the recently released IJB-A.},
	urldate = {2025-01-16},
	booktitle = {2017 12th {IEEE} {International} {Conference} on {Automatic} {Face} \& {Gesture} {Recognition} ({FG} 2017)},
	author = {Jiang, Huaizu and Learned-Miller, Erik},
	month = may,
	year = {2017},
	keywords = {Benchmark testing, Face, Face detection, Feature extraction, Object detection, Proposals, Training},
	pages = {650--657},
}

@article{larsen-freeman_transfer_2013,
	title = {Transfer of {Learning} {Transformed}},
	volume = {63},
	copyright = {© 2013 Language Learning Research Club, University of Michigan},
	issn = {1467-9922},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9922.2012.00740.x},
	doi = {10.1111/j.1467-9922.2012.00740.x},
	abstract = {Instruction is motivated by the assumption that students can transfer their learning, or apply what they have learned in school to another setting. A common problem arises when the expected transfer does not take place, what has been referred to as the inert knowledge problem. More than an academic inconvenience, the failure to transfer is a major problem, exacting individual and social costs. In this article, I trace the evolution of research on the transfer of learning, in general, and on language learning, in particular. Then, a different view of learning transfer is advanced. Rather than learners being seen to “export” what they have learned from one situation to the next, it is proposed that learners transform their learning. The article concludes by offering some suggestions for how to mitigate the inert knowledge problem from this perspective.},
	language = {en},
	number = {s1},
	urldate = {2025-01-15},
	journal = {Language Learning},
	author = {Larsen-Freeman, Diane},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9922.2012.00740.x},
	keywords = {adaptation, affordances, complexity theory, iteration, levels of processing framework, second language learning, transfer appropriate processing, transfer of learning},
	pages = {107--129},
}

@article{li_-line_2022,
	title = {On-line transfer learning for multi-fidelity data fusion with ensemble of deep neural networks},
	volume = {53},
	issn = {1474-0346},
	url = {https://www.sciencedirect.com/science/article/pii/S1474034622001495},
	doi = {10.1016/j.aei.2022.101689},
	abstract = {Deep Neural Network (DNN) is widely used in engineering applications for its ability to handle problems with almost any nonlinearities. However, it is generally difficult to obtain sufficient high-fidelity (HF) sample points for expensive optimization tasks, which may affect the generalization performance of DNN and result in inaccurate predictions. To solve this problem and improve the prediction accuracy of DNN, this paper proposes an on-line transfer learning based multi-fidelity data fusion (OTL-MFDF) method including two parts. In the first part, the ensemble of DNNs is established. Firstly, a large number of low-fidelity sample points and a few HF sample points are generated, which are used as the source dataset and target dataset, respectively. Then, the Bayesian Optimization (BO) is utilized to obtain several groups of hyperparameters, based on which DNNs are pre-trained using the source dataset. Next, these pre-trained DNNs are re-trained by fine-tuning on the target dataset, and the ensemble of DNNs is established by assigning different weights to each pre-trained DNN. In the second part, the on-line learning system is developed for adaptive updating of the ensemble of DNNs. To evaluate the uncertainty error of the predicted values of DNN and determine the location of the updated HF sample point, the query-by-committee strategy based on the ensemble of DNNs is developed. The Covariance Matrix Adaptation Evolutionary Strategies is employed as the optimizer to find out the location where the maximal disagreement is achieved by the ensemble of DNNs. The design space is partitioned by the Voronoi diagram method, and then the selected point is moved to its nearest Voronoi cell boundary to avoid clustering between the updated point and the existing sample points. Three different types of test problems and an engineering example are adopted to illustrate the effectiveness of the OTL-MFDF method. Results verify the outstanding efficiency, global prediction accuracy and applicability of the OTL-MFDF method.},
	urldate = {2025-01-16},
	journal = {Advanced Engineering Informatics},
	author = {Li, Zengcong and Zhang, Shu and Li, Hongqing and Tian, Kuo and Cheng, Zhizhong and Chen, Yan and Wang, Bo},
	month = aug,
	year = {2022},
	keywords = {Data fusion, Deep neural network, Ensemble of surrogates, Multi-fidelity model, Transfer learning},
	pages = {101689},
}

@inproceedings{lueckmann_benchmarking_2021,
	title = {Benchmarking {Simulation}-{Based} {Inference}},
	url = {https://proceedings.mlr.press/v130/lueckmann21a.html},
	abstract = {Recent advances in probabilistic modelling have led to a large number of simulation-based inference algorithms which do not require numerical evaluation of likelihoods. However, a public benchmark with appropriate performance metrics for such ’likelihood-free’ algorithms has been lacking. This has made it difficult to compare algorithms and identify their strengths and weaknesses. We set out to fill this gap: We provide a benchmark with inference tasks and suitable performance metrics, with an initial selection of algorithms including recent approaches employing neural networks and classical Approximate Bayesian Computation methods. We found that the choice of performance metric is critical, that even state-of-the-art algorithms have substantial room for improvement, and that sequential estimation improves sample efficiency. Neural network-based approaches generally exhibit better performance, but there is no uniformly best algorithm. We provide practical advice and highlight the potential of the benchmark to diagnose problems and improve algorithms. The results can be explored interactively on a companion website. All code is open source, making it possible to contribute further benchmark tasks and inference algorithms.},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David and Goncalves, Pedro and Macke, Jakob},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {343--351},
}

@inproceedings{lueckmann_flexible_2017,
	title = {Flexible statistical inference for mechanistic models of neural dynamics},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/addfa9b7e234254d26e9c7f2af1005cb-Abstract.html},
	abstract = {Mechanistic models of single-neuron dynamics have been extensively studied in computational neuroscience. However, identifying which models can quantitatively reproduce empirically measured data has been challenging. We propose to overcome this limitation by using likelihood-free inference approaches (also known as Approximate Bayesian Computation, ABC) to perform full Bayesian inference on single-neuron models. Our approach builds on recent advances in ABC by learning a neural network which maps features of the observed data to the posterior distribution over parameters. We learn a Bayesian mixture-density network approximating the posterior over multiple rounds of adaptively chosen simulations. Furthermore, we propose an efficient approach for handling missing features and parameter settings for which the simulator fails, as well as a strategy for automatically learning relevant features using recurrent neural networks. On synthetic data, our approach efficiently estimates posterior distributions and recovers ground-truth parameters. On in-vitro recordings of membrane voltages, we recover multivariate posteriors over biophysical parameters, which yield model-predicted voltage traces that accurately match empirical data. Our approach will enable neuroscientists to perform Bayesian inference on complex neuron models without having to design model-specific algorithms, closing the gap between mechanistic and statistical approaches to single-neuron modelling.},
	urldate = {2025-01-31},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lueckmann, Jan-Matthis and Goncalves, Pedro J and Bassetto, Giacomo and Öcal, Kaan and Nonnenmacher, Marcel and Macke, Jakob H},
	year = {2017},
}

@article{marjoram_markov_2003,
	title = {Markov chain {Monte} {Carlo} without likelihoods},
	volume = {100},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0306899100},
	doi = {10.1073/pnas.0306899100},
	abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
	number = {26},
	urldate = {2025-01-15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavaré, Simon},
	month = dec,
	year = {2003},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {15324--15328},
}

@article{maurais_multifidelity_2023,
	title = {Multifidelity {Covariance} {Estimation} via {Regression} on the {Manifold} of {Symmetric} {Positive} {Definite} {Matrices}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2307.12438},
	doi = {10.48550/ARXIV.2307.12438},
	abstract = {We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties enabling practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that the MRMF estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, such as data assimilation and metric learning, in which this property is essential.},
	urldate = {2025-01-30},
	journal = {SIAM Journal on Scientific Computing},
	author = {Maurais, Aimee and Alsup, Terrence and Peherstorfer, Benjamin and Marzouk, Youssef},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, FOS: Mathematics, Machine Learning (cs.LG), Numerical Analysis (math.NA)},
}

@article{nobile_multi_2015,
	title = {A {Multi} {Level} {Monte} {Carlo} method with control variate for elliptic {PDEs} with log-normal coefficients},
	volume = {3},
	issn = {2194-041X},
	url = {https://doi.org/10.1007/s40072-015-0055-9},
	doi = {10.1007/s40072-015-0055-9},
	abstract = {We consider the numerical approximation of the stochastic Darcy problem with log-normal permeability field and propose a novel Multi Level Monte Carlo (MLMC) approach with a control variate variance reduction technique on each level. We model the log-permeability as a stationary Gaussian random field with a covariance function belonging to the so called Matérn family, which includes both fields with very limited and very high spatial regularity. The control variate is obtained starting from the solution of an auxiliary problem with smoothed permeability coefficient and its expected value is effectively computed with a Stochastic Collocation method on the finest level in which the control variate is applied. We analyze the variance reduction induced by the control variate, and the total mean square error of the new estimator. To conclude we present some numerical examples and a comparison with the standard MLMC method, which shows the effectiveness of the proposed method.},
	language = {en},
	number = {3},
	urldate = {2024-12-17},
	journal = {Stochastic Partial Differential Equations: Analysis and Computations},
	author = {Nobile, Fabio and Tesei, Francesco},
	month = sep,
	year = {2015},
	keywords = {35R60, 60H35, 65C05, 65N15, 65N30, Control variate, Log-normal random-fields, Matérn covariance, Multi Level Monte Carlo, Stochastic Collocation, Stochastic Darcy Problem},
	pages = {398--444},
}

@inproceedings{panigrahi_survey_2021,
	address = {Singapore},
	title = {A {Survey} on {Transfer} {Learning}},
	isbn = {9789811559716},
	doi = {10.1007/978-981-15-5971-6_83},
	abstract = {To facilitate learning in a target domain, transfer learning borrows knowledge from a source domain. What and how to transfer are two main issues that need to be addressed in transferring learning. Different transfer learning algorithms result in different knowledge transferred between them for a couple of domains. To find the optimal transfer learning algorithm that maximizes learning efficiency in the target domain, scientists need to investigate all current computationally intractable transfer learning algorithms exhaustively. A sub-optimal algorithm is selected as a trade-off, which in an ad hoc way requires considerable expertise. In instructional psychology, meanwhile, it is commonly recognized that people enhance the transfer of teaching abilities to decide what to transfer. This paper discusses what is transfer learning, the different transfer learning techniques, future scope, and applications of it.},
	language = {en},
	booktitle = {Intelligent and {Cloud} {Computing}},
	publisher = {Springer},
	author = {Panigrahi, Santisudha and Nanda, Anuja and Swarnkar, Tripti},
	editor = {Mishra, Debahuti and Buyya, Rajkumar and Mohapatra, Prasant and Patnaik, Srikanta},
	year = {2021},
	keywords = {Computer vision, Deep learning, Domain adaptation, Machine learning, Transfer learning},
	pages = {781--789},
}

@inproceedings{papamakarios_fast_2016,
	title = {Fast {\textbackslash}epsilon -free {Inference} of {Simulation} {Models} with {Bayesian} {Conditional} {Density} {Estimation}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/6aca97005c68f1206823815f66102863-Abstract.html},
	abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ε-ball around the observed data, which is only correct in the limit ε→0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ε→0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
	urldate = {2024-12-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Papamakarios, George and Murray, Iain},
	year = {2016},
}

@inproceedings{papamakarios_fast_2018,
	title = {Fast \$ε\$-free {Inference} of {Simulation} {Models} with {Bayesian} {Conditional} {Density} {Estimation}},
	url = {http://arxiv.org/abs/1605.06376},
	abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an \${\textbackslash}epsilon\$-ball around the observed data, which is only correct in the limit \${\textbackslash}epsilon{\textbackslash}!{\textbackslash}rightarrow{\textbackslash}!0\$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as \${\textbackslash}epsilon{\textbackslash}!{\textbackslash}rightarrow{\textbackslash}!0\$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
	urldate = {2024-11-18},
	author = {Papamakarios, George and Murray, Iain},
	month = apr,
	year = {2018},
	note = {arXiv:1605.06376},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{peherstorfer_optimal_2016,
	title = {Optimal {Model} {Management} for {Multifidelity} {Monte} {Carlo} {Estimation}},
	volume = {38},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/abs/10.1137/15M1046472},
	doi = {10.1137/15M1046472},
	abstract = {Variance-based sensitivity analysis provides a quantitative measure of how uncertainty in a model input contributes to uncertainty in the model output. Such sensitivity analyses arise in a wide variety of applications and are typically computed using Monte Carlo estimation, but the many samples required for Monte Carlo to be sufficiently accurate can make these analyses intractable when the model is expensive. This work presents a multifidelity approach for estimating sensitivity indices that leverages cheaper low-fidelity models to reduce the cost of sensitivity analysis while retaining accuracy guarantees via recourse to the original, expensive model. This paper develops new multifidelity estimators for variance and for the Sobol' main and total effect sensitivity indices. We discuss strategies for dividing limited computational resources among models and specify a recommended strategy. Results are presented for the Ishigami function and a convection-diffusion-reaction model that demonstrate up to \$10{\textbackslash}times\$ speedups for fixed convergence levels. For the problems tested, the multifidelity approach allows inputs to be definitively ranked in importance when Monte Carlo alone fails to do so.},
	number = {5},
	urldate = {2024-12-17},
	journal = {SIAM Journal on Scientific Computing},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	month = jan,
	year = {2016},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {A3163--A3194},
}

@article{peherstorfer_survey_2018,
	title = {Survey of {Multifidelity} {Methods} in {Uncertainty} {Propagation}, {Inference}, and {Optimization}},
	volume = {60},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/16M1082469},
	doi = {10.1137/16M1082469},
	abstract = {This work presents an optimal model management strategy that exploits multifidelity surrogate models to accelerate the estimation of statistics of outputs of computationally expensive high-fidelity models. Existing acceleration methods typically exploit a multilevel hierarchy of surrogate models that follow a known rate of error decay and computational costs; however, a general collection of surrogate models, which may include projection-based reduced models, data-fit models, support vector machines, and simplified-physics models, does not necessarily give rise to such a hierarchy. Our multifidelity approach provides a framework to combine an arbitrary number of surrogate models of any type. Instead of relying on error and cost rates, an optimization problem balances the number of model evaluations across the high-fidelity and surrogate models with respect to error and costs. We show that a unique analytic solution of the model management optimization problem exists under mild conditions on the models. Our multifidelity method makes occasional recourse to the high-fidelity model; in doing so it provides an unbiased estimator of the statistics of the high-fidelity model, even in the absence of error bounds and error estimators for the surrogate models. Numerical experiments with linear and nonlinear examples show that speedups by orders of magnitude are obtained compared to Monte Carlo estimation that invokes a single model only.},
	number = {3},
	urldate = {2024-02-11},
	journal = {SIAM Review},
	author = {Peherstorfer, Benjamin and Willcox, Karen and Gunzburger, Max},
	month = jan,
	year = {2018},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Multifidelity},
	pages = {550--591},
}

@article{prescott_efficient_2024,
	title = {Efficient multifidelity likelihood-free {Bayesian} inference with adaptive computational resource allocation},
	volume = {496},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999123006721},
	doi = {10.1016/j.jcp.2023.112577},
	abstract = {Likelihood-free Bayesian inference algorithms are popular methods for inferring the parameters of complex stochastic models with intractable likelihoods. These algorithms characteristically rely heavily on repeated model simulations. However, whenever the computational cost of simulation is even moderately expensive, the significant burden incurred by likelihood-free algorithms leaves them infeasible for many practical applications. The multifidelity approach has been introduced in the context of approximate Bayesian computation to reduce the simulation burden of likelihood-free inference without loss of accuracy, by using the information provided by simulating computationally cheap, approximate models in place of the model of interest. In this work we demonstrate that multifidelity techniques can be applied in the general likelihood-free Bayesian inference setting. Analytical results on the optimal allocation of computational resources to simulations at different levels of fidelity are derived, and subsequently implemented practically. We provide an adaptive multifidelity likelihood-free inference algorithm that learns the relationships between models at different fidelities and adapts resource allocation accordingly, and demonstrate that this algorithm produces posterior estimates with near-optimal efficiency.},
	urldate = {2025-01-13},
	journal = {Journal of Computational Physics},
	author = {Prescott, Thomas P. and Warne, David J. and Baker, Ruth E.},
	month = jan,
	year = {2024},
	keywords = {Likelihood-free Bayesian inference, Multifidelity, Multifidelity approaches, SBI},
	pages = {112577},
}

@article{prescott_multifidelity_2020,
	title = {Multifidelity {Approximate} {Bayesian} {Computation}},
	volume = {8},
	url = {https://epubs.siam.org/doi/10.1137/18M1229742},
	doi = {10.1137/18M1229742},
	abstract = {Multifidelity approximate Bayesian computation (MF-ABC) is a likelihood-free technique for parameter inference that exploits model approximations to significantly increase the speed of ABC algorithms [T. P. Prescott and R. E. Baker, SIAM/ASA J. Uncertain. Quantif., 8 (2020), pp. 114--138]. Previous work has considered MF-ABC only in the context of rejection sampling, which does not explore parameter space particularly efficiently. In this work, we integrate the multifidelity approach with the ABC sequential Monte Carlo (ABC-SMC) algorithm into a new MF-ABC-SMC algorithm. We show that the improvements generated by each of ABC-SMC and MF-ABC to the efficiency of generating Monte Carlo samples and estimates from the ABC posterior are amplified when the two techniques are used together.},
	number = {1},
	urldate = {2025-01-30},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Prescott, Thomas P. and Baker, Ruth E.},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {114--138},
}

@article{prescott_multifidelity_2021,
	title = {Multifidelity {Approximate} {Bayesian} {Computation} with {Sequential} {Monte} {Carlo} {Parameter} {Sampling}},
	volume = {9},
	url = {https://epubs.siam.org/doi/abs/10.1137/20M1316160},
	doi = {10.1137/20M1316160},
	abstract = {A vital stage in the mathematical modeling of real-world systems is to calibrate a model's parameters to observed data. Likelihood-free parameter inference methods, such as approximate Bayesian computation (ABC), build Monte Carlo samples of the uncertain parameter distribution by comparing the data with large numbers of model simulations. However, the computational expense of generating these simulations forms a significant bottleneck in the practical application of such methods. We identify how simulations of corresponding cheap, low-fidelity models have been used separately in two complementary ways to reduce the computational expense of building these samples, at the cost of introducing additional variance to the resulting parameter estimates. We explore how these approaches can be unified so that cost and benefit are optimally balanced, and we characterize the optimal choice of how often to simulate from cheap, low-fidelity models in place of expensive, high-fidelity models in Monte Carlo ABC algorithms. The resulting early accept/reject multifidelity ABC algorithm that we propose is shown to give improved performance over existing multifidelity and high-fidelity approaches.},
	number = {2},
	urldate = {2023-10-23},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Prescott, Thomas P. and Baker, Ruth E.},
	month = jan,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Multifidelity, SBI},
	pages = {788--817},
}

@article{pritchard_population_1999,
	title = {Population growth of human {Y} chromosomes: a study of {Y} chromosome microsatellites.},
	volume = {16},
	issn = {0737-4038},
	shorttitle = {Population growth of human {Y} chromosomes},
	url = {https://doi.org/10.1093/oxfordjournals.molbev.a026091},
	doi = {10.1093/oxfordjournals.molbev.a026091},
	abstract = {We use variation at a set of eight human Y chromosome microsatellite loci to investigate the demographic history of the Y chromosome. Instead of assuming a population of constant size, as in most of the previous work on the Y chromosome, we consider a model which permits a period of recent population growth. We show that for most of the populations in our sample this model fits the data far better than a model with no growth. We estimate the demographic parameters of this model for each population and also the time to the most recent common ancestor. Since there is some uncertainty about the details of the microsatellite mutation process, we consider several plausible mutation schemes and estimate the variance in mutation size simultaneously with the demographic parameters of interest. Our finding of a recent common ancestor (probably in the last 120,000 years), coupled with a strong signal of demographic expansion in all populations, suggests either a recent human expansion from a small ancestral population, or natural selection acting on the Y chromosome.},
	number = {12},
	urldate = {2023-10-19},
	journal = {Molecular Biology and Evolution},
	author = {Pritchard, J K and Seielstad, M T and Perez-Lezaun, A and Feldman, M W},
	month = dec,
	year = {1999},
	keywords = {History, SBI},
	pages = {1791--1798},
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	urldate = {2025-01-16},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@misc{schmitt_detecting_2024,
	title = {Detecting {Model} {Misspecification} in {Amortized} {Bayesian} {Inference} with {Neural} {Networks}: {An} {Extended} {Investigation}},
	shorttitle = {Detecting {Model} {Misspecification} in {Amortized} {Bayesian} {Inference} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/2406.03154},
	doi = {10.48550/arXiv.2406.03154},
	abstract = {Recent advances in probabilistic deep learning enable efficient amortized Bayesian inference in settings where the likelihood function is only implicitly defined by a simulation program (simulation-based inference; SBI). But how faithful is such inference if the simulation represents reality somewhat inaccurately, that is, if the true system behavior at test time deviates from the one seen during training? We conceptualize the types of such model misspecification arising in SBI and systematically investigate how the performance of neural posterior approximators gradually deteriorates as a consequence, making inference results less and less trustworthy. To notify users about this problem, we propose a new misspecification measure that can be trained in an unsupervised fashion (i.e., without training data from the true distribution) and reliably detects model misspecification at test time. Our experiments clearly demonstrate the utility of our new measure both on toy examples with an analytical ground-truth and on representative scientific tasks in cell biology, cognitive decision making, disease outbreak dynamics, and computer vision. We show how the proposed misspecification test warns users about suspicious outputs, raises an alarm when predictions are not trustworthy, and guides model designers in their search for better simulators.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Schmitt, Marvin and Bürkner, Paul-Christian and Köthe, Ullrich and Radev, Stefan T.},
	month = jun,
	year = {2024},
	note = {arXiv:2406.03154 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{tavare_inferring_1997,
	title = {Inferring coalescence times from {DNA} sequence data},
	volume = {145},
	issn = {0016-6731},
	doi = {10.1093/genetics/145.2.505},
	abstract = {The paper is concerned with methods for the estimation of the coalescence time (time since the most recent common ancestor) of a sample of intraspecies DNA sequences. The methods take advantage of prior knowledge of population demography, in addition to the molecular data. While some theoretical results are presented, a central focus is on computational methods. These methods are easy to implement, and, since explicit formulae tend to be either unavailable or unilluminating, they are also more useful and more informative in most applications. Extensions are presented that allow for the effects of uncertainty in our knowledge of population size and mutation rates, for variability in population sizes, for regions of different mutation rate, and for inference concerning the coalescence time of the entire population. The methods are illustrated using recent data from the human Y chromosome.},
	language = {eng},
	number = {2},
	journal = {Genetics},
	author = {Tavaré, S. and Balding, D. J. and Griffiths, R. C. and Donnelly, P.},
	month = feb,
	year = {1997},
	pmid = {9071603},
	pmcid = {PMC1207814},
	keywords = {Algorithms, DNA, Databases, Factual, History, Humans, SBI, Time Factors},
	pages = {505--518},
}

@article{vo_bayesian_2019,
	title = {Bayesian {Estimation} for {Stochastic} {Gene} {Expression} {Using} {Multifidelity} {Models}},
	volume = {123},
	issn = {1520-5207},
	doi = {10.1021/acs.jpcb.8b10946},
	abstract = {The finite state projection (FSP) approach to solving the chemical master equation has enabled successful inference of discrete stochastic models to predict single-cell gene regulation dynamics. Unfortunately, the FSP approach is highly computationally intensive for all but the simplest models, an issue that is highly problematic when parameter inference and uncertainty quantification takes enormous numbers of parameter evaluations. To address this issue, we propose two new computational methods for the Bayesian inference of stochastic gene expression parameters given single-cell experiments. We formulate and verify an adaptive delayed acceptance Metropolis-Hastings (ADAMH) algorithm to utilize with reduced Krylov-basis projections of the FSP. We then introduce an extension of the ADAMH into a hybrid scheme that consists of an initial phase to construct a reduced model and a faster second phase to sample from the approximate posterior distribution determined by the constructed model. We test and compare both algorithms to an adaptive Metropolis algorithm with full FSP-based likelihood evaluations on three example models and simulated data to show that the new ADAMH variants achieve substantial speedup in comparison to the full FSP approach. By reducing the computational costs of parameter estimation, we expect the ADAMH approach to enable efficient data-driven estimation for more complex gene regulation models.},
	language = {eng},
	number = {10},
	journal = {The Journal of Physical Chemistry. B},
	author = {Vo, Huy D. and Fox, Zachary and Baetica, Ania and Munsky, Brian},
	month = mar,
	year = {2019},
	pmid = {30777763},
	pmcid = {PMC6697484},
	keywords = {Algorithms, Bayes Theorem, Biochemical Phenomena, Computer Simulation, Gene Expression, Models, Biological, Models, Chemical, Stochastic Processes},
	pages = {2217--2234},
}

@misc{wang_local_2024,
	title = {Local transfer learning {Gaussian} process modeling, with applications to surrogate modeling of expensive computer simulators},
	url = {http://arxiv.org/abs/2410.12690},
	doi = {10.48550/arXiv.2410.12690},
	abstract = {A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Wang, Xinming and Mak, Simon and Miller, John and Wu, Jianguo},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12690 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{ward_robust_2022,
	title = {Robust {Neural} {Posterior} {Estimation} and {Statistical} {Model} {Criticism}},
	url = {http://arxiv.org/abs/2210.06564},
	doi = {10.48550/arXiv.2210.06564},
	abstract = {Computer simulations have proven a valuable tool for understanding complex phenomena across the sciences. However, the utility of simulators for modelling and forecasting purposes is often restricted by low data quality, as well as practical limits to model fidelity. In order to circumvent these difficulties, we argue that modellers must treat simulators as idealistic representations of the true data generating process, and consequently should thoughtfully consider the risk of model misspecification. In this work we revisit neural posterior estimation (NPE), a class of algorithms that enable black-box parameter inference in simulation models, and consider the implication of a simulation-to-reality gap. While recent works have demonstrated reliable performance of these methods, the analyses have been performed using synthetic data generated by the simulator model itself, and have therefore only addressed the well-specified case. In this paper, we find that the presence of misspecification, in contrast, leads to unreliable inference when NPE is used naively. As a remedy we argue that principled scientific inquiry with simulators should incorporate a model criticism component, to facilitate interpretable identification of misspecification and a robust inference component, to fit 'wrong but useful' models. We propose robust neural posterior estimation (RNPE), an extension of NPE to simultaneously achieve both these aims, through explicitly modelling the discrepancies between simulations and the observed data. We assess the approach on a range of artificially misspecified examples, and find RNPE performs well across the tasks, whereas naively using NPE leads to misleading and erratic posteriors.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Ward, Daniel and Cannon, Patrick and Beaumont, Mark and Fasiolo, Matteo and Schmon, Sebastian M.},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06564 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{warne_multifidelity_2022,
	title = {Multifidelity multilevel {Monte} {Carlo} to accelerate approximate {Bayesian} parameter inference for partially observed stochastic processes},
	volume = {469},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999122006052},
	doi = {10.1016/j.jcp.2022.111543},
	abstract = {Semantic Scholar extracted view of "Multifidelity multilevel Monte Carlo to accelerate approximate Bayesian parameter inference for partially observed stochastic processes" by D. Warne et al.},
	language = {en},
	urldate = {2025-01-30},
	journal = {Journal of Computational Physics},
	author = {Warne, David J. and Prescott, Thomas P. and Baker, Ruth E. and Simpson, Matthew J.},
	month = nov,
	year = {2022},
	pages = {111543},
}

@misc{wehenkel_addressing_2024,
	title = {Addressing {Misspecification} in {Simulation}-based {Inference} through {Data}-driven {Calibration}},
	url = {http://arxiv.org/abs/2405.08719},
	doi = {10.48550/arXiv.2405.08719},
	abstract = {Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability. This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations. Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator. Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals.},
	urldate = {2024-09-16},
	publisher = {arXiv},
	author = {Wehenkel, Antoine and Gamella, Juan L. and Sener, Ozan and Behrmann, Jens and Sapiro, Guillermo and Cuturi, Marco and Jacobsen, Jörn-Henrik},
	month = may,
	year = {2024},
	note = {arXiv:2405.08719 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Multifidelity, SBI, Statistics - Machine Learning, Statistics - Methodology, important},
}

@article{zeng_multifidelity_2023,
	title = {Multifidelity uncertainty quantification with models based on dissimilar parameters},
	volume = {415},
	issn = {00457825},
	url = {http://arxiv.org/abs/2304.08644},
	doi = {10.1016/j.cma.2023.116205},
	abstract = {Multifidelity uncertainty quantification (MF UQ) sampling approaches have been shown to significantly reduce the variance of statistical estimators while preserving the bias of the highest-fidelity model, provided that the low-fidelity models are well correlated. However, maintaining a high level of correlation can be challenging, especially when models depend on different input uncertain parameters, which drastically reduces the correlation. Existing MF UQ approaches do not adequately address this issue. In this work, we propose a new sampling strategy that exploits a shared space to improve the correlation among models with dissimilar parametrization. We achieve this by transforming the original coordinates onto an auxiliary manifold using the adaptive basis (AB) method{\textasciitilde}{\textbackslash}cite\{Tipireddy2014\}. The AB method has two main benefits: (1) it provides an effective tool to identify the low-dimensional manifold on which each model can be represented, and (2) it enables easy transformation of polynomial chaos representations from high- to low-dimensional spaces. This latter feature is used to identify a shared manifold among models without requiring additional evaluations. We present two algorithmic flavors of the new estimator to cover different analysis scenarios, including those with legacy and non-legacy high-fidelity data. We provide numerical results for analytical examples, a direct field acoustic test, and a finite element model of a nuclear fuel assembly. For all examples, we compare the proposed strategy against both single-fidelity and MF estimators based on the original model parametrization.},
	urldate = {2024-12-17},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Zeng, Xiaoshu and Geraci, Gianluca and Eldred, Michael S. and Jakeman, John D. and Gorodetsky, Alex A. and Ghanem, Roger},
	month = oct,
	year = {2023},
	note = {arXiv:2304.08644 [physics]},
	keywords = {Physics - Data Analysis, Statistics and Probability},
	pages = {116205},
}

