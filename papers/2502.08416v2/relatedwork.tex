\section{Related work}
% \subsection{SBI via neural density estimation}

% The challenge of extending sampling-based methods \cite{tavare_inferring_1997, pritchard_population_1999} to high-dimensional problems has driven significant advancements in simulation-based inference, catalysed by developments in probabilistic deep learning \cite{}. Modern SBI approaches aim for the direct estimation of the posterior distribution (e.g., Neural Posterior Estimation (NPE) \cite{papamakarios_fast_2016, lueckmann_flexible_2017, greenberg_automatic_2019}), the likelihood function (e.g., Neural Likelihood Estimation (NLE) \cite{papamakarios_fast_2018}), or likelihood ratios (e.g., Neural Ratio Estimation (NRE) \cite{hermans_likelihood-free_2020}). Each class of these methods has sequential variants, enabling adaptive sampling to improve simulation efficiency given single empirical observations.

% Building on NPE, we introduce a multifidelity framework to extend its applicability to costly simulators. By incorporating multiple levels of fidelity in simulators, our approach enables the efficient and accurate estimation of posterior distributions even for computationally intensive scenarios.


\subsection{Multifidelity methods for inference} % in machine learning

%It is a common practice in science to develop models that vary in terms of accuracy and computational efficiency, in order to handle the complexity and computational cost of high-fidelity models \cite{peherstorfer_survey_2018, fernandez-godino_review_2023}. 


%Low-fidelity models are seen as simplifications of high-fidelity models, usually made possible through domain expertise \cite{han_improving_2013}, low-dimensional projection of the high-fidelity model \cite{berkooz_proper_1993}, or interpolation or regression through surrogate models \cite{forrester_recent_2009}.
Multifidelity has been widely explored in the context of likelihood-based inference \cite{peherstorfer_survey_2018}, from maximum likelihood estimation approaches \cite{maurais_multifidelity_2023} to Bayesian inference methods \cite{vo_bayesian_2019, catanach_bayesian_2020}.

Currently, few studies tackle the scenario where the likelihood is not explicitly available \cite{warne_multifidelity_2022, prescott_efficient_2024, prescott_multifidelity_2021}. For instance, some multifidelity sampling-based methods have been proposed within the framework of ABC \cite{prescott_multifidelity_2020} or sequential Monte Carlo ABC \cite{prescott_multifidelity_2021}. However, despite their potential for multifidelity inference, these methods inherit limitations of ABC approaches, particularly in high-dimensional parameter spaces, where neural density estimators offer more scalable alternatives to complex real-world problems \cite{lueckmann_benchmarking_2021}.

% 1. multifidelity for models where you have a likelihood-based inference (bayesian and non-bayesian: e.g., max likelihood)
% bayesian methods have all likelihood
% and aiming for maximizing likelihood or...
% 2. Likelihood-free methods
% We don't have a likelihood. Only other example:
% The use of multiple models in likelihood-free bayesian inference has been applied only for for ABC : MF-ABC\cite{prescott_multifidelity_2020, prescott_multifidelity_2021}.
% Inherits limitations of ABC approaches, namely scaling with dimensionality of parameter spaces
% How different from ours?

% To accelerate the outputs of computationally expensive (i.e., time-consuming) high-fidelity models, recent studies proposed approaches combining multifidelity and ABC \cite{prescott_multifidelity_2020, prescott_multifidelity_2021, prescott_efficient_2024}. 

% proposed to accelerate the outputs of summary statistics.
%  When the cost of generating individual stochastic samples is high,

% Multifidelity Monte Carlo approaches incorporate typically coarse approximations as low-fidelity models, with controllable discretization parameters \cite{peherstorfer_optimal_2016, nobile_multi_2015, giles_multilevel_2008, zeng_multifidelity_2023}, 
% However, the results do typically not scale well with the dimensionality of the spaces. 

Here, we combine multifidelity approaches with neural density estimation, enabling its application to inference problems with large numbers of model parameters.

\subsection{Transfer learning and simulators} 

% approach here:
% (1) describe transfer learning in general very briefly, with key citations;
% (2) discuss previous approaches to transfer learning in the multifidelity setting (if any exist);
% (3) discuss previous approaches to transfer learning in neural density estimation (if any exist);
% (4) discuss previous approaches to transfer learning in the SBI setting (if any exist)

To facilitate learning in a target domain, transfer learning borrows knowledge from a source domain \cite{panigrahi_survey_2021}. This is often done when the target dataset is smaller than the source dataset \cite{larsen-freeman_transfer_2013} and has successfully been applied to a range of machine learning tasks, e.g., in computer vision \cite{jiang_face_2017, hussain_study_2019}. 

%This approach has been successfully applied to a range of tasks, e.g., in computer vision \cite{jiang_face_2017, hussain_study_2019}.
In the context of numerical simulators, transfer learning approaches have been used to lower the simulation budget, for instance, in CO$_2$ forecasting \cite{falola_rapid_2023}, surrogate modeling \cite{wang_local_2024} and model inversion with physics-informed neural networks \cite{haghighat_physics-informed_2021}. To the best of our knowledge, the potential of transfer learning for computationally-efficient simulation-based inference has not been realized yet.

% We do not use the inverse properties of normalizing flow, removing the need to tune an additional hyperparameter.

% Point out: physics based models don't do inference, only fitting models to data


%(rather than inference), 
% Previous transfer learning approaches in a multifidelity setting include the work of \citet{chakraborty_transfer_2021}, which explores the use of physics-informed deep neural networks \cite{raissi_physics-informed_2019}. 

% Another direction explored by \citet{li_-line_2022} uses an online transfer learning approach that fuses multifidelity data within an ensemble of deep neural networks and incorporates Bayesian Optimization (BO) to lower the cost of sampling points for expensive optimization tasks. 

% In the context of normalizing flows, \citet{gambardella_transflow_2019} developed a method that does not require any additional training in the context of neural style transfer. The variance of the likelihood is tuned by hyperparameter $\lambda$, which is similar to the tolerance error in Approximate Bayesian Computation \cite{marjoram_markov_2003, beaumont_approximate_2002}.

% We do not use the inverse properties of normalizing flow, removing the need to tune an additional hyperparameter.
% explored transfer learning in the context of normalizing flows. Leveraging its inverse properties, they

% This is different from our approach since we do not rely on additional hyperparameter tuning for the likelihood variance.


\subsection{Model misspecification} 

Our work has connections to the issue of model misspecification, a fundamental and widely studied problem in SBI \cite{gao_generalized_2023, ward_robust_2022, huang_learning_2023, frazier_synthetic_2024, adachi_reversal_2010, schmitt_detecting_2024, cannon_investigating_2022}. The issue arises when the probability over the true observation does not fall within the family of estimated probability distributions defined by a model.

A misspecified model can be seen as a form of low-fidelity model of the data-generating process (the high-fidelity model) since this will often rely on several simplifying assumptions.
% \citet{schmitt_detecting_2024} proposed a misspecification measure that precludes the need for true observations, and \citet{cannon_investigating_2022} investigated the impact of misspecification through empirical tests. The problem has been addressed with a regularized loss function to penalize the statistics that increase the mismatch between the data and the model \cite{huang_learning_2023} 
With this perspective in mind, ROPE can be considered a multifidelity approach to SBI \cite{wehenkel_addressing_2024}: model misspecification is addressed by providing a small set of real-world observations that re-calibrate through optimal transport the posterior estimates of the misspecified model. In contrast with our work, this scheme does not allow the active sampling of high-fidelity simulations. 

%It also does not allow posterior predictive checks.
% They rely on calibration set: as high-fdielity model (as if high-feilty simulator). Similar problem, but not simulator, but real empirical observation data. Imprtant is that we have an online method (they don't have). You get what you measure, and don't generate with high fidleit model (so no synthetic data). So no active scheme because we cannot sample for informative samples! active sampling but fixed there set of hf simulations. Also, we have the high-fidelity simulator, can do ppc and so on, they dont

% Here, the low-fidelity model is often seen as the misspecified model while relying on a small calibration set  \cite{}.