\section{Related Work}
\label{sec:sec2}
This section surveys current research trends in 3D MOT systems and uncertainty estimation strategies for object detection and tracking.

\subsection{3D Multi-Object Tracking Systems}
\label{sec:sec2a}

\subsubsection{LiDAR-based 3D MOT}
LiDAR-based 3D MOT systems typically follow a tracking-by-detection paradigm and utilize CV motion models with Kalman filtering \cite{ab3dmot,probabilistic,score,simpletrack}. Recent trends focus on seamlessly integrating velocity or confidence cues from the detector \cite{centerpoint}, as well as refining the association stage through spatiotemporal attention \cite{3dmot}. These enhancements improve overall accuracy but can suffer under adverse weather or sensor occlusion.

\subsubsection{Camera-based 3D MOT}
Camera-only 3D MOT systems rely heavily on dense RGB inputs and leverage various network architectures (e.g., LSTM-based motion models \cite{mono,cc3dt} and attention mechanisms \cite{mutr3d,spatio}). While such systems benefit from rich semantic details, their performance is limited in low-visibility or adverse weather conditions.

\subsubsection{LiDAR–Camera Fusion 3D MOT}
Fusion-based approaches combine LiDAR’s precise depth measurements with camera’s dense appearance cues, either in a tracking-by-detection pipeline \cite{gnn3dmot} or a joint detection and tracking framework \cite{jmodt,alphatrack}. While these systems often achieve superior performance under good conditions, they degrade significantly when either sensor becomes unreliable.

\subsubsection{4D Radar-based 3D MOT}
4D Radar technology offers a robust solution for tracking under challenging conditions due to its ability to provide 3D position and Doppler velocity, remaining relatively unaffected by fog, rain, or snow. While noise and resolution are still limiting factors compared to LiDAR, 4D Radar is quickly advancing toward more accurate 3D object detection and tracking \cite{kradar,tj4d}. This paper focuses on 4D Radar as the primary sensor, aiming to leverage its resilience under adverse conditions and direct velocity measurements.

\subsection{Uncertainty Estimation for Object Detection and Tracking}
\label{sec:sec2b}

\subsubsection{Types of Uncertainty}
Uncertainty in deep learning broadly falls into two categories: \emph{aleatoric uncertainty}, which originates from inherent sensor noise or measurement inconsistencies, and \emph{epistemic uncertainty}, stemming from model uncertainty or limited training data \cite{lossattenuation}.

\subsubsection{Uncertainty Estimation Methods}
Common strategies for estimating uncertainty include MC Dropout \cite{mcdropout}, Deep Ensembles \cite{deepensemble}, and Loss Attenuation \cite{lossattenuation}. 
MC Dropout repeatedly samples the network at inference by randomly dropping neurons, approximating the posterior distribution over network parameters. Deep Ensembles trains multiple independent models and aggregate their predictions. Loss Attenuation uses Gaussian-based modeling to dynamically weight the training loss according to estimated input or output uncertainty. These methods have been successfully employed in depth estimation \cite{depth}, semantic segmentation \cite{stochasticseg}, object detection \cite{lidaruncertainty,griduncertainty}, and visual tracking \cite{trackuncertainty}.

\subsubsection{Uncertainty in Detection and Tracking}
Although uncertainty estimation has proven beneficial in a variety of perception tasks \cite{reviewuncertainty}, relatively few 3D MOT systems integrate uncertainty estimates in both detection and prediction modules. For instance, \cite{lidaruncertainty} and \cite{griduncertainty} compute epistemic and aleatoric uncertainties in 3D detection tasks. Meanwhile, \cite{trackuncertainty} uses uncertainty to filter out low-confidence training samples in visual tracking. This paper tries to address the gap by applying Bayesian approximation in both detection and prediction for a 4D Radar-based 3D MOT system, aiming to improve robustness in uncertain environments.