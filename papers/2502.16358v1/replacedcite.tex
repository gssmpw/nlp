\section{Related Work}
\label{s:related_work}
In this section, we review prior work on QA, MCQA, and QARA. While substantial progress has been made in these areas, most existing approaches lack explicit mechanisms for leveraging and evaluating the plausibility of candidate answers.

\subsection{QA Task}
QA has long been a core task in IR/NLP, leading to the development of numerous datasets designed to evaluate and enhance QA models____. Early datasets, such as SQuAD____, focus on extractive QA, where models locate text spans within a passage to form answers. More recent datasets, including CoQA____ and HotpotQA____, expand upon this by incorporating conversational QA and multi-hop reasoning. While these datasets prioritize factual correctness, they largely overlook the role of candidate answers in model evaluation.
Generative models have further transformed QA by producing free-form answers rather than relying solely on retrieval-based or extractive approaches____. LLMs such as GPT-4____, T0____, and T5____ generate fluent and contextually appropriate responses but do not account for candidate answers and focus only on correct answers.

To address this gap, our proposed pipeline extends existing QA datasets by generating candidate answers for questions and incorporating plausibility scores for candidate answers. This approach enables a more nuanced evaluation of QA models, moving beyond a binary correctness framework and offering deeper insights into model reasoning.

\subsection{QA Robustness Assessment}
QA robustness refers to a modelâ€™s ability to maintain accuracy across different types of adversarial inputs or variations in question phrasing____. Prior work has focused on adversarial QA datasets such as AddSent____, which introduces misleading sentences into contexts to challenge models. Similarly, ContrastiveQA____ evaluates robustness by modifying questions while keeping the correct answers unchanged. 

However, existing robustness studies often rely on explicit adversarial attacks such as manipulating input data to deliberately mislead models, altering text features to create confounding effects, or introducing subtly modified but incorrect information rather than naturally plausible wrong answers, which can also serve as a robustness test. \datasetname provides candidate answers with plausibility scores, allowing a more fine-grained robustness evaluation by measuring how well models distinguish between the correct answer and highly plausible candidate answers.

\subsection{MCQA Task}
MCQA requires models to select the correct answer from a set of options, often including distractors designed to challenge the model____. Datasets like RACE____ and ARC____ contain human-crafted distractors, while others, such as SciQ____, are generated automatically from scientific texts, with distractors derived from retrieved passages to improve model comprehension.
% of scientific knowledge.

Despite their effectiveness, most MCQA datasets do not rank distractors based on plausibility. This limits the ability to generate adaptive MCQA tasks based on difficulty levels. In contrast, our dataset incorporates plausibility scores, enabling dynamic MCQA assessments that adjust distractor selection based 
on difficulty.

% \subsection{QDE Task}
% QDE refers to question difficulty prediction using various approaches. Linguistic methods capture structural properties of questions and are often used with Random Forest models____. Readability index-based approaches assess text complexity____ and use it as an indicator of difficulty. Some methods leverage TF-IDF____ and word2vec embeddings____ to capture frequency-based and semantic representations, respectively, and estimate difficulty based on these features. Hybrid approaches combine multiple linguistic and statistical features to improve prediction accuracy____. More recently, Transformer-based models, such as BERT____ and DistilBERT____, have been fine-tuned for QDE____, leveraging contextual understanding for enhanced performance.

% One limitation of existing approaches is that they often rely solely on correct answers to gauge difficulty. Our work introduces entropy-based QDE using plausibility scores, where questions with highly plausible candidate answers are considered more difficult. This novel approach provides a more flexible and interpretable measure of difficulty, useful for educational applications and adaptive testing.