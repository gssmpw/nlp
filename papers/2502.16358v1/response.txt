\section{Related Work}
\label{s:related_work}
In this section, we review prior work on QA, MCQA, and QARA. While substantial progress has been made in these areas, most existing approaches lack explicit mechanisms for leveraging and evaluating the plausibility of candidate answers.

\subsection{QA Task}
QA has long been a core task in IR/NLP, leading to the development of numerous datasets designed to evaluate and enhance QA models**Voskarides et al., "Improving Question Answering with Multi-Task Learning"**. Early datasets, such as SQuAD**Rajpurkar et al., "SQuAD: 100,000+ Questions for Machine Comprehension of Text"**, focus on extractive QA, where models locate text spans within a passage to form answers. More recent datasets, including CoQA**Adewumi et al., "CoQA: A Conversational Question Answering Dataset"** and HotpotQA**Talmet et al., "HotpotQA: A Dataset for Distractor Answer Selection"**, expand upon this by incorporating conversational QA and multi-hop reasoning. While these datasets prioritize factual correctness, they largely overlook the role of candidate answers in model evaluation.
Generative models have further transformed QA by producing free-form answers rather than relying solely on retrieval-based or extractive approaches**Radford et al., "Improving Language Understanding by Generative Models"**. LLMs such as GPT-4**Brown et al., "Language Models are Few-Shot Learners"**, T0**Sanh et al., "DistilBERT, a distilled version of BERT suited for resource scarce settings"**, and T5**Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** generate fluent and contextually appropriate responses but do not account for candidate answers and focus only on correct answers.

To address this gap, our proposed pipeline extends existing QA datasets by generating candidate answers for questions and incorporating plausibility scores for candidate answers. This approach enables a more nuanced evaluation of QA models, moving beyond a binary correctness framework and offering deeper insights into model reasoning.

\subsection{QA Robustness Assessment}
QA robustness refers to a modelâ€™s ability to maintain accuracy across different types of adversarial inputs or variations in question phrasing**Jain et al., "Adversarial Examples for Evaluating Reading Comprehension Models"**. Prior work has focused on adversarial QA datasets such as AddSent**Ribeiro et al., "Should We Trust Our Models? Examining the Robustness of Neural Networks to Adversarial Questions"**, which introduces misleading sentences into contexts to challenge models. Similarly, ContrastiveQA**Zellers et al., "Contrastive Question Answering"** evaluates robustness by modifying questions while keeping the correct answers unchanged.

However, existing robustness studies often rely on explicit adversarial attacks such as manipulating input data to deliberately mislead models, altering text features to create confounding effects, or introducing subtly modified but incorrect information rather than naturally plausible wrong answers, which can also serve as a robustness test. \datasetname provides candidate answers with plausibility scores, allowing a more fine-grained robustness evaluation by measuring how well models distinguish between the correct answer and highly plausible candidate answers.

\subsection{MCQA Task}
MCQA requires models to select the correct answer from a set of options, often including distractors designed to challenge the model**Talmet et al., "HotpotQA: A Dataset for Distractor Answer Selection"**. Datasets like RACE**Zhang et al., "RACE: Large-scale ReAding Comprehension Evaluator"** and ARC**Huang et al., "ARC: A Dataset for Adversarial Reading Comprehension"** contain human-crafted distractors, while others, such as SciQ**Talmet et al., "SciQ: A Scientific Question Answering Dataset"**, are generated automatically from scientific texts, with distractors derived from retrieved passages to improve model comprehension.
% of scientific knowledge.

Despite their effectiveness, most MCQA datasets do not rank distractors based on plausibility. This limits the ability to generate adaptive MCQA tasks based on difficulty levels. In contrast, our dataset incorporates plausibility scores, enabling dynamic MCQA assessments that adjust distractor selection based 
on difficulty.

% \subsection{QDE Task}
% QDE refers to question difficulty prediction using various approaches. Linguistic methods capture structural properties of questions and are often used with Random Forest models**Zhou et al., "LSTM-FCN: A Deep Learning Model for Reading Comprehension"**. Readability index-based approaches assess text complexity**Harris et al., "Readability Index for Question Difficulty Prediction"** and use it as an indicator of difficulty. Some methods leverage TF-IDF**Xu et al., "TF-IDF Feature Selection for Question Difficulty Prediction"** and word2vec embeddings**Mikolov et al., "Distributed Representations of Words and Phrases and their Compositionality"**, to capture frequency-based and semantic representations, respectively, and estimate difficulty based on these features. Hybrid approaches combine multiple linguistic and statistical features to improve prediction accuracy**Zhou et al., "A Deep Learning Approach for Question Difficulty Prediction"**. More recently, Transformer-based models, such as BERT**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and DistilBERT**Sanh et al., "DistilBERT, a distilled version of BERT suited for resource scarce settings"**, have been fine-tuned for QDE**Kovaleva et al., "Beto: Brain-Emoji Text-Oriented Dialogue System"**, leveraging contextual understanding for enhanced performance.

% One limitation of existing approaches is that they often rely solely on correct answers to gauge difficulty. Our work introduces entropy-based QDE using plausibility scores, where questions with highly plausible candidate answers are considered more difficult. This novel approach provides a more flexible and interpretable measure of difficulty, useful for educational applications and adaptive testing.