\paragraph{Our \ours Outperforms the Baselines.}
We empirically evaluate our \ours using our \bench benchmark. The full experimental setup is specified in \Cref{app:setup}.

We solely apply \ours on SD3, due to its reliance on CLIP encoders and its strong text generation ability. 
Since SD3 leverages two CLIP text encoders \textit{$E_1$} and \textit{$E_2$}, we need to safety fine-tune both of them simultaneously. 
To reduce additional sources of potential NSFW behavior, we do not include the T5 model used by the original SD3, which was reported by~\citet{esser2024scalingSD3} to introduce marginal improvements in the model's generation ability.
The best fine-tuning hyperparameters for both encoders, identified through grid-search, are specified in \Cref{tab:ours_parameters}.
Once, safety fine-tuning terminates, we evaluate the success of our \ours. The empirical results in \Cref{fig:baselines} highlight that \ours outperforms the baselines by decreasing the NSFW text generation ability more than twice as much as the benign text generation ability, and thereby, being above the diagonal in the plot.
Additionally, when looking into the images generated after our \ours, in \Cref{fig:our_samples}, and comparing them to the samples generated after applying the baseline methods, see \Cref{fig:baseline_samples}, we observe that \ours is able to maintain benign text generation, and to mitigate NSFW text generation. We would like to note again that, due to the strict train and test split in our data, as described in \Cref{sec:bench_evaluation}, none of the NSFW words from the prompts in  \Cref{fig:our_samples} were seen during training. This highlights our approach's ability to mitigate text-to-image models' general NSFW text generation.
