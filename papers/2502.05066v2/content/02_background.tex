\section{Background and Related Work}

\textbf{Text-to-image Diffusion Models.}
DMs~\citep{song2020,ho2020,rombach2022high} learn to approximate a data distribution by training a model, $\epsilon_\theta(x_t, t, y)$, to denoise samples and reverse a stepwise diffusion process. Synthetic images are generated by initializing a sample with Gaussian noise, $x_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, and iteratively subtracting the estimated noise at each time step $t = T, \ldots, 1$, until a clean sample $x_0$ is reconstructed. 
Commonly, the denoising model $\epsilon_\theta(x_t, t, y)$ is implemented using a U-Net~\citep{ronneberger2015unet} (\eg DeepFloyd IF) or transformer-based architectures~\citep{vaswani2017attention} (\eg SD3~\citep{esser2024scalingSD3}). 
Text-to-image DMs~\citep{dalle_2,rombach2022high,DeepFloydIF} include additional conditioning on some textual description $y$ in the form of a text embedding that is obtained by a pre-trained text encoder, such as CLIP~\citep{clip} or T5~\citep{raffel2020exploringT5}.
Initially, DMs failed to produce legible and coherent text within visuals, however, newer architectures, such as FLUX, Deep Floyd IF, and SD3 integrate multiple text encoders like CLIP-based~\citep{clip} models or large language models like T5~\citep{raffel2020exploringT5} that significantly improved the quality of the generated text.

\textbf{Text-to-Image AutoRegressive Models.} Recently, a new paradigm of vision autoregressive models (\VARs) surpassed DMs in image synthesis~\citep{tian2024visual,tang2024hart}. They transfer the next-token-prediction pre-text task from the language domain to computer vision by using the next-scale (or resolution) prediction task. These models fulfill the unidirectional dependency assumption (where each next token depends only on the predecessors), preserve the 2D spatial locality, and significantly reduce the complexity of image generation. Currently, Infinity~\citep{han2024infinity} is the most performant autoregressive model for images that supports text-to-image generation. Infinity is also based on the next-scale prediction. It features an "\textit{infinite}" tokenizer with $2^{64}$ tokens, which substitutes index-wise with bitwise tokens. With this approach, Infinity outperforms previous state-of-the-art autoregressive and diffusion models. For the first time, we show that while featuring high-quality text rendering, Infinity also generates unsafe text in images.

\textbf{Harmful Visual Content Generation and Mitigation.}
Generative vision models have been shown to produce harmful content, such as NSFW imagery~\citep{qu2023unsafe,rando2022red,yang2024sneakyprompt}, even when such content is not explicitly specified in prompts~\citep{hao2024harm,li2024safegen}. 
To detect this type of behavior, multiple dedicated detectors, \eg~\citep{nsfwdetector, nudenet} have been developed. Alternatively, large visual language model-based classifiers, relying, for example, on LLaVA~\citep{liu2023llava}, InstructBLIP~\citep{instructblip}, or GPT4V~\citep{gpt4v} have shown to be effective. 
Various mitigation techniques have been proposed. For instance, Safe Latent Diffusion (SLD)~\citep{schramowski2023safe} guides generation away from unsafe concepts by adding a safety-conditioned loss during inference. Erase Stable Diffusion (ESD)~\citep{gandikota2023erasing} fine-tunes the model by steering the unconditional generation away from unsafe concepts using modified classifier-free guidance. 
Finally, \citet{zong2024safety} build a safety-alignment dataset for fine-tuning vision language models.
As an alternative, Safe-CLIP~\citep{poppi2025safe} targets the CLIP encoder underlying common DM architectures and performs multi-modal training that redirects inappropriate content while preserving embedding structure. However, these approaches are designed address visual NSFW content (\ie visual scenes of violence or nudity) and fail to tackle the issue of NSFW text embedded in the generated images as we show in \Cref{sub:existing_solutions}, leaving this severe threat unaddressed.

\textbf{Harmful Text Generation and Mitigation.}
Large language models (LLMs) have been shown to generate NSFW content~\citep{poppi2024towards,gehman2020realtoxicityprompts}, despite safety alignment being in place~\citep{wei2024assessing,ousidhoum2021probing}.
While NSFW text generation in those models considers the discrete tokens in the output space instead continuous images, the novel architectures of DMs and VARs include the textual component that can benefit from the mitigation strategies in LLMs.
Most work in the language domain focuses on fine-tuning the model to remove NSFW behavior, using either supervised examples~\citep{adolphs2023cringe} or reinforcement learning with human feedback~\citep{ouyang2022training,bai2022training}.
Other work operates on the neuron-level, identifies neurons that are responsible for toxic content and dampens these neurons~\citep{suau2024whispering}.
We evaluate the latest work on AURA~\citep{suau2024whispering} as a baseline and show that it suffers from the same limitations as existing solutions for the visual domain, highlighting the necessity of designing novel methods to address this threat in image generation.
