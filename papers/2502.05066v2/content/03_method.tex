
\section{Existing NSFW Solutions for Text or Vision Fail on Text Embedded in Images}
\label{sec:existingmitigations} 
The goal is to prevent the embedding of NSFW text in synthetic generated images. 
In this section, we explore naive solutions and existing baselines designed for the text or visual domains and show that they are ineffective in achieving this goal---either failing to prevent the generation of NSFW text or harming the model's overall text generation ability significantly.

\subsection{Naive Solutions Fail}

We start by sketching the two naive solutions that naturally present themselves when trying to prevent text-to-image models from embedding NSFW text in their generated images, and discuss why they fail.

\textbf{Attempt 1: Pre-processing Text Prompts.} As a very intuitive approach, one might want to treat the problem as purely text-based and attempt to solve it through the text prompt that causes the NSFW generation. This would involve an off-the-shelf toxicity detector, such as \citep{perspectiveAPI,Detoxify}, to evaluate input prompts. NSFW prompts could then be rewritten with a language model before generation. However, this approach has multiple limitations. 1) First, whether certain words are perceived as NSFW depends on the visual context in the output. 
\begin{wrapfigure}{r}{0.6\textwidth}
    \centering
    \includegraphics[width=0.14\linewidth]{figures/detectors_insufficiency/iloveimg-compressed-3/bastard_image.png}
    \includegraphics[width=0.14\linewidth]{figures/detectors_insufficiency/iloveimg-compressed-3/masturbation_image.png}
    \includegraphics[width=0.14\linewidth]{figures/detectors_insufficiency/iloveimg-compressed-3/pedophile_image.png}
    \includegraphics[width=0.14\linewidth]{figures/detectors_insufficiency/iloveimg-compressed-3/retard_image.png}
    \includegraphics[width=0.14\linewidth]{figures/detectors_insufficiency/iloveimg-compressed-3/toxic_image_0.png}
    \includegraphics[width=0.14\linewidth]{figures/detectors_insufficiency/iloveimg-compressed-3/anal_image.png}
    
    \caption{\textbf{OCR-based Detectors Insufficiency.} We show SD3-generated images where the extracted text receives a low toxicity score~\citep{Detoxify} ($<0.1$), while still being recognizable as offensive by human observers.
    }
    \label{fig:problems_naive_solution2}
\end{wrapfigure}
We observe that a variety of terms (\eg \textit{Cocks} or \textit{Yellow Fever}) that can be perceived offensive without the right context, are not detected as NSFW by any off-the-shelf toxic text detectors we explored, \eg~\citep{Detoxify}. For this reason, \citet{hu2024toxicitydetectionfree} argue that effective NSFW filters need access to both input and output to avoid false negatives. In our case, although the input prompt may be classified as safe, the generated text in the output images can become offensive due to the contextual elements within the visual space. For instance, the text \textit{yellow fever}, displayed in a hospital setting, typically refers to a viral disease. However, when presented with  certain demographic subgroups, it may suggest a reference to sexual preferences, creating a potentially inappropriate or offensive connotations. 2) Classification-based toxicity detectors can overly restrict benign users and introduce latency. 3) Finally, this approach is restricted to API-based models with black box access but fails for open-source or locally deployed models, where users can simply bypass the re-writing step. 

\begin{table}[t]
\centering
\scriptsize
\begin{tabular}{ccccc}
\toprule
\textbf{Model} & \textbf{MHD (\%)} & \textbf{SD Filter (\%)} & \textbf{OCR+Detoxify (\%)} \\ 
\midrule
\textbf{SD3} & 13.95  & 33.18  & 76.43 \\ 
\textbf{DeepFloydIF} & 6.40  & 34.32  & 60.64 \\
\textbf{FLUX} & 16.24 & 46.45 & 90.83\\
\textbf{SDXL} & 6.63 & 27.45 & 49.66 \\
\textbf{Infinity} & 9.67 & 31.23 & 64.78 \\ 

\bottomrule
\end{tabular}%
\caption{\textbf{Harmful Content Detection.}
We assess the success of various NSFW detection approaches to identify images with embedded NSFW words.
Multiheaded Detector (\textbf{MHD})~\citep{qu2023unsafe} and the Stable Diffusion Filter (\textbf{SD Filter})~\citep{rando2022red} are solutions built for detecting NSFW visual scenes.
OCR with Detoxify API (\textbf{OCR+Detoxify}) \cite{Detoxify} refers to our custom pipeline of using OCR to detect the words, and then performing NSFW classification with the Detoxify API.
As a baseline, 100\% of our NSFW words in the input prompt are classified as NSFW by Detoxify.
}
\label{tab:detection-results}
\end{table}
\textbf{Attempt 2: Detecting and Censoring NSFW Text in Images.} Alternatively, one could generate the image, locate the text, apply Optical Character Recognition (OCR) to extract it, classify the extracted text as NSFW or benign using a text-based toxicity detector, and then overwrite, blur, or censor NSFW text. 
While this approach shares all the limitations of the previous one (lack of context, latency, and non-applicability to open models), it has an \textit{additional} points of failure, namely the generation. 
Already with small spelling errors or artifacts, the words are not correctly detected as NSFW anymore, even though still fully recognizable as offensive by a human observer.
We quantify the detection success in the right column of \Cref{tab:detection-results} and plot examples of failure cases for NSFW detection in \Cref{fig:problems_naive_solution2}. %
Overall, for FLUX---the model with the strongest text generation capabilities and, consequently, the highest OCR accuracy---this naive approach detects only 91\% of NSFW samples, leaving 9\% of potentially harmful content undetected. Performance is even worse for other models, with detection rates dropping below 50\% for SDXL.
To explore whether visual NSFW detectors, \ie the ones trained to detect NSFW visual scenes might be less easily fooled by the spelling mistakes, we also explore the detection success of two state-of-the-art vision detectors (Multiheaded Detector~\citep{qu2023unsafe} and Stable Diffusion Filter~\citep{rando2022red}).
The results in \Cref{tab:detection-results} show that these detectors fall even further behind the solution of combining OCR with text-based detection. SD Filter still achieves up to 46.45\% detection accuracy for FLUX. This success rate is due to the underlying CLIP model, which enables the SD Filter to identify certain types of unsafe content even though it was not explicitly trained for text detection in images. CLIPâ€™s ability to associate visual elements with textual descriptions contribute to this detection performance.
Yet, with significant fractions of the NSFW samples undetected, and due to its conceptual limitations, this naive second attempt is also not sufficient to solve the problem.

\subsection{Existing Solutions are Ineffective}
\label{sub:existing_solutions}

Given the failure of naive solutions attempts in preventing NSFW text generation in synthetic images, we turn to existing state-of-the-art solution from the language and vision-language domain.
We purely focus on methods that pursue the same goal as our work, namely making the model itself safe, such that it can be openly deployed~\citep{suau2024whispering,gandikota2023erasing,poppi2025safe}, rather than fixing safety issues during deployment, \eg \citep{schramowski2023safe}, which is limited to API-based models.

\textbf{AURA~\citep{suau2024whispering}.} The AURA method was designed to prevent language models from generating NSFW text. Therefore, it identifies neurons that are active in toxic generation and dampens these.
We adapt the method for text-to-image generation models, as detailed in \Cref{app:aura}. In LLMs, AURA is applied to the feed-forward layers only. We perform extensive ablations to identify which layers benefit most from the intervention. Our results in \Cref{tab:aura_ablations} highlight that best results can be achieved when applying AURA to the text encoder's feed-forward layers, which is in line with the original AURA method intervention and yields the insights that the text encoder might be a suitable point for our improved mitigation. 
To achieve the best possible results for AURA in the comparison, we report its success when performing the intervention at the text encoder's feed-forward layers in our further experiments unless otherwise specified.

\textbf{ESD~\citep{gandikota2023erasing}.} The ESD method fine-tunes the model by steering the unconditional generation away from unsafe concepts using modified classifier-free guidance while fine-tuning weights in the cross-attention and multilayer perceptron (MLP) layers. Due to its inherent reliance on a static noise schedule, it is incompatible with newer models, such as SD3 which implements a flow-matching approach (we present more details in \Cref{app:esd}). Therefore, we assess ESD on Stable Diffusion version 1.4 (SD1.4) as done in their paper~\citep{gandikota2023erasing}. While the inherent text-generation ability of SD1.4 underperforms SD3 significantly, applying the method still allows us to quantify the changes incurred to benign and NSFW text generation, and to assess whether ESD is an effective solution to our problem. 

\textbf{Safe-CLIP~\citep{poppi2025safe}.} Safe-CLIP safety fine-tunes the CLIP model that yields the textual embeddings for DMs. It uses a custom dataset that contains unsafe images and captions with close safe counterparts and aims at mapping the unsafe inputs to their respective safe embeddings. It then fine-tunes the CLIP encoder with a combination of various losses that serve to push NSFW embeddings to a safe space, while, at the same time, preserving the embedding space on benign examples. We detail their approach further in \Cref{app:safeCLIP}.
For our experiments, we vary the weights that steer how much emphasis is put on each of the loss terms in order to assess the trade-offs between impeding NSFW generation and preserving benign performance.

\textbf{Experimental Setup.}
The full experimental setup used to implement and evaluate the baselines is presented in \Cref{app:setup}.
We assess the results both in terms of how the text generation changes on benign and NSFW words, and based on the quality of the generated images.
A good mitigation is characterized by causing high change in the NSFW text generation (we do not want to recognize the NSFW words anymore), and a low change in the benign text generation (we want to preserve benign performance). 
Additionally, the overall image quality should not be significantly affected.
Details on the metrics we use for evaluation are presented in \Cref{sec:bench_evaluation}.

\textbf{Baseline Trade-offs.}
In \Cref{fig:baselines}, we assess various trade-offs that can be achieved by the different baselines (for example, by applying AURA to different layers, different numbers of neurons, and with different thresholds, or by running Safe-CLIP with various weightings of the different loss terms).
We find that AURA demonstrates notable inconsistency in its ability to mitigate NSFW text generation. In some setups (Aura with Dampening as detailed in  \cref{tab:aura_ablations_2}) the $\Delta$1gramLD scores for both benign and NSFW are close to 0, indicating that the method fails to impact either text generation. When it does have an impact, it tends to affect both NSFW and benign text alike (the data points lie on the diagonal in \Cref{fig:baselines}), which undermines its objective of reducing \textit{only} NSFW text generation. 

\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/bubble_plot_1LD_Benign_NSFW_ours.pdf}
    \caption{\textbf{State-of-the-art Baselines.}
    We assess the applicability of state-of-the-art baselines in mitigating the generation of NSFW text in images.
    Results for AURA and Safe-CLIP are obtained on SD3, ESD is applied for  SD1.4 because of incompatibility with SD3.
    The results show that most interventions affect benign and NSFW samples proportionally, as evidenced by their alignment along the diagonal, indicating a lack of targeted toxicity mitigation, with SafeCLIP further degrading benign performance.
    }
    \label{fig:baselines}
\end{wrapfigure}

SafeCLIP adopts a more aggressive suppression of NSFW text, as indicated by higher $\Delta$1gramLD. However, this too comes at the cost of benign text getting affected. Additionally, SafeCLIP causes the highest image quality decrease on benign samples as reflected in its higher KID scores. On the first glance, it looks like better trade-offs are achieved using ESD. The best setup corresponds to a relatively small learning rate in the range of 1e-6 to 1e-5  during its concept removal step.
Yet, when analyzing the images generated after the different interventions for NSFW and benign prompts, see \Cref{fig:baseline_samples} in the Appendix, it becomes obvious that SD1.4, on which ESD is evaluated, exhibits difficulties in generating coherent text both before and after intervention, with no significant visual improvement post-intervention.
Therefore, the apparent better trade-offs might be an artifact of the overall low-quality generation and are likely not to transfer to better models.

When analyzing the best setup identified for each of the baseline methods in \Cref{tab:baselines}, we observe that for NSFW text, the other two methods, AURA and Safe-CLIP, exhibit an increase in NgramLev score, with AURA increasing by 2.56 and Safe-CLIP by 2.77, indicating a stronger modification of the original content.  
However, these modifications come at the expense of benign text generation, where AURA and Safe-CLIP also experience significant NgramLev score increase of 2.20 and 2.75, respectively. This trade-off suggests that while AURA and Safe-CLIP apply stronger transformations, they may also introduce more unintended changes to benign text.
Looking at \Cref{fig:baseline_samples}, AURA and Safe-CLIP both still do not achieve complete removal of NSFW text, resulting in residual occurrences in the generated images. Additionally, these interventions introduce distortions in benign text generation, leading to spelling inconsistencies within the output, and indicating undesirable trade-offs. 

\renewcommand{\mycolspace}{3.2pt}
\addtolength{\tabcolsep}{-\mycolspace} 
\begin{table}[h]
    \centering
    \tiny
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccccccc|cccccccccc}
    \toprule
        & \multicolumn{13}{c}{\textbf{Benign Text}} & \multicolumn{10}{c}{\textbf{NSFW Text}}\\ %
        & \multicolumn{3}{c}{F1} & \multicolumn{3}{c}{LD} & \multicolumn{1}{c}{KID} & 
        \multicolumn{3}{c}{CLIP-Score} &
        \multicolumn{3}{c}{NgramLev} &
        \multicolumn{3}{c}{F1} & \multicolumn{3}{c}{LD} & \multicolumn{1}{c}{KID} & 
        \multicolumn{3}{c}{NgramLev} 
        \\
     &  Before & After & $\Delta$ & Before & After & $\Delta$ & Value &  Before & After & $\Delta$ &  Before & After & $\Delta$ & Before & After & $\Delta$ &  Before & After & $\Delta$ & Value & Before & After & $\Delta$\\
    \midrule
    AURA     & 90 & 57.71 & -32.29 & 2.30 & 7.70 & 5.40 & 0.062 & 91.70 & 91.48 & -0.22 & 1.70 & 3.90 & 2.20 & 92.60 & 55.69 & -36.91 & 1.40 & 10.40 & 9 & 0.063 & 1 & 1.56 & 2.56 \\
    ESD     & 33.20 & 28.20 & -5 & 9.12 & 14.50 & 5.38  & 0.053 & 26.43 & 21.56 & -4.87 & 3.24 & 5.34 & 2.10 & 31.45 & 27.76 & -3.69 & 11.23 & 14.67 & 3.44 & 0.059 & 3.60 & 6.90 & 3.30 \\
    Safe-CLIP & 90 & 65.78 & -24.22 & 2.30 & 4.80 & 2.50 & 0.054 & 91.70 & 91.34 & -0.36 & 1.70 & 1.05 & 2.75 &  93.00 & 60.12  & 29.88 & 1.40 & 4.68 & 6.08 & 0.058 & 1 & 1.77 & 2.77 \\
    \midrule
    Ours & 90.0 & 60.00 & -30 & 2.30 & 6.95 & 4.75 & 0.052 & 91.70 & 91.30 & -0.4 & 1.7 & 2.45 & 0.75 & 92.6 & 69.11 & 22.69 & 1.40 & 5.96 & 4.56 & 0.054 & 1.00 & 3.05 & 2.05 \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Best Baselines.}
    We present the results for the baselines with the best parameters and our \ours method.
    }
    \label{tab:baselines}
\end{table}
\setlength{\tabcolsep}{\mycolspace}

\section{Our \ours and \bench Benchmark}
The shortcomings of the previous methods motivate the necessity to design methods targeted to mitigate the threat of NSFW text generation within synthetic images.
To facilitate this endeavor,  we introduce \bench, the first benchmark to assess generative text-to-image models' NSFW text generation ability.
Finally, we propose \ours to prevent NSFW text generation while leaving the model's benign and general generation abilities intact.

\subsection{\bench: Evaluating NSFW Text Generation}
\label{sec:bench_evaluation}


We describe our \bench, the first open source benchmark to assess generative models' ability to embed NSFW text into their outputs.
\bench consists of two main components, a curated dataset and an evaluation pipeline to assess the generated texts and overall image quality.

\paragraph{The Dataset.}
We create the \bench-dataset building on CreativeBench~\citep{yang2024glyphcontrol}, a creative text prompt benchmark adapted from GlyphDraw~\citep{ma2023glyphdraw} which holds various prompts to encourage the generation of text within the synthetic images.
Examples include, â€˜Little panda holding a sign that says "\textless word\textgreater".â€™ or â€˜A photographer wears a t-shirt with the word "\textless word\textgreater" printed on it.â€™ 
In total, there are 218 different prompt templates.
Additionally, we curate a list of 2954 English speaking slurs, based on DirtyNaughtyList~\citep{dirtynaughtylistupdated} and Toxic~\citep{toxiclist}, two frequently updated repositories with multilingual slurs. We pre-filter the list using the roberta-base classifier from Detoxify~\citep{Detoxify} and only keep those words that are classified as NSFW with a score above 0.9. 
This yields a total of 437 NSFW words.
We perform a random split of the slurs into a training set with 337 and a test set with 100.
This split makes sure that the same NSFW word that is seen during training does not appear at test time (with the only difference being the different prompt template). Thereby, we make sure that removing the model's NSFW text generation ability is not only limited to the words seen during training.
Then, we combine all prompt templates with the slurs, replacing the "\textless word\textgreater" token.
This yields a total of $73466 (218 \times 337)$ training data points and $21800 (218 \times 100)$ test data points.
Finally, we provide OCR annotations with every data point for evaluation.



\paragraph{The Evaluation Pipeline.}
We implement an open source pipeline to assess the models' text generation ability and image quality.
An overview of the pipeline is presented in \Cref{fig:pipeline}.
Overall, our pipeline operates as follows: We start with generated images that we want to evaluate.
\begin{wrapfigure}{r}{0.6\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Safety-Neuron.pdf}
    \caption{\textbf{\bench Evaluation Pipeline.} We depict \bench's evaluation pipeline. The pipeline is designed for two main use-cases, namely 1) evaluating text and image-based metrics, for example, with the aim of assessing the impact of a mitigation method, and 2) detecting NSFW text in generated images.}
    \label{fig:pipeline}
\end{wrapfigure}
Then, we perform OCR to extract all the characters embedded within. We integrate the EasyOCR model\footnote{https://github.com/JaidedAI/EasyOCR}, however, our pipeline can be easily extended to rely on other OCR models as well.
Based on the extracted characters, there are two major use-cases: 
1) Providing text and image-based metrics that can serve to assess mitigation methods. For example, if we want to assess whether the NSFW text generation ability of the model decreased through the mitigation, we have to generate the image twice, using the same prompt and random input seed, once with the model before putting the mitigation into place, and then with the protected model after mitigation.
2) The second use-case is assessing NSFW-ness of an image as standalone (as done in the right column of \Cref{tab:detection-results}): In this case, after the OCR, we run a text-toxicity detector~\citep{Detoxify} and report the score. 

\paragraph{The Metrics.}
Our metrics assess both text and overall image quality. 
For good mitigations, we expect that they decrease text generation for NSFW words while preserving text generation for benign words and image quality.
In the following, we detail our metrics:

\begin{itemize}
    \item \textbf{Kernel Inception Distance (KID)}:
    KID \cite{KID} is a metric designed to evaluate the quality of generated images by comparing their feature representations to a reference set. Unlike FrÃ©chet Inception Distance (FID) \cite{FID}, which assumes that feature embeddings follow a Gaussian distribution and relies on estimating mean and covariance matrices, KID is based on Maximum Mean Discrepancy (MMD) with a polynomial kernel. This makes KID more robust, particularly for small sample sizes, where FID can be unstable due to poor covariance estimation. KID provides an unbiased estimate of the distance between distributions, returning values in the range [0.0, 0.1], where lower values indicate higher similarity between distributions. In this setup, the reference distribution is defined by the images before intervention, and KID is computed on the images generated after intervention to quantify the impact of modifications on the generated image distribution.
    \item \textbf{F1-score}:   
The F1 Score is defined as
$
    \text{F1-Score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}},
$
where \textit{Precision} represents the proportion of predicted characters that match the keywords (out of all the characters returned by the OCR model), and \textit{Recall} indicates the proportion of characters from the keywords that are correctly predicted by the OCR model. It is a harmonic mean of the precision and recall which returns values in the range $[0,1]$ (for NSFW removal, lower is better, for benign alignment, higher is better).

\item \textbf{CLIP-score}: This metric for evaluating image captioning is used in our case to evaluate the overall alignment of prompt to image. 

\item \textbf{Levenshtein Distance (LD)}:
The LD between the words in the text prompt and the text predicted by the OCR model measures the degree of textual similarity. Specifically, it measures the minimum number of single-character edits (\ie insertions, deletions or substitutions) required to change one word into the other and ranges from 0 to the length of the longer string (for NSFW removal, higher is better and for benign alignment, lower is better).
\item \textbf{Ngram Levenshtein Distance}: 
We introduce this new metric building on the original LD described above. Observations have shown that generated text on images can be excessively long compared to the ground truth words from the input prompts of text-to-image models. This behavior is observed on \textit{all} of the model under scrutiny. For example, when asking them to generate the word "Newspaper", most models also generate a sample newspaper with actual template text.
In these cases, the original LD metric might not be sufficiently expressive anymore, as it would have extremely high values (due to the many insertions).
Instead, we propose to replace it by our Ngram Levenshtein Distance. Our metric first divides the OCR-generated text into a several lists. Each of the lists contains all the k-adjacent tokens of the OCR-generated text. This variable k is chosen with $k\in[1,n+1]$ with $n$ the number of tokens of the ground truth text to be generated on the image. This enables to capture efficiently any substring from the original OCR-generated text which tokenization is close to the ground truth word. Finally, LD is performed on each elements of the described lists and the lowest LD is then returned.
\end{itemize}


Providing a standardized, model-agnostic evaluation with a fixed set of metrics provides a rigorous benchmark to help the community measuring NSFW text generation in images.

\begin{figure}[t]
    \centering
    \setlength{\tabcolsep}{0pt}
    \begin{tabular}{cc}
        \begin{tabular}{c}
            \begin{tabular}{cccccc}
                \captionsetup{labelformat=empty}
                \subcaptionbox{Road}{\includegraphics[width=0.1\linewidth]
                {figures/main_method_images/non-toxic/iloveimg-compressed-5/road.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Field}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/non-toxic/iloveimg-compressed-5/field.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Laptop}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/non-toxic/iloveimg-compressed-5/laptop.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Radio}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/non-toxic/iloveimg-compressed-5/radio.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Story}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/non-toxic/iloveimg-compressed-5/story.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Travel}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/non-toxic/iloveimg-compressed-5/travel.png}}
            \end{tabular} 
            \\[0pt]
            \begin{tabular}{cccccc}
                \captionsetup{labelformat=empty}
                \subcaptionbox{fuckery}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/fuckery.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{fuckface}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/fuckface.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{fucks}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/fucks.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Assbag}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/assbag.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Masturbated}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/masturbated.png}} &
                \captionsetup{labelformat=empty}
                \subcaptionbox{Dothead}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/dothead.png}}
            \end{tabular}
        \end{tabular}
        &
        \hspace{1pt}
        \vrule width 1pt
        \hspace{1pt}
        \begin{tabular}{c}
            \captionsetup{labelformat=empty}
            \subcaptionbox{Alley}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/non-toxic/iloveimg-compressed-5/alley.png}} 
            \\[0pt]
            \captionsetup{labelformat=empty}
            \subcaptionbox{Giant Cocks}{\includegraphics[width=0.1\linewidth]{figures/main_method_images/toxic/iloveimg-compressed-4/giant_cock.png}} 
        \end{tabular}
    \end{tabular}
    \caption{\textbf{Overall \ours on NSFW and Benign words}. Samples of generated images on the test set of \bench for benign words (1st line) and NSFW words (2nd line). Overall a retaining of benign word generation is observed compared to a degradation of harmful words generation. A sample of two edge cases is presented on the right column with a spelling mistake for the word "alley" and the highly NSFW sample "giant cocks" is easily recognizable to the human eye.}
    \label{fig:our_samples}
\end{figure}

\subsection{\ours: Mitigating NSFW Text Generation in Images}

Finally, we propose \ours, as an initial step toward addressing the NSFW text generation in images.
Given, that the ablations on AURA suggest that the text encoder within DMs is a good points to implement the intervention (see \Cref{tab:aura_ablations}), our \ours safety fine-tunes this encoder with a custom dataset.
More precisely, we target the CLIP~\citep{clip} encoder for its wide applicability in DMs, following~\citep{poppi2025safe, zhang2024defensive}.
We find that this approach yields strong results in mitigating NSFW text while retaining the overall text and image generation ability on benign sample.


As a custom fine-tuning dataset, we extend our \bench-dataset with benign replacement words for the NSFW words. We generate these words with the goal of creating a close semantic and/or grammatical correspondence to their NSFW counterparts, although being completely harmless. For instance the corresponding benign word to the NSFW word ``scumbag" is ``stuff bag". The mappings are all prompted from GPT4~\citep{OpenAI_ChatGPT_2024}, the prompt used for obtaining the mappings is presented in \Cref{app:setup}, \Cref{fig:toxic_mappings_prompt}. Those mapped words are then integrated into our \bench by replacing their corresponding NSFW words with them.
Then, we instantiate a loss function with the objective of mapping NSFW prompts to their benign counterparts. We refer to the NSFW prompts as $x_{NSFW}$ and their generated counterparts as $x_{benign}$. We define this loss as:

\begin{equation}
    \begin{aligned}
            \text{NSFWLoss}(x_{NSFW},x_{benign}) = \\
            \text{CosSimLoss}(\hat{M}(x_{NSFW}),M^*(x_{benign}))
    \end{aligned}
    \label{eq:loss}
\end{equation}

with $\hat{M}$ and $M^*$ the fine-tuned and the frozen CLIP text encoders respectively.

Our choice of safety fine-tuning setup through mapping NSFW words to semantically close words has two major motivations:
1) A negative loss setup for ``forgetting" NFSW text embeddings in the CLIP Embedding space is hard to implement. We experimented with this setup and found that  this loss is very small ($\approx 10^{-9}$)
which makes it too small for training because of computational precision. In contrast, the loss defined by \Cref{eq:loss} is large enough to be computed without instability caused by this computations approximations.
2) The semantic closeness between the NSFW word and its replacement aims at lowering the initial training loss by using the properties of the CLIP embedding space. This makes learning succeed faster, and is better for mitigating utility drop on the benign samples.
