
\section{Summary}

We show that state-of-the-art visual generation models, including DMs and VARs, are highly susceptible to generating NSFW text embedded within images---a threat overlooked by prior mitigation efforts focused on visual content. We demonstrate that all leading DMs and VARs are vulnerable and that existing safety mechanisms fail to prevent harmful text generation without severely degrading benign text output. As an initial countermeasure, we fine-tune the text encoder in major DM architectures using a curated dataset, reducing NSFW text generation while maintaining image quality. To support further research, we introduce ToxicBench, an open-source benchmark designed to systematically evaluate and improve mitigation strategies for NSFW text generation in images.
Thereby, we hope to contribute towards a more trustworthy deployment of these models.

\section*{Acknowledgements}
This work was supported by the German Research Foundation (DFG) within the framework of the Weave Programme under the project titled "Protecting Creativity: On the Way to Safe Generative Models" with number 545047250. 