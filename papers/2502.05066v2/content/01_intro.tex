
\textcolor{red}{\textit{\textbf{Warning:} This paper contains examples of offensive language, including insults, and  sexual or explicit terms, used solely for research and analysis purposes.
}}

\section{Introduction}

State-of-the-art visual generation models, including Diffusion Models (\DMs)~\citep{esser2024scalingSD3,DeepFloydIF,flux} and the novel Vision Auto-Regressive Models~(\IARMs)~\citep{han2024infinity,tian2024visual}, have revolutionized the creation of realistic, detailed, and aesthetically impressive content. Despite their capabilities, these models often raise ethical and safety concerns, as they can inadvertently generate Not Safe For Work (NSFW) content, such as depictions of violence or nudity~\citep{qu2023unsafe,rando2022red,yang2024sneakyprompt}.

To mitigate the generation of NSFW content, prior work has focused extensively on addressing such issues in the visual space.
Beyond the development of powerful NSFW detectors~\citep{nsfwdetector,nudenet}, these efforts, which include modifying training data~\citep{zong2024safety}, adding safety-based loss functions~\citep{poppi2025safe,gandikota2023erasing}, and steering generation to safe subspaces~\citep{schramowski2023safe}, have shown promising results in reducing explicit or harmful visual scenes. 
However, as visual generation models have grown more powerful, their capabilities now extend beyond simply creating images. 
Instead, they also generate \textit{embedded text within those images}, such as captions, signs, or artistic typography~\citep{esser2024scalingSD3,textdiffuser,DeepFloydIF,flux}. This advancement introduces a new challenge: as we show in \Cref{fig:toxic_overview}, 
all prominent state-of-the-art models, including DMs, such as SD3~\citep{esser2024scalingSD3}, Flux~\citep{flux}, and DeepFloyd IF~\citep{DeepFloydIF}, as well as \VARs~\citep{han2024infinity}, can inadvertently produce NSFW or offensive text, such as explicit language or slurs that can be deeply offensive to viewers and raise significant ethical concerns.
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/image_grid_with_prompts.pdf}
    \caption{\textbf{Visual generative models output images with NSFW text.} We evaluate the state-of-the-art diffusion models (SD3, DeepFloyd IF, and FLUX) and vision autoregressive model (Infinity)
    and observe that they easily generate toxic text in the output images due to the lack of any safety guiderails.
    }
    \label{fig:toxic_overview}
\end{wrapfigure}
To mitigate this novel threat, in this work, we systematically analyze the generation of NSFW text within images. We demonstrate that existing NSFW
mitigation techniques~\citep{gandikota2023erasing,poppi2025safe,suau2024whispering}, while effective in addressing NSFW content in the visual or the language domain, are inadequate for handling embedded NSFW text in generated images without significantly degrading the models' overall and (benign) text generation ability. 

As a first step toward mitigating this threat, we explore safety fine-tuning of the CLIP text encoder, a core component of popular DM architectures. By curating a custom fine-tuning dataset that maps NSFW words to syntactically similar benign alternatives, we train the text encoder to reduce the generation of harmful text while preserving image quality for benign inputs.
While our approach is tailored to text-encoder-based models and does not directly apply to newer VARs, it offers a concrete starting point for addressing NSFW text generation. More broadly, our findings highlight the text encoder as a key intervention point for future mitigation strategies.

Finally, to evaluate the safety of vision generative models and equip the community with a reliable tool to monitor progress in this domain, we present \bench, a comprehensive open-source benchmark built upon CreativeBench~\citep{yang2024glyphcontrol}. \bench features a carefully curated dataset of textual prompts known to trigger NSFW text generation.
Additionally, it contains a new metric to analyze text generated in images, carefully selected additional metrics to assess text and image quality, and a robust pipeline for assessing mitigation strategies.
By exploring this novel threat vector and providing a standardized evaluation benchmark for the community, we aim to foster the development of safer multi-modal generative models.

In summary, we make the following contributions:
\begin{enumerate}
    \item We identify a novel threat vector in visual generation models: their ability to embed NSFW text into images.
    \item We evaluate mitigation approaches both from the vision and the language domain and find that they are ineffective for mitigating NSFW text generation while preserving benign generation abilities.
    \item We propose safety fine-tuning of the CLIP text encoder to mitigate NSFW text generation in DMs, preserving image quality while reducing harmful text output.
    \item We develop \bench, the first open source benchmark for evaluating NSFW text generation in text-to-image generative models, providing the community with tools to measure progress and advance the field.
\end{enumerate}
