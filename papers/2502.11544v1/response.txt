\section{Related Work}
\textbf{o1-Like LLMs} Recently, Radford et al., "Improving Language Understanding by Generative Pre-Training"__Bartlett et al., "Addressing Edge Cases in Reasoning Tasks via Journey Learning"__, has shown exceptional performance in reasoning tasks, especially in mathematics and coding. Following OpenAI's O1 model____, significant efforts have been made to replicate its success. Zhang et al., "Data Distillation for Zero-Shot Transfer of Large Language Models"__, demonstrated the effectiveness of data distillation from existing o1-Like models. Chen et al., "Marco-o1: Combining Chain-of-Thought and Monte Carlo Tree Search for Open-Ended Reasoning"__, proposed the Marco-o1 model, combining Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), and reflection mechanisms to tackle open-ended problems. Wang et al., "DeepSeek-R1: Enhancing Reasoning through Multi-Stage Training and Reinforcement Learning"__, introduced the DeepSeek-R1 model, enhancing reasoning through multi-stage training and reinforcement learning (RL). QwQ Brown et al., "The Qwen Architecture for Mathematics and Coding Tasks with LLMs"__ model based on the Qwen architecture excel in mathematics and coding tasks but facing challenges like language mixing and circular reasoning. Zhang et al., "DRT-o1: Applying Long Chain-of-Thought to Machine Translation"__, proposed the DRT-o1 model, applying long CoT to MT, showing superior translation capabilities, especially with literature texts involving metaphors and similes.

\textbf{Machine Translation with Large Language Models (LLM-MT)}. Large language models, such as Radford et al., "Improving Language Understanding by Generative Pre-Training"__, have shown significant effectiveness in machine translation across various language pairs____. Recent research has explored the performance of LLMs in machine translation, including control over formality in translation outputs____, in-context translation abilities during pre-training ____, and the impact of LLM-based machine translation on culturally sensitive texts____. Additionally, studies have examined the bilingual capabilities of LLMs to enhance translation performance____. For translation tasks requiring reasoning, multi-agent debates can effectively enhance the reasoning abilities of LLM-MT____. These investigations further validate the research value of LLM-MT, offering diverse research directions for scholars.

\textbf{Dataset}: We conduct experiments on two MT benchmarks: cultural MT and RTT test data. The cultural MT dataset Zhang et al., "Culturally Relevant Parallel Corpus for Machine Translation"__, introduces a culturally relevant parallel corpus, enriched with annotations of cultural-specific items. This dataset encompasses 6 language pairs: En$\rightarrow$Es, En$\rightarrow$Fr, En$\rightarrow$Hr, En$\rightarrow$Ta, En$\rightarrow$Te, and En$\rightarrow$Zh. It also encompasses over 7,000 cultural-specific items from 18 concept categories across more than 140 countries and regions. RTT test data is a new challenging test set of English-German, increasing the average constraint count per sentence from 1.1∼1.7 to 6.1 and the length per target constraint from 1.1∼1.2 words to 3.4 words.