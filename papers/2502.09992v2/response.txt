\section{Related Work}
Diffusion models**Ho et al., "Denoising Diffusion Probabilistic Models"** have excelled in visual domains but remain unverified for LLMs despite extensive efforts.

A simple approach is to continuousize text data and apply diffusion models directly**Song et al., "Improved Techniques for Training Score-Based Generative Models"**. Alternatively, some methods model continuous parameters of discrete distributions instead**Nijkamp et al., "Score-Based Generative Modeling through Diffusion"**. However, scalability remains a challenge, as a 1B parameter model requires \emph{64 times the compute} of an ARM to achieve comparable performance**Ho et al., "Denoising Diffusion Probabilistic Models"**.



Another approach replaces continuous diffusion with discrete processes featuring new forward and reverse dynamics**Song et al., "Improved Techniques for Training Score-Based Generative Models"**, leading to numerous variants**Ho et al., "Denoising Diffusion Probabilistic Models"**. Notably, **Nijkamp et al., "Score-Based Generative Modeling through Diffusion"** showed that masked diffusion, as a special case of discrete diffusion, achieves perplexity comparable to or surpassing ARMs at GPT-2 scale. **Ho et al., "Denoising Diffusion Probabilistic Models"** established fundamental theoretical results, which motivated our model design, training, and inference (see Appendix~\ref{app:formulation}).  **Song et al., "Improved Techniques for Training Score-Based Generative Models"** explored how MDM can be leveraged for language tasks such as question answering at GPT-2 scale. **Ho et al., "Denoising Diffusion Probabilistic Models"** fine-tune ARMs in the MDM formulation. However, improvements are confined to certain metrics, and it remains unclear whether this approach can yield a foundation model comparable to strong LLMs under a comprehensive evaluation.



In comparison, this study scales MDM to an unprecedented size of 8B parameters from scratch, achieving performance comparable to leading LLMs such as LLaMA 3.

Additionally, a parallel line of work on image generation**Song et al., "Improved Techniques for Training Score-Based Generative Models"** aligns well with the application of MDMs to text data. Moreover, MDMs have also shown promise in domains such as protein generation**Nijkamp et al., "Score-Based Generative Modeling through Diffusion"**, where they have achieved promising results. Notably, **Ho et al., "Denoising Diffusion Probabilistic Models"** demonstrate the potential of using distillation to accelerate MDMs sampling, further enhancing their efficiency.