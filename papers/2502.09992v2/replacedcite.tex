\section{Related Work}
Diffusion models____ have excelled in visual domains but remain unverified for LLMs despite extensive efforts.

A simple approach is to continuousize text data and apply diffusion models directly____. Alternatively, some methods model continuous parameters of discrete distributions instead____. However, scalability remains a challenge, as a 1B parameter model requires \emph{64 times the compute} of an ARM to achieve comparable performance____.



Another approach replaces continuous diffusion with discrete processes featuring new forward and reverse dynamics____, leading to numerous variants____. Notably, ____ showed that masked diffusion, as a special case of discrete diffusion, achieves perplexity comparable to or surpassing ARMs at GPT-2 scale. ____ established fundamental theoretical results, which motivated our model design, training, and inference (see Appendix~\ref{app:formulation}).  ____ explored how MDM can be leveraged for language tasks such as question answering at GPT-2 scale. ____ fine-tune ARMs in the MDM formulation. However, improvements are confined to certain metrics, and it remains unclear whether this approach can yield a foundation model comparable to strong LLMs under a comprehensive evaluation.



In comparison, this study scales MDM to an unprecedented size of 8B parameters from scratch, achieving performance comparable to leading LLMs such as LLaMA 3.

Additionally, a parallel line of work on image generation____ aligns well with the application of MDMs to text data. Moreover, MDMs have also shown promise in domains such as protein generation____, where they have achieved promising results. Notably, ____ demonstrate the potential of using distillation to accelerate MDMs sampling, further enhancing their efficiency.