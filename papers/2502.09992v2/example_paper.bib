@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}

@article{campbell2022continuous,
  title={A continuous time framework for discrete denoising models},
  author={Campbell, Andrew and Benton, Joe and De Bortoli, Valentin and Rainforth, Thomas and Deligiannidis, George and Doucet, Arnaud},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28266--28279},
  year={2022}
}

@article{sun2022score,
  title={Score-based continuous-time discrete diffusion models},
  author={Sun, Haoran and Yu, Lijun and Dai, Bo and Schuurmans, Dale and Dai, Hanjun},
  journal={arXiv preprint arXiv:2211.16750},
  year={2022}
}

@article{meng2022concrete,
  title={Concrete score matching: Generalized score matching for discrete data},
  author={Meng, Chenlin and Choi, Kristy and Song, Jiaming and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={34532--34545},
  year={2022}
}

@article{lou2023discrete,
  title={Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution},
  author={Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2310.16834},
  year={2023}
}

@article{gulrajani2024likelihood,
  title={Likelihood-based diffusion language models},
  author={Gulrajani, Ishaan and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lin2023text,
  title={Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise},
  author={Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu},
  booktitle={International Conference on Machine Learning},
  pages={21051--21064},
  year={2023},
  organization={PMLR}
}

@article{he2022diffusionbert,
  title={Diffusionbert: Improving generative masked language models with diffusion models},
  author={He, Zhengfu and Sun, Tianxiang and Wang, Kuanning and Huang, Xuanjing and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2211.15029},
  year={2022}
}

@article{li2022diffusion,
  title={Diffusion-lm improves controllable text generation},
  author={Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4328--4343},
  year={2022}
}

@article{gong2022diffuseq,
  title={Diffuseq: Sequence to sequence text generation with diffusion models},
  author={Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, LingPeng},
  journal={arXiv preprint arXiv:2210.08933},
  year={2022}
}

 @article{Bengio_Leonard_Courville_2013,  
 title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}, 
 journal={arXiv: Learning,arXiv: Learning}, 
 author={Bengio, Yoshua and Leonard, Nicholas and Courville, Aaron}, 
 year={2013}, 
 month={Aug}, 
 language={en-US} 
 }

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}


@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International conference on machine learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}


@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}


@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{sahoo2024simple,
  title={Simple and Effective Masked Diffusion Language Models},
  author={Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin T and Rush, Alexander and Kuleshov, Volodymyr},
  journal={arXiv preprint arXiv:2406.07524},
  year={2024}
}

@article{shi2024simplified,
  title={Simplified and Generalized Masked Diffusion for Discrete Data},
  author={Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K},
  journal={arXiv preprint arXiv:2406.04329},
  year={2024}
}

@article{ou2024your,
  title={Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data},
  author={Ou, Jingyang and Nie, Shen and Xue, Kaiwen and Zhu, Fengqi and Sun, Jiacheng and Li, Zhenguo and Li, Chongxuan},
  journal={arXiv preprint arXiv:2406.03736},
  year={2024}
}

@article{kingma2021variational,
  title={Variational diffusion models},
  author={Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={21696--21707},
  year={2021}
}

@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

@article{chang2023muse,
  title={Muse: Text-to-image generation via masked generative transformers},
  author={Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, AJ and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin and Freeman, William T and Rubinstein, Michael and others},
  journal={arXiv preprint arXiv:2301.00704},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}


@article{zhang2024tinyllama,
  title={Tinyllama: An open-source small language model},
  author={Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
  journal={arXiv preprint arXiv:2401.02385},
  year={2024}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2020}
}


@article{du2024understanding,
  title={Understanding emergent abilities of language models from the loss perspective},
  author={Du, Zhengxiao and Zeng, Aohan and Dong, Yuxiao and Tang, Jie},
  journal={arXiv preprint arXiv:2403.15796},
  year={2024}
}


@article{deschenaux2024promises,
  title={Promises, Outlooks and Challenges of Diffusion Language Modeling},
  author={Deschenaux, Justin and Gulcehre, Caglar},
  journal={arXiv preprint arXiv:2406.11473},
  year={2024}
}


@article{lai2017race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}

@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@misc{cerebras2023slimpajama,
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
  month = 06,
  year = 2023,
  howpublished = {\url{https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}},
  url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@article{muennighoff2024scaling,
  title={Scaling data-constrained language models},
  author={Muennighoff, Niklas and Rush, Alexander and Barak, Boaz and Le Scao, Teven and Tazi, Nouamane and Piktus, Aleksandra and Pyysalo, Sampo and Wolf, Thomas and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
 

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@inproceedings{sap2019social,
  title={Social IQa: Commonsense Reasoning about Social Interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4463--4473},
  year={2019}
}

@article{lovelace2024diffusion,
  title={Diffusion Guided Language Modeling},
  author={Lovelace, Justin and Kishore, Varsha and Chen, Yiwei and Weinberger, Kilian Q},
  journal={arXiv preprint arXiv:2408.04220},
  year={2024}
}

@article{gat2024discrete,
  title={Discrete Flow Matching},
  author={Gat, Itai and Remez, Tal and Shaul, Neta and Kreuk, Felix and Chen, Ricky TQ and Synnaeve, Gabriel and Adi, Yossi and Lipman, Yaron},
  journal={arXiv preprint arXiv:2407.15595},
  year={2024}
}

@article{zheng2024masked,
  title={Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling},
  author={Zheng, Kaiwen and Chen, Yongxin and Mao, Hanzi and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng},
  journal={arXiv preprint arXiv:2409.02908},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{chang2022maskgit,
  title={Maskgit: Masked generative image transformer},
  author={Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11315--11325},
  year={2022}
}

@article{berglund2023reversal,
  title={The reversal curse: Llms trained on" a is b" fail to learn" b is a"},
  author={Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  journal={arXiv preprint arXiv:2309.12288},
  year={2023}
}

@article{lv2023we,
  title={Are we falling in a middle-intelligence trap? an analysis and mitigation of the reversal curse},
  author={Lv, Ang and Zhang, Kaiyi and Xie, Shufang and Tu, Quan and Chen, Yuhan and Wen, Ji-Rong and Yan, Rui},
  journal={arXiv preprint arXiv:2311.07468},
  year={2023}
}

@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}


@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{vela2022temporal,
  title={Temporal quality degradation in AI models},
  author={Vela, Daniel and Sharp, Andrew and Zhang, Richard and Nguyen, Trang and Hoang, An and Pianykh, Oleg S},
  journal={Scientific Reports},
  volume={12},
  number={1},
  pages={11654},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@misc{penedo2024finewebdatasetsdecantingweb,
      title={The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale}, 
      author={Guilherme Penedo and Hynek Kydlíček and Loubna Ben allal and Anton Lozhkov and Margaret Mitchell and Colin Raffel and Leandro Von Werra and Thomas Wolf},
      year={2024},
      eprint={2406.17557},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.17557}, 
}

@article{chatgpt,
  title={{ChatGPT: Optimizing Language Models for Dialogue}},
  author={OpenAI},
  url={https://openai.com/blog/chatgpt/},
  journal={OpenAI blog},
  month={November},
  year={2022}
}


@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}


@article{kou2024cllms,
  title={Cllms: Consistency large language models},
  author={Kou, Siqi and Hu, Lanxiang and He, Zhezhi and Deng, Zhijie and Zhang, Hao},
  journal={arXiv preprint arXiv:2403.00835},
  year={2024}
}

@article{han2022ssd,
  title={Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control},
  author={Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2210.17432},
  year={2022}
}
 

@article{strudel2022self,
  title={Self-conditioned embedding diffusion for text generation},
  author={Strudel, Robin and Tallec, Corentin and Altch{\'e}, Florent and Du, Yilun and Ganin, Yaroslav and Mensch, Arthur and Grathwohl, Will and Savinov, Nikolay and Dieleman, Sander and Sifre, Laurent and others},
  journal={arXiv preprint arXiv:2211.04236},
  year={2022}
}


@article{chen2022analog,
  title={Analog bits: Generating discrete data using diffusion models with self-conditioning},
  author={Chen, Ting and Zhang, Ruixiang and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2208.04202},
  year={2022}
}

@article{xue2024unifying,
  title={Unifying Bayesian Flow Networks and Diffusion Models through Stochastic Differential Equations},
  author={Xue, Kaiwen and Zhou, Yuhao and Nie, Shen and Min, Xu and Zhang, Xiaolu and Zhou, Jun and Li, Chongxuan},
  journal={arXiv preprint arXiv:2404.15766},
  year={2024}
}

@article{graves2023bayesian,
  title={Bayesian flow networks},
  author={Graves, Alex and Srivastava, Rupesh Kumar and Atkinson, Timothy and Gomez, Faustino},
  journal={arXiv preprint arXiv:2308.07037},
  year={2023}
}


@article{dieleman2022continuous,
  title={Continuous diffusion for categorical data},
  author={Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and others},
  journal={arXiv preprint arXiv:2211.15089},
  year={2022}
}


@article{hoogeboom2021argmax,
  title={Argmax flows and multinomial diffusion: Learning categorical distributions},
  author={Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12454--12465},
  year={2021}
}

@article{hoogeboom2021autoregressive,
  title={Autoregressive diffusion models},
  author={Hoogeboom, Emiel and Gritsenko, Alexey A and Bastings, Jasmijn and Poole, Ben and Berg, Rianne van den and Salimans, Tim},
  journal={arXiv preprint arXiv:2110.02037},
  year={2021}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec},
  year={2018}
}


@article{bengio2000neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  journal={Advances in neural information processing systems},
  volume={13},
  year={2000}
}

@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4895--4901},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{park2023generative,
  title={Generative agents: Interactive simulacra of human behavior},
  author={Park, Joon Sung and O'Brien, Joseph and Cai, Carrie Jun and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 36th annual acm symposium on user interface software and technology},
  pages={1--22},
  year={2023}
}

@article{wang2024survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186345},
  year={2024},
  publisher={Springer}
}

 @article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{o1,
  title={Learning to reason with LLMs},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}

@misc{gemini1_5,
  title = {Our next-generation model: Gemini 1.5},
  author = {Google},
  url = {https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024},
  year={2024}
}


@misc{claude35sonnet,
  title = {Claude 3.5 Sonnet},
  author = {Anthropic},
  url = {https://www.anthropic.com/news/claude-3-5-sonnet},
  year={2024}
}

@article{guo2025deepseek,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
 
@misc{wu2023ardiffusion,
      title={AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation}, 
      author={Tong Wu and Zhihao Fan and Xiao Liu and Yeyun Gong and Yelong Shen and Jian Jiao and Hai-Tao Zheng and Juntao Li and Zhongyu Wei and Jian Guo and Nan Duan and Weizhu Chen},
      year={2023},
      eprint={2305.09515},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Shih2022TrainingAI,
  title={Training and Inference on Any-Order Autoregressive Models the Right Way},
  author={Andy Shih and Dorsa Sadigh and Stefano Ermon},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning},
  year={2022}
}

@article{nie2024scaling,
  title={Scaling up Masked Diffusion Models on Text},
  author={Nie, Shen and Zhu, Fengqi and Du, Chao and Pang, Tianyu and Liu, Qian and Zeng, Guangtao and Lin, Min and Li, Chongxuan},
  journal={arXiv preprint arXiv:2410.18514},
  year={2024}
}

@misc{reid2022diffuser,
      title={DiffusER: Discrete Diffusion via Edit-based Reconstruction}, 
      author={Machel Reid and Vincent J. Hellendoorn and Graham Neubig},
      year={2022},
      eprint={2210.16886},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{huang2024compression,
  title={Compression represents intelligence linearly},
  author={Huang, Yuzhen and Zhang, Jinghan and Shan, Zifei and He, Junxian},
  journal={arXiv preprint arXiv:2404.09937},
  year={2024}
}

@inproceedings{deletanglanguage,
  title={Language Modeling Is Compression},
  author={Deletang, Gregoire and Ruoss, Anian and Duquenne, Paul-Ambroise and Catt, Elliot and Genewein, Tim and Mattern, Christopher and Grau-Moya, Jordi and Wenliang, Li Kevin and Aitchison, Matthew and Orseau, Laurent and others},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{videoworldsimulators2024,
  title={Video generation models as world simulators},
  author={Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh},
  year={2024},
  url={https://openai.com/research/video-generation-models-as-world-simulators},
}


@inproceedings{bao2023all,
  title={All are worth words: A vit backbone for diffusion models},
  author={Bao, Fan and Nie, Shen and Xue, Kaiwen and Cao, Yue and Li, Chongxuan and Su, Hang and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22669--22679},
  year={2023}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@article{fisher1922mathematical,
  title={On the mathematical foundations of theoretical statistics},
  author={Fisher, Ronald A},
  journal={Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character},
  volume={222},
  number={594-604},
  pages={309--368},
  year={1922},
  publisher={The Royal Society London}
}

@misc{zheng2024maskeddiffusionmodelssecretly,
      title={Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling}, 
      author={Kaiwen Zheng and Yongxin Chen and Hanzi Mao and Ming-Yu Liu and Jun Zhu and Qinsheng Zhang},
      year={2024},
      eprint={2409.02908},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.02908}, 
}

@misc{lou2023reflected,
      title={Reflected Diffusion Models}, 
      author={Aaron Lou and Stefano Ermon},
      year={2023},
      eprint={2304.04740},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@article{Zheng2023ARD,
  title={A Reparameterized Discrete Diffusion Model for Text Generation},
  author={Lin Zheng and Jianbo Yuan and Lei Yu and Lingpeng Kong},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.05737},
}

@article{chen2023fast,
  title={Fast Sampling via De-randomization for Discrete Diffusion Models},
  author={Chen, Zixiang and Yuan, Huizhuo and Li, Yongqian and Kou, Yiwen and Zhang, Junkai and Gu, Quanquan},
  journal={arXiv preprint arXiv:2312.09193},
  year={2023}
}


@misc{mahabadi2024tess,
      title={TESS: Text-to-Text Self-Conditioned Simplex Diffusion}, 
      author={Rabeeh Karimi Mahabadi and Hamish Ivison and Jaesung Tae and James Henderson and Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2024},
      eprint={2305.08379},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{richemond2022categorical,
      title={Categorical SDEs with Simplex Diffusion}, 
      author={Pierre H. Richemond and Sander Dieleman and Arnaud Doucet},
      year={2022},
      eprint={2210.14784},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Su2021RoFormerET,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},
  journal = {Neurocomputing},
  year={2021},
}

@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@article{chu2024qwen2,
  title={Qwen2-audio technical report},
  author={Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2407.10759},
  year={2024}
}

@article{qwen2.5,
    title   = {Qwen2.5 Technical Report}, 
    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
    journal = {arXiv preprint arXiv:2412.15115},
    year    = {2024}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{li2023cmmlu,
  title={Cmmlu: Measuring massive multitask language understanding in chinese},
  author={Li, Haonan and Zhang, Yixuan and Koto, Fajri and Yang, Yifei and Zhao, Hai and Gong, Yeyun and Duan, Nan and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2306.09212},
  year={2023}
}


@article{huang2024c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{AL2023-knowledge2,
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi},
  title = {{Physics of Language Models: Part 3.2, Knowledge Manipulation}},
  journal = {ArXiv e-prints},
  year = 2023,
  month = sep,
  volume = {abs/2309.14402},
  note = {Full version available at \url{http://arxiv.org/abs/2309.14402}}
}

@article{YXLA2024-gsm1,
  author = {Ye, Tian and Xu, Zicheng and Li, Yuanzhi and {Allen-Zhu}, Zeyuan},
  title = {{Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process}},
  journal = {ArXiv e-prints},
  year = 2024,
  month = jul,
  volume = {abs/2407.20311},
  note = {Full version available at \url{http://arxiv.org/abs/2407.20311}}
}

@article{kitouni2024factorization,
  title={The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More},
  author={Kitouni, Ouail and Nolte, Niklas and Bouchacourt, Diane and Williams, Adina and Rabbat, Mike and Ibrahim, Mark},
  journal={arXiv preprint arXiv:2406.05183},
  year={2024}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{UriaML14,
  author    = {Benigno Uria and
               Iain Murray and
               Hugo Larochelle},
  title     = {A Deep and Tractable Density Estimator},
  booktitle = {Proceedings of the 31th International Conference on Machine Learning},
  year      = {2014}
}

@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}



@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}



@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{rein2023gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2311.12022},
  year={2023}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{bavarian2022efficient,
  title={Efficient training of language models to fill in the middle},
  author={Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  journal={arXiv preprint arXiv:2207.14255},
  year={2022}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}

@inproceedings{
anonymous2025interpolating,
title={Interpolating Autoregressive and Discrete Denoising Diffusion Language Models},
author={Anonymous},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=tyEyYT267x}
}

@article{gong2024scaling,
  title={Scaling Diffusion Language Models via Adaptation from Autoregressive Models},
  author={Gong, Shansan and Agarwal, Shivam and Zhang, Yizhe and Ye, Jiacheng and Zheng, Lin and Li, Mukai and An, Chenxin and Zhao, Peilin and Bi, Wei and Han, Jiawei and others},
  journal={arXiv preprint arXiv:2410.17891},
  year={2024}
}

@article{ye2023dinoiser,
  title={Dinoiser: Diffused conditional sequence learning by manipulating noises},
  author={Ye, Jiasheng and Zheng, Zaixiang and Bao, Yu and Qian, Lihua and Wang, Mingxuan},
  journal={arXiv preprint arXiv:2302.10025},
  year={2023}
}

@article{ye2023diffusion,
  title={Diffusion language models can perform many tasks with scaling and instruction-finetuning},
  author={Ye, Jiasheng and Zheng, Zaixiang and Bao, Yu and Qian, Lihua and Gu, Quanquan},
  journal={arXiv preprint arXiv:2308.12219},
  year={2023}
}

@article{wang2024diffusion,
  title={Diffusion Language Models Are Versatile Protein Learners},
  author={Wang, Xinyou and Zheng, Zaixiang and Ye, Fei and Xue, Dongyu and Huang, Shujian and Gu, Quanquan},
  journal={arXiv preprint arXiv:2402.18567},
  year={2024}
}



@article{wang2024dplm,
  title={Dplm-2: A multimodal diffusion protein language model},
  author={Wang, Xinyou and Zheng, Zaixiang and Ye, Fei and Xue, Dongyu and Huang, Shujian and Gu, Quanquan},
  journal={arXiv preprint arXiv:2410.13782},
  year={2024}
}

@article{xu2025show,
  title={Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation},
  author={Xu, Chenkai and Wang, Xu and Liao, Zhenyi and Li, Yishun and Hou, Tianqi and Deng, Zhijie},
  journal={arXiv preprint arXiv:2502.05415},
  year={2025}
}


@article{kitouni2023disk,
  title={DiSK: A Diffusion Model for Structured Knowledge},
  author={Kitouni, Ouail and Nolte, Niklas and Hensman, James and Mitra, Bhaskar},
  journal={arXiv preprint arXiv:2312.05253},
  year={2023}
}