\section{Related Work}
Diffusion models~\cite{sohl2015deep,ho2020denoising,song2020score} have excelled in visual domains but remain unverified for LLMs despite extensive efforts.

A simple approach is to continuousize text data and apply diffusion models directly~\cite{li2022diffusion,gong2022diffuseq,han2022ssd,strudel2022self,chen2022analog,dieleman2022continuous,richemond2022categorical,wu2023ardiffusion,mahabadi2024tess,ye2023dinoiser}. Alternatively, some methods model continuous parameters of discrete distributions instead~\cite{lou2023reflected,graves2023bayesian,lin2023text,xue2024unifying}. However, scalability remains a challenge, as a 1B parameter model requires \emph{64 times the compute} of an ARM to achieve comparable performance~\cite{gulrajani2024likelihood}.



Another approach replaces continuous diffusion with discrete processes featuring new forward and reverse dynamics~\cite{austin2021structured}, leading to numerous variants~\citep{hoogeboom2021argmax,hoogeboom2021autoregressive,he2022diffusionbert,campbell2022continuous,meng2022concrete,reid2022diffuser,sun2022score,kitouni2023disk,Zheng2023ARD,chen2023fast,ye2023diffusion,gat2024discrete,zheng2024maskeddiffusionmodelssecretly,sahoo2024simple,shi2024simplified}. Notably, \citet{lou2023discrete} showed that masked diffusion, as a special case of discrete diffusion, achieves perplexity comparable to or surpassing ARMs at GPT-2 scale. \citet{ou2024your} established fundamental theoretical results, which motivated our model design, training, and inference (see Appendix~\ref{app:formulation}).  \citet{nie2024scaling} explored how MDM can be leveraged for language tasks such as question answering at GPT-2 scale. \citet{gong2024scaling} fine-tune ARMs in the MDM formulation. However, improvements are confined to certain metrics, and it remains unclear whether this approach can yield a foundation model comparable to strong LLMs under a comprehensive evaluation.



In comparison, this study scales MDM to an unprecedented size of 8B parameters from scratch, achieving performance comparable to leading LLMs such as LLaMA 3.

Additionally, a parallel line of work on image generation~\cite{chang2022maskgit, chang2023muse} aligns well with the application of MDMs to text data. Moreover, MDMs have also shown promise in domains such as protein generation~\citep{wang2024diffusion, wang2024dplm}, where they have achieved promising results. Notably, \citet{kou2024cllms,xu2025show} demonstrate the potential of using distillation to accelerate MDMs sampling, further enhancing their efficiency.