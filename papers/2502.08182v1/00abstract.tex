\begin{abstract}
%
Offloading large language models~(LLMs) state to host memory during inference promises to reduce operational costs by supporting larger models, longer inputs, and larger batch sizes. 
%
However, the design of existing memory offloading mechanisms does not take latency service-level objectives~(SLOs) into consideration. 
%
As a result, they either lead to frequent SLO violations or underutilize host memory, thereby incurring economic loss and thus defeating the purpose of memory offloading. 


This paper presents \sys, a latency-SLO-aware memory offloading system for LLM serving. 
%
A key challenge in designing \sys is to reconcile the tension between meeting SLOs and maximizing host memory usage. 
%
\sys overcomes it by exploiting a unique characteristic of modern LLMs: 
during serving, the computation time of each decoder layer is deterministic. 
%
Leveraging this, \sys introduces \interval, an internal tunable knob that captures the tradeoff between SLOs and host memory usage, thereby 
reducing the aforementioned challenge to pick an optimal \interval. 
%
With that, \sys proposes a two-stage approach to automatically pick the \interval. 
%
The first stage is offline that generates the range of optimal \interval, 
while the second stage adjusts \interval at the granularity of inference iteration based on runtime hardware status. 
%
Our evaluation shows that \sys consistently meets SLOs and improves the serving throughput over existing mechanisms by 1.85\X due to maximizing the use of host memory. 


\end{abstract}