%-------------------------------------------------------------------------------
\section{Related Work}
%-------------------------------------------------------------------------------
\label{sec:relwk}

\PN{Efficient LLM serving.} 
Several recent studies address these challenges by proposing methods to enhance system performance and resource efficiency in LLM inference. 
Orca\cite{orca} introduces continuous batching to enhance GPU throughput. 
vLLM\cite{vllm} leverages PageAttention to optimize KV cache memory usage, enabling efficient resource allocation. 
SARATHI\cite{SARATHI} adopts a chunked-prefill strategy, dividing prefill requests into smaller chunks while combining them with decoding requests to achieve better hardware utilization. 
StreamingLLM\cite{StreamingLLM} extends LLM capabilities by allowing the generation of sequence lengths beyond their original training limits. 
\sys builds on some of these techniques like vLLM, and is designed to work in parallel with other approaches to further enhance performance and resource efficiency.

\PN{Offloading techniques.}
Existing works have explored various techniques to improve large-scale model inference performance, particularly on resource-constrained hardware. 
Systems such as DeepSpeed ZeRO-Inference\cite{zero-infer} and Hugging Face Accelerate\cite{huggingface} adopt offloading strategies originally designed for training scenarios. 
Infinite-LLM\cite{lin2024infinitellmefficientllmservice} manages the utilization of all GPU and CPU memory resources to store the KV cache.
These approaches may still cause computation to stall as they do not ensure data readiness at the required time.
InfiniGen\cite{infinigen} mitigates KV cache fetch overhead by speculatively prefetching essential KV entries, improving cache management efficiency. 
Neo\cite{jiangxuanlin} offloads part of attention compute and KV cache states from GPU to CPU to balance compute and memory resources.
These two works cannot handle models that exceed the GPU memory capacity, making them orthogonal to our approach.

\PN{Scheduling systems.} 
Recent works have explored efficient resource scheduling and allocation strategies for deep learning tasks, focusing on optimizing throughput\cite{pollux}, 
heterogeneous-aware scheduling\cite{sia}, preemption and latency-aware scheduling\cite{Clockwork, SHEPHERD}, 
and improving resource utilization through model parallelism\cite{AlpaServe} or iteration-level preemptive scheduling to mitigate queueing delays\cite{fastserve}.
There are also concurrent works that employ disaggregation techniques to decouple and balance resource allocation, 
improving efficiency in LLM inference, such as Splitwise\cite{splitwise}, TetriInfer\cite{TetriInfer}, DéjàVu\cite{dejavu}, and Distserve\cite{distserve}
\sys is orthogonal to the large body of work on scheduling, as its separation of prefill and decoding stages can be implemented using any of the aforementioned approaches.


