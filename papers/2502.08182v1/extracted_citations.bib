@misc{SARATHI,
      title={SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills}, 
      author={Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee},
      year={2023},
      eprint={2308.16369},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.16369}, 
}

@misc{StreamingLLM,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17453}, 
}

@misc{TetriInfer,
      title={Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads}, 
      author={Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2401.11181},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.11181}, 
}

@misc{dejavu,
      title={D\'ej\`aVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving}, 
      author={Foteini Strati and Sara Mcallister and Amar Phanishayee and Jakub Tarnawski and Ana Klimovic},
      year={2024},
      eprint={2403.01876},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2403.01876}, 
}

@misc{fastserve,
      title={Fast Distributed Inference Serving for Large Language Models}, 
      author={Bingyang Wu and Yinmin Zhong and Zili Zhang and Shengyu Liu and Fangyue Liu and Yuanhang Sun and Gang Huang and Xuanzhe Liu and Xin Jin},
      year={2024},
      eprint={2305.05920},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05920}, 
}

@misc{huggingface,
  title = {{Hugging face accelerate.}},
  year = 2025,
  urldate = {2025-01-14},
  url={https://huggingface.co/docs/accelerate/index},
}

@misc{jiangxuanlin,
      title={NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference}, 
      author={Xuanlin Jiang and Yang Zhou and Shiyi Cao and Ion Stoica and Minlan Yu},
      year={2024},
      eprint={2411.01142},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2411.01142}, 
}

@misc{lin2024infinitellmefficientllmservice,
      title={Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache}, 
      author={Bin Lin and Chen Zhang and Tao Peng and Hanyu Zhao and Wencong Xiao and Minmin Sun and Anmin Liu and Zhipeng Zhang and Lanbo Li and Xiafei Qiu and Shen Li and Zhigang Ji and Tao Xie and Yong Li and Wei Lin},
      year={2024},
      eprint={2401.02669},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.02669}, 
}

@inproceedings{sia,
author = {Jayaram Subramanya, Suhas and Arfeen, Daiyaan and Lin, Shouxu and Qiao, Aurick and Jia, Zhihao and Ganger, Gregory R.},
title = {Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613175},
doi = {10.1145/3600006.3613175},
abstract = {The Sia scheduler efficiently assigns heterogeneous deep learning (DL) cluster resources to elastic resource-adaptive jobs. Although some recent schedulers address one aspect or another (e.g., heterogeneity or resource-adaptivity), none addresses all and most scale poorly to large clusters and/or heavy workloads even without the full complexity of the combined scheduling problem. Sia introduces a new scheduling formulation that can scale to the search-space sizes and intentionally match jobs and their configurations to GPU types and counts, while adapting to changes in cluster load and job mix over time. Sia also introduces a low-profiling-overhead approach to bootstrapping (for each new job) throughput models used to evaluate possible resource assignments, and it is the first cluster scheduler to support elastic scaling of hybrid parallel jobs.Extensive evaluations show that Sia outperforms state-of-the-art schedulers. For example, even on relatively small 44- to 64-GPU clusters with a mix of three GPU types, Sia reduces average job completion time (JCT) by 30--93\%, 99th percentile JCT and makespan by 28--95\%, and GPU hours used by 12--55\% for workloads derived from 3 real-world environments. Additional experiments demonstrate that Sia scales to at least 2000-GPU clusters, provides improved fairness, and is not over-sensitive to scheduler parameter settings.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {642–657},
numpages = {16},
keywords = {cluster scheduling, resource allocation, deep learning training},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@INPROCEEDINGS{splitwise,
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Splitwise: Efficient Generative LLM Inference Using Phase Splitting}, 
  year={2024},
  volume={},
  number={},
  pages={118-132},
  keywords={Costs;Processor scheduling;Large language models;Computational modeling;Graphics processing units;Computer architecture;Throughput;Large language models;Cluster deployments;Scheduling;GPUs;Inference efficiency;Machine learning;Resource management},
  doi={10.1109/ISCA59077.2024.00019}}

@inproceedings{vllm,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613165},
doi = {10.1145/3600006.3613165},
abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {611–626},
numpages = {16},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@INPROCEEDINGS{zero-infer,
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and He, Yuxiong},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale}, 
  year={2022},
  volume={},
  number={},
  pages={1-15},
  keywords={Technological innovation;Computational modeling;Aggregates;High performance computing;Graphics processing units;Production;Transformers;Deep Learning;Distributed Inference;Mixture of Experts;PyTorch;DeepSpeed;Transformer models},
  doi={10.1109/SC41404.2022.00051}}

