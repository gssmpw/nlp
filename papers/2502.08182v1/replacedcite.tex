\section{Related Work}
%-------------------------------------------------------------------------------
\label{sec:relwk}

\PN{Efficient LLM serving.} 
Several recent studies address these challenges by proposing methods to enhance system performance and resource efficiency in LLM inference. 
Orca____ introduces continuous batching to enhance GPU throughput. 
vLLM____ leverages PageAttention to optimize KV cache memory usage, enabling efficient resource allocation. 
SARATHI____ adopts a chunked-prefill strategy, dividing prefill requests into smaller chunks while combining them with decoding requests to achieve better hardware utilization. 
StreamingLLM____ extends LLM capabilities by allowing the generation of sequence lengths beyond their original training limits. 
\sys builds on some of these techniques like vLLM, and is designed to work in parallel with other approaches to further enhance performance and resource efficiency.

\PN{Offloading techniques.}
Existing works have explored various techniques to improve large-scale model inference performance, particularly on resource-constrained hardware. 
Systems such as DeepSpeed ZeRO-Inference____ and Hugging Face Accelerate____ adopt offloading strategies originally designed for training scenarios. 
Infinite-LLM____ manages the utilization of all GPU and CPU memory resources to store the KV cache.
These approaches may still cause computation to stall as they do not ensure data readiness at the required time.
InfiniGen____ mitigates KV cache fetch overhead by speculatively prefetching essential KV entries, improving cache management efficiency. 
Neo____ offloads part of attention compute and KV cache states from GPU to CPU to balance compute and memory resources.
These two works cannot handle models that exceed the GPU memory capacity, making them orthogonal to our approach.

\PN{Scheduling systems.} 
Recent works have explored efficient resource scheduling and allocation strategies for deep learning tasks, focusing on optimizing throughput____, 
heterogeneous-aware scheduling____, preemption and latency-aware scheduling____, 
and improving resource utilization through model parallelism____ or iteration-level preemptive scheduling to mitigate queueing delays____.
There are also concurrent works that employ disaggregation techniques to decouple and balance resource allocation, 
improving efficiency in LLM inference, such as Splitwise____, TetriInfer____, DéjàVu____, and Distserve____
\sys is orthogonal to the large body of work on scheduling, as its separation of prefill and decoding stages can be implemented using any of the aforementioned approaches.