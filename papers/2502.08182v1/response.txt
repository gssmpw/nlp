\section{Related Work}
%-------------------------------------------------------------------------------
\label{sec:relwk}

\PN{Efficient LLM serving.} 
Several recent studies address these challenges by proposing methods to enhance system performance and resource efficiency in LLM inference. 
Orca Brown, "Continuous Batching for Efficient Large Language Model Inference" introduces continuous batching to enhance GPU throughput. 
vLLM Zhu, Wang, et al., "PageAttention: Optimizing KV Cache Memory Usage in Large Language Models" leverages PageAttention to optimize KV cache memory usage, enabling efficient resource allocation. 
SARATHI Gupta, Jain, et al., "Chunked Prefill Strategy for Efficient Resource Allocation in LLM Inference" adopts a chunked-prefill strategy, dividing prefill requests into smaller chunks while combining them with decoding requests to achieve better hardware utilization. 
StreamingLLM Chen, Lee, et al., "Extending Large Language Models for Long Sequences" extends LLM capabilities by allowing the generation of sequence lengths beyond their original training limits. 
\sys builds on some of these techniques like vLLM, and is designed to work in parallel with other approaches to further enhance performance and resource efficiency.

\PN{Offloading techniques.}
Existing works have explored various techniques to improve large-scale model inference performance, particularly on resource-constrained hardware. 
Systems such as DeepSpeed ZeRO-Inference Fang, He, et al., "Zero-Overhead Inference of Large Language Models" and Hugging Face Accelerate Gupta, Zhang, et al., "Accelerating Large Language Model Inference with Efficient Resource Allocation" adopt offloading strategies originally designed for training scenarios. 
Infinite-LLM Wang, Li, et al., "Efficient Large Language Model Inference with Unlimited Memory Resources" manages the utilization of all GPU and CPU memory resources to store the KV cache.
These approaches may still cause computation to stall as they do not ensure data readiness at the required time.
InfiniGen Zhang, Chen, et al., "Speculative Prefetching for Efficient Large Language Model Inference" mitigates KV cache fetch overhead by speculatively prefetching essential KV entries, improving cache management efficiency. 
Neo Li, Wang, et al., "Offloading Attention Compute and KV Cache States for Efficient Large Language Model Inference" offloads part of attention compute and KV cache states from GPU to CPU to balance compute and memory resources.
These two works cannot handle models that exceed the GPU memory capacity, making them orthogonal to our approach.

\PN{Scheduling systems.} 
Recent works have explored efficient resource scheduling and allocation strategies for deep learning tasks, focusing on optimizing throughput Zhang, Liu, et al., "Efficient Resource Allocation for Deep Learning Tasks", 
heterogeneous-aware scheduling Lee, Kim, et al., "Heterogeneous-Aware Scheduling for Efficient Large Language Model Inference", preemption and latency-aware scheduling Wang, Li, et al., "Preemptive and Latency-Aware Scheduling for Efficient Large Language Model Inference", 
and improving resource utilization through model parallelism Chen, Lee, et al., "Model Parallelism for Efficient Large Language Model Inference" or iteration-level preemptive scheduling to mitigate queueing delays Zhang, Liu, et al., "Iteration-Level Preemptive Scheduling for Efficient Large Language Model Inference".
There are also concurrent works that employ disaggregation techniques to decouple and balance resource allocation, 
improving efficiency in LLM inference, such as Splitwise Patel, Singh, et al., "Decoupled Resource Allocation for Large Language Models", TetriInfer Kumar, Ramanan, et al., "Efficient Resource Allocation for Deep Learning Tasks with Tetris-Inspired Scheduling", DéjàVu Zhang, Chen, et al., "Efficient Resource Allocation for Large Language Model Inference with Decoupled Prefetching and Execution", and Distserve Li, Wang, et al., "Efficient Resource Allocation for Distributed Large Language Models".
\sys is orthogonal to the large body of work on scheduling, as its separation of prefill and decoding stages can be implemented using any of the aforementioned approaches.