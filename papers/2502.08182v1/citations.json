[
  {
    "index": 0,
    "papers": [
      {
        "key": "orca",
        "author": "Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun",
        "title": "Orca: A Distributed Serving System for {Transformer-Based} Generative Models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "vllm",
        "author": "Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion",
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "SARATHI",
        "author": "Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee",
        "title": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "StreamingLLM",
        "author": "Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis",
        "title": "Efficient Streaming Language Models with Attention Sinks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zero-infer",
        "author": "Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and He, Yuxiong",
        "title": "DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "huggingface",
        "author": "Unknown",
        "title": "{Hugging face accelerate.}"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "lin2024infinitellmefficientllmservice",
        "author": "Bin Lin and Chen Zhang and Tao Peng and Hanyu Zhao and Wencong Xiao and Minmin Sun and Anmin Liu and Zhipeng Zhang and Lanbo Li and Xiafei Qiu and Shen Li and Zhigang Ji and Tao Xie and Yong Li and Wei Lin",
        "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "infinigen",
        "author": "Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim",
        "title": "{InfiniGen}: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jiangxuanlin",
        "author": "Xuanlin Jiang and Yang Zhou and Shiyi Cao and Ion Stoica and Minlan Yu",
        "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "pollux",
        "author": "Aurick Qiao and Sang Keun Choe and Suhas Jayaram Subramanya and Willie Neiswanger and Qirong Ho and Hao Zhang and Gregory R. Ganger and Eric P. Xing",
        "title": "Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "sia",
        "author": "Jayaram Subramanya, Suhas and Arfeen, Daiyaan and Lin, Shouxu and Qiao, Aurick and Jia, Zhihao and Ganger, Gregory R.",
        "title": "Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "Clockwork",
        "author": "Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace",
        "title": "Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up"
      },
      {
        "key": "SHEPHERD",
        "author": "Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica",
        "title": "{SHEPHERD}: Serving {DNNs} in the Wild"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "AlpaServe",
        "author": "Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica",
        "title": "{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "fastserve",
        "author": "Bingyang Wu and Yinmin Zhong and Zili Zhang and Shengyu Liu and Fangyue Liu and Yuanhang Sun and Gang Huang and Xuanzhe Liu and Xin Jin",
        "title": "Fast Distributed Inference Serving for Large Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "splitwise",
        "author": "Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\\'I}{\\~n}igo and Maleki, Saeed and Bianchini, Ricardo",
        "title": "Splitwise: Efficient Generative LLM Inference Using Phase Splitting"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "TetriInfer",
        "author": "Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan",
        "title": "Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "dejavu",
        "author": "Foteini Strati and Sara Mcallister and Amar Phanishayee and Jakub Tarnawski and Ana Klimovic",
        "title": "D\\'ej\\`aVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "distserve",
        "author": "Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang",
        "title": "{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
      }
    ]
  }
]