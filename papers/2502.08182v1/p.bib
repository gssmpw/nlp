
@InProceedings{flexgen,
  title = 	 {{F}lex{G}en: High-Throughput Generative Inference of Large Language Models with a Single {GPU}},
  author =       {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and Re, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {31094--31116},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/sheng23a/sheng23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/sheng23a.html},
  abstract = 	 {The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen.}
}

@INPROCEEDINGS{zero-infer,
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and He, Yuxiong},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={DeepSpeed- Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale}, 
  year={2022},
  volume={},
  number={},
  pages={1-15},
  keywords={Technological innovation;Computational modeling;Aggregates;High performance computing;Graphics processing units;Production;Transformers;Deep Learning;Distributed Inference;Mixture of Experts;PyTorch;DeepSpeed;Transformer models},
  doi={10.1109/SC41404.2022.00051}}

@inproceedings{vllm,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613165},
doi = {10.1145/3600006.3613165},
abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {611–626},
numpages = {16},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings {distserve,
author = {Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
title = {{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {193--210},
url = {https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin},
publisher = {USENIX Association},
month = jul
}

@inproceedings {infinigen,
author = {Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim},
title = {{InfiniGen}: Efficient Generative Inference of Large Language Models with Dynamic {KV} Cache Management},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {155--172},
url = {https://www.usenix.org/conference/osdi24/presentation/lee},
publisher = {USENIX Association},
month = jul
}

@inproceedings {llumnix,
author = {Biao Sun and Ziming Huang and Hanyu Zhao and Wencong Xiao and Xinyi Zhang and Yong Li and Wei Lin},
title = {Llumnix: Dynamic Scheduling for Large Language Model Serving},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {173--191},
url = {https://www.usenix.org/conference/osdi24/presentation/sun-biao},
publisher = {USENIX Association},
month = jul
}

@misc{demystifyingplatformrequirementsdiverse,
      title={Demystifying Platform Requirements for Diverse LLM Inference Use Cases}, 
      author={Abhimanyu Bambhaniya and Ritik Raj and Geonhwa Jeong and Souvik Kundu and Sudarshan Srinivasan and Midhilesh Elavazhagan and Madhu Kumar and Tushar Krishna},
      year={2024},
      eprint={2406.01698},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2406.01698}, 
}



@inproceedings {orca,
author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
title = {Orca: A Distributed Serving System for {Transformer-Based} Generative Models},
booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
year = {2022},
isbn = {978-1-939133-28-1},
address = {Carlsbad, CA},
pages = {521--538},
url = {https://www.usenix.org/conference/osdi22/presentation/yu},
publisher = {USENIX Association},
month = jul
}

@misc{jiangxuanlin,
      title={NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference}, 
      author={Xuanlin Jiang and Yang Zhou and Shiyi Cao and Ion Stoica and Minlan Yu},
      year={2024},
      eprint={2411.01142},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2411.01142}, 
}

@misc{pdserve,
      title={P/D-Serve: Serving Disaggregated Large Language Model at Scale}, 
      author={Yibo Jin and Tao Wang and Huimin Lin and Mingyang Song and Peiyang Li and Yipeng Ma and Yicheng Shan and Zhengfan Yuan and Cailong Li and Yajing Sun and Tiandeng Wu and Xing Chu and Ruizhi Huan and Li Ma and Xiao You and Wenting Zhou and Yunpeng Ye and Wen Liu and Xiangkun Xu and Yongsheng Zhang and Tiantian Dong and Jiawei Zhu and Zhe Wang and Xijian Ju and Jianxun Song and Haoliang Cheng and Xiaojing Li and Jiandong Ding and Hefei Guo and Zhengyong Zhang},
      year={2024},
      eprint={2408.08147},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.08147}, 
}

@misc{tetriserve,
      title={Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads}, 
      author={Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2401.11181},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.11181}, 
}

@inproceedings{dnnslo1,
author = {Crankshaw, Daniel and Sela, Gur-Eyal and Mo, Xiangxi and Zumar, Corey and Stoica, Ion and Gonzalez, Joseph and Tumanov, Alexey},
title = {InferLine: latency-aware provisioning and scaling for prediction serving pipelines},
year = {2020},
isbn = {9781450381376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419111.3421285},
doi = {10.1145/3419111.3421285},
abstract = {Serving ML prediction pipelines spanning multiple models and hardware accelerators is a key challenge in production machine learning. Optimally configuring these pipelines to meet tight end-to-end latency goals is complicated by the interaction between model batch size, the choice of hardware accelerator, and variation in the query arrival process.In this paper we introduce InferLine, a system which provisions and manages the individual stages of prediction pipelines to meet end-to-end tail latency constraints while minimizing cost. InferLine consists of a low-frequency combinatorial planner and a high-frequency auto-scaling tuner. The low-frequency planner leverages stage-wise profiling, discrete event simulation, and constrained combinatorial search to automatically select hardware type, replication, and batching parameters for each stage in the pipeline. The high-frequency tuner uses network calculus to auto-scale each stage to meet tail latency goals in response to changes in the query arrival process. We demonstrate that InferLine outperforms existing approaches by up to 7.6x in cost while achieving up to 34.5x lower latency SLO miss rate on realistic workloads and generalizes across state-of-the-art model serving frameworks.},
booktitle = {Proceedings of the 11th ACM Symposium on Cloud Computing},
pages = {477–491},
numpages = {15},
keywords = {serving, machine learning, inference, autoscaling},
location = {Virtual Event, USA},
series = {SoCC '20}
}

@inproceedings {dnnslo2,
author = {Daniel Crankshaw and Xin Wang and Guilio Zhou and Michael J. Franklin and Joseph E. Gonzalez and Ion Stoica},
title = {Clipper: A {Low-Latency} Online Prediction Serving System},
booktitle = {14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {613--627},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/crankshaw},
publisher = {USENIX Association},
month = mar
}

@inproceedings {dnnslo3,
author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
title = {Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {443--462},
url = {https://www.usenix.org/conference/osdi20/presentation/gujarati},
publisher = {USENIX Association},
month = nov
}

@inproceedings{dnnslo4,
author = {Shen, Haichen and Chen, Lequn and Jin, Yuchen and Zhao, Liangyu and Kong, Bingyu and Philipose, Matthai and Krishnamurthy, Arvind and Sundaram, Ravi},
title = {Nexus: a GPU cluster engine for accelerating DNN-based video analysis},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359658},
doi = {10.1145/3341301.3359658},
abstract = {We address the problem of serving Deep Neural Networks (DNNs) efficiently from a cluster of GPUs. In order to realize the promise of very low-cost processing made by accelerators such as GPUs, it is essential to run them at sustained high utilization. Doing so requires cluster-scale resource management that performs detailed scheduling of GPUs, reasoning about groups of DNN invocations that need to be co-scheduled, and moving from the conventional whole-DNN execution model to executing fragments of DNNs. Nexus is a fully implemented system that includes these innovations. In large-scale case studies on 16 GPUs, when required to stay within latency constraints at least 99\% of the time, Nexus can process requests at rates 1.8-12.7X higher than state of the art systems can. A long-running multi-application deployment stays within 84\% of optimal utilization and, on a 100-GPU cluster, violates latency SLOs on 0.27\% of requests.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {322–337},
numpages = {16},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@misc{schedule1,
      title={A System for Microserving of LLMs}, 
      author={Hongyi Jin and Ruihang Lai and Charlie F. Ruan and Yingcheng Wang and Todd C. Mowry and Xupeng Miao and Zhihao Jia and Tianqi Chen},
      year={2024},
      eprint={2412.12488},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2412.12488}, 
}

@misc{schedule2,
      title={SYMPHONY: Improving Memory Management for LLM Inference Workloads}, 
      author={Saurabh Agarwal and Anyong Mao and Aditya Akella and Shivaram Venkataraman},
      year={2024},
      eprint={2412.16434},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2412.16434}, 
}

@misc{schedule3,
      title={KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management}, 
      author={Rongxin Cheng and Yifan Peng and Yuxin Lai and Xingda Wei and Rong Chen and Haibo Chen},
      year={2024},
      eprint={2412.18169},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2412.18169}, 
}

@misc{applellm,
      title={LLM in a flash: Efficient Large Language Model Inference with Limited Memory}, 
      author={Keivan Alizadeh and Iman Mirzadeh and Dmitry Belenko and Karen Khatamifard and Minsik Cho and Carlo C Del Mundo and Mohammad Rastegari and Mehrdad Farajtabar},
      year={2024},
      eprint={2312.11514},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11514}, 
}

@inproceedings {zero-offload,
author = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyan Yang and Minjia Zhang and Dong Li and Yuxiong He},
title = {{ZeRO-Offload}: Democratizing {Billion-Scale} Model Training},
booktitle = {2021 USENIX Annual Technical Conference (USENIX ATC 21)},
year = {2021},
isbn = {978-1-939133-23-6},
pages = {551--564},
url = {https://www.usenix.org/conference/atc21/presentation/ren-jie},
publisher = {USENIX Association},
month = jul
}

@misc{hcache,
      title={Fast State Restoration in LLM Serving with HCache}, 
      author={Shiwei Gao and Youmin Chen and Jiwu Shu},
      year={2024},
      eprint={2410.05004},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2410.05004}, 
}
@misc{layerkv,
      title={LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management}, 
      author={Yi Xiong and Hao Wu and Changxu Shao and Ziqing Wang and Rui Zhang and Yuhong Guo and Junping Zhao and Ke Zhang and Zhenxuan Pan},
      year={2024},
      eprint={2410.00428},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2410.00428}, 
}

@misc{llmviewer,
      title={LLM Inference Unveiled: Survey and Roofline Model Insights}, 
      author={Zhihang Yuan and Yuzhang Shang and Yang Zhou and Zhen Dong and Zhe Zhou and Chenhao Xue and Bingzhe Wu and Zhikai Li and Qingyi Gu and Yong Jae Lee and Yan Yan and Beidi Chen and Guangyu Sun and Kurt Keutzer},
      year={2024},
      eprint={2402.16363},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16363}, 
}

@misc{lorazerooffload,
      title={Practical offloading for fine-tuning LLM on commodity GPU via learned subspace projectors}, 
      author={Siyuan Chen and Zelong Guan and Yudong Liu and Phillip B. Gibbons},
      year={2024},
      eprint={2406.10181},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.10181}, 
}

@misc{lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{llmsurvey,
      title={Efficient Training of Large Language Models on Distributed Infrastructures: A Survey}, 
      author={Jiangfei Duan and Shuo Zhang and Zerui Wang and Lijuan Jiang and Wenwen Qu and Qinghao Hu and Guoteng Wang and Qizhen Weng and Hang Yan and Xingcheng Zhang and Xipeng Qiu and Dahua Lin and Yonggang Wen and Xin Jin and Tianwei Zhang and Peng Sun},
      year={2024},
      eprint={2407.20018},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2407.20018}, 
}

@misc{cuibinoffload,
      title={Efficiently Training 7B LLM with 1 Million Sequence Length on 8 GPUs}, 
      author={Pinxue Zhao and Hailin Zhang and Fangcheng Fu and Xiaonan Nie and Qibin Liu and Fang Yang and Yuanbo Peng and Dian Jiao and Shuaipeng Li and Jinbao Xue and Yangyu Tao and Bin Cui},
      year={2024},
      eprint={2407.12117},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.12117}, 
}

@inproceedings{flash1,
 author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16344--16359},
 publisher = {Curran Associates, Inc.},
 title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{flash2,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.08691}, 
}

@misc{deepspeedfastgen,
      title={DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference}, 
      author={Connor Holmes and Masahiro Tanaka and Michael Wyatt and Ammar Ahmad Awan and Jeff Rasley and Samyam Rajbhandari and Reza Yazdani Aminabadi and Heyang Qin and Arash Bakhtiari and Lev Kurilenko and Yuxiong He},
      year={2024},
      eprint={2401.08671},
      archivePrefix={arXiv},
      primaryClass={cs.PF},
      url={https://arxiv.org/abs/2401.08671}, 
}

@article{gpt,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{opt,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

@article{transformer,
  title={Attention Is All You Need.(Nips), 2017},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  volume={10},
  pages={S0140525X16001837},
  year={2017}
}

@article{virtualassist,
author = {Sivakumar, Shanmugasundaram},
year = {2024},
month = {02},
pages = {1077-1096},
title = {Performance Optimization of Large Language Models (LLMs) in Web Applications},
volume = {8},
journal = {International Journal of Advanced Scientific Research}
}

@misc{chatbot1,
      title={A Complete Survey on LLM-based AI Chatbots}, 
      author={Sumit Kumar Dam and Choong Seon Hong and Yu Qiao and Chaoning Zhang},
      year={2024},
      eprint={2406.16937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16937}, 
}

@article{chatbot2,
  title={Integrating Large Language Models and the iDigBio Portal for Conversational Data Exploration and Retrieval},
  author={Elliott, Michael and Luciano, Manuel and Fortes, Jose},
  journal={Biodiversity Information Science and Standards},
  volume={8},
  pages={e142696},
  year={2024},
  publisher={Pensoft Publishers}
}

@inproceedings{dataanalyse1,
    title = "{I}nsight{P}ilot: An {LLM}-Empowered Automated Data Exploration System",
    author = "Ma, Pingchuan  and
      Ding, Rui  and
      Wang, Shuai  and
      Han, Shi  and
      Zhang, Dongmei",
    editor = "Feng, Yansong  and
      Lefever, Els",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-demo.31/",
    doi = "10.18653/v1/2023.emnlp-demo.31",
    pages = "346--352",
    abstract = "Exploring data is crucial in data analysis, as it helps users understand and interpret the data more effectively. However, performing effective data exploration requires in-depth knowledge of the dataset, the user intent and expertise in data analysis techniques. Not being familiar with either can create obstacles that make the process time-consuming and overwhelming. To address this issue, we introduce InsightPilot, an LLM (Large Language Model)-based, automated data exploration system designed to simplify the data exploration process. InsightPilot features a set of carefully designed analysis actions that streamline the data exploration process. Given a natural language question, InsightPilot collaborates with the LLM to issue a sequence of analysis actions, explore the data and generate insights. We demonstrate the effectiveness of InsightPilot in a user study and a case study, showing how it can help users gain valuable insights from their datasets."
}

@misc{dataanalyse2,
      title={JarviX: A LLM No code Platform for Tabular Data Analysis and Optimization}, 
      author={Shang-Ching Liu and ShengKun Wang and Wenqi Lin and Chung-Wei Hsiung and Yi-Chen Hsieh and Yu-Ping Cheng and Sian-Hong Luo and Tsungyao Chang and Jianwei Zhang},
      year={2023},
      eprint={2312.02213},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.02213}, 
}

@misc{dataanalyse3,
      title={Data Interpreter: An LLM Agent For Data Science}, 
      author={Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Ceyao Zhang and Chenxing Wei and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zhibin Gou and Zongze Xu and Chenglin Wu},
      year={2024},
      eprint={2402.18679},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2402.18679}, 
}

@misc{translation1,
      title={TransLLaMa: LLM-based Simultaneous Translation System}, 
      author={Roman Koshkin and Katsuhito Sudoh and Satoshi Nakamura},
      year={2024},
      eprint={2402.04636},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04636}, 
}

@misc{translation2,
      title={LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages}, 
      author={Yinquan Lu and Wenhao Zhu and Lei Li and Yu Qiao and Fei Yuan},
      year={2024},
      eprint={2407.05975},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.05975}, 
}

@misc{translation3,
      title={Improving LLM Abilities in Idiomatic Translation}, 
      author={Sundesh Donthi and Maximilian Spencer and Om Patel and Joon Doh and Eid Rodan},
      year={2024},
      eprint={2407.03518},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.03518}, 
}

@misc{code1,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@inproceedings{code2,
author = {Gu, Qiuhan},
title = {LLM-Based Code Generation Method for Golang Compiler Testing},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3617850},
doi = {10.1145/3611643.3617850},
abstract = {Modern optimizing compilers are among the most complex software systems humans build. One way to identify subtle compiler bugs is fuzzing. Both the quantity and the quality of testcases are crucial to the performance of fuzzing. Traditional testcase-generation methods, such as Csmith and YARPGen, have been proven successful at discovering compiler bugs. However, such generated testcases have limited coverage and quantity. In this paper, we present a code generation method for compiler testing based on LLM to maximize the quality and quantity of the generated code. In particular, to avoid undefined behavior and syntax errors in generated testcases, we design a filter strategy to clean the source code, preparing a high-quality dataset for the model training. Besides, we present a seed schedule strategy to improve code generation. We apply the method to test the Golang compiler and the result shows that our pipeline outperforms previous methods both qualitatively and quantitatively. It produces testcases with an average coverage of 3.38\%, in contrast to the testcases generated by GoFuzz, which have an average coverage of 0.44\%. Moreover, among all the generated testcases, only 2.79\% exhibited syntax errors, and none displayed undefined behavior.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {2201–2203},
numpages = {3},
keywords = {Code generation, Compiler testing, Go language, Large model},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@misc{code3,
      title={SynCode: LLM Generation with Grammar Augmentation}, 
      author={Shubham Ugare and Tarun Suresh and Hangoo Kang and Sasa Misailovic and Gagandeep Singh},
      year={2024},
      eprint={2403.01632},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.01632}, 
}

@inproceedings{code4,
author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
title = {LLM-based and Retrieval-Augmented Control Code Generation},
year = {2024},
isbn = {9798400705793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643795.3648384},
doi = {10.1145/3643795.3648384},
abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
booktitle = {Proceedings of the 1st International Workshop on Large Language Models for Code},
pages = {22–29},
numpages = {8},
keywords = {large language models, code generation, IEC 61131-3, industrial automation, PLC, DCS, ChatGPT, GPT-4},
location = {Lisbon, Portugal},
series = {LLM4Code '24}
}

@inproceedings{content1,
author = {Acharya, Arkadeep and Singh, Brijraj and Onoe, Naoyuki},
title = {LLM Based Generation of Item-Description for Recommendation System},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3610647},
doi = {10.1145/3604915.3610647},
abstract = {The description of an item plays a pivotal role in providing concise and informative summaries to captivate potential viewers and is essential for recommendation systems. Traditionally, such descriptions were obtained through manual web scraping techniques, which are time-consuming and susceptible to data inconsistencies. In recent years, Large Language Models (LLMs), such as GPT-3.5, and open source LLMs like Alpaca have emerged as powerful tools for natural language processing tasks. In this paper, we have explored how we can use LLMs to generate detailed descriptions of the items. To conduct the study, we have used the MovieLens 1M dataset comprising movie titles and the Goodreads Dataset consisting of names of books and subsequently, an open-sourced LLM, Alpaca, was prompted with few-shot prompting on this dataset to generate detailed movie descriptions considering multiple features like the names of the cast and directors for the ML dataset and the names of the author and publisher for the Goodreads dataset. The generated description was then compared with the scraped descriptions using a combination of Top Hits, MRR, and NDCG as evaluation metrics. The results demonstrated that LLM-based movie description generation exhibits significant promise, with results comparable to the ones obtained by web-scraped descriptions.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1204–1207},
numpages = {4},
keywords = {Large Language Models (LLMs), NLP, automated content generation., web scraping},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@misc{content2,
      title={Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation}, 
      author={Le Xiao and Xiaolin Chen},
      year={2023},
      eprint={2307.02839},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.02839}, 
}

@inproceedings{content3,
    title = "Planning First, Question Second: An {LLM}-Guided Method for Controllable Question Generation",
    author = "Li, Kunze  and
      Zhang, Yu",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.280/",
    doi = "10.18653/v1/2024.findings-acl.280",
    pages = "4715--4729",
    abstract = "In the field of education, for better assessment of students' abilities, generated questions often need to meet experts' requirements, indicating the need for controllable question generation (CQG). However, current CQG methods mainly focus on difficulty control, neglecting the control of question content and assessed abilities, which are also crucial in educational QG. In this paper, we propose an LLM-guided method PFQS (for Planning First, Question Second), which utilizes Llama 2 to generate an answer plan and then generates questions based on it. The plan not only includes candidate answers but also integrates LLM`s understanding and multiple requirements, which make question generation simple and controllable. We evaluate our approach on the FairytaleQA dataset, a well-structured QA dataset derived from child-friendly storybooks. In the dataset, the attribute label represents content control, while the local{\_}or{\_}sum and ex{\_}or{\_}im labels denote difficulty control. Experimental results demonstrate that our approach outperforms previous state-of-the-art results and achieves better consistency with requirements compared to prompt-based method. Further application of our method to Llama 2 and Mistral also leads to improved requirement consistency in a zero-shot setting."
}


@inproceedings {cachedattention,
author = {Bin Gao and Zhuomin He and Puru Sharma and Qingxuan Kang and Djordje Jevdjic and Junbo Deng and Xingkun Yang and Zhou Yu and Pengfei Zuo},
title = {{Cost-Efficient} Large Language Model Serving for Multi-turn Conversations with {CachedAttention}},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {111--126},
url = {https://www.usenix.org/conference/atc24/presentation/gao-bin-cost},
publisher = {USENIX Association},
month = jul
}

@ARTICLE{A100,
  author={Choquette, Jack and Gandhi, Wishwesh and Giroux, Olivier and Stam, Nick and Krashinsky, Ronny},
  journal={IEEE Micro}, 
  title={NVIDIA A100 Tensor Core GPU: Performance and Innovation}, 
  year={2021},
  volume={41},
  number={2},
  pages={29-35},
  keywords={Graphics processing units;Tensors;Bandwidth;Throughput;Parallel processing;Benchmark testing;Artificial intelligence;GPU;A100;NVLink;Deep Learning;Tensor Core;CUDA;C++20},
  doi={10.1109/MM.2021.3061394}}


@misc{SARATHI,
      title={SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills}, 
      author={Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S. Gulavani and Ramachandran Ramjee},
      year={2023},
      eprint={2308.16369},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.16369}, 
}

@misc{StreamingLLM,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2024},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.17453}, 
}

@inproceedings{sia,
author = {Jayaram Subramanya, Suhas and Arfeen, Daiyaan and Lin, Shouxu and Qiao, Aurick and Jia, Zhihao and Ganger, Gregory R.},
title = {Sia: Heterogeneity-aware, goodput-optimized ML-cluster scheduling},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613175},
doi = {10.1145/3600006.3613175},
abstract = {The Sia scheduler efficiently assigns heterogeneous deep learning (DL) cluster resources to elastic resource-adaptive jobs. Although some recent schedulers address one aspect or another (e.g., heterogeneity or resource-adaptivity), none addresses all and most scale poorly to large clusters and/or heavy workloads even without the full complexity of the combined scheduling problem. Sia introduces a new scheduling formulation that can scale to the search-space sizes and intentionally match jobs and their configurations to GPU types and counts, while adapting to changes in cluster load and job mix over time. Sia also introduces a low-profiling-overhead approach to bootstrapping (for each new job) throughput models used to evaluate possible resource assignments, and it is the first cluster scheduler to support elastic scaling of hybrid parallel jobs.Extensive evaluations show that Sia outperforms state-of-the-art schedulers. For example, even on relatively small 44- to 64-GPU clusters with a mix of three GPU types, Sia reduces average job completion time (JCT) by 30--93\%, 99th percentile JCT and makespan by 28--95\%, and GPU hours used by 12--55\% for workloads derived from 3 real-world environments. Additional experiments demonstrate that Sia scales to at least 2000-GPU clusters, provides improved fairness, and is not over-sensitive to scheduler parameter settings.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {642–657},
numpages = {16},
keywords = {cluster scheduling, resource allocation, deep learning training},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings {pollux,
author = {Aurick Qiao and Sang Keun Choe and Suhas Jayaram Subramanya and Willie Neiswanger and Qirong Ho and Hao Zhang and Gregory R. Ganger and Eric P. Xing},
title = {Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning},
booktitle = {15th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 21)},
year = {2021},
isbn = {978-1-939133-22-9},
pages = {1--18},
url = {https://www.usenix.org/conference/osdi21/presentation/qiao},
publisher = {{USENIX} Association},
month = jul
}

@inproceedings {Clockwork,
author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
title = {Serving {DNNs} like Clockwork: Performance Predictability from the Bottom Up},
booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
year = {2020},
isbn = {978-1-939133-19-9},
pages = {443--462},
url = {https://www.usenix.org/conference/osdi20/presentation/gujarati},
publisher = {USENIX Association},
month = nov
}

@inproceedings {SHEPHERD,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
title = {{SHEPHERD}: Serving {DNNs} in the Wild},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {787--808},
url = {https://www.usenix.org/conference/nsdi23/presentation/zhang-hong},
publisher = {USENIX Association},
month = apr
}

@inproceedings {AlpaServe,
author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
title = {{AlpaServe}: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
year = {2023},
isbn = {978-1-939133-34-2},
address = {Boston, MA},
pages = {663--679},
url = {https://www.usenix.org/conference/osdi23/presentation/li-zhouhan},
publisher = {USENIX Association},
month = jul
}

@misc{fastserve,
      title={Fast Distributed Inference Serving for Large Language Models}, 
      author={Bingyang Wu and Yinmin Zhong and Zili Zhang and Shengyu Liu and Fangyue Liu and Yuanhang Sun and Gang Huang and Xuanzhe Liu and Xin Jin},
      year={2024},
      eprint={2305.05920},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.05920}, 
}

@misc{TetriInfer,
      title={Inference without Interference: Disaggregate LLM Inference for Mixed Downstream Workloads}, 
      author={Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2401.11181},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.11181}, 
}

@misc{dejavu,
      title={D\'ej\`aVu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving}, 
      author={Foteini Strati and Sara Mcallister and Amar Phanishayee and Jakub Tarnawski and Ana Klimovic},
      year={2024},
      eprint={2403.01876},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2403.01876}, 
}

@misc{Unstructured1,
      title={Embedding-based Retrieval with LLM for Effective Agriculture Information Extracting from Unstructured Data}, 
      author={Ruoling Peng and Kang Liu and Po Yang and Zhipeng Yuan and Shunbao Li},
      year={2023},
      eprint={2308.03107},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.03107}, 
}

@misc{huggingface,
  title = {{Hugging face accelerate.}},
  year = 2025,
  urldate = {2025-01-14},
  url={https://huggingface.co/docs/accelerate/index},
}

@misc{sharegpt,
  title = {{Sharegpt teams.}},
  year = 2025,
  urldate = {2025-01-14},
  url={https://sharegpt.com/},
}

@INPROCEEDINGS{splitwise,
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, 
  title={Splitwise: Efficient Generative LLM Inference Using Phase Splitting}, 
  year={2024},
  volume={},
  number={},
  pages={118-132},
  keywords={Costs;Processor scheduling;Large language models;Computational modeling;Graphics processing units;Computer architecture;Throughput;Large language models;Cluster deployments;Scheduling;GPUs;Inference efficiency;Machine learning;Resource management},
  doi={10.1109/ISCA59077.2024.00019}}

////////////

@article{hjx1,
title = {Deep reinforcement and transfer learning for abstractive text summarization: A review},
journal = {Computer Speech \& Language},
volume = {71},
pages = {101276},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2021.101276},
url = {https://www.sciencedirect.com/science/article/pii/S0885230821000796},
author = {Ayham Alomari and Norisma Idris and Aznul Qalid Md Sabri and Izzat Alsmadi},
keywords = {Abstractive summarization, Sequence-to-sequence, Reinforcement learning, Pre-trained models},
abstract = {Automatic Text Summarization (ATS) is an important area in Natural Language Processing (NLP) with the goal of shortening a long text into a more compact version by conveying the most important points in a readable form. ATS applications continue to evolve and utilize effective approaches that are being evaluated and implemented by researchers. State-of-the-Art (SotA) technologies that demonstrate cutting-edge performance and accuracy in abstractive ATS are deep neural sequence-to-sequence models, Reinforcement Learning (RL) approaches, and Transfer Learning (TL) approaches, including Pre-Trained Language Models (PTLMs). The graph-based Transformer architecture and PTLMs have influenced tremendous advances in NLP applications. Additionally, the incorporation of recent mechanisms, such as the knowledge-enhanced mechanism, significantly enhanced the results. This study provides a comprehensive review of recent research advances in the area of abstractive text summarization for works spanning the past six years. Past and present problems are described, as well as their proposed solutions. In addition, abstractive ATS datasets and evaluation measurements are also highlighted. The paper concludes by comparing the best models and discussing future research directions.}
}

@article{hjx2,
  title={Recent automatic text summarization techniques: a survey},
  author={Gambhir, Mahak and Gupta, Vishal},
  journal={Artificial Intelligence Review},
  volume={47},
  number={1},
  pages={1--66},
  year={2017},
  publisher={Springer}
}

@inproceedings{hjx3,
    title = "Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization",
    author = "Zhu, Junnan  and
      Zhou, Yu  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.121/",
    doi = "10.18653/v1/2020.acl-main.121",
    pages = "1309--1321",
    abstract = "Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English). In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary. We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary. Specifically, we first employ the encoder-decoder attention distribution to attend to the source words. Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word. Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words. Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art."
}


////////////////////////////////////////////

@inproceedings{hjx4,
    title = "Beyond Traditional Benchmarks: Analyzing Behaviors of Open {LLM}s on Data-to-Text Generation",
    author = "Kasner, Zden{\v{e}}k  and
      Dusek, Ondrej",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.651/",
    doi = "10.18653/v1/2024.acl-long.651",
    pages = "12045--12072",
    abstract = "We analyze the behaviors of open large language models (LLMs) on the task of data-to-text (D2T) generation, i.e., generating coherent and relevant text from structured data. To avoid the issue of LLM training data contamination with standard benchmarks, we design Quintd - a tool for collecting novel structured data records from public APIs. We find that open LLMs (Llama 2, Mistral, and Zephyr) can generate fluent and coherent texts in zero-shot settings from data in common formats collected with Quintd. However, we show that the semantic accuracy of the outputs is a major issue: both according to human annotators and our reference-free metric based on GPT-4, more than 80{\%} of the outputs of open LLMs contain at least one semantic error. We publicly release the code, data, and model outputs."
}

////////////////////////////////////////////


@article{hjx5,
  title={LLM-Enhanced Data Management},
  author={Zhou, Xuanhe and Zhao, Xinyang and Li, Guoliang},
  journal={arXiv preprint arXiv:2402.02643},
  year={2024}
}

@article{hjx6,
  title={A comparative study of offline models and online llms in fake news detection},
  author={Xu, Ruoyu and Li, Gaoxiang},
  journal={arXiv preprint arXiv:2409.03067},
  year={2024}
}

@INPROCEEDINGS{pciecontention,
  author={Side, Mert and Yao, Fan and Zhang, Zhenkai},
  booktitle={2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P)}, 
  title={LockedDown: Exploiting Contention on Host-GPU PCIe Bus for Fun and Profit}, 
  year={2022},
  volume={},
  number={},
  pages={270-285},
  keywords={Cloud computing;Error analysis;Linux;Web pages;Graphics processing units;Fingerprint recognition;Parallel processing},
  doi={10.1109/EuroSP53844.2022.00025}}


@ARTICLE{pciecontention2,
  author={Li, Chen and Sun, Yifan and Jin, Lingling and Xu, Lingjie and Cao, Zheng and Fan, Pengfei and Kaeli, David and Ma, Sheng and Guo, Yang and Yang, Jun},
  journal={IEEE Computer Architecture Letters}, 
  title={Priority-Based PCIe Scheduling for Multi-Tenant Multi-GPU Systems}, 
  year={2019},
  volume={18},
  number={2},
  pages={157-160},
  keywords={Graphics processing units;Task analysis;Bandwidth;Data transfer;Switches;Quality of service;Throughput;Multi-GPU;multi-tenant;PCIe scheduling},
  doi={10.1109/LCA.2019.2955119}}

@misc{longform,
      title={AWESOME: GPU Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content}, 
      author={Shuyang Cao and Lu Wang},
      year={2023},
      eprint={2305.14806},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14806}, 
}

@inproceedings{SLOeconomy,
author = {Patke, Archit and Reddy, Dhemath and Jha, Saurabh and Qiu, Haoran and Pinto, Christian and Narayanaswami, Chandra and Kalbarczyk, Zbigniew and Iyer, Ravishankar},
title = {Queue Management for SLO-Oriented Large Language Model Serving},
year = {2024},
isbn = {9798400712869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698038.3698523},
doi = {10.1145/3698038.3698523},
abstract = {Large language model (LLM) serving is becoming an increasingly critical workload for cloud providers. Existing LLM serving systems focus on interactive requests, such as chatbots and coding assistants, with tight latency SLO requirements. However, when such systems execute batch requests that have relaxed SLOs along with interactive requests, it leads to poor multiplexing and inefficient resource utilization. To address these challenges, we propose QLM, a queue management system for LLM serving. QLM maintains batch and interactive requests across different models and SLOs in a request queue. Optimal ordering of the request queue is critical to maintain SLOs while ensuring high resource utilization. To generate this optimal ordering, QLM uses a Request Waiting Time (RWT) Estimator that estimates the waiting times for requests in the request queue. These estimates are used by a global scheduler to orchestrate LLM Serving Operations (LSOs) such as request pulling, request eviction, load balancing, and model swapping. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90\% and throughput by 20-400\% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems. QLM's evaluation is based on the production requirements of a cloud provider. QLM is publicly available at https://www.github.com/QLM-project/QLM.},
booktitle = {Proceedings of the 2024 ACM Symposium on Cloud Computing},
pages = {18–35},
numpages = {18},
keywords = {large language models, machine learning inference, queuing},
location = {Redmond, WA, USA},
series = {SoCC '24}
}

@misc{lin2024infinitellmefficientllmservice,
      title={Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache}, 
      author={Bin Lin and Chen Zhang and Tao Peng and Hanyu Zhao and Wencong Xiao and Minmin Sun and Anmin Liu and Zhipeng Zhang and Lanbo Li and Xiafei Qiu and Shen Li and Zhigang Ji and Tao Xie and Yong Li and Wei Lin},
      year={2024},
      eprint={2401.02669},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2401.02669}, 
}

@article{alpaca1,
  title={Stanford alpaca: an instruction-following llama model (2023)},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={URL https://github. com/tatsu-lab/stanford\_alpaca},
  volume={1},
  number={9},
  year={2023}
}

@misc{alpaca2,
      title={Self-Instruct: Aligning Language Models with Self-Generated Instructions}, 
      author={Yizhong Wang and Yeganeh Kordi and Swaroop Mishra and Alisa Liu and Noah A. Smith and Daniel Khashabi and Hannaneh Hajishirzi},
      year={2023},
      eprint={2212.10560},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.10560}, 
}

@misc{humanspeed,
      title={M\'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity}, 
      author={Tyler Griggs and Xiaoxuan Liu and Jiaxiang Yu and Doyoung Kim and Wei-Lin Chiang and Alvin Cheung and Ion Stoica},
      year={2024},
      eprint={2404.14527},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2404.14527}, 
}