%!TEX root = main.tex
\setlength{\intextsep}{0pt}
\begin{algorithm}[htp]
\caption{Generative Predictive Control (\nameshort)}
\label{alg:gpc}
% \SetAlgoLined
\KwIn{Expert action demonstrations $\calD^P_\expert = \{I_t^i \leftrightarrow a^i_{t:t+T}\}_{i=1}^{N_\expert}$; Expert trajectories and random exploration trajectories $\calD^W_\expert \cup \calD^W_\explore = \{ (I_t^i,a^i_{t:t+T}) \leftrightarrow I^i_{t+1:t+T+1} \}_{i=1}^{N_\expert + N_\explore}$; Reward model $\calR$; Positive integers $K$, $M$}

// \texttt{{Generative policy training}}  

$\calP(\cdot) \leftarrow \textsc{BehaviorCloning}(\calD^P_{N_\expert})$ \;

// \texttt{{Predictive world modeling}} (\S\ref{sec:world-model}) 

$\calW(\cdot) \leftarrow \textsc{DynamicsLearning}(\calD^W_\expert \cup \calD^W_\explore)$ \;

// \texttt{{Online planning given $I_t$}}

\For{$k=1,\dots,K$}{
    // \texttt{{Sample action proposal}}
    
    $a^{(k)}_{t:t+T} \sim \calP(I_t)$\;

    // \texttt{{Maximize reward}}
    
    Set $\hat{a}^{(0)}_{t:t+T} = a^{(k)}_{t:t+T}$\;
    
    \For{$\ell=1,\dots,M$}{
        $\hat{a}^{(\ell)}_{t:t+T} = \hat{a}^{(\ell-1)}_{t:t+T} + \eta^{(\ell)} \nabla_{a_{t:t+T}} \calR (\calW(I_t, \hat{a}_{t:t+T}^{(\ell-1)}))$\;}
    Set $a^{(k)}_{t:t+T} = \hat{a}^{(M)}_{t:t+T}$\;
}

// \texttt{{Rank optimized action proposals}}

Find $k_\star \in \argmax_{k=1,\dots,K} \ \calR(\calW(I_t, a_{t:t+T}^{(k)}))$ \;

Return $a_{t:t+T}^{(k_\star)}$ \;

\end{algorithm}