%!TEX root = ../main.tex

\section{Experiments}
\label{sec:experiments}

In this section, we focus on evaluation of the \nameshort framework on planar pushing and comparison to baseline behavior cloning. As mentioned in \S\ref{sec:world-model}, we choose planar pushing because it is an elementary but also quite challenging task due to the complicated contact and collision dynamics involved. 


\subsection{State-based Planar Pushing in Simulation}
\label{sec:push-T-sim-state}

We first study the setup where the groundtruth state of the object (i.e., $(p_x,p_y,\theta)$ is available through a simulator. We use the same simulator as in~\cite{chi2023diffusion} with a T-shape object.

\textbf{Policy}. We first train a state-based diffusion policy $\calP(\cdot)$ \cite{chi2023diffusion} from which we can draw \emph{action proposals}, as explained in \S\ref{sec:gpc}. We use $500$ expert demonstrations to train the policy network for $300$ epochs with AdamW optimizer of learning rate $10^{-4}$ and weight decay $10^{-6}$. We use observation horizon $H=2$, prediction horizon $T= 16$, and action horizon $8$.\footnote{This means the policy is executed in a receding horizon fashion. The policy predicts $16$ steps of future actions and only $8$ of them are executed in the environment, which creates implicit feedback.} 

% From this \emph{stochastic} diffusion policy, we draw $K$ action proposals, each with horizon of $16$. 

\textbf{World model}. Then we need the world model to predict the future consequences of each action proposal. As in \S\ref{sec:state-based}, we use MLP networks to build a single-step state predictor and recursively apply it $T$ times to form the world model $\calW(\cdot)$. As introduced in \S\ref{sec:gpc}, to ensure the dynamics of the world model is learned from a richer distribution than the expert demonstrations, the training dataset of the world model is expanded with random exploration demonstrations which attempt to record more pusher-object interactions at different contact points. 
% The goal of using random demos is to help world model learn a more accurate physics dynamics model in the environment. 
To be specific, we let human operators (who are different from the operator of the expert demonstrations) collect $7$ long random play demonstrations and 1 special long demonstration that randomly collides the object with the boundary wall. In addition, we also generate $7$ sets of ``noisy expert demonstrations'' where the original expert actions are contaminated with different scales of Gaussian noise.

\textbf{Reward}. After obtaining the predicted consequences of the action proposals (i.e., predicted future states of $p_x, p_y, \theta$ of the object), we define the reward $\calR(\cdot)$ using a \emph{registration loss} between the target and the object. Particularly, the registration loss computes the sum of the squared distances between the vertices of the object and the target. Clearly, this reward function is differentiable with respect to the predicted states. 
% In state-based experiments, since we have the information about target vertices and block vertices, $\calR(\cdot)$ can be calculated analytically. This design also ensures the reward function is differentiable, so that it can be used for \gpcopt.

\textbf{Online planning}. Now we can use the policy $\calP(\cdot)$, the world model $\calW(\cdot)$, and the reward model $\calR(\cdot)$ for online planning, as prescribed in the \nameshort Algorithm~\ref{alg:gpc}. Specifically, In \gpcrank, we pick the action chunk with highest reward (smallest registration loss) from the $K$ action proposals. In \gpcopt, we sample one action chunk from $\calP(\cdot)$ and perform $M$ steps of gradient ascent to optimize this action chunk for a higher reward (smaller registration loss). In \gpcrankopt, we sample $K$ action proposals, perform $M$ gradient ascent steps for each of the action proposal, and finally pick the \emph{optimized} action chunk with the highest reward.

\textbf{Questions}. We hope to answer the following questions through our experiments:
\begin{itemize}
    \item How much does \nameshort outperform the state-of-art behavior cloning method (e.g., diffusion policy) and how do $K$ and $M$ affect the performance? 
    \item How does \nameshort compare with \offlinerl based method? 
    \item What is the effect of training the world model using both expert and random exploration data?
\end{itemize}

\textbf{Results}. 
In Table~\ref{table:state-base-gpc}, we study how different variations of $K$ and $M$ affect the performance. When $K=1$ and $M=0$, this is the baseline state-based diffusion policy trained from behavior cloning \cite{chi2023diffusion}. In \gpcrank, if we increase the number of action proposals to $K=100$, the score increases to around 0.9, which is about $10\%$ increase from the baseline. Notably, due to parallelization inside \gpcrank, the runtime does not increase when $K=100$. We also tried larger $K$, such as $1000$ and $5000$, and the performance further increases to $0.934$. Note that in the last column of Table~\ref{table:state-base-gpc}, we provided a ``performance upper bound'' which is obtained by using the groundtruth simulator (instead of the learned world model) to rank action proposals at $K=5000$.
For \gpcopt, if we sample one action proposal and perform gradient ascent from it (e.g., doing $30$ gradient steps), the score can be increased to $0.886$, which is also quite significant compared to the baseline. Then in \gpcrankopt, we tried different combinations of $K$ and $M$. The improvement remains substantial, indicating the robustness of \nameshort and shows both sampling multiple proposals and gradient-based optimization consistently improve the performance.  
% which combines \gpcrank and \gpcopt. The scores for these can go over $0.9$, which means that in every evaluation seed from various starting positions, the pushing block is almost aligned with the target. To note that we test different combinations of $K$ and $M$ in \gpcrankopt and they all increase the performance obviously, which shows that 

In the first two columns of Table~\ref{table:state-base-gpc}, we present the results for another baseline of using \offlinerl to rank and select actions. The \offlinerl method follows that of~\cite{nakamoto2024steering} and is described in Appendix~\ref{app:app_offlineRL}. Crucially, the \offlinerl method only uses the expert demonstrations and does not leverage the random exploration data (because the random exploration data do not have labeled rewards). We can see that the performance of \offlinerl is similar or even worse than the case of behavior cloning. A plausible reason for this is that the \offlinerl method does not explicitly seek to learn the dynamics from the random explorations and hence the learned value function may be misleading for ranking action proposals.

% It manifests that \offlinerl is not able to accurately pick optimal action chunk from the proposals and this even has negative impact when the number of proposals increase (because this will pick the "fake" good action chunk). This also shows that using a good way to select from action proposals in \gpcrank is not a trivial work. 

Finally, in Table~\ref{table:state-base-compare-only-expert}, we present the results of using a world model trained without random exploration data. We observe that, when the world model is only trained from expert demonstrations, the performance of both \gpcrank and \gpcopt gets worse. This demonstrates the importance of using random exploration data to learn an accurate world model.

% \S\ref{sec:gpc}, using random exploration data to train world model is important and efficient.


\input{sections/table-state-base-GPC}
\input{sections/table-state-base-compare-only-expert}

\subsection{Vision-based Planar Pushing in Simulation}
\label{sec:push-T-sim-vision}

\input{sections/fig-compare-world-model}

We then proceed to vision-based planar pushing with the same T-shape object.

\textbf{Policy}. We train a vision-based diffusion policy $\calP(\cdot)$~\cite{chi2023diffusion} using ResNet18 as the vision encoder and UNet as the action diffusion network. We used $500$ expert demonstrations to train the policy for $300$ epochs with AdamW optimizer of learning rate $10^{-4}$ and weight decay $10^{-6}$. We use observation horizon of 2, prediction horizon of 16, and action horizon of 9. 

% From this \emph{stochastic} diffusion policy, we draw $K$ action proposals, each with horizon of $16$. 

\textbf{World model}. As in \S\ref{sec:vision-based}, we use a diffusion model to build a single-step image predictor and recursively apply it $T$ times to form the world model $\calW(\cdot)$. Same as in \S\ref{sec:push-T-sim-state}, the training data includes random explorations.

\textbf{Reward}. Different from the state-based planar pushing, since the future predictions are images, we cannot directly compute a reward signal. Therefore, we first train a pose prediction network with ResNet18 plus MLP, and then compute the registration loss using the predicted object pose. Note that this learned reward model $\calR(\cdot)$ is still differentiable.

% image. Then the reward is calculated with registration loss between target and block. 

\textbf{Online planning}: With the vision-based policy, the visual world model, and the learned reward model, we follow the same online planning protocol described in \S\ref{sec:push-T-sim-state}.

\textbf{Quality of visual world modeling}. Before presenting the results of \nameshort for online planning, we first study whether our visual world modeling technique is better than some baselines. Particularly, we chose two baselines, one from the pioneering deep visual foresight~\cite{finn2017deep}, and the other from the more recent AVDC paper~\cite{du2024learning}. The world model in deep visual foresight leverages CNN and LSTM for future prediction, and we follow the Github implementation.\footnote{\url{https://github.com/Xiaohui9607/physical_interaction_video_prediction_pytorch}} AVDC is originally designed for video diffusion conditioned on language, and we modify its official implementation\footnote{\url{https://github.com/flow-diffusion/AVDC}} to condition on robot actions. Notably, AVDC is a multiple-step world model as it simultaneously diffuses future images together, unlike our recursive structure. Fig.~\ref{fig:compare-world-model} compares the quality of future predictions of three methods (ours, deep visual foresight, and AVDC) against the groundtruth in two examples. We see that diffusion-based visual world modeling (ours and AVDC) clearly dominates the classical deep visual foresight. Between ours and AVDC, we can observe that the future predictions from our method are better aligned with the groundtruth.

\textbf{Planning performance}.
Same as~\S\ref{sec:push-T-sim-state}, we study the impact of $K$ and $M$ on the performance of \nameshort, its comparison with \offlinerl, and the importance of random exploration.

In Table \ref{table:vision-base-gpc}, we study how different variations of $K$ and $M$ affect the performance of \nameshort. We can see that (a) \gpcrank increases around $15\%$ from the diffusion policy baseline (when $K=1, M=0$); (b) \gpcopt increases around $25\%$ from the diffusion policy baseline; and (c) \gpcrankopt increases up to $40\%$ from the baseline. Contrasting the vision-based results with the state-based results, we observe that  \gpcopt appears to have a larger advantage in vision-based push-T. This indeed makes sense because the baseline vision-based diffusion policy only has a score of $0.642$, leaving a lot of room for optimization to find better actions. 

% is very far away from a perfect policy. In this case it is not enough to only select action proposals from the distribution generated with the action policy. We need corrections to these actions. Therefore, \gpcopt is an important method when the policy itself is inferior. 
In the first column of Table~\ref{table:vision-base-gpc}, we show the results of \offlinerl. Unfortunately, \offlinerl still does not perform as well as even the baseline vision-based diffusion policy. 

In Table~\ref{table:vision-base-compare-only-expert}, we compare the performance of \gpcrank and \gpcopt when trained with or without random exploration. Consistent with our observations in~\ref{sec:push-T-sim-state}, without training on random exploration, the performance of online planning significantly decreases. 




\input{sections/table-vision-base-GPC}
\input{sections/table-vision-base-compare-only-expert}


\subsection{Real-world Vision-based Planar Pushing}
\label{sec:real-world}

We then test \nameshort in real-world planar pushing. We not only show \nameshort is effective in ``plain push-T'', but also show it can work for push-T with collision objects. 

\input{sections/fig-realworld-setup}

\textbf{Setup}. Fig.~\ref{fig:real-world-setup} depicts our real-world setup for vision-based planar pushing. The pusher is attached to the end-effector of a Franka Panda robotic arm to push a T block into the target position. An overhead Intel RealSense camera is used to provide image observations to the pusher. To train the reward model, we need to obtain block information as training data so we attached three AprilTags~\citep{olson2011apriltag} to the T-block, which enables accurate pose estimation from image frames even in the case of occlusion. To collect real-world expert demonstrations, we follow~\citet{chi2023diffusion} and use a SpaceMouse to teleoperate the robotic arm to push the T block. A and R blocks are the objects that T may collide into during pushing. 

\subsubsection{Plain push-T}
We first test on plain push-T (i.e., without collision with other objects). We train the policy with $100$ expert push-T demonstrations using the architecture mentioned in \S\ref{sec:push-T-sim-vision}. Because of the slower moving speed of real-world pusher, we use a larger prediction horizon $T=32$. Same as \S\ref{sec:push-T-sim-vision}, we train the world model using all expert demonstrations and a few random explorations. For the reward model, we rely on AprilTag~\citep{olson2011apriltag} to obtain the ground-truth object poses and use them to train the reward model $\calR(\cdot)$. To be clear, our \nameshort pipeline in real world is still purely vision-based and the AprilTags are only used to obtain training data for the reward model (training the pose predictor). We follow the same online planning protocol described in \S\ref{sec:push-T-sim-state}. 
We test \gpcrank and \gpcopt on a set of 10 push-T tasks where the T block is initialized at positions not seen during training. 

\textbf{Performance}.
While the baseline model succeeds 5 out of 10 times, \gpcrank succeeds 7 out of 10 times and \gpcopt succeeds 7 out of 10 times. Fig.~\ref{fig:fig-real_world_main_paper}(a) shows the trajectories for one test. More testing trajectories are shown in Appendix~\ref{app:real_world} and supplementary videos.

\input{sections/fig-realworld-prediction-pusht-main-paper}
\textbf{Quality of world model predictions}.
Fig.~\ref{fig:push-t-worldmodel-prediction} shows the quality of world model prediction of push T for \gpcrank and \gpcopt. The images shown are all \textbf{predicted} by the world model. They not only have very good visual effect and look realistic, but also they can accurately predict the dynamics reaction from given actions. From the last column, we can see even when the differences between action sequences are very small, the world model can still capture the details and predict the reaction in a meticulous way. This is very important to make \nameshort work. 

\subsubsection{Push-T under collision}
Next, we hope to investigate how \nameshort works under more complex scenarios, involving harder tasks and more complex dynamics. We put an object of shape ``A'' on top of the target position so that when pushing T, it will inevitably collide with object A. The training data is $100$ push-T expert demos with collision of A and some random exploration demos. 

\textbf{Performance}. We test $5$ seeds in total, and baseline model, \gpcrank, \gpcopt succeed for $2$, $4$, $3$ seeds respectively.

We then proceed to test in an even harder situation: we put objects of shape ``A'' and ``R'' on top of the target position at the same time. Case 1: When pushing T, T may collide with A and R at the same time. Case 2: When pushing T, T would collide with one letter (e.g., ``R'') and this letter will collide with another letter (e.g., ``A''). We train the policy and world model here using $100$ plain push-T expert demonstrations, $100$ push-T colliding A expert demonstrations, and $100$ push-T colliding A\&R expert demonstrations. 

\textbf{Performance}. In the total of $5$ testing seeds, baseline model, \gpcrank, \gpcopt succeed $2$, $3$, $4$ respectively. 

Fig.~\ref{fig:fig-real_world_main_paper}(b)(c) show the trajectories for both of these tests. More testing trajectories are shown in Appendix~\ref{app:real_world} and supplementary videos.


\input{sections/fig-realworld-prediction-pushta-main-paper}
\input{sections/fig-realworld-prediction-pushart-main-paper}
\textbf{Quality of world model predictions}.
Fig.~\ref{fig:push-ta-worldmodel-prediction} and Fig.~\ref{fig:push-art-worldmodel-prediction} show the quality of world model prediction for \gpcrank and \gpcopt, having collisions during push-T. The images shown are all \textbf{predicted} by the world model. With adding some collision objects, the world model is not only able to give near perfect prediction (i.e. no deformation) for a complex scene composing several objects, but also it can accurately predict the dynamics among these objects. 



\subsubsection{Generalization to unseen cases}
Last but not least, we want to understand whether \nameshort can help when generalizing to unseen cases. We use the policy and world model trained with push-T, push-T colliding A, push-T colliding A\&R expert demos. But we perform the test to push T when an object of letter ``R'' is on top of the target. This situation has never been seen in any of the training data. We test three different starting positions for T and R. 

\textbf{Performance}.
\gpcrank and \gpcopt succeed in all of the three tests while baseline model fails in one of the cases. 
The reason behind this is that when generalizing to unseen cases, the policy's performance degrades. So we need to either generate more samples from the action policy's distribution (like giving more opportunities to generate an optimal proposal) or do gradient ascent to optimize the action proposal.



Fig.~\ref{fig:fig-real_world_main_paper_unseen} shows one test. More testing trajectories are shown in Appendix~\ref{app:real_world} and supplementary videos.

\input{sections/fig-realworld-prediction-pushtr-main-paper}
\textbf{Quality of world model predictions}.
Fig.~\ref{fig:push-tr-worldmodel-prediction} shows the quality of world model prediction of push T under collision in unseen scenarios for \gpcrank and \gpcopt. A scenario only composes of T and R is never seen in the training data. However, our world model can still give good predictions for action sequences here. It verifies the ability of generalization for our world model.   

\begin{remark}[Hallucination]\label{remark:hallucination}
    One of the important issues of generative modeling is \emph{hallucination}, and our visual world model is no exception. In particular, there are several cases where the world model predicts images of the future that violates rigid-body physics. For example, the objects T, A, and R may penetrate each other, as we show in Appendix~\ref{app:sec:halluciate}. Interestingly, even though the predictions can violate physics, using such predictions for planning still leads to \nameshort's superior performance. We believe learning a visual world model that strictly satisfies the laws of physics remains a challenging task, and strategies such as data scaling and baking in physics priors may help solve this challenge.
\end{remark}

%\textbf{Plain push-T}.
%\textbf{Push-T under collision}.
%\textbf{Generalization to unseen cases}. 





\input{sections/fig-real_world_main_paper}

\input{sections/fig-real_world_main_paper_unseen}





