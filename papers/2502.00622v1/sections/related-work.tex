%!TEX root = ../main.tex

\vspace{4mm}
\section{Related Work}
\label{sec:related-work}

% Robot learning has evolved from simple imitation to sophisticated reasoning systems. At its core, Learning from Demonstrations~\cite{schaal1996learning, osa2018algorithmic} provides the fundamental ability to learn from human demonstrations, serving as a stepping stone for more advanced approaches. While direct imitation proved effective, recent works have explore how Generative Modeling methods~\cite{janner2022planning,chi2023diffusion, chen2023genaug, jiang2022vima, urain2024deep} can built upon this foundation to create more adaptable and generalizable robotic skills. To achieve higher-level reasoning, Visual World Modeling techniques~\cite{ha2018world, hafner2019learning, du2024learning, yang2023learning, liang2024dreamitate} have emerged to capture and understand environmental dynamics. These capabilities culminate in Visual Foresight and Planning systems~\cite{byravan2017se3, lynch2020learning, yen2020learning, du2023video}, enabling robots to predict outcomes and make decisions in complex, real-world scenarios.

% \textbf{Behavior Cloning in Robotics}. Learning from demonstrations forms a critical foundation in robot learning, where \citet{levine2018learning} established a scalable approach for acquiring hand-eye coordination skills through extensive data collection. A significant breakthrough came with meta-learning techniques, as \citet{duan2017one} achieved one-shot imitation, dramatically reducing the required demonstration data. To further enhance this approach, IRIS~\cite{mandlekar2020iris} tackled the challenge of robust skill acquisition by combining imitation learning with self-supervised exploration. This foundation led to even more ambitious goals, as BC-Z~\cite{jang2022bc} enabled zero-shot transfer across tasks, while impressive real-world control abilities emerged from the RT-1 transformer architecture~\cite{brohan2022rt}. These advances catalyzed further innovations, with RT-2~\cite{brohan2023rt} leveraging vision-language models to transfer web knowledge to robotic control, followed by RT-X~\cite{o2023open}, which demonstrated scalable transfer across diverse robotic platforms through open-embodied learning. 

\textbf{Generative modeling for robotics}. A large body of recent work has explored how generative models can be integrated with robotics~\cite{urain2024deep}. Generative models have been used extensively to represent policies~\cite{mandlekar2020iris, lynch2020learning, jang2022bc, chi2023diffusion, brohan2023rt, ankile2024imitation, lee2024behavior, jiang2022vima}, where models are often trained using large datasets of task demonstrations. In addition, generative models have also been used as a source of generating additional data~\cite{wang2023robogen, wang2023gensim, chen2023genaug} as well as models of the dynamics of the world~\cite{liu2022structdiffusion,byravan2017se3, yang2023learning}.


 % Most related to our work, a recent body of work has explored how generative models can be used for planning.
 
 % explored  work has explored how learned generative models can be used to effectively plan for future sequences of actions.
 
Finally, a large body of work has explored how generative models can be used for planning~\cite{janner2022planning, ajay2022conditional, yang2024diffusion, carvalho2023motion, luo2024potential}. Such methods directly train a generative model over sequences of state and action trajectories, and use the generation process to optimize actions. Different from these works, our work proposes a generative and predictive control (\nameshort) framework that combines a generative policy with a predictive world model through online planning to obtain more robust policy execution on new instances of a task. The modular design of \nameshort also allows for training foundation policies and foundation world models separately, using different data.

% with a  a learned visual model as dynamics model to enable effective
% Different for these works, we introduce a framework where we optimize the actions of a policy with respect to video generative model.

% A breakthrough in visuomotor control came through diffusion policy~\cite{chi2023diffusion}, while parallel innovations in GenAug~\cite{chen2023genaug} enabled sophisticated behavior retargeting in novel situations. The scope of generative approaches expanded significantly with VIMA~\cite{jiang2022vima}, which introduced multimodal prompting for general manipulation tasks. At the current frontier, researchers have achieved remarkable advances in two key directions: developing sophisticated simulation task generation through language models~\cite{wang2023gensim}, while simultaneously creating powerful frameworks for infinite training data through generative simulation~\cite{wang2023robogen}.



\textbf{Visual world modeling}. Learning predictive visual models of the world has a long history~\cite{xue2016visual, oh2015action, ha2018world, oprea2020review}.  Recently, video generative models have emerged as a powerful tool for modeling the physical world~\cite{du2024learning, yang2023learning, liang2024dreamitate, ko2023learning, wang2024language, bharadhwaj2024gen2act, bar2024navigation}. Such models have been used to initialize policies~\cite{du2024learning, ko2023learning, bharadhwaj2024gen2act}, as interactive simulators~\cite{yang2023learning, liang2024dreamitate, bar2024navigation}, and integrated with downstream planning~\cite{du2023video,wang2024language}. 
Different from these works, our work uses a video model as an \emph{action-conditioned dynamics model} that enables us to build a generative control framework that online computes a sequence of actions to optimizes a task-specific reward function. 

% A crucial advancement in robotic reasoning came with the pioneering world models concept~\cite{ha2018world}, fundamentally changing how robots represent their environment. Building on this foundation, pixel-based planning evolved through sophisticated latent dynamics learning~\cite{hafner2019learning}, while Dreamer's latent imagination approach~\cite{hafner2019dream} introduced behavior learning through imagination by combining model-based planning with actor-critic learning in latent space. As the field tackled real-world complexity, innovative solutions emerged through interactive simulators~\cite{yang2023learning}. Dreamitate~\cite{liang2024dreamitate} proposed a novel approach that fine-tunes a video diffusion model on human demonstrations and uses common tools to bridge the embodiment gap between humans and robots, achieving better generalization than existing behavior cloning approaches through internet-scale pretraining.

% \textbf{Visual Foresight \& Planning}. The development of sophisticated robot planning capabilities has been significantly shaped by advances in visual prediction. Early work in visual motion prediction~\cite{finn2017deep} demonstrated the effectiveness of visual foresight for motion planning. This approach was further expanded through Visual Foresight~\cite{ebert2018visual}, which established robust visual prediction methods for control. Structured dynamics modeling gained new precision through SE3-Pose-Nets~\cite{byravan2017se3} enabling more accurate visuomotor planning. The exploration of play-based data yielded powerful latent planning capabilities~\cite{lynch2020learning}, while manipulation tasks saw new possibilities through innovative visual pre-training approaches~\cite{yen2020learning}. Most recently, the integration of language understanding with visual planning~\cite{du2023video} has opened exciting new frontiers for natural instruction-following robots.

