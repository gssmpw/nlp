%!TEX root = ../main.tex

\section{Conclusion}
\label{sec:conclusion}

We presented generative predictive control (\nameshort), a learning control paradigm that combines a generative policy with a predictive world model during online planning. It is worth noting that \nameshort bears great similarity with the classical literature on predictive control in the sense that the dynamics model is first identified and then optimization is performed online to decide actions. The new insights offered by \nameshort, perhaps, are that (a) modern conditional diffusion models provide a powerful tool to learn controllable action-conditioned visual world models; and (b) a generative policy trained from behavior cloning provides good warmstarts for online optimization that seem promising to partially tackle the nonconvexity of online optimization. Through an in-depth investigation on planar pushing---both simulated and real---we demonstrated the superior robustness of \nameshort in both state-based and vision-based robotic manipulation.

\textbf{Limitations}. Two issues are worth highlighting. The first issue is already mentioned in Remark~\ref{remark:hallucination} and concerns the problem of hallucinating future predictions that violate the laws of physics. To address this, it may be possible to leverage the so-called world foundation models~\cite{agarwal2025cosmos}---trained on Internet-scale videos---and finetune them for specific robotic tasks. The second issue is associated with the computational complexity of using an expensive diffusion-based world model in online planning. It is worth noting that, although \nameshort dominates the baseline in terms of success rates, it is generally slower than the baseline due to predicting the future using the world model. A promising solution is to leverage consistency models~\cite{song2023consistency} and one-step diffusion~\cite{yin2024one} for faster world model prediction.