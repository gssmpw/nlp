%!TEX root = ../main.tex

\vspace{-4mm}
\section{Introduction}
\label{sec:introduction}

State-of-the-art robot control policies are synthesized using two major paradigms:
\begin{itemize}
    \item \textbf{Predictive control}. With access to a predictive dynamical model of the robot and its physical environment, \emph{optimal control} (OC) and (deep) \emph{reinforcement learning} (RL) are the workhorses for designing performant controllers in locomotion~\cite{wensing2023optimization}, manipulation~\cite{akkaya2019solving}, and aviation~\cite{acikmese2007convex,kaufmann2020deep}. While OC typically designs controllers based on analytic dynamical models and online optimization~\cite{kang2024fast}, RL can learn a policy via interactions with a complicated and black-box dynamical model~\cite{rudin2022learning}, either through simulation or in the real world. The nature of predictive control is to \emph{look ahead}, by anticipating how the environment will change and how much reward will be collected.
    \item \textbf{Generative control}. Motivated by the success of generative modeling in vision and language, generative control---learning a controller from expert demonstrations using \emph{behavior cloning} with generative models---is rapidly rising to prominence~\cite{chi2023diffusion,urain2024deep}. Due to the abundance of demonstrations and the unprecedented distribution learning capacity of generative models, generative control has led to \emph{robot foundation policies} capable of solving diverse tasks from vision and language~\cite{firoozi2023foundation}. The nature of generative control is to \emph{look back}, making decisions based on expert policies in the past.
\end{itemize}

Each paradigm has its advantages and limitations. When dynamics modeling is accurate and tractable, predictive control delivers precision and robustness (e.g., Boston Dynamics parkour and SpaceX rocket landing). However, in many cases (e.g., manipulation with contact~\cite{le2024contact} and deformable objects~\cite{zhu2022challenges}, vision-based control~\cite{qi2024control}), the dynamics can be inaccurate or computationally expensive for optimization (as in OC) and interaction (as in RL). Generative control, on the other hand, circumvents the need for careful dynamics modeling and directly learns the policy via generative modeling of the expert demonstrations. However, generative control suffers from distribution shifts (due to its supervised learning nature~\cite{li2024evaluating,firoozi2023foundation}) and lack of precision and robustness (i.e., they can do lots of tasks but do few of them as well as specialized controllers~\cite{du2024learning,black2023zero}). Moreover, whether the ``scaling law'' can address these issues is questionable~\cite{ankile2024imitation,zhao2024aloha,yu2024lucidsim}.

Humans, nevertheless, look back \emph{and} look ahead when making decisions. We look back at similar past experiences to shortlist a set of proper actions, and look ahead---unroll the environment dynamics in ``imagination''---to predict which action plan is optimal in terms of certain metrics. 
% Looking back reduces the search space and looking ahead enables robustness and adaptation.

\emph{Can we establish a similar robot control paradigm that both looks back (i.e., generative) and looks ahead (i.e., predictive)?}

\textbf{Contribution}. We propose \emph{generative predictive control} (\nameshort), a framework that combines a generative robot policy with a predictive dynamical model of the world (i.e., a world model~\cite{ha2018world}). \nameshort contains three modules (\cf Fig.~\ref{fig:overview}):
\begin{itemize}
    \item \textbf{Generative policy training}. Given expert demonstrations, \nameshort uses generative modeling to train a policy that approximates the conditional distribution of expert ``\emph{action chunks}'' given past observations. 
    % At test time, one can sample many \emph{action proposals} from the policy, each being a \emph{chunk} of robot actions. 
    % This is not new and has become common practice in robot policy learning~\cite{chi2023diffusion,lee2024behavior}.
    \item \textbf{Predictive world modeling}. A world model simulates future observations conditioned on action chunks and past observations. One would naturally want to learn the world model using the same expert demonstrations as in policy training. However, {such a world model can only simulate dynamics of the expert policy and thus is not effective for correcting policy error} 
    (see \S\ref{sec:experiments}). 
    % \hy{are there prior work having similar observations?}
    % \yd{Not sure if there is prior work, I rewrote the sentence a little bit} 
    We show that adding ``\emph{random exploration}'' data---cheaper to collect---enriches the data distribution and yield a world model that strengthens the policy at test time. For state-based world modeling, a simple MLP is sufficient to learn accurate dynamics. For vision-based world modeling, we capitalize on \emph{conditional video diffusion}~\cite{ho2022video,alonso2024diffusion} to predict action-conditioned future observations.
    \item \textbf{Online planning}. Inspired by classical model predictive control, and assuming a reward model is available, \nameshort uses two lightweight online algorithms to strengthen the generative policy with the predictive world model. The first one, \gpcrank, samples many action proposals from the policy, unrolls the future conditioned on each proposal, and selects the proposal with highest reward. The second one, \gpcopt, treats the action proposal from the policy as a ``warmstart'' and optimizes it to maximize reward using gradient ascent, a technique known as \emph{single shooting}~\cite{diehl2011numerical}. These two algorithms can be easily combined to perform gradient-based optimization on multiple action proposals (warmstarts) and select the best one.
    % Assuming a reward model is available (e.g., learned using the same data as before), we propose two algorithms to online combine the policy and the world model.
\end{itemize}

We evaluate \nameshort in both simulated and real-world, state-based and vision-based robotic manipulation tasks, demonstrating significant improvement over pure behavior cloning. 

% \yd{I think the last statement is mostly correct but need to be carefully worded. There has been some work on using visual world models for robotic manipulation (i.e. Dreamitate, Unisim, or Video Language Planning), though they aren't explicitly action conditioned.}
\textbf{Novelty}. Among the three modules of \nameshort, the first one is not new and has become the common practice in robot policy learning~\cite{chi2023diffusion,urain2024deep}. The novelty of \nameshort lies in learning a predictive world model---crucially, using random exploration data---to strengthen the generative policy with \emph{online planning} (as opposed to, e.g., offline RL~\cite{hafner2020mastering,janner2022planning,wang2022diffusion,ding2024diffusion,nakamoto2024steering}). Moreover, different from previous works leveraging generative models for world modeling, \nameshort learns a world model that is explicitly \emph{action-conditioned}. In other words, the world model is ``controllable'' by the robot's actions. We show such a controllable world model enables robust real-world robotic manipulation.   

\textbf{Paper organization}. We formalize the overall pipeline of \nameshort in \S\ref{sec:gpc} and detail the approaches for state-based and vision-based world modeling in \S\ref{sec:world-model}. We present experimental results in \S\ref{sec:experiments} and conclude in \S\ref{sec:conclusion}. Review of related works and how \nameshort differs from them are described in \S\ref{sec:related-work}.









