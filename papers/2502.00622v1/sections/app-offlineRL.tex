%!TEX root = ../main.tex

\subsection{Offline RL}
\label{app:app_offlineRL}
While we rely on the world model to rank the action proposals, offline reinforcement learning ranks the action proposals using the value function~\cite{nakamoto2024steering}. It firstly learns the value function via offline RL pre-training, and then performs test-time action re-ranking via the learned value function. To train a value function $Q(I_t,a_{t:t+T})$ where $I_t$ is the state and $a_{t:t+T}$ is the action chunk, we need to obtain a reward function that can be used to supervise the value function training. Consistent with the \gpcrank method, we use registration error between the object and the target at $I_{t+T+1}$ as the reward. We use the same length of action chunk ($T$) as \gpcrank for a fair comparison. 

Implicit Q-learning (IQL)~\cite{kostrikov2021offline} is used as the offline RL method to pretrain a value function $Q(I_t,a_{t:t+T})$. For vision-based tasks, we use the pretrained vision encoder (ResNet18) from the action policy to encode the observation images $I_t$ to latent vectors $s_t$. We then train the value function $Q(s_t,a_{t:t+T})$. With the trained value function, we can rank and select the action proposal with the highest Q-value.
\begin{equation}
a_{t:t+T}^{(k\star)} = \argmax_{a_{t:t+T}^{(k)}, k=1 \ldots K} Q(s_t,a_{t:t+T}^{(k)}),
\end{equation}
which is the same as the \gpcrank method in~\eqref{eq:rank}.


% \vspace{-8mm}