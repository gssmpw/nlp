%!TEX root = ../main.tex


\input{sections/fig-planar-pushing}

\input{sections/fig-diffusion-world-model}

\section{World Model Learning}
\label{sec:world-model}

Recall that the world model takes as input $(I_t, a_{t:t+T})$ and outputs $I_{t+1:t+T+1}$. We will briefly present the world model design when $I_t = x_t$ contains low-dimensional states (\S\ref{sec:state-based}) and focus on a diffusion-based world model when $I_t = o_{t-H:t}$ contains high-dimensional images (\S\ref{sec:vision-based}). 

\textbf{Planar pushing}. Before getting started, since we will focus on planar pushing in our experiments \S\ref{sec:experiments}, we briefly introduce the problem setup. As shown in Fig.~\ref{fig:planar-pushing}, the planar pushing task involves pushing an object (e.g., a T block) to align with the target using a pusher. We assume the target position is fixed and known, and hence we do not need to predict it. The state of the object is fully described by its 2D coordinates $p=(p_x, p_y)$ and 1D angle $\theta$. The action space is $a \in \mathbb{R}^2$ containing pushing velocities in the $x$ and $y$ directions. We choose the planar pushing task for two reasons. First, it is easy to understand and simple to visualize. Second, it is also a long-standing difficult task because it is \emph{contact-rich}. Depending on whether the pusher makes or breaks contact and whether the contact is sticking or sliding, the dynamics of the task involves multiple different modes and requires intricate physics reasoning~\cite{hogan2020feedback}. This makes planar pushing a both elementary and challenging task for world model learning.



\subsection{State-based World Modeling}
\label{sec:state-based}

For state-based world modeling, with $x = (p,\theta)$ being the state of the object, we learn a single-step dynamics model:
\bea \label{eq:state-based-model}
x_{t+1} = f_\state(x_t, a_t).
\eea
As pointed out by~\cite{zhou2019continuity}, when rotations are involved in machine learning, it is crucial to choose a rotation representation that is continuous. For example, the angle representation $\theta$ is discontinuous due to the ``wrap $2\pi$'' issue. Therefore, we augment the state as
\bea 
x = (p, \cos(\theta), \sin(\theta))
\eea
for both the input and output of $f_\state$ in~\eqref{eq:state-based-model} (we normalize the output of $f_\state$ to satisfy $\sin^2(\cdot) + \cos^2(\cdot)=1$). We also found it beneficial to have two separate networks, one predicting the position and the other predicting the rotation. In our implementation, for each of the network, we used a six-layer MLP with hidden dimension $128$. 

During testing, we need to predict $T$ steps into the future given a sequence of actions $a_{t:t+T}$. We do so by recursively applying the single-step predictor~\eqref{eq:state-based-model}. As we will show in \S\ref{sec:experiments}, this simple MLP-based single-step predictor is sufficient for robust online planning. 
% \hy{@Han, please describe the MLP-based single-step prediction model.}

% For state-based world modeling, we use a MLP to predict state after a single-step action, and then recursively we can predict the final state after complete action chunk based on previous predicted states. We note $I_t = x_t = (\boldsymbol{x_t}, \boldsymbol{\theta_t})$, where $\boldsymbol{x_t}$ represents the state of position and $\boldsymbol{\theta_t}$ represents the state of angle. To conquer the problem of different units for position and angle, we use $(cos(\boldsymbol{\theta_t}), sin(\boldsymbol{\theta_t}))$ to replace $\boldsymbol{\theta_t}$. Let $x'_t = (\boldsymbol{x_t}, cos(\boldsymbol{\theta_t}), sin(\boldsymbol{\theta_t}))$. Also we use two MLP networks to predict the future position and angle respectively. H is the observation horizon. Therefore, $\boldsymbol{x_{t+1}} = $MLP$_x(\{x'_{t-H:t}\}, a_t)$. $(cos(\boldsymbol{\theta_{t+1}})), sin(\boldsymbol{\theta_{t+1}}))) = $MLP$_\theta(\{x'_{t-H:t}\}, a_t)$. 

% In training, we calculate the MSE loss between predicted $(\boldsymbol{x_{t+1}}, \boldsymbol{\theta_{t+1}})$ and ground-truth $(\boldsymbol{x_{t+1}}, \boldsymbol{\theta_{t+1}})$. In testing, we need to use this single-step prediction model to predict states after multistep actions. The outputs $\boldsymbol{x_{t+1}}$ and $(cos(\boldsymbol{\theta_{t+1}})), sin(\boldsymbol{\theta_{t+1}})))$ will form $x'_{t+1}$. Then we can predict $x'_{t+2}$ based on $x'_{t+1}$ and $a_{t+1}$. Recursively, we would get $x_{t+1:t+T+1}$ given input $(x_t, a_{t:t+T})$.

% In our implementation, we use observation horizon $H=2$ for state-base world modeling and two six-layer MLPs with hidden dimension 128. 


\subsection{Vision-based World Modeling}
\label{sec:vision-based}

State-based world modeling requires accurate state estimation, which is possible in a lab environment with infrastructure such as AprilTags~\cite{olson2011apriltag} or motion capture but can be challenging in the open environment. Therefore, it is desired to directly learn \emph{visual dynamics}, i.e., a visual world model.

\textbf{Diffusion-based visual world modeling}. Motivated by the success of using diffusion models for image generation, we design a visual world model based on conditional diffusion. Recall that the input to the world model is $I_t = o_{t-H:t}$ (the sequence of past images) and $a_{t:t+T}$, and the output is $I_{t+1:t+T+1}:= o_{t+1:t+T+1}$ (a sequence of future images). Similar as the state-based world modeling, we design the visual world model as a recursive application of single-step image predictors. We denote the single-step predictor as
\bea \label{eq:single-step-predictor}
o_{t+1} = f_\vision(o_{t-H:t}, a_t),
\eea 
where $f_\vision$ is a conditional diffusion model. The unique property of diffusion is that $o_{t+1}$ is not generated in a single feedforward step, but rather through a sequence of denoising steps. In particular, let $o_{t+1}^{\calT}$ be drawn from a white Gaussian noise distribution, then $f_\vision$ proceeds as:
\bea \label{eq:image-diffusion}
o_{t+1}^{\tau-1} = D_{\phi}(o_{t+1}^\tau, \tau, o_{t-H:t},a_t), \tau = \calT, \dots, 1,
\eea
where $\calT$ is the total number of denoising steps and the output of $f_\vision$ is $o_{t+1} = o_{t+1}^0$. In~\eqref{eq:image-diffusion}, $\tau$ is the denoising step index and $D_{\phi}$---the ``denoiser''---is a neural network with weights $\phi$. We use the same architecture for $D_\phi$ as in~\cite{alonso2024diffusionworldmodelingvisual}, which contains convolutions, action embedding, and a U-Net, as illustrated in Fig.~\ref{fig:visual-world-model}. The denoiser $D_\phi$ is trained by adding random noises to the clean images and then predicting the noise.

% \hy{@Han, please describe the diffusion-based single-step visual world model}

% Similarly as state-based world modeling, we use a single-step diffusion based model to predict $I_{t+1}$ given inputs $(I_{t-H:t}, a_t)$. We utilize score-based diffusion model to build the vision-based world model \cite{alonso2024diffusionworldmodelingvisual}. During training, we sample observation history $o_{t-H:t}$, action $a_t$, and ground-truth next step observation $o_{t+1}$ from the training dataset. We obtain the noised next observation $o_{t+1}^\tau \sim p^\tau(o_{t+1}^\tau \mid o_{t+1})$ by applying the $\tau$-level perturbation kernel. The objective in diffusion model becomes 
% \begin{equation}
% \label{eq:diffusion_1}
% \mathcal{L}(\theta) = \mathbb{E} [ \| \mathbf{D}_{\theta} (o_{t+1}^{\tau}, \tau, o_{t-H:t}, a_t) - o_{t+1} \|^2 ]. 
% \end{equation}

% For practical implementation, we follow the network structure in \cite{alonso2024diffusionworldmodelingvisual}, letting 
% \begin{equation}
% \begin{split}
% \label{eq:diffusion_2}
% \mathbf{D}_{\theta} (o_{t+1}^{\tau}, \tau, o_{t-H:t}, a_t) &= c_{\text{skip}}^\tau o_{t+1}^{\tau} \\ &\quad
% + c_{\text{out}}^\tau  \mathbf{F}_{\theta} (c_{\text{in}}^{\tau} \ o_{t+1}^{\tau}, c_{\text{noise}}^{\tau}, o_{t-H:t}, a_t)
% \end{split}
% \end{equation}, where $c_{\text{in}}^{\tau}$, $c_{\text{out}}^{\tau}$, $c_{\text{skip}}^{\tau}$, $c_{\text{noise}}^{\tau}$ are pre-selected for the noise level $\tau$. Combining Equations \ref{eq:diffusion_1} and \ref{eq:diffusion_2}, the training objective of $\mathbf{F}_{\theta}$ is 
% \begin{equation}
% \begin{split}
% \label{eq:diffusion_3}
% \mathcal{L}(\theta) &= \mathbb{E} [ \| \mathbf{F}_{\theta} (c_{\text{in}}^{\tau} o_{t+1}^{\tau}, c_{\text{noise}}^{\tau}, o_{t-H:t}, a_t)) \\ &\quad - \frac{1}{c_{\text{out}}^{\tau}}(o_{t+1} - c_{\text{skip}}^{\tau} o_{t+1}^{\tau})\| ^2].
% \end{split}
% \end{equation}

% $\mathbf{F}_{\theta}$ is implemented with a U-Net 2D network. We concatenate the past observations $o_{t-H:t}$ to the next noisy observation $o_{t+1}^{\tau}$ channel-wise, and input action $a_t$ through the adaptive group normalization layers \cite{zheng2020learning} in the residual blocks of the U-Net.

\textbf{Two-phase training}.
To improve the accuracy and consistency of visual world modeling, we train it in two phases. In phase one, we train only the single-step image predictor~\eqref{eq:single-step-predictor}, i.e., with supervision only on a single image $o_{t+1}$. In phase two, we recursively apply the single-step predictor~\eqref{eq:single-step-predictor} for $T$ times to obtain a sequence of future images $o_{t+1:t+T+1}$, and jointly supervise them with groundtruth images. 

We use observation horizon $H=4$ in the visual world modeling, and $\calT=3$ diffusion steps in $f_\vision$. 

% As shown by~\cite{alonso2024diffusionworldmodelingvisual}, such a small number of diffusion steps can already achieve strong 

\begin{remark}[Freeze the Noise]
    It should be emphasized that the single-step image predictor $f_\vision$ in~\eqref{eq:single-step-predictor} is ``stochastic'' where the stochasticity comes from the randomness in the initial noise $o_{t+1}^{\calT}$. In classical generative modeling, stochasticity is desired because we want the model to generate diverse images. However in our paper, we want to use the diffusion-based world model for control. That is, we want to understand the impact of the ``action'' on the future, rather than the impact of the random ``noise'' on the future. Therefore, we freeze the noise $o_{t+1}^{\calT}$ to be zero at inference time, which makes the world model deterministic---outputing the most likely future predictions (because $o_{t+1}^{\calT}=0$ attains the peak probability in the Gaussian distribution). In fact, without freezing the noise, \gpcopt does not work because the gradient of the reward with respect to the actions (\cf~\eqref{eq:ascent}) would be ``contaminated'' by the noise and thus does not guide online planning.
\end{remark}

% initially given $o_{t-H:t}$, we \textbf{recursively} apply the diffusion model to predict sequences of observations after $a_{t:t+T}$, based on last predictions, and optimize $\mathbf{F}_{\theta}$ with the objective in Equation \ref{eq:diffusion_3} from time $t+1$ to time $t+T+1$. We used observation horizon $H=4$ for vision-based world modeling.

% During testing, given past observations $o_{t-H:t}$ and action $a_t$ at time t, we let $o_{t+1}^{\tau}$ be an all zero image in shape of $(3, W, H)$. To note that this choice is different from original diffusion model, where $o_{t+1}^{\tau}$ is set to be random Gaussian noise. This is because our world model has to be deterministic and the starting noisy image needs to be fixed. All zero image corresponds to the maximum likelihood point of the random Gaussian noise. This is especially a key choice to make \gpcopt work.  