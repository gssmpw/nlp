
\begin{minipage}{\textwidth}
\includegraphics[width=\linewidth]{figures/demo_figure.pdf}
\captionof{figure}{\textsc{Generative Predictive Control} (\nameshort). Given visual (or state) observations, a generative policy---trained from behavior cloning---proposes an action chunk. Conditioned on the action chunk and past observations, a predictive world model---trained from diverse trajectories---predicts the future observations in ``imagination''. In (a) \gpcrank, the policy outputs multiple action proposals and for each proposal the world model predicts its future and associated reward. The proposal with highest reward in executed. In (b) \gpcopt, the policy outputs a single action proposal and multiple gradient ascent steps are performed to optimize the action proposal for higher reward. Both \gpcrank and \gpcopt dominate pure behavior cloning.
\label{fig:overview}}
\vspace{1mm}
\end{minipage}

