%!TEX root = ../main.tex

\begin{abstract}
    We present generative predictive control (\nameshort), a learning control framework that (\emph{i}) clones a \emph{generative} diffusion-based policy from expert demonstrations, (\emph{ii}) trains a \emph{predictive} action-conditioned world model from both expert demonstrations and random explorations, and (\emph{iii}) synthesizes an online planner that ranks and optimizes the action proposals from (\emph{i}) by looking ahead into the future using the world model from (\emph{ii}). Crucially, we show that \emph{conditional video diffusion} allows learning (near) physics-accurate \emph{visual} world models and enable robust visual foresight. Focusing on planar pushing with rich contact and collision, we show \nameshort dominates behavior cloning across state-based and vision-based, simulated and real-world experiments.
\end{abstract}
% \vspace{-8mm}