\section{Related Work}
\label{rw}
 

\subsection{Machine Unlearning}
 
Machine unlearning, a method designed to eliminate the contribution of certain data from a trained model, facilitates individuals' right to be forgotten. This concept is explored in works such as \cite{graves2021amnesiac,garg2020formalizing}. A straightforward but often impractical approach to machine unlearning involves retraining the machine learning model from scratch. This method can be prohibitively expensive in terms of computational overhead and storage requirements, particularly for complex deep learning tasks. Current strategies for machine unlearning are generally divided into two main types: ``fast retraining'' and ``approximate unlearning''.


 
Fast retraining methods in machine unlearning primarily focus on reducing the computational overhead associated with retraining models. These methods involve a partial redesign of learning algorithms and necessitate storing training data or intermediate parameters during the training process, thereby incurring increased storage costs \cite{cao2015towards,bourtoule2021machine,yanarcane2022unlearning,wu2022puma}. In \cite{cao2015towards}, \citeauthor{cao2015towards} restructured traditional machine learning algorithms into a summation-based framework. This innovation allows for quicker updates to the model when unlearning is required. Instead of retraining the entire model from scratch, only a few summations need to be modified, significantly accelerating the process. In \cite{bourtoule2021machine,yanarcane2022unlearning}, \citeauthor{bourtoule2021machine} and \citeauthor{yanarcane2022unlearning} proposed advanced methods for unlearning samples in deep neural networks. Their approach involves segmenting the complete dataset into smaller portions, referred to as shards, with individual sub-models trained on each shard. When data needs to be unlearned, it is removed from the relevant shard, and only the sub-model for that specific shard is retrained. This method, however, incurs significant storage costs, as it requires maintaining the intermediate training parameters for each shard and storing the entire training dataset. Additionally, its efficiency diminishes with an increase in the frequency of data removal requests.

 

 
 
Approximate unlearning methods aim to directly modify the original trained model to approximate ground truth (model retrained using the remaining dataset).
In \cite{guo2019certified,sekhari2021remember}, \citeauthor{guo2019certified} introduced a certified-removal method that draws parallels to the principles of differential privacy, as established by \citeauthor{dwork2006differential}\cite{dwork2006differential}. The mechanism ensures that a model, after data erasure, remains indistinguishable from a model that never incorporated the erased data. To address the potential decline in model utility due to unlearning, both \cite{guo2019certified} and \cite{sekhari2021remember} employed a strategy that limits the updates to model parameters. This approach aligns with the concept of differential unlearning, maintaining model effectiveness while adhering to unlearning requirements. In \cite{nguyen2020variational}, \citeauthor{nguyen2020variational} approached unlearning by approximating the posterior based on the data set for removal, utilizing Bayesian inference techniques as outlined in \cite{box2011bayesian}. While these methods employ bounds or thresholds to avert significant loss of unlearning, they can still lead to a reduction in model utility after unlearning. To address this, we introduce a remembering constraint and an unlearning rate in CRFU to diminish the accuracy decline caused by unlearning. 


%In \cite{nguyen2020variational}, \citeauthor{nguyen2020variational} unlearned an approximate posterior based on the erased data via the Bayesian inference \cite{box2011bayesian}. Though these methods typically use a bound or threshold to prevent catastrophic unlearning, they still easily decrease the model accuracy to some extent after unlearning. By contrast, we introduce a remembering constraint and an unlearning rate in RFU to mitigate the model utility decline caused by unlearning in the paper. 





 

%Similarly, \citeauthor{fu2022knowledge} \cite{fu2022knowledge}  proposed a machine unlearning algorithm for Markov Chain Monte Carlo (MCMC) \cite{gilks1995markov} and further defined a knowledge removal estimator to evaluate the knowledge erasure. 

\subsection{Privacy Attacks on Unlearning}
As machine unlearning becomes hot, it also brings new challenges of privacy threats. \citeauthor{chen2021machine}~\cite{chen2021machine} highlighted that the variance in a model's outputs before and after unlearning could inadvertently expose the privacy of the removed data. They further explored this issue by proposing a membership inference attack specifically targeting the unlearning process. \citeauthor{lu2022label} \cite{lu2022label} further proposed label‚Äêonly membership inference attacks targetting black-box machine unlearning. In addition to membership inference attacks on unlearning, \citeauthor{gao2022deletion}~\cite{gao2022deletion} introduced the concept of deleted reconstruction attacks. This form of model inversion attack~\cite{fredrikson2014privacy}, aims to reconstruct removed data by analyzing the outputs of both the original and the unlearned models. Similarly, \citeauthor{salem2020updates}~\cite{salem2020updates} proposed a reconstruction attack aimed at recovering specific data samples involved in the model updating process. This is achieved by comparing the model's outputs before and after the update.

 

However, there are few works have effectively addressed the threat caused by machine unlearning. In \cite{chen2021machine}, \citeauthor{chen2021machine} suggested various strategies to mitigate privacy leaks in machine unlearning, one of which includes the application of differential privacy. However, they noted that while these methods can enhance privacy protection, they often exacerbate the issue of utility degradation in the model. In the paper, we introduce CRFU, a novel approach that is tailored to unlearning IB-trained models by discarding the information of the specified data from the trained compressed representations. Since the data has been compressed during learning and unlearning, it can effectively thwart adversaries' attempts to infer private properties from the model outputs.