\section{Related Work}
In this section, we briefly review prior related works.

Algorithms and analysis for classic RL were well studied in the literature____. 
\revision{____ proposed the RL with segment feedback problem (they called it RL from bagged reward), and designed a transformer-based algorithm. However, their work is mostly empirical and does not provide theoretical guarantees. Nor do they rigorously quantify the influence of segments on learning.}

There are two prior works____ studying RL with trajectory feedback, which are most related to our work. ____ investigated RL with sum trajectory feedback, and designed upper confidence bound (UCB)-type and Thompson sampling (TS)-type algorithms with regret guarantees. \revision{
____ studied RL with binary trajectory feedback, but considered a different formulation for binary feedback from ours. 
Specifically, in their formulation, 
%the feature of a trajectory can be non-decomposable to state-action features, and 
the objective is to find the policy that maximizes the expected probability of generating feedback $1$, and their optimal policy can be non-Markovian due to the non-linearity of the sigmoid function; In our formulation, 
%the feature of a trajectory is the sum of the state-action features over this trajectory, and 
our objective is to find the optimal policy under the standard MDP definition by inferring rewards from binary feedback, and thus we consider Markovian policies. 
The algorithms in ____ are either computationally inefficient or have a suboptimal regret order due to the  non-linearity of their objective and direct maximization over all non-Markovian policies. 
Our algorithms are computationally efficient by adopting the TS algorithmic style and efficient MDP planning under Markovian policies.
Our regret results cannot be directly compared to those in ____ due to the difference in formulation.

Moreover, different from ____, we study RL with segment feedback, which allows feedback from multiple segments within a trajectory, with per-state-action feedback and trajectory feedback as the two extremes. Under sum feedback, we improve the result in ____ by a factor of $\sqrt{H}$ using experimental design, when the problem reduces to the trajectory feedback setting. Under binary feedback, we propose TS-style algorithms which are computationally efficient, and build a lower bound to reveal an inevitable exponential factor in the regret bound, which is novel to the RL literature.}

Our work is also related to linear bandits____ and logistic bandits____, and uses analytical techniques from that literature.