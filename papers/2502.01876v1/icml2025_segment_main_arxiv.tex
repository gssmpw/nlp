%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[a4paper]{article}
\usepackage[left=0.8in,right=0.8in]{geometry}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% My package
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{amsthm}
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{corollary}{Corollary}
%\newtheorem{proposition}{Proposition}
%\newtheorem{example}{Example}
%\newtheorem{conjecture}{Conjecture}
%\newtheorem{definition}{Definition}
%\newtheorem{assumption}{Assumption}

\usepackage{wrapfig}
\usepackage{bbm}
\usepackage{ifthen}
\usepackage{thm-restate}
\usepackage{tikz}
\usetikzlibrary{positioning,chains,fit,shapes,calc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumerate}
\usepackage{flushend}
\usepackage{mathrsfs}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{setspace}
\usepackage{xcolor}
%\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\hypersetup{
	hidelinks
	%	colorlinks   = true,
	%	urlcolor     = black,
	%	linkcolor    = black,
	%	citecolor    = black
}

\newcommand{\A}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\bOne}{\mathbbm{1}}
\newcommand{\bA}{\boldsymbol{A}}
\newcommand{\ba}{\boldsymbol{a}}
\newcommand{\bB}{\boldsymbol{B}}
\newcommand{\bI}{\boldsymbol{I}}
\newcommand{\bL}{\boldsymbol{L}}
\newcommand{\bO}{\boldsymbol{O}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bR}{\boldsymbol{R}}
\newcommand{\bS}{\boldsymbol{S}}
\renewcommand{\bT}{\boldsymbol{T}}
\newcommand{\bU}{\boldsymbol{U}}
\newcommand{\bu}{\boldsymbol{u}}
\newcommand{\bV}{\boldsymbol{V}}
\newcommand{\bv}{\boldsymbol{v}}
\newcommand{\bw}{\boldsymbol{w}}
\newcommand{\bW}{\boldsymbol{W}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bY}{\boldsymbol{Y}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\bZ}{\boldsymbol{Z}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\ttheta}{\tilde{\theta}}
\newcommand{\tR}{\tilde{R}}
\newcommand{\tP}{\tilde{P}}
\newcommand{\tQ}{\tilde{Q}}
\newcommand{\tY}{\tilde{Y}}
\newcommand{\tZ}{\tilde{Z}}
\newcommand{\hP}{\hat{P}}
\newcommand{\hZ}{\hat{Z}}
\newcommand{\vg}{\vec{g}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vzero}{\vec{0}}
\newcommand{\vtheta}{\vec{\theta}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\mathchardef\mhyphen="2D
\newcommand{\ex}{\mathbb{E}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\kl}{\textup{KL}}
\newcommand{\var}{\textup{Var}}
\newcommand{\cov}{\textup{Cov}}
\newcommand{\unitvec}{\textbf{1}}
\newcommand{\sbr}[1]{\left( #1 \right)}
\newcommand{\mbr}[1]{\left[ #1 \right]}
\newcommand{\lbr}[1]{\left\{ #1 \right\}}
\newcommand{\abr}[1]{\left| #1 \right|}
\newcommand{\nbr}[1]{\left\| #1 \right\|}
\newcommand{\algcomment}[1]{{\color{blue} \hfill // #1}}
\newcommand{\algcommentfront}[1]{{\color{blue} // #1}}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\trace}{\textup{tr}}
\newcommand{\unif}{\textup{unif}}
\newcommand{\poly}{\textup{poly}}

\newcommand{\regret}{\textup{Regret}}
\newcommand{\round}{\mathtt{ROUND}}
\newcommand{\bernoulli}{\mathcal{B}}
\newcommand{\seg}{\textup{seg}}
\newcommand{\bi}{\textup{B}}
\newcommand{\summing}{\textup{S}}
\newcommand{\sub}{\textup{sub}}

\newcommand{\bitsseg}{\mathtt{SegBiTS}}
\newcommand{\bitssegtran}{\mathtt{SegBiTS\mbox{-}Tran}}
\newcommand{\edlinucbseg}{\mathtt{E\mbox{-}LinUCB}}
\newcommand{\linucbtranseg}{\mathtt{LinUCB\mbox{-}Tran}}


% Macro for comments:
\newcommand{\compilehidecomments}{false}%HIDE comments
\ifthenelse{ \equal{\compilehidecomments}{true} }{
	\newcommand{\yihan}[1]{}
	\newcommand{\gal}[1]{}
	\newcommand{\srikant}[1]{}
	\newcommand{\shie}[1]{}
	\newcommand{\anna}[1]{}
}{
	\newcommand{\yihan}[1]{{\color{teal} [\text{Yihan:} #1]}}
	\newcommand{\gal}[1]{{\color{cyan} [\text{Gal:} #1]}}
	\newcommand{\srikant}[1]{{\color{red} [\text{Srikant:} #1]}}
	\newcommand{\shie}[1]{{\color{brown} [\text{Shie:} #1]}}
	\newcommand{\anna}[1]{{\color{orange} [\text{Anna:} #1]}}
}
% \newcommand{\revision}[1]{{\color{blue} #1}}
\newcommand{\revision}[1]{#1}


\allowdisplaybreaks
\abovedisplayskip=5pt
\belowdisplayskip=5pt
\setlength\textfloatsep{8pt}



%% The \icmltitle you define below is probably too long as a header.
%% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Reinforcement Learning with Segment Feedback}

\begin{document}

\title{Reinforcement Learning with Segment Feedback}

\author{Yihan Du\\UIUC\\yihandu@illinois.edu
	\and Anna Winnicki\\Stanford\\winnicka@stanford.edu
	\and Gal Dalal\\NVIDIA Research\\gdalal@nvidia.com
	\and Shie Mannor\\Technion\\NVIDIA Research\\smannor@nvidia.com
	% shie@ee.technion.ac.il
	\and R. Srikant\\UIUC\\rsrikant@illinois.edu}

\date{}

\maketitle
	
%\twocolumn[
%\icmltitle{Reinforcement Learning with Segment Feedback}
%
%% It is OKAY to include author information, even for blind
%% submissions: the style file will automatically remove it for you
%% unless you've provided the [accepted] option to the icml2025
%% package.
%
%% List of affiliations: The first argument should be a (short)
%% identifier you will use later to specify author affiliations
%% Academic affiliations should list Department, University, City, Region, Country
%% Industry affiliations should list Company, City, Region, Country
%
%% You can specify symbols, otherwise they are numbered in order.
%% Ideally, you should not use this facility. Affiliations will be numbered
%% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}
%
%\begin{icmlauthorlist}
%\icmlauthor{Firstname1 Lastname1}{equal,yyy}
%\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
%\icmlauthor{Firstname3 Lastname3}{comp}
%\icmlauthor{Firstname4 Lastname4}{sch}
%\icmlauthor{Firstname5 Lastname5}{yyy}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%%\icmlauthor{}{sch}
%%\icmlauthor{}{sch}
%\end{icmlauthorlist}
%
%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}
%
%\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}
%
%% You may provide any keywords that you
%% find helpful for describing your paper; these are used to populate
%% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Reinforcement learning (RL), segment feedback, binary feedback, sum feedback}
%
%\vskip 0.3in
%]
%
%% this must go after the closing bracket ] following \twocolumn[ ...
%
%% This command actually creates the footnote in the first column
%% listing the affiliations and the copyright notice.
%% The command takes one argument, which is text to display at the start of the footnote.
%% The \icmlEqualContribution command is standard text for equal contribution.
%% Remove it (just {}) if you do not need this facility.
%
%%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Standard reinforcement learning (RL) assumes that an agent can observe a reward for each state-action pair. However, in practical applications, it is often difficult and costly to collect a reward for each state-action pair. While there have been several works considering RL with trajectory feedback, it is unclear if trajectory feedback is inefficient for learning when trajectories are long. In this work, we consider a model named RL with segment feedback, which offers a general paradigm filling the gap between per-state-action feedback and trajectory feedback.
In this model, we consider an episodic Markov decision process (MDP), where each episode is divided into $m$ segments, and the agent observes reward feedback only at the end of each segment. Under this model, we study two popular feedback settings: binary feedback and sum feedback, where the agent observes a binary outcome and a reward sum according to the underlying reward function, respectively. To investigate the impact of the number of segments $m$ on learning performance, we design efficient algorithms and establish regret upper and lower bounds for both feedback settings. Our theoretical and experimental results show that: under binary feedback, increasing the number of segments $m$ decreases the regret at an exponential rate; in contrast, surprisingly, under sum feedback, increasing $m$ does not reduce the regret significantly.
\end{abstract}


\section{Introduction}

Reinforcement learning (RL) is a class of sequential decision-making algorithms, where an agent interacts with an unknown environment through time with the goal of maximizing the obtained reward.
RL has variant applications such as robotics, autonomous driving and game playing.

In classic RL, when the agent takes an action in a state, the environment will provide a reward for this state-action pair. 
However, in real-world applications, it is often difficult and costly to collect a reward for each state-action pair. For example, in robotics, when we instruct a robot to scramble eggs, it is hard to specify a reward for each individual action. In autonomous driving, it is difficult and onerous to evaluate each action, considering multiple criteria including safety, comfort and speed.

Motivated by this fact, there have been several works that consider RL with trajectory feedback~\cite{efroni2021reinforcement,chatterji2021theory}. In these works, the agent observes a reward signal only at the end of each episode, instead of at each step, with the signal indicating the quality of the trajectory generated during the episode. While these works mitigate the issue of impractical per-step reward feedback in classic RL, the relationship between the frequency of feedback and the performance of RL algorithms is unknown. In particular, if for example we get feedback twice in each trajectory, does that significantly improve performance over once per trajectory feedback?

To answer this question, we study a general model called RL with segment feedback, which bridges the gap between per-state-action feedback in classic RL~\cite{sutton2018reinforcement} and trajectory feedback in recent works~\cite{efroni2021reinforcement,chatterji2021theory}.
In this model, we consider an episodic Markov decision process (MDP), where an episode is equally divided into $m$ segments.
%, and each segment is of length $\frac{H}{m}$. 
%Here $H$ is the length of each episode. 
In each episode, at each step, the agent first observes the current state, and takes an action, and then transitions to a next state according to the transition distribution. The agent \emph{observes a reward signal at the end of each segment}. 
Under this model, we consider two reward feedback settings: binary feedback and sum feedback. In the binary feedback setting, the agent observes a binary outcome (e.g., thumbs up/down) generated by a sigmoid function of the reward on this segment. In the sum feedback setting, the agent observes the sum of the rewards over this segment. 
In our model, the agent needs to learn the underlying reward function (i.e., the expected reward as a function of states and actions) from binary or sum segment feedback, and maximize the expected reward achieved.
\revision{While \cite{tang2024reinforcement} also studied this segment model before (they called it RL from bagged reward), their work is mostly empirical, and does not provide theoretical guarantees for algorithms and rigorously reveal the influence of segments on learning.}


This model is applicable to many scenarios involving human queries. For instance, in autonomous driving, a driving trajectory is often divided into several segments, and human annotators are asked to provide feedback for each segment, e.g., thumbs up/down. Compared to state-action pairs or whole trajectories, segments are easier and more efficient to evaluate, since human annotators can focus on and rate behaviors in each segment, e.g., passing through intersections, reversing the car and parking.

In this segment model, there is an interesting balance between the number of segments (queries to humans) and the collected observations, i.e., we desire more observations, but we also want to reduce the number of queries. Therefore, in this problem, it is critical to investigate the trade-off between the benefits brought by segments and the increase of queries, which essentially comes down to a question: \emph{How does the number of segments $m$ impact learning performance?}

To answer this question, we design efficient algorithms for binary and sum feedback settings in both known and unknown transition cases. Regret upper and lower bounds are provided to rigorously show the influence of the number of segments on learning performance. We also present experiments to validate our theoretical results. 

\revision{
	Note that studying RL with equal segments is an important starting point and serves as a foundation for further investigation on more general models and analysis for RL with unequal segments. Even under equal segments, this problem is already very challenging: (i) This problem cannot be solved by applying prior trajectory feedback works, e.g., \cite{efroni2021reinforcement}, since they use the martingale property of subsequent trajectories in analysis, while subsequent segments are not a martingale due to  \emph{dependency among segments} within a trajectory. (ii) In prior trajectory feedback works~\cite{efroni2021reinforcement,chatterji2021theory}, there exists a gap between upper and lower bounds for sum feedback, and there is no lower bound for binary feedback. This fact poses a significant challenge for us when trying to understand the influence of the number of segments $m$ on learning performance.
}

Our work overcomes the above challenges and makes contributions as follows.

\begin{enumerate}
	\item We study a general model called RL with segment feedback, which bridges the gap between per-state-action feedback in classic RL and trajectory feedback seemlessly. Under this model, we consider two feedback settings: binary feedback and sum feedback.
	\item For binary feedback, we design computationally-efficient and sample-efficient algorithms $\bitsseg$ and $\bitssegtran$ for known and unknown transitions, respectively. We provide regret upper and lower bounds which depend on $\exp(\frac{Hr_{\max}}{2m})$, where $H$ is the length of each episode, and $r_{\max}$ is a universal upper bound of rewards. Our results exhibit that under binary feedback, increasing the number of segments $m$ significantly helps accelerate learning.
	\item For sum feedback, we devise algorithms $\edlinucbseg$ and $\linucbtranseg$, which achieve near-optimal regrets in terms of $H$ and $m$. We also establish lower bounds to validate the optimality, and show that optimal regrets do not depend on $m$. Our results reveal that surprisingly, under sum feedback, increasing the number of segments $m$ does not help expedite learning much. 
	\item We develop novel techniques which can be of independent interest, including the KL divergence analysis to derive an exponential lower bound under binary feedback, and the use of E-optimal experimental design in algorithm $\edlinucbseg$ to refine the eigenvalue of the covariance matrix and reduce the regret.
\end{enumerate}


\section{Related Work}

In this section, we briefly review prior related works.

Algorithms and analysis for classic RL were well studied in the literature~\cite{sutton2018reinforcement,jaksch2010near,azar2017minimax,jin2018q,zanette2019tighter}. 
\revision{\cite{tang2024reinforcement} proposed the RL with segment feedback problem (they called it RL from bagged reward), and designed a transformer-based algorithm. However, their work is mostly empirical and does not provide theoretical guarantees. Nor do they rigorously quantify the influence of segments on learning.}

There are two prior works~\cite{efroni2021reinforcement,chatterji2021theory} studying RL with trajectory feedback, which are most related to our work. \cite{efroni2021reinforcement} investigated RL with sum trajectory feedback, and designed upper confidence bound (UCB)-type and Thompson sampling (TS)-type algorithms with regret guarantees. \revision{
\cite{chatterji2021theory} studied RL with binary trajectory feedback, but considered a different formulation for binary feedback from ours. 
Specifically, in their formulation, 
%the feature of a trajectory can be non-decomposable to state-action features, and 
the objective is to find the policy that maximizes the expected probability of generating feedback $1$, and their optimal policy can be non-Markovian due to the non-linearity of the sigmoid function; In our formulation, 
%the feature of a trajectory is the sum of the state-action features over this trajectory, and 
our objective is to find the optimal policy under the standard MDP definition by inferring rewards from binary feedback, and thus we consider Markovian policies. 
The algorithms in \cite{chatterji2021theory} are either computationally inefficient or have a suboptimal regret order due to the  non-linearity of their objective and direct maximization over all non-Markovian policies. 
Our algorithms are computationally efficient by adopting the TS algorithmic style and efficient MDP planning under Markovian policies.
Our regret results cannot be directly compared to those in \cite{chatterji2021theory} due to the difference in formulation.

Moreover, different from \cite{efroni2021reinforcement,chatterji2021theory}, we study RL with segment feedback, which allows feedback from multiple segments within a trajectory, with per-state-action feedback and trajectory feedback as the two extremes. Under sum feedback, we improve the result in \cite{efroni2021reinforcement} by a factor of $\sqrt{H}$ using experimental design, when the problem reduces to the trajectory feedback setting. Under binary feedback, we propose TS-style algorithms which are computationally efficient, and build a lower bound to reveal an inevitable exponential factor in the regret bound, which is novel to the RL literature.}

Our work is also related to linear bandits~\cite{abbasi2011improved} and logistic bandits~\cite{filippi2010parametric,faury2020improved,russac2021self}, and uses analytical techniques from that literature.

\section{Formulation}
In this section, we present the formulation of RL with binary and sum segment feedback.

We consider an episodic MDP denoted by $\cM(\cS,\cA,H,r,p,\rho)$. Here $\cS$ is the state space, and $\cA$ is the action space. $H$ is the length of each episode. \revision{$r:\cS \times \cA \rightarrow [-r_{\max},r_{\max}]$ is an unknown reward function, where $r_{\max}>0$ is a universal constant.}
%and used to prevent the input of binary feedback (the sigmoid function) from being too large. 
Define the reward parameter $\theta^*:=[r(s,a)]_{(s,a) \in \cS \times \cA} \in \R^{|\cS||\cA|}$.
%	We will specify reward feedback settings shortly.
$p:\cS \times \cA \rightarrow \triangle_{\cS}$ is the transition distribution. For any $(s,a,s') \in \cS \times \cA \times \cS$, $p(s'|s,a)$ is the probability of transitioning to $s'$ if action $a$ is taken in state $s$. $\rho \in \triangle_{\cS}$ is an initial state distribution. 

A policy $\pi:\cS \times [H] \rightarrow \cA$ is defined as a mapping from the state space and step indices to the action space, so that $\pi_h(s)$ specifies what action to take in state $s$ at step $h$. 
For any policy $\pi$, $h \in [H]$ and $(s,a) \in \cS \times \cA$, let $V^{\pi}_h(s)$ be the state value function, and $Q^{\pi}_h(s,a)$ be the state-action value function, which denote the cumulative expected reward obtained under policy $\pi$ till the end of an episode, starting from $s$ and $(s,a)$ at step $h$, respectively. Formally, $V^{\pi}_h(s) := \ex[ \sum_{t=h}^{H} r(s_t,a_t) | s_h=s, \pi ]$, and $Q^{\pi}_h(s,a) := \ex[ \sum_{t=h}^{H} r(s_t,a_t)  | s_h=s, a_h=a, \pi ]$.
%\begin{align}
%	V^{\pi}_h(s) &:= \ex\mbr{\ \sum_{t=h}^{H} r(s_t,a_t) \ \Big|\ s_h=s, \pi } ,\quad \label{eq:def_Q_function}
%	\\
%	Q^{\pi}_h(s,a) &:= \ex\mbr{\ \sum_{t=h}^{H} r(s_t,a_t) \ \Big|\ s_h=s, a_h=a, \pi } . \nonumber
%\end{align}
The optimal policy is defined as $\pi^*=\argmax_{\pi}V^{\pi}_h(s)$ for all $s \in \cS$ and $h \in [H]$. For any $s \in \cS$ and $h \in [H]$, denote $V^*_h(s):=V^{\pi^*}_h(s)$.

%Below we introduce the settings of reward observation and the agent's interaction with the environment.


The process of RL with segment feedback is as follows. In each episode $k$, the agent chooses a policy $\pi^k$ at the beginning of this episode, and starts from $s^k_1 \sim \rho$. At each step $h \in [H]$, the agent first observes the current state $s^k_h$, and takes an action $a^k_h=\pi^k_h(s^k_h)$ according to her policy, and then transitions to a next state $s^k_{h+1} \sim p(\cdot|s^k_h,a^k_h)$. 

Each episode is equally divided into $m$ segments, and each segment is of length $\frac{H}{m}$. For convenience, assume that $H$ is divisible by $m$.
For any $k>0$ and $i \in [m]$, let $\tau^k=(s^k_1,a^k_1,\dots,s^k_h,a^k_h)$ denote the trajectory in episode $k$, and $\tau^k_i=(s^k_{\frac{H}{m}\cdot(i-1)+1},a^k_{\frac{H}{m}\cdot(i-1)+1},\dots,s^k_{\frac{H}{m}\cdot i},a^k_{\frac{H}{m}\cdot i})$ denote the $i$-th segment of the trajectory in episode $k$.

For any trajectory or trajectory segment $\tau$, $\phi^{\tau} \in \R^{|\cS||\cA|}$ denotes the vector where each entry $\phi^{\tau}(s,a)$ is the number of times $(s,a)$ is visited in $\tau$. 
For any policy $\pi$, $\phi^{\pi} \in \R^{|\cS||\cA|}$ denotes the vector where each entry $\phi^{\pi}(s,a)$ is the expected number of times $(s,a)$ is visited in an episode under policy $\pi$, i.e.,
$
	\phi^{\pi}(s,a):=\ex [ \sum_{h=1}^{H} \indicator\{s_h=s, a_h=a\} | \pi ]  
$.
% \label{eq:def_phi_pi}


In our model, the agent observes reward feedback \emph{only at the end of each segment}, instead of each step as in classic RL.
We consider two reward feedback settings as follows.

\paragraph{Binary Segment Feedback.}
Denote the sigmoid function by $\mu(x):=\frac{1}{1+\exp(-x)}$ for any $x\in\R$.
In the binary segment feedback setting, in each episode $k$, at the end of each segment $i \in [m]$, the agent observes a binary outcome 
\begin{align*}
	y^k_i = \left\{\begin{matrix}
		1, &\textup{ w.p. }  \mu((\phi^{\tau^k_i})^\top \theta^*) ,
		\\
		0, &\hspace*{1.73em} \textup{ w.p. }  1 - \mu((\phi^{\tau^k_i})^\top \theta^*) .
	\end{matrix}\right.
\end{align*}
%
%\begin{align*}
%	y^k_i = \left\{\begin{matrix}
%		1, &\textup{ w.p. } \mu \sbr{-\sum_{t=\frac{H}{m}\cdot(i-1)+1}^{\frac{H}{m}\cdot i} r(s^k_t,a^k_t)} = \mu((\phi^{\tau^k_i})^\top \theta^*) ,
%		\\
%		0, &\hspace*{3.4em} \textup{ w.p. } 1-\mu \sbr{-\sum_{t=\frac{H}{m}\cdot(i-1)+1}^{\frac{H}{m}\cdot i} r(s^k_t,a^k_t)} = 1 - \mu((\phi^{\tau^k_i})^\top \theta^*) .
%	\end{matrix}\right.
%\end{align*}

\revision{Note that our formulation is different from that in
prior work for binary feedback~\cite{chatterji2021theory}.  \cite{chatterji2021theory} aim to find the policy that maximizes the expected probability of generating feedback $1$, i.e., $\max_{\pi} \ex_{\tau \sim \pi, p}[\mu((\phi^{\tau})^\top \theta^*)]$, where the optimal policy can be non-Markovian due to the non-linearity of $\mu(\cdot)$. In contrast, we aim to find the optimal policy under the standard MDP definition, i.e., $\max_{\pi} \ex_{\tau \sim \pi, p}[(\phi^{\tau})^\top \theta^*]$, by inferring reward $\theta^*$ from binary feedback. Thus, we consider Markovian policies. Under our formulation, we design TS-type algorithms with  confidence bonuses added on $\theta^*$ element-wise to achieve computational efficiency, which cannot be done without sacrificing the regret order under the formulation of \cite{chatterji2021theory}.}





\paragraph{Sum Segment Feedback.}
In the sum segment feedback setting, in each episode $k$, at each step $h$, the environment generates an underlying random reward $R^k_h = r(s^k_h,s^k_h) + \varepsilon^k_h$, where $\varepsilon^k_h$ is a zero-mean and $1$-sub-Gaussian noise, and independent of transition. At the end of each segment $i \in [m]$, the agent observes the sum of random rewards
\begin{align*}
	R^k_i = \!\!\! \sum_{t=\frac{H}{m} (i-1)+1}^{\frac{H}{m}\cdot i} \!\!\! R(s^k_t,a^k_t) =  (\phi^{\tau^k_i})^\top \theta^* + \!\!\! \sum_{t=\frac{H}{m} (i-1)+1}^{\frac{H}{m}\cdot i} \!\!\! \varepsilon^k_t .
\end{align*}
Under sum feedback, when $m=H$, our model degenerates to  classic RL~\cite{azar2017minimax,sutton2018reinforcement}. 
%
When $m=1$, the above two settings reduce to the problems of RL with binary~\cite{chatterji2021theory} and sum trajectory feedback~\cite{efroni2021reinforcement}, respectively.

In our model, the agent needs to infer the reward function from sparse and implicit reward feedback. Let $K$ denote the number of episodes played. The goal of the agent is to minimize the cumulative regret, which is defined as
$
\cR(K):=\sum_{k=1}^{K} ( V^*_1(s_1) - V^{\pi^k}_1(s_1) )
$.



\section{Reinforcement Learning with Binary Segment Feedback}\label{sec:binary_feedback}

In this section, we investigate RL with binary segment feedback. To isolate the effect of segment feedback from transition model learning, we first design a computationally-efficient and sample-efficient algorithm $\bitsseg$ for the known transition case, and establish a novel lower bound to exhibit the indispensable exponential dependency in the result under binary feedback. Then, we further develop an algorithm $\bitssegtran$ with carefully-designed transition bonuses for the unknown transition case. 
%, which infers rewards from binary signals and  performs posterior sampling.

\subsection{Algorithm $\bitsseg$ for Known Transition}

\begin{algorithm}[t]
	\caption{$\bitsseg$} \label{alg:bits_sum_regret}
	\begin{algorithmic}[1]
		\STATE {\bfseries Input:} $\delta,\delta':=\frac{\delta}{3},\alpha := \exp(\frac{H r_{\max}}{m})+\exp(-\frac{H r_{\max}}{m})+2,\lambda$.
		\FOR{$k=1,\dots,K$}
		\STATE $\hat{\theta}_{k-1} \leftarrow \argmin_{\theta} -(\sum_{k'=1}^{k-1} \sum_{i=1}^{m} ( y^{k'}_i \cdot \log(\mu((\phi^{\tau^{k'}_i})^\top \theta)) + (1-y^{k'}_i) \cdot \log(1-\mu((\phi^{\tau^{k'}_i})^\top \theta) ) ) - \frac{1}{2} \lambda \|\theta\|_2^2)$\; \label{line:hat_theta_bits}
		\STATE $\Sigma_{k-1} \leftarrow  \sum_{k'=1}^{k-1} \sum_{i=1}^{m} \phi^{\tau^{k'}_i} (\phi^{\tau^{k'}_i})^\top + \alpha \lambda I$\; \label{line:Sigma_bits}
		\STATE Sample $\xi_k \sim \cN(0, \alpha \cdot \nu(k-1)^2 \cdot \Sigma_{k-1}^{-1})$, where $\nu(k-1)$ is defined in Eq.~\eqref{eq:def_nu}\; \label{line:noise_bits}
		\STATE $\tilde{\theta}_k \leftarrow \hat{\theta}_{k-1}+\xi_k$\; \label{line:tilde_theta_bits}
		\STATE $\pi^k \leftarrow \argmax_{\pi} (\phi^{\pi})^\top \tilde{\theta}_k$\; \label{line:pi_k_bits}
		% , where $\phi^{\pi}$ is defined in Eq.~\eqref{eq:def_phi_pi}
		\STATE Play episode $k$ with policy $\pi^k$. Observe trajectory $\tau^k$ and binary segment feedback $\{y^k_i\}_{i=1}^{m}$\; \label{line:play_bits}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

Building upon the Thompson sampling algorithm~\cite{thompson1933likelihood}, $\bitsseg$ adopts the maximum likelihood estimator (MLE) to learn rewards from binary feedback, and performs posterior sampling to compute the optimal policy. \revision{Different from prior trajectory feedback algorithms~\cite{chatterji2021theory} which are either computationally inefficient or have a $O(K^{\frac{2}{3}})$ regret bound, $\bitsseg$ is both computationally efficient and has a $O(\sqrt{K})$ regret bound.}

Algorithm~\ref{alg:bits_sum_regret} presents the procedure of $\bitsseg$. Specifically, in each episode $k$, $\bitsseg$ first employs MLE with past binary reward observations to obtain the estimated reward parameter $\hat{\theta}_{k-1}$ (Line~\ref{line:hat_theta_bits}). Then, $\bitsseg$ calculates the feature covariance matrix of past segments $\Sigma_{k-1}$ (Line~\ref{line:Sigma_bits}). After that, $\bitsseg$ samples a noise $\xi_k$ from Gaussian distribution $\cN(0, \alpha \cdot \nu(k-1)^2 \cdot \Sigma_{k-1}^{-1})$ (Line~\ref{line:noise_bits}). Here $\alpha$ is a universal upper bound of the inverse of the sigmoid function's derivative.
For any $k>0$, we define
\begin{align}
	& \nu(k):=\frac{m \sqrt{ \lambda}}{H} \Bigg( 1 + \frac{H r_{\max} \sqrt{|\cS| |\cA|}}{m}  + \frac{H}{m\sqrt{\lambda}} \cdot 
	\nonumber\\
	& \sqrt{ 1 + \frac{H r_{\max} \sqrt{|\cS| |\cA|}}{m} } \omega(k) + \frac{H^2}{m^2 \lambda} \cdot \omega(k)^2  \Bigg)^{\frac{3}{2}} , \label{eq:def_nu}
\end{align}
and
\begin{align}
	& \omega(k):=\sqrt{\lambda}\sbr{ r_{\max}\sqrt{|\cS||\cA|} + \frac{1}{2} } 
	\nonumber\\
	& + \frac{|\cS||\cA|}{\sqrt{\lambda}}\log\sbr{ \frac{4}{\delta'} \sbr{1+\frac{H^2 k}{4 |\cS||\cA| \lambda m}} } . \label{eq:def_omega}
\end{align}
\revision{$\nu(k)$ is the confidence radius factor of the MLE estimate $\hat{\theta}_{k}$. With high probability, we have $|\phi^\top \theta^* - \phi^\top \hat{\theta}_k| \leq \sqrt{\alpha} \cdot \nu(k) \|\phi\|_{\Sigma_k^{-1}}$, where  $\phi$ is the visitation indicator of any trajectory (Lemma~\ref{lemma:confence_interval_proj_free} in Appendix~\ref{apx:ub_binary_known_tran}).}

Adding noise $\xi_k$ to $\hat{\theta}_{k-1}$, $\bitsseg$ obtains a posterior reward estimate $\tilde{\theta}_{k}$ (Line~\ref{line:tilde_theta_bits}). 
\revision{Then, it computes the optimal policy $\pi^k$ under reward $\tilde{\theta}_{k}$, i.e.,  $\argmax_{\pi} (\phi^{\pi})^\top \tilde{\theta}_k$ (Line~\ref{line:pi_k_bits}). Note that this step is \emph{computationally efficient}, which can be easily solved by any MDP planning algorithm, e.g., value iteration, by taking    $\tilde{\theta}_{k}$ as the reward function.} 
After obtaining $\pi^k$, $\bitsseg$ plays episode $k$, and observes trajectory $\tau^k$ and binary feedback $\{y^k_i\}_{i=1}^{m}$ on each segment (Line~\ref{line:play_bits}).

Now we provide a regret upper bound for $\bitsseg$.
\begin{theorem}\label{thm:ub_bits}
	With probability at least $1-\delta$, for any $K>0$, the regret of algorithm $\bitsseg$ is bounded by
		\begin{align*}
			\cR(K) &= \tilde{O} \Bigg( \exp\sbr{\frac{H r_{\max}}{2m}} \nu(K) \sqrt{|\cS||\cA|} \cdot
			\\
			&\bigg( \sqrt{ Km |\cS| |\cA|  \max\lbr{ \frac{H^2}{m \alpha \lambda}, 1}  } + H\sqrt{ \frac{K}{\alpha \lambda} }  \bigg)
						\Bigg) .
%			%
%			&\mathcal{R}(K) = \tilde{O} \Bigg( \exp\bigg(\frac{H r_{\max}}{2m}\bigg) \cdot \bigg( \sqrt{ Km |\mathcal{S}| |\mathcal{A}| \max \Big\{ \frac{H^2}{m \alpha \lambda}, 1} \Big\} + H\sqrt{ \frac{K}{\alpha \lambda} }  \bigg) \cdot \frac{m \sqrt{\lambda |\mathcal{S}| |\mathcal{A}|}}{H} \cdot
%			\\
%			& \Bigg( 1 + \frac{H r_{\max} \sqrt{|\mathcal{S}| |\mathcal{A}|}}{m}  + \frac{H}{m\sqrt{\lambda}}  \sqrt{ 1 + \frac{H r_{\max} \sqrt{|\mathcal{S}| |\mathcal{A}|}}{m} } \bigg( \sqrt{\lambda} \Big( r_{\max}\sqrt{|\mathcal{S}| |\mathcal{A}|} + \frac{1}{2} \Big) + \frac{|\mathcal{S}| |\mathcal{A}|}{\sqrt{\lambda}} \bigg) 
%			\\
%			&+ \frac{H^2}{m^2 \lambda} \bigg( \sqrt{\lambda} \Big( r_{\max}\sqrt{|\mathcal{S}| |\mathcal{A}|} + \frac{1}{2} \Big) + \frac{|\mathcal{S}| |\mathcal{A}|}{\sqrt{\lambda}} \bigg)^2  \Bigg)^{\frac{3}{2}}
%			\Bigg) .
	\end{align*}
\end{theorem}

\revision{
In this result, the dependency on $|\mathcal{S}|$, $|\mathcal{A}|$ and $H$ are $|\mathcal{S}|^3$, $|\mathcal{A}|^3$ and $\exp(\frac{H r_{\max}}{2m}) H^2$, respectively. Our focus here is to reveal the exponential dependency on $\frac{Hr_{\max}}{m}$ in the regret bound under binary feedback,  instead of pursuing absolute tightness of every polynomial factor.
%Theorem~\ref{thm:ub_bits} exhibits that the regret of algorithm $\bitsseg$ has an exponential dependency $\exp(\frac{Hr_{\max}}{2m})$, which is usually the dominating factor. 
Since the exponential factor is usually the dominating factor, this result implies that as the number of segments $m$ increases, the regret decays rapidly. Thus, under binary feedback, increasing the number of segments significantly helps accelerate learning.

The intuition behind this exponential dependency is that when the reward scale $x=\frac{H r_{\max}}{m}$ is large, the binary feedback is generated from the range where the sigmoid function $\mu(x)=\frac{1}{1+\exp(-x)}$ is flat, i.e., the derivative of the sigmoid function $\mu'(x)$ is small. Then, the generated binary feedback is likely always $0$ or always $1$, and it is hard to distinguish between a good action and a bad action, leading to a higher regret; On the contrary, when the reward scale $x=\frac{H r_{\max}}{m}$ is small, the binary feedback is generated from the range where the sigmoid function $\mu(x)$ is steep, i.e., $\mu'(x)$ is large. Then, the generated binary feedback is more dispersed to be $0$ or $1$, and it is easier to distinguish between a good action and a bad action, leading to a lower regret. In other words, the regret bound depends on the inverse of the sigmoid function's derivative $\mu'(x)=\frac{1}{\exp(x)+\exp(-x)+2}$.

%	The intuition behind this is that the regret depends on the inverse
%	of the sigmoid function's derivative $\mu'(x)=\frac{1}{\exp(x)+\exp(-x)+2}$. When the scale of input $x$ (here $x=\frac{H r_{\max}}{m}$) is large, the output of the sigmoid function is close to $0$ or $1$, and the derivative is small (flat). In this case, the binary outcome is almost always $0$ or always $1$, and it is difficult to distinguish the rewards of two actions, leading to a high regret; In contrast, when the number of segments $m$ increases, the scale of input $x$ decreases, and the derivative of the sigmoid function becomes larger (steeper). Then, it is easier to distinguish the rewards of two actions, resulting in a lower regret.
}

% Below we establish a lower bound to demonstrate the inevitability of this exponential dependency.

\subsection{Regret Lower Bound for Known Transition}

Below we provide a lower bound, which \emph{firstly} demonstrates the inevitability of the exponential factor in the regret bound for RL with binary feedback.

\begin{theorem} \label{thm:lb_bi_known_tran}
	Consider RL with binary segment feedback and known transition. There exists a distribution of instances where for any $c_0 \in (0,\frac{1}{2})$, when $K \geq \exp( \frac{Hr_{\max}}{m} ) \frac{4 |\cS| |\cA| m }{ H^2 r_{\max}^2 c_0^2}$, the regret of any algorithm must be
	\begin{align*}
		\Omega\sbr{ \exp\sbr{ \Big(\frac{1}{2}-c_0\Big) \frac{Hr_{\max}}{m} } \sqrt{ |\cS| |\cA| m K } } . 
	\end{align*}
\end{theorem}
Theorem~\ref{thm:lb_bi_known_tran} shows that under binary feedback, the exponential dependency on $\frac{Hr_{\max}}{m}$ in the result is indispensable, and the $\exp(\frac{Hr_{\max}}{2m})$ factor in Theorem~\ref{thm:ub_bits} nearly matches the exponential factor in the lower bound up to an arbitrarily small factor $c_0$ in $\exp(\cdot)$.  Theorem~\ref{thm:lb_bi_known_tran} reveals that when the number of segments $m$ increases, the regret indeed decreases at an exponential rate. 
\revision{In addition, this lower bound also holds for the unknown transition case, by constructing the same problem instance as in its proof.
%, where the reward feedback is random and the transition is deterministic.


To the best of our knowledge, our lower bound for binary feedback and its analysis are novel in the RL literature. In the analysis, we calculate the KL divergence of Bernoulli distributions with the sigmoid function being in their parameters. Then, we employ Pinsker's inequality and the fact that $\mu'(x)=\mu(x)(1-\mu(x))$ to build a connection between the calculated KL divergence and $\mu'(\frac{H r_{\max}}{m})$. Since $\mu'(x)=\frac{1}{\exp(x)+\exp(-x)+2}$ contains an exponential factor, we can finally derive an exponential dependency in the lower bound. Below we give a proof sketch of Theorem~\ref{thm:lb_bi_known_tran}, and defer a full proof to Appendix~\ref{apx:lb_bi_known_tran}.
}


\emph{Proof Sketch.} Consider an instance as follows: there are $n$ bandit states $s_1,\dots,s_n$ (i.e., there is an optimal action and multiple suboptimal actions), a good absorbing state $s_{n+1}$ and a bad absorbing state $s_{n+2}$. 
The agent starts from $s_1,\dots,s_n$ with equal probability $\frac{1}{n}$. 
For any $i \in [n]$, in state $s_i$, under the optimal action $a^*_i$, the agent transitions to $s_{n+1}$ deterministically, and $r(s_i,a^*_i)=r_{\max}$; 
\begin{wrapfigure}[9]{r}{0.35\columnwidth}
	\centering
	\includegraphics[width=0.35\columnwidth]{fig/lower_bound_binary_main_text_crop.pdf}  
	\caption{Lower bound instance.}     
	\label{fig:lower_bound_main_text}
\end{wrapfigure}
Under any suboptimal action $a^{\sub}_i$, 
the agent transitions to $s_{n+2}$ deterministically, and $r(s_i,a^{\sub}_i)=(1-\varepsilon)r_{\max}$, 
where $\varepsilon \in (0,\frac{1}{2})$ is a parameter specified later. 
For all actions $a \in \cA$, $r(s_{n+1},a)=r_{\max}$ and $r(s_{n+2},a)=(1-\varepsilon)r_{\max}$.


Then, 
%	to learn the optimal policy, the agent needs to distinguish between the optimal action and suboptimal actions from binary segment feedback. 
the KL divergence of binary observations between the optimal action and suboptimal actions in an episode is 
\begin{align}
	& \sum_{i=1}^{m} \!\kl\! \sbr{ \bernoulli\sbr{ \mu\sbr{ \frac{(1-\varepsilon) H r_{\max}}{m}} } \Big\| \bernoulli\sbr{ \mu\sbr{  \frac{H r_{\max}}{m}} } } 
	\nonumber\\
	&\overset{\textup{(a)}}{\leq} m \cdot \frac{ \sbr{ \mu\sbr{\frac{(1-\varepsilon) H  r_{\max}}{m}} - \mu\sbr{ \frac{H r_{\max}}{m}} }^2 }{ \mu'\sbr{  \frac{H r_{\max}}{m}} }
	\nonumber\\
	&\overset{\textup{(b)}}{\leq} m \cdot \frac{ \mu'\sbr{ \frac{(1-\varepsilon) Hr_{\max}}{m} }^2 \sbr{\varepsilon \cdot \frac{Hr_{\max}}{m}}^2  }{ \mu'\sbr{ \frac{Hr_{\max}}{m} } } , \label{eq:kl_divergence}
\end{align}
Here $\bernoulli(p)$ denotes the Bernoulli distribution with parameter $p$. Inequality (a) uses the facts that $\kl(\bernoulli(p) \| \bernoulli(q)) \leq \frac{(p-q)^2}{q(1-q)}$ and $\mu'(x)=\mu(x)(1-\mu(x))$. Inequality (b) is due to that $\mu'(x)$ is monotonically decreasing when $x > 0$.

Furthermore, we consider the reward scale $Hr_{\max}$ in each episode, and the enumeration over each bandit state $s_i$ ($i \in [n]$) and each possible optimal action $a^*_i \in \cA$ in the lower bound derivation. Then, following the analysis in \cite{auer2002nonstochastic}, to learn the difference between the optimal action and suboptimal actions, the agent must suffer a regret
\begin{align*}
	&\Omega\sbr{ Hr_{\max} \sqrt{n|\cA|} \cdot \frac{1}{\sqrt{Eq.~\eqref{eq:kl_divergence}}} }
	\\
	&= \Omega\Bigg(  \frac{\sqrt{|\cA| n m}}{\varepsilon}  \sqrt{ \frac{  \mu'\sbr{ \frac{Hr_{\max}}{m} }  }{ \mu'\sbr{ (1-\varepsilon) \frac{Hr_{\max}}{m} }^2 } } \Bigg) .
\end{align*}
Recall that $\mu'(x)=\frac{1}{\exp(x)+\exp(-x)+2}$. Let $\varepsilon=\Theta(\frac{1}{\sqrt{K}})$.
For any constant $c_0 \in (0,\frac{1}{2})$, letting $K$ large enough ($\varepsilon$ small enough) to satisfy $\varepsilon \leq c_0$, then the regret is 
\begin{align*}
	&\Omega\sbr{  \sqrt{K |\cA| n m} \sqrt{ \exp\sbr{(1-2\varepsilon) \frac{Hr_{\max}}{m}}  } } 
	\\
	&= \Omega\sbr{ \sqrt{K |\cS| |\cA| m}  \exp\sbr{\sbr{\frac{1}{2}-c_0} \frac{Hr_{\max}}{m}}   } . \quad  \square
\end{align*}


\subsection{Algorithm $\bitssegtran$ for Unknown Transition}

Now we extend our results to the unknown transition case. 

We develop an efficient algorithm $\bitssegtran$ for binary segment feedback and unknown transition. $\bitssegtran$ includes a transition bonus $p^{pv}_{k-1}$ in posterior reward estimate $\tilde{\theta}^{b}_k$, and replaces visitation indicator $\phi^{\pi}$ by its estimate $\hat{\phi}^{\pi}_{k-1}$. For any $(s,a)$, $\hat{\phi}^{\pi}_{k-1}(s,a)$ is the expected number of times $(s,a)$ is visited in an episode under policy $\pi$ on empirical MDP $\hat{p}_{k-1}$, where $\hat{p}_{k-1}$ is the empirical estimate of transition distribution $p$. 
Then, $\bitssegtran$ computes the optimal policy via $\argmax_{\pi} (\hat{\phi}^{\pi}_{k-1})^\top \tilde{\theta}^{b}_k$, which can be efficiently solved by any MDP planning algorithm with transition distribution $\hat{p}_{k-1}$ and reward $\tilde{\theta}^{b}_k$.
We defer the details of $\bitssegtran$ to Appendix~\ref{apx:alg_bi_unknown_tran}, and present its regret performance as follows.

\begin{theorem}\label{thm:ub_bits_tran}
	With probability at least $1-\delta$, for any $K>0$, the regret of algorithm $\bitssegtran$ is bounded by
	\begin{align*}
		\tilde{O} \Bigg(& \exp\sbr{\frac{H r_{\max}}{2m}} \nu(K) \sqrt{|\cS||\cA|} \cdot
		\nonumber \\
		& \bigg( \sqrt{ Km |\cS| |\cA| \max\lbr{ \frac{H^2}{m \alpha \lambda}, 1}  } + H\sqrt{ \frac{K}{\alpha \lambda} }  \bigg)
		\\
		& + \bigg( \nu(K) \sqrt{ \frac{|\cS||\cA|}{\lambda} }  + Hr_{\max} \bigg) |\cS|^2 |\cA|^{\frac{3}{2}} H^{\frac{3}{2}}  \sqrt{ K }
		\Bigg) .
	\end{align*}
\end{theorem}

Similar to algorithm $\bitsseg$ (Theorem~\ref{thm:ub_bits}), the regret bound of algorithm $\bitssegtran$ also has a factor $\exp(\frac{H r_{\max}}{2m})$. When the number of segments $m$ increases, the regret of $\bitssegtran$ significantly decreases. Compared to $\bitsseg$, the regret of $\bitssegtran$ has an additional polynomial term in $|\cS|$, $|\cA|$, $H$ and $\sqrt{K}$, which is incurred due to learning the unknown transition distribution.

\begin{algorithm*}[t]
	\caption{$\edlinucbseg$} \label{alg:ed_linucb_seg}
	\begin{algorithmic}[1]
	\STATE {\bfseries Input:} $\delta,\delta':=\frac{\delta}{3},\lambda:=\frac{H}{r_{\max}^2 m},$ rounding procedure $\round$, rounding approximation parameter $\gamma:=\frac{1}{10}$. $\beta(k):=\sqrt{\frac{H|\cS||\cA|}{m} \log(1+\frac{kH^2}{\lambda|\cS||\cA|m})+2\log(\frac{1}{\delta'})} + r_{\max} \sqrt{\lambda|\cS||\cA|}, \forall k>0$.
	\STATE Let $w^* \in \triangle_{\Pi}$ and $z^*$ be the optimal solution and optimal value of the optimization:
	\begin{align}
		\min_{w \in \triangle_{\Pi}}   \Bigg\| \Bigg( \sum_{\pi \in \Pi} w(\pi) \bigg( \sum_{i=1}^{m} \ex_{\tau_i \sim \pi}\mbr{\phi^{\tau_i} (\phi^{\tau_i})^\top} \bigg) \Bigg)^{-1} \Bigg\| \label{eq:ed_opt}
	\end{align}  \label{line:e_optimal_design}\\
	\STATE $K_0 \leftarrow \lceil \max\{ 26 (1+\gamma)^2 (z^*)^2 H^4 \log(\frac{2|\cS||\cA|}{\delta'}),\ \frac{|\cS||\cA|}{\gamma^2} \} \rceil$  \label{line:K_0}\;
	\STATE $(\pi^1,\dots,\pi^{K_0}) \leftarrow \round(\{\sum_{i=1}^{m} \ex_{\tau_i \sim \pi}\mbr{\phi^{\tau_i} (\phi^{\tau_i})^\top} \}_{\pi \in \Pi}, w^*, \gamma, K_0)$ \label{line:round}\;
	\STATE Play $K_0$ episodes with policies $\pi^1,\dots,\pi^{K_0}$. Observe trajectories $\tau^1,\dots,\tau^{K_0}$ and rewards $\{R^1_i\}_{i=1}^{m},\dots,\{R^{K_0}_i\}_{i=1}^{m}$  \label{line:initial_exploration}\;
	\FOR{$k=K_0+1,\dots,K$}
		\STATE $\hat{\theta}_{k-1} \leftarrow (\lambda I + \sum_{k'=1}^{k-1} \sum_{i=1}^{m} \phi^{\tau^{k'}_i} (\phi^{\tau^{k'}_i})^\top)^{-1} \sum_{k'=1}^{k-1} \sum_{i=1}^{m} \phi^{\tau^{k'}_i} R^{k'}_i $  \label{line:hat_theta}\;
		\STATE $\Sigma_{k-1} \leftarrow \lambda I + \sum_{k'=1}^{k-1} \sum_{i=1}^{m} \phi^{\tau^{k'}_i} (\phi^{\tau^{k'}_i})^\top$  \label{line:Sigma}\;
		\STATE $\pi^k \leftarrow \argmax_{\pi \in \Pi} ((\phi^{\pi})^\top \hat{\theta}_{k-1} + \beta(k-1) \cdot  \|\phi^{\pi}\|_{(\Sigma_{k-1})^{-1}} )$  \label{line:pi_k_elinucb}\;
		\STATE Play episode $k$ with policy $\pi^k$. Observe trajectory $\tau^k$ and sum segment feedback $\{R^k_i\}_{i=1}^{m}$  \label{line:play_episode_elinucb}\;
	\ENDFOR
	\end{algorithmic}
\end{algorithm*}


\section{Reinforcement Learning with Sum Segment Feedback}

In this section, we turn to RL with sum segment feedback. 
\revision{Different from prior sum trajectory feedback algorithm~\cite{efroni2021reinforcement}, which directly uses the least squares estimator and has a suboptimal regret bound, we develop an algorithm $\edlinucbseg$ for the known transition case, which adopts experimental design to perform an initial exploration and achieves a near-optimal regret with respect to $H$ and $m$. To validate the optimality, we further establish a regret lower bound.} Moreover, we design an algorithm $\linucbtranseg$ equipped with a variance-aware transition bonus to handle the unknown transition case.

\subsection{Algorithm $\edlinucbseg$ for Known Transition}


If we regard visitation indicators $\phi^{\pi^k_i}$ as feature vectors and $\theta^*$ as the reward parameter, RL with sum segment feedback and known transition is similar to linear bandits.

Building upon the classic linear bandit algorithm $\mathtt{LinUCB}$~\cite{abbasi2011improved}, our algorithm $\edlinucbseg$ performs the E-optimal design~\cite{pukelsheim2006optimal} to conduct an initial exploration. This scheme ensures sufficient coverage of the covariance matrix and further sharpens the norm under the inverse of the covariance matrix, which enables an improved regret bound over prior trajectory feedback algorithm~\cite{efroni2021reinforcement}. 
%Then, it estimates the reward parameter via optimistic least squares with visitation indicators as features.

Algorithm~\ref{alg:ed_linucb_seg} shows the procedure of $\edlinucbseg$. Specifically, $\edlinucbseg$ first performs the E-optimal design to compute a distribution on policies $w^*$, which maximizes the minimum eigenvalue of the feature covariance matrix $\sum_{\pi \in \Pi} w(\pi) ( \sum_{i=1}^{m} \ex_{\tau_i \sim \pi} [\phi^{\tau_i} (\phi^{\tau_i})^\top] )$ (Line~\ref{line:e_optimal_design}). We assume that there exists a policy distribution $w$ under which this matrix is invertible. Then, $\edlinucbseg$ calculates the number of samples $K_0$ for initial exploration according to the optimal value of the E-optimal design (Line~\ref{line:K_0}).


Then, in Line~\ref{line:round}, $\edlinucbseg$ calls a rounding procedure $\round$~\cite{allen2021near} to transform sampling distribution $w^*$ into discrete sampling sequence $(\pi^1,\dots,\pi^{K_0})$, which satisfies (see Appendix~\ref{apx:rounding_procedure} for more details of $\round$)
\begin{align*}
	&\Bigg\| \Bigg( \sum_{k=1}^{K_0} \bigg( \sum_{i=1}^{m} \ex_{\tau_i \sim \pi_k}\mbr{\phi^{\tau_i} (\phi^{\tau_i})^\top} \bigg) \Bigg)^{-1} \Bigg\| 
	\\
	&\leq \! (1 \!+\! \gamma)  \Bigg\|  \Bigg(\! K_0\!\! \sum_{\pi \in \Pi} \! w^*(\pi) \bigg( \!\sum_{i=1}^{m} \! \ex_{\tau_i \sim \pi}\mbr{\phi^{\tau_i} (\phi^{\tau_i})^\top} \!\bigg) \Bigg)^{\!\!\!-1} \Bigg\| .
\end{align*}
After that, $\edlinucbseg$ plays $K_0$ episodes with $(\pi^1,\dots,\pi^{K_0})$ to perform initial exploration (Line~\ref{line:initial_exploration}).
Owing to the E-optimal design, the covariance matrix of initial exploration $\Sigma_{K_0}$ has an optimized minimum eigenvalue, and then $\|\phi^{\pi}\|_{(\Sigma_{k-1})^{-1}}$ has a sharp upper bound for any $k>K_0$. This is the key to the optimality of $\edlinucbseg$.
% with respect to $H$ and $m$.

% After initial exploration, 
In each episode $k>K_0$, $\edlinucbseg$ first calculates the least squares reward estimate $\hat{\theta}_{k-1}$ using past reward observations and covariance matrix $\Sigma_{k-1}$ (Lines~\ref{line:hat_theta}-\ref{line:Sigma}). Then, it computes the optimal policy with reward estimate $\hat{\theta}_{k-1}$ and reward confidence bonus $\|\phi^{\pi}\|_{(\Sigma_{k-1})^{-1}}$ (Line~\ref{line:pi_k_elinucb}).  $\edlinucbseg$ plays episode $k$ with the computed optimal policy $\pi^k$, and collects trajectory $\tau^k$ and reward observations on each segment  $\{R^k_i\}_{i=1}^{m}$ (Line~\ref{line:play_episode_elinucb}).
Below we present a regret upper bound for algorithm $\edlinucbseg$.

%\revision{
%Here in the sum feedback setting, we adopt a UCB-type algorithm to achieve the optimality with respect to $H$ and $m$. By contrast, in the aforementioned binary feedback setting (Section~\ref{sec:binary_feedback}), we use a TS-type algorithm to achieve computational efficiency, since our focus there is mainly to reveal the inevitable exponential dependency on $\frac{H r_{\max}}{m}$ in the regret result, instead of absolute tightness.	
%}

\begin{theorem} \label{thm:ub_sum_known_tran}
	With probability at least $1-\delta$, for any $K>0$, the regret of algorithm $\edlinucbseg$ is bounded by
	\begin{align*}
		 O\Bigg(& |\cS| |\cA| \sqrt{HK} \log\sbr{\sbr{1+\frac{KH r_{\max}}{|\cS||\cA|m}} \frac{1}{\delta}} 
		\\
		& + (z^*)^2 H^5  \log\sbr{\frac{|\cS||\cA|}{\delta}} + |\cS||\cA|H \Bigg) .
	\end{align*}
\end{theorem}

\revision{Surprisingly, under sum feedback, when the number of segments $m$ increases, the regret bound does not decrease significantly, e.g., at a rate of $\frac{1}{\sqrt{m}}$ or $\frac{1}{m}$.}
While this looks surprising at the first glance, we discover an \emph{intuition} through analysis: The performance in RL is measured by the expected reward sum of an episode, namely, we only need to accurately estimate the expected reward sum of an episode.
When the number of segments $m$ increases, while we obtain more observations, the segment features $\phi^{\tau^{k'}_i}$ contributed to covariance matrix $\Sigma_{k}$ shrink, which makes the reward estimation uncertainty $\|\phi^{\pi}\|_{(\Sigma_{k})^{-1}}$ inflate. 
When we focus on the estimation performance of the expected reward sum of an episode, these two effects cancel out with each other, and the regret result is not  influenced by $m$ distinctly.

\revision{When $m=1$, our problem reduces to RL with sum trajectory feedback~\cite{efroni2021reinforcement}, and our result \emph{improves} theirs by a factor of $\sqrt{H}$ and achieves the optimality with respect to $H$. This improvement comes from the fact that we conduct the E-optimal design and perform an initial exploration to guarantee that $\|\phi^{\pi}\|_{(\Sigma_{k-1})^{-1}} \leq 1$, instead of $\|\phi^{\pi}\|_{(\Sigma_{k-1})^{-1}} \leq \frac{H}{\sqrt{\lambda}}$ as used in~\cite{efroni2021reinforcement}.}
%, and then we use a refined elliptical potential bound in analysis.

Next, we study the lower bound to see if the number of segments $m$ really does not influence the regret bound much.

\begin{figure*}[t]
	\centering   
	\subfigure[Binary segment feedback]{
		\includegraphics[width=0.28\textwidth]{fig/Plot_Binary.pdf} \label{fig:experiment_bi}
	}
	\subfigure[Sum segment feedback]{
		\includegraphics[width=0.28\textwidth]{fig/Plot_Sum_DisplayLenFalse.pdf}
		\includegraphics[width=0.28\textwidth]{fig/Plot_Sum_DisplayLenTrue.pdf} \label{fig:experiment_sum}
	}
	\caption{Experimental results for RL with binary or sum segment feedback.
	}
\end{figure*}

\subsection{Regret Lower Bound for Known Transition}

We establish a lower bound for RL with sum segment feedback and known transition as follows.

\begin{theorem} \label{thm:lb_sum_known_tran}
	Consider RL with sum segment feedback and known transition. There exists a distribution of instances where the regret of any algorithm must be
	\begin{align*}
		\Omega\sbr{ \sqrt{|\cS||\cA|HK} } .
	\end{align*}
\end{theorem}

Theorem~\ref{thm:lb_sum_known_tran} demonstrates that our regret upper bound for algorithm $\edlinucbseg$ (Theorem~\ref{thm:ub_sum_known_tran}) is optimal with respect to $H$ and $m$ when ignoring logarithmic factors. In addition, this lower bound corroborates that the number of segments $m$ does not impact the regret result in essence.

%	\emph{Proof Idea.} Now we give the proof idea behind Theorem~\ref{thm:lb_sum_known_tran}. Consider the following instance: the agent starts from $s_1,\dots,s_n$ with equal probability $\frac{1}{|\cS|}$. 
%	For any $i \in [n]$, the optimal action $a^*_i$ is uniformly drawn from $\cA$.
%	For any $i \in [n]$, under the optimal action $a^*_i$, we have $r(s_i,a^*_i)=\frac{1}{2}+\varepsilon$, where $\varepsilon \in (0,\frac{1}{2})$ is a parameter specified later; Under any suboptimal action $a^{\sub}_i$, $r(s_i,a^{\sub}_i)=\frac{1}{2}$. 
%	Then, the KL divergence of segment reward observations between the optimal action and suboptimal action in each episode is
%	\begin{align}
	%		\sum_{i=1}^{m} \kl \sbr{ \cN\sbr{ \frac{1}{2} \cdot \frac{H}{m},\ \frac{H}{m}} \Big\| \cN\sbr{\sbr{\frac{1}{2}+\varepsilon} \frac{H}{m},\ \frac{H}{m}} } 
	%		&=  m \cdot \frac{ \sbr{\frac{H}{m} \cdot \varepsilon}^2 }{\frac{H}{m}}
	%		\nonumber\\
	%		&=  H \varepsilon^2 . \label{eq:lb_kl_sum}
	%	\end{align} 
%	
%	Note that the KL divergence here is not influenced by $m$, because the increase of $m$ offsets the decrease of the difference between the optimal action and suboptimal action.
%	
%	Let $\varepsilon=\Theta(\frac{1}{\sqrt{K}})$. Considering the reward scale of an episode $\varepsilon H$ and the enumeration over each state and each action, to distinguish the optimal action from suboptimal actions, the regret must be 
%	\begin{align*}
	%		\Omega\sbr{ H \sqrt{|\cS| |\cA|} \cdot \frac{1}{\sqrt{ \textup{Eq. \eqref{eq:lb_kl_sum}} }} } = \Omega\sbr{ \frac{\sqrt{H|\cS| |\cA|}}{ \varepsilon} } = \Omega\sbr{ \sqrt{K H |\cS| |\cA|} } .
	%	\end{align*}
%	\hfill $\square$




\subsection{Algorithm $\linucbtranseg$ for Unknown Transition}

Now we investigate RL with sum segment feedback in the unknown transition case. 

For unknown transition, we design an algorithm $\linucbtranseg$, which establishes a variance-aware uncertainty bound for the estimated visitation indicator $\hat{\phi}^{\pi}_{k}$, and incorporates this uncertainty bound into exploration bonuses. In analysis, we handle the estimation error of visitation indicators $\|\hat{\phi}_{k}^{\pi} - \phi^{\pi}\|_1$ by this variance-aware uncertainty bound, which enables us to achieve a near-optimal regret in terms of $H$. The details of $\linucbtranseg$ are deferred to Appendix~\ref{apx:alg_sum_unknown_tran}, and we state the regret performance of algorithm $\linucbtranseg$ below.

\begin{theorem} \label{thm:ub_sum_unknown_tran}
	With probability at least $1-\delta$, for any $K>0$, the regret of algorithm $\linucbtranseg$ is bounded by 
	\begin{align*}
		\tilde{O} \sbr{ (1+r_{\max}) |\cS|^{\frac{5}{2}} |\cA|^2 H \sqrt{K} } .
	\end{align*}
\end{theorem}
Theorem~\ref{thm:ub_sum_unknown_tran} shows that similar to algorithm $\edlinucbseg$, the regret of $\linucbtranseg$ does not depend on the number of segments $m$ when ignoring logarithmic factors. The heavier dependency on $|\cS|$, $|\cA|$ and $H$ is due to the estimation of the unknown transition distribution. 
We also provide a lower bound for the unknown transition case, which demonstrates that the optimal regret indeed does not depend on $m$ and our upper bound is near-optimal with respect to $H$ (see Appendix~\ref{apx:lb_sum_unknown_tran}).

%	Intuitively, the regret for RL with sum segment feedback ($m \geq 1$) is no higher than that for classic RL ($m=1$), since segments offer more observations.
%	Even for $m\geq1$, the regret of $\linucbtranseg$ matches the existing lower bound for classic RL~\cite{osband2016lower} in terms of $H$. Thus, this regret bound is optimal with respect to $H$ and $m$ for the sum segment feedback problem (we present a formal lower bound and a proof in Appendix~\ref{apx:lb_sum_unknown_tran}).


\section{Experiments}



Below we present experiments for RL with segment feedback to validate our theoretical results.

For the binary feedback setting, we evaluate our algorithms $\bitsseg$ and $\bitssegtran$ in known and unknown transition cases, respectively, and we set $|\cS|=9$, $|\cA|=5$ and $K=30000$.
For the sum feedback setting, similarly, we run our algorithms $\edlinucbseg$ and $\linucbtranseg$ in known and unknown transition cases, respectively. Since $\edlinucbseg$ and $\linucbtranseg$ are computationally inefficient (mainly designed to reveal the optimal dependency on $m$), we use a small MDP with $|\cS|=3$ and $|\cA|=5$, and set $K=1000$.
The details of the instances used in our experiments are described in Appendix~\ref{apx:experimental_setup}.
In both settings, we set $r_{\max}=0.5$, $\delta=0.005$, $H=100$ and $m \in \{1,2,4,5,10,20,25,50,100\}$. For each algorithm, we perform $20$ independent runs, and plot the average cumulative regret up to episode $K$ across runs with a $95\%$ confidence interval.

Figure~\ref{fig:experiment_bi} reports the regrets of algorithms $\bitsseg$ and $\bitssegtran$ under binary feedback. One sees that as the number of segments $m$ increases, the regret decreases rapidly. Specifically, when $m$ decreases from $20$ to $1$, i.e., $\frac{H}{2m}$ increases from $\exp(2.5)$ to $\exp(50)$, the regret grows explosively. This matches our theoretical results, i.e., Theorems~\ref{thm:ub_bits} and \ref{thm:ub_bits_tran}, which show a dependency on $\exp(\frac{Hr_{\max}}{2m})$.

Figure~\ref{fig:experiment_sum} plots the regrets of algorithms $\edlinucbseg$ and $\linucbtranseg$ under sum feedback. To see the impact of segments on regrets clearly, here we show the regrets with respect to the number of segments $m$ and the length of each segment $\frac{H}{m}$ in the left and right subfigures, respectively. In the left subfigure, when $m$ increases, the  regrets almost keep the same for small $m$ and slightly decrease for large $m$. To see the dependency on $m$ more clearly, we turn to the right subfigure: When the length of each segment $\frac{H}{m}$ increases, the regrets slightly increase in a logarithmic trend. This also matches our theoretical bounds in Theorems~\ref{thm:ub_sum_known_tran} and \ref{thm:ub_sum_unknown_tran}, which do not depend on $m$ except for the $\log(\frac{H}{m})$ factor.


\section{Conclusion}

In this work, we formulate a model named RL with segment feedback, which offers a general paradigm for feedback, bridging the gap between per-state-action feedback in classic RL and trajectory feedback. In the binary feedback setting, we deign efficient algorithms $\bitsseg$ and $\bitssegtran$, and provide regret upper and lower bounds which show a dependency on $\exp(\frac{Hr_{\max}}{2m})$. These results reveal that under binary feedback, increasing the number of segments $m$ greatly helps expedite learning. In the sum feedback setting, we develop near-optimal algorithms $\edlinucbseg$ and $\linucbtranseg$ in terms of $H$ and $m$, where the regret results do not depend on $m$ when ignoring logarithmic factors. These results exhibit that under sum feedback, increasing $m$ does not help accelerate learning much. 

There are several interesting directions worth further investigation. One direction is to consider segments of unequal lengths and study how to divide segments to optimize learning. Another direction is to generalize the results to the function approximation setting.



\bibliographystyle{plain}
\bibliography{icml2025_segment_ref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\input{icml2025_segment_supp}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
