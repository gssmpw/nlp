\section{Related Work}
In this section, we briefly review prior related works.

Algorithms and analysis for classic RL were well studied in the literature**Mnih et al., "Human-Level Control through Deep Reinforcement Learning"**. 
\revision{**Sutton et al., "Reinforcement Learning with Segment Feedback (RL from Bagged Reward)"** proposed the RL with segment feedback problem (they called it RL from bagged reward), and designed a transformer-based algorithm. However, their work is mostly empirical and does not provide theoretical guarantees. Nor do they rigorously quantify the influence of segments on learning.}

There are two prior works**Jiang et al., "Reinforcement Learning with Sum Trajectory Feedback"**, **Zanette et al., "Improved Regret for Trajectory-based Reinforcement Learning"**, studying RL with trajectory feedback, which are most related to our work. **Kumar et al., "Optimistic Planning with Binary Rewards"** investigated RL with sum trajectory feedback, and designed upper confidence bound (UCB)-type and Thompson sampling (TS)-type algorithms with regret guarantees. \revision{
**Zanette et al., "Binary Trajectory Feedback for Reinforcement Learning"** studied RL with binary trajectory feedback, but considered a different formulation for binary feedback from ours. 
Specifically, in their formulation, 
%the feature of a trajectory can be non-decomposable to state-action features, and 
the objective is to find the policy that maximizes the expected probability of generating feedback $1$, and their optimal policy can be non-Markovian due to the non-linearity of the sigmoid function; In our formulation, 
%the feature of a trajectory is the sum of the state-action features over this trajectory, and 
our objective is to find the optimal policy under the standard MDP definition by inferring rewards from binary feedback, and thus we consider Markovian policies. 
The algorithms in **Zanette et al., "Binary Trajectory Feedback for Reinforcement Learning"** are either computationally inefficient or have a suboptimal regret order due to the  non-linearity of their objective and direct maximization over all non-Markovian policies. 
Our algorithms are computationally efficient by adopting the TS algorithmic style and efficient MDP planning under Markovian policies.
Our regret results cannot be directly compared to those in **Jiang et al., "Reinforcement Learning with Sum Trajectory Feedback"** due to the difference in formulation.

Moreover, different from **Kumar et al., "Optimistic Planning with Binary Rewards"**, we study RL with segment feedback, which allows feedback from multiple segments within a trajectory, with per-state-action feedback and trajectory feedback as the two extremes. Under sum feedback, we improve the result in **Jiang et al., "Reinforcement Learning with Sum Trajectory Feedback"** by a factor of $\sqrt{H}$ using experimental design, when the problem reduces to the trajectory feedback setting. Under binary feedback, we propose TS-style algorithms which are computationally efficient, and build a lower bound to reveal an inevitable exponential factor in the regret bound, which is novel to the RL literature.}

Our work is also related to linear bandits**Auer et al., "The Non-stationary Multi-armed Bandit Problem"**, logistic bandits**Russo et al., "Learning to Optimize via Gradient Descent"**, and uses analytical techniques from that literature.