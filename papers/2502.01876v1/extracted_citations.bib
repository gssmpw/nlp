@inproceedings{abbasi2011improved,
	title={Improved algorithms for linear stochastic bandits},
	author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
	booktitle={Advances in Neural Information Processing Systems},
	volume={24},
	year={2011}
}

@inproceedings{azar2017minimax,
	title={Minimax regret bounds for reinforcement learning},
	author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
	booktitle={International Conference on Machine Learning},
	pages={263--272},
	year={2017},
	organization={PMLR}
}

@inproceedings{chatterji2021theory,
	title={On the theory of reinforcement learning with once-per-episode feedback},
	author={Chatterji, Niladri and Pacchiano, Aldo and Bartlett, Peter and Jordan, Michael},
	booktitle={Advances in Neural Information Processing Systems},
	volume={34},
	pages={3401--3412},
	year={2021}
}

@inproceedings{efroni2021reinforcement,
	title={Reinforcement learning with trajectory feedback},
	author={Efroni, Yonathan and Merlis, Nadav and Mannor, Shie},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={35},
	number={8},
	pages={7288--7295},
	year={2021}
}

@inproceedings{faury2020improved,
	title={Improved optimistic algorithms for logistic bandits},
	author={Faury, Louis and Abeille, Marc and Calauz{\`e}nes, Cl{\'e}ment and Fercoq, Olivier},
	booktitle={International Conference on Machine Learning},
	pages={3052--3060},
	year={2020},
	organization={PMLR}
}

@inproceedings{filippi2010parametric,
	title={Parametric bandits: The generalized linear case},
	author={Filippi, Sarah and Cappe, Olivier and Garivier, Aur{\'e}lien and Szepesv{\'a}ri, Csaba},
	booktitle={Advances in Neural Information Processing Systems},
	volume={23},
	year={2010}
}

@article{jaksch2010near,
	title={Near-optimal Regret Bounds for Reinforcement Learning},
	author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	journal={Journal of Machine Learning Research},
	volume={11},
	pages={1563--1600},
	year={2010}
}

@inproceedings{jin2018q,
	title={Is Q-learning provably efficient?},
	author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
	booktitle={Advances in Neural Information Processing Systems},
	volume={31},
	year={2018}
}

@inproceedings{russac2021self,
	title={Self-concordant analysis of generalized linear bandits with forgetting},
	author={Russac, Yoan and Faury, Louis and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={658--666},
	year={2021},
	organization={PMLR}
}

@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018},
	publisher={MIT press}
}

@inproceedings{tang2024reinforcement,
	title={Reinforcement learning from bagged reward},
	author={Tang, Yuting and Cai, Xin-Qiang and Ding, Yao-Xiang and Wu, Qiyu and Liu, Guoqing and Sugiyama, Masashi},
	booktitle={ICML 2024 Workshop: Aligning Reinforcement Learning Experimentalists and Theorists},
	year={2024}
}

@inproceedings{zanette2019tighter,
	title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
	author={Zanette, Andrea and Brunskill, Emma},
	booktitle={International Conference on Machine Learning},
	pages={7304--7312},
	year={2019},
	organization={PMLR}
}

