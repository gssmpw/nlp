%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

\newcommand{\ICML}[1]{}
\newcommand{\Arxiv}[1]{#1}
% \newcommand{\ICML}[1]{#1}
% \newcommand{\Arxiv}[1]{}
\ICML{\textcolor{red}{Remove the [11pt] part from the first line!}}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{diagbox}
\usepackage{array}

\Arxiv{
    \usepackage{float}
    \usepackage{natbib}
    \usepackage[margin=1in]{geometry}
}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\ICML{\usepackage{icml2025}}

% If accepted, instead use the following line for the camera-ready submission:
 % \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize, noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable, textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\ICML{\icmltitlerunning{Privacy amplification by random allocation}}

\begin{document}


\newcommand{\eps}{\epsilon}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\res}{o}
\newcommand{\resRV}{O}
\newcommand{\resDom}{\mathcal{O}}
\newcommand{\mech}{M}
\newcommand{\rand}{R}
\newcommand{\domain}{\mathcal{X}}
\newcommand{\dataset}{\boldsymbol{s}}
\newcommand{\view}{\boldsymbol{v}}
\newcommand{\viewRV}{\boldsymbol{V}}

\newcommand{\Renyi}[1]{\boldsymbol{R}_{#1}}
\newcommand{\RenyiDiv}[3]{\Renyi{#1}\left(#2 \Vert #3 \right)}

\newcommand{\Pois}[2]{\mathcal{P}_{#1}\left(#2\right)}
\newcommand{\PoisFunc}[3]{\mathcal{P}_{#1}\left(#2; #3\right)}
\newcommand{\shuf}[2]{\mathcal{S}_{#1}\left(#2\right)}
\newcommand{\shufFunc}[3]{\mathcal{S}_{#1}\left(#2; #3\right)}
\newcommand{\alloc}[2]{\mathcal{A}_{#1}\left(#2\right)}
\newcommand{\allocFunc}[3]{\mathcal{A}_{#1}\left(#2; #3\right)}
\newcommand{\post}[2]{\mathcal{T}_{#1}\left(#2\right)}
\newcommand{\postFunc}[3]{\mathcal{T}_{#1}\left(#2; #3\right)}

\ICML{
\twocolumn[
\icmltitle{Privacy amplification by random allocation}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\begin{icmlauthorlist}
\icmlauthor{Vitaly Feldman}{app}
\icmlauthor{Moshe Shenfeld}{HUJI}
\end{icmlauthorlist}

\icmlaffiliation{app}{Apple}
\icmlaffiliation{HUJI}{The Hebrew university of Jerusalem}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}




% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\ICML{\icmlkeywords{Differential Privacy}}

\vskip 0.3in
]
}
\Arxiv{
\title{Privacy amplification by random allocation}
\author{
Vitaly Feldman\\
Apple \\
\and
Moshe Shenfeld\footnote{Work partially done while author was an intern at Apple}\\
The Hebrew university of Jerusalem
}
\maketitle
}
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
We consider the privacy guarantees of an algorithm in which a user's data is used in $k$ steps randomly and uniformly chosen from a sequence (or set) of $t$ differentially private steps. We demonstrate that the privacy guarantees of this sampling scheme can be upper bound by the privacy guarantees of the well-studied independent (or Poisson) subsampling in which each step uses the user's data with probability $(1+ o(1))k/t $. Further, we provide two additional analysis techniques that lead to numerical improvements in some parameter regimes. 
The case of $k=1$ corresponds to partitioning the data points into $t$ disjoint batches independently of each other. It has been previously studied in the context of DP-SGD in \cite{BKMT20} and very recently in \cite{CGHLKKMSZ24, CCGHST24} as Balls-and-Bins sampling. Privacy analysis of \citet{BKMT20} relies on privacy amplification by shuffling which leads to overly conservative bounds. Privacy analysis of \citet{CGHLKKMSZ24, CCGHST24} relies on Monte Carlo simulations that are computationally prohibitive in many practical scenarios and have additional inherent limitations.% resulting in estimates of privacy 
%where approaches for estimating the amplified privacy parameters through Monte Carlo simulations have been explored. For this important special case we give a computationally efficient and exact moment-based privacy analysis that overcomes the limitations of the simulation-based approach.
\end{abstract}

\section{Introduction}
One of the central tools in the analysis of differentially private algorithms are so-called {\em privacy amplification} results where amplification results from sampling of the inputs. In these results one starts with a differentially private algorithms (or a sequence of such algorithms) and a randomized algorithm for selecting (or sampling) which of the $n$ elements in a dataset to run each of the $t$ algorithms on. Importantly, the random bits of the sampling scheme and the selected data elements are not revealed. For a variety of sampling schemes this additional uncertainty is known to lead to improved privacy guarantees of the resulting algorithm, that it, privacy amplification. 

In the simpler, single step, case a DP algorithm is run on a randomly chosen subset of the dataset. As first shown by \citet{KLNRS11}, if each element of the dataset is included in the subset with probability $\lambda$ (independently of other elements) then the privacy of the resulting algorithm is better (roughly) by a factor $\lambda$. This basic result has found numerous applications, most notably in the analysis of the differentially private stochastic gradient descent (DP-SGD) algorithm \citep{BST14}. In DP-SGD gradients are computed on randomly chosen batches of data points and then privatized through Gaussian noise addition. Privacy analysis of this algorithm is based on the so called Poisson sampling: elements in each batch and across batches are chosen randomly and independently of each other. The absence of dependence implies that the algorithm can be analyzed relatively easily as a direct composition of single step amplification results. The downside of this simplicity is that such sampling is less efficient and harder to implement within the standard ML pipelines. As a result, in practice some form of shuffling is used to define the batches in DP-SGD leading to a well-recognized discrepancy between the implementations of DP-SGD and their analysis \citep{CGKKMSZ24a, CGKKMSZ24b, ABDCH24}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{main_plot.png}
    \caption{Upper bounds on privacy parameter $\eps$ as a function of the noise parameter $\sigma$ for various schemes and the local mechanism (no amplification), all using the Gaussian mechanism with fixed parameters $\delta = 10^{-10}$, $t = 10^{6}$. In the Poisson scheme $\lambda = 1/t$. The "flat" part of the RDP based calculation is due to computational limitations, which was computed for the range $\alpha \in[2, 60]$.}
    \label{fig:main-fig}
\end{figure}


Motivated by the shuffle model of federated data analysis \citep{BEMMRLRKTS17}, \citet{CSUZZ19, EFMRTT19} have studied the privacy amplification of the shuffling scheme. In this scheme the $n$ elements are randomly and uniformly permuted and $i$-th element in the permuted order is used in the $i$-th step of the algorithm. This sampling scheme can be used to analyze the implementations of DP-SGD used in practice \citep{EFMRTT19, FMT21}. However, the analysis of this sampling scheme is more involved and nearly tight results are known only for relatively simple pure DP ($\delta =0$) algorithms \citep{FMT21,FMT23, GDDKS21}. 
In particular, applying these results to Gaussian noise addition requires using $(\eps,\delta)$-guarantees of the Gaussian noise. This leads to an additional $\sqrt{\ln(1/\delta)}$ factor in the asymptotic analysis and significantly worse numerical results (see Fig.~\ref{fig:main-fig} for comparison).

Note that shuffling differs from Poisson subsampling in that participation of elements is dependent both in each step (or batch) and across the steps. If the participation of elements in each step is dependent (by fixing the total number of participating elements) but the steps are independent then the sampling scheme can be tightly analyzed as a direct composition of fixed subset size sampling steps (e.g., using bound in \citet{BBG18,ZDW22}).
However, a more problematic aspect of Poisson sampling is the stochasticity in the number of times each element is used in all steps. For example, using Poisson sampling with sampling rate $1/t$ over $t$ batches will result in a roughly $1/e$ probability of not using the sample which implies dropping approximately $37\%$ of the data. In a distributed setting it is also often necessary to limit the maximum number of times a user participates in the analysis due to time or communication constraints on the protocol \citep{CSOK24,AFKRT25}. Poisson sampling does not allow to fully exploit the available limit potentially hurting the utility.  

%This motivates the need for analysis of sampling schemes with a strict bound on the number of participations by each user \citep{AFKRT25}.
Motivated by the privacy analysis of DP-SGD and the problem of communication-efficient high-dimensional private aggregation with two servers \citep{AFKRT25}, we analyze sampling schemes where each element participates in exactly $k$ randomly chosen steps out of the total $t$, independently of other elements. We refer to this sampling as $k$-out-of-$t$ {\em random allocation}.
For $k=1$, this scheme is a special case of the {\em random check-in} model of defining batches for DP-SGD in \citep{BKMT20}. Their analysis of this variant relies on the amplification properties of shuffling and thus does not lead to better privacy guarantees than those that are known for shuffling. Very recently, \citet{CGHLKKMSZ24} have studied such sampling (referring to it as {\em balls-and-bins sampling}) in the context of training neural networks via DP-SGD. Their main results show that from the point of view of utility (namely, accuracy of the final model) random allocation is essentially identical to shuffling and is noticeably better than Poisson sampling. Concurrently, \citet{CCGHST24} considered the same sampling scheme for the matrix mechanism in the context of DP-FTRL. The privacy analysis in these two works reduces the problem to analyzing the divergence of a specific pair of distributions on $\reals^t$. They then used Monte Carlo simulations to estimate the privacy parameters of this pair. Their numerical results suggest that privacy guarantees of $1$-out-of-$t$ random allocation are similar to those of the Poisson sampling with rate of $1/t$. While very encouraging, such simulations have several limitations, most notably, achieving high-confidence estimates for small $\delta$ and supporting composition appear to be computationally impractical. This approach also does not lead to provable privacy guarantees and does not lend itself to asymptotic analysis (such as the scaling of the privacy guarantees with $t$).

%Poisson subsampling and shuffling model differ in two key aspects. In Poisson subsampling, the participation of an element $x$ in step $i$ is independent of both other elements $y \ne x$ and other steps $j \ne i$. 

%On the other side, in the shuffle model all elements and steps are correlated, since each element can appear in one step and exactly one element appears at each step.

%This leaves two two additional sampling models. The other option which we refer to as \emph{random allocation}, where by each element is allocated in a fixed number of steps independently of the other elements received little attention so far. The $4$ sampling models are summarized in the table below.

% \Arxiv{
% \begin{table}[H] \label{tbl:samp}
%     \begin{tabular}{|l|>{\centering\arraybackslash}m{2cm}|>{\centering\arraybackslash}m{2cm}|}
%         \hline
%         \diagbox[width=2.5cm]{Steps}{Elements} & Independent & Dependent \\
%         \hline
%         Independent & Poisson & Without \newline replacement \\
%         \hline
%         Dependent & Random \newline allocation & Shuffle \\
%         \hline
%     \end{tabular}
% \end{table}
% }
%We prove in Lemma \ref{lem:singElem} that the dominating pair - the couple of dataset that induce the highest privacy loss, contains only a single element. In this special case, the random allocation model where each element is allocated to a single step can be also viewed as an instance of the shuffle model, and so its privacy can be bound using the known privacy guarantees of the shuffle model, as depicted in Figure \ref{fig:main-fig}. Unfortunately, while SOTA results for shuffle scheme of pure-DP mechanisms are tight, approximate-DP mechanisms such as the Gaussian mechanism used in DP-SGD, gain an additional $\ln(1/\delta)$ factor relative to the central model.

%Recently, \citet{CGHLKKMSZ24} considered a special case of random allocation in the context of DP-SGD with a single allocation under the name \emph{bins and balls}. They provided an exact expression for the privacy bounds in this setting which is a special case of Lemma \ref{lem:exctGauss}, and propose several Monte Carlo methods for estimating this term. Unfortunately, their method does not provide provable bounds (only high probability), its runtime increases with the number of steps, it does not provide natural ways to compute $\eps$ or extend to multiple allocations or composition of multiple rounds.

\subsection{Our contribution}
We provide three new analyses for of the random allocation setting that result in provable guarantees that nearly match or exceed those of the Poisson subsampling at rate $k/t$. The analyses rely on different techniques and lead to incomparable numerical results. We describe the specific results below and illustrate the resulting bounds in Fig.~\ref{fig:main-fig}. 



%will now describe these resultsresults are incomparable in terms We will now In p

In our main result we show that the privacy of random allocation is upper bounded by that of the Poisson scheme with sampling probability $\approx k/t$ up to lower order terms which are asymptotically vanishing in $t/k$. Specifically, we upper bound it by the $k$-wise composition of Poisson subsampling with rate $(1+\gamma) k/t$ applied to a dominating pair of distributions for the original algorithm (Def.~\ref{def:domPair})  with an additional $t \delta_0 + \delta'$ added to the $\delta$ parameter. Here,  $\gamma=O\left(e^{\eps_0} \sqrt{\frac{k \ln(k/\delta')}{t}}\right)$ and $\eps_0,\delta_0$ are the privacy parameters of the original algorithm. The formal statement of this result that includes all the constants can be found in Thm.~\ref{thm:asymBnd}.

We note that our result relies on $\eps_0,\delta_0$ parameters of the original algorithm. This may appear to lead to the same overheads as the results based on full shuffling analysis. However in our case these parameters only affect the lower order term, whereas for shuffling they are used as the basis for privacy amplification (Corollary \ref{cor:GaussAsymBnd}).

Our analysis relies on several simplification steps. Given a dominating pair of distributions for the original algorithm, we first derive an explicit dominating pair of distributions for random allocation (extending a similar result for Gaussian noise in \citep{CGHLKKMSZ24}). Equivalently we reduce the allocation for general multi-step adaptive algorithms to the analysis of random allocation for a single (non-adaptive) randomizer on two inputs.  %Therefore it suffices to analyze the divergence for this pair of distributions.
We also analyze only the case of $k=1$ and then use a reduction from general $k$ to $k=1$. This reduction relies on the recent concurrent composition results \citep{lyu22, VZ23}. Finally, our analysis of the non-adaptive randomizer for $k=1$ relies on a decomposition of the allocation scheme into a sequence of posterior sampling steps for which we then prove an upper bound on subsampling probability.

We note that, in general, the privacy of the composition of subsampling of the dominating pair of distributions can be worse than the privacy of the Poisson subsampling. However, all existing analyses of the Poisson sampling are effectively based on composition of subsampling for a dominating pair of distributions. Moreover, if the algorithm has a worst case input for which deletion leads to a dominating pair of distributions then our upper bound can be stated directly in terms of the entire Poisson subsampling scheme. Such {\em dominating input} exists for many standard algorithms including those based on Gaussian and Laplace noise addition.

While our result shows asymptotic equivalence of allocation and Poisson subsampling, it may lead to suboptimal bounds for small values of $t/k$ and large $\eps_0$. We address this using two additional techniques.

We first show that $\eps$ of random allocation with $k=1$ is at most a constant ($\approx 1.6$) factor times larger than $\eps$ of the Poisson sampling with rate $1/t$ for the same $\delta$ (see Theorem \ref{thm:dcmpBnd}). This upper bound does not asymptotically approach Poisson subsampling but applies in all parameter regimes. To prove this upper bound  we observe that Poisson subsampling is essentially a mixture of random allocation schemes with various values of $k$. We then prove a monotonicity property of random allocations showing that increasing $k$ leads to worse privacy. Combining these results with the advanced joint convexity property \cite{BBG18} gives the upper bound.


%We first analyze the general case of arbitrary datasets and mechanisms, prove (Lemma \ref{lem:domElem}) that it suffices to consider a dataset consisting of a single element, and show (Theorem \ref{thm:asymBnd}) that the privacy random allocation of a single step is upper bounded by that of the Poisson scheme with sampling probability $\approx 1/t$ up to lower order terms which are asymptotically vanishing in $t$.

%Next, we turn to consider the case where the worst case privacy loss of the mechanism is achieved by a pair of distributions that are independent of previous steps, which is achieved by many natural mechanisms such as the Gaussian mechanism. Under this setting, an exact expression can be derived (Lemma \ref{lem:exctGauss}), but as mentioned before, it still must be analytically bounded to provide a provable bound.
% We show how this can be done using Azuma-Bernstein inequality (Theorem \ref{thm:BernComp}), but this bound is not tight. 

Finally, we derive a closed form expression for the R\'{e}nyi DP \citep{Mironov17} of the dominating pair of distributions for allocation in terms of the RDP parameters of the original algorithm (Theorem \ref{thm:RDPbnd}). This method has two important advantages. First it gives a precise bound on the RDP parameters of integer order (as opposed to just an upper bound). Secondly, it is particularly easy to use in the typical setting where composition is used in addition to a sampling scheme (for example when $k>1$ or in multi-epoch DP-SGD). The primary disadvantage of this technique is that the conversion from RDP bounds to the regular $(\eps,\delta)$ bounds is known to be somewhat lossy. The same loss is also incurred when Poisson sampling is analyzed via RDP (referred to as moment accounting \citep{ACGMMT16}). The loss is typically within $10-20\%$ range in multi-epoch settings.
In our evaluations of this method for Gaussian distribution in most regimes the resulting bounds are almost indistinguishable from those obtained via RDP for Poisson distribution (see Fig.~\ref{fig:multi-epoch} for examples). In fact, in some regimes
it is better than Poisson sampling (Figure \ref{fig:Monte_carlo}). Two more limitations of this technique result from the restriction to the range $\alpha \ge 2$, and the computational complexity when $\alpha$ is in the high tens.

%This suggests that known privacy bounds that were derived using moments accountant would apply to random allocation   




%which we use to provide a third provable bound on the privacy of the random allocation scheme. This method is typically looser than the others, since RDP analysis is typically slightly looser than PLD based analysis in the case Poisson as well. Still, it has the advantage of providing a bound which does not depend on the comparison to 
%We note \citet{CGKKMSZ24a} conjectured (Conjecture 3.2) that the worst case pair of the Gaussian mechanism in the shuffle model is a pair of datasets consisting of $n-1$ copies of $-1$ followed by a single $1$ or $0$. If this is indeed the case, our first theorem naturally extends to this pair up to a constant factor, thus providing an improved upper bound for the shuffle model as well.

\subsection{Related work}
Our work builds heavily on tools and ideas developed for analysis of privacy amplification by subsampling, composition and shuffling. We have covered the work directly related to ours earlier and will describe some of the tools and their origins in the preliminaries. A more detailed technical and historical overview of subsampling and composition for DP can be found in the survey by \citet{Steinke22}.  The shuffle model was first proposed by \citet{BEMMRLRKTS17}. The formal analysis of the privacy guarantees in this model was initiated in \citep{EFMRTT19, CSUZZ19}.  \citet{EFMRTT19} defined the sequential shuffling scheme that we discuss here and proved the first general privacy amplification results for this scheme albeit only for pure DP algorithms. Improved analyses and extensions to approximate DP were given in \citep{BBGN19, BKMT20, FMT21, FMT23,GDDKS21,KHH22}.

DP-SGD was first defined and theoretically analyzed in the convex setting by \citet{BST14}. Its use in machine learning was spearheaded by the landmark work of 
\citet{ACGMMT16} who significantly improved the privacy analysis via the moments accounting technique and demonstrated the practical utility of the approach. In addition to a wide range of practical applications, this work  has motivated the development of more advanced techniques for analysis of sampling and composition. At the same time most analyses used in practice still assume Poisson subsampling when selecting batches whereas some type of shuffling is used in implementation. It was recently shown that it results in an actual difference between the reported and true privacy level in some regimes \citep{CGKKMSZ24a, CGKKMSZ24b, ABDCH24}.

In a concurrent and independent work \citet{DCO25} considered the same sampling method (referring to it as \emph{Balanced Iteration Subsampling}). They provide RDP-based bounds for the same dominating pair of distributions in the Gaussian case. 
 Their bound for general $k$ is incomparable to ours as it is based on a potentially loose upper bound for divergences of order $\alpha > 2$, while using an exact extension of their approximation to $k > 1$. In contrast, our RDP-based bound uses a reduction from general $k$ to $k=1$ that is potentially loose but our computation for the $k=1$ case is exact. We discuss these differences in more detail and provide numerical comparison in Appendix \ref{apd:comp_loose_RDP}.

%This gap persists throughout most implementations considered since \citep{PHKXDMVCT23}, and was lately proven to result in 
%significant  and progress  

%first proposed DP-SGD, a variant of SGD where at each iteration the sum of (clipped) gradients is released with additional Gaussian noise. Their privacy analysis relies on Poisson subsampling, though - as they pointed out, most implementation use shuffle instead. This gap persists throughout most implementations considered since \citep{PHKXDMVCT23}, and was lately proven to result in an actual difference between the reported and true privacy level in some regimes \citep{CGKKMSZ24a, CGKKMSZ24b, ABDCH24}.

%Random allocation is a sampling version that mostly ignored until lately. It can be seen as a special case of \emph{random check-ins} \citep{BKMT20} with $p_{i} = 1$ and $\resRV_{i} = [t]$ for all users, but the analysis provided in this work essentially relies on the privacy analysis of the shuffle scheme. Recently, \citet{CGHLKKMSZ24} proposed a DP-SGD flavor with this sampling scheme which they refer to as \emph{bins and balls}, but their analysis is limited to the Gaussian mechanism and a single allocation, and does not provide provable bounds but instead relies on a randomized bound \citep{WMWJM24}.


\section{Preliminaries}

%\subsection{Setting} \label{sec:sett}
We denote the domain of \emph{elements} by $\domain$ and the set of possible \emph{outputs} by $\resDom$. We describe a sequence of possibly adaptively chosen algorithms using a randomized algorithm $\mech : \domain^{*} \times \resDom^{*} \rightarrow \resDom$. The input to $\mech$ is a dataset and the sequence of previous results of running $\mech$, that is we run $\mech$ sequentially $t$ times while feeding the sequence of previous outputs as the input to the next execution (in addition to a dataset).
%to be some randomized function, and $t$ - the number of times this function was called. 
We will refer to functions that receive data elements or datasets and produce a single output as \emph{mechanisms}, and to functions that iteratively run some mechanism and output a sequence of outputs as \emph{schemes}. We refer to sequences of outputs as \emph{views} $\view^{t} \coloneqq (\res_{1}, \ldots, \res_{t})$ where $\view^{0} = \emptyset$. We use bold letters ($\view$) to denote sets or sequences, and capital letters ($\resRV$) to denote random variables.

Given an element $x \in \domain$, a view $\view \in \resDom^{*}$, and a output $\res \in \resDom$, we denote by $P_{\mech}(\res \vert x, \view) \coloneqq \underset{\resRV \sim \mech(x, \view)}{\mathbb{P}}(\resRV = \res)$ the probability of observing the output $\res$ as the output of the mechanism $\mech$ which was given element $x$ and view $\view$ as input.\Arxiv{\footnote{In case of measurable spaces, this quantity represents the probability density function rather than the probability mass function}} Similarly, $P_{\alloc{t}{\mech}}(\view \vert \dataset)$ represents the probability to observe $\view$ as the output of the \emph{Random allocation Scheme} (Definition \ref{def:allocScm}) given a dataset $\dataset \in \domain^{*}$ as input, and so on. We omit the subscript when the mechanism (scheme) is clear from the context.

\subsection{Privacy notions}
We consider the \emph{zero-out} adjacency notion \citep{KMSTTX21}, sometimes referred to as \emph{deletion} privacy. To do so, we embed the domain with a ``null'' element $\bot$, and associate it with the empty dataset, such that for any $\dataset \in \domain^{*}$, $\view \in \resDom^{*}$ we have $\mech(\dataset, \view) = \mech((\dataset, \bot), \view)$.
We say two datasets $\dataset, \dataset' \in \domain^{*}$ are \emph{zero-out neighbors} and denote it by $\dataset \simeq \dataset'$, if one of the two can be created by replacing a single element in the other dataset by $\bot$.

We rely on the hockey-stick divergence to quantify the privacy loss. 
\begin{definition}[\Arxiv{Hockey-stick divergence }\cite{BKOZB12}]\label{def:HSdiv}
    Given $\alpha \ge 0$ and two distributions $P, Q$ over some domain $\Omega$, the \emph{hockey-stick divergence} between them is defined to be $\boldsymbol{H}_{\alpha}(P \Vert Q) \coloneqq \underset{\omega \sim Q}{\mathbb{E}} \left[\left[e^{\ell(\omega; P, Q)} - \alpha \right]_{+} \right]$, where $\ell(\omega; P, Q) \coloneqq \ln \left(\frac{P(\omega)}{Q(\omega)}\right)$, $\frac{P(\omega)}{Q(\omega)}$ is the ratio of the probabilities for countable domain or the Radon Nikodym derivative in the continuous case, and $\left[x \right]_{+} \coloneqq \max\{0, x\}$.\Arxiv{\footnote{Despite its name, the hockey-stick divergence is actually not a true divergence under the common definition, since it does not satisfy part of the positivity condition which requires that the divergence is equal to $0$ only for two distributions that are identical almost everywhere, because it is not strictly convex at $1$. This has no effect on our results, since we don't use any claim that is based on this property of divergences.}}
    % We denote by $\boldsymbol{H}_{\alpha}^{\leftrightarrow}(P \Vert Q) \coloneqq \max \left\{\boldsymbol{H}_{\alpha}(P \Vert Q), \boldsymbol{H}_{\alpha}(Q \Vert P) \right\}$.
\end{definition}
When $P, Q$ are distributions induced by neighboring datasets $\dataset, \dataset'$, we refer to the ln probability ratio as the \emph{privacy loss random variable} and denote it by $\ell(\res; \dataset, \dataset')$.

\begin{definition}[Privacy profile \citep{BBG18}]\label{def:privProf}
    Given a mechanism $\mech : \domain^{*} \times \resDom^{*} \rightarrow \resDom$, the privacy profile $\delta_{\mech} : \reals^{+} \rightarrow [0, 1]$ is defined to be maximal hockey-stick divergence between the distributions induced by any query and two neighboring datasets. Formally, 
    \[
        \delta_{\mech}(\eps) \coloneqq \underset{\dataset \simeq \dataset' \in \domain^{*}, \view \in \resDom^{*}}{\sup} \left( \boldsymbol{H}_{e^{\eps}}(\mech(\dataset, \view) \Vert \mech(\dataset', \view)) \right).
    \]
\end{definition}

Another useful divergence notion is the \emph{R\'{e}nyi divergence}.
\begin{definition}[\Arxiv{R\'{e}nyi divergence}]\label{def:renDiv}
    Given $\alpha \ge 1$ and two distributions $P, Q$ over some domain $\Omega$, the \emph{R\'{e}nyi divergence} between them is defined to be $\RenyiDiv{\alpha}{P}{Q} \coloneqq \frac{1}{\alpha-1} \ln \left(\underset{\omega  \sim Q}{\mathbb{E}} \left[e^{\alpha \cdot \ell(\omega; P, Q)} \right] \right)$.\footnote{The cases of $\alpha = 1$ and $\alpha = \infty$ are defined by continuity which results in $\Renyi{1} = \boldsymbol{D}_{KL}$ - the KL divergence, and $\Renyi{\infty} = \boldsymbol{D}_{\infty}$ - the max divergence.}
\end{definition}

Since R\'{e}nyi divergence is effectively a bound on the moment generating function it can be used to bound the hockey-stick divergence which is effectively a tail bound.
\begin{lemma}[\Arxiv{R\'{e}nyi bounds Hockey-stick, }Prop. 12 \citep{CKS20}]
    Given two distributions $P, Q$, if $\RenyiDiv{\alpha}{P}{Q} \le \rho$ then $\delta(\eps) \le \frac{1}{\alpha-1}e^{(\alpha-1)(\rho - \eps)}\left(1 - \frac{1}{\alpha} \right)^{\alpha}$.
\end{lemma}

We can now formally define our privacy notions.

\begin{definition}[Differential privacy \citep{DKMMN06}]
    Given $\eps > 0$; $\delta \in [0, 1 ]$, a mechanism $\mech$ will be called \emph{$(\eps, \delta)$-differentially private (DP)}, if $\delta_{\mech}(\eps) \le \delta$.
\end{definition}

\begin{definition}[R\'{e}nyi differential privacy \citep{Mironov17}]
    Given $\alpha \ge 1$; $\rho > 0$, a mechanism $\mech$ will be called \emph{$(\alpha, \rho)$-R\'{e}nyi differentially private (RDP)}, 
    \[
        \underset{\dataset \simeq \dataset' \in \domain^{*}, \view \in \resDom^{*}}{\sup} \left( \RenyiDiv{\alpha}{\mech(\dataset, \view)}{\mech(\dataset', \view)} \right) \le \rho.
    \]
\end{definition}

One of the most common mechanisms is the Gaussian mechanism, which simply reports the sum of (some function of) the elements in the dataset with an addition of a Gaussian noise.

\Arxiv{

\begin{definition}[Gaussian mechanism]
    Given $d \in \naturals$; $\sigma > 0$, and a query function $q : \domain^{*} \times \resDom^{*} \rightarrow \reals^{d}$, let $\resDom \coloneqq \reals^{d}$. The \emph{Gaussian mechanism} $N_{\sigma}$ is defined as $N_{\sigma}(\dataset, \view) \coloneqq \mathcal{N}(\sum_{x \in \dataset} q(\dataset, \view), \sigma^{2} I_{d})$. We sometimes associate the elements with the vectors for simplicity, when it is clear from the context.
\end{definition}
}
One of the main advantages of the Gaussian mechanism is that we have closed form expressions of its privacy.
\begin{lemma}[\Arxiv{Gaussian mechanism DP guarantees, }\citep{BW18,Mironov17}] \label{lem:gaussPriv}
    Given $\sigma > 0$ and a Gaussian mechanism $N_{\sigma}$, if the range of the query function is the unit ball in $\reals^{d}$, we have $\delta_{N_{\sigma}}(\eps) = \Phi \left(\frac{1}{2 \sigma} - \eps \sigma \right) - e^{\eps} \Phi \left(-\frac{1}{2 \sigma} - \eps \sigma \right)$, where $\Phi$ is the CDF of the standard Normal distribution. Further,  for any $\alpha \ge 1$ $N_{\sigma}$ is $(\alpha, \alpha / (2 \sigma^{2})$-RDP.
\end{lemma}

\subsection{Schemes of interest}
We now formally define \emph{Poisson subsampling}, \emph{shuffling} and \emph{random allocation} schemes.%, which is parametrized by a number of steps $t \in \naturals$ and a sampling probability $\lambda \in [0, 1]$. Given a dataset $\dataset \in \domain^{*}$, it samples $t$ subsets $\dataset^{i} \subseteq \dataset$ using Poisson subsampling with parameter $\lambda$.

\begin{definition}[\Arxiv{Poisson subsampling scheme}] \label{def:PoisScm}
     A \emph{Poisson scheme} is a function $\Pois{t, \lambda}{\mech} : \domain^{*} \rightarrow \resDom^{t}$ parametrized by a mechanism $\mech : \domain^{*} \times \resDom^{*} \rightarrow \resDom$, a sampling probability $\lambda \in [0, 1]$, and number of steps $t \in \naturals$, which given a dataset $\dataset \in \domain^{*}$ samples $t$ subsets using Poisson sampling where each element is added to the subset with probability $\lambda$ independent of the other elements, and sequentially returns $\res_{i} = \mech \left(\dataset^{i}, \view^{i - 1} \right)$.
\end{definition}

%This sampling method has two layers of independency. The probability that an element $x \in \dataset$ will be part of a subset $\dataset^{i}$ is independent of the probability that a different element $y \ne x$ is part of that subset, and also independent of the probability that the same element is part of a different subset $\dataset^{j}$, $j \ne i$. 

%A common alternative sampling scheme is the shuffle model, where the participation of an element $x$ in subset $i$ is correlated both to other elements and other steps.
\begin{definition}[\Arxiv{Shuffling scheme}] \label{def:shuff}
     A \emph{shuffling scheme} is a function $\shuf{n}{\mech} : \domain^{n} \rightarrow \resDom^{n}$ parametrized by a mechanism $\mech : \domain^{*} \times \resDom^{*} \rightarrow \resDom$ and number of steps $n \in \naturals$, which given a dataset $\dataset \in \domain^{n}$ uniformly samples a permutation $\pi$ over $[n]$, and sequentially returns $\res_{i} = \mech \left(s_{\pi(i)}, \view^{i - 1} \right)$.    
\end{definition}

%This begs the question, what happens if only one of the two relations is independent. The case of \emph{sampling without replacement} where each subset $\dataset^{i}$ is uniformly sampled out of all possible subsets of some fixed size $k$, was discussed in several works \citep{BBG18}. In this setting, the sampling of each element at a given step depends on other elements, but independent of the sampling in other steps.

%We consider the alternative option of \emph{random allocation}, which is a general version of the \emph{bins and balls} scheme recently proposed by \citet{CGHLKKMSZ24}, where each element uniformly samples $k$ subsets out of $t$ to which it will be added, independently of other elements.

\begin{definition}[\Arxiv{Random allocation scheme}]\label{def:allocScm}
     A \emph{random allocation scheme} is a function $\alloc{t, k}{\mech} : \domain^{*} \rightarrow \resDom^{t}$ parametrized by a  mechanism $\mech$, a number of steps $t$, and a number of selected steps $k \in [t]$, which given a dataset $\dataset$ uniformly samples $k$ indices $\boldsymbol{i} = (i_{1}, \ldots, i_{k}) \subseteq [t]$ for each element, adds it to the corresponding subsets $\dataset^{i_{1}}, \ldots, \dataset^{i_{k}}$, and sequentially returns $\res_{i} = \mech \left(\dataset^{i}, \view^{i - 1} \right)$.

     When $k = 1$ we omit it from the notation for clarity.
\end{definition}

\subsection{Dominating pair of distributions}
As mentioned before, DP is defined as the supremum of the hockey-stick divergence over distributions induced by neighboring datasets (and past views), but in the general case, this supremum might be achieved by different datasets for different values of $\eps$. Fortunately, some mechanisms have a \emph{dominating pair} of datasets, neighboring datasets which induce the largest divergence for all $\eps$. %In such cases, the analysis of the schemes can be significantly simplified, by limiting our focus to repeated calls to these specific datasets.

\begin{definition}[\Arxiv{Dominating Pair } \citep{ZDW22}] \label{def:domPair}
    Given distributions $P, Q$ over some domain $\Omega$, and $P', Q'$ over $\Omega'$, we say $(P, Q)$ \emph{dominate} $(P', Q')$ if for all $\alpha \ge 0$ we have $\boldsymbol{H}_{\alpha}(P' \Vert Q') \le \boldsymbol{H}_{\alpha}(P \Vert Q)$.\Arxiv{\footnote{The $\alpha \in [0, 1]$ regime does not correspond to useful values of $\eps$, but yet is crucial for the following guarantees, as demonstrated by \citet{LRKS24}.}} If $\delta_{\mech}(\eps) \le \boldsymbol{H}_{e^{\eps}}(P \Vert Q)$ for all $\eps \in \reals$, we say $(P, Q)$ is a \emph{dominating pair} of distributions for $\mech$. If the inequality can be placed by an equality for all $\eps$, we say it is a \emph{tightly dominating pair}. If there exist some $\dataset \simeq \dataset' \in \domain^{*}$ such that $P = \mech(\dataset)$, $Q = \mech(\dataset')$ we say $(\dataset, \dataset')$ are the the dominating pair of datasets for $\mech$. By definition, a dominating pair of input datasets is tightly dominating. If the mechanism additionally receives a view as input, then dominating pair of distributions is not defined by a pair of datasets, but instead a pair of datasets accompanied by a view $\view$, such that $P = \mech(\dataset, \view)$, $Q = \mech(\dataset', \view)$. %In this work we will only consider dominating pair where $\view = \emptyset$, and will omit it from the notations for simplicity.
\end{definition}


% All mechanisms have a tightly dominating pair of distributions , but not necessarily dominating pair of datasets. Consequently, we can always use the dominating pair to upper bound the privacy profile, but this bound will be tight only if this dominating pair of distributions is induced by datasets.

We use the notion of dominating pair to define a dominating randomizer, which captures the privacy guarantees of the mechanism independently of its algorithmic adaptive properties. 


\begin{definition}[\Arxiv{Dominating randomizer}]\label{def:domRand}
    Given a mechanism $\mech$, we define a new \emph{randomizer} $\rand : \{\bot, *\} \rightarrow \resDom$ and say that $\mech$ is dominated by $\rand$, where $*$ is a symbol representing the randomizer getting access to some data, while $\bot$ represents the case where it got an empty set, and set $\rand(*) = P$, $\rand(\bot) = Q$ where $P, Q$ is the dominating pair of $M$.\footnote{This pair always exists \citep[Proposition 8]{ZDW22}.}
\end{definition}

Notice that the $\bot$ element of $\rand$ might differ from that of $\mech$, e.g., in the case of the Gaussian mechanism $N_{\sigma}$ the $\bot$ element w.r.t. $\mech$ is $\bar{0} \in \reals^{d}$ while the $\bot$ element w.r.t. $\rand$ is $0$ (Claim \ref{clm:domPairGauss}). We also note that domination is defined w.r.t. the zero-out adjacency notion. When using the add-remove notion, the dominating pair for add and remove might differ, in which case a tighter analysis can be achieved by considering both pairs separately.

From the definition, the privacy profile of $\mech$ is upper bounded by that of the $\rand$, and equality is achieved only of $\mech$ has a dominating pair of datasets. When it comes to schemes, it might be the case that even if $\mech$ has a dominating pair of datasets, this pair does not dominate the Poisson or allocation schemes defined by this mechanism, and in fact such pair might not exist. For example, while the Gaussian mechanism is dominated by the pair $(1, 0)$ (Claim \ref{clm:domPairGauss}), the DP-SGD algorithm \citep{ACGMMT16} which is essentially a Poisson scheme using the Gaussian mechanism might not have any dominating pair of datasets, which achieves the maximal divergence for all iterations. Since most state-of-the-art bounds currently used rely on the properties of the randomizer rather than leveraging the properties of the specific algorithm, this gap does not affect our privacy bounds.

% \begin{definition}[Uniformly dominating pair]\label{def:uniDomPair}
    
% \end{definition}
 
%     If $\mech$ additionally receives some view as an input, we say $(\dataset, \dataset')$ are \emph{uniformly dominating pair} of $\mech$ if $P = \mech(\dataset, \emptyset)$, $Q = \mech(\dataset', \emptyset)$ are a dominating pair of $\mech(\cdot, \view)$ for any $\view \in \resDom^{*}$.

An important property of domination is its equivalence to existence of postprocessing.
%Namely, there exists a randomized mapping $\varphi$ such that $\mech ' = \varphi \circ \mech$, then the privacy profile of $\mech '$ is bounded by that of $\mech$.
%Dominating pairs enjoy several useful properties.
\begin{lemma}[\Arxiv{Post processing, }Thm.~II.5 \citep{KOV15}]\label{lem:domPostProc}
    Given distributions $P, Q$ over some domain $\Omega$, and $P', Q'$ over $\Omega'$, $(P, Q)$ dominate $(P', Q')$ if and only if there exists a randomized function $\varphi : \Omega \rightarrow \Omega'$ such that $P' = \varphi(P)$ and $Q' = \varphi(Q)$.
\end{lemma}

We note that an alternative way to frame our results is using the local randomizer perspective used in the privacy analysis of shuffling (e.g.~\citep{EFMRTT19}). In this perspective, the local randomizer $\rand$ is fixed first and the goal is to analyze the privacy of the sequence of $t$ applications of $\rand$, where the given data element $x$ is used as an input in randomly chosen $k$ steps and $\bot$ is used as an input in all the other steps (with view being an additional input in the adaptive case). Our analysis corresponds more naturally to this local perspective. The definition of the dominating randomizer effectively allows us to reduce the central setting to the local one.

\section{Asymptotic bound} \label{sec:asym}
Roughly speaking our main theorem states that random allocation is asymptotically identical to the Poisson scheme with sampling probability $\approx k/t$ up to lower order terms. We do so by first bounding Poisson and allocation schemes using a pair of datasets containing a single element, then use this bound to prove the theorem for $k=1$, and finally describe a general reduction from general case to $k=1$.
Formal proofs and missing details of this section can be found in Appendix \ref{apd:asym}.

\subsection{Reduction to randomizer}
From the definition, if a mechanism $\mech$ is dominated by a randomizer $\rand$, for any $\eps \in \reals$ we have $\delta_{\mech}(\eps) \le \delta_{\rand}(\eps)$. We now prove that this is also the case for allocation scheme, that is $\delta_{\alloc{t, k}{\mech}}(\eps) \le \delta_{\alloc{t, k}{\rand}}(\eps)$, and that the supremum over neighboring datasets for $\alloc{t, k}{\rand}$ is achieved by the pair of datasets $\dataset = \{*\}$, $\dataset' = \{\bot\}$, so we can limit our analysis to this case. This results from he fact random allocation can be viewed as a two steps process, where first all elements but one are allocated, then the last one is allocated and the mechanism is ran for $t$ steps. From the convexity of the hockey-stick divergence we can upper bound the privacy profile of the random allocation scheme by the worst case allocation of all elements but the last one, from Lemma \ref{lem:domPostProc}, this profile is upper bounded by a sampling scheme over $P, Q$, and from the definition of $*, \bot$ this is achieved by these two elements.

\begin{lemma}\label{lem:singElem}
    Given $t \in \naturals$; $k \in [t]$, and a mechanism $\mech$ dominated by a randomizer $\rand$, for any $\eps > 0$ we have
    \[
        \delta_{\alloc{t, k}{\mech}}(\eps) \le \delta_{\alloc{t, k}{\rand}}(\eps) = \boldsymbol{H}_{e^{\eps}} \left(\allocFunc{t, k}{\rand}{*} \Vert \rand^{t}(\bot) \right). \footnote{Notice that $\allocFunc{t, k}{\rand}{\bot} = \rand^{t}(\bot)$, where $\rand^{t}(\bot)$ denotes $t$ consecutive calls to $\rand$.}
    \]
\end{lemma}
A special case of this result for Gaussian noise addition and $k=1$ was given by \citet[Theorem 1]{CGHLKKMSZ24}, and in the context of the matrix mechanism by \citet[Lemma 3.2]{CCGST23}. For this special case, \citet{CGHLKKMSZ24} give several Monte Carlo simulation based techniques to evaluate the privacy parameters. We include a brief discussion of this approach in Appendix \ref{sec:monte-carlo}. 
The same bound for the Poisson scheme is a direct result from the combination of Claim \ref{clm:domPairComp} and \citet[Theorem 11]{ZDW22}.

\Arxiv{
\begin{proof}
    Notice that for any dataset $\dataset \in \domain^{n}$ and elements $x, y \in \domain$ where either $x = \bot$ or $y = \bot$, the random allocation scheme $\allocFunc{t, k}{\mech}{(\dataset, x)}$ can be decomposed into two steps. First all elements in $\dataset$ are allocated, then $x$ is allocated and the outputs are sampled based on the allocations. Given any two neighboring datasets $(\dataset, x)$, $(\dataset, y)$, denote by $\boldsymbol{a}^{t, k}(n)$ the set of all possible allocations of $n$ elements into $k$ out of $t$ steps, and for any $a \in \boldsymbol{a}^{t, k}(n)$ let $\mathcal{A}_{t, k}^{a}(\mech; (\dataset, x))$ denote the allocation scheme conditioned on the allocation of $\dataset$ according to $a$. Using these notations and the quasi-convexity of the hockey-stick divergence we get,
    \begin{align*}
        \boldsymbol{H}_{\alpha} & \left(\allocFunc{t, k}{M}{(\dataset, x)} \Vert \allocFunc{t, k}{M}{(\dataset, y)} \right) 
        \\ & = \boldsymbol{H}_{\alpha}\left( \sum_{a \in \boldsymbol{a}^{t, k}(n)} P(a) \mathcal{A}_{t, k}^{a}(\mech; (\dataset, x)) \Vert \sum_{a \in \boldsymbol{a}^{t, k}(n)} P(a) \mathcal{A}_{t, k}^{a}(\mech; (\dataset, y)) \right) \\
        % & \le \sum_{a \in \boldsymbol{a}^{t, k}(n)} P(a) \cdot \boldsymbol{H}_{\alpha}\left( \mathcal{A}_{t, k}^{a}(\mech; (\dataset, x)) \Vert \mathcal{A}_{t, k}^{a}(\mech; (\dataset, y)) \right) \\
        & \le \underset{a \in \boldsymbol{a}^{t, k}(n)}{\max} \boldsymbol{H}_{\alpha}\left( \mathcal{A}_{t, k}^{a}(\mech; (\dataset, x)) \Vert \mathcal{A}_{t, k}^{a}(\mech; (\dataset, y)) \right).
    \end{align*}

    From the definition of the dominating pair and of $*$, $\bot$, for any $\alpha \ge 0$, index $i \in [t]$, allocation of $\dataset$ to $\dataset_{i}$, and view $\view_{i-1}$ we have
    \[
        \boldsymbol{H}_{\alpha}\left(\mech((\dataset_{i}, x), \view_{i-1}) \Vert \mech((\dataset_{i}, y), \view_{i-1}) \right) \le \boldsymbol{H}_{\alpha}\left(\rand(*) \Vert \rand(\bot) \right),
    \]
    so from Lemma \ref{lem:domPostProc}, there exists a mapping $\varphi$ which depends on $\dataset_{i}, x, y, \view_{i-1}$ such that $\mech((\dataset_{i}, x), \view_{i-1}) = \varphi(\rand(*))$ and $\mech((\dataset_{i}, y), \view_{i-1}) = \varphi(\rand(\bot))$. Sequentially applying $\varphi$ to the output of the allocation scheme implies $\allocFunc{t, k}{M}{(\dataset_{i}, x)} = \varphi(\allocFunc{t, k}{\rand}{*})$ and $\allocFunc{t, k}{M}{(\dataset_{i}, y)} = \varphi(\allocFunc{t, k}{\rand}{\bot})$. By invoking Lemma \ref{lem:domPostProc} again this implies the distributions pair $(\allocFunc{t, k}{\rand}{*}, \allocFunc{t, k}{\rand}{\bot})$ dominates $\alloc{t, k}{\mech}$.
\end{proof}
}

We note that the definition of the randomizer can be slightly tightened by considering a separate dominating pair $P_{\view}, Q_{\view}$ for any past view $\view$, and defining an adaptive randomizer $\rand(*, \view) = P_{\view}$, $\rand(\bot, \view) = Q_{\view}$. This will not affect the results of this section, but the proof of Lemma \ref{lem:allocMontn} and Theorem \ref{thm:RDPbnd} do relay on the fact that all randomizers are identical. Since current analysis of Poisson scheme do not leverage potential improvements resulting from the dependence on the views, we use the simpler version for brevity.

\subsection{Randomizer privacy bound}

We can now turn to prove the main theorem.

\begin{theorem} \label{thm:asymBnd}
    Given $\eps_{0} > 0$; $\delta_{0} \in [0, 1]$ and a $(\eps_{0}, \delta_{0})$-DP mechanism $\mech$ dominated by a randomizer $\rand$, for any $\eps, \delta > 0$ we have $\delta_{\alloc{t}{\mech}}(\eps) \le \delta_{\Pois{t, \eta}{\rand}}(\eps) + t \delta_{0} + \delta$,
    where $\eta \coloneqq \min \left\{\frac{1}{t(1-\gamma)}, 1 \right\}$ and $\gamma \coloneqq \min \left\{\cosh(\eps_{0}) \cdot \sqrt{\frac{2}{t} \ln \left(\frac{1}{\delta} \right)}, 1 \right\}$.
\end{theorem}

Since $\gamma = \Theta(1/\sqrt{t})$ and $\frac{1}{1-x} \approx 1+x$ for $x \ll 1$, the sampling probability is $\frac{1}{t}$ up to a lower order term in $t$, which implies the random allocation scheme is asymptotically bounded by the Poisson scheme.

The proof of this theorem consists of a sequences of reductions, which we will prove in the following lemmas.

Following \citep{EFMRTT19}, we start by introducing the posterior sampling scheme, where the sampling probability depends on the previous outputs. 
\begin{definition}[Posterior probability and scheme] \label{def:postSchem}
    Given a subset size $k \in [t]$, an index $i \in \left[t-1 \right]$, an element $x \in \domain$, a view $\view^{i} \in \resDom^{i}$, and a mechanism $\mech$, the $i+1$ \emph{posterior probability} of the $k$ allocation out of $t$ given $\view^{i}$ is the probability that the index $i+1$ was one of the $k$ steps chosen by the random allocation scheme, given that the view $\view^{i}$ was produced by the first $i$ rounds of $\allocFunc{t}{\mech}{x}$. Formally, $\lambda_{\view^{i}, k, x} \coloneqq P_{\allocFunc{t, k}{\mech}{x}}\left(i+1 \in \boldsymbol{I} \vert x, \view^{i} \right)$, where $\boldsymbol{I}$ is the subset of chosen steps.

    The \emph{posterior scheme} is a function $\post{t, k}{\mech}: \domain \rightarrow \resDom^{t}$ parametrized by a mechanism $\mech$, number of steps $t$, and number of selected steps $k$, which given an element $x \in \domain$, sequentially samples
    \[
        \res_{i+1} \sim \left(\lambda_{\view^{i}, k, x} \cdot \mech(x, \view^{i}) + (1-\lambda_{\view^{i}, k, x}) \cdot \mech(\bot, \view^{i}) \right), 
    \]
    where $\lambda_{\view^{0}, k, x} = k/t$. As before, we omit $k$ from the notations where $k=1$.
\end{definition}

Notice that the probabilities $\lambda_{\view^{i}, k, x}$ are data dependent, and so cannot be considered public information during the privacy analysis.

Though this scheme seems like a variation of the Poisson scheme, the following lemma shows that in fact its output is distributed like the output of random allocation.% This lemma can be viewed as a generalization of the resampling lemma by \citet{JLNRSS19}, and similar argument can be found in the analysis of the shuffle scheme by \citet{EFMRTT19}.
\begin{lemma} \label{lem:postEqAlloc}
    For any subset size $k \in [t]$, element $x \in \domain$, and mechanism $\mech$ dominated by a randomizer $\rand$, $\allocFunc{t, k}{\mech}{x}$ and $\postFunc{t, k}{\mech}{x}$ are identically distributed, which implies $\delta_{\alloc{t, k}{\rand}}(\eps) = \delta_{\post{t, k}{\rand}}(\eps)$ for any randomizer and all $\eps \ge 0$.
\end{lemma}

The crucial difference between these two schemes is the fact that unlike random allocation, the distribution over the outputs of any step of the posterior scheme is independent of the distribution over output of previous steps given the view and the dataset, since there is no shared randomness (such as the chosen allocation).

Next we define a truncated variant of the posterior distribution and use it to bound its privacy profile.
\begin{definition}\label{def:trncPostScm}
    The \emph{truncated posterior scheme} is a function $\post{t, k, \eta}{\mech}: \domain \rightarrow \resDom^{t}$ parametrized by a mechanism $\mech$, number of steps $t$, number of selected steps $k$, and threshold $\eta \in [0, 1]$, which given an element $x \in \domain$, sequentially samples
    \[
        \res_{i+1} \sim \left(\lambda_{\view^{i}, k, x}^{\eta} \cdot \mech(x, \view^{i}) + (1-\lambda_{\view^{i}, k, x}^{\eta}) \cdot \mech(\bot, \view^{i}) \right), 
    \]
    where $\lambda_{\view^{i}, k, x}^{\eta} \coloneqq \min \{\lambda_{\view^{i}, k, x},  \eta \}$.
\end{definition}
%VITALY: moved k into subscript

We can now bound the difference between the privacy profile of the truncated and original posterior distributions, by the probability that the posterior sampling probability will exceed the truncation threshold. A similar general result combining the next two lemmas was recently proven in an previous work \citep[Theorem 3.1]{CCGST23}.
\begin{lemma}\label{lem:truncPostBndPost}
    Given a randomizer $\rand$, for any $\eta \in [0,1]$; $\eps > 0$ we have
    \[
        \delta_{\post{t, k}{\rand}}(\eps) \le \delta_{\post{t, k, \eta}{\rand}}(\eps) + \beta_{\alloc{t, k}{\rand}} (\eta).
    \]
    where $\beta_{\alloc{t, k}{\rand}} (\eta) \coloneqq P_{\allocFunc{t, k}{\rand}{*}} \left(\mathcal{B}_{\eta}^{t, k} \right)$
    and $\mathcal{B}_{\eta}^{t, k} \coloneqq \left\{\view \in \resDom^{t} ~ \big \vert ~ \underset{i \in [t]}{\max}(\lambda_{\view^{i-1}, k, *}) > \eta  \right\}$.
\end{lemma}

The privacy profile of the truncated posterior scheme can be bounded by the privacy profile of the Poisson scheme, using the fact the privacy loss is monotonically increasing in the sampling probability.

\begin{lemma} \label{lem:truncBndPois}
    Given $k \in [t]$; $\eta \in [0, 1]$ and a randomizer $\rand$, for any $\eps > 0$ we have $\delta_{\post{t, k, \eta}{\rand}}(\eps) \le \delta_{\Pois{t, \eta}{\rand}}(\eps).$
    % \[
    %     \boldsymbol{H}_{e^{\eps}}(\postFunc{t, k, \eta}{\rand}{*} \Vert \rand^{t}(\bot)) \le \delta_{\Pois{t, \eta}{\rand}}(\eps).
    % \]
\end{lemma}

The only remaining task is to bound $\beta_{\alloc{t, k}{\rand}}(\eta)$, the probability that the posterior sampling probability will exceed $\eta$. We do so in two stages. First we reduce the analysis of general approximate-DP mechanisms to that of pure-DP ones, paying an additional $t \delta_{0}$ term in the probability.
\begin{lemma}\label{lem:approxToPure}
    Given $\eps_{0} > 0$; $\delta_{0} \in [0, 1]$ and a $(\eps_{0}, \delta_{0})$-DP randomizer $\rand$, there exists a randomized $\hat{\rand}$ which is $\eps_{0}$-DP, such that $\beta_{\alloc{t, k}{\rand}}(\eta) \le \beta_{\alloc{t, k}{\hat{\rand}}}(\eta) + t \delta_{0}$, where $\beta_{\alloc{t, k}{\rand}}(\eta)$ was defined in Lemma \ref{lem:truncPostBndPost}.
\end{lemma}

Finally, we prove that with high probability over the generated view, the random allocation scheme of the pure-DP mechanism will not produce a ``bad'' view, one that induce a posterior sampling probability exceeding $\eta$.
\begin{lemma} \label{lem:postProbBnd}
    Given $\eps_{0}, \gamma \ge 0$, an element $x \in \domain$, and a $\eps_{0}$-DP mechanism $\mech$, for any $\delta \ge 0$ we have
    \[
        \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\lambda_{\viewRV, x} > \frac{1}{t(1-\gamma)} \right) < \exp \left(- \frac{t \gamma^{2} }{2 \cosh^{2}(\eps)} \right).
    \]
\end{lemma}

Putting it all together completes the proof of the main theorem. 
\begin{proof} [Proof of Theorem \ref{thm:asymBnd}]
    \begin{align*}
		\delta_{\alloc{t}{\mech}}(\eps) & \overset{(1)}{\le} \delta_{\alloc{t}{\rand}}(\eps) \\
        & \overset{(2)}{=} \delta_{\post{t}{\rand}}(\eps) \\
		& \overset{(3)}{\le} \delta_{\post{t, \eta}{\rand}}(\eps) + \beta_{\alloc{t}{\rand}}(\eta) \\
		& \overset{(4)}{\le} \delta_{\Pois{t, \eta}{\rand}}(\eps) + \beta_{\alloc{t}{\rand}}(\eta) \\
		& \overset{(5)}{\le} \delta_{\Pois{t, \eta}{\rand}}(\eps) + t \delta_{0} + \beta_{\alloc{t}{\hat{\rand}}}(\eta) \\
		& \overset{(6)}{\le} \delta_{\Pois{t, \eta}{\rand}}(\eps) + t \delta_{0} + \delta \\
	\end{align*}
	where (1) results from Lemma \ref{lem:singElem}, (2) from Lemma \ref{lem:postEqAlloc}, (3) from Lemma \ref{lem:truncPostBndPost} where $\beta_{\alloc{t, k}{\rand}}(\eta)$ was defined, (4) from Lemma \ref{lem:truncBndPois}, (5) from Lemma \ref{lem:approxToPure}, and (6) from Lemma \ref{lem:postProbBnd} and the definition of $\gamma$.
\end{proof}

\Arxiv{
\begin{remark}
    Repeating the previous lemmas while changing the direction of the inequalities and the sign of the lower order terms, we can similarly prove that the random allocation scheme upper bounds the Poisson scheme up to lower order terms, which implies they are asymptotically identical.
\end{remark}
}

\subsection{Asymptotic analysis}
% \paragraph{General $k$:}
So far we considered only the case where the number of selected allocations $k = 1$, we now show how this bound naturally extends to the case of $k > 1$.

\begin{lemma}\label{lem:comp}
    Given $k \in \naturals$ and a mechanism $\mech$, for any $\eps > 0$ we have $\delta_{\alloc{t, k}{\mech}}(\eps) \le \delta_{\alloc{\lfloor t/k \rfloor}{\mech}}^{\otimes k}(\eps)$, where ${\otimes k}$ denotes the composition of $k$ runs of the mechanism or scheme which in our case is $\alloc{\lfloor t/k \rfloor}{\mech}$.
\end{lemma}

\Arxiv{
\begin{proof}
    Notice that the random allocation of $k$ indexes out of $t$ can be described as a two steps process, first randomly splitting $t$ into $k$ subsets of size $t/k$, \footnote{For simplicity we assume that $t$ is divisible by $k$.} then running $\alloc{t/k, 1}{\mech}$ on each of of the $k$ copies of the scheme. Using the same convexity argument as in the proof of Lemma \ref{lem:singElem}, the privacy profile of $\alloc{t, k}{\mech}$ is upper bounded by the composition of $k$ copies of $\alloc{t/k, 1}{\mech}$. Since the rounds of the various copies of the scheme are interleaved, this setting does not match the typical sequential composition, but can be modeled using concurrent composition \citep{VW21}, where the ``adversary'' is simultaneously interacting with all schemes, which was recently proven to provide the same privacy guarantees \citep{lyu22, VZ23}.    
\end{proof}
}

Combining this lemma with Theorem \ref{thm:asymBnd} leads to the next corollary.

\begin{corollary} \label{cor:asymBnd}
    Given $\eps_{0} > 0$; $\delta_{0} \in [0, 1]$ and a $(\eps_{0}, \delta_{0})$-DP mechanism $\mech$ dominated by a randomizer $\rand$, for any $\eps, \delta > 0$ we have $\delta_{\alloc{t, k}{\mech}}(\eps) \le \delta_{\Pois{t, \eta}{\rand}}(\eps) + t \delta_{0} + \delta$,
    where $\eta \coloneqq \min \left\{\frac{k}{t(1-\gamma)}, 1 \right\}$ and $\gamma \coloneqq \min \left\{\cosh(\eps_{0}) \cdot \sqrt{\frac{2 k}{t} \ln \left(\frac{k}{\delta} \right)}, 1 \right\}$.

    Furthermore, setting $\delta_{0} = \delta / t$, for any $\sigma > 8 \cdot \max \left\{\sqrt{\ln(t/\delta)}, \sqrt{\frac{k}{t}} \ln(t/\delta) \right\}$, we have $\delta_{\alloc{t, k}{N_{\sigma}}}(\eps) \le \delta_{\Pois{t, 2 k / t}{N_{\sigma}}}(\eps) + 2 \delta$, where $N_{\sigma}$ is the Gaussian mechanism.
\end{corollary}

% When combining this reduction with Theorem \ref{thm:asymBnd} we obtain a composition of a total of $t$ Poisson subsampling steps. We can ensure that truncation happens with probability at most $\delta$ by setting truncation probability to $\delta/k$ in each allocation. Thus the sampling probability of each Poisson step is $\frac{k}{(1-\gamma) t}$ for $\gamma \coloneqq \cosh(\eps_{0}) \cdot \sqrt{\frac{2k}{t} \ln \left(\frac{k}{\delta} \right)}$.


Extending Theorem \ref{thm:asymBnd} to directly account for allocation of $k$ steps might improve some lower order terms, but requires a more involved version of Lemma \ref{lem:postProbBnd}, specifically \ref{clm:postExp} on which its proof relies. We leave this for future work.

Our results in Corollary \ref{cor:asymBnd} allow to derive asymptotic bounds on the privacy guarantees of Gaussian noise addition amplified by random allocation. We start by recalling the asymptotic bounds for the Poisson scheme due to \citet{ACGMMT16}.\footnote{This is a variant of \citet[Theorem 1]{ACGMMT16} that is better suited for comparison. We prove this version in Appendix \ref{apd:asym}.} 
\begin{lemma}[\citep{ACGMMT16}] \label{lem:poisPriv}
    There exists constants $c_{1}, c_{2} > 0$ such that for any $t \in \naturals$; $\lambda \in [0, 1/16]$; $\delta \in [0,1]$, if $t \ge \ln(1/\delta)$ and $\sigma > \max\left\{1, c_{1} \frac{\sqrt{\ln(1/\delta})}{\lambda \sqrt{t}} \right\}$ then the Poisson scheme with the Gaussian mechanism  $\Pois{t, \lambda}{N_{\sigma}}$ is $(\eps, \delta)$-DP for any $\eps \ge c_{2} \max\left\{\frac{\lambda \sqrt{t \cdot \ln(1/\delta)}}{\sigma}, \lambda^{2} \sqrt{t \cdot \ln(1/\delta)} \right\}$.
\end{lemma}

This is a direct result of the fact the Gaussian mechanism is dominated by the one-dimensional Gaussian randomizer (Claim \ref{clm:domPairGauss}) where $\rand(*) = \mathcal{N}(1, \sigma^{2})$ and $\rand(\bot) = \mathcal{N}(0, \sigma^{2})$. Combining this Lemma with the second part of Corollary \ref{cor:asymBnd} implies a similar result for the random allocation scheme.
\begin{corollary}\label{cor:GaussAsymBnd}
	There exist constants $c_{1}, c_{2}$ such that for any $t \in \naturals$; $k \in [t/16]$; $\delta \in [0, 1]$; if    
    \[
        \sigma \ge c_{1} \cdot \max \left\{\sqrt{\ln(t/\delta)}, \sqrt{\frac{k}{t}} \ln(t/\delta), \frac{\sqrt{t \cdot \ln(1/\delta) \cdot \ln(t/k)}}{k} \right\},
    \]
    then the random allocation scheme with the Gaussian mechanism  $\alloc{t, k}{N_{\sigma}}$ is $(\eps, \delta)$-DP for any $\eps \ge c_{2} \max\left\{\frac{k \sqrt{\ln(1/\delta)}}{\sigma \sqrt{t}}, \frac{k^{2} \sqrt{\ln(1/\delta)}}{t^{1.5}}\right\}$.
\end{corollary}

We note that the dependence of $\eps$ on $\sigma$; $\delta$; $k$; and $t$ matches that of the Poisson scheme for $\lambda = k / t$ up to an additional logarithmic dependence on $t$, unlike the the shuffle scheme which acquire an additional $\sqrt{\ln(1/\delta)}$ by converting approximate the DP mechanism to pure DP first, resulting in the bound $\eps \ge c_{1} \frac{k \cdot \ln(1/\delta)}{\sigma \sqrt{t}}$ \citep{FMT21}. The second term in the bound on $\eps$ is due to the privacy profile of the Poisson scheme, and applies only in the uncommon regime when $\sigma > t/k$. One important difference between the privacy guarantees of the Poisson and random allocation schemes is in the bounds on $\sigma$, which are stricter for random allocation in the $k > \sqrt{t}$ regime (Remark \ref{rem:asymComp}).

\section{Non-asymptotic bounds}\label{sec:nonAsym}
While Theorem \ref{thm:asymBnd} provides a full asymptotic characterization of the random allocation scheme, the bounds it induces is vacuous for small $t$ or large $\eps_{0}$. In this section we provide two additional bounds that hold in all parameters regime. Formal proofs and missing details of this section can be found in Appendix \ref{apd:nonAsym}.

% % \section{Analyses via a dominating pair}\label{sec:DomPair}
% %While Theorem \ref{thm:asymBnd} provides a full asymptotic characterization of the random allocation scheme, the bounds it induces undergoes a sharp phase transition at $\eps_{0} \approx 1$, and become vacuous in the regime where the number of steps $t \le 2 e^{2 \eps_{0}} \ln (1/\delta)$.
% % , which in the case of the Gaussian mechanism occurs when $t \approx 2 \exp \left(\frac{2\sqrt{2 \ln(t/\delta_{0})}}{\sigma}\right) \cdot \ln (1/\delta)$.

% In this section we analyze the privacy profile of the random allocation scheme for mechanisms $\mech$ using a (tightly) dominating pair of distributions for this mechanism. For this purpose we define a non-adaptive (that is independent of the view) mechanism $N$ that outputs the tightly dominating pair of distributions for $\mech$ (on all views and inputs).
% \begin{definition}
%     For a mechanism $\mech : \domain \times \resDom^{*} \rightarrow \resDom$ we define a tightly dominating mechanism $N: \{x^{\star},\bot\} \rightarrow \resDom$ for $\mech$ to be the mechanism whose pair of output distributions $(N(*),\rand(\bot))$ are a tightly dominating pair of distributions for $\mech$.  
% \end{definition}
% For many mechanisms $\mech$, a tightly dominating mechanism is just a special case of running $\mech$ on some input and thus $N$ is a special case of $\mech$. This includes Laplace and Gaussian noise addition mechanisms, where the $\bot$ element corresponds to the $0$ vector and $*$ is the $\ell_1$/$\ell_2$ sensitivity of the value passed to the noise addition mechanism. 

% %The mechanism $N$ can be less private 
% %such that for some input $*$, the pair of distributions $N(*)$ and $\rand(\bot)$ dominate the pair of distributions $\mech(x,\view)$ and $\mech(\bot,\view)$ for any input $x$ and view $\view$. We refer to such $*$ as the dominating input. This includes Laplace and Gaussian noise addition mechanisms, where the $\bot$ element corresponds to the $0$ vector. 

% We will prove that applying random allocation to $N$ gives a dominating pair of distributions for random allocation of $\mech$. This reduces the analysis of an adaptive mechanism with various possible inputs to a non-adaptive mechanism over just two inputs. We then analyze the resulting pair of distributions via two approaches. Formal proofs and missing details of this section can be found in Appendix \ref{apd:nonAsym}.

% %assuming that the dominating pair of distributions for $\mech$ is realized by $\mech$ on some new input $z$ and $\mech(\bot)$. %explicitly defining 

% %In fact, it suffices to assume the dominating pair of distributions are of the form $P, \mech(\bot)$ for some distribution $P$, in which case the bounds might not be tight. 

% %We maintain the $*$ notation for simplicity. We provide two additional privacy bounds for this case, one based on a decomposition of the Poisson scheme into several allocation schemes, and the other based on RDP. We bound the case of a single allocation, but these bounds generalize to arbitrary number of steps using Lemma \ref{lem:comp} and RDP composition.

% %We also provide a direct analysis of the hockey-stick divergence
% %for the specific case of the Gaussian mechanism, which was recently considered by \citet{CGHLKKMSZ24} under the name \emph{bins and balls}. 


% %We start by proving $(*, \bot)$ is the dominating pair of $\alloc{t}{\mech}$ as well. This is a stronger claim than the one stated in Lemma \ref{lem:singElem}, which only guaranteed that either $(y^{*}, \bot)$ or $(\bot, y^{*})$ are the dominating pair for some element $y^{*}$ which might not be $*$ and might induce different distributions depending on the past view. 


% \begin{lemma}\label{lem:domElem}
%     Given a mechanism $\mech$ that is tightly dominated by $N$ we have that distributions gives by the output of allocation $(\allocFunc{t, k}{\rand}{*}, \allocFunc{t, k}{\rand}{\bot})$ are a dominating pair of $alloc{t}{\mech}$.
% \end{lemma}

% A special case of this result for Gaussian noise addition was given by \citet{CGHLKKMSZ24}. For this special case, \citet{CGHLKKMSZ24} give several Monte Carlo simulation based techniques to evaluate the privacy parameters. We include a brief discussion of this approach in Appendix \ref{sec:monte-carlo}. The proof of Lemma \ref{lem:domElem} is implied by repeated application of postprocessing in Lemma \ref{lem:domPostProc}.

\subsection{Decomposing Poisson}
We first show how to bound the privacy profile of the random allocation scheme using the privacy profile of of the Poisson scheme. While this bound is not asymptotically optimal, it applies for any number of steps and noise scale, and therefore is tighter that Theorem \ref{thm:asymBnd} in some regimes.

\begin{theorem}\label{thm:dcmpBnd}
    Given a mechanism $\mech$ dominated by a randomizer $\rand$, for any $\lambda \in [0, 1]$; $\eps > 0$ we have $\delta_{\alloc{t}{\mech}}(\eps) \le \frac{1}{\lambda'} \delta_{\Pois{t, \lambda}{\rand}}(\eps')$, where $\eps' \coloneqq \ln (1 + \lambda' (e^{\eps} - 1))$ and $\lambda' \coloneqq 1 - \left(1 - \lambda \right)^{t}$.
\end{theorem}

Setting $\lambda \coloneqq 1/t$ yields $\lambda' \approx 1-e^{-1}$, which can be used to bounds the difference between these two sampling methods up to a $\approx 1.6$ factor in $\eps$ in the $\eps < 1$ regime.

The proof of this theorem consists of two key steps, which we prove in the following lemmas. We start by showing that increasing the number of allocations can only harm the privacy.
\begin{lemma}\label{lem:allocMontn}
        Given $1 \le k \le k' \le t$ and a mechanism $\mech$ dominated by a randomizer $\rand$ we have $\delta_{\alloc{t, k}{\rand}}(\eps) \le \delta_{\alloc{t, k'}{\rand}}(\eps)$. Furthermore, for any sequence of integers $k \le k_{1} < \ldots < k_{j} \le t$, and non-negative $\lambda_{1}, \ldots, \lambda_{j}$ s.t. $\lambda_{1} + \ldots + \lambda_{j} = 1$, the privacy profile of $\alloc{t, k}{\rand}$ is upper-bounded by the privacy profile of $\lambda_{1} \alloc{t, k_{1}}{\rand} + \ldots + \lambda_{j} \alloc{t, k_{j}}{\rand}$, where we use convex combinations of algorithms to denote an algorithm that randomly chooses one of the algorithms with probability given in the coefficient.
\end{lemma}

Next we notice the Poisson scheme can be decomposed into a sequence of random allocation schemes, by first sampling the number of steps in which the element will participate, then running the random allocation scheme for the corresponding number of steps.
\begin{lemma}\label{lem:PoisDecom}
    For any $\lambda \in [0, 1]$, element $x \in \domain$, and mechanism $\mech$ we have, 
    \[
        \PoisFunc{t, \lambda}{\mech}{x} = \sum_{k = 0}^{t} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\mech}{x}, 
    \]
    where $B_{t, \lambda}$ is the PDF of the binomial distribution with parameters $t, \lambda$ and $\allocFunc{t, 0}{\mech}{x} \coloneqq \mech^{t}(\bot)$ simply calls $\mech(\bot)$ in all steps.
\end{lemma}

Combining this insight with the advanced joint convexity \ref{lem:advJntCvx} implies
\begin{lemma}\label{lem:PoisUpper}
    For any $\lambda \in [0, 1]$; $\eps > 0$ and randomizer $\rand$ we have
    \[
        \boldsymbol{H}_{e^{\eps}}\left(\mathcal{P}_{t, \lambda}^{+}(\rand; *) \Vert \rand^{t}(\bot) \right) = \frac{1}{\lambda'} \boldsymbol{H}_{e^{\eps'}}\left(\PoisFunc{t, \lambda}{\rand}{x} \Vert \rand^{t}(\bot) \right), 
    \]
    where 
    \[
        \mathcal{P}_{t, \lambda}^{+}(\rand; x) = \frac{1}{\lambda'} \sum_{k \in [t]} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{x}
    \]
    is the Poisson scheme conditioned on allocating the element at least once, and $\eps', \lambda'$ were defined in Theorem \ref{thm:dcmpBnd}.
\end{lemma}

Putting it all together completes the proof of the theorem. 
\begin{proof}[Proof of Theorem \ref{thm:dcmpBnd}]
    \begin{align*}
        \delta_{\alloc{t}{\mech}}(\eps) & \overset{(1)}{\le} \boldsymbol{H}_{e^{\eps}}(\allocFunc{t, k}{\rand}{*} \Vert \rand^{t}(\bot)) \\
        & = \boldsymbol{H}_{e^{\eps}}\left(\frac{1}{\lambda'} \sum_{k \in [t]} B_{t, \lambda}(k) \cdot \allocFunc{t, 1}{\rand}{*} \Vert \rand^{t}(\bot) \right) \\
        & \overset{(2)}{\le} \boldsymbol{H}_{e^{\eps}}\left(\frac{1}{\lambda'} \sum_{k \in [t]} B_{t, \lambda}(k)\cdot \allocFunc{t, k}{\rand}{*} \Vert \rand^{t}(\bot) \right) \\
        & \overset{(3)}{=} \boldsymbol{H}_{e^{\eps}}\left(\mathcal{P}_{t, \lambda}^{+}(\mech; *) \Vert \rand^{t}(\bot) \right) \\
        & \overset{(4)}{=} \frac{1}{\lambda'} \boldsymbol{H}_{e^{\eps'}}\left(\PoisFunc{t, \lambda}{\rand}{*} \Vert \rand^{t}(\bot) \right) \\
        & = \frac{1}{\lambda'} \delta_{\Pois{t, \lambda}{\rand}}(\eps'), \\
    \end{align*}
    where (1) results from Lemma \ref{lem:singElem}, (2) from Lemma \ref{lem:allocMontn}, (3) from the definition of $\mathcal{P}_{t, \lambda}^{+}$, and (4) from Lemma \ref{lem:PoisUpper}.
\end{proof}

Combining the Poisson decomposition perspective shown in Lemma \ref{lem:PoisDecom} with the monotonicity in number of allocations shown in Lemma \ref{lem:allocMontn}, additionally implies the following corollary.
\begin{corollary}\label{cor:TruncPois}
    For any $\lambda \in [0, 1]$; $k \in [t]$ and mechanism $\mech$ we have $\delta_{\Pois{t, \lambda, k}{\mech}}(\eps) \le \delta_{\Pois{t, \lambda}{\rand}}(\eps)$, where $\Pois{t, \lambda, k}{\mech}$ denote the Poisson scheme where the number of allocations is upper bounded by $k$.
\end{corollary}

% may also be used to upper bound the privacy profile a variant of the Poisson scheme which bounds the maximal possible number of times each element participate. By Lemma \ref{lem:}
% \begin{lemma} \label{lem:PoisUpper}
%     For any $\lambda \in [0, 1]$ and mechanism $\mech$, for any $k \in [t]$ we have 
%     \[
%         \delta_{\Pois{t, \lambda}{\mech}}(\eps) \le \sum_{k \in [t]} B_{t, \lambda}(k) \cdot \delta_{\alloc{t, k}{\mech}}(\eps).
%     \]
%     Furthermore, denoting by $\Pois{t, \lambda, t'}{\mech}$ the Poisson scheme conditioned on using each element at most $t'$ times, we have
%     \[
%         \delta_{\Pois{t, \lambda, t'}{\mech}}(\eps) \le \frac{1}{C} \sum_{k \in [t']} B_{t, \lambda}(k) \cdot \delta_{\alloc{t, k}{\mech}}(\eps),
%     \]
%     where $C \coloneqq \underset{C \sim B_{t, \lambda}}{\mathbb{P}}(C \le t')$.
% \end{lemma}

\subsection{RDP bound}
%One downside of the two previous bounds is that they only bound the privacy of the random allocation scheme in terms of the Poisson scheme, but as the direct analysis indicate, random allocation is more private than Poisson in some regimes. To remedy this limitation, 
We next provide an exact expression for the RDP of the random allocation scheme in terms of the RDP parameters of its tightly dominating mechanism. While the privacy bounds induced by RDP are typically looser than those relying on full analysis and composition of the privacy loss distribution (PRD), the gap nearly vanishes as the number of composed calls to the mechanism grows, as depicted in Figure \ref{fig:multi-epoch}.


%We first notice that if $\mech$ is dominated by this simple pair, then the R\'{e}nyi divergence of the its random allocation scheme can be expressed by a combination of the R\'{e}nyi divergence of $\mech$ itself.

Given two distributions $P, Q$ over some domain, for any $\alpha \ge 1$ denote the $\alpha$-moment of the density ratio by $\boldsymbol{D}_{\alpha}(P \Vert Q) \coloneqq \underset{\omega \sim Q}{\mathbb{E}}\left[\left(\frac{P(\omega)}{Q(\omega)}\right)^{\alpha} \right]$. Notice that $D_{1}(P \Vert Q)=1$ and for any $\alpha > 1$ we have $\RenyiDiv{\alpha}{P}{Q} = \frac{1}{\alpha-1}\ln \left(\boldsymbol{D}_{\alpha}(P \Vert Q) \right)$.
%\footnote{This quantity is sometimes referred to as $\alpha$-divergence up to normalizing factor. Notice that for any $P, Q$ we have $D_{1}(P \Vert Q)=1$ and for any $\alpha > 1$ we have $\RenyiDiv{\alpha}{P}{Q} = \frac{1}{\alpha-1}\ln \left(\boldsymbol{D}_{\alpha}(P \Vert Q) \right)$.}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{multiple_epochs.png}
    \caption{Upper bounds on privacy parameter $\eps$ for various schemes all using the Gaussian mechanism, as a function of $E$ the number of ``epochs'' - times the scheme was sequentially computed, for fixed parameters $\sigma=1$, $\delta = 10^{-8}$, $t = 10^{4}$. In the Poisson scheme $\lambda = 1/t$. The analytic bound was omitted, since it is dominated by the decomposition method in this regime. The RDP bounds for Poisson and allocation are nearly identical.}
    \label{fig:multi-epoch}
\end{figure}

\begin{theorem} \label{thm:RDPbnd}
    Given two integers $t, \alpha \in \naturals$, we denote by $\boldsymbol{P}_{t}(\alpha)$ the set of integer partitions of $\alpha$ consisting of $\le t$ elements.\footnote{If $t \ge \alpha$, $\boldsymbol{P}_{t}(\alpha) = \boldsymbol{P}(\alpha)$ is the set of all integer partitions.} Given a partition $P \in \boldsymbol{P}_{t}(\alpha)$, we denote by $\binom{t}{P} = \frac{t!}{(t-\alpha)! \prod_{p \in P} p!}$, and denote by $C(P)$ is the list of counts of unique values in $P$ (e.g. if $\alpha = 8$ and $P = [1, 2, 3, 3]$ then $C(P) = [1, 1, 2]$).

    For any randomizer $\rand$, and input $x$, we have\footnote{The first version of this work stated an incorrect combinatorial coefficient in this expression. The numerical comparisons were based on the correct expression.}
    \ICML{
    \begin{align*}
        \boldsymbol{D}_{\alpha} & \left( \Vert \rand^{t}(\bot) \right) \\
        & = \frac{1}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{P} \prod_{p \in P} \boldsymbol{D}_{p} \left(\rand(*) \Vert \rand(\bot) \right).
    \end{align*}
    }
    \Arxiv{
    \[
        \boldsymbol{D}_{\alpha} \left(\allocFunc{t}{\rand}{*} \Vert \rand^{t}(\bot) \right) = \frac{1}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} \prod_{p \in P} \boldsymbol{D}_{p} \left(\rand(*) \Vert \rand(\bot) \right).
    \]
    }
\end{theorem}

Since we have an exact expression for the R\'{e}nyi divergence of the Gaussian mechanism, this immediately implies the following corollary.
\begin{corollary}\label{cor:RDPGauss}
    Given $\alpha \in \naturals$ s.t. $1 < \alpha \le t$; $\sigma > 0$, and a Gaussian mechanism $N_{\sigma}$, 
    \ICML{
    \begin{align*}
        \Renyi{\alpha} & \left(\allocFunc{t}{N_{\sigma}}{1} \Vert N_{\sigma}^{t}(0) \right) = - \frac{\alpha}{\alpha - 1} \left(\frac{1}{2 \sigma^{2}} + \ln (t) \right)
        \\ & + \frac{1}{\alpha - 1}\ln \left(\sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{P} e^{\sum_{p \in P}\frac{p^{2}}{2 \sigma^{2}}} \right).
    \end{align*}
    }
    \Arxiv{
    \[
        \Renyi{\alpha} \left(\allocFunc{t}{N_{\sigma}}{1} \Vert N_{\sigma}^{t}(0) \right) = - \frac{\alpha}{\alpha - 1} \left(\frac{1}{2 \sigma^{2}} + \ln (t) \right) + \frac{1}{\alpha - 1}\ln \left(\sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} e^{\sum_{p \in P}\frac{p^{2}}{2 \sigma^{2}}} \right).
    \]
    }
\end{corollary}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{RDP.png}
    \caption{Upper bounds on privacy profile $\delta$ as a function of the number of steps $t$ for the Poisson and random allocation schemes. $\sigma=0.3, \eps=10, k=1$.}
    \label{fig:Monte_carlo}
\end{figure} 

%reached the same result for the Gaussian mechanism by direct analysis of R\'{e}nyi divergence's integral form, in the context of the shuffle scheme. Originally, the authors incorrectly assumes that the $(0, \ldots, 0, 1)$ and $(0, \ldots, 0)$ are the dominating pair of datasets for the shuffle Gaussian mechanism, an assumption they later corrected. While this assumption was not yet proven in the case of the shuffle scheme, Lemma \ref{lem:domElem} implies this is indeed the case for random allocation.

Corollary \ref{cor:RDPGauss} gives an simple way to exactly compute integer RDP parameters of random allocation with Gaussian noise. Interestingly, they closely match RDP parameters of the Poisson scheme with rate $1/t$ in most regimes (e.g. Fig.~\ref{fig:multi-epoch}). In fact, in some (primarily large $\eps$) parameter regimes the bounds based on RDP of allocation are lower than the PLD-based bounds for Poisson subsampling (Fig.~\ref{fig:Monte_carlo}). The restriction to integer values has negligible effect, which can be further mitigated using \citet[Corollary 10]{WBK19}. We also note that $\vert \boldsymbol{P}_{t}(\alpha) \vert$ is sub-exponential in $\alpha$ which leads to performance issues in the very high privacy ($\eps \ll 1$) regime (Large $\sigma$ values in Fig~\ref{fig:main-fig}). Since the typical value of $\alpha$ used for accounting is in the low tens, this quantity can be efficiently computed using several technical improvements which we discuss in Appendix \ref{apd:nonAsym}. On the other hand, in the very low privacy regime ($\eps \gg 1$), the $\alpha$ that leads to the best bound on $\eps$ is typically in the range $[1,2]$ which cannot be computed using method. Finally, we remark that though this result is stated only for $k=1$, it can be extended to $k>1$ using the same argument as in Lemma \ref{lem:comp}. In fact RDP based bounds are particularly convenient for subsequent composition which necessary to obtain bounds for $k>1$ or multi-epoch training algorithms.

\section{Discussion}
Our results give the first nearly-tight and provable bounds on privacy amplification of random allocation with Gaussian noise, notably showing that they nearly match bounds known for Poisson subsampling. Together with the results of \citet{CGHLKKMSZ24}, our results imply that random allocation (or balls-and-bins sampling) has the utility benefits of shuffling while having the privacy benefits of Poisson subsampling. This provides a (reasonably) practical way to reconcile a long-standing and concerning discrepancy between the practical implementations of DP-SGD and its commonly-used privacy analyses.

%Finally, we note that the pair of distributions we analyze is the same one that is conjectured by \citet{CGKKMSZ24a} (Conjecture 3.2) to be the worst case for shuffling with Gaussian noise. If this conjecture is true then all of our analyses will also apply to 
%is indeed the case, our first theorem naturally extends to this pair up to a constant factor, thus providing an improved upper bound for the shuffle model as well.
\subsection*{Acknowledgments}
We are grateful to Kunal Talwar for proposing the problem that we investigate here as well as suggesting the decomposition-based approach for the analysis of this problem (established in Theorem \ref{thm:dcmpBnd}). We also thank Hilal Asi, Hannah Keller, Guy Rothblum and Katrina Ligett for thoughtful comments and motivating discussions of these results and their application in \citep{AFKRT25}. % in the context of their applications \citep{}
Shenfelds work was supported by the Apple Scholars in AI/ML PhD Fellowship.

\newpage
\ICML{
\section{Impact Statement}
This paper provides several new tools for analyzing the privacy of machine learning algorithms. We do not anticipate any impacts beyond those typical for such results.
}

\bibliography{bibliography}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Missing proofs from Section \ref{sec:asym}} \label{apd:asym}

\ICML{
\subsection{Reduction to randomizer}
\begin{proof}[Proof of Lemma \ref{lem:singElem}]
    \textcolor{red}{Copy from the body of the paper.}
\end{proof}
}


\subsection{Single element bound}

\begin{proof}[Proof of Lemma \ref{lem:postEqAlloc}]
    We notice that for all $j \in [t-1]$ and $\view^{j} \in \resDom^{j}$, 
    \begin{align*}
        P_{\alloc{t, k}{\mech}}(\view^{j+1} \vert x, \view^{j}) & = \frac{P_{\alloc{t, k}{\mech}}(\view^{j+1} \vert x)}{P_{\alloc{t, k}{\mech}}(\view^{j} \vert x)} \\
        & \overset{(1)}{=} \frac{\underset{\boldsymbol{i} \subseteq [t], \vert \boldsymbol{i} \vert = k}{\sum} P_{\alloc{t, k}{\mech}}(\view^{j+1}, \boldsymbol{I} = \boldsymbol{i} \vert x)}{P_{\alloc{t, k}{\mech}}(\view^{j} \vert x)} \\
        & \overset{(2)}{=} \frac{\underset{\boldsymbol{i} \subseteq [t], \vert \boldsymbol{i} \vert = k}{\sum} P_{\alloc{t, k}{\mech}}(\view^{j}, \boldsymbol{I} = \boldsymbol{i} \vert x) \cdot P_{\alloc{t, k}{\mech}}(\res_{j+1} \vert x, \boldsymbol{I} = \boldsymbol{i}, \view^{j})}{P_{\alloc{t, k}{\mech}}(\view^{j} \vert x)} \\
        & \overset{(3)}{=} \left(\underset{\boldsymbol{i} \subseteq [t], \vert \boldsymbol{i} \vert = k, j+1 \notin \boldsymbol{i}}{\sum} \frac{P_{\alloc{t, k}{\mech}}(\view^{j}, \boldsymbol{I} = \boldsymbol{i} \vert x)}{P_{\alloc{t, k}{\mech}}(\view^{j} \vert x)} \right) P_{\mech}(\res_{j+1} \vert \bot, \view^{j}) \\
        & ~~~~ + \left(\underset{\boldsymbol{i} \subseteq [t], \vert \boldsymbol{i} \vert = k, j+1 \in \boldsymbol{i}}{\sum} \frac{P_{\alloc{t, k}{\mech}}(\view^{j}, \boldsymbol{I} = \boldsymbol{i}  \vert x)}{P_{\alloc{t, k}{\mech}}(\view^{j} \vert x)} \right) P_{\mech}(\res_{j+1} \vert x, \view^{j}) \\
        & = P_{\alloc{t, k}{\mech}}(j+1 \notin \boldsymbol{I} \vert x, \view^{j}) P_{\mech}(\res_{j+1} \vert \bot, \view^{j}) + P_{\alloc{t, k}{\mech}}(j+1 \in \boldsymbol{I} \vert x, \view^{j}) P_{\mech}(\res_{j+1} \vert x, \view^{j}) \\
        & = (1-\lambda_{\view^{j}, k, x}) \cdot P_{\mech}(\res_{j+1} \vert \bot, \view^{j}) + \lambda_{\view^{j}, k, x} \cdot P_{\mech}(\res_{j+1} \vert x, \view^{j}) \\
        & \overset{(3)}{=}  P_{\post{t, k}{\mech}}(\view^{j+1} \vert x, \view^{j}), 
    \end{align*}
    where (1) denotes the subset of steps selected by the allocation scheme by $\boldsymbol{I}$ so $\boldsymbol{I} = \boldsymbol{i}$ denotes the selected subset was $\boldsymbol{i}$, (2) results from the definition $\view^{j+1} = (\view^{j}, \res_{j+1})$ and Bayes law, (3) from the fact that if $j+1 \in \boldsymbol{I}$ then $\res_{j+1}$ depends only on a $x$ and if $j+1 \notin \boldsymbol{I}$ then $\res_{j+1}$ depends only on $\bot$, and (4) is a direct result of the posterior scheme definition.

    Since $P(\view \vert x) = \prod_{i\in[t-1]} P(\view^{j+1} \vert x, \view^{j})$ for any scheme, this completes the proof. 

    By Lemma \ref{lem:singElem}, $\bar{\delta}_{\alloc{t, k}{\rand}}(\eps)$ is achieved by a pair of datasets of size $1$, which proves the second part.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:truncPostBndPost}]
    For any $\mathcal{C} \subseteq \resDom^{t}$ we have
    \begin{align*}
        \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}} & (\viewRV \in \mathcal{C}) \\
        & = \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{C} / \mathcal{B}_{\eta}^{t, k}) + \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{C} \cap \mathcal{B}_{\eta}^{t, k}) \\
        & \overset{(1)}{=} \underset{\viewRV \sim \postFunc{t, k, \eta}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{C} / \mathcal{B}_{\eta}^{t, k}) + \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{B}_{\eta}^{t, k}) \\
        & \overset{(2)}{\le} e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}(\viewRV \in \mathcal{C} / \mathcal{B}_{\eta}^{t, k}) + \boldsymbol{H}_{e^{\eps}}(\postFunc{t, k, \eta}{\rand}{*} \Vert \rand^{t}(\bot)) + \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{B}_{\eta}^{t, k}) \\
        & \overset{(1)}{=} e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}(\viewRV \in \mathcal{C} / \mathcal{B}_{\eta}^{t, k}) + \boldsymbol{H}_{e^{\eps}}(\postFunc{t, k, \eta}{\rand}{*} \Vert \rand^{t}(\bot)) + \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{B}_{\eta}^{t, k}), \\
        & \le e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}(\viewRV \in \mathcal{C} ) + \boldsymbol{H}_{e^{\eps}}(\postFunc{t, k, \eta}{\rand}{*} \Vert \rand^{t}(\bot)) + \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{B}_{\eta}^{t, k}), \\
    \end{align*}
    which implies
    \begin{align*}
        \delta_{\post{t, k}{\rand}}(\eps) & = \boldsymbol{H}_{e^{\eps}}(\postFunc{t, k}{\rand}{*} \Vert \rand^{t}(\bot)) \\
        & \overset{(2)}{=} \underset{\mathcal{C} \subseteq \resDom^{t}}{\sup} \left(\underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{C}) - e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}(\viewRV \in \mathcal{C}) \right) \\
        & \le \boldsymbol{H}_{e^{\eps}}(\postFunc{t, k, \eta}{\rand}{*} \Vert \rand^{t}(\bot)) + \underset{\viewRV \sim \postFunc{t, k}{\rand}{*}}{\mathbb{P}}(\viewRV \in \mathcal{B}_{\eta}^{t, k}) \\
        & = \delta_{\post{t, k, \eta}{\rand}}(\eps) + \beta_{\alloc{t, k}{\rand}} (\eta) \\
    \end{align*}
    where (1) results from the definition of the truncated posterior scheme and the set $\mathcal{B}_{\eta}^{t, k}$, and (2) from the fact that for any couple of distributions $P, Q$ over some domain $\resDom$
    \[
        \boldsymbol{H}_{e^{\eps}}(P \Vert Q) = \underset{\mathcal{C} \subseteq \resDom^{t}}{\sup} \left(\underset{\resRV \sim P}{\mathbb{P}}(\resRV \in \mathcal{C}) - e^{\eps} \underset{\resRV \sim Q}{\mathbb{P}}(\resRV \in \mathcal{C}) \right).
    \]
\end{proof}

The proof of Lemma \ref{lem:truncBndPois} makes use of the next claim.
\begin{claim}[Theorem 10 in \citep{ZDW22}] \label{clm:domPairComp}
    If a pair of distributions $(P, Q)$ dominates a mechanism $M$ and $(P', Q')$ dominate $M'$, then $(P \times P', Q \times Q')$ dominate the composition of $M$ and $M'$.
\end{claim}

\begin{proof} [Proof of Lemma \ref{lem:truncBndPois}]
    We first notice that the the hockey-stick divergence of a mixture mechanism is monotonically increasing in its mixture parameter. For any $0 \le \lambda \le \lambda' \le 1$ and two distributions $P_{0}, P_{1}$ over some domain, denoting $Q_{\lambda} \coloneqq (1-\lambda) P_{0} + \lambda P_{1}$ we have, $Q_{\lambda'} = \frac{1-\lambda'}{1-\lambda} Q_{\lambda} + \frac{\lambda'-\lambda}{1-\lambda} P_{1}$. From the quasi-convexity of the hockey-stick divergence, for any $\alpha \ge 1$ we have 
    \[
        \boldsymbol{H}_{\alpha}(Q_{\lambda'} \Vert P_{1}) = \boldsymbol{H}_{\alpha}\left(\frac{1-\lambda'}{1-\lambda} Q_{\lambda} + \frac{\lambda'-\lambda}{1-\lambda} P_{1} \Vert P_{1} \right) \le \boldsymbol{H}_{\alpha}\left(Q_{\lambda} \Vert P_{1} \right).
    \]

    Using this fact we get that the privacy profile of a single call to a Poisson subsampling mechanism is monotonically increasing in its sampling probability, so the privacy profile of every step of $\post{t, k, \eta}{\rand}$ is upper bounded by that of $\Pois{1, \eta}{\rand}$, and from Claim \ref{clm:domPairComp} its $t$ times composition is the dominating pair of $\Pois{t, \eta}{\rand}$, which completes the proof.
\end{proof}


\begin{proof}[Proof of Lemma \ref{lem:approxToPure}]
    From Lemma 3.7 in \citep{FMT21}, there exists a randomizer $\hat{\rand}$ which is $\eps_{0}$-DP, and for any element $x \in \{*, \bot \}$ we have $D_{TV}(\rand(x) \Vert \hat{\rand}(x)) \le \delta_{0}$.
    
    For any $i \in [t]$ consider the posterior scheme $\post{t, k, (i)}{\hat{\rand}}$ which $\forall j < i$ returns 
    \[
        \res_{j+1} \sim \left(\lambda_{\view^{j}, k, *} \cdot \rand(*) + (1-\lambda_{\view^{j}, k, *}) \cdot \rand(\bot) \right), 
    \]
    and $\forall j \ge i$ returns
    \[
        \res_{j+1} \sim \left(\lambda_{\view^{i}, k, *} \cdot \hat{\rand}(*) + (1-\lambda_{\view^{j}, k, *}) \cdot \hat{\rand}(\bot) \right).
    \]
    
    Notice that $\post{t, k, (0)}{\hat{\rand}} = \post{t, k}{\rand}$ and $\post{t, k, (t)}{\hat{\rand}} = \post{t, k}{\hat{\rand}}$. From the definition, for any $i \in [t]$ we have $D_{TV}\left(\postFunc{t, k, (i-1)}{\hat{\rand}}{*} \Vert \postFunc{t, k, (i)}{\hat{\rand}}{*} \right) \le \delta_{0}$, which implies $D_{TV}\left(\postFunc{t, k}{\rand}{*} \Vert \postFunc{t, k}{\hat{\rand}}{*} \right) \le t \delta_{0}$.

    Combining this inequality with the fact that for any two distributions $P, Q$ over domain $\Omega$ and a subset $\mathcal{C} \subseteq \Omega$ we have $P(\mathcal{C}) \le Q(\mathcal{C}) + D_{TV}(P \Vert Q)$ completes the proof.
\end{proof}

The proof of Lemma \ref{lem:postProbBnd} is based on an explicit description of $\lambda_{\view^{i}, x}$ in terms of the induced privacy loss.
\begin{claim} \label{clm:postExp}
    Given $i \in [t-1]$, an element $x \in \domain$ and a view $\view^{i} \in \resDom^{i}$, we have
    \[
        \lambda_{\view^{i}, x} = \frac{1}{t + \sum_{j \in [i]} (e^{\ell(\res_{j}; x, \bot, \view^{j-1})} - 1)}
    \]
\end{claim}

\begin{proof}
    \begin{align*}
        \lambda_{\view^{i}, x} & = P_{\alloc{t}{\mech}} \left(I = i+1 \vert x, \view^{i} \right) \\
        & = \frac{P_{\alloc{t}{\mech}} \left(\view^{i} \vert x, I = i+1 \right)P\left(I = i+1 \right)}{P_{\alloc{t}{\mech}} \left(\view^{i} \vert x \right)} \\
        & = \frac{\frac{1}{t} P_{\alloc{t}{\mech}} \left(\view^{i} \vert x, I = i+1 \right)}{\frac{1}{t} \sum_{j \in [t]}P_{\alloc{t}{\mech}} \left(\view^{i} \vert x, I = j \right)} \\
        & = \frac{1}{\sum_{j \in [t]} \frac{P_{\alloc{t}{\mech}}\left(\view^{i} \vert x, I = j \right)}{P_{\alloc{t}{\mech}} \left(\view^{i} \vert x, I = i+1 \right)}} \\
        & = \frac{1}{t-i + \sum_{j \in [i]} \frac{P_{\mech}\left(\res_{j} \vert x, \view^{k-1} \right)}{P_{\mech} \left(\res_{j} \vert \bot, \view^{k-1} \right)}} \\
        & = \frac{1}{t + \sum_{j \in [i]} (e^{\ell(\res_{j}; x, \bot, \view^{j-1})} - 1)} \\
    \end{align*}
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:postProbBnd}]
    First notice that, 
    \[
        \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\lambda_{\viewRV, x} > \frac{1}{t(1 + \gamma)} \right) = \frac{1}{t} \sum_{l \in [t]} \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\lambda_{\viewRV, x} > \frac{1}{t(1 + \gamma)} ~ \vert ~ I=l \right), 
    \]
    and for any $l \in [t]$, 
    \begin{align*}
        \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} & \left(\lambda_{\viewRV, x} > \frac{1}{t(1 + \gamma)} ~ \vert ~ I=l \right) \\
        & \overset{(1)}{=} \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\underset{i \in [t-1]}{\max}\left(\lambda_{\viewRV_{i}, x} \right) > \frac{1}{t(1 + \gamma)} ~ \vert ~ I=l \right) \\
        & \overset{(2)}{=} \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\underset{i \in [t-1]}{\max}\left(\frac{1}{t + \sum_{j \in [i]} (e^{\ell(\res_{j}; x, \bot, \view^{j-1})} - 1)}\right) > \frac{1}{t(1 + \gamma)} ~ \vert ~ I=l \right) \\
        & = \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\underset{i \in [t-1]}{\max}\left(\sum_{j \in [i]} (1 - e^{\ell(\res_{j}; x, \bot, \view^{j-1})})\right) > \gamma t ~ \vert ~ I=l \right), \\
    \end{align*}
    where (1) results from the definition of $\lambda_{\view, x}$ and (2) from Claim \ref{clm:postExp}.

    We can now define the following martingale; $D_{0} \coloneqq 0$, $\forall j \in [t-1] : D_{j} \coloneqq 1 - e^{\ell(\resRV_{j}; *, \bot, \view^{j-1})}$, and $Y_{i} \coloneqq \sum_{j = 0}^{i} D_{j}$. Notice that this is a sub-martingale since for any $j \in [t-1]$
    \[
        \underset{\resRV \sim \mech(\bot, \view^{j})}{\mathbb{E}} \left[1 - e^{\ell(\resRV; x, \bot, \view^{j})} \right] = 1 - \underset{\resRV \sim \mech(\bot, \view^{j})}{\mathbb{E}} \left[\frac{P_{\mech}(\resRV \vert x, \view^{j})}{P_{\mech}(\resRV \vert \bot, \view^{j})} \right] = 0
    \]
    and 
    \[
        \underset{\resRV \sim \mech(x, \view^{j})}{\mathbb{E}} \left[1 - e^{\ell(\resRV; x, \bot, \view^{j})} \right] = 1 - \exp\left(\RenyiDiv{2}{\mech(x, \view^{j})}{\mech(\bot, \view^{j})} \right) \le 0, 
    \]
    where $\Renyi{\alpha}$ is the $\alpha$-R\'{e}nyi divergence (Definition \ref{def:renDiv}).
    
    From the fact $\mech$ is $\eps_{0}$-DP we have $1 - e^{-\eps_{0}} \le D_{j} \le 1 - e^{\eps_{0}}$ almost surely, so the range of $D_{j}$ is bounded by $e^{\eps_{0}} - e^{-\eps_{0}} = 2 \cosh(\eps_{0})$, and we can invoke the Maximal Azuma-Hoeffding inequality and get for any $l \in [t]$, 
    \begin{align*}
        \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} & \left(\lambda_{\viewRV, x} > \frac{1}{t(1 + \gamma)} ~ \vert ~ I=l \right) \\
        & = \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\underset{i \in [t-1]}{\max}\left(\sum_{j \in [i]} (1 - e^{\ell(\res_{j}; x, \bot, \view^{j-1})})\right) > \gamma t ~ \vert ~ I=l \right) \\
        & \le \underset{\viewRV \sim \allocFunc{t}{\rand}{\bot}}{\mathbb{P}} \left(\underset{i \in [t]}{\max}\left(Y_{i} \right) > \gamma t \right) \\
        \\ & \le \exp \left(- \frac{t \gamma^{2} }{2 \cosh^{2}(\eps_{0})} \right).
    \end{align*}

    Since this holds in for any $l \in [t]$, we have
    \begin{align*}
        \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\lambda_{\viewRV, x} > \frac{1}{t(1 + \gamma)} \right) & = \frac{1}{t} \sum_{l \in [t]} \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} \left(\lambda_{\viewRV, x} > \frac{1}{t(1 + \gamma)} ~ \vert ~ I=l \right) \\
        & \le \frac{1}{t} \sum_{l \in [t]} \exp \left(- \frac{t \gamma^{2} }{2 \cosh^{2}(\eps_{0})} \right) \\
        & = \exp \left(- \frac{t \gamma^{2} }{2 \cosh^{2}(\eps_{0})} \right).
    \end{align*}
\end{proof}

\subsection{Asymptotic analysis}
\ICML{
\begin{proof}[Proof of Lemma \ref{lem:comp}]
        \textcolor{red}{Copy from the body of the paper.}    
\end{proof}
}

The proof of the second part of Corollary i\ref{cor:asymBnd} is based on the identity of the dominating pair of the Gaussian mechanism.
\begin{claim}[Dominating pair for the Gaussian mechanism \citep{ACGMMT16}] \label{clm:domPairGauss}
    Given $\sigma > 0$, the Gaussian mechanism $N_{\sigma}$ is tightly dominated by the pair of distributions $(\mathcal{N}(1, \sigma^{2}), \mathcal{N}(0, \sigma^{2}))$, where $\bot \coloneqq 0$. This pair can be realized by datasets of arbitrary size $n$ of vectors in dimension $d$ by the pair $((\overset{n-1~\text{times}}{\overbrace{\bar{0}, \ldots, \bar{0}}}, e_{1}), (\overset{n~\text{times}}{\overbrace{\bar{0}, \ldots, \bar{0}}}))$.
\end{claim}
We note that the dominating pair of the Gaussian is one dimensional, regardless of the dimension of the original mechanism.

\begin{proof}[Proof of Corollary \ref{cor:asymBnd}]
    From Theorem \ref{thm:asymBnd}, each of the schemes has a privacy profile $\delta_{\alloc{t/k}{\mech}}(\eps) \le \delta_{\Pois{t/k, \eta}{\rand}}(\eps) + t/k \delta_{0} + \delta/k$,
    where $\eta \coloneqq \min \left\{\frac{k}{t(1-\gamma)}, 1 \right\}$ and $\gamma \coloneqq \min \left\{\cosh(\eps_{0}) \cdot \sqrt{\frac{2 k}{t} \ln \left(\frac{k}{\delta} \right)}, 1 \right\}$. Applying the union bound to the $t/k \delta_{0}$ and $\delta/k$ terms, and using the fact that the composition of Poisson schemes is a longer Poisson scheme completes the proof of the first part.

    From Lemma \ref{lem:gaussPriv} we have $\eps_0 = \frac{\sqrt{2\ln(1.25/ \delta_{0})}}{\sigma} = \frac{\sqrt{2\ln(1.25 t/ \delta)}}{\sigma}$ (see e.g., \citet{DR14} for exact derivation). From the first bound on $\sigma$ we get $\eps_{0} \le 1$ and therefore $\cosh(\eps_0) = (e^{\eps_0}-e^{\eps_0})/2 \leq 3 \eps_0/2$. Combining this with the second bound on $\sigma$ we get,
    \[
        \gamma \le 3 \eps_0 \sqrt{\frac{k}{2t} \ln \left(\frac{k}{\delta} \right)} \le 3 \frac{\sqrt{2\ln(1.25t/\delta)}}{\sigma} \sqrt{\frac{k}{2t} \ln \left(\frac{k}{\delta} \right)} \le \frac{3\sqrt{k} \ln(1.25t/\delta)}{\sqrt{t}\sigma} \le 1/2,
    \]
    which implies $\eta \leq \frac{2k}{t}$ and $\delta_{\Pois{t,\eta}{N_{\sigma}}}(\eps) \le \delta_{\Pois{t, 2 k / t}{N_{\sigma}}}(\eps)$, since the Poisson scheme's privacy profile is monotonic in the sampling probability as proven in Lemma \ref{lem:truncBndPois}.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:poisPriv}]
    From \citet[Lemma 3]{ACGMMT16}, there exists a constant $c_{3} > 1$ such that if $1 \le \sigma \le 1/(16 \lambda)$; then the Poisson scheme with Gaussian mechanism $\Pois{t, \lambda}{N_{\sigma}}$ is $(\alpha, (\alpha-1) \rho)$-RDP for any $\alpha \le 1 + \sigma^{2} \ln(1/(\sigma \lambda))$ where $\rho = c_{3} \frac{t \lambda^{2}}{\sigma^{2}}$. Setting $c_{2} \coloneqq 32 \sqrt{c_{3}}$ and $\alpha = 1 + \sqrt{\frac{\ln(1/\delta)}{\rho}}$ we get, $(\alpha-1) \rho \le \eps/2$ and $(\alpha-1) \eps / 2 \ge \ln(1/\delta)$ for $\eps \ge \frac{c_{2}}{16} \cdot \frac{\lambda \sqrt{t \cdot \ln(1/\delta)}}{\sigma}$. Setting $c_{1} \coloneqq 1/\sqrt{c_{3}}$ we get $\alpha \le 1 + \sigma^{2} \le 1 + \sigma^{2} \ln(1/(\sigma \lambda))$ where the second inequality results from the upper bound on $\sigma$, which implies
    \[
        \underset{\resRV \sim \PoisFunc{t, \lambda}{R}{*}}{\mathbb{P}}(\ell(\resRV; *, \bot) > \eps) \le e^{-(\alpha-1) \eps} \underset{\resRV \sim \PoisFunc{t, \lambda}{R}{*}}{\mathbb{E}}\left[e^{(\alpha-1) \ell(\resRV; *, \bot)} \right] \le e^{-(\alpha-1) (\eps + (\alpha-1) \rho)} \le \delta.
    \]

    If $\sigma > 1/(16 \lambda)$, we can bound the privacy profile of $\Pois{t, \lambda}{N_{\sigma}}$ by $\Pois{t, \lambda}{N_{\sigma'}}$ for $\sigma' \coloneqq 1/(16 \lambda)$. From the bounds on $\lambda$ and $t$, we have $\sigma' > \max\left\{1, c_{1} \frac{\sqrt{\ln(1/\delta})}{\lambda \sqrt{t}} \right\}$, so $\eps \ge \frac{c_{2}}{16} \cdot \frac{\lambda \sqrt{t \cdot \ln(1/\delta)}}{\sigma'} = c_{2} \lambda^{2} \sqrt{t \cdot \ln(1/\delta)}$.
\end{proof}

% \begin{proof}[Proof of Corollary \ref{cor:GaussAsymBnd}]
%     By the results of Corollary \ref{cor:asymBnd}, we have that for every $\eps > 0$, $\delta_{\alloc{t, k}{\mech}}(\eps) \le \delta_{\Pois{t, \eta}{\rand}}(\eps) + t \delta_{0} + \delta'$,
%     where $\eta \coloneqq \min \left\{\frac{k}{t(1-\gamma)}, 1 \right\}$ and $\gamma \coloneqq \min \left\{\cosh(\eps_{0}) \cdot \sqrt{\frac{2 k}{t} \ln \left(\frac{k}{\delta'} \right)}, 1 \right\}$.
    
%     Setting $\delta' \coloneqq \delta / 3$; $\delta_{0} \coloneqq \delta / (3 t)$ we get $\eps_0 = \frac{\sqrt{2\ln(1.25/ \delta_{0})}}{\sigma} = \frac{\sqrt{2\ln(15 t/(4 \delta))}}{\sigma}$ (Lemma \ref{lem:gaussPriv}, see e.g., \citet{DR14} for exact derivation). From the first bound on $\sigma$ we get $\eps_{0} \le 1$ and therefore $\cosh(\eps_0) = (e^{\eps_0}-e^{\eps_0})/2 \leq 3 \eps_0/2$. Combining this with the second bound on $\sigma$ we get,
%     \[
%         \gamma \le 3 \eps_0 \sqrt{\frac{k}{2t} \ln \left(\frac{3k}{\delta} \right)} \le \frac{3 \sqrt{2\ln(15t/(4\delta))}}{\sigma} \sqrt{\frac{k}{2t} \ln \left(\frac{3k}{\delta} \right)} \le \frac{3\sqrt{k} \ln(15t/(4\delta))}{\sqrt{t}\sigma} \le 1/2,
%     \]
%     which implies that $\eta \leq \frac{2k}{t}$ and $\delta_{\Pois{t,\eta}{N_{\sigma}}}(\eps) \le \delta_{\Pois{t, 2 k / t}{N_{\sigma}}}(\eps)$, since the Poisson scheme's privacy profile is monotonic in the sampling probability as proven in Lemma \ref{lem:truncBndPois}.
    
%     From the third bound on $\sigma$ we can invoke Lemma \ref{lem:poisPriv} and get, $\delta_{\Pois{t, 2 k / t}{N_{\sigma}}}(\eps) \le \delta/3$.
    
%     Putting it all together we get,
%     \[
%         \delta_{\alloc{t,k}{\mech}}(\eps) \le \delta_{\Pois{t,\eta}{\mech}}(\eps) + t \delta_0 + \delta' \le 3 \cdot \frac{\delta}{3} = \delta.
%     \]
% \end{proof}

\begin{remark} \label{rem:asymComp}
    While the asymptotic bound on $\eps$ for the Poisson and random allocation schemes is identical up to the additional logarithmic dependence on $t$, only the third bound on $\sigma$ stated for random allocation is required for Poisson. Notice that if $\sqrt{t} > k$ the third term upper bounds the first one, and if additionally $\ln(1/\delta) \le \frac{t^{2}}{k^{3}}$ the second term is bounded by the third one as well. While the first condition might not hold when each element is allocated to many steps, the latter does not hold only when $t < \ln^{2}(1 / \delta)$ which is an uncommon regime of parameters.
\end{remark}

\section{Missing proofs from Section \ref{sec:nonAsym}} \label{apd:nonAsym}

\subsection{Decomposing Poisson}
\begin{proof}[Proof of Lemma \ref{lem:allocMontn}]
    To prove this claim, we recall the technique used in the proof of Theorem \ref{thm:asymBnd}. We proved in Lemma \ref{lem:postEqAlloc} that $\allocFunc{t, k}{\rand}{*}$ and $\postFunc{t, k}{\rand}{*}$ are identically distributed. From the non-adaptivity assumption, this is just a sequence of repeated calls to the mixture mechanism $\lambda_{\view^{i}, k, *} \cdot \rand(*) + (1-\lambda_{\view^{i}, k, *}) \cdot \rand(\bot)$.
    
    Next we recall the fact proven in Lemma \ref{lem:truncBndPois} that the hockey-stick divergence between this mixture mechanism and $\rand(\bot)$ is monotonically increasing in $\lambda$. Since $\lambda_{\view^{i}, k', *} \ge \lambda_{\view^{i}, k, *}$ for any $k' > k$, this means the pair of distributions $(\lambda_{\view^{i}, k', *} \cdot \rand(*) + (1-\lambda_{\view^{i}, k', *}) \cdot \rand(\bot), \rand(\bot))$ dominates the pair $(\lambda_{\view^{i}, k', *} \cdot \rand(*) + (1-\lambda_{\view^{i}, k', *}) \cdot \rand(\bot), \rand(\bot))$ for any iteration $i$ and view $\view^{i}$. Using Claim \ref{clm:domPairComp} this implies we can iteratively apply this for all step and get $\delta_{\alloc{t, k}{\rand}}(\eps) \le \delta_{\alloc{t, k'}{\rand}}(\eps)$ for any $\eps > 0$, thus completing the proof of the first part.

    The proof of the second part is identical, since the posterior sampling probability induced by any mixture of $\alloc{t, k_{1}}{\rand},  \ldots, \alloc{t, k_{j}}{\rand}$ is greater than the one induced by $\alloc{t, k}{\rand}$ the same reasoning follows.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:PoisDecom}]
    This results from the fact that flipping $t$ coins with bias $\lambda$ can be modeled as first sampling an integer $k \in \{0, 1, \ldots, t\}$ from a binomial distribution with parameters $(t, \lambda)$, then uniformly sampling $i_{1}, \ldots, i_{k} \in [t]$, and setting the coins to $1$ for those indexes.
\end{proof}

\begin{lemma}[Advanced joint convexity \citep{BBG18}]\label{lem:advJntCvx}
    Given $\eta \in [0, 1]$; $\alpha \ge 0$ and three distribution $P_{0}, P_{1}, Q$ over some domain, we have
    \[
        \boldsymbol{H}_{\alpha}((1-\eta)Q + \eta P_{0} \Vert (1-\eta)Q + \eta P_{1}) = \eta \boldsymbol{H}_{\alpha'}(P_{0} \Vert (1-\eta')Q + \eta' P_{1}), 
    \]
    where $\alpha' \coloneqq 1 + (\alpha-1)/\eta$ and $\eta' \coloneqq \alpha / \alpha'$.
\end{lemma}

\begin{proof}[Proof of Lemma \ref{lem:PoisUpper}]
    First notice that, 
    \begin{align*}
        \PoisFunc{t, \lambda}{\rand}{x} & \overset{(1)}{=} \sum_{k = 0}^{t} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{x} \\
        & = B_{t, \lambda}(0) \cdot \allocFunc{t, 0}{\rand}{x} + \sum_{k \in [t]} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{x} \\
        & \overset{(2)}{=} \left(1 - \lambda \right)^{t} \cdot \allocFunc{t, 0}{\rand}{x} + \sum_{k \in [t]} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{x} \\
        & \overset{(3)}{=} (1 - \lambda') \cdot \rand^{t}(\bot) + \lambda' \cdot \mathcal{P}_{t, \lambda}^{+}(\rand; x), \\
    \end{align*}
    where (1) results from Lemma \ref{lem:PoisDecom}, (2) from the definition of the binomial distribution, $\lambda'$ and $\mathcal{P}_{t, \lambda}^{+}(\rand)$, and (3) from the definition of $\lambda'$ and the fact $\allocFunc{t, 0}{\mech}{x} = \rand^{t}(\bot)$.

    From Lemma \ref{lem:advJntCvx} we have, 
    \[
        \boldsymbol{H}_{\alpha}(\PoisFunc{t, \lambda}{\rand}{x} \Vert \PoisFunc{t, \lambda}{\rand}{\bot}) = \boldsymbol{H}_{\alpha}((1 - \lambda') \rand^{t}(\bot) + \lambda' \mathcal{P}_{t, \lambda}^{+}(\rand; x) \Vert \rand^{t}(\bot)) = \lambda' \boldsymbol{H}_{1 + (\alpha-1)/\lambda'}(\mathcal{P}_{t, \lambda}^{+}(\rand; x) \Vert \rand^{t}(\bot)).
    \]
    Setting $1 + (\alpha-1)/\lambda' = e^{\eps}$ and inverting the equation we get, 
    \[
        \boldsymbol{H}_{e^{\eps}}(\mathcal{P}_{t, \lambda}^{+}(\rand; x) \Vert \rand^{t}(\bot)) = \frac{1}{\lambda'} \boldsymbol{H}_{\alpha}(\PoisFunc{t, \lambda}{\rand}{x} \Vert \PoisFunc{t, \lambda}{\rand}{\bot}) = \frac{1}{\lambda'} \boldsymbol{H}_{e^{\eps'}}(\PoisFunc{t, \lambda}{\rand}{x} \Vert \PoisFunc{t, \lambda}{\rand}{\bot}), 
    \]
    which completes the proof
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:TruncPois}]
    Notice that,
    \begin{align*}
        \delta_{\Pois{t, \lambda, k}{\mech}}(\eps) & \overset{(1)}{\le} \delta_{\Pois{t, \lambda, k}{\rand}}(\eps) \\
        & = \boldsymbol{H}_{e^{\eps}}\left(\PoisFunc{t, \lambda, k}{\rand}{*} \Vert \PoisFunc{t, \lambda, k}{\rand}{\bot} \right) \\
        & \overset{(2)}{=} \boldsymbol{H}_{e^{\eps}}\left(\left(\sum_{i = 0}^{k-1} B_{t, \lambda}(i) \allocFunc{t, i}{\rand}{*} \right) + \left(\sum_{i = k}^{t} B_{t, \lambda}(i) \right) \allocFunc{t, k}{\rand}{*} \Vert \rand^{t}(\bot) \right) \\
        & \overset{(3)}{\le} \boldsymbol{H}_{e^{\eps}}\left(\sum_{i = 0}^{t} B_{t, \lambda}(i) \allocFunc{t, i}{\rand}{*} \Vert \rand^{t}(\bot) \right) \\
        & = \delta_{\Pois{t, \lambda}{\mech}}(\eps),
    \end{align*}
    where (1) results from Lemma \ref{lem:singElem}, (2) from Lemma \ref{lem:PoisDecom} and the definition of $\Pois{t, \lambda, k}{\rand}$, and (3) from Lemma \ref{lem:allocMontn}.
\end{proof}

% \begin{proof}
%     Notice that
%     \begin{align*}
%         \delta_{\Pois{t, \lambda}{\mech}}(\eps) & \overset{(1)}{\le} \delta_{\Pois{t, \lambda}{\rand}}(\eps) \\
%         & = \boldsymbol{H}_{e^{\eps}}\left(\PoisFunc{t, \lambda}{\rand}{*} \Vert \PoisFunc{t, \lambda}{\rand}{\bot} \right) \\
%         & \overset{(2)}{=} \boldsymbol{H}_{e^{\eps}}\left(\sum_{k = 0}^{t} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{*} \Vert \sum_{k = 0}^{t} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{\bot} \right) \\
%         & \overset{(3)}{\le} \sum_{k = 0}^{t} B_{t, \lambda}(k) \cdot  \boldsymbol{H}_{e^{\eps}}\left(\allocFunc{t, k}{\rand}{*} \Vert  \allocFunc{t, k}{\rand}{\bot} \right) \\
%         & = \sum_{k \in [t]} B_{t, \lambda}(k) \cdot \delta_{\alloc{t, k}{\mech}}(\eps),
%     \end{align*}
%     where (1) results from Lemma \ref{lem:singElem}, (2) from Lemma \ref{lem:PoisDecom}, and (3) from the joint convexity of the hockey-stick divergence.

%     Using the fact $\PoisFunc{t, \lambda, t'}{\mech}{x} = \frac{1}{C} \sum_{k = 0}^{t'} B_{t, \lambda}(k) \cdot \allocFunc{t, k}{\rand}{x}$, we can repeat the same proof for the second part as well.
% \end{proof}

\subsection{RDP bound}
We start by proving a supporting claim
\begin{claim} \label{clm:multiCount}
    Given $\alpha, t \in \naturals$  and a list of integers $i_{1}, \ldots, i_{t} \ge 0$ such that $i_{1} + \ldots + i_{t} = \alpha$, denote by $P(i_{1}, \ldots, i_{t})$ the integer partition of $\alpha$ associated with this list, e.g. if $i_{1} = 1, i_{2} = 0, i_{3} = 2, i_{4} = 1$, then $P = [1, 1, 2]$. Given an integer partition $P$ of $\alpha$, we have $\vert B_{P} \vert = \binom{t}{C(P)}$ where, 
    \[
        B_{P} = \left\{i_{1}, \ldots, i_{t} \ge 0 ~\vert~ P(i_{1}, \ldots, i_{t}) = P \right\},
    \]
    and $C(P)$ was defined in Theorem \ref{thm:RDPbnd}.
\end{claim}

\begin{proof}
    Given a partition $P$ with unique counts $C(P) = (c_{1}, \ldots, c_{j})$, and an assignments $i_{1}, \ldots, i_{t}$ such that $i_{1}, \ldots, i_{t} \ge 0$ and $P(i_{1}, \ldots, i_{t}) = P$, there are $\binom{t}{c_{1}}$ ways to assign the first value to $c_{1}$ indexes of the possible $t$, $\binom{t - c_{1}}{c_{2}}$ ways to assign the second value to $c_{2}$ indexes of of the remaining $t - c_{1}$ indexes, and so on. Multiplying these terms completes the proof.
\end{proof}

\begin{proof} [Proof of Theorem \ref{thm:RDPbnd}]
    Given a set of integers $i_{1}, \ldots, i_{t} \ge 0$ such that $i_{1} + \ldots + i_{t} = \alpha$ 
    we have,
    \[
        \prod_{k \in [t]} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{E}} \left[\left(\frac{P_{\rand}(\resRV_{k} \vert *)}{P_{\rand}(\resRV_{k} \vert \bot)} \right)^{i_{k}} \right] = \prod_{p \in P} \underset{\resRV \sim \rand(\bot)}{\mathbb{E}} \left[\left(\frac{P_{\rand}(\resRV \vert *)}{P_{\rand}(\resRV \vert \bot)} \right)^{p} \right],
    \]
    where $P$ is the integer partition of $\alpha$ defined by $i_{1}, \ldots, i_{t}$, e.g. if $i_{1} = 1, i_{2} = 0, i_{3} = 2, i_{4} = 1$, then $P = [1, 1, 2]$. This is a result of the fact $\resRV_{k}$ are all identically distributed. Notice that the same partition corresponds to many assignments, e.g. $P = [1, 1, 2]$ corresponds to $i_{1} = 0, i_{2} = 1, i_{3} = 1, i_{4} = 2$ as well. The number of assignments that correspond to a partition $P$ is $\binom{t}{C(P)}$. Using this fact we get,

    \begin{align*}
        \boldsymbol{D}_{\alpha} \left(\allocFunc{t}{\rand}{*} \Vert \rand^{t}(\bot) \right) & = \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{E}} \left[\left(\frac{P_{\alloc{t}{\rand}}(\viewRV \vert *)}{P_{\alloc{t}{\rand}}(\viewRV \vert \bot)} \right)^{\alpha} \right] \\
        & \overset{(1)}{=} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{E}} \left[\left(\frac{1}{t}\sum_{i \in [t]} \frac{P_{\rand}(\resRV_{i} \vert *)}{P_{\rand}(\resRV_{i} \vert \bot)} \right)^{\alpha} \right] \\
        & \overset{(2)}{=} \frac{1}{t^{\alpha}} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{E}} \left[\sum_{\substack{i_{1}, \ldots, i_{t} \in [\alpha]; \\ i_{1} + \ldots + i_{t} \ge 0}} \binom{\alpha}{i_{1}, \ldots, i_{t}} \prod_{k \in [t]} \left(\frac{P_{\rand}(\resRV_{k} \vert *)}{P_{\rand}(\resRV_{k} \vert \bot)} \right)^{i_{k}} \right] \\
        & \overset{(3)}{=} \frac{1}{t^{\alpha}} \sum_{\substack{i_{1}, \ldots, i_{t} \ge 0; \\ i_{1} + \ldots + i_{t} = \alpha}} \binom{\alpha}{i_{1}, \ldots, i_{t}} \prod_{k \in [t]} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{E}} \left[\left(\frac{P_{\rand}(\resRV_{k} \vert *)}{P_{\rand}(\resRV_{k} \vert \bot)} \right)^{i_{k}} \right] \\
        & \overset{(4)}{=} \frac{1}{t^{\alpha}} \sum_{\substack{i_{1}, \ldots, i_{t} \ge 0; \\ i_{1} + \ldots + i_{t} = \alpha}} \binom{\alpha}{i_{1}, \ldots, i_{t}} \prod_{p \in P(i_{1}, \ldots, i_{t})} \underset{\resRV \sim \rand(\bot)}{\mathbb{E}} \left[\left(\frac{P_{\rand}(\resRV \vert *)}{P_{\rand}(\resRV \vert \bot)} \right)^{p} \right] \\
        & \overset{(5)}{=} \frac{1}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} \prod_{p \in P} \underset{\resRV \sim \rand(\bot)}{\mathbb{E}} \left[\left(\frac{P_{\rand}(\resRV \vert *)}{P_{\rand}(\resRV \vert \bot)} \right)^{p} \right]  \\
        & = \frac{1}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} \prod_{p \in P} \boldsymbol{D}_{p} \left(\rand(*) \Vert \rand(\bot) \right), \\
    \end{align*}
    where (1) results from the definition of the allocation scheme, (2) is the multinomial theorem, (3) results from the fact $\resRV_{i}$ and $\resRV_{j}$ are independent for any $i \ne j$, (4) from the fact $\resRV_{k}$ are all identically and independently distributed with $P(i_{1}, \ldots, i_{t})$ defined in Claim \ref{clm:multiCount}, and (5) results from Claim \ref{clm:multiCount}.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:RDPGauss}]
    From the definition of the R\'{e}nyi divergence for the Gaussian mechanism, 
    \begin{align*}
        \Renyi{\alpha} \left(\allocFunc{t}{N_{\sigma}}{1} \Vert N_{\sigma}^{t}(0) \right)  & = \frac{1}{\alpha - 1}\ln \left( \boldsymbol{D}_{\alpha} \left(\allocFunc{t}{N_{\sigma}}{1} \Vert N_{\sigma}^{t}(0) \right)\right) \\
        & = \frac{1}{\alpha - 1}\ln \left(\frac{1}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} \prod_{p \in P} \boldsymbol{D}_{p} \left(N_{\sigma}(1) \Vert N_{\sigma}(0) \right) \right) \\
        & = \frac{1}{\alpha - 1}\ln \left(\frac{1}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} \prod_{p \in P} e^{\frac{p (p-1)}{2 \sigma^{2}}} \right) \\
        & = \frac{1}{\alpha - 1}\ln \left(\frac{e^{ - \frac{\alpha}{2 \sigma^{2}}}}{t^{\alpha}} \sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} e^{\sum_{p \in P}\frac{p^{2}}{2 \sigma^{2}}} \right) \\
        & = -\frac{\alpha}{2 (\alpha - 1) \sigma^{2}} -\frac{\alpha}{\alpha-1} \ln \left(t\right) + \frac{1}{\alpha - 1}\ln \left(\sum_{P \in \boldsymbol{P}_{t}(\alpha)} \binom{t}{C(P)} \binom{\alpha}{P} e^{\sum_{p \in P}\frac{p^{2}}{2 \sigma^{2}}} \right). \\
    \end{align*}
\end{proof}
We remark that the expression in Corollary \ref{cor:RDPGauss} was previously computed in \citet{LT22}, up to the improvement of using integer partitions. In this (unpublished) work the authors give an incorrect proof that datasets $(0, \ldots, 0, 1)$ and $(0, \ldots, 0)$ are a dominating pair of datasets for the shuffle scheme applied to Gaussian mechanism. Their analysis of the RDP bound for this pair of distributions is correct (even if significantly longer) and the final expression is identical to ours. 

\subsection{Comparison to \citet{DCO25}}\label{apd:comp_loose_RDP}
A recent independent work by \citet{DCO25} considered the same setting under the name Balanced Iteration Subsampling. In Theorem 3.1 they provide two RDP bounds, that are comparable to Theorem \ref{thm:RDPbnd} in our work. The first one is tight but computationally expensive even for the case of $k=1$, as it sums over $O(t^{k \alpha}$ terms (in the case of $k=1$ their expression matches the one proposed by \citet{LT22}, which is mathematically identical to our, but requires $O(t^{\alpha})$ summands rather than our $O(2^{\alpha})$ ones.). The second bound they propose requires summing only over a linear (in $k$) number of terms which is significantly more efficient than our term, but is lossy. This gap is more pronounced in some parameter regimes, and has a minor effect in others. On the other hand, this method allows for direct analysis of the $k > 1$ case, while our analysis relies on the reduction to composition of $k$ runs of the random allocation process with a selection of $1$ out of $t/k$ steps.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Loose_RDP_comp.png}
    \caption{Upper bounds on privacy parameter $\eps$ as a function of the the number of allocations $k$ for the Poisson and random allocation schemes, all using the Gaussian mechanism with fixed parameters $\delta = 10^{-6}$, $t = 1024$, $\sigma = 1$. In the Poisson scheme $\lambda = k/t$. The y-axis uses logarithmic scale to emphasize the relative performance.}
    \label{fig:Loose_RDP_comp}
\end{figure}

Figure \ref{fig:Loose_RDP_comp} depicts the spectrum of these effects. For small values of $k$, our RDP based bounds are tighter than the loose bound proposed by \citet{DCO25} by a factor of $\approx 0.6$, while for the large values of $k$ their bound is tighter by a factor of $\approx 0.9$. The bound for the Poisson scheme is closer to our bound throughout the full range.

\section{Implementation details}
Computation time of the naive implementation of our RDP calculation ranges between second and minutes on a typical personal computer, depending on the $\alpha$ value and other parameters, but can be improved by several orders of magnitude using several programming and analytic steps which we briefly discuss here.

On the programming side, we used vectorization and hashing to reduce runtime. To avoid overflow we computed most quantities in log form, and used and the LSE trick. While significantly reducing the runtime, programming improvements cannot escape the inevitable exponential (in $\alpha$) nature of this method. Luckily, in most settings, $\alpha^{*}$ - the $\alpha$ value which induces the tightest bound on $\eps$ is typically in the low $10$s. Unfortunately, finding $\alpha^{*}$ requires computing $\Renyi{\alpha}$, so reducing the range of $\alpha$ values for which $\Renyi{\alpha}$ is crucial.

We do so by proving an upper bound on $\alpha^{*}$ in terms of a known bound on $\eps$.
\begin{claim}
    Given $\delta \in (0,1)$ and two distributions $P, Q$ and, denote by $\varepsilon(\delta) \coloneqq \underset{x > 0}{\inf}(\delta(x) < \delta)$. Given $\eps > 0$, if $\varepsilon(\delta) \le \eps$ and $\RenyiDiv{\alpha}{P}{Q} > \eps$, then $\alpha^{*} < \alpha$.
\end{claim}
I direct implication of this Lemma is that searching on monotonically increasing values of $\alpha$ and using the best bound on $\eps$ achieved at any point to check the relevancy of $\alpha$, we don't have to compute many values of $\alpha$ greater than $\alpha^{*}$ before we stop.

\begin{proof}
    Denote by $\gamma_{\delta}(\alpha)$ the bound on $\eps$ achieved using $\RenyiDiv{\alpha}{P}{Q}$. From Proposition 12 in \citet{CKS20}, $\gamma_{\delta}(\alpha) = \RenyiDiv{\alpha}{P}{Q} + \phi(\alpha)$ for a non negative $\phi$ (except for the range $\alpha > 1/(2 \delta)$ which provides a vacuous bound). Since $\RenyiDiv{\alpha}{P}{Q}$ is monotonically non-decreasing in $\alpha$ we have for any $\alpha' \ge \alpha$, 
    \[
        \gamma_{\delta}(\alpha') \ge \RenyiDiv{\alpha'}{P}{Q} \ge \RenyiDiv{\alpha}{P}{Q} \ge \eps,
    \]
    so it cannot provide a better bound on $\alpha$.
\end{proof}

\section{Direct analysis and simulation}
\label{sec:monte-carlo}
For completeness, we state how one can directly estimate the hockey-stick divergence of the entire random allocation scheme. This technique was first presented in the context of the Gaussian mechanism by \cite{CGHLKKMSZ24}.

We first provide an exact expression for the privacy profile of the random allocation scheme.
\begin{lemma}\label{lem:exctGauss}
    For any randomizer $\rand$ and $\eps > 0$ we have, 
    \[
        \delta_{\alloc{t}{\rand}}(\eps) = \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{E}} \left[\left[\frac{1}{t} \sum_{i \in [t]}e^{\ell(\resRV_{i}; *, \bot)} - e^{\eps}  \right]_{+} \right], 
    \]
    where $\rand^{t}(\bot)$ denotes $t$ repeated calls to $\rand(\bot)$.\footnote{Using Monte Carlo simulation to estimate this quantity, is typically done using the $\underset{\omega \sim P}{\mathbb{E}} \left[\left[1 - \alpha e^{-\ell(\omega; P, Q)} \right]_{+} \right]$ representation of the hockey-stick divergence, so that numerical stability can be achieved by bounding the estimates quantity $\in [0,1]$.}
    
    Given $\sigma > 0$, if $N_{\sigma}$ is a Gaussian mechanism  with noise scale $\sigma$ we have, 
    \[
        \delta_{\alloc{t}{N_{\sigma}}}(\eps) = \underset{\boldsymbol{Z} \sim \mathcal{N}(\bar{0}, \sigma^{2} I_{t})}{\mathbb{E}} \left[\left[\frac{1}{t} \sum_{i \in [t]} e^{\frac{2 Z_{i} - 1}{2 \sigma^{2}}} - e^{\eps} \right]_{+} \right]
    \]
\end{lemma}

This quantity can be directly estimated using Monte Carlo simulation, and \cite{CGHLKKMSZ24} proposed several improved sampling methods in terms of run-time and stability.

\Arxiv{
We note that up to simple algebraic manipulations, this hockey-stick divergence is essentially the expectation of the right tail of the sum of $t$ independent ln-normal random variables, which can be approximated as a single ln-normal random variable \citep{MWMZ07}, but this approximation typically provide useful guarantees only for large number of steps. Instead, we use two different techniques to provide provable bounds for this quantity.
}

\begin{proof}
    Denote by $I$ the index of the selected allocation. Notice that for any $i \in [t]$ we have, 
    \[
        P_{\alloc{t}{\rand}}(\view \vert *, I=i) = \left(\prod_{j=1}^{i-1} P_{\rand}(\res_{j} \vert \bot, \boldsymbol) \right) P_{\rand}(\res_{i} \vert *) \left(\prod_{j=1}^{i-1} P_{\rand}(\res_{j} \vert \bot) \right) = P_{\alloc{t}{\rand}}(\view \vert \bot) \cdot \frac{P_{\rand}(\res_{i} \vert *)}{P_{\rand}(\res_{i} \vert \bot)}
    \]

    \[
        \Rightarrow P_{\alloc{t}{\rand}}(\view \vert *) = \frac{1}{t} \sum_{i \in [t]} P_{\alloc{t}{\rand}}(\view \vert *, I=i) = \frac{1}{t} P_{\alloc{t}{\rand}}(\view \vert \bot) \sum_{i \in [t]}  \frac{P_{\rand}(\res_{i} \vert *)}{P_{\rand}(\res_{i} \vert \bot)} 
    \]
    
    Using this identity we get, 
    \[
        \ell(\view; *, \bot) = \ln \left(\frac{P_{\alloc{t}{\rand}}(\view \vert x)}{P_{\alloc{t}{\rand}}(\view \vert \bot)} \right) = \ln \left(\frac{1}{t} \sum_{i \in [t]} \frac{P_{\rand}(\res_{i} \vert *)}{P_{\rand}(\res_{i} \vert \bot)} \right) = \ln \left(\frac{1}{t} \sum_{i \in [t]} e^{\ell(\resRV_{i}; x, \bot)} \right).
    \]
    Plugging this into the definition of the hockey-stick divergence completes the proof of the first part.


    The second part is a direct result of the fact the dominating pair of the random allocation scheme of the Gaussian mechanism is $1$ vs. $\bot$, and that in the case of the Gaussian mechanism $\ell(\res ; 1, 0) = \frac{2 \res_{i} - 1}{2 \sigma^{2}}$.
\end{proof}

% We present an additional provable bound on the privacy profile  derived in Lemma \ref{lem:exctGauss} using a refined version of advanced composition based on Bernstein's inequality, which we present in the next theorem, and might be of independent interest.

% \begin{theorem}\label{thm:BernComp}
%     Given $\eps_{0}, \delta_{0}, \gamma > 0$, a $(\eps_{0}, \delta_{0})$-DP mechanism $\mech$, if $\boldsymbol{D}_{\chi^{2}}(\mech(x, \view) \Vert \mech(\bot, \view) \le \gamma^{2}$ for any $\view \in \resDom^{*}$ where $\boldsymbol{D}_{\chi^{2}}$ is the Chi-square divergence, then for any $\eps > 0$ and element $x \in \domain$, we have
%     \begin{align*}
%         \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}} & \left(\ell(\viewRV; x, \bot) > \eps \right) \le t \delta_{0} \\
%         & + \exp \left(- t \cdot \min\left\{\frac{(e^{\eps}-1)^{2}}{2 \gamma^{2}}, \frac{e^{\eps}-1}{e^{\eps_{0}}} \right\} \right)
%     \end{align*}
% \end{theorem}

% \begin{proof}
%     If $\mech$ is $(\eps, \delta)$-DP we have
%     \begin{align*}
%         \underset{\viewRV \sim \allocFunc{t}{\mech}{x}}{\mathbb{P}}(\ell(\viewRV; x, \bot) > \eps') & \le e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}(\ell(\viewRV; x, \bot) > \eps') + \delta \\
%         & = e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}\left(\frac{1}{t}\sum_{i=1}^{t} e^{\ell(\resRV_{i}; x, \bot, \view^{i-1})} > e^{\eps'} \right) + \delta \\
%         & = e^{\eps} \underset{\viewRV \sim \rand^{t}(\bot)}{\mathbb{P}}\left(\frac{1}{t}\sum_{i=1}^{t} \left(e^{\ell(\resRV_{i}; x, \bot, \view^{i-1})} - 1 \right) > e^{\eps'} - 1 \right) + \delta \\
%     \end{align*}
    
%     We can now define a sequence of random variables, $D_{i} = \min \left\{e^{\ell(\resRV_{i}; x, \bot, \view^{i-1})}, e^{\eps} \right\} -1$, such that $\resRV_{i} \sim \mech(\bot, \view^{i-1})$.

%     Notice that
%     \[
%         \mathbb{E}[D_{i}] \le \underset{\resRV_{i} \sim \mech(\bot, \view^{i-1})}{\mathbb{E}} \left[\frac{P(\resRV_{i} \vert x, \view^{i-1})}{P(\resRV_{i} \vert \bot, \view^{i-1})} - 1 \right] = 0, 
%     \]
%     and
%     \[
%         \mathbb{E}[D_{i}^{2}] \le \underset{\resRV_{i} \sim \mech(\bot, \view^{i-1})}{\mathbb{E}} \left[\left(\frac{P(\resRV_{i} \vert x, \view^{i-1})}{P(\resRV_{i} \vert \bot, \view^{i-1})} - 1\right)^{2} \right] = \boldsymbol{D}_{\chi^{2}}(\mech(x, \view^{i-1}) \Vert \mech(\bot, \view^{i-1})) \le \gamma^{2}
%     \]
%     From the Definition this is a sub martingale difference sequence, bounded in the range $[-1, e^{\eps} - 1]$ and variance bounded by $\gamma$.
    
%     We can now use Azuma-Bernstein's inequality and get\footnote{Azuma-Bernstein is an unofficial name we use for the folklore inequality version which applies the variance-based Bernstein's inequality to the martingale setting considered by Azuma, which - to the best of our knowledge - is not attributed to any one. See e.g. these \href{https://pillowmath.github.io/Stat 210B/Lec5.pdf }{lecture notes}.}
%     \begin{align*}
%         \underset{\viewRV \sim \mathcal{A}^{t}(\dataset')}{\mathbb{P}} & \left(\frac{1}{t}\sum_{i=1}^{t} \left(e^{\ell(\resRV_{i}; x, \bot, \view^{i-1})} - 1 \right) > e^{\eps'} - 1 \right) \\
%         & \le \underset{\viewRV \sim \mathcal{A}^{t}(\dataset')}{\mathbb{P}} \left(\frac{1}{t}\sum_{i=1}^{t} \left(\min\left\{e^{\ell(\resRV_{i}; x, \bot, \view^{i-1})}, e^{\eps} \right\} - 1 \right) > e^{\eps'} - 1 \right) + t \delta \\
%         & = \underset{D_{1}, \ldots, D_{t}}{\mathbb{P}}\left(\sum_{i=1}^{t} D_{i} > t (e^{\eps'} - 1) \right) \\
%         & \le \exp \left(- \min\left\{\frac{t(e^{\eps'}-1)^{2}}{2 \gamma^{2}}, t \cdot e^{-\eps}(e^{\eps'}-1) \right\} \right) + t \delta_{0} \\
%     \end{align*}
% \end{proof}

% Using the fact that for the Gaussian mechanism $\gamma^{2} = (e^{1/\sigma^{2}}-1)/2$ implies the following corollary.
% \begin{corollary}
%     Given $\sigma > 0$ and a Gaussian mechanism $\mech$ with noise scale $\sigma$, for any $\eps > 0$; $\delta_{0} \in [0, 1]$ we have $\delta_{\alloc{t}{\mech}}(\eps) \le t \delta_{0} + \delta'$, where 
%     \[
%         \delta' =  \exp \left(- t \cdot \min\left\{\frac{2 \left(e^{\eps}-1\right)^{2}}{e^{1/\sigma^{2}}-1}, \frac{e^{\eps}-1}{e^{\frac{\sqrt{2 \ln(1.25/\delta_{0})}}{\sigma}}} \right\} \right).
%     \]
% \end{corollary}


\end{document}