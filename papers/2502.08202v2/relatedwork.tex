\section{Related work}
Our work builds heavily on tools and ideas developed for analysis of privacy amplification by subsampling, composition and shuffling. We have covered the work directly related to ours earlier and will describe some of the tools and their origins in the preliminaries. A more detailed technical and historical overview of subsampling and composition for DP can be found in the survey by \citet{Steinke22}.  The shuffle model was first proposed by \citet{BEMMRLRKTS17}. The formal analysis of the privacy guarantees in this model was initiated in \citep{EFMRTT19, CSUZZ19}.  \citet{EFMRTT19} defined the sequential shuffling scheme that we discuss here and proved the first general privacy amplification results for this scheme albeit only for pure DP algorithms. Improved analyses and extensions to approximate DP were given in \citep{BBGN19, BKMT20, FMT21, FMT23,GDDKS21,KHH22}.

DP-SGD was first defined and theoretically analyzed in the convex setting by \citet{BST14}. Its use in machine learning was spearheaded by the landmark work of 
\citet{ACGMMT16} who significantly improved the privacy analysis via the moments accounting technique and demonstrated the practical utility of the approach. In addition to a wide range of practical applications, this work  has motivated the development of more advanced techniques for analysis of sampling and composition. At the same time most analyses used in practice still assume Poisson subsampling when selecting batches whereas some type of shuffling is used in implementation. It was recently shown that it results in an actual difference between the reported and true privacy level in some regimes \citep{CGKKMSZ24a, CGKKMSZ24b, ABDCH24}.

In a concurrent and independent work \citet{DCO25} considered the same sampling method (referring to it as \emph{Balanced Iteration Subsampling}). They provide RDP-based bounds for the same dominating pair of distributions in the Gaussian case. 
 Their bound for general $k$ is incomparable to ours as it is based on a potentially loose upper bound for divergences of order $\alpha > 2$, while using an exact extension of their approximation to $k > 1$. In contrast, our RDP-based bound uses a reduction from general $k$ to $k=1$ that is potentially loose but our computation for the $k=1$ case is exact. We discuss these differences in more detail and provide numerical comparison in Appendix \ref{apd:comp_loose_RDP}.

%This gap persists throughout most implementations considered since \citep{PHKXDMVCT23}, and was lately proven to result in 
%significant  and progress  

%first proposed DP-SGD, a variant of SGD where at each iteration the sum of (clipped) gradients is released with additional Gaussian noise. Their privacy analysis relies on Poisson subsampling, though - as they pointed out, most implementation use shuffle instead. This gap persists throughout most implementations considered since \citep{PHKXDMVCT23}, and was lately proven to result in an actual difference between the reported and true privacy level in some regimes \citep{CGKKMSZ24a, CGKKMSZ24b, ABDCH24}.

%Random allocation is a sampling version that mostly ignored until lately. It can be seen as a special case of \emph{random check-ins} \citep{BKMT20} with $p_{i} = 1$ and $\resRV_{i} = [t]$ for all users, but the analysis provided in this work essentially relies on the privacy analysis of the shuffle scheme. Recently, \citet{CGHLKKMSZ24} proposed a DP-SGD flavor with this sampling scheme which they refer to as \emph{bins and balls}, but their analysis is limited to the Gaussian mechanism and a single allocation, and does not provide provable bounds but instead relies on a randomized bound \citep{WMWJM24}.