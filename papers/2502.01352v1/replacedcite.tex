\section{Related work}
\label{sec:sota}

The incorporation of DP in data science ecosystems, especially in relation to the training of ML/DL models, is a topic that has been extensively studied in the field of privacy-preserving machine learning ____. Regarding the training of deep learning models (more specifically deep neural networks), the use of DP during the model training has been explored in multiple works following the differentially private stochastic gradient descent (DP-SGD) approach ____, which consists of adding noise during the process of stochastic gradient descent. This is implemented in different Python libraries, like TensorFlow Privacy ____ or Opacus ____. 

FL is susceptible to privacy attacks, despite the fact that each client trains their model locally, without revealing their local dataset.
Among various inference attacks, the Membership Inference Attack (MIA)  ____ determines whether a particular record is part of a dataset or not.  To do so, it is assumed that the adversary (central server) owns a so-called shadow model of the target client, which is similar to the target clientâ€™s model (i.e. follows the same distribution) but is trained on a different, disjoint, dataset.
MIAs are widely employed in the literature for evaluating privacy risks ____.

The Source Inference Attack (SIA)____ is a more advanced inference attack. An honest-but-curious server in FL attempts to identify exactly which client owns a specific data point used for training. To estimate the source of the data point, the server leverages the prediction losses of local models on particular data points by using a Bayesian approach. 


Among reconstruction attacks, arguably the most well-known is Deep Leakage from Gradients (DLG) ____. DLG usually exploits the shared gradients during the training process in federated learning to reconstruct sensitive training data. This attack shows that even gradient information can leak important details about the original data. In both reconstruction and inference attacks, applying DP is crucial to protect sensitive information of the participating clients.

However, in FL adversaries may also take the form of participants or other external actors with access to the (shared) aggregated model. In the case of an FL architecture where the server that orchestrates the training is not trusted, clients can add DP during training by applying DP-SGD, but they can also do so once the model is trained, adding noise to the model updates ____. This will prevent the central server from extracting information from the data used in the training. 

Concerning medical imaging, in ____ the authors apply DP-SGD in a FL architecture for medical image analysis, specifically using histopathology images. On the other hand, several works have explored the incorporation of DP to FL architectures. For example, in ____ DP is added from each client to the model updates before sending the model to the server, specifically in a personalized federated learning approach.

In this work we assume that the server is trusted. The attacker can either be another client who tries to infer information about other clients (e.g. find out if a certain client is participating in the training). Hence the model that we aim to protect is the aggregated (global) one. Therefore, the server can globally add DP to the aggregated model after receiving the updates from all the clients. This will prevent the extraction of information by the clients in each round. Also it will protect the final resulting model when it is made publicly available.
Few works can be found in the literature concerning applying DP from the server side. For example, in ____ local and central DP are compared in a FL setting showing the protection against backdoor attacks, defining central DP (CDP) as the case in which \textit{``the FL aggregation function is perturbed by the server''}. In this work we refer to this approach as \emph{global-DP}.