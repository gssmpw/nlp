\section{Related Works}
Pruning techniques for deep neural networks have been extensively explored to enhance computational efficiency without significantly degrading performance. Traditional approaches primarily focus on weight magnitude-based pruning, where small-magnitude connections are removed to achieve sparsity \cite{han2015learning}. However, such methods often disregard the topological and functional properties of the network, leading to suboptimal pruning results in recurrent architectures.\\

Structured pruning techniques aim to eliminate entire neurons, layers, or modules, maintaining the overall network structure while reducing redundancy \cite{narang2017exploring}. In the case of recurrent neural networks (RNNs), techniques such as gate-based pruning \cite{zhang2018systematic} and block-sparsity methods \cite{wen2016learning} have been proposed to selectively remove unnecessary neurons while preserving temporal dependencies. Recent advances leverage information-theoretic and graph-theoretic insights to identify critical pathways within RNNs, ensuring robust pruning while sustaining performance \cite{mocanu2018scalable}.\\

Graph-based representations of neural networks have gained attention for their ability to capture hierarchical dependencies within layers and neurons \cite{wang2019neural}. By representing neural architectures as graphs, researchers have applied spectral analysis, modularity detection, and community structures to guide pruning strategies. The introduction of lattice-based approaches further refines these representations by incorporating the principles of order theory and dependency structures \cite{su2017lattice}. In particular, lattice models provide a rigorous framework for analyzing neuron interactions, enabling novel pruning mechanisms that consider meet-irreducibility and hierarchical centrality \cite{ritter2003lattice}.\\

Recent work has explored the application of dependency lattices to pruning tasks, particularly in convolutional and recurrent architectures \cite{bardella2024lattice}. The key advantage of lattice-based pruning lies in its ability to preserve functional connectivity while reducing computational costs. By modeling RNNs as partially ordered sets (posets), pruning can be guided by structural constraints that maintain critical dependencies. Prior studies have demonstrated that this approach yields superior performance compared to conventional magnitude-based pruning, especially in multilayer networks with top-down feedback \cite{tsotsos2014cognitive, tsotsos2021computational}.\\