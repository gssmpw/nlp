\begin{figure*}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{figures/diagram.jpg}
  \caption{\textbf{Overview of {\ours}.}  (a) We fine-tune the pre-trained T2V diffusion model (T2V-DM) using the \emph{motion feature matching} objective. Unlike the standard \emph{pixel-level} DDPM loss, we align the motion features of the predicted noisy video $\pd$ with those of the ground truth noisy video $\gt$. To extract motion features from \emph{noisy latent videos}, we use a pre-trained T2V-DM (frozen) as a feature extractor. (b) We leverage the cross-attention (CA) maps and temporal self-attention (TSA) maps in the pre-trained T2V diffusion model to extract motion cues. The final motion features are the combination of the CA maps and TSA maps.}
  \label{fig:diagram}
\end{figure*}