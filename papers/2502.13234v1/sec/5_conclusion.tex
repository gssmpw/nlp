\section{Conclusion}
\label{sec:conclusion}

We presented \ours, a feature-level fine-tuning framework for motion customization. {\ours} transforms the \emph{pixel-level} DDPM objective into the \emph{motion feature matching} objective, aiming to learn the target motion at the \emph{feature level}. To extract motion features, {\ours} leverages the pre-trained T2V diffusion model as a deep feature extractor and identify valuable motion cues from two attention mechanisms within the model, representing both object movements and camera framing in videos. In the experiments, {\ours} demonstrated superior joint controllability of text and motion to prior approaches. These results suggest that {\ours} enhances control over scene staging in AI-generated videos, benefiting  real-world applications in computer-generated imagery (CGI). For a discussion of {\ours}'s limitations, please refer to the supplementary material.