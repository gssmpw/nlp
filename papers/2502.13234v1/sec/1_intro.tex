\section{Introduction}
\label{sec:intro}

%-------------------------------------------------------------------------
To control the rhythm of a movie scene, movie directors would carefully arrange the precise movements and positioning of both the actors and the camera for each shot (as known as staging/blocking). Similarly, to control the pacing and flow of AI-generated videos, users should have control over the dynamics and composition of videos produced by generative models. To this end, numerous motion control methods~\cite{dragnuwa, mctrl, draga, mb, trailblazer, peekaboo, boximator} have been proposed to control moving object trajectories in videos generated by text-to-video (T2V) diffusion models~\cite{vdm, vldm}. Motion customization, in particular, aims to control T2V diffusion models with the motion of a reference video~\cite{mc, md, vmc, dmt, sma}. With the assistance of the reference video, users are able to specify the desired object movements and camera framing in detail. Formally speaking, given a reference video, motion customization aims to adjust a pre-trained T2V diffusion model, so the output videos sampled from the adjusted model follow the object movements and camera framing of the reference video (see \cref{fig:teaser} for an example). Given that motion is a high-level concept involving both spatial and temporal dimensions~\cite{moft, dmt}, motion customization is considered a non-trivial task.

%-------------------------------------------------------------------------
Recently, many motion customization methods have been proposed to eliminate the influence of visual appearance in the reference video. Among them, a standout strategy is fine-tuning the pre-trained T2V diffusion model to reconstruct the frame differences of the reference video. For instance, VMC~\cite{vmc} and SMA~\cite{sma} use a motion distillation objective that reconstructs the residual frames of the reference video. MotionDirector~\cite{md} proposes an appearance-debiased objective that reconstructs the differences between an anchor frame and all other frames. However, we find that frame differences do not accurately represent motion. For example, two videos with the same motion, such as a red car and a blue car both driving leftward, can yield completely different frame differences because the pixel changes occur in different color channels in each video. Moreover, since frame differences only process videos at the pixel level, they cannot capture complex motion that requires a high-level understanding of video, such as rapid movements or movements in low-texture regions. In these cases, the strategy of reconstructing frame differences fails to reproduce the target motion.

%-------------------------------------------------------------------------
To address this issue, we propose {\ours}, a novel fine-tuning framework for motion customization via motion feature matching. Instead of aligning pixel values or frame differences as in previous methods, {\ours} aligns the projected motion features extracted from a pre-trained feature extractor. Since these motion features are calculated with a sophisticated pre-trained model, they are capable of capturing complex motion that requires a high-level, spatio-temporal understanding of video. This effectively addresses the limitation of previous work, where frame differences fail to capture complex motion.

%-------------------------------------------------------------------------
{\ours} differs from traditional fine-tuning approaches. At each fine-tuning step, it starts off by using a feature extractor to compute the motion features of the output video and the motion features of the reconstruction ground truth video. Our feature matching objective then minimizes the L2 distance between the two feature vectors. However, since the output videos of T2V diffusion models are in latent space and at certain noise levels, the feature extractor must be able to process latent noisy videos. To obtain such a feature extractor, we take advantages of (1) pre-trained T2V diffusion models' ability in extracting features from noisy, latent videos and (2) the spatio-temporal information encoded in attention maps. We find that cross-attention maps (CA) in pre-trained diffusion models contain information about camera framing, while temporal self-attention maps (TSA) represent object movements. Therefore, we utilize them to represent motion features. Ultimately, the design of our framework is validated through detailed analysis and extensive experiments.

%-------------------------------------------------------------------------
To summarize, our key contributions include:
\begin{itemize}
  \item We propose {\bf\ours}, a feature-level fine-tuning framework for motion customization. It leverages a pre-trained feature extractor to map videos into a motion feature space, capturing high-level motion information. By aligning the motion features, the diffusion model learns to generate videos with the target motion.
  \item To extract features from \emph{noisy latent videos}, we utilize the pre-trained diffusion model as a feature extractor, as it naturally processes such inputs.
  \item We identify two sources of motion cues---cross-attention maps and temporal self-attention maps---and use them to form the motion features.
  \item We demonstrate that {\ours} achieves state-of-the-art performance through comprehensive experiments. It offers superior joint controllability of text and motion, advancing scene staging in AI-generated videos.
\end{itemize}