\section{Method}
\label{sec:method}

\input{figures/diagram}

%-------------------------------------------------------------------------
\paragraph{Problem formulation}
To control scene staging in AI-generated videos, we tackle the problem of motion customization, specifically as defined in DMT~\cite{dmt}. Given a reference video $z_0$ and a text prompt $y$ associated with it, we aim to adjust a pre-trained T2V diffusion model $\epsilon_{\theta}$, so that the output videos sampled from the adjusted model replicate both the \emph{object movements} and \emph{camera framing} in $z_0$.

%-------------------------------------------------------------------------
\subsection{Preliminary: Text-to-video diffusion models}

Text-to-video (T2V) diffusion models are probabilistic generative models that synthesize videos by gradually denoising a sequence of randomly sampled Gaussian noise frames (in latent space), guided by a textual condition $y$.

\paragraph{Architecture}
To model temporal information, T2V diffusion models typically inflate a pre-trained text-to-image (T2I) diffusion model by inserting temporal layers. These temporal layers are made up of feedforward networks and temporal self-attentions, where \emph{temporal self-attentions} (TSA) apply self-attention along the frame axis.

\paragraph{Training}
T2V diffusion models $\epsilon_{\theta}$ are trained by minimizing a weighted noise-prediction objective:
\begin{equation}
    \E\left[w_t\left\|
\epsilon-\epsilon_{\theta}(z_t, t, y)
\right\|^2\right]
  \label{eq:ddpm_eps},
\end{equation}
where $z_t=\sqrt{\bar\alpha_t}z_0+\sqrt{1-\bar\alpha_t}\epsilon$ is the noised video at timestep $t$, $\epsilon\sim\mathcal N(\bf{0},\bf{I})$ is Gaussian noise, and $w_t$ is a time-dependent weighting term. This noise-prediction objective is also equivalent to predicting the previous noised video at timestep $t-1$ through a different parametrization~\cite{ddpm}:
\begin{equation}
    \E\left[w'_t\left\|
v_t(z_t,\epsilon)-v_t(z_t,\epsilon_{\theta}(z_t, t, y))
\right\|^2\right]
  \label{eq:ddpm_v},
\end{equation}
where $v_t(z_t,\epsilon):=\frac 1 {\sqrt{\alpha_{t}}}z_t+\left(-\frac {\sqrt{1-\bar\alpha_t}} {\sqrt{\alpha_{t}}}+\sqrt{1-\bar\alpha_{t-1}}\right)\epsilon$ is a function that estimates the previous noised video $z_{t-1}$ based on the current video state $z_t$ and noise $\epsilon$, and $w'_t$ is the time-dependent weight after reparametrization (See supplementary material for more details). For simplicity, we will use $\pd$ to denote the model prediction $v_t(z_t,\epsilon_{\theta}(z_t, t, y))$, and use $\gt$ to denote the ground truth $v_t(z_t,\epsilon)$. The objective can therefore be rewritten as:
\begin{equation}
    \E\left[w'_t\left\|
\gt-\pd
\right\|^2\right]
  \label{eq:ddpm_sv},
\end{equation}
where $w'_t$ is the time-dependent weight in \cref{eq:ddpm_v}.

%-------------------------------------------------------------------------
\subsection{Learning motion at the feature level}

Identifying motion in video requires a \emph{high-level} understanding of both the spatial and temporal aspects of the video, so using the standard \emph{pixel-level} DDPM reconstruction loss (\cref{eq:ddpm_sv}) for motion customization cannot accurately learn motion, and may introduce irrelevant information, such as content and visual appearance.

To this end, we introduce the \emph{motion feature matching} objective, where a deep feature extractor $\mfe$ is used to extract motion information from videos at a high level. Instead of directly aligning the predicted noisy video $\pd$ with the ground truth $\gt$ at the pixel level, we align their high-level motion features (extracted by $\mfe$):
\begin{equation}
    \mathcal{L}_{\rm mot}(\theta)=\E\left[w'_t\left\|
\mfe(\gt)-\mfe(\pd)
\right\|^2\right],
  \label{eq:mfm}
\end{equation}
where $\mfe$ is a motion feature extractor for \emph{noisy latent videos}, and $w'_t$ is the time-dependent weight in \cref{eq:ddpm_sv}. As illustrated in \cref{fig:diagram}(a), this \emph{motion feature matching} objective aims to minimize the L2 discrepancy between the two videos in the motion feature space, ensuring that the motion in output video matches the motion in the reference video.

However, designing the motion feature extractor $\mfe$ in \cref{eq:mfm} is non-trivial, as it needs to extract features from \emph{noisy latent videos}. First of all, most feature extractors, such as ViViT~\cite{vivit}, EfficientNet~\cite{eff}, DenseNet-201~\cite{densenet}, and ResNet-50~\cite{resnet}, are trained on clean visual data, so we cannot directly applied them to noisy videos. Secondly, since the videos $\gt$ and $\pd$ in \cref{eq:mfm} are in latent space, our feature extractor must be designed to process \emph{latent videos} directly. Otherwise, we would need to decode them back into pixel-space videos before applying off-the-shelf feature extractors. This would incur substantial computational and memory overhead during training, due to both backpropagation through the large VAE decoder and the cost of processing ``full-resolution'' videos. 

Here we claim that the pre-trained T2V diffusion model serve as a proper feature extractor for \emph{noisy latent videos}. Firstly, recent work has shown both theoretically and experimentally that pre-trained diffusion models are capable of extracting high-level semantics and structural information from visual data, making them a ``unified feature extractor''~\cite{ins1, ins2}. Secondly, since diffusion models are trained on \emph{noisy latent inputs}, using them as feature extractors for \emph{noisy latent videos} helps prevent a training-inference gap. For these reasons, {\ours} leverages the \textbf{pre-trained T2V diffusion model} as the motion feature extractor $\mfe$.

%-------------------------------------------------------------------------
\subsection{Extracting motion cues from diffusion models}

In this section, we identify the locations within the intermediate layers of diffusion models from which motion-specific features can be extracted.

\paragraph{Extracting cues for camera framing}
Recent studies have shown that the cross-attention (CA) maps in diffusion models closely reflect the spatial arrangement of objects within the frame~\cite{boxdiff, db, trailblazer, peekaboo, direct}. Building on this, we leverage the CA maps from T2V diffusion models to describe the composition of each video frame (see \cref{fig:diagram}(b)), thereby determining the camera framing throughout the video (\eg, shot size and composition).

Formally speaking, CA maps are calculated by first reshaping the intermediate 3D activations $\Phi\in\mathbb{R}^{H\times W\times F\times D}$ into the shape $(H\times W\times F)\times D$, where $F$, $H$, $W$, and $D$ denote the number of frames, height, width, and depth of the activations. Cross-attention is then performed between the activations $\Phi$ and word embeddings $\tau(y)$ as follows :
\begin{equation}
\mca=\mathrm{Softmax}\left(\frac{Q(\Phi)K(\tau(y))^T}{\sqrt{D}}\right)
\label{eq:features},
\end{equation}
where $\tau$ denotes the text encoder used in the T2V diffusion model, and $y$ is the text prompt given by the user. In $\mca\in\mathbb[0,1]^{F\times H\times W \times |c|}$, each element $(\mca)_{i,j,k,l}$ represents the correlation between the spatial-temporal coordinate $(i,j,k)$ and the $l$'th word in the text prompt. As shown in \cref{fig:mca}, $\mca$ highlights the region within the frame that corresponds to an object. It focuses on structural information and eliminates visual appearance.

\paragraph{Extracting cues for object movements}
Since cross-attention maps cannot describe motion that does not involve spatial shifts (\eg, rotation and non-rigid motion), it is crucial to extract additional cues to represent such object movements. Since we discover that the temporal self-attention (TSA) maps in T2V diffusion models can capture detailed object movements, we also incorporate them into the motion features (see \cref{fig:diagram}(b)).

To compute temporal self-attention (TSA) maps $\mtsa$, we begin by reshaping the model's intermediate 3D activations $\Phi\in\mathbb{R}^{H\times W\times F\times D}$ into the shape $(H\times W)\times F\times D$. For each particular spatial coordinate $(i,j)$, we compute the self-attention weights between frames as follows:
\begin{equation}
(\mtsa)_{i,j}=\mathrm{Softmax}\left(\frac{Q(\Phi_{i,j})K(\Phi_{i,j})^T}{\sqrt{D}}\right)
\label{eq:features},
\end{equation}
where $i$ and $j$ denote the spatial coordinates. Specifically, each element $(\mtsa)_{i,j,k,l}$ of the TSA map $\mtsa\in [0,1]^{H\times W\times F\times F}$ represents the degree of relevance between the $k$'th and $l$'th frames at the spatial coordinate $(i,j)$, capturing the dynamics of the video. As visualized in ~\cref{fig:mtsa}, the darker regions, which indicate low correlation between frames, correspond closely to areas where significant changes occur between the two frames. Therefore, by collecting the TSA maps for all $F\times F$ frame pairs, we can capture the inter-frame dynamics in detail.

With the cross-attention maps capturing camera framing, and the temporal self-attention maps reflecting object movements, we combine both to form the motion features:
\begin{equation}(\wca\mca)\oplus (\wtsa\mtsa)
\label{eq:features},
\end{equation}
where $\wca$ and $\wtsa$ are weights that control the contributions of each component.

\input{figures/mca}
\input{figures/mtsa}
\input{figures/qualitative}

%-------------------------------------------------------------------------
\subsection{Motion-aware LoRA fine-tuning}

After extracting the motion features, we fine-tune the pre-trained T2V diffusion model using the \emph{motion feature matching} objective in \cref{eq:mfm}. By aligning the $\mca$ component, we ensure that the \emph{camera framing} in the generated video matches that of the reference video, and aligning $\mtsa$ ensures that the \emph{dynamics} in the generated video align with those of the reference video.

To preserve the model's pre-trained knowledge while fine-tuning, we apply low-rank adaptations (LoRAs)~\cite{lora} to fine-tune the model with fewer trainable parameters: 
\begin{equation}
    \argmin_{\Delta\theta} \mathcal{L}_{\rm mot}(\theta+\Delta\theta),
\end{equation}
where $\Delta\theta$ is a low-rank parameter increment.
Having these motion-aware LoRAs, {\ours} is capable of synthesizing videos that are guided by both the textual description and the motion in the user-provided reference video.