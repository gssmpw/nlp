\section{Experiments}
\label{sec:experiments}

%-------------------------------------------------------------------------
\subsection{Experiment setup}
\label{sec:setup}

\paragraph{Dataset}
To evaluate {\ours}'s ability to transfer motion from a reference video to a new scene, we collect a dataset of 42 video-text pairs. These videos encompass a wide range of motion types, such as fast object movement, rotation, non-rigid motion, and camera movement. We also ensure that the scenes in the editing text prompts are distinct from the scene in the reference video while remaining compatible with its motion.

\paragraph{Implementation details}
 For a fair comparison, we use Zeroscope~\cite{zeroscope} as the base T2V diffusion model across all methods, given its ability to model complex motion and widespread usage in previous work~\cite{md,dmt,sma}. We fine-tune the model with LoRA~\cite{lora} for 400 steps at a learning rate of 0.0005. To extract motion features, we obtain attention maps $\mca$ and $\mtsa$ from down\_block.2, with weights $\wca$ and $\wtsa$ both set to 2000. These hyperparameters are chosen to balance control over camera framing and object movements. After extracting features from intermediate layers, we stop the forward pass to avoid unnecessary computation. For further implementation details, please refer to the supplementary material.

\paragraph{Baselines}
We compare our method against four recent approaches to motion customization, including two fine-tuning methods---VMC~\cite{vmc} and MotionDirector~\cite{md}---and two training-free methods---DMT~\cite{dmt} and MotionClone~\cite{mc}. Detailed descriptions of these methods are provided in \cref{sec:motion_customization}.

\input{figures/human_study}
\input{figures/quantitative}
\input{figures/trade-off}

%-------------------------------------------------------------------------
\subsection{Evaluation metrics}
\label{sec:metrics}

We use four automatic metrics to evaluate the effectiveness of motion customization:
(1) {\bf CLIP-T:} To measure text alignment, we calculate the average CLIP~\cite{clip} cosine similarity between the text prompt and all output frames.
(2) {\bf Frame consistency:} We compute the average CLIP cosine similarity between each pair of consecutive frames to assess frame consistency.
(3) {\bf ImageReward:} We calculate the average ImageReward~\cite{ir} score for each frame, which evaluates both text alignment and image quality based on human preference.
(4) {\bf Motion discrepancy:} To quantify motion similarity between reference videos and generated videos, we leverage CoTracker3~\cite{cotracker3}, a state-of-the-art point tracker that densely tracks the motion trajectories of 2D points throughout a video. Specifically, we use CoTracker3 to generate $N$ 2D point trajectories for the reference video, denoted as $\hat{T}_0, \hat{T}_1,\cdots,\hat{T}_N\in \mathbb{R}^{F\times 2}$, and $N$ 2D point trajectories for the generated video, denoted as $T_0, T_1,\cdots,T_N\in \mathbb{R}^{F\times 2}$. To measure the similarity between these two sets of $F\times 2$ dimensional vectors, we use the Chamfer distance, a metric commonly used to assess the similarity between two sets of points in point cloud generation~\cite{pcg1,pcg2, pcg3, pcg4}. Accordingly, the {\it motion discrepancy} score is defined as:
\begin{equation}
C\left(\frac 1 N \sum_{i} \min_j \left\|T_i-\hat{T}_j\right\|^2 + \frac 1 N \sum_{j} \min_i \left\|T_i-\hat{T}_j\right\|^2\right)
\label{eq:motion_alignment},
\end{equation}
where $C=\frac 1 {2FHW}$ is a normalization constant.

%-------------------------------------------------------------------------
\subsection{Main results}
\label{sec:quantitative}

\paragraph{Quantitative results}
The quantitative results are reported in \cref{tab:quantitative}. Our method outperforms all baseline approaches in metrics such as CLIP-T, frame consistency, and ImageReward, demonstrating its superiority in preserving the prior knowledge in the base model during fine-tuning.

We also visualize the trade-off between text controllability and motion controllability in \cref{fig:trade-off}. As shown, our method provides significantly better joint controllability of both text and motion than existing motion customization approaches.

%-------------------------------------------------------------------------
\paragraph{Qualitative results}
In \cref{fig:qualitative}, we present qualitative comparisons with baseline approaches across various types of motion. In the first example, only our method successfully reproduces the fast displacement in the reference video, confirming the effectiveness of our motion feature extractor in capturing complex motion. In the second example, VMC and MotionClone misposition the object within the frame, whereas MotionDirector and DMT fail to generate realistic videos complying with the text prompt. In contrast, our method faithfully follows the text prompt and places the object correctly. In the third and forth examples, our method also exhibits superior visual and motion quality.

These results conclude that our method preserves \emph{the most} pre-trained knowledge during fine-tuning, while providing \emph{the strongest} controllability for complex motion. For more results, please refer to \cref{fig:teaser} and the appendix.

\section{Ablation study}
\label{sec:ablation}

We conduct an ablation study to examine the impact of incorporating $\mca$ and $\mtsa$ in motion features. As illustrated in \cref{fig:ablation}, without cross-attention maps $\mca$, the model struggles to correctly position all the element of the scene. Meanwhile, removing temporal self-attention maps $\mtsa$ reduces the precision of fine-grained dynamics. The quantitative results in \cref{tab:ablation_quant} further validate the importance of both attention maps in controlling motion. These results confirm that both the \emph{camera framing}, informed by $\mca$, and \emph{inter-frame dynamics}, informed by $\mtsa$, are essential for capturing overall motion.

\subsection{Human user study}

For a more accurate evaluation, we conduct a user study comparing our method with existing approaches based on human preferences. Following previous work~\cite{md,dmt}, we adopt the Two-alternative Forced Choice (2AFC) protocol. In the survey, the participants are presented with one video generated by our method and another video generated by a baseline approach. They are asked to compare the videos across three key aspects of motion customization:
(1) {\bf Video quality:}  the degree to which the output video appears realistic and visually appealing,
(2) {\bf Text alignment:} how well the output video matches the text prompt, and
(3) {\bf Motion alignment:} the similarity in motion between the output video and the reference video. Ultimately, we collected 192 human evaluations per baseline and metric, totaling 2,304 human evaluations. These responses were gathered from 24 participants recruited via the Prolific platform.

As shown in \cref{fig:human_study}, human users prefer our method over existing approaches in all aspects. These results further confirm the superiority of our method.

\input{figures/ablation}
\input{figures/ablation_quant}