\section{Related work}
\label{sec:related_work}

%-------------------------------------------------------------------------
\subsection{Text-to-video generation}
\label{sec:t2v}

Text-to-video (T2V) generation models aim to synthesize videos that comply with user-provided text descriptions. Previously, a large number of T2V models have been proposed, including GANs~\cite{gan1, gan2, gan3, gan4}, autoregressive models~\cite{ar1, ar2, ar3, ar4}, and diffusion models~\cite{vdm, vldm, cogvideo}.

Following the success of text-to-image (T2I) diffusion models~\cite{dm1, dm2, dm3}, researchers have also put considerable effort into training T2V diffusion models recently. To achieve this, a commonly used approach is inflating a pre-trained T2I diffusion model by inserting temporal layers and finetuning the whole model on video data~\cite{modelscope, videocrafter, latentvdm, imagen, makeavideo, show1, lavie}. On the other hand, models like AnimateDiff~\cite{animatediff} and VideoLDM~\cite{vldm} also insert additional temporal layers, but they only finetune the newly-added temporal layers for decoupling purposes. In contrast to the first approach, these models are typically limited to generating simple motion~\cite{animatezero}. To ensure motion complexity, we adopt the former type of model as the base model in this work.

%-------------------------------------------------------------------------
\subsection{Motion control in T2V generation}
\label{sec:motion_control}

To enable detailed control over camera framing and object movements in T2V generation, recent research has explored trajectory-based~\cite{dragnuwa, mctrl, draga, moft}, box-based~\cite{mb, trailblazer, peekaboo, boximator}, and reference-based motion control. Trajectory-based and box-based motion control are typically achieved by conditioning T2V diffusion models on additional motion signal and training them on large video datasets~\cite{dragnuwa, draga, mctrl, boximator}, or by directly manipulating attention maps at the inference stage~\cite{mb, trailblazer, peekaboo}. However, these approaches require users to explicitly define the trajectories of moving objects within frames, which is usually laborious and provides limited control over the entire scene. In contrast, reference-based motion control can specify the target motion more comprehensively via a reference video~\cite{mc, md, vmc, dmt, sma}. In this work, we focus on motion customization, which is considered reference-based motion control.

%-------------------------------------------------------------------------
\subsection{Motion customization of T2V diffusion models}
\label{sec:motion_customization}

Recently, motion customization has emerged as a new area of research. It adapts the pre-trained T2V diffusion model to generate videos that replicate the camera framing and object movements of a user-provided reference video. To avoid learning visual appearance, VMC~\cite{vmc} and SMA~\cite{sma} fine-tune the pre-trained T2V diffusion model by aligning the residual frames of the output video with the residual frames of the reference video. MotionDirector~\cite{md} proposes a dual-path fine-tuning method to avoid learning visual appearance and simultaneously utilizes an objective that matches frame differences. However, since frame differences do not accurately represent motion, these methods struggle to replicate complex motion.

Another strategy is using diffusion guidance~\cite{cfg, dsg, dragon} to achieve controllable generation. Specifically, DMT~\cite{dmt} employs the intermediate spatio-temporal features in diffusion models as a guidance signal, whereas MotionClone~\cite{mc} uses intermediate temporal attention maps for guidance. Despite being training-free, these methods need to compute additional gradients during inference, resulting in a lengthy sampling process. Moreover, as noted in~\cite{gw1, gw2}, the large guidance weights used in diffusion guidance can lead to the generation of out-of-distribution samples.

While other motion customization approaches exist, they address different tasks. For instance, DreamVideo~\cite{dreamvideo} and Customize-A-Video~\cite{cav} focus solely on replicating object movements without preserving the camera framing, whereas MotionMaster~\cite{mm} deals exclusively with camera movements. In contrast, our method provides control over both object movements and camera framing.